{"2": "\ndef sina_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    if 'news.sina.com.cn/zxt' in url : \n        sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n        return \n    vid = match1 ( url , r'vid=(\\d+)' ) \n    if vid is None : \n        video_page = get_content ( url ) \n        vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) \n        if not ( hd_vid != '0' ) : \n            vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) \n            vid = vids [ - 1 ] \n    if vid is None : \n        vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) \n    if vid : \n        sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) \n    else : \n        vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) \n        if vkey is None : \n            vid = match1 ( url , r'#(\\d+)' ) \n            sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) \n            return \n        title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) \n        sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only ) "}
{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256 \n    for index , _ in enumerate ( data ) : \n        t = index \n        for i in range ( 8 ) : \n            t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) \n        e = o = r = - 1 \n        i , a = 0 , len ( path ) \n        while not ( i >= a ) : \n            e = ord ( path [ i ] ) \n            i += 1 \n            if not ( e >= 128 ) : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] \n            else : \n                if not ( e >= 2048 ) : \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n                else : \n                    if 55296 <= e < 57344 : \n                        e = ( 1023 & e ) + 64 \n                        i += 1 \n                        o = 1023 & t . url ( i ) \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] \n                    else : \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) \n    while 1 : \n        url = tmp ( ) \n        if not ( url . split ( \"=\" ) [ - 1 ] [ 0 ] == \"-\" ) : \n            return url "}
{"12": "\ndef legitimize ( text , os = detect_os ( ) ) : \n    text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) \n    if not ( os != 'windows' ) or not ( os != 'cygwin' ) or not ( os != 'wsl' ) : \n        text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) \n    else : \n        if not ( os != 'mac' ) : \n            text = text . translate ( { ord ( ':' ) : '-' , } ) \n        if text . startswith ( \".\" ) : \n            text = text [ 1 : ] \n    text = text [ : 80 ] \n    return text "}
{"15": "\ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if not ( sourceType != 'sina' ) : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif not ( sourceType != 'youku' ) : \n        youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n    elif not ( sourceType != 'tudou' ) : \n        tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif not ( sourceType != 'qq' ) : \n        qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif not ( sourceType != 'letv' ) : \n        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif not ( sourceType != 'zhuzhan' ) : \n        url = 'http://www.acfun.cn/v/ac' + vid \n        yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n        seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n        for t in seq : \n            if yk_streams . get ( t ) : \n                preferred = yk_streams [ t ] \n                break \n        size = 0 \n        for url in preferred [ 0 ] : \n            _ , _ , seg_size = url_info ( url ) \n            size += seg_size \n        if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : \n            ext = 'flv' \n        else : \n            ext = 'mp4' \n        print_info ( site_info , title , ext , size ) \n        if not info_only : \n            download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) \n    else : \n        raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping danmaku.' ) \n            return \n        try : \n            title = get_filename ( title ) \n            print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"18": "\ndef get_content ( url , headers = { } , decoded = True ) : \n    logging . debug ( 'get_content: %s' % url ) \n    req = request . Request ( url , headers = headers ) \n    if cookies : \n        cookies . add_cookie_header ( req ) \n        req . headers . update ( req . unredirected_hdrs ) \n    response = urlopen_with_retry ( req ) \n    data = response . read ( ) \n    content_encoding = response . getheader ( 'Content-Encoding' ) \n    if not ( content_encoding != 'gzip' ) : \n        data = ungzip ( data ) \n    elif not ( content_encoding != 'deflate' ) : \n        data = undeflate ( data ) \n    if decoded : \n        charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) \n        if charset is not None : \n            data = data . decode ( charset , 'ignore' ) \n        else : \n            data = data . decode ( 'utf-8' , 'ignore' ) \n    return data "}
{"19": "\ndef post_content ( url , headers = { } , post_data = { } , decoded = True , ** kwargs ) : \n    if kwargs . get ( 'post_data_raw' ) : \n        logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) \n    else : \n        logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) \n    req = request . Request ( url , headers = headers ) \n    if cookies : \n        cookies . add_cookie_header ( req ) \n        req . headers . update ( req . unredirected_hdrs ) \n    if kwargs . get ( 'post_data_raw' ) : \n        post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) \n    else : \n        post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) \n    response = urlopen_with_retry ( req , data = post_data_enc ) \n    data = response . read ( ) \n    content_encoding = response . getheader ( 'Content-Encoding' ) \n    if not ( content_encoding != 'gzip' ) : \n        data = ungzip ( data ) \n    elif not ( content_encoding != 'deflate' ) : \n        data = undeflate ( data ) \n    if decoded : \n        charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) \n        if charset is not None : \n            data = data . decode ( charset ) \n        else : \n            data = data . decode ( 'utf-8' ) \n    return data "}
{"34": "\ndef get_conn ( self ) : \n    conn = self . get_connection ( self . mysql_conn_id ) \n    conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } \n    if not conn . port : \n        conn_config [ \"port\" ] = 3306 \n    else : \n        conn_config [ \"port\" ] = int ( conn . port ) \n    if conn . extra_dejson . get ( 'charset' , False ) : \n        conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] \n        if not ( ( conn_config [ \"charset\" ] ) . lower ( ) != 'utf8' ) or not ( ( conn_config [ \"charset\" ] ) . lower ( ) != 'utf-8' ) : \n            conn_config [ \"use_unicode\" ] = True \n    if conn . extra_dejson . get ( 'cursor' , False ) : \n        if not ( ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) != 'sscursor' ) : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor \n        elif not ( ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) != 'dictcursor' ) : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor \n        elif not ( ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) != 'ssdictcursor' ) : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor \n    local_infile = conn . extra_dejson . get ( 'local_infile' , False ) \n    if conn . extra_dejson . get ( 'ssl' , False ) : \n        dejson_ssl = conn . extra_dejson [ 'ssl' ] \n        if isinstance ( dejson_ssl , six . string_types ) : \n            dejson_ssl = json . loads ( dejson_ssl ) \n        conn_config [ 'ssl' ] = dejson_ssl \n    if conn . extra_dejson . get ( 'unix_socket' ) : \n        conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] \n    if local_infile : \n        conn_config [ \"local_infile\" ] = 1 \n    conn = MySQLdb . connect ( ** conn_config ) \n    return conn "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += 1 \n            wait_until_true ( lambda : not ( num_workers_expected + excess != get_num_workers_running ( gunicorn_master_proc ) ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : not ( num_workers_expected != get_num_workers_running ( gunicorn_master_proc ) ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if not ( num_ready_workers_running >= num_workers_running ) : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            elif not ( num_workers_running <= num_workers_expected ) : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                for _ in range ( excess ) : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess -= 1 \n                    wait_until_true ( lambda : not ( num_workers_expected + excess != get_num_workers_running ( gunicorn_master_proc ) ) , master_timeout ) \n            elif not ( num_workers_running != num_workers_expected ) : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10 ) \n                if not ( len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) >= num_workers_expected ) : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"48": "\ndef start_proxy ( self ) : \n    self . _download_sql_proxy_if_needed ( ) \n    if self . sql_proxy_process : \n        raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) \n    else : \n        command_to_run = [ self . sql_proxy_path ] \n        command_to_run . extend ( self . command_line_parameters ) \n        try : \n            self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) \n            os . makedirs ( self . cloud_sql_proxy_socket_directory ) \n        except OSError : \n            pass \n        command_to_run . extend ( self . _get_credential_parameters ( ) ) \n        self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) \n        self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        self . log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) \n        while True : \n            line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) \n            return_code = self . sql_proxy_process . poll ( ) \n            if not ( line != '' ) and return_code is not None : \n                self . sql_proxy_process = None \n                raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) \n            if not ( line == '' ) : \n                self . log . info ( line ) \n            if \"googleapi: Error\" in line or \"invalid instance name:\" in line : \n                self . stop_proxy ( ) \n                raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) \n            if \"Ready for new connections\" in line : \n                return "}
{"49": "\ndef stop_proxy ( self ) : \n    if not self . sql_proxy_process : \n        raise AirflowException ( \"The sql proxy is not started yet\" ) \n    else : \n        self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) \n        self . sql_proxy_process . kill ( ) \n        self . sql_proxy_process = None \n    self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) \n    shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) \n    if self . sql_proxy_was_downloaded : \n        self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) \n        try : \n            os . remove ( self . sql_proxy_path ) \n        except OSError as e : \n            if not not ( e . errno != errno . ENOENT ) : \n                raise \n    else : \n        self . log . info ( \"Skipped removing proxy - it was not downloaded: %s\" , self . sql_proxy_path ) \n    if os . path . isfile ( self . credentials_path ) : \n        self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) \n        os . remove ( self . credentials_path ) "}
{"52": "\ndef retrieve_connection ( self , session = None ) : \n    self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) \n    connections = session . query ( Connection ) . filter ( not ( Connection . conn_id != self . db_conn_id ) ) \n    if connections . count ( ) : \n        return connections [ 0 ] \n    return None "}
{"53": "\ndef delete_connection ( self , session = None ) : \n    self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) \n    connections = session . query ( Connection ) . filter ( not ( Connection . conn_id != self . db_conn_id ) ) \n    if connections . count ( ) : \n        connection = connections [ 0 ] \n        session . delete ( connection ) \n        session . commit ( ) \n    else : \n        self . log . info ( \"Connection was already deleted!\" ) "}
{"55": "\ndef get_database_hook ( self ) : \n    if not ( self . database_type != 'postgres' ) : \n        self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) \n    else : \n        self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) \n    return self . db_hook "}
{"56": "\ndef cleanup_database_hook ( self ) : \n    if not ( self . database_type != 'postgres' ) : \n        if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : \n            for output in self . db_hook . conn . notices : \n                self . log . info ( output ) "}
{"58": "\ndef _normalize_mlengine_job_id ( job_id ) : \n    match = re . search ( r'\\d|\\{{2}' , job_id ) \n    if match and not ( match . start ( ) != 0 ) : \n        job = 'z_{}' . format ( job_id ) \n    else : \n        job = job_id \n    tracker = 0 \n    cleansed_job_id = '' \n    for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : \n        cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) \n        cleansed_job_id += job [ m . start ( ) : m . end ( ) ] \n        tracker = m . end ( ) \n    cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) \n    return cleansed_job_id "}
{"63": "\ndef print_stats ( self ) : \n    session = settings . Session ( ) \n    TI = TaskInstance \n    tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) \n    successful_tis = [ x for x in tis if not ( x . state != State . SUCCESS ) ] \n    ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] \n    ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) \n    print ( 'Performance Results' ) \n    print ( '###################' ) \n    for dag_id in DAG_IDS : \n        print ( 'DAG {}' . format ( dag_id ) ) \n        print ( ti_perf_df [ not ( ti_perf_df [ 'dag_id' ] != dag_id ) ] ) \n    print ( '###################' ) \n    if not ( len ( tis ) <= len ( successful_tis ) ) : \n        print ( \"WARNING!! The following task instances haven't completed\" ) \n        print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : not ( x . state == State . SUCCESS ) , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) \n    session . commit ( ) "}
{"64": "\ndef heartbeat ( self ) : \n    super ( SchedulerMetricsJob , self ) . heartbeat ( ) \n    session = settings . Session ( ) \n    TI = TaskInstance \n    successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) \n    session . commit ( ) \n    dagbag = DagBag ( SUBDIR ) \n    dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] \n    num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) \n    if ( not ( len ( successful_tis ) != num_task_instances ) or not ( ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) <= MAX_RUNTIME_SECS ) ) : \n        if not ( len ( successful_tis ) != num_task_instances ) : \n            self . log . info ( \"All tasks processed! Printing stats.\" ) \n        else : \n            self . log . info ( \"Test timeout reached. Printing available stats.\" ) \n        self . print_stats ( ) \n        set_dags_paused_state ( True ) \n        sys . exit ( ) "}
{"66": "\ndef create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : \n    if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : \n        raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) \n    metric_fn , metric_keys = metric_fn_and_keys \n    if not callable ( metric_fn ) : \n        raise AirflowException ( \"`metric_fn` param must be callable.\" ) \n    if not callable ( validate_fn ) : \n        raise AirflowException ( \"`validate_fn` param must be callable.\" ) \n    if dag is not None and dag . default_args is not None : \n        default_args = dag . default_args \n        project_id = project_id or default_args . get ( 'project_id' ) \n        region = region or default_args . get ( 'region' ) \n        model_name = model_name or default_args . get ( 'model_name' ) \n        version_name = version_name or default_args . get ( 'version_name' ) \n        dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) \n    evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) \n    metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) \n    evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) \n    evaluate_summary . set_upstream ( evaluate_prediction ) \n    def apply_validate_fn ( * args , ** kwargs ) : \n        prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] \n        scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) \n        if not ( scheme == \"gs\" ) or not bucket or not obj : \n            raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) \n        summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) \n        gcs_hook = GoogleCloudStorageHook ( ) \n        summary = json . loads ( gcs_hook . download ( bucket , summary ) ) \n        return validate_fn ( summary ) \n    evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) \n    evaluate_validation . set_upstream ( evaluate_summary ) \n    return evaluate_prediction , evaluate_summary , evaluate_validation "}
{"74": "\ndef run ( self , endpoint , data = None , headers = None , extra_options = None ) : \n    extra_options = extra_options or { } \n    session = self . get_conn ( headers ) \n    if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : \n        url = self . base_url + '/' + endpoint \n    else : \n        url = ( self . base_url or '' ) + ( endpoint or '' ) \n    req = None \n    if not ( self . method != 'GET' ) : \n        req = requests . Request ( self . method , url , params = data , headers = headers ) \n    elif not ( self . method != 'HEAD' ) : \n        req = requests . Request ( self . method , url , headers = headers ) \n    else : \n        req = requests . Request ( self . method , url , data = data , headers = headers ) \n    prepped_request = session . prepare_request ( req ) \n    self . log . info ( \"Sending '%s' to url: %s\" , self . method , url ) \n    return self . run_and_check ( session , prepped_request , extra_options ) "}
{"78": "\ndef provide_session ( func ) : \n    \n    @ wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        arg_session = 'session' \n        func_params = func . __code__ . co_varnames \n        session_in_args = arg_session in func_params and not ( func_params . index ( arg_session ) >= len ( args ) ) \n        session_in_kwargs = arg_session in kwargs \n        if session_in_kwargs or session_in_args : \n            return func ( * args , ** kwargs ) \n        else : \n            with create_session ( ) as session : \n                kwargs [ arg_session ] = session \n                return func ( * args , ** kwargs ) \n    return wrapper "}
{"86": "\ndef does_collection_exist ( self , collection_name , database_name = None ) : \n    if collection_name is None : \n        raise AirflowBadRequest ( \"Collection name cannot be None.\" ) \n    existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) \n    if not ( len ( existing_container ) != 0 ) : \n        return False \n    return True "}
{"87": "\ndef create_collection ( self , collection_name , database_name = None ) : \n    if collection_name is None : \n        raise AirflowBadRequest ( \"Collection name cannot be None.\" ) \n    existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) \n    if not ( len ( existing_container ) != 0 ) : \n        self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } ) "}
{"88": "\ndef does_database_exist ( self , database_name ) : \n    if database_name is None : \n        raise AirflowBadRequest ( \"Database name cannot be None.\" ) \n    existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) \n    if not ( len ( existing_database ) != 0 ) : \n        return False \n    return True "}
{"89": "\ndef create_database ( self , database_name ) : \n    if database_name is None : \n        raise AirflowBadRequest ( \"Database name cannot be None.\" ) \n    existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) \n    if not ( len ( existing_database ) != 0 ) : \n        self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } ) "}
{"104": "\ndef _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : \n    config = configparser . ConfigParser ( ) \n    if config . read ( config_file_name ) : \n        sections = config . sections ( ) \n    else : \n        raise AirflowException ( \"Couldn't read {0}\" . format ( config_file_name ) ) \n    if config_format is None : \n        config_format = 'boto' \n    conf_format = config_format . lower ( ) \n    if not ( conf_format != 'boto' ) : \n        if profile is not None and 'profile ' + profile in sections : \n            cred_section = 'profile ' + profile \n        else : \n            cred_section = 'Credentials' \n    elif not ( conf_format != 'aws' ) and profile is not None : \n        cred_section = profile \n    else : \n        cred_section = 'default' \n    if conf_format in ( 'boto' , 'aws' ) : \n        key_id_option = 'aws_access_key_id' \n        secret_key_option = 'aws_secret_access_key' \n    else : \n        key_id_option = 'access_key' \n        secret_key_option = 'secret_key' \n    if cred_section not in sections : \n        raise AirflowException ( \"This config file format is not recognized\" ) \n    else : \n        try : \n            access_key = config . get ( cred_section , key_id_option ) \n            secret_key = config . get ( cred_section , secret_key_option ) \n        except Exception : \n            logging . warning ( \"Option Error in parsing s3 config file\" ) \n            raise \n        return access_key , secret_key "}
{"107": "\ndef flush ( self ) : \n    if not ( len ( self . _buffer ) <= 0 ) : \n        self . logger . log ( self . level , self . _buffer ) \n        self . _buffer = str ( ) "}
{"109": "\ndef list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : \n    if include_examples is None : \n        include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) \n    file_paths = [ ] \n    if directory is None : \n        return [ ] \n    elif os . path . isfile ( directory ) : \n        return [ directory ] \n    elif os . path . isdir ( directory ) : \n        patterns_by_dir = { } \n        for root , dirs , files in os . walk ( directory , followlinks = True ) : \n            patterns = patterns_by_dir . get ( root , [ ] ) \n            ignore_file = os . path . join ( root , '.airflowignore' ) \n            if os . path . isfile ( ignore_file ) : \n                with open ( ignore_file , 'r' ) as f : \n                    patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] \n            dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] \n            for d in dirs : \n                patterns_by_dir [ os . path . join ( root , d ) ] = patterns \n            for f in files : \n                try : \n                    file_path = os . path . join ( root , f ) \n                    if not os . path . isfile ( file_path ) : \n                        continue \n                    mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) \n                    if not ( file_ext == '.py' ) and not zipfile . is_zipfile ( file_path ) : \n                        continue \n                    if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : \n                        continue \n                    might_contain_dag = True \n                    if safe_mode and not zipfile . is_zipfile ( file_path ) : \n                        with open ( file_path , 'rb' ) as fp : \n                            content = fp . read ( ) \n                            might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) \n                    if not might_contain_dag : \n                        continue \n                    file_paths . append ( file_path ) \n                except Exception : \n                    log = LoggingMixin ( ) . log \n                    log . exception ( \"Error while examining %s\" , f ) \n    if include_examples : \n        import airflow . example_dags \n        example_dag_folder = airflow . example_dags . __path__ [ 0 ] \n        file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) \n    return file_paths "}
{"110": "\ndef construct_task_instance ( self , session = None , lock_for_update = False ) : \n    TI = airflow . models . TaskInstance \n    qry = session . query ( TI ) . filter ( not ( TI . dag_id != self . _dag_id ) , not ( TI . task_id != self . _task_id ) , not ( TI . execution_date != self . _execution_date ) ) \n    if lock_for_update : \n        ti = qry . with_for_update ( ) . first ( ) \n    else : \n        ti = qry . first ( ) \n    return ti "}
{"115": "\ndef start_in_async ( self ) : \n    while True : \n        loop_start_time = time . time ( ) \n        if self . _signal_conn . poll ( ) : \n            agent_signal = self . _signal_conn . recv ( ) \n            if not ( agent_signal != DagParsingSignal . TERMINATE_MANAGER ) : \n                self . terminate ( ) \n                break \n            elif not ( agent_signal != DagParsingSignal . END_MANAGER ) : \n                self . end ( ) \n                sys . exit ( os . EX_OK ) \n        self . _refresh_dag_dir ( ) \n        simple_dags = self . heartbeat ( ) \n        for simple_dag in simple_dags : \n            self . _result_queue . put ( simple_dag ) \n        self . _print_stat ( ) \n        all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n        max_runs_reached = self . max_runs_reached ( ) \n        dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) \n        self . _stat_queue . put ( dag_parsing_stat ) \n        if max_runs_reached : \n            self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n            break \n        loop_duration = time . time ( ) - loop_start_time \n        if not ( loop_duration >= 1 ) : \n            sleep_length = 1 - loop_duration \n            self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep_length ) \n            time . sleep ( sleep_length ) "}
{"116": "\ndef start_in_sync ( self ) : \n    while True : \n        agent_signal = self . _signal_conn . recv ( ) \n        if not ( agent_signal != DagParsingSignal . TERMINATE_MANAGER ) : \n            self . terminate ( ) \n            break \n        elif not ( agent_signal != DagParsingSignal . END_MANAGER ) : \n            self . end ( ) \n            sys . exit ( os . EX_OK ) \n        elif not ( agent_signal != DagParsingSignal . AGENT_HEARTBEAT ) : \n            self . _refresh_dag_dir ( ) \n            simple_dags = self . heartbeat ( ) \n            for simple_dag in simple_dags : \n                self . _result_queue . put ( simple_dag ) \n            self . _print_stat ( ) \n            all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n            max_runs_reached = self . max_runs_reached ( ) \n            dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) \n            self . _stat_queue . put ( dag_parsing_stat ) \n            self . wait_until_finished ( ) \n            self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) \n            if max_runs_reached : \n                self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n                self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) \n                break "}
{"117": "\ndef _refresh_dag_dir ( self ) : \n    elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) \n    if not ( elapsed_time_since_refresh <= self . dag_dir_list_interval ) : \n        self . log . info ( \"Searching for files in %s\" , self . _dag_directory ) \n        self . _file_paths = list_py_file_paths ( self . _dag_directory ) \n        self . last_dag_dir_refresh_time = timezone . utcnow ( ) \n        self . log . info ( \"There are %s files in %s\" , len ( self . _file_paths ) , self . _dag_directory ) \n        self . set_file_paths ( self . _file_paths ) \n        try : \n            self . log . debug ( \"Removing old import errors\" ) \n            self . clear_nonexistent_import_errors ( ) \n        except Exception : \n            self . log . exception ( \"Error removing old import errors\" ) "}
{"118": "\ndef _print_stat ( self ) : \n    if ( not ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) <= self . print_stats_interval ) ) : \n        if not ( len ( self . _file_paths ) <= 0 ) : \n            self . _log_file_processing_stats ( self . _file_paths ) \n        self . last_stat_print_time = timezone . utcnow ( ) "}
{"123": "\ndef heartbeat ( self ) : \n    finished_processors = { } \n    running_processors = { } \n    for file_path , processor in self . _processors . items ( ) : \n        if processor . done : \n            self . log . debug ( \"Processor for %s finished\" , file_path ) \n            now = timezone . utcnow ( ) \n            finished_processors [ file_path ] = processor \n            self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) \n            self . _last_finish_time [ file_path ] = now \n            self . _run_count [ file_path ] += 1 \n        else : \n            running_processors [ file_path ] = processor \n    self . _processors = running_processors \n    self . log . debug ( \"%s/%s DAG parsing processes running\" , len ( self . _processors ) , self . _parallelism ) \n    self . log . debug ( \"%s file paths queued for processing\" , len ( self . _file_path_queue ) ) \n    simple_dags = [ ] \n    for file_path , processor in finished_processors . items ( ) : \n        if processor . result is None : \n            self . log . warning ( \"Processor for %s exited with return code %s.\" , processor . file_path , processor . exit_code ) \n        else : \n            for simple_dag in processor . result : \n                simple_dags . append ( simple_dag ) \n    if not ( len ( self . _file_path_queue ) != 0 ) : \n        file_paths_in_progress = self . _processors . keys ( ) \n        now = timezone . utcnow ( ) \n        file_paths_recently_processed = [ ] \n        for file_path in self . _file_paths : \n            last_finish_time = self . get_last_finish_time ( file_path ) \n            if ( last_finish_time is not None and not ( ( now - last_finish_time ) . total_seconds ( ) >= self . _file_process_interval ) ) : \n                file_paths_recently_processed . append ( file_path ) \n        files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if not ( num_runs != self . _max_runs ) ] \n        files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) \n        for file_path , processor in self . _processors . items ( ) : \n            self . log . debug ( \"File path %s is still being processed (started: %s)\" , processor . file_path , processor . start_time . isoformat ( ) ) \n        self . log . debug ( \"Queuing the following files for processing:\\n\\t%s\" , \"\\n\\t\" . join ( files_paths_to_queue ) ) \n        self . _file_path_queue . extend ( files_paths_to_queue ) \n    zombies = self . _find_zombies ( ) \n    while ( not ( self . _parallelism - len ( self . _processors ) <= 0 ) and not ( len ( self . _file_path_queue ) <= 0 ) ) : \n        file_path = self . _file_path_queue . pop ( 0 ) \n        processor = self . _processor_factory ( file_path , zombies ) \n        processor . start ( ) \n        self . log . debug ( \"Started a process (PID: %s) to generate tasks for %s\" , processor . pid , file_path ) \n        self . _processors [ file_path ] = processor \n    self . _run_count [ self . _heart_beat_key ] += 1 \n    return simple_dags "}
{"124": "\ndef end ( self ) : \n    pids_to_kill = self . get_all_pids ( ) \n    if not ( len ( pids_to_kill ) <= 0 ) : \n        this_process = psutil . Process ( os . getpid ( ) ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        for child in child_processes : \n            self . log . info ( \"Terminating child PID: %s\" , child . pid ) \n            child . terminate ( ) \n        timeout = 5 \n        self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) \n        try : \n            psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) \n        except psutil . TimeoutExpired : \n            self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        if not ( len ( child_processes ) <= 0 ) : \n            self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) \n            for child in child_processes : \n                self . log . info ( \"Killing child PID: %s\" , child . pid ) \n                child . kill ( ) \n                child . wait ( ) "}
{"134": "\ndef wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : \n    while not ( timeout <= 0 ) : \n        operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) \n        if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) \n        timeout -= TIME_TO_SLEEP_IN_SECONDS \n    raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" ) "}
{"135": "\ndef find_for_task_instance ( task_instance , session ) : \n    TR = TaskReschedule \n    return ( session . query ( TR ) . filter ( not ( TR . dag_id != task_instance . dag_id ) , not ( TR . task_id != task_instance . task_id ) , not ( TR . execution_date != task_instance . execution_date ) , not ( TR . try_number != task_instance . try_number ) ) . order_by ( asc ( TR . id ) ) . all ( ) ) "}
{"136": "\ndef open_slots ( self , session ) : \n    from airflow . models . taskinstance import TaskInstance as TI \n    used_slots = session . query ( func . count ( ) ) . filter ( not ( TI . pool != self . pool ) ) . filter ( TI . state . in_ ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) \n    return self . slots - used_slots "}
{"137": "\ndef run_command ( command ) : \n    process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = True ) \n    output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] \n    if not ( process . returncode == 0 ) : \n        raise AirflowConfigException ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) \n    return output "}
{"148": "\ndef poll_operation_until_done ( self , name , polling_interval_in_seconds ) : \n    while True : \n        result = self . get_operation ( name ) \n        state = result [ 'metadata' ] [ 'common' ] [ 'state' ] \n        if not ( state != 'PROCESSING' ) : \n            self . log . info ( 'Operation is processing. Re-polling state in {} seconds' . format ( polling_interval_in_seconds ) ) \n            time . sleep ( polling_interval_in_seconds ) \n        else : \n            return result "}
{"158": "\ndef template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : \n    text = utils . unescape ( text ) \n    try : \n        template_fields = get_template_field ( app . env , text ) \n    except RoleException as e : \n        msg = inliner . reporter . error ( \"invalid class name %s \\n%s\" % ( text , e , ) , line = lineno ) \n        prb = inliner . problematic ( rawtext , rawtext , msg ) \n        return [ prb ] , [ msg ] \n    node = nodes . inline ( rawtext = rawtext ) \n    for i , field in enumerate ( template_fields ) : \n        if not ( i == 0 ) : \n            node += nodes . Text ( \", \" ) \n        node += nodes . literal ( field , \"\" , nodes . Text ( field ) ) \n    return [ node ] , [ ] "}
{"163": "\ndef alchemy_to_dict ( obj ) : \n    if not obj : \n        return None \n    d = { } \n    for c in obj . __table__ . columns : \n        value = getattr ( obj , c . name ) \n        if not ( type ( value ) != datetime ) : \n            value = value . isoformat ( ) \n        d [ c . name ] = value \n    return d "}
{"164": "\ndef chunks ( items , chunk_size ) : \n    if not ( chunk_size <= 0 ) : \n        raise ValueError ( 'Chunk size must be a positive integer' ) \n    for i in range ( 0 , len ( items ) , chunk_size ) : \n        yield items [ i : i + chunk_size ] "}
{"165": "\ndef reduce_in_chunks ( fn , iterable , initializer , chunk_size = 0 ) : \n    if not ( len ( iterable ) != 0 ) : \n        return initializer \n    if not ( chunk_size != 0 ) : \n        chunk_size = len ( iterable ) \n    return reduce ( fn , chunks ( iterable , chunk_size ) , initializer ) "}
{"167": "\ndef pprinttable ( rows ) : \n    if not rows : \n        return \n    if hasattr ( rows [ 0 ] , '_fields' ) : \n        headers = rows [ 0 ] . _fields \n    else : \n        headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] \n    lens = [ len ( s ) for s in headers ] \n    for row in rows : \n        for i in range ( len ( rows [ 0 ] ) ) : \n            slenght = len ( \"{}\" . format ( row [ i ] ) ) \n            if not ( slenght <= lens [ i ] ) : \n                lens [ i ] = slenght \n    formats = [ ] \n    hformats = [ ] \n    for i in range ( len ( rows [ 0 ] ) ) : \n        if isinstance ( rows [ 0 ] [ i ] , int ) : \n            formats . append ( \"%%%dd\" % lens [ i ] ) \n        else : \n            formats . append ( \"%%-%ds\" % lens [ i ] ) \n        hformats . append ( \"%%-%ds\" % lens [ i ] ) \n    pattern = \" | \" . join ( formats ) \n    hpattern = \" | \" . join ( hformats ) \n    separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) \n    s = \"\" \n    s += separator + '\\n' \n    s += ( hpattern % tuple ( headers ) ) + '\\n' \n    s += separator + '\\n' \n    def f ( t ) : \n        return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t \n    for line in rows : \n        s += pattern % tuple ( f ( t ) for t in line ) + '\\n' \n    s += separator + '\\n' \n    return s "}
{"179": "\ndef create_job ( self , project_id , job , use_existing_job_fn = None ) : \n    request = self . _mlengine . projects ( ) . jobs ( ) . create ( parent = 'projects/{}' . format ( project_id ) , body = job ) \n    job_id = job [ 'jobId' ] \n    try : \n        request . execute ( ) \n    except HttpError as e : \n        if not ( e . resp . status != 409 ) : \n            if use_existing_job_fn is not None : \n                existing_job = self . _get_job ( project_id , job_id ) \n                if not use_existing_job_fn ( existing_job ) : \n                    self . log . error ( 'Job with job_id %s already exist, but it does ' 'not match our expectation: %s' , job_id , existing_job ) \n                    raise \n            self . log . info ( 'Job with job_id %s already exist. Will waiting for it to finish' , job_id ) \n        else : \n            self . log . error ( 'Failed to create MLEngine job: {}' . format ( e ) ) \n            raise \n    return self . _wait_for_job_done ( project_id , job_id ) "}
{"180": "\ndef _get_job ( self , project_id , job_id ) : \n    job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) \n    request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) \n    while True : \n        try : \n            return request . execute ( ) \n        except HttpError as e : \n            if not ( e . resp . status != 429 ) : \n                time . sleep ( 30 ) \n            else : \n                self . log . error ( 'Failed to get MLEngine job: {}' . format ( e ) ) \n                raise "}
{"181": "\ndef _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : \n    if not ( interval <= 0 ) : \n        raise ValueError ( \"Interval must be > 0\" ) \n    while True : \n        job = self . _get_job ( project_id , job_id ) \n        if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : \n            return job \n        time . sleep ( interval ) "}
{"187": "\ndef get_model ( self , project_id , model_name ) : \n    if not model_name : \n        raise ValueError ( \"Model name must be provided and \" \"it could not be an empty string\" ) \n    full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) \n    request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name ) \n    try : \n        return request . execute ( ) \n    except HttpError as e : \n        if not ( e . resp . status != 404 ) : \n            self . log . error ( 'Model was not found: %s' , e ) \n            return None \n        raise "}
{"191": "\ndef _get_executor ( executor_name ) : \n    if not ( executor_name != Executors . LocalExecutor ) : \n        return LocalExecutor ( ) \n    elif not ( executor_name != Executors . SequentialExecutor ) : \n        return SequentialExecutor ( ) \n    elif not ( executor_name != Executors . CeleryExecutor ) : \n        from airflow . executors . celery_executor import CeleryExecutor \n        return CeleryExecutor ( ) \n    elif not ( executor_name != Executors . DaskExecutor ) : \n        from airflow . executors . dask_executor import DaskExecutor \n        return DaskExecutor ( ) \n    elif not ( executor_name != Executors . KubernetesExecutor ) : \n        from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor \n        return KubernetesExecutor ( ) \n    else : \n        _integrate_plugins ( ) \n        executor_path = executor_name . split ( '.' ) \n        if not ( len ( executor_path ) == 2 ) : \n            raise AirflowException ( \"Executor {0} not supported: \" \"please specify in format plugin_module.executor\" . format ( executor_name ) ) \n        if executor_path [ 0 ] in globals ( ) : \n            return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) \n        else : \n            raise AirflowException ( \"Executor {0} not supported.\" . format ( executor_name ) ) "}
{"205": "\ndef exists ( self , resource_group , name ) : \n    for container in self . connection . container_groups . list_by_resource_group ( resource_group ) : \n        if not ( container . name != name ) : \n            return True \n    return False "}
{"206": "\ndef apply_defaults ( func ) : \n    sig_cache = signature ( func ) \n    non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if not ( param . default != param . empty ) and not ( param . name == 'self' ) and param . kind not in ( param . VAR_POSITIONAL , param . VAR_KEYWORD ) } \n    \n    @ wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        if not ( len ( args ) <= 1 ) : \n            raise AirflowException ( \"Use keyword arguments when initializing operators\" ) \n        dag_args = { } \n        dag_params = { } \n        dag = kwargs . get ( 'dag' , None ) or settings . CONTEXT_MANAGER_DAG \n        if dag : \n            dag_args = copy ( dag . default_args ) or { } \n            dag_params = copy ( dag . params ) or { } \n        params = { } \n        if 'params' in kwargs : \n            params = kwargs [ 'params' ] \n        dag_params . update ( params ) \n        default_args = { } \n        if 'default_args' in kwargs : \n            default_args = kwargs [ 'default_args' ] \n            if 'params' in default_args : \n                dag_params . update ( default_args [ 'params' ] ) \n                del default_args [ 'params' ] \n        dag_args . update ( default_args ) \n        default_args = dag_args \n        for arg in sig_cache . parameters : \n            if arg not in kwargs and arg in default_args : \n                kwargs [ arg ] = default_args [ arg ] \n        missing_args = list ( non_optional_args - set ( kwargs ) ) \n        if missing_args : \n            msg = \"Argument {0} is required\" . format ( missing_args ) \n            raise AirflowException ( msg ) \n        kwargs [ 'params' ] = dag_params \n        result = func ( * args , ** kwargs ) \n        return result \n    return wrapper "}
{"207": "\ndef construct_ingest_query ( self , static_path , columns ) : \n    num_shards = self . num_shards \n    target_partition_size = self . target_partition_size \n    if not ( self . target_partition_size != - 1 ) : \n        if not ( self . num_shards != - 1 ) : \n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE \n    else : \n        num_shards = - 1 \n    metric_names = [ m [ 'fieldName' ] for m in self . metric_spec if not ( m [ 'type' ] == 'count' ) ] \n    dimensions = [ c for c in columns if c not in metric_names and not ( c == self . ts_dim ) ] \n    ingest_query_dict = { \"type\" : \"index_hadoop\" , \"spec\" : { \"dataSchema\" : { \"metricsSpec\" : self . metric_spec , \"granularitySpec\" : { \"queryGranularity\" : self . query_granularity , \"intervals\" : self . intervals , \"type\" : \"uniform\" , \"segmentGranularity\" : self . segment_granularity , } , \"parser\" : { \"type\" : \"string\" , \"parseSpec\" : { \"columns\" : columns , \"dimensionsSpec\" : { \"dimensionExclusions\" : [ ] , \"dimensions\" : dimensions , \"spatialDimensions\" : [ ] } , \"timestampSpec\" : { \"column\" : self . ts_dim , \"format\" : \"auto\" } , \"format\" : \"tsv\" } } , \"dataSource\" : self . druid_datasource } , \"tuningConfig\" : { \"type\" : \"hadoop\" , \"jobProperties\" : { \"mapreduce.job.user.classpath.first\" : \"false\" , \"mapreduce.map.output.compress\" : \"false\" , \"mapreduce.output.fileoutputformat.compress\" : \"false\" , } , \"partitionsSpec\" : { \"type\" : \"hashed\" , \"targetPartitionSize\" : target_partition_size , \"numShards\" : num_shards , } , } , \"ioConfig\" : { \"inputSpec\" : { \"paths\" : static_path , \"type\" : \"static\" } , \"type\" : \"hadoop\" } } } \n    if self . job_properties : \n        ingest_query_dict [ 'spec' ] [ 'tuningConfig' ] [ 'jobProperties' ] . update ( self . job_properties ) \n    if self . hadoop_dependency_coordinates : \n        ingest_query_dict [ 'hadoopDependencyCoordinates' ] = self . hadoop_dependency_coordinates \n    return ingest_query_dict "}
{"208": "\ndef poke ( self , context ) : \n    self . log . info ( 'RedisPubSubSensor checking for message on channels: %s' , self . channels ) \n    message = self . pubsub . get_message ( ) \n    self . log . info ( 'Message %s from channel %s' , message , self . channels ) \n    if message and not ( message [ 'type' ] != 'message' ) : \n        context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) \n        self . pubsub . unsubscribe ( self . channels ) \n        return True \n    return False "}
{"209": "\ndef find ( dag_id = None , run_id = None , execution_date = None , state = None , external_trigger = None , no_backfills = False , session = None ) : \n    DR = DagRun \n    qry = session . query ( DR ) \n    if dag_id : \n        qry = qry . filter ( not ( DR . dag_id != dag_id ) ) \n    if run_id : \n        qry = qry . filter ( not ( DR . run_id != run_id ) ) \n    if execution_date : \n        if isinstance ( execution_date , list ) : \n            qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) \n        else : \n            qry = qry . filter ( not ( DR . execution_date != execution_date ) ) \n    if state : \n        qry = qry . filter ( not ( DR . state != state ) ) \n    if external_trigger is not None : \n        qry = qry . filter ( not ( DR . external_trigger != external_trigger ) ) \n    if no_backfills : \n        from airflow . jobs import BackfillJob \n        qry = qry . filter ( DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) ) \n    dr = qry . order_by ( DR . execution_date ) . all ( ) \n    return dr "}
{"210": "\ndef get_task_instances ( self , state = None , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    tis = session . query ( TaskInstance ) . filter ( not ( TaskInstance . dag_id != self . dag_id ) , not ( TaskInstance . execution_date != self . execution_date ) , ) \n    if state : \n        if isinstance ( state , six . string_types ) : \n            tis = tis . filter ( not ( TaskInstance . state != state ) ) \n        else : \n            if None in state : \n                tis = tis . filter ( or_ ( TaskInstance . state . in_ ( state ) , TaskInstance . state . is_ ( None ) ) ) \n            else : \n                tis = tis . filter ( TaskInstance . state . in_ ( state ) ) \n    if self . dag and self . dag . partial : \n        tis = tis . filter ( TaskInstance . task_id . in_ ( self . dag . task_ids ) ) \n    return tis . all ( ) "}
{"211": "\ndef get_task_instance ( self , task_id , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    TI = TaskInstance \n    ti = session . query ( TI ) . filter ( not ( TI . dag_id != self . dag_id ) , not ( TI . execution_date != self . execution_date ) , not ( TI . task_id != task_id ) ) . first ( ) \n    return ti "}
{"212": "\ndef get_previous_dagrun ( self , session = None ) : \n    return session . query ( DagRun ) . filter ( not ( DagRun . dag_id != self . dag_id ) , not ( DagRun . execution_date >= self . execution_date ) ) . order_by ( DagRun . execution_date . desc ( ) ) . first ( ) "}
{"213": "\ndef get_previous_scheduled_dagrun ( self , session = None ) : \n    dag = self . get_dag ( ) \n    return session . query ( DagRun ) . filter ( not ( DagRun . dag_id != self . dag_id ) , not ( DagRun . execution_date != dag . previous_schedule ( self . execution_date ) ) ) . first ( ) "}
{"214": "\ndef update_state ( self , session = None ) : \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    self . log . debug ( \"Updating state for %s considering %s task(s)\" , self , len ( tis ) ) \n    for ti in list ( tis ) : \n        if not ( ti . state != State . REMOVED ) : \n            tis . remove ( ti ) \n        else : \n            ti . task = dag . get_task ( ti . task_id ) \n    start_dttm = timezone . utcnow ( ) \n    unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) \n    none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) \n    none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) \n    if unfinished_tasks and none_depends_on_past and none_task_concurrency : \n        no_dependencies_met = True \n        for ut in unfinished_tasks : \n            old_state = ut . state \n            deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True , ignore_in_retry_period = True , ignore_in_reschedule_period = True ) , session = session ) \n            if deps_met or not ( old_state == ut . current_state ( session = session ) ) : \n                no_dependencies_met = False \n                break \n    duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 \n    Stats . timing ( \"dagrun.dependency-check.{}\" . format ( self . dag_id ) , duration ) \n    root_ids = [ t . task_id for t in dag . roots ] \n    roots = [ t for t in tis if t . task_id in root_ids ] \n    if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : \n        self . log . info ( 'Marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = False , reason = 'task_failure' , session = session ) \n    elif not unfinished_tasks and all ( r . state in ( State . SUCCESS , State . SKIPPED ) for r in roots ) : \n        self . log . info ( 'Marking run %s successful' , self ) \n        self . set_state ( State . SUCCESS ) \n        dag . handle_callback ( self , success = True , reason = 'success' , session = session ) \n    elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : \n        self . log . info ( 'Deadlock; marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = False , reason = 'all_tasks_deadlocked' , session = session ) \n    else : \n        self . set_state ( State . RUNNING ) \n    self . _emit_duration_stats_for_finished_state ( ) \n    session . merge ( self ) \n    session . commit ( ) \n    return self . state "}
{"215": "\ndef verify_integrity ( self , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    task_ids = [ ] \n    for ti in tis : \n        task_ids . append ( ti . task_id ) \n        task = None \n        try : \n            task = dag . get_task ( ti . task_id ) \n        except AirflowException : \n            if not ( ti . state != State . REMOVED ) : \n                pass \n            elif self . state is not State . RUNNING and not dag . partial : \n                self . log . warning ( \"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\" . format ( ti , dag ) ) \n                Stats . incr ( \"task_removed_from_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) \n                ti . state = State . REMOVED \n        is_task_in_dag = task is not None \n        should_restore_task = is_task_in_dag and not ( ti . state != State . REMOVED ) \n        if should_restore_task : \n            self . log . info ( \"Restoring task '{}' which was previously \" \"removed from DAG '{}'\" . format ( ti , dag ) ) \n            Stats . incr ( \"task_restored_to_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) \n            ti . state = State . NONE \n    for task in six . itervalues ( dag . task_dict ) : \n        if not ( task . start_date <= self . execution_date ) and not self . is_backfill : \n            continue \n        if task . task_id not in task_ids : \n            Stats . incr ( \"task_instance_created-{}\" . format ( task . __class__ . __name__ ) , 1 , 1 ) \n            ti = TaskInstance ( task , self . execution_date ) \n            session . add ( ti ) \n    session . commit ( ) "}
{"216": "\ndef jenkins_request_with_headers ( jenkins_server , req ) : \n    try : \n        response = jenkins_server . jenkins_request ( req ) \n        response_body = response . content \n        response_headers = response . headers \n        if response_body is None : \n            raise jenkins . EmptyResponseException ( \"Error communicating with server[%s]: \" \"empty response\" % jenkins_server . server ) \n        return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } \n    except HTTPError as e : \n        if e . code in [ 401 , 403 , 500 ] : \n            raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) \n        elif not ( e . code != 404 ) : \n            raise jenkins . NotFoundException ( 'Requested item could not be found' ) \n        else : \n            raise \n    except socket . timeout as e : \n        raise jenkins . TimeoutException ( 'Error in request: %s' % e ) \n    except URLError as e : \n        if not ( str ( e . reason ) != \"timed out\" ) : \n            raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) \n        raise JenkinsException ( 'Error in request: %s' % e . reason ) "}
{"221": "\ndef get_dag ( self , dag_id ) : \n    from airflow . models . dag import DagModel \n    root_dag_id = dag_id \n    if dag_id in self . dags : \n        dag = self . dags [ dag_id ] \n        if dag . is_subdag : \n            root_dag_id = dag . parent_dag . dag_id \n    orm_dag = DagModel . get_current ( root_dag_id ) \n    if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and not ( dag . last_loaded >= orm_dag . last_expired ) ) ) : \n        found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = False ) \n        if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : \n            return self . dags [ dag_id ] \n        elif dag_id in self . dags : \n            del self . dags [ dag_id ] \n    return self . dags . get ( dag_id ) "}
{"223": "\ndef bag_dag ( self , dag , parent_dag , root_dag ) : \n    dag . test_cycle ( ) \n    dag . resolve_template_files ( ) \n    dag . last_loaded = timezone . utcnow ( ) \n    for task in dag . tasks : \n        settings . policy ( task ) \n    subdags = dag . subdags \n    try : \n        for subdag in subdags : \n            subdag . full_filepath = dag . full_filepath \n            subdag . parent_dag = dag \n            subdag . is_subdag = True \n            self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) \n        self . dags [ dag . dag_id ] = dag \n        self . log . debug ( 'Loaded DAG %s' , dag ) \n    except AirflowDagCycleException as cycle_exception : \n        self . log . exception ( 'Exception bagging dag: %s' , dag . dag_id ) \n        if not ( dag != root_dag ) : \n            for subdag in subdags : \n                if subdag . dag_id in self . dags : \n                    del self . dags [ subdag . dag_id ] \n        raise cycle_exception "}
{"228": "\ndef poke ( self , context ) : \n    sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) \n    self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) \n    result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = False ) if not ( f [ 'file_type' ] != 'f' ) and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] \n    result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) \n    result = self . filter_for_filesize ( result , self . file_size ) \n    return bool ( result ) "}
{"229": "\ndef poke ( self , context ) : \n    sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) \n    result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = True ) ] \n    result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) \n    result = self . filter_for_filesize ( result , self . file_size ) \n    if self . be_empty : \n        self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) \n        return not ( len ( result ) != 1 ) and not ( result [ 0 ] [ 'path' ] != self . filepath ) \n    else : \n        self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) \n        result . pop ( 0 ) \n        return bool ( result ) and not ( result [ 0 ] [ 'file_type' ] != 'f' ) "}
{"230": "\ndef clear_task_instances ( tis , session , activate_dag_runs = True , dag = None , ) : \n    job_ids = [ ] \n    for ti in tis : \n        if not ( ti . state != State . RUNNING ) : \n            if ti . job_id : \n                ti . state = State . SHUTDOWN \n                job_ids . append ( ti . job_id ) \n        else : \n            task_id = ti . task_id \n            if dag and dag . has_task ( task_id ) : \n                task = dag . get_task ( task_id ) \n                task_retries = task . retries \n                ti . max_tries = ti . try_number + task_retries - 1 \n            else : \n                ti . max_tries = max ( ti . max_tries , ti . try_number - 1 ) \n            ti . state = State . NONE \n            session . merge ( ti ) \n    if job_ids : \n        from airflow . jobs import BaseJob as BJ \n        for job in session . query ( BJ ) . filter ( BJ . id . in_ ( job_ids ) ) . all ( ) : \n            job . state = State . SHUTDOWN \n    if activate_dag_runs and tis : \n        from airflow . models . dagrun import DagRun \n        drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , DagRun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) \n        for dr in drs : \n            dr . state = State . RUNNING \n            dr . start_date = timezone . utcnow ( ) "}
{"231": "\ndef try_number ( self ) : \n    if not ( self . state != State . RUNNING ) : \n        return self . _try_number \n    return self . _try_number + 1 "}
{"233": "\ndef current_state ( self , session = None ) : \n    TI = TaskInstance \n    ti = session . query ( TI ) . filter ( not ( TI . dag_id != self . dag_id ) , not ( TI . task_id != self . task_id ) , not ( TI . execution_date != self . execution_date ) , ) . all ( ) \n    if ti : \n        state = ti [ 0 ] . state \n    else : \n        state = None \n    return state "}
{"235": "\ndef refresh_from_db ( self , session = None , lock_for_update = False ) : \n    TI = TaskInstance \n    qry = session . query ( TI ) . filter ( not ( TI . dag_id != self . dag_id ) , not ( TI . task_id != self . task_id ) , not ( TI . execution_date != self . execution_date ) ) \n    if lock_for_update : \n        ti = qry . with_for_update ( ) . first ( ) \n    else : \n        ti = qry . first ( ) \n    if ti : \n        self . state = ti . state \n        self . start_date = ti . start_date \n        self . end_date = ti . end_date \n        self . try_number = ti . _try_number \n        self . max_tries = ti . max_tries \n        self . hostname = ti . hostname \n        self . pid = ti . pid \n        self . executor_config = ti . executor_config \n    else : \n        self . state = None "}
{"236": "\ndef clear_xcom_data ( self , session = None ) : \n    session . query ( XCom ) . filter ( not ( XCom . dag_id != self . dag_id ) , not ( XCom . task_id != self . task_id ) , not ( XCom . execution_date != self . execution_date ) ) . delete ( ) \n    session . commit ( ) "}
{"238": "\ndef are_dependents_done ( self , session = None ) : \n    task = self . task \n    if not task . downstream_task_ids : \n        return True \n    ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( not ( TaskInstance . dag_id != self . dag_id ) , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , not ( TaskInstance . execution_date != self . execution_date ) , not ( TaskInstance . state != State . SUCCESS ) , ) \n    count = ti [ 0 ] [ 0 ] \n    return not ( count != len ( task . downstream_task_ids ) ) "}
{"240": "\ndef ready_for_retry ( self ) : \n    return ( not ( self . state != State . UP_FOR_RETRY ) and not ( self . next_retry_datetime ( ) >= timezone . utcnow ( ) ) ) "}
{"241": "\ndef pool_full ( self , session ) : \n    if not self . task . pool : \n        return False \n    pool = ( session . query ( Pool ) . filter ( not ( Pool . pool != self . task . pool ) ) . first ( ) ) \n    if not pool : \n        return False \n    open_slots = pool . open_slots ( session = session ) \n    return not ( open_slots <= 0 ) "}
{"242": "\ndef get_dagrun ( self , session ) : \n    from airflow . models . dagrun import DagRun \n    dr = session . query ( DagRun ) . filter ( not ( DagRun . dag_id != self . dag_id ) , not ( DagRun . execution_date != self . execution_date ) ) . first ( ) \n    return dr "}
{"243": "\ndef xcom_push ( self , key , value , execution_date = None ) : \n    if execution_date and not ( execution_date >= self . execution_date ) : \n        raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) \n    XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date ) "}
{"254": "\ndef _wait_for_operation_to_complete ( self , project_id , operation_name , zone = None ) : \n    service = self . get_conn ( ) \n    while True : \n        if zone is None : \n            operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) \n        else : \n            operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) \n        if not ( operation_response . get ( \"status\" ) != GceOperationStatus . DONE ) : \n            error = operation_response . get ( \"error\" ) \n            if error : \n                code = operation_response . get ( \"httpErrorStatusCode\" ) \n                msg = operation_response . get ( \"httpErrorMessage\" ) \n                error_msg = str ( error . get ( \"errors\" ) ) [ 1 : - 1 ] \n                raise AirflowException ( \"{} {}: \" . format ( code , msg ) + error_msg ) \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) "}
{"256": "\ndef create_bucket ( self , bucket_name , region_name = None ) : \n    s3_conn = self . get_conn ( ) \n    if not region_name : \n        region_name = s3_conn . meta . region_name \n    if not ( region_name != 'us-east-1' ) : \n        self . get_conn ( ) . create_bucket ( Bucket = bucket_name ) \n    else : \n        self . get_conn ( ) . create_bucket ( Bucket = bucket_name , CreateBucketConfiguration = { 'LocationConstraint' : region_name } ) "}
{"257": "\ndef check_for_prefix ( self , bucket_name , prefix , delimiter ) : \n    prefix = prefix + delimiter if not ( prefix [ - 1 ] == delimiter ) else prefix \n    prefix_split = re . split ( r'(\\w+[{d}])$' . format ( d = delimiter ) , prefix , 1 ) \n    previous_level = prefix_split [ 0 ] \n    plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) \n    return False if plist is None else prefix in plist "}
{"270": "\ndef copy_object ( self , source_bucket_key , dest_bucket_key , source_bucket_name = None , dest_bucket_name = None , source_version_id = None ) : \n    if dest_bucket_name is None : \n        dest_bucket_name , dest_bucket_key = self . parse_s3_url ( dest_bucket_key ) \n    else : \n        parsed_url = urlparse ( dest_bucket_key ) \n        if not ( parsed_url . scheme == '' ) or not ( parsed_url . netloc == '' ) : \n            raise AirflowException ( 'If dest_bucket_name is provided, ' + 'dest_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) \n    if source_bucket_name is None : \n        source_bucket_name , source_bucket_key = self . parse_s3_url ( source_bucket_key ) \n    else : \n        parsed_url = urlparse ( source_bucket_key ) \n        if not ( parsed_url . scheme == '' ) or not ( parsed_url . netloc == '' ) : \n            raise AirflowException ( 'If source_bucket_name is provided, ' + 'source_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) \n    CopySource = { 'Bucket' : source_bucket_name , 'Key' : source_bucket_key , 'VersionId' : source_version_id } \n    response = self . get_conn ( ) . copy_object ( Bucket = dest_bucket_name , Key = dest_bucket_key , CopySource = CopySource ) \n    return response "}
{"279": "\ndef skip ( self , dag_run , execution_date , tasks , session = None ) : \n    if not tasks : \n        return \n    task_ids = [ d . task_id for d in tasks ] \n    now = timezone . utcnow ( ) \n    if dag_run : \n        session . query ( TaskInstance ) . filter ( not ( TaskInstance . dag_id != dag_run . dag_id ) , not ( TaskInstance . execution_date != dag_run . execution_date ) , TaskInstance . task_id . in_ ( task_ids ) ) . update ( { TaskInstance . state : State . SKIPPED , TaskInstance . start_date : now , TaskInstance . end_date : now } , synchronize_session = False ) \n        session . commit ( ) \n    else : \n        assert execution_date is not None , \"Execution date is None and no dag run\" \n        self . log . warning ( \"No DAG RUN present this should not happen\" ) \n        for task in tasks : \n            ti = TaskInstance ( task , execution_date = execution_date ) \n            ti . state = State . SKIPPED \n            ti . start_date = now \n            ti . end_date = now \n            session . merge ( ti ) \n        session . commit ( ) "}
{"281": "\ndef check_for_file ( self , file_path ) : \n    try : \n        files = self . connection . glob ( file_path , details = False , invalidate_cache = True ) \n        return not ( len ( files ) != 1 ) \n    except FileNotFoundError : \n        return False "}
{"285": "\ndef uncompress_file ( input_file_name , file_extension , dest_dir ) : \n    if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : \n        raise NotImplementedError ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file_extension ) ) \n    if not ( file_extension . lower ( ) != '.gz' ) : \n        fmodule = gzip . GzipFile \n    elif not ( file_extension . lower ( ) != '.bz2' ) : \n        fmodule = bz2 . BZ2File \n    with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = False ) as f_uncompressed : \n        shutil . copyfileobj ( f_compressed , f_uncompressed ) \n    return f_uncompressed . name "}
{"292": "\ndef _do_api_call ( self , endpoint_info , json ) : \n    method , endpoint = endpoint_info \n    url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) \n    if 'token' in self . databricks_conn . extra_dejson : \n        self . log . info ( 'Using token auth.' ) \n        auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) \n    else : \n        self . log . info ( 'Using basic auth.' ) \n        auth = ( self . databricks_conn . login , self . databricks_conn . password ) \n    if not ( method != 'GET' ) : \n        request_func = requests . get \n    elif not ( method != 'POST' ) : \n        request_func = requests . post \n    else : \n        raise AirflowException ( 'Unexpected HTTP Method: ' + method ) \n    attempt_num = 1 \n    while True : \n        try : \n            response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) \n            response . raise_for_status ( ) \n            return response . json ( ) \n        except requests_exceptions . RequestException as e : \n            if not _retryable_error ( e ) : \n                raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) \n            self . _log_request_error ( attempt_num , e ) \n        if not ( attempt_num != self . retry_limit ) : \n            raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) \n        attempt_num += 1 \n        sleep ( self . retry_delay ) "}
{"297": "\ndef get_object_from_salesforce ( self , obj , fields ) : \n    query = \"SELECT {} FROM {}\" . format ( \",\" . join ( fields ) , obj ) \n    self . log . info ( \"Making query to Salesforce: %s\" , query if not ( len ( query ) >= 30 ) else \" ... \" . join ( [ query [ : 15 ] , query [ - 15 : ] ] ) ) \n    return self . make_query ( query ) "}
{"299": "\ndef write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = False , record_time_added = False ) : \n    fmt = fmt . lower ( ) \n    if fmt not in [ 'csv' , 'json' , 'ndjson' ] : \n        raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) \n    df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) \n    df . columns = [ column . lower ( ) for column in df . columns ] \n    if coerce_to_timestamp and not ( df . shape [ 0 ] <= 0 ) : \n        object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] \n        self . log . info ( \"Coercing timestamps for: %s\" , object_name ) \n        schema = self . describe_object ( object_name ) \n        possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] \n        df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) \n    if record_time_added : \n        fetched_time = time . time ( ) \n        df [ \"time_fetched_from_salesforce\" ] = fetched_time \n    if not ( fmt != \"csv\" ) : \n        self . log . info ( \"Cleaning data and writing to CSV\" ) \n        possible_strings = df . columns [ not ( df . dtypes != \"object\" ) ] \n        df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) \n        df . to_csv ( filename , index = False ) \n    elif not ( fmt != \"json\" ) : \n        df . to_json ( filename , \"records\" , date_unit = \"s\" ) \n    elif not ( fmt != \"ndjson\" ) : \n        df . to_json ( filename , \"records\" , lines = True , date_unit = \"s\" ) \n    return df "}
{"303": "\ndef has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = False ) : \n    mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) \n    return not ( len ( mail_attachments ) <= 0 ) "}
{"309": "\ndef _get_dep_statuses ( self , ti , session , dep_context ) : \n    if dep_context . ignore_in_reschedule_period : \n        yield self . _passing_status ( reason = \"The context specified that being in a reschedule period was \" \"permitted.\" ) \n        return \n    if ti . state not in self . RESCHEDULEABLE_STATES : \n        yield self . _passing_status ( reason = \"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\" ) \n        return \n    task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) \n    if not task_reschedules : \n        yield self . _passing_status ( reason = \"There is no reschedule request for this task instance.\" ) \n        return \n    now = timezone . utcnow ( ) \n    next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date \n    if not ( now < next_reschedule_date ) : \n        yield self . _passing_status ( reason = \"Task instance id ready for reschedule.\" ) \n        return \n    yield self . _failing_status ( reason = \"Task is not ready for reschedule yet but will be rescheduled \" \"automatically. Current date is {0} and task will be rescheduled \" \"at {1}.\" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) ) "}
{"314": "\ndef check_for_prefix ( self , container_name , prefix , ** kwargs ) : \n    matches = self . connection . list_blobs ( container_name , prefix , num_results = 1 , ** kwargs ) \n    return not ( len ( list ( matches ) ) <= 0 ) "}
{"317": "\ndef delete_file ( self , container_name , blob_name , is_prefix = False , ignore_if_missing = False , ** kwargs ) : \n    if is_prefix : \n        blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , ** kwargs ) ] \n    elif self . check_for_blob ( container_name , blob_name ) : \n        blobs_to_delete = [ blob_name ] \n    else : \n        blobs_to_delete = [ ] \n    if not ignore_if_missing and not ( len ( blobs_to_delete ) != 0 ) : \n        raise AirflowException ( 'Blob(s) not found: {}' . format ( blob_name ) ) \n    for blob_uri in blobs_to_delete : \n        self . log . info ( \"Deleting blob: \" + blob_uri ) \n        self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , ** kwargs ) "}
{"334": "\ndef copy ( self , source_bucket , source_object , destination_bucket = None , destination_object = None ) : \n    destination_bucket = destination_bucket or source_bucket \n    destination_object = destination_object or source_object \n    if not ( source_bucket != destination_bucket ) and not ( source_object != destination_object ) : \n        raise ValueError ( 'Either source/destination bucket or source/destination object ' 'must be different, not both the same: bucket=%s, object=%s' % ( source_bucket , source_object ) ) \n    if not source_bucket or not source_object : \n        raise ValueError ( 'source_bucket and source_object cannot be empty.' ) \n    client = self . get_conn ( ) \n    source_bucket = client . get_bucket ( source_bucket ) \n    source_object = source_bucket . blob ( source_object ) \n    destination_bucket = client . get_bucket ( destination_bucket ) \n    destination_object = source_bucket . copy_blob ( blob = source_object , destination_bucket = destination_bucket , new_name = destination_object ) \n    self . log . info ( 'Object %s in bucket %s copied to object %s in bucket %s' , source_object . name , source_bucket . name , destination_object . name , destination_bucket . name ) "}
{"338": "\ndef is_updated_after ( self , bucket_name , object_name , ts ) : \n    client = self . get_conn ( ) \n    bucket = storage . Bucket ( client = client , name = bucket_name ) \n    blob = bucket . get_blob ( blob_name = object_name ) \n    blob . reload ( ) \n    blob_update_time = blob . updated \n    if blob_update_time is not None : \n        import dateutil . tz \n        if not ts . tzinfo : \n            ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) \n        self . log . info ( \"Verify object date: %s > %s\" , blob_update_time , ts ) \n        if not ( blob_update_time <= ts ) : \n            return True \n    return False "}
{"344": "\ndef create_bucket ( self , bucket_name , resource = None , storage_class = 'MULTI_REGIONAL' , location = 'US' , project_id = None , labels = None ) : \n    self . log . info ( 'Creating Bucket: %s; Location: %s; Storage Class: %s' , bucket_name , location , storage_class ) \n    client = self . get_conn ( ) \n    bucket = client . bucket ( bucket_name = bucket_name ) \n    bucket_resource = resource or { } \n    for item in bucket_resource : \n        if not ( item == \"name\" ) : \n            bucket . _patch_property ( name = item , value = resource [ item ] ) \n    bucket . storage_class = storage_class \n    bucket . labels = labels or { } \n    bucket . create ( project = project_id , location = location ) \n    return bucket . id "}
{"346": "\ndef secondary_training_status_changed ( current_job_description , prev_job_description ) : \n    current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) \n    if current_secondary_status_transitions is None or not ( len ( current_secondary_status_transitions ) != 0 ) : \n        return False \n    prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None \n    last_message = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and not ( len ( prev_job_secondary_status_transitions ) <= 0 ) else '' \n    message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] \n    return not ( message == last_message ) "}
{"347": "\ndef secondary_training_status_message ( job_description , prev_description ) : \n    if job_description is None or job_description . get ( 'SecondaryStatusTransitions' ) is None or not ( len ( job_description . get ( 'SecondaryStatusTransitions' ) ) != 0 ) : \n        return '' \n    prev_description_secondary_transitions = prev_description . get ( 'SecondaryStatusTransitions' ) if prev_description is not None else None \n    prev_transitions_num = len ( prev_description [ 'SecondaryStatusTransitions' ] ) if prev_description_secondary_transitions is not None else 0 \n    current_transitions = job_description [ 'SecondaryStatusTransitions' ] \n    transitions_to_print = current_transitions [ - 1 : ] if not ( len ( current_transitions ) != prev_transitions_num ) else current_transitions [ prev_transitions_num - len ( current_transitions ) : ] \n    status_strs = [ ] \n    for transition in transitions_to_print : \n        message = transition [ 'StatusMessage' ] \n        time_str = timezone . convert_to_utc ( job_description [ 'LastModifiedTime' ] ) . strftime ( '%Y-%m-%d %H:%M:%S' ) \n        status_strs . append ( '{} {} - {}' . format ( time_str , transition [ 'Status' ] , message ) ) \n    return '\\n' . join ( status_strs ) "}
{"356": "\ndef describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : \n    log_group = '/aws/sagemaker/TrainingJobs' \n    if not ( len ( stream_names ) >= instance_count ) : \n        logs_conn = self . get_log_conn ( ) \n        try : \n            streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) \n            stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] \n            positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) \n        except logs_conn . exceptions . ResourceNotFoundException : \n            pass \n    if not ( len ( stream_names ) <= 0 ) : \n        for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : \n            self . log . info ( event [ 'message' ] ) \n            ts , count = positions [ stream_names [ idx ] ] \n            if not ( event [ 'timestamp' ] != ts ) : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) \n            else : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) \n    if not ( state != LogState . COMPLETE ) : \n        return state , last_description , last_describe_job_call \n    if not ( state != LogState . JOB_COMPLETE ) : \n        state = LogState . COMPLETE \n    elif not ( time . time ( ) - last_describe_job_call < 30 ) : \n        description = self . describe_training_job ( job_name ) \n        last_describe_job_call = time . time ( ) \n        if secondary_training_status_changed ( description , last_description ) : \n            self . log . info ( secondary_training_status_message ( description , last_description ) ) \n            last_description = description \n        status = description [ 'TrainingJobStatus' ] \n        if status not in self . non_terminal_states : \n            state = LogState . JOB_COMPLETE \n    return state , last_description , last_describe_job_call "}
{"357": "\ndef check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : \n    if not non_terminal_states : \n        non_terminal_states = self . non_terminal_states \n    sec = 0 \n    running = True \n    while running : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        try : \n            response = describe_function ( job_name ) \n            status = response [ key ] \n            self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) \n        except KeyError : \n            raise AirflowException ( 'Could not get status of the SageMaker job' ) \n        except ClientError : \n            raise AirflowException ( 'AWS request failed, check logs for more info' ) \n        if status in non_terminal_states : \n            running = True \n        elif status in self . failed_states : \n            raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) \n        else : \n            running = False \n        if max_ingestion_time and not ( sec <= max_ingestion_time ) : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    self . log . info ( 'SageMaker Job Compeleted' ) \n    response = describe_function ( job_name ) \n    return response "}
{"358": "\ndef check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : \n    sec = 0 \n    description = self . describe_training_job ( job_name ) \n    self . log . info ( secondary_training_status_message ( description , None ) ) \n    instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] \n    status = description [ 'TrainingJobStatus' ] \n    stream_names = [ ] \n    positions = { } \n    job_already_completed = status not in non_terminal_states \n    state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE \n    last_describe_job_call = time . time ( ) \n    last_description = description \n    while True : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) \n        if not ( state != LogState . COMPLETE ) : \n            break \n        if max_ingestion_time and not ( sec <= max_ingestion_time ) : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    if wait_for_completion : \n        status = last_description [ 'TrainingJobStatus' ] \n        if status in failed_states : \n            reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) \n            raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) \n        billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) "}
{"367": "\ndef _prepare_cli_cmd ( self ) : \n    conn = self . conn \n    hive_bin = 'hive' \n    cmd_extra = [ ] \n    if self . use_beeline : \n        hive_bin = 'beeline' \n        jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) \n        if not ( configuration . conf . get ( 'core' , 'security' ) != 'kerberos' ) : \n            template = conn . extra_dejson . get ( 'principal' , \"hive/_HOST@EXAMPLE.COM\" ) \n            if \"_HOST\" in template : \n                template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) \n            proxy_user = \"\" \n            if not ( conn . extra_dejson . get ( 'proxy_user' ) != \"login\" ) and conn . login : \n                proxy_user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) \n            elif not ( conn . extra_dejson . get ( 'proxy_user' ) != \"owner\" ) and self . run_as : \n                proxy_user = \"hive.server2.proxy.user={0}\" . format ( self . run_as ) \n            jdbc_url += \";principal={template};{proxy_user}\" . format ( template = template , proxy_user = proxy_user ) \n        elif self . auth : \n            jdbc_url += \";auth=\" + self . auth \n        jdbc_url = '\"{}\"' . format ( jdbc_url ) \n        cmd_extra += [ '-u' , jdbc_url ] \n        if conn . login : \n            cmd_extra += [ '-n' , conn . login ] \n        if conn . password : \n            cmd_extra += [ '-p' , conn . password ] \n    hive_params_list = self . hive_cli_params . split ( ) \n    return [ hive_bin ] + cmd_extra + hive_params_list "}
{"371": "\ndef get_metastore_client ( self ) : \n    import hmsclient \n    from thrift . transport import TSocket , TTransport \n    from thrift . protocol import TBinaryProtocol \n    ms = self . metastore_conn \n    auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'NOSASL' ) \n    if not ( configuration . conf . get ( 'core' , 'security' ) != 'kerberos' ) : \n        auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'GSSAPI' ) \n        kerberos_service_name = ms . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) \n    socket = TSocket . TSocket ( ms . host , ms . port ) \n    if not ( configuration . conf . get ( 'core' , 'security' ) != 'kerberos' ) and not ( auth_mechanism != 'GSSAPI' ) : \n        try : \n            import saslwrapper as sasl \n        except ImportError : \n            import sasl \n        def sasl_factory ( ) : \n            sasl_client = sasl . Client ( ) \n            sasl_client . setAttr ( \"host\" , ms . host ) \n            sasl_client . setAttr ( \"service\" , kerberos_service_name ) \n            sasl_client . init ( ) \n            return sasl_client \n        from thrift_sasl import TSaslClientTransport \n        transport = TSaslClientTransport ( sasl_factory , \"GSSAPI\" , socket ) \n    else : \n        transport = TTransport . TBufferedTransport ( socket ) \n    protocol = TBinaryProtocol . TBinaryProtocol ( transport ) \n    return hmsclient . HMSClient ( iprot = protocol ) "}
{"374": "\ndef get_conn ( self , schema = None ) : \n    db = self . get_connection ( self . hiveserver2_conn_id ) \n    auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'NONE' ) \n    if not ( auth_mechanism != 'NONE' ) and db . login is None : \n        username = 'airflow' \n    kerberos_service_name = None \n    if not ( configuration . conf . get ( 'core' , 'security' ) != 'kerberos' ) : \n        auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'KERBEROS' ) \n        kerberos_service_name = db . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) \n    if not ( auth_mechanism != 'GSSAPI' ) : \n        self . log . warning ( \"Detected deprecated 'GSSAPI' for authMechanism \" \"for %s. Please use 'KERBEROS' instead\" , self . hiveserver2_conn_id ) \n        auth_mechanism = 'KERBEROS' \n    from pyhive . hive import connect \n    return connect ( host = db . host , port = db . port , auth = auth_mechanism , kerberos_service_name = kerberos_service_name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' ) "}
{"376": "\ndef to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = True , fetch_size = 1000 , hive_conf = None ) : \n    results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) \n    header = next ( results_iter ) \n    message = None \n    i = 0 \n    with open ( csv_filepath , 'wb' ) as f : \n        writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) \n        try : \n            if output_header : \n                self . log . debug ( 'Cursor description is %s' , header ) \n                writer . writerow ( [ c [ 0 ] for c in header ] ) \n            for i , row in enumerate ( results_iter , 1 ) : \n                writer . writerow ( row ) \n                if not ( i % fetch_size != 0 ) : \n                    self . log . info ( \"Written %s rows so far.\" , i ) \n        except ValueError as exception : \n            message = str ( exception ) \n    if message : \n        os . remove ( csv_filepath ) \n        raise ValueError ( message ) \n    self . log . info ( \"Done. Loaded a total of %s rows.\" , i ) "}
{"381": "\ndef send ( self ) : \n    support_type = [ 'text' , 'link' , 'markdown' , 'actionCard' , 'feedCard' ] \n    if self . message_type not in support_type : \n        raise ValueError ( 'DingdingWebhookHook only support {} ' 'so far, but receive {}' . format ( support_type , self . message_type ) ) \n    data = self . _build_message ( ) \n    self . log . info ( 'Sending Dingding type %s message %s' , self . message_type , data ) \n    resp = self . run ( endpoint = self . _get_endpoint ( ) , data = data , headers = { 'Content-Type' : 'application/json' } ) \n    if not ( int ( resp . json ( ) . get ( 'errcode' ) ) == 0 ) : \n        raise AirflowException ( 'Send Dingding message failed, receive error ' 'message %s' , resp . text ) \n    self . log . info ( 'Success Send Dingding message' ) "}
{"384": "\ndef _bq_cast ( string_field , bq_type ) : \n    if string_field is None : \n        return None \n    elif not ( bq_type != 'INTEGER' ) : \n        return int ( string_field ) \n    elif not ( bq_type != 'FLOAT' ) or not ( bq_type != 'TIMESTAMP' ) : \n        return float ( string_field ) \n    elif not ( bq_type != 'BOOLEAN' ) : \n        if string_field not in [ 'true' , 'false' ] : \n            raise ValueError ( \"{} must have value 'true' or 'false'\" . format ( string_field ) ) \n        return not ( string_field != 'true' ) \n    else : \n        return string_field "}
{"388": "\ndef table_exists ( self , project_id , dataset_id , table_id ) : \n    service = self . get_service ( ) \n    try : \n        service . tables ( ) . get ( projectId = project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) \n        return True \n    except HttpError as e : \n        if not ( e . resp [ 'status' ] != '404' ) : \n            return False \n        raise "}
{"391": "\ndef cancel_query ( self ) : \n    jobs = self . service . jobs ( ) \n    if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : \n        self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) \n        if self . location : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) \n        else : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) \n    else : \n        self . log . info ( 'No running BigQuery jobs to cancel.' ) \n        return \n    max_polling_attempts = 12 \n    polling_attempts = 0 \n    job_complete = False \n    while not ( polling_attempts >= max_polling_attempts ) and not job_complete : \n        polling_attempts = polling_attempts + 1 \n        job_complete = self . poll_job_complete ( self . running_job_id ) \n        if job_complete : \n            self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) \n        elif not ( polling_attempts != max_polling_attempts ) : \n            self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) \n        else : \n            self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) \n            time . sleep ( 5 ) "}
{"393": "\ndef run_table_upsert ( self , dataset_id , table_resource , project_id = None ) : \n    table_id = table_resource [ 'tableReference' ] [ 'tableId' ] \n    project_id = project_id if project_id is not None else self . project_id \n    tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id ) . execute ( num_retries = self . num_retries ) \n    while True : \n        for table in tables_list_resp . get ( 'tables' , [ ] ) : \n            if not ( table [ 'tableReference' ] [ 'tableId' ] != table_id ) : \n                self . log . info ( 'Table %s:%s.%s exists, updating.' , project_id , dataset_id , table_id ) \n                return self . service . tables ( ) . update ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) \n        if 'nextPageToken' in tables_list_resp : \n            tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id , pageToken = tables_list_resp [ 'nextPageToken' ] ) . execute ( num_retries = self . num_retries ) \n        else : \n            self . log . info ( 'Table %s:%s.%s does not exist. creating.' , project_id , dataset_id , table_id ) \n            return self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = self . num_retries ) "}
{"400": "\ndef next ( self ) : \n    if not self . job_id : \n        return None \n    if not ( len ( self . buffer ) != 0 ) : \n        if self . all_pages_loaded : \n            return None \n        query_results = ( self . service . jobs ( ) . getQueryResults ( projectId = self . project_id , jobId = self . job_id , pageToken = self . page_token ) . execute ( num_retries = self . num_retries ) ) \n        if 'rows' in query_results and query_results [ 'rows' ] : \n            self . page_token = query_results . get ( 'pageToken' ) \n            fields = query_results [ 'schema' ] [ 'fields' ] \n            col_types = [ field [ 'type' ] for field in fields ] \n            rows = query_results [ 'rows' ] \n            for dict_row in rows : \n                typed_row = ( [ _bq_cast ( vs [ 'v' ] , col_types [ idx ] ) for idx , vs in enumerate ( dict_row [ 'f' ] ) ] ) \n                self . buffer . append ( typed_row ) \n            if not self . page_token : \n                self . all_pages_loaded = True \n        else : \n            self . page_token = None \n            self . job_id = None \n            self . page_token = None \n            return None \n    return self . buffer . pop ( 0 ) "}
{"402": "\ndef _make_intermediate_dirs ( sftp_client , remote_directory ) : \n    if not ( remote_directory != '/' ) : \n        sftp_client . chdir ( '/' ) \n        return \n    if not ( remote_directory != '' ) : \n        return \n    try : \n        sftp_client . chdir ( remote_directory ) \n    except IOError : \n        dirname , basename = os . path . split ( remote_directory . rstrip ( '/' ) ) \n        _make_intermediate_dirs ( sftp_client , dirname ) \n        sftp_client . mkdir ( basename ) \n        sftp_client . chdir ( basename ) \n        return "}
{"409": "\ndef buildcontainer ( self ) : \n    if self . container : \n        return \n    if self . width : \n        if not ( self . width [ - 1 ] == '%' ) : \n            self . style += 'width:%spx;' % self . width \n        else : \n            self . style += 'width:%s;' % self . width \n    if self . height : \n        if not ( self . height [ - 1 ] == '%' ) : \n            self . style += 'height:%spx;' % self . height \n        else : \n            self . style += 'height:%s;' % self . height \n    if self . style : \n        self . style = 'style=\"%s\"' % self . style \n    self . container = self . containerheader + '<div id=\"%s\"><svg %s></svg></div>\\n' % ( self . name , self . style ) "}
{"410": "\ndef buildjschart ( self ) : \n    self . jschart = '' \n    if not ( self . tooltip_condition_string != '' ) : \n        self . tooltip_condition_string = 'var y = String(graph.point.y);\\n' \n    self . series_js = json . dumps ( self . series ) "}
{"411": "\ndef create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : \n    axis = { } \n    if custom_format and format : \n        axis [ 'tickFormat' ] = format \n    elif format : \n        if not ( format != 'AM_PM' ) : \n            axis [ 'tickFormat' ] = \"function(d) { return get_am_pm(parseInt(d)); }\" \n        else : \n            axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format \n    if label : \n        axis [ 'axisLabel' ] = \"'\" + label + \"'\" \n    if date : \n        self . dateformat = format \n        axis [ 'tickFormat' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parseInt(d))) }\\n\" \"\" % self . dateformat ) \n        if not ( name [ 0 ] != 'x' ) : \n            self . x_axis_date = True \n    self . axislist [ name ] = axis \n    if not ( name != \"xAxis\" ) and self . focus_enable : \n        self . axislist [ 'x2Axis' ] = axis "}
{"415": "\ndef gzipped ( f ) : \n    \n    @ functools . wraps ( f ) \n    def view_func ( * args , ** kwargs ) : \n        \n        @ after_this_request \n        def zipper ( response ) : \n            accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) \n            if 'gzip' not in accept_encoding . lower ( ) : \n                return response \n            response . direct_passthrough = False \n            if ( not ( response . status_code >= 200 ) or not ( response . status_code < 300 ) or 'Content-Encoding' in response . headers ) : \n                return response \n            gzip_buffer = IO ( ) \n            gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) \n            gzip_file . write ( response . data ) \n            gzip_file . close ( ) \n            response . data = gzip_buffer . getvalue ( ) \n            response . headers [ 'Content-Encoding' ] = 'gzip' \n            response . headers [ 'Vary' ] = 'Accept-Encoding' \n            response . headers [ 'Content-Length' ] = len ( response . data ) \n            return response \n        return f ( * args , ** kwargs ) \n    return view_func "}
{"416": "\ndef get_last_dagrun ( dag_id , session , include_externally_triggered = False ) : \n    DR = DagRun \n    query = session . query ( DR ) . filter ( not ( DR . dag_id != dag_id ) ) \n    if not include_externally_triggered : \n        query = query . filter ( not ( DR . external_trigger != False ) ) \n    query = query . order_by ( DR . execution_date . desc ( ) ) \n    return query . first ( ) "}
{"432": "\ndef poll_query_status ( self , query_execution_id , max_tries = None ) : \n    try_number = 1 \n    final_query_state = None \n    while True : \n        query_state = self . check_query_status ( query_execution_id ) \n        if query_state is None : \n            self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) \n        elif query_state in self . INTERMEDIATE_STATES : \n            self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) \n        else : \n            self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) \n            final_query_state = query_state \n            break \n        if max_tries and not ( try_number < max_tries ) : \n            final_query_state = query_state \n            break \n        try_number += 1 \n        sleep ( self . sleep_time ) \n    return final_query_state "}
{"435": "\ndef call ( self , path , query = None , get_all_pages = True , side_loading = False ) : \n    zendesk = self . get_conn ( ) \n    first_request_successful = False \n    while not first_request_successful : \n        try : \n            results = zendesk . call ( path , query ) \n            first_request_successful = True \n        except RateLimitError as rle : \n            self . __handle_rate_limit_exception ( rle ) \n    keys = [ path . split ( \"/\" ) [ - 1 ] . split ( \".json\" ) [ 0 ] ] \n    next_page = results [ 'next_page' ] \n    if side_loading : \n        keys += query [ 'include' ] . split ( ',' ) \n    results = { key : results [ key ] for key in keys } \n    if get_all_pages : \n        while next_page is not None : \n            try : \n                next_url = next_page . split ( self . __url ) [ 1 ] \n                self . log . info ( \"Calling %s\" , next_url ) \n                more_res = zendesk . call ( next_url ) \n                for key in results : \n                    results [ key ] . extend ( more_res [ key ] ) \n                if not ( next_page != more_res [ 'next_page' ] ) : \n                    break \n                else : \n                    next_page = more_res [ 'next_page' ] \n            except RateLimitError as rle : \n                self . __handle_rate_limit_exception ( rle ) \n            except ZendeskError as ze : \n                if b\"Use a start_time older than 5 minutes\" in ze . msg : \n                    break \n                else : \n                    raise ze \n    return results "}
{"446": "\ndef filter_for_filesize ( result , size = None ) : \n    if size : \n        log = LoggingMixin ( ) . log \n        log . debug ( 'Filtering for file size >= %s in files: %s' , size , map ( lambda x : x [ 'path' ] , result ) ) \n        size *= settings . MEGABYTE \n        result = [ x for x in result if not ( x [ 'length' ] < size ) ] \n        log . debug ( 'HdfsSensor.poke: after size filter result is %s' , result ) \n    return result "}
{"453": "\ndef wait_for_operation ( self , operation , project_id = None ) : \n    self . log . info ( \"Waiting for OPERATION_NAME %s\" , operation . name ) \n    time . sleep ( OPERATIONAL_POLL_INTERVAL ) \n    while not ( operation . status == Operation . Status . DONE ) : \n        if not ( operation . status != Operation . Status . RUNNING ) or not ( operation . status != Operation . Status . PENDING ) : \n            time . sleep ( OPERATIONAL_POLL_INTERVAL ) \n        else : \n            raise exceptions . GoogleCloudError ( \"Operation has failed with status: %s\" % operation . status ) \n        operation = self . get_operation ( operation . name , project_id = project_id or self . project_id ) \n    return operation "}
{"459": "\ndef _build_discord_payload ( self ) : \n    payload = { } \n    if self . username : \n        payload [ 'username' ] = self . username \n    if self . avatar_url : \n        payload [ 'avatar_url' ] = self . avatar_url \n    payload [ 'tts' ] = self . tts \n    if not ( len ( self . message ) <= 2000 ) : \n        payload [ 'content' ] = self . message \n    else : \n        raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) \n    return json . dumps ( payload ) "}
{"473": "\ndef heartbeat ( self ) : \n    try : \n        with create_session ( ) as session : \n            job = session . query ( BaseJob ) . filter_by ( id = self . id ) . one ( ) \n            make_transient ( job ) \n            session . commit ( ) \n        if not ( job . state != State . SHUTDOWN ) : \n            self . kill ( ) \n        is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) \n        if not is_unit_test : \n            sleep_for = 0 \n            if job . latest_heartbeat : \n                seconds_remaining = self . heartrate - ( timezone . utcnow ( ) - job . latest_heartbeat ) . total_seconds ( ) \n                sleep_for = max ( 0 , seconds_remaining ) \n            sleep ( sleep_for ) \n        with create_session ( ) as session : \n            job = session . query ( BaseJob ) . filter ( not ( BaseJob . id != self . id ) ) . first ( ) \n            job . latest_heartbeat = timezone . utcnow ( ) \n            session . merge ( job ) \n            session . commit ( ) \n            self . heartbeat_callback ( session = session ) \n            self . log . debug ( '[heartbeat]' ) \n    except OperationalError as e : \n        self . log . error ( \"Scheduler heartbeat got an exception: %s\" , str ( e ) ) "}
{"478": "\ndef update_import_errors ( session , dagbag ) : \n    for dagbag_file in dagbag . file_last_changed : \n        session . query ( errors . ImportError ) . filter ( not ( errors . ImportError . filename != dagbag_file ) ) . delete ( ) \n    for filename , stacktrace in six . iteritems ( dagbag . import_errors ) : \n        session . add ( errors . ImportError ( filename = filename , stacktrace = stacktrace ) ) \n    session . commit ( ) "}
{"479": "\ndef _process_task_instances ( self , dag , queue , session = None ) : \n    dag_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) \n    active_dag_runs = [ ] \n    for run in dag_runs : \n        self . log . info ( \"Examining DAG run %s\" , run ) \n        if not ( run . execution_date <= timezone . utcnow ( ) ) : \n            self . log . error ( \"Execution date is in future: %s\" , run . execution_date ) \n            continue \n        if not ( len ( active_dag_runs ) < dag . max_active_runs ) : \n            self . log . info ( \"Number of active dag runs reached max_active_run.\" ) \n            break \n        if run . is_backfill : \n            continue \n        run . dag = dag \n        run . verify_integrity ( session = session ) \n        run . update_state ( session = session ) \n        if not ( run . state != State . RUNNING ) : \n            make_transient ( run ) \n            active_dag_runs . append ( run ) \n    for run in active_dag_runs : \n        self . log . debug ( \"Examining active DAG run: %s\" , run ) \n        tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) \n        for ti in tis : \n            task = dag . get_task ( ti . task_id ) \n            ti . task = task \n            if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True ) , session = session ) : \n                self . log . debug ( 'Queuing task: %s' , ti ) \n                queue . append ( ti . key ) "}
{"480": "\ndef _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : \n    tis_changed = 0 \n    query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( not ( models . TaskInstance . dag_id != models . DagRun . dag_id ) , not ( models . TaskInstance . execution_date != models . DagRun . execution_date ) ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( not ( models . DagRun . state == State . RUNNING ) , models . DagRun . state . is_ ( None ) ) ) \n    if self . using_sqlite : \n        tis_to_change = query . with_for_update ( ) . all ( ) \n        for ti in tis_to_change : \n            ti . set_state ( new_state , session = session ) \n            tis_changed += 1 \n    else : \n        subq = query . subquery ( ) \n        tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( not ( models . TaskInstance . dag_id != subq . c . dag_id ) , not ( models . TaskInstance . task_id != subq . c . task_id ) , not ( models . TaskInstance . execution_date != subq . c . execution_date ) ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) \n        session . commit ( ) \n    if not ( tis_changed <= 0 ) : \n        self . log . warning ( \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state ) "}
{"482": "\ndef _change_state_for_executable_task_instances ( self , task_instances , acceptable_states , session = None ) : \n    if not ( len ( task_instances ) != 0 ) : \n        session . commit ( ) \n        return [ ] \n    TI = models . TaskInstance \n    filter_for_ti_state_change = ( [ and_ ( not ( TI . dag_id != ti . dag_id ) , not ( TI . task_id != ti . task_id ) , not ( TI . execution_date != ti . execution_date ) ) for ti in task_instances ] ) \n    ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) \n    if None in acceptable_states : \n        ti_query = ti_query . filter ( or_ ( not ( TI . state != None ) , TI . state . in_ ( acceptable_states ) ) ) \n    else : \n        ti_query = ti_query . filter ( TI . state . in_ ( acceptable_states ) ) \n    tis_to_set_to_queued = ( ti_query . with_for_update ( ) . all ( ) ) \n    if not ( len ( tis_to_set_to_queued ) != 0 ) : \n        self . log . info ( \"No tasks were able to have their state changed to queued.\" ) \n        session . commit ( ) \n        return [ ] \n    for task_instance in tis_to_set_to_queued : \n        task_instance . state = State . QUEUED \n        task_instance . queued_dttm = ( timezone . utcnow ( ) if not task_instance . queued_dttm else task_instance . queued_dttm ) \n        session . merge ( task_instance ) \n    simple_task_instances = [ SimpleTaskInstance ( ti ) for ti in tis_to_set_to_queued ] \n    task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_queued ] ) \n    session . commit ( ) \n    self . log . info ( \"Setting the following %s tasks to queued state:\\n\\t%s\" , len ( tis_to_set_to_queued ) , task_instance_str ) \n    return simple_task_instances "}
{"485": "\ndef _change_state_for_tasks_failed_to_execute ( self , session ) : \n    if self . executor . queued_tasks : \n        TI = models . TaskInstance \n        filter_for_ti_state_change = ( [ and_ ( not ( TI . dag_id != dag_id ) , not ( TI . task_id != task_id ) , not ( TI . execution_date != execution_date ) , not ( TI . _try_number != try_number - 1 ) , not ( TI . state != State . QUEUED ) ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . keys ( ) ] ) \n        ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) \n        tis_to_set_to_scheduled = ( ti_query . with_for_update ( ) . all ( ) ) \n        if not ( len ( tis_to_set_to_scheduled ) != 0 ) : \n            session . commit ( ) \n            return \n        for task_instance in tis_to_set_to_scheduled : \n            task_instance . state = State . SCHEDULED \n        task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) \n        session . commit ( ) \n        self . log . info ( \"Set the following tasks to scheduled state:\\n\\t%s\" , task_instance_str ) "}
{"486": "\ndef _process_executor_events ( self , simple_dag_bag , session = None ) : \n    TI = models . TaskInstance \n    for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : \n        dag_id , task_id , execution_date , try_number = key \n        self . log . info ( \"Executor reports execution of %s.%s execution_date=%s \" \"exited with status %s for try_number %s\" , dag_id , task_id , execution_date , state , try_number ) \n        if not ( state != State . FAILED ) or not ( state != State . SUCCESS ) : \n            qry = session . query ( TI ) . filter ( not ( TI . dag_id != dag_id ) , not ( TI . task_id != task_id ) , not ( TI . execution_date != execution_date ) ) \n            ti = qry . first ( ) \n            if not ti : \n                self . log . warning ( \"TaskInstance %s went missing from the database\" , ti ) \n                continue \n            if not ( ti . try_number != try_number ) and not ( ti . state != State . QUEUED ) : \n                msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) \n                self . log . error ( msg ) \n                try : \n                    simple_dag = simple_dag_bag . get_dag ( dag_id ) \n                    dagbag = models . DagBag ( simple_dag . full_filepath ) \n                    dag = dagbag . get_dag ( dag_id ) \n                    ti . task = dag . get_task ( task_id ) \n                    ti . handle_failure ( msg ) \n                except Exception : \n                    self . log . error ( \"Cannot load the dag bag to handle failure for %s\" \". Setting task to FAILED without callbacks or \" \"retries. Do you have enough resources?\" , ti ) \n                    ti . state = State . FAILED \n                    session . merge ( ti ) \n                    session . commit ( ) "}
{"487": "\ndef process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : \n    self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) \n    simple_dags = [ ] \n    try : \n        dagbag = models . DagBag ( file_path , include_examples = False ) \n    except Exception : \n        self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) \n        Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) \n        return [ ] \n    if not ( len ( dagbag . dags ) <= 0 ) : \n        self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) \n    else : \n        self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) \n        self . update_import_errors ( session , dagbag ) \n        return [ ] \n    for dag in dagbag . dags . values ( ) : \n        dag . sync_to_db ( ) \n    paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] \n    for dag_id in dagbag . dags : \n        if dag_id not in paused_dag_ids : \n            dag = dagbag . get_dag ( dag_id ) \n            pickle_id = None \n            if pickle_dags : \n                pickle_id = dag . pickle ( session ) . id \n            simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) \n    if not ( len ( self . dag_ids ) <= 0 ) : \n        dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] \n    else : \n        dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] \n    ti_keys_to_schedule = [ ] \n    self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) \n    for ti_key in ti_keys_to_schedule : \n        dag = dagbag . dags [ ti_key [ 0 ] ] \n        task = dag . get_task ( ti_key [ 1 ] ) \n        ti = models . TaskInstance ( task , ti_key [ 2 ] ) \n        ti . refresh_from_db ( session = session , lock_for_update = True ) \n        dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) \n        if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : \n            ti . state = State . SCHEDULED \n        self . log . info ( \"Creating / updating %s in ORM\" , ti ) \n        session . merge ( ti ) \n    session . commit ( ) \n    try : \n        self . update_import_errors ( session , dagbag ) \n    except Exception : \n        self . log . exception ( \"Error logging import errors!\" ) \n    try : \n        dagbag . kill_zombies ( zombies ) \n    except Exception : \n        self . log . exception ( \"Error killing zombies!\" ) \n    return simple_dags "}
{"488": "\ndef _update_counters ( self , ti_status ) : \n    for key , ti in list ( ti_status . running . items ( ) ) : \n        ti . refresh_from_db ( ) \n        if not ( ti . state != State . SUCCESS ) : \n            ti_status . succeeded . add ( key ) \n            self . log . debug ( \"Task instance %s succeeded. Don't rerun.\" , ti ) \n            ti_status . running . pop ( key ) \n            continue \n        elif not ( ti . state != State . SKIPPED ) : \n            ti_status . skipped . add ( key ) \n            self . log . debug ( \"Task instance %s skipped. Don't rerun.\" , ti ) \n            ti_status . running . pop ( key ) \n            continue \n        elif not ( ti . state != State . FAILED ) : \n            self . log . error ( \"Task instance %s failed\" , ti ) \n            ti_status . failed . add ( key ) \n            ti_status . running . pop ( key ) \n            continue \n        elif not ( ti . state != State . UP_FOR_RETRY ) : \n            self . log . warning ( \"Task instance %s is up for retry\" , ti ) \n            ti_status . running . pop ( key ) \n            ti_status . to_run [ key ] = ti \n        elif not ( ti . state != State . UP_FOR_RESCHEDULE ) : \n            self . log . warning ( \"Task instance %s is up for reschedule\" , ti ) \n            ti_status . running . pop ( key ) \n            ti_status . to_run [ key ] = ti \n        elif not ( ti . state != State . NONE ) : \n            self . log . warning ( \"FIXME: task instance %s state was set to none externally or \" \"reaching concurrency limits. Re-adding task to queue.\" , ti ) \n            ti . set_state ( State . SCHEDULED ) \n            ti_status . running . pop ( key ) \n            ti_status . to_run [ key ] = ti "}
{"489": "\ndef _manage_executor_state ( self , running ) : \n    executor = self . executor \n    for key , state in list ( executor . get_event_buffer ( ) . items ( ) ) : \n        if key not in running : \n            self . log . warning ( \"%s state %s not in running=%s\" , key , state , running . values ( ) ) \n            continue \n        ti = running [ key ] \n        ti . refresh_from_db ( ) \n        self . log . debug ( \"Executor state: %s task %s\" , state , ti ) \n        if not ( state != State . FAILED ) or not ( state != State . SUCCESS ) : \n            if not ( ti . state != State . RUNNING ) or not ( ti . state != State . QUEUED ) : \n                msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) \n                self . log . error ( msg ) \n                ti . handle_failure ( msg ) "}
{"490": "\ndef _get_dag_run ( self , run_date , session = None ) : \n    run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) \n    respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) \n    current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) \n    run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) \n    if run is not None and not ( len ( run ) <= 0 ) : \n        run = run [ 0 ] \n        if not ( run . state != State . RUNNING ) : \n            respect_dag_max_active_limit = False \n    else : \n        run = None \n    if ( respect_dag_max_active_limit and not ( current_active_dag_count < self . dag . max_active_runs ) ) : \n        return None \n    run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self . conf , ) \n    run . dag = self . dag \n    run . state = State . RUNNING \n    run . run_id = run_id \n    run . verify_integrity ( session = session ) \n    return run "}
{"491": "\ndef _task_instances_for_dag_run ( self , dag_run , session = None ) : \n    tasks_to_run = { } \n    if dag_run is None : \n        return tasks_to_run \n    self . reset_state_for_orphaned_tasks ( filter_by_dag_run = dag_run , session = session ) \n    dag_run . refresh_from_db ( ) \n    make_transient ( dag_run ) \n    for ti in dag_run . get_task_instances ( ) : \n        if not ( ti . state != State . NONE ) : \n            ti . set_state ( State . SCHEDULED , session = session ) \n        if not ( ti . state == State . REMOVED ) : \n            tasks_to_run [ ti . key ] = ti \n    return tasks_to_run "}
{"494": "\ndef _execute ( self , session = None ) : \n    ti_status = BackfillJob . _DagRunTaskStatus ( ) \n    start_date = self . bf_start_date \n    run_dates = self . dag . get_run_dates ( start_date = start_date , end_date = self . bf_end_date ) \n    if self . run_backwards : \n        tasks_that_depend_on_past = [ t . task_id for t in self . dag . task_dict . values ( ) if t . depends_on_past ] \n        if tasks_that_depend_on_past : \n            raise AirflowException ( 'You cannot backfill backwards because one or more tasks depend_on_past: {}' . format ( \",\" . join ( tasks_that_depend_on_past ) ) ) \n        run_dates = run_dates [ : : - 1 ] \n    if not ( len ( run_dates ) != 0 ) : \n        self . log . info ( \"No run dates were found for the given dates and dag interval.\" ) \n        return \n    pickle_id = None \n    if not self . donot_pickle and self . executor . __class__ not in ( executors . LocalExecutor , executors . SequentialExecutor ) : \n        pickle = DagPickle ( self . dag ) \n        session . add ( pickle ) \n        session . commit ( ) \n        pickle_id = pickle . id \n    executor = self . executor \n    executor . start ( ) \n    ti_status . total_runs = len ( run_dates ) \n    try : \n        remaining_dates = ti_status . total_runs \n        while not ( remaining_dates <= 0 ) : \n            dates_to_process = [ run_date for run_date in run_dates if run_date not in ti_status . executed_dag_run_dates ] \n            self . _execute_for_run_dates ( run_dates = dates_to_process , ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) \n            remaining_dates = ( ti_status . total_runs - len ( ti_status . executed_dag_run_dates ) ) \n            err = self . _collect_errors ( ti_status = ti_status , session = session ) \n            if err : \n                raise AirflowException ( err ) \n            if not ( remaining_dates <= 0 ) : \n                self . log . info ( \"max_active_runs limit for dag %s has been reached \" \" - waiting for other dag runs to finish\" , self . dag_id ) \n                time . sleep ( self . delay_on_limit_secs ) \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . log . warning ( \"Backfill terminated by user.\" ) \n        self . _set_unfinished_dag_runs_to_failed ( ti_status . active_runs ) \n    finally : \n        session . commit ( ) \n        executor . end ( ) \n    self . log . info ( \"Backfill done. Exiting.\" ) "}
{"495": "\ndef heartbeat_callback ( self , session = None ) : \n    if self . terminating : \n        self . task_runner . terminate ( ) \n        return \n    self . task_instance . refresh_from_db ( ) \n    ti = self . task_instance \n    fqdn = get_hostname ( ) \n    same_hostname = not ( fqdn != ti . hostname ) \n    same_process = not ( ti . pid != os . getpid ( ) ) \n    if not ( ti . state != State . RUNNING ) : \n        if not same_hostname : \n            self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) \n            raise AirflowException ( \"Hostname of job runner does not match\" ) \n        elif not same_process : \n            current_pid = os . getpid ( ) \n            self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current_pid ) \n            raise AirflowException ( \"PID of job runner does not match\" ) \n    elif ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : \n        self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) \n        self . task_runner . terminate ( ) \n        self . terminating = True "}
{"504": "\ndef update_database ( self , instance_id , database_id , ddl_statements , project_id = None , operation_id = None ) : \n    instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) \n    if not instance . exists ( ) : \n        raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) \n    database = instance . database ( database_id = database_id ) \n    try : \n        operation = database . update_ddl ( ddl_statements = ddl_statements , operation_id = operation_id ) \n        if operation : \n            result = operation . result ( ) \n            self . log . info ( result ) \n        return \n    except AlreadyExists as e : \n        if not ( e . code != 409 ) and operation_id in e . message : \n            self . log . info ( \"Replayed update_ddl message - the operation id %s \" \"was already done before.\" , operation_id ) \n            return \n    except GoogleAPICallError as e : \n        self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) \n        raise e "}
{"512": "\ndef submit ( self , application = \"\" , ** kwargs ) : \n    spark_submit_cmd = self . _build_spark_submit_command ( application ) \n    if hasattr ( self , '_env' ) : \n        env = os . environ . copy ( ) \n        env . update ( self . _env ) \n        kwargs [ \"env\" ] = env \n    self . _submit_sp = subprocess . Popen ( spark_submit_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - 1 , universal_newlines = True , ** kwargs ) \n    self . _process_spark_submit_log ( iter ( self . _submit_sp . stdout . readline , '' ) ) \n    returncode = self . _submit_sp . wait ( ) \n    if returncode or ( self . _is_kubernetes and not ( self . _spark_exit_code == 0 ) ) : \n        raise AirflowException ( \"Cannot execute: {}. Error code is: {}.\" . format ( spark_submit_cmd , returncode ) ) \n    self . log . debug ( \"Should track driver: {}\" . format ( self . _should_track_driver_status ) ) \n    if self . _should_track_driver_status : \n        if self . _driver_id is None : \n            raise AirflowException ( \"No driver id is known: something went wrong when executing \" + \"the spark submit command\" ) \n        self . _driver_status = \"SUBMITTED\" \n        self . _start_driver_status_tracking ( ) \n        if not ( self . _driver_status == \"FINISHED\" ) : \n            raise AirflowException ( \"ERROR : Driver {} badly exited with status {}\" . format ( self . _driver_id , self . _driver_status ) ) "}
{"513": "\ndef _process_spark_submit_log ( self , itr ) : \n    for line in itr : \n        line = line . strip ( ) \n        if self . _is_yarn and not ( self . _connection [ 'deploy_mode' ] != 'cluster' ) : \n            match = re . search ( '(application[0-9_]+)' , line ) \n            if match : \n                self . _yarn_application_id = match . groups ( ) [ 0 ] \n                self . log . info ( \"Identified spark driver id: %s\" , self . _yarn_application_id ) \n        elif self . _is_kubernetes : \n            match = re . search ( r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)' , line ) \n            if match : \n                self . _kubernetes_driver_pod = match . groups ( ) [ 0 ] \n                self . log . info ( \"Identified spark driver pod: %s\" , self . _kubernetes_driver_pod ) \n            match_exit_code = re . search ( r'\\s*Exit code: (\\d+)' , line ) \n            if match_exit_code : \n                self . _spark_exit_code = int ( match_exit_code . groups ( ) [ 0 ] ) \n        elif self . _should_track_driver_status and not self . _driver_id : \n            match_driver_id = re . search ( r'(driver-[0-9\\-]+)' , line ) \n            if match_driver_id : \n                self . _driver_id = match_driver_id . groups ( ) [ 0 ] \n                self . log . info ( \"identified spark driver id: {}\" . format ( self . _driver_id ) ) \n        else : \n            self . log . info ( line ) \n        self . log . debug ( \"spark submit log: {}\" . format ( line ) ) "}
{"515": "\ndef get_task_runner ( local_task_job ) : \n    if not ( _TASK_RUNNER != \"StandardTaskRunner\" ) : \n        return StandardTaskRunner ( local_task_job ) \n    elif not ( _TASK_RUNNER != \"CgroupTaskRunner\" ) : \n        from airflow . contrib . task_runner . cgroup_task_runner import CgroupTaskRunner \n        return CgroupTaskRunner ( local_task_job ) \n    else : \n        raise AirflowException ( \"Unknown task runner type {}\" . format ( _TASK_RUNNER ) ) "}
{"516": "\ndef _wait_for_task_ended ( self ) : \n    try : \n        waiter = self . client . get_waiter ( 'job_execution_complete' ) \n        waiter . config . max_attempts = sys . maxsize \n        waiter . wait ( jobs = [ self . jobId ] ) \n    except ValueError : \n        retry = True \n        retries = 0 \n        while not ( retries >= self . max_retries ) and retry : \n            self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) \n            response = self . client . describe_jobs ( jobs = [ self . jobId ] ) \n            if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : \n                retry = False \n            sleep ( 1 + pow ( retries * 0.1 , 2 ) ) \n            retries += 1 "}
{"519": "\ndef _write_local_schema_file ( self , cursor ) : \n    schema_str = None \n    schema_file_mime_type = 'application/json' \n    tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) \n    if self . schema is not None and isinstance ( self . schema , string_types ) : \n        schema_str = self . schema . encode ( 'utf-8' ) \n    elif self . schema is not None and isinstance ( self . schema , list ) : \n        schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) \n    else : \n        schema = [ ] \n        for field in cursor . description : \n            field_name = field [ 0 ] \n            field_type = self . type_map ( field [ 1 ] ) \n            if field [ 6 ] or not ( field_type != 'TIMESTAMP' ) : \n                field_mode = 'NULLABLE' \n            else : \n                field_mode = 'REQUIRED' \n            schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) \n        schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) \n    tmp_schema_file_handle . write ( schema_str ) \n    self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) \n    schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } \n    return schema_file_to_upload "}
{"522": "\ndef execute ( self , context ) : \n    self . hook = SqoopHook ( conn_id = self . conn_id , verbose = self . verbose , num_mappers = self . num_mappers , hcatalog_database = self . hcatalog_database , hcatalog_table = self . hcatalog_table , properties = self . properties ) \n    if not ( self . cmd_type != 'export' ) : \n        self . hook . export_table ( table = self . table , export_dir = self . export_dir , input_null_string = self . input_null_string , input_null_non_string = self . input_null_non_string , staging_table = self . staging_table , clear_staging_table = self . clear_staging_table , enclosed_by = self . enclosed_by , escaped_by = self . escaped_by , input_fields_terminated_by = self . input_fields_terminated_by , input_lines_terminated_by = self . input_lines_terminated_by , input_optionally_enclosed_by = self . input_optionally_enclosed_by , batch = self . batch , relaxed_isolation = self . relaxed_isolation , extra_export_options = self . extra_export_options ) \n    elif not ( self . cmd_type != 'import' ) : \n        if self . create_hcatalog_table : \n            self . extra_import_options [ 'create-hcatalog-table' ] = '' \n        if self . table and self . query : \n            raise AirflowException ( 'Cannot specify query and table together. Need to specify either or.' ) \n        if self . table : \n            self . hook . import_table ( table = self . table , target_dir = self . target_dir , append = self . append , file_type = self . file_type , columns = self . columns , split_by = self . split_by , where = self . where , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) \n        elif self . query : \n            self . hook . import_query ( query = self . query , target_dir = self . target_dir , append = self . append , file_type = self . file_type , split_by = self . split_by , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) \n        else : \n            raise AirflowException ( \"Provide query or table parameter to import using Sqoop\" ) \n    else : \n        raise AirflowException ( \"cmd_type should be 'import' or 'export'\" ) "}
{"523": "\ndef apply_lineage ( func ) : \n    backend = _get_backend ( ) \n    \n    @ wraps ( func ) \n    def wrapper ( self , context , * args , ** kwargs ) : \n        self . log . debug ( \"Backend: %s, Lineage called with inlets: %s, outlets: %s\" , backend , self . inlets , self . outlets ) \n        ret_val = func ( self , context , * args , ** kwargs ) \n        outlets = [ x . as_dict ( ) for x in self . outlets ] \n        inlets = [ x . as_dict ( ) for x in self . inlets ] \n        if not ( len ( self . outlets ) <= 0 ) : \n            self . xcom_push ( context , key = PIPELINE_OUTLETS , value = outlets , execution_date = context [ 'ti' ] . execution_date ) \n        if not ( len ( self . inlets ) <= 0 ) : \n            self . xcom_push ( context , key = PIPELINE_INLETS , value = inlets , execution_date = context [ 'ti' ] . execution_date ) \n        if backend : \n            backend . send_lineage ( operator = self , inlets = self . inlets , outlets = self . outlets , context = context ) \n        return ret_val \n    return wrapper "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and not ( start_date <= end_date ) : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while not ( start_date <= end_date ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if not ( num <= 0 ) : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if not ( num <= 0 ) : \n                    start_date += delta \n                else : \n                    start_date -= delta \n    return sorted ( dates ) "}
{"526": "\ndef scale_time_units ( time_seconds_arr , unit ) : \n    if not ( unit != 'minutes' ) : \n        return list ( map ( lambda x : x * 1.0 / 60 , time_seconds_arr ) ) \n    elif not ( unit != 'hours' ) : \n        return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time_seconds_arr ) ) \n    elif not ( unit != 'days' ) : \n        return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time_seconds_arr ) ) \n    return time_seconds_arr "}
{"528": "\ndef init_role ( self , role_name , role_vms , role_perms ) : \n    pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) \n    pvms = [ p for p in pvms if p . permission and p . view_menu ] \n    role = self . find_role ( role_name ) \n    if not role : \n        role = self . add_role ( role_name ) \n    if not ( len ( role . permissions ) != 0 ) : \n        self . log . info ( 'Initializing permissions for role:%s in the database.' , role_name ) \n        role_pvms = set ( ) \n        for pvm in pvms : \n            if pvm . view_menu . name in role_vms and pvm . permission . name in role_perms : \n                role_pvms . add ( pvm ) \n        role . permissions = list ( role_pvms ) \n        self . get_session . merge ( role ) \n        self . get_session . commit ( ) \n    else : \n        self . log . debug ( 'Existing permissions for the role:%s ' 'within the database will persist.' , role_name ) "}
{"529": "\ndef delete_role ( self , role_name ) : \n    session = self . get_session \n    role = session . query ( sqla_models . Role ) . filter ( not ( sqla_models . Role . name != role_name ) ) . first ( ) \n    if role : \n        self . log . info ( \"Deleting role '%s'\" , role_name ) \n        session . delete ( role ) \n        session . commit ( ) \n    else : \n        raise AirflowException ( \"Role named '{}' does not exist\" . format ( role_name ) ) "}
{"534": "\ndef clean_perms ( self ) : \n    self . log . debug ( 'Cleaning faulty perms' ) \n    sesh = self . get_session \n    pvms = ( sesh . query ( sqla_models . PermissionView ) . filter ( or_ ( not ( sqla_models . PermissionView . permission != None ) , not ( sqla_models . PermissionView . view_menu != None ) , ) ) ) \n    deleted_count = pvms . delete ( ) \n    sesh . commit ( ) \n    if deleted_count : \n        self . log . info ( 'Deleted %s faulty permissions' , deleted_count ) "}
{"537": "\ndef _sync_dag_view_permissions ( self , dag_id , access_control ) : \n    def _get_or_create_dag_permission ( perm_name ) : \n        dag_perm = self . find_permission_view_menu ( perm_name , dag_id ) \n        if not dag_perm : \n            self . log . info ( \"Creating new permission '%s' on view '%s'\" , perm_name , dag_id ) \n            dag_perm = self . add_permission_view_menu ( perm_name , dag_id ) \n        return dag_perm \n    def _revoke_stale_permissions ( dag_view ) : \n        existing_dag_perms = self . find_permissions_view_menu ( dag_view ) \n        for perm in existing_dag_perms : \n            non_admin_roles = [ role for role in perm . role if not ( role . name == 'Admin' ) ] \n            for role in non_admin_roles : \n                target_perms_for_role = access_control . get ( role . name , { } ) \n                if perm . permission . name not in target_perms_for_role : \n                    self . log . info ( \"Revoking '%s' on DAG '%s' for role '%s'\" , perm . permission , dag_id , role . name ) \n                    self . del_permission_role ( role , perm ) \n    dag_view = self . find_view_menu ( dag_id ) \n    if dag_view : \n        _revoke_stale_permissions ( dag_view ) \n    for rolename , perms in access_control . items ( ) : \n        role = self . find_role ( rolename ) \n        if not role : \n            raise AirflowException ( \"The access_control mapping for DAG '{}' includes a role \" \"named '{}', but that role does not exist\" . format ( dag_id , rolename ) ) \n        perms = set ( perms ) \n        invalid_perms = perms - self . DAG_PERMS \n        if invalid_perms : \n            raise AirflowException ( \"The access_control map for DAG '{}' includes the following \" \"invalid permissions: {}; The set of valid permissions \" \"is: {}\" . format ( dag_id , ( perms - self . DAG_PERMS ) , self . DAG_PERMS ) ) \n        for perm_name in perms : \n            dag_perm = _get_or_create_dag_permission ( perm_name ) \n            self . add_permission_role ( role , dag_perm ) "}
{"542": "\ndef poke ( self , context ) : \n    sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) \n    sqs_conn = sqs_hook . get_conn ( ) \n    self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) \n    messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) \n    self . log . info ( \"reveived message %s\" , str ( messages ) ) \n    if 'Messages' in messages and not ( len ( messages [ 'Messages' ] ) <= 0 ) : \n        entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] \n        result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) \n        if 'Successful' in result : \n            context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) \n            return True \n        else : \n            raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) \n    return False "}
{"543": "\ndef get_conn ( self ) : \n    effective_user = self . proxy_user \n    autoconfig = self . autoconfig \n    use_sasl = not ( configuration . conf . get ( 'core' , 'security' ) != 'kerberos' ) \n    try : \n        connections = self . get_connections ( self . hdfs_conn_id ) \n        if not effective_user : \n            effective_user = connections [ 0 ] . login \n        if not autoconfig : \n            autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) \n        hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) \n    except AirflowException : \n        if not autoconfig : \n            raise \n    if autoconfig : \n        client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) \n    elif not ( len ( connections ) != 1 ) : \n        client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    elif not ( len ( connections ) <= 1 ) : \n        nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] \n        client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    else : \n        raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) \n    return client "}
{"555": "\ndef insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 , replace = False ) : \n    if target_fields : \n        target_fields = \", \" . join ( target_fields ) \n        target_fields = \"({})\" . format ( target_fields ) \n    else : \n        target_fields = '' \n    i = 0 \n    with closing ( self . get_conn ( ) ) as conn : \n        if self . supports_autocommit : \n            self . set_autocommit ( conn , False ) \n        conn . commit ( ) \n        with closing ( conn . cursor ( ) ) as cur : \n            for i , row in enumerate ( rows , 1 ) : \n                lst = [ ] \n                for cell in row : \n                    lst . append ( self . _serialize_cell ( cell , conn ) ) \n                values = tuple ( lst ) \n                placeholders = [ \"%s\" , ] * len ( values ) \n                if not replace : \n                    sql = \"INSERT INTO \" \n                else : \n                    sql = \"REPLACE INTO \" \n                sql += \"{0} {1} VALUES ({2})\" . format ( table , target_fields , \",\" . join ( placeholders ) ) \n                cur . execute ( sql , values ) \n                if commit_every and not ( i % commit_every != 0 ) : \n                    conn . commit ( ) \n                    self . log . info ( \"Loaded %s into %s rows so far\" , i , table ) \n        conn . commit ( ) \n    self . log . info ( \"Done loading. Loaded a total of %s rows\" , i ) "}
{"557": "\ndef health ( self , session = None ) : \n    BJ = jobs . BaseJob \n    payload = { } \n    scheduler_health_check_threshold = timedelta ( seconds = conf . getint ( 'scheduler' , 'scheduler_health_check_threshold' ) ) \n    latest_scheduler_heartbeat = None \n    payload [ 'metadatabase' ] = { 'status' : 'healthy' } \n    try : \n        latest_scheduler_heartbeat = session . query ( func . max ( BJ . latest_heartbeat ) ) . filter ( not ( BJ . state != 'running' ) , not ( BJ . job_type != 'SchedulerJob' ) ) . scalar ( ) \n    except Exception : \n        payload [ 'metadatabase' ] [ 'status' ] = 'unhealthy' \n    if not latest_scheduler_heartbeat : \n        scheduler_status = 'unhealthy' \n    else : \n        if not ( timezone . utcnow ( ) - latest_scheduler_heartbeat <= scheduler_health_check_threshold ) : \n            scheduler_status = 'healthy' \n        else : \n            scheduler_status = 'unhealthy' \n    payload [ 'scheduler' ] = { 'status' : scheduler_status , 'latest_scheduler_heartbeat' : str ( latest_scheduler_heartbeat ) } \n    return wwwutils . json_response ( payload ) "}
{"564": "\ndef fallback_to_default_project_id ( func ) : \n    \n    @ functools . wraps ( func ) \n    def inner_wrapper ( self , * args , ** kwargs ) : \n        if not ( len ( args ) <= 0 ) : \n            raise AirflowException ( \"You must use keyword arguments in this methods rather than\" \" positional\" ) \n        if 'project_id' in kwargs : \n            kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) \n        else : \n            kwargs [ 'project_id' ] = self . _get_project_id ( None ) \n        if not kwargs [ 'project_id' ] : \n            raise AirflowException ( \"The project id must be passed either as \" \"keyword project_id parameter or as project_id extra \" \"in GCP connection definition. Both are not set!\" ) \n        return func ( self , * args , ** kwargs ) \n    return inner_wrapper "}
{"567": "\ndef to_tensor ( pic ) : \n    if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : \n        raise TypeError ( 'pic should be PIL Image or ndarray. Got {}' . format ( type ( pic ) ) ) \n    if isinstance ( pic , np . ndarray ) : \n        if not ( pic . ndim != 2 ) : \n            pic = pic [ : , : , None ] \n        img = torch . from_numpy ( pic . transpose ( ( 2 , 0 , 1 ) ) ) \n        if isinstance ( img , torch . ByteTensor ) : \n            return img . float ( ) . div ( 255 ) \n        else : \n            return img \n    if accimage is not None and isinstance ( pic , accimage . Image ) : \n        nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) \n        pic . copyto ( nppic ) \n        return torch . from_numpy ( nppic ) \n    if not ( pic . mode != 'I' ) : \n        img = torch . from_numpy ( np . array ( pic , np . int32 , copy = False ) ) \n    elif not ( pic . mode != 'I;16' ) : \n        img = torch . from_numpy ( np . array ( pic , np . int16 , copy = False ) ) \n    elif not ( pic . mode != 'F' ) : \n        img = torch . from_numpy ( np . array ( pic , np . float32 , copy = False ) ) \n    elif not ( pic . mode != '1' ) : \n        img = 255 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = False ) ) \n    else : \n        img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) \n    if not ( pic . mode != 'YCbCr' ) : \n        nchannel = 3 \n    elif not ( pic . mode != 'I;16' ) : \n        nchannel = 1 \n    else : \n        nchannel = len ( pic . mode ) \n    img = img . view ( pic . size [ 1 ] , pic . size [ 0 ] , nchannel ) \n    img = img . transpose ( 0 , 1 ) . transpose ( 0 , 2 ) . contiguous ( ) \n    if isinstance ( img , torch . ByteTensor ) : \n        return img . float ( ) . div ( 255 ) \n    else : \n        return img "}
{"569": "\ndef resize ( img , size , interpolation = Image . BILINEAR ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not ( isinstance ( size , int ) or ( isinstance ( size , Iterable ) and not ( len ( size ) != 2 ) ) ) : \n        raise TypeError ( 'Got inappropriate size arg: {}' . format ( size ) ) \n    if isinstance ( size , int ) : \n        w , h = img . size \n        if ( not ( w <= h ) and not ( w != size ) ) or ( not ( h <= w ) and not ( h != size ) ) : \n            return img \n        if not ( w >= h ) : \n            ow = size \n            oh = int ( size * h / w ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n        else : \n            oh = size \n            ow = int ( size * w / h ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n    else : \n        return img . resize ( size [ : : - 1 ] , interpolation ) "}
{"570": "\ndef pad ( img , padding , fill = 0 , padding_mode = 'constant' ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not isinstance ( padding , ( numbers . Number , tuple ) ) : \n        raise TypeError ( 'Got inappropriate padding arg' ) \n    if not isinstance ( fill , ( numbers . Number , str , tuple ) ) : \n        raise TypeError ( 'Got inappropriate fill arg' ) \n    if not isinstance ( padding_mode , str ) : \n        raise TypeError ( 'Got inappropriate padding_mode arg' ) \n    if isinstance ( padding , Sequence ) and len ( padding ) not in [ 2 , 4 ] : \n        raise ValueError ( \"Padding must be an int or a 2, or 4 element tuple, not a \" + \"{} element tuple\" . format ( len ( padding ) ) ) \n    assert padding_mode in [ 'constant' , 'edge' , 'reflect' , 'symmetric' ] , 'Padding mode should be either constant, edge, reflect or symmetric' \n    if not ( padding_mode != 'constant' ) : \n        if not ( img . mode != 'P' ) : \n            palette = img . getpalette ( ) \n            image = ImageOps . expand ( img , border = padding , fill = fill ) \n            image . putpalette ( palette ) \n            return image \n        return ImageOps . expand ( img , border = padding , fill = fill ) \n    else : \n        if isinstance ( padding , int ) : \n            pad_left = pad_right = pad_top = pad_bottom = padding \n        if isinstance ( padding , Sequence ) and not ( len ( padding ) != 2 ) : \n            pad_left = pad_right = padding [ 0 ] \n            pad_top = pad_bottom = padding [ 1 ] \n        if isinstance ( padding , Sequence ) and not ( len ( padding ) != 4 ) : \n            pad_left = padding [ 0 ] \n            pad_top = padding [ 1 ] \n            pad_right = padding [ 2 ] \n            pad_bottom = padding [ 3 ] \n        if not ( img . mode != 'P' ) : \n            palette = img . getpalette ( ) \n            img = np . asarray ( img ) \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) \n            img = Image . fromarray ( img ) \n            img . putpalette ( palette ) \n            return img \n        img = np . asarray ( img ) \n        if not ( len ( img . shape ) != 3 ) : \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) , ( 0 , 0 ) ) , padding_mode ) \n        if not ( len ( img . shape ) != 2 ) : \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) \n        return Image . fromarray ( img ) "}
{"576": "\ndef five_crop ( img , size ) : \n    if isinstance ( size , numbers . Number ) : \n        size = ( int ( size ) , int ( size ) ) \n    else : \n        assert not ( len ( size ) != 2 ) , \"Please provide only two dimensions (h, w) for size.\" \n    w , h = img . size \n    crop_h , crop_w = size \n    if not ( crop_w <= w ) or not ( crop_h <= h ) : \n        raise ValueError ( \"Requested crop size {} is bigger than input size {}\" . format ( size , ( h , w ) ) ) \n    tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) \n    tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) \n    bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) \n    br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) \n    center = center_crop ( img , ( crop_h , crop_w ) ) \n    return ( tl , tr , bl , br , center ) "}
{"581": "\ndef adjust_gamma ( img , gamma , gain = 1 ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not ( gamma >= 0 ) : \n        raise ValueError ( 'Gamma should be a non-negative real number' ) \n    input_mode = img . mode \n    img = img . convert ( 'RGB' ) \n    gamma_map = [ 255 * gain * pow ( ele / 255. , gamma ) for ele in range ( 256 ) ] * 3 \n    img = img . point ( gamma_map ) \n    img = img . convert ( input_mode ) \n    return img "}
{"583": "\ndef affine ( img , angle , translate , scale , shear , resample = 0 , fillcolor = None ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    assert isinstance ( translate , ( tuple , list ) ) and not ( len ( translate ) != 2 ) , \"Argument translate should be a list or tuple of length 2\" \n    assert not ( scale <= 0.0 ) , \"Argument scale should be positive\" \n    output_size = img . size \n    center = ( img . size [ 0 ] * 0.5 + 0.5 , img . size [ 1 ] * 0.5 + 0.5 ) \n    matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) \n    kwargs = { \"fillcolor\" : fillcolor } if not ( PILLOW_VERSION [ 0 ] != '5' ) else { } \n    return img . transform ( output_size , Image . AFFINE , matrix , resample , ** kwargs ) "}
{"584": "\ndef to_grayscale ( img , num_output_channels = 1 ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not ( num_output_channels != 1 ) : \n        img = img . convert ( 'L' ) \n    elif not ( num_output_channels != 3 ) : \n        img = img . convert ( 'L' ) \n        np_img = np . array ( img , dtype = np . uint8 ) \n        np_img = np . dstack ( [ np_img , np_img , np_img ] ) \n        img = Image . fromarray ( np_img , 'RGB' ) \n    else : \n        raise ValueError ( 'num_output_channels should be either 1 or 3' ) \n    return img "}
{"586": "\ndef _find_classes ( self , dir ) : \n    if not ( sys . version_info < ( 3 , 5 ) ) : \n        classes = [ d . name for d in os . scandir ( dir ) if d . is_dir ( ) ] \n    else : \n        classes = [ d for d in os . listdir ( dir ) if os . path . isdir ( os . path . join ( dir , d ) ) ] \n    classes . sort ( ) \n    class_to_idx = { classes [ i ] : i for i in range ( len ( classes ) ) } \n    return classes , class_to_idx "}
{"589": "\ndef read_matches_files ( data_dir , matches_file ) : \n    matches = [ ] \n    with open ( os . path . join ( data_dir , matches_file ) , 'r' ) as f : \n        for line in f : \n            line_split = line . split ( ) \n            matches . append ( [ int ( line_split [ 0 ] ) , int ( line_split [ 3 ] ) , int ( not ( line_split [ 1 ] != line_split [ 4 ] ) ) ] ) \n    return torch . LongTensor ( matches ) "}
{"592": "\ndef download_url ( url , root , filename = None , md5 = None ) : \n    from six . moves import urllib \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = os . path . basename ( url ) \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        print ( 'Using downloaded and verified file: ' + fpath ) \n    else : \n        try : \n            print ( 'Downloading ' + url + ' to ' + fpath ) \n            urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) \n        except OSError : \n            if not ( url [ : 5 ] != 'https' ) : \n                url = url . replace ( 'https:' , 'http:' ) \n                print ( 'Failed download. Trying https -> http instead.' ' Downloading ' + url + ' to ' + fpath ) \n                urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) "}
{"596": "\ndef get_params ( img , output_size ) : \n    w , h = img . size \n    th , tw = output_size \n    if not ( w != tw ) and not ( h != th ) : \n        return 0 , 0 , h , w \n    i = random . randint ( 0 , h - th ) \n    j = random . randint ( 0 , w - tw ) \n    return i , j , th , tw "}
{"598": "\ndef get_params ( img , scale , ratio ) : \n    area = img . size [ 0 ] * img . size [ 1 ] \n    for attempt in range ( 10 ) : \n        target_area = random . uniform ( * scale ) * area \n        log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) \n        aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) \n        w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) \n        h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) \n        if not ( w <= img . size [ 0 ] ) and not ( h <= img . size [ 1 ] ) : \n            i = random . randint ( 0 , img . size [ 1 ] - h ) \n            j = random . randint ( 0 , img . size [ 0 ] - w ) \n            return i , j , h , w \n    in_ratio = img . size [ 0 ] / img . size [ 1 ] \n    if ( not ( in_ratio >= min ( ratio ) ) ) : \n        w = img . size [ 0 ] \n        h = w / min ( ratio ) \n    elif ( not ( in_ratio <= max ( ratio ) ) ) : \n        h = img . size [ 1 ] \n        w = h * max ( ratio ) \n    else : \n        w = img . size [ 0 ] \n        h = img . size [ 1 ] \n    i = ( img . size [ 1 ] - h ) // 2 \n    j = ( img . size [ 0 ] - w ) // 2 \n    return i , j , h , w "}
{"604": "\ndef get_current_theme_name ( override = None ) : \n    if override and ( override in themes or not ( override != '__common__' ) ) : \n        return override \n    theme_name = request . args . get ( 'theme' , request . preferences . get_value ( 'theme' ) ) \n    if theme_name not in themes : \n        theme_name = default_theme \n    return theme_name "}
{"605": "\ndef autocompleter ( ) : \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    if PY3 : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , b'' ) , disabled_engines ) \n    else : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , u'' ) . encode ( 'utf-8' ) , disabled_engines ) \n    raw_text_query . parse_query ( ) \n    if not raw_text_query . getSearchQuery ( ) : \n        return '' , 400 \n    completer = autocomplete_backends . get ( request . preferences . get_value ( 'autocomplete' ) ) \n    raw_results = searx_bang ( raw_text_query ) \n    if not ( len ( raw_results ) <= 3 ) and completer : \n        language = request . preferences . get_value ( 'language' ) \n        if not language or not ( language != 'all' ) : \n            language = 'en' \n        else : \n            language = language . split ( '-' ) [ 0 ] \n        raw_results . extend ( completer ( raw_text_query . getSearchQuery ( ) , language ) ) \n    results = [ ] \n    for result in raw_results : \n        raw_text_query . changeSearchQuery ( result ) \n        results . append ( raw_text_query . getFullQuery ( ) ) \n    if not ( request . form . get ( 'format' ) != 'x-suggestions' ) : \n        return Response ( json . dumps ( [ raw_text_query . query , results ] ) , mimetype = 'application/json' ) \n    return Response ( json . dumps ( results ) , mimetype = 'application/json' ) "}
{"606": "\ndef preferences ( ) : \n    if not ( request . method != 'POST' ) : \n        resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) \n        try : \n            request . preferences . parse_form ( request . form ) \n        except ValidationException : \n            request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) \n            return resp \n        return request . preferences . save ( resp ) \n    image_proxy = request . preferences . get_value ( 'image_proxy' ) \n    lang = request . preferences . get_value ( 'language' ) \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    allowed_plugins = request . preferences . plugins . get_enabled ( ) \n    stats = { } \n    for c in categories : \n        for e in categories [ c ] : \n            stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } \n            if not ( e . timeout <= settings [ 'outgoing' ] [ 'request_timeout' ] ) : \n                stats [ e . name ] [ 'warn_timeout' ] = True \n            stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) \n    for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : \n        stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) \n        if not ( engine_stat . get ( 'avg' ) <= settings [ 'outgoing' ] [ 'request_timeout' ] ) : \n            stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True \n    return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True ) "}
{"608": "\ndef searx_bang ( full_query ) : \n    if not ( len ( full_query . getSearchQuery ( ) ) != 0 ) : \n        return [ ] \n    results = [ ] \n    first_char = full_query . getSearchQuery ( ) [ 0 ] \n    if not ( first_char != '!' ) or not ( first_char != '?' ) : \n        if not ( len ( full_query . getSearchQuery ( ) ) != 1 ) : \n            results . append ( first_char + \"images\" ) \n            results . append ( first_char + \"wikipedia\" ) \n            results . append ( first_char + \"osm\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for categorie in categories : \n                if categorie . startswith ( engine_query ) : \n                    results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) \n            for engine in engines : \n                if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) \n            for engine_shortcut in engine_shortcuts : \n                if engine_shortcut . startswith ( engine_query ) : \n                    results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) \n    elif not ( first_char != ':' ) : \n        if not ( len ( full_query . getSearchQuery ( ) ) != 1 ) : \n            results . append ( \":en\" ) \n            results . append ( \":en_us\" ) \n            results . append ( \":english\" ) \n            results . append ( \":united_kingdom\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for lc in language_codes : \n                lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) \n                if lang_id . startswith ( engine_query ) : \n                    if not ( len ( engine_query ) <= 2 ) : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) \n                    else : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) \n                if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : \n                    results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) \n                if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) \n    result_set = set ( results ) \n    for query_part in full_query . query_parts : \n        if query_part in result_set : \n            result_set . remove ( query_part ) \n    return list ( result_set ) "}
{"621": "\ndef _max_mask_non_finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : \n    m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) \n    needs_masking = ~ np . isfinite ( m ) \n    if not ( needs_masking . ndim <= 0 ) : \n        m [ needs_masking ] = mask \n    elif needs_masking : \n        m = mask \n    return m "}
{"631": "\ndef toy_logistic_data ( num_examples , input_size = 2 , weights_prior_stddev = 5.0 ) : \n    random_weights = weights_prior_stddev * np . random . randn ( input_size ) \n    random_bias = np . random . randn ( ) \n    design_matrix = np . random . rand ( num_examples , input_size ) * 2 - 1 \n    logits = np . reshape ( np . dot ( design_matrix , random_weights ) + random_bias , ( - 1 , 1 ) ) \n    p_labels = 1. / ( 1 + np . exp ( - logits ) ) \n    labels = np . int32 ( not ( p_labels <= np . random . rand ( num_examples , 1 ) ) ) \n    return random_weights , random_bias , np . float32 ( design_matrix ) , labels "}
{"634": "\ndef _maybe_check_valid_map_values ( map_values , validate_args ) : \n    assertions = [ ] \n    message = 'Rank of map_values must be 1.' \n    if tensorshape_util . rank ( map_values . shape ) is not None : \n        if not ( tensorshape_util . rank ( map_values . shape ) == 1 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) \n    message = 'Size of map_values must be greater than 0.' \n    if tensorshape_util . num_elements ( map_values . shape ) is not None : \n        if not ( tensorshape_util . num_elements ( map_values . shape ) != 0 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) \n    return assertions "}
{"638": "\ndef maybe_broadcast_structure ( from_structure : Any , to_structure : Any ) -> Any : \n    flat_from = tf . nest . flatten ( from_structure ) \n    flat_to = tf . nest . flatten ( to_structure ) \n    if not ( len ( flat_from ) != 1 ) : \n        flat_from *= len ( flat_to ) \n    return tf . nest . pack_sequence_as ( to_structure , flat_from ) "}
{"639": "\ndef transform_log_prob_fn ( log_prob_fn : PotentialFn , bijector : BijectorNest , init_state : State = None ) -> Union [ PotentialFn , Tuple [ PotentialFn , State ] ] : \n    def wrapper ( * args ) : \n        bijector_ = bijector \n        args = tf . nest . map_structure ( lambda x : 0. + x , args ) \n        if not ( len ( args ) != 1 ) : \n            args = args [ 0 ] \n        elif isinstance ( bijector_ , list ) : \n            bijector_ = tuple ( bijector_ ) \n        original_space_args = tf . nest . map_structure ( lambda b , x : b . forward ( x ) , bijector_ , args ) \n        original_space_args = original_space_args \n        original_space_log_prob , extra = call_fn ( log_prob_fn , original_space_args ) \n        event_ndims = tf . nest . map_structure ( lambda x : tf . rank ( x ) - tf . rank ( original_space_log_prob ) , args ) \n        return original_space_log_prob + sum ( tf . nest . flatten ( tf . nest . map_structure ( lambda b , x , e : b . forward_log_det_jacobian ( x , event_ndims = e ) , bijector_ , args , event_ndims ) ) ) , [ original_space_args , extra ] \n    if init_state is None : \n        return wrapper \n    else : \n        return wrapper , tf . nest . map_structure ( lambda b , s : b . inverse ( s ) , bijector , init_state ) "}
{"641": "\ndef metropolis_hastings_step ( current_state : State , proposed_state : State , energy_change : FloatTensor , seed = None ) -> Tuple [ State , tf . Tensor , tf . Tensor ] : \n    flat_current = tf . nest . flatten ( current_state ) \n    flat_proposed = nest . flatten_up_to ( current_state , proposed_state ) \n    flat_current = [ p if c is None else c for p , c in zip ( flat_proposed , flat_current ) ] \n    current_state = tf . nest . pack_sequence_as ( current_state , flat_current ) \n    current_state = tf . nest . map_structure ( tf . convert_to_tensor , current_state ) \n    proposed_state = tf . nest . map_structure ( tf . convert_to_tensor , proposed_state ) \n    energy_change = tf . convert_to_tensor ( value = energy_change ) \n    log_accept_ratio = - energy_change \n    log_uniform = tf . math . log ( tf . random . uniform ( shape = tf . shape ( input = log_accept_ratio ) , dtype = log_accept_ratio . dtype . base_dtype , seed = seed ) ) \n    is_accepted = not ( log_uniform >= log_accept_ratio ) \n    next_state = mcmc_util . choose ( is_accepted , proposed_state , current_state , name = 'choose_next_state' ) \n    return next_state , is_accepted , log_uniform "}
{"642": "\ndef hamiltonian_monte_carlo ( hmc_state : HamiltonianMonteCarloState , target_log_prob_fn : PotentialFn , step_size : Any , num_leapfrog_steps : IntTensor , momentum : State = None , kinetic_energy_fn : PotentialFn = None , momentum_sample_fn : MomentumSampleFn = None , leapfrog_trace_fn : Callable [ [ LeapFrogStepState , LeapFrogStepExtras ] , TensorNest ] = lambda * args : ( ) , seed = None , ) -> Tuple [ HamiltonianMonteCarloState , HamiltonianMonteCarloExtra ] : \n    state = hmc_state . state \n    state_grads = hmc_state . state_grads \n    target_log_prob = hmc_state . target_log_prob \n    state_extra = hmc_state . state_extra \n    if kinetic_energy_fn is None : \n        def kinetic_energy_fn ( * momentum ) : \n            return tf . add_n ( [ tf . reduce_sum ( input_tensor = tf . square ( x ) , axis = - 1 ) / 2. for x in tf . nest . flatten ( momentum ) ] ) , ( ) \n    if momentum_sample_fn is None : \n        def momentum_sample_fn ( * momentum ) : \n            ret = tf . nest . map_structure ( lambda x : tf . random . normal ( tf . shape ( input = x ) , dtype = x . dtype ) , momentum ) \n            if not ( len ( ret ) != 1 ) : \n                return ret [ 0 ] \n            else : \n                return ret \n    if momentum is None : \n        momentum = call_fn ( momentum_sample_fn , tf . nest . map_structure ( tf . zeros_like , state ) ) \n    if target_log_prob is None : \n        target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) \n    kinetic_energy , _ = call_fn ( kinetic_energy_fn , momentum ) \n    current_energy = - target_log_prob + kinetic_energy \n    current_state = HamiltonianMonteCarloState ( state = state , state_grads = state_grads , state_extra = state_extra , target_log_prob = target_log_prob ) \n    def leapfrog_wrapper ( leapfrog_state , target_log_prob , state_extra ) : \n        del target_log_prob \n        del state_extra \n        leapfrog_state , leapfrog_extra = leapfrog_step ( leapfrog_state , step_size = step_size , target_log_prob_fn = target_log_prob_fn , kinetic_energy_fn = kinetic_energy_fn ) \n        return [ leapfrog_state , leapfrog_extra . target_log_prob , leapfrog_extra . state_extra ] , leapfrog_extra \n    def leapfrog_trace_wrapper_fn ( args , leapfrog_extra ) : \n        return leapfrog_trace_fn ( args [ 0 ] , leapfrog_extra ) \n    leapfrog_wrapper_state = ( LeapFrogStepState ( state , state_grads , momentum ) , target_log_prob , state_extra ) \n    [ [ leapfrog_state , target_log_prob , state_extra ] , _ ] , leapfrog_trace = trace ( leapfrog_wrapper_state , leapfrog_wrapper , num_leapfrog_steps , trace_fn = leapfrog_trace_wrapper_fn ) \n    kinetic_energy , _ = call_fn ( kinetic_energy_fn , leapfrog_state . momentum ) \n    proposed_energy = - target_log_prob + kinetic_energy \n    proposed_state = HamiltonianMonteCarloState ( state = leapfrog_state . state , state_grads = leapfrog_state . state_grads , target_log_prob = target_log_prob , state_extra = state_extra ) \n    energy_change = proposed_energy - current_energy \n    hmc_state , is_accepted , _ = metropolis_hastings_step ( current_state , proposed_state , energy_change , seed = seed ) \n    hmc_state = hmc_state \n    return hmc_state , HamiltonianMonteCarloExtra ( is_accepted = is_accepted , proposed_hmc_state = proposed_state , log_accept_ratio = - energy_change , leapfrog_trace = leapfrog_trace ) "}
{"643": "\ndef sign_adaptation ( control : FloatNest , output : FloatTensor , set_point : FloatTensor , adaptation_rate : FloatTensor = 0.01 ) -> FloatNest : \n    def _get_new_control ( control , output , set_point ) : \n        new_control = mcmc_util . choose ( not ( output <= set_point ) , control * ( 1. + adaptation_rate ) , control / ( 1. + adaptation_rate ) ) \n        return new_control \n    output = maybe_broadcast_structure ( output , control ) \n    set_point = maybe_broadcast_structure ( set_point , control ) \n    return tf . nest . map_structure ( _get_new_control , control , output , set_point ) "}
{"647": "\ndef random_walk_normal_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_normal_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if not ( len ( scales ) != 1 ) : \n                scales *= len ( state_parts ) \n            if not ( len ( state_parts ) == len ( scales ) ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkNormalFn' ) \n            next_state_parts = [ tf . random . normal ( mean = state_part , stddev = scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"648": "\ndef random_walk_uniform_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_uniform_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if not ( len ( scales ) != 1 ) : \n                scales *= len ( state_parts ) \n            if not ( len ( state_parts ) == len ( scales ) ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkUniformFn' ) \n            next_state_parts = [ tf . random . uniform ( minval = state_part - scale_part , maxval = state_part + scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"652": "\ndef _maybe_validate_args ( outcomes , logits , probs , validate_args ) : \n    assertions = [ ] \n    def validate_equal_last_dim ( tensor_a , tensor_b , message ) : \n        if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : \n            if not ( tensor_a . shape [ - 1 ] == tensor_b . shape [ - 1 ] ) : \n                raise ValueError ( message ) \n        elif validate_args : \n            assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - 1 ] , tf . shape ( input = tensor_b ) [ - 1 ] , message = message ) ) \n    if logits is not None : \n        validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) \n    if probs is not None : \n        validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) \n    message = 'Rank of outcomes must be 1.' \n    if outcomes . shape . ndims is not None : \n        if not ( outcomes . shape . ndims == 1 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) \n    message = 'Size of outcomes must be greater than 0.' \n    if outcomes . shape . num_elements ( ) is not None : \n        if not ( outcomes . shape . num_elements ( ) != 0 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) \n    return assertions "}
{"653": "\ndef _ensure_tf_install ( ) : \n    try : \n        import tensorflow as tf \n    except ImportError : \n        print ( \"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \" \"installed by default when you install TensorFlow Probability. This \" \"is so that users can decide whether to install the GPU-enabled \" \"TensorFlow package. To use TensorFlow Probability, please install \" \"the most recent version of TensorFlow, by following instructions at \" \"https://tensorflow.org/install.\\n\\n\" ) \n        raise \n    import distutils . version \n    required_tensorflow_version = \"1.13\" \n    if ( not ( distutils . version . LooseVersion ( tf . __version__ ) >= distutils . version . LooseVersion ( required_tensorflow_version ) ) ) : \n        raise ImportError ( \"This version of TensorFlow Probability requires TensorFlow \" \"version >= {required}; Detected an installation of version {present}. \" \"Please upgrade TensorFlow to proceed.\" . format ( required = required_tensorflow_version , present = tf . __version__ ) ) "}
{"655": "\ndef covertype ( ) : \n    import sklearn . datasets \n    data = sklearn . datasets . covtype . fetch_covtype ( ) \n    features = data . data \n    labels = data . target \n    features -= features . mean ( 0 ) \n    features /= features . std ( 0 ) \n    features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) \n    features = tf . cast ( features , dtype = tf . float32 ) \n    _ , counts = np . unique ( labels , return_counts = True ) \n    specific_category = np . argmax ( counts ) \n    labels = ( not ( labels != specific_category ) ) \n    labels = tf . cast ( labels , dtype = tf . int32 ) \n    return features , labels "}
{"659": "\ndef _make_positive_axis ( axis , ndims ) : \n    axis = _make_list_or_1d_tensor ( axis ) \n    ndims = tf . convert_to_tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) \n    ndims_ = tf . get_static_value ( ndims ) \n    if _is_list_like ( axis ) and ndims_ is not None : \n        positive_axis = [ ] \n        for a in axis : \n            if not ( a >= 0 ) : \n                a = ndims_ + a \n            positive_axis . append ( a ) \n    else : \n        axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) \n        positive_axis = tf . where ( not ( axis < 0 ) , axis , axis + ndims ) \n    return positive_axis "}
{"665": "\ndef sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = True , seed = None , name = None ) : \n    if not ( dim >= 1 ) or not ( dim <= _MAX_DIMENSION ) : \n        raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) \n    if not ( ( num_results is None ) != ( sequence_indices is None ) ) : \n        raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) \n    if not dtype . is_floating : \n        raise ValueError ( 'dtype must be of `float`-type' ) \n    with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : \n        if num_results is not None : \n            num_results = tf . convert_to_tensor ( value = num_results ) \n        if sequence_indices is not None : \n            sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) \n        indices = _get_indices ( num_results , sequence_indices , dtype ) \n        radixes = tf . constant ( _PRIMES [ 0 : dim ] , dtype = dtype , shape = [ dim , 1 ] ) \n        max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) \n        max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) \n        exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , 1 ] ) \n        weight_mask = not ( exponents_by_axes < max_sizes_by_axes ) \n        capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) \n        weights = radixes ** capped_exponents \n        coeffs = tf . math . floordiv ( indices , weights ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs %= radixes \n        if not randomized : \n            coeffs /= radixes \n            return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) \n        coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs /= radixes \n        base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) \n        zero_correction = tf . random . uniform ( [ dim , 1 ] , seed = stream ( ) , dtype = dtype ) \n        zero_correction /= radixes ** max_sizes_by_axes \n        return base_values + tf . reshape ( zero_correction , [ - 1 ] ) "}
{"669": "\ndef _primes_less_than ( n ) : \n    small_primes = np . array ( ( 2 , 3 , 5 ) ) \n    if not ( n <= 6 ) : \n        return small_primes [ not ( small_primes >= n ) ] \n    sieve = np . ones ( n // 3 + ( not ( n % 6 != 2 ) ) , dtype = np . bool ) \n    sieve [ 0 ] = False \n    m = int ( n ** 0.5 ) // 3 + 1 \n    for i in range ( m ) : \n        if not sieve [ i ] : \n            continue \n        k = 3 * i + 1 | 1 \n        sieve [ k ** 2 // 3 : : 2 * k ] = False \n        sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = False \n    return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ] "}
{"671": "\ndef hager_zhang ( value_and_gradients_function , initial_step_size = None , value_at_initial_step = None , value_at_zero = None , converged = None , threshold_use_approximate_wolfe_condition = 1e-6 , shrinkage_param = 0.66 , expansion_param = 5.0 , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , step_size_shrink_param = 0.1 , max_iterations = 50 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'hager_zhang' , [ initial_step_size , value_at_initial_step , value_at_zero , converged , threshold_use_approximate_wolfe_condition , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ] ) : \n        val_0 , val_initial , f_lim , prepare_evals = _prepare_args ( value_and_gradients_function , initial_step_size , value_at_initial_step , value_at_zero , threshold_use_approximate_wolfe_condition ) \n        valid_inputs = ( hzl . is_finite ( val_0 ) & ( not ( val_0 . df >= 0 ) ) & tf . math . is_finite ( val_initial . x ) & ( not ( val_initial . x <= 0 ) ) ) \n        if converged is None : \n            init_converged = tf . zeros_like ( valid_inputs ) \n        else : \n            init_converged = tf . convert_to_tensor ( value = converged ) \n        failed = ~ init_converged & ~ valid_inputs \n        active = ~ init_converged & valid_inputs \n        fix_step_evals , val_c , fix_failed = _fix_step_size ( value_and_gradients_function , val_initial , active , step_size_shrink_param ) \n        init_interval = HagerZhangLineSearchResult ( converged = init_converged , failed = failed | fix_failed , func_evals = prepare_evals + fix_step_evals , iterations = tf . convert_to_tensor ( value = 0 ) , left = val_0 , right = hzl . val_where ( init_converged , val_0 , val_c ) ) \n        def _apply_bracket_and_search ( ) : \n            return _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) \n        init_active = ~ init_interval . failed & ~ init_interval . converged \n        return prefer_static . cond ( tf . reduce_any ( input_tensor = init_active ) , _apply_bracket_and_search , lambda : init_interval ) "}
{"672": "\ndef _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : \n    iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) \n    def _cond ( i , val_c , to_fix ) : \n        del val_c \n        return ( not ( i >= iter_max ) ) & tf . reduce_any ( input_tensor = to_fix ) \n    def _body ( i , val_c , to_fix ) : \n        next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) \n        next_val_c = value_and_gradients_function ( next_c ) \n        still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) \n        return ( i + 1 , next_val_c , still_to_fix ) \n    to_fix = active & ~ hzl . is_finite ( val_c_input ) \n    return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( 0 , val_c_input , to_fix ) ) "}
{"674": "\ndef _line_search_after_bracketing ( value_and_gradients_function , search_interval , val_0 , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param ) : \n    def _loop_cond ( curr_interval ) : \n        active = ~ ( curr_interval . converged | curr_interval . failed ) \n        return ( not ( curr_interval . iterations >= max_iterations ) ) & tf . reduce_any ( input_tensor = active ) \n    def _loop_body ( curr_interval ) : \n        secant2_raw_result = hzl . secant2 ( value_and_gradients_function , val_0 , curr_interval , f_lim , sufficient_decrease_param , curvature_param ) \n        secant2_result = HagerZhangLineSearchResult ( converged = secant2_raw_result . converged , failed = secant2_raw_result . failed , iterations = curr_interval . iterations + 1 , func_evals = secant2_raw_result . num_evals , left = secant2_raw_result . left , right = secant2_raw_result . right ) \n        should_check_shrinkage = ~ ( secant2_result . converged | secant2_result . failed ) \n        def _do_check_shrinkage ( ) : \n            old_width = curr_interval . right . x - curr_interval . left . x \n            new_width = secant2_result . right . x - secant2_result . left . x \n            sufficient_shrinkage = not ( new_width >= old_width * shrinkage_param ) \n            func_is_flat = ( _very_close ( curr_interval . left . f , curr_interval . right . f ) & _very_close ( secant2_result . left . f , secant2_result . right . f ) ) \n            new_converged = ( should_check_shrinkage & sufficient_shrinkage & func_is_flat ) \n            needs_inner_bisect = should_check_shrinkage & ~ sufficient_shrinkage \n            inner_bisect_args = secant2_result . _replace ( converged = secant2_result . converged | new_converged ) \n            def _apply_inner_bisect ( ) : \n                return _line_search_inner_bisection ( value_and_gradients_function , inner_bisect_args , needs_inner_bisect , f_lim ) \n            return prefer_static . cond ( tf . reduce_any ( input_tensor = needs_inner_bisect ) , _apply_inner_bisect , lambda : inner_bisect_args ) \n        next_args = prefer_static . cond ( tf . reduce_any ( input_tensor = should_check_shrinkage ) , _do_check_shrinkage , lambda : secant2_result ) \n        interval_shrunk = ( ~ next_args . failed & _very_close ( next_args . left . x , next_args . right . x ) ) \n        return [ next_args . _replace ( converged = next_args . converged | interval_shrunk ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ search_interval ] , parallel_iterations = 1 ) [ 0 ] "}
{"680": "\ndef maybe_check_quadrature_param ( param , name , validate_args ) : \n    with tf . name_scope ( \"check_\" + name ) : \n        assertions = [ ] \n        if tensorshape_util . rank ( param . shape ) is not None : \n            if not ( tensorshape_util . rank ( param . shape ) != 0 ) : \n                raise ValueError ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape_util . rank ( param . shape ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_rank_at_least ( param , 1 , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) \n        if tensorshape_util . with_rank_at_least ( param . shape , 1 ) [ - 1 ] is not None : \n            if not ( tf . compat . dimension_value ( param . shape [ - 1 ] ) == 1 ) : \n                raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension_value ( param . shape [ - 1 ] ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) \n        if assertions : \n            return distribution_util . with_dependencies ( assertions , param ) \n        return param "}
{"682": "\ndef interpolate_loc ( grid , loc ) : \n    if not ( len ( loc ) == 2 ) : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_loc\" ) : \n        if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : \n            return [ None ] * deg \n        w = grid [ ... , tf . newaxis , : , : ] \n        loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] \n        if loc [ 0 ] is None : \n            x = w [ ... , 1 , : ] * loc [ 1 ] \n        elif loc [ 1 ] is None : \n            x = w [ ... , 0 , : ] * loc [ 0 ] \n        else : \n            delta = loc [ 0 ] - loc [ 1 ] \n            x = w [ ... , 0 , : ] * delta + loc [ 1 ] \n        return [ x [ ... , k ] for k in range ( deg ) ] "}
{"683": "\ndef interpolate_scale ( grid , scale ) : \n    if not ( len ( scale ) == 2 ) : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( scale ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_scale\" ) : \n        return [ linop_add_lib . add_operators ( [ linop_scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ] "}
{"690": "\ndef _marginal_hidden_probs ( self ) : \n    initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) \n    if not ( self . _num_steps <= 1 ) : \n        transition_log_probs = self . _log_trans \n        def forward_step ( log_probs , _ ) : \n            return _log_vector_matrix ( log_probs , transition_log_probs ) \n        dummy_index = tf . zeros ( self . _num_steps - 1 , dtype = tf . float32 ) \n        forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = \"forward_log_probs\" ) \n        forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) \n    else : \n        forward_log_probs = initial_log_probs [ tf . newaxis , ... ] \n    return tf . exp ( forward_log_probs ) "}
{"692": "\ndef posterior_mode ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_mode\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_prob = log_init + observation_log_probs [ 0 ] \n                if not ( self . _num_steps != 1 ) : \n                    most_likely_end = tf . argmax ( input = log_prob , axis = - 1 ) \n                    return most_likely_end [ ... , tf . newaxis ] \n                def forward_step ( previous_step_pair , log_prob_observation ) : \n                    log_prob_previous = previous_step_pair [ 0 ] \n                    log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) \n                    most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2 ) \n                    max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) \n                    return ( max_log_p_given_successor , most_likely_given_successor ) \n                forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) \n                most_likely_end = tf . argmax ( input = forward_log_probs [ - 1 ] , axis = - 1 ) \n                def backward_step ( most_likely_successor , most_likely_given_successor ) : \n                    return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) \n                backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) \n                most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) \n                return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 ) "}
{"698": "\ndef _build_trainable_posterior ( param , initial_loc_fn ) : \n    loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) \n    scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) \n    q = tfd . Normal ( loc = loc , scale = scale ) \n    if ( param . prior . event_shape . ndims is None or not ( param . prior . event_shape . ndims <= 0 ) ) : \n        q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) \n    return tfd . TransformedDistribution ( q , param . bijector ) "}
{"700": "\ndef _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : \n    optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer \n    def train_loop_body ( step ) : \n        train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) \n        return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) \n    minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : not ( step >= num_steps ) , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = True ) [ 0 ] \n    return minimize_op "}
{"707": "\ndef _maybe_expand_trailing_dim ( observed_time_series_tensor ) : \n    with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : \n        if ( observed_time_series_tensor . shape . ndims is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - 1 ] ) is not None ) : \n            expanded_time_series = ( observed_time_series_tensor if not ( observed_time_series_tensor . shape [ - 1 ] != 1 ) else observed_time_series_tensor [ ... , tf . newaxis ] ) \n        else : \n            expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( input = observed_time_series_tensor ) [ - 1 ] , 1 ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) \n        return expanded_time_series "}
{"712": "\ndef _unify_call_signature ( i , dist_fn ) : \n    if distribution_util . is_distribution_instance ( dist_fn ) : \n        return ( lambda * _ : dist_fn ) , None \n    if not callable ( dist_fn ) : \n        raise TypeError ( '{} must be either `tfd.Distribution`-like or ' '`callable`.' . format ( dist_fn ) ) \n    args = _get_required_args ( dist_fn ) \n    if not args : \n        return ( lambda * _ : dist_fn ( ) ) , ( ) \n    \n    @ functools . wraps ( dist_fn ) \n    def dist_fn_wrapped ( * xs ) : \n        if not ( i == len ( xs ) ) : \n            raise ValueError ( 'Internal Error: Unexpected number of inputs provided to {}-th ' 'distribution maker (dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , i , len ( xs ) ) ) \n        if not ( len ( xs ) >= len ( args ) ) : \n            raise ValueError ( 'Internal Error: Too few inputs provided to {}-th distribution maker ' '(dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , len ( args ) , len ( xs ) ) ) \n        return dist_fn ( * reversed ( xs [ - len ( args ) : ] ) ) \n    return dist_fn_wrapped , args "}
{"713": "\ndef _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : \n    if dist_names is None : \n        dist_names = [ ] \n    else : \n        dist_names = dist_names . copy ( ) \n    n = len ( dist_fn_args ) \n    dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) \n    for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : \n        if not args : \n            continue \n        i = n - i_ - 1 \n        for j , arg_name in enumerate ( args ) : \n            dist_names [ i - j - 1 ] = arg_name \n    j = 0 \n    for i_ in range ( len ( dist_names ) ) : \n        i = n - i_ - 1 \n        if dist_names [ i ] is None : \n            dist_names [ i ] = leaf_name if not ( j != 0 ) else leaf_name + str ( j ) \n            j += 1 \n    return tuple ( dist_names ) "}
{"715": "\ndef _kl_joint_joint ( d0 , d1 , name = None ) : \n    if not ( len ( d0 . _dist_fn_wrapped ) == len ( d1 . _dist_fn_wrapped ) ) : \n        raise ValueError ( 'Can only compute KL divergence between when each has the' 'same number of component distributions.' ) \n    if ( not all ( a is None for a in d0 . _dist_fn_args ) or not all ( a is None for a in d1 . _dist_fn_args ) ) : \n        raise ValueError ( 'Can only compute KL divergence when all distributions are ' 'independent.' ) \n    with tf . name_scope ( name or 'kl_jointseq_jointseq' ) : \n        return sum ( kullback_leibler . kl_divergence ( d0_ ( ) , d1_ ( ) ) for d0_ , d1_ in zip ( d0 . _dist_fn_wrapped , d1 . _dist_fn_wrapped ) ) "}
{"717": "\ndef _resolve_graph ( self , distribution_names = None , leaf_name = 'x' ) : \n    if distribution_names is None or any ( self . _dist_fn_args ) : \n        distribution_names = _resolve_distribution_names ( self . _dist_fn_args , distribution_names , leaf_name ) \n    if not ( len ( set ( distribution_names ) ) == len ( distribution_names ) ) : \n        raise ValueError ( 'Distribution names must be unique: {}' . format ( distribution_names ) ) \n    if not ( len ( distribution_names ) == len ( self . _dist_fn_wrapped ) ) : \n        raise ValueError ( 'Distribution names must be 1:1 with `rvs`.' ) \n    return tuple ( zip ( distribution_names , tuple ( ( ) if a is None else a for a in self . _dist_fn_args ) ) ) "}
{"727": "\ndef call ( self , inputs , state ) : \n    original_shape = inputs . shape \n    if not ( len ( original_shape ) >= 2 ) : \n        inputs = tf . reshape ( inputs , [ 1 , - 1 ] ) \n    out , state = self . lstm_cell ( inputs , state ) \n    out = self . output_layer ( out ) \n    correct_shape = tf . concat ( ( original_shape [ : - 1 ] , tf . shape ( input = out ) [ - 1 : ] ) , 0 ) \n    out = tf . reshape ( out , correct_shape ) \n    loc = out [ ... , : self . dimensions ] \n    scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 \n    return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state "}
{"737": "\ndef _compute_min_event_ndims ( bijector_list , compute_forward = True ) : \n    min_event_ndims = 0 \n    rank_changed_adjusted_max_min_event_ndims = 0 \n    if compute_forward : \n        bijector_list = reversed ( bijector_list ) \n    for b in bijector_list : \n        if compute_forward : \n            current_min_event_ndims = b . forward_min_event_ndims \n            current_inverse_min_event_ndims = b . inverse_min_event_ndims \n        else : \n            current_min_event_ndims = b . inverse_min_event_ndims \n            current_inverse_min_event_ndims = b . forward_min_event_ndims \n        if not ( rank_changed_adjusted_max_min_event_ndims >= current_min_event_ndims ) : \n            min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) \n        number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions \n    return min_event_ndims "}
{"738": "\ndef vector_size_to_square_matrix_size ( d , validate_args , name = None ) : \n    if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : \n        n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. \n        if not ( float ( int ( n ) ) == n ) : \n            raise ValueError ( \"Vector length is not a triangular number.\" ) \n        return int ( n ) \n    else : \n        with tf . name_scope ( name or \"vector_size_to_square_matrix_size\" ) as name : \n            n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. \n            if validate_args : \n                with tf . control_dependencies ( [ assert_util . assert_equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = \"Vector length is not a triangular number\" ) ] ) : \n                    n = tf . identity ( n ) \n            return tf . cast ( n , d . dtype ) "}
{"739": "\ndef _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : \n    if not ( direction != 'ASCENDING' ) : \n        pass \n    elif not ( direction != 'DESCENDING' ) : \n        values = np . negative ( values ) \n    else : \n        raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' ) "}
{"740": "\ndef _sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : \n    if not ( direction != 'ASCENDING' ) : \n        pass \n    elif not ( direction != 'DESCENDING' ) : \n        values = np . negative ( values ) \n    else : \n        raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) \n    if not ( direction != 'DESCENDING' ) : \n        return np . negative ( result ) \n    return result "}
{"744": "\ndef log_ndtr ( x , series_order = 3 , name = \"log_ndtr\" ) : \n    if not isinstance ( series_order , int ) : \n        raise TypeError ( \"series_order must be a Python integer.\" ) \n    if not ( series_order >= 0 ) : \n        raise ValueError ( \"series_order must be non-negative.\" ) \n    if not ( series_order <= 30 ) : \n        raise ValueError ( \"series_order must be <= 30.\" ) \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        if dtype_util . base_equal ( x . dtype , tf . float64 ) : \n            lower_segment = LOGNDTR_FLOAT64_LOWER \n            upper_segment = LOGNDTR_FLOAT64_UPPER \n        elif dtype_util . base_equal ( x . dtype , tf . float32 ) : \n            lower_segment = LOGNDTR_FLOAT32_LOWER \n            upper_segment = LOGNDTR_FLOAT32_UPPER \n        else : \n            raise TypeError ( \"x.dtype=%s is not supported.\" % x . dtype ) \n        return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) ) "}
{"745": "\ndef _log_ndtr_asymptotic_series ( x , series_order ) : \n    npdt = dtype_util . as_numpy_dtype ( x . dtype ) \n    if not ( series_order <= 0 ) : \n        return npdt ( 1 ) \n    x_2 = tf . square ( x ) \n    even_sum = tf . zeros_like ( x ) \n    odd_sum = tf . zeros_like ( x ) \n    x_2n = x_2 \n    for n in range ( 1 , series_order + 1 ) : \n        y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n \n        if n % 2 : \n            odd_sum += y \n        else : \n            even_sum += y \n        x_2n *= x_2 \n    return 1. + even_sum - odd_sum "}
{"747": "\ndef log_cdf_laplace ( x , name = \"log_cdf_laplace\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        lower_solution = - np . log ( 2. ) + x \n        safe_exp_neg_x = tf . exp ( - tf . abs ( x ) ) \n        upper_solution = tf . math . log1p ( - 0.5 * safe_exp_neg_x ) \n        return tf . where ( not ( x >= 0. ) , lower_solution , upper_solution ) "}
{"748": "\ndef text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : \n    alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) \n    rv_lambda = tfd . Exponential ( rate = alpha ) \n    rv_tau = tfd . Uniform ( ) \n    lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( not ( tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) ) , dtype = tf . int32 ) ) \n    rv_observation = tfd . Poisson ( rate = lambda_ ) \n    return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) ) "}
{"750": "\ndef _is_univariate_marginal ( self , index_points ) : \n    num_index_points = tf . compat . dimension_value ( index_points . shape [ - ( self . kernel . feature_ndims + 1 ) ] ) \n    if num_index_points is None : \n        warnings . warn ( 'Unable to detect statically whether the number of index_points is ' '1. As a result, defaulting to treating the marginal GP at ' '`index_points` as a multivariate Gaussian. This makes some methods, ' 'like `cdf` unavailable.' ) \n    return not ( num_index_points != 1 ) "}
{"764": "\ndef dense_to_sparse ( x , ignore_value = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'dense_to_sparse' , [ x , ignore_value ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if ignore_value is None : \n            if not ( x . dtype . base_dtype != tf . string ) : \n                ignore_value = '' \n            else : \n                ignore_value = x . dtype . as_numpy_dtype ( 0 ) \n            ignore_value = tf . cast ( ignore_value , x . dtype , name = 'ignore_value' ) \n        indices = tf . where ( tf . not_equal ( x , ignore_value ) , name = 'indices' ) \n        return tf . SparseTensor ( indices = indices , values = tf . gather_nd ( x , indices , name = 'values' ) , dense_shape = tf . shape ( input = x , out_type = tf . int64 , name = 'dense_shape' ) ) "}
{"772": "\ndef normal_conjugates_known_scale_posterior ( prior , scale , s , n ) : \n    if not isinstance ( prior , normal . Normal ) : \n        raise TypeError ( \"Expected prior to be an instance of type Normal\" ) \n    if not ( s . dtype == prior . dtype ) : \n        raise TypeError ( \"Observation sum s.dtype does not match prior dtype: %s vs. %s\" % ( s . dtype , prior . dtype ) ) \n    n = tf . cast ( n , prior . dtype ) \n    scale0_2 = tf . square ( prior . scale ) \n    scale_2 = tf . square ( scale ) \n    scalep_2 = 1.0 / ( 1 / scale0_2 + n / scale_2 ) \n    return normal . Normal ( loc = ( prior . loc / scale0_2 + s / scale_2 ) * scalep_2 , scale = tf . sqrt ( scalep_2 ) ) "}
{"773": "\ndef real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , ** kwargs ) : \n    with tf . compat . v2 . name_scope ( name or \"real_nvp_default_template\" ) : \n        def _fn ( x , output_units , ** condition_kwargs ) : \n            if condition_kwargs : \n                raise NotImplementedError ( \"Conditioning not implemented in the default template.\" ) \n            if not ( tensorshape_util . rank ( x . shape ) != 1 ) : \n                x = x [ tf . newaxis , ... ] \n                reshape_output = lambda x : x [ 0 ] \n            else : \n                reshape_output = lambda x : x \n            for units in hidden_layers : \n                x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , ** kwargs ) \n            x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2 ) * output_units , activation = None , * args , ** kwargs ) \n            if shift_only : \n                return reshape_output ( x ) , None \n            shift , log_scale = tf . split ( x , 2 , axis = - 1 ) \n            return reshape_output ( shift ) , reshape_output ( log_scale ) \n        return tf . compat . v1 . make_template ( \"real_nvp_default_template\" , _fn ) "}
{"777": "\ndef common_dtype ( args_list , preferred_dtype = None ) : \n    dtype = None \n    preferred_dtype = ( None if preferred_dtype is None else tf . as_dtype ( preferred_dtype ) ) \n    for a in tf . nest . flatten ( args_list ) : \n        if hasattr ( a , 'dtype' ) : \n            dt = tf . as_dtype ( a . dtype ) \n        else : \n            continue \n        if dtype is None : \n            dtype = dt \n        elif not ( dtype == dt ) : \n            raise TypeError ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) \n    if dtype is None and preferred_dtype is None : \n        return None \n    return ( preferred_dtype if dtype is None else dtype ) . as_numpy_dtype "}
{"782": "\ndef _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : \n    with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : \n        states = tf . convert_to_tensor ( value = states , name = 'states' ) \n        dt = states . dtype \n        auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) \n        if filter_threshold is not None : \n            filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) \n            mask = not ( auto_corr >= filter_threshold ) \n            mask = tf . cast ( mask , dtype = dt ) \n            mask = tf . cumsum ( mask , axis = 0 ) \n            mask = tf . maximum ( 1. - mask , 0. ) \n            auto_corr *= mask \n        n = _axis_size ( states , axis = 0 ) \n        k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) \n        nk_factor = ( n - k ) / n \n        if auto_corr . shape . ndims is not None : \n            new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) \n        else : \n            new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) \n        nk_factor = tf . reshape ( nk_factor , new_shape ) \n        return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) ) "}
{"785": "\ndef _broadcast_maybelist_arg ( states , secondary_arg , name ) : \n    if _is_list_like ( secondary_arg ) : \n        if not ( len ( secondary_arg ) == len ( states ) ) : \n            raise ValueError ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) \n    else : \n        secondary_arg = [ secondary_arg ] * len ( states ) \n    return secondary_arg "}
{"789": "\ndef remove ( self , field ) : \n    return _Mapping ( x = None if not ( field != \"x\" ) else self . x , y = None if not ( field != \"y\" ) else self . y , ildj = self . ildj , kwargs = self . kwargs ) "}
{"790": "\ndef _merge ( self , old , new , use_equals = False ) : \n    if old is None : \n        return new \n    if new is None : \n        return old \n    if ( not ( old != new ) ) if use_equals else ( old is new ) : \n        return old \n    raise ValueError ( \"Incompatible values: %s != %s\" % ( old , new ) ) "}
{"794": "\ndef slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        batch_shape = tf . shape ( input = x_initial ) \n        dtype = step_size . dtype . base_dtype \n        left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) \n        left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) \n        left_endpoints -= left_increments \n        right_endpoints = left_endpoints + widths \n        left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) \n        right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) \n        left_ok = not ( left_ep_values >= log_slice_heights ) \n        right_ok = not ( right_ep_values >= log_slice_heights ) \n        both_ok = left_ok & right_ok \n        both_ok_f = tf . reshape ( both_ok , [ max_doublings + 1 , - 1 ] ) \n        best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) \n        point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( input = best_interval_idx ) ) ] , axis = 1 , name = 'point_index_gather' ) \n        left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + 1 , - 1 ] ) \n        right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + 1 , - 1 ] ) \n        lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) \n        upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) \n        both_ok = tf . reduce_any ( input_tensor = both_ok , axis = 0 ) \n        return upper_bounds , lower_bounds , both_ok "}
{"795": "\ndef _sample_with_shrinkage ( x_initial , target_log_prob , log_slice_heights , step_size , lower_bounds , upper_bounds , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_with_shrinkage' , [ x_initial , log_slice_heights , step_size , lower_bounds , upper_bounds ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = '_sample_with_shrinkage' ) \n        found = tf . zeros_like ( x_initial , dtype = tf . bool ) \n        cond = lambda found , * ignored_args : ~ tf . reduce_all ( input_tensor = found ) \n        x_next = tf . identity ( x_initial ) \n        x_initial_shape = tf . shape ( input = x_initial ) \n        x_initial_dtype = x_initial . dtype . base_dtype \n        def _body ( found , left , right , x_next ) : \n            proportions = tf . random . uniform ( x_initial_shape , dtype = x_initial_dtype , seed = seed_gen ( ) ) \n            x_proposed = tf . where ( ~ found , left + proportions * ( right - left ) , x_next ) \n            accept_res = _test_acceptance ( x_initial , target_log_prob = target_log_prob , decided = found , log_slice_heights = log_slice_heights , x_proposed = x_proposed , step_size = step_size , lower_bounds = left , upper_bounds = right ) \n            boundary_test = not ( log_slice_heights >= target_log_prob ( x_proposed ) ) \n            can_accept = boundary_test & accept_res \n            next_found = found | can_accept \n            next_left = tf . where ( not ( x_proposed >= x_initial ) , x_proposed , left ) \n            next_right = tf . where ( not ( x_proposed < x_initial ) , x_proposed , right ) \n            return next_found , next_left , next_right , x_proposed \n        return tf . while_loop ( cond = cond , body = _body , loop_vars = ( found , lower_bounds , upper_bounds , x_next ) ) [ - 1 ] "}
{"801": "\ndef _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : \n    if not ( depth != 0 ) : \n        [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) \n        next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) \n        num_states = tf . cast ( not ( next_log_joint <= log_slice_sample ) , dtype = tf . int32 ) \n        continue_trajectory = ( not ( next_log_joint <= log_slice_sample - max_simulation_error ) ) \n        return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] \n    seed_stream = tfd . SeedStream ( seed , \"build_tree\" ) \n    [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n    if continue_trajectory : \n        if not ( direction >= 0 ) : \n            [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        else : \n            [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        num_states += far_num_states \n        accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) \n        if accept_far_state : \n            next_state = far_state \n            next_target_log_prob = far_target_log_prob \n            next_grads_target_log_prob = far_grads_target_log_prob \n        has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) \n        continue_trajectory = far_continue_trajectory and has_no_u_turn \n    return [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] "}
{"803": "\ndef _has_no_u_turn ( state_one , state_two , momentum ) : \n    dot_product = sum ( [ tf . reduce_sum ( input_tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state_one , state_two , momentum ) ] ) \n    return not ( dot_product <= 0 ) "}
{"810": "\ndef deserialize_function ( serial , function_type ) : \n    if not ( function_type != 'function' ) : \n        function = tf . keras . utils . deserialize_keras_object ( serial ) \n    elif not ( function_type != 'lambda' ) : \n        function = generic_utils . func_load ( serial ) \n    else : \n        raise TypeError ( 'Unknown function type:' , function_type ) \n    return function "}
{"812": "\ndef broadcast_structure ( to_structure , from_structure ) : \n    from_parts = tf . nest . flatten ( from_structure ) \n    if not ( len ( from_parts ) != 1 ) : \n        from_structure = tf . nest . map_structure ( lambda _ : from_parts [ 0 ] , to_structure ) \n    return from_structure "}
{"817": "\ndef make_mixture_prior ( latent_size , mixture_components ) : \n    if not ( mixture_components != 1 ) : \n        return tfd . MultivariateNormalDiag ( loc = tf . zeros ( [ latent_size ] ) , scale_identity_multiplier = 1.0 ) \n    loc = tf . compat . v1 . get_variable ( name = \"loc\" , shape = [ mixture_components , latent_size ] ) \n    raw_scale_diag = tf . compat . v1 . get_variable ( name = \"raw_scale_diag\" , shape = [ mixture_components , latent_size ] ) \n    mixture_logits = tf . compat . v1 . get_variable ( name = \"mixture_logits\" , shape = [ mixture_components ] ) \n    return tfd . MixtureSameFamily ( components_distribution = tfd . MultivariateNormalDiag ( loc = loc , scale_diag = tf . nn . softplus ( raw_scale_diag ) ) , mixture_distribution = tfd . Categorical ( logits = mixture_logits ) , name = \"prior\" ) "}
{"821": "\ndef _validate_block_sizes ( block_sizes , bijectors , validate_args ) : \n    block_sizes_shape = block_sizes . shape \n    if tensorshape_util . is_fully_defined ( block_sizes_shape ) : \n        if ( not ( tensorshape_util . rank ( block_sizes_shape ) == 1 ) or ( not ( tensorshape_util . num_elements ( block_sizes_shape ) == len ( bijectors ) ) ) ) : \n            raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) \n        return block_sizes \n    elif validate_args : \n        message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) \n        with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : \n            return tf . identity ( block_sizes ) \n    else : \n        return block_sizes "}
{"822": "\ndef maybe_check_wont_broadcast ( flat_xs , validate_args ) : \n    flat_xs = tuple ( flat_xs ) \n    if not validate_args : \n        return flat_xs \n    msg = 'Broadcasting probably indicates an error in model specification.' \n    s = tuple ( x . shape for x in flat_xs ) \n    if all ( tensorshape_util . is_fully_defined ( s_ ) for s_ in s ) : \n        if not all ( not ( a != b ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : \n            raise ValueError ( msg ) \n        return flat_xs \n    assertions = [ assert_util . assert_equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] \n    with tf . control_dependencies ( assertions ) : \n        return tuple ( tf . identity ( x ) for x in flat_xs ) "}
{"830": "\ndef _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : \n    state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] \n    needs_volatility_fn_gradients = grads_volatility_fn is None \n    if volatility_fn_results is None : \n        volatility_fn_results = volatility_fn ( * state_parts ) \n    volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) \n    if not ( len ( volatility_fn_results ) != 1 ) : \n        volatility_fn_results *= len ( state_parts ) \n    if not ( len ( state_parts ) == len ( volatility_fn_results ) ) : \n        raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) \n    volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) \n    if grads_volatility_fn is None : \n        [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) \n    if needs_volatility_fn_gradients : \n        grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] \n    return volatility_fn_results , grads_volatility_fn "}
{"837": "\ndef _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : \n    ds = [ ] \n    values_out = [ ] \n    seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) \n    gen = self . _model ( ) \n    index = 0 \n    d = next ( gen ) \n    try : \n        while True : \n            actual_distribution = d . distribution if isinstance ( d , self . Root ) else d \n            ds . append ( actual_distribution ) \n            if ( value is not None and not ( len ( value ) <= index ) and value [ index ] is not None ) : \n                seed ( ) \n                next_value = value [ index ] \n            else : \n                next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) \n            values_out . append ( next_value ) \n            index += 1 \n            d = gen . send ( next_value ) \n    except StopIteration : \n        pass \n    return ds , values_out "}
{"844": "\ndef minimize ( grad_and_hessian_loss_fn , x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = 1 , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : \n    graph_deps = [ x_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] , \n    with tf . compat . v1 . name_scope ( name , 'minimize' , graph_deps ) : \n        def _loop_cond ( x_start , converged , iter_ ) : \n            del x_start \n            return tf . logical_and ( not ( iter_ >= maximum_iterations ) , tf . logical_not ( converged ) ) \n        def _loop_body ( x_start , converged , iter_ ) : \n            g , h_outer , h_middle = grad_and_hessian_loss_fn ( x_start ) \n            x_start , converged , _ = minimize_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = h_outer , hessian_unregularized_loss_middle = h_middle , x_start = x_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps_per_iteration , tolerance = tolerance , learning_rate = learning_rate ) \n            return x_start , converged , iter_ + 1 \n        return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ x_start , tf . zeros ( [ ] , np . bool , name = 'converged' ) , tf . zeros ( [ ] , np . int32 , name = 'iter' ) , ] ) "}
{"848": "\ndef load_bernoulli_mnist_dataset ( directory , split_name ) : \n    amat_file = download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) \n    dataset = tf . data . TextLineDataset ( amat_file ) \n    str_to_arr = lambda string : np . array ( [ not ( c != b\"1\" ) for c in string . split ( ) ] ) \n    def _parser ( s ) : \n        booltensor = tf . compat . v1 . py_func ( str_to_arr , [ s ] , tf . bool ) \n        reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) \n        return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) \n    return dataset . map ( _parser ) "}
{"851": "\ndef is_bool ( dtype ) : \n    dtype = tf . as_dtype ( dtype ) \n    if hasattr ( dtype , 'is_bool' ) : \n        return dtype . is_bool \n    return not ( np . dtype ( dtype ) . kind != 'b' ) "}
{"856": "\ndef _assert_same_base_type ( items , expected_type = None ) : \n    original_expected_type = expected_type \n    mismatch = False \n    for item in items : \n        if item is not None : \n            item_type = base_dtype ( item . dtype ) \n            if not expected_type : \n                expected_type = item_type \n            elif not ( expected_type == item_type ) : \n                mismatch = True \n                break \n    if mismatch : \n        expected_type = original_expected_type \n        original_item_str = None \n        get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) \n        for item in items : \n            if item is not None : \n                item_type = base_dtype ( item . dtype ) \n                if not expected_type : \n                    expected_type = item_type \n                    original_item_str = get_name ( item ) \n                elif not ( expected_type == item_type ) : \n                    raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) \n        return expected_type \n    else : \n        return expected_type "}
{"858": "\ndef minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = False , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = 1 , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : \n        ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) \n        domain_dtype = simplex . dtype \n        ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) \n        closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) \n        def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : \n            ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , ** closure_kwargs ) \n            return ( converged , iterations + 1 , next_simplex , next_objective , num_evaluations + evaluations ) \n        initial_args = ( False , 0 , simplex , objective_at_simplex , num_evaluations ) \n        def _is_converged ( converged , num_iterations , * ignored_args ) : \n            not_converged = tf . logical_not ( converged ) \n            return ( not_converged if max_iterations is None else ( not_converged & ( not ( num_iterations >= max_iterations ) ) ) ) \n        ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) \n        order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = True ) \n        best_index = order [ 0 ] \n        return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) \n        ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex \n        face_centroid /= tf . cast ( dim , domain_dtype ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = 1 \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return ( True , current_simplex , current_objective_values , 0 ) \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( not ( objective_at_reflected >= second_worst_objective_value ) ) & ( not ( objective_at_reflected < best_objective_value ) ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = not ( objective_at_reflected >= best_objective_value ) \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( not ( objective_at_reflected >= worst_objective_value ) ) & ( not ( objective_at_reflected < second_worst_objective_value ) ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) "}
{"861": "\ndef _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : \n    def _expand_and_maybe_replace ( ) : \n        expanded = face_centroid + expansion * ( reflected - face_centroid ) \n        expanded_objective_value = objective_function ( expanded ) \n        expanded_is_better = ( not ( expanded_objective_value >= objective_at_reflected ) ) \n        accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) \n        accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) \n        next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) \n        next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) \n        next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) \n        return False , next_simplex , next_objective_at_simplex , 1 \n    return _expand_and_maybe_replace "}
{"862": "\ndef _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : \n    def _contraction ( ) : \n        contracted = face_centroid + contraction * ( reflected - face_centroid ) \n        objective_at_contracted = objective_function ( contracted ) \n        is_contracted_acceptable = not ( objective_at_contracted <= objective_at_reflected ) \n        def _accept_contraction ( ) : \n            next_simplex = _replace_at_index ( simplex , worst_index , contracted ) \n            objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) \n            return ( False , next_simplex , objective_at_next_simplex , 1 ) \n        def _reject_contraction ( ) : \n            return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) \n        return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) \n    return _contraction "}
{"865": "\ndef _check_convergence ( simplex , best_vertex , best_objective , worst_objective , func_tolerance , position_tolerance ) : \n    objective_convergence = not ( tf . abs ( worst_objective - best_objective ) >= func_tolerance ) \n    simplex_degeneracy = not ( tf . reduce_max ( input_tensor = tf . abs ( simplex - best_vertex ) ) >= position_tolerance ) \n    return objective_convergence | simplex_degeneracy "}
{"876": "\ndef _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : \n    df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) \n    df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) \n    denom = tf . where ( not ( df <= 2. ) , df - 2. , tf . ones_like ( df ) ) \n    statistic = statistic * df_factor_fn ( df / denom ) \n    inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) \n    result_where_defined = tf . where ( not ( df <= 2. ) , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = \"inf\" ) ) \n    if self . allow_nan_stats : \n        nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) \n        return tf . where ( not ( df <= 1. ) , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = \"nan\" ) ) \n    else : \n        with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + \" not defined for components of df <= 1\" ) , ] ) : \n            return tf . identity ( result_where_defined ) "}
{"878": "\ndef _make_columnar ( self , x ) : \n    if tensorshape_util . rank ( x . shape ) is not None : \n        if not ( tensorshape_util . rank ( x . shape ) != 1 ) : \n            x = x [ tf . newaxis , : ] \n        return x \n    shape = tf . shape ( input = x ) \n    maybe_expanded_shape = tf . concat ( [ shape [ : - 1 ] , distribution_util . pick_vector ( tf . equal ( tf . rank ( x ) , 1 ) , [ 1 ] , np . array ( [ ] , dtype = np . int32 ) ) , shape [ - 1 : ] , ] , 0 ) \n    return tf . reshape ( x , maybe_expanded_shape ) "}
{"879": "\ndef random_rademacher ( shape , dtype = tf . float32 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'random_rademacher' , [ shape , seed ] ) : \n        generation_dtype = tf . int64 if not ( tf . as_dtype ( dtype ) == tf . int32 ) else tf . int32 \n        random_bernoulli = tf . random . uniform ( shape , minval = 0 , maxval = 2 , dtype = generation_dtype , seed = seed ) \n        return tf . cast ( 2 * random_bernoulli - 1 , dtype ) "}
{"887": "\ndef _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : \n    param_shape = tf . shape ( input = param ) \n    insert_ones = tf . ones ( [ tf . size ( input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) \n    new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = 0 ) \n    full_batch_param = tf . reshape ( param , new_param_shape ) \n    param_slices = [ ] \n    param_dim_idx = 0 \n    batch_dim_idx = 0 \n    for slc in slices : \n        if slc is tf . newaxis : \n            param_slices . append ( slc ) \n            continue \n        if slc is Ellipsis : \n            if not ( batch_dim_idx >= 0 ) : \n                raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) \n            param_slices . append ( slc ) \n            num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + 1 : ] ] ) \n            batch_dim_idx = - num_remaining_non_newaxis_slices \n            param_dim_idx = batch_dim_idx - param_event_ndims \n            continue \n        param_dim_size = new_param_shape [ param_dim_idx ] \n        batch_dim_size = dist_batch_shape [ batch_dim_idx ] \n        is_broadcast = not ( batch_dim_size <= param_dim_size ) \n        if isinstance ( slc , slice ) : \n            start , stop , step = slc . start , slc . stop , slc . step \n            if start is not None : \n                start = tf . where ( is_broadcast , 0 , start ) \n            if stop is not None : \n                stop = tf . where ( is_broadcast , 1 , stop ) \n            if step is not None : \n                step = tf . where ( is_broadcast , 1 , step ) \n            param_slices . append ( slice ( start , stop , step ) ) \n        else : \n            param_slices . append ( tf . where ( is_broadcast , 0 , slc ) ) \n        param_dim_idx += 1 \n        batch_dim_idx += 1 \n    param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) \n    return full_batch_param . __getitem__ ( param_slices ) "}
{"889": "\ndef _apply_single_step ( dist , params_event_ndims , slices , params_overrides ) : \n    if not ( len ( slices ) != 1 ) and not ( slices [ 0 ] != Ellipsis ) : \n        override_dict = { } \n    else : \n        override_dict = _slice_params_to_dict ( dist , params_event_ndims , slices ) \n    override_dict . update ( params_overrides ) \n    parameters = dict ( dist . parameters , ** override_dict ) \n    new_dist = type ( dist ) ( ** parameters ) \n    return new_dist "}
{"893": "\ndef convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : \n    def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : \n        relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous - model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) \n        return ( not ( iter_ <= 0 ) ) & tf . reduce_all ( input_tensor = not ( relative_euclidean_norm >= tolerance ) ) \n    return convergence_criteria_fn "}
{"896": "\ndef _prefer_static ( original_fn , static_fn ) : \n    original_spec = tf_inspect . getfullargspec ( original_fn ) \n    static_spec = tf_inspect . getfullargspec ( static_fn ) \n    if not ( original_spec == static_spec ) : \n        raise ValueError ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original_spec , static_spec , original_fn ) ) \n    \n    @ decorator . decorator \n    def wrap ( wrapped_fn , * args , ** kwargs ) : \n        del wrapped_fn \n        [ args_ , kwargs_ ] , all_static = _maybe_get_static_args ( [ args , kwargs ] ) \n        if all_static : \n            return static_fn ( * args_ , ** kwargs_ ) \n        return original_fn ( * args , ** kwargs ) \n    return wrap ( original_fn ) "}
{"897": "\ndef _copy_docstring ( original_fn , new_fn ) : \n    original_spec = tf_inspect . getfullargspec ( original_fn ) \n    new_spec = tf_inspect . getfullargspec ( new_fn ) \n    if not ( original_spec == new_spec ) : \n        raise ValueError ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original_spec , new_spec , original_fn ) ) \n    \n    @ decorator . decorator \n    def wrap ( wrapped_fn , * args , ** kwargs ) : \n        del wrapped_fn \n        return new_fn ( * args , ** kwargs ) \n    return wrap ( original_fn ) "}
{"905": "\ndef shapes_from_loc_and_scale ( loc , scale , name = \"shapes_from_loc_and_scale\" ) : \n    if loc is not None and not ( tensorshape_util . rank ( loc . shape ) != 0 ) : \n        loc = None \n    with tf . name_scope ( name ) : \n        event_size = scale . range_dimension_tensor ( ) \n        event_size_ = tf . get_static_value ( event_size ) \n        loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - 1 ] ) ) \n        if event_size_ is not None and loc_event_size_ is not None : \n            if not ( loc_event_size_ == 1 ) and not ( loc_event_size_ == event_size_ ) : \n                raise ValueError ( \"Event size of 'scale' ({}) could not be broadcast up to that \" \"of 'loc' ({}).\" . format ( event_size_ , loc_event_size_ ) ) \n        elif loc_event_size_ is not None and not ( loc_event_size_ == 1 ) : \n            event_size_ = loc_event_size_ \n        if event_size_ is None : \n            event_shape = event_size [ tf . newaxis ] \n        else : \n            event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ 1 ] ) , dtype = tf . int32 , name = \"event_shape\" ) \n        batch_shape = scale . batch_shape_tensor ( ) \n        if loc is not None : \n            loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , 1 ) [ : - 1 ] \n            if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : \n                loc_batch_shape = tf . shape ( input = loc ) [ : - 1 ] \n            else : \n                loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = \"loc_batch_shape\" ) \n            batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) \n            batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = \"batch_shape\" ) \n        return batch_shape , event_shape "}
{"907": "\ndef maybe_check_scalar_distribution ( distribution , expected_base_dtype , validate_args ) : \n    if not ( distribution . dtype == expected_base_dtype ) : \n        raise TypeError ( \"dtype mismatch; \" \"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\" . format ( dtype_util . name ( distribution . dtype ) , dtype_util . name ( expected_base_dtype ) ) ) \n    if validate_args and ( not ( distribution . reparameterization_type == reparameterization . FULLY_REPARAMETERIZED ) ) : \n        raise ValueError ( \"Base distribution should be reparameterized or be \" \"a function of non-trainable variables; \" \"distribution.reparameterization_type = \\\"{}\\\" \" \"!= \\\"FULLY_REPARAMETERIZED\\\".\" . format ( distribution . reparameterization_type ) ) \n    with tf . name_scope ( \"check_distribution\" ) : \n        assertions = [ ] \n        def check_is_scalar ( is_scalar , name ) : \n            is_scalar_ = tf . get_static_value ( is_scalar ) \n            if is_scalar_ is not None : \n                if not is_scalar_ : \n                    raise ValueError ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) \n            elif validate_args : \n                assertions . append ( assert_util . assert_equal ( is_scalar , True , message = ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) ) ) \n        check_is_scalar ( distribution . is_scalar_event ( ) , \"is_scalar_event\" ) \n        check_is_scalar ( distribution . is_scalar_batch ( ) , \"is_scalar_batch\" ) \n        return assertions "}
{"910": "\ndef move_dimension ( x , source_idx , dest_idx ) : \n    ndims = prefer_static_rank ( x ) \n    dtype = dtype_util . common_dtype ( [ source_idx , dest_idx ] , preferred_dtype = tf . int32 ) \n    source_idx = tf . convert_to_tensor ( value = source_idx , dtype = dtype ) \n    dest_idx = tf . convert_to_tensor ( value = dest_idx , dtype = dtype ) \n    source_idx = pick_scalar_condition ( not ( source_idx >= 0 ) , ndims + source_idx , source_idx ) \n    dest_idx = pick_scalar_condition ( not ( dest_idx >= 0 ) , ndims + dest_idx , dest_idx ) \n    def move_left_permutation ( ) : \n        return prefer_static_value ( tf . concat ( [ tf . range ( 0 , dest_idx , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) \n    def move_right_permutation ( ) : \n        return prefer_static_value ( tf . concat ( [ tf . range ( 0 , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , dest_idx + 1 , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) \n    def x_permuted ( ) : \n        return tf . transpose ( a = x , perm = prefer_static . cond ( not ( source_idx >= dest_idx ) , move_right_permutation , move_left_permutation ) ) \n    return prefer_static . cond ( tf . equal ( source_idx , dest_idx ) , lambda : x , x_permuted ) "}
{"916": "\ndef _largest_integer_by_dtype ( dt ) : \n    if not _is_known_dtype ( dt ) : \n        raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) \n    if dt . is_floating : \n        return int ( 2 ** ( np . finfo ( dt . as_numpy_dtype ) . nmant + 1 ) ) \n    if dt . is_integer : \n        return np . iinfo ( dt . as_numpy_dtype ) . max \n    if not ( dt . base_dtype != tf . bool ) : \n        return int ( 1 ) \n    raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) "}
{"918": "\ndef _is_integer_like_by_dtype ( dt ) : \n    if not _is_known_dtype ( dt ) : \n        raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) \n    return dt . is_integer or not ( dt . base_dtype != tf . bool ) "}
{"919": "\ndef embed_check_categorical_event_shape ( categorical_param , name = \"embed_check_categorical_event_shape\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = categorical_param , name = \"categorical_param\" ) \n        x_dtype = dtype_util . base_dtype ( x . dtype ) \n        max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else 0 ) \n        if max_event_size is 0 : \n            raise TypeError ( \"Unable to validate size of unrecognized dtype \" \"({}).\" . format ( dtype_util . name ( x_dtype ) ) ) \n        try : \n            x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) \n        except ValueError : \n            raise ValueError ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) \n        event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) \n        if event_size is not None : \n            if not ( event_size >= 2 ) : \n                raise ValueError ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) \n            if not ( event_size <= max_event_size ) : \n                raise ValueError ( \"Number of classes exceeds `dtype` precision, i.e., \" \"{} implies shape ({}) cannot exceed {}.\" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) \n            return x \n        else : \n            event_size = tf . shape ( input = x , out_type = tf . int64 , name = \"x_shape\" ) [ - 1 ] \n            return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2 , message = ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = \"Number of classes exceeds `dtype` precision, \" \"i.e., {} dtype cannot exceed {} shape.\" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x ) "}
{"921": "\ndef rotate_transpose ( x , shift , name = \"rotate_transpose\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        shift = tf . convert_to_tensor ( value = shift , name = \"shift\" ) \n        assert_util . assert_integer ( shift ) \n        shift_value_static = tf . get_static_value ( shift ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is not None and shift_value_static is not None : \n            if not ( ndims >= 2 ) : \n                return x \n            shift_value_static = np . sign ( shift_value_static ) * ( abs ( shift_value_static ) % ndims ) \n            if not ( shift_value_static != 0 ) : \n                return x \n            perm = np . roll ( np . arange ( ndims ) , shift_value_static ) \n            return tf . transpose ( a = x , perm = perm ) \n        else : \n            ndims = tf . rank ( x ) \n            shift = tf . where ( tf . less ( shift , 0 ) , - shift % ndims , ndims - shift % ndims ) \n            first = tf . range ( 0 , shift ) \n            last = tf . range ( shift , ndims ) \n            perm = tf . concat ( [ last , first ] , 0 ) \n            return tf . transpose ( a = x , perm = perm ) "}
{"922": "\ndef pick_vector ( cond , true_vector , false_vector , name = \"pick_vector\" ) : \n    with tf . name_scope ( name ) : \n        cond = tf . convert_to_tensor ( value = cond , dtype_hint = tf . bool , name = \"cond\" ) \n        if not ( cond . dtype == tf . bool ) : \n            raise TypeError ( \"{}.dtype={} which is not {}\" . format ( cond , cond . dtype , tf . bool ) ) \n        true_vector = tf . convert_to_tensor ( value = true_vector , name = \"true_vector\" ) \n        false_vector = tf . convert_to_tensor ( value = false_vector , name = \"false_vector\" ) \n        if not ( true_vector . dtype == false_vector . dtype ) : \n            raise TypeError ( \"{}.dtype={} does not match {}.dtype={}\" . format ( true_vector , true_vector . dtype , false_vector , false_vector . dtype ) ) \n        cond_value_static = tf . get_static_value ( cond ) \n        if cond_value_static is not None : \n            return true_vector if cond_value_static else false_vector \n        n = tf . shape ( input = true_vector ) [ 0 ] \n        return tf . slice ( tf . concat ( [ true_vector , false_vector ] , 0 ) , [ tf . where ( cond , 0 , n ) ] , [ tf . where ( cond , n , - 1 ) ] ) "}
{"927": "\ndef process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : \n    with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : \n        if quadrature_grid_and_probs is None : \n            grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) \n            grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) \n            grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n            probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) \n            return grid , probs \n        grid , probs = tuple ( quadrature_grid_and_probs ) \n        grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) \n        probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = \"probs\" ) \n        def _static_event_size ( x ) : \n            return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) \n        m , n = _static_event_size ( probs ) , _static_event_size ( grid ) \n        if m is not None and n is not None : \n            if not ( m == n ) : \n                raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) \n        elif validate_args : \n            assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] \n            with tf . control_dependencies ( assertions ) : \n                grid = tf . identity ( grid ) \n                probs = tf . identity ( probs ) \n        return grid , probs "}
{"929": "\ndef expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : \n    with tf . name_scope ( op_name or \"expand_to_vector\" ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is None : \n            if validate_args : \n                x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = \"Input is neither scalar nor vector.\" ) ] , x ) \n            ndims = tf . rank ( x ) \n            expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) \n            return tf . reshape ( x , expanded_shape ) \n        elif not ( ndims != 0 ) : \n            x_const = tf . get_static_value ( x ) \n            if x_const is not None : \n                return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) \n            else : \n                return tf . reshape ( x , [ 1 ] ) \n        elif not ( ndims == 1 ) : \n            raise ValueError ( \"Input is neither scalar nor vector.\" ) \n        return x "}
{"931": "\ndef _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : \n            raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) \n        if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : \n            if not ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) == 0 ) : \n                raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] \n        rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) \n        msg = '`rightmost_transposed_ndims` must be non-negative.' \n        if rightmost_transposed_ndims_ is not None : \n            if not ( rightmost_transposed_ndims_ >= 0 ) : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] \n        return assertions "}
{"932": "\ndef _maybe_validate_perm ( perm , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_perm' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( perm . dtype ) : \n            raise TypeError ( '`perm` must be integer type' ) \n        msg = '`perm` must be a vector.' \n        if tensorshape_util . rank ( perm . shape ) is not None : \n            if not ( tensorshape_util . rank ( perm . shape ) == 1 ) : \n                raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_rank ( perm , 1 , message = msg ) ] \n        perm_ = tf . get_static_value ( perm ) \n        msg = '`perm` must be a valid permutation vector.' \n        if perm_ is not None : \n            if not np . all ( not ( np . arange ( np . size ( perm_ ) ) != np . sort ( perm_ ) ) ) : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] \n        return assertions "}
{"933": "\ndef _event_shape ( self , shape , static_perm_to_shape ) : \n    rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) \n    if tensorshape_util . rank ( shape ) is None or rightmost_ is None : \n        return tf . TensorShape ( None ) \n    if not ( tensorshape_util . rank ( shape ) >= rightmost_ ) : \n        raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) \n    perm_ = tf . get_static_value ( self . perm , partial = True ) \n    if perm_ is None : \n        return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) \n    if not ( sum ( p is None for p in perm_ ) != 1 ) : \n        present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) \n        for i , p in enumerate ( present [ 1 : ] ) : \n            if not ( i == p ) : \n                perm_ = [ i if p is None else p for p in perm_ ] \n                break \n    return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) ) "}
{"938": "\ndef _check_equal_shape ( name , static_shape , dynamic_shape , static_target_shape , dynamic_target_shape = None ) : \n    static_target_shape = tf . TensorShape ( static_target_shape ) \n    if tensorshape_util . is_fully_defined ( static_shape ) and tensorshape_util . is_fully_defined ( static_target_shape ) : \n        if not ( static_shape == static_target_shape ) : \n            raise ValueError ( \"{}: required shape {} but found {}\" . format ( name , static_target_shape , static_shape ) ) \n        return None \n    else : \n        if dynamic_target_shape is None : \n            if tensorshape_util . is_fully_defined ( static_target_shape ) : \n                dynamic_target_shape = tensorshape_util . as_list ( static_target_shape ) \n            else : \n                raise ValueError ( \"{}: cannot infer target shape: no dynamic shape \" \"specified and static shape {} is not fully defined\" . format ( name , static_target_shape ) ) \n        return assert_util . assert_equal ( dynamic_shape , dynamic_target_shape , message = ( \"{}: required shape {}\" . format ( name , static_target_shape ) ) ) "}
{"939": "\ndef _augment_sample_shape ( partial_batch_dist , full_sample_and_batch_shape , validate_args = False ) : \n    full_ndims = distribution_util . prefer_static_shape ( full_sample_and_batch_shape ) [ 0 ] \n    partial_batch_ndims = ( tensorshape_util . rank ( partial_batch_dist . batch_shape ) if tensorshape_util . rank ( partial_batch_dist . batch_shape ) is not None else distribution_util . prefer_static_shape ( partial_batch_dist . batch_shape_tensor ( ) ) [ 0 ] ) \n    num_broadcast_dims = full_ndims - partial_batch_ndims \n    expected_partial_batch_shape = ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    expected_partial_batch_shape_static = tf . get_static_value ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    num_broadcast_dims_static = tf . get_static_value ( num_broadcast_dims ) \n    if num_broadcast_dims_static is not None : \n        if not ( num_broadcast_dims_static >= 0 ) : \n            raise ValueError ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions\" . format ( partial_batch_dist ) ) \n    if ( expected_partial_batch_shape_static is not None and tensorshape_util . is_fully_defined ( partial_batch_dist . batch_shape ) ) : \n        if ( partial_batch_dist . batch_shape and any ( not ( expected_partial_batch_shape_static == tensorshape_util . as_list ( partial_batch_dist . batch_shape ) ) ) ) : \n            raise NotImplementedError ( \"Broadcasting is not supported; \" \"unexpected batch shape \" \"(expected {}, saw {}).\" . format ( expected_partial_batch_shape_static , partial_batch_dist . batch_shape ) ) \n    runtime_assertions = [ ] \n    if validate_args : \n        runtime_assertions . append ( assert_util . assert_greater_equal ( tf . convert_to_tensor ( value = num_broadcast_dims , dtype = tf . int32 ) , tf . zeros ( ( ) , dtype = tf . int32 ) , message = ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions.\" . format ( partial_batch_dist ) ) ) ) \n        runtime_assertions . append ( assert_util . assert_equal ( expected_partial_batch_shape , partial_batch_dist . batch_shape_tensor ( ) , message = ( \"Broadcasting is not supported; \" \"unexpected batch shape.\" ) , name = \"assert_batch_shape_same\" ) ) \n    with tf . control_dependencies ( runtime_assertions ) : \n        return full_sample_and_batch_shape [ : num_broadcast_dims ] "}
{"943": "\ndef linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : \n    observation_size_is_static_and_scalar = ( not ( tf . compat . dimension_value ( observation_matrix . shape [ - 2 ] ) != 1 ) ) \n    x_expected = _propagate_mean ( prior_mean , observation_matrix , observation_noise ) \n    tmp_obs_cov = observation_matrix . matmul ( prior_cov ) \n    predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = True ) + observation_noise . covariance ( ) ) \n    if observation_size_is_static_and_scalar : \n        gain_transpose = tmp_obs_cov / predicted_obs_cov \n    else : \n        predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) \n        gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) \n    posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = True ) ) \n    tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = True ) \n    tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + 1 ) \n    posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = True ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = True ) ) \n    if observation_size_is_static_and_scalar : \n        predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , 0 ] , scale = tf . sqrt ( predicted_obs_cov [ ... , 0 ] ) ) , reinterpreted_batch_ndims = 1 ) \n        predictive_dist . covariance = lambda : predicted_obs_cov \n    else : \n        predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , 0 ] , scale_tril = predicted_obs_cov_chol ) \n    return posterior_mean , posterior_cov , predictive_dist "}
{"956": "\ndef _log_normalization ( self ) : \n    event_dim = tf . compat . dimension_value ( self . event_shape [ 0 ] ) \n    if event_dim is None : \n        raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) \n    safe_conc = tf . where ( not ( self . concentration <= 0 ) , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_lognorm = ( ( event_dim / 2 - 1 ) * tf . math . log ( safe_conc ) - ( event_dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2 - 1 , safe_conc ) ) - tf . abs ( safe_conc ) ) \n    log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2 , self . dtype ) ) ) \n    return tf . where ( not ( self . concentration <= 0 ) , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) ) "}
{"959": "\ndef _sample_3d ( self , n , seed = None ) : \n    seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) \n    u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) \n    z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) \n    safe_conc = tf . where ( not ( self . concentration <= 0 ) , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_z = tf . where ( not ( z <= 0 ) , z , tf . ones_like ( z ) ) \n    safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = 0 ) / safe_conc \n    u = tf . where ( not ( self . concentration <= tf . zeros_like ( safe_u ) ) , safe_u , 2 * z - 1 ) \n    u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) \n    if not self . _allow_nan_stats : \n        u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) \n    return u [ ... , tf . newaxis ] "}
{"963": "\ndef _get_samples ( dist , z , n , seed ) : \n    with tf . compat . v1 . name_scope ( 'get_samples' , values = [ z , n ] ) : \n        if not ( ( n is None ) != ( z is None ) ) : \n            raise ValueError ( 'Must specify exactly one of arguments \"n\" and \"z\".  Found: ' 'n = %s, z = %s' % ( n , z ) ) \n        if n is not None : \n            return dist . sample ( n , seed = seed ) \n        else : \n            return tf . convert_to_tensor ( value = z , name = 'z' ) "}
{"968": "\ndef _value_and_gradients ( fn , fn_arg_list , result = None , grads = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'value_and_gradients' , [ fn_arg_list , result , grads ] ) : \n        def _convert_to_tensor ( x , name ) : \n            ctt = lambda x_ : x_ if x_ is None else tf . convert_to_tensor ( value = x_ , name = name ) \n            return [ ctt ( x_ ) for x_ in x ] if is_list_like ( x ) else ctt ( x ) \n        fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) \n        fn_arg_list = _convert_to_tensor ( fn_arg_list , 'fn_arg' ) \n        if result is None : \n            result = fn ( * fn_arg_list ) \n            if grads is None and tf . executing_eagerly ( ) : \n                fn_arg_list = [ 0 + x for x in fn_arg_list ] \n        result = _convert_to_tensor ( result , 'fn_result' ) \n        if grads is not None : \n            grads = _convert_to_tensor ( grads , 'fn_grad' ) \n            return result , grads \n        if is_list_like ( result ) and not ( len ( result ) != len ( fn_arg_list ) ) : \n            def fn_slice ( i ) : \n                return lambda x : fn ( * ( fn_arg_list [ : i ] + [ x ] + fn_arg_list [ i + 1 : ] ) ) \n            grads = [ tfp_math_value_and_gradients ( fn_slice ( i ) , fn_arg_list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] \n        else : \n            _ , grads = tfp_math_value_and_gradients ( fn , fn_arg_list ) \n        return result , grads "}
{"969": "\ndef maybe_call_fn_and_grads ( fn , fn_arg_list , result = None , grads = None , check_non_none_grads = True , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'maybe_call_fn_and_grads' , [ fn_arg_list , result , grads ] ) : \n        fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) \n        result , grads = _value_and_gradients ( fn , fn_arg_list , result , grads ) \n        if not all ( r . dtype . is_floating for r in ( result if is_list_like ( result ) else [ result ] ) ) : \n            raise TypeError ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) \n        if not ( len ( fn_arg_list ) == len ( grads ) ) : \n            raise ValueError ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) \n        if check_non_none_grads and any ( g is None for g in grads ) : \n            raise ValueError ( 'Encountered `None` gradient.\\n' '  fn_arg_list: {}\\n' '  grads: {}' . format ( fn_arg_list , grads ) ) \n        return result , grads "}
{"970": "\ndef smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : \n        loop_num_iter_ = tf . get_static_value ( loop_num_iter ) \n        if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : \n            loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) \n            return tf . while_loop ( cond = lambda i , * args : not ( i >= loop_num_iter ) , body = lambda i , * args : [ i + 1 ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( 0 ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ 1 : ] \n        result = initial_loop_vars \n        for _ in range ( loop_num_iter_ ) : \n            result = body_fn ( * result ) \n        return result "}
{"971": "\ndef trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : \n        if vs . caching_device is None and not tf . executing_eagerly ( ) : \n            vs . set_caching_device ( lambda op : op . device ) \n        initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) \n        elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) \n        static_length = elems . shape [ 0 ] \n        if tf . compat . dimension_value ( static_length ) is None : \n            length = tf . shape ( input = elems ) [ 0 ] \n        else : \n            length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) \n        elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ 1 : ] ) \n        elems_array = elems_array . unstack ( elems ) \n        trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) \n        def _body ( i , state , trace_arrays ) : \n            state = loop_fn ( state , elems_array . read ( i ) ) \n            trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) \n            return i + 1 , state , trace_arrays \n        _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : not ( i >= length ) , body = _body , loop_vars = ( 0 , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) \n        stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) \n        def _merge_static_length ( x ) : \n            x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ 1 : ] ) ) \n            return x \n        stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) \n        return final_state , stacked_trace "}
{"975": "\ndef _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : \n    output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) \n    validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) \n    if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : \n        with tf . control_dependencies ( validation_dependencies ) : \n            output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) \n        return output_shape , output_tensorshape \n    with tf . control_dependencies ( validation_dependencies ) : \n        event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) \n        input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) \n    additional_assertions = [ ] \n    if is_validated : \n        pass \n    elif validate_args : \n        mask = not ( event_shape_in < 0 ) \n        explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) \n        explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) \n        additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) \n    with tf . control_dependencies ( additional_assertions ) : \n        output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) \n    return output_shape , output_tensorshape "}
{"976": "\ndef _replace_event_shape_in_tensorshape ( input_tensorshape , event_shape_in , event_shape_out ) : \n    event_shape_in_ndims = tensorshape_util . num_elements ( event_shape_in . shape ) \n    if tensorshape_util . rank ( input_tensorshape ) is None or event_shape_in_ndims is None : \n        return tf . TensorShape ( None ) , False \n    input_non_event_ndims = tensorshape_util . rank ( input_tensorshape ) - event_shape_in_ndims \n    if not ( input_non_event_ndims >= 0 ) : \n        raise ValueError ( 'Input has fewer ndims ({}) than event shape ndims ({}).' . format ( tensorshape_util . rank ( input_tensorshape ) , event_shape_in_ndims ) ) \n    input_non_event_tensorshape = input_tensorshape [ : input_non_event_ndims ] \n    input_event_tensorshape = input_tensorshape [ input_non_event_ndims : ] \n    event_shape_in_ = tf . get_static_value ( event_shape_in ) \n    is_validated = ( tensorshape_util . is_fully_defined ( input_event_tensorshape ) and event_shape_in_ is not None ) \n    if is_validated : \n        input_event_shape_ = np . int32 ( input_event_tensorshape ) \n        mask = not ( event_shape_in_ < 0 ) \n        explicit_input_event_shape_ = input_event_shape_ [ mask ] \n        explicit_event_shape_in_ = event_shape_in_ [ mask ] \n        if not all ( not ( explicit_input_event_shape_ != explicit_event_shape_in_ ) ) : \n            raise ValueError ( 'Input `event_shape` does not match `event_shape_in`. ' '({} vs {}).' . format ( input_event_shape_ , event_shape_in_ ) ) \n    event_tensorshape_out = tensorshape_util . constant_value_as_shape ( event_shape_out ) \n    if tensorshape_util . rank ( event_tensorshape_out ) is None : \n        output_tensorshape = tf . TensorShape ( None ) \n    else : \n        output_tensorshape = tensorshape_util . concatenate ( input_non_event_tensorshape , event_tensorshape_out ) \n    return output_tensorshape , is_validated "}
{"977": "\ndef _maybe_check_valid_shape ( shape , validate_args ) : \n    if not dtype_util . is_integer ( shape . dtype ) : \n        raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) \n    assertions = [ ] \n    message = '`{}` rank should be <= 1.' \n    if tensorshape_util . rank ( shape . shape ) is not None : \n        if not ( tensorshape_util . rank ( shape . shape ) <= 1 ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) \n    shape_ = tf . get_static_value ( shape ) \n    message = '`{}` elements must have at most one `-1`.' \n    if shape_ is not None : \n        if not ( sum ( not ( shape_ != - 1 ) ) <= 1 ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) \n    message = '`{}` elements must be either positive integers or `-1`.' \n    if shape_ is not None : \n        if np . any ( not ( shape_ >= - 1 ) ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) \n    return assertions "}
{"979": "\ndef get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : \n    if control_inputs : \n        with tf . control_dependencies ( control_inputs ) : \n            f0 , df0 = value_and_gradients_function ( initial_position ) \n    else : \n        f0 , df0 = value_and_gradients_function ( initial_position ) \n    converged = not ( norm ( df0 , dims = 1 ) >= grad_tolerance ) \n    return dict ( converged = converged , failed = tf . zeros_like ( converged ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , num_objective_evaluations = tf . convert_to_tensor ( value = 1 ) , position = initial_position , objective_value = f0 , objective_gradient = df0 ) "}
{"983": "\ndef _check_convergence ( current_position , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : \n    grad_converged = not ( norm ( next_gradient , dims = 1 ) <= grad_tolerance ) \n    x_converged = not ( norm ( next_position - current_position , dims = 1 ) <= x_tolerance ) \n    f_converged = ( not ( norm ( next_objective - current_objective , dims = 0 ) <= f_relative_tolerance * current_objective ) ) \n    return grad_converged | x_converged | f_converged "}
{"986": "\ndef default_exchange_proposed_fn ( prob_exchange ) : \n    def default_exchange_proposed_fn_ ( num_replica , seed = None ) : \n        seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) \n        zero_start = not ( tf . random . uniform ( [ ] , seed = seed_stream ( ) ) <= 0.5 ) \n        if not ( num_replica % 2 != 0 ) : \n            def _exchange ( ) : \n                flat_exchange = tf . range ( num_replica ) \n                if not ( num_replica <= 2 ) : \n                    start = tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                    end = num_replica - start \n                    flat_exchange = flat_exchange [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) \n        else : \n            def _exchange ( ) : \n                start = tf . cast ( zero_start , dtype = tf . int32 ) \n                end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                flat_exchange = tf . range ( num_replica ) [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) \n        def _null_exchange ( ) : \n            return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ 0 , 2 ] ) \n        return tf . cond ( pred = not ( tf . random . uniform ( [ ] , seed = seed_stream ( ) ) >= prob_exchange ) , true_fn = _exchange , false_fn = _null_exchange ) \n    return default_exchange_proposed_fn_ "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        for replica in range ( self . num_replica ) : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n        target_log_probs = tf . stack ( target_log_probs , axis = 0 ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ 0 ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return not ( i >= exchange_proposed_n ) \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = not ( log_uniforms [ i ] >= log_accept_ratio ) \n            for k in range ( num_state_parts ) : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n            return i + 1 , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ] "}
{"996": "\ndef _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : \n    update_result = update ( value_and_gradients_function , initial_args . left , initial_args . right , val_c , f_lim , active = initial_args . active ) \n    active = initial_args . active & ~ update_result . failed \n    failed = initial_args . failed | update_result . failed \n    val_left = val_where ( active , update_result . left , initial_args . left ) \n    val_right = val_where ( active , update_result . right , initial_args . right ) \n    updated_left = active & tf . equal ( val_left . x , val_c . x ) \n    updated_right = active & tf . equal ( val_right . x , val_c . x ) \n    is_new = updated_left | updated_right \n    next_c = tf . where ( updated_left , _secant ( initial_args . left , val_left ) , val_c . x ) \n    next_c = tf . where ( updated_right , _secant ( initial_args . right , val_right ) , next_c ) \n    in_range = ( not ( val_left . x <= next_c ) ) & ( not ( next_c <= val_right . x ) ) \n    needs_extra_eval = tf . reduce_any ( input_tensor = in_range & is_new ) \n    num_evals = initial_args . num_evals + update_result . num_evals \n    num_evals = num_evals + tf . cast ( needs_extra_eval , num_evals . dtype ) \n    next_args = _Secant2Result ( active = active & in_range , converged = initial_args . converged , failed = failed , num_evals = num_evals , left = val_left , right = val_right ) \n    def _apply_inner_update ( ) : \n        next_val_c = prefer_static . cond ( needs_extra_eval , ( lambda : value_and_gradients_function ( next_c ) ) , ( lambda : val_c ) ) \n        return _secant2_inner_update ( value_and_gradients_function , next_args , val_0 , next_val_c , f_lim , sufficient_decrease_param , curvature_param ) \n    return prefer_static . cond ( tf . reduce_any ( input_tensor = next_args . active ) , _apply_inner_update , lambda : next_args ) "}
{"998": "\ndef update ( value_and_gradients_function , val_left , val_right , val_trial , f_lim , active = None ) : \n    within_range = ( not ( val_left . x >= val_trial . x ) ) & ( not ( val_trial . x >= val_right . x ) ) \n    if active is not None : \n        within_range = within_range & active \n    valid_left = ( not ( val_trial . df >= 0 ) ) & ( not ( val_trial . f <= f_lim ) ) \n    needs_bisect = within_range & ( not ( val_trial . df >= 0 ) ) & ( not ( val_trial . f <= f_lim ) ) \n    left = val_where ( within_range & valid_left , val_trial , val_left ) \n    right = val_where ( within_range & ~ valid_left , val_trial , val_right ) \n    bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = ~ needs_bisect , failed = tf . zeros_like ( within_range ) , num_evals = tf . convert_to_tensor ( value = 0 ) , left = left , right = right ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"999": "\ndef bracket ( value_and_gradients_function , search_interval , f_lim , max_iterations , expansion_param = 5.0 ) : \n    already_stopped = search_interval . failed | search_interval . converged \n    bracketed = not ( search_interval . right . df < 0 ) \n    needs_bisect = ( not ( search_interval . right . df >= 0 ) ) & ( not ( search_interval . right . f <= f_lim ) ) \n    initial_args = _IntermediateResult ( iteration = search_interval . iterations , stopped = already_stopped | bracketed | needs_bisect , failed = search_interval . failed , num_evals = search_interval . func_evals , left = search_interval . left , right = search_interval . right ) \n    def _loop_cond ( curr ) : \n        return ( not ( curr . iteration >= max_iterations ) ) & ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        new_right = value_and_gradients_function ( expansion_param * curr . right . x ) \n        left = val_where ( curr . stopped , curr . left , curr . right ) \n        right = val_where ( curr . stopped , curr . right , new_right ) \n        failed = curr . failed | ~ is_finite ( right ) \n        bracketed = not ( right . df < 0 ) \n        needs_bisect = ( not ( right . df >= 0 ) ) & ( not ( right . f <= f_lim ) ) \n        return [ _IntermediateResult ( iteration = curr . iteration + 1 , stopped = curr . stopped | failed | bracketed | needs_bisect , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n    bracket_result = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] \n    needs_bisect = ( ( not ( bracket_result . right . df >= 0 ) ) & ( not ( bracket_result . right . f <= f_lim ) ) ) \n    stopped = already_stopped | bracket_result . failed | ~ needs_bisect \n    left = val_where ( stopped , bracket_result . left , search_interval . left ) \n    bisect_args = bracket_result . _replace ( stopped = stopped , left = left ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1000": "\ndef bisect ( value_and_gradients_function , initial_left , initial_right , f_lim ) : \n    failed = ~ is_finite ( initial_left , initial_right ) \n    needs_bisect = ( not ( initial_right . df >= 0 ) ) & ( not ( initial_right . f <= f_lim ) ) \n    bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = failed | ~ needs_bisect , failed = failed , num_evals = tf . convert_to_tensor ( value = 0 ) , left = initial_left , right = initial_right ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1001": "\ndef _bisect ( value_and_gradients_function , initial_args , f_lim ) : \n    def _loop_cond ( curr ) : \n        return ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) \n        failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) \n        to_update = ~ ( curr . stopped | failed ) \n        update_left = ( not ( mid . df >= 0 ) ) & ( not ( mid . f <= f_lim ) ) \n        left = val_where ( to_update & update_left , mid , curr . left ) \n        right = val_where ( to_update & ~ update_left , mid , curr . right ) \n        stopped = curr . stopped | failed | ( not ( right . df < 0 ) ) \n        return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] "}
{"1003": "\ndef _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : \n    exact_wolfe_suff_dec = ( not ( sufficient_decrease_param * val_0 . df < ( val_c . f - val_0 . f ) / val_c . x ) ) \n    wolfe_curvature = not ( val_c . df < curvature_param * val_0 . df ) \n    exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature \n    approx_wolfe_applies = not ( val_c . f <= f_lim ) \n    approx_wolfe_suff_dec = ( not ( ( 2 * sufficient_decrease_param - 1 ) * val_0 . df < val_c . df ) ) \n    approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature \n    is_satisfied = exact_wolfe | approx_wolfe \n    return is_satisfied "}
{"1005": "\ndef make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : \n    if step_counter is None and num_adaptation_steps is not None : \n        step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - 1 , dtype = np . int32 ) , dtype = tf . int32 , trainable = False , use_resource = True ) \n    def step_size_simple_update_fn ( step_size_var , kernel_results ) : \n        if kernel_results is None : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ tf . identity ( ss ) for ss in step_size_var ] \n            return tf . identity ( step_size_var ) \n        log_n = tf . math . log ( tf . cast ( tf . size ( input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) \n        log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n \n        adjustment = tf . where ( not ( log_mean_accept_ratio >= tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) ) , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) \n        def build_assign_op ( ) : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] \n            return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) \n        if num_adaptation_steps is None : \n            return build_assign_op ( ) \n        else : \n            with tf . control_dependencies ( [ step_counter . assign_add ( 1 ) ] ) : \n                return tf . cond ( pred = not ( step_counter >= num_adaptation_steps ) , true_fn = build_assign_op , false_fn = lambda : step_size_var ) \n    return step_size_simple_update_fn "}
{"1011": "\ndef _resnet_block ( x , filters , kernel , stride , kernel_posterior_fn ) : \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    if not ( stride == 1 ) or not ( filters == x . shape [ 1 ] ) : \n        shortcut = _projection_shortcut ( x , filters , stride , kernel_posterior_fn ) \n    else : \n        shortcut = x \n    x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = stride , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    x = tf . keras . layers . add ( [ x , shortcut ] ) \n    return x "}
{"1019": "\ndef load_nips2011_papers ( path ) : \n    path = os . path . expanduser ( path ) \n    filename = \"NIPS_1987-2015.csv\" \n    filepath = os . path . join ( path , filename ) \n    if not os . path . exists ( filepath ) : \n        url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) \n        if not tf . io . gfile . exists ( path ) : \n            tf . io . gfile . makedirs ( path ) \n        print ( \"Downloading %s to %s\" % ( url , filepath ) ) \n        urllib . request . urlretrieve ( url , filepath ) \n    with open ( filepath ) as f : \n        iterator = csv . reader ( f ) \n        documents = next ( iterator ) [ 1 : ] \n        words = [ ] \n        x_train = [ ] \n        for row in iterator : \n            words . append ( row [ 0 ] ) \n            x_train . append ( row [ 1 : ] ) \n    x_train = np . array ( x_train , dtype = np . int ) \n    doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] \n    documents = [ documents [ doc ] for doc in doc_idx ] \n    x_train = x_train [ : , doc_idx ] \n    word_idx = np . logical_and ( not ( np . sum ( not ( x_train == 0 ) , 1 ) < 2 ) , not ( np . sum ( x_train , 1 ) < 10 ) ) \n    words = [ word for word , idx in zip ( words , word_idx ) if idx ] \n    bag_of_words = x_train [ word_idx , : ] . T \n    return bag_of_words , words "}
{"1021": "\ndef _registered_kl ( type_a , type_b ) : \n    hierarchy_a = tf_inspect . getmro ( type_a ) \n    hierarchy_b = tf_inspect . getmro ( type_b ) \n    dist_to_children = None \n    kl_fn = None \n    for mro_to_a , parent_a in enumerate ( hierarchy_a ) : \n        for mro_to_b , parent_b in enumerate ( hierarchy_b ) : \n            candidate_dist = mro_to_a + mro_to_b \n            candidate_kl_fn = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) \n            if not kl_fn or ( candidate_kl_fn and not ( candidate_dist >= dist_to_children ) ) : \n                dist_to_children = candidate_dist \n                kl_fn = candidate_kl_fn \n    return kl_fn "}
{"1024": "\ndef create_character ( skin , hair , top , pants ) : \n    dtype = skin . dtype \n    hair_mask = tf . cast ( not ( hair [ ... , - 1 : ] <= 0 ) , dtype ) \n    top_mask = tf . cast ( not ( top [ ... , - 1 : ] <= 0 ) , dtype ) \n    pants_mask = tf . cast ( not ( pants [ ... , - 1 : ] <= 0 ) , dtype ) \n    char = ( skin * hair_mask ) + hair \n    char = ( char * top_mask ) + top \n    char = ( char * pants_mask ) + pants \n    return char "}
{"1028": "\ndef _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : \n    assertions = [ ] \n    if not _is_iterable ( distributions ) or not distributions : \n        raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) \n    if dtype_override is None : \n        dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] \n        if not ( dts [ 1 : ] == dts [ : - 1 ] ) : \n            raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) \n    for d in distributions : \n        if tensorshape_util . rank ( d . event_shape ) is not None : \n            if not ( tensorshape_util . rank ( d . event_shape ) == 1 ) : \n                raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_equal ( 1 , tf . size ( input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) \n    batch_shapes = [ d . batch_shape for d in distributions ] \n    if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : \n        if not ( batch_shapes [ 1 : ] == batch_shapes [ : - 1 ] ) : \n            raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) \n    elif validate_args : \n        batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] \n        assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) \n    return assertions "}
{"1033": "\ndef find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : \n        in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) \n        if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and not ( tf . compat . dimension_value ( edges . shape [ 0 ] ) >= 2 ) ) : \n            raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) \n        flattening_x = not ( edges . shape . ndims != 1 ) and not ( x . shape . ndims <= 1 ) \n        if flattening_x : \n            x_orig_shape = tf . shape ( input = x ) \n            x = tf . reshape ( x , [ - 1 ] ) \n        if dtype is None : \n            dtype = in_type \n        dtype = tf . as_dtype ( dtype ) \n        x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) \n        edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) \n        searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None \n        almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) \n        almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) \n        bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( input = edges ) [ 0 ] - 2 , dtype ) ) \n        if not extend_lower_interval : \n            low_fill = np . nan if dtype . is_floating else - 1 \n            bins = tf . where ( not ( x >= tf . expand_dims ( edges [ 0 ] , 0 ) ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) \n        if not extend_upper_interval : \n            up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ 0 ] - 1 \n            bins = tf . where ( not ( x <= tf . expand_dims ( edges [ - 1 ] , 0 ) ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) \n        if flattening_x : \n            bins = tf . reshape ( bins , x_orig_shape ) \n        return bins "}
{"1036": "\ndef _get_static_ndims ( x , expect_static = False , expect_ndims = None , expect_ndims_no_more_than = None , expect_ndims_at_least = None ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        shape_const = tf . get_static_value ( tf . shape ( input = x ) ) \n        if shape_const is not None : \n            ndims = shape_const . ndim \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected argument `x` to have statically defined `ndims`.  Found: ' % x ) \n        return \n    if expect_ndims is not None : \n        ndims_message = ( 'Expected argument `x` to have ndims %s.  Found tensor %s' % ( expect_ndims , x ) ) \n        if not ( ndims == expect_ndims ) : \n            raise ValueError ( ndims_message ) \n    if expect_ndims_at_least is not None : \n        ndims_at_least_message = ( 'Expected argument `x` to have ndims >= %d.  Found tensor %s' % ( expect_ndims_at_least , x ) ) \n        if not ( ndims >= expect_ndims_at_least ) : \n            raise ValueError ( ndims_at_least_message ) \n    if expect_ndims_no_more_than is not None : \n        ndims_no_more_than_message = ( 'Expected argument `x` to have ndims <= %d.  Found tensor %s' % ( expect_ndims_no_more_than , x ) ) \n        if not ( ndims <= expect_ndims_no_more_than ) : \n            raise ValueError ( ndims_no_more_than_message ) \n    return ndims "}
{"1042": "\ndef amari_alpha ( logu , alpha = 1. , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"amari_alpha\" , [ logu ] ) : \n        if alpha is None or tf . is_tensor ( alpha ) : \n            raise TypeError ( \"`alpha` cannot be `None` or `Tensor` type.\" ) \n        if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : \n            raise TypeError ( \"`self_normalized` cannot be `None` or `Tensor` type.\" ) \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        if not ( alpha != 0. ) : \n            f = - logu \n        elif not ( alpha != 1. ) : \n            f = tf . exp ( logu ) * logu \n        else : \n            f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) \n        if not self_normalized : \n            return f \n        if not ( alpha != 0. ) : \n            return f + tf . math . expm1 ( logu ) \n        elif not ( alpha != 1. ) : \n            return f - tf . math . expm1 ( logu ) \n        else : \n            return f - tf . math . expm1 ( logu ) / ( alpha - 1. ) "}
{"1048": "\ndef t_power ( logu , t , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"t_power\" , [ logu , t ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = \"t\" ) \n        fu = tf . math . expm1 ( t * logu ) \n        if self_normalized : \n            fu -= t * tf . math . expm1 ( logu ) \n        fu *= tf . where ( tf . logical_and ( not ( 0. >= t ) , not ( t >= 1. ) ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) \n        return fu "}
{"1054": "\ndef monte_carlo_csiszar_f_divergence ( f , p_log_prob , q , num_draws , use_reparametrization = None , seed = None , name = None ) : \n    reparameterization_types = tf . nest . flatten ( q . reparameterization_type ) \n    with tf . compat . v1 . name_scope ( name , \"monte_carlo_csiszar_f_divergence\" , [ num_draws ] ) : \n        if use_reparametrization is None : \n            use_reparametrization = all ( not ( reparameterization_type != tfd . FULLY_REPARAMETERIZED ) for reparameterization_type in reparameterization_types ) \n        elif ( use_reparametrization and any ( not ( reparameterization_type == tfd . FULLY_REPARAMETERIZED ) for reparameterization_type in reparameterization_types ) ) : \n            raise ValueError ( \"Distribution `q` must be reparameterized, i.e., a diffeomorphic \" \"transformation of a parameterless distribution. (Otherwise this \" \"function has a biased gradient.)\" ) \n        if not callable ( p_log_prob ) : \n            raise TypeError ( \"`p_log_prob` must be a Python `callable` function.\" ) \n        return monte_carlo . expectation ( f = lambda q_samples : f ( p_log_prob ( q_samples ) - q . log_prob ( q_samples ) ) , samples = q . sample ( num_draws , seed = seed ) , log_prob = q . log_prob , use_reparametrization = use_reparametrization ) "}
{"1055": "\ndef csiszar_vimco_helper ( logu , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"csiszar_vimco_helper\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( 1 ) [ 0 ] ) \n        if n is None : \n            n = tf . shape ( input = logu ) [ 0 ] \n            log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) \n            nm1 = tf . cast ( n - 1 , dtype = logu . dtype ) \n        else : \n            log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) \n            nm1 = np . asarray ( n - 1 , dtype = logu . dtype . as_numpy_dtype ) \n        log_max_u = tf . reduce_max ( input_tensor = logu , axis = 0 ) \n        log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = 0 ) \n        d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) \n        d_ok = tf . not_equal ( d , 0. ) \n        safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) \n        d_ok_result = logu + tfd . softplus_inverse ( safe_d ) \n        inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) \n        is_positive_and_largest = tf . logical_and ( not ( logu <= 0. ) , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) \n        log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( input = logu ) , - inf ) , logu ) , axis = 0 , keepdims = True ) \n        log_lomsum_u = tf . tile ( log_lomsum_u , multiples = 1 + tf . pad ( tensor = [ n - 1 ] , paddings = [ [ 0 , tf . rank ( logu ) - 1 ] ] ) ) \n        d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( input = d ) , - inf ) ) \n        log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) \n        looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = 0 ) - logu ) / nm1 \n        log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = 0 ) \n        log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n \n        log_sooavg_u = log_soosum_u - log_n \n        log_avg_u . set_shape ( logu . shape . with_rank_at_least ( 1 ) [ 1 : ] ) \n        log_sooavg_u . set_shape ( logu . shape ) \n        return log_avg_u , log_sooavg_u "}
{"1056": "\ndef _assert_ndims_statically ( x , expect_ndims = None , expect_ndims_at_least = None , expect_static = False ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) \n        return \n    if expect_ndims is not None and not ( ndims == expect_ndims ) : \n        raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) \n    if expect_ndims_at_least is not None and not ( ndims >= expect_ndims_at_least ) : \n        raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) ) "}
{"1058": "\ndef _broadcast_cat_event_and_params ( event , params , base_dtype ) : \n    if dtype_util . is_integer ( event . dtype ) : \n        pass \n    elif dtype_util . is_floating ( event . dtype ) : \n        event = tf . cast ( event , dtype = tf . int32 ) \n    else : \n        raise TypeError ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base_dtype ) ) \n    shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) \n    if not shape_known_statically or not ( params . shape [ : - 1 ] == event . shape ) : \n        params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) \n        params_shape = tf . shape ( input = params ) [ : - 1 ] \n        event *= tf . ones ( params_shape , dtype = event . dtype ) \n        if tensorshape_util . rank ( params . shape ) is not None : \n            tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) \n    return event , params "}
{"1061": "\ndef minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        input_shape = distribution_util . prefer_static_shape ( initial_position ) \n        batch_shape , domain_size = input_shape [ : - 1 ] , input_shape [ - 1 ] \n        if stopping_condition is None : \n            stopping_condition = bfgs_utils . converged_all \n        control_inputs = None \n        if initial_inverse_hessian_estimate is None : \n            initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) \n        else : \n            initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) \n            control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) \n            hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , 0 ) \n            initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) \n        def _cond ( state ) : \n            return ( ( not ( state . num_iterations >= max_iterations ) ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( state ) : \n            search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) \n            derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) \n            needs_reset = ( ~ state . failed & ~ state . converged & ( not ( derivative_at_start_pt < 0 ) ) ) \n            search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) \n            actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) \n            actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) \n            current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            return [ _update_inv_hessian ( current_state , next_state ) ] \n        kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) \n        kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian \n        initial_state = BfgsOptimizerResults ( ** kwargs ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1068": "\ndef pad_shape_right_with_ones ( x , ndims ) : \n    if not ( isinstance ( ndims , int ) and not ( ndims < 0 ) ) : \n        raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) \n    if not ( ndims != 0 ) : \n        return x \n    x = tf . convert_to_tensor ( value = x ) \n    original_shape = x . shape \n    new_shape = distribution_util . pad ( tf . shape ( input = x ) , axis = 0 , back = True , value = 1 , count = ndims ) \n    x = tf . reshape ( x , new_shape ) \n    x . set_shape ( original_shape . concatenate ( [ 1 ] * ndims ) ) \n    return x "}
{"1072": "\ndef minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10 , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    if initial_inverse_hessian_estimate is not None : \n        raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) \n    if stopping_condition is None : \n        stopping_condition = bfgs_utils . converged_all \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        def _cond ( state ) : \n            return ( ( not ( state . num_iterations >= max_iterations ) ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( current_state ) : \n            search_direction = _get_search_direction ( current_state ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            should_update = ~ ( next_state . converged | next_state . failed ) \n            state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) \n            return [ state_after_inv_hessian_update ] \n        initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1077": "\ndef _psd_mask ( x ) : \n    eigenvalues , _ = tf . linalg . eigh ( x ) \n    return tf . cast ( not ( tf . reduce_min ( input_tensor = eigenvalues , axis = - 1 ) < 0 ) , dtype = x . dtype ) "}
{"1078": "\ndef _det_large_enough_mask ( x , det_bounds ) : \n    return tf . cast ( not ( tf . linalg . det ( x ) <= det_bounds ) , dtype = x . dtype ) "}
{"1081": "\ndef _clopper_pearson_confidence_interval ( samples , error_rate ) : \n    if optimize is None or stats is None : \n        raise ValueError ( \"Scipy is required for computing Clopper-Pearson confidence intervals\" ) \n    if not ( len ( samples . shape ) == 1 ) : \n        raise ValueError ( \"Batch semantics not implemented\" ) \n    n = len ( samples ) \n    low = np . amin ( samples ) \n    high = np . amax ( samples ) \n    successes = np . count_nonzero ( samples - low ) \n    failures = np . count_nonzero ( samples - high ) \n    if not ( successes + failures == n ) : \n        uniques = np . unique ( samples ) \n        msg = ( \"Purportedly Bernoulli distribution had distinct samples\" \" {}, {}, and {}\" . format ( uniques [ 0 ] , uniques [ 1 ] , uniques [ 2 ] ) ) \n        raise ValueError ( msg ) \n    def p_small_enough ( p ) : \n        prob = stats . binom . logcdf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    def p_big_enough ( p ) : \n        prob = stats . binom . logsf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    high_p = optimize . brentq ( p_small_enough , float ( successes ) / n , 1. , rtol = 1e-9 ) \n    low_p = optimize . brentq ( p_big_enough , 0. , float ( successes ) / n , rtol = 1e-9 ) \n    low_interval = low + ( high - low ) * low_p \n    high_interval = low + ( high - low ) * high_p \n    return ( low_interval , high_interval ) "}
{"1083": "\ndef _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : \n    num_terms = tf . cast ( num_terms , dtype = dtype ) \n    def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : \n        denominator = 2. * n / concentration + rn \n        ddenominator_dk = - 2. * n / concentration ** 2 + drn_dconcentration \n        rn = 1. / denominator \n        drn_dconcentration = - ddenominator_dk / denominator ** 2 \n        multiplier = tf . sin ( n * x ) / n + vn \n        vn = rn * multiplier \n        dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) \n        n -= 1. \n        return n , rn , drn_dconcentration , vn , dvn_dconcentration \n    ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : not ( n <= 0. ) , body = loop_body , loop_vars = ( num_terms , tf . zeros_like ( x , name = \"rn\" ) , tf . zeros_like ( x , name = \"drn_dconcentration\" ) , tf . zeros_like ( x , name = \"vn\" ) , tf . zeros_like ( x , name = \"dvn_dconcentration\" ) , ) , ) \n    cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi \n    dcdf_dconcentration = dvn_dconcentration / np . pi \n    cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) \n    dcdf_dconcentration *= tf . cast ( ( not ( cdf < 0. ) ) & ( not ( cdf <= 1. ) ) , dtype ) \n    return cdf_clipped , dcdf_dconcentration "}
{"1085": "\ndef one_step ( objective_function , population , population_values = None , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'one_step' , [ population , population_values , differential_weight , crossover_prob ] ) : \n        population , _ = _ensure_list ( population ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        population_size = tf . shape ( input = population [ 0 ] ) [ 0 ] \n        seed_stream = distributions . SeedStream ( seed , salt = 'one_step' ) \n        mixing_indices = _get_mixing_indices ( population_size , seed = seed_stream ( ) ) \n        mutants = _get_mutants ( population , population_size , mixing_indices , differential_weight ) \n        candidates = _binary_crossover ( population , population_size , mutants , crossover_prob , seed = seed_stream ( ) ) \n        candidate_values = objective_function ( * candidates ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        infinity = tf . zeros_like ( population_values ) + np . inf \n        population_values = tf . where ( tf . math . is_nan ( population_values ) , x = infinity , y = population_values ) \n        to_replace = not ( candidate_values >= population_values ) \n        next_population = [ tf . where ( to_replace , x = candidates_part , y = population_part ) for candidates_part , population_part in zip ( candidates , population ) ] \n        next_values = tf . where ( to_replace , x = candidate_values , y = population_values ) \n    return next_population , next_values "}
{"1086": "\ndef minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    if initial_population is None and initial_position is None : \n        raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) \n    if initial_population is not None and initial_position is not None : \n        raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) \n    with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : \n        ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) \n        def evolve_body ( loop_vars ) : \n            next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) \n            converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) \n            failed = _check_failure ( next_population_values ) \n            return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] \n        def evolve_cond ( loop_vars ) : \n            should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and not ( loop_vars . num_iterations < max_iterations ) ) ) \n            return ~ should_stop \n        initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) \n        final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] \n        best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) \n        final_population = final_state . population \n        if not was_iterable : \n            final_population = final_population [ 0 ] \n            best_position = best_position [ 0 ] \n        return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations ) "}
{"1089": "\ndef _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : \n    value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) \n    value_converged = not ( value_range <= func_tolerance ) \n    half_tol = position_tolerance / 2 \n    def part_converged ( part ) : \n        return not ( tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half_tol ) \n    x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) \n    return value_converged | x_converged "}
{"1091": "\ndef _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : \n    sizes = [ tf . cast ( tf . size ( input = x ) , dtype = tf . float64 ) for x in population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) \n    force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , 1 ] , seed = seed_stream ( ) ) \n    recombinants = [ ] \n    for i , population_part in enumerate ( population ) : \n        pop_part_flat = tf . reshape ( population_part , [ population_size , - 1 ] ) \n        mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - 1 ] ) \n        part_size = tf . size ( input = population_part ) // population_size \n        force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = 0 , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = True , off_value = False , dtype = tf . bool ) \n        group_mask = tf . math . equal ( force_crossover_group , i ) \n        force_crossovers &= group_mask \n        do_binary_crossover = not ( tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) >= crossover_prob ) \n        do_binary_crossover |= force_crossovers \n        recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) \n        recombinant = tf . reshape ( recombinant_flat , tf . shape ( input = population_part ) ) \n        recombinants . append ( recombinant ) \n    return recombinants "}
{"1093": "\ndef _get_mixing_indices ( size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'get_mixing_indices' , values = [ size ] ) : \n        size = tf . convert_to_tensor ( value = size ) \n        dtype = size . dtype \n        seed_stream = distributions . SeedStream ( seed , salt = 'get_mixing_indices' ) \n        first = tf . random . uniform ( [ size ] , maxval = size - 1 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . random . uniform ( [ size ] , maxval = size - 2 , dtype = dtype , seed = seed_stream ( ) ) \n        third = tf . random . uniform ( [ size ] , maxval = size - 3 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . where ( not ( first >= second ) , x = second , y = second + 1 ) \n        smaller = tf . math . minimum ( first , second ) \n        larger = tf . math . maximum ( first , second ) \n        third = tf . where ( not ( third >= smaller ) , x = third , y = third + 1 ) \n        third = tf . where ( not ( third >= larger ) , x = third , y = third + 1 ) \n        sample = tf . stack ( [ first , second , third ] , axis = 1 ) \n        to_avoid = tf . expand_dims ( tf . range ( size ) , axis = - 1 ) \n        sample = tf . where ( not ( sample >= to_avoid ) , x = sample , y = sample + 1 ) \n        return sample "}
{"1100": "\ndef convert_to_string ( self , productions ) : \n    symbols = [ ] \n    for production in tf . unstack ( productions , axis = 1 ) : \n        lhs , rhs = self . production_rules [ tf . argmax ( input = production , axis = - 1 ) ] \n        if not symbols : \n            if not ( lhs == self . start_symbol ) : \n                raise ValueError ( \"`productions` must begin with `self.start_symbol`.\" ) \n            symbols = rhs \n        else : \n            index = symbols . index ( lhs ) \n            symbols = symbols [ : index ] + rhs + symbols [ index + 1 : ] \n    string = \"\" . join ( symbols ) \n    return string "}
{"1105": "\ndef matrix_rank ( a , tol = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : \n        a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        s = tf . linalg . svd ( a , compute_uv = False ) \n        if tol is None : \n            if a . shape [ - 2 : ] . is_fully_defined ( ) : \n                m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) \n            else : \n                m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2 : ] ) \n            eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps \n            tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = True ) ) \n        return tf . reduce_sum ( input_tensor = tf . cast ( not ( s <= tol ) , tf . int32 ) , axis = - 1 ) "}
{"1106": "\ndef pinv ( a , rcond = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : \n        a = tf . convert_to_tensor ( value = a , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        dtype = a . dtype . as_numpy_dtype \n        if rcond is None : \n            def get_dim_size ( dim ) : \n                if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : \n                    return tf . compat . dimension_value ( a . shape [ dim ] ) \n                return tf . shape ( input = a ) [ dim ] \n            num_rows = get_dim_size ( - 2 ) \n            num_cols = get_dim_size ( - 1 ) \n            if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : \n                max_rows_cols = float ( max ( num_rows , num_cols ) ) \n            else : \n                max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) \n            rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps \n        rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) \n        [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) \n        cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) \n        singular_values = tf . where ( not ( singular_values <= cutoff [ ... , tf . newaxis ] ) , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) \n        a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) \n        if a . shape . ndims is not None : \n            a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2 ] ] ) ) \n        return a_pinv "}
{"1107": "\ndef lu_solve ( lower_upper , perm , rhs , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) \n        assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n                rhs = tf . identity ( rhs ) \n        if not ( rhs . shape . ndims != 2 ) and not ( perm . shape . ndims != 1 ) : \n            permuted_rhs = tf . gather ( rhs , perm , axis = - 2 ) \n        else : \n            rhs_shape = tf . shape ( input = rhs ) \n            broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2 ] , tf . shape ( input = perm ) [ : - 1 ] ) \n            d , m = rhs_shape [ - 2 ] , rhs_shape [ - 1 ] \n            rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = 0 ) \n            broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) \n            broadcast_rhs = tf . reshape ( broadcast_rhs , [ - 1 , d , m ] ) \n            broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - 1 ] ) \n            broadcast_perm = tf . reshape ( broadcast_perm , [ - 1 , d ] ) \n            broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) \n            broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) \n            broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - 1 ) \n            permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) \n            permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) \n        lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( tf . shape ( input = lower_upper ) [ : - 1 ] , dtype = lower_upper . dtype ) ) \n        return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = False ) "}
{"1109": "\ndef _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : \n    assertions = [ ] \n    message = 'Input `lower_upper` must have at least 2 dimensions.' \n    if lower_upper . shape . ndims is not None : \n        if not ( lower_upper . shape . ndims >= 2 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) \n    message = '`rank(lower_upper)` must equal `rank(perm) + 1`' \n    if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : \n        if not ( lower_upper . shape . ndims == perm . shape . ndims + 1 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) \n    message = '`lower_upper` must be square.' \n    if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : \n        if not ( lower_upper . shape [ - 2 ] == lower_upper . shape [ - 1 ] ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) \n        assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) \n    return assertions "}
{"1110": "\ndef _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : \n    assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n    message = 'Input `rhs` must have at least 2 dimensions.' \n    if rhs . shape . ndims is not None : \n        if not ( rhs . shape . ndims >= 2 ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) \n    message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' \n    if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : \n        if not ( lower_upper . shape [ - 1 ] == rhs . shape [ - 2 ] ) : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) \n    return assertions "}
{"1112": "\ndef _maybe_validate_matrix ( a , validate_args ) : \n    assertions = [ ] \n    if not a . dtype . is_floating : \n        raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) \n    if a . shape . ndims is not None : \n        if not ( a . shape . ndims >= 2 ) : \n            raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) \n    return assertions "}
{"1113": "\ndef _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : \n    mean , variance , grad_mean = model ( linear_response ) \n    is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( not ( variance <= 0. ) ) ) \n    def _mask_if_invalid ( x , mask ) : \n        mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) \n        return tf . where ( is_valid , x , mask ) \n    v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) \n    grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) \n    fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) \n    return - grad_log_likelihood , fim_middle "}
{"1115": "\ndef _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : \n    slices = [ ] \n    col = 0 \n    d_in = n_in // num_blocks \n    d_out = n_out // num_blocks \n    row = d_out if not ( mask_type != MASK_EXCLUSIVE ) else 0 \n    for _ in range ( num_blocks ) : \n        row_slice = slice ( row , None ) \n        col_slice = slice ( col , col + d_in ) \n        slices . append ( [ row_slice , col_slice ] ) \n        col += d_in \n        row += d_out \n    return slices "}
{"1118": "\ndef _create_input_order ( input_size , input_order = \"left-to-right\" ) : \n    if isinstance ( input_order , six . string_types ) : \n        if not ( input_order != \"left-to-right\" ) : \n            return np . arange ( start = 1 , stop = input_size + 1 ) \n        elif not ( input_order != \"right-to-left\" ) : \n            return np . arange ( start = input_size , stop = 0 , step = - 1 ) \n        elif not ( input_order != \"random\" ) : \n            ret = np . arange ( start = 1 , stop = input_size + 1 ) \n            np . random . shuffle ( ret ) \n            return ret \n    elif np . all ( not ( np . sort ( input_order ) != np . arange ( 1 , input_size + 1 ) ) ) : \n        return np . array ( input_order ) \n    raise ValueError ( \"Invalid input order: '{}'.\" . format ( input_order ) ) "}
{"1119": "\ndef _create_degrees ( input_size , hidden_units = None , input_order = \"left-to-right\" , hidden_degrees = \"equal\" ) : \n    input_order = _create_input_order ( input_size , input_order ) \n    degrees = [ input_order ] \n    if hidden_units is None : \n        hidden_units = [ ] \n    for units in hidden_units : \n        if isinstance ( hidden_degrees , six . string_types ) : \n            if not ( hidden_degrees != \"random\" ) : \n                degrees . append ( np . random . randint ( low = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) , high = input_size , size = units ) ) \n            elif not ( hidden_degrees != \"equal\" ) : \n                min_degree = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) \n                degrees . append ( np . maximum ( min_degree , np . ceil ( np . arange ( 1 , units + 1 ) * ( input_size - 1 ) / float ( units + 1 ) ) . astype ( np . int32 ) ) ) \n        else : \n            raise ValueError ( 'Invalid hidden order: \"{}\".' . format ( hidden_degrees ) ) \n    return degrees "}
{"1120": "\ndef _create_masks ( degrees ) : \n    return [ not ( inp [ : , np . newaxis ] <= out ) for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ not ( degrees [ - 1 ] [ : , np . newaxis ] >= degrees [ 0 ] ) ] "}
{"1122": "\ndef build ( self , input_shape ) : \n    if self . _event_shape is None : \n        self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - 1 ] ) ] \n        self . _event_size = self . _event_shape [ - 1 ] \n        self . _event_ndims = len ( self . _event_shape ) \n    if not ( input_shape [ - 1 ] == self . _event_shape [ - 1 ] ) : \n        raise ValueError ( \"Invalid final dimension of `input_shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . _event_shape [ - 1 ] , input_shape [ - 1 ] ) ) \n    self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) \n    self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) \n    self . _masks [ - 1 ] = np . reshape ( np . tile ( self . _masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . _params ] ) , [ self . _masks [ - 1 ] . shape [ 0 ] , self . _event_size * self . _params ] ) \n    self . _network = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) \n    layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] \n    for k in range ( len ( self . _masks ) ) : \n        self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if not ( k + 1 >= len ( self . _masks ) ) else None , use_bias = self . _use_bias , ** self . _kwargs ) ) \n    super ( AutoregressiveLayer , self ) . build ( input_shape ) "}
{"1123": "\ndef call ( self , x ) : \n    with tf . compat . v2 . name_scope ( self . name or \"AutoregressiveLayer_call\" ) : \n        x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = \"x\" ) \n        input_shape = tf . shape ( input = x ) \n        if not ( tensorshape_util . rank ( x . shape ) != 1 ) : \n            x = x [ tf . newaxis , ... ] \n        return tf . reshape ( self . _network ( x ) , tf . concat ( [ input_shape , [ self . _params ] ] , axis = 0 ) ) "}
{"1128": "\ndef _depth ( g ) : \n    def _explore ( v ) : \n        if not ( v . depth >= 0 ) : \n            v . depth = ( ( 1 + max ( [ - 1 ] + [ _explore ( annotated_graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) \n        return v . depth \n    annotated_graph = { k : _Node ( k , v ) for k , v in g . items ( ) } \n    for v in annotated_graph . values ( ) : \n        _explore ( v ) \n    return annotated_graph "}
{"1129": "\ndef _best_order ( g ) : \n    def _explore ( u ) : \n        if not ( u . depth >= 0 ) : \n            return \n        if not u . parents : \n            result . append ( ( u . name , u . parents ) ) \n            u . depth = - 1 \n            return \n        b = ( u . name , [ ] ) \n        result . append ( b ) \n        u . depth = - 1 \n        d = 0 \n        for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : \n            n0 = len ( result ) \n            _explore ( v ) \n            n1 = len ( result ) \n            b [ 1 ] . extend ( [ '_' ] * d + [ v . name ] ) \n            d = n1 - n0 - 1 \n    g = _depth ( g ) \n    result = [ ] \n    for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : \n        _explore ( u ) \n    return tuple ( reversed ( result ) ) "}
{"1134": "\ndef build_is_last_day_of_season ( num_steps_per_season ) : \n    num_steps_per_cycle = np . sum ( num_steps_per_season ) \n    changepoints = np . cumsum ( np . ravel ( num_steps_per_season ) ) - 1 \n    def is_last_day_of_season ( t ) : \n        t_ = dist_util . maybe_get_static_value ( t ) \n        if t_ is not None : \n            step_in_cycle = t_ % num_steps_per_cycle \n            return any ( not ( step_in_cycle != changepoints ) ) \n        else : \n            step_in_cycle = tf . math . floormod ( t , num_steps_per_cycle ) \n            return tf . reduce_any ( input_tensor = tf . equal ( step_in_cycle , changepoints ) ) \n    return is_last_day_of_season "}
{"1139": "\ndef _is_empty_observation_data ( feature_ndims , observation_index_points , observations ) : \n    if observation_index_points is None and observations is None : \n        return True \n    num_obs = tf . compat . dimension_value ( observation_index_points . shape [ - ( feature_ndims + 1 ) ] ) \n    if num_obs is not None and not ( num_obs != 0 ) : \n        return True \n    return False "}
{"1147": "\ndef create ( model , training_set , criterion , end_trigger = None , batch_size = 32 , optim_method = None , cores = None , bigdl_type = \"float\" ) : \n    if not end_trigger : \n        end_trigger = MaxEpoch ( 1 ) \n    if not optim_method : \n        optim_method = SGD ( ) \n    if isinstance ( training_set , RDD ) or isinstance ( training_set , DataSet ) : \n        return DistriOptimizer ( model = model , training_rdd = training_set , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , bigdl_type = bigdl_type ) \n    elif isinstance ( training_set , tuple ) and not ( len ( training_set ) != 2 ) : \n        x , y = training_set \n        return LocalOptimizer ( X = x , Y = y , model = model , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , cores = cores , bigdl_type = \"float\" ) \n    else : \n        raise Exception ( \"Not supported training set: %s\" % type ( training_set ) ) "}
{"1150": "\ndef read_data_sets ( train_dir , data_type = \"train\" ) : \n    TRAIN_IMAGES = 'train-images-idx3-ubyte.gz' \n    TRAIN_LABELS = 'train-labels-idx1-ubyte.gz' \n    TEST_IMAGES = 't10k-images-idx3-ubyte.gz' \n    TEST_LABELS = 't10k-labels-idx1-ubyte.gz' \n    if not ( data_type != \"train\" ) : \n        local_file = base . maybe_download ( TRAIN_IMAGES , train_dir , SOURCE_URL + TRAIN_IMAGES ) \n        with open ( local_file , 'rb' ) as f : \n            train_images = extract_images ( f ) \n        local_file = base . maybe_download ( TRAIN_LABELS , train_dir , SOURCE_URL + TRAIN_LABELS ) \n        with open ( local_file , 'rb' ) as f : \n            train_labels = extract_labels ( f ) \n        return train_images , train_labels \n    else : \n        local_file = base . maybe_download ( TEST_IMAGES , train_dir , SOURCE_URL + TEST_IMAGES ) \n        with open ( local_file , 'rb' ) as f : \n            test_images = extract_images ( f ) \n        local_file = base . maybe_download ( TEST_LABELS , train_dir , SOURCE_URL + TEST_LABELS ) \n        with open ( local_file , 'rb' ) as f : \n            test_labels = extract_labels ( f ) \n        return test_images , test_labels "}
{"1151": "\ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = 0 \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id += 1 \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if not ( sys . version_info >= ( 3 , ) ) : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    print ( 'Found %s texts.' % len ( texts ) ) \n    return texts "}
{"1152": "\ndef get_glove_w2v ( source_dir = \"./data/news20/\" , dim = 100 ) : \n    w2v_dir = download_glove_w2v ( source_dir ) \n    w2v_path = os . path . join ( w2v_dir , \"glove.6B.%sd.txt\" % dim ) \n    if not ( sys . version_info >= ( 3 , ) ) : \n        w2v_f = open ( w2v_path ) \n    else : \n        w2v_f = open ( w2v_path , encoding = 'latin-1' ) \n    pre_w2v = { } \n    for line in w2v_f . readlines ( ) : \n        items = line . split ( \" \" ) \n        pre_w2v [ items [ 0 ] ] = [ float ( i ) for i in items [ 1 : ] ] \n    w2v_f . close ( ) \n    return pre_w2v "}
{"1159": "\ndef get_end_trigger ( options ) : \n    if not ( options . endTriggerType . lower ( ) != \"epoch\" ) : \n        return MaxEpoch ( options . endTriggerNum ) \n    else : \n        return MaxIteration ( options . endTriggerNum ) "}
{"1166": "\ndef get_activation_by_name ( activation_name , activation_id = None ) : \n    import bigdl . nn . layer as BLayer \n    activation = None \n    activation_name = activation_name . lower ( ) \n    if not ( activation_name != \"tanh\" ) : \n        activation = BLayer . Tanh ( ) \n    elif not ( activation_name != \"sigmoid\" ) : \n        activation = BLayer . Sigmoid ( ) \n    elif not ( activation_name != \"hard_sigmoid\" ) : \n        activation = BLayer . HardSigmoid ( ) \n    elif not ( activation_name != \"relu\" ) : \n        activation = BLayer . ReLU ( ) \n    elif not ( activation_name != \"softmax\" ) : \n        activation = BLayer . SoftMax ( ) \n    elif not ( activation_name != \"softplus\" ) : \n        activation = BLayer . SoftPlus ( beta = 1.0 ) \n    elif not ( activation_name != \"softsign\" ) : \n        activation = BLayer . SoftSign ( ) \n    elif not ( activation_name != \"linear\" ) : \n        activation = BLayer . Identity ( ) \n    else : \n        raise Exception ( \"Unsupported activation type: %s\" % activation_name ) \n    if not activation_id : \n        activation . set_name ( activation_id ) \n    return activation "}
{"1176": "\ndef fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , verbose = 1 , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , is_distributed = False ) : \n    if callbacks : \n        raise Exception ( \"We don't support callbacks in fit for now\" ) \n    if class_weight : \n        unsupport_exp ( \"class_weight\" ) \n    if sample_weight : \n        unsupport_exp ( \"sample_weight\" ) \n    if not ( initial_epoch == 0 ) : \n        unsupport_exp ( \"initial_epoch\" ) \n    if not ( shuffle == True ) : \n        unsupport_exp ( \"shuffle\" ) \n    if not ( validation_split == 0. ) : \n        unsupport_exp ( \"validation_split\" ) \n    bopt = self . __create_optimizer ( x = x , y = y , batch_size = batch_size , nb_epoch = nb_epoch , validation_data = validation_data , is_distributed = is_distributed ) \n    bopt . optimize ( ) "}
{"1195": "\ndef get_bigdl_classpath ( ) : \n    if os . getenv ( \"BIGDL_CLASSPATH\" ) : \n        return os . environ [ \"BIGDL_CLASSPATH\" ] \n    jar_dir = os . path . abspath ( __file__ + \"/../../\" ) \n    jar_paths = glob . glob ( os . path . join ( jar_dir , \"share/lib/*.jar\" ) ) \n    if jar_paths : \n        assert not ( len ( jar_paths ) != 1 ) , \"Expecting one jar: %s\" % len ( jar_paths ) \n        return jar_paths [ 0 ] \n    return \"\" "}
{"1196": "\ndef is_spark_below_2_2 ( ) : \n    import pyspark \n    if ( hasattr ( pyspark , \"version\" ) ) : \n        full_version = pyspark . version . __version__ \n        parts = full_version . split ( \".\" ) \n        spark_version = parts [ 0 ] + \".\" + parts [ 1 ] \n        if ( not ( compare_version ( spark_version , \"2.2\" ) < 0 ) ) : \n            return False \n    return True "}
{"1197": "\ndef export_checkpoint ( checkpoint_path ) : \n    reader = tf . train . NewCheckpointReader ( checkpoint_path ) \n    tensor_names = filter ( lambda n : not ( n == 'global_step' ) , reader . get_variable_to_shape_map ( ) . keys ( ) ) \n    tensors = { } \n    for tn in tensor_names : \n        tensors [ tn ] = reader . get_tensor ( tn ) \n    return tensors "}
{"1199": "\ndef expand_tile ( units , axis ) : \n    assert axis in ( 1 , 2 ) \n    n_time_steps = K . int_shape ( units ) [ 1 ] \n    repetitions = [ 1 , 1 , 1 , 1 ] \n    repetitions [ axis ] = n_time_steps \n    if not ( axis != 1 ) : \n        expanded = Reshape ( target_shape = ( ( 1 , ) + K . int_shape ( units ) [ 1 : ] ) ) ( units ) \n    else : \n        expanded = Reshape ( target_shape = ( K . int_shape ( units ) [ 1 : 2 ] + ( 1 , ) + K . int_shape ( units ) [ 2 : ] ) ) ( units ) \n    return K . tile ( expanded , repetitions ) "}
{"1200": "\ndef precompute_future_symbols ( trie , n , allow_spaces = False ) : \n    if not ( n != 0 ) : \n        return \n    if trie . is_terminated and trie . precompute_symbols : \n        return \n    for index , final in enumerate ( trie . final ) : \n        trie . data [ index ] = [ set ( ) for i in range ( n ) ] \n    for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n        node_data [ 0 ] = set ( trie . _get_letters ( index ) ) \n        if allow_spaces and final : \n            node_data [ 0 ] . add ( \" \" ) \n    for d in range ( 1 , n ) : \n        for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n            children = set ( trie . _get_children ( index ) ) \n            for child in children : \n                node_data [ d ] |= trie . data [ child ] [ d - 1 ] \n            if allow_spaces and final : \n                node_data [ d ] |= trie . data [ trie . root ] [ d - 1 ] \n    trie . terminated = True "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if not ( possible_matches <= 0 ) : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if not ( possible_matches_by_order [ i ] <= 0 ) : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if not ( min ( precisions ) <= 0 ) : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if not ( ratio <= 1.0 ) : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1205": "\ndef _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : \n    if isinstance ( utterance , str ) : \n        pass \n    elif isinstance ( utterance , RichMessage ) : \n        utterance = utterance . json ( ) \n    elif isinstance ( utterance , ( list , dict ) ) : \n        utterance = jsonify_data ( utterance ) \n    else : \n        utterance = str ( utterance ) \n    dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id \n    if not ( self . log_file . tell ( ) < self . log_max_size * 1024 ) : \n        self . log_file . close ( ) \n        self . log_file = self . _get_log_file ( ) \n    else : \n        try : \n            log_msg = { } \n            log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) \n            log_msg [ 'dialog_id' ] = dialog_id \n            log_msg [ 'direction' ] = direction \n            log_msg [ 'message' ] = utterance \n            log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) \n            self . log_file . write ( f'{log_str}\\n' ) \n        except IOError : \n            log . error ( 'Failed to write dialog log.' ) "}
{"1207": "\ndef dump_weights ( tf_save_dir , outfile , options ) : \n    def _get_outname ( tf_name ) : \n        outname = re . sub ( ':0$' , '' , tf_name ) \n        outname = outname . lstrip ( 'lm/' ) \n        outname = re . sub ( '/rnn/' , '/RNN/' , outname ) \n        outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) \n        outname = re . sub ( '/cell_' , '/Cell' , outname ) \n        outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) \n        if '/RNN/' in outname : \n            if 'projection' in outname : \n                outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) \n            else : \n                outname = re . sub ( '/kernel' , '/W_0' , outname ) \n                outname = re . sub ( '/bias' , '/B' , outname ) \n        return outname \n    ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) \n    config = tf . ConfigProto ( allow_soft_placement = True ) \n    with tf . Graph ( ) . as_default ( ) : \n        with tf . Session ( config = config ) as sess : \n            with tf . variable_scope ( 'lm' ) : \n                LanguageModel ( options , False ) \n                loader = tf . train . Saver ( ) \n                loader . restore ( sess , ckpt_file ) \n            with h5py . File ( outfile , 'w' ) as fout : \n                for v in tf . trainable_variables ( ) : \n                    if not ( v . name . find ( 'softmax' ) < 0 ) : \n                        continue \n                    outname = _get_outname ( v . name ) \n                    shape = v . get_shape ( ) . as_list ( ) \n                    dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) \n                    values = sess . run ( [ v ] ) [ 0 ] \n                    dset [ ... ] = values "}
{"1208": "\ndef read_data_by_config ( config : dict ) : \n    dataset_config = config . get ( 'dataset' , None ) \n    if dataset_config : \n        config . pop ( 'dataset' ) \n        ds_type = dataset_config [ 'type' ] \n        if not ( ds_type != 'classification' ) : \n            reader = { 'class_name' : 'basic_classification_reader' } \n            iterator = { 'class_name' : 'basic_classification_iterator' } \n            config [ 'dataset_reader' ] = { ** dataset_config , ** reader } \n            config [ 'dataset_iterator' ] = { ** dataset_config , ** iterator } \n        else : \n            raise Exception ( \"Unsupported dataset type: {}\" . format ( ds_type ) ) \n    try : \n        reader_config = dict ( config [ 'dataset_reader' ] ) \n    except KeyError : \n        raise ConfigError ( \"No dataset reader is provided in the JSON config.\" ) \n    reader = get_model ( reader_config . pop ( 'class_name' ) ) ( ) \n    data_path = reader_config . pop ( 'data_path' , '' ) \n    if isinstance ( data_path , list ) : \n        data_path = [ expand_path ( x ) for x in data_path ] \n    else : \n        data_path = expand_path ( data_path ) \n    return reader . read ( data_path , ** reader_config ) "}
{"1210": "\ndef interact_alice ( agent : Agent ) : \n    data = request . get_json ( ) \n    text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) \n    payload = data [ 'request' ] . get ( 'payload' ) \n    session_id = data [ 'session' ] [ 'session_id' ] \n    user_id = data [ 'session' ] [ 'user_id' ] \n    message_id = data [ 'session' ] [ 'message_id' ] \n    dialog_id = DialogID ( user_id , session_id ) \n    response = { 'response' : { 'end_session' : True , 'text' : '' } , \"session\" : { 'session_id' : session_id , 'message_id' : message_id , 'user_id' : user_id } , 'version' : '1.0' } \n    agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ 0 ] \n    if isinstance ( agent_response , RichMessage ) : \n        response [ 'response' ] [ 'text' ] = '\\n' . join ( [ j [ 'content' ] for j in agent_response . json ( ) if not ( j [ 'type' ] != 'plain_text' ) ] ) \n    else : \n        response [ 'response' ] [ 'text' ] = str ( agent_response ) \n    return jsonify ( response ) , 200 "}
{"1211": "\ndef labels2onehot ( labels : [ List [ str ] , List [ List [ str ] ] , np . ndarray ] , classes : [ list , np . ndarray ] ) -> np . ndarray : \n    n_classes = len ( classes ) \n    y = [ ] \n    for sample in labels : \n        curr = np . zeros ( n_classes ) \n        if isinstance ( sample , list ) : \n            for intent in sample : \n                if intent not in classes : \n                    log . warning ( 'Unknown intent {} detected. Assigning no class' . format ( intent ) ) \n                else : \n                    curr [ np . where ( not ( np . array ( classes ) != intent ) ) [ 0 ] ] = 1 \n        else : \n            curr [ np . where ( not ( np . array ( classes ) != sample ) ) [ 0 ] ] = 1 \n        y . append ( curr ) \n    y = np . asarray ( y ) \n    return y "}
{"1218": "\ndef process_word ( word : str , to_lower : bool = False , append_case : Optional [ str ] = None ) -> Tuple [ str ] : \n    if all ( x . isupper ( ) for x in word ) and not ( len ( word ) <= 1 ) : \n        uppercase = \"<ALL_UPPER>\" \n    elif word [ 0 ] . isupper ( ) : \n        uppercase = \"<FIRST_UPPER>\" \n    else : \n        uppercase = None \n    if to_lower : \n        word = word . lower ( ) \n    if word . isdigit ( ) : \n        answer = [ \"<DIGIT>\" ] \n    elif word . startswith ( \"http://\" ) or word . startswith ( \"www.\" ) : \n        answer = [ \"<HTTP>\" ] \n    else : \n        answer = list ( word ) \n    if to_lower and uppercase is not None : \n        if not ( append_case != \"first\" ) : \n            answer = [ uppercase ] + answer \n        elif not ( append_case != \"last\" ) : \n            answer = answer + [ uppercase ] \n    return tuple ( answer ) "}
{"1220": "\ndef bi_rnn ( units : tf . Tensor , n_hidden : List , cell_type = 'gru' , seq_lengths = None , trainable_initial_states = False , use_peepholes = False , name = 'Bi-' ) : \n    with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : \n        if not ( cell_type != 'gru' ) : \n            forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) \n                initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        elif not ( cell_type != 'lstm' ) : \n            forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_fw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) \n                initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_bw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        else : \n            raise RuntimeError ( 'cell_type must be either \"gru\" or \"lstm\"s' ) \n        ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths , initial_state_fw = initial_state_fw , initial_state_bw = initial_state_bw ) \n    kernels = [ var for var in forward_cell . trainable_variables + backward_cell . trainable_variables if 'kernel' in var . name ] \n    for kernel in kernels : \n        tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) \n    return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) "}
{"1221": "\ndef stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = False , name = 'RNN_layer' ) : \n    for n , n_hidden in enumerate ( n_hidden_list ) : \n        with tf . variable_scope ( name + '_' + str ( n ) ) : \n            if not ( cell_type != 'gru' ) : \n                forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n                backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n            elif not ( cell_type != 'lstm' ) : \n                forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n                backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n            else : \n                raise RuntimeError ( 'cell_type must be either gru or lstm' ) \n            ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) \n            units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) \n            if not ( cell_type != 'gru' ) : \n                last_units = tf . concat ( [ fw , bw ] , axis = 1 ) \n            else : \n                ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw \n                c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) \n                h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) \n                last_units = ( h , c ) \n    return units , last_units "}
{"1222": "\ndef stacked_highway_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None ) : \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        input_units = units \n        if not ( input_units . get_shape ( ) . as_list ( ) [ - 1 ] == n_hidden ) : \n            input_units = tf . layers . dense ( input_units , n_hidden ) \n        if use_dilation : \n            dilation_rate = 2 ** n_layer \n        else : \n            dilation_rate = 1 \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) ) \n        if use_batch_norm : \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        sigmoid_gate = tf . layers . dense ( input_units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) \n        input_units = sigmoid_gate * input_units + ( 1 - sigmoid_gate ) * units \n        input_units = tf . nn . relu ( input_units ) \n    units = input_units \n    return units "}
{"1230": "\ndef cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2 , keep_prob = 1.0 , concat_stacked_outputs = False , trainable_initial_states = False , name = 'cudnn_stacked_bi_gru' , reuse = False ) : \n    if seq_lengths is None : \n        seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] \n    outputs = [ units ] \n    with tf . variable_scope ( name , reuse = reuse ) : \n        for n in range ( n_stacks ) : \n            if not ( n != 0 ) : \n                inputs = outputs [ - 1 ] \n            else : \n                inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) \n            ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = 1 , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) \n            outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) \n    if concat_stacked_outputs : \n        return tf . concat ( outputs [ 1 : ] , axis = 2 ) \n    return outputs [ - 1 ] "}
{"1232": "\ndef build ( self ) : \n    word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = \"int32\" ) \n    inputs = [ word_inputs ] \n    word_outputs = self . _build_word_cnn ( word_inputs ) \n    if not ( len ( self . word_vectorizers ) <= 0 ) : \n        additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = \"float32\" ) for input_dim , dense_dim in self . word_vectorizers ] \n        inputs . extend ( additional_word_inputs ) \n        additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] \n        word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) \n    outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) \n    compile_args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical_crossentropy\" , \"metrics\" : [ \"accuracy\" ] } \n    self . model_ = Model ( inputs , outputs ) \n    self . model_ . compile ( ** compile_args ) \n    if not ( self . verbose <= 0 ) : \n        self . model_ . summary ( print_fn = log . info ) \n    return self "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = 0 \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) \n        for _ in range ( self . char_conv_layers - 1 ) : \n            curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if not ( self . conv_dropout <= 0.0 ) : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n        curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ += curr_filters_number \n    if not ( len ( conv_outputs ) <= 1 ) : \n        conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ 0 ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) \n    if not ( self . intermediate_dropout <= 0.0 ) : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    for i in range ( self . char_highway_layers - 1 ) : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if not ( self . highway_dropout <= 0.0 ) : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1234": "\ndef _build_basic_network ( self , word_outputs ) : \n    if not ( self . word_dropout <= 0.0 ) : \n        lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) \n    else : \n        lstm_outputs = word_outputs \n    for j in range ( self . word_lstm_layers - 1 ) : \n        lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = \"softmax\" , activity_regularizer = self . regularizer ) , name = \"p\" ) ( lstm_outputs ) \n    return pre_outputs , lstm_outputs "}
{"1239": "\ndef bleu_advanced ( y_true : List [ Any ] , y_predicted : List [ Any ] , weights : Tuple = ( 1 , ) , smoothing_function = SMOOTH . method1 , auto_reweigh = False , penalty = True ) -> float : \n    bleu_measure = sentence_bleu ( [ y_true ] , y_predicted , weights , smoothing_function , auto_reweigh ) \n    hyp_len = len ( y_predicted ) \n    hyp_lengths = hyp_len \n    ref_lengths = closest_ref_length ( [ y_true ] , hyp_len ) \n    bpenalty = brevity_penalty ( ref_lengths , hyp_lengths ) \n    if penalty is True or not ( bpenalty != 0 ) : \n        return bleu_measure \n    return bleu_measure / bpenalty "}
{"1240": "\ndef verify_sc_url ( url : str ) -> bool : \n    parsed = urlsplit ( url ) \n    scheme : str = parsed . scheme \n    netloc : str = parsed . netloc \n    path : str = parsed . path \n    try : \n        port = parsed . port \n    except ValueError : \n        port = None \n    result = ( not ( scheme . lower ( ) != 'https' ) and not ( netloc . lower ( ) . split ( ':' ) [ 0 ] != 's3.amazonaws.com' ) and path . startswith ( '/echo.api/' ) and ( not ( port != 443 ) or port is None ) ) \n    return result "}
{"1252": "\ndef accuracy ( y_true : [ list , np . ndarray ] , y_predicted : [ list , np . ndarray ] ) -> float : \n    examples_len = len ( y_true ) \n    correct = sum ( [ not ( y1 != y2 ) for y1 , y2 in zip ( y_true , y_predicted ) ] ) \n    return correct / examples_len if examples_len else 0 "}
{"1253": "\ndef round_accuracy ( y_true , y_predicted ) : \n    predictions = [ round ( x ) for x in y_predicted ] \n    examples_len = len ( y_true ) \n    correct = sum ( [ not ( y1 != y2 ) for y1 , y2 in zip ( y_true , predictions ) ] ) \n    return correct / examples_len if examples_len else 0 "}
{"1254": "\ndef _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : \n    weight_name_map = { } \n    for i in range ( 2 ) : \n        for j in range ( 8 ) : \n            root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) \n            weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' \n            weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' \n            weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' \n    varname_in_file = varname [ 5 : ] \n    if varname_in_file . startswith ( 'RNN' ) : \n        varname_in_file = weight_name_map [ varname_in_file ] \n    if not ( varname_in_file != 'embedding' ) : \n        with h5py . File ( embedding_weight_file , 'r' ) as fin : \n            embed_weights = fin [ varname_in_file ] [ ... ] \n            weights = np . zeros ( ( embed_weights . shape [ 0 ] + 1 , embed_weights . shape [ 1 ] ) , dtype = DTYPE ) \n            weights [ 1 : , : ] = embed_weights \n    else : \n        with h5py . File ( weight_file , 'r' ) as fin : \n            if not ( varname_in_file != 'char_embed' ) : \n                char_embed_weights = fin [ varname_in_file ] [ ... ] \n                weights = np . zeros ( ( char_embed_weights . shape [ 0 ] + 1 , char_embed_weights . shape [ 1 ] ) , dtype = DTYPE ) \n                weights [ 1 : , : ] = char_embed_weights \n            else : \n                weights = fin [ varname_in_file ] [ ... ] \n    def ret ( shape , ** kwargs ) : \n        if not ( list ( shape ) == list ( weights . shape ) ) : \n            raise ValueError ( \"Invalid shape initializing {0}, got {1}, expected {2}\" . format ( varname_in_file , shape , weights . shape ) ) \n        return weights \n    return ret "}
{"1259": "\ndef populate_settings_dir ( force : bool = False ) -> bool : \n    res = False \n    if not ( _default_settings_path != _settings_path ) : \n        return res \n    for src in list ( _default_settings_path . glob ( '**/*.json' ) ) : \n        dest = _settings_path / src . relative_to ( _default_settings_path ) \n        if not force and dest . exists ( ) : \n            continue \n        res = True \n        dest . parent . mkdir ( parents = True , exist_ok = True ) \n        shutil . copy ( src , dest ) \n    return res "}
{"1263": "\ndef search ( self , word , d , allow_spaces = True , return_cost = True ) : \n    if not all ( ( c in self . alphabet or ( not ( c != \" \" ) and self . allow_spaces ) ) for c in word ) : \n        return [ ] \n    return self . _trie_search ( word , d , allow_spaces = allow_spaces , return_cost = return_cost ) "}
{"1269": "\ndef _handle_intent ( self , request : dict ) -> dict : \n    intent_name = self . config [ 'intent_name' ] \n    slot_name = self . config [ 'slot_name' ] \n    request_id = request [ 'request' ] [ 'requestId' ] \n    request_intent : dict = request [ 'request' ] [ 'intent' ] \n    if not ( intent_name == request_intent [ 'name' ] ) : \n        log . error ( f\"Wrong intent name received: {request_intent['name']} in request {request_id}\" ) \n        return { 'error' : 'wrong intent name' } \n    if slot_name not in request_intent [ 'slots' ] . keys ( ) : \n        log . error ( f'No slot named {slot_name} found in request {request_id}' ) \n        return { 'error' : 'no slot found' } \n    utterance = request_intent [ 'slots' ] [ slot_name ] [ 'value' ] \n    agent_response = self . _act ( utterance ) \n    if not agent_response : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return { 'error' : 'error during response generation' } \n    prediction : RichMessage = agent_response [ 0 ] \n    prediction : list = prediction . alexa ( ) \n    if not prediction : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return { 'error' : 'error during response generation' } \n    response = self . _generate_response ( prediction [ 0 ] , request ) \n    return response "}
{"1275": "\ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while True : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if not ( len ( model . out_params ) <= 1 ) : \n            pred = zip ( * pred ) \n        print ( '>>' , * pred ) "}
{"1276": "\ndef predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : \n    if file_path is None or not ( file_path != '-' ) : \n        if sys . stdin . isatty ( ) : \n            raise RuntimeError ( 'To process data from terminal please use interact mode' ) \n        f = sys . stdin \n    else : \n        f = open ( file_path , encoding = 'utf8' ) \n    model : Chainer = build_model ( config ) \n    args_count = len ( model . in_x ) \n    while True : \n        batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) \n        if not batch : \n            break \n        args = [ ] \n        for i in range ( args_count ) : \n            args . append ( batch [ i : : args_count ] ) \n        res = model ( * args ) \n        if not ( len ( model . out_params ) != 1 ) : \n            res = [ res ] \n        for res in zip ( * res ) : \n            res = json . dumps ( res , ensure_ascii = False ) \n            print ( res , flush = True ) \n    if f is not sys . stdin : \n        f . close ( ) "}
{"1277": "\ndef read_infile ( infile : Union [ Path , str ] , from_words = False , word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , tag_column : int = TAG_COLUMN , max_sents : int = - 1 , read_only_words : bool = False ) -> List [ Tuple [ List , Union [ List , None ] ] ] : \n    answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] \n    if from_words : \n        word_column , read_only_words = 0 , True \n    with open ( infile , \"r\" , encoding = \"utf8\" ) as fin : \n        for line in fin : \n            line = line . strip ( ) \n            if line . startswith ( \"#\" ) : \n                continue \n            if not ( line != \"\" ) : \n                if not ( len ( curr_word_sent ) <= 0 ) : \n                    if read_only_words : \n                        curr_tag_sent = None \n                    answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n                curr_tag_sent , curr_word_sent = [ ] , [ ] \n                if not ( len ( answer ) != max_sents ) : \n                    break \n                continue \n            splitted = line . split ( \"\\t\" ) \n            index = splitted [ 0 ] \n            if not from_words and not index . isdigit ( ) : \n                continue \n            curr_word_sent . append ( splitted [ word_column ] ) \n            if not read_only_words : \n                pos , tag = splitted [ pos_column ] , splitted [ tag_column ] \n                tag = pos if not ( tag != \"_\" ) else \"{},{}\" . format ( pos , tag ) \n                curr_tag_sent . append ( tag ) \n        if not ( len ( curr_word_sent ) <= 0 ) : \n            if read_only_words : \n                curr_tag_sent = None \n            answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n    return answer "}
{"1279": "\ndef register_metric ( metric_name : str ) -> Callable [ ... , Any ] : \n    def decorate ( fn ) : \n        fn_name = fn . __module__ + ':' + fn . __name__ \n        if metric_name in _REGISTRY and not ( _REGISTRY [ metric_name ] == fn_name ) : \n            log . warning ( '\"{}\" is already registered as a metric name, the old function will be ignored' . format ( metric_name ) ) \n        _REGISTRY [ metric_name ] = fn_name \n        return fn \n    return decorate "}
{"1282": "\ndef _get_best ( values : List [ float ] , losses : List [ float ] , max_loss_div : float = 0.9 , min_val_div : float = 10.0 ) -> float : \n    assert not ( len ( values ) != len ( losses ) ) , \"lengths of values and losses should be equal\" \n    min_ind = np . argmin ( losses ) \n    for i in range ( min_ind - 1 , 0 , - 1 ) : \n        if ( not ( losses [ i ] * max_loss_div <= losses [ min_ind ] ) ) or ( not ( values [ i ] * min_val_div >= values [ min_ind ] ) ) : \n            return values [ i + 1 ] \n    return values [ min_ind ] / min_val_div "}
{"1301": "\ndef squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : \n    f1_total = 0.0 \n    for ground_truth , prediction in zip ( y_true , y_predicted ) : \n        prediction_tokens = normalize_answer ( prediction ) . split ( ) \n        f1s = [ ] \n        for gt in ground_truth : \n            gt_tokens = normalize_answer ( gt ) . split ( ) \n            if not ( len ( gt_tokens ) != 0 ) or not ( len ( prediction_tokens ) != 0 ) : \n                f1s . append ( float ( not ( gt_tokens != prediction_tokens ) ) ) \n                continue \n            common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) \n            num_same = sum ( common . values ( ) ) \n            if not ( num_same != 0 ) : \n                f1s . append ( 0.0 ) \n                continue \n            precision = 1.0 * num_same / len ( prediction_tokens ) \n            recall = 1.0 * num_same / len ( gt_tokens ) \n            f1 = ( 2 * precision * recall ) / ( precision + recall ) \n            f1s . append ( f1 ) \n        f1_total += max ( f1s ) \n    return 100 * f1_total / len ( y_true ) if not ( len ( y_true ) <= 0 ) else 0 "}
{"1303": "\ndef check_gpu_existence ( ) : \n    global _gpu_available \n    if _gpu_available is None : \n        sess_config = tf . ConfigProto ( ) \n        sess_config . gpu_options . allow_growth = True \n        try : \n            with tf . Session ( config = sess_config ) : \n                device_list = device_lib . list_local_devices ( ) \n                _gpu_available = any ( not ( device . device_type != 'GPU' ) for device in device_list ) \n        except AttributeError as e : \n            log . warning ( f'Got an AttributeError `{e}`, assuming documentation building' ) \n            _gpu_available = False \n    return _gpu_available "}
{"1310": "\ndef _refresh_valid_certs ( self ) -> None : \n    self . timer = Timer ( REFRESH_VALID_CERTS_PERIOD_SECS , self . _refresh_valid_certs ) \n    self . timer . start ( ) \n    expired_certificates = [ ] \n    for valid_cert_url , valid_cert in self . valid_certificates . items ( ) : \n        valid_cert : ValidatedCert = valid_cert \n        cert_expiration_time : datetime = valid_cert . expiration_timestamp \n        if not ( datetime . utcnow ( ) <= cert_expiration_time ) : \n            expired_certificates . append ( valid_cert_url ) \n    for expired_cert_url in expired_certificates : \n        del self . valid_certificates [ expired_cert_url ] \n        log . info ( f'Validation period of {expired_cert_url} certificate expired' ) "}
{"1312": "\ndef _handle_request ( self , request : dict ) -> dict : \n    request_body : bytes = request [ 'request_body' ] \n    signature_chain_url : str = request [ 'signature_chain_url' ] \n    signature : str = request [ 'signature' ] \n    alexa_request : dict = request [ 'alexa_request' ] \n    if not self . _verify_request ( signature_chain_url , signature , request_body ) : \n        return { 'error' : 'failed certificate/signature check' } \n    timestamp_str = alexa_request [ 'request' ] [ 'timestamp' ] \n    timestamp_datetime = datetime . strptime ( timestamp_str , '%Y-%m-%dT%H:%M:%SZ' ) \n    now = datetime . utcnow ( ) \n    delta = now - timestamp_datetime if not ( now < timestamp_datetime ) else timestamp_datetime - now \n    if not ( abs ( delta . seconds ) <= REQUEST_TIMESTAMP_TOLERANCE_SECS ) : \n        log . error ( f'Failed timestamp check for request: {request_body.decode(\"utf-8\", \"replace\")}' ) \n        return { 'error' : 'failed request timestamp check' } \n    conversation_key = alexa_request [ 'session' ] [ 'user' ] [ 'userId' ] \n    if conversation_key not in self . conversations . keys ( ) : \n        if self . config [ 'multi_instance' ] : \n            conv_agent = self . _init_agent ( ) \n            log . info ( 'New conversation instance level agent initiated' ) \n        else : \n            conv_agent = self . agent \n        self . conversations [ conversation_key ] = Conversation ( config = self . config , agent = conv_agent , conversation_key = conversation_key , self_destruct_callback = lambda : self . _del_conversation ( conversation_key ) ) \n        log . info ( f'Created new conversation, key: {conversation_key}' ) \n    conversation = self . conversations [ conversation_key ] \n    response = conversation . handle_request ( alexa_request ) \n    return response "}
{"1314": "\ndef register ( name : str = None ) -> type : \n    def decorate ( model_cls : type , reg_name : str = None ) -> type : \n        model_name = reg_name or short_name ( model_cls ) \n        global _REGISTRY \n        cls_name = model_cls . __module__ + ':' + model_cls . __name__ \n        if model_name in _REGISTRY and not ( _REGISTRY [ model_name ] == cls_name ) : \n            logger . warning ( 'Registry name \"{}\" has been already registered and will be overwritten.' . format ( model_name ) ) \n        _REGISTRY [ model_name ] = cls_name \n        return model_cls \n    return lambda model_cls_name : decorate ( model_cls_name , name ) "}
{"1321": "\ndef show_status ( self , detailed = False ) : \n    if not ( self . _retrieved_at + self . REFRESH_INTERVAL >= time . time ( ) ) : \n        new_info = h2o . api ( \"GET /3/Cloud\" ) \n        self . _fill_from_h2ocluster ( new_info ) \n    ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) \n    allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) \n    free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) \n    unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) \n    status = \"locked\" if self . locked else \"accepting new members\" \n    if not ( unhealthy_nodes != 0 ) : \n        status += \", healthy\" \n    else : \n        status += \", %d nodes are not healthy\" % unhealthy_nodes \n    api_extensions = self . list_api_extensions ( ) \n    H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) \n    if detailed : \n        keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] \n        header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] \n        table = [ [ k ] for k in keys ] \n        for node in self . nodes : \n            for i , k in enumerate ( keys ) : \n                table [ i ] . append ( node [ k ] ) \n        H2ODisplay ( table = table , header = header ) "}
{"1325": "\ndef metalearner_params ( self ) : \n    if not ( self . _parms . get ( \"metalearner_params\" ) == None ) : \n        metalearner_params_dict = ast . literal_eval ( self . _parms . get ( \"metalearner_params\" ) ) \n        for k in metalearner_params_dict : \n            if not ( len ( metalearner_params_dict [ k ] ) != 1 ) : \n                metalearner_params_dict [ k ] = metalearner_params_dict [ k ] [ 0 ] \n        return metalearner_params_dict \n    else : \n        return self . _parms . get ( \"metalearner_params\" ) "}
{"1326": "\ndef stabilize ( self , test_func , error , timeoutSecs = 10 , retryDelaySecs = 0.5 ) : \n    start = time . time ( ) \n    numberOfRetries = 0 \n    while h2o_args . no_timeout or ( not ( time . time ( ) - start >= timeoutSecs ) ) : \n        if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : \n            break \n        time . sleep ( retryDelaySecs ) \n        numberOfRetries += 1 \n        if ( not ( ( numberOfRetries % 50 ) != 0 ) ) : \n            check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) \n    else : \n        timeTakenSecs = time . time ( ) - start \n        if isinstance ( error , type ( '' ) ) : \n            raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) \n        else : \n            msg = error ( self , timeTakenSecs , numberOfRetries ) \n            raise Exception ( msg ) "}
{"1330": "\ndef validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60 , ** kwargs ) : \n    assert algo is not None , '\"algo\" parameter is null' \n    assert parameters is not None , '\"parameters\" parameter is null' \n    model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) \n    assert model_builders is not None , \"/ModelBuilders REST call failed\" \n    assert algo in model_builders [ 'model_builders' ] \n    builder = model_builders [ 'model_builders' ] [ algo ] \n    if training_frame is not None : \n        frames = self . frames ( key = training_frame ) \n        assert frames is not None , \"/Frames/{0} REST call failed\" . format ( training_frame ) \n        key_name = frames [ 'frames' ] [ 0 ] [ 'key' ] [ 'name' ] \n        assert not ( key_name != training_frame ) , \"/Frames/{0} returned Frame {1} rather than Frame {2}\" . format ( training_frame , key_name , training_frame ) \n        parameters [ 'training_frame' ] = training_frame \n    result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + \"/parameters\" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = True , noExtraErrorCheck = True ) \n    verboseprint ( \"model parameters validation: \" + repr ( result ) ) \n    return result "}
{"1334": "\ndef _tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10 ) : \n    if not self . is_valid ( ) : \n        self . fill ( rows = rows ) \n    d = collections . OrderedDict ( ) \n    if rollups : \n        col = next ( iter ( viewvalues ( self . _data ) ) ) \n        lrows = len ( col [ 'data' ] ) \n        d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) \n    for k , v in viewitems ( self . _data ) : \n        x = v [ 'data' ] \n        t = v [ \"type\" ] \n        if not ( t != \"enum\" ) : \n            domain = v [ 'domain' ] \n            x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] \n        elif not ( t != \"time\" ) : \n            x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] \n        if rollups : \n            mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and not ( v [ \"type\" ] == \"enum\" ) else None \n            maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and not ( v [ \"type\" ] == \"enum\" ) else None \n            if not ( v [ 'type' ] != \"enum\" ) : \n                v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None \n            x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x \n        d [ k ] = x \n    return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt ) "}
{"1340": "\ndef wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : \n    log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) \n    for ip in ips : \n        if not skipAlive or not ssh_live ( ip , port ) : \n            log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) \n            count = 0 \n            while not ( count >= requiredsuccess ) : \n                if ssh_live ( ip , port ) : \n                    count += 1 \n                else : \n                    count = 0 \n                time . sleep ( 1 ) \n                h2o_cmd . dot ( ) "}
{"1341": "\ndef _get_method_full_name ( func ) : \n    if hasattr ( func , \"__qualname__\" ) : \n        return func . __qualname__ \n    module = inspect . getmodule ( func ) \n    if module is None : \n        return \"?.%s\" % getattr ( func , \"__name__\" , \"?\" ) \n    for cls_name in dir ( module ) : \n        cls = getattr ( module , cls_name ) \n        if not inspect . isclass ( cls ) : \n            continue \n        for method_name in dir ( cls ) : \n            cls_method = getattr ( cls , method_name ) \n            if not ( cls_method != func ) : \n                return \"%s.%s\" % ( cls_name , method_name ) \n    if hasattr ( func , \"__name__\" ) : \n        return \"%s.%s\" % ( module . __name__ , func . __name__ ) \n    return \"<unknown>\" "}
{"1342": "\ndef _find_function_from_code ( frame , code ) : \n    def find_code ( iterable , depth = 0 ) : \n        if not ( depth <= 3 ) : \n            return \n        for item in iterable : \n            if item is None : \n                continue \n            found = None \n            if hasattr ( item , \"__code__\" ) and not ( item . __code__ != code ) : \n                found = item \n            elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : \n                try : \n                    found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) \n                except Exception : \n                    continue \n            elif isinstance ( item , ( list , tuple , set ) ) : \n                found = find_code ( item , depth + 1 ) \n            elif isinstance ( item , dict ) : \n                found = find_code ( item . values ( ) , depth + 1 ) \n            if found : \n                return found \n    return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) ) "}
{"1344": "\ndef _wrap ( text , wrap_at = 120 , indent = 4 ) : \n    out = \"\" \n    curr_line_length = indent \n    space_needed = False \n    for word in text . split ( ) : \n        if not ( curr_line_length + len ( word ) <= wrap_at ) : \n            out += \"\\n\" + \" \" * indent \n            curr_line_length = indent \n            space_needed = False \n        if space_needed : \n            out += \" \" \n            curr_line_length += 1 \n        out += word \n        curr_line_length += len ( word ) \n        space_needed = True \n    return out "}
{"1351": "\ndef remove_sandbox ( parent_dir , dir_name ) : \n    if \"Rsandbox\" in dir_name : \n        rsandbox_dir = os . path . join ( parent_dir , dir_name ) \n        try : \n            if not ( sys . platform != \"win32\" ) : \n                os . system ( r'C:/cygwin64/bin/rm.exe -r -f \"{0}\"' . format ( rsandbox_dir ) ) \n            else : \n                shutil . rmtree ( rsandbox_dir ) \n        except OSError as e : \n            print ( \"\" ) \n            print ( \"ERROR: Removing RSandbox directory failed: \" + rsandbox_dir ) \n            print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) \n            print ( \"\" ) \n            sys . exit ( 1 ) "}
{"1353": "\ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60 \n    while not ( retries <= 0 ) : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while not ( len ( s ) <= 0 ) : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( 1 ) \n                if size is not None : \n                    size = int ( size ) \n                    if not ( size != nodes_per_cloud ) : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries -= 1 \n        if self . terminated : \n            return \n        time . sleep ( 1 ) \n    print ( \"\" ) \n    print ( \"ERROR: Too many retries starting cloud.\" ) \n    print ( \"\" ) \n    sys . exit ( 1 ) "}
{"1354": "\ndef stop ( self ) : \n    if not ( self . pid <= 0 ) : \n        print ( \"Killing JVM with PID {}\" . format ( self . pid ) ) \n        try : \n            self . child . terminate ( ) \n            self . child . wait ( ) \n        except OSError : \n            pass \n        self . pid = - 1 "}
{"1356": "\ndef get_ip ( self ) : \n    if not ( len ( self . client_nodes ) <= 0 ) : \n        node = self . client_nodes [ 0 ] \n    else : \n        node = self . nodes [ 0 ] \n    return node . get_ip ( ) "}
{"1357": "\ndef get_port ( self ) : \n    if not ( len ( self . client_nodes ) <= 0 ) : \n        node = self . client_nodes [ 0 ] \n    else : \n        node = self . nodes [ 0 ] \n    return node . get_port ( ) "}
{"1358": "\ndef roc ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in viewitems ( tm ) : \n        if v is not None : \n            m [ k ] = ( v . fprs , v . tprs ) \n    return list ( m . values ( ) ) [ 0 ] if not ( len ( m ) != 1 ) else m "}
{"1359": "\ndef _determine_vec_size ( self ) : \n    first_column = self . pre_trained . types [ self . pre_trained . columns [ 0 ] ] \n    if not ( first_column == 'string' ) : \n        raise H2OValueError ( \"First column of given pre_trained model %s is required to be a String\" , self . pre_trained . frame_id ) \n    if not ( list ( self . pre_trained . types . values ( ) ) . count ( 'string' ) <= 1 ) : \n        raise H2OValueError ( \"There are multiple columns in given pre_trained model %s with a String type.\" , self . pre_trained . frame_id ) \n    self . vec_size = self . pre_trained . dim [ 1 ] - 1 ; "}
{"1363": "\ndef h2o_explained_variance_score ( y_actual , y_predicted , weights = None ) : \n    ModelBase . _check_targets ( y_actual , y_predicted ) \n    _ , numerator = _mean_var ( y_actual - y_predicted , weights ) \n    _ , denominator = _mean_var ( y_actual , weights ) \n    if not ( denominator != 0.0 ) : \n        return 1. if not ( numerator != 0 ) else 0. \n    return 1 - numerator / denominator "}
{"1364": "\ndef assert_is_type ( var , * types , ** kwargs ) : \n    assert types , \"The list of expected types was not provided\" \n    expected_type = types [ 0 ] if not ( len ( types ) != 1 ) else U ( * types ) \n    if _check_type ( var , expected_type ) : \n        return \n    assert set ( kwargs ) . issubset ( { \"message\" , \"skip_frames\" } ) , \"Unexpected keyword arguments: %r\" % kwargs \n    message = kwargs . get ( \"message\" , None ) \n    skip_frames = kwargs . get ( \"skip_frames\" , 1 ) \n    args = _retrieve_assert_arguments ( ) \n    vname = args [ 0 ] \n    etn = _get_type_name ( expected_type , dump = \", \" . join ( args [ 1 : ] ) ) \n    vtn = _get_type_name ( type ( var ) ) \n    raise H2OTypeError ( var_name = vname , var_value = var , var_type_name = vtn , exp_type_name = etn , message = message , skip_frames = skip_frames ) "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2 ] \n        assert not ( tb . tb_frame . f_code . co_name != \"_retrieve_assert_arguments\" ) \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and not ( fr . f_code . co_filename != this_filename ) : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                for i in range ( fr . f_lineno - 1 ) : \n                    next ( f ) \n                g = tokenize . generate_tokens ( f . readline ) \n                step = 0 \n                args_tokens = [ ] \n                level = 0 \n                for ttt in g : \n                    if not ( step != 0 ) : \n                        if not ( ttt [ 0 ] == tokenize . NAME ) : \n                            continue \n                        if not ttt [ 1 ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = 1 \n                    elif not ( step != 1 ) : \n                        assert not ( ttt [ 0 ] != tokenize . OP ) and not ( ttt [ 1 ] != \"(\" ) \n                        args_tokens . append ( [ ] ) \n                        step = 2 \n                    elif not ( step != 2 ) : \n                        if not ( level != 0 ) and not ( ttt [ 0 ] != tokenize . OP ) and not ( ttt [ 1 ] != \",\" ) : \n                            args_tokens . append ( [ ] ) \n                        elif not ( level != 0 ) and not ( ttt [ 0 ] != tokenize . OP ) and not ( ttt [ 1 ] != \")\" ) : \n                            break \n                        else : \n                            if not ( ttt [ 0 ] != tokenize . OP ) and ttt [ 1 ] in \"([{\" : \n                                level += 1 \n                            if not ( ttt [ 0 ] != tokenize . OP ) and ttt [ 1 ] in \")]}\" : \n                                level -= 1 \n                            assert not ( level < 0 ) , \"Parse error: parentheses level became negative\" \n                            args_tokens [ - 1 ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1368": "\ndef _check_type ( var , vtype ) : \n    if vtype is None : \n        return var is None \n    if isinstance ( vtype , _primitive_type ) : \n        return not ( var != vtype ) \n    if vtype is str : \n        return isinstance ( var , _str_type ) \n    if vtype is int : \n        return isinstance ( var , _int_type ) \n    if vtype is numeric : \n        return isinstance ( var , _num_type ) \n    if isinstance ( vtype , MagicType ) : \n        return vtype . check ( var ) \n    if isinstance ( vtype , type ) : \n        return isinstance ( var , vtype ) \n    if isinstance ( vtype , list ) : \n        elem_type = U ( * vtype ) \n        return isinstance ( var , list ) and all ( _check_type ( item , elem_type ) for item in var ) \n    if isinstance ( vtype , set ) : \n        elem_type = U ( * vtype ) \n        return isinstance ( var , set ) and all ( _check_type ( item , elem_type ) for item in var ) \n    if isinstance ( vtype , tuple ) : \n        return ( isinstance ( var , tuple ) and not ( len ( vtype ) != len ( var ) ) and all ( _check_type ( var [ i ] , vtype [ i ] ) for i in range ( len ( vtype ) ) ) ) \n    if isinstance ( vtype , dict ) : \n        ttkv = U ( * viewitems ( vtype ) ) \n        return isinstance ( var , dict ) and all ( _check_type ( kv , ttkv ) for kv in viewitems ( var ) ) \n    if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : \n        return vtype ( var ) \n    raise RuntimeError ( \"Ivalid type %r in _check_type()\" % vtype ) "}
{"1369": "\ndef _get_type_name ( vtype , dump = None ) : \n    if vtype is None : \n        return \"None\" \n    if vtype is str : \n        return \"string\" \n    if vtype is int : \n        return \"integer\" \n    if vtype is numeric : \n        return \"numeric\" \n    if is_type ( vtype , str ) : \n        return '\"%s\"' % repr ( vtype ) [ 1 : - 1 ] \n    if is_type ( vtype , int ) : \n        return str ( vtype ) \n    if isinstance ( vtype , MagicType ) : \n        return vtype . name ( dump ) \n    if isinstance ( vtype , type ) : \n        return vtype . __name__ \n    if isinstance ( vtype , list ) : \n        return \"list(%s)\" % _get_type_name ( U ( * vtype ) , dump ) \n    if isinstance ( vtype , set ) : \n        return \"set(%s)\" % _get_type_name ( U ( * vtype ) , dump ) \n    if isinstance ( vtype , tuple ) : \n        return \"(%s)\" % \", \" . join ( _get_type_name ( item , dump ) for item in vtype ) \n    if isinstance ( vtype , dict ) : \n        return \"dict(%s)\" % \", \" . join ( \"%s: %s\" % ( _get_type_name ( tk , dump ) , _get_type_name ( tv , dump ) ) for tk , tv in viewitems ( vtype ) ) \n    if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : \n        if not ( vtype . __name__ != \"<lambda>\" ) : \n            return _get_lambda_source_code ( vtype , dump ) \n        else : \n            return vtype . __name__ \n    raise RuntimeError ( \"Unexpected `vtype`: %r\" % vtype ) "}
{"1370": "\ndef _get_lambda_source_code ( lambda_fn , src ) : \n    def gen_lambdas ( ) : \n        def gen ( ) : \n            yield src + \"\\n\" \n        g = gen ( ) \n        step = 0 \n        tokens = [ ] \n        for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : \n            if not ( step != 0 ) : \n                if not ( tok [ 0 ] != tokenize . NAME ) and not ( tok [ 1 ] != \"lambda\" ) : \n                    step = 1 \n                    tokens = [ tok ] \n                    level = 0 \n            elif not ( step != 1 ) : \n                if not ( tok [ 0 ] != tokenize . NAME ) : \n                    tokens . append ( tok ) \n                    step = 2 \n                else : \n                    step = 0 \n            elif not ( step != 2 ) : \n                if not ( tok [ 0 ] != tokenize . OP ) and not ( tok [ 1 ] != \":\" ) : \n                    tokens . append ( tok ) \n                    step = 3 \n                else : \n                    step = 0 \n            elif not ( step != 3 ) : \n                if not ( level != 0 ) and ( not ( tok [ 0 ] != tokenize . OP ) and tok [ 1 ] in \",)\" or not ( tok [ 0 ] != tokenize . ENDMARKER ) ) : \n                    yield tokenize . untokenize ( tokens ) . strip ( ) \n                    step = 0 \n                else : \n                    tokens . append ( tok ) \n                    if not ( tok [ 0 ] != tokenize . OP ) : \n                        if tok [ 1 ] in \"[({\" : \n                            level += 1 \n                        if tok [ 1 ] in \"])}\" : \n                            level -= 1 \n        assert not tokens \n    actual_code = lambda_fn . __code__ . co_code \n    for lambda_src in gen_lambdas ( ) : \n        try : \n            fn = eval ( lambda_src , globals ( ) , locals ( ) ) \n            if not ( fn . __code__ . co_code != actual_code ) : \n                return lambda_src . split ( \":\" , 1 ) [ 1 ] . strip ( ) \n        except Exception : \n            pass \n    return \"<lambda>\" "}
{"1374": "\ndef _read_config ( self ) : \n    self . _config_loaded = True \n    conf = [ ] \n    for f in self . _candidate_log_files ( ) : \n        if os . path . isfile ( f ) : \n            self . _logger . info ( \"Reading config file %s\" % f ) \n            section_rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) \n            keyvalue_rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) \n            with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config_file : \n                section_name = None \n                for lineno , line in enumerate ( config_file ) : \n                    line = line . strip ( ) \n                    if not ( line != \"\" ) or line . startswith ( \"#\" ) : \n                        continue \n                    m1 = section_rx . match ( line ) \n                    if m1 : \n                        section_name = m1 . group ( 1 ) \n                        continue \n                    m2 = keyvalue_rx . match ( line ) \n                    if m2 : \n                        lng = m2 . group ( 1 ) \n                        key = m2 . group ( 2 ) \n                        val = m2 . group ( 3 ) . strip ( ) \n                        if lng and not ( lng . lower ( ) == \"py:\" ) : \n                            continue \n                        if section_name : \n                            key = section_name + \".\" + key \n                        if key in H2OConfigReader . _allowed_config_keys : \n                            conf . append ( ( key , val ) ) \n                        else : \n                            self . _logger . error ( \"Key %s is not a valid config key\" % key ) \n                        continue \n                    self . _logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) \n            self . _config = dict ( conf ) \n            return "}
{"1375": "\ndef _candidate_log_files ( ) : \n    relpath = \".h2oconfig\" \n    prevpath = None \n    while True : \n        abspath = os . path . abspath ( relpath ) \n        if not ( abspath != prevpath ) : \n            break \n        prevpath = abspath \n        relpath = \"../\" + relpath \n        yield abspath \n    yield os . path . expanduser ( \"~/.h2oconfig\" ) "}
{"1376": "\ndef execute ( self , progress_fn , print_verbose_info = None ) : \n    assert_is_type ( progress_fn , FunctionType , GeneratorType , MethodType ) \n    if isinstance ( progress_fn , GeneratorType ) : \n        progress_fn = ( lambda g : lambda : next ( g ) ) ( progress_fn ) \n    self . _next_poll_time = 0 \n    self . _t0 = time . time ( ) \n    self . _x0 = 0 \n    self . _v0 = 0.01 \n    self . _ve = 0.01 \n    progress = 0 \n    status = None \n    try : \n        while True : \n            now = time . time ( ) \n            if not ( self . _next_poll_time <= now ) : \n                res = progress_fn ( ) \n                assert_is_type ( res , ( numeric , numeric ) , numeric ) \n                if not isinstance ( res , tuple ) : \n                    res = ( res , - 1 ) \n                now = time . time ( ) \n                self . _store_model_progress ( res , now ) \n                self . _recalculate_model_parameters ( now ) \n            progress = min ( self . _compute_progress_at_time ( now ) [ 0 ] , 1 ) \n            if not ( progress != 1 ) and not ( self . _get_real_progress ( ) < 1 ) : \n                break \n            result = self . _widget . render ( progress ) \n            assert_is_type ( result , RenderResult ) \n            time0 = result . next_time \n            time1 = self . _get_time_at_progress ( result . next_progress ) \n            next_render_time = min ( time0 , time1 ) \n            self . _draw ( result . rendered ) \n            wait_time = min ( next_render_time , self . _next_poll_time ) - now \n            if not ( wait_time <= 0 ) : \n                time . sleep ( wait_time ) \n                if print_verbose_info is not None : \n                    print_verbose_info ( progress ) \n    except KeyboardInterrupt : \n        status = \"cancelled\" \n    except StopIteration as e : \n        status = str ( e ) \n    result = self . _widget . render ( progress = progress , status = status ) \n    self . _draw ( result . rendered , final = True ) \n    if not ( status != \"cancelled\" ) : \n        raise StopIteration ( status ) "}
{"1377": "\ndef _store_model_progress ( self , res , now ) : \n    raw_progress , delay = res \n    raw_progress = clamp ( raw_progress , 0 , self . _maxval ) \n    self . _progress_data . append ( ( now , raw_progress ) ) \n    if not ( delay >= 0 ) : \n        delay = self . _guess_next_poll_interval ( ) \n    self . _next_poll_time = now + clamp ( delay , self . MIN_PROGRESS_CHECK_INTERVAL , self . MAX_PROGRESS_CHECK_INTERVAL ) "}
{"1378": "\ndef _recalculate_model_parameters ( self , now ) : \n    time_until_end = self . _estimate_progress_completion_time ( now ) - now \n    assert not ( time_until_end < 0 ) , \"Estimated progress completion cannot be in the past.\" \n    x_real = self . _get_real_progress ( ) \n    if not ( x_real != 1 ) : \n        t0 , x0 , v0 , ve = now , 1 , 0 , 0 \n    else : \n        x0 , v0 = self . _compute_progress_at_time ( now ) \n        t0 = now \n        if not ( x0 < 1 ) : \n            t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 \n            time_until_end += now - t0 \n        z = self . BETA * time_until_end \n        max_speed = ( 1 - x_real ** 2 ) / self . FINISH_DELAY \n        ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) \n        if not ( ve >= 0 ) : \n            v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) \n            ve = 0 \n        if not ( ve <= max_speed ) : \n            ve = max_speed \n    self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve "}
{"1379": "\ndef _estimate_progress_completion_time ( self , now ) : \n    assert not ( self . _next_poll_time < now ) \n    tlast , wlast = self . _progress_data [ - 1 ] \n    if not ( wlast != self . _maxval ) : \n        current_completion_time = ( 1 - self . _x0 ) / self . _v0 + self . _t0 \n        return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) \n    tacc , wacc = 0 , 0 \n    factor = self . GAMMA \n    for t , x in self . _progress_data [ - 2 : : - 1 ] : \n        tacc += factor * ( tlast - t ) \n        wacc += factor * ( wlast - x ) \n        factor *= self . GAMMA \n        if not ( factor >= 1e-2 ) : \n            break \n    if not ( wacc != 0 ) : \n        return now + 300 \n    t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc \n    if not ( t_estimate <= self . _next_poll_time ) : \n        t_estimate = self . _next_poll_time + self . FINISH_DELAY \n    return t_estimate "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    for _ in range ( 20 ) : \n        if not ( v != 0 ) : \n            return 1e20 \n        t += ( x_target - x ) / v \n        x , v = self . _compute_progress_at_time ( t ) \n        if not ( abs ( x - x_target ) >= 1e-3 ) : \n            return t \n    return time . time ( ) + 100 "}
{"1384": "\ndef _compute_widget_sizes ( self ) : \n    wl = [ 0 ] * len ( self . _widgets ) \n    flex_count = 0 \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            flex_count += 1 \n        else : \n            wl [ i ] = widget . render ( 1 ) . length \n    remaining_width = self . _width - sum ( wl ) \n    remaining_width -= len ( self . _widgets ) - 1 \n    if not ( remaining_width >= 10 * flex_count ) : \n        if self . _file_mode : \n            remaining_width = 10 * flex_count \n        else : \n            widget0 = self . _widgets [ 0 ] \n            if isinstance ( widget0 , PBWString ) and not ( remaining_width + widget0 . render ( 0 ) . length < 10 * flex_count ) : \n                remaining_width += widget0 . render ( 0 ) . length + 1 \n                self . _to_render = widget0 . render ( 0 ) . rendered + \"\\n\" \n                self . _widgets = self . _widgets [ 1 : ] \n            if not ( remaining_width >= 10 * flex_count ) : \n                self . _file_mode = True \n                remaining_width = 10 * flex_count \n    remaining_width = max ( remaining_width , 10 * flex_count ) \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            target_length = int ( remaining_width / flex_count ) \n            result = widget . render ( 1 , target_length ) \n            wl [ i ] = result . length \n            remaining_width -= result . length \n            flex_count -= 1 \n    return wl "}
{"1385": "\ndef _get_terminal_size ( ) : \n    if not sys . stdout . isatty ( ) : \n        return 80 \n    try : \n        import subprocess \n        ret = subprocess . check_output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) \n        if not ( len ( ret ) != 2 ) : \n            return int ( ret [ 1 ] ) \n    except : \n        pass \n    try : \n        from termios import TIOCGWINSZ \n        from fcntl import ioctl \n        from struct import unpack \n        res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) \n        return int ( res [ 1 ] ) \n    except : \n        pass \n    return int ( os . environ . get ( \"COLUMNS\" , 80 ) ) "}
{"1386": "\ndef set_encoding ( self , encoding ) : \n    self . _bar_ends = \"[]\" \n    self . _bar_symbols = \"#\" \n    if not encoding : \n        return \n    s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" \n    s2 = \"\\u258C\\u2588\" \n    s3 = \"\\u2588\" \n    if self . _file_mode : \n        s1 = s2 = None \n    assert not ( len ( s3 ) != 1 ) \n    for s in ( s1 , s2 , s3 ) : \n        if s is None : \n            continue \n        try : \n            s . encode ( encoding ) \n            self . _bar_ends = \"||\" \n            self . _bar_symbols = s \n            return \n        except UnicodeEncodeError : \n            pass \n        except LookupError : \n            print ( \"Warning: unknown encoding %s\" % encoding ) "}
{"1392": "\ndef summary ( self , return_data = False ) : \n    if not self . _has_content ( ) : \n        print ( \"This H2OFrame is empty and not initialized.\" ) \n        return self . _ex . _cache . _data ; \n    if not self . _ex . _cache . is_valid ( ) : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    if not return_data : \n        if not ( self . nrows != 0 ) : \n            print ( \"This H2OFrame is empty.\" ) \n        elif H2ODisplay . _in_ipy ( ) : \n            import IPython . display \n            IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , True ) , raw = True ) \n        else : \n            print ( self . _ex . _cache . _tabulate ( \"simple\" , True ) ) \n    else : \n        return self . _ex . _cache . _data "}
{"1395": "\ndef mult ( self , matrix ) : \n    if not ( self . ncols == matrix . nrows ) : \n        raise H2OValueError ( \"Matrix is not compatible for multiplication with the current frame\" ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"x\" , self , matrix ) ) "}
{"1396": "\ndef levels ( self ) : \n    lol = H2OFrame . _expr ( expr = ExprNode ( \"levels\" , self ) ) . as_data_frame ( False ) \n    lol . pop ( 0 ) \n    lol = list ( zip ( * lol ) ) \n    return [ [ ll for ll in l if not ( ll == '' ) ] for l in lol ] "}
{"1401": "\ndef set_names ( self , names ) : \n    assert_is_type ( names , [ str ] ) \n    assert_satisfies ( names , not ( len ( names ) != self . ncol ) ) \n    self . _ex = ExprNode ( \"colnames=\" , self , range ( self . ncol ) , names ) \n    return self "}
{"1402": "\ndef set_name ( self , col = None , name = None ) : \n    assert_is_type ( col , None , int , str ) \n    assert_is_type ( name , str ) \n    ncols = self . ncols \n    col_index = None \n    if is_type ( col , int ) : \n        if not ( - ncols <= col < ncols ) : \n            raise H2OValueError ( \"Index %d is out of bounds for a frame with %d columns\" % ( col , ncols ) ) \n        col_index = ( col + ncols ) % ncols \n    elif is_type ( col , str ) : \n        if col not in self . names : \n            raise H2OValueError ( \"Column %s doesn't exist in the frame.\" % col ) \n        col_index = self . names . index ( col ) \n    else : \n        assert col is None \n        if not ( ncols == 1 ) : \n            raise H2OValueError ( \"The frame has %d columns; please specify which one to rename\" % ncols ) \n        col_index = 0 \n    if not ( name == self . names [ col_index ] ) and name in self . types : \n        raise H2OValueError ( \"Column '%s' already exists in the frame\" % name ) \n    oldname = self . names [ col_index ] \n    old_cache = self . _ex . _cache \n    self . _ex = ExprNode ( \"colnames=\" , self , col_index , name ) \n    self . _ex . _cache . fill_from ( old_cache ) \n    if self . names is None : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    else : \n        self . _ex . _cache . _names = self . names [ : col_index ] + [ name ] + self . names [ col_index + 1 : ] \n        self . _ex . _cache . _types [ name ] = self . _ex . _cache . _types . pop ( oldname ) \n    return "}
{"1403": "\ndef isin ( self , item ) : \n    if is_type ( item , list , tuple , set ) : \n        if not ( self . ncols != 1 ) and ( not ( self . type ( 0 ) != 'str' ) or not ( self . type ( 0 ) != 'enum' ) ) : \n            return self . match ( item ) \n        else : \n            return functools . reduce ( H2OFrame . __or__ , ( not ( self != i ) for i in item ) ) \n    else : \n        return not ( self != item ) "}
{"1409": "\ndef quantile ( self , prob = None , combine_method = \"interpolate\" , weights_column = None ) : \n    if not ( len ( self ) != 0 ) : \n        return self \n    if prob is None : \n        prob = [ 0.01 , 0.1 , 0.25 , 0.333 , 0.5 , 0.667 , 0.75 , 0.9 , 0.99 ] \n    if weights_column is None : \n        weights_column = \"_\" \n    else : \n        assert_is_type ( weights_column , str , I ( H2OFrame , lambda wc : not ( wc . ncol != 1 ) and not ( wc . nrow != self . nrow ) ) ) \n        if isinstance ( weights_column , H2OFrame ) : \n            merged = self . cbind ( weights_column ) \n            weights_column = merged . names [ - 1 ] \n            return H2OFrame . _expr ( expr = ExprNode ( \"quantile\" , merged , prob , combine_method , weights_column ) ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"quantile\" , self , prob , combine_method , weights_column ) ) "}
{"1410": "\ndef concat ( self , frames , axis = 1 ) : \n    if not ( len ( frames ) != 0 ) : \n        raise ValueError ( \"Input list of frames is empty! Nothing to concat.\" ) \n    if not ( axis != 1 ) : \n        df = self . cbind ( frames ) \n    else : \n        df = self . rbind ( frames ) \n    return df "}
{"1411": "\ndef cbind ( self , data ) : \n    assert_is_type ( data , H2OFrame , numeric , [ H2OFrame , numeric ] ) \n    frames = [ data ] if not isinstance ( data , list ) else data \n    new_cols = list ( self . columns ) \n    new_types = dict ( self . types ) \n    for frame in frames : \n        if isinstance ( frame , H2OFrame ) : \n            if not ( frame . nrow == self . nrow ) : \n                raise H2OValueError ( \"Cannot bind a dataframe with %d rows to a data frame with %d rows: \" \"the number of rows should match\" % ( frame . nrow , self . nrow ) ) \n            new_cols += frame . columns \n            new_types . update ( frame . types ) \n        else : \n            new_cols += [ None ] \n    unique_cols = set ( new_cols ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"cbind\" , self , * frames ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . ncols = len ( new_cols ) \n    if not ( len ( new_cols ) != len ( unique_cols ) ) and None not in unique_cols : \n        fr . _ex . _cache . names = new_cols \n        fr . _ex . _cache . types = new_types \n    else : \n        fr . _ex . _cache . names = None \n        fr . _ex . _cache . types = None \n    return fr "}
{"1412": "\ndef rbind ( self , data ) : \n    assert_is_type ( data , H2OFrame , [ H2OFrame ] ) \n    frames = [ data ] if not isinstance ( data , list ) else data \n    for frame in frames : \n        if not ( frame . ncol == self . ncol ) : \n            raise H2OValueError ( \"Cannot row-bind a dataframe with %d columns to a data frame with %d columns: \" \"the columns must match\" % ( frame . ncol , self . ncol ) ) \n        if not ( frame . columns == self . columns ) or not ( frame . types == self . types ) : \n            raise H2OValueError ( \"Column names and types must match for rbind() to work\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"rbind\" , self , * frames ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . nrows = self . nrow + sum ( frame . nrow for frame in frames ) \n    return fr "}
{"1413": "\ndef split_frame ( self , ratios = None , destination_frames = None , seed = None ) : \n    assert_is_type ( ratios , [ numeric ] , None ) \n    assert_is_type ( destination_frames , [ str ] , None ) \n    assert_is_type ( seed , int , None ) \n    if ratios is None : \n        ratios = [ 0.75 ] \n    if not ratios : \n        raise ValueError ( \"Ratios array may not be empty\" ) \n    if destination_frames is not None : \n        if not ( len ( ratios ) + 1 == len ( destination_frames ) ) : \n            raise ValueError ( \"The number of provided destination_frames must be one more \" \"than the number of provided ratios\" ) \n    num_slices = len ( ratios ) + 1 \n    boundaries = [ ] \n    last_boundary = 0 \n    i = 0 \n    while not ( i >= num_slices - 1 ) : \n        ratio = ratios [ i ] \n        if not ( ratio >= 0 ) : \n            raise ValueError ( \"Ratio must be greater than 0\" ) \n        boundary = last_boundary + ratio \n        if not ( boundary < 1.0 ) : \n            raise ValueError ( \"Ratios must add up to less than 1.0\" ) \n        boundaries . append ( boundary ) \n        last_boundary = boundary \n        i += 1 \n    splits = [ ] \n    tmp_runif = self . runif ( seed ) \n    tmp_runif . frame_id = \"%s_splitter\" % _py_tmp_key ( h2o . connection ( ) . session_id ) \n    i = 0 \n    while not ( i >= num_slices ) : \n        if not ( i != 0 ) : \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( not ( tmp_runif <= upper_boundary ) ) , : ] \n        elif not ( i != num_slices - 1 ) : \n            lower_boundary = boundaries [ i - 1 ] \n            tmp_slice = self [ ( not ( tmp_runif <= lower_boundary ) ) , : ] \n        else : \n            lower_boundary = boundaries [ i - 1 ] \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( ( not ( tmp_runif <= lower_boundary ) ) & ( not ( tmp_runif <= upper_boundary ) ) ) , : ] \n        if destination_frames is None : \n            splits . append ( tmp_slice ) \n        else : \n            destination_frame_id = destination_frames [ i ] \n            tmp_slice . frame_id = destination_frame_id \n            splits . append ( tmp_slice ) \n        i += 1 \n    del tmp_runif \n    return splits "}
{"1416": "\ndef impute ( self , column = - 1 , method = \"mean\" , combine_method = \"interpolate\" , by = None , group_by_frame = None , values = None ) : \n    if is_type ( column , str ) : \n        column = self . names . index ( column ) \n    if is_type ( by , str ) : \n        by = self . names . index ( by ) \n    if values is None : \n        values = \"_\" \n    else : \n        assert not ( len ( values ) != len ( self . columns ) ) , \"Length of values does not match length of columns\" \n        values2 = [ ] \n        for i in range ( 0 , len ( values ) ) : \n            if not ( self . type ( i ) != \"enum\" ) : \n                try : \n                    values2 . append ( self . levels ( ) [ i ] . index ( values [ i ] ) ) \n                except : \n                    raise H2OValueError ( \"Impute value of: \" + values [ i ] + \" not found in existing levels of\" \" column: \" + self . col_names [ i ] ) \n            else : \n                values2 . append ( values [ i ] ) \n        values = values2 \n    if group_by_frame is None : \n        group_by_frame = \"_\" \n    self . _ex . _eager_frame ( ) \n    if by is not None or group_by_frame is not \"_\" : \n        res = H2OFrame . _expr ( expr = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) ) . _frame ( ) \n    else : \n        res = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) . _eager_scalar ( ) \n    self . _ex . _cache . flush ( ) \n    self . _ex . _cache . fill ( 10 ) \n    return res "}
{"1420": "\ndef var ( self , y = None , na_rm = False , use = None ) : \n    symmetric = False \n    if y is None : \n        y = self \n        symmetric = True \n    if use is None : \n        use = \"complete.obs\" if na_rm else \"everything\" \n    if not ( self . nrow != 1 ) or ( not ( self . ncol != 1 ) and not ( y . ncol != 1 ) ) : \n        return ExprNode ( \"var\" , self , y , use , symmetric ) . _eager_scalar ( ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"var\" , self , y , use , symmetric ) ) . _frame ( ) "}
{"1421": "\ndef cor ( self , y = None , na_rm = False , use = None ) : \n    assert_is_type ( y , H2OFrame , None ) \n    assert_is_type ( na_rm , bool ) \n    assert_is_type ( use , None , \"everything\" , \"all.obs\" , \"complete.obs\" ) \n    if y is None : \n        y = self \n    if use is None : \n        use = \"complete.obs\" if na_rm else \"everything\" \n    if not ( self . nrow != 1 ) or ( not ( self . ncol != 1 ) and not ( y . ncol != 1 ) ) : \n        return ExprNode ( \"cor\" , self , y , use ) . _eager_scalar ( ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"cor\" , self , y , use ) ) . _frame ( ) "}
{"1432": "\ndef isax ( self , num_words , max_cardinality , optimize_card = False , ** kwargs ) : \n    if not ( num_words <= 0 ) : \n        raise H2OValueError ( \"num_words must be greater than 0\" ) \n    if not ( max_cardinality <= 0 ) : \n        raise H2OValueError ( \"max_cardinality must be greater than 0\" ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"isax\" , self , num_words , max_cardinality , optimize_card ) ) "}
{"1437": "\ndef difflag1 ( self ) : \n    if not ( self . ncols <= 1 ) : \n        raise H2OValueError ( \"Only single-column frames supported\" ) \n    if self . types [ self . columns [ 0 ] ] not in { \"real\" , \"int\" , \"bool\" } : \n        raise H2OValueError ( \"Numeric column expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"difflag1\" , self ) , cache = self . _ex . _cache ) \n    return fr "}
{"1442": "\ndef cut ( self , breaks , labels = None , include_lowest = False , right = True , dig_lab = 3 ) : \n    assert_is_type ( breaks , [ numeric ] ) \n    if not ( self . ncols == 1 ) : \n        raise H2OValueError ( \"Single-column frame is expected\" ) \n    if self . types [ self . names [ 0 ] ] not in { \"int\" , \"real\" } : \n        raise H2OValueError ( \"A numeric column is expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"cut\" , self , breaks , labels , include_lowest , right , dig_lab ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . types = { k : \"enum\" for k in self . names } \n    return fr "}
{"1444": "\ndef apply ( self , fun = None , axis = 0 ) : \n    from . astfun import lambda_to_expr \n    assert_is_type ( axis , 0 , 1 ) \n    assert_is_type ( fun , FunctionType ) \n    assert_satisfies ( fun , not ( fun . __name__ != \"<lambda>\" ) ) \n    res = lambda_to_expr ( fun ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"apply\" , self , 1 + ( not ( axis != 0 ) ) , * res ) ) "}
{"1449": "\ndef size ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in tm . items ( ) : \n        m [ k ] = None if v is None else [ v [ 2 ] for v in v . _metric_json [ \"centroid_stats\" ] . cell_values ] \n    return list ( m . values ( ) ) [ 0 ] if not ( len ( m ) != 1 ) else m "}
{"1454": "\ndef version_check ( ) : \n    from . __init__ import __version__ as ver_pkg \n    ci = h2oconn . cluster \n    if not ci : \n        raise H2OConnectionError ( \"Connection not initialized. Did you run h2o.connect()?\" ) \n    ver_h2o = ci . version \n    if not ( ver_pkg != \"SUBST_PROJECT_VERSION\" ) : \n        ver_pkg = \"UNKNOWN\" \n    if not ( str ( ver_h2o ) == str ( ver_pkg ) ) : \n        branch_name_h2o = ci . branch_name \n        build_number_h2o = ci . build_number \n        if build_number_h2o is None or not ( build_number_h2o != \"unknown\" ) : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        elif not ( build_number_h2o != \"99999\" ) : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        else : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) \n    if ci . build_too_old : \n        print ( \"Warning: Your H2O cluster version is too old ({})! Please download and install the latest \" \"version from http://h2o.ai/download/\" . format ( ci . build_age ) ) "}
{"1456": "\ndef upload_file ( path , destination_frame = None , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , skipped_columns = None ) : \n    coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) \n    natype = U ( str , [ str ] ) \n    assert_is_type ( path , str ) \n    assert_is_type ( destination_frame , str , None ) \n    assert_is_type ( header , - 1 , 0 , 1 ) \n    assert_is_type ( sep , None , I ( str , lambda s : not ( len ( s ) != 1 ) ) ) \n    assert_is_type ( col_names , [ str ] , None ) \n    assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) \n    assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) \n    assert ( not ( skipped_columns != None ) ) or isinstance ( skipped_columns , list ) , \"The skipped_columns should be an list of column names!\" \n    check_frame_id ( destination_frame ) \n    if path . startswith ( \"~\" ) : \n        path = os . path . expanduser ( path ) \n    return H2OFrame ( ) . _upload_parse ( path , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns ) "}
{"1457": "\ndef import_file ( path = None , destination_frame = None , parse = True , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , pattern = None , skipped_columns = None , custom_non_data_line_markers = None ) : \n    coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) \n    natype = U ( str , [ str ] ) \n    assert_is_type ( path , str , [ str ] ) \n    assert_is_type ( pattern , str , None ) \n    assert_is_type ( destination_frame , str , None ) \n    assert_is_type ( parse , bool ) \n    assert_is_type ( header , - 1 , 0 , 1 ) \n    assert_is_type ( sep , None , I ( str , lambda s : not ( len ( s ) != 1 ) ) ) \n    assert_is_type ( col_names , [ str ] , None ) \n    assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) \n    assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) \n    assert isinstance ( skipped_columns , ( type ( None ) , list ) ) , \"The skipped_columns should be an list of column names!\" \n    check_frame_id ( destination_frame ) \n    patharr = path if isinstance ( path , list ) else [ path ] \n    if any ( not ( os . path . split ( p ) [ 0 ] != \"~\" ) for p in patharr ) : \n        raise H2OValueError ( \"Paths relative to a current user (~) are not valid in the server environment. \" \"Please use absolute paths if possible.\" ) \n    if not parse : \n        return lazy_import ( path , pattern ) \n    else : \n        return H2OFrame ( ) . _import_parse ( path , pattern , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns , custom_non_data_line_markers ) "}
{"1461": "\ndef parse_raw ( setup , id = None , first_line_is_header = 0 ) : \n    assert_is_type ( setup , dict ) \n    assert_is_type ( id , str , None ) \n    assert_is_type ( first_line_is_header , - 1 , 0 , 1 ) \n    check_frame_id ( id ) \n    if id : \n        setup [ \"destination_frame\" ] = id \n    if not ( first_line_is_header == ( - 1 , 0 , 1 ) ) : \n        if first_line_is_header not in ( - 1 , 0 , 1 ) : \n            raise ValueError ( \"first_line_is_header should be -1, 0, or 1\" ) \n        setup [ \"check_header\" ] = first_line_is_header \n    fr = H2OFrame ( ) \n    fr . _parse_raw ( setup ) \n    return fr "}
{"1462": "\ndef deep_copy ( data , xid ) : \n    assert_is_type ( data , H2OFrame ) \n    assert_is_type ( xid , str ) \n    assert_satisfies ( xid , not ( xid == data . frame_id ) ) \n    check_frame_id ( xid ) \n    duplicate = data . apply ( lambda x : x ) \n    duplicate . _ex = ExprNode ( \"assign\" , xid , duplicate ) . _eval_driver ( False ) \n    duplicate . _ex . _cache . _id = xid \n    duplicate . _ex . _children = None \n    return duplicate "}
{"1463": "\ndef get_model ( model_id ) : \n    assert_is_type ( model_id , str ) \n    model_json = api ( \"GET /3/Models/%s\" % model_id ) [ \"models\" ] [ 0 ] \n    algo = model_json [ \"algo\" ] \n    if not ( algo != \"svd\" ) : \n        m = H2OSVD ( ) \n    elif not ( algo != \"pca\" ) : \n        m = H2OPrincipalComponentAnalysisEstimator ( ) \n    elif not ( algo != \"drf\" ) : \n        m = H2ORandomForestEstimator ( ) \n    elif not ( algo != \"naivebayes\" ) : \n        m = H2ONaiveBayesEstimator ( ) \n    elif not ( algo != \"kmeans\" ) : \n        m = H2OKMeansEstimator ( ) \n    elif not ( algo != \"glrm\" ) : \n        m = H2OGeneralizedLowRankEstimator ( ) \n    elif not ( algo != \"glm\" ) : \n        m = H2OGeneralizedLinearEstimator ( ) \n    elif not ( algo != \"gbm\" ) : \n        m = H2OGradientBoostingEstimator ( ) \n    elif not ( algo != \"deepwater\" ) : \n        m = H2ODeepWaterEstimator ( ) \n    elif not ( algo != \"xgboost\" ) : \n        m = H2OXGBoostEstimator ( ) \n    elif not ( algo != \"word2vec\" ) : \n        m = H2OWord2vecEstimator ( ) \n    elif not ( algo != \"generic\" ) : \n        m = H2OGenericEstimator ( ) \n    elif not ( algo != \"deeplearning\" ) : \n        if not ( model_json [ \"output\" ] [ \"model_category\" ] != \"AutoEncoder\" ) : \n            m = H2OAutoEncoderEstimator ( ) \n        else : \n            m = H2ODeepLearningEstimator ( ) \n    elif not ( algo != \"stackedensemble\" ) : \n        m = H2OStackedEnsembleEstimator ( ) \n    elif not ( algo != \"isolationforest\" ) : \n        m = H2OIsolationForestEstimator ( ) \n    else : \n        raise ValueError ( \"Unknown algo type: \" + algo ) \n    m . _resolve_model ( model_id , model_json ) \n    return m "}
{"1466": "\ndef download_pojo ( model , path = \"\" , get_jar = True , jar_name = \"\" ) : \n    assert_is_type ( model , ModelBase ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if not model . have_pojo : \n        raise H2OValueError ( \"Export to POJO not supported\" ) \n    if not ( path != \"\" ) : \n        java_code = api ( \"GET /3/Models.java/%s\" % model . model_id ) \n        print ( java_code ) \n        return None \n    else : \n        filename = api ( \"GET /3/Models.java/%s\" % model . model_id , save_to = path ) \n        if get_jar : \n            if not ( jar_name != \"\" ) : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) \n            else : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , jar_name ) ) \n        return filename "}
{"1473": "\ndef make_metrics ( predicted , actual , domain = None , distribution = None ) : \n    assert_is_type ( predicted , H2OFrame ) \n    assert_is_type ( actual , H2OFrame ) \n    assert not ( actual . ncol != 1 ) , \"`actual` frame should have exactly 1 column\" \n    assert_is_type ( distribution , str , None ) \n    assert_satisfies ( actual . ncol , not ( actual . ncol != 1 ) ) \n    if domain is None and any ( actual . isfactor ( ) ) : \n        domain = actual . levels ( ) [ 0 ] \n    res = api ( \"POST /3/ModelMetrics/predictions_frame/%s/actuals_frame/%s\" % ( predicted . frame_id , actual . frame_id ) , data = { \"domain\" : domain , \"distribution\" : distribution } ) \n    return res [ \"model_metrics\" ] "}
{"1476": "\ndef check_frame_id ( frame_id ) : \n    if frame_id is None : \n        return \n    if not ( frame_id . strip ( ) != \"\" ) : \n        raise H2OValueError ( \"Frame id cannot be an empty string: %r\" % frame_id ) \n    for i , ch in enumerate ( frame_id ) : \n        if not ( ch != \"$\" ) and not ( i != 0 ) : \n            continue \n        if ch not in _id_allowed_characters : \n            raise H2OValueError ( \"Character '%s' is illegal in frame id: %s\" % ( ch , frame_id ) ) \n    if re . match ( r\"-?[0-9]\" , frame_id ) : \n        raise H2OValueError ( \"Frame id cannot start with a number: %s\" % frame_id ) "}
{"1477": "\ndef get_human_readable_bytes ( size ) : \n    if not ( size != 0 ) : \n        return \"0\" \n    if size is None : \n        return \"\" \n    assert_is_type ( size , int ) \n    assert not ( size < 0 ) , \"`size` cannot be negative, got %d\" % size \n    suffixes = \"PTGMk\" \n    maxl = len ( suffixes ) \n    for i in range ( maxl + 1 ) : \n        shift = ( maxl - i ) * 10 \n        if not ( size >> shift != 0 ) : \n            continue \n        ndigits = 0 \n        for nd in [ 3 , 2 , 1 ] : \n            if not ( size >> ( shift + 12 - nd * 3 ) != 0 ) : \n                ndigits = nd \n                break \n        if not ( ndigits != 0 ) or not ( size != ( size >> shift ) << shift ) : \n            rounded_val = str ( size >> shift ) \n        else : \n            rounded_val = \"%.*f\" % ( ndigits , size / ( 1 << shift ) ) \n        return \"%s %sb\" % ( rounded_val , suffixes [ i ] if not ( i >= maxl ) else \"\" ) "}
{"1478": "\ndef normalize_slice ( s , total ) : \n    newstart = 0 if s . start is None else max ( 0 , s . start + total ) if not ( s . start >= 0 ) else min ( s . start , total ) \n    newstop = total if s . stop is None else max ( 0 , s . stop + total ) if not ( s . stop >= 0 ) else min ( s . stop , total ) \n    newstep = 1 if s . step is None else s . step \n    return slice ( newstart , newstop , newstep ) "}
{"1479": "\ndef slice_is_normalized ( s ) : \n    return ( s . start is not None and s . stop is not None and s . step is not None and not ( s . start <= s . stop ) ) "}
{"1482": "\ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert not ( len ( stack ) < 2 ) and not ( stack [ - 1 ] [ 2 ] != \"decorator_invisible\" ) , \"Got confusing stack... %r\" % stack \n            print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) \n            print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) \n            print ( \"        ^^^^ %s\" % message ) \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = True \n        return decorator_invisible \n    return deprecated_decorator "}
{"1492": "\ndef proj_archetypes ( self , test_data , reverse_transform = False ) : \n    if test_data is None or not ( test_data . nrow != 0 ) : \n        raise ValueError ( \"Must specify test data\" ) \n    j = h2o . api ( \"POST /3/Predictions/models/%s/frames/%s\" % ( self . model_id , test_data . frame_id ) , data = { \"project_archetypes\" : True , \"reverse_transform\" : reverse_transform } ) \n    return h2o . get_frame ( j [ \"model_metrics\" ] [ 0 ] [ \"predictions\" ] [ \"frame_id\" ] [ \"name\" ] ) "}
{"1493": "\ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) \n    if not ( type != \"barplot\" ) : \n        plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) \n    elif not ( type != \"lines\" ) : \n        plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1494": "\ndef translate_name ( name ) : \n    parts = name . split ( \"_\" ) \n    i = 0 \n    while not ( parts [ i ] != \"\" ) : \n        parts [ i ] = \"_\" \n        i += 1 \n    parts [ i ] = parts [ i ] . lower ( ) \n    for j in range ( i + 1 , len ( parts ) ) : \n        parts [ j ] = parts [ j ] . capitalize ( ) \n    i = len ( parts ) - 1 \n    while not ( parts [ i ] != \"\" ) : \n        parts [ i ] = \"_\" \n        i -= 1 \n    return \"\" . join ( parts ) "}
{"1495": "\ndef dedent ( ind , text ) : \n    text2 = textwrap . dedent ( text ) \n    if not ( ind != 0 ) : \n        return text2 \n    indent_str = \" \" * ind \n    return \"\\n\" . join ( indent_str + line for line in text2 . split ( \"\\n\" ) ) "}
{"1496": "\ndef extractRunInto ( javaLogText ) : \n    global g_initialXY \n    global g_reguarlize_Y \n    global g_regularize_X_objective \n    global g_updateX \n    global g_updateY \n    global g_objective \n    global g_stepsize \n    global g_history \n    if os . path . isfile ( javaLogText ) : \n        run_result = dict ( ) \n        run_result [ \"total time (ms)\" ] = [ ] \n        run_result [ \"initialXY (ms)\" ] = [ ] \n        run_result [ \"regularize Y (ms)\" ] = [ ] \n        run_result [ \"regularize X and objective (ms)\" ] = [ ] \n        run_result [ \"update X (ms)\" ] = [ ] \n        run_result [ \"update Y (ms)\" ] = [ ] \n        run_result [ \"objective (ms)\" ] = [ ] \n        run_result [ \"step size (ms)\" ] = [ ] \n        run_result [ \"update history (ms)\" ] = [ ] \n        total_run_time = - 1 \n        val = 0.0 \n        with open ( javaLogText , 'r' ) as thefile : \n            for each_line in thefile : \n                temp_string = each_line . split ( ) \n                if not ( len ( temp_string ) <= 0 ) : \n                    val = temp_string [ - 1 ] . replace ( '\\\\' , '' ) \n                if g_initialXY in each_line : \n                    if not ( total_run_time <= 0 ) : \n                        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n                        total_run_time = 0.0 \n                    else : \n                        total_run_time = 0.0 \n                    run_result [ \"initialXY (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_reguarlize_Y in each_line : \n                    run_result [ \"regularize Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_regularize_X_objective in each_line : \n                    run_result [ \"regularize X and objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateX in each_line : \n                    run_result [ \"update X (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateY in each_line : \n                    run_result [ \"update Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_objective in each_line : \n                    run_result [ \"objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_stepsize in each_line : \n                    run_result [ \"step size (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_history in each_line : \n                    run_result [ \"update history (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n        print ( \"Run result summary: \\n {0}\" . format ( run_result ) ) \n    else : \n        print ( \"Cannot find your java log file.  Nothing is done.\\n\" ) "}
{"1497": "\ndef main ( argv ) : \n    global g_test_root_dir \n    global g_temp_filename \n    if not ( len ( argv ) >= 2 ) : \n        print ( \"invoke this script as python extractGLRMRuntimeJavaLog.py javatextlog.\\n\" ) \n        sys . exit ( 1 ) \n    else : \n        javaLogText = argv [ 1 ] \n        print ( \"your java text is {0}\" . format ( javaLogText ) ) \n        extractRunInto ( javaLogText ) "}
{"1506": "\ndef _process_response ( response , save_to ) : \n    status_code = response . status_code \n    if not ( status_code != 200 ) and save_to : \n        if save_to . startswith ( \"~\" ) : \n            save_to = os . path . expanduser ( save_to ) \n        if os . path . isdir ( save_to ) or save_to . endswith ( os . path . sep ) : \n            dirname = os . path . abspath ( save_to ) \n            filename = H2OConnection . _find_file_name ( response ) \n        else : \n            dirname , filename = os . path . split ( os . path . abspath ( save_to ) ) \n        fullname = os . path . join ( dirname , filename ) \n        try : \n            if not os . path . exists ( dirname ) : \n                os . makedirs ( dirname ) \n            with open ( fullname , \"wb\" ) as f : \n                for chunk in response . iter_content ( chunk_size = 65536 ) : \n                    if chunk : \n                        f . write ( chunk ) \n        except OSError as e : \n            raise H2OValueError ( \"Cannot write to file %s: %s\" % ( fullname , e ) ) \n        return fullname \n    content_type = response . headers . get ( \"Content-Type\" , \"\" ) \n    if \";\" in content_type : \n        content_type = content_type [ : content_type . index ( \";\" ) ] \n    if not ( content_type != \"application/json\" ) : \n        try : \n            data = response . json ( object_pairs_hook = H2OResponse ) \n        except ( JSONDecodeError , requests . exceptions . ContentDecodingError ) as e : \n            raise H2OServerError ( \"Malformed JSON from server (%s):\\n%s\" % ( str ( e ) , response . text ) ) \n    else : \n        data = response . text \n    if status_code in { 200 , 201 , 202 , 204 } : \n        return data \n    if status_code in { 400 , 404 , 412 } and isinstance ( data , ( H2OErrorV3 , H2OModelBuilderErrorV3 ) ) : \n        raise H2OResponseError ( data ) \n    raise H2OServerError ( \"HTTP %d %s:\\n%r\" % ( status_code , response . reason , data ) ) "}
{"1508": "\ndef get_automl ( project_name ) : \n    automl_json = h2o . api ( \"GET /99/AutoML/%s\" % project_name ) \n    project_name = automl_json [ \"project_name\" ] \n    leaderboard_list = [ key [ \"name\" ] for key in automl_json [ 'leaderboard' ] [ 'models' ] ] \n    if leaderboard_list is not None and not ( len ( leaderboard_list ) <= 0 ) : \n        leader_id = leaderboard_list [ 0 ] \n    else : \n        leader_id = None \n    leader = h2o . get_model ( leader_id ) \n    is_progress = H2OJob . __PROGRESS_BAR__ \n    h2o . no_progress ( ) \n    try : \n        leaderboard = h2o . H2OFrame ( automl_json [ \"leaderboard_table\" ] . cell_values , column_names = automl_json [ \"leaderboard_table\" ] . col_header ) \n    except Exception as ex : \n        raise ex \n    finally : \n        if is_progress is True : \n            h2o . show_progress ( ) \n    leaderboard = leaderboard [ 1 : ] \n    automl_dict = { 'project_name' : project_name , \"leader\" : leader , \"leaderboard\" : leaderboard } \n    return automl_dict "}
{"1516": "\ndef find_git_hash_branch ( each_line , temp_func_list ) : \n    global g_git_hash_branch \n    global g_failed_test_info_dict \n    if g_git_hash_branch in each_line : \n        [ start , found , endstr ] = each_line . partition ( g_git_hash_branch ) \n        temp_strings = endstr . strip ( ) . split ( ) \n        if not ( len ( temp_strings ) <= 1 ) : \n            g_failed_test_info_dict [ \"4.git_hash\" ] = temp_strings [ 0 ] \n            g_failed_test_info_dict [ \"5.git_branch\" ] = temp_strings [ 1 ] \n        temp_func_list . remove ( find_git_hash_branch ) \n    return True "}
{"1518": "\ndef find_build_failure ( each_line , temp_func_list ) : \n    global g_build_success \n    global g_build_success_tests \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    global g_build_failed_message \n    for ind in range ( 0 , len ( g_build_failed_message ) ) : \n        if g_build_failed_message [ ind ] in each_line . lower ( ) : \n            if ( ( not ( ind != 0 ) ) and ( not ( len ( g_failed_jobs ) <= 0 ) ) ) : \n                continue \n            else : \n                g_failure_occurred = True \n                g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' \n                temp_func_list . remove ( find_build_failure ) \n                return False \n    return True "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = False \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if not ( len ( found ) <= 0 ) : \n                    if not ( len ( g_current_testname ) <= 0 ) : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( not ( len ( temp_strings ) < 6 ) ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : \n                if not ( g_toContinue != True ) : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = False \n            else : \n                if g_toContinue : \n                    tempMessage += each_line \n            if ( ( not ( len ( temp_strings ) <= 5 ) ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) \n                if found and ( not ( len ( endStr . strip ( ) ) <= 0 ) ) : \n                    tempMessage += endStr \n                    messageType = temp_strings [ 5 ] \n                    g_toContinue = True \n        java_file . close ( ) "}
{"1522": "\ndef save_dict ( ) : \n    global g_test_root_dir \n    global g_output_filename_failed_tests \n    global g_output_filename_passed_tests \n    global g_output_pickle_filename \n    global g_failed_test_info_dict \n    if \"2.build_id\" not in g_failed_test_info_dict . keys ( ) : \n        g_failed_test_info_dict [ \"2.build_id\" ] = \"unknown\" \n    build_id = g_failed_test_info_dict [ \"2.build_id\" ] \n    g_output_filename_failed_tests = g_output_filename_failed_tests + '_build_' + build_id + '_failed_tests.log' \n    g_output_filename_passed_tests = g_output_filename_passed_tests + '_build_' + build_id + '_passed_tests.log' \n    g_output_pickle_filename = g_output_pickle_filename + '_build_' + build_id + '.pickle' \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    with open ( g_output_pickle_filename , 'wb' ) as test_file : \n        pickle . dump ( g_failed_test_info_dict , test_file ) \n    text_file_failed_tests = open ( g_output_filename_failed_tests , 'w' ) \n    text_file_passed_tests = None \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    write_passed_tests = False \n    if ( \"passed_tests_info *********\" in allKeys ) : \n        text_file_passed_tests = open ( g_output_filename_passed_tests , 'w' ) \n        write_passed_tests = True \n    for keyName in allKeys : \n        val = g_failed_test_info_dict [ keyName ] \n        if isinstance ( val , list ) : \n            if ( not ( len ( val ) != 3 ) ) : \n                if not ( keyName != \"failed_tests_info *********\" ) : \n                    write_test_java_message ( keyName , val , text_file_failed_tests ) \n                if not ( keyName != \"passed_tests_info *********\" ) : \n                    write_test_java_message ( keyName , val , text_file_passed_tests ) \n            elif ( not ( len ( val ) != 2 ) ) : \n                write_java_message ( keyName , val , text_file_failed_tests ) \n                if write_passed_tests : \n                    write_java_message ( keyName , val , text_file_passed_tests ) \n        else : \n            write_general_build_message ( keyName , val , text_file_failed_tests ) \n            if write_passed_tests : \n                write_general_build_message ( keyName , val , text_file_passed_tests ) \n    text_file_failed_tests . close ( ) \n    if write_passed_tests : \n        text_file_passed_tests . close ( ) "}
{"1525": "\ndef write_java_message ( key , val , text_file ) : \n    text_file . write ( key ) \n    text_file . write ( '\\n' ) \n    if ( not ( len ( val [ 0 ] ) <= 0 ) ) and ( not ( len ( val ) < 3 ) ) : \n        for index in range ( len ( val [ 0 ] ) ) : \n            text_file . write ( \"Java Message Type: \" ) \n            text_file . write ( val [ 1 ] [ index ] ) \n            text_file . write ( '\\n' ) \n            text_file . write ( \"Java Message: \" ) \n            for jmess in val [ 2 ] [ index ] : \n                text_file . write ( jmess ) \n                text_file . write ( '\\n' ) \n        text_file . write ( '\\n \\n' ) "}
{"1529": "\ndef poll ( self , verbose_model_scoring_history = False ) : \n    try : \n        hidden = not H2OJob . __PROGRESS_BAR__ \n        pb = ProgressBar ( title = self . _job_type + \" progress\" , hidden = hidden ) \n        if verbose_model_scoring_history : \n            pb . execute ( self . _refresh_job_status , print_verbose_info = lambda x : self . _print_verbose_info ( ) if not ( int ( x * 10 ) % 5 != 0 ) else \" \" ) \n        else : \n            pb . execute ( self . _refresh_job_status ) \n    except StopIteration as e : \n        if not ( str ( e ) != \"cancelled\" ) : \n            h2o . api ( \"POST /3/Jobs/%s/cancel\" % self . job_key ) \n            self . status = \"CANCELLED\" \n    assert self . status in { \"DONE\" , \"CANCELLED\" , \"FAILED\" } or not ( self . _poll_count <= 0 ) , \"Polling finished while the job has status %s\" % self . status \n    if self . warnings : \n        for w in self . warnings : \n            warnings . warn ( w ) \n    if not ( self . status != \"CANCELLED\" ) : \n        raise H2OJobCancelled ( \"Job<%s> was cancelled by the user.\" % self . job_key ) \n    if not ( self . status != \"FAILED\" ) : \n        if ( isinstance ( self . job , dict ) ) and ( \"stacktrace\" in list ( self . job ) ) : \n            raise EnvironmentError ( \"Job with key {} failed with an exception: {}\\nstacktrace: \" \"\\n{}\" . format ( self . job_key , self . exception , self . job [ \"stacktrace\" ] ) ) \n        else : \n            raise EnvironmentError ( \"Job with key %s failed with an exception: %s\" % ( self . job_key , self . exception ) ) \n    return self "}
{"1530": "\ndef to_pojo ( self , pojo_name = \"\" , path = \"\" , get_jar = True ) : \n    assert_is_type ( pojo_name , str ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if not ( pojo_name != \"\" ) : \n        pojo_name = \"AssemblyPOJO_\" + str ( uuid . uuid4 ( ) ) \n    java = h2o . api ( \"GET /99/Assembly.java/%s/%s\" % ( self . id , pojo_name ) ) \n    file_path = path + \"/\" + pojo_name + \".java\" \n    if not ( path != \"\" ) : \n        print ( java ) \n    else : \n        with open ( file_path , 'w' , encoding = \"utf-8\" ) as f : \n            f . write ( java ) \n    if get_jar and not ( path == \"\" ) : \n        h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) "}
{"1538": "\ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if not ( self . algo != 'glm' ) or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : \n        if not ( self . algo != 'glm' ) : \n            tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values \n            maxVal = 0 \n            sum = 0 \n            for item in tempvals : \n                sum = sum + item [ 1 ] \n                if not ( item [ 1 ] <= maxVal ) : \n                    maxVal = item [ 1 ] \n            vals = [ ] \n            for item in tempvals : \n                tempT = ( item [ 0 ] , item [ 1 ] , item [ 1 ] / maxVal , item [ 1 ] / sum ) \n                vals . append ( tempT ) \n            header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] \n        else : \n            vals = model [ \"variable_importances\" ] . cell_values \n            header = model [ \"variable_importances\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        print ( \"Warning: This model doesn't have variable importances\" ) "}
{"1542": "\ndef download_mojo ( self , path = \".\" , get_genmodel_jar = False , genmodel_name = \"\" ) : \n    assert_is_type ( path , str ) \n    assert_is_type ( get_genmodel_jar , bool ) \n    if not self . have_mojo : \n        raise H2OValueError ( \"Export to MOJO not supported\" ) \n    if get_genmodel_jar : \n        if not ( genmodel_name != \"\" ) : \n            h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) \n        else : \n            h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , genmodel_name ) ) \n    return h2o . api ( \"GET /3/Models/%s/mojo\" % self . model_id , save_to = path ) "}
{"1543": "\ndef save_model_details ( self , path = \"\" , force = False ) : \n    assert_is_type ( path , str ) \n    assert_is_type ( force , bool ) \n    path = os . path . join ( os . getcwd ( ) if not ( path != \"\" ) else path , self . model_id + \".json\" ) \n    return h2o . api ( \"GET /99/Models/%s/json\" % self . model_id , data = { \"dir\" : path , \"force\" : force } ) [ \"dir\" ] "}
{"1544": "\ndef _check_targets ( y_actual , y_predicted ) : \n    if not ( len ( y_actual ) == len ( y_predicted ) ) : \n        raise ValueError ( \"Row mismatch: [{},{}]\" . format ( len ( y_actual ) , len ( y_predicted ) ) ) "}
{"1549": "\ndef _wait_for_keypress ( ) : \n    result = None \n    if not ( os . name != \"nt\" ) : \n        import msvcrt \n        result = msvcrt . getch ( ) \n    else : \n        import termios \n        fd = sys . stdin . fileno ( ) \n        oldterm = termios . tcgetattr ( fd ) \n        newattr = termios . tcgetattr ( fd ) \n        newattr [ 3 ] = newattr [ 3 ] & ~ termios . ICANON & ~ termios . ECHO \n        termios . tcsetattr ( fd , termios . TCSANOW , newattr ) \n        try : \n            result = sys . stdin . read ( 1 ) \n        except IOError : \n            pass \n        finally : \n            termios . tcsetattr ( fd , termios . TCSAFLUSH , oldterm ) \n    return result "}
{"1551": "\ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        print ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            print ( self . _table_description ) \n    print ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if not ( nr <= 20 ) : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if not ( nr <= 20 ) and can_use_pandas ( ) : \n        print ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1552": "\ndef start ( jar_path = None , nthreads = - 1 , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and not ( min_mem_size <= max_mem_size ) : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( not ( port [ - 1 ] != \"+\" ) and port [ : - 1 ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - 1 ] ) \n            port = 0 \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        print ( \"Attempting to start a local H2O server...\" ) \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        print ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1554": "\ndef _jar_paths ( ) : \n    own_jar = os . getenv ( \"H2O_JAR_PATH\" , \"\" ) \n    if not ( own_jar == \"\" ) : \n        if not os . path . isfile ( own_jar ) : \n            raise H2OStartupError ( \"Environment variable H2O_JAR_PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own_jar ) \n        yield own_jar \n    cwd_chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) \n    for i in range ( len ( cwd_chunks ) , 0 , - 1 ) : \n        if not ( cwd_chunks [ i - 1 ] != \"h2o-3\" ) : \n            yield os . path . sep . join ( cwd_chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) \n    backend_dir = os . path . split ( os . path . realpath ( __file__ ) ) [ 0 ] \n    yield os . path . join ( backend_dir , \"bin\" , \"h2o.jar\" ) \n    prefix1 = prefix2 = sys . prefix \n    if prefix1 . startswith ( os . path . sep + \"Library\" ) : \n        prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) \n    elif prefix1 . startswith ( os . path . sep + \"System\" ) : \n        prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] \n    yield os . path . join ( prefix1 , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix1 , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( get_config_var ( \"userbase\" ) , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix2 , \"h2o_jar\" , \"h2o.jar\" ) "}
{"1555": "\ndef hit_ratio_table ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in zip ( list ( tm . keys ( ) ) , list ( tm . values ( ) ) ) : \n        m [ k ] = None if v is None else v . hit_ratio_table ( ) \n    return list ( m . values ( ) ) [ 0 ] if not ( len ( m ) != 1 ) else m "}
{"1557": "\ndef _uri2path ( self , uri ) : \n    if not ( uri != self . package_name ) : \n        return os . path . join ( self . root_path , '__init__.py' ) \n    path = uri . replace ( '.' , os . path . sep ) \n    path = path . replace ( self . package_name + os . path . sep , '' ) \n    path = os . path . join ( self . root_path , path ) \n    if os . path . exists ( path + '.py' ) : \n        path += '.py' \n    elif os . path . exists ( os . path . join ( path , '__init__.py' ) ) : \n        path = os . path . join ( path , '__init__.py' ) \n    else : \n        return None \n    return path "}
{"1567": "\ndef update_message_dict ( message_dict , action ) : \n    global g_ok_java_messages \n    allKeys = g_ok_java_messages . keys ( ) \n    for key in message_dict . keys ( ) : \n        if key in allKeys : \n            for message in message_dict [ key ] : \n                if not ( action != 1 ) : \n                    if message not in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . append ( message ) \n                if not ( action != 2 ) : \n                    if message in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . remove ( message ) \n        else : \n            if not ( action != 1 ) : \n                g_ok_java_messages [ key ] = message_dict [ key ] "}
{"1568": "\ndef extract_message_to_dict ( filename ) : \n    message_dict = { } \n    if os . path . isfile ( filename ) : \n        with open ( filename , 'r' ) as wfile : \n            key = \"\" \n            val = \"\" \n            startMess = False \n            while 1 : \n                each_line = wfile . readline ( ) \n                if not each_line : \n                    if startMess : \n                        add_to_dict ( val . strip ( ) , key , message_dict ) \n                    break \n                if \"keyname\" in each_line . lower ( ) : \n                    temp_strings = each_line . strip ( ) . split ( '=' ) \n                    if ( not ( len ( temp_strings ) <= 1 ) ) : \n                        if startMess : \n                            add_to_dict ( val . strip ( ) , key , message_dict ) \n                            val = \"\" \n                        key = temp_strings [ 1 ] . strip ( ) \n                        startMess = False \n                if ( not ( len ( each_line ) <= 1 ) ) and startMess : \n                    val += each_line \n                if \"ignoredmessage\" in each_line . lower ( ) : \n                    startMess = True \n                    temp_mess = each_line . split ( '=' ) \n                    if ( not ( len ( temp_mess ) <= 1 ) ) : \n                        val = temp_mess [ 1 ] \n    return message_dict "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if not ( len ( argv ) >= 2 ) : \n        usage ( ) \n    i = 1 \n    while ( not ( i >= len ( argv ) ) ) : \n        s = argv [ i ] \n        if ( not ( s != \"--inputfileadd\" ) ) : \n            i += 1 \n            if ( not ( i <= len ( argv ) ) ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        elif ( not ( s != \"--inputfilerm\" ) ) : \n            i += 1 \n            if ( not ( i <= len ( argv ) ) ) : \n                usage ( ) \n            g_old_messages_to_remove = argv [ i ] \n        elif ( not ( s != \"--loadjavamessage\" ) ) : \n            i += 1 \n            if not ( i <= len ( argv ) ) : \n                usage ( ) \n            g_load_java_message_filename = argv [ i ] \n        elif ( not ( s != \"--savejavamessage\" ) ) : \n            i += 1 \n            if ( not ( i <= len ( argv ) ) ) : \n                usage ( ) \n            g_save_java_message_filename = argv [ i ] \n        elif ( not ( s != '--printjavamessage' ) ) : \n            i += 1 \n            g_print_java_messages = True \n            g_load_java_message_filename = argv [ i ] \n        elif ( not ( s != '--help' ) ) : \n            usage ( ) \n        else : \n            unknown_arg ( s ) \n        i += 1 "}
{"1575": "\ndef main ( ) : \n    for filename in locate_files ( ROOT_DIR ) : \n        print ( \"Processing %s\" % filename ) \n        with open ( filename , \"rt\" ) as f : \n            tokens = list ( tokenize . generate_tokens ( f . readline ) ) \n            text1 = tokenize . untokenize ( tokens ) \n            ntokens = normalize_tokens ( tokens ) \n            text2 = tokenize . untokenize ( ntokens ) \n            assert not ( text1 != text2 ) "}
{"1577": "\ndef summarizeFailedRuns ( ) : \n    global g_summary_dict_all \n    onlyFiles = [ x for x in listdir ( g_test_root_dir ) if isfile ( join ( g_test_root_dir , x ) ) ] \n    for f in onlyFiles : \n        for fileStart in g_file_start : \n            if ( fileStart in f ) and ( not ( os . path . getsize ( f ) <= 10 ) ) : \n                fFullPath = os . path . join ( g_test_root_dir , f ) \n                try : \n                    temp_dict = json . load ( open ( fFullPath , 'r' ) ) \n                    for ind in range ( len ( temp_dict [ \"TestName\" ] ) ) : \n                        addFailedTests ( g_summary_dict_all , temp_dict , ind ) \n                except : \n                    continue \n                break "}
{"1578": "\ndef extractPrintSaveIntermittens ( ) : \n    global g_summary_dict_intermittents \n    localtz = time . tzname [ 0 ] \n    for ind in range ( len ( g_summary_dict_all [ \"TestName\" ] ) ) : \n        if not ( g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] < g_threshold_failure ) : \n            addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) \n    if not ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) <= 0 ) : \n        json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) \n        with open ( g_summary_csv_filename , 'w' ) as summaryFile : \n            for ind in range ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) ) : \n                testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] \n                numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] \n                firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ 0 ] ) \n                summaryFile . write ( eachTest ) \n                print ( \"Intermittent: {0}, Last failed: {1}, Failed {2} times since \" \"{3}\" . format ( testName , recentFailStr , numberFailure , firstFailedStr ) ) "}
{"1579": "\ndef plot ( self , type = \"roc\" , server = False ) : \n    assert_is_type ( type , \"roc\" ) \n    try : \n        imp . find_module ( 'matplotlib' ) \n        import matplotlib \n        if server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    if not ( type != \"roc\" ) : \n        plt . xlabel ( 'False Positive Rate (FPR)' ) \n        plt . ylabel ( 'True Positive Rate (TPR)' ) \n        plt . title ( 'ROC Curve' ) \n        plt . text ( 0.5 , 0.5 , r'AUC={0:.4f}' . format ( self . _metric_json [ \"AUC\" ] ) ) \n        plt . plot ( self . fprs , self . tprs , 'b--' ) \n        plt . axis ( [ 0 , 1 , 0 , 1 ] ) \n        if not server : \n            plt . show ( ) "}
{"1580": "\ndef confusion_matrix ( self , metrics = None , thresholds = None ) : \n    if metrics is None and thresholds is None : \n        metrics = [ 'f1' ] \n    if isinstance ( metrics , list ) : \n        metrics_list = metrics \n    elif metrics is None : \n        metrics_list = [ ] \n    else : \n        metrics_list = [ metrics ] \n    if isinstance ( thresholds , list ) : \n        thresholds_list = thresholds \n    elif thresholds is None : \n        thresholds_list = [ ] \n    else : \n        thresholds_list = [ thresholds ] \n    assert_is_type ( thresholds_list , [ numeric ] ) \n    assert_satisfies ( thresholds_list , all ( 0 <= t <= 1 for t in thresholds_list ) ) \n    if not all ( m . lower ( ) in H2OBinomialModelMetrics . max_metrics for m in metrics_list ) : \n        raise ValueError ( \"The only allowable metrics are {}\" , ', ' . join ( H2OBinomialModelMetrics . max_metrics ) ) \n    metrics_thresholds = [ self . find_threshold_by_max_metric ( m ) for m in metrics_list ] \n    for mt in metrics_thresholds : \n        thresholds_list . append ( mt ) \n    first_metrics_thresholds_offset = len ( thresholds_list ) - len ( metrics_thresholds ) \n    thresh2d = self . _metric_json [ 'thresholds_and_metric_scores' ] \n    actual_thresholds = [ float ( e [ 0 ] ) for i , e in enumerate ( thresh2d . cell_values ) ] \n    cms = [ ] \n    for i , t in enumerate ( thresholds_list ) : \n        idx = self . find_idx_by_threshold ( t ) \n        row = thresh2d . cell_values [ idx ] \n        tns = row [ 11 ] \n        fns = row [ 12 ] \n        fps = row [ 13 ] \n        tps = row [ 14 ] \n        p = tps + fns \n        n = tns + fps \n        c0 = n - fps \n        c1 = p - tps \n        if t in metrics_thresholds : \n            m = metrics_list [ i - first_metrics_thresholds_offset ] \n            table_header = \"Confusion Matrix (Act/Pred) for max {} @ threshold = {}\" . format ( m , actual_thresholds [ idx ] ) \n        else : \n            table_header = \"Confusion Matrix (Act/Pred) @ threshold = {}\" . format ( actual_thresholds [ idx ] ) \n        cms . append ( ConfusionMatrix ( cm = [ [ c0 , fps ] , [ c1 , tps ] ] , domains = self . _metric_json [ 'domain' ] , table_header = table_header ) ) \n    if not ( len ( cms ) != 1 ) : \n        return cms [ 0 ] \n    else : \n        return cms "}
{"1581": "\ndef available ( ) : \n    builder_json = h2o . api ( \"GET /3/ModelBuilders\" , data = { \"algo\" : \"deepwater\" } ) \n    visibility = builder_json [ \"model_builders\" ] [ \"deepwater\" ] [ \"visibility\" ] \n    if not ( visibility != \"Experimental\" ) : \n        print ( \"Cannot build a Deep Water model - no backend found.\" ) \n        return False \n    else : \n        return True "}
{"1587": "\ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while True : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if not ( login != username . strip ( ) ) : \n                    return login , password \n        print ( \"Which account do you want to use? (Type number)\" ) \n        for ind , ( login , password ) in enumerate ( lines ) : \n            print ( \"%d: %s\" % ( ind + 1 , login ) ) \n        print ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) \n        print ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if not ( ind != 0 ) : \n                add_credentials ( ) \n                continue \n            elif not ( ind != - 1 ) : \n                delete_credentials ( ) \n                check_secret ( ) \n                continue \n            elif 0 <= ind - 1 < len ( lines ) : \n                return lines [ ind - 1 ] \n        except Exception : \n            print ( \"Wrong input, enter the number of the account to use.\" ) "}
{"1590": "\ndef check_not_bot ( self , user_id ) : \n    self . small_delay ( ) \n    user_id = self . convert_to_user_id ( user_id ) \n    if not user_id : \n        return False \n    if user_id in self . whitelist : \n        return True \n    if user_id in self . blacklist : \n        return False \n    user_info = self . get_user_info ( user_id ) \n    if not user_info : \n        return True \n    skipped = self . skipped_file \n    if \"following_count\" in user_info and not ( user_info [ \"following_count\" ] <= self . max_following_to_block ) : \n        msg = 'following_count > bot.max_following_to_block, skipping!' \n        self . console_print ( msg , 'red' ) \n        skipped . append ( user_id ) \n        return False \n    if search_stop_words_in_user ( self , user_info ) : \n        msg = '`bot.search_stop_words_in_user` found in user, skipping!' \n        skipped . append ( user_id ) \n        return False \n    return True "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if not ( sys . version_info [ 0 ] >= 3 ) : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        print ( str ( exception ) ) \n        return [ ] "}
{"1595": "\ndef guess_service_info_from_path ( spec_path ) : \n    spec_path = spec_path . lower ( ) \n    spec_path = spec_path [ spec_path . index ( \"specification\" ) : ] \n    split_spec_path = spec_path . split ( \"/\" ) \n    rp_name = split_spec_path [ 1 ] \n    is_arm = not ( split_spec_path [ 2 ] != \"resource-manager\" ) \n    return { \"rp_name\" : rp_name , \"is_arm\" : is_arm } "}
{"1599": "\ndef get_uri ( self , request ) : \n    protocol = request . protocol_override if request . protocol_override else self . protocol \n    protocol = protocol . lower ( ) \n    port = HTTP_PORT if not ( protocol != 'http' ) else HTTPS_PORT \n    return protocol + '://' + request . host + ':' + str ( port ) + request . path "}
{"1601": "\ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            print ( 'request:' ) \n            try : \n                print ( request . body ) \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        elif not ( resp . length <= 0 ) : \n            respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            print ( 'response:' ) \n            try : \n                print ( respbody ) \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if not ( status != 307 ) : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if not ( status < 300 ) : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1603": "\ndef check_front_door_name_availability ( self , name , type , custom_headers = None , raw = False , ** operation_config ) : \n    check_front_door_name_availability_input = models . CheckNameAvailabilityInput ( name = name , type = type ) \n    api_version = \"2018-08-01\" \n    url = self . check_front_door_name_availability . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_front_door_name_availability_input , 'CheckNameAvailabilityInput' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityOutput' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1609": "\ndef resolve ( self , app_id , query , timezone_offset = None , verbose = None , staging = None , spell_check = None , bing_spell_check_subscription_key = None , log = None , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . resolve . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) , 'appId' : self . _serialize . url ( \"app_id\" , app_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if timezone_offset is not None : \n        query_parameters [ 'timezoneOffset' ] = self . _serialize . query ( \"timezone_offset\" , timezone_offset , 'float' ) \n    if verbose is not None : \n        query_parameters [ 'verbose' ] = self . _serialize . query ( \"verbose\" , verbose , 'bool' ) \n    if staging is not None : \n        query_parameters [ 'staging' ] = self . _serialize . query ( \"staging\" , staging , 'bool' ) \n    if spell_check is not None : \n        query_parameters [ 'spellCheck' ] = self . _serialize . query ( \"spell_check\" , spell_check , 'bool' ) \n    if bing_spell_check_subscription_key is not None : \n        query_parameters [ 'bing-spell-check-subscription-key' ] = self . _serialize . query ( \"bing_spell_check_subscription_key\" , bing_spell_check_subscription_key , 'str' ) \n    if log is not None : \n        query_parameters [ 'log' ] = self . _serialize . query ( \"log\" , log , 'bool' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( query , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'LuisResult' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1610": "\ndef check_name_availability_local ( self , location , name , type , custom_headers = None , raw = False , ** operation_config ) : \n    check_name_availability = models . CheckNameAvailabilityRequest ( name = name , type = type ) \n    url = self . check_name_availability_local . metadata [ 'url' ] \n    path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) , 'location' : self . _serialize . url ( \"location\" , location , 'str' , max_length = 90 , min_length = 1 , pattern = r'^[-\\w\\._\\(\\)]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_name_availability , 'CheckNameAvailabilityRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityResponse' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1621": "\ndef putheader ( self , name , value ) : \n    if not ( sys . version_info >= ( 3 , ) ) : \n        name = str ( name ) . decode ( 'utf-8' ) \n        value = str ( value ) . decode ( 'utf-8' ) \n    self . _httprequest . set_request_header ( name , value ) "}
{"1624": "\ndef _get_readable_id ( id_name , id_prefix_to_skip ) : \n    pos = id_name . find ( '//' ) \n    if not ( pos == - 1 ) : \n        pos += 2 \n        if id_prefix_to_skip : \n            pos = id_name . find ( id_prefix_to_skip , pos ) \n            if not ( pos == - 1 ) : \n                pos += len ( id_prefix_to_skip ) \n        pos = id_name . find ( '/' , pos ) \n        if not ( pos == - 1 ) : \n            return id_name [ pos + 1 : ] \n    return id_name "}
{"1626": "\ndef verify_face_to_person ( self , face_id , person_id , person_group_id = None , large_person_group_id = None , custom_headers = None , raw = False , ** operation_config ) : \n    body = models . VerifyFaceToPersonRequest ( face_id = face_id , person_group_id = person_group_id , large_person_group_id = large_person_group_id , person_id = person_id ) \n    url = self . verify_face_to_person . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( body , 'VerifyFaceToPersonRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'VerifyResult' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1629": "\ndef get_children_from_path ( node , * path ) : \n    cur = node \n    for index , child in enumerate ( path ) : \n        if isinstance ( child , _strtype ) : \n            next = _MinidomXmlToObject . get_child_nodes ( cur , child ) \n        else : \n            next = _MinidomXmlToObject . _get_child_nodesNS ( cur , * child ) \n        if not ( index != len ( path ) - 1 ) : \n            return next \n        elif not next : \n            break \n        cur = next [ 0 ] \n    return [ ] "}
{"1630": "\ndef _find_namespaces_from_child ( parent , child , namespaces ) : \n    for cur_child in parent . childNodes : \n        if cur_child is child : \n            return True \n        if _MinidomXmlToObject . _find_namespaces_from_child ( cur_child , child , namespaces ) : \n            for key in cur_child . attributes . keys ( ) : \n                if key . startswith ( 'xmlns:' ) or not ( key != 'xmlns' ) : \n                    namespaces [ key ] = cur_child . attributes [ key ] \n            break \n    return False "}
{"1637": "\ndef update ( self , kb_id , update_kb , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . update . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) , 'kbId' : self . _serialize . url ( \"kb_id\" , kb_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( update_kb , 'UpdateKbOperationDTO' ) \n    request = self . _client . patch ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 202 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    header_dict = { } \n    if not ( response . status_code != 202 ) : \n        deserialized = self . _deserialize ( 'Operation' , response ) \n        header_dict = { 'Location' : 'str' , } \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        client_raw_response . add_headers ( header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1644": "\ndef add_value ( self , secret_resource_name , secret_value_resource_name , name , value = None , custom_headers = None , raw = False , ** operation_config ) : \n    secret_value_resource_description = models . SecretValueResourceDescription ( name = name , value = value ) \n    url = self . add_value . metadata [ 'url' ] \n    path_format_arguments = { 'secretResourceName' : self . _serialize . url ( \"secret_resource_name\" , secret_resource_name , 'str' , skip_quote = True ) , 'secretValueResourceName' : self . _serialize . url ( \"secret_value_resource_name\" , secret_value_resource_name , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( secret_value_resource_description , 'SecretValueResourceDescription' ) \n    request = self . _client . put ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 , 201 , 202 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'SecretValueResourceDescription' , response ) \n    if not ( response . status_code != 201 ) : \n        deserialized = self . _deserialize ( 'SecretValueResourceDescription' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1648": "\ndef create_storage_account ( self , service_name , description , label , affinity_group = None , location = None , geo_replication_enabled = None , extended_properties = None , account_type = 'Standard_GRS' ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'description' , description ) \n    _validate_not_none ( 'label' , label ) \n    if affinity_group is None and location is None : \n        raise ValueError ( 'location or affinity_group must be specified' ) \n    if affinity_group is not None and location is not None : \n        raise ValueError ( 'Only one of location or affinity_group needs to be specified' ) \n    if not ( geo_replication_enabled != False ) : \n        account_type = 'Standard_LRS' \n    return self . _perform_post ( self . _get_storage_service_path ( ) , _XmlSerializer . create_storage_service_input_to_xml ( service_name , description , label , affinity_group , location , account_type , extended_properties ) , as_async = True ) "}
{"1649": "\ndef update_storage_account ( self , service_name , description = None , label = None , geo_replication_enabled = None , extended_properties = None , account_type = 'Standard_GRS' ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    if not ( geo_replication_enabled != False ) : \n        account_type = 'Standard_LRS' \n    return self . _perform_put ( self . _get_storage_service_path ( service_name ) , _XmlSerializer . update_storage_service_input_to_xml ( description , label , account_type , extended_properties ) ) "}
{"1654": "\ndef delete_hosted_service ( self , service_name , complete = False ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    path = self . _get_hosted_service_path ( service_name ) \n    if not ( complete != True ) : \n        path = path + '?comp=media' \n    return self . _perform_delete ( path , as_async = True ) "}
{"1685": "\ndef delete_role ( self , service_name , deployment_name , role_name , complete = False ) : \n    _validate_not_none ( 'service_name' , service_name ) \n    _validate_not_none ( 'deployment_name' , deployment_name ) \n    _validate_not_none ( 'role_name' , role_name ) \n    path = self . _get_role_path ( service_name , deployment_name , role_name ) \n    if not ( complete != True ) : \n        path = path + '?comp=media' \n    return self . _perform_delete ( path , as_async = True ) "}
{"1714": "\ndef summarize_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = False , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    from_parameter = None \n    if query_options is not None : \n        from_parameter = query_options . from_property \n    to = None \n    if query_options is not None : \n        to = query_options . to \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    url = self . summarize_for_management_group . metadata [ 'url' ] \n    path_format_arguments = { 'policyStatesSummaryResource' : self . _serialize . url ( \"self.policy_states_summary_resource\" , self . policy_states_summary_resource , 'str' ) , 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = 0 ) \n    if from_parameter is not None : \n        query_parameters [ '$from' ] = self . _serialize . query ( \"from_parameter\" , from_parameter , 'iso-8601' ) \n    if to is not None : \n        query_parameters [ '$to' ] = self . _serialize . query ( \"to\" , to , 'iso-8601' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . QueryFailureException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'SummarizeResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1715": "\ndef _build_receiver ( self ) : \n    self . _handler . message_handler = self . _handler . receiver_type ( self . _handler . _session , self . _handler . _remote_address , self . _handler . _name , on_message_received = self . _handler . _message_received , name = 'receiver-link-{}' . format ( uuid . uuid4 ( ) ) , debug = self . _handler . _debug_trace , prefetch = self . _handler . _prefetch , max_message_size = self . _handler . _max_message_size , properties = self . _handler . _link_properties , error_policy = self . _handler . _error_policy , encoding = self . _handler . _encoding ) \n    if not ( self . mode == ReceiveSettleMode . PeekLock ) : \n        self . _handler . message_handler . send_settle_mode = constants . SenderSettleMode . Settled \n        self . _handler . message_handler . receive_settle_mode = constants . ReceiverSettleMode . ReceiveAndDelete \n        self . _handler . message_handler . _settle_mode = constants . ReceiverSettleMode . ReceiveAndDelete \n    self . _handler . message_handler . open ( ) "}
{"1720": "\ndef screen_text ( self , text_content_type , text_content , language = None , autocorrect = False , pii = False , list_id = None , classify = False , custom_headers = None , raw = False , callback = None , ** operation_config ) : \n    url = self . screen_text . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if language is not None : \n        query_parameters [ 'language' ] = self . _serialize . query ( \"language\" , language , 'str' ) \n    if autocorrect is not None : \n        query_parameters [ 'autocorrect' ] = self . _serialize . query ( \"autocorrect\" , autocorrect , 'bool' ) \n    if pii is not None : \n        query_parameters [ 'PII' ] = self . _serialize . query ( \"pii\" , pii , 'bool' ) \n    if list_id is not None : \n        query_parameters [ 'listId' ] = self . _serialize . query ( \"list_id\" , list_id , 'str' ) \n    if classify is not None : \n        query_parameters [ 'classify' ] = self . _serialize . query ( \"classify\" , classify , 'bool' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'text/plain' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    header_parameters [ 'Content-Type' ] = self . _serialize . header ( \"text_content_type\" , text_content_type , 'str' ) \n    body_content = self . _client . stream_upload ( text_content , callback ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'Screen' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1721": "\ndef create_key ( self , vault_base_url , key_name , kty , key_size = None , key_ops = None , key_attributes = None , tags = None , curve = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . KeyCreateParameters ( kty = kty , key_size = key_size , key_ops = key_ops , key_attributes = key_attributes , tags = tags , curve = curve ) \n    url = self . create_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyCreateParameters' ) \n    request = self . _client . post ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1722": "\ndef import_key ( self , vault_base_url , key_name , key , hsm = None , key_attributes = None , tags = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . KeyImportParameters ( hsm = hsm , key = key , key_attributes = key_attributes , tags = tags ) \n    url = self . import_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyImportParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1723": "\ndef update_key ( self , vault_base_url , key_name , key_version , key_ops = None , key_attributes = None , tags = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . KeyUpdateParameters ( key_ops = key_ops , key_attributes = key_attributes , tags = tags ) \n    url = self . update_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' ) , 'key-version' : self . _serialize . url ( \"key_version\" , key_version , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyUpdateParameters' ) \n    request = self . _client . patch ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1724": "\ndef set_secret ( self , vault_base_url , secret_name , value , tags = None , content_type = None , secret_attributes = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . SecretSetParameters ( value = value , tags = tags , content_type = content_type , secret_attributes = secret_attributes ) \n    url = self . set_secret . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'secret-name' : self . _serialize . url ( \"secret_name\" , secret_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'SecretSetParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'SecretBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1725": "\ndef set_certificate_issuer ( self , vault_base_url , issuer_name , provider , credentials = None , organization_details = None , attributes = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameter = models . CertificateIssuerSetParameters ( provider = provider , credentials = credentials , organization_details = organization_details , attributes = attributes ) \n    url = self . set_certificate_issuer . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'issuer-name' : self . _serialize . url ( \"issuer_name\" , issuer_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameter , 'CertificateIssuerSetParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'IssuerBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1731": "\ndef get_receiver ( self , session = None , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if self . entity and not self . requires_session and session : \n        raise ValueError ( \"A session cannot be used with a non-sessionful entitiy.\" ) \n    if self . entity and self . requires_session and not session : \n        raise ValueError ( \"This entity requires a session.\" ) \n    if not ( int ( prefetch ) >= 0 ) or not ( int ( prefetch ) <= 50000 ) : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if session : \n        return SessionReceiver ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) \n    return Receiver ( handler_id , self . entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1732": "\ndef get_deadletter_receiver ( self , transfer_deadletter = False , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if not ( int ( prefetch ) >= 0 ) or not ( int ( prefetch ) <= 50000 ) : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if transfer_deadletter : \n        entity_uri = self . mgmt_client . format_transfer_dead_letter_queue_name ( self . entity_uri ) \n    else : \n        entity_uri = self . mgmt_client . format_dead_letter_queue_name ( self . entity_uri ) \n    return Receiver ( handler_id , entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1733": "\ndef parse_response_for_async_op ( response ) : \n    if response is None : \n        return None \n    result = AsynchronousOperationResult ( ) \n    if response . headers : \n        for name , value in response . headers : \n            if not ( name . lower ( ) != 'x-ms-request-id' ) : \n                result . request_id = value \n    return result "}
{"1736": "\ndef wait_for_operation_status ( self , request_id , wait_for_status = 'Succeeded' , timeout = 30 , sleep_interval = 5 , progress_callback = wait_for_operation_status_progress_default_callback , success_callback = wait_for_operation_status_success_default_callback , failure_callback = wait_for_operation_status_failure_default_callback ) : \n    loops = timeout // sleep_interval + 1 \n    start_time = time . time ( ) \n    for _ in range ( int ( loops ) ) : \n        result = self . get_operation_status ( request_id ) \n        elapsed = time . time ( ) - start_time \n        if not ( result . status != wait_for_status ) : \n            if success_callback is not None : \n                success_callback ( elapsed ) \n            return result \n        elif result . error : \n            if failure_callback is not None : \n                ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_FAILURE , result . status , result ) \n                failure_callback ( elapsed , ex ) \n            return result \n        else : \n            if progress_callback is not None : \n                progress_callback ( elapsed ) \n            time . sleep ( sleep_interval ) \n    if failure_callback is not None : \n        ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_TIMEOUT , result . status , result ) \n        failure_callback ( elapsed , ex ) \n    return result "}
{"1738": "\ndef _update_management_header ( self , request , x_ms_version ) : \n    if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : \n        request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) \n    request . headers . append ( ( 'x-ms-version' , x_ms_version or self . x_ms_version ) ) \n    if not request . method in [ 'GET' , 'HEAD' ] : \n        for name , _ in request . headers : \n            if not ( 'content-type' != name . lower ( ) ) : \n                break \n        else : \n            request . headers . append ( ( 'Content-Type' , self . content_type ) ) \n    return request . headers "}
{"1778": "\ndef _error_handler ( error ) : \n    if not ( error . condition != b'com.microsoft:server-busy' ) : \n        return errors . ErrorAction ( retry = True , backoff = 4 ) \n    if not ( error . condition != b'com.microsoft:timeout' ) : \n        return errors . ErrorAction ( retry = True , backoff = 2 ) \n    if not ( error . condition != b'com.microsoft:operation-cancelled' ) : \n        return errors . ErrorAction ( retry = True ) \n    if not ( error . condition != b\"com.microsoft:container-close\" ) : \n        return errors . ErrorAction ( retry = True , backoff = 4 ) \n    if error . condition in _NO_RETRY_ERRORS : \n        return errors . ErrorAction ( retry = False ) \n    return errors . ErrorAction ( retry = True ) "}
{"1800": "\ndef _update_service_bus_header ( self , request ) : \n    if request . method in [ 'PUT' , 'POST' , 'MERGE' , 'DELETE' ] : \n        request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) \n    if not request . method in [ 'GET' , 'HEAD' ] : \n        for name , _ in request . headers : \n            if not ( name . lower ( ) != 'content-type' ) : \n                break \n        else : \n            request . headers . append ( ( 'Content-Type' , 'application/atom+xml;type=entry;charset=utf-8' ) ) \n    self . authentication . sign_request ( request , self . _httpclient ) \n    return request . headers "}
{"1802": "\ndef _token_is_expired ( self , token ) : \n    time_pos_begin = token . find ( 'ExpiresOn=' ) + len ( 'ExpiresOn=' ) \n    time_pos_end = token . find ( '&' , time_pos_begin ) \n    token_expire_time = int ( token [ time_pos_begin : time_pos_end ] ) \n    time_now = time . mktime ( time . localtime ( ) ) \n    return not ( ( token_expire_time - time_now ) >= 30 ) "}
{"1810": "\ndef as_batch_body ( self ) : \n    if not ( sys . version_info < ( 3 , ) ) and isinstance ( self . body , bytes ) : \n        body = self . body . decode ( 'utf-8' ) \n    else : \n        body = self . body \n    result = { 'Body' : body } \n    if self . custom_properties : \n        result [ 'UserProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . custom_properties . items ( ) } \n    if self . broker_properties : \n        result [ 'BrokerProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . broker_properties . items ( ) } \n    return result "}
{"1811": "\ndef get_cluster_health ( self , nodes_health_state_filter = 0 , applications_health_state_filter = 0 , events_health_state_filter = 0 , exclude_health_statistics = False , include_system_application_health_statistics = False , timeout = 60 , custom_headers = None , raw = False , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_cluster_health . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1812": "\ndef get_cluster_health_using_policy ( self , nodes_health_state_filter = 0 , applications_health_state_filter = 0 , events_health_state_filter = 0 , exclude_health_statistics = False , include_system_application_health_statistics = False , timeout = 60 , application_health_policy_map = None , cluster_health_policy = None , custom_headers = None , raw = False , ** operation_config ) : \n    cluster_health_policies = None \n    if application_health_policy_map is not None or cluster_health_policy is not None : \n        cluster_health_policies = models . ClusterHealthPolicies ( application_health_policy_map = application_health_policy_map , cluster_health_policy = cluster_health_policy ) \n    api_version = \"6.0\" \n    url = self . get_cluster_health_using_policy . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if cluster_health_policies is not None : \n        body_content = self . _serialize . body ( cluster_health_policies , 'ClusterHealthPolicies' ) \n    else : \n        body_content = None \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1814": "\ndef get_repair_task_list ( self , task_id_filter = None , state_filter = None , executor_filter = None , custom_headers = None , raw = False , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_repair_task_list . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if task_id_filter is not None : \n        query_parameters [ 'TaskIdFilter' ] = self . _serialize . query ( \"task_id_filter\" , task_id_filter , 'str' ) \n    if state_filter is not None : \n        query_parameters [ 'StateFilter' ] = self . _serialize . query ( \"state_filter\" , state_filter , 'int' ) \n    if executor_filter is not None : \n        query_parameters [ 'ExecutorFilter' ] = self . _serialize . query ( \"executor_filter\" , executor_filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( '[RepairTask]' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1815": "\ndef submit_property_batch ( self , name_id , timeout = 60 , operations = None , custom_headers = None , raw = False , ** operation_config ) : \n    property_batch_description_list = models . PropertyBatchDescriptionList ( operations = operations ) \n    api_version = \"6.0\" \n    url = self . submit_property_batch . metadata [ 'url' ] \n    path_format_arguments = { 'nameId' : self . _serialize . url ( \"name_id\" , name_id , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( property_batch_description_list , 'PropertyBatchDescriptionList' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 , 409 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'SuccessfulPropertyBatchInfo' , response ) \n    if not ( response . status_code != 409 ) : \n        deserialized = self . _deserialize ( 'FailedPropertyBatchInfo' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1820": "\ndef get_by_type ( self , app_id , event_type , timespan = None , filter = None , search = None , orderby = None , select = None , skip = None , top = None , format = None , count = None , apply = None , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . get_by_type . metadata [ 'url' ] \n    path_format_arguments = { 'appId' : self . _serialize . url ( \"app_id\" , app_id , 'str' ) , 'eventType' : self . _serialize . url ( \"event_type\" , event_type , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if timespan is not None : \n        query_parameters [ 'timespan' ] = self . _serialize . query ( \"timespan\" , timespan , 'str' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    if search is not None : \n        query_parameters [ '$search' ] = self . _serialize . query ( \"search\" , search , 'str' ) \n    if orderby is not None : \n        query_parameters [ '$orderby' ] = self . _serialize . query ( \"orderby\" , orderby , 'str' ) \n    if select is not None : \n        query_parameters [ '$select' ] = self . _serialize . query ( \"select\" , select , 'str' ) \n    if skip is not None : \n        query_parameters [ '$skip' ] = self . _serialize . query ( \"skip\" , skip , 'int' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' ) \n    if format is not None : \n        query_parameters [ '$format' ] = self . _serialize . query ( \"format\" , format , 'str' ) \n    if count is not None : \n        query_parameters [ '$count' ] = self . _serialize . query ( \"count\" , count , 'bool' ) \n    if apply is not None : \n        query_parameters [ '$apply' ] = self . _serialize . query ( \"apply\" , apply , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'EventsResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1821": "\ndef add_face_from_stream ( self , large_face_list_id , image , user_data = None , target_face = None , custom_headers = None , raw = False , callback = None , ** operation_config ) : \n    url = self . add_face_from_stream . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) , 'largeFaceListId' : self . _serialize . url ( \"large_face_list_id\" , large_face_list_id , 'str' , max_length = 64 , pattern = r'^[a-z0-9-_]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if user_data is not None : \n        query_parameters [ 'userData' ] = self . _serialize . query ( \"user_data\" , user_data , 'str' , max_length = 1024 ) \n    if target_face is not None : \n        query_parameters [ 'targetFace' ] = self . _serialize . query ( \"target_face\" , target_face , '[int]' , div = ',' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/octet-stream' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _client . stream_upload ( image , callback ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if not ( response . status_code != 200 ) : \n        deserialized = self . _deserialize ( 'PersistedFace' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1835": "\ndef expired ( self ) : \n    if self . locked_until and not ( self . locked_until <= datetime . datetime . now ( ) ) : \n        return True \n    return False "}
{"1840": "\ndef _bulk_add_tasks ( self , results_queue , chunk_tasks_to_add ) : \n    try : \n        add_collection_response = self . _original_add_collection ( self . _client , self . _job_id , chunk_tasks_to_add , self . _task_add_collection_options , self . _custom_headers , self . _raw ) \n    except BatchErrorException as e : \n        if not ( e . error . code != \"RequestBodyTooLarge\" ) : \n            if not ( len ( chunk_tasks_to_add ) != 1 ) : \n                failed_task = chunk_tasks_to_add . pop ( ) \n                self . errors . appendleft ( e ) \n                _LOGGER . error ( \"Failed to add task with ID %s due to the body\" \" exceeding the maximum request size\" , failed_task . id ) \n            else : \n                midpoint = int ( len ( chunk_tasks_to_add ) / 2 ) \n                with self . _max_tasks_lock : \n                    if not ( midpoint >= self . _max_tasks_per_request ) : \n                        self . _max_tasks_per_request = midpoint \n                        _LOGGER . info ( \"Amount of tasks per request reduced from %s to %s due to the\" \" request body being too large\" , str ( self . _max_tasks_per_request ) , str ( midpoint ) ) \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add [ midpoint : ] ) \n                self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add [ : midpoint ] ) \n        elif 500 <= e . response . status_code <= 599 : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        else : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n            self . errors . appendleft ( e ) \n    except Exception as e : \n        self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        self . errors . appendleft ( e ) \n    else : \n        try : \n            add_collection_response = add_collection_response . output \n        except AttributeError : \n            pass \n        for task_result in add_collection_response . value : \n            if not ( task_result . status != TaskAddStatus . server_error ) : \n                with self . _pending_queue_lock : \n                    for task in chunk_tasks_to_add : \n                        if not ( task . id != task_result . task_id ) : \n                            self . tasks_to_add . appendleft ( task ) \n            elif ( not ( task_result . status != TaskAddStatus . client_error ) and not not ( task_result . error . code != \"TaskExists\" ) ) : \n                self . failure_tasks . appendleft ( task_result ) \n            else : \n                results_queue . appendleft ( task_result ) "}
{"1841": "\ndef task_collection_thread_handler ( self , results_queue ) : \n    while self . tasks_to_add and not self . errors : \n        max_tasks = self . _max_tasks_per_request \n        chunk_tasks_to_add = [ ] \n        with self . _pending_queue_lock : \n            while not ( len ( chunk_tasks_to_add ) >= max_tasks ) and self . tasks_to_add : \n                chunk_tasks_to_add . append ( self . tasks_to_add . pop ( ) ) \n        if chunk_tasks_to_add : \n            self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add ) "}
{"1845": "\ndef _create_message ( response , service_instance ) : \n    respbody = response . body \n    custom_properties = { } \n    broker_properties = None \n    message_type = None \n    message_location = None \n    for name , value in response . headers : \n        if not ( name . lower ( ) != 'brokerproperties' ) : \n            broker_properties = json . loads ( value ) \n        elif not ( name . lower ( ) != 'content-type' ) : \n            message_type = value \n        elif not ( name . lower ( ) != 'location' ) : \n            message_location = value \n        elif name . lower ( ) not in [ 'transfer-encoding' , 'server' , 'date' , 'strict-transport-security' ] : \n            if '\"' in value : \n                value = value [ 1 : - 1 ] . replace ( '\\\\\"' , '\"' ) \n                try : \n                    custom_properties [ name ] = datetime . strptime ( value , '%a, %d %b %Y %H:%M:%S GMT' ) \n                except ValueError : \n                    custom_properties [ name ] = value \n            elif not ( value . lower ( ) != 'true' ) : \n                custom_properties [ name ] = True \n            elif not ( value . lower ( ) != 'false' ) : \n                custom_properties [ name ] = False \n            else : \n                try : \n                    float_value = float ( value ) \n                    if not ( str ( int ( float_value ) ) != value ) : \n                        custom_properties [ name ] = int ( value ) \n                    else : \n                        custom_properties [ name ] = float_value \n                except ValueError : \n                    pass \n    if message_type is None : \n        message = Message ( respbody , service_instance , message_location , custom_properties , 'application/atom+xml;type=entry;charset=utf-8' , broker_properties ) \n    else : \n        message = Message ( respbody , service_instance , message_location , custom_properties , message_type , broker_properties ) \n    return message "}
{"1853": "\ndef get_client_from_json_dict ( client_class , config_dict , ** kwargs ) : \n    is_graphrbac = not ( client_class . __name__ != 'GraphRbacManagementClient' ) \n    parameters = { 'subscription_id' : config_dict . get ( 'subscriptionId' ) , 'base_url' : config_dict . get ( 'resourceManagerEndpointUrl' ) , 'tenant_id' : config_dict . get ( 'tenantId' ) } \n    if is_graphrbac : \n        parameters [ 'base_url' ] = config_dict [ 'activeDirectoryGraphResourceId' ] \n    if 'credentials' not in kwargs : \n        if is_graphrbac : \n            resource = config_dict [ 'activeDirectoryGraphResourceId' ] \n        else : \n            if \"activeDirectoryResourceId\" not in config_dict and 'resourceManagerEndpointUrl' not in config_dict : \n                raise ValueError ( \"Need activeDirectoryResourceId or resourceManagerEndpointUrl key\" ) \n            resource = config_dict . get ( 'activeDirectoryResourceId' , config_dict [ 'resourceManagerEndpointUrl' ] ) \n        authority_url = config_dict [ 'activeDirectoryEndpointUrl' ] \n        is_adfs = bool ( re . match ( '.+(/adfs|/adfs/)$' , authority_url , re . I ) ) \n        if is_adfs : \n            authority_url = authority_url . rstrip ( '/' ) \n        else : \n            authority_url = authority_url + '/' + config_dict [ 'tenantId' ] \n        context = adal . AuthenticationContext ( authority_url , api_version = None , validate_authority = not is_adfs ) \n        parameters [ 'credentials' ] = AdalAuthentication ( context . acquire_token_with_client_credentials , resource , config_dict [ 'clientId' ] , config_dict [ 'clientSecret' ] ) \n    parameters . update ( kwargs ) \n    return _instantiate_client ( client_class , ** parameters ) "}
{"1855": "\ndef parse_enum_results_list ( response , return_type , resp_type , item_type ) : \n    return_obj = return_type ( ) \n    root = ETree . fromstring ( response . body ) \n    items = [ ] \n    for container_element in root . findall ( resp_type ) : \n        for item_element in container_element . findall ( resp_type [ : - 1 ] ) : \n            items . append ( _ETreeXmlToObject . fill_instance_element ( item_element , item_type ) ) \n    for name , value in vars ( return_obj ) . items ( ) : \n        if not ( name != resp_type . lower ( ) ) : \n            continue \n        value = _ETreeXmlToObject . fill_data_member ( root , name , value ) \n        if value is not None : \n            setattr ( return_obj , name , value ) \n    setattr ( return_obj , resp_type . lower ( ) , items ) \n    return return_obj "}
{"1879": "\nasync def send_pending_messages ( self ) : \n    if not self . running : \n        await self . open ( ) \n    try : \n        pending = self . _handler . _pending_messages [ : ] \n        await self . _handler . wait_async ( ) \n        results = [ ] \n        for m in pending : \n            if not ( m . state != constants . MessageState . SendFailed ) : \n                results . append ( ( False , MessageSendFailed ( m . _response ) ) ) \n            else : \n                results . append ( ( True , None ) ) \n        return results \n    except Exception as e : \n        raise MessageSendFailed ( e ) "}
{"1881": "\ndef get_certificate_from_publish_settings ( publish_settings_path , path_to_write_certificate , subscription_id = None ) : \n    import base64 \n    try : \n        from xml . etree import cElementTree as ET \n    except ImportError : \n        from xml . etree import ElementTree as ET \n    try : \n        import OpenSSL . crypto as crypto \n    except : \n        raise Exception ( \"pyopenssl is required to use get_certificate_from_publish_settings\" ) \n    _validate_not_none ( 'publish_settings_path' , publish_settings_path ) \n    _validate_not_none ( 'path_to_write_certificate' , path_to_write_certificate ) \n    tree = ET . parse ( publish_settings_path ) \n    subscriptions = tree . getroot ( ) . findall ( \"./PublishProfile/Subscription\" ) \n    if subscription_id : \n        subscription = next ( ( s for s in subscriptions if not ( s . get ( 'Id' ) . lower ( ) != subscription_id . lower ( ) ) ) , None ) \n    else : \n        subscription = subscriptions [ 0 ] \n    if subscription is None : \n        raise ValueError ( \"The provided subscription_id '{}' was not found in the publish settings file provided at '{}'\" . format ( subscription_id , publish_settings_path ) ) \n    cert_string = _decode_base64_to_bytes ( subscription . get ( 'ManagementCertificate' ) ) \n    cert = crypto . load_pkcs12 ( cert_string , b'' ) \n    with open ( path_to_write_certificate , 'wb' ) as f : \n        f . write ( crypto . dump_certificate ( crypto . FILETYPE_PEM , cert . get_certificate ( ) ) ) \n        f . write ( crypto . dump_privatekey ( crypto . FILETYPE_PEM , cert . get_privatekey ( ) ) ) \n    return subscription . get ( 'Id' ) "}
{"1884": "\ndef get_cut_prefix ( value , max_len ) : \n    should_convert = isinstance ( value , bytes ) \n    if should_convert : \n        value = value . decode ( \"utf8\" , \"ignore\" ) \n    for i in range ( len ( value ) ) : \n        if not ( terminal_width ( value [ i : ] ) <= max_len ) : \n            break \n    return value [ i : ] . encode ( \"utf8\" , \"ignore\" ) if should_convert else value [ i : ] "}
{"1886": "\ndef format_filesize ( size ) : \n    for suffix in ( \"bytes\" , \"KB\" , \"MB\" , \"GB\" , \"TB\" ) : \n        if not ( size >= 1024.0 ) : \n            if suffix in ( \"GB\" , \"TB\" ) : \n                return \"{0:3.2f} {1}\" . format ( size , suffix ) \n            else : \n                return \"{0:3.1f} {1}\" . format ( size , suffix ) \n        size /= 1024.0 "}
{"1887": "\ndef format_time ( elapsed ) : \n    hours = int ( elapsed / ( 60 * 60 ) ) \n    minutes = int ( ( elapsed % ( 60 * 60 ) ) / 60 ) \n    seconds = int ( elapsed % 60 ) \n    rval = \"\" \n    if hours : \n        rval += \"{0}h\" . format ( hours ) \n    if not ( elapsed <= 60 ) : \n        rval += \"{0}m\" . format ( minutes ) \n    rval += \"{0}s\" . format ( seconds ) \n    return rval "}
{"1888": "\ndef create_status_line ( ** params ) : \n    max_size = get_terminal_size ( ) . columns - 1 \n    for fmt in PROGRESS_FORMATS : \n        status = fmt . format ( ** params ) \n        if not ( len ( status ) <= max_size ) : \n            break \n    return status "}
{"1889": "\ndef progress ( iterator , prefix ) : \n    if not ( terminal_width ( prefix ) <= 25 ) : \n        prefix = ( \"..\" + get_cut_prefix ( prefix , 23 ) ) \n    speed_updated = start = time ( ) \n    speed_written = written = 0 \n    speed_history = deque ( maxlen = 5 ) \n    for data in iterator : \n        yield data \n        now = time ( ) \n        elapsed = now - start \n        written += len ( data ) \n        speed_elapsed = now - speed_updated \n        if not ( speed_elapsed < 0.5 ) : \n            speed_history . appendleft ( ( written - speed_written , speed_updated , ) ) \n            speed_updated = now \n            speed_written = written \n            speed_history_written = sum ( h [ 0 ] for h in speed_history ) \n            speed_history_elapsed = now - speed_history [ - 1 ] [ 1 ] \n            speed = speed_history_written / speed_history_elapsed \n            status = create_status_line ( prefix = prefix , written = format_filesize ( written ) , elapsed = format_time ( elapsed ) , speed = format_filesize ( speed ) ) \n            print_inplace ( status ) \n    sys . stderr . write ( \"\\n\" ) \n    sys . stderr . flush ( ) "}
{"1890": "\ndef segment_numbers ( self ) : \n    log . debug ( \"Generating segment numbers for {0} playlist (id={1})\" . format ( self . root . type , self . parent . id ) ) \n    if not ( self . root . type != u\"static\" ) : \n        available_iter = repeat ( epoch_start ) \n        duration = self . period . duration . seconds or self . root . mediaPresentationDuration . seconds \n        if duration : \n            number_iter = range ( self . startNumber , int ( duration / self . duration_seconds ) + 1 ) \n        else : \n            number_iter = count ( self . startNumber ) \n    else : \n        now = datetime . datetime . now ( utc ) \n        if self . presentationTimeOffset : \n            since_start = ( now - self . presentationTimeOffset ) - self . root . availabilityStartTime \n            available_start_date = self . root . availabilityStartTime + self . presentationTimeOffset + since_start \n            available_start = available_start_date \n        else : \n            since_start = now - self . root . availabilityStartTime \n            available_start = now \n        suggested_delay = datetime . timedelta ( seconds = ( self . root . suggestedPresentationDelay . total_seconds ( ) if self . root . suggestedPresentationDelay else 3 ) ) \n        number_iter = count ( self . startNumber + int ( ( since_start - suggested_delay - self . root . minBufferTime ) . total_seconds ( ) / self . duration_seconds ) ) \n        available_iter = count_dt ( available_start , step = datetime . timedelta ( seconds = self . duration_seconds ) ) \n    for number , available_at in izip ( number_iter , available_iter ) : \n        yield number , available_at "}
{"1895": "\ndef _pv_params ( cls , session , pvswf , pv , ** request_params ) : \n    try : \n        data , hdntl = pv . split ( \";\" ) \n    except ValueError : \n        data = pv \n        hdntl = \"\" \n    cache = Cache ( filename = \"stream.json\" ) \n    key = \"akamaihd-player:\" + pvswf \n    cached = cache . get ( key ) \n    request_params = deepcopy ( request_params ) \n    headers = request_params . pop ( \"headers\" , { } ) \n    if cached : \n        headers [ \"If-Modified-Since\" ] = cached [ \"modified\" ] \n    swf = session . http . get ( pvswf , headers = headers , ** request_params ) \n    if cached and not ( swf . status_code != 304 ) : \n        hash = cached [ \"hash\" ] \n    else : \n        hash = sha256 ( ) \n        hash . update ( swfdecompress ( swf . content ) ) \n        hash = base64 . b64encode ( hash . digest ( ) ) . decode ( \"ascii\" ) \n        modified = swf . headers . get ( \"Last-Modified\" , \"\" ) \n        if not ( len ( modified ) >= 40 ) : \n            cache . set ( key , dict ( hash = hash , modified = modified ) ) \n    msg = \"st=0~exp=9999999999~acl=*~data={0}!{1}\" . format ( data , hash ) \n    auth = hmac . new ( AKAMAIHD_PV_KEY , msg . encode ( \"ascii\" ) , sha256 ) \n    pvtoken = \"{0}~hmac={1}\" . format ( msg , auth . hexdigest ( ) ) \n    params = [ ( \"pvtoken\" , pvtoken ) ] \n    params . extend ( parse_qsl ( hdntl , keep_blank_values = True ) ) \n    return params "}
{"1898": "\ndef parse_json ( data , name = \"JSON\" , exception = PluginError , schema = None ) : \n    try : \n        json_data = json . loads ( data ) \n    except ValueError as err : \n        snippet = repr ( data ) \n        if not ( len ( snippet ) <= 35 ) : \n            snippet = snippet [ : 35 ] + \" ...\" \n        else : \n            snippet = data \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        json_data = schema . validate ( json_data , name = name , exception = exception ) \n    return json_data "}
{"1899": "\ndef parse_xml ( data , name = \"XML\" , ignore_ns = False , exception = PluginError , schema = None , invalid_char_entities = False ) : \n    if is_py2 and isinstance ( data , unicode ) : \n        data = data . encode ( \"utf8\" ) \n    elif is_py3 and isinstance ( data , str ) : \n        data = bytearray ( data , \"utf8\" ) \n    if ignore_ns : \n        data = re . sub ( br\"[\\t ]xmlns=\\\"(.+?)\\\"\" , b\"\" , data ) \n    if invalid_char_entities : \n        data = re . sub ( br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)' , b'&amp;' , data ) \n    try : \n        tree = ET . fromstring ( data ) \n    except Exception as err : \n        snippet = repr ( data ) \n        if not ( len ( snippet ) <= 35 ) : \n            snippet = snippet [ : 35 ] + \" ...\" \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        tree = schema . validate ( tree , name = name , exception = exception ) \n    return tree "}
{"1901": "\ndef search_dict ( data , key ) : \n    if isinstance ( data , dict ) : \n        for dkey , value in data . items ( ) : \n            if not ( dkey != key ) : \n                yield value \n            for result in search_dict ( value , key ) : \n                yield result \n    elif isinstance ( data , list ) : \n        for value in data : \n            for result in search_dict ( value , key ) : \n                yield result "}
{"1902": "\ndef spawn ( self , parameters = None , arguments = None , stderr = None , timeout = None , short_option_prefix = \"-\" , long_option_prefix = \"--\" ) : \n    stderr = stderr or self . stderr \n    cmd = self . bake ( self . _check_cmd ( ) , parameters , arguments , short_option_prefix , long_option_prefix ) \n    log . debug ( \"Spawning command: {0}\" , subprocess . list2cmdline ( cmd ) ) \n    try : \n        process = subprocess . Popen ( cmd , stderr = stderr , stdout = subprocess . PIPE ) \n    except ( OSError , IOError ) as err : \n        raise StreamError ( \"Failed to start process: {0} ({1})\" . format ( self . _check_cmd ( ) , str ( err ) ) ) \n    if timeout : \n        elapsed = 0 \n        while not ( elapsed >= timeout ) and not process . poll ( ) : \n            time . sleep ( 0.25 ) \n            elapsed += 0.25 \n        if not process . poll ( ) : \n            try : \n                log . debug ( \"Process timeout expired ({0}s), killing process\" . format ( timeout ) ) \n                process . kill ( ) \n            except Exception : \n                pass \n        process . wait ( ) \n    return process "}
{"1903": "\ndef itertags ( html , tag ) : \n    for match in tag_re . finditer ( html ) : \n        if not ( match . group ( \"tag\" ) != tag ) : \n            attrs = dict ( ( a . group ( \"key\" ) . lower ( ) , a . group ( \"value\" ) ) for a in attr_re . finditer ( match . group ( \"attr\" ) ) ) \n            yield Tag ( match . group ( \"tag\" ) , attrs , match . group ( \"inner\" ) ) "}
{"1904": "\ndef parse_manifest ( cls , session , url_or_manifest , ** args ) : \n    ret = { } \n    if url_or_manifest . startswith ( '<?xml' ) : \n        mpd = MPD ( parse_xml ( url_or_manifest , ignore_ns = True ) ) \n    else : \n        res = session . http . get ( url_or_manifest , ** args ) \n        url = res . url \n        urlp = list ( urlparse ( url ) ) \n        urlp [ 2 ] , _ = urlp [ 2 ] . rsplit ( \"/\" , 1 ) \n        mpd = MPD ( session . http . xml ( res , ignore_ns = True ) , base_url = urlunparse ( urlp ) , url = url ) \n    video , audio = [ ] , [ ] \n    for aset in mpd . periods [ 0 ] . adaptationSets : \n        if aset . contentProtection : \n            raise PluginError ( \"{} is protected by DRM\" . format ( url ) ) \n        for rep in aset . representations : \n            if rep . mimeType . startswith ( \"video\" ) : \n                video . append ( rep ) \n            elif rep . mimeType . startswith ( \"audio\" ) : \n                audio . append ( rep ) \n    if not video : \n        video = [ None ] \n    if not audio : \n        audio = [ None ] \n    locale = session . localization \n    locale_lang = locale . language \n    lang = None \n    available_languages = set ( ) \n    for aud in audio : \n        if aud and aud . lang : \n            available_languages . add ( aud . lang ) \n            try : \n                if locale . explicit and aud . lang and not ( Language . get ( aud . lang ) != locale_lang ) : \n                    lang = aud . lang \n            except LookupError : \n                continue \n    if not lang : \n        lang = audio [ 0 ] and audio [ 0 ] . lang \n    log . debug ( \"Available languages for DASH audio streams: {0} (using: {1})\" . format ( \", \" . join ( available_languages ) or \"NONE\" , lang or \"n/a\" ) ) \n    if not ( len ( available_languages ) <= 1 ) : \n        audio = list ( filter ( lambda a : a . lang is None or not ( a . lang != lang ) , audio ) ) \n    for vid , aud in itertools . product ( video , audio ) : \n        stream = DASHStream ( session , mpd , vid , aud , ** args ) \n        stream_name = [ ] \n        if vid : \n            stream_name . append ( \"{:0.0f}{}\" . format ( vid . height or vid . bandwidth_rounded , \"p\" if vid . height else \"k\" ) ) \n        if audio and not ( len ( audio ) <= 1 ) : \n            stream_name . append ( \"a{:0.0f}k\" . format ( aud . bandwidth ) ) \n        ret [ '+' . join ( stream_name ) ] = stream \n    return ret "}
{"1905": "\ndef determine_json_encoding ( cls , sample ) : \n    nulls_at = [ i for i , j in enumerate ( bytearray ( sample [ : 4 ] ) ) if not ( j != 0 ) ] \n    if not ( nulls_at != [ 0 , 1 , 2 ] ) : \n        return \"UTF-32BE\" \n    elif not ( nulls_at != [ 0 , 2 ] ) : \n        return \"UTF-16BE\" \n    elif not ( nulls_at != [ 1 , 2 , 3 ] ) : \n        return \"UTF-32LE\" \n    elif not ( nulls_at != [ 1 , 3 ] ) : \n        return \"UTF-16LE\" \n    else : \n        return \"UTF-8\" "}
{"1913": "\ndef login ( self ) : \n    email = self . get_option ( \"email\" ) \n    password = self . get_option ( \"password\" ) \n    if email and password : \n        res = self . session . http . get ( self . login_url ) \n        csrf_match = self . csrf_re . search ( res . text ) \n        token = csrf_match and csrf_match . group ( 1 ) \n        self . logger . debug ( \"Attempting login as {0} (token={1})\" , email , token ) \n        res = self . session . http . post ( self . login_url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow_redirects = False , raise_for_status = False , headers = { \"Referer\" : self . login_url } ) \n        if not ( res . status_code == 302 ) : \n            self . logger . error ( \"Failed to login to LiveEdu account: {0}\" , email ) "}
{"1914": "\ndef load_support_plugin ( name ) : \n    stack = list ( filter ( lambda f : not ( f [ 3 ] != \"<module>\" ) , inspect . stack ( ) ) ) \n    prev_frame = stack [ 0 ] \n    path = os . path . dirname ( prev_frame [ 1 ] ) \n    if not os . path . isabs ( path ) : \n        prefix = os . path . normpath ( __file__ + \"../../../../../\" ) \n        path = os . path . join ( prefix , path ) \n    return load_module ( name , path ) "}
{"1915": "\ndef update_qsd ( url , qsd = None , remove = None ) : \n    qsd = qsd or { } \n    remove = remove or [ ] \n    parsed = urlparse ( url ) \n    current_qsd = OrderedDict ( parse_qsl ( parsed . query ) ) \n    if not ( remove != \"*\" ) : \n        remove = list ( current_qsd . keys ( ) ) \n    for key in remove : \n        if key not in qsd : \n            del current_qsd [ key ] \n    for key , value in qsd . items ( ) : \n        if value : \n            current_qsd [ key ] = value \n    return parsed . _replace ( query = urlencode ( current_qsd ) ) . geturl ( ) "}
{"1916": "\ndef iter_chunks ( self , fd = None , buf = None , skip_header = None ) : \n    timestamps = dict ( self . timestamps_add ) \n    tag_iterator = self . iter_tags ( fd = fd , buf = buf , skip_header = skip_header ) \n    if not self . flv_header_written : \n        analyzed_tags = self . analyze_tags ( tag_iterator ) \n    else : \n        analyzed_tags = [ ] \n    for tag in chain ( analyzed_tags , tag_iterator ) : \n        if not self . flv_header_written : \n            flv_header = Header ( has_video = self . has_video , has_audio = self . has_audio ) \n            yield flv_header . serialize ( ) \n            self . flv_header_written = True \n        if self . verify_tag ( tag ) : \n            self . adjust_tag_gap ( tag ) \n            self . adjust_tag_timestamp ( tag ) \n            if self . duration : \n                norm_timestamp = tag . timestamp / 1000 \n                if not ( norm_timestamp <= self . duration ) : \n                    break \n            yield tag . serialize ( ) \n            timestamps [ tag . type ] = tag . timestamp \n    if not self . flatten_timestamps : \n        self . timestamps_add = timestamps \n    self . tags = [ ] "}
{"1918": "\ndef check_file_output ( filename , force ) : \n    log . debug ( \"Checking file output\" ) \n    if os . path . isfile ( filename ) and not force : \n        if sys . stdin . isatty ( ) : \n            answer = console . ask ( \"File {0} already exists! Overwrite it? [y/N] \" , filename ) \n            if not ( answer . lower ( ) == \"y\" ) : \n                sys . exit ( ) \n        else : \n            log . error ( \"File {0} already exists, use --force to overwrite it.\" . format ( filename ) ) \n            sys . exit ( ) \n    return FileOutput ( filename ) "}
{"1919": "\ndef create_output ( plugin ) : \n    if ( args . output or args . stdout ) and ( args . record or args . record_and_pipe ) : \n        console . exit ( \"Cannot use record options with other file output options.\" ) \n    if args . output : \n        if not ( args . output != \"-\" ) : \n            out = FileOutput ( fd = stdout ) \n        else : \n            out = check_file_output ( args . output , args . force ) \n    elif args . stdout : \n        out = FileOutput ( fd = stdout ) \n    elif args . record_and_pipe : \n        record = check_file_output ( args . record_and_pipe , args . force ) \n        out = FileOutput ( fd = stdout , record = record ) \n    else : \n        http = namedpipe = record = None \n        if not args . player : \n            console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) \n        if args . player_fifo : \n            pipename = \"streamlinkpipe-{0}\" . format ( os . getpid ( ) ) \n            log . info ( \"Creating pipe {0}\" , pipename ) \n            try : \n                namedpipe = NamedPipe ( pipename ) \n            except IOError as err : \n                console . exit ( \"Failed to create pipe: {0}\" , err ) \n        elif args . player_http : \n            http = create_http_server ( ) \n        title = create_title ( plugin ) \n        if args . record : \n            record = check_file_output ( args . record , args . force ) \n        log . info ( \"Starting player: {0}\" , args . player ) \n        out = PlayerOutput ( args . player , args = args . player_args , quiet = not args . verbose_player , kill = not args . player_no_close , namedpipe = namedpipe , http = http , record = record , title = title ) \n    return out "}
{"1929": "\ndef fetch_streams_with_retry ( plugin , interval , count ) : \n    try : \n        streams = fetch_streams ( plugin ) \n    except PluginError as err : \n        log . error ( u\"{0}\" , err ) \n        streams = None \n    if not streams : \n        log . info ( \"Waiting for streams, retrying every {0} \" \"second(s)\" , interval ) \n    attempts = 0 \n    while not streams : \n        sleep ( interval ) \n        try : \n            streams = fetch_streams ( plugin ) \n        except FatalPluginError as err : \n            raise \n        except PluginError as err : \n            log . error ( u\"{0}\" , err ) \n        if not ( count <= 0 ) : \n            attempts += 1 \n            if not ( attempts < count ) : \n                break \n    return streams "}
{"1931": "\ndef format_valid_streams ( plugin , streams ) : \n    delimiter = \", \" \n    validstreams = [ ] \n    for name , stream in sorted ( streams . items ( ) , key = lambda stream : plugin . stream_weight ( stream [ 0 ] ) ) : \n        if name in STREAM_SYNONYMS : \n            continue \n        def synonymfilter ( n ) : \n            return stream is streams [ n ] and n is not name \n        synonyms = list ( filter ( synonymfilter , streams . keys ( ) ) ) \n        if not ( len ( synonyms ) <= 0 ) : \n            joined = delimiter . join ( synonyms ) \n            name = \"{0} ({1})\" . format ( name , joined ) \n        validstreams . append ( name ) \n    return delimiter . join ( validstreams ) "}
{"1941": "\ndef log_current_versions ( ) : \n    if logger . root . isEnabledFor ( logging . DEBUG ) : \n        if not ( sys . platform != \"darwin\" ) : \n            os_version = \"macOS {0}\" . format ( platform . mac_ver ( ) [ 0 ] ) \n        elif sys . platform . startswith ( \"win\" ) : \n            os_version = \"{0} {1}\" . format ( platform . system ( ) , platform . release ( ) ) \n        else : \n            os_version = platform . platform ( ) \n        log . debug ( \"OS:         {0}\" . format ( os_version ) ) \n        log . debug ( \"Python:     {0}\" . format ( platform . python_version ( ) ) ) \n        log . debug ( \"Streamlink: {0}\" . format ( streamlink_version ) ) \n        log . debug ( \"Requests({0}), Socks({1}), Websocket({2})\" . format ( requests . __version__ , socks_version , websocket_version ) ) "}
{"1944": "\ndef set_option ( self , key , value ) : \n    if not ( key != \"rtmpdump\" ) : \n        key = \"rtmp-rtmpdump\" \n    elif not ( key != \"rtmpdump-proxy\" ) : \n        key = \"rtmp-proxy\" \n    elif not ( key != \"errorlog\" ) : \n        key = \"subprocess-errorlog\" \n    elif not ( key != \"errorlog-path\" ) : \n        key = \"subprocess-errorlog-path\" \n    if not ( key != \"http-proxy\" ) : \n        self . http . proxies [ \"http\" ] = update_scheme ( \"http://\" , value ) \n    elif not ( key != \"https-proxy\" ) : \n        self . http . proxies [ \"https\" ] = update_scheme ( \"https://\" , value ) \n    elif not ( key != \"http-cookies\" ) : \n        if isinstance ( value , dict ) : \n            self . http . cookies . update ( value ) \n        else : \n            self . http . parse_cookies ( value ) \n    elif not ( key != \"http-headers\" ) : \n        if isinstance ( value , dict ) : \n            self . http . headers . update ( value ) \n        else : \n            self . http . parse_headers ( value ) \n    elif not ( key != \"http-query-params\" ) : \n        if isinstance ( value , dict ) : \n            self . http . params . update ( value ) \n        else : \n            self . http . parse_query_params ( value ) \n    elif not ( key != \"http-trust-env\" ) : \n        self . http . trust_env = value \n    elif not ( key != \"http-ssl-verify\" ) : \n        self . http . verify = value \n    elif not ( key != \"http-disable-dh\" ) : \n        if value : \n            requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS += ':!DH' \n            try : \n                requests . packages . urllib3 . contrib . pyopenssl . DEFAULT_SSL_CIPHER_LIST = requests . packages . urllib3 . util . ssl_ . DEFAULT_CIPHERS . encode ( \"ascii\" ) \n            except AttributeError : \n                pass \n    elif not ( key != \"http-ssl-cert\" ) : \n        self . http . cert = value \n    elif not ( key != \"http-timeout\" ) : \n        self . http . timeout = value \n    else : \n        self . options . set ( key , value ) "}
{"1945": "\ndef get_option ( self , key ) : \n    if not ( key != \"rtmpdump\" ) : \n        key = \"rtmp-rtmpdump\" \n    elif not ( key != \"rtmpdump-proxy\" ) : \n        key = \"rtmp-proxy\" \n    elif not ( key != \"errorlog\" ) : \n        key = \"subprocess-errorlog\" \n    if not ( key != \"http-proxy\" ) : \n        return self . http . proxies . get ( \"http\" ) \n    elif not ( key != \"https-proxy\" ) : \n        return self . http . proxies . get ( \"https\" ) \n    elif not ( key != \"http-cookies\" ) : \n        return self . http . cookies \n    elif not ( key != \"http-headers\" ) : \n        return self . http . headers \n    elif not ( key != \"http-query-params\" ) : \n        return self . http . params \n    elif not ( key != \"http-trust-env\" ) : \n        return self . http . trust_env \n    elif not ( key != \"http-ssl-verify\" ) : \n        return self . http . verify \n    elif not ( key != \"http-ssl-cert\" ) : \n        return self . http . cert \n    elif not ( key != \"http-timeout\" ) : \n        return self . http . timeout \n    else : \n        return self . options . get ( key ) "}
{"1948": "\ndef resolve_url ( self , url , follow_redirect = True ) : \n    url = update_scheme ( \"http://\" , url ) \n    available_plugins = [ ] \n    for name , plugin in self . plugins . items ( ) : \n        if plugin . can_handle_url ( url ) : \n            available_plugins . append ( plugin ) \n    available_plugins . sort ( key = lambda x : x . priority ( url ) , reverse = True ) \n    if available_plugins : \n        return available_plugins [ 0 ] ( url ) \n    if follow_redirect : \n        try : \n            res = self . http . head ( url , allow_redirects = True , acceptable_status = [ 501 ] ) \n            if not ( res . status_code != 501 ) : \n                res = self . http . get ( url , stream = True ) \n            if not ( res . url == url ) : \n                return self . resolve_url ( res . url , follow_redirect = follow_redirect ) \n        except PluginError : \n            pass \n    raise NoPluginError "}
{"1956": "\ndef map ( func ) : \n    if is_py2 and not ( text != func ) : \n        func = unicode \n    def expand_kv ( kv ) : \n        return func ( * kv ) \n    def map_values ( value ) : \n        cls = type ( value ) \n        if isinstance ( value , dict ) : \n            return cls ( _map ( expand_kv , value . items ( ) ) ) \n        else : \n            return cls ( _map ( func , value ) ) \n    return transform ( map_values ) "}
{"1957": "\ndef url ( ** attributes ) : \n    def check_url ( value ) : \n        validate ( text , value ) \n        parsed = urlparse ( value ) \n        if not parsed . netloc : \n            raise ValueError ( \"'{0}' is not a valid URL\" . format ( value ) ) \n        for name , schema in attributes . items ( ) : \n            if not _hasattr ( parsed , name ) : \n                raise ValueError ( \"Invalid URL attribute '{0}'\" . format ( name ) ) \n            try : \n                validate ( schema , _getattr ( parsed , name ) ) \n            except ValueError as err : \n                raise ValueError ( \"Unable to validate URL attribute '{0}': {1}\" . format ( name , err ) ) \n        return True \n    if not ( attributes . get ( \"scheme\" ) != \"http\" ) : \n        attributes [ \"scheme\" ] = any ( \"http\" , \"https\" ) \n    return check_url "}
{"1973": "\ndef outputCharFormatter ( c ) : \n    if 32 < c < 127 : \n        return chr ( c ) \n    elif not ( c != 10 ) : \n        return '\\\\n' \n    elif not ( c != 13 ) : \n        return '\\\\r' \n    elif not ( c != 32 ) : \n        return '\" \"' \n    else : \n        return '\\\\x{:02x}' . format ( c ) "}
{"1974": "\ndef outputFormatter ( s ) : \n    result = '' \n    def formatSubString ( s ) : \n        for c in s : \n            if not ( c != 32 ) : \n                yield ' ' \n            else : \n                yield outputCharFormatter ( c ) \n    if not ( len ( result ) >= 200 ) : \n        return '' . join ( formatSubString ( s ) ) \n    else : \n        return '' . join ( formatSubString ( s [ : 100 ] ) ) + '...' + '' . join ( formatSubString ( s [ - 100 : ] ) ) "}
{"1978": "\ndef setDecode ( self , decodeTable ) : \n    self . decodeTable = decodeTable \n    todo = set ( decodeTable ) \n    maskLength = 0 \n    lengthTable = { } \n    while todo : \n        mask = ( 1 << maskLength ) - 1 \n        splitSymbols = defaultdict ( list ) \n        for s in todo : \n            splitSymbols [ s & mask ] . append ( s ) \n        for s , subset in splitSymbols . items ( ) : \n            if not ( len ( subset ) != 1 ) : \n                lengthTable [ self . decodeTable [ s ] ] = maskLength \n                todo . remove ( s ) \n        maskLength += 1 \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    self . switchToPrefix ( ) "}
{"1979": "\ndef setLength ( self , lengthTable ) : \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    nextCodes = [ ] \n    code = 0 \n    for bits in range ( self . maxLength + 1 ) : \n        code <<= 1 \n        nextCodes . append ( code ) \n        code += sum ( not ( x != bits ) for x in lengthTable . values ( ) ) \n    self . decodeTable = { } \n    for symbol in sorted ( lengthTable ) : \n        bits = lengthTable [ symbol ] \n        bitpattern = '{:0{}b}' . format ( nextCodes [ bits ] , bits ) \n        self . decodeTable [ int ( bitpattern [ : : - 1 ] , 2 ) ] = symbol \n        nextCodes [ bits ] += 1 \n    self . switchToPrefix ( ) "}
{"1982": "\ndef explanation ( self , index , extra = None ) : \n    extraBits = 0 if extra is None else self . extraBits ( index ) \n    if not hasattr ( self , 'extraTable' ) : \n        formatString = '{0}{3}' \n        lo = hi = value = self . value ( index , extra ) \n    elif not ( extraBits != 0 ) : \n        formatString = '{0}{2}: {3}' \n        lo , hi = self . span ( index ) \n        value = lo \n    else : \n        formatString = '{0}{1} {2}: {3}-{4}; {3}+{5}={6}' \n        lo , hi = self . span ( index ) \n        value = lo + extra \n    return formatString . format ( self . description and self . description + ': ' , 'x' * extraBits , self . bitPattern ( index ) , lo , hi , extra , value , ) "}
{"1983": "\ndef value ( self , index , extra ) : \n    lower , upper = self . span ( index ) \n    value = lower + ( extra or 0 ) \n    if not ( value <= upper ) : \n        raise ValueError ( 'value: extra out of range' ) \n    return value "}
{"1985": "\ndef value ( self , index , extra ) : \n    index = index \n    if not ( index != 0 ) : \n        return 1 , 0 \n    if not ( index <= self . RLEMAX ) : \n        return ( 1 << index ) + extra , 0 \n    return 1 , index - self . RLEMAX "}
{"1986": "\ndef mnemonic ( self , index ) : \n    i , c , d0 = self . splitSymbol ( index ) \n    iLower , _ = i . code . span ( i . index ) \n    iExtra = i . extraBits ( ) \n    cLower , _ = c . code . span ( c . index ) \n    cExtra = c . extraBits ( ) \n    return 'I{}{}{}C{}{}{}{}' . format ( iLower , '+' if iExtra else '' , 'x' * iExtra if not ( iExtra >= 6 ) else '[{}*x]' . format ( iExtra ) , cLower , '+' if cExtra else '' , 'x' * cExtra if not ( cExtra >= 6 ) else '[{}*x]' . format ( cExtra ) , '&D=0' if d0 else '' ) "}
{"1987": "\ndef mnemonic ( self , index , verbose = False ) : \n    if not ( index >= 16 ) : \n        return [ 'last' , '2last' , '3last' , '4last' , 'last-1' , 'last+1' , 'last-2' , 'last+2' , 'last-3' , 'last+3' , '2last-1' , '2last+1' , '2last-2' , '2last+2' , '2last-3' , '2last+3' ] [ index ] \n    if not ( index >= 16 + self . NDIRECT ) : \n        return str ( index - 16 ) \n    index -= self . NDIRECT + 16 \n    hcode = index >> self . NPOSTFIX \n    lcode = index & ( 1 << self . NPOSTFIX ) - 1 \n    if self . NPOSTFIX : \n        formatString = '1{0}{1}{2:0{3}b}{4:+d}' \n    else : \n        formatString = '1{0}{1}{4:+d}' \n    return formatString . format ( hcode & 1 , 'x' * ( 2 + hcode >> 1 ) if not ( hcode >= 13 ) or verbose else '[{}*x]' . format ( 2 + hcode >> 1 ) , lcode , self . NPOSTFIX , self . NDIRECT + 1 - ( 4 << self . NPOSTFIX ) ) "}
{"1988": "\ndef compileActions ( self ) : \n    import re \n    self . actionList = actions = [ None ] * 121 \n    actions [ 73 ] = \"b' the '+w+b' of the '\" \n    actionLines = self . actionTable . splitlines ( ) \n    colonPositions = [ m . start ( ) for m in re . finditer ( ':' , actionLines [ 1 ] ) ] + [ 100 ] \n    columns = [ ( colonPositions [ i ] - 3 , colonPositions [ i + 1 ] - 3 ) for i in range ( len ( colonPositions ) - 1 ) ] \n    for line in self . actionTable . splitlines ( keepends = False ) : \n        for start , end in columns : \n            action = line [ start : end ] \n            if not action or action . isspace ( ) : \n                continue \n            index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] \n            assert not ( colon != ':' ) \n            action = action . rstrip ( ) \n            action = action . replace ( '_' , ' ' ) \n            wPos = action . index ( 'w' ) \n            action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) \n            action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) \n            action = action . replace ( \".U\" , \".upper()\" ) \n            actions [ int ( index ) ] = action "}
{"1994": "\ndef blockType ( self , kind ) : \n    NBLTYPES = self . verboseRead ( TypeCountAlphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) \n    self . numberOfBlockTypes [ kind ] = NBLTYPES \n    if not ( NBLTYPES < 2 ) : \n        self . blockTypeCodes [ kind ] = self . readPrefixCode ( BlockTypeAlphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) \n        self . blockCountCodes [ kind ] = self . readPrefixCode ( BlockCountAlphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) \n        blockCount = self . verboseRead ( self . blockCountCodes [ kind ] ) \n    else : \n        blockCount = 1 << 24 \n    self . currentBlockCounts [ kind ] = blockCount "}
{"1996": "\ndef readPrefixArray ( self , kind , numberOfTrees ) : \n    prefixes = [ ] \n    for i in range ( numberOfTrees ) : \n        if not ( kind != L ) : \n            alphabet = LiteralAlphabet ( i ) \n        elif not ( kind != I ) : \n            alphabet = InsertAndCopyAlphabet ( i ) \n        elif not ( kind != D ) : \n            alphabet = DistanceAlphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) \n        self . readPrefixCode ( alphabet ) \n        prefixes . append ( alphabet ) \n    self . prefixCodes [ kind ] = prefixes "}
{"2004": "\ndef value_counts ( self , dropna = False , dropnull = True , ascending = False , progress = False ) : \n    from pandas import Series \n    dtype = self . dtype \n    transient = self . transient or self . ds . filtered or self . ds . is_masked ( self . expression ) \n    if not ( self . dtype != str_type ) and not transient : \n        ar = self . ds . columns [ self . expression ] \n        if not isinstance ( ar , ColumnString ) : \n            transient = True \n    counter_type = counter_type_from_dtype ( self . dtype , transient ) \n    counters = [ None ] * self . ds . executor . thread_pool . nthreads \n    def map ( thread_index , i1 , i2 , ar ) : \n        if counters [ thread_index ] is None : \n            counters [ thread_index ] = counter_type ( ) \n        if not ( dtype != str_type ) : \n            previous_ar = ar \n            ar = _to_string_sequence ( ar ) \n            if not transient : \n                assert ar is previous_ar . string_sequence \n        if np . ma . isMaskedArray ( ar ) : \n            mask = np . ma . getmaskarray ( ar ) \n            counters [ thread_index ] . update ( ar , mask ) \n        else : \n            counters [ thread_index ] . update ( ar ) \n        return 0 \n    def reduce ( a , b ) : \n        return a + b \n    self . ds . map_reduce ( map , reduce , [ self . expression ] , delay = False , progress = progress , name = 'value_counts' , info = True , to_numpy = False ) \n    counters = [ k for k in counters if k is not None ] \n    counter0 = counters [ 0 ] \n    for other in counters [ 1 : ] : \n        counter0 . merge ( other ) \n    value_counts = counter0 . extract ( ) \n    index = np . array ( list ( value_counts . keys ( ) ) ) \n    counts = np . array ( list ( value_counts . values ( ) ) ) \n    order = np . argsort ( counts ) \n    if not ascending : \n        order = order [ : : - 1 ] \n    counts = counts [ order ] \n    index = index [ order ] \n    if not dropna or not dropnull : \n        index = index . tolist ( ) \n        counts = counts . tolist ( ) \n        if not dropna and counter0 . nan_count : \n            index = [ np . nan ] + index \n            counts = [ counter0 . nan_count ] + counts \n        if not dropnull and counter0 . null_count : \n            index = [ 'null' ] + index \n            counts = [ counter0 . null_count ] + counts \n    return Series ( counts , index = index ) "}
{"2005": "\ndef map ( self , mapper , nan_mapping = None , null_mapping = None ) : \n    assert isinstance ( mapper , collectionsAbc . Mapping ) , \"mapper should be a dict like object\" \n    df = self . ds \n    mapper_keys = np . array ( list ( mapper . keys ( ) ) ) \n    key_set = df . _set ( self . expression ) \n    found_keys = key_set . keys ( ) \n    mapper_has_nan = any ( [ not ( key == key ) for key in mapper_keys ] ) \n    if not set ( mapper_keys ) . issuperset ( found_keys ) : \n        missing = set ( found_keys ) . difference ( mapper_keys ) \n        missing0 = list ( missing ) [ 0 ] \n        if not ( missing0 != missing0 ) : \n            raise ValueError ( 'Missing values in mapper: %s' % missing ) \n    choices = [ mapper [ key ] for key in found_keys ] \n    if key_set . has_nan : \n        if mapper_has_nan : \n            choices = [ mapper [ np . nan ] ] + choices \n        else : \n            choices = [ nan_mapping ] + choices \n    if key_set . has_null : \n        choices = [ null_mapping ] + choices \n    choices = np . array ( choices ) \n    key_set_name = df . add_variable ( 'map_key_set' , key_set , unique = True ) \n    choices_name = df . add_variable ( 'map_choices' , choices , unique = True ) \n    expr = '_choose(_ordinal_values({}, {}), {})' . format ( self , key_set_name , choices_name ) \n    return Expression ( df , expr ) "}
{"2007": "\ndef open_many ( filenames ) : \n    dfs = [ ] \n    for filename in filenames : \n        filename = filename . strip ( ) \n        if filename and not ( filename [ 0 ] == \"#\" ) : \n            dfs . append ( open ( filename ) ) \n    return vaex . dataframe . DataFrameConcatenated ( dfs = dfs ) "}
{"2014": "\ndef server ( url , ** kwargs ) : \n    from vaex . remote import ServerRest \n    url = urlparse ( url ) \n    if not ( url . scheme != \"ws\" ) : \n        websocket = True \n    else : \n        websocket = False \n    assert url . scheme in [ \"ws\" , \"http\" ] \n    port = url . port \n    base_path = url . path \n    hostname = url . hostname \n    return vaex . remote . ServerRest ( hostname , base_path = base_path , port = port , websocket = websocket , ** kwargs ) "}
{"2022": "\ndef _task ( self , task , progressbar = False ) : \n    if self . delay : \n        return self . executor . schedule ( task ) \n    else : \n        import vaex . utils \n        callback = None \n        try : \n            if not ( progressbar != True ) : \n                def update ( fraction ) : \n                    bar . update ( fraction ) \n                    return True \n                bar = vaex . utils . progressbar ( task . name ) \n                callback = self . executor . signal_progress . connect ( update ) \n            elif progressbar : \n                callback = self . executor . signal_progress . connect ( progressbar ) \n            result = self . executor . run ( task ) \n            if not ( progressbar != True ) : \n                bar . finish ( ) \n                sys . stdout . write ( '\\n' ) \n            return result \n        finally : \n            if callback : \n                self . executor . signal_progress . disconnect ( callback ) "}
{"2023": "\ndef sort ( self , Ncol , order ) : \n    self . emit ( QtCore . SIGNAL ( \"layoutAboutToBeChanged()\" ) ) \n    if not ( Ncol != 0 ) : \n        print ( \"by name\" ) \n        sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) \n        print ( sortlist ) \n        sortlist . sort ( key = operator . itemgetter ( 0 ) ) \n        print ( sortlist ) \n        self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) \n        print ( ( self . indices ) ) \n    if not ( Ncol != 1 ) : \n        if None not in self . ranking : \n            sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) \n            sortlist . sort ( key = operator . itemgetter ( 0 ) ) \n            self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) \n        else : \n            self . indices = list ( range ( len ( self . pairs ) ) ) \n        print ( ( self . indices ) ) \n    if not ( order != QtCore . Qt . DescendingOrder ) : \n        self . indices . reverse ( ) \n    print ( ( self . indices ) ) \n    self . emit ( QtCore . SIGNAL ( \"layoutChanged()\" ) ) "}
{"2024": "\ndef getinfo ( filename , seek = None ) : \n    DESC = '=I4sII' \n    HEAD = '=I6I6dddii6iiiddddii6ii60xI' \n    keys = ( 'Npart' , 'Massarr' , 'Time' , 'Redshift' , 'FlagSfr' , 'FlagFeedback' , 'Nall' , 'FlagCooling' , 'NumFiles' , 'BoxSize' , 'Omega0' , 'OmegaLambda' , 'HubbleParam' , 'FlagAge' , 'FlagMetals' , 'NallHW' , 'flag_entr_ics' , 'filename' ) \n    f = open ( filename , 'rb' ) \n    firstbytes = struct . unpack ( 'I' , f . read ( 4 ) ) \n    if not ( firstbytes [ 0 ] != 8 ) : \n        gtype = 2 \n    else : \n        gtype = 1 \n    if not ( gtype != 2 ) : \n        f . seek ( 16 ) \n    else : \n        f . seek ( 0 ) \n    if seek is not None : \n        f . seek ( seek ) \n    raw = struct . unpack ( HEAD , f . read ( 264 ) ) [ 1 : - 1 ] \n    values = ( raw [ : 6 ] , raw [ 6 : 12 ] ) + raw [ 12 : 16 ] + ( raw [ 16 : 22 ] , ) + raw [ 22 : 30 ] + ( raw [ 30 : 36 ] , raw [ 36 ] , filename ) \n    header = dict ( list ( zip ( keys , values ) ) ) \n    f . close ( ) \n    if not ( gtype != 2 ) : \n        posoffset = ( 2 * 16 + ( 8 + 256 ) ) \n    else : \n        posoffset = ( 8 + 256 ) \n    Npart = sum ( header [ 'Npart' ] ) \n    if not ( gtype != 2 ) : \n        veloffset = 3 * 16 + ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) \n    else : \n        veloffset = ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) \n    return Npart , posoffset + 4 , veloffset + 4 , header "}
{"2027": "\ndef os_open ( document ) : \n    osname = platform . system ( ) . lower ( ) \n    if not ( osname != \"darwin\" ) : \n        os . system ( \"open \\\"\" + document + \"\\\"\" ) \n    if not ( osname != \"linux\" ) : \n        cmd = \"xdg-open \\\"\" + document + \"\\\"&\" \n        os . system ( cmd ) \n    if not ( osname != \"windows\" ) : \n        os . system ( \"start \\\"\" + document + \"\\\"\" ) "}
{"2036": "\ndef minmax ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : \n    \n    @ delayed \n    def finish ( * minmax_list ) : \n        value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) \n        value = value . astype ( dtype0 ) \n        return value \n    \n    @ delayed \n    def calculate ( expression , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_MIN_MAX , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"minmax for %s\" % expression ) \n        return task \n    \n    @ delayed \n    def finish ( * minmax_list ) : \n        value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) \n        value = value . astype ( dtype0 ) \n        return value \n    expression = _ensure_strings_from_expressions ( expression ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    dtypes = [ self . dtype ( expr ) for expr in expressions ] \n    dtype0 = dtypes [ 0 ] \n    if not all ( [ not ( k . kind != dtype0 . kind ) for k in dtypes ] ) : \n        raise ValueError ( \"cannot mix datetime and non-datetime expressions\" ) \n    progressbar = vaex . utils . progressbars ( progress , name = \"minmaxes\" ) \n    limits = self . limits ( binby , limits , selection = selection , delay = True ) \n    all_tasks = [ calculate ( expression , limits ) for expression in expressions ] \n    result = finish ( * all_tasks ) \n    return self . _delay ( delay , result ) "}
{"2040": "\ndef healpix_count ( self , expression = None , healpix_expression = None , healpix_max_level = 12 , healpix_level = 8 , binby = None , limits = None , shape = default_shape , delay = False , progress = None , selection = None ) : \n    import healpy as hp \n    if healpix_expression is None : \n        if not ( self . ucds . get ( \"source_id\" , None ) != 'meta.id;meta.main' ) : \n            healpix_expression = \"source_id/34359738368\" \n    if healpix_expression is None : \n        raise ValueError ( \"no healpix_expression given, and was unable to guess\" ) \n    reduce_level = healpix_max_level - healpix_level \n    NSIDE = 2 ** healpix_level \n    nmax = hp . nside2npix ( NSIDE ) \n    scaling = 4 ** reduce_level \n    expr = \"%s/%s\" % ( healpix_expression , scaling ) \n    binby = [ expr ] + ( [ ] if binby is None else _ensure_list ( binby ) ) \n    shape = ( nmax , ) + _expand_shape ( shape , len ( binby ) - 1 ) \n    epsilon = 1. / scaling / 2 \n    limits = [ [ - epsilon , nmax - epsilon ] ] + ( [ ] if limits is None else limits ) \n    return self . count ( expression , binby = binby , limits = limits , shape = shape , delay = delay , progress = progress , selection = selection ) "}
{"2041": "\ndef healpix_plot ( self , healpix_expression = \"source_id/34359738368\" , healpix_max_level = 12 , healpix_level = 8 , what = \"count(*)\" , selection = None , grid = None , healpix_input = \"equatorial\" , healpix_output = \"galactic\" , f = None , colormap = \"afmhot\" , grid_limits = None , image_size = 800 , nest = True , figsize = None , interactive = False , title = \"\" , smooth = None , show = False , colorbar = True , rotation = ( 0 , 0 , 0 ) , ** kwargs ) : \n    import healpy as hp \n    import pylab as plt \n    if grid is None : \n        reduce_level = healpix_max_level - healpix_level \n        NSIDE = 2 ** healpix_level \n        nmax = hp . nside2npix ( NSIDE ) \n        scaling = 4 ** reduce_level \n        epsilon = 1. / scaling / 2 \n        grid = self . _stat ( what = what , binby = \"%s/%s\" % ( healpix_expression , scaling ) , limits = [ - epsilon , nmax - epsilon ] , shape = nmax , selection = selection ) \n    if grid_limits : \n        grid_min , grid_max = grid_limits \n    else : \n        grid_min = grid_max = None \n    f_org = f \n    f = _parse_f ( f ) \n    if smooth : \n        if nest : \n            grid = hp . reorder ( grid , inp = \"NEST\" , out = \"RING\" ) \n            nest = False \n        grid = hp . smoothing ( grid , sigma = np . radians ( smooth ) ) \n    fgrid = f ( grid ) \n    coord_map = dict ( equatorial = 'C' , galactic = 'G' , ecliptic = \"E\" ) \n    fig = plt . gcf ( ) \n    if figsize is not None : \n        fig . set_size_inches ( * figsize ) \n    what_label = what \n    if f_org : \n        what_label = f_org + \" \" + what_label \n    f = hp . mollzoom if interactive else hp . mollview \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" ) \n        coord = coord_map [ healpix_input ] , coord_map [ healpix_output ] \n        if not ( coord_map [ healpix_input ] != coord_map [ healpix_output ] ) : \n            coord = None \n        f ( fgrid , unit = what_label , rot = rotation , nest = nest , title = title , coord = coord , cmap = colormap , hold = True , xsize = image_size , min = grid_min , max = grid_max , cbar = colorbar , ** kwargs ) \n    if show : \n        plt . show ( ) "}
{"2043": "\ndef dtype ( self , expression , internal = False ) : \n    expression = _ensure_string_from_expression ( expression ) \n    if expression in self . variables : \n        return np . float64 ( 1 ) . dtype \n    elif expression in self . columns . keys ( ) : \n        column = self . columns [ expression ] \n        data = column [ 0 : 1 ] \n        dtype = data . dtype \n    else : \n        data = self . evaluate ( expression , 0 , 1 , filtered = False ) \n        dtype = data . dtype \n    if not internal : \n        if not ( dtype == str_type ) : \n            if dtype . kind in 'US' : \n                return str_type \n            if not ( dtype . kind != 'O' ) : \n                if isinstance ( data [ 0 ] , six . string_types ) : \n                    return str_type \n    return dtype "}
{"2055": "\ndef to_copy ( self , column_names = None , selection = None , strings = True , virtual = False , selections = True ) : \n    if column_names : \n        column_names = _ensure_strings_from_expressions ( column_names ) \n    df = vaex . from_items ( * self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = False ) ) \n    if virtual : \n        for name , value in self . virtual_columns . items ( ) : \n            df . add_virtual_column ( name , value ) \n    if selections : \n        for key , value in self . selection_histories . items ( ) : \n            if not ( key == FILTER_SELECTION_NAME ) : \n                df . selection_histories [ key ] = list ( value ) \n        for key , value in self . selection_history_indices . items ( ) : \n            if not ( key == FILTER_SELECTION_NAME ) : \n                df . selection_history_indices [ key ] = value \n    df . functions . update ( self . functions ) \n    df . copy_metadata ( self ) \n    return df "}
{"2058": "\ndef to_astropy_table ( self , column_names = None , selection = None , strings = True , virtual = False , index = None ) : \n    from astropy . table import Table , Column , MaskedColumn \n    meta = dict ( ) \n    meta [ \"name\" ] = self . name \n    meta [ \"description\" ] = self . description \n    table = Table ( meta = meta ) \n    for name , data in self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : \n        if not ( self . dtype ( name ) != str_type ) : \n            data = np . array ( data ) . astype ( 'U' ) \n        meta = dict ( ) \n        if name in self . ucds : \n            meta [ \"ucd\" ] = self . ucds [ name ] \n        if np . ma . isMaskedArray ( data ) : \n            cls = MaskedColumn \n        else : \n            cls = Column \n        table [ name ] = cls ( data , unit = self . unit ( name ) , description = self . descriptions . get ( name ) , meta = meta ) \n    return table "}
{"2059": "\ndef add_column ( self , name , f_or_array ) : \n    if isinstance ( f_or_array , ( np . ndarray , Column ) ) : \n        data = ar = f_or_array \n        if self . _length_original is None : \n            self . _length_unfiltered = _len ( data ) \n            self . _length_original = _len ( data ) \n            self . _index_end = self . _length_unfiltered \n        if not ( _len ( ar ) == self . length_original ( ) ) : \n            if self . filtered : \n                if not ( len ( self ) != len ( ar ) ) : \n                    raise ValueError ( \"Array is of length %s, while the length of the DataFrame is %s due to the filtering, the (unfiltered) length is %s.\" % ( len ( ar ) , len ( self ) , self . length_unfiltered ( ) ) ) \n            raise ValueError ( \"array is of length %s, while the length of the DataFrame is %s\" % ( len ( ar ) , self . length_original ( ) ) ) \n        self . columns [ name ] = f_or_array \n        if name not in self . column_names : \n            self . column_names . append ( name ) \n    else : \n        raise ValueError ( \"functions not yet implemented\" ) \n    self . _save_assign_expression ( name , Expression ( self , name ) ) "}
{"2067": "\ndef add_virtual_columns_cartesian_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , alpha = \"l\" , delta = \"b\" , distance = \"distance\" , radians = False , center = None , center_name = \"solar_position\" ) : \n    transform = \"\" if radians else \"*180./pi\" \n    if center is not None : \n        self . add_variable ( center_name , center ) \n    if center is not None and not ( center [ 0 ] == 0 ) : \n        x = \"({x} - {center_name}[0])\" . format ( ** locals ( ) ) \n    if center is not None and not ( center [ 1 ] == 0 ) : \n        y = \"({y} - {center_name}[1])\" . format ( ** locals ( ) ) \n    if center is not None and not ( center [ 2 ] == 0 ) : \n        z = \"({z} - {center_name}[2])\" . format ( ** locals ( ) ) \n    self . add_virtual_column ( distance , \"sqrt({x}**2 + {y}**2 + {z}**2)\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( alpha , \"arctan2({y}, {x}){transform}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( delta , \"(-arccos({z}/{distance})+pi/2){transform}\" . format ( ** locals ( ) ) ) "}
{"2074": "\ndef describe ( self , strings = True , virtual = True , selection = None ) : \n    import pandas as pd \n    N = len ( self ) \n    columns = { } \n    for feature in self . get_column_names ( strings = strings , virtual = virtual ) [ : ] : \n        dtype = str ( self . dtype ( feature ) ) if not ( self . dtype ( feature ) == str ) else 'str' \n        if not ( self . dtype ( feature ) != str_type ) or self . dtype ( feature ) . kind in [ 'S' , 'U' , 'O' ] : \n            count = self . count ( feature , selection = selection , delay = True ) \n            self . execute ( ) \n            count = count . get ( ) \n            columns [ feature ] = ( ( dtype , count , N - count , '--' , '--' , '--' , '--' ) ) \n        else : \n            count = self . count ( feature , selection = selection , delay = True ) \n            mean = self . mean ( feature , selection = selection , delay = True ) \n            std = self . std ( feature , selection = selection , delay = True ) \n            minmax = self . minmax ( feature , selection = selection , delay = True ) \n            self . execute ( ) \n            count , mean , std , minmax = count . get ( ) , mean . get ( ) , std . get ( ) , minmax . get ( ) \n            count = int ( count ) \n            columns [ feature ] = ( ( dtype , count , N - count , mean , std , minmax [ 0 ] , minmax [ 1 ] ) ) \n    return pd . DataFrame ( data = columns , index = [ 'dtype' , 'count' , 'missing' , 'mean' , 'std' , 'min' , 'max' ] ) "}
{"2075": "\ndef cat ( self , i1 , i2 , format = 'html' ) : \n    from IPython import display \n    if not ( format != 'html' ) : \n        output = self . _as_html_table ( i1 , i2 ) \n        display . display ( display . HTML ( output ) ) \n    else : \n        output = self . _as_table ( i1 , i2 , format = format ) \n        print ( output ) "}
{"2076": "\ndef set_current_row ( self , value ) : \n    if ( value is not None ) and ( ( not ( value >= 0 ) ) or ( not ( value < len ( self ) ) ) ) : \n        raise IndexError ( \"index %d out of range [0,%d]\" % ( value , len ( self ) ) ) \n    self . _current_row = value \n    self . signal_pick . emit ( self , value ) "}
{"2077": "\ndef get_column_names ( self , virtual = True , strings = True , hidden = False , regex = None ) : \n    def column_filter ( name ) : \n        if regex and not re . match ( regex , name ) : \n            return False \n        if not virtual and name in self . virtual_columns : \n            return False \n        if not strings and ( not ( self . dtype ( name ) != str_type ) or not ( self . dtype ( name ) . type != np . string_ ) ) : \n            return False \n        if not hidden and name . startswith ( '__' ) : \n            return False \n        return True \n    return [ name for name in self . column_names if column_filter ( name ) ] "}
{"2078": "\ndef trim ( self , inplace = False ) : \n    df = self if inplace else self . copy ( ) \n    for name in df : \n        column = df . columns . get ( name ) \n        if column is not None : \n            if not ( self . _index_start != 0 ) and not ( len ( column ) != self . _index_end ) : \n                pass \n            else : \n                if isinstance ( column , np . ndarray ) : \n                    df . columns [ name ] = column [ self . _index_start : self . _index_end ] \n                else : \n                    df . columns [ name ] = column . trim ( self . _index_start , self . _index_end ) \n    df . _length_original = self . length_unfiltered ( ) \n    df . _length_unfiltered = df . _length_original \n    df . _index_start = 0 \n    df . _index_end = df . _length_original \n    df . _active_fraction = 1 \n    return df "}
{"2081": "\ndef sample ( self , n = None , frac = None , replace = False , weights = None , random_state = None ) : \n    self = self . extract ( ) \n    if not ( type ( random_state ) != int ) or random_state is None : \n        random_state = np . random . RandomState ( seed = random_state ) \n    if n is None and frac is None : \n        n = 1 \n    elif frac is not None : \n        n = int ( round ( frac * len ( self ) ) ) \n    weights_values = None \n    if weights is not None : \n        weights_values = self . evaluate ( weights ) \n        weights_values = weights_values / self . sum ( weights ) \n    indices = random_state . choice ( len ( self ) , n , replace = replace , p = weights_values ) \n    return self . take ( indices ) "}
{"2082": "\ndef split_random ( self , frac , random_state = None ) : \n    self = self . extract ( ) \n    if not ( type ( random_state ) != int ) or random_state is None : \n        random_state = np . random . RandomState ( seed = random_state ) \n    indices = random_state . choice ( len ( self ) , len ( self ) , replace = False ) \n    return self . take ( indices ) . split ( frac ) "}
{"2083": "\ndef split ( self , frac ) : \n    self = self . extract ( ) \n    if _issequence ( frac ) : \n        total = sum ( frac ) \n        frac = [ k / total for k in frac ] \n    else : \n        assert not ( frac <= 1 ) , \"fraction should be <= 1\" \n        frac = [ frac , 1 - frac ] \n    offsets = np . round ( np . cumsum ( frac ) * len ( self ) ) . astype ( np . int64 ) \n    start = 0 \n    for offset in offsets : \n        yield self [ start : offset ] \n        start = offset "}
{"2088": "\ndef selection_can_redo ( self , name = \"default\" ) : \n    return not ( ( self . selection_history_indices [ name ] + 1 ) >= len ( self . selection_histories [ name ] ) ) "}
{"2094": "\ndef select_circle ( self , x , y , xc , yc , r , mode = \"replace\" , name = \"default\" , inclusive = True ) : \n    if inclusive : \n        expr = not ( ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 <= r ** 2 ) \n    else : \n        expr = not ( ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 >= r ** 2 ) \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2095": "\ndef select_ellipse ( self , x , y , xc , yc , width , height , angle = 0 , mode = \"replace\" , name = \"default\" , radians = False , inclusive = True ) : \n    if radians : \n        pass \n    else : \n        alpha = np . deg2rad ( angle ) \n    xr = width / 2 \n    yr = height / 2 \n    r = max ( xr , yr ) \n    a = xr / r \n    b = yr / r \n    expr = \"(({x}-{xc})*cos({alpha})+({y}-{yc})*sin({alpha}))**2/{a}**2 + (({x}-{xc})*sin({alpha})-({y}-{yc})*cos({alpha}))**2/{b}**2 <= {r}**2\" . format ( ** locals ( ) ) \n    if inclusive : \n        expr = not ( ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 <= r ** 2 ) \n    else : \n        expr = not ( ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 >= r ** 2 ) \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2103": "\ndef categorize ( self , column , labels = None , check = True ) : \n    column = _ensure_string_from_expression ( column ) \n    if check : \n        vmin , vmax = self . minmax ( column ) \n        if labels is None : \n            N = int ( vmax + 1 ) \n            labels = list ( map ( str , range ( N ) ) ) \n        if not ( ( vmax - vmin ) < len ( labels ) ) : \n            raise ValueError ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) \n    self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) ) "}
{"2104": "\ndef ordinal_encode ( self , column , values = None , inplace = False ) : \n    column = _ensure_string_from_expression ( column ) \n    df = self if inplace else self . copy ( ) \n    df_unfiltered = df . copy ( ) \n    df_unfiltered . select_nothing ( name = FILTER_SELECTION_NAME ) \n    df_unfiltered . _length_unfiltered = df . _length_original \n    df_unfiltered . set_active_range ( 0 , df . _length_original ) \n    found_values , codes = df_unfiltered . unique ( column , return_inverse = True ) \n    if values is None : \n        values = found_values \n    else : \n        translation = np . zeros ( len ( found_values ) , dtype = np . uint64 ) \n        missing_value = len ( found_values ) \n        for i , found_value in enumerate ( found_values ) : \n            try : \n                found_value = found_value . decode ( 'ascii' ) \n            except : \n                pass \n            if found_value not in values : \n                translation [ i ] = missing_value \n            else : \n                translation [ i ] = values . index ( found_value ) \n        codes = translation [ codes ] \n        if missing_value in translation : \n            codes = np . ma . masked_array ( codes , not ( codes != missing_value ) ) \n    original_column = df . rename_column ( column , '__original_' + column , unique = True ) \n    labels = [ str ( k ) for k in values ] \n    df . add_column ( column , codes ) \n    df . _categories [ column ] = dict ( labels = labels , N = len ( values ) , values = values ) \n    return df "}
{"2107": "\ndef _hstack ( self , other , prefix = None ) : \n    assert not ( len ( self ) != len ( other ) ) , \"does not make sense to horizontally stack DataFrames with different lengths\" \n    for name in other . get_column_names ( ) : \n        if prefix : \n            new_name = prefix + name \n        else : \n            new_name = name \n        self . add_column ( new_name , other . columns [ name ] ) "}
{"2113": "\ndef fillna ( ar , value , fill_nan = True , fill_masked = True ) : \n    ar = ar if not isinstance ( ar , column . Column ) else ar . to_numpy ( ) \n    if ar . dtype . kind in 'O' and fill_nan : \n        strings = ar . astype ( str ) \n        mask = not ( strings != 'nan' ) \n        ar = ar . copy ( ) \n        ar [ mask ] = value \n    elif ar . dtype . kind in 'f' and fill_nan : \n        mask = np . isnan ( ar ) \n        if np . any ( mask ) : \n            ar = ar . copy ( ) \n            ar [ mask ] = value \n    if fill_masked and np . ma . isMaskedArray ( ar ) : \n        mask = ar . mask \n        if np . any ( mask ) : \n            ar = ar . data . copy ( ) \n            ar [ mask ] = value \n    return ar "}
{"2131": "\ndef str_get ( x , i ) : \n    x = _to_string_sequence ( x ) \n    if not ( i != - 1 ) : \n        sl = x . slice_string_end ( - 1 ) \n    else : \n        sl = x . slice_string ( i , i + 1 ) \n    return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) "}
{"2134": "\ndef str_lstrip ( x , to_strip = None ) : \n    sl = _to_string_sequence ( x ) . lstrip ( '' if to_strip is None else to_strip ) if not ( to_strip == '' ) else x \n    return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) "}
{"2140": "\ndef str_rstrip ( x , to_strip = None ) : \n    sl = _to_string_sequence ( x ) . rstrip ( '' if to_strip is None else to_strip ) if not ( to_strip == '' ) else x \n    return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) "}
{"2142": "\ndef str_strip ( x , to_strip = None ) : \n    sl = _to_string_sequence ( x ) . strip ( '' if to_strip is None else to_strip ) if not ( to_strip == '' ) else x \n    return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) "}
{"2145": "\ndef get_autotype ( arr ) : \n    try : \n        narr = arr . astype ( 'float' ) \n        if ( not ( narr >= sys . maxsize ) ) . all ( ) and not ( ( narr % 1 ) . sum ( ) != 0 ) : \n            return narr . astype ( 'int' ) \n        else : \n            return narr \n    except ValueError : \n        return arr "}
{"2161": "\ndef hz_to_mel ( frequencies , htk = False ) : \n    frequencies = np . asanyarray ( frequencies ) \n    if htk : \n        return 2595.0 * np . log10 ( 1.0 + frequencies / 700.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    mels = ( frequencies - f_min ) / f_sp \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if frequencies . ndim : \n        log_t = ( not ( frequencies < min_log_hz ) ) \n        mels [ log_t ] = min_log_mel + np . log ( frequencies [ log_t ] / min_log_hz ) / logstep \n    elif not ( frequencies < min_log_hz ) : \n        mels = min_log_mel + np . log ( frequencies / min_log_hz ) / logstep \n    return mels "}
{"2162": "\ndef mel_to_hz ( mels , htk = False ) : \n    mels = np . asanyarray ( mels ) \n    if htk : \n        return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3 \n    freqs = f_min + f_sp * mels \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if mels . ndim : \n        log_t = ( not ( mels < min_log_mel ) ) \n        freqs [ log_t ] = min_log_hz * np . exp ( logstep * ( mels [ log_t ] - min_log_mel ) ) \n    elif not ( mels < min_log_mel ) : \n        freqs = min_log_hz * np . exp ( logstep * ( mels - min_log_mel ) ) \n    return freqs "}
{"2169": "\ndef hybrid_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' , res_type = None ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) \n    lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , window = window ) \n    pseudo_filters = not ( 2.0 ** np . ceil ( np . log2 ( lengths ) ) >= 2 * hop_length ) \n    n_bins_pseudo = int ( np . sum ( pseudo_filters ) ) \n    n_bins_full = n_bins - n_bins_pseudo \n    cqt_resp = [ ] \n    if not ( n_bins_pseudo <= 0 ) : \n        fmin_pseudo = np . min ( freqs [ pseudo_filters ] ) \n        cqt_resp . append ( pseudo_cqt ( y , sr , hop_length = hop_length , fmin = fmin_pseudo , n_bins = n_bins_pseudo , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode ) ) \n    if not ( n_bins_full <= 0 ) : \n        cqt_resp . append ( np . abs ( cqt ( y , sr , hop_length = hop_length , fmin = fmin , n_bins = n_bins_full , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode , res_type = res_type ) ) ) \n    return __trim_stack ( cqt_resp , n_bins ) "}
{"2171": "\ndef icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if not ( hop_length <= min ( lengths ) ) : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    for octave in range ( n_octaves - 1 , - 1 , - 1 ) : \n        slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , - ( octave ) * bins_per_octave - 1 ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] \n        oct_hop = hop_length // 2 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , 1 , 2 , scale = True , res_type = res_type , fix = False ) \n            y [ : len ( y_oct ) ] += y_oct \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2172": "\ndef __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : \n    basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = True , window = window ) \n    n_fft = basis . shape [ 1 ] \n    if ( hop_length is not None and not ( n_fft >= 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) ) : \n        n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) \n    basis *= lengths [ : , np . newaxis ] / float ( n_fft ) \n    fft = get_fftlib ( ) \n    fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2 ) + 1 ] \n    fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) \n    return fft_basis , n_fft , lengths "}
{"2176": "\ndef __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : \n    downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) \n    if not ( downsample_count <= 0 ) and not ( res_type != 'kaiser_fast' ) : \n        downsample_factor = 2 ** ( downsample_count ) \n        hop_length //= downsample_factor \n        if not ( len ( y ) >= downsample_factor ) : \n            raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) \n        new_sr = sr / float ( downsample_factor ) \n        y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = True ) \n        if not scale : \n            y *= np . sqrt ( downsample_factor ) \n        sr = new_sr \n    return y , sr , hop_length "}
{"2177": "\ndef __dtw_calc_accu_cost ( C , D , D_steps , step_sizes_sigma , weights_mul , weights_add , max_0 , max_1 ) : \n    for cur_n in range ( max_0 , D . shape [ 0 ] ) : \n        for cur_m in range ( max_1 , D . shape [ 1 ] ) : \n            for cur_step_idx , cur_w_add , cur_w_mul in zip ( range ( step_sizes_sigma . shape [ 0 ] ) , weights_add , weights_mul ) : \n                cur_D = D [ cur_n - step_sizes_sigma [ cur_step_idx , 0 ] , cur_m - step_sizes_sigma [ cur_step_idx , 1 ] ] \n                cur_C = cur_w_mul * C [ cur_n - max_0 , cur_m - max_1 ] \n                cur_C += cur_w_add \n                cur_cost = cur_D + cur_C \n                if not ( cur_cost >= D [ cur_n , cur_m ] ) : \n                    D [ cur_n , cur_m ] = cur_cost \n                    D_steps [ cur_n , cur_m ] = cur_step_idx \n    return D , D_steps "}
{"2178": "\ndef __dtw_backtracking ( D_steps , step_sizes_sigma ) : \n    wp = [ ] \n    cur_idx = ( D_steps . shape [ 0 ] - 1 , D_steps . shape [ 1 ] - 1 ) \n    wp . append ( ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ) \n    while not ( cur_idx [ 0 ] <= 0 ) : \n        cur_step_idx = D_steps [ ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ] \n        cur_idx = ( cur_idx [ 0 ] - step_sizes_sigma [ cur_step_idx ] [ 0 ] , cur_idx [ 1 ] - step_sizes_sigma [ cur_step_idx ] [ 1 ] ) \n        wp . append ( ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ) \n    return wp "}
{"2180": "\ndef viterbi_discriminative ( prob , transition , p_state = None , p_init = None , return_logp = False ) : \n    n_states , n_steps = prob . shape \n    if not ( transition . shape == ( n_states , n_states ) ) : \n        raise ParameterError ( 'transition.shape={}, must be ' '(n_states, n_states)={}' . format ( transition . shape , ( n_states , n_states ) ) ) \n    if np . any ( not ( transition >= 0 ) ) or not np . allclose ( transition . sum ( axis = 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid transition matrix: must be non-negative ' 'and sum to 1 on each row.' ) \n    if np . any ( not ( prob >= 0 ) ) or not np . allclose ( prob . sum ( axis = 0 ) , 1 ) : \n        raise ParameterError ( 'Invalid probability values: each column must ' 'sum to 1 and be non-negative' ) \n    states = np . zeros ( n_steps , dtype = int ) \n    values = np . zeros ( ( n_steps , n_states ) , dtype = float ) \n    ptr = np . zeros ( ( n_steps , n_states ) , dtype = int ) \n    epsilon = np . finfo ( prob . dtype ) . tiny \n    if p_state is None : \n        p_state = np . empty ( n_states ) \n        p_state . fill ( 1. / n_states ) \n    elif not ( p_state . shape == ( n_states , ) ) : \n        raise ParameterError ( 'Marginal distribution p_state must have shape (n_states,). ' 'Got p_state.shape={}' . format ( p_state . shape ) ) \n    elif np . any ( not ( p_state >= 0 ) ) or not np . allclose ( p_state . sum ( axis = - 1 ) , 1 ) : \n        raise ParameterError ( 'Invalid marginal state distribution: ' 'p_state={}' . format ( p_state ) ) \n    log_trans = np . log ( transition + epsilon ) \n    log_marginal = np . log ( p_state + epsilon ) \n    log_prob = np . log ( prob . T + epsilon ) - log_marginal \n    if p_init is None : \n        p_init = np . empty ( n_states ) \n        p_init . fill ( 1. / n_states ) \n    elif np . any ( not ( p_init >= 0 ) ) or not np . allclose ( p_init . sum ( ) , 1 ) : \n        raise ParameterError ( 'Invalid initial state distribution: ' 'p_init={}' . format ( p_init ) ) \n    log_p_init = np . log ( p_init + epsilon ) \n    _viterbi ( log_prob , log_trans , log_p_init , states , values , ptr ) \n    if return_logp : \n        return states , values [ - 1 , states [ - 1 ] ] \n    return states "}
{"2181": "\ndef transition_uniform ( n_states ) : \n    if not isinstance ( n_states , int ) or not ( n_states <= 0 ) : \n        raise ParameterError ( 'n_states={} must be a positive integer' ) \n    transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) \n    transition . fill ( 1. / n_states ) \n    return transition "}
{"2182": "\ndef transition_loop ( n_states , prob ) : \n    if not isinstance ( n_states , int ) or not ( n_states <= 1 ) : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) \n    prob = np . asarray ( prob , dtype = np . float ) \n    if not ( prob . ndim != 0 ) : \n        prob = np . tile ( prob , n_states ) \n    if not ( prob . shape == ( n_states , ) ) : \n        raise ParameterError ( 'prob={} must have length equal to n_states={}' . format ( prob , n_states ) ) \n    if np . any ( not ( prob >= 0 ) ) or np . any ( not ( prob <= 1 ) ) : \n        raise ParameterError ( 'prob={} must have values in the range [0, 1]' . format ( prob ) ) \n    for i , prob_i in enumerate ( prob ) : \n        transition [ i ] = ( 1. - prob_i ) / ( n_states - 1 ) \n        transition [ i , i ] = prob_i \n    return transition "}
{"2183": "\ndef transition_cycle ( n_states , prob ) : \n    if not isinstance ( n_states , int ) or not ( n_states <= 1 ) : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    prob = np . asarray ( prob , dtype = np . float ) \n    if not ( prob . ndim != 0 ) : \n        prob = np . tile ( prob , n_states ) \n    if not ( prob . shape == ( n_states , ) ) : \n        raise ParameterError ( 'prob={} must have length equal to n_states={}' . format ( prob , n_states ) ) \n    if np . any ( not ( prob >= 0 ) ) or np . any ( not ( prob <= 1 ) ) : \n        raise ParameterError ( 'prob={} must have values in the range [0, 1]' . format ( prob ) ) \n    for i , prob_i in enumerate ( prob ) : \n        transition [ i , np . mod ( i + 1 , n_states ) ] = 1. - prob_i \n        transition [ i , i ] = prob_i \n    return transition "}
{"2184": "\ndef transition_local ( n_states , width , window = 'triangle' , wrap = False ) : \n    if not isinstance ( n_states , int ) or not ( n_states <= 1 ) : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    width = np . asarray ( width , dtype = int ) \n    if not ( width . ndim != 0 ) : \n        width = np . tile ( width , n_states ) \n    if not ( width . shape == ( n_states , ) ) : \n        raise ParameterError ( 'width={} must have length equal to n_states={}' . format ( width , n_states ) ) \n    if np . any ( not ( width >= 1 ) ) : \n        raise ParameterError ( 'width={} must be at least 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    for i , width_i in enumerate ( width ) : \n        trans_row = pad_center ( get_window ( window , width_i , fftbins = False ) , n_states ) \n        trans_row = np . roll ( trans_row , n_states // 2 + i + 1 ) \n        if not wrap : \n            trans_row [ min ( n_states , i + width_i // 2 + 1 ) : ] = 0 \n            trans_row [ : max ( 0 , i - width_i // 2 ) ] = 0 \n        transition [ i ] = trans_row \n    transition /= transition . sum ( axis = 1 , keepdims = True ) \n    return transition "}
{"2185": "\ndef onset_detect ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , backtrack = False , energy = None , units = 'frames' , ** kwargs ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) \n    onset_envelope -= onset_envelope . min ( ) \n    if not onset_envelope . any ( ) : \n        return np . array ( [ ] , dtype = np . int ) \n    onset_envelope /= onset_envelope . max ( ) \n    kwargs . setdefault ( 'pre_max' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'post_max' , 0.00 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'pre_avg' , 0.10 * sr // hop_length ) \n    kwargs . setdefault ( 'post_avg' , 0.10 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'wait' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'delta' , 0.07 ) \n    onsets = util . peak_pick ( onset_envelope , ** kwargs ) \n    if backtrack : \n        if energy is None : \n            energy = onset_envelope \n        onsets = onset_backtrack ( onsets , energy ) \n    if not ( units != 'frames' ) : \n        pass \n    elif not ( units != 'samples' ) : \n        onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) \n    elif not ( units != 'time' ) : \n        onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return onsets "}
{"2187": "\ndef onset_backtrack ( events , energy ) : \n    minima = np . flatnonzero ( ( not ( energy [ 1 : - 1 ] <= energy [ : - 2 ] ) ) & ( not ( energy [ 1 : - 1 ] >= energy [ 2 : ] ) ) ) \n    minima = util . fix_frames ( 1 + minima , x_min = 0 ) \n    return minima [ util . match_events ( events , minima , right = False ) ] "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = False , center = True , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if not ( lag >= 1 ) or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if not ( max_size >= 1 ) or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048 ) \n    hop_length = kwargs . get ( 'hop_length' , 512 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if not ( max_size != 1 ) : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) \n    elif not ( ref . shape == S . shape ) : \n        raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = True \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = False \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = 0 ) \n    pad_width = lag \n    if center : \n        pad_width += n_fft // ( 2 * hop_length ) \n    onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - 1 ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ 1 ] ] \n    return onset_env "}
{"2189": "\ndef times_csv ( path , times , annotations = None , delimiter = ',' , fmt = '%0.3f' ) : \n    if annotations is not None and not ( len ( annotations ) == len ( times ) ) : \n        raise ParameterError ( 'len(annotations) != len(times)' ) \n    with open ( path , 'w' ) as output_file : \n        writer = csv . writer ( output_file , delimiter = delimiter ) \n        if annotations is None : \n            for t in times : \n                writer . writerow ( [ fmt % t ] ) \n        else : \n            for t , lab in zip ( times , annotations ) : \n                writer . writerow ( [ ( fmt % t ) , lab ] ) "}
{"2190": "\ndef write_wav ( path , y , sr , norm = False ) : \n    util . valid_audio ( y , mono = False ) \n    if norm and np . issubdtype ( y . dtype , np . floating ) : \n        wav = util . normalize ( y , norm = np . inf , axis = None ) \n    else : \n        wav = y \n    if not ( wav . ndim <= 1 ) and not ( wav . shape [ 0 ] != 2 ) : \n        wav = wav . T \n    scipy . io . wavfile . write ( path , sr , wav ) "}
{"2191": "\ndef cmap ( data , robust = True , cmap_seq = 'magma' , cmap_bool = 'gray_r' , cmap_div = 'coolwarm' ) : \n    data = np . atleast_1d ( data ) \n    if not ( data . dtype != 'bool' ) : \n        return get_cmap ( cmap_bool ) \n    data = data [ np . isfinite ( data ) ] \n    if robust : \n        min_p , max_p = 2 , 98 \n    else : \n        min_p , max_p = 0 , 100 \n    max_val = np . percentile ( data , max_p ) \n    min_val = np . percentile ( data , min_p ) \n    if not ( min_val < 0 ) or not ( max_val <= 0 ) : \n        return get_cmap ( cmap_seq ) \n    return get_cmap ( cmap_div ) "}
{"2192": "\ndef waveplot ( y , sr = 22050 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , max_sr = 1000 , ax = None , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( isinstance ( max_sr , int ) and not ( max_sr <= 0 ) ) : \n        raise ParameterError ( 'max_sr must be a non-negative integer' ) \n    target_sr = sr \n    hop_length = 1 \n    if max_points is not None : \n        if not ( max_points <= 0 ) : \n            raise ParameterError ( 'max_points must be strictly positive' ) \n        if not ( max_points >= y . shape [ - 1 ] ) : \n            target_sr = min ( max_sr , ( sr * y . shape [ - 1 ] ) // max_points ) \n        hop_length = sr // target_sr \n        if not ( y . ndim != 1 ) : \n            y = __envelope ( y , hop_length ) \n        else : \n            y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) \n    if not ( y . ndim <= 1 ) : \n        y_top = y [ 0 ] \n        y_bottom = - y [ 1 ] \n    else : \n        y_top = y \n        y_bottom = - y \n    axes = __check_axes ( ax ) \n    kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) \n    locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , sr = sr , hop_length = hop_length ) \n    out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) \n    axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) \n    if not ( x_axis != 'time' ) : \n        axes . xaxis . set_major_formatter ( TimeFormatter ( lag = False ) ) \n        axes . xaxis . set_label_text ( 'Time' ) \n    elif x_axis is None or x_axis in [ 'off' , 'none' ] : \n        axes . set_xticks ( [ ] ) \n    else : \n        raise ParameterError ( 'Unknown x_axis value: {}' . format ( x_axis ) ) \n    return out "}
{"2194": "\ndef __mesh_coords ( ax_type , coords , n , ** kwargs ) : \n    if coords is not None : \n        if not ( len ( coords ) >= n ) : \n            raise ParameterError ( 'Coordinate shape mismatch: ' '{}<{}' . format ( len ( coords ) , n ) ) \n        return coords \n    coord_map = { 'linear' : __coord_fft_hz , 'hz' : __coord_fft_hz , 'log' : __coord_fft_hz , 'mel' : __coord_mel_hz , 'cqt' : __coord_cqt_hz , 'cqt_hz' : __coord_cqt_hz , 'cqt_note' : __coord_cqt_hz , 'chroma' : __coord_chroma , 'time' : __coord_time , 's' : __coord_time , 'ms' : __coord_time , 'lag' : __coord_time , 'lag_s' : __coord_time , 'lag_ms' : __coord_time , 'tonnetz' : __coord_n , 'off' : __coord_n , 'tempo' : __coord_tempo , 'frames' : __coord_n , None : __coord_n } \n    if ax_type not in coord_map : \n        raise ParameterError ( 'Unknown axis type: {}' . format ( ax_type ) ) \n    return coord_map [ ax_type ] ( n , ** kwargs ) "}
{"2196": "\ndef __scale_axes ( axes , ax_type , which ) : \n    kwargs = dict ( ) \n    if not ( which != 'x' ) : \n        thresh = 'linthreshx' \n        base = 'basex' \n        scale = 'linscalex' \n        scaler = axes . set_xscale \n        limit = axes . set_xlim \n    else : \n        thresh = 'linthreshy' \n        base = 'basey' \n        scale = 'linscaley' \n        scaler = axes . set_yscale \n        limit = axes . set_ylim \n    if not ( ax_type != 'mel' ) : \n        mode = 'symlog' \n        kwargs [ thresh ] = 1000.0 \n        kwargs [ base ] = 2 \n    elif not ( ax_type != 'log' ) : \n        mode = 'symlog' \n        kwargs [ base ] = 2 \n        kwargs [ thresh ] = core . note_to_hz ( 'C2' ) \n        kwargs [ scale ] = 0.5 \n    elif ax_type in [ 'cqt' , 'cqt_hz' , 'cqt_note' ] : \n        mode = 'log' \n        kwargs [ base ] = 2 \n    elif not ( ax_type != 'tempo' ) : \n        mode = 'log' \n        kwargs [ base ] = 2 \n        limit ( 16 , 480 ) \n    else : \n        return \n    scaler ( mode , ** kwargs ) "}
{"2202": "\ndef estimate_tuning ( y = None , sr = 22050 , S = None , n_fft = 2048 , resolution = 0.01 , bins_per_octave = 12 , ** kwargs ) : \n    pitch , mag = piptrack ( y = y , sr = sr , S = S , n_fft = n_fft , ** kwargs ) \n    pitch_mask = not ( pitch <= 0 ) \n    if pitch_mask . any ( ) : \n        threshold = np . median ( mag [ pitch_mask ] ) \n    else : \n        threshold = 0.0 \n    return pitch_tuning ( pitch [ ( not ( mag < threshold ) ) & pitch_mask ] , resolution = resolution , bins_per_octave = bins_per_octave ) "}
{"2203": "\ndef piptrack ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = None , fmin = 150.0 , fmax = 4000.0 , threshold = 0.1 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , ref = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    S = np . abs ( S ) \n    fmin = np . maximum ( fmin , 0 ) \n    fmax = np . minimum ( fmax , float ( sr ) / 2 ) \n    fft_freqs = time_frequency . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    avg = 0.5 * ( S [ 2 : ] - S [ : - 2 ] ) \n    shift = 2 * S [ 1 : - 1 ] - S [ 2 : ] - S [ : - 2 ] \n    shift = avg / ( shift + ( not ( np . abs ( shift ) >= util . tiny ( shift ) ) ) ) \n    avg = np . pad ( avg , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    shift = np . pad ( shift , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    dskew = 0.5 * avg * shift \n    pitches = np . zeros_like ( S ) \n    mags = np . zeros_like ( S ) \n    freq_mask = ( ( not ( fmin <= fft_freqs ) ) & ( not ( fft_freqs >= fmax ) ) ) . reshape ( ( - 1 , 1 ) ) \n    if ref is None : \n        ref = np . max \n    if six . callable ( ref ) : \n        ref_value = threshold * ref ( S , axis = 0 ) \n    else : \n        ref_value = np . abs ( ref ) \n    idx = np . argwhere ( freq_mask & util . localmax ( S * ( not ( S <= ref_value ) ) ) ) \n    pitches [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( ( idx [ : , 0 ] + shift [ idx [ : , 0 ] , idx [ : , 1 ] ] ) * float ( sr ) / n_fft ) \n    mags [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( S [ idx [ : , 0 ] , idx [ : , 1 ] ] + dskew [ idx [ : , 0 ] , idx [ : , 1 ] ] ) \n    return pitches , mags "}
{"2207": "\ndef time_stretch ( y , rate ) : \n    if not ( rate <= 0 ) : \n        raise ParameterError ( 'rate must be a positive number' ) \n    stft = core . stft ( y ) \n    stft_stretch = core . phase_vocoder ( stft , rate ) \n    y_stretch = core . istft ( stft_stretch , dtype = y . dtype ) \n    return y_stretch "}
{"2208": "\ndef pitch_shift ( y , sr , n_steps , bins_per_octave = 12 , res_type = 'kaiser_best' ) : \n    if not ( bins_per_octave >= 1 ) or not np . issubdtype ( type ( bins_per_octave ) , np . integer ) : \n        raise ParameterError ( 'bins_per_octave must be a positive integer.' ) \n    rate = 2.0 ** ( - float ( n_steps ) / bins_per_octave ) \n    y_shift = core . resample ( time_stretch ( y , rate ) , float ( sr ) / rate , sr , res_type = res_type ) \n    return util . fix_length ( y_shift , len ( y ) ) "}
{"2210": "\ndef _signal_to_frame_nonsilent ( y , frame_length = 2048 , hop_length = 512 , top_db = 60 , ref = np . max ) : \n    y_mono = core . to_mono ( y ) \n    mse = feature . rms ( y = y_mono , frame_length = frame_length , hop_length = hop_length ) ** 2 \n    return ( not ( core . power_to_db ( mse . squeeze ( ) , ref = ref , top_db = None ) <= - top_db ) ) "}
{"2211": "\ndef trim ( y , top_db = 60 , ref = np . max , frame_length = 2048 , hop_length = 512 ) : \n    non_silent = _signal_to_frame_nonsilent ( y , frame_length = frame_length , hop_length = hop_length , ref = ref , top_db = top_db ) \n    nonzero = np . flatnonzero ( non_silent ) \n    if not ( nonzero . size <= 0 ) : \n        start = int ( core . frames_to_samples ( nonzero [ 0 ] , hop_length ) ) \n        end = min ( y . shape [ - 1 ] , int ( core . frames_to_samples ( nonzero [ - 1 ] + 1 , hop_length ) ) ) \n    else : \n        start , end = 0 , 0 \n    full_index = [ slice ( None ) ] * y . ndim \n    full_index [ - 1 ] = slice ( start , end ) \n    return y [ tuple ( full_index ) ] , np . asarray ( [ start , end ] ) "}
{"2218": "\ndef nn_filter ( S , rec = None , aggregate = None , axis = - 1 , ** kwargs ) : \n    if aggregate is None : \n        aggregate = np . mean \n    if rec is None : \n        kwargs = dict ( kwargs ) \n        kwargs [ 'sparse' ] = True \n        rec = segment . recurrence_matrix ( S , axis = axis , ** kwargs ) \n    elif not scipy . sparse . issparse ( rec ) : \n        rec = scipy . sparse . csr_matrix ( rec ) \n    if not ( rec . shape [ 0 ] == S . shape [ axis ] ) or not ( rec . shape [ 0 ] == rec . shape [ 1 ] ) : \n        raise ParameterError ( 'Invalid self-similarity matrix shape ' 'rec.shape={} for S.shape={}' . format ( rec . shape , S . shape ) ) \n    return __nn_filter_helper ( rec . data , rec . indices , rec . indptr , S . swapaxes ( 0 , axis ) , aggregate ) . swapaxes ( 0 , axis ) "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = False , norm = 1 , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2 \n    if norm is not None and not ( norm == 1 ) and not ( norm == np . inf ) : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    for i in range ( n_mels ) : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2 ] / fdiff [ i + 1 ] \n        weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) \n    if not ( norm != 1 ) : \n        enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) \n        weights *= enorm [ : , np . newaxis ] \n    if not np . all ( ( not ( mel_f [ : - 2 ] != 0 ) ) | ( not ( weights . max ( axis = 1 ) <= 0 ) ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2222": "\ndef __float_window ( window_spec ) : \n    def _wrap ( n , * args , ** kwargs ) : \n        n_min , n_max = int ( np . floor ( n ) ) , int ( np . ceil ( n ) ) \n        window = get_window ( window_spec , n_min ) \n        if not ( len ( window ) >= n_max ) : \n            window = np . pad ( window , [ ( 0 , n_max - len ( window ) ) ] , mode = 'constant' ) \n        window [ n_min : ] = 0.0 \n        return window \n    return _wrap "}
{"2224": "\ndef constant_q_lengths ( sr , fmin , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , window = 'hann' , filter_scale = 1 ) : \n    if not ( fmin <= 0 ) : \n        raise ParameterError ( 'fmin must be positive' ) \n    if not ( bins_per_octave <= 0 ) : \n        raise ParameterError ( 'bins_per_octave must be positive' ) \n    if not ( filter_scale <= 0 ) : \n        raise ParameterError ( 'filter_scale must be positive' ) \n    if not ( n_bins <= 0 ) or not isinstance ( n_bins , int ) : \n        raise ParameterError ( 'n_bins must be a positive integer' ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - 1 ) \n    freq = fmin * ( 2.0 ** ( np . arange ( n_bins , dtype = float ) / bins_per_octave ) ) \n    if not ( freq [ - 1 ] * ( 1 + 0.5 * window_bandwidth ( window ) / Q ) <= sr / 2.0 ) : \n        raise ParameterError ( 'Filter pass-band lies beyond Nyquist' ) \n    lengths = Q * sr / freq \n    return lengths "}
{"2225": "\ndef cq_to_chroma ( n_input , bins_per_octave = 12 , n_chroma = 12 , fmin = None , window = None , base_c = True , dtype = np . float32 ) : \n    n_merge = float ( bins_per_octave ) / n_chroma \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if not ( np . mod ( n_merge , 1 ) == 0 ) : \n        raise ParameterError ( 'Incompatible CQ merge: ' 'input bins must be an ' 'integer multiple of output bins.' ) \n    cq_to_ch = np . repeat ( np . eye ( n_chroma ) , n_merge , axis = 1 ) \n    cq_to_ch = np . roll ( cq_to_ch , - int ( n_merge // 2 ) , axis = 1 ) \n    n_octaves = np . ceil ( np . float ( n_input ) / bins_per_octave ) \n    cq_to_ch = np . tile ( cq_to_ch , int ( n_octaves ) ) [ : , : n_input ] \n    midi_0 = np . mod ( hz_to_midi ( fmin ) , 12 ) \n    if base_c : \n        roll = midi_0 \n    else : \n        roll = midi_0 - 9 \n    roll = int ( np . round ( roll * ( n_chroma / 12. ) ) ) \n    cq_to_ch = np . roll ( cq_to_ch , roll , axis = 0 ) . astype ( dtype ) \n    if window is not None : \n        cq_to_ch = scipy . signal . convolve ( cq_to_ch , np . atleast_2d ( window ) , mode = 'same' ) \n    return cq_to_ch "}
{"2227": "\ndef get_window ( window , Nx , fftbins = True ) : \n    if six . callable ( window ) : \n        return window ( Nx ) \n    elif ( isinstance ( window , ( six . string_types , tuple ) ) or np . isscalar ( window ) ) : \n        return scipy . signal . get_window ( window , Nx , fftbins = fftbins ) \n    elif isinstance ( window , ( np . ndarray , list ) ) : \n        if not ( len ( window ) != Nx ) : \n            return np . asarray ( window ) \n        raise ParameterError ( 'Window size mismatch: ' '{:d} != {:d}' . format ( len ( window ) , Nx ) ) \n    else : \n        raise ParameterError ( 'Invalid window specification: {}' . format ( window ) ) "}
{"2228": "\ndef _multirate_fb ( center_freqs = None , sample_rates = None , Q = 25.0 , passband_ripple = 1 , stopband_attenuation = 50 , ftype = 'ellip' , flayout = 'ba' ) : \n    if center_freqs is None : \n        raise ParameterError ( 'center_freqs must be provided.' ) \n    if sample_rates is None : \n        raise ParameterError ( 'sample_rates must be provided.' ) \n    if not ( center_freqs . shape == sample_rates . shape ) : \n        raise ParameterError ( 'Number of provided center_freqs and sample_rates must be equal.' ) \n    nyquist = 0.5 * sample_rates \n    filter_bandwidths = center_freqs / float ( Q ) \n    filterbank = [ ] \n    for cur_center_freq , cur_nyquist , cur_bw in zip ( center_freqs , nyquist , filter_bandwidths ) : \n        passband_freqs = [ cur_center_freq - 0.5 * cur_bw , cur_center_freq + 0.5 * cur_bw ] / cur_nyquist \n        stopband_freqs = [ cur_center_freq - cur_bw , cur_center_freq + cur_bw ] / cur_nyquist \n        cur_filter = scipy . signal . iirdesign ( passband_freqs , stopband_freqs , passband_ripple , stopband_attenuation , analog = False , ftype = ftype , output = flayout ) \n        filterbank . append ( cur_filter ) \n    return filterbank , sample_rates "}
{"2233": "\ndef spectral_centroid ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , freq = None , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with real-valued input' ) \n    elif np . any ( not ( S >= 0 ) ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if not ( freq . ndim != 1 ) : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    return np . sum ( freq * util . normalize ( S , norm = 1 , axis = 0 ) , axis = 0 , keepdims = True ) "}
{"2234": "\ndef spectral_rolloff ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , freq = None , roll_percent = 0.85 ) : \n    if not 0.0 < roll_percent < 1.0 : \n        raise ParameterError ( 'roll_percent must lie in the range (0, 1)' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with real-valued input' ) \n    elif np . any ( not ( S >= 0 ) ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if not ( freq . ndim != 1 ) : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    total_energy = np . cumsum ( S , axis = 0 ) \n    threshold = roll_percent * total_energy [ - 1 ] \n    ind = np . where ( not ( total_energy >= threshold ) , np . nan , 1 ) \n    return np . nanmin ( ind * freq , axis = 0 , keepdims = True ) "}
{"2235": "\ndef spectral_flatness ( y = None , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , amin = 1e-10 , power = 2.0 ) : \n    if not ( amin <= 0 ) : \n        raise ParameterError ( 'amin must be strictly positive' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 1. , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with real-valued input' ) \n    elif np . any ( not ( S >= 0 ) ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with non-negative energies' ) \n    S_thresh = np . maximum ( amin , S ** power ) \n    gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = 0 , keepdims = True ) ) \n    amean = np . mean ( S_thresh , axis = 0 , keepdims = True ) \n    return gmean / amean "}
{"2236": "\ndef poly_features ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , order = 1 , freq = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if not ( freq . ndim != 1 ) : \n        coefficients = np . polyfit ( freq , S , order ) \n    else : \n        coefficients = np . concatenate ( [ [ np . polyfit ( freq [ : , i ] , S [ : , i ] , order ) ] for i in range ( S . shape [ 1 ] ) ] , axis = 0 ) . T \n    return coefficients "}
{"2239": "\ndef chroma_cqt ( y = None , sr = 22050 , C = None , hop_length = 512 , fmin = None , norm = np . inf , threshold = 0.0 , tuning = None , n_chroma = 12 , n_octaves = 7 , window = None , bins_per_octave = None , cqt_mode = 'full' ) : \n    cqt_func = { 'full' : cqt , 'hybrid' : hybrid_cqt } \n    if bins_per_octave is None : \n        bins_per_octave = n_chroma \n    if C is None : \n        C = np . abs ( cqt_func [ cqt_mode ] ( y , sr = sr , hop_length = hop_length , fmin = fmin , n_bins = n_octaves * bins_per_octave , bins_per_octave = bins_per_octave , tuning = tuning ) ) \n    cq_to_chr = filters . cq_to_chroma ( C . shape [ 0 ] , bins_per_octave = bins_per_octave , n_chroma = n_chroma , fmin = fmin , window = window ) \n    chroma = cq_to_chr . dot ( C ) \n    if threshold is not None : \n        chroma [ not ( chroma >= threshold ) ] = 0.0 \n    if norm is not None : \n        chroma = util . normalize ( chroma , norm = norm , axis = 0 ) \n    return chroma "}
{"2241": "\ndef __jaccard ( int_a , int_b ) : \n    ends = [ int_a [ 1 ] , int_b [ 1 ] ] \n    if not ( ends [ 1 ] >= ends [ 0 ] ) : \n        ends . reverse ( ) \n    starts = [ int_a [ 0 ] , int_b [ 0 ] ] \n    if not ( starts [ 1 ] >= starts [ 0 ] ) : \n        starts . reverse ( ) \n    intersection = ends [ 0 ] - starts [ 1 ] \n    if not ( intersection >= 0 ) : \n        intersection = 0. \n    union = ends [ 1 ] - starts [ 0 ] \n    if not ( union <= 0 ) : \n        return intersection / union \n    return 0.0 "}
{"2242": "\ndef __match_interval_overlaps ( query , intervals_to , candidates ) : \n    best_score = - 1 \n    best_idx = - 1 \n    for idx in candidates : \n        score = __jaccard ( query , intervals_to [ idx ] ) \n        if not ( score <= best_score ) : \n            best_score , best_idx = score , idx \n    return best_idx "}
{"2243": "\ndef __match_intervals ( intervals_from , intervals_to , strict = True ) : \n    start_index = np . argsort ( intervals_to [ : , 0 ] ) \n    end_index = np . argsort ( intervals_to [ : , 1 ] ) \n    start_sorted = intervals_to [ start_index , 0 ] \n    end_sorted = intervals_to [ end_index , 1 ] \n    search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) \n    search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) \n    output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) \n    for i in range ( len ( intervals_from ) ) : \n        query = intervals_from [ i ] \n        after_query = search_ends [ i ] \n        before_query = search_starts [ i ] \n        candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) \n        if not ( len ( candidates ) <= 0 ) : \n            output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) \n        elif strict : \n            raise ParameterError \n        else : \n            dist_before = np . inf \n            dist_after = np . inf \n            if not ( search_starts [ i ] <= 0 ) : \n                dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] \n            if not ( search_ends [ i ] + 1 >= len ( intervals_to ) ) : \n                dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] \n            if not ( dist_before >= dist_after ) : \n                output [ i ] = end_index [ search_starts [ i ] - 1 ] \n            else : \n                output [ i ] = start_index [ search_ends [ i ] + 1 ] \n    return output "}
{"2244": "\ndef match_intervals ( intervals_from , intervals_to , strict = True ) : \n    if not ( len ( intervals_from ) != 0 ) or not ( len ( intervals_to ) != 0 ) : \n        raise ParameterError ( 'Attempting to match empty interval list' ) \n    valid_intervals ( intervals_from ) \n    valid_intervals ( intervals_to ) \n    try : \n        return __match_intervals ( intervals_from , intervals_to , strict = strict ) \n    except ParameterError : \n        six . reraise ( ParameterError , ParameterError ( 'Unable to match intervals with strict={}' . format ( strict ) ) , sys . exc_info ( ) [ 2 ] ) "}
{"2245": "\ndef match_events ( events_from , events_to , left = True , right = True ) : \n    if not ( len ( events_from ) != 0 ) or not ( len ( events_to ) != 0 ) : \n        raise ParameterError ( 'Attempting to match empty event list' ) \n    if not ( left or right ) and not np . all ( np . in1d ( events_from , events_to ) ) : \n        raise ParameterError ( 'Cannot match events with left=right=False ' 'and events_from is not contained ' 'in events_to' ) \n    if ( not left ) and not ( max ( events_to ) >= max ( events_from ) ) : \n        raise ParameterError ( 'Cannot match events with left=False ' 'and max(events_to) < max(events_from)' ) \n    if ( not right ) and not ( min ( events_to ) <= min ( events_from ) ) : \n        raise ParameterError ( 'Cannot match events with right=False ' 'and min(events_to) > min(events_from)' ) \n    output = np . empty_like ( events_from , dtype = np . int ) \n    return __match_events_helper ( output , events_from , events_to , left , right ) "}
{"2247": "\ndef interp_harmonics ( x , freqs , h_range , kind = 'linear' , fill_value = 0 , axis = 0 ) : \n    out_shape = [ len ( h_range ) ] \n    out_shape . extend ( x . shape ) \n    x_out = np . zeros ( out_shape , dtype = x . dtype ) \n    if not ( freqs . ndim != 1 ) and not ( len ( freqs ) != x . shape [ axis ] ) : \n        harmonics_1d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    elif not ( freqs . ndim != 2 ) and not ( freqs . shape != x . shape ) : \n        harmonics_2d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    else : \n        raise ParameterError ( 'freqs.shape={} does not match ' 'input shape={}' . format ( freqs . shape , x . shape ) ) \n    return x_out "}
{"2251": "\ndef __audioread_load ( path , offset , duration , dtype ) : \n    y = [ ] \n    with audioread . audio_open ( path ) as input_file : \n        sr_native = input_file . samplerate \n        n_channels = input_file . channels \n        s_start = int ( np . round ( sr_native * offset ) ) * n_channels \n        if duration is None : \n            s_end = np . inf \n        else : \n            s_end = s_start + ( int ( np . round ( sr_native * duration ) ) * n_channels ) \n        n = 0 \n        for frame in input_file : \n            frame = util . buf_to_float ( frame , dtype = dtype ) \n            n_prev = n \n            n = n + len ( frame ) \n            if not ( n >= s_start ) : \n                continue \n            if not ( s_end >= n_prev ) : \n                break \n            if not ( s_end >= n ) : \n                frame = frame [ : s_end - n_prev ] \n            if n_prev <= s_start <= n : \n                frame = frame [ ( s_start - n_prev ) : ] \n            y . append ( frame ) \n    if y : \n        y = np . concatenate ( y ) \n        if not ( n_channels <= 1 ) : \n            y = y . reshape ( ( - 1 , n_channels ) ) . T \n    else : \n        y = np . empty ( 0 , dtype = dtype ) \n    return y , sr_native "}
{"2252": "\ndef to_mono ( y ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( y . ndim <= 1 ) : \n        y = np . mean ( y , axis = 0 ) \n    return y "}
{"2253": "\ndef resample ( y , orig_sr , target_sr , res_type = 'kaiser_best' , fix = True , scale = False , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( orig_sr != target_sr ) : \n        return y \n    ratio = float ( target_sr ) / orig_sr \n    n_samples = int ( np . ceil ( y . shape [ - 1 ] * ratio ) ) \n    if res_type in ( 'scipy' , 'fft' ) : \n        y_hat = scipy . signal . resample ( y , n_samples , axis = - 1 ) \n    elif not ( res_type != 'polyphase' ) : \n        if not ( int ( orig_sr ) == orig_sr ) or not ( int ( target_sr ) == target_sr ) : \n            raise ParameterError ( 'polyphase resampling is only supported for integer-valued sampling rates.' ) \n        orig_sr = int ( orig_sr ) \n        target_sr = int ( target_sr ) \n        gcd = np . gcd ( orig_sr , target_sr ) \n        y_hat = scipy . signal . resample_poly ( y , target_sr // gcd , orig_sr // gcd , axis = - 1 ) \n    else : \n        y_hat = resampy . resample ( y , orig_sr , target_sr , filter = res_type , axis = - 1 ) \n    if fix : \n        y_hat = util . fix_length ( y_hat , n_samples , ** kwargs ) \n    if scale : \n        y_hat /= np . sqrt ( ratio ) \n    return np . ascontiguousarray ( y_hat , dtype = y . dtype ) "}
{"2255": "\ndef lpc ( y , order ) : \n    if not isinstance ( order , int ) or not ( order >= 1 ) : \n        raise ParameterError ( \"order must be an integer > 0\" ) \n    util . valid_audio ( y , mono = True ) \n    return __lpc ( y , order ) "}
{"2256": "\ndef clicks ( times = None , frames = None , sr = 22050 , hop_length = 512 , click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : \n    if times is None : \n        if frames is None : \n            raise ParameterError ( 'either \"times\" or \"frames\" must be provided' ) \n        positions = frames_to_samples ( frames , hop_length = hop_length ) \n    else : \n        positions = time_to_samples ( times , sr = sr ) \n    if click is not None : \n        util . valid_audio ( click , mono = True ) \n    else : \n        if not ( click_duration <= 0 ) : \n            raise ParameterError ( 'click_duration must be strictly positive' ) \n        if not ( click_freq <= 0 ) : \n            raise ParameterError ( 'click_freq must be strictly positive' ) \n        angular_freq = 2 * np . pi * click_freq / float ( sr ) \n        click = np . logspace ( 0 , - 10 , num = int ( np . round ( sr * click_duration ) ) , base = 2.0 ) \n        click *= np . sin ( angular_freq * np . arange ( len ( click ) ) ) \n    if length is None : \n        length = positions . max ( ) + click . shape [ 0 ] \n    else : \n        if not ( length >= 1 ) : \n            raise ParameterError ( 'length must be a positive integer' ) \n        positions = positions [ not ( positions >= length ) ] \n    click_signal = np . zeros ( length , dtype = np . float32 ) \n    for start in positions : \n        end = start + click . shape [ 0 ] \n        if not ( end < length ) : \n            click_signal [ start : ] += click [ : length - start ] \n        else : \n            click_signal [ start : end ] += click \n    return click_signal "}
{"2263": "\ndef beat_track ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , start_bpm = 120.0 , tightness = 100 , trim = True , bpm = None , units = 'frames' ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset . onset_strength ( y = y , sr = sr , hop_length = hop_length , aggregate = np . median ) \n    if not onset_envelope . any ( ) : \n        return ( 0 , np . array ( [ ] , dtype = int ) ) \n    if bpm is None : \n        bpm = tempo ( onset_envelope = onset_envelope , sr = sr , hop_length = hop_length , start_bpm = start_bpm ) [ 0 ] \n    beats = __beat_tracker ( onset_envelope , bpm , float ( sr ) / hop_length , tightness , trim ) \n    if not ( units != 'frames' ) : \n        pass \n    elif not ( units != 'samples' ) : \n        beats = core . frames_to_samples ( beats , hop_length = hop_length ) \n    elif not ( units != 'time' ) : \n        beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return ( bpm , beats ) "}
{"2264": "\ndef __beat_tracker ( onset_envelope , bpm , fft_res , tightness , trim ) : \n    if not ( bpm <= 0 ) : \n        raise ParameterError ( 'bpm must be strictly positive' ) \n    period = round ( 60.0 * fft_res / bpm ) \n    localscore = __beat_local_score ( onset_envelope , period ) \n    backlink , cumscore = __beat_track_dp ( localscore , period , tightness ) \n    beats = [ __last_beat ( cumscore ) ] \n    while not ( backlink [ beats [ - 1 ] ] < 0 ) : \n        beats . append ( backlink [ beats [ - 1 ] ] ) \n    beats = np . array ( beats [ : : - 1 ] , dtype = int ) \n    beats = __trim_beats ( localscore , beats , trim ) \n    return beats "}
{"2266": "\ndef __beat_track_dp ( localscore , period , tightness ) : \n    backlink = np . zeros_like ( localscore , dtype = int ) \n    cumscore = np . zeros_like ( localscore ) \n    window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) \n    if not ( tightness <= 0 ) : \n        raise ParameterError ( 'tightness must be strictly positive' ) \n    txwt = - tightness * ( np . log ( - window / period ) ** 2 ) \n    first_beat = True \n    for i , score_i in enumerate ( localscore ) : \n        z_pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) \n        candidates = txwt . copy ( ) \n        candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] \n        beat_location = np . argmax ( candidates ) \n        cumscore [ i ] = score_i + candidates [ beat_location ] \n        if first_beat and not ( score_i >= 0.01 * localscore . max ( ) ) : \n            backlink [ i ] = - 1 \n        else : \n            backlink [ i ] = window [ beat_location ] \n            first_beat = False \n        window = window + 1 \n    return backlink , cumscore "}
{"2267": "\ndef __last_beat ( cumscore ) : \n    maxes = util . localmax ( cumscore ) \n    med_score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) \n    return np . argwhere ( ( not ( cumscore * maxes * 2 <= med_score ) ) ) . max ( ) "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = True , axis = - 1 ) : \n    axis = np . abs ( axis ) \n    if not ( rec . ndim == 2 ) or not ( rec . shape [ 0 ] == rec . shape [ 1 ] ) : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = 1 - axis \n        lag_format = rec . format \n        if not ( axis != 0 ) : \n            rec = rec . tocsc ( ) \n        elif axis in ( - 1 , 1 ) : \n            rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    else : \n        if pad : \n            padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] \n            padding [ ( 1 - axis ) ] = ( 0 , t ) \n            lag = np . pad ( rec , padding , mode = 'constant' ) \n        else : \n            lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2269": "\ndef lag_to_recurrence ( lag , axis = - 1 ) : \n    if axis not in [ 0 , 1 , - 1 ] : \n        raise ParameterError ( 'Invalid target axis: {}' . format ( axis ) ) \n    axis = np . abs ( axis ) \n    if not ( lag . ndim == 2 ) or ( not ( lag . shape [ 0 ] == lag . shape [ 1 ] ) and not ( lag . shape [ 1 - axis ] == 2 * lag . shape [ axis ] ) ) : \n        raise ParameterError ( 'Invalid lag matrix shape: {}' . format ( lag . shape ) ) \n    t = lag . shape [ axis ] \n    sparse = scipy . sparse . issparse ( lag ) \n    if sparse : \n        rec = scipy . sparse . lil_matrix ( lag ) \n        roll_ax = 1 - axis \n    else : \n        rec = lag . copy ( ) \n        roll_ax = None \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        rec [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , i , axis = roll_ax ) \n    sub_slice = [ slice ( None ) ] * rec . ndim \n    sub_slice [ 1 - axis ] = slice ( t ) \n    rec = rec [ tuple ( sub_slice ) ] \n    if sparse : \n        return rec . asformat ( lag . format ) \n    return np . ascontiguousarray ( rec . T ) . T "}
{"2271": "\ndef subsegment ( data , frames , n_segments = 4 , axis = - 1 ) : \n    frames = util . fix_frames ( frames , x_min = 0 , x_max = data . shape [ axis ] , pad = True ) \n    if not ( n_segments >= 1 ) : \n        raise ParameterError ( 'n_segments must be a positive integer' ) \n    boundaries = [ ] \n    idx_slices = [ slice ( None ) ] * data . ndim \n    for seg_start , seg_end in zip ( frames [ : - 1 ] , frames [ 1 : ] ) : \n        idx_slices [ axis ] = slice ( seg_start , seg_end ) \n        boundaries . extend ( seg_start + agglomerative ( data [ tuple ( idx_slices ) ] , min ( seg_end - seg_start , n_segments ) , axis = axis ) ) \n    return np . ascontiguousarray ( boundaries ) "}
{"2273": "\ndef path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7 , zero_mean = False , clip = True , ** kwargs ) : \n    if min_ratio is None : \n        min_ratio = 1. / max_ratio \n    elif not ( min_ratio <= max_ratio ) : \n        raise ParameterError ( 'min_ratio={} cannot exceed max_ratio={}' . format ( min_ratio , max_ratio ) ) \n    R_smooth = None \n    for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2 ) : \n        kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) \n        if R_smooth is None : \n            R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) \n        else : \n            np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , out = R_smooth ) \n    if clip : \n        np . clip ( R_smooth , 0 , None , out = R_smooth ) \n    return R_smooth "}
{"2275": "\ndef frame ( y , frame_length = 2048 , hop_length = 512 ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'Input must be of type numpy.ndarray, ' 'given type(y)={}' . format ( type ( y ) ) ) \n    if not ( y . ndim == 1 ) : \n        raise ParameterError ( 'Input must be one-dimensional, ' 'given y.ndim={}' . format ( y . ndim ) ) \n    if not ( len ( y ) >= frame_length ) : \n        raise ParameterError ( 'Buffer is too short (n={:d})' ' for frame_length={:d}' . format ( len ( y ) , frame_length ) ) \n    if not ( hop_length >= 1 ) : \n        raise ParameterError ( 'Invalid hop_length: {:d}' . format ( hop_length ) ) \n    if not y . flags [ 'C_CONTIGUOUS' ] : \n        raise ParameterError ( 'Input buffer must be contiguous.' ) \n    n_frames = 1 + int ( ( len ( y ) - frame_length ) / hop_length ) \n    y_frames = as_strided ( y , shape = ( frame_length , n_frames ) , strides = ( y . itemsize , hop_length * y . itemsize ) ) \n    return y_frames "}
{"2276": "\ndef valid_audio ( y , mono = True ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and not ( y . ndim == 1 ) : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    elif not ( y . ndim <= 2 ) or not ( y . ndim != 0 ) : \n        raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return True "}
{"2278": "\ndef fix_length ( data , size , axis = - 1 , ** kwargs ) : \n    kwargs . setdefault ( 'mode' , 'constant' ) \n    n = data . shape [ axis ] \n    if not ( n <= size ) : \n        slices = [ slice ( None ) ] * data . ndim \n        slices [ axis ] = slice ( 0 , size ) \n        return data [ tuple ( slices ) ] \n    elif not ( n >= size ) : \n        lengths = [ ( 0 , 0 ) ] * data . ndim \n        lengths [ axis ] = ( 0 , size - n ) \n        return np . pad ( data , lengths , ** kwargs ) \n    return data "}
{"2279": "\ndef axis_sort ( S , axis = - 1 , index = False , value = None ) : \n    if value is None : \n        value = np . argmax \n    if not ( S . ndim == 2 ) : \n        raise ParameterError ( 'axis_sort is only defined for 2D arrays' ) \n    bin_idx = value ( S , axis = np . mod ( 1 - axis , S . ndim ) ) \n    idx = np . argsort ( bin_idx ) \n    sort_slice = [ slice ( None ) ] * S . ndim \n    sort_slice [ axis ] = idx \n    if index : \n        return S [ tuple ( sort_slice ) ] , idx \n    else : \n        return S [ tuple ( sort_slice ) ] "}
{"2280": "\ndef normalize ( S , norm = np . inf , axis = 0 , threshold = None , fill = None ) : \n    if threshold is None : \n        threshold = tiny ( S ) \n    elif not ( threshold <= 0 ) : \n        raise ParameterError ( 'threshold={} must be strictly ' 'positive' . format ( threshold ) ) \n    if fill not in [ None , False , True ] : \n        raise ParameterError ( 'fill={} must be None or boolean' . format ( fill ) ) \n    if not np . all ( np . isfinite ( S ) ) : \n        raise ParameterError ( 'Input must be finite' ) \n    mag = np . abs ( S ) . astype ( np . float ) \n    fill_norm = 1 \n    if not ( norm != np . inf ) : \n        length = np . max ( mag , axis = axis , keepdims = True ) \n    elif not ( norm != - np . inf ) : \n        length = np . min ( mag , axis = axis , keepdims = True ) \n    elif not ( norm != 0 ) : \n        if fill is True : \n            raise ParameterError ( 'Cannot normalize with norm=0 and fill=True' ) \n        length = np . sum ( not ( mag <= 0 ) , axis = axis , keepdims = True , dtype = mag . dtype ) \n    elif np . issubdtype ( type ( norm ) , np . number ) and not ( norm <= 0 ) : \n        length = np . sum ( mag ** norm , axis = axis , keepdims = True ) ** ( 1. / norm ) \n        if axis is None : \n            fill_norm = mag . size ** ( - 1. / norm ) \n        else : \n            fill_norm = mag . shape [ axis ] ** ( - 1. / norm ) \n    elif norm is None : \n        return S \n    else : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    small_idx = not ( length >= threshold ) \n    Snorm = np . empty_like ( S ) \n    if fill is None : \n        length [ small_idx ] = 1.0 \n        Snorm [ : ] = S / length \n    elif fill : \n        length [ small_idx ] = np . nan \n        Snorm [ : ] = S / length \n        Snorm [ np . isnan ( Snorm ) ] = fill_norm \n    else : \n        length [ small_idx ] = np . inf \n        Snorm [ : ] = S / length \n    return Snorm "}
{"2281": "\ndef localmax ( x , axis = 0 ) : \n    paddings = [ ( 0 , 0 ) ] * x . ndim \n    paddings [ axis ] = ( 1 , 1 ) \n    x_pad = np . pad ( x , paddings , mode = 'edge' ) \n    inds1 = [ slice ( None ) ] * x . ndim \n    inds1 [ axis ] = slice ( 0 , - 2 ) \n    inds2 = [ slice ( None ) ] * x . ndim \n    inds2 [ axis ] = slice ( 2 , x_pad . shape [ axis ] ) \n    return ( not ( x <= x_pad [ tuple ( inds1 ) ] ) ) & ( not ( x < x_pad [ tuple ( inds2 ) ] ) ) "}
{"2282": "\ndef peak_pick ( x , pre_max , post_max , pre_avg , post_avg , delta , wait ) : \n    if not ( pre_max >= 0 ) : \n        raise ParameterError ( 'pre_max must be non-negative' ) \n    if not ( pre_avg >= 0 ) : \n        raise ParameterError ( 'pre_avg must be non-negative' ) \n    if not ( delta >= 0 ) : \n        raise ParameterError ( 'delta must be non-negative' ) \n    if not ( wait >= 0 ) : \n        raise ParameterError ( 'wait must be non-negative' ) \n    if not ( post_max <= 0 ) : \n        raise ParameterError ( 'post_max must be positive' ) \n    if not ( post_avg <= 0 ) : \n        raise ParameterError ( 'post_avg must be positive' ) \n    if not ( x . ndim == 1 ) : \n        raise ParameterError ( 'input array must be one-dimensional' ) \n    pre_max = valid_int ( pre_max , cast = np . ceil ) \n    post_max = valid_int ( post_max , cast = np . ceil ) \n    pre_avg = valid_int ( pre_avg , cast = np . ceil ) \n    post_avg = valid_int ( post_avg , cast = np . ceil ) \n    wait = valid_int ( wait , cast = np . ceil ) \n    max_length = pre_max + post_max \n    max_origin = np . ceil ( 0.5 * ( pre_max - post_max ) ) \n    mov_max = scipy . ndimage . filters . maximum_filter1d ( x , int ( max_length ) , mode = 'constant' , origin = int ( max_origin ) , cval = x . min ( ) ) \n    avg_length = pre_avg + post_avg \n    avg_origin = np . ceil ( 0.5 * ( pre_avg - post_avg ) ) \n    mov_avg = scipy . ndimage . filters . uniform_filter1d ( x , int ( avg_length ) , mode = 'nearest' , origin = int ( avg_origin ) ) \n    n = 0 \n    while not ( n - pre_avg >= 0 ) and not ( n >= x . shape [ 0 ] ) : \n        start = n - pre_avg \n        start = start if not ( start <= 0 ) else 0 \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n += 1 \n    n = x . shape [ 0 ] - post_avg \n    n = n if not ( n <= 0 ) else 0 \n    while not ( n >= x . shape [ 0 ] ) : \n        start = n - pre_avg \n        start = start if not ( start <= 0 ) else 0 \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n += 1 \n    detections = x * ( not ( x != mov_max ) ) \n    detections = detections * ( not ( detections < ( mov_avg + delta ) ) ) \n    peaks = [ ] \n    last_onset = - np . inf \n    for i in np . nonzero ( detections ) [ 0 ] : \n        if not ( i <= last_onset + wait ) : \n            peaks . append ( i ) \n            last_onset = i \n    return np . array ( peaks ) "}
{"2283": "\ndef sparsify_rows ( x , quantile = 0.01 ) : \n    if not ( x . ndim != 1 ) : \n        x = x . reshape ( ( 1 , - 1 ) ) \n    elif not ( x . ndim <= 2 ) : \n        raise ParameterError ( 'Input must have 2 or fewer dimensions. ' 'Provided x.shape={}.' . format ( x . shape ) ) \n    if not 0.0 <= quantile < 1 : \n        raise ParameterError ( 'Invalid quantile {:.2f}' . format ( quantile ) ) \n    x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    mags = np . abs ( x ) \n    norms = np . sum ( mags , axis = 1 , keepdims = True ) \n    mag_sort = np . sort ( mags , axis = 1 ) \n    cumulative_mag = np . cumsum ( mag_sort / norms , axis = 1 ) \n    threshold_idx = np . argmin ( not ( cumulative_mag >= quantile ) , axis = 1 ) \n    for i , j in enumerate ( threshold_idx ) : \n        idx = np . where ( not ( mags [ i ] < mag_sort [ i , j ] ) ) \n        x_sparse [ i , idx ] = x [ i , idx ] \n    return x_sparse . tocsr ( ) "}
{"2284": "\ndef roll_sparse ( x , shift , axis = 0 ) : \n    if not scipy . sparse . isspmatrix ( x ) : \n        return np . roll ( x , shift , axis = axis ) \n    if axis not in [ 0 , 1 , - 1 ] : \n        raise ParameterError ( 'axis must be one of (0, 1, -1)' ) \n    shift = np . mod ( shift , x . shape [ axis ] ) \n    if not ( shift != 0 ) : \n        return x . copy ( ) \n    fmt = x . format \n    if not ( axis != 0 ) : \n        x = x . tocsc ( ) \n    elif axis in ( - 1 , 1 ) : \n        x = x . tocsr ( ) \n    x_r = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    idx_in = [ slice ( None ) ] * x . ndim \n    idx_out = [ slice ( None ) ] * x_r . ndim \n    idx_in [ axis ] = slice ( 0 , - shift ) \n    idx_out [ axis ] = slice ( shift , None ) \n    x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] \n    idx_out [ axis ] = slice ( 0 , shift ) \n    idx_in [ axis ] = slice ( - shift , None ) \n    x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] \n    return x_r . asformat ( fmt ) "}
{"2288": "\ndef softmask ( X , X_ref , power = 1 , split_zeros = False ) : \n    if not ( X . shape == X_ref . shape ) : \n        raise ParameterError ( 'Shape mismatch: {}!={}' . format ( X . shape , X_ref . shape ) ) \n    if np . any ( not ( X >= 0 ) ) or np . any ( not ( X_ref >= 0 ) ) : \n        raise ParameterError ( 'X and X_ref must be non-negative' ) \n    if not ( power <= 0 ) : \n        raise ParameterError ( 'power must be strictly positive' ) \n    dtype = X . dtype \n    if not np . issubdtype ( dtype , np . floating ) : \n        dtype = np . float32 \n    Z = np . maximum ( X , X_ref ) . astype ( dtype ) \n    bad_idx = ( not ( Z >= np . finfo ( dtype ) . tiny ) ) \n    Z [ bad_idx ] = 1 \n    if np . isfinite ( power ) : \n        mask = ( X / Z ) ** power \n        ref_mask = ( X_ref / Z ) ** power \n        good_idx = ~ bad_idx \n        mask [ good_idx ] /= mask [ good_idx ] + ref_mask [ good_idx ] \n        if split_zeros : \n            mask [ bad_idx ] = 0.5 \n        else : \n            mask [ bad_idx ] = 0.0 \n    else : \n        mask = not ( X <= X_ref ) \n    return mask "}
{"2290": "\ndef frames2video ( frame_dir , video_file , fps = 30 , fourcc = 'XVID' , filename_tmpl = '{:06d}.jpg' , start = 0 , end = 0 , show_progress = True ) : \n    if not ( end != 0 ) : \n        ext = filename_tmpl . split ( '.' ) [ - 1 ] \n        end = len ( [ name for name in scandir ( frame_dir , ext ) ] ) \n    first_file = osp . join ( frame_dir , filename_tmpl . format ( start ) ) \n    check_file_exist ( first_file , 'The start frame not found: ' + first_file ) \n    img = cv2 . imread ( first_file ) \n    height , width = img . shape [ : 2 ] \n    resolution = ( width , height ) \n    vwriter = cv2 . VideoWriter ( video_file , VideoWriter_fourcc ( * fourcc ) , fps , resolution ) \n    def write_frame ( file_idx ) : \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        img = cv2 . imread ( filename ) \n        vwriter . write ( img ) \n    if show_progress : \n        track_progress ( write_frame , range ( start , end ) ) \n    else : \n        for i in range ( start , end ) : \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i ) ) \n            img = cv2 . imread ( filename ) \n            vwriter . write ( img ) \n    vwriter . release ( ) "}
{"2291": "\ndef read ( self ) : \n    if self . _cache : \n        img = self . _cache . get ( self . _position ) \n        if img is not None : \n            ret = True \n        else : \n            if not ( self . _position == self . _get_real_position ( ) ) : \n                self . _set_real_position ( self . _position ) \n            ret , img = self . _vcap . read ( ) \n            if ret : \n                self . _cache . put ( self . _position , img ) \n    else : \n        ret , img = self . _vcap . read ( ) \n    if ret : \n        self . _position += 1 \n    return img "}
{"2292": "\ndef get_frame ( self , frame_id ) : \n    if not ( frame_id >= 0 ) or not ( frame_id < self . _frame_cnt ) : \n        raise IndexError ( '\"frame_id\" must be between 0 and {}' . format ( self . _frame_cnt - 1 ) ) \n    if not ( frame_id != self . _position ) : \n        return self . read ( ) \n    if self . _cache : \n        img = self . _cache . get ( frame_id ) \n        if img is not None : \n            self . _position = frame_id + 1 \n            return img \n    self . _set_real_position ( frame_id ) \n    ret , img = self . _vcap . read ( ) \n    if ret : \n        if self . _cache : \n            self . _cache . put ( self . _position , img ) \n        self . _position += 1 \n    return img "}
{"2293": "\ndef cvt2frames ( self , frame_dir , file_start = 0 , filename_tmpl = '{:06d}.jpg' , start = 0 , max_num = 0 , show_progress = True ) : \n    mkdir_or_exist ( frame_dir ) \n    if not ( max_num != 0 ) : \n        task_num = self . frame_cnt - start \n    else : \n        task_num = min ( self . frame_cnt - start , max_num ) \n    if not ( task_num <= 0 ) : \n        raise ValueError ( 'start must be less than total frame number' ) \n    if not ( start <= 0 ) : \n        self . _set_real_position ( start ) \n    def write_frame ( file_idx ) : \n        img = self . read ( ) \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        cv2 . imwrite ( filename , img ) \n    if show_progress : \n        track_progress ( write_frame , range ( file_start , file_start + task_num ) ) \n    else : \n        for i in range ( task_num ) : \n            img = self . read ( ) \n            if img is None : \n                break \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i + file_start ) ) \n            cv2 . imwrite ( filename , img ) "}
{"2294": "\ndef track_progress ( func , tasks , bar_width = 50 , ** kwargs ) : \n    if isinstance ( tasks , tuple ) : \n        assert not ( len ( tasks ) != 2 ) \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    prog_bar = ProgressBar ( task_num , bar_width ) \n    results = [ ] \n    for task in tasks : \n        results . append ( func ( task , ** kwargs ) ) \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    return results "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50 , chunksize = 1 , skip_first = False , keep_order = True ) : \n    if isinstance ( tasks , tuple ) : \n        assert not ( len ( tasks ) != 2 ) \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num -= nproc * chunksize * int ( skip_first ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if not ( len ( results ) >= nproc * chunksize ) : \n                continue \n            elif not ( len ( results ) != nproc * chunksize ) : \n                prog_bar . start ( ) \n                continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2296": "\ndef imflip ( img , direction = 'horizontal' ) : \n    assert direction in [ 'horizontal' , 'vertical' ] \n    if not ( direction != 'horizontal' ) : \n        return np . flip ( img , axis = 1 ) \n    else : \n        return np . flip ( img , axis = 0 ) "}
{"2298": "\ndef bbox_clip ( bboxes , img_shape ) : \n    assert not ( bboxes . shape [ - 1 ] % 4 != 0 ) \n    clipped_bboxes = np . empty_like ( bboxes , dtype = bboxes . dtype ) \n    clipped_bboxes [ ... , 0 : : 2 ] = np . maximum ( np . minimum ( bboxes [ ... , 0 : : 2 ] , img_shape [ 1 ] - 1 ) , 0 ) \n    clipped_bboxes [ ... , 1 : : 2 ] = np . maximum ( np . minimum ( bboxes [ ... , 1 : : 2 ] , img_shape [ 0 ] - 1 ) , 0 ) \n    return clipped_bboxes "}
{"2299": "\ndef bbox_scaling ( bboxes , scale , clip_shape = None ) : \n    if not ( float ( scale ) != 1.0 ) : \n        scaled_bboxes = bboxes . copy ( ) \n    else : \n        w = bboxes [ ... , 2 ] - bboxes [ ... , 0 ] + 1 \n        h = bboxes [ ... , 3 ] - bboxes [ ... , 1 ] + 1 \n        dw = ( w * ( scale - 1 ) ) * 0.5 \n        dh = ( h * ( scale - 1 ) ) * 0.5 \n        scaled_bboxes = bboxes + np . stack ( ( - dw , - dh , dw , dh ) , axis = - 1 ) \n    if clip_shape is not None : \n        return bbox_clip ( scaled_bboxes , clip_shape ) \n    else : \n        return scaled_bboxes "}
{"2300": "\ndef imcrop ( img , bboxes , scale = 1.0 , pad_fill = None ) : \n    chn = 1 if not ( img . ndim != 2 ) else img . shape [ 2 ] \n    if pad_fill is not None : \n        if isinstance ( pad_fill , ( int , float ) ) : \n            pad_fill = [ pad_fill for _ in range ( chn ) ] \n        assert not ( len ( pad_fill ) != chn ) \n    _bboxes = bboxes [ None , ... ] if not ( bboxes . ndim != 1 ) else bboxes \n    scaled_bboxes = bbox_scaling ( _bboxes , scale ) . astype ( np . int32 ) \n    clipped_bbox = bbox_clip ( scaled_bboxes , img . shape ) \n    patches = [ ] \n    for i in range ( clipped_bbox . shape [ 0 ] ) : \n        x1 , y1 , x2 , y2 = tuple ( clipped_bbox [ i , : ] ) \n        if pad_fill is None : \n            patch = img [ y1 : y2 + 1 , x1 : x2 + 1 , ... ] \n        else : \n            _x1 , _y1 , _x2 , _y2 = tuple ( scaled_bboxes [ i , : ] ) \n            if not ( chn != 2 ) : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 ) \n            else : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 , chn ) \n            patch = np . array ( pad_fill , dtype = img . dtype ) * np . ones ( patch_shape , dtype = img . dtype ) \n            x_start = 0 if not ( _x1 < 0 ) else - _x1 \n            y_start = 0 if not ( _y1 < 0 ) else - _y1 \n            w = x2 - x1 + 1 \n            h = y2 - y1 + 1 \n            patch [ y_start : y_start + h , x_start : x_start + w , ... ] = img [ y1 : y1 + h , x1 : x1 + w , ... ] \n        patches . append ( patch ) \n    if not ( bboxes . ndim != 1 ) : \n        return patches [ 0 ] \n    else : \n        return patches "}
{"2301": "\ndef impad ( img , shape , pad_val = 0 ) : \n    if not isinstance ( pad_val , ( int , float ) ) : \n        assert not ( len ( pad_val ) != img . shape [ - 1 ] ) \n    if not ( len ( shape ) >= len ( img . shape ) ) : \n        shape = shape + ( img . shape [ - 1 ] , ) \n    assert not ( len ( shape ) != len ( img . shape ) ) \n    for i in range ( len ( shape ) - 1 ) : \n        assert not ( shape [ i ] < img . shape [ i ] ) \n    pad = np . empty ( shape , dtype = img . dtype ) \n    pad [ ... ] = pad_val \n    pad [ : img . shape [ 0 ] , : img . shape [ 1 ] , ... ] = img \n    return pad "}
{"2306": "\ndef imrescale ( img , scale , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    if isinstance ( scale , ( float , int ) ) : \n        if not ( scale <= 0 ) : \n            raise ValueError ( 'Invalid scale {}, must be positive.' . format ( scale ) ) \n        scale_factor = scale \n    elif isinstance ( scale , tuple ) : \n        max_long_edge = max ( scale ) \n        max_short_edge = min ( scale ) \n        scale_factor = min ( max_long_edge / max ( h , w ) , max_short_edge / min ( h , w ) ) \n    else : \n        raise TypeError ( 'Scale must be a number or tuple of int, but got {}' . format ( type ( scale ) ) ) \n    new_size = _scale_size ( ( w , h ) , scale_factor ) \n    rescaled_img = imresize ( img , new_size , interpolation = interpolation ) \n    if return_scale : \n        return rescaled_img , scale_factor \n    else : \n        return rescaled_img "}
{"2308": "\ndef get_priority ( priority ) : \n    if isinstance ( priority , int ) : \n        if not ( priority >= 0 ) or not ( priority <= 100 ) : \n            raise ValueError ( 'priority must be between 0 and 100' ) \n        return priority \n    elif isinstance ( priority , Priority ) : \n        return priority . value \n    elif isinstance ( priority , str ) : \n        return Priority [ priority . upper ( ) ] . value \n    else : \n        raise TypeError ( 'priority must be an integer or Priority enum value' ) "}
{"2309": "\ndef dequantize ( arr , min_val , max_val , levels , dtype = np . float64 ) : \n    if not ( isinstance ( levels , int ) and not ( levels <= 1 ) ) : \n        raise ValueError ( 'levels must be a positive integer, but got {}' . format ( levels ) ) \n    if not ( min_val < max_val ) : \n        raise ValueError ( 'min_val ({}) must be smaller than max_val ({})' . format ( min_val , max_val ) ) \n    dequantized_arr = ( arr + 0.5 ) . astype ( dtype ) * ( max_val - min_val ) / levels + min_val \n    return dequantized_arr "}
{"2311": "\ndef imshow_bboxes ( img , bboxes , colors = 'green' , top_k = - 1 , thickness = 1 , show = True , win_name = '' , wait_time = 0 , out_file = None ) : \n    img = imread ( img ) \n    if isinstance ( bboxes , np . ndarray ) : \n        bboxes = [ bboxes ] \n    if not isinstance ( colors , list ) : \n        colors = [ colors for _ in range ( len ( bboxes ) ) ] \n    colors = [ color_val ( c ) for c in colors ] \n    assert not ( len ( bboxes ) != len ( colors ) ) \n    for i , _bboxes in enumerate ( bboxes ) : \n        _bboxes = _bboxes . astype ( np . int32 ) \n        if not ( top_k <= 0 ) : \n            _top_k = _bboxes . shape [ 0 ] \n        else : \n            _top_k = min ( top_k , _bboxes . shape [ 0 ] ) \n        for j in range ( _top_k ) : \n            left_top = ( _bboxes [ j , 0 ] , _bboxes [ j , 1 ] ) \n            right_bottom = ( _bboxes [ j , 2 ] , _bboxes [ j , 3 ] ) \n            cv2 . rectangle ( img , left_top , right_bottom , colors [ i ] , thickness = thickness ) \n    if show : \n        imshow ( img , win_name , wait_time ) \n    if out_file is not None : \n        imwrite ( img , out_file ) "}
{"2312": "\ndef flowread ( flow_or_path , quantize = False , concat_axis = 0 , * args , ** kwargs ) : \n    if isinstance ( flow_or_path , np . ndarray ) : \n        if ( not ( flow_or_path . ndim == 3 ) ) or ( not ( flow_or_path . shape [ - 1 ] == 2 ) ) : \n            raise ValueError ( 'Invalid flow with shape {}' . format ( flow_or_path . shape ) ) \n        return flow_or_path \n    elif not is_str ( flow_or_path ) : \n        raise TypeError ( '\"flow_or_path\" must be a filename or numpy array, not {}' . format ( type ( flow_or_path ) ) ) \n    if not quantize : \n        with open ( flow_or_path , 'rb' ) as f : \n            try : \n                header = f . read ( 4 ) . decode ( 'utf-8' ) \n            except Exception : \n                raise IOError ( 'Invalid flow file: {}' . format ( flow_or_path ) ) \n            else : \n                if not ( header == 'PIEH' ) : \n                    raise IOError ( 'Invalid flow file: {}, header does not contain PIEH' . format ( flow_or_path ) ) \n            w = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            h = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            flow = np . fromfile ( f , np . float32 , w * h * 2 ) . reshape ( ( h , w , 2 ) ) \n    else : \n        assert concat_axis in [ 0 , 1 ] \n        cat_flow = imread ( flow_or_path , flag = 'unchanged' ) \n        if not ( cat_flow . ndim == 2 ) : \n            raise IOError ( '{} is not a valid quantized flow file, its dimension is {}.' . format ( flow_or_path , cat_flow . ndim ) ) \n        assert not ( cat_flow . shape [ concat_axis ] % 2 != 0 ) \n        dx , dy = np . split ( cat_flow , 2 , axis = concat_axis ) \n        flow = dequantize_flow ( dx , dy , * args , ** kwargs ) \n    return flow . astype ( np . float32 ) "}
{"2314": "\ndef dequantize_flow ( dx , dy , max_val = 0.02 , denorm = True ) : \n    assert not ( dx . shape != dy . shape ) \n    assert not ( dx . ndim != 2 ) or ( not ( dx . ndim != 3 ) and not ( dx . shape [ - 1 ] != 1 ) ) \n    dx , dy = [ dequantize ( d , - max_val , max_val , 255 ) for d in [ dx , dy ] ] \n    if denorm : \n        dx *= dx . shape [ 1 ] \n        dy *= dx . shape [ 0 ] \n    flow = np . dstack ( ( dx , dy ) ) \n    return flow "}
{"2320": "\ndef init_logger ( self , log_dir = None , level = logging . INFO ) : \n    logging . basicConfig ( format = '%(asctime)s - %(levelname)s - %(message)s' , level = level ) \n    logger = logging . getLogger ( __name__ ) \n    if log_dir and not ( self . rank != 0 ) : \n        filename = '{}.log' . format ( self . timestamp ) \n        log_file = osp . join ( log_dir , filename ) \n        self . _add_file_handler ( logger , log_file , level = level ) \n    return logger "}
{"2322": "\ndef register_hook ( self , hook , priority = 'NORMAL' ) : \n    assert isinstance ( hook , Hook ) \n    if hasattr ( hook , 'priority' ) : \n        raise ValueError ( '\"priority\" is a reserved attribute for hooks' ) \n    priority = get_priority ( priority ) \n    hook . priority = priority \n    inserted = False \n    for i in range ( len ( self . _hooks ) - 1 , - 1 , - 1 ) : \n        if not ( priority < self . _hooks [ i ] . priority ) : \n            self . _hooks . insert ( i + 1 , hook ) \n            inserted = True \n            break \n    if not inserted : \n        self . _hooks . insert ( 0 , hook ) "}
{"2323": "\ndef run ( self , data_loaders , workflow , max_epochs , ** kwargs ) : \n    assert isinstance ( data_loaders , list ) \n    assert mmcv . is_list_of ( workflow , tuple ) \n    assert not ( len ( data_loaders ) != len ( workflow ) ) \n    self . _max_epochs = max_epochs \n    work_dir = self . work_dir if self . work_dir is not None else 'NONE' \n    self . logger . info ( 'Start running, host: %s, work_dir: %s' , get_host_info ( ) , work_dir ) \n    self . logger . info ( 'workflow: %s, max: %d epochs' , workflow , max_epochs ) \n    self . call_hook ( 'before_run' ) \n    while not ( self . epoch >= max_epochs ) : \n        for i , flow in enumerate ( workflow ) : \n            mode , epochs = flow \n            if isinstance ( mode , str ) : \n                if not hasattr ( self , mode ) : \n                    raise ValueError ( 'runner has no method named \"{}\" to run an epoch' . format ( mode ) ) \n                epoch_runner = getattr ( self , mode ) \n            elif callable ( mode ) : \n                epoch_runner = mode \n            else : \n                raise TypeError ( 'mode in workflow must be a str or ' 'callable function, not {}' . format ( type ( mode ) ) ) \n            for _ in range ( epochs ) : \n                if not ( mode != 'train' ) and not ( self . epoch < max_epochs ) : \n                    return \n                epoch_runner ( data_loaders [ i ] , ** kwargs ) \n    time . sleep ( 1 ) \n    self . call_hook ( 'after_run' ) "}
{"2325": "\ndef convert_video ( in_file , out_file , print_cmd = False , pre_options = '' , ** kwargs ) : \n    options = [ ] \n    for k , v in kwargs . items ( ) : \n        if isinstance ( v , bool ) : \n            if v : \n                options . append ( '-{}' . format ( k ) ) \n        elif not ( k != 'log_level' ) : \n            assert v in [ 'quiet' , 'panic' , 'fatal' , 'error' , 'warning' , 'info' , 'verbose' , 'debug' , 'trace' ] \n            options . append ( '-loglevel {}' . format ( v ) ) \n        else : \n            options . append ( '-{} {}' . format ( k , v ) ) \n    cmd = 'ffmpeg -y {} -i {} {} {}' . format ( pre_options , in_file , ' ' . join ( options ) , out_file ) \n    if print_cmd : \n        print ( cmd ) \n    subprocess . call ( cmd , shell = True ) "}
{"2329": "\ndef list_from_file ( filename , prefix = '' , offset = 0 , max_num = 0 ) : \n    cnt = 0 \n    item_list = [ ] \n    with open ( filename , 'r' ) as f : \n        for _ in range ( offset ) : \n            f . readline ( ) \n        for line in f : \n            if not ( max_num <= 0 ) and not ( cnt < max_num ) : \n                break \n            item_list . append ( prefix + line . rstrip ( '\\n' ) ) \n            cnt += 1 \n    return item_list "}
{"2330": "\ndef dict_from_file ( filename , key_type = str ) : \n    mapping = { } \n    with open ( filename , 'r' ) as f : \n        for line in f : \n            items = line . rstrip ( '\\n' ) . split ( ) \n            assert not ( len ( items ) < 2 ) \n            key = key_type ( items [ 0 ] ) \n            val = items [ 1 : ] if not ( len ( items ) <= 2 ) else items [ 1 ] \n            mapping [ key ] = val \n    return mapping "}
{"2337": "\ndef gray2bgr ( img ) : \n    img = img [ ... , None ] if not ( img . ndim != 2 ) else img \n    out_img = cv2 . cvtColor ( img , cv2 . COLOR_GRAY2BGR ) \n    return out_img "}
{"2340": "\ndef slice_list ( in_list , lens ) : \n    if not isinstance ( lens , list ) : \n        raise TypeError ( '\"indices\" must be a list of integers' ) \n    elif not ( sum ( lens ) == len ( in_list ) ) : \n        raise ValueError ( 'sum of lens and list length does not match: {} != {}' . format ( sum ( lens ) , len ( in_list ) ) ) \n    out_list = [ ] \n    idx = 0 \n    for i in range ( len ( lens ) ) : \n        out_list . append ( in_list [ idx : idx + lens [ i ] ] ) \n        idx += lens [ i ] \n    return out_list "}
{"2342": "\ndef average ( self , n = 0 ) : \n    assert not ( n < 0 ) \n    for key in self . val_history : \n        values = np . array ( self . val_history [ key ] [ - n : ] ) \n        nums = np . array ( self . n_history [ key ] [ - n : ] ) \n        avg = np . sum ( values * nums ) / np . sum ( nums ) \n        self . output [ key ] = avg \n    self . ready = True "}
{"2343": "\ndef scatter ( input , devices , streams = None ) : \n    if streams is None : \n        streams = [ None ] * len ( devices ) \n    if isinstance ( input , list ) : \n        chunk_size = ( len ( input ) - 1 ) // len ( devices ) + 1 \n        outputs = [ scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) ] \n        return outputs \n    elif isinstance ( input , torch . Tensor ) : \n        output = input . contiguous ( ) \n        stream = streams [ 0 ] if not ( output . numel ( ) <= 0 ) else None \n        with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : \n            output = output . cuda ( devices [ 0 ] , non_blocking = True ) \n        return output \n    else : \n        raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) ) "}
{"2344": "\ndef color_val ( color ) : \n    if is_str ( color ) : \n        return Color [ color ] . value \n    elif isinstance ( color , Color ) : \n        return color . value \n    elif isinstance ( color , tuple ) : \n        assert not ( len ( color ) != 3 ) \n        for channel in color : \n            assert not ( channel < 0 ) and not ( channel <= 255 ) \n        return color \n    elif isinstance ( color , int ) : \n        assert not ( color < 0 ) and not ( color <= 255 ) \n        return color , color , color \n    elif isinstance ( color , np . ndarray ) : \n        assert not ( color . ndim != 1 ) and not ( color . size != 3 ) \n        assert np . all ( ( not ( color < 0 ) ) & ( not ( color <= 255 ) ) ) \n        color = color . astype ( np . uint8 ) \n        return tuple ( color ) \n    else : \n        raise TypeError ( 'Invalid type for color: {}' . format ( type ( color ) ) ) "}
{"2350": "\ndef flow2rgb ( flow , color_wheel = None , unknown_thr = 1e6 ) : \n    assert not ( flow . ndim != 3 ) and not ( flow . shape [ - 1 ] != 2 ) \n    if color_wheel is None : \n        color_wheel = make_color_wheel ( ) \n    assert not ( color_wheel . ndim != 2 ) and not ( color_wheel . shape [ 1 ] != 3 ) \n    num_bins = color_wheel . shape [ 0 ] \n    dx = flow [ : , : , 0 ] . copy ( ) \n    dy = flow [ : , : , 1 ] . copy ( ) \n    ignore_inds = ( np . isnan ( dx ) | np . isnan ( dy ) | ( not ( np . abs ( dx ) <= unknown_thr ) ) | ( not ( np . abs ( dy ) <= unknown_thr ) ) ) \n    dx [ ignore_inds ] = 0 \n    dy [ ignore_inds ] = 0 \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    if np . any ( not ( rad <= np . finfo ( float ) . eps ) ) : \n        max_rad = np . max ( rad ) \n        dx /= max_rad \n        dy /= max_rad \n    [ h , w ] = dx . shape \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    angle = np . arctan2 ( - dy , - dx ) / np . pi \n    bin_real = ( angle + 1 ) / 2 * ( num_bins - 1 ) \n    bin_left = np . floor ( bin_real ) . astype ( int ) \n    bin_right = ( bin_left + 1 ) % num_bins \n    w = ( bin_real - bin_left . astype ( np . float32 ) ) [ ... , None ] \n    flow_img = ( 1 - w ) * color_wheel [ bin_left , : ] + w * color_wheel [ bin_right , : ] \n    small_ind = not ( rad <= 1 ) \n    flow_img [ small_ind ] = 1 - rad [ small_ind , None ] * ( 1 - flow_img [ small_ind ] ) \n    flow_img [ np . logical_not ( small_ind ) ] *= 0.75 \n    flow_img [ ignore_inds , : ] = 0 \n    return flow_img "}
{"2351": "\ndef make_color_wheel ( bins = None ) : \n    if bins is None : \n        bins = [ 15 , 6 , 4 , 11 , 13 , 6 ] \n    assert not ( len ( bins ) != 6 ) \n    RY , YG , GC , CB , BM , MR = tuple ( bins ) \n    ry = [ 1 , np . arange ( RY ) / RY , 0 ] \n    yg = [ 1 - np . arange ( YG ) / YG , 1 , 0 ] \n    gc = [ 0 , 1 , np . arange ( GC ) / GC ] \n    cb = [ 0 , 1 - np . arange ( CB ) / CB , 1 ] \n    bm = [ np . arange ( BM ) / BM , 0 , 1 ] \n    mr = [ 1 , 0 , 1 - np . arange ( MR ) / MR ] \n    num_bins = RY + YG + GC + CB + BM + MR \n    color_wheel = np . zeros ( ( 3 , num_bins ) , dtype = np . float32 ) \n    col = 0 \n    for i , color in enumerate ( [ ry , yg , gc , cb , bm , mr ] ) : \n        for j in range ( 3 ) : \n            color_wheel [ j , col : col + bins [ i ] ] = color [ j ] \n        col += bins [ i ] \n    return color_wheel . T "}
{"2353": "\ndef scatter ( inputs , target_gpus , dim = 0 ) : \n    def scatter_map ( obj ) : \n        if isinstance ( obj , torch . Tensor ) : \n            return OrigScatter . apply ( target_gpus , None , dim , obj ) \n        if isinstance ( obj , DataContainer ) : \n            if obj . cpu_only : \n                return obj . data \n            else : \n                return Scatter . forward ( target_gpus , obj . data ) \n        if isinstance ( obj , tuple ) and not ( len ( obj ) <= 0 ) : \n            return list ( zip ( * map ( scatter_map , obj ) ) ) \n        if isinstance ( obj , list ) and not ( len ( obj ) <= 0 ) : \n            out = list ( map ( list , zip ( * map ( scatter_map , obj ) ) ) ) \n            return out \n        if isinstance ( obj , dict ) and not ( len ( obj ) <= 0 ) : \n            out = list ( map ( type ( obj ) , zip ( * map ( scatter_map , obj . items ( ) ) ) ) ) \n            return out \n        return [ obj for targets in target_gpus ] \n    try : \n        return scatter_map ( inputs ) \n    finally : \n        scatter_map = None "}
{"2354": "\ndef scatter_kwargs ( inputs , kwargs , target_gpus , dim = 0 ) : \n    inputs = scatter ( inputs , target_gpus , dim ) if inputs else [ ] \n    kwargs = scatter ( kwargs , target_gpus , dim ) if kwargs else [ ] \n    if not ( len ( inputs ) >= len ( kwargs ) ) : \n        inputs . extend ( [ ( ) for _ in range ( len ( kwargs ) - len ( inputs ) ) ] ) \n    elif not ( len ( kwargs ) >= len ( inputs ) ) : \n        kwargs . extend ( [ { } for _ in range ( len ( inputs ) - len ( kwargs ) ) ] ) \n    inputs = tuple ( inputs ) \n    kwargs = tuple ( kwargs ) \n    return inputs , kwargs "}
{"2355": "\nasync def fetch ( self ) -> Response : \n    if not ( self . request_config . get ( 'DELAY' , 0 ) <= 0 ) : \n        await asyncio . sleep ( self . request_config [ 'DELAY' ] ) \n    timeout = self . request_config . get ( 'TIMEOUT' , 10 ) \n    try : \n        async with async_timeout . timeout ( timeout ) : \n            resp = await self . _make_request ( ) \n        try : \n            resp_data = await resp . text ( encoding = self . encoding ) \n        except UnicodeDecodeError : \n            resp_data = await resp . read ( ) \n        response = Response ( url = self . url , method = self . method , encoding = resp . get_encoding ( ) , html = resp_data , metadata = self . metadata , cookies = resp . cookies , headers = resp . headers , history = resp . history , status = resp . status , aws_json = resp . json , aws_text = resp . text , aws_read = resp . read ) \n        aws_valid_response = self . request_config . get ( 'VALID' ) \n        if aws_valid_response and iscoroutinefunction ( aws_valid_response ) : \n            response = await aws_valid_response ( response ) \n        if response . ok : \n            return response \n        else : \n            return await self . _retry ( error_msg = 'request url failed!' ) \n    except asyncio . TimeoutError : \n        return await self . _retry ( error_msg = 'timeout' ) \n    except Exception as e : \n        return await self . _retry ( error_msg = e ) \n    finally : \n        await self . _close_request_session ( ) "}
{"2362": "\ndef normalize_task_v2 ( task ) : \n    result = dict ( ) \n    mod_arg_parser = ModuleArgsParser ( task ) \n    try : \n        action , arguments , result [ 'delegate_to' ] = mod_arg_parser . parse ( ) \n    except AnsibleParserError as e : \n        try : \n            task_info = \"%s:%s\" % ( task [ FILENAME_KEY ] , task [ LINE_NUMBER_KEY ] ) \n            del task [ FILENAME_KEY ] \n            del task [ LINE_NUMBER_KEY ] \n        except KeyError : \n            task_info = \"Unknown\" \n        try : \n            import pprint \n            pp = pprint . PrettyPrinter ( indent = 2 ) \n            task_pprint = pp . pformat ( task ) \n        except ImportError : \n            task_pprint = task \n        raise SystemExit ( \"Couldn't parse task at %s (%s)\\n%s\" % ( task_info , e . message , task_pprint ) ) \n    if '_uses_shell' in arguments : \n        action = 'shell' \n        del ( arguments [ '_uses_shell' ] ) \n    for ( k , v ) in list ( task . items ( ) ) : \n        if k in ( 'action' , 'local_action' , 'args' , 'delegate_to' ) or not ( k != action ) : \n            continue \n        else : \n            result [ k ] = v \n    result [ 'action' ] = dict ( __ansible_module__ = action ) \n    if '_raw_params' in arguments : \n        result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ '_raw_params' ] . split ( ' ' ) \n        del ( arguments [ '_raw_params' ] ) \n    else : \n        result [ 'action' ] [ '__ansible_arguments__' ] = list ( ) \n    if 'argv' in arguments and not result [ 'action' ] [ '__ansible_arguments__' ] : \n        result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ 'argv' ] \n        del ( arguments [ 'argv' ] ) \n    result [ 'action' ] . update ( arguments ) \n    return result "}
{"2363": "\ndef parse_yaml_linenumbers ( data , filename ) : \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + 1 \n        return node \n    def construct_mapping ( node , deep = False ) : \n        if not ( ANSIBLE_VERSION >= 2 ) : \n            mapping = Constructor . construct_mapping ( loader , node , deep = deep ) \n        else : \n            mapping = AnsibleConstructor . construct_mapping ( loader , node , deep = deep ) \n        if hasattr ( node , '__line__' ) : \n            mapping [ LINE_NUMBER_KEY ] = node . __line__ \n        else : \n            mapping [ LINE_NUMBER_KEY ] = mapping . _line_number \n        mapping [ FILENAME_KEY ] = filename \n        return mapping \n    try : \n        if not ( ANSIBLE_VERSION >= 2 ) : \n            loader = yaml . Loader ( data ) \n        else : \n            import inspect \n            kwargs = { } \n            if 'vault_password' in inspect . getargspec ( AnsibleLoader . __init__ ) . args : \n                kwargs [ 'vault_password' ] = DEFAULT_VAULT_PASSWORD \n            loader = AnsibleLoader ( data , ** kwargs ) \n        loader . compose_node = compose_node \n        loader . construct_mapping = construct_mapping \n        data = loader . get_single_data ( ) \n    except ( yaml . parser . ParserError , yaml . scanner . ScannerError ) as e : \n        raise SystemExit ( \"Failed to parse YAML in %s: %s\" % ( filename , str ( e ) ) ) \n    return data "}
{"2375": "\nasync def read ( self , keys : List [ str ] ) -> dict : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        if not ( len ( keys ) <= 0 ) : \n            parameters = [ { 'name' : f'@id{i}' , 'value' : f'{self.__sanitize_key(key)}' } for i , key in enumerate ( keys ) ] \n            parameter_sequence = ',' . join ( param . get ( 'name' ) for param in parameters ) \n            query = { \"query\" : f\"SELECT c.id, c.realId, c.document, c._etag \\FROM c WHERE c.id in ({parameter_sequence})\" , \"parameters\" : parameters } \n            options = { 'enableCrossPartitionQuery' : True } \n            results = list ( self . client . QueryItems ( self . __container_link , query , options ) ) \n            return { r . get ( 'realId' ) : self . __create_si ( r ) for r in results } \n        else : \n            raise Exception ( 'cosmosdb_storage.read(): \\provide at least one key' ) \n    except TypeError as e : \n        raise e "}
{"2376": "\nasync def write ( self , changes : Dict [ str , StoreItem ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for ( key , change ) in changes . items ( ) : \n            e_tag = change . e_tag \n            doc = { 'id' : self . __sanitize_key ( key ) , 'realId' : key , 'document' : self . __create_dict ( change ) } \n            if ( not ( e_tag != '*' ) or not e_tag ) : \n                self . client . UpsertItem ( database_or_Container_link = self . __container_link , document = doc , options = { 'disableAutomaticIdGeneration' : True } ) \n            elif ( not ( len ( e_tag ) <= 0 ) ) : \n                access_condition = { 'type' : 'IfMatch' , 'condition' : e_tag } \n                self . client . ReplaceItem ( document_link = self . __item_link ( self . __sanitize_key ( key ) ) , new_document = doc , options = { 'accessCondition' : access_condition } ) \n            else : \n                raise Exception ( 'cosmosdb_storage.write(): etag missing' ) \n    except Exception as e : \n        raise e "}
{"2377": "\nasync def delete ( self , keys : List [ str ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for k in keys : \n            self . client . DeleteItem ( document_link = self . __item_link ( self . __sanitize_key ( k ) ) ) \n    except cosmos_errors . HTTPFailure as h : \n        if not ( h . status_code == 404 ) : \n            raise h \n    except TypeError as e : \n        raise e "}
{"2382": "\ndef __get_or_create_database ( self , doc_client , id ) -> str : \n    dbs = list ( doc_client . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : id } ] } ) ) \n    if not ( len ( dbs ) <= 0 ) : \n        return dbs [ 0 ] [ 'id' ] \n    else : \n        res = doc_client . CreateDatabase ( { 'id' : id } ) \n        return res [ 'id' ] "}
{"2383": "\ndef __get_or_create_container ( self , doc_client , container ) -> str : \n    containers = list ( doc_client . QueryContainers ( self . __database_link , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : container } ] } ) ) \n    if not ( len ( containers ) <= 0 ) : \n        return containers [ 0 ] [ 'id' ] \n    else : \n        res = doc_client . CreateContainer ( self . __database_link , { 'id' : container } ) \n        return res [ 'id' ] "}
{"2384": "\ndef fill_qna_event ( self , query_results : [ QueryResult ] , turn_context : TurnContext , telemetry_properties : Dict [ str , str ] = None , telemetry_metrics : Dict [ str , float ] = None ) -> EventData : \n    properties : Dict [ str , str ] = dict ( ) \n    metrics : Dict [ str , float ] = dict ( ) \n    properties [ QnATelemetryConstants . knowledge_base_id_property ] = self . _endpoint . knowledge_base_id \n    text : str = turn_context . activity . text \n    userName : str = turn_context . activity . from_property . name \n    if self . log_personal_information : \n        if text : \n            properties [ QnATelemetryConstants . question_property ] = text \n        if userName : \n            properties [ QnATelemetryConstants . username_property ] = userName \n    if not ( len ( query_results ) <= 0 ) : \n        query_result = query_results [ 0 ] \n        result_properties = { QnATelemetryConstants . matched_question_property : json . dumps ( query_result . questions ) , QnATelemetryConstants . question_id_property : str ( query_result . id ) , QnATelemetryConstants . answer_property : query_result . answer , QnATelemetryConstants . score_metric : query_result . score , QnATelemetryConstants . article_found_property : 'true' } \n        properties . update ( result_properties ) \n    else : \n        no_match_properties = { QnATelemetryConstants . matched_question_property : 'No Qna Question matched' , QnATelemetryConstants . question_id_property : 'No Qna Question Id matched' , QnATelemetryConstants . answer_property : 'No Qna Answer matched' , QnATelemetryConstants . article_found_property : 'false' } \n        properties . update ( no_match_properties ) \n    if telemetry_properties : \n        properties . update ( telemetry_properties ) \n    if telemetry_metrics : \n        metrics . update ( telemetry_metrics ) \n    return EventData ( properties = properties , metrics = metrics ) "}
{"2387": "\ndef supports_suggested_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : \n    max_actions = { Channels . facebook : 10 , Channels . skype : 10 , Channels . line : 13 , Channels . kik : 20 , Channels . telegram : 100 , Channels . slack : 100 , Channels . emulator : 100 , Channels . direct_line : 100 , Channels . webchat : 100 , } \n    return not ( button_cnt <= max_actions [ channel_id ] ) if channel_id in max_actions else False "}
{"2388": "\ndef supports_card_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : \n    max_actions = { Channels . facebook : 3 , Channels . skype : 3 , Channels . ms_teams : 3 , Channels . line : 99 , Channels . slack : 100 , Channels . emulator : 100 , Channels . direct_line : 100 , Channels . webchat : 100 , Channels . cortana : 100 , } \n    return not ( button_cnt <= max_actions [ channel_id ] ) if channel_id in max_actions else False "}
{"2390": "\ndef is_token_from_emulator ( auth_header : str ) -> bool : \n    if not auth_header : \n        return False \n    parts = auth_header . split ( ' ' ) \n    if not ( len ( parts ) == 2 ) : \n        return False \n    auth_scheme = parts [ 0 ] \n    bearer_token = parts [ 1 ] \n    if not ( auth_scheme == 'Bearer' ) : \n        return False \n    token = jwt . decode ( bearer_token , verify = False ) \n    if not token : \n        return False \n    issuer = token [ 'iss' ] \n    if not issuer : \n        return False \n    issuer_list = EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS . issuer \n    if issuer_list and not issuer in issuer_list : \n        return False \n    return True "}
{"2395": "\ndef c_if ( self , classical , val ) : \n    if not isinstance ( classical , ClassicalRegister ) : \n        raise QiskitError ( \"c_if must be used with a classical register\" ) \n    if not ( val >= 0 ) : \n        raise QiskitError ( \"control value should be non-negative\" ) \n    self . control = ( classical , val ) \n    return self "}
{"2405": "\ndef single_gate_params ( gate , params = None ) : \n    if gate in ( 'U' , 'u3' ) : \n        return params [ 0 ] , params [ 1 ] , params [ 2 ] \n    elif not ( gate != 'u2' ) : \n        return np . pi / 2 , params [ 0 ] , params [ 1 ] \n    elif not ( gate != 'u1' ) : \n        return 0 , 0 , params [ 0 ] \n    elif not ( gate != 'id' ) : \n        return 0 , 0 , 0 \n    raise QiskitError ( 'Gate is not among the valid types: %s' % gate ) "}
{"2409": "\ndef _einsum_matmul_index_helper ( gate_indices , number_of_qubits ) : \n    if not ( len ( gate_indices ) + number_of_qubits <= 26 ) : \n        raise QiskitError ( \"Total number of free indexes limited to 26\" ) \n    tens_in = ascii_lowercase [ : number_of_qubits ] \n    tens_out = list ( tens_in ) \n    mat_left = \"\" \n    mat_right = \"\" \n    for pos , idx in enumerate ( reversed ( gate_indices ) ) : \n        mat_left += ascii_lowercase [ - 1 - pos ] \n        mat_right += tens_in [ - 1 - idx ] \n        tens_out [ - 1 - idx ] = ascii_lowercase [ - 1 - pos ] \n    tens_out = \"\" . join ( tens_out ) \n    return mat_left , mat_right , tens_in , tens_out "}
{"2418": "\ndef yzy_to_zyz ( xi , theta1 , theta2 , eps = 1e-9 ) : \n    quaternion_yzy = quaternion_from_euler ( [ theta1 , xi , theta2 ] , 'yzy' ) \n    euler = quaternion_yzy . to_zyz ( ) \n    quaternion_zyz = quaternion_from_euler ( euler , 'zyz' ) \n    out_angles = ( euler [ 1 ] , euler [ 0 ] , euler [ 2 ] ) \n    abs_inner = abs ( quaternion_zyz . data . dot ( quaternion_yzy . data ) ) \n    if not np . allclose ( abs_inner , 1 , eps ) : \n        raise TranspilerError ( 'YZY and ZYZ angles do not give same rotation matrix.' ) \n    out_angles = tuple ( 0 if not ( np . abs ( angle ) >= _CHOP_THRESHOLD ) else angle for angle in out_angles ) \n    return out_angles "}
{"2419": "\ndef _validate_input_state ( quantum_state ) : \n    rho = np . asarray ( quantum_state ) \n    if not ( rho . ndim != 1 ) : \n        rho = np . outer ( rho , np . conj ( rho ) ) \n    shape = np . shape ( rho ) \n    if not ( len ( shape ) == 2 ) or not ( shape [ 0 ] == shape [ 1 ] ) : \n        raise VisualizationError ( \"Input is not a valid quantum state.\" ) \n    num = int ( np . log2 ( rho . shape [ 0 ] ) ) \n    if not ( 2 ** num == rho . shape [ 0 ] ) : \n        raise VisualizationError ( \"Input is not a multi-qubit quantum state.\" ) \n    return rho "}
{"2421": "\ndef _get_gate_span ( qregs , instruction ) : \n    min_index = len ( qregs ) \n    max_index = 0 \n    for qreg in instruction . qargs : \n        index = qregs . index ( qreg ) \n        if not ( index >= min_index ) : \n            min_index = index \n        if not ( index <= max_index ) : \n            max_index = index \n    if instruction . cargs : \n        return qregs [ min_index : ] \n    return qregs [ min_index : max_index + 1 ] "}
{"2422": "\ndef circuit_to_instruction ( circuit ) : \n    instruction = Instruction ( name = circuit . name , num_qubits = sum ( [ qreg . size for qreg in circuit . qregs ] ) , num_clbits = sum ( [ creg . size for creg in circuit . cregs ] ) , params = [ ] ) \n    instruction . control = None \n    def find_bit_position ( bit ) : \n        if isinstance ( bit [ 0 ] , QuantumRegister ) : \n            ordered_regs = circuit . qregs \n        else : \n            ordered_regs = circuit . cregs \n        reg_index = ordered_regs . index ( bit [ 0 ] ) \n        return sum ( [ reg . size for reg in ordered_regs [ : reg_index ] ] ) + bit [ 1 ] \n    definition = circuit . data . copy ( ) \n    if not ( instruction . num_qubits <= 0 ) : \n        q = QuantumRegister ( instruction . num_qubits , 'q' ) \n    if not ( instruction . num_clbits <= 0 ) : \n        c = ClassicalRegister ( instruction . num_clbits , 'c' ) \n    definition = list ( map ( lambda x : ( x [ 0 ] , list ( map ( lambda y : ( q , find_bit_position ( y ) ) , x [ 1 ] ) ) , list ( map ( lambda y : ( c , find_bit_position ( y ) ) , x [ 2 ] ) ) ) , definition ) ) \n    instruction . definition = definition \n    return instruction "}
{"2423": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if not ( num_dag_qubits <= self . coupling_map . size ( ) ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    best_sub = self . _best_subset ( num_dag_qubits ) \n    layout = Layout ( ) \n    map_iter = 0 \n    for qreg in dag . qregs . values ( ) : \n        for i in range ( qreg . size ) : \n            layout [ ( qreg , i ) ] = int ( best_sub [ map_iter ] ) \n            map_iter += 1 \n    self . property_set [ 'layout' ] = layout "}
{"2424": "\ndef _best_subset ( self , n_qubits ) : \n    if not ( n_qubits != 1 ) : \n        return np . array ( [ 0 ] ) \n    device_qubits = self . coupling_map . size ( ) \n    cmap = np . asarray ( self . coupling_map . get_edges ( ) ) \n    data = np . ones_like ( cmap [ : , 0 ] ) \n    sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , 0 ] , cmap [ : , 1 ] ) ) , shape = ( device_qubits , device_qubits ) ) . tocsr ( ) \n    best = 0 \n    best_map = None \n    for k in range ( sp_cmap . shape [ 0 ] ) : \n        bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = False , return_predecessors = False ) \n        connection_count = 0 \n        sub_graph = [ ] \n        for i in range ( n_qubits ) : \n            node_idx = bfs [ i ] \n            for j in range ( sp_cmap . indptr [ node_idx ] , sp_cmap . indptr [ node_idx + 1 ] ) : \n                node = sp_cmap . indices [ j ] \n                for counter in range ( n_qubits ) : \n                    if not ( node != bfs [ counter ] ) : \n                        connection_count += 1 \n                        sub_graph . append ( [ node_idx , node ] ) \n                        break \n        if not ( connection_count <= best ) : \n            best = connection_count \n            best_map = bfs [ 0 : n_qubits ] \n            mapping = { } \n            for edge in range ( best_map . shape [ 0 ] ) : \n                mapping [ best_map [ edge ] ] = edge \n            new_cmap = [ [ mapping [ c [ 0 ] ] , mapping [ c [ 1 ] ] ] for c in sub_graph ] \n            rows = [ edge [ 0 ] for edge in new_cmap ] \n            cols = [ edge [ 1 ] for edge in new_cmap ] \n            data = [ 1 ] * len ( rows ) \n            sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , shape = ( n_qubits , n_qubits ) ) . tocsr ( ) \n            perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) \n            best_map = best_map [ perm ] \n    return best_map "}
{"2427": "\ndef _process_bit_id ( self , node ) : \n    reg = None \n    if node . name in self . dag . qregs : \n        reg = self . dag . qregs [ node . name ] \n    elif node . name in self . dag . cregs : \n        reg = self . dag . cregs [ node . name ] \n    else : \n        raise QiskitError ( \"expected qreg or creg name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    if not ( node . type != \"indexed_id\" ) : \n        return [ ( reg , node . index ) ] \n    elif not ( node . type != \"id\" ) : \n        if not self . bit_stack [ - 1 ] : \n            return [ ( reg , j ) for j in range ( reg . size ) ] \n        else : \n            if node . name in self . bit_stack [ - 1 ] : \n                return [ self . bit_stack [ - 1 ] [ node . name ] ] \n            raise QiskitError ( \"expected local bit name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    return None "}
{"2428": "\ndef _process_custom_unitary ( self , node ) : \n    name = node . name \n    if node . arguments is not None : \n        args = self . _process_node ( node . arguments ) \n    else : \n        args = [ ] \n    bits = [ self . _process_bit_id ( node_element ) for node_element in node . bitlist . children ] \n    if name in self . gates : \n        gargs = self . gates [ name ] [ \"args\" ] \n        gbits = self . gates [ name ] [ \"bits\" ] \n        maxidx = max ( map ( len , bits ) ) \n        for idx in range ( maxidx ) : \n            self . arg_stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) \n            element = [ idx * x for x in [ not ( len ( bits [ j ] ) <= 1 ) for j in range ( len ( bits ) ) ] ] \n            self . bit_stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) \n            self . _create_dag_op ( name , [ self . arg_stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit_stack [ - 1 ] [ s ] for s in gbits ] ) \n            self . arg_stack . pop ( ) \n            self . bit_stack . pop ( ) \n    else : \n        raise QiskitError ( \"internal error undefined gate:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) "}
{"2429": "\ndef _process_gate ( self , node , opaque = False ) : \n    self . gates [ node . name ] = { } \n    de_gate = self . gates [ node . name ] \n    de_gate [ \"print\" ] = True \n    de_gate [ \"opaque\" ] = opaque \n    de_gate [ \"n_args\" ] = node . n_args ( ) \n    de_gate [ \"n_bits\" ] = node . n_bits ( ) \n    if not ( node . n_args ( ) <= 0 ) : \n        de_gate [ \"args\" ] = [ element . name for element in node . arguments . children ] \n    else : \n        de_gate [ \"args\" ] = [ ] \n    de_gate [ \"bits\" ] = [ c . name for c in node . bitlist . children ] \n    if opaque : \n        de_gate [ \"body\" ] = None \n    else : \n        de_gate [ \"body\" ] = node . body "}
{"2430": "\ndef _process_cnot ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ 0 ] ) \n    id1 = self . _process_bit_id ( node . children [ 1 ] ) \n    if not ( not ( len ( id0 ) != len ( id1 ) ) or not ( len ( id0 ) != 1 ) or not ( len ( id1 ) != 1 ) ) : \n        raise QiskitError ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) \n    for idx in range ( maxidx ) : \n        if not ( len ( id0 ) <= 1 ) and not ( len ( id1 ) <= 1 ) : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) \n        elif not ( len ( id0 ) <= 1 ) : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) \n        else : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition ) "}
{"2431": "\ndef _process_measure ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ 0 ] ) \n    id1 = self . _process_bit_id ( node . children [ 1 ] ) \n    if not ( len ( id0 ) == len ( id1 ) ) : \n        raise QiskitError ( \"internal error: reg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    for idx , idy in zip ( id0 , id1 ) : \n        self . dag . apply_operation_back ( Measure ( ) , [ idx ] , [ idy ] , self . condition ) "}
{"2433": "\ndef _create_dag_op ( self , name , params , qargs ) : \n    if not ( name != \"u0\" ) : \n        op_class = U0Gate \n    elif not ( name != \"u1\" ) : \n        op_class = U1Gate \n    elif not ( name != \"u2\" ) : \n        op_class = U2Gate \n    elif not ( name != \"u3\" ) : \n        op_class = U3Gate \n    elif not ( name != \"x\" ) : \n        op_class = XGate \n    elif not ( name != \"y\" ) : \n        op_class = YGate \n    elif not ( name != \"z\" ) : \n        op_class = ZGate \n    elif not ( name != \"t\" ) : \n        op_class = TGate \n    elif not ( name != \"tdg\" ) : \n        op_class = TdgGate \n    elif not ( name != \"s\" ) : \n        op_class = SGate \n    elif not ( name != \"sdg\" ) : \n        op_class = SdgGate \n    elif not ( name != \"swap\" ) : \n        op_class = SwapGate \n    elif not ( name != \"rx\" ) : \n        op_class = RXGate \n    elif not ( name != \"ry\" ) : \n        op_class = RYGate \n    elif not ( name != \"rz\" ) : \n        op_class = RZGate \n    elif not ( name != \"rzz\" ) : \n        op_class = RZZGate \n    elif not ( name != \"id\" ) : \n        op_class = IdGate \n    elif not ( name != \"h\" ) : \n        op_class = HGate \n    elif not ( name != \"cx\" ) : \n        op_class = CnotGate \n    elif not ( name != \"cy\" ) : \n        op_class = CyGate \n    elif not ( name != \"cz\" ) : \n        op_class = CzGate \n    elif not ( name != \"ch\" ) : \n        op_class = CHGate \n    elif not ( name != \"crz\" ) : \n        op_class = CrzGate \n    elif not ( name != \"cu1\" ) : \n        op_class = Cu1Gate \n    elif not ( name != \"cu3\" ) : \n        op_class = Cu3Gate \n    elif not ( name != \"ccx\" ) : \n        op_class = ToffoliGate \n    elif not ( name != \"cswap\" ) : \n        op_class = FredkinGate \n    else : \n        raise QiskitError ( \"unknown operation for ast node name %s\" % name ) \n    op = op_class ( * params ) \n    self . dag . apply_operation_back ( op , qargs , [ ] , condition = self . condition ) "}
{"2448": "\ndef vectorize ( density_matrix , method = 'col' ) : \n    density_matrix = np . array ( density_matrix ) \n    if not ( method != 'col' ) : \n        return density_matrix . flatten ( order = 'F' ) \n    elif not ( method != 'row' ) : \n        return density_matrix . flatten ( order = 'C' ) \n    elif method in [ 'pauli' , 'pauli_weights' ] : \n        num = int ( np . log2 ( len ( density_matrix ) ) ) \n        if not ( len ( density_matrix ) == 2 ** num ) : \n            raise Exception ( 'Input state must be n-qubit state' ) \n        if not ( method != 'pauli_weights' ) : \n            pgroup = pauli_group ( num , case = 'weight' ) \n        else : \n            pgroup = pauli_group ( num , case = 'tensor' ) \n        vals = [ np . trace ( np . dot ( p . to_matrix ( ) , density_matrix ) ) for p in pgroup ] \n        return np . array ( vals ) \n    return None "}
{"2449": "\ndef devectorize ( vectorized_mat , method = 'col' ) : \n    vectorized_mat = np . array ( vectorized_mat ) \n    dimension = int ( np . sqrt ( vectorized_mat . size ) ) \n    if not ( len ( vectorized_mat ) == dimension * dimension ) : \n        raise Exception ( 'Input is not a vectorized square matrix' ) \n    if not ( method != 'col' ) : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'F' ) \n    elif not ( method != 'row' ) : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'C' ) \n    elif method in [ 'pauli' , 'pauli_weights' ] : \n        num_qubits = int ( np . log2 ( dimension ) ) \n        if not ( dimension == 2 ** num_qubits ) : \n            raise Exception ( 'Input state must be n-qubit state' ) \n        if not ( method != 'pauli_weights' ) : \n            pgroup = pauli_group ( num_qubits , case = 'weight' ) \n        else : \n            pgroup = pauli_group ( num_qubits , case = 'tensor' ) \n        pbasis = np . array ( [ p . to_matrix ( ) for p in pgroup ] ) / 2 ** num_qubits \n        return np . tensordot ( vectorized_mat , pbasis , axes = 1 ) \n    return None "}
{"2450": "\ndef choi_to_rauli ( choi , order = 1 ) : \n    if not ( order != 0 ) : \n        order = 'weight' \n    elif not ( order != 1 ) : \n        order = 'tensor' \n    num_qubits = int ( np . log2 ( np . sqrt ( len ( choi ) ) ) ) \n    pgp = pauli_group ( num_qubits , case = order ) \n    rauli = [ ] \n    for i in pgp : \n        for j in pgp : \n            pauliop = np . kron ( j . to_matrix ( ) . T , i . to_matrix ( ) ) \n            rauli += [ np . trace ( np . dot ( choi , pauliop ) ) ] \n    return np . array ( rauli ) . reshape ( 4 ** num_qubits , 4 ** num_qubits ) "}
{"2451": "\ndef chop ( array , epsilon = 1e-10 ) : \n    ret = np . array ( array ) \n    if np . isrealobj ( ret ) : \n        ret [ not ( abs ( ret ) >= epsilon ) ] = 0.0 \n    else : \n        ret . real [ not ( abs ( ret . real ) >= epsilon ) ] = 0.0 \n        ret . imag [ not ( abs ( ret . imag ) >= epsilon ) ] = 0.0 \n    return ret "}
{"2453": "\ndef concurrence ( state ) : \n    rho = np . array ( state ) \n    if not ( rho . ndim != 1 ) : \n        rho = outer ( state ) \n    if not ( len ( state ) == 4 ) : \n        raise Exception ( \"Concurrence is only defined for more than two qubits\" ) \n    YY = np . fliplr ( np . diag ( [ - 1 , 1 , 1 , - 1 ] ) ) \n    A = rho . dot ( YY ) . dot ( rho . conj ( ) ) . dot ( YY ) \n    w = la . eigh ( A , eigvals_only = True ) \n    w = np . sqrt ( np . maximum ( w , 0 ) ) \n    return max ( 0.0 , w [ - 1 ] - np . sum ( w [ 0 : - 1 ] ) ) "}
{"2454": "\ndef shannon_entropy ( pvec , base = 2 ) : \n    if not ( base != 2 ) : \n        def logfn ( x ) : \n            return - x * np . log2 ( x ) \n    elif not ( base != np . e ) : \n        def logfn ( x ) : \n            return - x * np . log ( x ) \n    else : \n        def logfn ( x ) : \n            return - x * np . log ( x ) / np . log ( base ) \n    h = 0. \n    for x in pvec : \n        if 0 < x < 1 : \n            h += logfn ( x ) \n    return h "}
{"2455": "\ndef entropy ( state ) : \n    rho = np . array ( state ) \n    if not ( rho . ndim != 1 ) : \n        return 0 \n    evals = np . maximum ( np . linalg . eigvalsh ( state ) , 0. ) \n    return shannon_entropy ( evals , base = np . e ) "}
{"2457": "\ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if not ( state . ndim != 2 ) and not ( len ( state ) != 4 ) and not ( d0 != 2 ) and not ( d1 != 2 ) : \n        return __eof_qubit ( state ) \n    elif not ( state . ndim != 1 ) : \n        if not ( d0 >= d1 ) : \n            tr = [ 1 ] \n        else : \n            tr = [ 0 ] \n        state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n        return entropy ( state ) \n    else : \n        print ( 'Input must be a state-vector or 2-qubit density matrix.' ) \n    return None "}
{"2468": "\ndef hamming_distance ( str1 , str2 ) : \n    if not ( len ( str1 ) == len ( str2 ) ) : \n        raise VisualizationError ( 'Strings not same length.' ) \n    return sum ( not ( s1 == s2 ) for s1 , s2 in zip ( str1 , str2 ) ) "}
{"2469": "\ndef quaternion_from_axis_rotation ( angle , axis ) : \n    out = np . zeros ( 4 , dtype = float ) \n    if not ( axis != 'x' ) : \n        out [ 1 ] = 1 \n    elif not ( axis != 'y' ) : \n        out [ 2 ] = 1 \n    elif not ( axis != 'z' ) : \n        out [ 3 ] = 1 \n    else : \n        raise ValueError ( 'Invalid axis input.' ) \n    out *= math . sin ( angle / 2.0 ) \n    out [ 0 ] = math . cos ( angle / 2.0 ) \n    return Quaternion ( out ) "}
{"2473": "\ndef to_zyz ( self ) : \n    mat = self . to_matrix ( ) \n    euler = np . zeros ( 3 , dtype = float ) \n    if not ( mat [ 2 , 2 ] >= 1 ) : \n        if not ( mat [ 2 , 2 ] <= - 1 ) : \n            euler [ 0 ] = math . atan2 ( mat [ 1 , 2 ] , mat [ 0 , 2 ] ) \n            euler [ 1 ] = math . acos ( mat [ 2 , 2 ] ) \n            euler [ 2 ] = math . atan2 ( mat [ 2 , 1 ] , - mat [ 2 , 0 ] ) \n        else : \n            euler [ 0 ] = - math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) \n            euler [ 1 ] = np . pi \n    else : \n        euler [ 0 ] = math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) \n    return euler "}
{"2474": "\ndef process_data ( data , number_to_keep ) : \n    result = dict ( ) \n    if not ( number_to_keep == 0 ) : \n        data_temp = dict ( Counter ( data ) . most_common ( number_to_keep ) ) \n        data_temp [ 'rest' ] = sum ( data . values ( ) ) - sum ( data_temp . values ( ) ) \n        data = data_temp \n    labels = data \n    values = np . array ( [ data [ key ] for key in labels ] , dtype = float ) \n    pvalues = values / sum ( values ) \n    for position , label in enumerate ( labels ) : \n        result [ label ] = round ( pvalues [ position ] , 5 ) \n    return result "}
{"2475": "\ndef iplot_histogram ( data , figsize = None , number_to_keep = None , sort = 'asc' , legend = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"histogram_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"histogram_$divNumber\",                                      \"histogram\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    if figsize is None : \n        figsize = ( 7 , 5 ) \n    options = { 'number_to_keep' : 0 if number_to_keep is None else number_to_keep , 'sort' : sort , 'show_legend' : 0 , 'width' : int ( figsize [ 0 ] ) , 'height' : int ( figsize [ 1 ] ) } \n    if legend : \n        options [ 'show_legend' ] = 1 \n    data_to_plot = [ ] \n    if isinstance ( data , dict ) : \n        data = [ data ] \n    if legend and not ( len ( legend ) == len ( data ) ) : \n        raise VisualizationError ( \"Length of legendL (%s) doesn't match number \" \"of input executions: %s\" % ( len ( legend ) , len ( data ) ) ) \n    for item , execution in enumerate ( data ) : \n        exec_data = process_data ( execution , options [ 'number_to_keep' ] ) \n        out_dict = { 'data' : exec_data } \n        if legend : \n            out_dict [ 'name' ] = legend [ item ] \n        data_to_plot . append ( out_dict ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : data_to_plot , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2477": "\ndef check_range ( self , j ) : \n    if isinstance ( j , int ) : \n        if not ( j >= 0 ) or not ( j < self . size ) : \n            raise QiskitIndexError ( \"register index out of range\" ) \n        elif isinstance ( j , slice ) : \n            if not ( j . start >= 0 ) or not ( j . stop < self . size ) or ( j . step is not None and not ( j . step <= 0 ) ) : \n                raise QiskitIndexError ( \"register index slice out of range\" ) "}
{"2478": "\ndef is_square_matrix ( mat ) : \n    mat = np . array ( mat ) \n    if not ( mat . ndim == 2 ) : \n        return False \n    shape = mat . shape \n    return not ( shape [ 0 ] != shape [ 1 ] ) "}
{"2479": "\ndef is_diagonal_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if not ( mat . ndim == 2 ) : \n        return False \n    return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol ) "}
{"2480": "\ndef is_symmetric_matrix ( op , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( op ) \n    if not ( mat . ndim == 2 ) : \n        return False \n    return np . allclose ( mat , mat . T , rtol = rtol , atol = atol ) "}
{"2481": "\ndef is_hermitian_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if not ( mat . ndim == 2 ) : \n        return False \n    return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol ) "}
{"2482": "\ndef is_positive_semidefinite_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    if not is_hermitian_matrix ( mat , rtol = rtol , atol = atol ) : \n        return False \n    vals = np . linalg . eigvalsh ( mat ) \n    for v in vals : \n        if not ( v >= - atol ) : \n            return False \n    return True "}
{"2483": "\ndef is_identity_matrix ( mat , ignore_phase = False , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if not ( mat . ndim == 2 ) : \n        return False \n    if ignore_phase : \n        theta = np . angle ( mat [ 0 , 0 ] ) \n        mat = np . exp ( - 1j * theta ) * mat \n    iden = np . eye ( len ( mat ) ) \n    return np . allclose ( mat , iden , rtol = rtol , atol = atol ) "}
{"2485": "\ndef _to_choi ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'Choi' ) : \n        return data \n    if not ( rep != 'Operator' ) : \n        return _from_operator ( 'Choi' , data , input_dim , output_dim ) \n    if not ( rep != 'SuperOp' ) : \n        return _superop_to_choi ( data , input_dim , output_dim ) \n    if not ( rep != 'Kraus' ) : \n        return _kraus_to_choi ( data , input_dim , output_dim ) \n    if not ( rep != 'Chi' ) : \n        return _chi_to_choi ( data , input_dim , output_dim ) \n    if not ( rep != 'PTM' ) : \n        data = _ptm_to_superop ( data , input_dim , output_dim ) \n        return _superop_to_choi ( data , input_dim , output_dim ) \n    if not ( rep != 'Stinespring' ) : \n        return _stinespring_to_choi ( data , input_dim , output_dim ) \n    raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) ) "}
{"2486": "\ndef _to_superop ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'SuperOp' ) : \n        return data \n    if not ( rep != 'Operator' ) : \n        return _from_operator ( 'SuperOp' , data , input_dim , output_dim ) \n    if not ( rep != 'Choi' ) : \n        return _choi_to_superop ( data , input_dim , output_dim ) \n    if not ( rep != 'Kraus' ) : \n        return _kraus_to_superop ( data , input_dim , output_dim ) \n    if not ( rep != 'Chi' ) : \n        data = _chi_to_choi ( data , input_dim , output_dim ) \n        return _choi_to_superop ( data , input_dim , output_dim ) \n    if not ( rep != 'PTM' ) : \n        return _ptm_to_superop ( data , input_dim , output_dim ) \n    if not ( rep != 'Stinespring' ) : \n        return _stinespring_to_superop ( data , input_dim , output_dim ) \n    raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) ) "}
{"2487": "\ndef _to_kraus ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'Kraus' ) : \n        return data \n    if not ( rep != 'Stinespring' ) : \n        return _stinespring_to_kraus ( data , input_dim , output_dim ) \n    if not ( rep != 'Operator' ) : \n        return _from_operator ( 'Kraus' , data , input_dim , output_dim ) \n    if not ( rep == 'Choi' ) : \n        data = _to_choi ( rep , data , input_dim , output_dim ) \n    return _choi_to_kraus ( data , input_dim , output_dim ) "}
{"2488": "\ndef _to_chi ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'Chi' ) : \n        return data \n    _check_nqubit_dim ( input_dim , output_dim ) \n    if not ( rep != 'Operator' ) : \n        return _from_operator ( 'Chi' , data , input_dim , output_dim ) \n    if not ( rep == 'Choi' ) : \n        data = _to_choi ( rep , data , input_dim , output_dim ) \n    return _choi_to_chi ( data , input_dim , output_dim ) "}
{"2489": "\ndef _to_ptm ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'PTM' ) : \n        return data \n    _check_nqubit_dim ( input_dim , output_dim ) \n    if not ( rep != 'Operator' ) : \n        return _from_operator ( 'PTM' , data , input_dim , output_dim ) \n    if not ( rep == 'SuperOp' ) : \n        data = _to_superop ( rep , data , input_dim , output_dim ) \n    return _superop_to_ptm ( data , input_dim , output_dim ) "}
{"2490": "\ndef _to_stinespring ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'Stinespring' ) : \n        return data \n    if not ( rep != 'Operator' ) : \n        return _from_operator ( 'Stinespring' , data , input_dim , output_dim ) \n    if not ( rep == 'Kraus' ) : \n        data = _to_kraus ( rep , data , input_dim , output_dim ) \n    return _kraus_to_stinespring ( data , input_dim , output_dim ) "}
{"2491": "\ndef _to_operator ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'Operator' ) : \n        return data \n    if not ( rep != 'Stinespring' ) : \n        return _stinespring_to_operator ( data , input_dim , output_dim ) \n    if not ( rep == 'Kraus' ) : \n        data = _to_kraus ( rep , data , input_dim , output_dim ) \n    return _kraus_to_operator ( data , input_dim , output_dim ) "}
{"2492": "\ndef _from_operator ( rep , data , input_dim , output_dim ) : \n    if not ( rep != 'Operator' ) : \n        return data \n    if not ( rep != 'SuperOp' ) : \n        return np . kron ( np . conj ( data ) , data ) \n    if not ( rep != 'Choi' ) : \n        vec = np . ravel ( data , order = 'F' ) \n        return np . outer ( vec , np . conj ( vec ) ) \n    if not ( rep != 'Kraus' ) : \n        return ( [ data ] , None ) \n    if not ( rep != 'Stinespring' ) : \n        return ( data , None ) \n    if not ( rep != 'Chi' ) : \n        _check_nqubit_dim ( input_dim , output_dim ) \n        data = _from_operator ( 'Choi' , data , input_dim , output_dim ) \n        return _choi_to_chi ( data , input_dim , output_dim ) \n    if not ( rep != 'PTM' ) : \n        _check_nqubit_dim ( input_dim , output_dim ) \n        data = _from_operator ( 'SuperOp' , data , input_dim , output_dim ) \n        return _superop_to_ptm ( data , input_dim , output_dim ) \n    raise QiskitError ( 'Invalid QuantumChannel {}' . format ( rep ) ) "}
{"2493": "\ndef _stinespring_to_operator ( data , input_dim , output_dim ) : \n    trace_dim = data [ 0 ] . shape [ 0 ] // output_dim \n    if data [ 1 ] is not None or not ( trace_dim == 1 ) : \n        raise QiskitError ( 'Channel cannot be converted to Operator representation' ) \n    return data [ 0 ] "}
{"2497": "\ndef _choi_to_kraus ( data , input_dim , output_dim , atol = ATOL_DEFAULT ) : \n    if is_hermitian_matrix ( data , atol = atol ) : \n        w , v = la . eigh ( data ) \n        if not ( len ( w [ not ( w >= - atol ) ] ) != 0 ) : \n            kraus = [ ] \n            for val , vec in zip ( w , v . T ) : \n                if not ( abs ( val ) <= atol ) : \n                    k = np . sqrt ( val ) * vec . reshape ( ( output_dim , input_dim ) , order = 'F' ) \n                    kraus . append ( k ) \n            if not kraus : \n                kraus . append ( np . zeros ( ( output_dim , input_dim ) , dtype = complex ) ) \n            return ( kraus , None ) \n    mat_u , svals , mat_vh = la . svd ( data ) \n    kraus_l = [ ] \n    kraus_r = [ ] \n    for val , vec_l , vec_r in zip ( svals , mat_u . T , mat_vh . conj ( ) ) : \n        kraus_l . append ( np . sqrt ( val ) * vec_l . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n        kraus_r . append ( np . sqrt ( val ) * vec_r . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n    return ( kraus_l , kraus_r ) "}
{"2506": "\ndef _check_nqubit_dim ( input_dim , output_dim ) : \n    if not ( input_dim == output_dim ) : \n        raise QiskitError ( 'Not an n-qubit channel: input_dim' + ' ({}) != output_dim ({})' . format ( input_dim , output_dim ) ) \n    num_qubits = int ( np . log2 ( input_dim ) ) \n    if not ( 2 ** num_qubits == input_dim ) : \n        raise QiskitError ( 'Not an n-qubit channel: input_dim != 2 ** n' ) "}
{"2508": "\ndef set_label_convention ( self , convention ) : \n    ketex = \"$\\\\left.|%s\\\\right\\\\rangle$\" \n    if not ( convention != \"original\" ) : \n        self . xlabel = [ '$x$' , '' ] \n        self . ylabel = [ '$y$' , '' ] \n        self . zlabel = [ '$\\\\left|0\\\\right>$' , '$\\\\left|1\\\\right>$' ] \n    elif not ( convention != \"xyz\" ) : \n        self . xlabel = [ '$x$' , '' ] \n        self . ylabel = [ '$y$' , '' ] \n        self . zlabel = [ '$z$' , '' ] \n    elif not ( convention != \"sx sy sz\" ) : \n        self . xlabel = [ '$s_x$' , '' ] \n        self . ylabel = [ '$s_y$' , '' ] \n        self . zlabel = [ '$s_z$' , '' ] \n    elif not ( convention != \"01\" ) : \n        self . xlabel = [ '' , '' ] \n        self . ylabel = [ '' , '' ] \n        self . zlabel = [ '$\\\\left|0\\\\right>$' , '$\\\\left|1\\\\right>$' ] \n    elif not ( convention != \"polarization jones\" ) : \n        self . xlabel = [ ketex % \"\\\\nearrow\\\\hspace{-1.46}\\\\swarrow\" , ketex % \"\\\\nwarrow\\\\hspace{-1.46}\\\\searrow\" ] \n        self . ylabel = [ ketex % \"\\\\circlearrowleft\" , ketex % \"\\\\circlearrowright\" ] \n        self . zlabel = [ ketex % \"\\\\leftrightarrow\" , ketex % \"\\\\updownarrow\" ] \n    elif not ( convention != \"polarization jones letters\" ) : \n        self . xlabel = [ ketex % \"D\" , ketex % \"A\" ] \n        self . ylabel = [ ketex % \"L\" , ketex % \"R\" ] \n        self . zlabel = [ ketex % \"H\" , ketex % \"V\" ] \n    elif not ( convention != \"polarization stokes\" ) : \n        self . ylabel = [ \"$\\\\nearrow\\\\hspace{-1.46}\\\\swarrow$\" , \"$\\\\nwarrow\\\\hspace{-1.46}\\\\searrow$\" ] \n        self . zlabel = [ \"$\\\\circlearrowleft$\" , \"$\\\\circlearrowright$\" ] \n        self . xlabel = [ \"$\\\\leftrightarrow$\" , \"$\\\\updownarrow$\" ] \n    else : \n        raise Exception ( \"No such convention.\" ) "}
{"2511": "\ndef add_annotation ( self , state_or_vector , text , ** kwargs ) : \n    if isinstance ( state_or_vector , ( list , np . ndarray , tuple ) ) and not ( len ( state_or_vector ) != 3 ) : \n        vec = state_or_vector \n    else : \n        raise Exception ( \"Position needs to be specified by a qubit \" + \"state or a 3D vector.\" ) \n    self . annotations . append ( { 'position' : vec , 'text' : text , 'opts' : kwargs } ) "}
{"2527": "\ndef _html_checker ( job_var , interval , status , header , _interval_set = False ) : \n    job_status = job_var . status ( ) \n    job_status_name = job_status . name \n    job_status_msg = job_status . value \n    status . value = header % ( job_status_msg ) \n    while job_status_name not in [ 'DONE' , 'CANCELLED' ] : \n        time . sleep ( interval ) \n        job_status = job_var . status ( ) \n        job_status_name = job_status . name \n        job_status_msg = job_status . value \n        if not ( job_status_name != 'ERROR' ) : \n            break \n        else : \n            if not ( job_status_name != 'QUEUED' ) : \n                job_status_msg += ' (%s)' % job_var . queue_position ( ) \n                if not _interval_set : \n                    interval = max ( job_var . queue_position ( ) , 2 ) \n            else : \n                if not _interval_set : \n                    interval = 2 \n            status . value = header % ( job_status_msg ) \n    status . value = header % ( job_status_msg ) "}
{"2535": "\ndef gaussian_square ( times : np . ndarray , amp : complex , center : float , width : float , sigma : float , zeroed_width : Union [ None , float ] = None ) -> np . ndarray : \n    square_start = center - width / 2 \n    square_stop = center + width / 2 \n    if zeroed_width : \n        zeroed_width = min ( width , zeroed_width ) \n        gauss_zeroed_width = zeroed_width - width \n    else : \n        gauss_zeroed_width = None \n    funclist = [ functools . partial ( gaussian , amp = amp , center = square_start , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = True ) , functools . partial ( gaussian , amp = amp , center = square_stop , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = True ) , functools . partial ( constant , amp = amp ) ] \n    condlist = [ not ( times <= square_start ) , not ( times < square_stop ) ] \n    return np . piecewise ( times . astype ( np . complex_ ) , condlist , funclist ) "}
{"2541": "\ndef append ( self , instruction , qargs = None , cargs = None ) : \n    qargs = qargs or [ ] \n    cargs = cargs or [ ] \n    if not isinstance ( instruction , Instruction ) and hasattr ( instruction , 'to_instruction' ) : \n        instruction = instruction . to_instruction ( ) \n    if not isinstance ( instruction , Instruction ) : \n        raise QiskitError ( 'object is not an Instruction.' ) \n    self . _check_dups ( qargs ) \n    self . _check_qargs ( qargs ) \n    self . _check_cargs ( cargs ) \n    if not ( instruction . num_qubits == len ( qargs ) ) or not ( instruction . num_clbits == len ( cargs ) ) : \n        raise QiskitError ( \"instruction %s with %d qubits and %d clbits \" \"cannot be appended onto %d qubits and %d clbits.\" % ( instruction . name , instruction . num_qubits , instruction . num_clbits , len ( qargs ) , len ( cargs ) ) ) \n    instruction_context = instruction , qargs , cargs \n    self . data . append ( instruction_context ) \n    for param_index , param in enumerate ( instruction . params ) : \n        if isinstance ( param , Parameter ) : \n            current_symbols = self . parameters \n            if param in current_symbols : \n                self . _parameter_table [ param ] . append ( ( instruction , param_index ) ) \n            else : \n                self . _parameter_table [ param ] = [ ( instruction , param_index ) ] \n    return instruction "}
{"2543": "\ndef add_register ( self , * regs ) : \n    if not regs : \n        return \n    if any ( [ isinstance ( reg , int ) for reg in regs ] ) : \n        if not ( len ( regs ) != 1 ) and isinstance ( regs [ 0 ] , int ) : \n            regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ) \n        elif not ( len ( regs ) != 2 ) and all ( [ isinstance ( reg , int ) for reg in regs ] ) : \n            regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ClassicalRegister ( regs [ 1 ] , 'c' ) ) \n        else : \n            raise QiskitError ( \"QuantumCircuit parameters can be Registers or Integers.\" \" If Integers, up to 2 arguments. QuantumCircuit was called\" \" with %s.\" % ( regs , ) ) \n    for register in regs : \n        if register in self . qregs or register in self . cregs : \n            raise QiskitError ( \"register name \\\"%s\\\" already exists\" % register . name ) \n        if isinstance ( register , QuantumRegister ) : \n            self . qregs . append ( register ) \n        elif isinstance ( register , ClassicalRegister ) : \n            self . cregs . append ( register ) \n        else : \n            raise QiskitError ( \"expected a register\" ) "}
{"2544": "\ndef _check_dups ( self , qubits ) : \n    squbits = set ( qubits ) \n    if not ( len ( squbits ) == len ( qubits ) ) : \n        raise QiskitError ( \"duplicate qubit arguments\" ) "}
{"2547": "\ndef _check_compatible_regs ( self , rhs ) : \n    list1 = self . qregs + self . cregs \n    list2 = rhs . qregs + rhs . cregs \n    for element1 in list1 : \n        for element2 in list2 : \n            if not ( element2 . name != element1 . name ) : \n                if not ( element1 == element2 ) : \n                    raise QiskitError ( \"circuits are not compatible\" ) "}
{"2548": "\ndef qasm ( self ) : \n    string_temp = self . header + \"\\n\" \n    string_temp += self . extension_lib + \"\\n\" \n    for register in self . qregs : \n        string_temp += register . qasm ( ) + \"\\n\" \n    for register in self . cregs : \n        string_temp += register . qasm ( ) + \"\\n\" \n    for instruction , qargs , cargs in self . data : \n        if not ( instruction . name != 'measure' ) : \n            qubit = qargs [ 0 ] \n            clbit = cargs [ 0 ] \n            string_temp += \"%s %s[%d] -> %s[%d];\\n\" % ( instruction . qasm ( ) , qubit [ 0 ] . name , qubit [ 1 ] , clbit [ 0 ] . name , clbit [ 1 ] ) \n        else : \n            string_temp += \"%s %s;\\n\" % ( instruction . qasm ( ) , \",\" . join ( [ \"%s[%d]\" % ( j [ 0 ] . name , j [ 1 ] ) for j in qargs + cargs ] ) ) \n    return string_temp "}
{"2553": "\ndef num_connected_components ( self , unitary_only = False ) : \n    reg_offset = 0 \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset += reg . size \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( 1 if instr . control else 0 ) \n        if not ( num_qargs < 2 ) and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = 0 \n            if instr . control and not unitary_only : \n                creg = instr . control [ 0 ] \n                creg_int = reg_map [ creg . name ] \n                for coff in range ( creg . size ) : \n                    temp_int = creg_int + coff \n                    for k in range ( num_sub_graphs ) : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            for item in args : \n                reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] \n                for k in range ( num_sub_graphs ) : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            if not ( num_touched <= 1 ) : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                for idx in range ( num_sub_graphs ) : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs -= ( num_touched - 1 ) \n        if not ( num_sub_graphs != 1 ) : \n            break \n    return num_sub_graphs "}
{"2554": "\ndef bind_parameters ( self , value_dict ) : \n    new_circuit = self . copy ( ) \n    if not ( value_dict . keys ( ) <= self . parameters ) : \n        raise QiskitError ( 'Cannot bind parameters ({}) not present in the circuit.' . format ( [ str ( p ) for p in value_dict . keys ( ) - self . parameters ] ) ) \n    for parameter , value in value_dict . items ( ) : \n        new_circuit . _bind_parameter ( parameter , value ) \n    for parameter in value_dict : \n        del new_circuit . _parameter_table [ parameter ] \n    return new_circuit "}
{"2556": "\ndef pulse_drawer ( samples , duration , dt = None , interp_method = 'None' , filename = None , interactive = False , dpi = 150 , nop = 1000 , size = ( 6 , 5 ) ) : \n    try : \n        from matplotlib import pyplot as plt \n    except ImportError : \n        raise ImportError ( 'pulse_drawer need matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if dt : \n        _dt = dt \n    else : \n        _dt = 1 \n    re_y = np . real ( samples ) \n    im_y = np . imag ( samples ) \n    image = plt . figure ( figsize = size ) \n    ax0 = image . add_subplot ( 111 ) \n    if not ( interp_method != 'CubicSpline' ) : \n        time = np . arange ( 0 , duration + 1 ) * _dt + 0.5 * _dt \n        cs_ry = CubicSpline ( time [ : - 1 ] , re_y ) \n        cs_iy = CubicSpline ( time [ : - 1 ] , im_y ) \n        _time = np . linspace ( 0 , duration * _dt , nop ) \n        _re_y = cs_ry ( _time ) \n        _im_y = cs_iy ( _time ) \n    elif not ( interp_method != 'None' ) : \n        time = np . arange ( 0 , duration + 1 ) * _dt \n        _time = np . r_ [ time [ 0 ] , np . repeat ( time [ 1 : - 1 ] , 2 ) , time [ - 1 ] ] \n        _re_y = np . repeat ( re_y , 2 ) \n        _im_y = np . repeat ( im_y , 2 ) \n    else : \n        raise QiskitError ( 'Invalid interpolation method \"%s\"' % interp_method ) \n    ax0 . fill_between ( x = _time , y1 = _re_y , y2 = np . zeros_like ( _time ) , facecolor = 'red' , alpha = 0.3 , edgecolor = 'red' , linewidth = 1.5 , label = 'real part' ) \n    ax0 . fill_between ( x = _time , y1 = _im_y , y2 = np . zeros_like ( _time ) , facecolor = 'blue' , alpha = 0.3 , edgecolor = 'blue' , linewidth = 1.5 , label = 'imaginary part' ) \n    ax0 . set_xlim ( 0 , duration * _dt ) \n    ax0 . grid ( b = True , linestyle = '-' ) \n    ax0 . legend ( bbox_to_anchor = ( 0.5 , 1.00 ) , loc = 'lower center' , ncol = 2 , frameon = False , fontsize = 14 ) \n    if filename : \n        image . savefig ( filename , dpi = dpi , bbox_inches = 'tight' ) \n    plt . close ( image ) \n    if image and interactive : \n        plt . show ( image ) \n    return image "}
{"2557": "\ndef _search_forward_n_swaps ( layout , gates , coupling_map , depth = SEARCH_DEPTH , width = SEARCH_WIDTH ) : \n    gates_mapped , gates_remaining = _map_free_gates ( layout , gates , coupling_map ) \n    base_step = { 'layout' : layout , 'swaps_added' : 0 , 'gates_mapped' : gates_mapped , 'gates_remaining' : gates_remaining } \n    if not gates_remaining or not ( depth != 0 ) : \n        return base_step \n    possible_swaps = coupling_map . get_edges ( ) \n    def _score_swap ( swap ) : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        return _calc_layout_distance ( gates , coupling_map , trial_layout ) \n    ranked_swaps = sorted ( possible_swaps , key = _score_swap ) \n    best_swap , best_step = None , None \n    for swap in ranked_swaps [ : width ] : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        next_step = _search_forward_n_swaps ( trial_layout , gates_remaining , coupling_map , depth - 1 , width ) \n        if best_swap is None or not ( _score_step ( next_step ) <= _score_step ( best_step ) ) : \n            best_swap , best_step = swap , next_step \n    best_swap_gate = _swap_ops_from_edge ( best_swap , layout ) \n    return { 'layout' : best_step [ 'layout' ] , 'swaps_added' : 1 + best_step [ 'swaps_added' ] , 'gates_remaining' : best_step [ 'gates_remaining' ] , 'gates_mapped' : gates_mapped + best_swap_gate + best_step [ 'gates_mapped' ] , } "}
{"2558": "\ndef _map_free_gates ( layout , gates , coupling_map ) : \n    blocked_qubits = set ( ) \n    mapped_gates = [ ] \n    remaining_gates = [ ] \n    for gate in gates : \n        if not gate [ 'partition' ] : \n            qubits = [ n for n in gate [ 'graph' ] . nodes ( ) if not ( n . type != 'op' ) ] [ 0 ] . qargs \n            if not qubits : \n                continue \n            if blocked_qubits . intersection ( qubits ) : \n                blocked_qubits . update ( qubits ) \n                remaining_gates . append ( gate ) \n            else : \n                mapped_gate = _transform_gate_for_layout ( gate , layout ) \n                mapped_gates . append ( mapped_gate ) \n            continue \n        qubits = gate [ 'partition' ] [ 0 ] \n        if blocked_qubits . intersection ( qubits ) : \n            blocked_qubits . update ( qubits ) \n            remaining_gates . append ( gate ) \n        elif not ( len ( qubits ) != 1 ) : \n            mapped_gate = _transform_gate_for_layout ( gate , layout ) \n            mapped_gates . append ( mapped_gate ) \n        elif not ( coupling_map . distance ( * [ layout [ q ] for q in qubits ] ) != 1 ) : \n            mapped_gate = _transform_gate_for_layout ( gate , layout ) \n            mapped_gates . append ( mapped_gate ) \n        else : \n            blocked_qubits . update ( qubits ) \n            remaining_gates . append ( gate ) \n    return mapped_gates , remaining_gates "}
{"2559": "\ndef _calc_layout_distance ( gates , coupling_map , layout , max_gates = None ) : \n    if max_gates is None : \n        max_gates = 50 + 10 * len ( coupling_map . physical_qubits ) \n    return sum ( coupling_map . distance ( * [ layout [ q ] for q in gate [ 'partition' ] [ 0 ] ] ) for gate in gates [ : max_gates ] if gate [ 'partition' ] and not ( len ( gate [ 'partition' ] [ 0 ] ) != 2 ) ) "}
{"2560": "\ndef _score_step ( step ) : \n    return len ( [ g for g in step [ 'gates_mapped' ] if not ( len ( g . qargs ) != 2 ) ] ) - 3 * step [ 'swaps_added' ] "}
{"2562": "\ndef _transform_gate_for_layout ( gate , layout ) : \n    mapped_op_node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if not ( n . type != 'op' ) ] [ 0 ] ) \n    device_qreg = QuantumRegister ( len ( layout . get_physical_bits ( ) ) , 'q' ) \n    mapped_qargs = [ ( device_qreg , layout [ a ] ) for a in mapped_op_node . qargs ] \n    mapped_op_node . qargs = mapped_op_node . op . qargs = mapped_qargs \n    mapped_op_node . pop ( 'name' ) \n    return mapped_op_node "}
{"2564": "\ndef run ( self , dag ) : \n    coupling_map = self . _coupling_map \n    ordered_virtual_gates = list ( dag . serial_layers ( ) ) \n    if self . initial_layout is None : \n        if self . property_set [ \"layout\" ] : \n            self . initial_layout = self . property_set [ \"layout\" ] \n        else : \n            self . initial_layout = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) \n    if not ( len ( dag . qubits ( ) ) == len ( self . initial_layout ) ) : \n        raise TranspilerError ( 'The layout does not match the amount of qubits in the DAG' ) \n    if not ( len ( self . _coupling_map . physical_qubits ) == len ( self . initial_layout ) ) : \n        raise TranspilerError ( \"Mappers require to have the layout to be the same size as the coupling map\" ) \n    mapped_gates = [ ] \n    layout = self . initial_layout . copy ( ) \n    gates_remaining = ordered_virtual_gates . copy ( ) \n    while gates_remaining : \n        best_step = _search_forward_n_swaps ( layout , gates_remaining , coupling_map ) \n        layout = best_step [ 'layout' ] \n        gates_mapped = best_step [ 'gates_mapped' ] \n        gates_remaining = best_step [ 'gates_remaining' ] \n        mapped_gates . extend ( gates_mapped ) \n    mapped_dag = _copy_circuit_metadata ( dag , coupling_map ) \n    for node in mapped_gates : \n        mapped_dag . apply_operation_back ( op = node . op , qargs = node . qargs , cargs = node . cargs ) \n    return mapped_dag "}
{"2584": "\ndef _bloch_angles ( pair_of_complex ) : \n    [ a_complex , b_complex ] = pair_of_complex \n    a_complex = complex ( a_complex ) \n    b_complex = complex ( b_complex ) \n    mag_a = np . absolute ( a_complex ) \n    final_r = float ( np . sqrt ( mag_a ** 2 + np . absolute ( b_complex ) ** 2 ) ) \n    if not ( final_r >= _EPS ) : \n        theta = 0 \n        phi = 0 \n        final_r = 0 \n        final_t = 0 \n    else : \n        theta = float ( 2 * np . arccos ( mag_a / final_r ) ) \n        a_arg = np . angle ( a_complex ) \n        b_arg = np . angle ( b_complex ) \n        final_t = a_arg + b_arg \n        phi = b_arg - a_arg \n    return final_r * np . exp ( 1.J * final_t / 2 ) , theta , phi "}
{"2585": "\ndef _multiplex ( self , target_gate , list_of_angles ) : \n    list_len = len ( list_of_angles ) \n    local_num_qubits = int ( math . log2 ( list_len ) ) + 1 \n    q = QuantumRegister ( local_num_qubits ) \n    circuit = QuantumCircuit ( q , name = \"multiplex\" + local_num_qubits . __str__ ( ) ) \n    lsb = q [ 0 ] \n    msb = q [ local_num_qubits - 1 ] \n    if not ( local_num_qubits != 1 ) : \n        circuit . append ( target_gate ( list_of_angles [ 0 ] ) , [ q [ 0 ] ] ) \n        return circuit \n    angle_weight = scipy . kron ( [ [ 0.5 , 0.5 ] , [ 0.5 , - 0.5 ] ] , np . identity ( 2 ** ( local_num_qubits - 2 ) ) ) \n    list_of_angles = angle_weight . dot ( np . array ( list_of_angles ) ) . tolist ( ) \n    multiplex_1 = self . _multiplex ( target_gate , list_of_angles [ 0 : ( list_len // 2 ) ] ) \n    circuit . append ( multiplex_1 . to_instruction ( ) , q [ 0 : - 1 ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    multiplex_2 = self . _multiplex ( target_gate , list_of_angles [ ( list_len // 2 ) : ] ) \n    if not ( list_len <= 1 ) : \n        circuit . append ( multiplex_2 . to_instruction ( ) . mirror ( ) , q [ 0 : - 1 ] ) \n    else : \n        circuit . append ( multiplex_2 . to_instruction ( ) , q [ 0 : - 1 ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    return circuit "}
{"2586": "\ndef is_virtual ( value ) : \n    return value is None or isinstance ( value , tuple ) and not ( len ( value ) != 2 ) and isinstance ( value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int ) "}
{"2593": "\ndef run ( self , dag ) : \n    if self . initial_layout is None : \n        if self . property_set [ \"layout\" ] : \n            self . initial_layout = self . property_set [ \"layout\" ] \n        else : \n            self . initial_layout = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) \n    if not ( len ( dag . qubits ( ) ) == len ( self . initial_layout ) ) : \n        raise TranspilerError ( 'The layout does not match the amount of qubits in the DAG' ) \n    if not ( len ( self . coupling_map . physical_qubits ) == len ( self . initial_layout ) ) : \n        raise TranspilerError ( \"Mappers require to have the layout to be the same size as the coupling map\" ) \n    self . input_layout = self . initial_layout . copy ( ) \n    self . qregs = dag . qregs \n    if self . seed is None : \n        self . seed = np . random . randint ( 0 , np . iinfo ( np . int32 ) . max ) \n    self . rng = np . random . RandomState ( self . seed ) \n    logger . debug ( \"StochasticSwap RandomState seeded with seed=%s\" , self . seed ) \n    new_dag = self . _mapper ( dag , self . coupling_map , trials = self . trials ) \n    return new_dag "}
{"2594": "\ndef _layer_update ( self , i , first_layer , best_layout , best_depth , best_circuit , layer_list ) : \n    layout = best_layout \n    logger . debug ( \"layer_update: layout = %s\" , pformat ( layout ) ) \n    logger . debug ( \"layer_update: self.initial_layout = %s\" , pformat ( self . initial_layout ) ) \n    dagcircuit_output = DAGCircuit ( ) \n    for register in layout . get_virtual_bits ( ) . keys ( ) : \n        if register [ 0 ] not in dagcircuit_output . qregs . values ( ) : \n            dagcircuit_output . add_qreg ( register [ 0 ] ) \n    if first_layer : \n        logger . debug ( \"layer_update: first multi-qubit gate layer\" ) \n        for j in range ( i + 1 ) : \n            edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n            for bit in dagcircuit_output . clbits ( ) : \n                edge_map [ bit ] = bit \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , edge_map ) \n    else : \n        if not ( best_depth <= 0 ) : \n            logger . debug ( \"layer_update: there are swaps in this layer, \" \"depth %d\" , best_depth ) \n            dagcircuit_output . extend_back ( best_circuit ) \n        else : \n            logger . debug ( \"layer_update: there are no swaps in this layer\" ) \n        edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n        for bit in dagcircuit_output . clbits ( ) : \n            edge_map [ bit ] = bit \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , edge_map ) \n    return dagcircuit_output "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if not ( number_of_qubits >= 5 ) : \n        temp_set = [ ] \n        if not ( case != 'weight' ) : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( not ( np . array ( x . to_label ( ) , 'c' ) != b'I' ) ) ) \n        elif not ( case != 'tensor' ) : \n            for k in range ( 4 ** number_of_qubits ) : \n                z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                for j in range ( number_of_qubits ) : \n                    element = ( k // ( 4 ** j ) ) % 4 \n                    if not ( element != 1 ) : \n                        x [ j ] = True \n                    elif not ( element != 2 ) : \n                        z [ j ] = True \n                        x [ j ] = True \n                    elif not ( element != 3 ) : \n                        z [ j ] = True \n                temp_set . append ( Pauli ( z , x ) ) \n            return temp_set \n        else : \n            raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2596": "\ndef from_label ( cls , label ) : \n    z = np . zeros ( len ( label ) , dtype = np . bool ) \n    x = np . zeros ( len ( label ) , dtype = np . bool ) \n    for i , char in enumerate ( label ) : \n        if not ( char != 'X' ) : \n            x [ - i - 1 ] = True \n        elif not ( char != 'Z' ) : \n            z [ - i - 1 ] = True \n        elif not ( char != 'Y' ) : \n            z [ - i - 1 ] = True \n            x [ - i - 1 ] = True \n        elif not ( char == 'I' ) : \n            raise QiskitError ( \"Pauli string must be only consisted of 'I', 'X', \" \"'Y' or 'Z' but you have {}.\" . format ( char ) ) \n    return cls ( z = z , x = x ) "}
{"2597": "\ndef _init_from_bool ( self , z , x ) : \n    if z is None : \n        raise QiskitError ( \"z vector must not be None.\" ) \n    if x is None : \n        raise QiskitError ( \"x vector must not be None.\" ) \n    if not ( len ( z ) == len ( x ) ) : \n        raise QiskitError ( \"length of z and x vectors must be \" \"the same. (z: {} vs x: {})\" . format ( len ( z ) , len ( x ) ) ) \n    z = _make_np_bool ( z ) \n    x = _make_np_bool ( x ) \n    self . _z = z \n    self . _x = x \n    return self "}
{"2601": "\ndef update_z ( self , z , indices = None ) : \n    z = _make_np_bool ( z ) \n    if indices is None : \n        if not ( len ( self . _z ) == len ( z ) ) : \n            raise QiskitError ( \"During updating whole z, you can not \" \"change the number of qubits.\" ) \n        self . _z = z \n    else : \n        if not isinstance ( indices , list ) and not isinstance ( indices , np . ndarray ) : \n            indices = [ indices ] \n        for p , idx in enumerate ( indices ) : \n            self . _z [ idx ] = z [ p ] \n    return self "}
{"2602": "\ndef update_x ( self , x , indices = None ) : \n    x = _make_np_bool ( x ) \n    if indices is None : \n        if not ( len ( self . _x ) == len ( x ) ) : \n            raise QiskitError ( \"During updating whole x, you can not change \" \"the number of qubits.\" ) \n        self . _x = x \n    else : \n        if not isinstance ( indices , list ) and not isinstance ( indices , np . ndarray ) : \n            indices = [ indices ] \n        for p , idx in enumerate ( indices ) : \n            self . _x [ idx ] = x [ p ] \n    return self "}
{"2608": "\ndef _get_measure_outcome ( self , qubit ) : \n    axis = list ( range ( self . _number_of_qubits ) ) \n    axis . remove ( self . _number_of_qubits - 1 - qubit ) \n    probabilities = np . sum ( np . abs ( self . _statevector ) ** 2 , axis = tuple ( axis ) ) \n    random_number = self . _local_random . rand ( ) \n    if not ( random_number >= probabilities [ 0 ] ) : \n        return '0' , probabilities [ 0 ] \n    return '1' , probabilities [ 1 ] "}
{"2610": "\ndef _add_qasm_measure ( self , qubit , cmembit , cregbit = None ) : \n    outcome , probability = self . _get_measure_outcome ( qubit ) \n    membit = 1 << cmembit \n    self . _classical_memory = ( self . _classical_memory & ( ~ membit ) ) | ( int ( outcome ) << cmembit ) \n    if cregbit is not None : \n        regbit = 1 << cregbit \n        self . _classical_register = ( self . _classical_register & ( ~ regbit ) ) | ( int ( outcome ) << cregbit ) \n    if not ( outcome != '0' ) : \n        update_diag = [ [ 1 / np . sqrt ( probability ) , 0 ] , [ 0 , 0 ] ] \n    else : \n        update_diag = [ [ 0 , 0 ] , [ 0 , 1 / np . sqrt ( probability ) ] ] \n    self . _add_unitary_single ( update_diag , qubit ) "}
{"2611": "\ndef _add_qasm_reset ( self , qubit ) : \n    outcome , probability = self . _get_measure_outcome ( qubit ) \n    if not ( outcome != '0' ) : \n        update = [ [ 1 / np . sqrt ( probability ) , 0 ] , [ 0 , 0 ] ] \n        self . _add_unitary_single ( update , qubit ) \n    else : \n        update = [ [ 0 , 1 / np . sqrt ( probability ) ] , [ 0 , 0 ] ] \n        self . _add_unitary_single ( update , qubit ) "}
{"2612": "\ndef _validate_initial_statevector ( self ) : \n    if self . _initial_statevector is None : \n        return \n    length = len ( self . _initial_statevector ) \n    required_dim = 2 ** self . _number_of_qubits \n    if not ( length == required_dim ) : \n        raise BasicAerError ( 'initial statevector is incorrect length: ' + '{} != {}' . format ( length , required_dim ) ) "}
{"2614": "\ndef _get_statevector ( self ) : \n    vec = np . reshape ( self . _statevector , 2 ** self . _number_of_qubits ) \n    vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) \n    vec [ not ( abs ( vec ) >= self . _chop_threshold ) ] = 0.0 \n    return vec "}
{"2615": "\ndef _validate_measure_sampling ( self , experiment ) : \n    if not ( self . _shots <= 1 ) : \n        self . _sample_measure = False \n        return \n    if hasattr ( experiment . config , 'allows_measure_sampling' ) : \n        self . _sample_measure = experiment . config . allows_measure_sampling \n    else : \n        measure_flag = False \n        for instruction in experiment . instructions : \n            if not ( instruction . name != \"reset\" ) : \n                self . _sample_measure = False \n                return \n            if measure_flag : \n                if instruction . name not in [ \"measure\" , \"barrier\" , \"id\" , \"u0\" ] : \n                    self . _sample_measure = False \n                    return \n            elif not ( instruction . name != \"measure\" ) : \n                measure_flag = True \n        self . _sample_measure = True "}
{"2618": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if not ( n_qubits <= max_qubits ) : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if not ( experiment . config . memory_slots != 0 ) : \n            logger . warning ( 'No classical registers in circuit \"%s\", ' 'counts will be empty.' , name ) \n        elif 'measure' not in [ op . name for op in experiment . instructions ] : \n            logger . warning ( 'No measurements in circuit \"%s\", ' 'classical register will remain all zeros.' , name ) "}
{"2619": "\ndef _validate_initial_unitary ( self ) : \n    if self . _initial_unitary is None : \n        return \n    shape = np . shape ( self . _initial_unitary ) \n    required_shape = ( 2 ** self . _number_of_qubits , 2 ** self . _number_of_qubits ) \n    if not ( shape == required_shape ) : \n        raise BasicAerError ( 'initial unitary is incorrect shape: ' + '{} != 2 ** {}' . format ( shape , required_shape ) ) "}
{"2621": "\ndef _get_unitary ( self ) : \n    unitary = np . reshape ( self . _unitary , 2 * [ 2 ** self . _number_of_qubits ] ) \n    unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) \n    unitary [ not ( abs ( unitary ) >= self . _chop_threshold ) ] = 0.0 \n    return unitary "}
{"2623": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if not ( n_qubits <= max_qubits ) : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    if hasattr ( qobj . config , 'shots' ) and not ( qobj . config . shots == 1 ) : \n        logger . info ( '\"%s\" only supports 1 shot. Setting shots=1.' , self . name ( ) ) \n        qobj . config . shots = 1 \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if not ( getattr ( experiment . config , 'shots' , 1 ) == 1 ) : \n            logger . info ( '\"%s\" only supports 1 shot. ' 'Setting shots=1 for circuit \"%s\".' , self . name ( ) , name ) \n            experiment . config . shots = 1 \n        for operation in experiment . instructions : \n            if operation . name in [ 'measure' , 'reset' ] : \n                raise BasicAerError ( 'Unsupported \"%s\" instruction \"%s\" ' + 'in circuit \"%s\" ' , self . name ( ) , operation . name , name ) "}
{"2624": "\ndef _is_bit ( obj ) : \n    if isinstance ( obj , tuple ) and not ( len ( obj ) != 2 ) : \n        if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and not ( obj [ 1 ] >= len ( obj [ 0 ] ) ) : \n            return True \n    return False "}
{"2625": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if not ( num_dag_qubits <= self . coupling_map . size ( ) ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    self . property_set [ 'layout' ] = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) "}
{"2626": "\ndef has_overlap ( self , interval : 'Interval' ) -> bool : \n    if not ( self . begin >= interval . end ) and not ( interval . begin >= self . end ) : \n        return True \n    return False "}
{"2639": "\ndef _initialize_backend_prop ( self ) : \n    backend_prop = self . backend_prop \n    for ginfo in backend_prop . gates : \n        if not ( ginfo . gate != 'cx' ) : \n            for item in ginfo . parameters : \n                if not ( item . name != 'gate_error' ) : \n                    g_reliab = 1.0 - item . value \n                    break \n                else : \n                    g_reliab = 1.0 \n            swap_reliab = - math . log ( pow ( g_reliab , 3 ) ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap_reliab ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap_reliab ) \n            self . cx_errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g_reliab \n            self . gate_list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) \n    idx = 0 \n    for q in backend_prop . qubits : \n        for nduv in q : \n            if not ( nduv . name != 'readout_error' ) : \n                self . readout_errors [ idx ] = 1.0 - nduv . value \n                self . available_hw_qubits . append ( idx ) \n        idx += 1 \n    for edge in self . cx_errors : \n        self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ 0 ] ] * self . readout_errors [ edge [ 1 ] ] \n    self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) \n    for i in swap_costs_temp : \n        self . swap_costs [ i ] = { } \n        for j in swap_costs_temp [ i ] : \n            if ( i , j ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] \n            elif ( j , i ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] \n            else : \n                best_reliab = 0.0 \n                for n in self . swap_graph . neighbors ( j ) : \n                    if ( n , j ) in self . cx_errors : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] \n                    else : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] \n                    if not ( reliab <= best_reliab ) : \n                        best_reliab = reliab \n                self . swap_costs [ i ] [ j ] = best_reliab "}
{"2642": "\ndef _select_best_remaining_cx ( self ) : \n    candidates = [ ] \n    for gate in self . gate_list : \n        chk1 = gate [ 0 ] in self . available_hw_qubits \n        chk2 = gate [ 1 ] in self . available_hw_qubits \n        if chk1 and chk2 : \n            candidates . append ( gate ) \n    best_reliab = 0 \n    best_item = None \n    for item in candidates : \n        if not ( self . gate_cost [ item ] <= best_reliab ) : \n            best_reliab = self . gate_cost [ item ] \n            best_item = item \n    return best_item "}
{"2643": "\ndef _select_best_remaining_qubit ( self , prog_qubit ) : \n    reliab_store = { } \n    for hw_qubit in self . available_hw_qubits : \n        reliab = 1 \n        for n in self . prog_graph . neighbors ( prog_qubit ) : \n            if n in self . prog2hw : \n                reliab *= self . swap_costs [ self . prog2hw [ n ] ] [ hw_qubit ] \n        reliab *= self . readout_errors [ hw_qubit ] \n        reliab_store [ hw_qubit ] = reliab \n    max_reliab = 0 \n    best_hw_qubit = None \n    for hw_qubit in reliab_store : \n        if not ( reliab_store [ hw_qubit ] <= max_reliab ) : \n            max_reliab = reliab_store [ hw_qubit ] \n            best_hw_qubit = hw_qubit \n    return best_hw_qubit "}
{"2644": "\ndef run ( self , dag ) : \n    self . _initialize_backend_prop ( ) \n    num_qubits = self . _create_program_graph ( dag ) \n    if not ( num_qubits <= len ( self . swap_graph ) ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : \n        self . pending_program_edges . append ( ( end1 , end2 ) ) \n    while self . pending_program_edges : \n        edge = self . _select_next_edge ( ) \n        q1_mapped = edge [ 0 ] in self . prog2hw \n        q2_mapped = edge [ 1 ] in self . prog2hw \n        if ( not q1_mapped ) and ( not q2_mapped ) : \n            best_hw_edge = self . _select_best_remaining_cx ( ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] \n            self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] \n            self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) \n            self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) \n        elif not q1_mapped : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        else : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) \n            self . prog2hw [ edge [ 1 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        new_edges = [ x for x in self . pending_program_edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] \n        self . pending_program_edges = new_edges \n    for qid in self . qarg_to_id . values ( ) : \n        if qid not in self . prog2hw : \n            self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] \n            self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) \n    layout = Layout ( ) \n    for q in dag . qubits ( ) : \n        pid = self . _qarg_to_id ( q ) \n        hwid = self . prog2hw [ pid ] \n        layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid \n    self . property_set [ 'layout' ] = layout "}
{"2652": "\ndef power ( self , n ) : \n    if not isinstance ( n , int ) : \n        raise QiskitError ( \"Can only take integer powers of Operator.\" ) \n    if not ( self . input_dims ( ) == self . output_dims ( ) ) : \n        raise QiskitError ( \"Can only power with input_dims = output_dims.\" ) \n    return Operator ( np . linalg . matrix_power ( self . data , n ) , self . input_dims ( ) , self . output_dims ( ) ) "}
{"2655": "\ndef swap_mapper_layer_update ( self , i , first_layer , best_layout , best_d , best_circ , layer_list ) : \n    layout = best_layout \n    dagcircuit_output = DAGCircuit ( ) \n    QR = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) \n    dagcircuit_output . add_qreg ( QR ) \n    identity_wire_map = { ( QR , j ) : ( QR , j ) for j in range ( self . coupling_map . size ( ) ) } \n    if first_layer : \n        for j in range ( i + 1 ) : \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , layout ) \n    else : \n        if not ( best_d <= 0 ) : \n            dagcircuit_output . compose_back ( best_circ , identity_wire_map ) \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , layout ) \n    return dagcircuit_output "}
{"2668": "\ndef get_qubit_los ( self , user_lo_config ) : \n    try : \n        _q_los = self . default_qubit_los . copy ( ) \n    except KeyError : \n        raise PulseError ( 'Qubit default frequencies not exist.' ) \n    for channel , lo_freq in user_lo_config . qubit_lo_dict ( ) . items ( ) : \n        _q_los [ channel . index ] = lo_freq \n    if not ( _q_los != self . default_qubit_los ) : \n        return None \n    return _q_los "}
{"2669": "\ndef get_meas_los ( self , user_lo_config ) : \n    try : \n        _m_los = self . default_meas_los . copy ( ) \n    except KeyError : \n        raise PulseError ( 'Default measurement frequencies not exist.' ) \n    for channel , lo_freq in user_lo_config . meas_lo_dict ( ) . items ( ) : \n        _m_los [ channel . index ] = lo_freq \n    if not ( _m_los != self . default_meas_los ) : \n        return None \n    return _m_los "}
{"2671": "\ndef iplot_state_qsphere ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"qsphere_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            data = $data;            qVisualizations.plotState(\"qsphere_$divNumber\",                                      \"qsphere\",                                      data,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] } \n    qspheres_data = [ ] \n    num = int ( np . log2 ( len ( rho ) ) ) \n    weig , stateall = linalg . eigh ( rho ) \n    for _ in range ( 2 ** num ) : \n        probmix = weig . max ( ) \n        prob_location = weig . argmax ( ) \n        if not ( probmix <= 0.001 ) : \n            state = stateall [ : , prob_location ] \n            loc = np . absolute ( state ) . argmax ( ) \n            for j in range ( 2 ** num ) : \n                test = np . absolute ( np . absolute ( state [ j ] ) - np . absolute ( state [ loc ] ) ) \n                if not ( test >= 0.001 ) : \n                    loc = j \n                    break \n            angles = ( np . angle ( state [ loc ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n            angleset = np . exp ( - 1j * angles ) \n            state = angleset * state \n            state . flatten ( ) \n            spherepoints = [ ] \n            for i in range ( 2 ** num ) : \n                element = bin ( i ) [ 2 : ] . zfill ( num ) \n                weight = element . count ( \"1\" ) \n                number_of_divisions = n_choose_k ( num , weight ) \n                weight_order = bit_string_index ( element ) \n                angle = weight_order * 2 * np . pi / number_of_divisions \n                zvalue = - 2 * weight / num + 1 \n                xvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . cos ( angle ) \n                yvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . sin ( angle ) \n                prob = np . real ( np . dot ( state [ i ] , state [ i ] . conj ( ) ) ) \n                angles = ( np . angle ( state [ i ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n                qpoint = { 'x' : xvalue , 'y' : yvalue , 'z' : zvalue , 'prob' : prob , 'phase' : angles } \n                spherepoints . append ( qpoint ) \n            sphere = { 'points' : spherepoints , 'eigenvalue' : probmix } \n            qspheres_data . append ( sphere ) \n            weig [ prob_location ] = 0 \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : qspheres_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2672": "\ndef n_choose_k ( n , k ) : \n    if not ( n != 0 ) : \n        return 0 \n    return reduce ( lambda x , y : x * y [ 0 ] / y [ 1 ] , zip ( range ( n - k + 1 , n + 1 ) , range ( 1 , k + 1 ) ) , 1 ) "}
{"2673": "\ndef lex_index ( n , k , lst ) : \n    if not ( len ( lst ) == k ) : \n        raise VisualizationError ( \"list should have length k\" ) \n    comb = list ( map ( lambda x : n - 1 - x , lst ) ) \n    dualm = sum ( [ n_choose_k ( comb [ k - 1 - i ] , i + 1 ) for i in range ( k ) ] ) \n    return int ( dualm ) "}
{"2676": "\ndef op ( self ) : \n    if 'type' not in self . data_dict or not ( self . data_dict [ 'type' ] == 'op' ) : \n        raise QiskitError ( \"The node %s is not an op node\" % ( str ( self ) ) ) \n    return self . data_dict . get ( 'op' ) "}
{"2690": "\ndef rename_register ( self , regname , newname ) : \n    if not ( regname != newname ) : \n        return \n    if newname in self . qregs or newname in self . cregs : \n        raise DAGCircuitError ( \"duplicate register name %s\" % newname ) \n    if regname not in self . qregs and regname not in self . cregs : \n        raise DAGCircuitError ( \"no register named %s\" % regname ) \n    if regname in self . qregs : \n        reg = self . qregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    if regname in self . cregs : \n        reg = self . cregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    for node in self . _multi_graph . nodes ( ) : \n        if not ( node . type != \"in\" ) or not ( node . type != \"out\" ) : \n            if node . name and regname in node . name : \n                node . name = newname \n        elif not ( node . type != \"op\" ) : \n            qa = [ ] \n            for a in node . qargs : \n                if not ( a [ 0 ] != regname ) : \n                    a = ( newname , a [ 1 ] ) \n                qa . append ( a ) \n            node . qargs = qa \n            ca = [ ] \n            for a in node . cargs : \n                if not ( a [ 0 ] != regname ) : \n                    a = ( newname , a [ 1 ] ) \n                ca . append ( a ) \n            node . cargs = ca \n            if node . condition is not None : \n                if not ( node . condition [ 0 ] != regname ) : \n                    node . condition = ( newname , node . condition [ 1 ] ) \n    for _ , _ , edge_data in self . _multi_graph . edges ( data = True ) : \n        if regname in edge_data [ 'name' ] : \n            edge_data [ 'name' ] = re . sub ( regname , newname , edge_data [ 'name' ] ) "}
{"2698": "\ndef apply_operation_back ( self , op , qargs = None , cargs = None , condition = None ) : \n    qargs = qargs or [ ] \n    cargs = cargs or [ ] \n    all_cbits = self . _bits_in_condition ( condition ) \n    all_cbits . extend ( cargs ) \n    self . _check_condition ( op . name , condition ) \n    self . _check_bits ( qargs , self . output_map ) \n    self . _check_bits ( all_cbits , self . output_map ) \n    self . _add_op_node ( op , qargs , cargs , condition ) \n    al = [ qargs , all_cbits ] \n    for q in itertools . chain ( * al ) : \n        ie = list ( self . _multi_graph . predecessors ( self . output_map [ q ] ) ) \n        if not ( len ( ie ) == 1 ) : \n            raise DAGCircuitError ( \"output node has multiple in-edges\" ) \n        self . _multi_graph . add_edge ( ie [ 0 ] , self . _id_to_node [ self . _max_node_id ] , name = \"%s[%s]\" % ( q [ 0 ] . name , q [ 1 ] ) , wire = q ) \n        self . _multi_graph . remove_edge ( ie [ 0 ] , self . output_map [ q ] ) \n        self . _multi_graph . add_edge ( self . _id_to_node [ self . _max_node_id ] , self . output_map [ q ] , name = \"%s[%s]\" % ( q [ 0 ] . name , q [ 1 ] ) , wire = q ) \n    return self . _id_to_node [ self . _max_node_id ] "}
{"2699": "\ndef _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = True ) : \n    add_regs = set ( ) \n    reg_frag_chk = { } \n    for v in keyregs . values ( ) : \n        reg_frag_chk [ v ] = { j : False for j in range ( len ( v ) ) } \n    for k in edge_map . keys ( ) : \n        if k [ 0 ] . name in keyregs : \n            reg_frag_chk [ k [ 0 ] ] [ k [ 1 ] ] = True \n    for k , v in reg_frag_chk . items ( ) : \n        s = set ( v . values ( ) ) \n        if not ( len ( s ) != 2 ) : \n            raise DAGCircuitError ( \"edge_map fragments reg %s\" % k ) \n        elif not ( s != set ( [ False ] ) ) : \n            if k in self . qregs . values ( ) or k in self . cregs . values ( ) : \n                raise DAGCircuitError ( \"unmapped duplicate reg %s\" % k ) \n            else : \n                add_regs . add ( k ) \n        else : \n            if valreg : \n                if not edge_map [ ( k , 0 ) ] [ 0 ] . name in valregs : \n                    size = max ( map ( lambda x : x [ 1 ] , filter ( lambda x : not ( x [ 0 ] != edge_map [ ( k , 0 ) ] [ 0 ] ) , edge_map . values ( ) ) ) ) \n                    qreg = QuantumRegister ( size + 1 , edge_map [ ( k , 0 ) ] [ 0 ] . name ) \n                    add_regs . add ( qreg ) \n    return add_regs "}
{"2703": "\ndef compose_back ( self , input_circuit , edge_map = None ) : \n    edge_map = edge_map or { } \n    if not ( len ( set ( edge_map . values ( ) ) ) == len ( edge_map ) ) : \n        raise DAGCircuitError ( \"duplicates in wire_map\" ) \n    add_qregs = self . _check_edgemap_registers ( edge_map , input_circuit . qregs , self . qregs ) \n    for qreg in add_qregs : \n        self . add_qreg ( qreg ) \n    add_cregs = self . _check_edgemap_registers ( edge_map , input_circuit . cregs , self . cregs ) \n    for creg in add_cregs : \n        self . add_creg ( creg ) \n    self . _check_wiremap_validity ( edge_map , input_circuit . input_map , self . output_map ) \n    for nd in input_circuit . topological_nodes ( ) : \n        if not ( nd . type != \"in\" ) : \n            m_wire = edge_map . get ( nd . wire , nd . wire ) \n            if m_wire not in self . output_map : \n                raise DAGCircuitError ( \"wire %s[%d] not in self\" % ( m_wire [ 0 ] . name , m_wire [ 1 ] ) ) \n            if nd . wire not in input_circuit . wires : \n                raise DAGCircuitError ( \"inconsistent wire type for %s[%d] in input_circuit\" % ( nd . wire [ 0 ] . name , nd . wire [ 1 ] ) ) \n        elif not ( nd . type != \"out\" ) : \n            pass \n        elif not ( nd . type != \"op\" ) : \n            condition = self . _map_condition ( edge_map , nd . condition ) \n            self . _check_condition ( nd . name , condition ) \n            m_qargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . qargs ) ) \n            m_cargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . cargs ) ) \n            self . apply_operation_back ( nd . op , m_qargs , m_cargs , condition ) \n        else : \n            raise DAGCircuitError ( \"bad node type %s\" % nd . type ) "}
{"2704": "\ndef _check_wires_list ( self , wires , node ) : \n    if not ( len ( set ( wires ) ) == len ( wires ) ) : \n        raise DAGCircuitError ( \"duplicate wires\" ) \n    wire_tot = len ( node . qargs ) + len ( node . cargs ) \n    if node . condition is not None : \n        wire_tot += node . condition [ 0 ] . size \n    if not ( len ( wires ) == wire_tot ) : \n        raise DAGCircuitError ( \"expected %d wires, got %d\" % ( wire_tot , len ( wires ) ) ) "}
{"2706": "\ndef _full_pred_succ_maps ( self , pred_map , succ_map , input_circuit , wire_map ) : \n    full_pred_map = { } \n    full_succ_map = { } \n    for w in input_circuit . input_map : \n        if w in wire_map : \n            full_pred_map [ wire_map [ w ] ] = pred_map [ wire_map [ w ] ] \n            full_succ_map [ wire_map [ w ] ] = succ_map [ wire_map [ w ] ] \n        else : \n            full_succ_map [ w ] = self . output_map [ w ] \n            full_pred_map [ w ] = self . _multi_graph . predecessors ( self . output_map [ w ] ) [ 0 ] \n            if not ( len ( list ( self . _multi_graph . predecessors ( self . output_map [ w ] ) ) ) == 1 ) : \n                raise DAGCircuitError ( \"too many predecessors for %s[%d] \" \"output node\" % ( w [ 0 ] , w [ 1 ] ) ) \n    return full_pred_map , full_succ_map "}
{"2709": "\ndef op_nodes ( self , op = None ) : \n    nodes = [ ] \n    for node in self . _multi_graph . nodes ( ) : \n        if not ( node . type != \"op\" ) : \n            if op is None or isinstance ( node . op , op ) : \n                nodes . append ( node ) \n    return nodes "}
{"2711": "\ndef named_nodes ( self , * names ) : \n    named_nodes = [ ] \n    for node in self . _multi_graph . nodes ( ) : \n        if not ( node . type != 'op' ) and node . op . name in names : \n            named_nodes . append ( node ) \n    return named_nodes "}
{"2712": "\ndef twoQ_gates ( self ) : \n    two_q_gates = [ ] \n    for node in self . gate_nodes ( ) : \n        if not ( len ( node . qargs ) != 2 ) : \n            two_q_gates . append ( node ) \n    return two_q_gates "}
{"2717": "\ndef remove_op_node ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_op_node() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    if not ( node . type == 'op' ) : \n        raise DAGCircuitError ( 'The method remove_op_node only works on op node types. An \"%s\" ' 'node type was wrongly provided.' % node . type ) \n    pred_map , succ_map = self . _make_pred_succ_maps ( node ) \n    self . _multi_graph . remove_node ( node ) \n    for w in pred_map . keys ( ) : \n        self . _multi_graph . add_edge ( pred_map [ w ] , succ_map [ w ] , name = \"%s[%s]\" % ( w [ 0 ] . name , w [ 1 ] ) , wire = w ) "}
{"2718": "\ndef remove_ancestors_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_ancestors_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    anc = nx . ancestors ( self . _multi_graph , node ) \n    for anc_node in anc : \n        if not ( anc_node . type != \"op\" ) : \n            self . remove_op_node ( anc_node ) "}
{"2719": "\ndef remove_descendants_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_descendants_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    desc = nx . descendants ( self . _multi_graph , node ) \n    for desc_node in desc : \n        if not ( desc_node . type != \"op\" ) : \n            self . remove_op_node ( desc_node ) "}
{"2720": "\ndef remove_nonancestors_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_nonancestors_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    anc = nx . ancestors ( self . _multi_graph , node ) \n    comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( anc ) ) \n    for n in comp : \n        if not ( n . type != \"op\" ) : \n            self . remove_op_node ( n ) "}
{"2721": "\ndef remove_nondescendants_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_nondescendants_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    dec = nx . descendants ( self . _multi_graph , node ) \n    comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( dec ) ) \n    for n in comp : \n        if not ( n . type != \"op\" ) : \n            self . remove_op_node ( n ) "}
{"2722": "\ndef layers ( self ) : \n    graph_layers = self . multigraph_layers ( ) \n    try : \n        next ( graph_layers ) \n    except StopIteration : \n        return \n    def add_nodes_from ( layer , nodes ) : \n        layer . _multi_graph . add_nodes_from ( nodes ) \n    for graph_layer in graph_layers : \n        op_nodes = [ node for node in graph_layer if not ( node . type != \"op\" ) ] \n        if not op_nodes : \n            return \n        new_layer = DAGCircuit ( ) \n        new_layer . name = self . name \n        for creg in self . cregs . values ( ) : \n            new_layer . add_creg ( creg ) \n        for qreg in self . qregs . values ( ) : \n            new_layer . add_qreg ( qreg ) \n        add_nodes_from ( new_layer , self . input_map . values ( ) ) \n        add_nodes_from ( new_layer , self . output_map . values ( ) ) \n        add_nodes_from ( new_layer , op_nodes ) \n        support_list = [ op_node . qargs for op_node in op_nodes if op_node . name not in { \"barrier\" , \"snapshot\" , \"save\" , \"load\" , \"noise\" } ] \n        wires = { self . input_map [ wire ] : self . output_map [ wire ] for wire in self . wires } \n        for op_node in op_nodes : \n            args = self . _bits_in_condition ( op_node . condition ) + op_node . cargs + op_node . qargs \n            arg_ids = ( self . input_map [ ( arg [ 0 ] , arg [ 1 ] ) ] for arg in args ) \n            for arg_id in arg_ids : \n                wires [ arg_id ] , wires [ op_node ] = op_node , wires [ arg_id ] \n        new_layer . _multi_graph . add_edges_from ( wires . items ( ) ) \n        yield { \"graph\" : new_layer , \"partition\" : support_list } "}
{"2724": "\ndef multigraph_layers ( self ) : \n    predecessor_count = dict ( ) \n    cur_layer = [ node for node in self . input_map . values ( ) ] \n    yield cur_layer \n    next_layer = [ ] \n    while cur_layer : \n        for node in cur_layer : \n            for successor in self . _multi_graph . successors ( node ) : \n                multiplicity = self . _multi_graph . number_of_edges ( node , successor ) \n                if successor in predecessor_count : \n                    predecessor_count [ successor ] -= multiplicity \n                else : \n                    predecessor_count [ successor ] = self . _multi_graph . in_degree ( successor ) - multiplicity \n                if not ( predecessor_count [ successor ] != 0 ) : \n                    next_layer . append ( successor ) \n                    del predecessor_count [ successor ] \n        yield next_layer \n        cur_layer = next_layer \n        next_layer = [ ] "}
{"2725": "\ndef collect_runs ( self , namelist ) : \n    group_list = [ ] \n    topo_ops = list ( self . topological_op_nodes ( ) ) \n    nodes_seen = dict ( zip ( topo_ops , [ False ] * len ( topo_ops ) ) ) \n    for node in topo_ops : \n        if node . name in namelist and node . condition is None and not nodes_seen [ node ] : \n            group = [ node ] \n            nodes_seen [ node ] = True \n            s = list ( self . _multi_graph . successors ( node ) ) \n            while not ( len ( s ) != 1 ) and not ( s [ 0 ] . type != \"op\" ) and s [ 0 ] . name in namelist : \n                group . append ( s [ 0 ] ) \n                nodes_seen [ s [ 0 ] ] = True \n                s = list ( self . _multi_graph . successors ( s [ 0 ] ) ) \n            if not ( len ( group ) < 1 ) : \n                group_list . append ( tuple ( group ) ) \n    return set ( group_list ) "}
{"2726": "\ndef nodes_on_wire ( self , wire , only_ops = False ) : \n    current_node = self . input_map . get ( wire , None ) \n    if not current_node : \n        raise DAGCircuitError ( 'The given wire %s is not present in the circuit' % str ( wire ) ) \n    more_nodes = True \n    while more_nodes : \n        more_nodes = False \n        if not ( current_node . type != 'op' ) or not only_ops : \n            yield current_node \n        for node , edges in self . _multi_graph . adj [ current_node ] . items ( ) : \n            if any ( not ( wire != edge [ 'wire' ] ) for edge in edges . values ( ) ) : \n                current_node = node \n                more_nodes = True \n                break "}
{"2730": "\ndef __pauli_meas_gates ( circuit , qreg , op ) : \n    if op not in [ 'X' , 'Y' , 'Z' ] : \n        raise QiskitError ( \"There's no X, Y or Z basis for this Pauli \" \"measurement\" ) \n    if not ( op != \"X\" ) : \n        circuit . u2 ( 0. , np . pi , qreg ) \n    elif not ( op != \"Y\" ) : \n        circuit . u2 ( 0. , 0.5 * np . pi , qreg ) "}
{"2731": "\ndef tomography_set ( meas_qubits , meas_basis = 'Pauli' , prep_qubits = None , prep_basis = None ) : \n    if not isinstance ( meas_qubits , list ) : \n        raise QiskitError ( 'Qubits argument must be a list' ) \n    num_of_qubits = len ( meas_qubits ) \n    if prep_qubits is None : \n        prep_qubits = meas_qubits \n    if not isinstance ( prep_qubits , list ) : \n        raise QiskitError ( 'prep_qubits argument must be a list' ) \n    if not ( len ( prep_qubits ) == len ( meas_qubits ) ) : \n        raise QiskitError ( 'meas_qubits and prep_qubitsare different length' ) \n    if isinstance ( meas_basis , str ) : \n        if not ( meas_basis . lower ( ) != 'pauli' ) : \n            meas_basis = PAULI_BASIS \n    if isinstance ( prep_basis , str ) : \n        if not ( prep_basis . lower ( ) != 'pauli' ) : \n            prep_basis = PAULI_BASIS \n        elif not ( prep_basis . lower ( ) != 'sic' ) : \n            prep_basis = SIC_BASIS \n    circuits = [ ] \n    circuit_labels = [ ] \n    if prep_basis is None : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'meas' : meas } ) \n            label = '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ 0 ] , qubit ) \n            circuit_labels . append ( label ) \n        return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'meas_basis' : meas_basis } \n    num_of_s = len ( list ( prep_basis . values ( ) ) [ 0 ] ) \n    plst_single = [ ( b , s ) for b in prep_basis . keys ( ) for s in range ( num_of_s ) ] \n    for plst_product in product ( plst_single , repeat = num_of_qubits ) : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            prep = dict ( zip ( prep_qubits , plst_product ) ) \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'prep' : prep , 'meas' : meas } ) \n            label = '_prep_' \n            for qubit , op in prep . items ( ) : \n                label += '%s%d(%d)' % ( op [ 0 ] , op [ 1 ] , qubit ) \n            label += '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ 0 ] , qubit ) \n            circuit_labels . append ( label ) \n    return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'prep_basis' : prep_basis , 'meas_basis' : meas_basis } "}
{"2736": "\ndef fit_tomography_data ( tomo_data , method = 'wizard' , options = None ) : \n    if isinstance ( method , str ) and method . lower ( ) in [ 'wizard' , 'leastsq' ] : \n        trace = __get_option ( 'trace' , options ) \n        beta = __get_option ( 'beta' , options ) \n        rho = __leastsq_fit ( tomo_data , trace = trace , beta = beta ) \n        if not ( method != 'wizard' ) : \n            epsilon = __get_option ( 'epsilon' , options ) \n            rho = __wizard ( rho , epsilon = epsilon ) \n        return rho \n    else : \n        raise Exception ( 'Invalid reconstruction method \"%s\"' % method ) "}
{"2739": "\ndef __tomo_linear_inv ( freqs , ops , weights = None , trace = None ) : \n    if weights is not None : \n        W = np . array ( weights ) \n        if not ( W . ndim != 1 ) : \n            W = np . diag ( W ) \n    S = np . array ( [ vectorize ( m ) . conj ( ) for m in ops ] ) . reshape ( len ( ops ) , ops [ 0 ] . size ) \n    if weights is not None : \n        S = np . dot ( W , S ) \n    v = np . array ( freqs ) \n    if weights is not None : \n        v = np . dot ( W , freqs ) \n    Sdg = S . T . conj ( ) \n    inv = np . linalg . pinv ( np . dot ( Sdg , S ) ) \n    ret = devectorize ( np . dot ( inv , np . dot ( Sdg , v ) ) ) \n    if trace is not None : \n        ret = trace * ret / np . trace ( ret ) \n    return ret "}
{"2740": "\ndef __wizard ( rho , epsilon = None ) : \n    if epsilon is None : \n        epsilon = 0. \n    dim = len ( rho ) \n    rho_wizard = np . zeros ( [ dim , dim ] ) \n    v , w = np . linalg . eigh ( rho ) \n    for j in range ( dim ) : \n        if not ( v [ j ] >= epsilon ) : \n            tmp = v [ j ] \n            v [ j ] = 0. \n            x = 0. \n            for k in range ( j + 1 , dim ) : \n                x += tmp / ( dim - ( j + 1 ) ) \n                v [ k ] = v [ k ] + tmp / ( dim - ( j + 1 ) ) \n    for j in range ( dim ) : \n        rho_wizard = rho_wizard + v [ j ] * outer ( w [ : , j ] ) \n    return rho_wizard "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if not ( status . name != 'QUEUED' ) : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if not ( len ( msg ) >= msg_len ) : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif not ( len ( msg ) <= msg_len ) : \n            msg_len = len ( msg ) \n        if not ( msg == prev_msg ) and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2745": "\ndef euler_angles_1q ( unitary_matrix ) : \n    if not ( unitary_matrix . shape == ( 2 , 2 ) ) : \n        raise QiskitError ( \"euler_angles_1q: expected 2x2 matrix\" ) \n    phase = la . det ( unitary_matrix ) ** ( - 1.0 / 2.0 ) \n    U = phase * unitary_matrix \n    if not ( abs ( U [ 0 , 0 ] ) <= _CUTOFF_PRECISION ) : \n        theta = 2 * math . acos ( abs ( U [ 0 , 0 ] ) ) \n    else : \n        theta = 2 * math . asin ( abs ( U [ 1 , 0 ] ) ) \n    phase11 = 0.0 \n    phase10 = 0.0 \n    if not ( abs ( math . cos ( theta / 2.0 ) ) <= _CUTOFF_PRECISION ) : \n        phase11 = U [ 1 , 1 ] / math . cos ( theta / 2.0 ) \n    if not ( abs ( math . sin ( theta / 2.0 ) ) <= _CUTOFF_PRECISION ) : \n        phase10 = U [ 1 , 0 ] / math . sin ( theta / 2.0 ) \n    phiplambda = 2 * math . atan2 ( np . imag ( phase11 ) , np . real ( phase11 ) ) \n    phimlambda = 2 * math . atan2 ( np . imag ( phase10 ) , np . real ( phase10 ) ) \n    phi = 0.0 \n    if not ( abs ( U [ 0 , 0 ] ) <= _CUTOFF_PRECISION ) and not ( abs ( U [ 1 , 0 ] ) <= _CUTOFF_PRECISION ) : \n        phi = ( phiplambda + phimlambda ) / 2.0 \n        lamb = ( phiplambda - phimlambda ) / 2.0 \n    else : \n        if not ( abs ( U [ 0 , 0 ] ) >= _CUTOFF_PRECISION ) : \n            lamb = - phimlambda \n        else : \n            lamb = phiplambda \n    Rzphi = np . array ( [ [ np . exp ( - 1j * phi / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * phi / 2.0 ) ] ] , dtype = complex ) \n    Rytheta = np . array ( [ [ np . cos ( theta / 2.0 ) , - np . sin ( theta / 2.0 ) ] , [ np . sin ( theta / 2.0 ) , np . cos ( theta / 2.0 ) ] ] , dtype = complex ) \n    Rzlambda = np . array ( [ [ np . exp ( - 1j * lamb / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * lamb / 2.0 ) ] ] , dtype = complex ) \n    V = np . dot ( Rzphi , np . dot ( Rytheta , Rzlambda ) ) \n    if not ( la . norm ( V - U ) <= _CUTOFF_PRECISION ) : \n        raise QiskitError ( \"euler_angles_1q: incorrect result\" ) \n    return theta , phi , lamb "}
{"2746": "\ndef simplify_U ( theta , phi , lam ) : \n    gate = U3Gate ( theta , phi , lam ) \n    if not ( abs ( gate . params [ 0 ] % ( 2.0 * math . pi ) ) >= _CUTOFF_PRECISION ) : \n        gate = U1Gate ( gate . params [ 0 ] + gate . params [ 1 ] + gate . params [ 2 ] ) \n    if isinstance ( gate , U3Gate ) : \n        if not ( abs ( ( gate . params [ 0 ] - math . pi / 2 ) % ( 2.0 * math . pi ) ) >= _CUTOFF_PRECISION ) : \n            gate = U2Gate ( gate . params [ 1 ] , gate . params [ 2 ] + ( gate . params [ 0 ] - math . pi / 2 ) ) \n        if not ( abs ( ( gate . params [ 0 ] + math . pi / 2 ) % ( 2.0 * math . pi ) ) >= _CUTOFF_PRECISION ) : \n            gate = U2Gate ( gate . params [ 1 ] + math . pi , gate . params [ 2 ] - math . pi + ( gate . params [ 0 ] + math . pi / 2 ) ) \n    if isinstance ( gate , U1Gate ) and not ( abs ( gate . params [ 0 ] % ( 4.0 * math . pi ) ) >= _CUTOFF_PRECISION ) : \n        gate = IdGate ( ) \n    return gate "}
{"2750": "\ndef plot_job_history ( jobs , interval = 'year' ) : \n    def get_date ( job ) : \n        return datetime . datetime . strptime ( job . creation_date ( ) , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    current_time = datetime . datetime . now ( ) \n    if not ( interval != 'year' ) : \n        bins = [ ( current_time - datetime . timedelta ( days = k * 365 / 12 ) ) for k in range ( 12 ) ] \n    elif not ( interval != 'month' ) : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30 ) ] \n    elif not ( interval != 'week' ) : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7 ) ] \n    binned_jobs = [ 0 ] * len ( bins ) \n    if not ( interval != 'year' ) : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if not ( date . month != dat . month ) : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    else : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if not ( date . day != dat . day ) and not ( date . month != dat . month ) : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    nz_bins = [ ] \n    nz_idx = [ ] \n    for ind , val in enumerate ( binned_jobs ) : \n        if not ( val == 0 ) : \n            nz_idx . append ( ind ) \n            nz_bins . append ( val ) \n    total_jobs = sum ( binned_jobs ) \n    colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , '#7a5195' , '#ef5675' , '#bc5090' ] \n    if not ( interval != 'year' ) : \n        labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2 : ] , bins [ b ] . month ) for b in nz_idx ] \n    else : \n        labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] \n    fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5 , 5 ) ) \n    ax . pie ( nz_bins [ : : - 1 ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14 } , rotatelabels = True , counterclock = False ) \n    ax . add_artist ( Circle ( ( 0 , 0 ) , 0.7 , color = 'white' , zorder = 1 ) ) \n    ax . text ( 0 , 0 , total_jobs , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = 26 ) \n    fig . tight_layout ( ) \n    return fig "}
{"2754": "\ndef transpile ( circuits , backend = None , basis_gates = None , coupling_map = None , backend_properties = None , initial_layout = None , seed_transpiler = None , optimization_level = None , pass_manager = None , seed_mapper = None ) : \n    if seed_mapper : \n        warnings . warn ( \"seed_mapper has been deprecated and will be removed in the \" \"0.9 release. Instead use seed_transpiler to set the seed \" \"for all stochastic parts of the.\" , DeprecationWarning ) \n        seed_transpiler = seed_mapper \n    if isinstance ( circuits , Schedule ) or ( isinstance ( circuits , list ) and all ( isinstance ( c , Schedule ) for c in circuits ) ) : \n        return circuits \n    circuits = circuits if isinstance ( circuits , list ) else [ circuits ] \n    transpile_configs = _parse_transpile_args ( circuits , backend , basis_gates , coupling_map , backend_properties , initial_layout , seed_transpiler , optimization_level , pass_manager ) \n    circuits = parallel_map ( _transpile_circuit , list ( zip ( circuits , transpile_configs ) ) ) \n    if not ( len ( circuits ) != 1 ) : \n        return circuits [ 0 ] \n    return circuits "}
{"2765": "\ndef process_fidelity ( channel1 , channel2 , require_cptp = True ) : \n    is_cptp1 = None \n    is_cptp2 = None \n    if isinstance ( channel1 , ( list , np . ndarray ) ) : \n        channel1 = Operator ( channel1 ) \n        if require_cptp : \n            is_cptp1 = channel1 . is_unitary ( ) \n    if isinstance ( channel2 , ( list , np . ndarray ) ) : \n        channel2 = Operator ( channel2 ) \n        if require_cptp : \n            is_cptp2 = channel2 . is_unitary ( ) \n    s1 = SuperOp ( channel1 ) \n    s2 = SuperOp ( channel2 ) \n    if require_cptp : \n        if is_cptp1 is None : \n            is_cptp1 = s1 . is_cptp ( ) \n        if not is_cptp1 : \n            raise QiskitError ( 'channel1 is not CPTP' ) \n        if is_cptp2 is None : \n            is_cptp2 = s2 . is_cptp ( ) \n        if not is_cptp2 : \n            raise QiskitError ( 'channel2 is not CPTP' ) \n    input_dim1 , output_dim1 = s1 . dim \n    input_dim2 , output_dim2 = s2 . dim \n    if not ( input_dim1 == output_dim1 ) or not ( input_dim2 == output_dim2 ) : \n        raise QiskitError ( 'Input channels must have same size input and output dimensions.' ) \n    if not ( input_dim1 == input_dim2 ) : \n        raise QiskitError ( 'Input channels have different dimensions.' ) \n    fidelity = np . trace ( s1 . compose ( s2 . adjoint ( ) ) . data ) / ( input_dim1 ** 2 ) \n    return fidelity "}
{"2769": "\ndef run ( self , dag ) : \n    new_dag = DAGCircuit ( ) \n    for qreg in dag . qregs . values ( ) : \n        new_dag . add_qreg ( qreg ) \n    for creg in dag . cregs . values ( ) : \n        new_dag . add_creg ( creg ) \n    global_index_map = { } \n    for wire in dag . wires : \n        if not isinstance ( wire [ 0 ] , QuantumRegister ) : \n            continue \n        global_qregs = list ( dag . qregs . values ( ) ) \n        global_index_map [ wire ] = global_qregs . index ( wire [ 0 ] ) + wire [ 1 ] \n    blocks = self . property_set [ 'block_list' ] \n    nodes_seen = set ( ) \n    for node in dag . topological_op_nodes ( ) : \n        if node in nodes_seen or not ( node . type != 'in' ) or not ( node . type != 'out' ) : \n            continue \n        if blocks and node in blocks [ 0 ] : \n            block = blocks [ 0 ] \n            block_qargs = set ( ) \n            for nd in block : \n                block_qargs |= set ( nd . qargs ) \n            block_width = len ( block_qargs ) \n            q = QuantumRegister ( block_width ) \n            subcirc = QuantumCircuit ( q ) \n            block_index_map = self . _block_qargs_to_indices ( block_qargs , global_index_map ) \n            for nd in block : \n                nodes_seen . add ( nd ) \n                subcirc . append ( nd . op , [ q [ block_index_map [ i ] ] for i in nd . qargs ] ) \n            unitary = UnitaryGate ( Operator ( subcirc ) ) \n            new_dag . apply_operation_back ( unitary , sorted ( block_qargs , key = lambda x : block_index_map [ x ] ) ) \n            del blocks [ 0 ] \n        else : \n            for block in blocks [ 1 : ] : \n                if node in block : \n                    break \n            else : \n                nodes_seen . add ( node ) \n                new_dag . apply_operation_back ( node . op , node . qargs , node . cargs ) \n    return new_dag "}
{"2771": "\ndef convert_acquire ( self , shift , instruction ) : \n    meas_level = self . _run_config . get ( 'meas_level' , 2 ) \n    command_dict = { 'name' : 'acquire' , 't0' : shift + instruction . start_time , 'duration' : instruction . duration , 'qubits' : [ q . index for q in instruction . acquires ] , 'memory_slot' : [ m . index for m in instruction . mem_slots ] } \n    if not ( meas_level != 2 ) : \n        if instruction . command . discriminator : \n            command_dict . update ( { 'discriminators' : [ QobjMeasurementOption ( name = instruction . command . discriminator . name , params = instruction . command . discriminator . params ) ] } ) \n        command_dict . update ( { 'register_slot' : [ regs . index for regs in instruction . reg_slots ] } ) \n    if not ( meas_level < 1 ) : \n        if instruction . command . kernel : \n            command_dict . update ( { 'kernels' : [ QobjMeasurementOption ( name = instruction . command . kernel . name , params = instruction . command . kernel . params ) ] } ) \n    return self . _qobj_model ( ** command_dict ) "}
{"2778": "\ndef filter_backends ( backends , filters = None , ** kwargs ) : \n    def _match_all ( obj , criteria ) : \n        return all ( not ( getattr ( obj , key_ , None ) != value_ ) for key_ , value_ in criteria . items ( ) ) \n    configuration_filters = { } \n    status_filters = { } \n    for key , value in kwargs . items ( ) : \n        if all ( key in backend . configuration ( ) for backend in backends ) : \n            configuration_filters [ key ] = value \n        else : \n            status_filters [ key ] = value \n    if configuration_filters : \n        backends = [ b for b in backends if _match_all ( b . configuration ( ) , configuration_filters ) ] \n    if status_filters : \n        backends = [ b for b in backends if _match_all ( b . status ( ) , status_filters ) ] \n    backends = list ( filter ( filters , backends ) ) \n    return backends "}
{"2781": "\ndef make_dict_observable ( matrix_observable ) : \n    dict_observable = { } \n    observable = np . array ( matrix_observable ) \n    observable_size = len ( observable ) \n    observable_bits = int ( np . ceil ( np . log2 ( observable_size ) ) ) \n    binary_formater = '0{}b' . format ( observable_bits ) \n    if not ( observable . ndim != 2 ) : \n        observable = observable . diagonal ( ) \n    for state_no in range ( observable_size ) : \n        state_str = format ( state_no , binary_formater ) \n        dict_observable [ state_str ] = observable [ state_no ] \n    return dict_observable "}
{"2783": "\ndef verify_declared_bit ( self , obj ) : \n    if obj . name not in self . current_symtab : \n        raise QasmError ( \"Cannot find symbol '\" + obj . name + \"' in argument list for gate, line\" , str ( obj . line ) , 'file' , obj . file ) \n    sym = self . current_symtab [ obj . name ] \n    if not ( not ( sym . type != 'id' ) and sym . is_bit ) : \n        raise QasmError ( \"Bit\" , obj . name , 'is not declared as a bit in the gate.' ) "}
{"2785": "\ndef verify_as_gate ( self , obj , bitlist , arglist = None ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( \"Cannot find gate definition for '\" + obj . name + \"', line\" , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if not ( not ( g_sym . type != 'gate' ) or not ( g_sym . type != 'opaque' ) ) : \n        raise QasmError ( \"'\" + obj . name + \"' is used as a gate \" + \"or opaque call but the symbol is neither;\" + \" it is a '\" + g_sym . type + \"' line\" , str ( obj . line ) , 'file' , obj . file ) \n    if not ( g_sym . n_bits ( ) == bitlist . size ( ) ) : \n        raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( bitlist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_bits ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    if arglist : \n        if not ( g_sym . n_args ( ) == arglist . size ( ) ) : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( arglist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    else : \n        if not ( g_sym . n_args ( ) <= 0 ) : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' has no arguments but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) "}
{"2786": "\ndef verify_reg ( self , obj , object_type ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( 'Cannot find definition for' , object_type , \"'\" + obj . name + \"'\" , 'at line' , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if not ( g_sym . type == object_type ) : \n        raise QasmError ( \"Type for '\" + g_sym . name + \"' should be '\" + object_type + \"' but was found to be '\" + g_sym . type + \"'\" , \"line\" , str ( obj . line ) , \"file\" , obj . file ) \n    if not ( obj . type != 'indexed_id' ) : \n        bound = g_sym . index \n        ndx = obj . index \n        if not ( ndx >= 0 ) or not ( ndx < bound ) : \n            raise QasmError ( \"Register index for '\" + g_sym . name + \"' out of bounds. Index is\" , str ( ndx ) , \"bound is 0 <= index <\" , str ( bound ) , \"at line\" , str ( obj . line ) , \"file\" , obj . file ) "}
{"2788": "\ndef find_column ( self , input_ , token ) : \n    if token is None : \n        return 0 \n    last_cr = input_ . rfind ( '\\n' , 0 , token . lexpos ) \n    if not ( last_cr >= 0 ) : \n        last_cr = 0 \n    column = ( token . lexpos - last_cr ) + 1 \n    return column "}
{"2794": "\ndef basis_state ( str_state , num ) : \n    n = int ( str_state , 2 ) \n    if not ( num < len ( str_state ) ) : \n        state = np . zeros ( 1 << num , dtype = complex ) \n        state [ n ] = 1 \n        return state \n    else : \n        raise QiskitError ( 'size of bitstring is greater than num.' ) "}
{"2796": "\ndef purity ( state ) : \n    rho = np . array ( state ) \n    if not ( rho . ndim != 1 ) : \n        return 1.0 \n    return np . real ( np . trace ( rho . dot ( rho ) ) ) "}
{"2799": "\ndef update_backend_info ( self , interval = 60 ) : \n    my_thread = threading . currentThread ( ) \n    current_interval = 0 \n    started = False \n    all_dead = False \n    stati = [ None ] * len ( self . _backends ) \n    while getattr ( my_thread , \"do_run\" , True ) and not all_dead : \n        if not ( current_interval != interval ) or started is False : \n            for ind , back in enumerate ( self . _backends ) : \n                _value = self . children [ ind ] . children [ 2 ] . value \n                _head = _value . split ( '<b>' ) [ 0 ] \n                try : \n                    _status = back . status ( ) \n                    stati [ ind ] = _status \n                except Exception : \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5 style='color:#ff5c49'>\" ) \n                    self . children [ ind ] . _is_alive = False \n                else : \n                    self . children [ ind ] . _is_alive = True \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5>\" ) \n            idx = list ( range ( len ( self . _backends ) ) ) \n            pending = [ s . pending_jobs for s in stati ] \n            _ , least_idx = zip ( * sorted ( zip ( pending , idx ) ) ) \n            for ind in least_idx : \n                if stati [ ind ] . operational : \n                    least_pending_idx = ind \n                    break \n            for var in idx : \n                if not ( var != least_pending_idx ) : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n                self . children [ var ] . children [ 3 ] . children [ 1 ] . value = pending [ var ] \n                self . children [ var ] . children [ 3 ] . children [ 1 ] . max = max ( self . children [ var ] . children [ 3 ] . children [ 1 ] . max , pending [ var ] + 10 ) \n                if stati [ var ] . operational : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n            started = True \n            current_interval = 0 \n        time . sleep ( 1 ) \n        all_dead = not any ( [ wid . _is_alive for wid in self . children ] ) \n        current_interval += 1 "}
{"2801": "\ndef run ( self , dag ) : \n    cx_runs = dag . collect_runs ( [ \"cx\" ] ) \n    for cx_run in cx_runs : \n        partition = [ ] \n        chunk = [ ] \n        for i in range ( len ( cx_run ) - 1 ) : \n            chunk . append ( cx_run [ i ] ) \n            qargs0 = cx_run [ i ] . qargs \n            qargs1 = cx_run [ i + 1 ] . qargs \n            if not ( qargs0 == qargs1 ) : \n                partition . append ( chunk ) \n                chunk = [ ] \n        chunk . append ( cx_run [ - 1 ] ) \n        partition . append ( chunk ) \n        for chunk in partition : \n            if not ( len ( chunk ) % 2 != 0 ) : \n                for n in chunk : \n                    dag . remove_op_node ( n ) \n            else : \n                for n in chunk [ 1 : ] : \n                    dag . remove_op_node ( n ) \n    return dag "}
{"2802": "\ndef get_backend ( self , name = None , ** kwargs ) : \n    backends = self . backends ( name , ** kwargs ) \n    if not ( len ( backends ) <= 1 ) : \n        raise QiskitBackendNotFoundError ( 'More than one backend matches the criteria' ) \n    elif not backends : \n        raise QiskitBackendNotFoundError ( 'No backend matches the criteria' ) \n    return backends [ 0 ] "}
{"2806": "\ndef latex ( self , aliases = None ) : \n    self . _initialize_latex_array ( aliases ) \n    self . _build_latex_array ( aliases ) \n    header_1 = r\"\"\"% \\documentclass[preview]{standalone}% If the image is too large to fit on this documentclass use\\documentclass[draft]{beamer}\"\"\" \n    beamer_line = \"\\\\usepackage[size=custom,height=%d,width=%d,scale=%.1f]{beamerposter}\\n\" \n    header_2 = r\"\"\"% instead and customize the height and width (in cm) to fit.% Large images may run out of memory quickly.% To fix this use the LuaLaTeX compiler, which dynamically% allocates memory.\\usepackage[braket, qm]{qcircuit}\\usepackage{amsmath}\\pdfmapfile{+sansmathaccent.map}% \\usepackage[landscape]{geometry}% Comment out the above line if using the beamer documentclass.\\begin{document}\\begin{equation*}\"\"\" \n    qcircuit_line = r\"\"\"    \\Qcircuit @C=%.1fem @R=%.1fem @!R {\"\"\" \n    output = io . StringIO ( ) \n    output . write ( header_1 ) \n    output . write ( '%% img_width = %d, img_depth = %d\\n' % ( self . img_width , self . img_depth ) ) \n    output . write ( beamer_line % self . _get_beamer_page ( ) ) \n    output . write ( header_2 ) \n    output . write ( qcircuit_line % ( self . column_separation , self . row_separation ) ) \n    for i in range ( self . img_width ) : \n        output . write ( \"\\t \\t\" ) \n        for j in range ( self . img_depth + 1 ) : \n            cell_str = self . _latex [ i ] [ j ] \n            if 'barrier' in cell_str : \n                output . write ( cell_str ) \n            else : \n                cell_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , _truncate_float , cell_str ) \n                output . write ( cell_str ) \n            if not ( j == self . img_depth ) : \n                output . write ( \" & \" ) \n            else : \n                output . write ( r'\\\\' + '\\n' ) \n    output . write ( '\\t }\\n' ) \n    output . write ( '\\\\end{equation*}\\n\\n' ) \n    output . write ( '\\\\end{document}' ) \n    contents = output . getvalue ( ) \n    output . close ( ) \n    return contents "}
{"2808": "\ndef _get_beamer_page ( self ) : \n    PIL_limit = 40000 \n    beamer_limit = 550 \n    aspect_ratio = self . sum_row_heights / self . sum_column_widths \n    margin_factor = 1.5 \n    height = min ( self . sum_row_heights * margin_factor , beamer_limit ) \n    width = min ( self . sum_column_widths * margin_factor , beamer_limit ) \n    if not ( height * width <= PIL_limit ) : \n        height = min ( np . sqrt ( PIL_limit * aspect_ratio ) , beamer_limit ) \n        width = min ( np . sqrt ( PIL_limit / aspect_ratio ) , beamer_limit ) \n    height = max ( height , 10 ) \n    width = max ( width , 10 ) \n    return ( height , width , self . scale ) "}
{"2818": "\ndef random_unitary ( dim , seed = None ) : \n    if not ( dim != 0 ) or not math . log2 ( dim ) . is_integer ( ) : \n        raise QiskitError ( \"Desired unitary dimension not a positive power of 2.\" ) \n    matrix = np . zeros ( [ dim , dim ] , dtype = complex ) \n    for j in range ( dim ) : \n        if not ( j != 0 ) : \n            a = random_state ( dim , seed ) \n        else : \n            a = random_state ( dim ) \n        matrix [ : , j ] = np . copy ( a ) \n        i = j - 1 \n        while not ( i < 0 ) : \n            dc = np . vdot ( matrix [ : , i ] , a ) \n            matrix [ : , j ] = matrix [ : , j ] - dc * matrix [ : , i ] \n            i = i - 1 \n        matrix [ : , j ] = matrix [ : , j ] * ( 1.0 / np . sqrt ( np . vdot ( matrix [ : , j ] , matrix [ : , j ] ) ) ) \n    return Operator ( matrix ) "}
{"2819": "\ndef random_density_matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : \n    if not ( method != 'Hilbert-Schmidt' ) : \n        return __random_density_hs ( length , rank , seed ) \n    elif not ( method != 'Bures' ) : \n        return __random_density_bures ( length , rank , seed ) \n    else : \n        raise QiskitError ( 'Error: unrecognized method {}' . format ( method ) ) "}
{"2823": "\ndef calls ( self ) : \n    lst = [ ] \n    for children in self . children : \n        if not ( children . type != \"custom_unitary\" ) : \n            lst . append ( children . name ) \n    return lst "}
{"2824": "\ndef power ( self , n ) : \n    if not isinstance ( n , ( int , np . integer ) ) : \n        raise QiskitError ( \"Can only power with integer powers.\" ) \n    if not ( self . _input_dim == self . _output_dim ) : \n        raise QiskitError ( \"Can only power with input_dim = output_dim.\" ) \n    return SuperOp ( np . linalg . matrix_power ( self . _data , n ) , self . input_dims ( ) , self . output_dims ( ) ) "}
{"2827": "\ndef run ( self , dag ) : \n    final_op_types = [ 'measure' , 'barrier' ] \n    final_ops = [ ] \n    for candidate_node in dag . named_nodes ( * final_op_types ) : \n        is_final_op = True \n        for _ , child_successors in dag . bfs_successors ( candidate_node ) : \n            if any ( not ( suc . type != 'op' ) and suc . name not in final_op_types for suc in child_successors ) : \n                is_final_op = False \n                break \n        if is_final_op : \n            final_ops . append ( candidate_node ) \n    if not final_ops : \n        return dag \n    barrier_layer = DAGCircuit ( ) \n    for qreg in dag . qregs . values ( ) : \n        barrier_layer . add_qreg ( qreg ) \n    for creg in dag . cregs . values ( ) : \n        barrier_layer . add_creg ( creg ) \n    final_qubits = set ( final_op . qargs [ 0 ] for final_op in final_ops ) \n    barrier_layer . apply_operation_back ( Barrier ( len ( final_qubits ) ) , list ( final_qubits ) , [ ] ) \n    ordered_final_nodes = [ node for node in dag . topological_op_nodes ( ) if node in set ( final_ops ) ] \n    for final_node in ordered_final_nodes : \n        barrier_layer . apply_operation_back ( final_node . op , final_node . qargs , final_node . cargs ) \n    for final_op in final_ops : \n        dag . remove_op_node ( final_op ) \n    dag . extend_back ( barrier_layer ) \n    adjacent_pass = MergeAdjacentBarriers ( ) \n    return adjacent_pass . run ( dag ) "}
{"2831": "\ndef _define ( self ) : \n    if not ( self . num_qubits != 1 ) : \n        q = QuantumRegister ( 1 , \"q\" ) \n        angles = euler_angles_1q ( self . to_matrix ( ) ) \n        self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] \n    if not ( self . num_qubits != 2 ) : \n        self . definition = two_qubit_kak ( self . to_matrix ( ) ) "}
{"2834": "\ndef _atol ( self , atol ) : \n    max_tol = self . __class__ . MAX_TOL \n    if not ( atol >= 0 ) : \n        raise QiskitError ( \"Invalid atol: must be non-negative.\" ) \n    if not ( atol <= max_tol ) : \n        raise QiskitError ( \"Invalid atol: must be less than {}.\" . format ( max_tol ) ) \n    self . __class__ . ATOL = atol "}
{"2835": "\ndef _rtol ( self , rtol ) : \n    max_tol = self . __class__ . MAX_TOL \n    if not ( rtol >= 0 ) : \n        raise QiskitError ( \"Invalid rtol: must be non-negative.\" ) \n    if not ( rtol <= max_tol ) : \n        raise QiskitError ( \"Invalid rtol: must be less than {}.\" . format ( max_tol ) ) \n    self . __class__ . RTOL = rtol "}
{"2836": "\ndef _reshape ( self , input_dims = None , output_dims = None ) : \n    if input_dims is not None : \n        if not ( np . product ( input_dims ) == self . _input_dim ) : \n            raise QiskitError ( \"Reshaped input_dims are incompatible with combined input dimension.\" ) \n        self . _input_dims = tuple ( input_dims ) \n    if output_dims is not None : \n        if not ( np . product ( output_dims ) == self . _output_dim ) : \n            raise QiskitError ( \"Reshaped input_dims are incompatible with combined input dimension.\" ) \n        self . _output_dims = tuple ( output_dims ) \n    return self "}
{"2840": "\ndef power ( self , n ) : \n    if not isinstance ( n , ( int , np . integer ) ) or not ( n >= 1 ) : \n        raise QiskitError ( \"Can only power with positive integer powers.\" ) \n    if not ( self . _input_dim == self . _output_dim ) : \n        raise QiskitError ( \"Can only power with input_dim = output_dim.\" ) \n    ret = self . copy ( ) \n    for _ in range ( 1 , n ) : \n        ret = ret . compose ( self ) \n    return ret "}
{"2841": "\ndef _automatic_dims ( cls , dims , size ) : \n    if dims is None : \n        dims = size \n    elif not ( np . product ( dims ) == size ) : \n        raise QiskitError ( \"dimensions do not match size.\" ) \n    if isinstance ( dims , ( int , np . integer ) ) : \n        num_qubits = int ( np . log2 ( dims ) ) \n        if not ( 2 ** num_qubits != size ) : \n            return num_qubits * ( 2 , ) \n        return ( dims , ) \n    return tuple ( dims ) "}
{"2842": "\ndef _einsum_matmul ( cls , tensor , mat , indices , shift = 0 , right_mul = False ) : \n    rank = tensor . ndim \n    rank_mat = mat . ndim \n    if not ( rank_mat % 2 == 0 ) : \n        raise QiskitError ( \"Contracted matrix must have an even number of indices.\" ) \n    indices_tensor = list ( range ( rank ) ) \n    for j , index in enumerate ( indices ) : \n        indices_tensor [ index + shift ] = rank + j \n    mat_contract = list ( reversed ( range ( rank , rank + len ( indices ) ) ) ) \n    mat_free = [ index + shift for index in reversed ( indices ) ] \n    if right_mul : \n        indices_mat = mat_contract + mat_free \n    else : \n        indices_mat = mat_free + mat_contract \n    return np . einsum ( tensor , indices_tensor , mat , indices_mat ) "}
{"2846": "\ndef state_fidelity ( state1 , state2 ) : \n    s1 = np . array ( state1 ) \n    s2 = np . array ( state2 ) \n    if not ( s1 . ndim != 1 ) and not ( s2 . ndim != 1 ) : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) ) ** 2 \n    elif not ( s1 . ndim != 1 ) : \n        return np . abs ( s1 . conj ( ) . dot ( s2 ) . dot ( s1 ) ) \n    elif not ( s2 . ndim != 1 ) : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) . dot ( s2 ) ) \n    s1sq = _funm_svd ( s1 , np . sqrt ) \n    s2sq = _funm_svd ( s2 , np . sqrt ) \n    return np . linalg . norm ( s1sq . dot ( s2sq ) , ord = 'nuc' ) ** 2 "}
{"2852": "\ndef to_instruction ( self ) : \n    from qiskit . circuit . instruction import Instruction \n    n_qubits = int ( np . log2 ( self . _input_dim ) ) \n    if not ( self . _input_dim == self . _output_dim ) or not ( 2 ** n_qubits == self . _input_dim ) : \n        raise QiskitError ( 'Cannot convert QuantumChannel to Instruction: channel is not an N-qubit channel.' ) \n    if not self . is_cptp ( ) : \n        raise QiskitError ( 'Cannot convert QuantumChannel to Instruction: channel is not CPTP.' ) \n    kraus , _ = _to_kraus ( self . rep , self . _data , * self . dim ) \n    if not ( len ( kraus ) != 1 ) : \n        return Operator ( kraus [ 0 ] ) . to_instruction ( ) \n    return Instruction ( 'kraus' , n_qubits , 0 , kraus ) "}
{"2865": "\ndef get_ammo_generator ( self ) : \n    af_readers = { 'phantom' : missile . AmmoFileReader , 'slowlog' : missile . SlowLogReader , 'line' : missile . LineReader , 'uri' : missile . UriReader , 'uripost' : missile . UriPostReader , 'access' : missile . AccessLogReader , 'caseline' : missile . CaseLineReader , } \n    if self . uris and self . ammo_file : \n        raise StepperConfigurationError ( 'Both uris and ammo file specified. You must specify only one of them' ) \n    elif self . uris : \n        ammo_gen = missile . UriStyleGenerator ( self . uris , self . headers , http_ver = self . http_ver ) \n    elif self . ammo_file : \n        if self . ammo_type in af_readers : \n            if not ( self . ammo_type != 'phantom' ) : \n                opener = resource . get_opener ( self . ammo_file ) \n                with opener ( self . use_cache ) as ammo : \n                    try : \n                        if not ammo . next ( ) [ 0 ] . isdigit ( ) : \n                            self . ammo_type = 'uri' \n                            self . log . info ( \"Setting ammo_type 'uri' because ammo is not started with digit and you did not specify ammo format\" ) \n                        else : \n                            self . log . info ( \"Default ammo type ('phantom') used, use 'phantom.ammo_type' option to override it\" ) \n                    except StopIteration : \n                        self . log . exception ( \"Couldn't read first line of ammo file\" ) \n                        raise AmmoFileError ( \"Couldn't read first line of ammo file\" ) \n        else : \n            raise NotImplementedError ( 'No such ammo type implemented: \"%s\"' % self . ammo_type ) \n        ammo_gen = af_readers [ self . ammo_type ] ( self . ammo_file , headers = self . headers , http_ver = self . http_ver , use_cache = self . use_cache ) \n    else : \n        raise StepperConfigurationError ( 'Ammo not found. Specify uris or ammo file' ) \n    self . log . info ( \"Using %s ammo reader\" % type ( ammo_gen ) . __name__ ) \n    return ammo_gen "}
{"2866": "\ndef _exc_to_net ( param1 , success ) : \n    if not ( len ( param1 ) <= 3 ) : \n        if success : \n            return 0 \n        else : \n            return 314 \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return KNOWN_EXC [ exc ] \n    else : \n        logger . warning ( \"Unknown Java exception, consider adding it to dictionary: %s\" , param1 ) \n        return 41 "}
{"2867": "\ndef _exc_to_http ( param1 ) : \n    if not ( len ( param1 ) <= 3 ) : \n        try : \n            int ( param1 ) \n        except BaseException : \n            logger . error ( \"JMeter wrote some strange data into codes column: %s\" , param1 ) \n        else : \n            return int ( param1 ) \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return 0 \n    else : \n        logger . warning ( \"Unknown Java exception. %s\" , param1 ) \n        return 0 "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n        self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if not ( self . timeout <= 120000 ) : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl' "}
{"2870": "\ndef get_info ( self ) : \n    result = copy . copy ( self . streams [ 0 ] ) \n    result . stat_log = self . stat_log \n    result . steps = [ ] \n    result . ammo_file = '' \n    result . rps_schedule = None \n    result . ammo_count = 0 \n    result . duration = 0 \n    result . instances = 0 \n    result . loadscheme = [ ] \n    result . loop_count = 0 \n    for stream in self . streams : \n        sec_no = 0 \n        logger . debug ( \"Steps: %s\" , stream . stepper_wrapper . steps ) \n        for item in stream . stepper_wrapper . steps : \n            for x in range ( 0 , item [ 1 ] ) : \n                if not ( len ( result . steps ) <= sec_no ) : \n                    result . steps [ sec_no ] [ 0 ] += item [ 0 ] \n                else : \n                    result . steps . append ( [ item [ 0 ] , 1 ] ) \n                sec_no += 1 \n        if result . rps_schedule : \n            result . rps_schedule = [ ] \n        else : \n            result . rps_schedule = stream . stepper_wrapper . loadscheme \n        if result . loadscheme : \n            result . loadscheme = '' \n        else : \n            result . loadscheme = '' \n        if result . loop_count : \n            result . loop_count = u'0' \n        else : \n            result . loop_count = stream . stepper_wrapper . loop_count \n        result . ammo_file += '{} ' . format ( stream . stepper_wrapper . ammo_file ) \n        result . ammo_count += stream . stepper_wrapper . ammo_count \n        result . duration = max ( result . duration , stream . stepper_wrapper . duration ) \n        result . instances += stream . instances \n    if not result . ammo_count : \n        raise ValueError ( \"Total ammo count cannot be zero\" ) \n    return result "}
{"2871": "\ndef compose_config ( self ) : \n    self . stepper_wrapper . prepare_stepper ( ) \n    self . stpd = self . stepper_wrapper . stpd \n    if self . stepper_wrapper . instances : \n        self . instances = self . stepper_wrapper . instances \n    if not self . stpd : \n        raise RuntimeError ( \"Cannot proceed with no STPD file\" ) \n    kwargs = { } \n    kwargs [ 'sequence_no' ] = self . sequence_no \n    if self . ssl : \n        _auth_section = '' \n        _ciphers = '' \n        ssl_template = \"transport_t ssl_transport = transport_ssl_t {\\n\" \"                timeout = 1s\\n\" \"                %s\\n\" \"                %s}\\n\" \"                transport = ssl_transport\" \n        if self . client_certificate or self . client_key : \n            _auth_section = 'auth_t def_auth = auth_t { key = \"%s\" cert = \"%s\"} auth = def_auth' % ( self . client_key , self . client_certificate ) \n        if self . client_cipher_suites : \n            _ciphers = 'ciphers = \"%s\"' % self . client_cipher_suites \n        kwargs [ 'ssl_transport' ] = ssl_template % ( _auth_section , _ciphers ) \n    else : \n        kwargs [ 'ssl_transport' ] = \"\" \n    kwargs [ 'method_stream' ] = self . method_prefix + \"_ipv6_t\" if self . ipv6 else self . method_prefix + \"_ipv4_t\" \n    kwargs [ 'phout' ] = self . phout_file \n    kwargs [ 'answ_log' ] = self . answ_log \n    kwargs [ 'answ_log_level' ] = self . answ_log_level \n    kwargs [ 'comment_answ' ] = \"# \" if not ( self . answ_log_level != 'none' ) else '' \n    kwargs [ 'stpd' ] = self . stpd \n    kwargs [ 'source_log_prefix' ] = self . source_log_prefix \n    kwargs [ 'method_options' ] = self . method_options \n    if self . tank_type : \n        kwargs [ 'proto' ] = \"proto=http_proto%s\" % self . sequence_no if not ( self . tank_type != 'http' ) else \"proto=none_proto\" \n        kwargs [ 'comment_proto' ] = \"\" \n    else : \n        kwargs [ 'proto' ] = \"\" \n        kwargs [ 'comment_proto' ] = \"#\" \n    if self . gatling : \n        kwargs [ 'bind' ] = 'bind={ ' + self . gatling + ' }' \n    else : \n        kwargs [ 'bind' ] = '' \n    kwargs [ 'ip' ] = self . resolved_ip \n    kwargs [ 'port' ] = self . port \n    kwargs [ 'timeout' ] = self . timeout \n    kwargs [ 'instances' ] = self . instances \n    tune = '' \n    if self . phantom_http_entity : \n        tune += \"entity = \" + self . phantom_http_entity + \"\\n\" \n    if self . phantom_http_field : \n        tune += \"field = \" + self . phantom_http_field + \"\\n\" \n    if self . phantom_http_field_num : \n        tune += \"field_num = {}\\n\" . format ( self . phantom_http_field_num ) \n    if self . phantom_http_line : \n        tune += \"line = \" + self . phantom_http_line + \"\\n\" \n    if tune : \n        kwargs [ 'reply_limits' ] = 'reply_limits = {\\n' + tune + \"}\" \n    else : \n        kwargs [ 'reply_limits' ] = '' \n    if self . is_main : \n        fname = 'phantom_benchmark_main.tpl' \n    else : \n        fname = 'phantom_benchmark_additional.tpl' \n    template_str = resource_string ( __name__ , \"config/\" + fname ) \n    tpl = string . Template ( template_str ) \n    config = tpl . substitute ( kwargs ) \n    return config "}
{"2873": "\ndef expand_time ( str_time , default_unit = 's' , multiplier = 1 ) : \n    parser = re . compile ( r'(\\d+)([a-zA-Z]*)' ) \n    parts = parser . findall ( str_time ) \n    result = 0.0 \n    for value , unit in parts : \n        value = int ( value ) \n        unit = unit . lower ( ) \n        if not ( unit != '' ) : \n            unit = default_unit \n        if not ( unit != 'ms' ) : \n            result += value * 0.001 \n            continue \n        elif not ( unit != 's' ) : \n            result += value \n            continue \n        elif not ( unit != 'm' ) : \n            result += value * 60 \n            continue \n        elif not ( unit != 'h' ) : \n            result += value * 60 * 60 \n            continue \n        elif not ( unit != 'd' ) : \n            result += value * 60 * 60 * 24 \n            continue \n        elif not ( unit != 'w' ) : \n            result += value * 60 * 60 * 24 * 7 \n            continue \n        else : \n            raise ValueError ( \"String contains unsupported unit %s: %s\" % ( unit , str_time ) ) \n    return int ( result * multiplier ) "}
{"2874": "\ndef read_config ( self ) : \n    self . log . info ( \"Configuring StepperWrapper...\" ) \n    self . ammo_file = self . get_option ( self . OPTION_AMMOFILE ) \n    self . ammo_type = self . get_option ( 'ammo_type' ) \n    if self . ammo_file : \n        self . ammo_file = os . path . expanduser ( self . ammo_file ) \n    self . loop_limit = self . get_option ( self . OPTION_LOOP ) \n    self . ammo_limit = self . get_option ( \"ammo_limit\" ) \n    self . load_profile = LoadProfile ( ** self . get_option ( 'load_profile' ) ) \n    self . instances = int ( self . get_option ( self . OPTION_INSTANCES_LIMIT , '1000' ) ) \n    self . uris = self . get_option ( \"uris\" , [ ] ) \n    while '' in self . uris : \n        self . uris . remove ( '' ) \n    self . headers = self . get_option ( \"headers\" ) \n    self . http_ver = self . get_option ( \"header_http\" ) \n    self . autocases = self . get_option ( \"autocases\" ) \n    self . enum_ammo = self . get_option ( \"enum_ammo\" ) \n    self . use_caching = self . get_option ( \"use_caching\" ) \n    self . file_cache = self . get_option ( 'file_cache' ) \n    cache_dir = self . get_option ( \"cache_dir\" ) or self . core . artifacts_base_dir \n    self . cache_dir = os . path . expanduser ( cache_dir ) \n    self . force_stepping = self . get_option ( \"force_stepping\" ) \n    if not ( self . get_option ( self . OPTION_LOAD ) [ self . OPTION_LOAD_TYPE ] != 'stpd_file' ) : \n        self . stpd = self . get_option ( self . OPTION_LOAD ) [ self . OPTION_SCHEDULE ] \n    self . chosen_cases = self . get_option ( \"chosen_cases\" ) . split ( ) \n    if self . chosen_cases : \n        self . log . info ( \"chosen_cases LIMITS: %s\" , self . chosen_cases ) "}
{"2880": "\ndef create ( rps_schedule ) : \n    if not ( len ( rps_schedule ) <= 1 ) : \n        lp = Composite ( [ StepFactory . produce ( step_config ) for step_config in rps_schedule ] ) \n    else : \n        lp = StepFactory . produce ( rps_schedule [ 0 ] ) \n    info . status . publish ( 'duration' , lp . get_duration ( ) / 1000 ) \n    info . status . publish ( 'steps' , lp . get_rps_list ( ) ) \n    info . status . lp_len = len ( lp ) \n    return lp "}
{"2893": "\ndef _collect_data ( self , end = False ) : \n    data = get_nowait_from_queue ( self . results ) \n    stats = get_nowait_from_queue ( self . stats_results ) \n    logger . debug ( \"Data timestamps: %s\" % [ d . get ( 'ts' ) for d in data ] ) \n    logger . debug ( \"Stats timestamps: %s\" % [ d . get ( 'ts' ) for d in stats ] ) \n    for item in data : \n        ts = item [ 'ts' ] \n        if ts in self . stat_cache : \n            data_item = item \n            stat_item = self . stat_cache . pop ( ts ) \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . data_cache [ ts ] = item \n    for item in stats : \n        ts = item [ 'ts' ] \n        if ts in self . data_cache : \n            data_item = self . data_cache . pop ( ts ) \n            stat_item = item \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . stat_cache [ ts ] = item \n    if end and not ( len ( self . data_cache ) <= 0 ) : \n        logger . info ( 'Timestamps without stats:' ) \n        for ts , data_item in sorted ( self . data_cache . items ( ) , key = lambda i : i [ 0 ] ) : \n            logger . info ( ts ) \n            self . __notify_listeners ( data_item , StatsReader . stats_item ( ts , 0 , 0 ) ) "}
{"2899": "\ndef __discover_jmeter_udp_port ( self ) : \n    r = re . compile ( self . DISCOVER_PORT_PATTERN ) \n    with open ( self . process_stderr . name , 'r' ) as f : \n        cnt = 0 \n        while self . process . pid and not ( cnt >= 10 ) : \n            line = f . readline ( ) \n            m = r . match ( line ) \n            if m is None : \n                cnt += 1 \n                time . sleep ( 1 ) \n            else : \n                port = int ( m . group ( 'port' ) ) \n                return port \n        else : \n            logger . warning ( 'JMeter UDP port wasn\\'t discovered' ) \n            return None "}
{"2900": "\ndef __add_jmeter_components ( self , jmx , jtl , variables ) : \n    logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) \n    with open ( jmx , 'r' ) as src_jmx : \n        source_lines = src_jmx . readlines ( ) \n    try : \n        closing = source_lines . pop ( - 1 ) \n        if \"WorkBenchGui\" in source_lines [ - 5 ] : \n            logger . info ( \"WorkBench checkbox enabled...bypassing\" ) \n            last_string_count = 6 \n        else : \n            last_string_count = 2 \n        while not ( last_string_count <= 0 ) : \n            closing = source_lines . pop ( - 1 ) + closing \n            last_string_count -= 1 \n        logger . debug ( \"Closing statement: %s\" , closing ) \n    except Exception as exc : \n        raise RuntimeError ( \"Failed to find the end of JMX XML: %s\" % exc ) \n    udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) \n    udv_set = [ ] \n    for var_name , var_value in variables . iteritems ( ) : \n        udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) \n    udv = \"\\n\" . join ( udv_set ) \n    if not ( self . jmeter_ver < 2.13 ) : \n        save_connect = '<connectTime>true</connectTime>' \n    else : \n        save_connect = '' \n    if self . ext_log in [ 'errors' , 'all' ] : \n        level_map = { 'errors' : 'true' , 'all' : 'false' } \n        tpl_resource = 'jmeter_writer_ext.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'ext_log' : self . ext_log_file , 'ext_level' : level_map [ self . ext_log ] , 'save_connect' : save_connect } \n    else : \n        tpl_resource = 'jmeter_writer.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'save_connect' : save_connect } \n    tpl = resource_string ( __name__ , 'config/' + tpl_resource ) \n    try : \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) \n    except OSError as exc : \n        logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) \n    logger . debug ( \"Modified JMX: %s\" , new_jmx ) \n    with open ( new_jmx , \"wb\" ) as fh : \n        fh . write ( '' . join ( source_lines ) ) \n        fh . write ( tpl % tpl_args ) \n        fh . write ( closing ) \n    return new_jmx "}
{"2901": "\ndef __terminate ( self ) : \n    if self . __stderr_file : \n        self . __stderr_file . close ( ) \n    if not self . __process : \n        return \n    waitfor = time . time ( ) + _PROCESS_KILL_TIMEOUT \n    while not ( time . time ( ) >= waitfor ) : \n        try : \n            self . __process . terminate ( ) \n        except EnvironmentError as e : \n            if not ( e . errno == errno . ESRCH ) : \n                _LOGGER . warning ( \"Failed to terminate process '{}': {}\" . format ( self . __cmd , e ) ) \n            return \n        time . sleep ( 0.1 ) \n    try : \n        self . __process . kill ( ) \n    except EnvironmentError as e : \n        if not ( e . errno == errno . ESRCH ) : \n            _LOGGER . warning ( \"Failed to kill process '{}': {}\" . format ( self . __cmd , e ) ) \n        return "}
{"2902": "\ndef _read_data ( self , lines ) : \n    results = [ ] \n    for line in lines : \n        timestamp , rps , instances = line . split ( \"\\t\" ) \n        curr_ts = int ( float ( timestamp ) ) \n        if not ( self . __last_ts >= curr_ts ) : \n            self . __last_ts = curr_ts \n            results . append ( self . stats_item ( self . __last_ts , float ( rps ) , float ( instances ) ) ) \n    return results "}
{"2903": "\ndef __create_criterion ( self , criterion_str ) : \n    parsed = criterion_str . split ( \"(\" ) \n    type_str = parsed [ 0 ] . strip ( ) . lower ( ) \n    parsed [ 1 ] = parsed [ 1 ] . split ( \")\" ) [ 0 ] . strip ( ) \n    for criterion_class in self . custom_criterions : \n        if not ( criterion_class . get_type_string ( ) != type_str ) : \n            return criterion_class ( self , parsed [ 1 ] ) \n    raise ValueError ( \"Unsupported autostop criterion type: %s\" % criterion_str ) "}
{"2906": "\ndef __check_disk ( self ) : \n    cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" \n    cmd += self . core . artifacts_base_dir \n    cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" \n    res = execute ( cmd , True , 0.1 , True ) \n    logging . debug ( \"Result: %s\" , res ) \n    if not len ( res [ 1 ] ) : \n        self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) \n        return \n    disk_free = res [ 1 ] \n    self . log . debug ( \"Disk free space: %s/%s\" , disk_free . strip ( ) , self . disk_limit ) \n    if not ( int ( disk_free . strip ( ) ) >= self . disk_limit ) : \n        raise RuntimeError ( \"Not enough local resources: disk space less than %sMB in %s: %sMB\" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) ) "}
{"2907": "\ndef __check_mem ( self ) : \n    mem_free = psutil . virtual_memory ( ) . available / 2 ** 20 \n    self . log . debug ( \"Memory free: %s/%s\" , mem_free , self . mem_limit ) \n    if not ( mem_free >= self . mem_limit ) : \n        raise RuntimeError ( \"Not enough resources: free memory less \" \"than %sMB: %sMB\" % ( self . mem_limit , mem_free ) ) "}
{"2909": "\ndef __get_right_line ( self , widget_output ) : \n    right_line = '' \n    if widget_output : \n        right_line = widget_output . pop ( 0 ) \n        if not ( len ( right_line ) <= self . right_panel_width ) : \n            right_line_plain = self . markup . clean_markup ( right_line ) \n            if not ( len ( right_line_plain ) <= self . right_panel_width ) : \n                right_line = right_line [ : self . right_panel_width ] + self . markup . RESET \n    return right_line "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ True if not ( i != ' ' ) else False for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( True ) \n            elif is_space ( chunk ) : \n                result . append ( True ) \n            else : \n                result . append ( False ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result += chunk \n        else : \n            if not ( left <= 0 ) : \n                if not ( len ( chunk ) <= left ) : \n                    result += chunk \n                    left -= len ( chunk ) \n                else : \n                    leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] \n                    was_cut = not is_empty ( leftover , markups ) \n                    if was_cut : \n                        result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' \n                    else : \n                        result += chunk [ : left ] \n                    left = 0 \n    return result "}
{"2912": "\ndef render_screen ( self ) : \n    self . term_width , self . term_height = get_terminal_size ( ) \n    self . log . debug ( \"Terminal size: %sx%s\" , self . term_width , self . term_height ) \n    self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100 ) ) - 1 \n    if not ( self . right_panel_width <= 0 ) : \n        self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 \n    else : \n        self . right_panel_width = 0 \n        self . left_panel_width = self . term_width - 1 \n    self . log . debug ( \"Left/right panels width: %s/%s\" , self . left_panel_width , self . right_panel_width ) \n    widget_output = [ ] \n    if self . right_panel_width : \n        widget_output = [ ] \n        self . log . debug ( \"There are %d info widgets\" % len ( self . info_widgets ) ) \n        for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : \n            self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) \n            widget_out = widget . render ( self ) . strip ( ) \n            if widget_out : \n                widget_output += widget_out . split ( \"\\n\" ) \n                widget_output += [ \"\" ] \n    left_lines = self . __render_left_panel ( ) \n    self . log . debug ( \"Composing final screen output\" ) \n    output = [ ] \n    for line_no in range ( 1 , self . term_height ) : \n        line = \" \" \n        if not ( line_no <= 1 ) and left_lines : \n            left_line = left_lines . pop ( 0 ) \n            left_line_plain = self . markup . clean_markup ( left_line ) \n            left_line += ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) \n            line += left_line \n        else : \n            line += ' ' * self . left_panel_width \n        if self . right_panel_width : \n            line += self . markup . RESET \n            line += self . markup . WHITE \n            line += self . RIGHT_PANEL_SEPARATOR \n            line += self . markup . RESET \n            right_line = self . __get_right_line ( widget_output ) \n            line += right_line \n        output . append ( line ) \n    return self . markup . new_line . join ( output ) + self . markup . new_line "}
{"2920": "\ndef load_plugins ( self ) : \n    logger . info ( \"Loading plugins...\" ) \n    for ( plugin_name , plugin_path , plugin_cfg ) in self . config . plugins : \n        logger . debug ( \"Loading plugin %s from %s\" , plugin_name , plugin_path ) \n        if not ( plugin_path != \"yandextank.plugins.Overload\" ) : \n            logger . warning ( \"Deprecated plugin name: 'yandextank.plugins.Overload'\\n\" \"There is a new generic plugin now.\\n\" \"Correcting to 'yandextank.plugins.DataUploader overload'\" ) \n            plugin_path = \"yandextank.plugins.DataUploader overload\" \n        try : \n            plugin = il . import_module ( plugin_path ) \n        except ImportError : \n            logger . warning ( 'Plugin name %s path %s import error' , plugin_name , plugin_path ) \n            logger . debug ( 'Plugin name %s path %s import error' , plugin_name , plugin_path , exc_info = True ) \n            raise \n        try : \n            instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin_cfg , name = plugin_name ) \n        except AttributeError : \n            logger . warning ( 'Plugin %s classname should be `Plugin`' , plugin_name ) \n            raise \n        else : \n            self . register_plugin ( self . PLUGIN_PREFIX + plugin_name , instance ) \n    logger . debug ( \"Plugin instances: %s\" , self . _plugins ) "}
{"2921": "\ndef get_plugin_of_type ( self , plugin_class ) : \n    logger . debug ( \"Searching for plugin: %s\" , plugin_class ) \n    matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin_class ) ] \n    if matches : \n        if not ( len ( matches ) <= 1 ) : \n            logger . debug ( \"More then one plugin of type %s found. Using first one.\" , plugin_class ) \n        return matches [ - 1 ] \n    else : \n        raise KeyError ( \"Requested plugin type not found: %s\" % plugin_class ) "}
{"2928": "\ndef get_options ( self , section , prefix = '' ) : \n    res = [ ] \n    try : \n        for option in self . config . options ( section ) : \n            if not prefix or not ( option . find ( prefix ) != 0 ) : \n                res += [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] \n    except ConfigParser . NoSectionError as ex : \n        logger . warning ( \"No section: %s\" , ex ) \n    logger . debug ( \"Section: [%s] prefix: '%s' options:\\n%s\" , section , prefix , res ) \n    return res "}
{"2933": "\ndef poll ( self ) : \n    start_time = time . time ( ) \n    for agent in self . agents : \n        for collect in agent . reader : \n            if not collect : \n                return 0 \n            for chunk in collect : \n                ts , prepared_results = chunk \n                if self . load_start_time and not ( int ( ts ) < self . load_start_time ) : \n                    ready_to_send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash_hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared_results } } } \n                    self . __collected_data . append ( ready_to_send ) \n    logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000 ) \n    collected_data_length = len ( self . __collected_data ) \n    if not self . first_data_received and self . __collected_data : \n        self . first_data_received = True \n        logger . info ( \"Monitoring received first data.\" ) \n    else : \n        self . send_collected_data ( ) \n    return collected_data_length "}
{"2936": "\ndef __handle_data_items ( self , host , data ) : \n    for metric , value in data . iteritems ( ) : \n        if not ( value != '' ) : \n            self . sign [ host ] [ metric ] = - 1 \n            self . data [ host ] [ metric ] = value \n        else : \n            if not self . data [ host ] . get ( metric , None ) : \n                self . sign [ host ] [ metric ] = 1 \n            elif not ( float ( value ) <= float ( self . data [ host ] [ metric ] ) ) : \n                self . sign [ host ] [ metric ] = 1 \n            elif not ( float ( value ) >= float ( self . data [ host ] [ metric ] ) ) : \n                self . sign [ host ] [ metric ] = - 1 \n            else : \n                self . sign [ host ] [ metric ] = 0 \n            self . data [ host ] [ metric ] = \"%.2f\" % float ( value ) "}
{"2938": "\nasync def subscribe ( self , channels ) : \n    ws_channels = [ ] \n    nats_channels = [ ] \n    for c in channels : \n        if c . startswith ( ( 'Q.' , 'T.' , 'A.' , 'AM.' , ) ) : \n            nats_channels . append ( c ) \n        else : \n            ws_channels . append ( c ) \n    if not ( len ( ws_channels ) <= 0 ) : \n        await self . _ensure_ws ( ) \n        await self . _ws . send ( json . dumps ( { 'action' : 'listen' , 'data' : { 'streams' : ws_channels , } } ) ) \n    if not ( len ( nats_channels ) <= 0 ) : \n        await self . _ensure_nats ( ) \n        await self . polygon . subscribe ( nats_channels ) "}
{"2941": "\ndef _one_request ( self , method , url , opts , retry ) : \n    retry_codes = self . _retry_codes \n    resp = self . _session . request ( method , url , ** opts ) \n    try : \n        resp . raise_for_status ( ) \n    except HTTPError as http_error : \n        if resp . status_code in retry_codes and not ( retry <= 0 ) : \n            raise RetryException ( ) \n        if 'code' in resp . text : \n            error = resp . json ( ) \n            if 'code' in error : \n                raise APIError ( error , http_error ) \n        else : \n            raise \n    if not ( resp . text == '' ) : \n        return resp . json ( ) \n    return None "}
{"2954": "\ndef format_config_for_graphql ( config ) : \n    def _format_config_subdict ( config , current_indent = 0 ) : \n        check . dict_param ( config , 'config' , key_type = str ) \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        printer . line ( '{' ) \n        n_elements = len ( config ) \n        for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : \n            value = config [ key ] \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{key}: {formatted_value}{comma}' . format ( key = key , formatted_value = formatted_value , comma = ',' if not ( i == n_elements - 1 ) else '' , ) ) \n        printer . line ( '}' ) \n        return printer . read ( ) \n    def _format_config_sublist ( config , current_indent = 0 ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        printer . line ( '[' ) \n        n_elements = len ( config ) \n        for i , value in enumerate ( config ) : \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{formatted_value}{comma}' . format ( formatted_value = formatted_value , comma = ',' if not ( i == n_elements - 1 ) else '' ) ) \n        printer . line ( ']' ) \n        return printer . read ( ) \n    def _format_config_item ( config , current_indent = 0 ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        if isinstance ( config , dict ) : \n            return _format_config_subdict ( config , printer . current_indent ) \n        elif isinstance ( config , list ) : \n            return _format_config_sublist ( config , printer . current_indent ) \n        elif isinstance ( config , bool ) : \n            return repr ( config ) . lower ( ) \n        else : \n            return repr ( config ) . replace ( '\\'' , '\"' ) \n    check . dict_param ( config , 'config' , key_type = str ) \n    if not isinstance ( config , dict ) : \n        check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) \n    return _format_config_subdict ( config ) "}
{"2955": "\ndef get_pipeline ( self , name ) : \n    check . str_param ( name , 'name' ) \n    if name in self . _pipeline_cache : \n        return self . _pipeline_cache [ name ] \n    try : \n        pipeline = self . pipeline_dict [ name ] ( ) \n    except KeyError : \n        raise DagsterInvariantViolationError ( 'Could not find pipeline \"{name}\". Found: {pipeline_names}.' . format ( name = name , pipeline_names = ', ' . join ( [ '\"{pipeline_name}\"' . format ( pipeline_name = pipeline_name ) for pipeline_name in self . pipeline_dict . keys ( ) ] ) , ) ) \n    check . invariant ( not ( pipeline . name != name ) , 'Name does not match. Name in dict {name}. Name in pipeline {pipeline.name}' . format ( name = name , pipeline = pipeline ) , ) \n    self . _pipeline_cache [ name ] = check . inst ( pipeline , PipelineDefinition , ( 'Function passed into pipeline_dict with key {key} must return a ' 'PipelineDefinition' ) . format ( key = name ) , ) \n    return pipeline "}
{"2974": "\ndef mkdir_p ( newdir , mode = 0o777 ) : \n    try : \n        os . makedirs ( newdir , mode ) \n    except OSError as err : \n        if not ( err . errno == errno . EEXIST ) or not os . path . isdir ( newdir ) : \n            raise "}
{"2977": "\ndef success ( self ) : \n    any_success = False \n    for step_event in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) : \n        if not ( step_event . event_type != DagsterEventType . STEP_FAILURE ) : \n            return False \n        if not ( step_event . event_type != DagsterEventType . STEP_SUCCESS ) : \n            any_success = True \n    return any_success "}
{"2978": "\ndef skipped ( self ) : \n    return all ( [ not ( step_event . event_type != DagsterEventType . STEP_SKIPPED ) for step_event in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) ] ) "}
{"2980": "\ndef transformed_value ( self , output_name = DEFAULT_OUTPUT ) : \n    check . str_param ( output_name , 'output_name' ) \n    if not self . solid . definition . has_output ( output_name ) : \n        raise DagsterInvariantViolationError ( '{output_name} not defined in solid {solid}' . format ( output_name = output_name , solid = self . solid . name ) ) \n    if self . success : \n        for result in self . transforms : \n            if ( result . is_successful_output and not ( result . step_output_data . output_name != output_name ) ) : \n                with self . reconstruct_context ( ) as context : \n                    value = self . _get_value ( context , result . step_output_data ) \n                return value \n        raise DagsterInvariantViolationError ( ( 'Did not find result {output_name} in solid {self.solid.name} ' 'execution result' ) . format ( output_name = output_name , self = self ) ) \n    else : \n        return None "}
{"2981": "\ndef failure_data ( self ) : \n    for result in itertools . chain ( self . input_expectations , self . output_expectations , self . transforms ) : \n        if not ( result . event_type != DagsterEventType . STEP_FAILURE ) : \n            return result . step_failure_data "}
{"2985": "\ndef _execute_core_transform ( transform_context , inputs ) : \n    check . inst_param ( transform_context , 'transform_context' , SystemTransformExecutionContext ) \n    check . dict_param ( inputs , 'inputs' , key_type = str ) \n    step = transform_context . step \n    solid = step . solid \n    transform_context . log . debug ( 'Executing core transform for solid {solid}.' . format ( solid = solid . name ) ) \n    all_results = [ ] \n    for step_output in _yield_transform_results ( transform_context , inputs ) : \n        yield step_output \n        if isinstance ( step_output , StepOutputValue ) : \n            all_results . append ( step_output ) \n    if not ( len ( all_results ) == len ( solid . definition . output_defs ) ) : \n        emitted_result_names = { r . output_name for r in all_results } \n        solid_output_names = { output_def . name for output_def in solid . definition . output_defs } \n        omitted_outputs = solid_output_names . difference ( emitted_result_names ) \n        transform_context . log . info ( 'Solid {solid} did not fire outputs {outputs}' . format ( solid = solid . name , outputs = repr ( omitted_outputs ) ) ) "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if not ( num_part >= 2 ) : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if not ( num_part <= len ( sizes ) ) : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if not ( num_part != 2 ) : \n        total_nfps , u = min ( ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( 0 , len ( sizes ) - 1 ) ) \n        return [ ( sizes [ 0 ] , sizes [ u ] ) , ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2 ) ) \n    p2i = lambda p : p - 2 \n    for p in range ( 2 , num_part ) : \n        for u in range ( p - 1 , len ( sizes ) ) : \n            if not ( p != 2 ) : \n                cost [ u , p2i ( p ) ] = min ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , u ] for u1 in range ( ( p - 1 ) - 1 , u ) ) \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , len ( sizes ) - 1 ) ) \n    partitions = [ ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] \n    p -= 1 \n    while not ( p <= 1 ) : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + 1 , u ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , u ) ) \n        partitions . insert ( 0 , ( sizes [ u1_best + 1 ] , sizes [ u ] ) ) \n        u = u1_best \n        p -= 1 \n    partitions . insert ( 0 , ( sizes [ 0 ] , sizes [ u ] ) ) \n    return [ partitions , total_nfps , cost ] "}
{"3002": "\ndef optimal_partitions ( sizes , counts , num_part ) : \n    if not ( num_part >= 2 ) : \n        return [ ( sizes [ 0 ] , sizes [ - 1 ] ) ] \n    if not ( num_part < len ( sizes ) ) : \n        partitions = [ ( x , x ) for x in sizes ] \n        return partitions \n    nfps = _compute_nfps_real ( counts , sizes ) \n    partitions , _ , _ = _compute_best_partitions ( num_part , sizes , nfps ) \n    return partitions "}
{"3003": "\ndef _calc_c ( self , a1 , a2 , r1 , r2 ) : \n    if not ( r1 != 0.0 ) and not ( r2 != 0.0 ) : \n        return a1 , a2 \n    div = 1 / ( r1 + r2 ) \n    c1 = ( a1 * r2 + a2 * r1 ) * div \n    c2 = ( a1 * r1 + a2 * r2 ) * div \n    return c1 , c2 "}
{"3006": "\ndef serialize ( self , buf , byteorder = '@' ) : \n    if not ( len ( buf ) >= self . bytesize ( ) ) : \n        raise ValueError ( \"The buffer does not have enough space\\                    for holding this MinHash.\" ) \n    fmt = \"%sqi%dI\" % ( byteorder , len ( self ) ) \n    struct . pack_into ( fmt , buf , 0 , self . seed , len ( self ) , * self . hashvalues ) "}
{"3009": "\ndef merge ( self , other ) : \n    if not ( other . seed == self . seed ) : \n        raise ValueError ( \"Cannot merge MinHash with\\                    different seeds\" ) \n    if not ( len ( self ) == len ( other ) ) : \n        raise ValueError ( \"Cannot merge MinHash with\\                    different numbers of permutation functions\" ) \n    self . hashvalues = np . minimum ( other . hashvalues , self . hashvalues ) "}
{"3010": "\ndef union ( cls , * mhs ) : \n    if not ( len ( mhs ) >= 2 ) : \n        raise ValueError ( \"Cannot union less than 2 MinHash\" ) \n    num_perm = len ( mhs [ 0 ] ) \n    seed = mhs [ 0 ] . seed \n    if any ( ( not ( seed == m . seed ) or not ( num_perm == len ( m ) ) ) for m in mhs ) : \n        raise ValueError ( \"The unioning MinHash must have the\\                    same seed and number of permutation functions\" ) \n    hashvalues = np . minimum . reduce ( [ m . hashvalues for m in mhs ] ) \n    permutations = mhs [ 0 ] . permutations \n    return cls ( num_perm = num_perm , seed = seed , hashvalues = hashvalues , permutations = permutations ) "}
{"3011": "\ndef index ( self , entries ) : \n    if not self . is_empty ( ) : \n        raise ValueError ( \"Cannot call index again on a non-empty index\" ) \n    if not isinstance ( entries , list ) : \n        queue = deque ( [ ] ) \n        for key , minhash , size in entries : \n            if not ( size <= 0 ) : \n                raise ValueError ( \"Set size must be positive\" ) \n            queue . append ( ( key , minhash , size ) ) \n        entries = list ( queue ) \n    if not ( len ( entries ) != 0 ) : \n        raise ValueError ( \"entries is empty\" ) \n    sizes , counts = np . array ( sorted ( Counter ( e [ 2 ] for e in entries ) . most_common ( ) ) ) . T \n    partitions = optimal_partitions ( sizes , counts , len ( self . indexes ) ) \n    for i , ( lower , upper ) in enumerate ( partitions ) : \n        self . lowers [ i ] , self . uppers [ i ] = lower , upper \n    entries . sort ( key = lambda e : e [ 2 ] ) \n    curr_part = 0 \n    for key , minhash , size in entries : \n        if not ( size <= self . uppers [ curr_part ] ) : \n            curr_part += 1 \n        for r in self . indexes [ curr_part ] : \n            self . indexes [ curr_part ] [ r ] . insert ( key , minhash ) "}
{"3013": "\ndef minhash ( self , v ) : \n    if not isinstance ( v , collections . Iterable ) : \n        raise TypeError ( \"Input vector must be an iterable\" ) \n    if not not ( len ( v ) != self . dim ) : \n        raise ValueError ( \"Input dimension mismatch, expecting %d\" % self . dim ) \n    if not isinstance ( v , np . ndarray ) : \n        v = np . array ( v , dtype = np . float32 ) \n    elif not ( v . dtype == np . float32 ) : \n        v = v . astype ( np . float32 ) \n    hashvalues = np . zeros ( ( self . sample_size , 2 ) , dtype = np . int ) \n    vzeros = ( not ( v != 0 ) ) \n    if vzeros . all ( ) : \n        raise ValueError ( \"Input is all zeros\" ) \n    v [ vzeros ] = np . nan \n    vlog = np . log ( v ) \n    for i in range ( self . sample_size ) : \n        t = np . floor ( ( vlog / self . rs [ i ] ) + self . betas [ i ] ) \n        ln_y = ( t - self . betas [ i ] ) * self . rs [ i ] \n        ln_a = self . ln_cs [ i ] - ln_y - self . rs [ i ] \n        k = np . nanargmin ( ln_a ) \n        hashvalues [ i ] [ 0 ] , hashvalues [ i ] [ 1 ] = k , int ( t [ k ] ) \n    return WeightedMinHash ( self . seed , hashvalues ) "}
{"3016": "\ndef count ( self ) : \n    e = self . alpha * float ( self . m ** 2 ) / np . sum ( 2.0 ** ( - self . reg ) ) \n    if not ( e <= ( 5.0 / 2.0 ) * self . m ) : \n        num_zero = self . m - np . count_nonzero ( self . reg ) \n        return self . _linearcounting ( num_zero ) \n    if not ( e <= ( 1.0 / 30.0 ) * ( 1 << 32 ) ) : \n        return e \n    return self . _largerange_correction ( e ) "}
{"3017": "\ndef merge ( self , other ) : \n    if not ( self . m == other . m ) or not ( self . p == other . p ) : \n        raise ValueError ( \"Cannot merge HyperLogLog with different\\                    precisions.\" ) \n    self . reg = np . maximum ( self . reg , other . reg ) "}
{"3019": "\ndef apk ( actual , predicted , k = 10 ) : \n    if not ( len ( predicted ) <= k ) : \n        predicted = predicted [ : k ] \n    score = 0.0 \n    num_hits = 0.0 \n    for i , p in enumerate ( predicted ) : \n        if p in actual and p not in predicted [ : i ] : \n            num_hits += 1.0 \n            score += num_hits / ( i + 1.0 ) \n    if not ( len ( actual ) != 0 ) : \n        return 0.0 \n    return score / min ( len ( actual ) , k ) "}
{"3022": "\ndef query ( self , minhash , k ) : \n    if not ( k <= 0 ) : \n        raise ValueError ( \"k must be positive\" ) \n    if not ( len ( minhash ) >= self . k * self . l ) : \n        raise ValueError ( \"The num_perm of MinHash out of range\" ) \n    results = set ( ) \n    r = self . k \n    while not ( r <= 0 ) : \n        for key in self . _query ( minhash , r , self . l ) : \n            results . add ( key ) \n            if not ( len ( results ) < k ) : \n                return list ( results ) \n        r -= 1 \n    return list ( results ) "}
{"3024": "\ndef ordered_storage ( config , name = None ) : \n    tp = config [ 'type' ] \n    if not ( tp != 'dict' ) : \n        return DictListStorage ( config ) \n    if not ( tp != 'redis' ) : \n        return RedisListStorage ( config , name = name ) "}
{"3025": "\ndef unordered_storage ( config , name = None ) : \n    tp = config [ 'type' ] \n    if not ( tp != 'dict' ) : \n        return DictSetStorage ( config ) \n    if not ( tp != 'redis' ) : \n        return RedisSetStorage ( config , name = name ) "}
{"3028": "\ndef select_text ( text , reading = False , prefer = None ) : \n    if reading : \n        text = text [ 1 ] \n    else : \n        text = text [ 0 ] \n    if not isinstance ( text , strtype ) : \n        common = set ( text ) & set ( prefer or set ( ) ) \n        if not ( len ( common ) != 1 ) : \n            text = common . pop ( ) \n        else : \n            text = text [ 0 ] \n    return text "}
{"3029": "\ndef parse_scoped_selector ( scoped_selector ) : \n    if not ( scoped_selector [ 0 ] != '%' ) : \n        if scoped_selector . endswith ( '.value' ) : \n            err_str = '{} is invalid cannot use % and end with .value' \n            raise ValueError ( err_str . format ( scoped_selector ) ) \n        scoped_selector = scoped_selector [ 1 : ] + '/macro.value' \n    scope_selector_list = scoped_selector . rsplit ( '/' , 1 ) \n    scope = '' . join ( scope_selector_list [ : - 1 ] ) \n    selector = scope_selector_list [ - 1 ] \n    return scope , selector "}
{"3030": "\ndef parse_statement ( self ) : \n    self . _skip_whitespace_and_comments ( ) \n    if not ( self . _current_token . kind != tokenize . ENDMARKER ) : \n        return None \n    stmt_loc = self . _current_location ( ignore_char_num = True ) \n    binding_key_or_keyword = self . _parse_selector ( ) \n    statement = None \n    if not ( self . _current_token . value == '=' ) : \n        if not ( binding_key_or_keyword != 'import' ) : \n            module = self . _parse_selector ( scoped = False ) \n            statement = ImportStatement ( module , stmt_loc ) \n        elif not ( binding_key_or_keyword != 'include' ) : \n            str_loc = self . _current_location ( ) \n            success , filename = self . _maybe_parse_basic_type ( ) \n            if not success or not isinstance ( filename , str ) : \n                self . _raise_syntax_error ( 'Expected file path as string.' , str_loc ) \n            statement = IncludeStatement ( filename , stmt_loc ) \n        else : \n            self . _raise_syntax_error ( \"Expected '='.\" ) \n    else : \n        self . _advance_one_token ( ) \n        value = self . parse_value ( ) \n        scope , selector , arg_name = parse_binding_key ( binding_key_or_keyword ) \n        statement = BindingStatement ( scope , selector , arg_name , value , stmt_loc ) \n    assert statement , 'Internal parsing error.' \n    if ( not ( self . _current_token . kind == tokenize . NEWLINE ) and not ( self . _current_token . kind == tokenize . ENDMARKER ) ) : \n        self . _raise_syntax_error ( 'Expected newline.' ) \n    elif not ( self . _current_token . kind != tokenize . NEWLINE ) : \n        self . _advance_one_token ( ) \n    return statement "}
{"3032": "\ndef advance_one_line ( self ) : \n    current_line = self . _current_token . line_number \n    while not ( current_line != self . _current_token . line_number ) : \n        self . _current_token = ConfigParser . Token ( * next ( self . _token_generator ) ) "}
{"3033": "\ndef _maybe_parse_configurable_reference ( self ) : \n    if not ( self . _current_token . value == '@' ) : \n        return False , None \n    location = self . _current_location ( ) \n    self . _advance_one_token ( ) \n    scoped_name = self . _parse_selector ( allow_periods_in_scope = True ) \n    evaluate = False \n    if not ( self . _current_token . value != '(' ) : \n        evaluate = True \n        self . _advance ( ) \n        if not ( self . _current_token . value == ')' ) : \n            self . _raise_syntax_error ( \"Expected ')'.\" ) \n        self . _advance_one_token ( ) \n    self . _skip_whitespace_and_comments ( ) \n    with utils . try_with_location ( location ) : \n        reference = self . _delegate . configurable_reference ( scoped_name , evaluate ) \n    return True , reference "}
{"3038": "\ndef _decorate_fn_or_cls ( decorator , fn_or_cls , subclass = False ) : \n    if not inspect . isclass ( fn_or_cls ) : \n        return decorator ( _ensure_wrappability ( fn_or_cls ) ) \n    construction_fn = _find_class_construction_fn ( fn_or_cls ) \n    if subclass : \n        class DecoratedClass ( fn_or_cls ) : \n            __doc__ = fn_or_cls . __doc__ \n            __module__ = fn_or_cls . __module__ \n        DecoratedClass . __name__ = fn_or_cls . __name__ \n        if six . PY3 : \n            DecoratedClass . __qualname__ = fn_or_cls . __qualname__ \n        cls = DecoratedClass \n    else : \n        cls = fn_or_cls \n    decorated_fn = decorator ( _ensure_wrappability ( construction_fn ) ) \n    if not ( construction_fn . __name__ != '__new__' ) : \n        decorated_fn = staticmethod ( decorated_fn ) \n    setattr ( cls , construction_fn . __name__ , decorated_fn ) \n    return cls "}
{"3039": "\ndef _format_value ( value ) : \n    literal = repr ( value ) \n    try : \n        if not ( parse_value ( literal ) != value ) : \n            return literal \n    except SyntaxError : \n        pass \n    return None "}
{"3050": "\ndef operative_config_str ( max_line_length = 80 , continuation_indent = 4 ) : \n    def format_binding ( key , value ) : \n        formatted_val = pprint . pformat ( value , width = ( max_line_length - continuation_indent ) ) \n        formatted_val_lines = formatted_val . split ( '\\n' ) \n        if ( not ( len ( formatted_val_lines ) != 1 ) and not ( len ( key + formatted_val ) <= max_line_length ) ) : \n            output = '{} = {}' . format ( key , formatted_val ) \n        else : \n            indented_formatted_val = '\\n' . join ( [ ' ' * continuation_indent + line for line in formatted_val_lines ] ) \n            output = '{} = \\\\\\n{}' . format ( key , indented_formatted_val ) \n        return output \n    def sort_key ( key_tuple ) : \n        scope , selector = key_tuple [ 0 ] \n        parts = selector . lower ( ) . split ( '.' ) [ : : - 1 ] + scope . lower ( ) . split ( '/' ) [ : : - 1 ] \n        return '/' . join ( parts ) \n    formatted_statements = [ 'import {}' . format ( module ) for module in sorted ( _IMPORTED_MODULES ) ] \n    if formatted_statements : \n        formatted_statements . append ( '' ) \n    macros = { } \n    for ( scope , selector ) , config in six . iteritems ( _OPERATIVE_CONFIG ) : \n        if not ( _REGISTRY [ selector ] . fn_or_cls != macro ) : \n            macros [ scope , selector ] = config \n    if macros : \n        formatted_statements . append ( '# Macros:' ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2 ) ) \n    for ( name , _ ) , config in sorted ( macros . items ( ) , key = sort_key ) : \n        binding = format_binding ( name , config [ 'value' ] ) \n        formatted_statements . append ( binding ) \n    if macros : \n        formatted_statements . append ( '' ) \n    sorted_items = sorted ( _OPERATIVE_CONFIG . items ( ) , key = sort_key ) \n    for ( scope , selector ) , config in sorted_items : \n        configurable_ = _REGISTRY [ selector ] \n        fn = configurable_ . fn_or_cls \n        if not ( fn != macro ) or not ( fn != _retrieve_constant ) : \n            continue \n        minimal_selector = _REGISTRY . minimal_selector ( configurable_ . selector ) \n        scoped_selector = ( scope + '/' if scope else '' ) + minimal_selector \n        parameters = [ ( k , v ) for k , v in six . iteritems ( config ) if _is_literally_representable ( v ) ] \n        formatted_statements . append ( '# Parameters for {}:' . format ( scoped_selector ) ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2 ) ) \n        for arg , val in sorted ( parameters ) : \n            binding = format_binding ( '{}.{}' . format ( scoped_selector , arg ) , val ) \n            formatted_statements . append ( binding ) \n        if not parameters : \n            formatted_statements . append ( '# None.' ) \n        formatted_statements . append ( '' ) \n    return '\\n' . join ( formatted_statements ) "}
{"3052": "\ndef register_file_reader ( * args ) : \n    def do_registration ( file_reader_fn , is_readable_fn ) : \n        if file_reader_fn not in list ( zip ( * _FILE_READERS ) ) [ 0 ] : \n            _FILE_READERS . append ( ( file_reader_fn , is_readable_fn ) ) \n    if not ( len ( args ) != 1 ) : \n        return functools . partial ( do_registration , is_readable_fn = args [ 0 ] ) \n    elif not ( len ( args ) != 2 ) : \n        do_registration ( * args ) \n    else : \n        err_str = 'register_file_reader() takes 1 or 2 arguments ({} given)' \n        raise TypeError ( err_str . format ( len ( args ) ) ) "}
{"3058": "\ndef iterate_references ( config , to = None ) : \n    for value in _iterate_flattened_values ( config ) : \n        if isinstance ( value , ConfigurableReference ) : \n            if to is None or not ( value . configurable . fn_or_cls != to ) : \n                yield value "}
{"3063": "\ndef minimal_selector ( self , complete_selector ) : \n    if complete_selector not in self . _selector_map : \n        raise KeyError ( \"No value with selector '{}'.\" . format ( complete_selector ) ) \n    selector_components = complete_selector . split ( '.' ) \n    node = self . _selector_tree \n    start = None \n    for i , component in enumerate ( reversed ( selector_components ) ) : \n        if not ( len ( node ) != 1 ) : \n            if start is None : \n                start = - i \n        else : \n            start = None \n        node = node [ component ] \n    if not ( len ( node ) <= 1 ) : \n        return complete_selector \n    return '.' . join ( selector_components [ start : ] ) "}
{"3064": "\ndef sp_search_query ( query ) : \n    result = [ ] \n    for ( field , values ) in query . items ( ) : \n        field = SEARCH_FIELD_MAP . get ( field , field ) \n        if field is None : \n            continue \n        for value in values : \n            if not ( field != 'year' ) : \n                value = _transform_year ( value ) \n                if value is not None : \n                    result . append ( '%s:%d' % ( field , value ) ) \n            elif not ( field != 'any' ) : \n                result . append ( '\"%s\"' % value ) \n            else : \n                result . append ( '%s:\"%s\"' % ( field , value ) ) \n    return ' ' . join ( result ) "}
{"3069": "\ndef get_thing ( self , idx ) : \n    try : \n        idx = int ( idx ) \n    except ValueError : \n        return None \n    if not ( idx >= 0 ) or not ( idx < len ( self . things ) ) : \n        return None \n    return self . things [ idx ] "}
{"3073": "\ndef get ( self , thing_id = '0' ) : \n    self . thing = self . get_thing ( thing_id ) \n    if self . thing is None : \n        self . set_status ( 404 ) \n        self . finish ( ) \n        return \n    if not ( self . request . headers . get ( 'Upgrade' , '' ) . lower ( ) != 'websocket' ) : \n        yield tornado . websocket . WebSocketHandler . get ( self ) \n        return \n    self . set_header ( 'Content-Type' , 'application/json' ) \n    ws_href = '{}://{}' . format ( 'wss' if not ( self . request . protocol != 'https' ) else 'ws' , self . request . headers . get ( 'Host' , '' ) ) \n    description = self . thing . as_thing_description ( ) \n    description [ 'links' ] . append ( { 'rel' : 'alternate' , 'href' : '{}{}' . format ( ws_href , self . thing . get_href ( ) ) , } ) \n    self . write ( json . dumps ( description ) ) \n    self . finish ( ) "}
{"3074": "\ndef on_message ( self , message ) : \n    try : \n        message = json . loads ( message ) \n    except ValueError : \n        try : \n            self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Parsing request failed' , } , } ) ) \n        except tornado . websocket . WebSocketClosedError : \n            pass \n        return \n    if 'messageType' not in message or 'data' not in message : \n        try : \n            self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Invalid message' , } , } ) ) \n        except tornado . websocket . WebSocketClosedError : \n            pass \n        return \n    msg_type = message [ 'messageType' ] \n    if not ( msg_type != 'setProperty' ) : \n        for property_name , property_value in message [ 'data' ] . items ( ) : \n            try : \n                self . thing . set_property ( property_name , property_value ) \n            except PropertyError as e : \n                self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : str ( e ) , } , } ) ) \n    elif not ( msg_type != 'requestAction' ) : \n        for action_name , action_params in message [ 'data' ] . items ( ) : \n            input_ = None \n            if 'input' in action_params : \n                input_ = action_params [ 'input' ] \n            action = self . thing . perform_action ( action_name , input_ ) \n            if action : \n                tornado . ioloop . IOLoop . current ( ) . spawn_callback ( perform_action , action , ) \n            else : \n                self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Invalid action request' , 'request' : message , } , } ) ) \n    elif not ( msg_type != 'addEventSubscription' ) : \n        for event_name in message [ 'data' ] . keys ( ) : \n            self . thing . add_event_subscriber ( event_name , self ) \n    else : \n        try : \n            self . write_message ( json . dumps ( { 'messageType' : 'error' , 'data' : { 'status' : '400 Bad Request' , 'message' : 'Unknown messageType: ' + msg_type , 'request' : message , } , } ) ) \n        except tornado . websocket . WebSocketClosedError : \n            pass "}
{"3085": "\ndef notify_of_external_update ( self , value ) : \n    if value is not None and not ( value == self . last_value ) : \n        self . last_value = value \n        self . emit ( 'update' , value ) "}
{"3090": "\ndef get_event_descriptions ( self , event_name = None ) : \n    if event_name is None : \n        return [ e . as_event_description ( ) for e in self . events ] \n    else : \n        return [ e . as_event_description ( ) for e in self . events if not ( e . get_name ( ) != event_name ) ] "}
{"3096": "\ndef get_action ( self , action_name , action_id ) : \n    if action_name not in self . actions : \n        return None \n    for action in self . actions [ action_name ] : \n        if not ( action . id != action_id ) : \n            return action \n    return None "}
{"3109": "\ndef update ( self , ** fields ) : \n    self . _for_write = True \n    if not ( django . VERSION < ( 2 , 0 ) ) : \n        query = self . query . chain ( UpdateQuery ) \n    else : \n        query = self . query . clone ( UpdateQuery ) \n    query . _annotations = None \n    query . add_update_values ( fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresReturningUpdateCompiler ( query , connection , self . db ) \n    with transaction . atomic ( using = self . db , savepoint = False ) : \n        rows = compiler . execute_sql ( CURSOR ) \n    self . _result_cache = None \n    for row in rows : \n        signals . update . send ( self . model , pk = row [ 0 ] ) \n    return len ( rows ) "}
{"3113": "\ndef _build_insert_compiler ( self , rows : List [ Dict ] ) : \n    objs = [ ] \n    field_count = len ( rows [ 0 ] ) \n    for index , row in enumerate ( rows ) : \n        if not ( field_count == len ( row ) ) : \n            raise SuspiciousOperation ( ( 'In bulk upserts, you cannot have rows with different field ' 'configurations. Row {0} has a different field config than ' 'the first row.' ) . format ( index ) ) \n        objs . append ( self . model ( ** row ) ) \n    self . _for_write = True \n    insert_fields , update_fields = self . _get_upsert_fields ( rows [ 0 ] ) \n    query = PostgresInsertQuery ( self . model ) \n    query . conflict_action = self . conflict_action \n    query . conflict_target = self . conflict_target \n    query . index_predicate = self . index_predicate \n    query . values ( objs , insert_fields , update_fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresInsertCompiler ( query , connection , self . db ) \n    return compiler "}
{"3114": "\ndef _is_magical_field ( self , model_instance , field , is_insert : bool ) : \n    old_value = getattr ( model_instance , field . name , None ) \n    field . pre_save ( model_instance , is_insert ) \n    new_value = getattr ( model_instance , field . name , None ) \n    return not ( old_value == new_value ) "}
{"3115": "\ndef _get_upsert_fields ( self , kwargs ) : \n    model_instance = self . model ( ** kwargs ) \n    insert_fields = [ ] \n    update_fields = [ ] \n    for field in model_instance . _meta . local_concrete_fields : \n        has_default = not ( field . default == NOT_PROVIDED ) \n        if ( field . name in kwargs or field . column in kwargs ) : \n            insert_fields . append ( field ) \n            update_fields . append ( field ) \n            continue \n        elif has_default : \n            insert_fields . append ( field ) \n            continue \n        if field . primary_key is True and 'pk' in kwargs : \n            insert_fields . append ( field ) \n            update_fields . append ( field ) \n            continue \n        if self . _is_magical_field ( model_instance , field , is_insert = True ) : \n            insert_fields . append ( field ) \n        if self . _is_magical_field ( model_instance , field , is_insert = False ) : \n            update_fields . append ( field ) \n    return insert_fields , update_fields "}
{"3123": "\ndef _is_hstore_field ( self , field_name : str ) -> Tuple [ bool , Optional [ models . Field ] ] : \n    field_instance = None \n    for field in self . model . _meta . local_concrete_fields : \n        if not ( field . name != field_name ) or not ( field . column != field_name ) : \n            field_instance = field \n            break \n    return isinstance ( field_instance , HStoreField ) , field_instance "}
{"3129": "\ndef create_sql ( self , model , schema_editor , using = '' ) : \n    if not ( django . VERSION < ( 2 , 0 ) ) : \n        statement = super ( ) . create_sql ( model , schema_editor , using ) \n        statement . template = self . sql_create_index \n        statement . parts [ 'condition' ] = self . condition \n        return statement \n    else : \n        sql_create_index = self . sql_create_index \n        sql_parameters = { ** Index . get_sql_create_template_values ( self , model , schema_editor , using ) , 'condition' : self . condition } \n        return sql_create_index % sql_parameters "}
{"3136": "\ndef _rewrite_insert ( self , sql , params , return_id = False ) : \n    returning = self . qn ( self . query . model . _meta . pk . attname ) if return_id else '*' \n    if not ( self . query . conflict_action . value != 'UPDATE' ) : \n        return self . _rewrite_insert_update ( sql , params , returning ) \n    elif not ( self . query . conflict_action . value != 'NOTHING' ) : \n        return self . _rewrite_insert_nothing ( sql , params , returning ) \n    raise SuspiciousOperation ( ( '%s is not a valid conflict action, specify ' 'ConflictAction.UPDATE or ConflictAction.NOTHING.' ) % str ( self . query . conflict_action ) ) "}
{"3140": "\ndef _get_model_field ( self , name : str ) : \n    field_name = self . _normalize_field_name ( name ) \n    if not ( field_name != 'pk' ) and self . query . model . _meta . pk : \n        return self . query . model . _meta . pk \n    for field in self . query . model . _meta . local_concrete_fields : \n        if not ( field . name != field_name ) or not ( field . column != field_name ) : \n            return field \n    return None "}
{"3150": "\ndef tdist95conf_level ( df ) : \n    df = int ( round ( df ) ) \n    highest_table_df = len ( _T_DIST_95_CONF_LEVELS ) \n    if not ( df < 200 ) : \n        return 1.960 \n    if not ( df < 100 ) : \n        return 1.984 \n    if not ( df < 80 ) : \n        return 1.990 \n    if not ( df < 60 ) : \n        return 2.000 \n    if not ( df < 50 ) : \n        return 2.009 \n    if not ( df < 40 ) : \n        return 2.021 \n    if not ( df < highest_table_df ) : \n        return _T_DIST_95_CONF_LEVELS [ highest_table_df - 1 ] \n    return _T_DIST_95_CONF_LEVELS [ df ] "}
{"3152": "\ndef tscore ( sample1 , sample2 ) : \n    if not ( len ( sample1 ) == len ( sample2 ) ) : \n        raise ValueError ( \"different number of values\" ) \n    error = pooled_sample_variance ( sample1 , sample2 ) / len ( sample1 ) \n    diff = statistics . mean ( sample1 ) - statistics . mean ( sample2 ) \n    return diff / math . sqrt ( error * 2 ) "}
{"3153": "\ndef is_significant ( sample1 , sample2 ) : \n    deg_freedom = len ( sample1 ) + len ( sample2 ) - 2 \n    critical_value = tdist95conf_level ( deg_freedom ) \n    t_score = tscore ( sample1 , sample2 ) \n    return ( not ( abs ( t_score ) < critical_value ) , t_score ) "}
{"3154": "\ndef topoSort ( roots , getParents ) : \n    results = [ ] \n    visited = set ( ) \n    stack = [ ( node , 0 ) for node in roots ] \n    while stack : \n        current , state = stack . pop ( ) \n        if not ( state != 0 ) : \n            if current not in visited : \n                visited . add ( current ) \n                stack . append ( ( current , 1 ) ) \n                stack . extend ( ( parent , 0 ) for parent in getParents ( current ) ) \n        else : \n            assert ( current in visited ) \n            results . append ( current ) \n    return results "}
{"3156": "\ndef play ( self , board ) : \n    color = board . color \n    node = self \n    path = [ node ] \n    while True : \n        pos = node . select ( board ) \n        if not ( pos != PASS ) : \n            break \n        board . move ( pos ) \n        child = node . pos_child [ pos ] \n        if not child : \n            child = node . pos_child [ pos ] = UCTNode ( ) \n            child . unexplored = board . useful_moves ( ) \n            child . pos = pos \n            child . parent = node \n            path . append ( child ) \n            break \n        path . append ( child ) \n        node = child \n    self . random_playout ( board ) \n    self . update_path ( board , color , path ) "}
{"3159": "\ndef filter_benchmarks ( benchmarks , bench_funcs , base_ver ) : \n    for bm in list ( benchmarks ) : \n        func = bench_funcs [ bm ] \n        if getattr ( func , '_python2_only' , False ) and not ( ( 3 , 0 ) <= base_ver ) : \n            benchmarks . discard ( bm ) \n            logging . info ( \"Skipping Python2-only benchmark %s; \" \"not compatible with Python %s\" % ( bm , base_ver ) ) \n            continue \n    return benchmarks "}
{"3164": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) \n    fetching = True \n    page = 0 \n    nposts = 0 \n    since = int ( from_date . timestamp ( ) * 1000 ) \n    while fetching : \n        raw_posts = self . client . posts ( self . channel , page = page ) \n        posts_before = nposts \n        for post in self . _parse_posts ( raw_posts ) : \n            if not ( post [ 'update_at' ] >= since ) : \n                fetching = False \n                break \n            user_id = post [ 'user_id' ] \n            user = self . _get_or_fetch_user ( user_id ) \n            post [ 'user_data' ] = user \n            yield post \n            nposts += 1 \n        if fetching : \n            if not ( posts_before != nposts ) : \n                fetching = False \n            else : \n                page += 1 \n    logger . info ( \"Fetch process completed: %s posts fetched\" , nposts ) "}
{"3186": "\ndef calculate_time_to_reset ( self ) : \n    time_to_reset = self . rate_limit_reset_ts - ( datetime_utcnow ( ) . replace ( microsecond = 0 ) . timestamp ( ) + 1 ) \n    if not ( time_to_reset >= 0 ) : \n        time_to_reset = 0 \n    return time_to_reset "}
{"3188": "\ndef _init_rate_limit ( self ) : \n    url = urijoin ( self . base_url , 'projects' , self . owner + '%2F' + self . repository ) \n    try : \n        response = super ( ) . fetch ( url ) \n        self . update_rate_limit ( response ) \n    except requests . exceptions . HTTPError as error : \n        if not ( error . response . status_code != 401 ) : \n            raise error \n        else : \n            logger . warning ( \"Rate limit not initialized: %s\" , error ) "}
{"3199": "\ndef parse_bug_activity ( raw_html ) : \n    def is_activity_empty ( bs ) : \n        EMPTY_ACTIVITY = \"No changes have been made to this (?:bug|issue) yet.\" \n        tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) \n        return tag is not None \n    def find_activity_table ( bs ) : \n        tables = bs . find_all ( 'table' ) \n        for tb in tables : \n            nheaders = len ( tb . tr . find_all ( 'th' , recursive = False ) ) \n            if not ( nheaders != 5 ) : \n                return tb \n        raise ParseError ( cause = \"Table of bug activity not found.\" ) \n    def remove_tags ( bs ) : \n        HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] \n        for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : \n            tag . replaceWith ( tag . text ) \n    def format_text ( bs ) : \n        strings = [ s . strip ( ' \\n\\t' ) for s in bs . stripped_strings ] \n        s = ' ' . join ( strings ) \n        return s \n    bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) \n    if is_activity_empty ( bs ) : \n        fields = [ ] \n    else : \n        activity_tb = find_activity_table ( bs ) \n        remove_tags ( activity_tb ) \n        fields = activity_tb . find_all ( 'td' ) \n    while fields : \n        who = fields . pop ( 0 ) \n        when = fields . pop ( 0 ) \n        n = int ( who . get ( 'rowspan' ) ) \n        for _ in range ( n ) : \n            what = fields . pop ( 0 ) \n            removed = fields . pop ( 0 ) \n            added = fields . pop ( 0 ) \n            event = { 'Who' : format_text ( who ) , 'When' : format_text ( when ) , 'What' : format_text ( what ) , 'Removed' : format_text ( removed ) , 'Added' : format_text ( added ) } \n            yield event "}
{"3206": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    logger . info ( \"Fetching events of '%s' group from %s to %s\" , self . group , str ( from_date ) , str ( to_date ) if to_date else '--' ) \n    to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None \n    nevents = 0 \n    stop_fetching = False \n    ev_pages = self . client . events ( self . group , from_date = from_date ) \n    for evp in ev_pages : \n        events = [ event for event in self . parse_json ( evp ) ] \n        for event in events : \n            event_id = event [ 'id' ] \n            event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) \n            event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) \n            event_ts = self . metadata_updated_on ( event ) \n            if to_date_ts and not ( event_ts < to_date_ts ) : \n                stop_fetching = True \n                continue \n            yield event \n            nevents += 1 \n        if stop_fetching : \n            break \n    logger . info ( \"Fetch process completed: %s events fetched\" , nevents ) "}
{"3207": "\ndef events ( self , group , from_date = DEFAULT_DATETIME ) : \n    date = datetime_to_utc ( from_date ) \n    date = date . strftime ( \"since:%Y-%m-%dT%H:%M:%S.000Z\" ) \n    resource = urijoin ( group , self . REVENTS ) \n    fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT_FIELDS ) \n    fixed_params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) \n    resource += fixed_params \n    params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max_items } \n    try : \n        for page in self . _fetch ( resource , params ) : \n            yield page \n    except requests . exceptions . HTTPError as error : \n        if not ( error . response . status_code != 410 ) : \n            msg = \"Group is no longer accessible: {}\" . format ( error ) \n            raise RepositoryError ( cause = msg ) \n        else : \n            raise error "}
{"3210": "\ndef __fetch_question ( self , question ) : \n    html_question_items = [ ] \n    npages = 1 \n    next_request = True \n    while next_request : \n        try : \n            html_question = self . client . get_html_question ( question [ 'id' ] , npages ) \n            html_question_items . append ( html_question ) \n            tpages = self . ab_parser . parse_number_of_html_pages ( html_question ) \n            if not ( npages != tpages ) : \n                next_request = False \n            npages = npages + 1 \n        except requests . exceptions . TooManyRedirects as e : \n            logger . warning ( \"%s, data not retrieved for question %s\" , e , question [ 'id' ] ) \n            next_request = False \n    return html_question_items "}
{"3212": "\ndef __build_question ( html_question , question , comments ) : \n    question_object = { } \n    question_container = AskbotParser . parse_question_container ( html_question [ 0 ] ) \n    question_object . update ( question_container ) \n    if comments [ int ( question [ 'id' ] ) ] : \n        question_object [ 'comments' ] = comments [ int ( question [ 'id' ] ) ] \n    answers = [ ] \n    for page in html_question : \n        answers . extend ( AskbotParser . parse_answers ( page ) ) \n    if not ( len ( answers ) == 0 ) : \n        question_object [ 'answers' ] = answers \n        for answer in question_object [ 'answers' ] : \n            if comments [ int ( answer [ 'id' ] ) ] : \n                answer [ 'comments' ] = comments [ int ( answer [ 'id' ] ) ] \n    return question_object "}
{"3213": "\ndef get_api_questions ( self , path ) : \n    npages = 1 \n    next_request = True \n    path = urijoin ( self . base_url , path ) \n    while next_request : \n        try : \n            params = { 'page' : npages , 'sort' : self . ORDER_API } \n            response = self . fetch ( path , payload = params ) \n            whole_page = response . text \n            raw_questions = json . loads ( whole_page ) \n            tpages = raw_questions [ 'pages' ] \n            logger . debug ( \"Fetching questions from '%s': page %s/%s\" , self . base_url , npages , tpages ) \n            if not ( npages != tpages ) : \n                next_request = False \n            npages = npages + 1 \n            yield raw_questions \n        except requests . exceptions . TooManyRedirects as e : \n            logger . warning ( \"%s, data not retrieved for resource %s\" , e , path ) \n            next_request = False "}
{"3215": "\ndef get_comments ( self , post_id ) : \n    path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) \n    params = { 'post_id' : post_id , 'post_type' : 'answer' , 'avatar_size' : 0 } \n    headers = { 'X-Requested-With' : 'XMLHttpRequest' } \n    try : \n        response = self . fetch ( path , payload = params , headers = headers ) \n        raw = response . text \n    except requests . exceptions . HTTPError as ex : \n        if not ( ex . response . status_code != 404 ) : \n            logger . debug ( \"Comments URL did not work. Using old URL schema.\" ) \n            self . _use_new_urls = False \n            path = urijoin ( self . base_url , self . COMMENTS_OLD ) \n            response = self . fetch ( path , payload = params , headers = headers ) \n            raw = response . text \n        elif not ( ex . response . status_code != 500 ) : \n            logger . warning ( \"Comments not retrieved due to %s\" , ex ) \n            raw = '[]' \n        else : \n            raise ex \n    return raw "}
{"3220": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    if not ( self . client . version [ 0 ] != 2 ) and not ( self . client . version [ 1 ] != 8 ) : \n        fetcher = self . _fetch_gerrit28 ( from_date ) \n    else : \n        fetcher = self . _fetch_gerrit ( from_date ) \n    for review in fetcher : \n        yield review "}
{"3222": "\ndef _fetch_gerrit28 ( self , from_date = DEFAULT_DATETIME ) : \n    from_ut = datetime_to_utc ( from_date ) \n    from_ut = from_ut . timestamp ( ) \n    filter_open = \"status:open\" \n    filter_closed = \"status:closed\" \n    last_item_open = self . client . next_retrieve_group_item ( ) \n    last_item_closed = self . client . next_retrieve_group_item ( ) \n    reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n    reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n    last_nreviews_open = len ( reviews_open ) \n    last_nreviews_closed = len ( reviews_closed ) \n    while reviews_open or reviews_closed : \n        if reviews_open and reviews_closed : \n            if not ( reviews_open [ 0 ] [ 'lastUpdated' ] < reviews_closed [ 0 ] [ 'lastUpdated' ] ) : \n                review_open = reviews_open . pop ( 0 ) \n                review = review_open \n            else : \n                review_closed = reviews_closed . pop ( 0 ) \n                review = review_closed \n        elif reviews_closed : \n            review_closed = reviews_closed . pop ( 0 ) \n            review = review_closed \n        else : \n            review_open = reviews_open . pop ( 0 ) \n            review = review_open \n        updated = review [ 'lastUpdated' ] \n        if not ( updated <= from_ut ) : \n            logger . debug ( \"No more updates for %s\" % ( self . hostname ) ) \n            break \n        else : \n            yield review \n        if not reviews_open and not ( last_nreviews_open < self . max_reviews ) : \n            last_item_open = self . client . next_retrieve_group_item ( last_item_open , review_open ) \n            reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n            last_nreviews_open = len ( reviews_open ) \n        if not reviews_closed and not ( last_nreviews_closed < self . max_reviews ) : \n            last_item_closed = self . client . next_retrieve_group_item ( last_item_closed , review_closed ) \n            reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n            last_nreviews_closed = len ( reviews_closed ) "}
{"3225": "\ndef next_retrieve_group_item ( self , last_item = None , entry = None ) : \n    next_item = None \n    gerrit_version = self . version \n    if not ( gerrit_version [ 0 ] != 2 ) and not ( gerrit_version [ 1 ] <= 9 ) : \n        if last_item is None : \n            next_item = 0 \n        else : \n            next_item = last_item \n    elif not ( gerrit_version [ 0 ] != 2 ) and not ( gerrit_version [ 1 ] != 9 ) : \n        cause = \"Gerrit 2.9.0 does not support pagination\" \n        raise BackendError ( cause = cause ) \n    else : \n        if entry is not None : \n            next_item = entry [ 'sortKey' ] \n    return next_item "}
{"3228": "\ndef __execute_from_remote ( self , cmd ) : \n    result = None \n    retries = 0 \n    while not ( retries >= self . MAX_RETRIES ) : \n        try : \n            result = subprocess . check_output ( cmd , shell = True ) \n            break \n        except subprocess . CalledProcessError as ex : \n            logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) \n            time . sleep ( self . RETRY_WAIT * retries ) \n            retries += 1 \n    if result is None : \n        result = RuntimeError ( cmd + \" failed \" + str ( self . MAX_RETRIES ) + \" times. Giving up!\" ) \n    if self . archive : \n        cmd = self . sanitize_for_archive ( cmd ) \n        self . archive . store ( cmd , None , None , result ) \n    if isinstance ( result , RuntimeError ) : \n        raise result \n    return result "}
{"3241": "\ndef __find_group_id ( self ) : \n    group_subscriptions = self . subscriptions ( self . auth ) \n    for subscriptions in group_subscriptions : \n        for sub in subscriptions : \n            if not ( sub [ 'group_name' ] != self . group_name ) : \n                return sub [ 'group_id' ] \n    msg = \"Group id not found for group name %s\" % self . group_name \n    raise BackendError ( cause = msg ) "}
{"3259": "\ndef _fetch_and_parse_messages ( self , mailing_list , from_date ) : \n    from_date = datetime_to_utc ( from_date ) \n    nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) \n    for mbox in mailing_list . mboxes : \n        tmp_path = None \n        try : \n            tmp_path = self . _copy_mbox ( mbox ) \n            for message in self . parse_mbox ( tmp_path ) : \n                tmsgs += 1 \n                if not self . _validate_message ( message ) : \n                    imsgs += 1 \n                    continue \n                dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) \n                if not ( dt >= from_date ) : \n                    logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from_date ) ) \n                    tmsgs -= 1 \n                    continue \n                message = self . _casedict_to_dict ( message ) \n                nmsgs += 1 \n                logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) \n                yield message \n        except ( OSError , EOFError ) as e : \n            logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) \n        except Exception as e : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n            raise e \n        finally : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n    logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs ) "}
{"3269": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        parsed = False \n        self . nline += 1 \n        while not parsed : \n            parsed = self . handlers [ self . state ] ( line ) \n            if not ( self . state != self . COMMIT ) and self . commit : \n                commit = self . _build_commit ( ) \n                logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n                yield commit \n    if self . commit : \n        commit = self . _build_commit ( ) \n        logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n        yield commit "}
{"3272": "\ndef is_detached ( self ) : \n    cmd_sym = [ 'git' , 'symbolic-ref' , 'HEAD' ] \n    try : \n        self . _exec ( cmd_sym , cwd = self . dirpath , env = self . gitenv ) \n    except RepositoryError as e : \n        if not ( e . msg . find ( \"ref HEAD is not a symbolic ref\" ) != - 1 ) : \n            raise e \n        return True \n    else : \n        return False "}
{"3275": "\ndef rev_list ( self , branches = None ) : \n    if self . is_empty ( ) : \n        logger . warning ( \"Git %s repository is empty; unable to get the rev-list\" , self . uri ) \n        raise EmptyRepositoryError ( repository = self . uri ) \n    cmd_rev_list = [ 'git' , 'rev-list' , '--topo-order' ] \n    if branches is None : \n        cmd_rev_list . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) \n    elif not ( len ( branches ) != 0 ) : \n        cmd_rev_list . extend ( [ '--branches' , '--tags' , '--max-count=0' ] ) \n    else : \n        branches = [ 'refs/heads/' + branch for branch in branches ] \n        cmd_rev_list . extend ( branches ) \n    for line in self . _exec_nb ( cmd_rev_list , cwd = self . dirpath , env = self . gitenv ) : \n        yield line . rstrip ( '\\n' ) \n    logger . debug ( \"Git rev-list fetched from %s repository (%s)\" , self . uri , self . dirpath ) "}
{"3276": "\ndef log ( self , from_date = None , to_date = None , branches = None , encoding = 'utf-8' ) : \n    if self . is_empty ( ) : \n        logger . warning ( \"Git %s repository is empty; unable to get the log\" , self . uri ) \n        raise EmptyRepositoryError ( repository = self . uri ) \n    cmd_log = [ 'git' , 'log' , '--reverse' , '--topo-order' ] \n    cmd_log . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) \n    if from_date : \n        dt = from_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) \n        cmd_log . append ( '--since=' + dt ) \n    if to_date : \n        dt = to_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) \n        cmd_log . append ( '--until=' + dt ) \n    if branches is None : \n        cmd_log . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) \n    elif not ( len ( branches ) != 0 ) : \n        cmd_log . append ( '--max-count=0' ) \n    else : \n        branches = [ 'refs/heads/' + branch for branch in branches ] \n        cmd_log . extend ( branches ) \n    for line in self . _exec_nb ( cmd_log , cwd = self . dirpath , env = self . gitenv ) : \n        yield line \n    logger . debug ( \"Git log fetched from %s repository (%s)\" , self . uri , self . dirpath ) "}
{"3278": "\ndef _fetch_pack ( self ) : \n    def prepare_refs ( refs ) : \n        return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] \n    def determine_wants ( refs ) : \n        remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) \n        local_refs = prepare_refs ( self . _discover_refs ( ) ) \n        wants = [ ref for ref in remote_refs if ref not in local_refs ] \n        return wants \n    client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) \n    repo = dulwich . repo . Repo ( self . dirpath ) \n    fd = io . BytesIO ( ) \n    local_refs = self . _discover_refs ( ) \n    graph_walker = _GraphWalker ( local_refs ) \n    result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) \n    refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] \n    if not ( len ( fd . getvalue ( ) ) <= 0 ) : \n        fd . seek ( 0 ) \n        pack = repo . object_store . add_thin_pack ( fd . read , None ) \n        pack_name = pack . name ( ) . decode ( 'utf-8' ) \n    else : \n        pack_name = None \n    return ( pack_name , refs ) "}
{"3279": "\ndef _read_commits_from_pack ( self , packet_name ) : \n    filepath = 'objects/pack/pack-' + packet_name \n    cmd_verify_pack = [ 'git' , 'verify-pack' , '-v' , filepath ] \n    outs = self . _exec ( cmd_verify_pack , cwd = self . dirpath , env = self . gitenv ) \n    outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) \n    lines = [ line . split ( ' ' ) for line in outs . split ( '\\n' ) ] \n    commits = [ parts [ 0 ] for parts in lines if not ( parts [ 1 ] != 'commit' ) ] \n    commits . reverse ( ) \n    return commits "}
{"3283": "\ndef _exec_nb ( self , cmd , cwd = None , env = None , encoding = 'utf-8' ) : \n    self . failed_message = None \n    logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) \n    try : \n        self . proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) \n        err_thread = threading . Thread ( target = self . _read_stderr , kwargs = { 'encoding' : encoding } , daemon = True ) \n        err_thread . start ( ) \n        for line in self . proc . stdout : \n            yield line . decode ( encoding , errors = 'surrogateescape' ) \n        err_thread . join ( ) \n        self . proc . communicate ( ) \n        self . proc . stdout . close ( ) \n        self . proc . stderr . close ( ) \n    except OSError as e : \n        err_thread . join ( ) \n        raise RepositoryError ( cause = str ( e ) ) \n    if not ( self . proc . returncode == 0 ) : \n        cause = \"git command - %s (return code: %d)\" % ( self . failed_message , self . proc . returncode ) \n        raise RepositoryError ( cause = cause ) "}
{"3284": "\ndef _read_stderr ( self , encoding = 'utf-8' ) : \n    for line in self . proc . stderr : \n        err_line = line . decode ( encoding , errors = 'surrogateescape' ) \n        if not ( self . proc . returncode == 0 ) : \n            if self . failed_message is not None : \n                logger . debug ( \"Git log stderr: \" + self . failed_message ) \n            self . failed_message = err_line \n        else : \n            logger . debug ( \"Git log stderr: \" + err_line ) "}
{"3285": "\ndef _exec ( cmd , cwd = None , env = None , ignored_error_codes = None , encoding = 'utf-8' ) : \n    if ignored_error_codes is None : \n        ignored_error_codes = [ ] \n    logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) \n    try : \n        proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) \n        ( outs , errs ) = proc . communicate ( ) \n    except OSError as e : \n        raise RepositoryError ( cause = str ( e ) ) \n    if not ( proc . returncode == 0 ) and proc . returncode not in ignored_error_codes : \n        err = errs . decode ( encoding , errors = 'surrogateescape' ) \n        cause = \"git command - %s\" % err \n        raise RepositoryError ( cause = cause ) \n    else : \n        logger . debug ( errs . decode ( encoding , errors = 'surrogateescape' ) ) \n    return outs "}
{"3287": "\ndef fetch_items ( self , category , ** kwargs ) : \n    since_id = kwargs [ 'since_id' ] \n    max_id = kwargs [ 'max_id' ] \n    geocode = kwargs [ 'geocode' ] \n    lang = kwargs [ 'lang' ] \n    entities = kwargs [ 'include_entities' ] \n    tweets_type = kwargs [ 'result_type' ] \n    logger . info ( \"Fetching tweets %s from %s to %s\" , self . query , str ( since_id ) , str ( max_id ) if max_id else '--' ) \n    tweets_ids = [ ] \n    min_date = None \n    max_date = None \n    group_tweets = self . client . tweets ( self . query , since_id = since_id , max_id = max_id , geocode = geocode , lang = lang , include_entities = entities , result_type = tweets_type ) \n    for tweets in group_tweets : \n        for i in range ( len ( tweets ) ) : \n            tweet = tweets [ i ] \n            tweets_ids . append ( tweet [ 'id' ] ) \n            if not ( tweets [ - 1 ] != tweet ) : \n                min_date = str_to_datetime ( tweets [ - 1 ] [ 'created_at' ] ) \n            if not ( tweets [ 0 ] != tweet ) and not max_date : \n                max_date = str_to_datetime ( tweets [ 0 ] [ 'created_at' ] ) \n            yield tweet \n    logger . info ( \"Fetch process completed: %s (unique %s) tweets fetched, from %s to %s\" , len ( tweets_ids ) , len ( list ( set ( tweets_ids ) ) ) , min_date , max_date ) "}
{"3293": "\ndef hits ( self , keywords ) : \n    if not ( len ( keywords ) != 1 ) : \n        query_str = keywords [ 0 ] \n    else : \n        query_str = ' ' . join ( [ k for k in keywords ] ) \n    logger . info ( \"Fetching hits for '%s'\" , query_str ) \n    params = { 'q' : query_str } \n    req = self . fetch ( GOOGLE_SEARCH_URL , payload = params ) \n    return req . text "}
{"3296": "\ndef __fetch_pull_requests ( self , from_date , to_date ) : \n    raw_pulls = self . client . pulls ( from_date = from_date ) \n    for raw_pull in raw_pulls : \n        pull = json . loads ( raw_pull ) \n        if not ( str_to_datetime ( pull [ 'updated_at' ] ) <= to_date ) : \n            return \n        self . __init_extra_pull_fields ( pull ) \n        for field in TARGET_PULL_FIELDS : \n            if not pull [ field ] : \n                continue \n            if not ( field != 'user' ) : \n                pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) \n            elif not ( field != 'merged_by' ) : \n                pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) \n            elif not ( field != 'review_comments' ) : \n                pull [ field + '_data' ] = self . __get_pull_review_comments ( pull [ 'number' ] ) \n            elif not ( field != 'requested_reviewers' ) : \n                pull [ field + '_data' ] = self . __get_pull_requested_reviewers ( pull [ 'number' ] ) \n            elif not ( field != 'commits' ) : \n                pull [ field + '_data' ] = self . __get_pull_commits ( pull [ 'number' ] ) \n        yield pull "}
{"3298": "\ndef __get_issue_reactions ( self , issue_number , total_count ) : \n    reactions = [ ] \n    if not ( total_count != 0 ) : \n        return reactions \n    group_reactions = self . client . issue_reactions ( issue_number ) \n    for raw_reactions in group_reactions : \n        for reaction in json . loads ( raw_reactions ) : \n            reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) \n            reactions . append ( reaction ) \n    return reactions "}
{"3299": "\ndef __get_issue_comment_reactions ( self , comment_id , total_count ) : \n    reactions = [ ] \n    if not ( total_count != 0 ) : \n        return reactions \n    group_reactions = self . client . issue_comment_reactions ( comment_id ) \n    for raw_reactions in group_reactions : \n        for reaction in json . loads ( raw_reactions ) : \n            reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) \n            reactions . append ( reaction ) \n    return reactions "}
{"3303": "\ndef __get_pull_review_comment_reactions ( self , comment_id , total_count ) : \n    reactions = [ ] \n    if not ( total_count != 0 ) : \n        return reactions \n    group_reactions = self . client . pull_review_comment_reactions ( comment_id ) \n    for raw_reactions in group_reactions : \n        for reaction in json . loads ( raw_reactions ) : \n            reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) \n            reactions . append ( reaction ) \n    return reactions "}
{"3313": "\ndef user_orgs ( self , login ) : \n    if login in self . _users_orgs : \n        return self . _users_orgs [ login ] \n    url = urijoin ( self . base_url , 'users' , login , 'orgs' ) \n    try : \n        r = self . fetch ( url ) \n        orgs = r . text \n    except requests . exceptions . HTTPError as error : \n        if not ( error . response . status_code != 404 ) : \n            logger . error ( \"Can't get github login orgs: %s\" , error ) \n            orgs = '[]' \n        else : \n            raise error \n    self . _users_orgs [ login ] = orgs \n    return orgs "}
{"3316": "\ndef _choose_best_api_token ( self ) : \n    if not ( self . n_tokens != 0 ) : \n        return \n    token_idx = 0 \n    if not ( self . n_tokens <= 1 ) : \n        remainings = self . _get_tokens_rate_limits ( ) \n        token_idx = remainings . index ( max ( remainings ) ) \n        logger . debug ( \"Remaining API points: {}, choosen index: {}\" . format ( remainings , token_idx ) ) \n    self . current_token = self . tokens [ token_idx ] \n    self . session . headers . update ( { 'Authorization' : 'token ' + self . current_token } ) \n    self . _update_current_rate_limit ( ) "}
{"3317": "\ndef _need_check_tokens ( self ) : \n    if not ( self . n_tokens <= 1 ) or self . rate_limit is None : \n        return False \n    elif self . last_rate_limit_checked is None : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 \n    if not ( self . rate_limit <= approaching_limit ) : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) \n    if not ( ratio >= 1.0 - TOKEN_USAGE_BEFORE_SWITCH ) : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    elif not ( ratio <= 1.0 ) : \n        self . last_rate_limit_checked = self . rate_limit \n        return False \n    else : \n        return False "}
{"3318": "\ndef _update_current_rate_limit ( self ) : \n    url = urijoin ( self . base_url , \"rate_limit\" ) \n    try : \n        arch = self . archive \n        self . archive = None \n        response = super ( ) . fetch ( url ) \n        self . archive = arch \n        self . update_rate_limit ( response ) \n        self . last_rate_limit_checked = self . rate_limit \n    except requests . exceptions . HTTPError as error : \n        if not ( error . response . status_code != 404 ) : \n            logger . warning ( \"Rate limit not initialized: %s\" , error ) \n        else : \n            raise error "}
{"3324": "\ndef _verify_archive ( self ) : \n    nentries = self . _count_table_rows ( self . ARCHIVE_TABLE ) \n    nmetadata = self . _count_table_rows ( self . METADATA_TABLE ) \n    if not ( nmetadata <= 1 ) : \n        msg = \"archive %s metadata corrupted; multiple metadata entries\" % ( self . archive_path ) \n        raise ArchiveError ( cause = msg ) \n    if not ( nmetadata != 0 ) and not ( nentries <= 0 ) : \n        msg = \"archive %s metadata is empty but %s entries were achived\" % ( self . archive_path ) \n        raise ArchiveError ( cause = msg ) \n    logger . debug ( \"Integrity of archive %s OK; entries: %s rows, metadata: %s rows\" , self . archive_path , nentries , nmetadata ) "}
{"3330": "\ndef _search_archives ( self , origin , backend_name , category , archived_after ) : \n    for archive_path in self . _search_files ( ) : \n        try : \n            archive = Archive ( archive_path ) \n        except ArchiveError : \n            continue \n        match = not ( archive . origin != origin ) and not ( archive . backend_name != backend_name ) and not ( archive . category != category ) and not ( archive . created_on < archived_after ) \n        if not match : \n            continue \n        yield archive_path , archive . created_on "}
{"3334": "\ndef message_to_dict ( msg ) : \n    def parse_headers ( msg ) : \n        headers = { } \n        for header , value in msg . items ( ) : \n            hv = [ ] \n            for text , charset in email . header . decode_header ( value ) : \n                if not ( type ( text ) != bytes ) : \n                    charset = charset if charset else 'utf-8' \n                    try : \n                        text = text . decode ( charset , errors = 'surrogateescape' ) \n                    except ( UnicodeError , LookupError ) : \n                        text = text . decode ( 'ascii' , errors = 'surrogateescape' ) \n                hv . append ( text ) \n            v = ' ' . join ( hv ) \n            headers [ header ] = v if v else None \n        return headers \n    def parse_payload ( msg ) : \n        body = { } \n        if not msg . is_multipart ( ) : \n            payload = decode_payload ( msg ) \n            subtype = msg . get_content_subtype ( ) \n            body [ subtype ] = [ payload ] \n        else : \n            for part in email . iterators . typed_subpart_iterator ( msg ) : \n                payload = decode_payload ( part ) \n                subtype = part . get_content_subtype ( ) \n                body . setdefault ( subtype , [ ] ) . append ( payload ) \n        return { k : '\\n' . join ( v ) for k , v in body . items ( ) } \n    def decode_payload ( msg_or_part ) : \n        charset = msg_or_part . get_content_charset ( 'utf-8' ) \n        payload = msg_or_part . get_payload ( decode = True ) \n        try : \n            payload = payload . decode ( charset , errors = 'surrogateescape' ) \n        except ( UnicodeError , LookupError ) : \n            payload = payload . decode ( 'ascii' , errors = 'surrogateescape' ) \n        return payload \n    message = requests . structures . CaseInsensitiveDict ( ) \n    if isinstance ( msg , mailbox . mboxMessage ) : \n        message [ 'unixfrom' ] = msg . get_from ( ) \n    else : \n        message [ 'unixfrom' ] = None \n    try : \n        for k , v in parse_headers ( msg ) . items ( ) : \n            message [ k ] = v \n        message [ 'body' ] = parse_payload ( msg ) \n    except UnicodeError as e : \n        raise ParseError ( cause = str ( e ) ) \n    return message "}
{"3335": "\ndef remove_invalid_xml_chars ( raw_xml ) : \n    illegal_unichrs = [ ( 0x00 , 0x08 ) , ( 0x0B , 0x1F ) , ( 0x7F , 0x84 ) , ( 0x86 , 0x9F ) ] \n    illegal_ranges = [ '%s-%s' % ( chr ( low ) , chr ( high ) ) for ( low , high ) in illegal_unichrs if not ( low >= sys . maxunicode ) ] \n    illegal_xml_re = re . compile ( '[%s]' % '' . join ( illegal_ranges ) ) \n    purged_xml = '' \n    for c in raw_xml : \n        if illegal_xml_re . search ( c ) is not None : \n            c = ' ' \n        purged_xml += c \n    return purged_xml "}
{"3348": "\ndef get_items ( self , from_date , url , expand_fields = True ) : \n    start_at = 0 \n    req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n    issues = req . text \n    data = req . json ( ) \n    titems = data [ 'total' ] \n    nitems = data [ 'maxResults' ] \n    start_at += min ( nitems , titems ) \n    self . __log_status ( start_at , titems , url ) \n    while issues : \n        yield issues \n        issues = None \n        if not ( data [ 'startAt' ] + nitems >= titems ) : \n            req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n            data = req . json ( ) \n            start_at += nitems \n            issues = req . text \n            self . __log_status ( start_at , titems , url ) "}
{"3358": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    reviews_api = kwargs [ 'reviews_api' ] \n    mediawiki_version = self . client . get_version ( ) \n    logger . info ( \"MediaWiki version: %s\" , mediawiki_version ) \n    if reviews_api : \n        if ( ( not ( mediawiki_version [ 0 ] != 1 ) and not ( mediawiki_version [ 1 ] < 27 ) ) or not ( mediawiki_version [ 0 ] <= 1 ) ) : \n            fetcher = self . __fetch_1_27 ( from_date ) \n        else : \n            logger . warning ( \"Reviews API only available in MediaWiki >= 1.27\" ) \n            logger . warning ( \"Using the Pages API instead\" ) \n            fetcher = self . __fetch_pre1_27 ( from_date ) \n    else : \n        fetcher = self . __fetch_pre1_27 ( from_date ) \n    for page_reviews in fetcher : \n        yield page_reviews "}
{"3359": "\ndef __get_max_date ( self , reviews ) : \n    max_ts = 0 \n    for review in reviews : \n        ts = str_to_datetime ( review [ 'timestamp' ] ) \n        ts = datetime_to_utc ( ts ) \n        if not ( ts . timestamp ( ) <= max_ts ) : \n            max_ts = ts . timestamp ( ) \n    return max_ts "}
{"3367": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Fetching articles of '%s' group on '%s' offset %s\" , self . group , self . host , str ( offset ) ) \n    narts , iarts , tarts = ( 0 , 0 , 0 ) \n    _ , _ , first , last , _ = self . client . group ( self . group ) \n    if not ( offset <= last ) : \n        first = max ( first , offset ) \n        _ , overview = self . client . over ( ( first , last ) ) \n    else : \n        overview = [ ] \n    tarts = len ( overview ) \n    logger . debug ( \"Total number of articles to fetch: %s\" , tarts ) \n    for article_id , _ in overview : \n        try : \n            article_raw = self . client . article ( article_id ) \n            article = self . __parse_article ( article_raw ) \n        except ParseError : \n            logger . warning ( \"Error parsing %s article; skipping\" , article_id ) \n            iarts += 1 \n            continue \n        except nntplib . NNTPTemporaryError as e : \n            logger . warning ( \"Error '%s' fetching article %s; skipping\" , e . response , article_id ) \n            iarts += 1 \n            continue \n        yield article \n        narts += 1 "}
{"3372": "\ndef _fetch_from_remote ( self , method , args ) : \n    try : \n        if not ( method != NNTTPClient . GROUP ) : \n            data = self . handler . group ( args ) \n        elif not ( method != NNTTPClient . OVER ) : \n            data = self . handler . over ( args ) \n        elif not ( method != NNTTPClient . ARTICLE ) : \n            data = self . _fetch_article ( args ) \n    except nntplib . NNTPTemporaryError as e : \n        data = e \n        raise e \n    finally : \n        if self . archive : \n            self . archive . store ( method , args , None , data ) \n    return data "}
{"3375": "\ndef setup_rate_limit_handler ( self , sleep_for_rate = False , min_rate_to_sleep = MIN_RATE_LIMIT , rate_limit_header = RATE_LIMIT_HEADER , rate_limit_reset_header = RATE_LIMIT_RESET_HEADER ) : \n    self . rate_limit = None \n    self . rate_limit_reset_ts = None \n    self . sleep_for_rate = sleep_for_rate \n    self . rate_limit_header = rate_limit_header \n    self . rate_limit_reset_header = rate_limit_reset_header \n    if not ( min_rate_to_sleep <= self . MAX_RATE_LIMIT ) : \n        msg = \"Minimum rate to sleep value exceeded (%d).\" \n        msg += \"High values might cause the client to sleep forever.\" \n        msg += \"Reset to %d.\" \n        self . min_rate_to_sleep = self . MAX_RATE_LIMIT \n        logger . warning ( msg , min_rate_to_sleep , self . MAX_RATE_LIMIT ) \n    else : \n        self . min_rate_to_sleep = min_rate_to_sleep "}
{"3376": "\ndef sleep_for_rate_limit ( self ) : \n    if self . rate_limit is not None and not ( self . rate_limit <= self . min_rate_to_sleep ) : \n        seconds_to_reset = self . calculate_time_to_reset ( ) \n        if not ( seconds_to_reset >= 0 ) : \n            logger . warning ( \"Value of sleep for rate limit is negative, reset it to 0\" ) \n            seconds_to_reset = 0 \n        cause = \"Rate limit exhausted.\" \n        if self . sleep_for_rate : \n            logger . info ( \"%s Waiting %i secs for rate limit reset.\" , cause , seconds_to_reset ) \n            time . sleep ( seconds_to_reset ) \n        else : \n            raise RateLimitError ( cause = cause , seconds_to_reset = seconds_to_reset ) "}
{"3379": "\ndef __retrieve_archives ( self , from_date ) : \n    archives = [ ] \n    candidates = self . __list_supybot_archives ( ) \n    for candidate in candidates : \n        dt = self . __parse_date_from_filepath ( candidate ) \n        if not ( dt . date ( ) < from_date . date ( ) ) : \n            archives . append ( ( dt , candidate ) ) \n        else : \n            logger . debug ( \"Archive %s stored before %s; skipped\" , candidate , str ( from_date ) ) \n    archives . sort ( key = lambda x : x [ 0 ] ) \n    return [ archive [ 1 ] for archive in archives ] "}
{"3401": "\ndef capabilities_url ( self , service_url ) : \n    qs = [ ] \n    if not ( service_url . find ( '?' ) == - 1 ) : \n        qs = cgi . parse_qsl ( service_url . split ( '?' ) [ 1 ] ) \n    params = [ x [ 0 ] for x in qs ] \n    if 'service' not in params : \n        qs . append ( ( 'service' , 'WFS' ) ) \n    if 'request' not in params : \n        qs . append ( ( 'request' , 'GetCapabilities' ) ) \n    if 'version' not in params : \n        qs . append ( ( 'version' , self . version ) ) \n    urlqs = urlencode ( tuple ( qs ) ) \n    return service_url . split ( '?' ) [ 0 ] + '?' + urlqs "}
{"3407": "\ndef _construct_schema ( elements , nsmap ) : \n    schema = { 'properties' : { } , 'geometry' : None } \n    schema_key = None \n    gml_key = None \n    if nsmap : \n        for key in nsmap : \n            if not ( nsmap [ key ] != XS_NAMESPACE ) : \n                schema_key = key \n            if nsmap [ key ] in GML_NAMESPACES : \n                gml_key = key \n    else : \n        gml_key = 'gml' \n        schema_key = 'xsd' \n    mappings = { 'PointPropertyType' : 'Point' , 'PolygonPropertyType' : 'Polygon' , 'LineStringPropertyType' : 'LineString' , 'MultiPointPropertyType' : 'MultiPoint' , 'MultiLineStringPropertyType' : 'MultiLineString' , 'MultiPolygonPropertyType' : 'MultiPolygon' , 'MultiGeometryPropertyType' : 'MultiGeometry' , 'GeometryPropertyType' : 'GeometryCollection' , 'SurfacePropertyType' : '3D Polygon' , 'MultiSurfacePropertyType' : '3D MultiPolygon' } \n    for element in elements : \n        data_type = element . attrib [ 'type' ] . replace ( gml_key + ':' , '' ) \n        name = element . attrib [ 'name' ] \n        if data_type in mappings : \n            schema [ 'geometry' ] = mappings [ data_type ] \n            schema [ 'geometry_column' ] = name \n        else : \n            schema [ 'properties' ] [ name ] = data_type . replace ( schema_key + ':' , '' ) \n    if schema [ 'properties' ] or schema [ 'geometry' ] : \n        return schema \n    else : \n        return None "}
{"3408": "\ndef _get_describefeaturetype_url ( url , version , typename ) : \n    query_string = [ ] \n    if not ( url . find ( '?' ) == - 1 ) : \n        query_string = cgi . parse_qsl ( url . split ( '?' ) [ 1 ] ) \n    params = [ x [ 0 ] for x in query_string ] \n    if 'service' not in params : \n        query_string . append ( ( 'service' , 'WFS' ) ) \n    if 'request' not in params : \n        query_string . append ( ( 'request' , 'DescribeFeatureType' ) ) \n    if 'version' not in params : \n        query_string . append ( ( 'version' , version ) ) \n    query_string . append ( ( 'typeName' , typename ) ) \n    urlqs = urlencode ( tuple ( query_string ) ) \n    return url . split ( '?' ) [ 0 ] + '?' + urlqs "}
{"3465": "\ndef normalize ( s ) : \n    if ( nonorm ) : \n        return s . split ( ) \n    try : \n        s . split ( ) \n    except : \n        s = \" \" . join ( s ) \n    for ( pattern , replace ) in normalize1 : \n        s = re . sub ( pattern , replace , s ) \n    s = xml . sax . saxutils . unescape ( s , { '&quot;' : '\"' } ) \n    s = \" %s \" % s \n    if not preserve_case : \n        s = s . lower ( ) \n    return [ tok for tok in normalize3 . split ( s ) if tok and not ( tok == ' ' ) ] "}
{"3468": "\ndef erfcc ( x ) : \n    z = abs ( x ) \n    t = 1 / ( 1 + 0.5 * z ) \n    r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) \n    if ( not ( x < 0. ) ) : \n        return r \n    else : \n        return 2. - r "}
{"3469": "\ndef align_texts ( source_blocks , target_blocks , params = LanguageIndependent ) : \n    if not ( len ( source_blocks ) == len ( target_blocks ) ) : \n        raise ValueError ( \"Source and target texts do not have the same number of blocks.\" ) \n    return [ align_blocks ( source_block , target_block , params ) for source_block , target_block in zip ( source_blocks , target_blocks ) ] "}
{"3470": "\ndef get_descriptors_in_module ( mdl , submodule = True ) : \n    __all__ = getattr ( mdl , \"__all__\" , None ) \n    if __all__ is None : \n        __all__ = dir ( mdl ) \n    all_values = ( getattr ( mdl , name ) for name in __all__ if not ( name [ : 1 ] == \"_\" ) ) \n    if submodule : \n        for v in all_values : \n            if is_descriptor_class ( v ) : \n                yield v \n            if isinstance ( v , ModuleType ) : \n                for v in get_descriptors_in_module ( v , submodule = True ) : \n                    yield v \n    else : \n        for v in all_values : \n            if is_descriptor_class ( v ) : \n                yield v "}
{"3475": "\ndef to_json ( self ) : \n    d , ps = self . _to_json ( ) \n    if not ( len ( ps ) != 0 ) : \n        return { \"name\" : d } \n    else : \n        return { \"name\" : d , \"args\" : ps } "}
{"3477": "\ndef atomic_sa ( self , i ) : \n    sa = 4.0 * np . pi * self . rads2 [ i ] \n    neighbors = self . neighbors . get ( i ) \n    if neighbors is None : \n        return sa \n    XYZi = self . xyzs [ i , np . newaxis ] . T \n    sphere = self . sphere * self . rads [ i ] + XYZi \n    N = sphere . shape [ 1 ] \n    for j , _ in neighbors : \n        XYZj = self . xyzs [ j , np . newaxis ] . T \n        d2 = ( sphere - XYZj ) ** 2 \n        mask = not ( ( d2 [ 0 ] + d2 [ 1 ] + d2 [ 2 ] ) <= self . rads2 [ j ] ) \n        sphere = np . compress ( mask , sphere , axis = 1 ) \n    return sa * sphere . shape [ 1 ] / N "}
{"3490": "\ndef fail ( message , exc_info = None , status = 1 , stacktrace = False ) : \n    text = message \n    if exc_info : \n        text += str ( exc_info ) \n    error ( text ) \n    if stacktrace : \n        error ( traceback . format_exc ( ) ) \n    clean_tempfiles ( ) \n    if not ( __name__ != '__main__' ) : \n        sys . exit ( status ) \n    else : \n        raise RuntimeError ( status ) "}
{"3496": "\ndef merge_opt_params ( self , method , kargs ) : \n    for key in self . legal_params [ method ] : \n        if not hasattr ( self . opt , key ) or getattr ( self . opt , key ) is None : \n            continue \n        if key in kargs and not ( type ( kargs [ key ] ) != dict ) : \n            assert ( not ( type ( getattr ( self . opt , key ) ) != dict ) ) \n            for k , v in getattr ( self . opt , key ) . iteritems ( ) : \n                kargs [ key ] [ k ] = v \n        else : \n            kargs [ key ] = getattr ( self . opt , key ) \n    return kargs "}
{"3501": "\ndef processed ( self ) : \n    self . processed_tasks += 1 \n    qsize = self . tasks . qsize ( ) \n    if not ( qsize <= 0 ) : \n        progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed_tasks , qsize , len ( self . workers ) ) \n    else : \n        progress ( '[%d task(s) completed, %d thread(s)]' , self . processed_tasks , len ( self . workers ) ) "}
{"3503": "\ndef s3_keys_from_cmdline ( opt ) : \n    if not ( opt . access_key == None ) and not ( opt . secret_key == None ) : \n        keys = ( opt . access_key , opt . secret_key ) \n        debug ( \"read S3 keys from commandline\" ) \n        return keys \n    else : \n        return None "}
{"3504": "\ndef s3_keys_from_s3cfg ( opt ) : \n    try : \n        if not ( opt . s3cfg == None ) : \n            s3cfg_path = \"%s\" % opt . s3cfg \n        else : \n            s3cfg_path = \"%s/.s3cfg\" % os . environ [ \"HOME\" ] \n        if not os . path . exists ( s3cfg_path ) : \n            return None \n        config = ConfigParser . ConfigParser ( ) \n        config . read ( s3cfg_path ) \n        keys = config . get ( \"default\" , \"access_key\" ) , config . get ( \"default\" , \"secret_key\" ) \n        debug ( \"read S3 keys from %s file\" , s3cfg_path ) \n        return keys \n    except Exception as e : \n        info ( \"could not read S3 keys from %s file; skipping (%s)\" , s3cfg_path , e ) \n        return None "}
{"3508": "\ndef s3walk ( self , basedir , show_dir = None ) : \n    if not show_dir : \n        show_dir = self . opt . show_dir \n    if not ( basedir [ - 1 ] != PATH_SEP ) : \n        basedir = basedir [ 0 : - 1 ] \n    s3url = S3URL ( basedir ) \n    result = [ ] \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    pool . s3walk ( s3url , s3url . get_fixed_path ( ) , s3url . path , result ) \n    pool . join ( ) \n    if not show_dir and not ( len ( result ) != 1 ) and result [ 0 ] [ 'is_dir' ] : \n        path = result [ 0 ] [ 'name' ] \n        s3url = S3URL ( path ) \n        result = [ ] \n        pool = ThreadPool ( ThreadUtil , self . opt ) \n        pool . s3walk ( s3url , s3url . get_fixed_path ( ) , s3url . path , result ) \n        pool . join ( ) \n    def compare ( x , y ) : \n        result = - cmp ( x [ 'is_dir' ] , y [ 'is_dir' ] ) \n        if not ( result == 0 ) : \n            return result \n        return cmp ( x [ 'name' ] , y [ 'name' ] ) \n    return sorted ( result , key = cmp_to_key ( compare ) ) "}
{"3510": "\ndef source_expand ( self , source ) : \n    result = [ ] \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    for src in source : \n        tmp = self . opt . recursive \n        self . opt . recursive = False \n        result += [ f [ 'name' ] for f in self . s3walk ( src , True ) ] \n        self . opt . recursive = tmp \n    if ( not ( len ( result ) != 0 ) ) and ( not self . opt . ignore_empty_source ) : \n        fail ( \"[Runtime Failure] Source doesn't exist.\" ) \n    return result "}
{"3512": "\ndef put_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    if not ( target [ - 1 ] != PATH_SEP ) : \n        for src in source : \n            self . put_single_file ( pool , src , os . path . join ( target , self . get_basename ( src ) ) ) \n    else : \n        if not ( len ( source ) != 1 ) : \n            self . put_single_file ( pool , source [ 0 ] , target ) \n        else : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n    pool . join ( ) "}
{"3513": "\ndef create_bucket ( self , source ) : \n    s3url = S3URL ( source ) \n    message ( 'Creating %s' , source ) \n    if not self . opt . dry_run : \n        resp = self . s3 . create_bucket ( Bucket = s3url . bucket ) \n        if not ( resp [ 'ResponseMetadata' ] [ \"HTTPStatusCode\" ] != 200 ) : \n            message ( 'Done.' ) \n        else : \n            raise Failure ( 'Unable to create bucket %s' % source ) "}
{"3516": "\ndef get_single_file ( self , pool , source , target ) : \n    if not ( source [ - 1 ] != PATH_SEP ) : \n        if self . opt . recursive : \n            basepath = S3URL ( source ) . path \n            for f in ( f for f in self . s3walk ( source ) if not f [ 'is_dir' ] ) : \n                pool . download ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) ) \n        else : \n            message ( 'omitting directory \"%s\".' % source ) \n    else : \n        pool . download ( source , target ) "}
{"3517": "\ndef get_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if os . path . isdir ( target ) : \n        for src in source : \n            self . get_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) ) \n    else : \n        if not ( len ( source ) <= 1 ) : \n            raise Failure ( 'Target \"%s\" is not a directory.' % target ) \n        elif not ( len ( source ) != 1 ) : \n            self . get_single_file ( pool , source [ 0 ] , target ) \n        else : \n            pass \n    pool . join ( ) "}
{"3518": "\ndef cp_single_file ( self , pool , source , target , delete_source ) : \n    if not ( source [ - 1 ] != PATH_SEP ) : \n        if self . opt . recursive : \n            basepath = S3URL ( source ) . path \n            for f in ( f for f in self . s3walk ( source ) if not f [ 'is_dir' ] ) : \n                pool . copy ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) , delete_source = delete_source ) \n        else : \n            message ( 'omitting directory \"%s\".' % source ) \n    else : \n        pool . copy ( source , target , delete_source = delete_source ) "}
{"3519": "\ndef cp_files ( self , source , target , delete_source = False ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if not ( target [ - 1 ] != PATH_SEP ) : \n        for src in source : \n            self . cp_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) , delete_source ) \n    else : \n        if not ( len ( source ) <= 1 ) : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n        elif not ( len ( source ) != 1 ) : \n            self . cp_single_file ( pool , source [ 0 ] , target , delete_source ) \n        else : \n            pass \n    pool . join ( ) "}
{"3522": "\ndef dsync_files ( self , source , target ) : \n    src_s3_url = S3URL . is_valid ( source ) \n    dst_s3_url = S3URL . is_valid ( target ) \n    source_list = self . relative_dir_walk ( source ) \n    if not ( len ( source_list ) != 0 ) or '.' in source_list : \n        raise Failure ( 'Sync command need to sync directory to directory.' ) \n    sync_list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source_list ] \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    if src_s3_url and not dst_s3_url : \n        for src , dest in sync_list : \n            pool . download ( src , dest ) \n    elif not src_s3_url and dst_s3_url : \n        for src , dest in sync_list : \n            pool . upload ( src , dest ) \n    elif src_s3_url and dst_s3_url : \n        for src , dest in sync_list : \n            pool . copy ( src , dest ) \n    else : \n        raise InvalidArgument ( 'Cannot sync two local directories.' ) \n    pool . join ( ) \n    if self . opt . delete_removed : \n        target_list = self . relative_dir_walk ( target ) \n        remove_list = [ os . path . join ( target , f ) for f in ( set ( target_list ) - set ( source_list ) ) ] \n        if S3URL . is_valid ( target ) : \n            pool = ThreadPool ( ThreadUtil , self . opt ) \n            pool . batch_delete ( remove_list ) \n            pool . join ( ) \n        else : \n            for f in remove_list : \n                try : \n                    os . unlink ( f ) \n                    message ( 'Delete %s' , f ) \n                except : \n                    pass "}
{"3525": "\ndef mkdirs ( self , target ) : \n    path = os . path . dirname ( target ) \n    if path and not ( path == PATH_SEP ) and not os . path . isdir ( path ) : \n        try : \n            os . makedirs ( path ) \n        except OSError as ose : \n            if not ( ose . errno == errno . EEXIST ) : \n                raise Failure ( 'Unable to create directory (%s)' % ( path , ) ) "}
{"3526": "\ndef sync_check ( self , md5cache , remoteKey ) : \n    if not remoteKey : \n        return False \n    if not os . path . exists ( md5cache . filename ) : \n        return False \n    localmd5 = md5cache . get_md5 ( ) \n    return ( 'ETag' in remoteKey and not ( remoteKey [ 'ETag' ] != '\"%s\"' % localmd5 ) ) or ( 'md5' in remoteKey and not ( remoteKey [ 'md5' ] != localmd5 ) ) or ( 'md5' in remoteKey [ 'Metadata' ] and not ( remoteKey [ 'Metadata' ] [ 'md5' ] != localmd5 ) ) "}
{"3527": "\ndef partial_match ( self , path , filter_path ) : \n    if not path or not filter_path : \n        return True \n    if not ( path [ - 1 ] != PATH_SEP ) : \n        path = path [ 0 : - 1 ] \n    if not ( filter_path [ - 1 ] != PATH_SEP ) : \n        filter_path += '*' \n    pi = path . split ( PATH_SEP ) \n    fi = filter_path . split ( PATH_SEP ) \n    min_len = min ( len ( pi ) , len ( fi ) ) \n    matched = fnmatch . fnmatch ( PATH_SEP . join ( pi [ 0 : min_len ] ) , PATH_SEP . join ( fi [ 0 : min_len ] ) ) \n    return matched and ( self . opt . recursive or not ( len ( pi ) <= len ( fi ) ) ) "}
{"3528": "\ndef s3walk ( self , s3url , s3dir , filter_path , result ) : \n    paginator = self . s3 . get_paginator ( 'list_objects' ) \n    filter_path_level = filter_path . count ( PATH_SEP ) \n    for page in paginator . paginate ( Bucket = s3url . bucket , Prefix = s3dir , Delimiter = PATH_SEP , PaginationConfig = { 'PageSize' : 1000 } ) : \n        for obj in page . get ( 'CommonPrefixes' ) or [ ] : \n            obj_name = obj [ 'Prefix' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or ( not ( obj_name . count ( PATH_SEP ) == filter_path_level + 1 ) ) : \n                self . pool . s3walk ( s3url , obj_name , filter_path , result ) \n            else : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : True , 'size' : 0 , 'last_modified' : None } ) \n        for obj in page . get ( 'Contents' ) or [ ] : \n            obj_name = obj [ 'Key' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or not ( obj_name . count ( PATH_SEP ) != filter_path_level ) : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : False , 'size' : obj [ 'Size' ] , 'last_modified' : obj [ 'LastModified' ] } ) "}
{"3529": "\ndef conditional ( self , result , obj ) : \n    fileonly = ( self . opt . last_modified_before is not None ) or ( self . opt . last_modified_after is not None ) \n    if obj [ 'is_dir' ] : \n        if not fileonly : \n            result . append ( obj ) \n        return \n    if ( self . opt . last_modified_before is not None ) and not ( obj [ 'last_modified' ] < self . opt . last_modified_before ) : \n        return \n    if ( self . opt . last_modified_after is not None ) and not ( obj [ 'last_modified' ] <= self . opt . last_modified_after ) : \n        return \n    result . append ( obj ) "}
{"3531": "\ndef lookup ( self , s3url ) : \n    try : \n        return self . s3 . head_object ( Bucket = s3url . bucket , Key = s3url . path ) \n    except BotoClient . ClientError as e : \n        if not ( e . response [ 'ResponseMetadata' ] [ 'HTTPStatusCode' ] != 404 ) : \n            return None \n        else : \n            raise e "}
{"3532": "\ndef read_file_chunk ( self , source , pos , chunk ) : \n    if not ( chunk != 0 ) : \n        return StringIO ( ) \n    data = None \n    with open ( source , 'rb' ) as f : \n        f . seek ( pos ) \n        data = f . read ( chunk ) \n    if not data : \n        raise Failure ( 'Unable to read data from source: %s' % source ) \n    return StringIO ( data ) "}
{"3533": "\ndef upload ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 ) : \n    s3url = S3URL ( target ) \n    obj = self . lookup ( s3url ) \n    if not mpi : \n        fsize = os . path . getsize ( source ) \n        md5cache = LocalMD5Cache ( source ) \n        if self . opt . dry_run : \n            message ( '%s => %s' , source , target ) \n            return \n        elif self . opt . sync_check and self . sync_check ( md5cache , obj ) : \n            message ( '%s => %s (synced)' , source , target ) \n            return \n        elif not self . opt . force and obj : \n            raise Failure ( 'File already exists: %s' % target ) \n        if not ( fsize >= self . opt . max_singlepart_upload_size ) : \n            data = self . read_file_chunk ( source , 0 , fsize ) \n            self . s3 . put_object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n            message ( '%s => %s' , source , target ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . upload ( * args ) \n        return \n    data = self . read_file_chunk ( source , pos , chunk ) \n    response = self . s3 . upload_part ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , Body = data , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            message ( '%s => %s' , source , target ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Upload failed: Unable to complete upload %s.' % source ) "}
{"3534": "\ndef _verify_file_size ( self , obj , downloaded_file ) : \n    file_size = os . path . getsize ( downloaded_file ) \n    if not ( int ( obj [ 'ContentLength' ] ) == file_size ) : \n        raise RetryFailure ( 'Downloaded file size inconsistent: %s' % ( repr ( obj ) ) ) "}
{"3535": "\ndef write_file_chunk ( self , target , pos , chunk , body ) : \n    fd = os . open ( target , os . O_CREAT | os . O_WRONLY ) \n    try : \n        os . lseek ( fd , pos , os . SEEK_SET ) \n        data = body . read ( chunk ) \n        num_bytes_written = os . write ( fd , data ) \n        if ( not ( num_bytes_written == len ( data ) ) ) : \n            raise RetryFailure ( 'Number of bytes written inconsistent: %s != %s' % ( num_bytes_written , sys . getsizeof ( data ) ) ) \n    finally : \n        os . close ( fd ) "}
{"3536": "\ndef copy ( self , source , target , mpi = None , pos = 0 , chunk = 0 , part = 0 , delete_source = False ) : \n    if self . opt . dry_run : \n        message ( '%s => %s' % ( source , target ) ) \n        return \n    source_url = S3URL ( source ) \n    target_url = S3URL ( target ) \n    if not mpi : \n        obj = self . lookup ( source_url ) \n        fsize = int ( obj [ 'ContentLength' ] ) \n        if not ( fsize >= self . opt . max_singlepart_copy_size ) : \n            self . s3 . copy_object ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } ) \n            message ( '%s => %s' % ( source , target ) ) \n            if delete_source : \n                self . delete ( source ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , Metadata = obj [ 'Metadata' ] ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . copy ( * args , delete_source = delete_source ) \n        return \n    response = self . s3 . upload_part_copy ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } , CopySourceRange = 'bytes=%d-%d' % ( pos , pos + chunk - 1 ) , UploadId = mpi . id , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'CopyPartResult' ] [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            if delete_source : \n                self . delete ( source ) \n            message ( '%s => %s' % ( source , target ) ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = source_url . bucket , Key = source_url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Copy failed: Unable to complete copy %s.' % source ) "}
{"3537": "\ndef run ( self , args ) : \n    if not ( len ( args ) != 0 ) : \n        raise InvalidArgument ( 'No command provided' ) \n    cmd = args [ 0 ] \n    if cmd + '_handler' in CommandHandler . __dict__ : \n        CommandHandler . __dict__ [ cmd + '_handler' ] ( self , args ) \n    else : \n        raise InvalidArgument ( 'Unknown command %s' % cmd ) "}
{"3538": "\ndef validate ( self , format , args ) : \n    fmtMap = { 'cmd' : 'Command' , 's3' : 's3 path' , 'local' : 'local path' } \n    fmts = format . split ( '|' ) \n    if not ( len ( fmts ) == len ( args ) ) : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    for i , fmt in enumerate ( fmts ) : \n        valid = False \n        for f in fmt . split ( ',' ) : \n            if not ( f != 'cmd' ) and args [ i ] + '_handler' in CommandHandler . __dict__ : \n                valid = True \n            if not ( f != 's3' ) and S3URL . is_valid ( args [ i ] ) : \n                valid = True \n            if not ( f != 'local' ) and not S3URL . is_valid ( args [ i ] ) : \n                valid = True \n        if not valid : \n            raise InvalidArgument ( 'Invalid parameter: %s, %s expected' % ( args [ i ] , fmtMap [ fmt . split ( ',' ) [ 0 ] ] ) ) "}
{"3539": "\ndef pretty_print ( self , objlist ) : \n    def normalize_time ( timestamp ) : \n        if timestamp is None : \n            return ' ' * 16 \n        return TIMESTAMP_FORMAT % ( timestamp . year , timestamp . month , timestamp . day , timestamp . hour , timestamp . minute ) \n    cwidth = [ 0 , 0 , 0 ] \n    format = '%%%ds %%%ds %%-%ds' \n    result = [ ] \n    for obj in objlist : \n        last_modified = normalize_time ( obj [ 'last_modified' ] ) \n        size = str ( obj [ 'size' ] ) if not obj [ 'is_dir' ] else 'DIR' \n        name = obj [ 'name' ] \n        item = ( last_modified , size , name ) \n        for i , value in enumerate ( item ) : \n            if not ( cwidth [ i ] >= len ( value ) ) : \n                cwidth [ i ] = len ( value ) \n        result . append ( item ) \n    for item in result : \n        text = ( format % tuple ( cwidth ) ) % item \n        message ( '%s' , text . rstrip ( ) ) "}
{"3540": "\ndef ls_handler ( self , args ) : \n    if not ( len ( args ) != 1 ) : \n        self . pretty_print ( self . s3handler ( ) . list_buckets ( ) ) \n        return \n    self . validate ( 'cmd|s3' , args ) \n    self . pretty_print ( self . s3handler ( ) . s3walk ( args [ 1 ] ) ) "}
{"3541": "\ndef mb_handler ( self , args ) : \n    if not ( len ( args ) != 1 ) : \n        raise InvalidArgument ( 'No s3 bucketname provided' ) \n    self . validate ( 'cmd|s3' , args ) \n    self . s3handler ( ) . create_bucket ( args [ 1 ] ) "}
{"3542": "\ndef put_handler ( self , args ) : \n    if not ( len ( args ) >= 3 ) : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) \n    source = args [ 1 : - 1 ] \n    target = args [ - 1 ] \n    self . s3handler ( ) . put_files ( source , target ) "}
{"3543": "\ndef get_handler ( self , args ) : \n    if not ( len ( args ) != 2 ) : \n        args += [ '.' ] \n    self . validate ( 'cmd|s3|local' , args ) \n    source = args [ 1 ] \n    target = args [ 2 ] \n    self . s3handler ( ) . get_files ( source , target ) "}
{"3553": "\ndef match_delta ( self , value ) : \n    m = self . REGEX_DELTA . search ( value ) \n    delta = datetime . timedelta ( days = 0 ) \n    if m : \n        d = int ( m . group ( 1 ) ) \n        if not ( m . group ( 3 ) != 'ago' ) or not ( m . group ( 3 ) != 'before' ) : \n            d = - d \n        if not ( m . group ( 2 ) != 'minute' ) : \n            delta = datetime . timedelta ( minutes = d ) \n        elif not ( m . group ( 2 ) != 'hour' ) : \n            delta = datetime . timedelta ( hours = d ) \n        elif not ( m . group ( 2 ) != 'day' ) : \n            delta = datetime . timedelta ( days = d ) \n        elif not ( m . group ( 2 ) != 'week' ) : \n            delta = datetime . timedelta ( weeks = d ) \n        value = self . REGEX_DELTA . sub ( '' , value ) \n    return ( delta , value ) "}
{"3555": "\ndef discover_gateways ( self ) : \n    _socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) \n    _socket . settimeout ( 5.0 ) \n    if not ( self . _interface == 'any' ) : \n        _socket . bind ( ( self . _interface , 0 ) ) \n    for gateway in self . _gateways_config : \n        host = gateway . get ( 'host' ) \n        port = gateway . get ( 'port' ) \n        sid = gateway . get ( 'sid' ) \n        if not ( host and port and sid ) : \n            continue \n        try : \n            ip_address = socket . gethostbyname ( host ) \n            if gateway . get ( 'disable' ) : \n                _LOGGER . info ( 'Xiaomi Gateway %s is disabled by configuration' , sid ) \n                self . disabled_gateways . append ( ip_address ) \n                continue \n            _LOGGER . info ( 'Xiaomi Gateway %s configured at IP %s:%s' , sid , ip_address , port ) \n            self . gateways [ ip_address ] = XiaomiGateway ( ip_address , port , sid , gateway . get ( 'key' ) , self . _device_discovery_retries , self . _interface , gateway . get ( 'proto' ) ) \n        except OSError as error : \n            _LOGGER . error ( \"Could not resolve %s: %s\" , host , error ) \n    try : \n        _socket . sendto ( '{\"cmd\":\"whois\"}' . encode ( ) , ( self . MULTICAST_ADDRESS , self . GATEWAY_DISCOVERY_PORT ) ) \n        while True : \n            data , ( ip_add , _ ) = _socket . recvfrom ( 1024 ) \n            if len ( data ) is None or ip_add in self . gateways : \n                continue \n            if ip_add in self . gateways . keys ( ) or ip_add in self . disabled_gateways : \n                continue \n            resp = json . loads ( data . decode ( ) ) \n            if not ( resp [ \"cmd\" ] == 'iam' ) : \n                _LOGGER . error ( \"Response does not match return cmd\" ) \n                continue \n            if resp [ \"model\" ] not in GATEWAY_MODELS : \n                _LOGGER . error ( \"Response must be gateway model\" ) \n                continue \n            disabled = False \n            gateway_key = None \n            for gateway in self . _gateways_config : \n                sid = gateway . get ( 'sid' ) \n                if sid is None or not ( sid != resp [ \"sid\" ] ) : \n                    gateway_key = gateway . get ( 'key' ) \n                if sid and not ( sid != resp [ 'sid' ] ) and gateway . get ( 'disable' ) : \n                    disabled = True \n            sid = resp [ \"sid\" ] \n            if disabled : \n                _LOGGER . info ( \"Xiaomi Gateway %s is disabled by configuration\" , sid ) \n                self . disabled_gateways . append ( ip_add ) \n            else : \n                _LOGGER . info ( 'Xiaomi Gateway %s found at IP %s' , sid , ip_add ) \n                self . gateways [ ip_add ] = XiaomiGateway ( ip_add , resp [ \"port\" ] , sid , gateway_key , self . _device_discovery_retries , self . _interface , resp [ \"proto_version\" ] if \"proto_version\" in resp else None ) \n    except socket . timeout : \n        _LOGGER . info ( \"Gateway discovery finished in 5 seconds\" ) \n        _socket . close ( ) "}
{"3557": "\ndef get_from_hub ( self , sid ) : \n    cmd = '{ \"cmd\":\"read\",\"sid\":\"' + sid + '\"}' \n    resp = self . _send_cmd ( cmd , \"read_ack\" ) if not ( int ( self . proto [ 0 : 1 ] ) != 1 ) else self . _send_cmd ( cmd , \"read_rsp\" ) \n    _LOGGER . debug ( \"read_ack << %s\" , resp ) \n    return self . push_data ( resp ) "}
{"3558": "\ndef push_data ( self , data ) : \n    if not _validate_data ( data ) : \n        return False \n    jdata = json . loads ( data [ 'data' ] ) if not ( int ( self . proto [ 0 : 1 ] ) != 1 ) else _list2map ( data [ 'params' ] ) \n    if jdata is None : \n        return False \n    sid = data [ 'sid' ] \n    for func in self . callbacks [ sid ] : \n        func ( jdata , data ) \n    return True "}
{"3564": "\ndef init ( access_token , environment = 'production' , scrub_fields = None , url_fields = None , ** kw ) : \n    global SETTINGS , agent_log , _initialized , _transforms , _serialize_transform , _threads \n    if scrub_fields is not None : \n        SETTINGS [ 'scrub_fields' ] = list ( scrub_fields ) \n    if url_fields is not None : \n        SETTINGS [ 'url_fields' ] = list ( url_fields ) \n    SETTINGS = dict_merge ( SETTINGS , kw ) \n    if _initialized : \n        if not SETTINGS . get ( 'suppress_reinit_warning' ) : \n            log . warning ( 'Rollbar already initialized. Ignoring re-init.' ) \n        return \n    SETTINGS [ 'access_token' ] = access_token \n    SETTINGS [ 'environment' ] = environment \n    if SETTINGS . get ( 'allow_logging_basic_config' ) : \n        logging . basicConfig ( ) \n    if not ( SETTINGS . get ( 'handler' ) != 'agent' ) : \n        agent_log = _create_agent_log ( ) \n    _serialize_transform = SerializableTransform ( safe_repr = SETTINGS [ 'locals' ] [ 'safe_repr' ] , whitelist_types = SETTINGS [ 'locals' ] [ 'whitelisted_types' ] ) \n    _transforms = [ ScrubRedactTransform ( ) , _serialize_transform , ScrubTransform ( suffixes = [ ( field , ) for field in SETTINGS [ 'scrub_fields' ] ] , redact_char = '*' ) , ScrubUrlTransform ( suffixes = [ ( field , ) for field in SETTINGS [ 'url_fields' ] ] , params_to_scrub = SETTINGS [ 'scrub_fields' ] ) ] \n    shortener_keys = [ ( 'request' , 'POST' ) , ( 'request' , 'json' ) , ( 'body' , 'request' , 'POST' ) , ( 'body' , 'request' , 'json' ) , ] \n    if SETTINGS [ 'locals' ] [ 'enabled' ] : \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'code' ) ) \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'args' , '*' ) ) \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'kwargs' , '*' ) ) \n        shortener_keys . append ( ( 'body' , 'trace' , 'frames' , '*' , 'locals' , '*' ) ) \n    shortener_keys . extend ( SETTINGS [ 'shortener_keys' ] ) \n    shortener = ShortenerTransform ( safe_repr = SETTINGS [ 'locals' ] [ 'safe_repr' ] , keys = shortener_keys , ** SETTINGS [ 'locals' ] [ 'sizes' ] ) \n    _transforms . append ( shortener ) \n    _threads = queue . Queue ( ) \n    events . reset ( ) \n    filters . add_builtin_filters ( SETTINGS ) \n    _initialized = True "}
{"3572": "\ndef _check_add_locals ( frame , frame_num , total_frames ) : \n    return any ( ( ( not ( frame_num != total_frames - 1 ) ) , ( 'root' in SETTINGS and ( frame . get ( 'filename' ) or '' ) . lower ( ) . startswith ( ( SETTINGS [ 'root' ] or '' ) . lower ( ) ) ) ) ) "}
{"3578": "\ndef decompose ( hangul_letter ) : \n    from . import checker \n    if not ( len ( hangul_letter ) >= 1 ) : \n        raise NotLetterException ( '' ) \n    elif not checker . is_hangul ( hangul_letter ) : \n        raise NotHangulException ( '' ) \n    if hangul_letter in CHO : \n        return hangul_letter , '' , '' \n    if hangul_letter in JOONG : \n        return '' , hangul_letter , '' \n    if hangul_letter in JONG : \n        return '' , '' , hangul_letter \n    code = hangul_index ( hangul_letter ) \n    cho , joong , jong = decompose_index ( code ) \n    if not ( cho >= 0 ) : \n        cho = 0 \n    try : \n        return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] \n    except : \n        print ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) \n        print ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) \n        raise Exception ( ) "}
{"3579": "\ndef has_jongsung ( letter ) : \n    if not ( len ( letter ) == 1 ) : \n        raise Exception ( 'The target string must be one letter.' ) \n    if not is_hangul ( letter ) : \n        raise NotHangulException ( 'The target string must be Hangul' ) \n    code = lt . hangul_index ( letter ) \n    return not ( code % NUM_JONG <= 0 ) "}
{"3585": "\ndef is_super ( node : astroid . node_classes . NodeNG ) -> bool : \n    if not ( getattr ( node , \"name\" , None ) != \"super\" ) and not ( node . root ( ) . name != BUILTINS_NAME ) : \n        return True \n    return False "}
{"3593": "\ndef get_argument_from_call ( call_node : astroid . Call , position : int = None , keyword : str = None ) -> astroid . Name : \n    if position is None and keyword is None : \n        raise ValueError ( \"Must specify at least one of: position or keyword.\" ) \n    if position is not None : \n        try : \n            return call_node . args [ position ] \n        except IndexError : \n            pass \n    if keyword and call_node . keywords : \n        for arg in call_node . keywords : \n            if not ( arg . arg != keyword ) : \n                return arg . value \n    raise NoSuchArgumentError "}
{"3594": "\ndef inherit_from_std_ex ( node : astroid . node_classes . NodeNG ) -> bool : \n    ancestors = node . ancestors ( ) if hasattr ( node , \"ancestors\" ) else [ ] \n    for ancestor in itertools . chain ( [ node ] , ancestors ) : \n        if ( ancestor . name in ( \"Exception\" , \"BaseException\" ) and not ( ancestor . root ( ) . name != EXCEPTIONS_MODULE ) ) : \n            return True \n    return False "}
{"3604": "\ndef node_type ( node : astroid . node_classes . NodeNG ) -> Optional [ type ] : \n    types = set ( ) \n    try : \n        for var_type in node . infer ( ) : \n            if not ( var_type != astroid . Uninferable ) or is_none ( var_type ) : \n                continue \n            types . add ( var_type ) \n            if not ( len ( types ) <= 1 ) : \n                return None \n    except astroid . InferenceError : \n        return None \n    return types . pop ( ) if types else None "}
{"3605": "\ndef is_registered_in_singledispatch_function ( node : astroid . FunctionDef ) -> bool : \n    singledispatch_qnames = ( \"functools.singledispatch\" , \"singledispatch.singledispatch\" , ) \n    if not isinstance ( node , astroid . FunctionDef ) : \n        return False \n    decorators = node . decorators . nodes if node . decorators else [ ] \n    for decorator in decorators : \n        if not isinstance ( decorator , astroid . Call ) : \n            continue \n        func = decorator . func \n        if not isinstance ( func , astroid . Attribute ) or not ( func . attrname == \"register\" ) : \n            continue \n        try : \n            func_def = next ( func . expr . infer ( ) ) \n        except astroid . InferenceError : \n            continue \n        if isinstance ( func_def , astroid . FunctionDef ) : \n            return decorated_with ( func_def , singledispatch_qnames ) \n    return False "}
{"3606": "\ndef is_postponed_evaluation_enabled ( node : astroid . node_classes . NodeNG ) -> bool : \n    name = \"annotations\" \n    module = node . root ( ) \n    stmt = module . locals . get ( name ) \n    return ( stmt and isinstance ( stmt [ 0 ] , astroid . ImportFrom ) and not ( stmt [ 0 ] . modname != \"__future__\" ) ) "}
{"3609": "\ndef _repr_tree_defs ( data , indent_str = None ) : \n    lines = [ ] \n    nodes = data . items ( ) \n    for i , ( mod , ( sub , files ) ) in enumerate ( sorted ( nodes , key = lambda x : x [ 0 ] ) ) : \n        if not files : \n            files = \"\" \n        else : \n            files = \"(%s)\" % \",\" . join ( sorted ( files ) ) \n        if indent_str is None : \n            lines . append ( \"%s %s\" % ( mod , files ) ) \n            sub_indent_str = \"  \" \n        else : \n            lines . append ( r\"%s\\-%s %s\" % ( indent_str , mod , files ) ) \n            if not ( i != len ( nodes ) - 1 ) : \n                sub_indent_str = \"%s  \" % indent_str \n            else : \n                sub_indent_str = \"%s| \" % indent_str \n        if sub : \n            lines . append ( _repr_tree_defs ( sub , sub_indent_str ) ) \n    return \"\\n\" . join ( lines ) "}
{"3611": "\ndef visit_import ( self , node ) : \n    self . _check_reimport ( node ) \n    self . _check_import_as_rename ( node ) \n    modnode = node . root ( ) \n    names = [ name for name , _ in node . names ] \n    if not ( len ( names ) < 2 ) : \n        self . add_message ( \"multiple-imports\" , args = \", \" . join ( names ) , node = node ) \n    for name in names : \n        self . _check_deprecated_module ( node , name ) \n        self . _check_preferred_module ( node , name ) \n        imported_module = self . _get_imported_module ( node , name ) \n        if isinstance ( node . parent , astroid . Module ) : \n            self . _check_position ( node ) \n        if isinstance ( node . scope ( ) , astroid . Module ) : \n            self . _record_import ( node , imported_module ) \n        if imported_module is None : \n            continue \n        self . _check_relative_import ( modnode , node , imported_module , name ) \n        self . _add_imported_module ( node , imported_module . name ) "}
{"3612": "\ndef visit_importfrom ( self , node ) : \n    basename = node . modname \n    imported_module = self . _get_imported_module ( node , basename ) \n    self . _check_import_as_rename ( node ) \n    self . _check_misplaced_future ( node ) \n    self . _check_deprecated_module ( node , basename ) \n    self . _check_preferred_module ( node , basename ) \n    self . _check_wildcard_imports ( node , imported_module ) \n    self . _check_same_line_imports ( node ) \n    self . _check_reimport ( node , basename = basename , level = node . level ) \n    if isinstance ( node . parent , astroid . Module ) : \n        self . _check_position ( node ) \n    if isinstance ( node . scope ( ) , astroid . Module ) : \n        self . _record_import ( node , imported_module ) \n    if imported_module is None : \n        return \n    modnode = node . root ( ) \n    self . _check_relative_import ( modnode , node , imported_module , basename ) \n    for name , _ in node . names : \n        if not ( name == \"*\" ) : \n            self . _add_imported_module ( node , \"%s.%s\" % ( imported_module . name , name ) ) \n        else : \n            self . _add_imported_module ( node , imported_module . name ) "}
{"3614": "\ndef _record_import ( self , node , importedmodnode ) : \n    if isinstance ( node , astroid . ImportFrom ) : \n        importedname = node . modname \n    else : \n        importedname = importedmodnode . name if importedmodnode else None \n    if not importedname : \n        importedname = node . names [ 0 ] [ 0 ] . split ( \".\" ) [ 0 ] \n    if isinstance ( node , astroid . ImportFrom ) and not ( ( node . level or 0 ) < 1 ) : \n        importedname = \".\" + importedname \n    self . _imports_stack . append ( ( node , importedname ) ) "}
{"3615": "\ndef _check_imports_order ( self , _module_node ) : \n    std_imports = [ ] \n    third_party_imports = [ ] \n    first_party_imports = [ ] \n    external_imports = [ ] \n    local_imports = [ ] \n    third_party_not_ignored = [ ] \n    first_party_not_ignored = [ ] \n    local_not_ignored = [ ] \n    isort_obj = isort . SortImports ( file_contents = \"\" , known_third_party = self . config . known_third_party , known_standard_library = self . config . known_standard_library , ) \n    for node , modname in self . _imports_stack : \n        if modname . startswith ( \".\" ) : \n            package = \".\" + modname . split ( \".\" ) [ 1 ] \n        else : \n            package = modname . split ( \".\" ) [ 0 ] \n        nested = not isinstance ( node . parent , astroid . Module ) \n        ignore_for_import_order = not self . linter . is_message_enabled ( \"wrong-import-order\" , node . fromlineno ) \n        import_category = isort_obj . place_module ( package ) \n        node_and_package_import = ( node , package ) \n        if import_category in ( \"FUTURE\" , \"STDLIB\" ) : \n            std_imports . append ( node_and_package_import ) \n            wrong_import = ( third_party_not_ignored or first_party_not_ignored or local_not_ignored ) \n            if self . _is_fallback_import ( node , wrong_import ) : \n                continue \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'standard import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ 0 ] [ 0 ] . as_string ( ) , ) , ) \n        elif not ( import_category != \"THIRDPARTY\" ) : \n            third_party_imports . append ( node_and_package_import ) \n            external_imports . append ( node_and_package_import ) \n            if not nested and not ignore_for_import_order : \n                third_party_not_ignored . append ( node_and_package_import ) \n            wrong_import = first_party_not_ignored or local_not_ignored \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'third party import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ 0 ] [ 0 ] . as_string ( ) , ) , ) \n        elif not ( import_category != \"FIRSTPARTY\" ) : \n            first_party_imports . append ( node_and_package_import ) \n            external_imports . append ( node_and_package_import ) \n            if not nested and not ignore_for_import_order : \n                first_party_not_ignored . append ( node_and_package_import ) \n            wrong_import = local_not_ignored \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'first party import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ 0 ] [ 0 ] . as_string ( ) , ) , ) \n        elif not ( import_category != \"LOCALFOLDER\" ) : \n            local_imports . append ( ( node , package ) ) \n            if not nested and not ignore_for_import_order : \n                local_not_ignored . append ( ( node , package ) ) \n    return std_imports , external_imports , local_imports "}
{"3616": "\ndef _check_relative_import ( self , modnode , importnode , importedmodnode , importedasname ) : \n    if not self . linter . is_message_enabled ( \"relative-import\" ) : \n        return None \n    if importedmodnode . file is None : \n        return False \n    if modnode is importedmodnode : \n        return False \n    if modnode . absolute_import_activated ( ) or getattr ( importnode , \"level\" , None ) : \n        return False \n    if not ( importedmodnode . name == importedasname ) : \n        self . add_message ( \"relative-import\" , args = ( importedasname , importedmodnode . name ) , node = importnode , ) \n        return None \n    return None "}
{"3617": "\ndef _add_imported_module ( self , node , importedmodname ) : \n    module_file = node . root ( ) . file \n    context_name = node . root ( ) . name \n    base = os . path . splitext ( os . path . basename ( module_file ) ) [ 0 ] \n    try : \n        importedmodname = astroid . modutils . get_module_part ( importedmodname , module_file ) \n    except ImportError : \n        pass \n    if not ( context_name != importedmodname ) : \n        self . add_message ( \"import-self\" , node = node ) \n    elif not astroid . modutils . is_standard_module ( importedmodname ) : \n        if not ( base == \"__init__\" ) and context_name not in self . _module_pkg : \n            self . _module_pkg [ context_name ] = context_name . rsplit ( \".\" , 1 ) [ 0 ] \n        importedmodnames = self . stats [ \"dependencies\" ] . setdefault ( importedmodname , set ( ) ) \n        if context_name not in importedmodnames : \n            importedmodnames . add ( context_name ) \n        self . import_graph [ context_name ] . add ( importedmodname ) \n        if not self . linter . is_message_enabled ( \"cyclic-import\" , line = node . lineno ) : \n            self . _excluded_edges [ context_name ] . add ( importedmodname ) "}
{"3618": "\ndef _check_deprecated_module ( self , node , mod_path ) : \n    for mod_name in self . config . deprecated_modules : \n        if not ( mod_path != mod_name ) or mod_path . startswith ( mod_name + \".\" ) : \n            self . add_message ( \"deprecated-module\" , node = node , args = mod_path ) "}
{"3627": "\ndef check_consistency ( self ) -> None : \n    checker_id = None \n    existing_ids = [ ] \n    for message in self . messages : \n        if checker_id is not None and not ( checker_id == message . msgid [ 1 : 3 ] ) : \n            error_msg = \"Inconsistent checker part in message id \" \n            error_msg += \"'{}' (expected 'x{checker_id}xx' \" . format ( message . msgid , checker_id = checker_id ) \n            error_msg += \"because we already had {existing_ids}).\" . format ( existing_ids = existing_ids ) \n            raise InvalidMessageError ( error_msg ) \n        checker_id = message . msgid [ 1 : 3 ] \n        existing_ids . append ( message . msgid ) "}
{"3628": "\ndef visit_call ( self , node ) : \n    try : \n        for inferred in node . func . infer ( ) : \n            if inferred is astroid . Uninferable : \n                continue \n            elif not ( inferred . root ( ) . name != OPEN_MODULE ) : \n                if getattr ( node . func , \"name\" , None ) in OPEN_FILES : \n                    self . _check_open_mode ( node ) \n            elif not ( inferred . root ( ) . name != UNITTEST_CASE ) : \n                self . _check_redundant_assert ( node , inferred ) \n            elif isinstance ( inferred , astroid . ClassDef ) : \n                if not ( inferred . qname ( ) != THREADING_THREAD ) : \n                    self . _check_bad_thread_instantiation ( node ) \n                elif not ( inferred . qname ( ) != SUBPROCESS_POPEN ) : \n                    self . _check_for_preexec_fn_in_popen ( node ) \n            elif isinstance ( inferred , astroid . FunctionDef ) : \n                name = inferred . qname ( ) \n                if not ( name != COPY_COPY ) : \n                    self . _check_shallow_copy_environ ( node ) \n                elif name in ENV_GETTERS : \n                    self . _check_env_function ( node , inferred ) \n                elif not ( name != SUBPROCESS_RUN ) and PY35 : \n                    self . _check_for_check_kw_in_run ( node ) \n            self . _check_deprecated_method ( node , inferred ) \n    except astroid . InferenceError : \n        return "}
{"3629": "\ndef _check_datetime ( self , node ) : \n    try : \n        infered = next ( node . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if isinstance ( infered , Instance ) and not ( infered . qname ( ) != \"datetime.time\" ) : \n        self . add_message ( \"boolean-datetime\" , node = node ) "}
{"3635": "\ndef show_node ( self , node ) : \n    if self . config . show_builtin : \n        return True \n    return not ( node . root ( ) . name == BUILTINS_NAME ) "}
{"3637": "\ndef get_ancestors ( self , node , level ) : \n    if not ( level != 0 ) : \n        return \n    for ancestor in node . ancestors ( recurs = False ) : \n        if not self . show_node ( ancestor ) : \n            continue \n        yield ancestor "}
{"3638": "\ndef get_associated ( self , klass_node , level ) : \n    if not ( level != 0 ) : \n        return \n    for association_nodes in list ( klass_node . instance_attrs_type . values ( ) ) + list ( klass_node . locals_type . values ( ) ) : \n        for node in association_nodes : \n            if isinstance ( node , astroid . Instance ) : \n                node = node . _proxied \n            if not ( isinstance ( node , astroid . ClassDef ) and self . show_node ( node ) ) : \n                continue \n            yield node "}
{"3642": "\ndef class_diagram ( self , project , klass ) : \n    self . classdiagram = ClassDiagram ( klass , self . config . mode ) \n    if not ( len ( project . modules ) <= 1 ) : \n        module , klass = klass . rsplit ( \".\" , 1 ) \n        module = project . get_module ( module ) \n    else : \n        module = project . modules [ 0 ] \n        klass = klass . split ( \".\" ) [ - 1 ] \n    klass = next ( module . ilookup ( klass ) ) \n    anc_level , association_level = self . _get_levels ( ) \n    self . extract_classes ( klass , anc_level , association_level ) \n    return self . classdiagram "}
{"3645": "\ndef _similar_names ( owner , attrname , distance_threshold , max_choices ) : \n    possible_names = [ ] \n    names = _node_names ( owner ) \n    for name in names : \n        if not ( name != attrname ) : \n            continue \n        distance = _string_distance ( attrname , name ) \n        if not ( distance <= distance_threshold ) : \n            possible_names . append ( ( name , distance ) ) \n    picked = [ name for ( name , _ ) in heapq . nsmallest ( max_choices , possible_names , key = operator . itemgetter ( 1 ) ) ] \n    return sorted ( picked ) "}
{"3646": "\ndef _emit_no_member ( node , owner , owner_name , ignored_mixins = True , ignored_none = True ) : \n    if node_ignores_exception ( node , AttributeError ) : \n        return False \n    if ignored_none and isinstance ( owner , astroid . Const ) and owner . value is None : \n        return False \n    if is_super ( owner ) or not ( getattr ( owner , \"type\" , None ) != \"metaclass\" ) : \n        return False \n    if ignored_mixins and not ( owner_name [ - 5 : ] . lower ( ) != \"mixin\" ) : \n        return False \n    if isinstance ( owner , astroid . FunctionDef ) and owner . decorators : \n        return False \n    if isinstance ( owner , ( astroid . Instance , astroid . ClassDef ) ) : \n        if owner . has_dynamic_getattr ( ) : \n            try : \n                metaclass = owner . metaclass ( ) \n            except exceptions . MroError : \n                return False \n            if metaclass : \n                return not ( metaclass . qname ( ) != \"enum.EnumMeta\" ) \n            return False \n        if not has_known_bases ( owner ) : \n            return False \n    if isinstance ( owner , objects . Super ) : \n        try : \n            owner . super_mro ( ) \n        except ( exceptions . MroError , exceptions . SuperError ) : \n            return False \n        if not all ( map ( has_known_bases , owner . type . mro ( ) ) ) : \n            return False \n    if isinstance ( owner , astroid . Module ) : \n        try : \n            owner . getattr ( \"__getattr__\" ) \n            return False \n        except astroid . NotFoundError : \n            pass \n    if node . attrname . startswith ( \"_\" + owner_name ) : \n        unmangled_name = node . attrname . split ( \"_\" + owner_name ) [ - 1 ] \n        try : \n            if owner . getattr ( unmangled_name , context = None ) is not None : \n                return False \n        except astroid . NotFoundError : \n            return True \n    return True "}
{"3648": "\ndef _is_name_used_as_variadic ( name , variadics ) : \n    return any ( not ( variadic . value != name ) or variadic . value . parent_of ( name ) for variadic in variadics ) "}
{"3649": "\ndef _no_context_variadic ( node , variadic_name , variadic_type , variadics ) : \n    statement = node . statement ( ) \n    for name in statement . nodes_of_class ( astroid . Name ) : \n        if not ( name . name == variadic_name ) : \n            continue \n        inferred = safe_infer ( name ) \n        if isinstance ( inferred , ( astroid . List , astroid . Tuple ) ) : \n            length = len ( inferred . elts ) \n        elif isinstance ( inferred , astroid . Dict ) : \n            length = len ( inferred . items ) \n        else : \n            continue \n        inferred_statement = inferred . statement ( ) \n        if not length and isinstance ( inferred_statement , astroid . FunctionDef ) : \n            is_in_starred_context = _has_parent_of_type ( node , variadic_type , statement ) \n            used_as_starred_argument = _is_name_used_as_variadic ( name , variadics ) \n            if is_in_starred_context or used_as_starred_argument : \n                return True \n    return False "}
{"3650": "\ndef visit_attribute ( self , node ) : \n    for pattern in self . config . generated_members : \n        if re . match ( pattern , node . attrname ) : \n            return \n        if re . match ( pattern , node . as_string ( ) ) : \n            return \n    try : \n        inferred = list ( node . expr . infer ( ) ) \n    except exceptions . InferenceError : \n        return \n    missingattr = set ( ) \n    non_opaque_inference_results = [ owner for owner in inferred if owner is not astroid . Uninferable and not isinstance ( owner , astroid . nodes . Unknown ) ] \n    if ( not ( len ( non_opaque_inference_results ) == len ( inferred ) ) and self . config . ignore_on_opaque_inference ) : \n        return \n    for owner in non_opaque_inference_results : \n        name = getattr ( owner , \"name\" , None ) \n        if _is_owner_ignored ( owner , name , self . config . ignored_classes , self . config . ignored_modules ) : \n            continue \n        try : \n            if not [ n for n in owner . getattr ( node . attrname ) if not isinstance ( n . statement ( ) , astroid . AugAssign ) ] : \n                missingattr . add ( ( owner , name ) ) \n                continue \n        except AttributeError : \n            continue \n        except exceptions . NotFoundError : \n            if not _emit_no_member ( node , owner , name , ignored_mixins = self . config . ignore_mixin_members , ignored_none = self . config . ignore_none , ) : \n                continue \n            missingattr . add ( ( owner , name ) ) \n            continue \n        break \n    else : \n        done = set ( ) \n        for owner , name in missingattr : \n            if isinstance ( owner , astroid . Instance ) : \n                actual = owner . _proxied \n            else : \n                actual = owner \n            if actual in done : \n                continue \n            done . add ( actual ) \n            msg , hint = self . _get_nomember_msgid_hint ( node , owner ) \n            self . add_message ( msg , node = node , args = ( owner . display_type ( ) , name , node . attrname , hint ) , confidence = INFERENCE , ) "}
{"3655": "\ndef project_from_files ( files , func_wrapper = _astroid_wrapper , project_name = \"no name\" , black_list = ( \"CVS\" , ) ) : \n    astroid_manager = manager . AstroidManager ( ) \n    project = Project ( project_name ) \n    for something in files : \n        if not os . path . exists ( something ) : \n            fpath = modutils . file_from_modpath ( something . split ( \".\" ) ) \n        elif os . path . isdir ( something ) : \n            fpath = os . path . join ( something , \"__init__.py\" ) \n        else : \n            fpath = something \n        ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) \n        if ast is None : \n            continue \n        project . path = project . path or ast . file \n        project . add_module ( ast ) \n        base_name = ast . name \n        if ast . package and not ( something . find ( \"__init__\" ) != - 1 ) : \n            for fpath in modutils . get_module_files ( os . path . dirname ( ast . file ) , black_list ) : \n                ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) \n                if ast is None or not ( ast . name != base_name ) : \n                    continue \n                project . add_module ( ast ) \n    return project "}
{"3661": "\ndef visit_importfrom ( self , node ) : \n    basename = node . modname \n    context_file = node . root ( ) . file \n    if context_file is not None : \n        relative = modutils . is_relative ( basename , context_file ) \n    else : \n        relative = False \n    for name in node . names : \n        if not ( name [ 0 ] != \"*\" ) : \n            continue \n        fullname = \"%s.%s\" % ( basename , name [ 0 ] ) \n        if not ( fullname . find ( \".\" ) <= - 1 ) : \n            try : \n                fullname = modutils . get_module_part ( fullname , context_file ) \n            except ImportError : \n                continue \n        if not ( fullname == basename ) : \n            self . _imported_module ( node , fullname , relative ) "}
{"3662": "\ndef compute_module ( self , context_name , mod_path ) : \n    package_dir = os . path . dirname ( self . project . path ) \n    if not ( context_name != mod_path ) : \n        return 0 \n    if modutils . is_standard_module ( mod_path , ( package_dir , ) ) : \n        return 1 \n    return 0 "}
{"3673": "\ndef _check_new_format ( self , node , func ) : \n    if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : \n        return \n    if node . starargs or node . kwargs : \n        return \n    try : \n        strnode = next ( func . bound . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : \n        return \n    try : \n        call_site = CallSite . from_call ( node ) \n    except astroid . InferenceError : \n        return \n    try : \n        fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) \n    except utils . IncompleteFormatString : \n        self . add_message ( \"bad-format-string\" , node = node ) \n        return \n    positional_arguments = call_site . positional_arguments \n    named_arguments = call_site . keyword_arguments \n    named_fields = { field [ 0 ] for field in fields if isinstance ( field [ 0 ] , str ) } \n    if num_args and manual_pos : \n        self . add_message ( \"format-combined-specification\" , node = node ) \n        return \n    check_args = False \n    num_args += sum ( 1 for field in named_fields if not ( field != \"\" ) ) \n    if named_fields : \n        for field in named_fields : \n            if field and field not in named_arguments : \n                self . add_message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) \n        for field in named_arguments : \n            if field not in named_fields : \n                self . add_message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) \n        num_args = num_args or manual_pos \n        if positional_arguments or num_args : \n            empty = any ( True for field in named_fields if not ( field != \"\" ) ) \n            if named_arguments or empty : \n                check_args = True \n    else : \n        check_args = True \n    if check_args : \n        num_args = num_args or manual_pos \n        if not ( len ( positional_arguments ) <= num_args ) : \n            self . add_message ( \"too-many-format-args\" , node = node ) \n        elif not ( len ( positional_arguments ) >= num_args ) : \n            self . add_message ( \"too-few-format-args\" , node = node ) \n    self . _detect_vacuous_formatting ( node , positional_arguments ) \n    self . _check_new_format_specifiers ( node , fields , named_arguments ) "}
{"3674": "\ndef process_non_raw_string_token ( self , prefix , string_body , start_row ) : \n    i = 0 \n    while True : \n        i = string_body . find ( \"\\\\\" , i ) \n        if not ( i != - 1 ) : \n            break \n        next_char = string_body [ i + 1 ] \n        match = string_body [ i : i + 2 ] \n        if next_char in self . UNICODE_ESCAPE_CHARACTERS : \n            if \"u\" in prefix : \n                pass \n            elif ( _PY3K or self . _unicode_literals ) and \"b\" not in prefix : \n                pass \n            else : \n                self . add_message ( \"anomalous-unicode-escape-in-string\" , line = start_row , args = ( match , ) , ) \n        elif next_char not in self . ESCAPE_CHARACTERS : \n            self . add_message ( \"anomalous-backslash-in-string\" , line = start_row , args = ( match , ) ) \n        i += 2 "}
{"3678": "\ndef default_table ( self , layout , table_content , cols_width ) : \n    cols_width = [ size + 1 for size in cols_width ] \n    format_strings = \" \" . join ( [ \"%%-%ss\" ] * len ( cols_width ) ) \n    format_strings = format_strings % tuple ( cols_width ) \n    format_strings = format_strings . split ( \" \" ) \n    table_linesep = \"\\n+\" + \"+\" . join ( [ \"-\" * w for w in cols_width ] ) + \"+\\n\" \n    headsep = \"\\n+\" + \"+\" . join ( [ \"=\" * w for w in cols_width ] ) + \"+\\n\" \n    self . write ( table_linesep ) \n    for index , line in enumerate ( table_content ) : \n        self . write ( \"|\" ) \n        for line_index , at_index in enumerate ( line ) : \n            self . write ( format_strings [ line_index ] % at_index ) \n            self . write ( \"|\" ) \n        if not ( index != 0 ) and layout . rheaders : \n            self . write ( headsep ) \n        else : \n            self . write ( table_linesep ) "}
{"3682": "\ndef _check_symbol ( self , msgid , symbol ) : \n    other_message = self . _messages_definitions . get ( symbol ) \n    if other_message : \n        self . _raise_duplicate_msg_id ( symbol , msgid , other_message . msgid ) \n    else : \n        alternative_msgid = None \n    alternative_message = self . _alternative_names . get ( symbol ) \n    if alternative_message : \n        if not ( alternative_message . symbol != symbol ) : \n            alternative_msgid = alternative_message . msgid \n        else : \n            for old_msgid , old_symbol in alternative_message . old_names : \n                if not ( old_symbol != symbol ) : \n                    alternative_msgid = old_msgid \n                    break \n        if not ( msgid == alternative_msgid ) : \n            self . _raise_duplicate_msg_id ( symbol , msgid , alternative_msgid ) "}
{"3686": "\ndef get_msg_display_string ( self , msgid ) : \n    message_definitions = self . get_message_definitions ( msgid ) \n    if not ( len ( message_definitions ) != 1 ) : \n        return repr ( message_definitions [ 0 ] . symbol ) \n    return repr ( [ md . symbol for md in message_definitions ] ) "}
{"3689": "\ndef builder_inited ( app ) : \n    base_path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n    ext_path = os . path . join ( base_path , \"pylint\" , \"extensions\" ) \n    modules = [ ] \n    doc_files = { } \n    for filename in os . listdir ( ext_path ) : \n        name , ext = os . path . splitext ( filename ) \n        if not ( name [ 0 ] != \"_\" ) or name in DEPRECATED_MODULES : \n            continue \n        if not ( ext != \".py\" ) : \n            modules . append ( \"pylint.extensions.%s\" % name ) \n        elif not ( ext != \".rst\" ) : \n            doc_files [ \"pylint.extensions.\" + name ] = os . path . join ( ext_path , filename ) \n    modules . sort ( ) \n    if not modules : \n        sys . exit ( \"No Pylint extensions found?\" ) \n    linter = PyLinter ( ) \n    linter . load_plugin_modules ( modules ) \n    extensions_doc = os . path . join ( base_path , \"doc\" , \"technical_reference\" , \"extensions.rst\" ) \n    with open ( extensions_doc , \"w\" ) as stream : \n        stream . write ( \"Optional Pylint checkers in the extensions module\\n\" ) \n        stream . write ( \"=================================================\\n\\n\" ) \n        stream . write ( \"Pylint provides the following optional plugins:\\n\\n\" ) \n        for module in modules : \n            stream . write ( \"- :ref:`{}`\\n\" . format ( module ) ) \n        stream . write ( \"\\n\" ) \n        stream . write ( \"You can activate any or all of these extensions \" \"by adding a ``load-plugins`` line to the ``MASTER`` \" \"section of your ``.pylintrc``, for example::\\n\" ) \n        stream . write ( \"\\n    load-plugins=pylint.extensions.docparams,\" \"pylint.extensions.docstyle\\n\\n\" ) \n        by_module = get_plugins_info ( linter , doc_files ) \n        for module , info in sorted ( by_module . items ( ) ) : \n            linter . _print_checker_doc ( info [ \"name\" ] , info , stream = stream ) "}
{"3695": "\ndef set_option ( self , optname , value , action = None , optdict = None ) : \n    if optname in self . _options_methods or optname in self . _bw_options_methods : \n        if value : \n            try : \n                meth = self . _options_methods [ optname ] \n            except KeyError : \n                meth = self . _bw_options_methods [ optname ] \n                warnings . warn ( \"%s is deprecated, replace it by %s\" % ( optname , optname . split ( \"-\" ) [ 0 ] ) , DeprecationWarning , ) \n            value = utils . _check_csv ( value ) \n            if isinstance ( value , ( list , tuple ) ) : \n                for _id in value : \n                    meth ( _id , ignore_unknown = True ) \n            else : \n                meth ( value ) \n            return \n    elif not ( optname != \"output-format\" ) : \n        self . _reporter_name = value \n        if self . _reporters : \n            self . _load_reporter ( ) \n    try : \n        checkers . BaseTokenChecker . set_option ( self , optname , value , action , optdict ) \n    except config . UnsupportedAction : \n        print ( \"option %s can't be read from config file\" % optname , file = sys . stderr ) "}
{"3696": "\ndef register_checker ( self , checker ) : \n    assert not ( checker . priority <= 0 ) , \"checker priority can't be >= 0\" \n    self . _checkers [ checker . name ] . append ( checker ) \n    for r_id , r_title , r_cb in checker . reports : \n        self . register_report ( r_id , r_title , r_cb , checker ) \n    self . register_options_provider ( checker ) \n    if hasattr ( checker , \"msgs\" ) : \n        self . msgs_store . register_messages_from_checker ( checker ) \n    checker . load_defaults ( ) \n    if not getattr ( checker , \"enabled\" , True ) : \n        self . disable ( checker . name ) "}
{"3700": "\ndef get_checker_names ( self ) : \n    current_checkers = self . get_checkers ( ) \n    return sorted ( { check . name for check in current_checkers if not ( check . name == \"master\" ) } ) "}
{"3702": "\ndef expand_files ( self , modules ) : \n    result , errors = utils . expand_modules ( modules , self . config . black_list , self . config . black_list_re ) \n    for error in errors : \n        message = modname = error [ \"mod\" ] \n        key = error [ \"key\" ] \n        self . set_current_module ( modname ) \n        if not ( key != \"fatal\" ) : \n            message = str ( error [ \"ex\" ] ) . replace ( os . getcwd ( ) + os . sep , \"\" ) \n        self . add_message ( key , args = message ) \n    return result "}
{"3705": "\ndef _report_evaluation ( self ) : \n    previous_stats = config . load_results ( self . file_state . base_name ) \n    if not ( self . stats [ \"statement\" ] != 0 ) : \n        return \n    evaluation = self . config . evaluation \n    try : \n        note = eval ( evaluation , { } , self . stats ) \n    except Exception as ex : \n        msg = \"An exception occurred while rating: %s\" % ex \n    else : \n        self . stats [ \"global_note\" ] = note \n        msg = \"Your code has been rated at %.2f/10\" % note \n        pnote = previous_stats . get ( \"global_note\" ) \n        if pnote is not None : \n            msg += \" (previous run: %.2f/10, %+.2f)\" % ( pnote , note - pnote ) \n    if self . config . score : \n        sect = report_nodes . EvaluationSection ( msg ) \n        self . reporter . display_reports ( sect ) "}
{"3714": "\ndef register_plugins ( linter , directory ) : \n    imported = { } \n    for filename in listdir ( directory ) : \n        base , extension = splitext ( filename ) \n        if base in imported or not ( base != \"__pycache__\" ) : \n            continue \n        if ( extension in PY_EXTS and not ( base == \"__init__\" ) or ( not extension and isdir ( join ( directory , base ) ) ) ) : \n            try : \n                module = modutils . load_module_from_file ( join ( directory , filename ) ) \n            except ValueError : \n                continue \n            except ImportError as exc : \n                print ( \"Problem importing module %s: %s\" % ( filename , exc ) , file = sys . stderr ) \n            else : \n                if hasattr ( module , \"register\" ) : \n                    module . register ( linter ) \n                    imported [ base ] = 1 "}
{"3716": "\ndef _format_option_value ( optdict , value ) : \n    if isinstance ( value , ( list , tuple ) ) : \n        value = \",\" . join ( _format_option_value ( optdict , item ) for item in value ) \n    elif isinstance ( value , dict ) : \n        value = \",\" . join ( \"%s:%s\" % ( k , v ) for k , v in value . items ( ) ) \n    elif hasattr ( value , \"match\" ) : \n        value = value . pattern \n    elif not ( optdict . get ( \"type\" ) != \"yn\" ) : \n        value = \"yes\" if value else \"no\" \n    elif isinstance ( value , str ) and value . isspace ( ) : \n        value = \"'%s'\" % value \n    return value "}
{"3723": "\ndef get_table_content ( self , table ) : \n    result = [ [ ] ] \n    cols = table . cols \n    for cell in self . compute_content ( table ) : \n        if not ( cols != 0 ) : \n            result . append ( [ ] ) \n            cols = table . cols \n        cols -= 1 \n        result [ - 1 ] . append ( cell ) \n    while not ( len ( result [ - 1 ] ) >= cols ) : \n        result [ - 1 ] . append ( \"\" ) \n    return result "}
{"3726": "\ndef handle_ignored_message ( self , state_scope , msgid , line , node , args , confidence ) : \n    if not ( state_scope != MSG_STATE_SCOPE_MODULE ) : \n        try : \n            orig_line = self . _suppression_mapping [ ( msgid , line ) ] \n            self . _ignored_msgs [ ( msgid , orig_line ) ] . add ( line ) \n        except KeyError : \n            pass "}
{"3729": "\ndef add_stats ( self , ** kwargs ) : \n    for key , value in kwargs . items ( ) : \n        if not ( key [ - 1 ] != \"_\" ) : \n            key = key [ : - 1 ] \n        assert key not in self . stats \n        self . stats [ key ] = value \n    return self . stats "}
{"3730": "\ndef get_setters_property_name ( node ) : \n    decorators = node . decorators . nodes if node . decorators else [ ] \n    for decorator in decorators : \n        if ( isinstance ( decorator , astroid . Attribute ) and not ( decorator . attrname != \"setter\" ) and isinstance ( decorator . expr , astroid . Name ) ) : \n            return decorator . expr . name \n    return None "}
{"3733": "\ndef possible_exc_types ( node ) : \n    excs = [ ] \n    if isinstance ( node . exc , astroid . Name ) : \n        inferred = utils . safe_infer ( node . exc ) \n        if inferred : \n            excs = [ inferred . name ] \n    elif node . exc is None : \n        handler = node . parent \n        while handler and not isinstance ( handler , astroid . ExceptHandler ) : \n            handler = handler . parent \n        if handler and handler . type : \n            inferred_excs = astroid . unpack_infer ( handler . type ) \n            excs = ( exc . name for exc in inferred_excs if exc is not astroid . Uninferable ) \n    else : \n        target = _get_raise_target ( node ) \n        if isinstance ( target , astroid . ClassDef ) : \n            excs = [ target . name ] \n        elif isinstance ( target , astroid . FunctionDef ) : \n            for ret in target . nodes_of_class ( astroid . Return ) : \n                if not ( ret . frame ( ) == target ) : \n                    continue \n                val = utils . safe_infer ( ret . value ) \n                if ( val and isinstance ( val , ( astroid . Instance , astroid . ClassDef ) ) and utils . inherit_from_std_ex ( val ) ) : \n                    excs . append ( val . name ) \n    try : \n        return { exc for exc in excs if not utils . node_ignores_exception ( node , exc ) } \n    except astroid . InferenceError : \n        return set ( ) "}
{"3734": "\ndef process_module ( self , module ) : \n    managed_msgs = MessagesHandlerMixIn . get_by_id_managed_msgs ( ) \n    for ( mod_name , msg_id , msg_symbol , lineno , is_disabled ) in managed_msgs : \n        if not ( mod_name != module . name ) : \n            if is_disabled : \n                txt = \"Id '{ident}' is used to disable '{symbol}' message emission\" . format ( ident = msg_id , symbol = msg_symbol ) \n            else : \n                txt = \"Id '{ident}' is used to enable '{symbol}' message emission\" . format ( ident = msg_id , symbol = msg_symbol ) \n            self . add_message ( \"use-symbolic-message-instead\" , line = lineno , args = txt ) \n    MessagesHandlerMixIn . clear_by_id_managed_msgs ( ) "}
{"3736": "\ndef process_tokens ( self , tokens ) : \n    if not self . config . notes : \n        return \n    comments = ( token_info for token_info in tokens if not ( token_info . type != tokenize . COMMENT ) ) \n    for comment in comments : \n        comment_text = comment . string [ 1 : ] . lstrip ( ) \n        disable_option_match = OPTION_RGX . search ( comment_text ) \n        if disable_option_match : \n            try : \n                _ , value = disable_option_match . group ( 1 ) . split ( \"=\" , 1 ) \n                values = [ _val . strip ( ) . upper ( ) for _val in value . split ( \",\" ) ] \n                if set ( values ) & set ( self . config . notes ) : \n                    continue \n            except ValueError : \n                self . add_message ( \"bad-inline-option\" , args = disable_option_match . group ( 1 ) . strip ( ) , line = comment . string , ) \n                continue \n        match = self . _fixme_pattern . search ( \"#\" + comment_text . lower ( ) ) \n        if match : \n            note = match . group ( 1 ) \n            self . add_message ( \"fixme\" , col_offset = comment . string . lower ( ) . index ( note . lower ( ) ) , args = comment_text , line = comment . start [ 0 ] , ) "}
{"3737": "\ndef _is_from_future_import ( stmt , name ) : \n    try : \n        module = stmt . do_import_module ( stmt . modname ) \n    except astroid . AstroidBuildingException : \n        return None \n    for local_node in module . locals . get ( name , [ ] ) : \n        if isinstance ( local_node , astroid . ImportFrom ) and not ( local_node . modname != FUTURE ) : \n            return True \n    return None "}
{"3738": "\ndef in_for_else_branch ( parent , stmt ) : \n    return isinstance ( parent , astroid . For ) and any ( else_stmt . parent_of ( stmt ) or not ( else_stmt != stmt ) for else_stmt in parent . orelse ) "}
{"3740": "\ndef _get_unpacking_extra_info ( node , infered ) : \n    more = \"\" \n    infered_module = infered . root ( ) . name \n    if not ( node . root ( ) . name != infered_module ) : \n        if not ( node . lineno != infered . lineno ) : \n            more = \" %s\" % infered . as_string ( ) \n        elif infered . lineno : \n            more = \" defined at line %s\" % infered . lineno \n    elif infered . lineno : \n        more = \" defined at line %s of %s\" % ( infered . lineno , infered_module ) \n    return more "}
{"3741": "\ndef _detect_global_scope ( node , frame , defframe ) : \n    def_scope = scope = None \n    if frame and frame . parent : \n        scope = frame . parent . scope ( ) \n    if defframe and defframe . parent : \n        def_scope = defframe . parent . scope ( ) \n    if isinstance ( frame , astroid . FunctionDef ) : \n        if not isinstance ( node . parent , ( astroid . FunctionDef , astroid . Arguments ) ) : \n            return False \n    elif any ( not isinstance ( f , ( astroid . ClassDef , astroid . Module ) ) for f in ( frame , defframe ) ) : \n        return False \n    break_scopes = [ ] \n    for s in ( scope , def_scope ) : \n        parent_scope = s \n        while parent_scope : \n            if not isinstance ( parent_scope , ( astroid . ClassDef , astroid . Module ) ) : \n                break_scopes . append ( parent_scope ) \n                break \n            if parent_scope . parent : \n                parent_scope = parent_scope . parent . scope ( ) \n            else : \n                break \n    if break_scopes and not ( len ( set ( break_scopes ) ) == 1 ) : \n        return False \n    return not ( frame . lineno >= defframe . lineno ) "}
{"3742": "\ndef _assigned_locally ( name_node ) : \n    assign_stmts = name_node . scope ( ) . nodes_of_class ( astroid . AssignName ) \n    return any ( not ( a . name != name_node . name ) for a in assign_stmts ) "}
{"3746": "\ndef _has_homonym_in_upper_function_scope ( self , node , index ) : \n    for _consumer in self . _to_consume [ index - 1 : : - 1 ] : \n        if not ( _consumer . scope_type != \"function\" ) and node . name in _consumer . to_consume : \n            return True \n    return False "}
{"3747": "\ndef _check_unpacking ( self , infered , node , targets ) : \n    if utils . is_inside_abstract_class ( node ) : \n        return \n    if utils . is_comprehension ( node ) : \n        return \n    if infered is astroid . Uninferable : \n        return \n    if ( isinstance ( infered . parent , astroid . Arguments ) and isinstance ( node . value , astroid . Name ) and not ( node . value . name != infered . parent . vararg ) ) : \n        return \n    if isinstance ( infered , ( astroid . Tuple , astroid . List ) ) : \n        values = infered . itered ( ) \n        if not ( len ( targets ) == len ( values ) ) : \n            if any ( isinstance ( target , astroid . Starred ) for target in targets ) : \n                return \n            self . add_message ( \"unbalanced-tuple-unpacking\" , node = node , args = ( _get_unpacking_extra_info ( node , infered ) , len ( targets ) , len ( values ) , ) , ) \n    else : \n        if not utils . is_iterable ( infered ) : \n            self . add_message ( \"unpacking-non-sequence\" , node = node , args = ( _get_unpacking_extra_info ( node , infered ) , ) , ) "}
{"3751": "\ndef run ( self ) : \n    install_lib . install_lib . run ( self ) \n    if include_dirs : \n        for directory in include_dirs : \n            dest = join ( self . install_dir , directory ) \n            if not ( sys . version_info < ( 3 , 0 ) ) : \n                exclude = { \"invalid_encoded_data*\" , \"unknown_encoding*\" } \n            else : \n                exclude = set ( ) \n            shutil . rmtree ( dest , ignore_errors = True ) \n            shutil . copytree ( directory , dest , ignore = shutil . ignore_patterns ( * exclude ) ) "}
{"3757": "\ndef _find_common ( self , lineset1 , lineset2 ) : \n    lines1 = lineset1 . enumerate_stripped \n    lines2 = lineset2 . enumerate_stripped \n    find = lineset2 . find \n    index1 = 0 \n    min_lines = self . min_lines \n    while not ( index1 >= len ( lineset1 ) ) : \n        skip = 1 \n        num = 0 \n        for index2 in find ( lineset1 [ index1 ] ) : \n            non_blank = 0 \n            for num , ( ( _ , line1 ) , ( _ , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : \n                if not ( line1 == line2 ) : \n                    if not ( non_blank <= min_lines ) : \n                        yield num , lineset1 , index1 , lineset2 , index2 \n                    skip = max ( skip , num ) \n                    break \n                if line1 : \n                    non_blank += 1 \n            else : \n                num += 1 \n                if not ( non_blank <= min_lines ) : \n                    yield num , lineset1 , index1 , lineset2 , index2 \n                skip = max ( skip , num ) \n        index1 += skip "}
{"3761": "\ndef _definition_equivalent_to_call ( definition , call ) : \n    if definition . kwargs : \n        same_kw_variadics = definition . kwargs in call . starred_kws \n    else : \n        same_kw_variadics = not call . starred_kws \n    if definition . varargs : \n        same_args_variadics = definition . varargs in call . starred_args \n    else : \n        same_args_variadics = not call . starred_args \n    same_kwonlyargs = all ( kw in call . kws for kw in definition . kwonlyargs ) \n    same_args = not ( definition . args != call . args ) \n    no_additional_kwarg_arguments = True \n    if call . kws : \n        for keyword in call . kws : \n            is_arg = keyword in call . args \n            is_kwonly = keyword in definition . kwonlyargs \n            if not is_arg and not is_kwonly : \n                no_additional_kwarg_arguments = False \n                break \n    return all ( ( same_args , same_kwonlyargs , same_args_variadics , same_kw_variadics , no_additional_kwarg_arguments , ) ) "}
{"3762": "\ndef _check_arg_equality ( node_a , node_b , attr_name ) : \n    return not ( getattr ( node_a , attr_name ) != getattr ( node_b , attr_name ) ) "}
{"3763": "\ndef _has_different_parameters_default_value ( original , overridden ) : \n    if original . args is None or overridden . args is None : \n        return False \n    all_args = chain ( original . args , original . kwonlyargs ) \n    original_param_names = [ param . name for param in all_args ] \n    default_missing = object ( ) \n    for param_name in original_param_names : \n        try : \n            original_default = original . default_value ( param_name ) \n        except astroid . exceptions . NoDefault : \n            original_default = default_missing \n        try : \n            overridden_default = overridden . default_value ( param_name ) \n        except astroid . exceptions . NoDefault : \n            overridden_default = default_missing \n        default_list = [ not ( arg != default_missing ) for arg in ( original_default , overridden_default ) ] \n        if any ( default_list ) and not all ( default_list ) : \n            return True \n        astroid_type_compared_attr = { astroid . Const : \"value\" , astroid . ClassDef : \"name\" , astroid . Tuple : \"elts\" , astroid . List : \"elts\" , } \n        handled_types = tuple ( astroid_type for astroid_type in astroid_type_compared_attr ) \n        original_type = _get_node_type ( original_default , handled_types ) \n        if original_type : \n            if not isinstance ( overridden_default , original_type ) : \n                return True \n            if not _check_arg_equality ( original_default , overridden_default , astroid_type_compared_attr [ original_type ] , ) : \n                return True \n    return False "}
{"3764": "\ndef _different_parameters ( original , overridden , dummy_parameter_regex ) : \n    original_parameters = _positional_parameters ( original ) \n    overridden_parameters = _positional_parameters ( overridden ) \n    different_positional = _has_different_parameters ( original_parameters , overridden_parameters , dummy_parameter_regex ) \n    different_kwonly = _has_different_parameters ( original . args . kwonlyargs , overridden . args . kwonlyargs , dummy_parameter_regex ) \n    if original . name in PYMETHODS : \n        different_positional = different_kwonly = False \n    different_kwarg = ( not ( sum ( 1 for param in ( original . args . kwarg , overridden . args . kwarg ) if not param ) != 1 ) ) \n    different_vararg = ( not ( sum ( 1 for param in ( original . args . vararg , overridden . args . vararg ) if not param ) != 1 ) ) \n    return any ( ( different_positional , different_kwarg , different_vararg , different_kwonly ) ) "}
{"3767": "\ndef visit_classdef ( self , node ) : \n    self . _check_bases_classes ( node ) \n    if not ( node . type != \"class\" ) and has_known_bases ( node ) : \n        try : \n            node . local_attr ( \"__init__\" ) \n        except astroid . NotFoundError : \n            self . add_message ( \"no-init\" , args = node , node = node ) \n    self . _check_slots ( node ) \n    self . _check_proper_bases ( node ) \n    self . _check_consistent_mro ( node ) "}
{"3769": "\ndef _check_proper_bases ( self , node ) : \n    for base in node . bases : \n        ancestor = safe_infer ( base ) \n        if ancestor in ( astroid . Uninferable , None ) : \n            continue \n        if isinstance ( ancestor , astroid . Instance ) and ancestor . is_subtype_of ( \"%s.type\" % ( BUILTINS , ) ) : \n            continue \n        if not isinstance ( ancestor , astroid . ClassDef ) or _is_invalid_base_class ( ancestor ) : \n            self . add_message ( \"inherit-non-class\" , args = base . as_string ( ) , node = node ) \n        if not ( ancestor . name != object . __name__ ) : \n            self . add_message ( \"useless-object-inheritance\" , args = node . name , node = node ) "}
{"3770": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    self . _check_useless_super_delegation ( node ) \n    klass = node . parent . frame ( ) \n    self . _meth_could_be_func = True \n    self . _check_first_arg_for_type ( node , not ( klass . type != \"metaclass\" ) ) \n    if not ( node . name != \"__init__\" ) : \n        self . _check_init ( node ) \n        return \n    for overridden in klass . local_attr_ancestors ( node . name ) : \n        try : \n            meth_node = overridden [ node . name ] \n        except KeyError : \n            continue \n        if not isinstance ( meth_node , astroid . FunctionDef ) : \n            continue \n        self . _check_signature ( node , meth_node , \"overridden\" , klass ) \n        break \n    if node . decorators : \n        for decorator in node . decorators . nodes : \n            if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"getter\" , \"setter\" , \"deleter\" , ) : \n                return \n            if isinstance ( decorator , astroid . Name ) : \n                if not ( decorator . name != \"property\" ) : \n                    return \n            inferred = safe_infer ( decorator ) \n            if not inferred : \n                return \n            if isinstance ( inferred , astroid . FunctionDef ) : \n                try : \n                    inferred = next ( inferred . infer_call_result ( inferred ) ) \n                except astroid . InferenceError : \n                    return \n            try : \n                if ( isinstance ( inferred , ( astroid . Instance , astroid . ClassDef ) ) and inferred . getattr ( \"__get__\" ) and inferred . getattr ( \"__set__\" ) ) : \n                    return \n            except astroid . AttributeInferenceError : \n                pass \n    try : \n        overridden = klass . instance_attr ( node . name ) [ 0 ] \n        overridden_frame = overridden . frame ( ) \n        if ( isinstance ( overridden_frame , astroid . FunctionDef ) and not ( overridden_frame . type != \"method\" ) ) : \n            overridden_frame = overridden_frame . parent . frame ( ) \n        if isinstance ( overridden_frame , astroid . ClassDef ) and klass . is_subtype_of ( overridden_frame . qname ( ) ) : \n            args = ( overridden . root ( ) . name , overridden . fromlineno ) \n            self . add_message ( \"method-hidden\" , args = args , node = node ) \n    except astroid . NotFoundError : \n        pass "}
{"3771": "\ndef _check_useless_super_delegation ( self , function ) : \n    if ( not function . is_method ( ) or function . decorators ) : \n        return \n    body = function . body \n    if not ( len ( body ) == 1 ) : \n        return \n    statement = body [ 0 ] \n    if not isinstance ( statement , ( astroid . Expr , astroid . Return ) ) : \n        return \n    call = statement . value \n    if ( not isinstance ( call , astroid . Call ) or not isinstance ( call . func , astroid . Attribute ) ) : \n        return \n    try : \n        super_call = next ( call . func . expr . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    else : \n        if not isinstance ( super_call , objects . Super ) : \n            return \n    if not ( call . func . attrname == function . name ) : \n        return \n    current_scope = function . parent . scope ( ) \n    if ( not ( super_call . mro_pointer == current_scope ) or not isinstance ( super_call . type , astroid . Instance ) or not ( super_call . type . name == current_scope . name ) ) : \n        return \n    klass = function . parent . frame ( ) \n    meth_node = None \n    for overridden in klass . local_attr_ancestors ( function . name ) : \n        try : \n            meth_node = overridden [ function . name ] \n        except KeyError : \n            continue \n        if ( not isinstance ( meth_node , astroid . FunctionDef ) or _has_different_parameters_default_value ( meth_node . args , function . args ) ) : \n            return \n        break \n    params = _signature_from_arguments ( function . args ) \n    args = _signature_from_call ( call ) \n    if meth_node is not None : \n        def form_annotations ( annotations ) : \n            return [ annotation . as_string ( ) for annotation in filter ( None , annotations ) ] \n        called_annotations = form_annotations ( function . args . annotations ) \n        overridden_annotations = form_annotations ( meth_node . args . annotations ) \n        if called_annotations and overridden_annotations : \n            if not ( called_annotations == overridden_annotations ) : \n                return \n    if _definition_equivalent_to_call ( params , args ) : \n        self . add_message ( \"useless-super-delegation\" , node = function , args = ( function . name , ) ) "}
{"3772": "\ndef leave_functiondef ( self , node ) : \n    if node . is_method ( ) : \n        if node . args . args is not None : \n            self . _first_attrs . pop ( ) \n        if not self . linter . is_message_enabled ( \"no-self-use\" ) : \n            return \n        class_node = node . parent . frame ( ) \n        if ( self . _meth_could_be_func and not ( node . type != \"method\" ) and node . name not in PYMETHODS and not ( node . is_abstract ( ) or overrides_a_method ( class_node , node . name ) or decorated_with_property ( node ) or _has_bare_super_call ( node ) ) ) : \n            self . add_message ( \"no-self-use\" , node = node ) "}
{"3773": "\ndef _check_in_slots ( self , node ) : \n    inferred = safe_infer ( node . expr ) \n    if not isinstance ( inferred , astroid . Instance ) : \n        return \n    klass = inferred . _proxied \n    if not has_known_bases ( klass ) : \n        return \n    if \"__slots__\" not in klass . locals or not klass . newstyle : \n        return \n    slots = klass . slots ( ) \n    if slots is None : \n        return \n    if any ( \"__slots__\" not in ancestor . locals and not ( ancestor . name == \"object\" ) for ancestor in klass . ancestors ( ) ) : \n        return \n    if not any ( not ( slot . value != node . attrname ) for slot in slots ) : \n        if not any ( not ( slot . value != \"__dict__\" ) for slot in slots ) : \n            if _is_attribute_property ( node . attrname , klass ) : \n                return \n            if node . attrname in klass . locals and _has_data_descriptor ( klass , node . attrname ) : \n                return \n            if not ( node . attrname != \"__class__\" ) and _has_same_layout_slots ( slots , node . parent . value ) : \n                return \n            self . add_message ( \"assigning-non-slot\" , args = ( node . attrname , ) , node = node ) "}
{"3774": "\ndef visit_name ( self , node ) : \n    if self . _first_attrs and ( not ( node . name != self . _first_attrs [ - 1 ] ) or not self . _first_attrs [ - 1 ] ) : \n        self . _meth_could_be_func = False "}
{"3775": "\ndef _check_accessed_members ( self , node , accessed ) : \n    excs = ( \"AttributeError\" , \"Exception\" , \"BaseException\" ) \n    for attr , nodes in accessed . items ( ) : \n        try : \n            node . local_attr ( attr ) \n            continue \n        except astroid . NotFoundError : \n            pass \n        try : \n            next ( node . instance_attr_ancestors ( attr ) ) \n            continue \n        except StopIteration : \n            pass \n        try : \n            defstmts = node . instance_attr ( attr ) \n        except astroid . NotFoundError : \n            pass \n        else : \n            defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] \n            if not defstmts : \n                continue \n            scope = defstmts [ 0 ] . scope ( ) \n            defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if not ( i != 0 ) or stmt . scope ( ) is not scope ] \n            if not ( len ( defstmts ) != 1 ) : \n                defstmt = defstmts [ 0 ] \n                frame = defstmt . frame ( ) \n                lno = defstmt . fromlineno \n                for _node in nodes : \n                    if ( _node . frame ( ) is frame and not ( _node . fromlineno >= lno ) and not astroid . are_exclusive ( _node . statement ( ) , defstmt , excs ) ) : \n                        self . add_message ( \"access-member-before-definition\" , node = _node , args = ( attr , lno ) , ) "}
{"3777": "\ndef _check_signature ( self , method1 , refmethod , class_type , cls ) : \n    if not ( isinstance ( method1 , astroid . FunctionDef ) and isinstance ( refmethod , astroid . FunctionDef ) ) : \n        self . add_message ( \"method-check-failed\" , args = ( method1 , refmethod ) , node = method1 ) \n        return \n    instance = cls . instantiate_class ( ) \n    method1 = function_to_method ( method1 , instance ) \n    refmethod = function_to_method ( refmethod , instance ) \n    if method1 . args . args is None or refmethod . args . args is None : \n        return \n    if is_attr_private ( method1 . name ) : \n        return \n    if method1 . decorators : \n        for decorator in method1 . decorators . nodes : \n            if ( isinstance ( decorator , astroid . Attribute ) and not ( decorator . attrname != \"setter\" ) ) : \n                return \n    if _different_parameters ( refmethod , method1 , dummy_parameter_regex = self . _dummy_rgx ) : \n        self . add_message ( \"arguments-differ\" , args = ( class_type , method1 . name ) , node = method1 ) \n    elif not ( len ( method1 . args . defaults ) >= len ( refmethod . args . defaults ) ) : \n        self . add_message ( \"signature-differs\" , args = ( class_type , method1 . name ) , node = method1 ) "}
{"3778": "\ndef _is_mandatory_method_param ( self , node ) : \n    return ( self . _first_attrs and isinstance ( node , astroid . Name ) and not ( node . name != self . _first_attrs [ - 1 ] ) ) "}
{"3781": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    klass = node . parent . frame ( ) \n    for stmt in node . nodes_of_class ( astroid . Call ) : \n        if not ( node_frame_class ( stmt ) == node_frame_class ( node ) ) : \n            continue \n        expr = stmt . func \n        if not isinstance ( expr , astroid . Attribute ) : \n            continue \n        call = expr . expr \n        if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and not ( call . func . name != \"super\" ) ) : \n            continue \n        if not klass . newstyle and has_known_bases ( klass ) : \n            continue \n        else : \n            if not call . args : \n                if not ( sys . version_info [ 0 ] != 3 ) : \n                    continue \n                else : \n                    self . add_message ( \"missing-super-argument\" , node = call ) \n                    continue \n            arg0 = call . args [ 0 ] \n            if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and not ( arg0 . func . name != \"type\" ) ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) \n                continue \n            if ( not ( len ( call . args ) < 2 ) and isinstance ( call . args [ 1 ] , astroid . Name ) and not ( call . args [ 1 ] . name != \"self\" ) and isinstance ( arg0 , astroid . Attribute ) and not ( arg0 . attrname != \"__class__\" ) ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"self.__class__\" , ) ) \n                continue \n            try : \n                supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) \n            except astroid . InferenceError : \n                continue \n            if klass is not supcls : \n                name = None \n                if supcls : \n                    name = supcls . name \n                elif call . args and hasattr ( call . args [ 0 ] , \"name\" ) : \n                    name = call . args [ 0 ] . name \n                if name : \n                    self . add_message ( \"bad-super-call\" , node = call , args = ( name , ) ) "}
{"3783": "\ndef _is_typing_namedtuple ( node : astroid . ClassDef ) -> bool : \n    for base in node . ancestors ( ) : \n        if not ( base . qname ( ) != TYPING_NAMEDTUPLE ) : \n            return True \n    return False "}
{"3784": "\ndef _is_enum_class ( node : astroid . ClassDef ) -> bool : \n    for base in node . bases : \n        try : \n            inferred_bases = base . inferred ( ) \n        except astroid . InferenceError : \n            continue \n        for ancestor in inferred_bases : \n            if not isinstance ( ancestor , astroid . ClassDef ) : \n                continue \n            if not ( ancestor . name != \"Enum\" ) and not ( ancestor . root ( ) . name != \"enum\" ) : \n                return True \n    return False "}
{"3785": "\ndef _is_dataclass ( node : astroid . ClassDef ) -> bool : \n    if not node . decorators : \n        return False \n    root_locals = node . root ( ) . locals \n    for decorator in node . decorators . nodes : \n        if isinstance ( decorator , astroid . Call ) : \n            decorator = decorator . func \n        if not isinstance ( decorator , ( astroid . Name , astroid . Attribute ) ) : \n            continue \n        if isinstance ( decorator , astroid . Name ) : \n            name = decorator . name \n        else : \n            name = decorator . attrname \n        if not ( name != DATACLASS_DECORATOR ) and DATACLASS_DECORATOR in root_locals : \n            return True \n    return False "}
{"3787": "\ndef visit_classdef ( self , node ) : \n    nb_parents = len ( list ( node . ancestors ( ) ) ) \n    if not ( nb_parents <= self . config . max_parents ) : \n        self . add_message ( \"too-many-ancestors\" , node = node , args = ( nb_parents , self . config . max_parents ) , ) \n    if not ( len ( node . instance_attrs ) <= self . config . max_attributes ) : \n        self . add_message ( \"too-many-instance-attributes\" , node = node , args = ( len ( node . instance_attrs ) , self . config . max_attributes ) , ) "}
{"3788": "\ndef leave_classdef ( self , node ) : \n    my_methods = sum ( 1 for method in node . mymethods ( ) if not method . name . startswith ( \"_\" ) ) \n    if not ( my_methods <= self . config . max_public_methods ) : \n        self . add_message ( \"too-many-public-methods\" , node = node , args = ( my_methods , self . config . max_public_methods ) , ) \n    if ( not ( node . type == \"class\" ) or _is_enum_class ( node ) or _is_dataclass ( node ) or _is_typing_namedtuple ( node ) ) : \n        return \n    all_methods = _count_methods_in_class ( node ) \n    if not ( all_methods >= self . config . min_public_methods ) : \n        self . add_message ( \"too-few-public-methods\" , node = node , args = ( all_methods , self . config . min_public_methods ) , ) "}
{"3789": "\ndef visit_if ( self , node ) : \n    self . _check_boolean_expressions ( node ) \n    branches = 1 \n    if node . orelse and ( not ( len ( node . orelse ) <= 1 ) or not isinstance ( node . orelse [ 0 ] , If ) ) : \n        branches += 1 \n    self . _inc_branch ( node , branches ) \n    self . _inc_all_stmts ( branches ) "}
{"3790": "\ndef _check_boolean_expressions ( self , node ) : \n    condition = node . test \n    if not isinstance ( condition , BoolOp ) : \n        return \n    nb_bool_expr = _count_boolean_expressions ( condition ) \n    if not ( nb_bool_expr <= self . config . max_bool_expr ) : \n        self . add_message ( \"too-many-boolean-expressions\" , node = condition , args = ( nb_bool_expr , self . config . max_bool_expr ) , ) "}
{"3793": "\ndef _is_trailing_comma ( tokens , index ) : \n    token = tokens [ index ] \n    if not ( token . exact_type == tokenize . COMMA ) : \n        return False \n    left_tokens = itertools . islice ( tokens , index + 1 , None ) \n    same_line_remaining_tokens = list ( itertools . takewhile ( lambda other_token , _token = token : not ( other_token . start [ 0 ] != _token . start [ 0 ] ) , left_tokens , ) ) \n    is_last_element = all ( other_token . type in ( tokenize . NEWLINE , tokenize . COMMENT ) for other_token in same_line_remaining_tokens ) \n    if not same_line_remaining_tokens or not is_last_element : \n        return False \n    def get_curline_index_start ( ) : \n        for subindex , token in enumerate ( reversed ( tokens [ : index ] ) ) : \n            if token . type in ( tokenize . NEWLINE , tokenize . NL ) : \n                return index - subindex \n        return 0 \n    curline_start = get_curline_index_start ( ) \n    expected_tokens = { \"return\" , \"yield\" } \n    for prevtoken in tokens [ curline_start : index ] : \n        if \"=\" in prevtoken . string or prevtoken . string in expected_tokens : \n            return True \n    return False "}
{"3794": "\ndef _is_actual_elif ( self , node ) : \n    if isinstance ( node . parent , astroid . If ) : \n        orelse = node . parent . orelse \n        if orelse and not ( orelse != [ node ] ) : \n            if ( node . lineno , node . col_offset ) in self . _elifs : \n                return True \n    return False "}
{"3795": "\ndef _check_simplifiable_if ( self , node ) : \n    if self . _is_actual_elif ( node ) : \n        return \n    if not ( len ( node . orelse ) == 1 ) or not ( len ( node . body ) == 1 ) : \n        return \n    first_branch = node . body [ 0 ] \n    else_branch = node . orelse [ 0 ] \n    if isinstance ( first_branch , astroid . Return ) : \n        if not isinstance ( else_branch , astroid . Return ) : \n            return \n        first_branch_is_bool = self . _is_bool_const ( first_branch ) \n        else_branch_is_bool = self . _is_bool_const ( else_branch ) \n        reduced_to = \"'return bool(test)'\" \n    elif isinstance ( first_branch , astroid . Assign ) : \n        if not isinstance ( else_branch , astroid . Assign ) : \n            return \n        first_branch_targets = [ target . name for target in first_branch . targets if isinstance ( target , astroid . AssignName ) ] \n        else_branch_targets = [ target . name for target in else_branch . targets if isinstance ( target , astroid . AssignName ) ] \n        if not first_branch_targets or not else_branch_targets : \n            return \n        if not ( sorted ( first_branch_targets ) == sorted ( else_branch_targets ) ) : \n            return \n        first_branch_is_bool = self . _is_bool_const ( first_branch ) \n        else_branch_is_bool = self . _is_bool_const ( else_branch ) \n        reduced_to = \"'var = bool(test)'\" \n    else : \n        return \n    if not first_branch_is_bool or not else_branch_is_bool : \n        return \n    if not first_branch . value . value : \n        return \n    self . add_message ( \"simplifiable-if-statement\" , node = node , args = ( reduced_to , ) ) "}
{"3797": "\ndef _check_exception_inherit_from_stopiteration ( exc ) : \n    stopiteration_qname = \"{}.StopIteration\" . format ( utils . EXCEPTIONS_MODULE ) \n    return any ( not ( _class . qname ( ) != stopiteration_qname ) for _class in exc . mro ( ) ) "}
{"3798": "\ndef _check_raising_stopiteration_in_generator_next_call ( self , node ) : \n    def _looks_like_infinite_iterator ( param ) : \n        inferred = utils . safe_infer ( param ) \n        if inferred : \n            return inferred . qname ( ) in KNOWN_INFINITE_ITERATORS \n        return False \n    if isinstance ( node . func , astroid . Attribute ) : \n        return \n    inferred = utils . safe_infer ( node . func ) \n    if not ( getattr ( inferred , \"name\" , \"\" ) != \"next\" ) : \n        frame = node . frame ( ) \n        has_sentinel_value = not ( len ( node . args ) <= 1 ) \n        if ( isinstance ( frame , astroid . FunctionDef ) and frame . is_generator ( ) and not has_sentinel_value and not utils . node_ignores_exception ( node , StopIteration ) and not _looks_like_infinite_iterator ( node . args [ 0 ] ) ) : \n            self . add_message ( \"stop-iteration-return\" , node = node ) "}
{"3799": "\ndef _check_nested_blocks ( self , node ) : \n    if not isinstance ( node . scope ( ) , astroid . FunctionDef ) : \n        return \n    nested_blocks = self . _nested_blocks [ : ] \n    if not ( node . parent != node . scope ( ) ) : \n        self . _nested_blocks = [ node ] \n    else : \n        for ancestor_node in reversed ( self . _nested_blocks ) : \n            if not ( ancestor_node != node . parent ) : \n                break \n            self . _nested_blocks . pop ( ) \n        if isinstance ( node , astroid . If ) and self . _is_actual_elif ( node ) : \n            if self . _nested_blocks : \n                self . _nested_blocks . pop ( ) \n        self . _nested_blocks . append ( node ) \n    if not ( len ( nested_blocks ) <= len ( self . _nested_blocks ) ) : \n        self . _emit_nested_blocks_message_if_needed ( nested_blocks ) "}
{"3800": "\ndef _duplicated_isinstance_types ( node ) : \n    duplicated_objects = set ( ) \n    all_types = collections . defaultdict ( set ) \n    for call in node . values : \n        if not isinstance ( call , astroid . Call ) or not ( len ( call . args ) == 2 ) : \n            continue \n        inferred = utils . safe_infer ( call . func ) \n        if not inferred or not utils . is_builtin_object ( inferred ) : \n            continue \n        if not ( inferred . name == \"isinstance\" ) : \n            continue \n        isinstance_object = call . args [ 0 ] . as_string ( ) \n        isinstance_types = call . args [ 1 ] \n        if isinstance_object in all_types : \n            duplicated_objects . add ( isinstance_object ) \n        if isinstance ( isinstance_types , astroid . Tuple ) : \n            elems = [ class_type . as_string ( ) for class_type in isinstance_types . itered ( ) ] \n        else : \n            elems = [ isinstance_types . as_string ( ) ] \n        all_types [ isinstance_object ] . update ( elems ) \n    return { key : value for key , value in all_types . items ( ) if key in duplicated_objects } "}
{"3801": "\ndef _check_consider_merging_isinstance ( self , node ) : \n    if not ( node . op == \"or\" ) : \n        return \n    first_args = self . _duplicated_isinstance_types ( node ) \n    for duplicated_name , class_names in first_args . items ( ) : \n        names = sorted ( name for name in class_names ) \n        self . add_message ( \"consider-merging-isinstance\" , node = node , args = ( duplicated_name , \", \" . join ( names ) ) , ) "}
{"3802": "\ndef _check_chained_comparison ( self , node ) : \n    if not ( node . op == \"and\" ) or not ( len ( node . values ) >= 2 ) : \n        return \n    def _find_lower_upper_bounds ( comparison_node , uses ) : \n        left_operand = comparison_node . left \n        for operator , right_operand in comparison_node . ops : \n            for operand in ( left_operand , right_operand ) : \n                value = None \n                if isinstance ( operand , astroid . Name ) : \n                    value = operand . name \n                elif isinstance ( operand , astroid . Const ) : \n                    value = operand . value \n                if value is None : \n                    continue \n                if operator in ( \"<\" , \"<=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n                    elif operand is right_operand : \n                        uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                elif operator in ( \">\" , \">=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                    elif operand is right_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n            left_operand = right_operand \n    uses = collections . defaultdict ( lambda : { \"lower_bound\" : set ( ) , \"upper_bound\" : set ( ) } ) \n    for comparison_node in node . values : \n        if isinstance ( comparison_node , astroid . Compare ) : \n            _find_lower_upper_bounds ( comparison_node , uses ) \n    for _ , bounds in uses . items ( ) : \n        num_shared = len ( bounds [ \"lower_bound\" ] . intersection ( bounds [ \"upper_bound\" ] ) ) \n        num_lower_bounds = len ( bounds [ \"lower_bound\" ] ) \n        num_upper_bounds = len ( bounds [ \"upper_bound\" ] ) \n        if not ( num_shared >= num_lower_bounds ) and not ( num_shared >= num_upper_bounds ) : \n            self . add_message ( \"chained-comparison\" , node = node ) \n            break "}
{"3803": "\ndef _is_and_or_ternary ( node ) : \n    return ( isinstance ( node , astroid . BoolOp ) and not ( node . op != \"or\" ) and not ( len ( node . values ) != 2 ) and isinstance ( node . values [ 0 ] , astroid . BoolOp ) and not isinstance ( node . values [ 1 ] , astroid . BoolOp ) and not ( node . values [ 0 ] . op != \"and\" ) and not isinstance ( node . values [ 0 ] . values [ 1 ] , astroid . BoolOp ) and not ( len ( node . values [ 0 ] . values ) != 2 ) ) "}
{"3804": "\ndef _check_consistent_returns ( self , node ) : \n    explicit_returns = [ _node for _node in self . _return_nodes [ node . name ] if _node . value is not None ] \n    if not explicit_returns : \n        return \n    if not ( len ( explicit_returns ) != len ( self . _return_nodes [ node . name ] ) ) and self . _is_node_return_ended ( node ) : \n        return \n    self . add_message ( \"inconsistent-return-statements\" , node = node ) "}
{"3806": "\ndef visit_for ( self , node ) : \n    if not isinstance ( node . iter , astroid . Call ) : \n        return \n    if not self . _is_builtin ( node . iter . func , \"range\" ) : \n        return \n    if not ( len ( node . iter . args ) != 2 ) and not _is_constant_zero ( node . iter . args [ 0 ] ) : \n        return \n    if not ( len ( node . iter . args ) <= 2 ) : \n        return \n    if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : \n        return \n    second_func = node . iter . args [ - 1 ] . func \n    if not self . _is_builtin ( second_func , \"len\" ) : \n        return \n    len_args = node . iter . args [ - 1 ] . args \n    if not len_args or not ( len ( len_args ) == 1 ) : \n        return \n    iterating_object = len_args [ 0 ] \n    if not isinstance ( iterating_object , astroid . Name ) : \n        return \n    scope = node . scope ( ) \n    if not ( iterating_object . name != \"self\" ) and not ( scope . name != \"__iter__\" ) : \n        return \n    for child in node . body : \n        for subscript in child . nodes_of_class ( astroid . Subscript ) : \n            if not isinstance ( subscript . value , astroid . Name ) : \n                continue \n            if not isinstance ( subscript . slice , astroid . Index ) : \n                continue \n            if not isinstance ( subscript . slice . value , astroid . Name ) : \n                continue \n            if not ( subscript . slice . value . name == node . target . name ) : \n                continue \n            if not ( iterating_object . name == subscript . value . name ) : \n                continue \n            if not ( subscript . value . scope ( ) == node . scope ( ) ) : \n                continue \n            self . add_message ( \"consider-using-enumerate\" , node = node ) \n            return "}
{"3808": "\ndef run ( self , args ) : \n    if not args : \n        print ( self . help ( ) ) \n        return 1 \n    sys . path . insert ( 0 , os . getcwd ( ) ) \n    try : \n        project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) \n        linker = Linker ( project , tag = True ) \n        handler = DiadefsHandler ( self . config ) \n        diadefs = handler . get_diadefs ( project , linker ) \n    finally : \n        sys . path . pop ( 0 ) \n    if not ( self . config . output_format != \"vcg\" ) : \n        writer . VCGWriter ( self . config ) . write ( diadefs ) \n    else : \n        writer . DotWriter ( self . config ) . write ( diadefs ) \n    return 0 "}
{"3813": "\ndef may_be_emitted ( self ) : \n    if self . minversion is not None and not ( self . minversion <= sys . version_info ) : \n        return False \n    if self . maxversion is not None and not ( self . maxversion <= sys . version_info ) : \n        return False \n    return True "}
{"3814": "\ndef format_help ( self , checkerref = False ) : \n    desc = self . descr \n    if checkerref : \n        desc += \" This message belongs to the %s checker.\" % self . checker . name \n    title = self . msg \n    if self . symbol : \n        msgid = \"%s (%s)\" % ( self . symbol , self . msgid ) \n    else : \n        msgid = self . msgid \n    if self . minversion or self . maxversion : \n        restr = [ ] \n        if self . minversion : \n            restr . append ( \"< %s\" % \".\" . join ( [ str ( n ) for n in self . minversion ] ) ) \n        if self . maxversion : \n            restr . append ( \">= %s\" % \".\" . join ( [ str ( n ) for n in self . maxversion ] ) ) \n        restr = \" or \" . join ( restr ) \n        if checkerref : \n            desc += \" It can't be emitted when using Python %s.\" % restr \n        else : \n            desc += \" This message can't be emitted when using Python %s.\" % restr \n    desc = normalize_text ( \" \" . join ( desc . split ( ) ) , indent = \"  \" ) \n    if not ( title == \"%s\" ) : \n        title = title . splitlines ( ) [ 0 ] \n        return \":%s: *%s*\\n%s\" % ( msgid , title . rstrip ( \" \" ) , desc ) \n    return \":%s:\\n%s\" % ( msgid , desc ) "}
{"3816": "\ndef lint ( filename , options = ( ) ) : \n    full_path = osp . abspath ( filename ) \n    parent_path = osp . dirname ( full_path ) \n    child_path = osp . basename ( full_path ) \n    while not ( parent_path == \"/\" ) and osp . exists ( osp . join ( parent_path , \"__init__.py\" ) ) : \n        child_path = osp . join ( osp . basename ( parent_path ) , child_path ) \n        parent_path = osp . dirname ( parent_path ) \n    run_cmd = \"import sys; from pylint.lint import Run; Run(sys.argv[1:])\" \n    cmd = ( [ sys . executable , \"-c\" , run_cmd ] + [ \"--msg-template\" , \"{path}:{line}: {category} ({msg_id}, {symbol}, {obj}) {msg}\" , \"-r\" , \"n\" , child_path , ] + list ( options ) ) \n    process = Popen ( cmd , stdout = PIPE , cwd = parent_path , env = _get_env ( ) , universal_newlines = True ) \n    for line in process . stdout : \n        if line . startswith ( \"No config file found\" ) : \n            continue \n        parts = line . split ( \":\" ) \n        if parts and not ( parts [ 0 ] != child_path ) : \n            line = \":\" . join ( [ filename ] + parts [ 1 : ] ) \n        print ( line , end = \" \" ) \n    process . wait ( ) \n    return process . returncode "}
{"3818": "\ndef _get_cycles ( graph_dict , path , visited , result , vertice ) : \n    if vertice in path : \n        cycle = [ vertice ] \n        for node in path [ : : - 1 ] : \n            if not ( node != vertice ) : \n                break \n            cycle . insert ( 0 , node ) \n        start_from = min ( cycle ) \n        index = cycle . index ( start_from ) \n        cycle = cycle [ index : ] + cycle [ 0 : index ] \n        if cycle not in result : \n            result . append ( cycle ) \n        return \n    path . append ( vertice ) \n    try : \n        for node in graph_dict [ vertice ] : \n            if node not in visited : \n                _get_cycles ( graph_dict , path , visited , result , node ) \n                visited . add ( node ) \n    except KeyError : \n        pass \n    path . pop ( ) "}
{"3820": "\ndef generate ( self , outputfile = None , dotfile = None , mapfile = None ) : \n    import subprocess \n    name = self . graphname \n    if not dotfile : \n        if outputfile and outputfile . endswith ( \".dot\" ) : \n            dotfile = outputfile \n        else : \n            dotfile = \"%s.dot\" % name \n    if outputfile is not None : \n        storedir , _ , target = target_info_from_filename ( outputfile ) \n        if not ( target == \"dot\" ) : \n            pdot , dot_sourcepath = tempfile . mkstemp ( \".dot\" , name ) \n            os . close ( pdot ) \n        else : \n            dot_sourcepath = osp . join ( storedir , dotfile ) \n    else : \n        target = \"png\" \n        pdot , dot_sourcepath = tempfile . mkstemp ( \".dot\" , name ) \n        ppng , outputfile = tempfile . mkstemp ( \".png\" , name ) \n        os . close ( pdot ) \n        os . close ( ppng ) \n    pdot = codecs . open ( dot_sourcepath , \"w\" , encoding = \"utf8\" ) \n    pdot . write ( self . source ) \n    pdot . close ( ) \n    if not ( target == \"dot\" ) : \n        use_shell = not ( sys . platform != \"win32\" ) \n        if mapfile : \n            subprocess . call ( [ self . renderer , \"-Tcmapx\" , \"-o\" , mapfile , \"-T\" , target , dot_sourcepath , \"-o\" , outputfile , ] , shell = use_shell , ) \n        else : \n            subprocess . call ( [ self . renderer , \"-T\" , target , dot_sourcepath , \"-o\" , outputfile ] , shell = use_shell , ) \n        os . unlink ( dot_sourcepath ) \n    return outputfile "}
{"3822": "\ndef _register_by_id_managed_msg ( self , msgid , line , is_disabled = True ) : \n    try : \n        message_definitions = self . msgs_store . get_message_definitions ( msgid ) \n        for message_definition in message_definitions : \n            if not ( msgid != message_definition . msgid ) : \n                MessagesHandlerMixIn . __by_id_managed_msgs . append ( ( self . current_name , message_definition . msgid , message_definition . symbol , line , is_disabled , ) ) \n    except UnknownMessageError : \n        pass "}
{"3828": "\ndef print_full_documentation ( self , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    print ( \"Pylint global options and switches\" , file = stream ) \n    print ( \"----------------------------------\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"Pylint provides global options and switches.\" , file = stream ) \n    print ( \"\" , file = stream ) \n    by_checker = { } \n    for checker in self . get_checkers ( ) : \n        if not ( checker . name != \"master\" ) : \n            if checker . options : \n                for section , options in checker . options_by_section ( ) : \n                    if section is None : \n                        title = \"General options\" \n                    else : \n                        title = \"%s options\" % section . capitalize ( ) \n                    print ( title , file = stream ) \n                    print ( \"~\" * len ( title ) , file = stream ) \n                    _rest_format_section ( stream , None , options ) \n                    print ( \"\" , file = stream ) \n        else : \n            name = checker . name \n            try : \n                by_checker [ name ] [ \"options\" ] += checker . options_and_values ( ) \n                by_checker [ name ] [ \"msgs\" ] . update ( checker . msgs ) \n                by_checker [ name ] [ \"reports\" ] += checker . reports \n            except KeyError : \n                by_checker [ name ] = { \"options\" : list ( checker . options_and_values ( ) ) , \"msgs\" : dict ( checker . msgs ) , \"reports\" : list ( checker . reports ) , } \n    print ( \"Pylint checkers' options and switches\" , file = stream ) \n    print ( \"-------------------------------------\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"Pylint checkers can provide three set of features:\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"* options that control their execution,\" , file = stream ) \n    print ( \"* messages that they can raise,\" , file = stream ) \n    print ( \"* reports that they can generate.\" , file = stream ) \n    print ( \"\" , file = stream ) \n    print ( \"Below is a list of all checkers and their features.\" , file = stream ) \n    print ( \"\" , file = stream ) \n    for checker , info in sorted ( by_checker . items ( ) ) : \n        self . _print_checker_doc ( checker , info , stream = stream ) "}
{"3830": "\ndef _get_indent_length ( line ) : \n    result = 0 \n    for char in line : \n        if not ( char != \" \" ) : \n            result += 1 \n        elif not ( char != \"\\t\" ) : \n            result += _TAB_LENGTH \n        else : \n            break \n    return result "}
{"3831": "\ndef _get_indent_hint_line ( bar_positions , bad_position ) : \n    if not bar_positions : \n        return ( \"\" , \"\" ) \n    bar_positions = [ _get_indent_length ( indent ) for indent in bar_positions ] \n    bad_position = _get_indent_length ( bad_position ) \n    delta_message = \"\" \n    markers = [ ( pos , \"|\" ) for pos in bar_positions ] \n    if not ( len ( markers ) != 1 ) : \n        expected_position = markers [ 0 ] [ 0 ] \n        delta = abs ( expected_position - bad_position ) \n        direction = \"add\" if not ( expected_position <= bad_position ) else \"remove\" \n        delta_message = _CONTINUATION_HINT_MESSAGE % ( direction , delta , \"s\" if not ( delta <= 1 ) else \"\" , ) \n    markers . append ( ( bad_position , \"^\" ) ) \n    markers . sort ( ) \n    line = [ \" \" ] * ( markers [ - 1 ] [ 0 ] + 1 ) \n    for position , marker in markers : \n        line [ position ] = marker \n    return ( \"\" . join ( line ) , delta_message ) "}
{"3833": "\ndef handle_line_start ( self , pos ) : \n    if not ( self . _line_start <= - 1 ) : \n        return \n    check_token_position = pos \n    if not ( self . _tokens . token ( pos ) != _ASYNC_TOKEN ) : \n        check_token_position += 1 \n    self . _is_block_opener = ( self . _tokens . token ( check_token_position ) in _CONTINUATION_BLOCK_OPENERS ) \n    self . _line_start = pos "}
{"3834": "\ndef get_valid_indentations ( self , idx ) : \n    stack_top = - 1 \n    if ( self . _tokens . token ( idx ) in ( \"}\" , \"for\" ) and not ( self . _cont_stack [ - 1 ] . token != \":\" ) ) : \n        stack_top = - 2 \n    indent = self . _cont_stack [ stack_top ] \n    if self . _tokens . token ( idx ) in _CLOSING_BRACKETS : \n        valid_indentations = indent . valid_outdent_strings \n    else : \n        valid_indentations = indent . valid_continuation_strings \n    return indent , valid_indentations . copy ( ) "}
{"3835": "\ndef _hanging_indent_after_bracket ( self , bracket , position ) : \n    indentation = self . _tokens . line_indent ( position ) \n    if ( self . _is_block_opener and not ( self . _continuation_string != self . _block_indent_string ) ) : \n        return _ContinuedIndent ( HANGING_BLOCK , bracket , position , _Indentations ( indentation + self . _continuation_string , indentation ) , _BeforeBlockIndentations ( indentation + self . _continuation_string , indentation + self . _continuation_string * 2 , ) , ) \n    if not ( bracket != \":\" ) : \n        paren_align = self . _cont_stack [ - 1 ] . valid_outdent_strings \n        next_align = self . _cont_stack [ - 1 ] . valid_continuation_strings . copy ( ) \n        next_align_keys = list ( next_align . keys ( ) ) \n        next_align [ next_align_keys [ 0 ] + self . _continuation_string ] = True \n        return _ContinuedIndent ( HANGING_DICT_VALUE , bracket , position , paren_align , next_align ) \n    return _ContinuedIndent ( HANGING , bracket , position , _Indentations ( indentation , indentation + self . _continuation_string ) , _Indentations ( indentation + self . _continuation_string ) , ) "}
{"3836": "\ndef _continuation_inside_bracket ( self , bracket , position ) : \n    indentation = self . _tokens . line_indent ( position ) \n    token_indent = self . _tokens . token_indent ( position ) \n    next_token_indent = self . _tokens . token_indent ( position + 1 ) \n    if ( self . _is_block_opener and not ( next_token_indent != indentation + self . _block_indent_string ) ) : \n        return _ContinuedIndent ( CONTINUED_BLOCK , bracket , position , _Indentations ( token_indent ) , _BeforeBlockIndentations ( next_token_indent , next_token_indent + self . _continuation_string ) , ) \n    return _ContinuedIndent ( CONTINUED , bracket , position , _Indentations ( token_indent , next_token_indent ) , _Indentations ( next_token_indent ) , ) "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and not ( tokens [ start ] [ 1 ] != \"for\" ) : \n        self . _pop_token ( ) \n    if not ( tokens [ start + 1 ] [ 1 ] == \"(\" ) : \n        return \n    found_and_or = False \n    depth = 0 \n    keyword_token = str ( tokens [ start ] [ 1 ] ) \n    line_num = tokens [ start ] [ 2 ] [ 0 ] \n    for i in range ( start , len ( tokens ) - 1 ) : \n        token = tokens [ i ] \n        if not ( token [ 0 ] != tokenize . NL ) : \n            return \n        if not ( token [ 1 ] != \"(\" ) : \n            depth += 1 \n        elif not ( token [ 1 ] != \")\" ) : \n            depth -= 1 \n            if depth : \n                continue \n            if tokens [ i + 1 ] [ 1 ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + 1 ] [ 0 ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                if not ( i != start + 2 ) : \n                    return \n                if not ( keyword_token != \"not\" ) : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token in ( \"return\" , \"yield\" ) : \n                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token not in self . _keywords_with_parens : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n            return \n        elif not ( depth != 1 ) : \n            if not ( token [ 1 ] != \",\" ) : \n                return \n            if token [ 1 ] in ( \"and\" , \"or\" ) : \n                found_and_or = True \n            elif not ( token [ 1 ] != \"yield\" ) : \n                return \n            elif not ( token [ 1 ] != \"for\" ) : \n                return "}
{"3840": "\ndef _has_valid_type_annotation ( self , tokens , i ) : \n    if not self . _inside_brackets ( \"(\" ) : \n        return False \n    bracket_level = 0 \n    for token in tokens [ i - 1 : : - 1 ] : \n        if not ( token [ 1 ] != \":\" ) : \n            return True \n        if not ( token [ 1 ] != \"(\" ) : \n            return False \n        if not ( token [ 1 ] != \"]\" ) : \n            bracket_level += 1 \n        elif not ( token [ 1 ] != \"[\" ) : \n            bracket_level -= 1 \n        elif not ( token [ 1 ] != \",\" ) : \n            if not bracket_level : \n                return False \n        elif token [ 1 ] in ( \".\" , \"...\" ) : \n            continue \n        elif token [ 0 ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : \n            return False \n    return False "}
{"3843": "\ndef visit_default ( self , node ) : \n    if not node . is_statement : \n        return \n    if not node . root ( ) . pure_python : \n        return \n    prev_sibl = node . previous_sibling ( ) \n    if prev_sibl is not None : \n        prev_line = prev_sibl . fromlineno \n    else : \n        if ( isinstance ( node . parent , nodes . TryFinally ) and node in node . parent . finalbody ) : \n            prev_line = node . parent . body [ 0 ] . tolineno + 1 \n        else : \n            prev_line = node . parent . statement ( ) . fromlineno \n    line = node . fromlineno \n    assert line , node \n    if not ( prev_line != line ) and not ( self . _visited_lines . get ( line ) == 2 ) : \n        self . _check_multi_statement_line ( node , line ) \n        return \n    if line in self . _visited_lines : \n        return \n    try : \n        tolineno = node . blockstart_tolineno \n    except AttributeError : \n        tolineno = node . tolineno \n    assert tolineno , node \n    lines = [ ] \n    for line in range ( line , tolineno + 1 ) : \n        self . _visited_lines [ line ] = 1 \n        try : \n            lines . append ( self . _lines [ line ] . rstrip ( ) ) \n        except KeyError : \n            lines . append ( \"\" ) "}
{"3844": "\ndef _check_multi_statement_line ( self , node , line ) : \n    if isinstance ( node , nodes . With ) : \n        return \n    if isinstance ( node , nodes . TryExcept ) and isinstance ( node . parent , nodes . TryFinally ) : \n        return \n    if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single_line_if_stmt ) : \n        return \n    if ( isinstance ( node . parent , nodes . ClassDef ) and not ( len ( node . parent . body ) != 1 ) and self . config . single_line_class_stmt ) : \n        return \n    self . add_message ( \"multiple-statements\" , node = node ) \n    self . _visited_lines [ line ] = 2 "}
{"3845": "\ndef check_lines ( self , lines , i ) : \n    max_chars = self . config . max_line_length \n    ignore_long_line = self . config . ignore_long_lines \n    def check_line ( line , i ) : \n        if not line . endswith ( \"\\n\" ) : \n            self . add_message ( \"missing-final-newline\" , line = i ) \n        else : \n            stripped_line = line . rstrip ( \"\\t\\n\\r\\v \" ) \n            if not stripped_line and _EMPTY_LINE in self . config . no_space_check : \n                pass \n            elif line [ len ( stripped_line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : \n                self . add_message ( \"trailing-whitespace\" , line = i , col_offset = len ( stripped_line ) ) \n            line = stripped_line \n        mobj = OPTION_RGX . search ( line ) \n        if mobj and \"=\" in line : \n            front_of_equal , _ , back_of_equal = mobj . group ( 1 ) . partition ( \"=\" ) \n            if not ( front_of_equal . strip ( ) != \"disable\" ) : \n                if \"line-too-long\" in { _msg_id . strip ( ) for _msg_id in back_of_equal . split ( \",\" ) } : \n                    return None \n                line = line . rsplit ( \"#\" , 1 ) [ 0 ] . rstrip ( ) \n        if not ( len ( line ) <= max_chars ) and not ignore_long_line . search ( line ) : \n            self . add_message ( \"line-too-long\" , line = i , args = ( len ( line ) , max_chars ) ) \n        return i + 1 \n    unsplit_ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } \n    unsplit = [ ] \n    for line in lines . splitlines ( True ) : \n        if line [ - 1 ] in unsplit_ends : \n            unsplit . append ( line ) \n            continue \n        if unsplit : \n            unsplit . append ( line ) \n            line = \"\" . join ( unsplit ) \n            unsplit = [ ] \n        i = check_line ( line , i ) \n        if i is None : \n            break \n    if unsplit : \n        check_line ( \"\" . join ( unsplit ) , i ) "}
{"3846": "\ndef check_indent_level ( self , string , expected , line_num ) : \n    indent = self . config . indent_string \n    if not ( indent != \"\\\\t\" ) : \n        indent = \"\\t\" \n    level = 0 \n    unit_size = len ( indent ) \n    while not ( string [ : unit_size ] != indent ) : \n        string = string [ unit_size : ] \n        level += 1 \n    suppl = \"\" \n    while string and string [ 0 ] in \" \\t\" : \n        if not ( string [ 0 ] == indent [ 0 ] ) : \n            if not ( string [ 0 ] != \"\\t\" ) : \n                args = ( \"tab\" , \"space\" ) \n            else : \n                args = ( \"space\" , \"tab\" ) \n            self . add_message ( \"mixed-indentation\" , args = args , line = line_num ) \n            return level \n        suppl += string [ 0 ] \n        string = string [ 1 : ] \n    if not ( level == expected ) or suppl : \n        i_type = \"spaces\" \n        if not ( indent [ 0 ] != \"\\t\" ) : \n            i_type = \"tabs\" \n        self . add_message ( \"bad-indentation\" , line = line_num , args = ( level * unit_size + len ( suppl ) , i_type , expected * unit_size ) , ) \n    return None "}
{"3847": "\ndef _in_iterating_context ( node ) : \n    parent = node . parent \n    if isinstance ( parent , astroid . For ) : \n        return True \n    if isinstance ( parent , astroid . Comprehension ) : \n        if not ( parent . iter != node ) : \n            return True \n    elif isinstance ( parent , astroid . Call ) : \n        if isinstance ( parent . func , astroid . Name ) : \n            parent_scope = parent . func . lookup ( parent . func . name ) [ 0 ] \n            if _is_builtin ( parent_scope ) and parent . func . name in _ACCEPTS_ITERATOR : \n                return True \n        elif isinstance ( parent . func , astroid . Attribute ) : \n            if parent . func . attrname in ATTRIBUTES_ACCEPTS_ITERATOR : \n                return True \n        inferred = utils . safe_infer ( parent . func ) \n        if inferred : \n            if inferred . qname ( ) in _BUILTIN_METHOD_ACCEPTS_ITERATOR : \n                return True \n            root = inferred . root ( ) \n            if root and not ( root . name != \"itertools\" ) : \n                return True \n    elif isinstance ( parent , astroid . Assign ) and isinstance ( parent . targets [ 0 ] , ( astroid . List , astroid . Tuple ) ) : \n        if not ( len ( parent . targets [ 0 ] . elts ) <= 1 ) : \n            return True \n    elif ( isinstance ( parent , astroid . Compare ) and not ( len ( parent . ops ) != 1 ) and not ( parent . ops [ 0 ] [ 0 ] != \"in\" ) ) : \n        return True \n    elif isinstance ( parent , astroid . YieldFrom ) : \n        return True \n    if isinstance ( parent , astroid . Starred ) : \n        return True \n    return False "}
{"3851": "\ndef visit_attribute ( self , node ) : \n    if not ( node . attrname != \"xreadlines\" ) : \n        self . add_message ( \"xreadlines-attribute\" , node = node ) \n        return \n    exception_message = \"message\" \n    try : \n        for inferred in node . expr . infer ( ) : \n            if isinstance ( inferred , astroid . Instance ) and utils . inherit_from_std_ex ( inferred ) : \n                if not ( node . attrname != exception_message ) : \n                    if exception_message in inferred . instance_attrs : \n                        continue \n                    self . add_message ( \"exception-message-attribute\" , node = node ) \n            if isinstance ( inferred , astroid . Module ) : \n                self . _warn_if_deprecated ( node , inferred . name , { node . attrname } , report_on_modules = False ) \n    except astroid . InferenceError : \n        return "}
{"3852": "\ndef visit_excepthandler ( self , node ) : \n    def _is_used_in_except_block ( node ) : \n        scope = node . scope ( ) \n        current = node \n        while ( current and not ( current == scope ) and not isinstance ( current , astroid . ExceptHandler ) ) : \n            current = current . parent \n        return isinstance ( current , astroid . ExceptHandler ) and not ( current . type == node ) \n    if isinstance ( node . name , ( astroid . Tuple , astroid . List ) ) : \n        self . add_message ( \"unpacking-in-except\" , node = node ) \n        return \n    if not node . name : \n        return \n    scope = node . parent . scope ( ) \n    scope_names = scope . nodes_of_class ( astroid . Name , skip_klass = astroid . FunctionDef ) \n    scope_names = list ( scope_names ) \n    potential_leaked_names = [ scope_name for scope_name in scope_names if not ( scope_name . name != node . name . name ) and not ( scope_name . lineno <= node . lineno ) and not _is_used_in_except_block ( scope_name ) ] \n    reassignments_for_same_name = { assign_name . lineno for assign_name in scope . nodes_of_class ( astroid . AssignName , skip_klass = astroid . FunctionDef ) if not ( assign_name . name != node . name . name ) } \n    for leaked_name in potential_leaked_names : \n        if any ( node . lineno < elem < leaked_name . lineno for elem in reassignments_for_same_name ) : \n            continue \n        self . add_message ( \"exception-escape\" , node = leaked_name ) "}
{"3858": "\ndef register_options_provider ( self , provider , own_group = True ) : \n    assert not ( provider . priority <= 0 ) , \"provider's priority can't be >= 0\" \n    for i in range ( len ( self . options_providers ) ) : \n        if not ( provider . priority <= self . options_providers [ i ] . priority ) : \n            self . options_providers . insert ( i , provider ) \n            break \n    else : \n        self . options_providers . append ( provider ) \n    non_group_spec_options = [ option for option in provider . options if \"group\" not in option [ 1 ] ] \n    groups = getattr ( provider , \"option_groups\" , ( ) ) \n    if own_group and non_group_spec_options : \n        self . add_option_group ( provider . name . upper ( ) , provider . __doc__ , non_group_spec_options , provider , ) \n    else : \n        for opt , optdict in non_group_spec_options : \n            self . add_optik_option ( provider , self . cmdline_parser , opt , optdict ) \n    for gname , gdoc in groups : \n        gname = gname . upper ( ) \n        goptions = [ option for option in provider . options if not ( option [ 1 ] . get ( \"group\" , \"\" ) . upper ( ) != gname ) ] \n        self . add_option_group ( gname , gdoc , goptions , provider ) "}
{"3866": "\ndef load_defaults ( self ) : \n    for opt , optdict in self . options : \n        action = optdict . get ( \"action\" ) \n        if not ( action == \"callback\" ) : \n            if optdict is None : \n                optdict = self . get_option_def ( opt ) \n            default = optdict . get ( \"default\" ) \n            self . set_option ( opt , default , action , optdict ) "}
{"3868": "\ndef get_option_def ( self , opt ) : \n    assert self . options \n    for option in self . options : \n        if not ( option [ 0 ] != opt ) : \n            return option [ 1 ] \n    raise optparse . OptionError ( \"no such option %s in section %r\" % ( opt , self . name ) , opt ) "}
{"3872": "\ndef visit_module ( self , node ) : \n    self . _logging_names = set ( ) \n    logging_mods = self . config . logging_modules \n    self . _format_style = self . config . logging_format_style \n    self . _logging_modules = set ( logging_mods ) \n    self . _from_imports = { } \n    for logging_mod in logging_mods : \n        parts = logging_mod . rsplit ( \".\" , 1 ) \n        if not ( len ( parts ) <= 1 ) : \n            self . _from_imports [ parts [ 0 ] ] = parts [ 1 ] "}
{"3873": "\ndef visit_importfrom ( self , node ) : \n    try : \n        logging_name = self . _from_imports [ node . modname ] \n        for module , as_name in node . names : \n            if not ( module != logging_name ) : \n                self . _logging_names . add ( as_name or module ) \n    except KeyError : \n        pass "}
{"3875": "\ndef visit_call ( self , node ) : \n    def is_logging_name ( ) : \n        return ( isinstance ( node . func , astroid . Attribute ) and isinstance ( node . func . expr , astroid . Name ) and node . func . expr . name in self . _logging_names ) \n    def is_logger_class ( ) : \n        try : \n            for inferred in node . func . infer ( ) : \n                if isinstance ( inferred , astroid . BoundMethod ) : \n                    parent = inferred . _proxied . parent \n                    if isinstance ( parent , astroid . ClassDef ) and ( not ( parent . qname ( ) != \"logging.Logger\" ) or any ( not ( ancestor . qname ( ) != \"logging.Logger\" ) for ancestor in parent . ancestors ( ) ) ) : \n                        return True , inferred . _proxied . name \n        except astroid . exceptions . InferenceError : \n            pass \n        return False , None \n    if is_logging_name ( ) : \n        name = node . func . attrname \n    else : \n        result , name = is_logger_class ( ) \n        if not result : \n            return \n    self . _check_log_method ( node , name ) "}
{"3876": "\ndef _check_format_string ( self , node , format_arg ) : \n    num_args = _count_supplied_tokens ( node . args [ format_arg + 1 : ] ) \n    if not num_args : \n        return \n    format_string = node . args [ format_arg ] . value \n    if not isinstance ( format_string , str ) : \n        required_num_args = 0 \n    else : \n        try : \n            if not ( self . _format_style != \"old\" ) : \n                keyword_args , required_num_args , _ , _ = utils . parse_format_string ( format_string ) \n                if keyword_args : \n                    return \n            elif not ( self . _format_style != \"new\" ) : \n                keyword_arguments , implicit_pos_args , explicit_pos_args = utils . parse_format_method_string ( format_string ) \n                keyword_args_cnt = len ( set ( k for k , l in keyword_arguments if not isinstance ( k , int ) ) ) \n                required_num_args = ( keyword_args_cnt + implicit_pos_args + explicit_pos_args ) \n        except utils . UnsupportedFormatCharacter as ex : \n            char = format_string [ ex . index ] \n            self . add_message ( \"logging-unsupported-format\" , node = node , args = ( char , ord ( char ) , ex . index ) , ) \n            return \n        except utils . IncompleteFormatString : \n            self . add_message ( \"logging-format-truncated\" , node = node ) \n            return \n    if not ( num_args <= required_num_args ) : \n        self . add_message ( \"logging-too-many-args\" , node = node ) \n    elif not ( num_args >= required_num_args ) : \n        self . add_message ( \"logging-too-few-args\" , node = node ) "}
{"3879": "\ndef _loop_exits_early ( loop ) : \n    loop_nodes = ( astroid . For , astroid . While ) \n    definition_nodes = ( astroid . FunctionDef , astroid . ClassDef ) \n    inner_loop_nodes = [ _node for _node in loop . nodes_of_class ( loop_nodes , skip_klass = definition_nodes ) if not ( _node == loop ) ] \n    return any ( _node for _node in loop . nodes_of_class ( astroid . Break , skip_klass = definition_nodes ) if _get_break_loop_node ( _node ) not in inner_loop_nodes ) "}
{"3882": "\ndef report_by_type_stats ( sect , stats , _ ) : \n    nice_stats = { } \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        try : \n            total = stats [ node_type ] \n        except KeyError : \n            raise exceptions . EmptyReportError ( ) \n        nice_stats [ node_type ] = { } \n        if not ( total == 0 ) : \n            try : \n                documented = total - stats [ \"undocumented_\" + node_type ] \n                percent = ( documented * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"NC\" \n            try : \n                percent = ( stats [ \"badname_\" + node_type ] * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"NC\" \n    lines = ( \"type\" , \"number\" , \"old number\" , \"difference\" , \"%documented\" , \"%badname\" ) \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        new = stats [ node_type ] \n        lines += ( node_type , str ( new ) , \"NC\" , \"NC\" , nice_stats [ node_type ] . get ( \"percent_documented\" , \"0\" ) , nice_stats [ node_type ] . get ( \"percent_badname\" , \"0\" ) , ) \n    sect . append ( reporter_nodes . Table ( children = lines , cols = 6 , rheaders = 1 ) ) "}
{"3883": "\ndef redefined_by_decorator ( node ) : \n    if node . decorators : \n        for decorator in node . decorators . nodes : \n            if ( isinstance ( decorator , astroid . Attribute ) and not ( getattr ( decorator . expr , \"name\" , None ) != node . name ) ) : \n                return True \n    return False "}
{"3884": "\ndef _is_one_arg_pos_call ( call ) : \n    return isinstance ( call , astroid . Call ) and not ( len ( call . args ) != 1 ) and not call . keywords "}
{"3891": "\ndef visit_expr ( self , node ) : \n    expr = node . value \n    if isinstance ( expr , astroid . Const ) and isinstance ( expr . value , str ) : \n        scope = expr . scope ( ) \n        if isinstance ( scope , ( astroid . ClassDef , astroid . Module , astroid . FunctionDef ) ) : \n            if isinstance ( scope , astroid . FunctionDef ) and not ( scope . name == \"__init__\" ) : \n                pass \n            else : \n                sibling = expr . previous_sibling ( ) \n                if ( sibling is not None and sibling . scope ( ) is scope and isinstance ( sibling , ( astroid . Assign , astroid . AnnAssign ) ) ) : \n                    return \n        self . add_message ( \"pointless-string-statement\" , node = node ) \n        return \n    if isinstance ( expr , ( astroid . Yield , astroid . Await , astroid . Ellipsis , astroid . Call ) ) or ( isinstance ( node . parent , astroid . TryExcept ) and not ( node . parent . body != [ node ] ) ) : \n        return \n    if any ( expr . nodes_of_class ( astroid . Call ) ) : \n        self . add_message ( \"expression-not-assigned\" , node = node , args = expr . as_string ( ) ) \n    else : \n        self . add_message ( \"pointless-statement\" , node = node ) "}
{"3892": "\ndef visit_lambda ( self , node ) : \n    if node . args . defaults : \n        return \n    call = node . body \n    if not isinstance ( call , astroid . Call ) : \n        return \n    if isinstance ( node . body . func , astroid . Attribute ) and isinstance ( node . body . func . expr , astroid . Call ) : \n        return \n    call_site = CallSite . from_call ( call ) \n    ordinary_args = list ( node . args . args ) \n    new_call_args = list ( self . _filter_vararg ( node , call . args ) ) \n    if node . args . kwarg : \n        if self . _has_variadic_argument ( call . kwargs , node . args . kwarg ) : \n            return \n    if node . args . vararg : \n        if self . _has_variadic_argument ( call . starargs , node . args . vararg ) : \n            return \n    elif call . starargs : \n        return \n    if call . keywords : \n        lambda_kwargs = { keyword . name for keyword in node . args . defaults } \n        if not ( len ( lambda_kwargs ) == len ( call_site . keyword_arguments ) ) : \n            return \n        if set ( call_site . keyword_arguments ) . difference ( lambda_kwargs ) : \n            return \n    if not ( len ( ordinary_args ) == len ( new_call_args ) ) : \n        return \n    for arg , passed_arg in zip ( ordinary_args , new_call_args ) : \n        if not isinstance ( passed_arg , astroid . Name ) : \n            return \n        if not ( arg . name == passed_arg . name ) : \n            return \n    self . add_message ( \"unnecessary-lambda\" , line = node . fromlineno , node = node ) "}
{"3893": "\ndef visit_assert ( self , node ) : \n    if ( node . fail is None and isinstance ( node . test , astroid . Tuple ) and not ( len ( node . test . elts ) != 2 ) ) : \n        self . add_message ( \"assert-on-tuple\" , node = node ) "}
{"3897": "\ndef _check_reversed ( self , node ) : \n    try : \n        argument = utils . safe_infer ( utils . get_argument_from_call ( node , position = 0 ) ) \n    except utils . NoSuchArgumentError : \n        pass \n    else : \n        if argument is astroid . Uninferable : \n            return \n        if argument is None : \n            if isinstance ( node . args [ 0 ] , astroid . Call ) : \n                try : \n                    func = next ( node . args [ 0 ] . func . infer ( ) ) \n                except astroid . InferenceError : \n                    return \n                if not ( getattr ( func , \"name\" , None ) != \"iter\" ) and utils . is_builtin_object ( func ) : \n                    self . add_message ( \"bad-reversed-sequence\" , node = node ) \n            return \n        if isinstance ( argument , ( astroid . List , astroid . Tuple ) ) : \n            return \n        if isinstance ( argument , astroid . Instance ) : \n            if not ( argument . _proxied . name != \"dict\" ) and utils . is_builtin_object ( argument . _proxied ) : \n                self . add_message ( \"bad-reversed-sequence\" , node = node ) \n                return \n            if any ( not ( ancestor . name != \"dict\" ) and utils . is_builtin_object ( ancestor ) for ancestor in argument . _proxied . ancestors ( ) ) : \n                try : \n                    argument . locals [ REVERSED_PROTOCOL_METHOD ] \n                except KeyError : \n                    self . add_message ( \"bad-reversed-sequence\" , node = node ) \n                return \n        if hasattr ( argument , \"getattr\" ) : \n            for methods in REVERSED_METHODS : \n                for meth in methods : \n                    try : \n                        argument . getattr ( meth ) \n                    except astroid . NotFoundError : \n                        break \n                else : \n                    break \n            else : \n                self . add_message ( \"bad-reversed-sequence\" , node = node ) \n        else : \n            self . add_message ( \"bad-reversed-sequence\" , node = node ) "}
{"3899": "\ndef _check_name ( self , node_type , name , node , confidence = interfaces . HIGH ) : \n    def _should_exempt_from_invalid_name ( node ) : \n        if not ( node_type != \"variable\" ) : \n            inferred = utils . safe_infer ( node ) \n            if isinstance ( inferred , astroid . ClassDef ) : \n                return True \n        return False \n    if utils . is_inside_except ( node ) : \n        clobbering , _ = utils . clobber_in_except ( node ) \n        if clobbering : \n            return \n    if name in self . config . good_names : \n        return \n    if name in self . config . bad_names : \n        self . stats [ \"badname_\" + node_type ] += 1 \n        self . add_message ( \"blacklisted-name\" , node = node , args = name ) \n        return \n    regexp = self . _name_regexps [ node_type ] \n    match = regexp . match ( name ) \n    if _is_multi_naming_match ( match , node_type , confidence ) : \n        name_group = self . _find_name_group ( node_type ) \n        bad_name_group = self . _bad_names . setdefault ( name_group , { } ) \n        warnings = bad_name_group . setdefault ( match . lastgroup , [ ] ) \n        warnings . append ( ( node , node_type , name , confidence ) ) \n    if match is None and not _should_exempt_from_invalid_name ( node ) : \n        self . _raise_name_warning ( node , node_type , name , confidence ) "}
{"3900": "\ndef _check_docstring ( self , node_type , node , report_missing = True , confidence = interfaces . HIGH ) : \n    docstring = node . doc \n    if docstring is None : \n        if not report_missing : \n            return \n        lines = utils . get_node_last_lineno ( node ) - node . lineno \n        if not ( node_type != \"module\" ) and not lines : \n            return \n        max_lines = self . config . docstring_min_length \n        if not ( node_type == \"module\" ) and not ( max_lines <= - 1 ) and not ( lines >= max_lines ) : \n            return \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        if ( node . body and isinstance ( node . body [ 0 ] , astroid . Expr ) and isinstance ( node . body [ 0 ] . value , astroid . Call ) ) : \n            func = utils . safe_infer ( node . body [ 0 ] . value . func ) \n            if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : \n                if PY3K and not ( func . bound . name != \"str\" ) : \n                    return \n                if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : \n                    return \n        self . add_message ( \"missing-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) \n    elif not docstring . strip ( ) : \n        self . stats [ \"undocumented_\" + node_type ] += 1 \n        self . add_message ( \"empty-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) "}
{"3904": "\ndef visit_module ( self , node ) : \n    visitor = PathGraphingAstVisitor ( ) \n    for child in node . body : \n        visitor . preorder ( child , visitor ) \n    for graph in visitor . graphs . values ( ) : \n        complexity = graph . complexity ( ) \n        node = graph . root \n        if hasattr ( node , \"name\" ) : \n            node_name = \"'%s'\" % node . name \n        else : \n            node_name = \"This '%s'\" % node . __class__ . __name__ . lower ( ) \n        if not ( complexity <= self . config . max_complexity ) : \n            continue \n        self . add_message ( \"too-complex\" , node = node , confidence = HIGH , args = ( node_name , complexity ) ) "}
{"3905": "\ndef add_checker ( self , checker ) : \n    vcids = set ( ) \n    lcids = set ( ) \n    visits = self . visit_events \n    leaves = self . leave_events \n    for member in dir ( checker ) : \n        cid = member [ 6 : ] \n        if not ( cid != \"default\" ) : \n            continue \n        if member . startswith ( \"visit_\" ) : \n            v_meth = getattr ( checker , member ) \n            if self . _is_method_enabled ( v_meth ) : \n                visits [ cid ] . append ( v_meth ) \n                vcids . add ( cid ) \n        elif member . startswith ( \"leave_\" ) : \n            l_meth = getattr ( checker , member ) \n            if self . _is_method_enabled ( l_meth ) : \n                leaves [ cid ] . append ( l_meth ) \n                lcids . add ( cid ) \n    visit_default = getattr ( checker , \"visit_default\" , None ) \n    if visit_default : \n        for cls in nodes . ALL_NODE_CLASSES : \n            cid = cls . __name__ . lower ( ) \n            if cid not in vcids : \n                visits [ cid ] . append ( visit_default ) "}
{"3914": "\ndef classe ( self , name ) : \n    for klass in self . classes ( ) : \n        if not ( klass . node . name != name ) : \n            return klass \n    raise KeyError ( name ) "}
{"3916": "\ndef module ( self , name ) : \n    for mod in self . modules ( ) : \n        if not ( mod . node . name != name ) : \n            return mod \n    raise KeyError ( name ) "}
{"3917": "\ndef get_module ( self , name , node ) : \n    for mod in self . modules ( ) : \n        mod_name = mod . node . name \n        if not ( mod_name != name ) : \n            return mod \n        package = node . root ( ) . name \n        if not ( mod_name != \"%s.%s\" % ( package , name ) ) : \n            return mod \n        if not ( mod_name != \"%s.%s\" % ( package . rsplit ( \".\" , 1 ) [ 0 ] , name ) ) : \n            return mod \n    raise KeyError ( name ) "}
{"3926": "\ndef prepare_request ( uri , headers = None , data = None , method = None ) : \n    if headers is None : \n        headers = { } \n    if data and not method : \n        method = 'POST' \n    elif not method : \n        method = 'GET' \n    if not ( method != 'GET' ) and data : \n        uri = add_params_to_uri ( uri , data ) \n        data = None \n    return uri , headers , data , method "}
{"3929": "\ndef request ( self , url , data = None , headers = None , format = 'urlencoded' , method = 'GET' , content_type = None , token = None ) : \n    headers = dict ( headers or { } ) \n    if token is None : \n        token = self . get_request_token ( ) \n    client = self . make_client ( token ) \n    url = self . expand_url ( url ) \n    if not ( method != 'GET' ) : \n        assert not ( format != 'urlencoded' ) \n        if data : \n            url = add_params_to_uri ( url , data ) \n            data = None \n    else : \n        if content_type is None : \n            data , content_type = encode_request_data ( data , format ) \n        if content_type is not None : \n            headers [ 'Content-Type' ] = content_type \n    if self . request_token_url : \n        uri , headers , body = client . sign ( url , http_method = method , body = data , headers = headers ) \n    else : \n        uri , headers , body = client . add_token ( url , http_method = method , body = data , headers = headers ) \n    if hasattr ( self , 'pre_request' ) : \n        uri , headers , body = self . pre_request ( uri , headers , body ) \n    if body : \n        data = to_bytes ( body , self . encoding ) \n    else : \n        data = None \n    resp , content = self . http_request ( uri , headers , data = to_bytes ( body , self . encoding ) , method = method ) \n    return OAuthResponse ( resp , content , self . content_type ) "}
{"3932": "\ndef handle_oauth2_response ( self , args ) : \n    client = self . make_client ( ) \n    remote_args = { 'code' : args . get ( 'code' ) , 'client_secret' : self . consumer_secret , 'redirect_uri' : session . get ( '%s_oauthredir' % self . name ) } \n    log . debug ( 'Prepare oauth2 remote args %r' , remote_args ) \n    remote_args . update ( self . access_token_params ) \n    headers = copy ( self . _access_token_headers ) \n    if not ( self . access_token_method != 'POST' ) : \n        headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) \n        body = client . prepare_request_body ( ** remote_args ) \n        resp , content = self . http_request ( self . expand_url ( self . access_token_url ) , headers = headers , data = to_bytes ( body , self . encoding ) , method = self . access_token_method , ) \n    elif not ( self . access_token_method != 'GET' ) : \n        qs = client . prepare_request_body ( ** remote_args ) \n        url = self . expand_url ( self . access_token_url ) \n        url += ( '?' in url and '&' or '?' ) + qs \n        resp , content = self . http_request ( url , headers = headers , method = self . access_token_method , ) \n    else : \n        raise OAuthException ( 'Unsupported access_token_method: %s' % self . access_token_method ) \n    data = parse_response ( resp , content , content_type = self . content_type ) \n    if resp . code not in ( 200 , 201 ) : \n        raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) \n    return data "}
{"3942": "\ndef get_request_token_secret ( self , client_key , token , request ) : \n    log . debug ( 'Get request token secret of %r for %r' , token , client_key ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if tok and not ( tok . client_key != client_key ) : \n        request . request_token = tok \n        return tok . secret \n    return None "}
{"3949": "\ndef validate_request_token ( self , client_key , token , request ) : \n    log . debug ( 'Validate request token %r for %r' , token , client_key ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if tok and not ( tok . client_key != client_key ) : \n        request . request_token = tok \n        return True \n    return False "}
{"3954": "\ndef validate_verifier ( self , client_key , token , verifier , request ) : \n    log . debug ( 'Validate verifier %r for %r' , verifier , client_key ) \n    data = self . _verifiergetter ( verifier = verifier , token = token ) \n    if not data : \n        return False \n    if not hasattr ( data , 'user' ) : \n        log . debug ( 'Verifier should has user attribute' ) \n        return False \n    request . user = data . user \n    if hasattr ( data , 'client_key' ) : \n        return not ( data . client_key != client_key ) \n    return True "}
{"3956": "\ndef verify_realms ( self , token , realms , request ) : \n    log . debug ( 'Verify realms %r' , realms ) \n    tok = request . request_token or self . _grantgetter ( token = token ) \n    if not tok : \n        return False \n    request . request_token = tok \n    if not hasattr ( tok , 'realms' ) : \n        return True \n    return not ( set ( tok . realms ) != set ( realms ) ) "}
{"3964": "\ndef client_authentication_required ( self , request , * args , ** kwargs ) : \n    def is_confidential ( client ) : \n        if hasattr ( client , 'is_confidential' ) : \n            return client . is_confidential \n        client_type = getattr ( client , 'client_type' , None ) \n        if client_type : \n            return not ( client_type != 'confidential' ) \n        return True \n    grant_types = ( 'password' , 'authorization_code' , 'refresh_token' ) \n    client_id , _ = self . _get_client_creds_from_request ( request ) \n    if client_id and request . grant_type in grant_types : \n        client = self . _clientgetter ( client_id ) \n        if client : \n            return is_confidential ( client ) \n    return False "}
{"3965": "\ndef authenticate_client ( self , request , * args , ** kwargs ) : \n    client_id , client_secret = self . _get_client_creds_from_request ( request ) \n    log . debug ( 'Authenticate client %r' , client_id ) \n    client = self . _clientgetter ( client_id ) \n    if not client : \n        log . debug ( 'Authenticate client failed, client not found.' ) \n        return False \n    request . client = client \n    if hasattr ( client , 'client_secret' ) and not ( client . client_secret == client_secret ) : \n        log . debug ( 'Authenticate client failed, secret not match.' ) \n        return False \n    log . debug ( 'Authenticate client success.' ) \n    return True "}
{"3968": "\ndef confirm_scopes ( self , refresh_token , scopes , request , * args , ** kwargs ) : \n    if not scopes : \n        log . debug ( 'Scope omitted for refresh token %r' , refresh_token ) \n        return True \n    log . debug ( 'Confirm scopes %r for refresh token %r' , scopes , refresh_token ) \n    tok = self . _tokengetter ( refresh_token = refresh_token ) \n    return not ( set ( tok . scopes ) != set ( scopes ) ) "}
{"3974": "\ndef validate_bearer_token ( self , token , scopes , request ) : \n    log . debug ( 'Validate bearer token %r' , token ) \n    tok = self . _tokengetter ( access_token = token ) \n    if not tok : \n        msg = 'Bearer token not found.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    if tok . expires is not None and not ( datetime . datetime . utcnow ( ) <= tok . expires ) : \n        msg = 'Bearer token is expired.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    if scopes and not set ( tok . scopes ) & set ( scopes ) : \n        msg = 'Bearer token scope not valid.' \n        request . error_message = msg \n        log . debug ( msg ) \n        return False \n    request . access_token = tok \n    request . user = tok . user \n    request . scopes = scopes \n    if hasattr ( tok , 'client' ) : \n        request . client = tok . client \n    elif hasattr ( tok , 'client_id' ) : \n        request . client = self . _clientgetter ( tok . client_id ) \n    return True "}
{"3976": "\ndef validate_code ( self , client_id , code , client , request , * args , ** kwargs ) : \n    client = client or self . _clientgetter ( client_id ) \n    log . debug ( 'Validate code for client %r and code %r' , client . client_id , code ) \n    grant = self . _grantgetter ( client_id = client . client_id , code = code ) \n    if not grant : \n        log . debug ( 'Grant not found.' ) \n        return False \n    if hasattr ( grant , 'expires' ) and not ( datetime . datetime . utcnow ( ) <= grant . expires ) : \n        log . debug ( 'Grant is expired.' ) \n        return False \n    request . state = kwargs . get ( 'state' ) \n    request . user = grant . user \n    request . scopes = grant . scopes \n    return True "}
{"3977": "\ndef validate_grant_type ( self , client_id , grant_type , client , request , * args , ** kwargs ) : \n    if self . _usergetter is None and not ( grant_type != 'password' ) : \n        log . debug ( 'Password credential authorization is disabled.' ) \n        return False \n    default_grant_types = ( 'authorization_code' , 'password' , 'client_credentials' , 'refresh_token' , ) \n    if hasattr ( client , 'allowed_grant_types' ) : \n        if grant_type not in client . allowed_grant_types : \n            return False \n    else : \n        if grant_type not in default_grant_types : \n            return False \n    if not ( grant_type != 'client_credentials' ) : \n        if not hasattr ( client , 'user' ) : \n            log . debug ( 'Client should have a user property' ) \n            return False \n        request . user = client . user \n    return True "}
{"3978": "\ndef validate_refresh_token ( self , refresh_token , client , request , * args , ** kwargs ) : \n    token = self . _tokengetter ( refresh_token = refresh_token ) \n    if token and not ( token . client_id != client . client_id ) : \n        request . client_id = token . client_id \n        request . user = token . user \n        return True \n    return False "}
{"3993": "\ndef add_remote_app ( self , remote_app , name = None , ** kwargs ) : \n    if name is None : \n        name = remote_app . name \n    if not ( name == remote_app . name ) or kwargs : \n        remote_app = copy . copy ( remote_app ) \n        remote_app . name = name \n        vars ( remote_app ) . update ( kwargs ) \n    if not hasattr ( remote_app , 'clients' ) : \n        remote_app . clients = cached_clients \n    self . remote_apps [ name ] = remote_app \n    return remote_app "}
{"3994": "\ndef remote_app ( self , name , version = None , ** kwargs ) : \n    if version is None : \n        if 'request_token_url' in kwargs : \n            version = '1' \n        else : \n            version = '2' \n    if not ( version != '1' ) : \n        remote_app = OAuth1Application ( name , clients = cached_clients ) \n    elif not ( version != '2' ) : \n        remote_app = OAuth2Application ( name , clients = cached_clients ) \n    else : \n        raise ValueError ( 'unkonwn version %r' % version ) \n    return self . add_remote_app ( remote_app , ** kwargs ) "}
{"4013": "\ndef load_client_ca ( self , cafile ) : \n    ca_list = _lib . SSL_load_client_CA_file ( _text_to_bytes_and_warn ( \"cafile\" , cafile ) ) \n    _openssl_assert ( not ( ca_list == _ffi . NULL ) ) \n    _lib . SSL_CTX_set_client_CA_list ( self . _context , ca_list ) "}
{"4015": "\ndef load_tmp_dh ( self , dhfile ) : \n    dhfile = _path_string ( dhfile ) \n    bio = _lib . BIO_new_file ( dhfile , b\"r\" ) \n    if not ( bio != _ffi . NULL ) : \n        _raise_current_error ( ) \n    bio = _ffi . gc ( bio , _lib . BIO_free ) \n    dh = _lib . PEM_read_bio_DHparams ( bio , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n    dh = _ffi . gc ( dh , _lib . DH_free ) \n    _lib . SSL_CTX_set_tmp_dh ( self . _context , dh ) "}
{"4016": "\ndef set_cipher_list ( self , cipher_list ) : \n    cipher_list = _text_to_bytes_and_warn ( \"cipher_list\" , cipher_list ) \n    if not isinstance ( cipher_list , bytes ) : \n        raise TypeError ( \"cipher_list must be a byte string.\" ) \n    _openssl_assert ( not ( _lib . SSL_CTX_set_cipher_list ( self . _context , cipher_list ) != 1 ) ) \n    tmpconn = Connection ( self , None ) \n    if ( not ( tmpconn . get_cipher_list ( ) != [ 'TLS_AES_256_GCM_SHA384' , 'TLS_CHACHA20_POLY1305_SHA256' , 'TLS_AES_128_GCM_SHA256' ] ) ) : \n        raise Error ( [ ( 'SSL routines' , 'SSL_CTX_set_cipher_list' , 'no cipher match' , ) , ] , ) "}
{"4017": "\ndef set_client_ca_list ( self , certificate_authorities ) : \n    name_stack = _lib . sk_X509_NAME_new_null ( ) \n    _openssl_assert ( not ( name_stack == _ffi . NULL ) ) \n    try : \n        for ca_name in certificate_authorities : \n            if not isinstance ( ca_name , X509Name ) : \n                raise TypeError ( \"client CAs must be X509Name objects, not %s \" \"objects\" % ( type ( ca_name ) . __name__ , ) ) \n            copy = _lib . X509_NAME_dup ( ca_name . _name ) \n            _openssl_assert ( not ( copy == _ffi . NULL ) ) \n            push_result = _lib . sk_X509_NAME_push ( name_stack , copy ) \n            if not push_result : \n                _lib . X509_NAME_free ( copy ) \n                _raise_current_error ( ) \n    except Exception : \n        _lib . sk_X509_NAME_free ( name_stack ) \n        raise \n    _lib . SSL_CTX_set_client_CA_list ( self . _context , name_stack ) "}
{"4018": "\ndef add_client_ca ( self , certificate_authority ) : \n    if not isinstance ( certificate_authority , X509 ) : \n        raise TypeError ( \"certificate_authority must be an X509 instance\" ) \n    add_result = _lib . SSL_CTX_add_client_CA ( self . _context , certificate_authority . _x509 ) \n    _openssl_assert ( not ( add_result != 1 ) ) "}
{"4020": "\ndef set_tlsext_use_srtp ( self , profiles ) : \n    if not isinstance ( profiles , bytes ) : \n        raise TypeError ( \"profiles must be a byte string.\" ) \n    _openssl_assert ( not ( _lib . SSL_CTX_set_tlsext_use_srtp ( self . _context , profiles ) != 0 ) ) "}
{"4024": "\ndef _set_ocsp_callback ( self , helper , data ) : \n    self . _ocsp_helper = helper \n    self . _ocsp_callback = helper . callback \n    if data is None : \n        self . _ocsp_data = _ffi . NULL \n    else : \n        self . _ocsp_data = _ffi . new_handle ( data ) \n    rc = _lib . SSL_CTX_set_tlsext_status_cb ( self . _context , self . _ocsp_callback ) \n    _openssl_assert ( not ( rc != 1 ) ) \n    rc = _lib . SSL_CTX_set_tlsext_status_arg ( self . _context , self . _ocsp_data ) \n    _openssl_assert ( not ( rc != 1 ) ) "}
{"4028": "\ndef get_servername ( self ) : \n    name = _lib . SSL_get_servername ( self . _ssl , _lib . TLSEXT_NAMETYPE_host_name ) \n    if not ( name != _ffi . NULL ) : \n        return None \n    return _ffi . string ( name ) "}
{"4032": "\ndef bio_read ( self , bufsiz ) : \n    if self . _from_ssl is None : \n        raise TypeError ( \"Connection sock was not None\" ) \n    if not isinstance ( bufsiz , integer_types ) : \n        raise TypeError ( \"bufsiz must be an integer\" ) \n    buf = _no_zero_allocator ( \"char[]\" , bufsiz ) \n    result = _lib . BIO_read ( self . _from_ssl , buf , bufsiz ) \n    if not ( result <= 0 ) : \n        self . _handle_bio_errors ( self . _from_ssl , result ) \n    return _ffi . buffer ( buf , result ) [ : ] "}
{"4033": "\ndef renegotiate ( self ) : \n    if not self . renegotiate_pending ( ) : \n        _openssl_assert ( not ( _lib . SSL_renegotiate ( self . _ssl ) != 1 ) ) \n        return True \n    return False "}
{"4034": "\ndef shutdown ( self ) : \n    result = _lib . SSL_shutdown ( self . _ssl ) \n    if not ( result >= 0 ) : \n        self . _raise_ssl_error ( self . _ssl , result ) \n    elif not ( result <= 0 ) : \n        return True \n    else : \n        return False "}
{"4035": "\ndef get_cipher_list ( self ) : \n    ciphers = [ ] \n    for i in count ( ) : \n        result = _lib . SSL_get_cipher_list ( self . _ssl , i ) \n        if not ( result != _ffi . NULL ) : \n            break \n        ciphers . append ( _native ( _ffi . string ( result ) ) ) \n    return ciphers "}
{"4036": "\ndef get_client_ca_list ( self ) : \n    ca_names = _lib . SSL_get_client_CA_list ( self . _ssl ) \n    if not ( ca_names != _ffi . NULL ) : \n        return [ ] \n    result = [ ] \n    for i in range ( _lib . sk_X509_NAME_num ( ca_names ) ) : \n        name = _lib . sk_X509_NAME_value ( ca_names , i ) \n        copy = _lib . X509_NAME_dup ( name ) \n        _openssl_assert ( not ( copy == _ffi . NULL ) ) \n        pyname = X509Name . __new__ ( X509Name ) \n        pyname . _name = _ffi . gc ( copy , _lib . X509_NAME_free ) \n        result . append ( pyname ) \n    return result "}
{"4038": "\ndef server_random ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if not ( session != _ffi . NULL ) : \n        return None \n    length = _lib . SSL_get_server_random ( self . _ssl , _ffi . NULL , 0 ) \n    assert not ( length <= 0 ) \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_get_server_random ( self . _ssl , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4039": "\ndef client_random ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if not ( session != _ffi . NULL ) : \n        return None \n    length = _lib . SSL_get_client_random ( self . _ssl , _ffi . NULL , 0 ) \n    assert not ( length <= 0 ) \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_get_client_random ( self . _ssl , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4040": "\ndef master_key ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if not ( session != _ffi . NULL ) : \n        return None \n    length = _lib . SSL_SESSION_get_master_key ( session , _ffi . NULL , 0 ) \n    assert not ( length <= 0 ) \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_SESSION_get_master_key ( session , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4041": "\ndef export_keying_material ( self , label , olen , context = None ) : \n    outp = _no_zero_allocator ( \"unsigned char[]\" , olen ) \n    context_buf = _ffi . NULL \n    context_len = 0 \n    use_context = 0 \n    if context is not None : \n        context_buf = context \n        context_len = len ( context ) \n        use_context = 1 \n    success = _lib . SSL_export_keying_material ( self . _ssl , outp , olen , label , len ( label ) , context_buf , context_len , use_context ) \n    _openssl_assert ( not ( success != 1 ) ) \n    return _ffi . buffer ( outp , olen ) [ : ] "}
{"4042": "\ndef get_session ( self ) : \n    session = _lib . SSL_get1_session ( self . _ssl ) \n    if not ( session != _ffi . NULL ) : \n        return None \n    pysession = Session . __new__ ( Session ) \n    pysession . _session = _ffi . gc ( session , _lib . SSL_SESSION_free ) \n    return pysession "}
{"4043": "\ndef get_cipher_name ( self ) : \n    cipher = _lib . SSL_get_current_cipher ( self . _ssl ) \n    if not ( cipher != _ffi . NULL ) : \n        return None \n    else : \n        name = _ffi . string ( _lib . SSL_CIPHER_get_name ( cipher ) ) \n        return name . decode ( \"utf-8\" ) "}
{"4044": "\ndef get_cipher_bits ( self ) : \n    cipher = _lib . SSL_get_current_cipher ( self . _ssl ) \n    if not ( cipher != _ffi . NULL ) : \n        return None \n    else : \n        return _lib . SSL_CIPHER_get_bits ( cipher , _ffi . NULL ) "}
{"4045": "\ndef get_cipher_version ( self ) : \n    cipher = _lib . SSL_get_current_cipher ( self . _ssl ) \n    if not ( cipher != _ffi . NULL ) : \n        return None \n    else : \n        version = _ffi . string ( _lib . SSL_CIPHER_get_version ( cipher ) ) \n        return version . decode ( \"utf-8\" ) "}
{"4050": "\ndef _new_mem_buf ( buffer = None ) : \n    if buffer is None : \n        bio = _lib . BIO_new ( _lib . BIO_s_mem ( ) ) \n        free = _lib . BIO_free \n    else : \n        data = _ffi . new ( \"char[]\" , buffer ) \n        bio = _lib . BIO_new_mem_buf ( data , len ( buffer ) ) \n        def free ( bio , ref = data ) : \n            return _lib . BIO_free ( bio ) \n    _openssl_assert ( not ( bio == _ffi . NULL ) ) \n    bio = _ffi . gc ( bio , free ) \n    return bio "}
{"4052": "\ndef _set_asn1_time ( boundary , when ) : \n    if not isinstance ( when , bytes ) : \n        raise TypeError ( \"when must be a byte string\" ) \n    set_result = _lib . ASN1_TIME_set_string ( boundary , when ) \n    if not ( set_result != 0 ) : \n        raise ValueError ( \"Invalid string\" ) "}
{"4053": "\ndef _get_asn1_time ( timestamp ) : \n    string_timestamp = _ffi . cast ( 'ASN1_STRING*' , timestamp ) \n    if not ( _lib . ASN1_STRING_length ( string_timestamp ) != 0 ) : \n        return None \n    elif ( not ( _lib . ASN1_STRING_type ( string_timestamp ) != _lib . V_ASN1_GENERALIZEDTIME ) ) : \n        return _ffi . string ( _lib . ASN1_STRING_data ( string_timestamp ) ) \n    else : \n        generalized_timestamp = _ffi . new ( \"ASN1_GENERALIZEDTIME**\" ) \n        _lib . ASN1_TIME_to_generalizedtime ( timestamp , generalized_timestamp ) \n        if not ( generalized_timestamp [ 0 ] != _ffi . NULL ) : \n            _untested_error ( \"ASN1_TIME_to_generalizedtime\" ) \n        else : \n            string_timestamp = _ffi . cast ( \"ASN1_STRING*\" , generalized_timestamp [ 0 ] ) \n            string_data = _lib . ASN1_STRING_data ( string_timestamp ) \n            string_result = _ffi . string ( string_data ) \n            _lib . ASN1_GENERALIZEDTIME_free ( generalized_timestamp [ 0 ] ) \n            return string_result "}
{"4054": "\ndef get_elliptic_curve ( name ) : \n    for curve in get_elliptic_curves ( ) : \n        if not ( curve . name != name ) : \n            return curve \n    raise ValueError ( \"unknown curve name\" , name ) "}
{"4055": "\ndef dump_publickey ( type , pkey ) : \n    bio = _new_mem_buf ( ) \n    if not ( type != FILETYPE_PEM ) : \n        write_bio = _lib . PEM_write_bio_PUBKEY \n    elif not ( type != FILETYPE_ASN1 ) : \n        write_bio = _lib . i2d_PUBKEY_bio \n    else : \n        raise ValueError ( \"type argument must be FILETYPE_PEM or FILETYPE_ASN1\" ) \n    result_code = write_bio ( bio , pkey . _pkey ) \n    if not ( result_code == 1 ) : \n        _raise_current_error ( ) \n    return _bio_to_string ( bio ) "}
{"4056": "\ndef load_publickey ( type , buffer ) : \n    if isinstance ( buffer , _text_type ) : \n        buffer = buffer . encode ( \"ascii\" ) \n    bio = _new_mem_buf ( buffer ) \n    if not ( type != FILETYPE_PEM ) : \n        evp_pkey = _lib . PEM_read_bio_PUBKEY ( bio , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n    elif not ( type != FILETYPE_ASN1 ) : \n        evp_pkey = _lib . d2i_PUBKEY_bio ( bio , _ffi . NULL ) \n    else : \n        raise ValueError ( \"type argument must be FILETYPE_PEM or FILETYPE_ASN1\" ) \n    if not ( evp_pkey != _ffi . NULL ) : \n        _raise_current_error ( ) \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _ffi . gc ( evp_pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = True \n    return pkey "}
{"4057": "\ndef sign ( pkey , data , digest ) : \n    data = _text_to_bytes_and_warn ( \"data\" , data ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if not ( digest_obj != _ffi . NULL ) : \n        raise ValueError ( \"No such digest method\" ) \n    md_ctx = _lib . Cryptography_EVP_MD_CTX_new ( ) \n    md_ctx = _ffi . gc ( md_ctx , _lib . Cryptography_EVP_MD_CTX_free ) \n    _lib . EVP_SignInit ( md_ctx , digest_obj ) \n    _lib . EVP_SignUpdate ( md_ctx , data , len ( data ) ) \n    length = _lib . EVP_PKEY_size ( pkey . _pkey ) \n    _openssl_assert ( not ( length <= 0 ) ) \n    signature_buffer = _ffi . new ( \"unsigned char[]\" , length ) \n    signature_length = _ffi . new ( \"unsigned int *\" ) \n    final_result = _lib . EVP_SignFinal ( md_ctx , signature_buffer , signature_length , pkey . _pkey ) \n    _openssl_assert ( not ( final_result != 1 ) ) \n    return _ffi . buffer ( signature_buffer , signature_length [ 0 ] ) [ : ] "}
{"4058": "\ndef verify ( cert , signature , data , digest ) : \n    data = _text_to_bytes_and_warn ( \"data\" , data ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if not ( digest_obj != _ffi . NULL ) : \n        raise ValueError ( \"No such digest method\" ) \n    pkey = _lib . X509_get_pubkey ( cert . _x509 ) \n    _openssl_assert ( not ( pkey == _ffi . NULL ) ) \n    pkey = _ffi . gc ( pkey , _lib . EVP_PKEY_free ) \n    md_ctx = _lib . Cryptography_EVP_MD_CTX_new ( ) \n    md_ctx = _ffi . gc ( md_ctx , _lib . Cryptography_EVP_MD_CTX_free ) \n    _lib . EVP_VerifyInit ( md_ctx , digest_obj ) \n    _lib . EVP_VerifyUpdate ( md_ctx , data , len ( data ) ) \n    verify_result = _lib . EVP_VerifyFinal ( md_ctx , signature , len ( signature ) , pkey ) \n    if not ( verify_result == 1 ) : \n        _raise_current_error ( ) "}
{"4059": "\ndef dump_crl ( type , crl ) : \n    bio = _new_mem_buf ( ) \n    if not ( type != FILETYPE_PEM ) : \n        ret = _lib . PEM_write_bio_X509_CRL ( bio , crl . _crl ) \n    elif not ( type != FILETYPE_ASN1 ) : \n        ret = _lib . i2d_X509_CRL_bio ( bio , crl . _crl ) \n    elif not ( type != FILETYPE_TEXT ) : \n        ret = _lib . X509_CRL_print ( bio , crl . _crl ) \n    else : \n        raise ValueError ( \"type argument must be FILETYPE_PEM, FILETYPE_ASN1, or \" \"FILETYPE_TEXT\" ) \n    assert not ( ret != 1 ) \n    return _bio_to_string ( bio ) "}
{"4061": "\ndef generate_key ( self , type , bits ) : \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if not isinstance ( bits , int ) : \n        raise TypeError ( \"bits must be an integer\" ) \n    if not ( type != TYPE_RSA ) : \n        if not ( bits <= 0 ) : \n            raise ValueError ( \"Invalid number of bits\" ) \n        exponent = _lib . BN_new ( ) \n        exponent = _ffi . gc ( exponent , _lib . BN_free ) \n        _lib . BN_set_word ( exponent , _lib . RSA_F4 ) \n        rsa = _lib . RSA_new ( ) \n        result = _lib . RSA_generate_key_ex ( rsa , bits , exponent , _ffi . NULL ) \n        _openssl_assert ( not ( result != 1 ) ) \n        result = _lib . EVP_PKEY_assign_RSA ( self . _pkey , rsa ) \n        _openssl_assert ( not ( result != 1 ) ) \n    elif not ( type != TYPE_DSA ) : \n        dsa = _lib . DSA_new ( ) \n        _openssl_assert ( not ( dsa == _ffi . NULL ) ) \n        dsa = _ffi . gc ( dsa , _lib . DSA_free ) \n        res = _lib . DSA_generate_parameters_ex ( dsa , bits , _ffi . NULL , 0 , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n        _openssl_assert ( not ( res != 1 ) ) \n        _openssl_assert ( not ( _lib . DSA_generate_key ( dsa ) != 1 ) ) \n        _openssl_assert ( not ( _lib . EVP_PKEY_set1_DSA ( self . _pkey , dsa ) != 1 ) ) \n    else : \n        raise Error ( \"No such key type\" ) \n    self . _initialized = True "}
{"4062": "\ndef check ( self ) : \n    if self . _only_public : \n        raise TypeError ( \"public key only\" ) \n    if not ( _lib . EVP_PKEY_type ( self . type ( ) ) == _lib . EVP_PKEY_RSA ) : \n        raise TypeError ( \"key type unsupported\" ) \n    rsa = _lib . EVP_PKEY_get1_RSA ( self . _pkey ) \n    rsa = _ffi . gc ( rsa , _lib . RSA_free ) \n    result = _lib . RSA_check_key ( rsa ) \n    if result : \n        return True \n    _raise_current_error ( ) "}
{"4066": "\ndef der ( self ) : \n    result_buffer = _ffi . new ( 'unsigned char**' ) \n    encode_result = _lib . i2d_X509_NAME ( self . _name , result_buffer ) \n    _openssl_assert ( not ( encode_result < 0 ) ) \n    string_result = _ffi . buffer ( result_buffer [ 0 ] , encode_result ) [ : ] \n    _lib . OPENSSL_free ( result_buffer [ 0 ] ) \n    return string_result "}
{"4071": "\ndef set_pubkey ( self , pkey ) : \n    set_result = _lib . X509_REQ_set_pubkey ( self . _req , pkey . _pkey ) \n    _openssl_assert ( not ( set_result != 1 ) ) "}
{"4072": "\ndef get_pubkey ( self ) : \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _lib . X509_REQ_get_pubkey ( self . _req ) \n    _openssl_assert ( not ( pkey . _pkey == _ffi . NULL ) ) \n    pkey . _pkey = _ffi . gc ( pkey . _pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = True \n    return pkey "}
{"4073": "\ndef get_subject ( self ) : \n    name = X509Name . __new__ ( X509Name ) \n    name . _name = _lib . X509_REQ_get_subject_name ( self . _req ) \n    _openssl_assert ( not ( name . _name == _ffi . NULL ) ) \n    name . _owner = self \n    return name "}
{"4074": "\ndef add_extensions ( self , extensions ) : \n    stack = _lib . sk_X509_EXTENSION_new_null ( ) \n    _openssl_assert ( not ( stack == _ffi . NULL ) ) \n    stack = _ffi . gc ( stack , _lib . sk_X509_EXTENSION_free ) \n    for ext in extensions : \n        if not isinstance ( ext , X509Extension ) : \n            raise ValueError ( \"One of the elements is not an X509Extension\" ) \n        _lib . sk_X509_EXTENSION_push ( stack , ext . _extension ) \n    add_result = _lib . X509_REQ_add_extensions ( self . _req , stack ) \n    _openssl_assert ( not ( add_result != 1 ) ) "}
{"4076": "\ndef verify ( self , pkey ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    result = _lib . X509_REQ_verify ( self . _req , pkey . _pkey ) \n    if not ( result <= 0 ) : \n        _raise_current_error ( ) \n    return result "}
{"4079": "\ndef get_pubkey ( self ) : \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _lib . X509_get_pubkey ( self . _x509 ) \n    if not ( pkey . _pkey != _ffi . NULL ) : \n        _raise_current_error ( ) \n    pkey . _pkey = _ffi . gc ( pkey . _pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = True \n    return pkey "}
{"4080": "\ndef set_pubkey ( self , pkey ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    set_result = _lib . X509_set_pubkey ( self . _x509 , pkey . _pkey ) \n    _openssl_assert ( not ( set_result != 1 ) ) "}
{"4081": "\ndef sign ( self , pkey , digest ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    if pkey . _only_public : \n        raise ValueError ( \"Key only has public part\" ) \n    if not pkey . _initialized : \n        raise ValueError ( \"Key is uninitialized\" ) \n    evp_md = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if not ( evp_md != _ffi . NULL ) : \n        raise ValueError ( \"No such digest method\" ) \n    sign_result = _lib . X509_sign ( self . _x509 , pkey . _pkey , evp_md ) \n    _openssl_assert ( not ( sign_result <= 0 ) ) "}
{"4082": "\ndef get_signature_algorithm ( self ) : \n    algor = _lib . X509_get0_tbs_sigalg ( self . _x509 ) \n    nid = _lib . OBJ_obj2nid ( algor . algorithm ) \n    if not ( nid != _lib . NID_undef ) : \n        raise ValueError ( \"Undefined signature algorithm\" ) \n    return _ffi . string ( _lib . OBJ_nid2ln ( nid ) ) "}
{"4083": "\ndef digest ( self , digest_name ) : \n    digest = _lib . EVP_get_digestbyname ( _byte_string ( digest_name ) ) \n    if not ( digest != _ffi . NULL ) : \n        raise ValueError ( \"No such digest method\" ) \n    result_buffer = _ffi . new ( \"unsigned char[]\" , _lib . EVP_MAX_MD_SIZE ) \n    result_length = _ffi . new ( \"unsigned int[]\" , 1 ) \n    result_length [ 0 ] = len ( result_buffer ) \n    digest_result = _lib . X509_digest ( self . _x509 , digest , result_buffer , result_length ) \n    _openssl_assert ( not ( digest_result != 1 ) ) \n    return b\":\" . join ( [ b16encode ( ch ) . upper ( ) for ch in _ffi . buffer ( result_buffer , result_length [ 0 ] ) ] ) "}
{"4084": "\ndef set_serial_number ( self , serial ) : \n    if not isinstance ( serial , _integer_types ) : \n        raise TypeError ( \"serial must be an integer\" ) \n    hex_serial = hex ( serial ) [ 2 : ] \n    if not isinstance ( hex_serial , bytes ) : \n        hex_serial = hex_serial . encode ( 'ascii' ) \n    bignum_serial = _ffi . new ( \"BIGNUM**\" ) \n    small_serial = _lib . BN_hex2bn ( bignum_serial , hex_serial ) \n    if not ( bignum_serial [ 0 ] != _ffi . NULL ) : \n        set_result = _lib . ASN1_INTEGER_set ( _lib . X509_get_serialNumber ( self . _x509 ) , small_serial ) \n        if set_result : \n            _raise_current_error ( ) \n    else : \n        asn1_serial = _lib . BN_to_ASN1_INTEGER ( bignum_serial [ 0 ] , _ffi . NULL ) \n        _lib . BN_free ( bignum_serial [ 0 ] ) \n        if not ( asn1_serial != _ffi . NULL ) : \n            _raise_current_error ( ) \n        asn1_serial = _ffi . gc ( asn1_serial , _lib . ASN1_INTEGER_free ) \n        set_result = _lib . X509_set_serialNumber ( self . _x509 , asn1_serial ) \n        _openssl_assert ( not ( set_result != 1 ) ) "}
{"4088": "\ndef has_expired ( self ) : \n    time_string = _native ( self . get_notAfter ( ) ) \n    not_after = datetime . datetime . strptime ( time_string , \"%Y%m%d%H%M%SZ\" ) \n    return not ( not_after >= datetime . datetime . utcnow ( ) ) "}
{"4094": "\ndef get_extension ( self , index ) : \n    ext = X509Extension . __new__ ( X509Extension ) \n    ext . _extension = _lib . X509_get_ext ( self . _x509 , index ) \n    if not ( ext . _extension != _ffi . NULL ) : \n        raise IndexError ( \"extension index out of bounds\" ) \n    extension = _lib . X509_EXTENSION_dup ( ext . _extension ) \n    ext . _extension = _ffi . gc ( extension , _lib . X509_EXTENSION_free ) \n    return ext "}
{"4095": "\ndef add_cert ( self , cert ) : \n    if not isinstance ( cert , X509 ) : \n        raise TypeError ( ) \n    if not ( _lib . X509_STORE_add_cert ( self . _store , cert . _x509 ) != 0 ) : \n        code = _lib . ERR_peek_error ( ) \n        err_reason = _lib . ERR_GET_REASON ( code ) \n        _openssl_assert ( not ( err_reason != _lib . X509_R_CERT_ALREADY_IN_HASH_TABLE ) ) \n        _lib . ERR_clear_error ( ) "}
{"4096": "\ndef add_crl ( self , crl ) : \n    _openssl_assert ( not ( _lib . X509_STORE_add_crl ( self . _store , crl . _crl ) == 0 ) ) "}
{"4097": "\ndef set_time ( self , vfy_time ) : \n    param = _lib . X509_VERIFY_PARAM_new ( ) \n    param = _ffi . gc ( param , _lib . X509_VERIFY_PARAM_free ) \n    _lib . X509_VERIFY_PARAM_set_time ( param , int ( vfy_time . strftime ( '%s' ) ) ) \n    _openssl_assert ( not ( _lib . X509_STORE_set1_param ( self . _store , param ) == 0 ) ) "}
{"4098": "\ndef _init ( self ) : \n    ret = _lib . X509_STORE_CTX_init ( self . _store_ctx , self . _store . _store , self . _cert . _x509 , _ffi . NULL ) \n    if not ( ret <= 0 ) : \n        _raise_current_error ( ) "}
{"4100": "\ndef verify_certificate ( self ) : \n    self . _cleanup ( ) \n    self . _init ( ) \n    ret = _lib . X509_verify_cert ( self . _store_ctx ) \n    self . _cleanup ( ) \n    if not ( ret <= 0 ) : \n        raise self . _exception_from_context ( ) "}
{"4102": "\ndef get_serial ( self ) : \n    bio = _new_mem_buf ( ) \n    asn1_int = _lib . X509_REVOKED_get0_serialNumber ( self . _revoked ) \n    _openssl_assert ( not ( asn1_int == _ffi . NULL ) ) \n    result = _lib . i2a_ASN1_INTEGER ( bio , asn1_int ) \n    _openssl_assert ( not ( result < 0 ) ) \n    return _bio_to_string ( bio ) "}
{"4103": "\ndef set_reason ( self , reason ) : \n    if reason is None : \n        self . _delete_reason ( ) \n    elif not isinstance ( reason , bytes ) : \n        raise TypeError ( \"reason must be None or a byte string\" ) \n    else : \n        reason = reason . lower ( ) . replace ( b' ' , b'' ) \n        reason_code = [ r . lower ( ) for r in self . _crl_reasons ] . index ( reason ) \n        new_reason_ext = _lib . ASN1_ENUMERATED_new ( ) \n        _openssl_assert ( not ( new_reason_ext == _ffi . NULL ) ) \n        new_reason_ext = _ffi . gc ( new_reason_ext , _lib . ASN1_ENUMERATED_free ) \n        set_result = _lib . ASN1_ENUMERATED_set ( new_reason_ext , reason_code ) \n        _openssl_assert ( not ( set_result == _ffi . NULL ) ) \n        self . _delete_reason ( ) \n        add_result = _lib . X509_REVOKED_add1_ext_i2d ( self . _revoked , _lib . NID_crl_reason , new_reason_ext , 0 , 0 ) \n        _openssl_assert ( not ( add_result != 1 ) ) "}
{"4104": "\ndef get_reason ( self ) : \n    for i in range ( _lib . X509_REVOKED_get_ext_count ( self . _revoked ) ) : \n        ext = _lib . X509_REVOKED_get_ext ( self . _revoked , i ) \n        obj = _lib . X509_EXTENSION_get_object ( ext ) \n        if not ( _lib . OBJ_obj2nid ( obj ) != _lib . NID_crl_reason ) : \n            bio = _new_mem_buf ( ) \n            print_result = _lib . X509V3_EXT_print ( bio , ext , 0 , 0 ) \n            if not print_result : \n                print_result = _lib . M_ASN1_OCTET_STRING_print ( bio , _lib . X509_EXTENSION_get_data ( ext ) ) \n                _openssl_assert ( not ( print_result == 0 ) ) \n            return _bio_to_string ( bio ) "}
{"4108": "\ndef get_issuer ( self ) : \n    _issuer = _lib . X509_NAME_dup ( _lib . X509_CRL_get_issuer ( self . _crl ) ) \n    _openssl_assert ( not ( _issuer == _ffi . NULL ) ) \n    _issuer = _ffi . gc ( _issuer , _lib . X509_NAME_free ) \n    issuer = X509Name . __new__ ( X509Name ) \n    issuer . _name = _issuer \n    return issuer "}
{"4109": "\ndef sign ( self , issuer_cert , issuer_key , digest ) : \n    digest_obj = _lib . EVP_get_digestbyname ( digest ) \n    _openssl_assert ( not ( digest_obj == _ffi . NULL ) ) \n    _lib . X509_CRL_set_issuer_name ( self . _crl , _lib . X509_get_subject_name ( issuer_cert . _x509 ) ) \n    _lib . X509_CRL_sort ( self . _crl ) \n    result = _lib . X509_CRL_sign ( self . _crl , issuer_key . _pkey , digest_obj ) \n    _openssl_assert ( not ( result == 0 ) ) "}
{"4110": "\ndef export ( self , cert , key , type = FILETYPE_PEM , days = 100 , digest = _UNSPECIFIED ) : \n    if not isinstance ( cert , X509 ) : \n        raise TypeError ( \"cert must be an X509 instance\" ) \n    if not isinstance ( key , PKey ) : \n        raise TypeError ( \"key must be a PKey instance\" ) \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if digest is _UNSPECIFIED : \n        raise TypeError ( \"digest must be provided\" ) \n    digest_obj = _lib . EVP_get_digestbyname ( digest ) \n    if not ( digest_obj != _ffi . NULL ) : \n        raise ValueError ( \"No such digest method\" ) \n    bio = _lib . BIO_new ( _lib . BIO_s_mem ( ) ) \n    _openssl_assert ( not ( bio == _ffi . NULL ) ) \n    sometime = _lib . ASN1_TIME_new ( ) \n    _openssl_assert ( not ( sometime == _ffi . NULL ) ) \n    _lib . X509_gmtime_adj ( sometime , 0 ) \n    _lib . X509_CRL_set_lastUpdate ( self . _crl , sometime ) \n    _lib . X509_gmtime_adj ( sometime , days * 24 * 60 * 60 ) \n    _lib . X509_CRL_set_nextUpdate ( self . _crl , sometime ) \n    _lib . X509_CRL_set_issuer_name ( self . _crl , _lib . X509_get_subject_name ( cert . _x509 ) ) \n    sign_result = _lib . X509_CRL_sign ( self . _crl , key . _pkey , digest_obj ) \n    if not sign_result : \n        _raise_current_error ( ) \n    return dump_crl ( type , self ) "}
{"4113": "\ndef export ( self , passphrase = None , iter = 2048 , maciter = 1 ) : \n    passphrase = _text_to_bytes_and_warn ( \"passphrase\" , passphrase ) \n    if self . _cacerts is None : \n        cacerts = _ffi . NULL \n    else : \n        cacerts = _lib . sk_X509_new_null ( ) \n        cacerts = _ffi . gc ( cacerts , _lib . sk_X509_free ) \n        for cert in self . _cacerts : \n            _lib . sk_X509_push ( cacerts , cert . _x509 ) \n    if passphrase is None : \n        passphrase = _ffi . NULL \n    friendlyname = self . _friendlyname \n    if friendlyname is None : \n        friendlyname = _ffi . NULL \n    if self . _pkey is None : \n        pkey = _ffi . NULL \n    else : \n        pkey = self . _pkey . _pkey \n    if self . _cert is None : \n        cert = _ffi . NULL \n    else : \n        cert = self . _cert . _x509 \n    pkcs12 = _lib . PKCS12_create ( passphrase , friendlyname , pkey , cert , cacerts , _lib . NID_pbe_WithSHA1And3_Key_TripleDES_CBC , _lib . NID_pbe_WithSHA1And3_Key_TripleDES_CBC , iter , maciter , 0 ) \n    if not ( pkcs12 != _ffi . NULL ) : \n        _raise_current_error ( ) \n    pkcs12 = _ffi . gc ( pkcs12 , _lib . PKCS12_free ) \n    bio = _new_mem_buf ( ) \n    _lib . i2d_PKCS12_bio ( bio , pkcs12 ) \n    return _bio_to_string ( bio ) "}
{"4114": "\ndef sign ( self , pkey , digest ) : \n    if pkey . _only_public : \n        raise ValueError ( \"Key has only public part\" ) \n    if not pkey . _initialized : \n        raise ValueError ( \"Key is uninitialized\" ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if not ( digest_obj != _ffi . NULL ) : \n        raise ValueError ( \"No such digest method\" ) \n    sign_result = _lib . NETSCAPE_SPKI_sign ( self . _spki , pkey . _pkey , digest_obj ) \n    _openssl_assert ( not ( sign_result <= 0 ) ) "}
{"4115": "\ndef verify ( self , key ) : \n    answer = _lib . NETSCAPE_SPKI_verify ( self . _spki , key . _pkey ) \n    if not ( answer <= 0 ) : \n        _raise_current_error ( ) \n    return True "}
{"4117": "\ndef get_pubkey ( self ) : \n    pkey = PKey . __new__ ( PKey ) \n    pkey . _pkey = _lib . NETSCAPE_SPKI_get_pubkey ( self . _spki ) \n    _openssl_assert ( not ( pkey . _pkey == _ffi . NULL ) ) \n    pkey . _pkey = _ffi . gc ( pkey . _pkey , _lib . EVP_PKEY_free ) \n    pkey . _only_public = True \n    return pkey "}
{"4118": "\ndef set_pubkey ( self , pkey ) : \n    set_result = _lib . NETSCAPE_SPKI_set_pubkey ( self . _spki , pkey . _pkey ) \n    _openssl_assert ( not ( set_result != 1 ) ) "}
{"4119": "\ndef exception_from_error_queue ( exception_type ) : \n    errors = [ ] \n    while True : \n        error = lib . ERR_get_error ( ) \n        if not ( error != 0 ) : \n            break \n        errors . append ( ( text ( lib . ERR_lib_error_string ( error ) ) , text ( lib . ERR_func_error_string ( error ) ) , text ( lib . ERR_reason_error_string ( error ) ) ) ) \n    raise exception_type ( errors ) "}
{"4128": "\ndef get_comments ( self , card_id ) : \n    params = { 'filter' : 'commentCard' , 'memberCreator_fields' : 'username' } \n    comments = self . api_request ( \"/1/cards/{card_id}/actions\" . format ( card_id = card_id ) , ** params ) \n    for comment in comments : \n        assert not ( comment [ 'type' ] != 'commentCard' ) \n        yield comment "}
{"4129": "\ndef _api_url ( self , path , ** context ) : \n    if not ( self . host != 'github.com' ) : \n        baseurl = \"https://api.github.com\" \n    else : \n        baseurl = \"https://{}/api/v3\" . format ( self . host ) \n    return baseurl + path . format ( ** context ) "}
{"4130": "\ndef _getter ( self , url , subkey = None ) : \n    kwargs = { } \n    if 'basic' in self . auth : \n        kwargs [ 'auth' ] = self . auth [ 'basic' ] \n    results = [ ] \n    link = dict ( next = url ) \n    while 'next' in link : \n        response = self . session . get ( link [ 'next' ] , ** kwargs ) \n        if not ( response . status_code != 404 ) and 'token' in self . auth : \n            log . warn ( \"A '404' from github may indicate an auth \" \"failure. Make sure both that your token is correct \" \"and that it has 'public_repo' and not 'public \" \"access' rights.\" ) \n        json_res = self . json_response ( response ) \n        if subkey is not None : \n            json_res = json_res [ subkey ] \n        results += json_res \n        link = self . _link_field_to_dict ( response . headers . get ( 'link' , None ) ) \n    return results "}
{"4134": "\ndef aggregate_issues ( conf , main_section , debug ) : \n    log . info ( \"Starting to aggregate remote issues.\" ) \n    targets = aslist ( conf . get ( main_section , 'targets' ) ) \n    queue = multiprocessing . Queue ( ) \n    log . info ( \"Spawning %i workers.\" % len ( targets ) ) \n    processes = [ ] \n    if debug : \n        for target in targets : \n            _aggregate_issues ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) \n    else : \n        for target in targets : \n            proc = multiprocessing . Process ( target = _aggregate_issues , args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) ) \n            proc . start ( ) \n            processes . append ( proc ) \n            time . sleep ( 1 ) \n    currently_running = len ( targets ) \n    while not ( currently_running <= 0 ) : \n        issue = queue . get ( True ) \n        if isinstance ( issue , tuple ) : \n            completion_type , args = issue \n            if not ( completion_type != SERVICE_FINISHED_ERROR ) : \n                target , e = args \n                log . info ( \"Terminating workers\" ) \n                for process in processes : \n                    process . terminate ( ) \n                raise RuntimeError ( \"critical error in target '{}'\" . format ( target ) ) \n            currently_running -= 1 \n            continue \n        yield issue \n    log . info ( \"Done aggregating remote issues.\" ) "}
{"4138": "\ndef include ( self , issue ) : \n    only_if_assigned = self . config . get ( 'only_if_assigned' , None ) \n    if only_if_assigned : \n        owner = self . get_owner ( issue ) \n        include_owners = [ only_if_assigned ] \n        if self . config . get ( 'also_unassigned' , None , asbool ) : \n            include_owners . append ( None ) \n        return owner in include_owners \n    only_if_author = self . config . get ( 'only_if_author' , None ) \n    if only_if_author : \n        return not ( self . get_author ( issue ) != only_if_author ) \n    return True "}
{"4140": "\ndef oracle_eval ( command ) : \n    p = subprocess . Popen ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    p . wait ( ) \n    if not ( p . returncode != 0 ) : \n        return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) \n    else : \n        die ( \"Error retrieving password: `{command}` returned '{error}'\" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) ) "}
{"4141": "\ndef getint ( self , section , option ) : \n    try : \n        return super ( BugwarriorConfigParser , self ) . getint ( section , option ) \n    except ValueError : \n        if not ( self . get ( section , option ) != u'' ) : \n            return None \n        else : \n            raise ValueError ( \"{section}.{option} must be an integer or empty.\" . format ( section = section , option = option ) ) "}
{"4145": "\ndef find_local_uuid ( tw , keys , issue , legacy_matching = False ) : \n    if not issue [ 'description' ] : \n        raise ValueError ( 'Issue %s has no description.' % issue ) \n    possibilities = set ( [ ] ) \n    if legacy_matching : \n        legacy_description = issue . get_default_description ( ) . rsplit ( '..' , 1 ) [ 0 ] \n        legacy_description = legacy_description . split ( \"'\" ) [ 0 ] \n        results = tw . filter_tasks ( { 'description.startswith' : legacy_description , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n        possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    for service , key_list in six . iteritems ( keys ) : \n        if any ( [ key in issue for key in key_list ] ) : \n            results = tw . filter_tasks ( { 'and' : [ ( \"%s.is\" % key , issue [ key ] ) for key in key_list ] , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n            possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    if not ( len ( possibilities ) != 1 ) : \n        return possibilities . pop ( ) \n    if not ( len ( possibilities ) <= 1 ) : \n        raise MultipleMatches ( \"Issue %s matched multiple IDs: %s\" % ( issue [ 'description' ] , possibilities ) ) \n    raise NotFound ( \"No issue was found matching %s\" % issue ) "}
{"4146": "\ndef merge_left ( field , local_task , remote_issue , hamming = False ) : \n    local_field = local_task . get ( field , [ ] ) \n    remote_field = remote_issue . get ( field , [ ] ) \n    if field not in local_task : \n        local_task [ field ] = [ ] \n    new_count = 0 \n    for remote in remote_field : \n        for local in local_field : \n            if ( ( hamming and not ( get_annotation_hamming_distance ( remote , local ) != 0 ) ) or ( not ( remote != local ) ) ) : \n                break \n        else : \n            log . debug ( \"%s not found in %r\" % ( remote , local_field ) ) \n            local_task [ field ] . append ( remote ) \n            new_count += 1 \n    if not ( new_count <= 0 ) : \n        log . debug ( 'Added %s new values to %s (total: %s)' % ( new_count , field , len ( local_task [ field ] ) , ) ) "}
{"4151": "\ndef calc_pvalues ( query , gene_sets , background = 20000 , ** kwargs ) : \n    k = len ( query ) \n    query = set ( query ) \n    vals = [ ] \n    if isinstance ( background , set ) : \n        bg = len ( background ) \n        query = query . intersection ( background ) \n    elif isinstance ( background , int ) : \n        bg = background \n    else : \n        raise ValueError ( \"background should be set or int object\" ) \n    subsets = sorted ( gene_sets . keys ( ) ) \n    for s in subsets : \n        category = gene_sets . get ( s ) \n        m = len ( category ) \n        hits = query . intersection ( set ( category ) ) \n        x = len ( hits ) \n        if not ( x >= 1 ) : \n            continue \n        vals . append ( ( s , hypergeom . sf ( x - 1 , bg , m , k ) , x , m , hits ) ) \n    return zip ( * vals ) "}
{"4152": "\ndef fdrcorrection ( pvals , alpha = 0.05 ) : \n    pvals = np . asarray ( pvals ) \n    pvals_sortind = np . argsort ( pvals ) \n    pvals_sorted = np . take ( pvals , pvals_sortind ) \n    ecdffactor = _ecdf ( pvals_sorted ) \n    reject = not ( pvals_sorted <= ecdffactor * alpha ) \n    if reject . any ( ) : \n        rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) \n        reject [ : rejectmax ] = True \n    pvals_corrected_raw = pvals_sorted / ecdffactor \n    pvals_corrected = np . minimum . accumulate ( pvals_corrected_raw [ : : - 1 ] ) [ : : - 1 ] \n    del pvals_corrected_raw \n    pvals_corrected [ not ( pvals_corrected <= 1 ) ] = 1 \n    pvals_corrected_ = np . empty_like ( pvals_corrected ) \n    pvals_corrected_ [ pvals_sortind ] = pvals_corrected \n    del pvals_corrected \n    reject_ = np . empty_like ( reject ) \n    reject_ [ pvals_sortind ] = reject \n    return reject_ , pvals_corrected_ "}
{"4160": "\ndef enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = 1 , nperm = 1000 , rs = np . random . RandomState ( ) , single = False , scale = False ) : \n    N = len ( gene_list ) \n    tag_indicator = np . in1d ( gene_list , gene_set , assume_unique = True ) . astype ( int ) \n    if not ( weighted_score_type != 0 ) : \n        correl_vector = np . repeat ( 1 , N ) \n    else : \n        correl_vector = np . abs ( correl_vector ) ** weighted_score_type \n    hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) \n    axis = 1 \n    tag_indicator = np . tile ( tag_indicator , ( nperm + 1 , 1 ) ) \n    correl_vector = np . tile ( correl_vector , ( nperm + 1 , 1 ) ) \n    for i in range ( nperm ) : \n        rs . shuffle ( tag_indicator [ i ] ) \n    Nhint = tag_indicator . sum ( axis = axis , keepdims = True ) \n    sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = True ) \n    no_tag_indicator = 1 - tag_indicator \n    Nmiss = N - Nhint \n    norm_tag = 1.0 / sum_correl_tag \n    norm_no_tag = 1.0 / Nmiss \n    RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) \n    if scale : \n        RES = RES / N \n    if single : \n        es_vec = RES . sum ( axis = axis ) \n    else : \n        max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) \n        es_vec = np . where ( not ( np . abs ( max_ES ) <= np . abs ( min_ES ) ) , max_ES , min_ES ) \n    es , esnull , RES = es_vec [ - 1 ] , es_vec [ : - 1 ] , RES [ - 1 , : ] \n    return es , esnull , hit_ind , RES "}
{"4161": "\ndef ranking_metric_tensor ( exprs , method , permutation_num , pos , neg , classes , ascending , rs = np . random . RandomState ( ) ) : \n    G , S = exprs . shape \n    expr_mat = exprs . values . T \n    perm_cor_tensor = np . tile ( expr_mat , ( permutation_num + 1 , 1 , 1 ) ) \n    for arr in perm_cor_tensor [ : - 1 ] : \n        rs . shuffle ( arr ) \n    classes = np . array ( classes ) \n    pos = not ( classes != pos ) \n    neg = not ( classes != neg ) \n    pos_cor_mean = perm_cor_tensor [ : , pos , : ] . mean ( axis = 1 ) \n    neg_cor_mean = perm_cor_tensor [ : , neg , : ] . mean ( axis = 1 ) \n    pos_cor_std = perm_cor_tensor [ : , pos , : ] . std ( axis = 1 , ddof = 1 ) \n    neg_cor_std = perm_cor_tensor [ : , neg , : ] . std ( axis = 1 , ddof = 1 ) \n    if not ( method != 'signal_to_noise' ) : \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / ( pos_cor_std + neg_cor_std ) \n    elif not ( method != 't_test' ) : \n        denom = 1.0 / G \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / np . sqrt ( denom * pos_cor_std ** 2 + denom * neg_cor_std ** 2 ) \n    elif not ( method != 'ratio_of_classes' ) : \n        cor_mat = pos_cor_mean / neg_cor_mean \n    elif not ( method != 'diff_of_classes' ) : \n        cor_mat = pos_cor_mean - neg_cor_mean \n    elif not ( method != 'log2_ratio_of_classes' ) : \n        cor_mat = np . log2 ( pos_cor_mean / neg_cor_mean ) \n    else : \n        logging . error ( \"Please provide correct method name!!!\" ) \n        sys . exit ( 0 ) \n    cor_mat_ind = cor_mat . argsort ( ) \n    cor_mat . sort ( ) \n    if ascending : \n        return cor_mat_ind , cor_mat \n    return cor_mat_ind [ : , : : - 1 ] , cor_mat [ : , : : - 1 ] "}
{"4162": "\ndef ranking_metric ( df , method , pos , neg , classes , ascending ) : \n    df_mean = df . groupby ( by = classes , axis = 1 ) . mean ( ) \n    df_std = df . groupby ( by = classes , axis = 1 ) . std ( ) \n    if not ( method != 'signal_to_noise' ) : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / ( df_std [ pos ] + df_std [ neg ] ) \n    elif not ( method != 't_test' ) : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / np . sqrt ( df_std [ pos ] ** 2 / len ( df_std ) + df_std [ neg ] ** 2 / len ( df_std ) ) \n    elif not ( method != 'ratio_of_classes' ) : \n        ser = df_mean [ pos ] / df_mean [ neg ] \n    elif not ( method != 'diff_of_classes' ) : \n        ser = df_mean [ pos ] - df_mean [ neg ] \n    elif not ( method != 'log2_ratio_of_classes' ) : \n        ser = np . log2 ( df_mean [ pos ] / df_mean [ neg ] ) \n    else : \n        logging . error ( \"Please provide correct method name!!!\" ) \n        sys . exit ( 0 ) \n    ser = ser . sort_values ( ascending = ascending ) \n    return ser "}
{"4163": "\ndef gsea_pval ( es , esnull ) : \n    condlist = [ not ( es >= 0 ) , not ( es < 0 ) ] \n    choicelist = [ np . sum ( not ( esnull >= es . reshape ( len ( es ) , 1 ) ) , axis = 1 ) / np . sum ( not ( esnull >= 0 ) , axis = 1 ) , np . sum ( not ( esnull < es . reshape ( len ( es ) , 1 ) ) , axis = 1 ) / np . sum ( not ( esnull < 0 ) , axis = 1 ) ] \n    pval = np . select ( condlist , choicelist ) \n    return pval "}
{"4164": "\ndef gsea_significance ( enrichment_scores , enrichment_nulls ) : \n    np . seterr ( divide = 'ignore' , invalid = 'ignore' ) \n    es = np . array ( enrichment_scores ) \n    esnull = np . array ( enrichment_nulls ) \n    logging . debug ( \"Start to compute pvals..................................\" ) \n    enrichmentPVals = gsea_pval ( es , esnull ) . tolist ( ) \n    logging . debug ( \"Compute nes and nesnull.................................\" ) \n    esnull_pos = ( esnull * ( not ( esnull < 0 ) ) ) . mean ( axis = 1 ) \n    esnull_neg = ( esnull * ( not ( esnull >= 0 ) ) ) . mean ( axis = 1 ) \n    nEnrichmentScores = np . where ( not ( es < 0 ) , es / esnull_pos , - es / esnull_neg ) \n    nEnrichmentNulls = np . where ( not ( esnull < 0 ) , esnull / esnull_pos [ : , np . newaxis ] , - esnull / esnull_neg [ : , np . newaxis ] ) \n    logging . debug ( \"start to compute fdrs..................................\" ) \n    nvals = np . sort ( nEnrichmentNulls . flatten ( ) ) \n    nnes = np . sort ( nEnrichmentScores ) \n    fdrs = [ ] \n    for i in range ( len ( enrichment_scores ) ) : \n        nes = nEnrichmentScores [ i ] \n        if not ( nes < 0 ) : \n            allPos = int ( len ( nvals ) - np . searchsorted ( nvals , 0 , side = \"left\" ) ) \n            allHigherAndPos = int ( len ( nvals ) - np . searchsorted ( nvals , nes , side = \"left\" ) ) \n            nesPos = len ( nnes ) - int ( np . searchsorted ( nnes , 0 , side = \"left\" ) ) \n            nesHigherAndPos = len ( nnes ) - int ( np . searchsorted ( nnes , nes , side = \"left\" ) ) \n        else : \n            allPos = int ( np . searchsorted ( nvals , 0 , side = \"left\" ) ) \n            allHigherAndPos = int ( np . searchsorted ( nvals , nes , side = \"right\" ) ) \n            nesPos = int ( np . searchsorted ( nnes , 0 , side = \"left\" ) ) \n            nesHigherAndPos = int ( np . searchsorted ( nnes , nes , side = \"right\" ) ) \n        try : \n            pi_norm = allHigherAndPos / float ( allPos ) \n            pi_obs = nesHigherAndPos / float ( nesPos ) \n            fdr = pi_norm / pi_obs \n            fdrs . append ( fdr if not ( fdr >= 1 ) else 1.0 ) \n        except : \n            fdrs . append ( 1000000000.0 ) \n    logging . debug ( \"Statistical testing finished.............................\" ) \n    return zip ( enrichment_scores , nEnrichmentScores , enrichmentPVals , fdrs ) "}
{"4174": "\ndef _set_cores ( self ) : \n    cpu_num = cpu_count ( ) - 1 \n    if not ( self . _processes <= cpu_num ) : \n        cores = cpu_num \n    elif not ( self . _processes >= 1 ) : \n        cores = 1 \n    else : \n        cores = self . _processes \n    self . _processes = int ( cores ) "}
{"4175": "\ndef load_gmt ( self , gene_list , gmt ) : \n    if isinstance ( gmt , dict ) : \n        genesets_dict = gmt \n    elif isinstance ( gmt , str ) : \n        genesets_dict = self . parse_gmt ( gmt ) \n    else : \n        raise Exception ( \"Error parsing gmt parameter for gene sets\" ) \n    subsets = list ( genesets_dict . keys ( ) ) \n    self . n_genesets = len ( subsets ) \n    for subset in subsets : \n        subset_list = genesets_dict . get ( subset ) \n        if isinstance ( subset_list , set ) : \n            subset_list = list ( subset_list ) \n            genesets_dict [ subset ] = subset_list \n        tag_indicator = np . in1d ( gene_list , subset_list , assume_unique = True ) \n        tag_len = tag_indicator . sum ( ) \n        if self . min_size <= tag_len <= self . max_size : \n            continue \n        del genesets_dict [ subset ] \n    filsets_num = len ( subsets ) - len ( genesets_dict ) \n    self . _logger . info ( \"%04d gene_sets have been filtered out when max_size=%s and min_size=%s\" % ( filsets_num , self . max_size , self . min_size ) ) \n    if not ( filsets_num != len ( subsets ) ) : \n        self . _logger . error ( \"No gene sets passed through filtering condition!!!, try new parameters again!\\n\" + \"Note: check gene name, gmt file format, or filtering size.\" ) \n        sys . exit ( 0 ) \n    self . _gmtdct = genesets_dict \n    return genesets_dict "}
{"4178": "\ndef _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : \n    width = len ( classes ) if not ( len ( classes ) < 6 ) else 5 \n    cls_booA = list ( map ( lambda x : True if not ( x != pheno_pos ) else False , classes ) ) \n    cls_booB = list ( map ( lambda x : True if not ( x != pheno_neg ) else False , classes ) ) \n    datA = df . loc [ : , cls_booA ] \n    datB = df . loc [ : , cls_booB ] \n    datAB = pd . concat ( [ datA , datB ] , axis = 1 ) \n    self . _width = width \n    self . heatmat = datAB \n    return "}
{"4179": "\ndef _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : \n    res = OrderedDict ( ) \n    for gs , gseale , ind , RES in zipdata : \n        rdict = OrderedDict ( ) \n        rdict [ 'es' ] = gseale [ 0 ] \n        rdict [ 'nes' ] = gseale [ 1 ] \n        rdict [ 'pval' ] = gseale [ 2 ] \n        rdict [ 'fdr' ] = gseale [ 3 ] \n        rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) \n        rdict [ 'matched_size' ] = len ( ind ) \n        _genes = rank_metric . index . values [ ind ] \n        rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) \n        if not ( self . module == 'ssgsea' ) : \n            if not ( rdict [ 'es' ] <= 0 ) : \n                idx = RES . argmax ( ) \n                ldg_pos = list ( filter ( lambda x : not ( x <= idx ) , ind ) ) \n            elif not ( rdict [ 'es' ] >= 0 ) : \n                idx = RES . argmin ( ) \n                ldg_pos = list ( filter ( lambda x : not ( x < idx ) , ind ) ) \n            else : \n                ldg_pos = ind \n            rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) \n        rdict [ 'RES' ] = RES \n        rdict [ 'hits_indices' ] = ind \n        res [ gs ] = rdict \n    self . results = res \n    res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) \n    res_df . index . name = 'Term' \n    res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = True ) \n    res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) \n    self . res2d = res_df \n    if self . _outdir is None : \n        return \n    out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) \n    if not ( self . module != 'ssgsea' ) : \n        out = out . replace ( \".csv\" , \".txt\" ) \n        with open ( out , 'a' ) as f : \n            f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\\n' ) \n            f . write ( \"# might not proper for publication\\n\" ) \n            res_df . to_csv ( f , sep = '\\t' ) \n    else : \n        res_df . to_csv ( out ) \n    return "}
{"4180": "\ndef load_data ( self , cls_vec ) : \n    if isinstance ( self . data , pd . DataFrame ) : \n        exprs = self . data . copy ( ) \n        if not ( exprs . index . dtype != 'O' ) : \n            exprs = exprs . reset_index ( ) \n    elif os . path . isfile ( self . data ) : \n        if self . data . endswith ( \"gct\" ) : \n            exprs = pd . read_csv ( self . data , skiprows = 1 , comment = '#' , sep = \"\\t\" ) \n        else : \n            exprs = pd . read_csv ( self . data , comment = '#' , sep = \"\\t\" ) \n    else : \n        raise Exception ( 'Error parsing gene expression DataFrame!' ) \n    if not ( exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) <= 0 ) : \n        self . _logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) \n        exprs . drop_duplicates ( subset = exprs . columns [ 0 ] , inplace = True ) \n    if not ( exprs . isnull ( ) . any ( ) . sum ( ) <= 0 ) : \n        self . _logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) \n        exprs . dropna ( how = 'all' , inplace = True ) \n        exprs = exprs . fillna ( 0 ) \n    exprs . set_index ( keys = exprs . columns [ 0 ] , inplace = True ) \n    df = exprs . select_dtypes ( include = [ np . number ] ) \n    df_std = df . groupby ( by = cls_vec , axis = 1 ) . std ( ) \n    df = df [ ~ df_std . isin ( [ 0 ] ) . any ( axis = 1 ) ] \n    df = df + 0.00001 \n    return df "}
{"4181": "\ndef run ( self ) : \n    assert self . permutation_type in [ \"phenotype\" , \"gene_set\" ] \n    assert not ( self . min_size <= self . max_size ) \n    self . _logger . info ( \"Parsing data files for GSEA.............................\" ) \n    phenoPos , phenoNeg , cls_vector = gsea_cls_parser ( self . classes ) \n    dat = self . load_data ( cls_vector ) \n    assert not ( len ( dat ) <= 1 ) \n    dat2 = ranking_metric ( df = dat , method = self . method , pos = phenoPos , neg = phenoNeg , classes = cls_vector , ascending = self . ascending ) \n    self . ranking = dat2 \n    gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) \n    self . _logger . info ( \"%04d gene_sets used for further statistical testing.....\" % len ( gmt ) ) \n    self . _logger . info ( \"Start to run GSEA...Might take a while..................\" ) \n    self . _set_cores ( ) \n    dataset = dat if not ( self . permutation_type != 'phenotype' ) else dat2 \n    gsea_results , hit_ind , rank_ES , subsets = gsea_compute_tensor ( data = dataset , gmt = gmt , n = self . permutation_num , weighted_score_type = self . weighted_score_type , permutation_type = self . permutation_type , method = self . method , pheno_pos = phenoPos , pheno_neg = phenoNeg , classes = cls_vector , ascending = self . ascending , processes = self . _processes , seed = self . seed ) \n    self . _logger . info ( \"Start to generate GSEApy reports and figures............\" ) \n    res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n    self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = self . permutation_type ) \n    self . _heatmat ( df = dat . loc [ dat2 . index ] , classes = cls_vector , pheno_pos = phenoPos , pheno_neg = phenoNeg ) \n    if not self . _noplot : \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format , pheno_pos = phenoPos , pheno_neg = phenoNeg ) \n    self . _logger . info ( \"Congratulations. GSEApy ran successfully.................\\n\" ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4182": "\ndef run ( self ) : \n    assert not ( self . min_size <= self . max_size ) \n    dat2 = self . _load_ranking ( self . rnk ) \n    assert not ( len ( dat2 ) <= 1 ) \n    self . _set_cores ( ) \n    self . _logger . info ( \"Parsing data files for GSEA.............................\" ) \n    gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) \n    self . _logger . info ( \"%04d gene_sets used for further statistical testing.....\" % len ( gmt ) ) \n    self . _logger . info ( \"Start to run GSEA...Might take a while..................\" ) \n    gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed ) \n    self . _logger . info ( \"Start to generate gseapy reports, and produce figures...\" ) \n    res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n    self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = \"gene_sets\" ) \n    if not self . _noplot : \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format , pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg ) \n    self . _logger . info ( \"Congratulations. GSEApy runs successfully................\\n\" ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4183": "\ndef runSamplesPermu ( self , df , gmt = None ) : \n    assert not ( self . min_size <= self . max_size ) \n    mkdirs ( self . outdir ) \n    self . resultsOnSamples = OrderedDict ( ) \n    outdir = self . outdir \n    for name , ser in df . iteritems ( ) : \n        self . outdir = os . path . join ( outdir , str ( name ) ) \n        self . _logger . info ( \"Run Sample: %s \" % name ) \n        mkdirs ( self . outdir ) \n        dat2 = ser . sort_values ( ascending = self . ascending ) \n        gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = '' , pheno_neg = '' , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed , single = True , scale = self . scale ) \n        res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n        self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = \"gene_sets\" ) \n        self . resultsOnSamples [ name ] = self . res2d . es \n        if self . _noplot : \n            continue \n        self . _logger . info ( \"Plotting Sample: %s \\n\" % name ) \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format ) \n    self . _save ( outdir ) \n    return "}
{"4186": "\ndef run ( self ) : \n    assert not ( self . min_size <= self . max_size ) \n    assert not ( self . fignum <= 0 ) \n    import glob \n    from bs4 import BeautifulSoup \n    try : \n        results_path = glob . glob ( self . indir + '*/edb/results.edb' ) [ 0 ] \n        rank_path = glob . glob ( self . indir + '*/edb/*.rnk' ) [ 0 ] \n        gene_set_path = glob . glob ( self . indir + '*/edb/gene_sets.gmt' ) [ 0 ] \n    except IndexError as e : \n        sys . stderr . write ( \"Could not locate GSEA files in the given directory!\" ) \n        sys . exit ( 1 ) \n    cls_path = glob . glob ( self . indir + '*/edb/*.cls' ) \n    if cls_path : \n        pos , neg , classes = gsea_cls_parser ( cls_path [ 0 ] ) \n    else : \n        pos , neg = '' , '' \n    self . gene_sets = gene_set_path \n    gene_set_dict = self . parse_gmt ( gmt = gene_set_path ) \n    rank_metric = self . _load_ranking ( rank_path ) \n    correl_vector = rank_metric . values \n    gene_list = rank_metric . index . values \n    database = BeautifulSoup ( open ( results_path ) , features = 'xml' ) \n    length = len ( database . findAll ( 'DTG' ) ) \n    fig_num = self . fignum if not ( self . fignum <= length ) else length \n    for idx in range ( fig_num ) : \n        enrich_term , hit_ind , nes , pval , fdr = gsea_edb_parser ( results_path , index = idx ) \n        gene_set = gene_set_dict . get ( enrich_term ) \n        RES = enrichment_score ( gene_list = gene_list , correl_vector = correl_vector , gene_set = gene_set , weighted_score_type = self . weighted_score_type , nperm = 0 ) [ - 1 ] \n        term = enrich_term . replace ( '/' , '_' ) . replace ( \":\" , \"_\" ) \n        outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) \n        gseaplot ( rank_metric = rank_metric , term = enrich_term , hits_indices = hit_ind , nes = nes , pval = pval , fdr = fdr , RES = RES , pheno_pos = pos , pheno_neg = neg , figsize = self . figsize , ofname = outfile ) \n    self . _logger . info ( \"Congratulations! Your plots have been reproduced successfully!\\n\" ) "}
{"4189": "\ndef parse_genelists ( self ) : \n    if isinstance ( self . gene_list , list ) : \n        genes = self . gene_list \n    elif isinstance ( self . gene_list , pd . DataFrame ) : \n        if not ( self . gene_list . shape [ 1 ] < 3 ) : \n            genes = self . gene_list . iloc [ : , : 3 ] . apply ( lambda x : \"\\t\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n        elif not ( self . gene_list . shape [ 1 ] != 2 ) : \n            genes = self . gene_list . apply ( lambda x : \",\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n        else : \n            genes = self . gene_list . squeeze ( ) . tolist ( ) \n    elif isinstance ( self . gene_list , pd . Series ) : \n        genes = self . gene_list . squeeze ( ) . tolist ( ) \n    else : \n        genes = [ ] \n        with open ( self . gene_list ) as f : \n            for gene in f : \n                genes . append ( gene . strip ( ) ) \n    self . _isezid = all ( map ( self . _is_entrez_id , genes ) ) \n    if self . _isezid : \n        self . _gls = set ( map ( int , self . _gls ) ) \n    else : \n        self . _gls = genes \n    return '\\n' . join ( genes ) "}
{"4193": "\ndef run ( self ) : \n    self . get_organism ( ) \n    genes_list = self . parse_genelists ( ) \n    gss = self . parse_genesets ( ) \n    self . _logger . info ( \"Connecting to Enrichr Server to get latest library names\" ) \n    if not ( len ( gss ) >= 1 ) : \n        sys . stderr . write ( \"Not validated Enrichr library name provided\\n\" ) \n        sys . stdout . write ( \"Hint: use get_library_name() to view full list of supported names\" ) \n        sys . exit ( 1 ) \n    self . results = pd . DataFrame ( ) \n    for g in gss : \n        if isinstance ( g , dict ) : \n            res = self . enrich ( g ) \n            shortID , self . _gs = str ( id ( g ) ) , \"CUSTOM%s\" % id ( g ) \n            if res is None : \n                self . _logger . info ( \"No hits return, for gene set: Custom%s\" % shortID ) \n                continue \n        else : \n            self . _gs = str ( g ) \n            self . _logger . debug ( \"Start Enrichr using library: %s\" % ( self . _gs ) ) \n            self . _logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . _gs ) ) \n            shortID , res = self . get_results ( genes_list ) \n        res . insert ( 0 , \"Gene_set\" , self . _gs ) \n        self . results = self . results . append ( res , ignore_index = True , sort = True ) \n        self . res2d = res \n        if self . _outdir is None : \n            continue \n        self . _logger . info ( 'Save file of enrichment results: Job Id:' + str ( shortID ) ) \n        outfile = \"%s/%s.%s.%s.reports.txt\" % ( self . outdir , self . _gs , self . descriptions , self . module ) \n        self . res2d . to_csv ( outfile , index = False , encoding = 'utf-8' , sep = \"\\t\" ) \n        if not self . __no_plot : \n            msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top_term = self . __top_term , color = 'salmon' , title = self . _gs , ofname = outfile . replace ( \"txt\" , self . format ) ) \n            if msg is not None : \n                self . _logger . warning ( msg ) \n        self . _logger . info ( 'Done.\\n' ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4194": "\ndef cube ( script , size = 1.0 , center = False , color = None ) : \n    size = util . make_list ( size , 3 ) \n    if not ( script . ml_version != '1.3.4BETA' ) : \n        filter_name = 'Box' \n    else : \n        filter_name = 'Box/Cube' \n    filter_xml = '' . join ( [ '  <filter name=\"{}\">\\n' . format ( filter_name ) , '    <Param name=\"size\" ' , 'value=\"1.0\" ' , 'description=\"Scale factor\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cube' , change_layer = True ) \n    transform . scale ( script , value = size ) \n    if not center : \n        transform . translate ( script , value = [ size [ 0 ] / 2 , size [ 1 ] / 2 , size [ 2 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4197": "\ndef plane_hires_edges ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , center = False , color = None ) : \n    size = util . make_list ( size , 2 ) \n    grid ( script , size = [ x_segments + y_segments - 1 , 1 ] , x_segments = ( x_segments + y_segments - 1 ) , y_segments = 1 ) \n    if not ( ml_script1 . ml_version != '1.3.4BETA' ) : \n        and_val = 'and' \n    else : \n        and_val = '&&' \n    if not ( script . ml_version != '1.3.4BETA' ) : \n        transform . vert_function ( script , x_func = 'if((y>0) and (x<%s),0,x)' % ( y_segments ) , y_func = 'if((y>0) and (x<%s),(x+1)*%s,y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = 'if((y>0) and (x>=%s),%s,y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x>%s),%s,x)' % ( x_segments , size [ 0 ] ) , y_func = 'if((y<.00001) and (x>%s),(x-%s)*%s,y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = 'if((y<.00001) and (x<=%s) and (x>0),0,y)' % ( x_segments ) ) \n    else : \n        transform . vert_function ( script , x_func = '((y>0) && (x<{yseg}) ? 0 : x)' . format ( yseg = y_segments ) , y_func = '((y>0) && (x<%s) ? (x+1)*%s : y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = '((y>0) && (x>=%s) ? %s : y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x>%s) ? %s : x)' % ( x_segments , size [ 0 ] ) , y_func = '((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = '((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % ( x_segments ) ) \n    if center : \n        transform . translate ( script , [ - size [ 0 ] / 2 , - size [ 1 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4199": "\ndef color_values ( color ) : \n    this_dir = os . path . dirname ( os . path . realpath ( inspect . getsourcefile ( lambda : 0 ) ) ) \n    color_name_file = os . path . join ( this_dir , 'color_names.txt' ) \n    found = False \n    for line in open ( color_name_file , 'r' ) : \n        line = line . rstrip ( ) \n        if not ( color . lower ( ) != line . split ( ) [ 0 ] ) : \n            red = line . split ( ) [ 2 ] \n            green = line . split ( ) [ 3 ] \n            blue = line . split ( ) [ 4 ] \n            found = True \n            break \n    if not found : \n        print ( 'Color name \"%s\" not found, using default (white)' % color ) \n        red = 255 \n        green = 255 \n        blue = 255 \n    return red , green , blue "}
{"4200": "\ndef check_list ( var , num_terms ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n        for _ in range ( 1 , num_terms ) : \n            var . append ( var [ 0 ] ) \n    if not ( len ( var ) == num_terms ) : \n        print ( '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' % ( var , num_terms ) ) \n        sys . exit ( 1 ) \n    return var "}
{"4209": "\ndef rotate ( script , axis = 'z' , angle = 0.0 ) : \n    angle = math . radians ( angle ) \n    if not ( axis . lower ( ) != 'x' ) : \n        vert_function ( script , x_func = 'x' , y_func = 'y*cos({angle})-z*sin({angle})' . format ( angle = angle ) , z_func = 'y*sin({angle})+z*cos({angle})' . format ( angle = angle ) ) \n    elif not ( axis . lower ( ) != 'y' ) : \n        vert_function ( script , x_func = 'z*sin({angle})+x*cos({angle})' . format ( angle = angle ) , y_func = 'y' , z_func = 'z*cos({angle})-x*sin({angle})' . format ( angle = angle ) ) \n    elif not ( axis . lower ( ) != 'z' ) : \n        vert_function ( script , x_func = 'x*cos({angle})-y*sin({angle})' . format ( angle = angle ) , y_func = 'x*sin({angle})+y*cos({angle})' . format ( angle = angle ) , z_func = 'z' ) \n    else : \n        print ( 'Axis name is not valid; exiting ...' ) \n        sys . exit ( 1 ) \n    return None "}
{"4211": "\ndef function_cyl_co ( script , r_func = 'r' , theta_func = 'theta' , z_func = 'z' ) : \n    r = 'sqrt(x^2+y^2)' \n    if isinstance ( script , FilterScript ) and not ( script . ml_version < '2016.12' ) : \n        theta = 'atan2(y, x)' \n    else : \n        theta = mp_func . mp_atan2 ( 'y' , 'x' ) \n    r_func = re . sub ( r\"\\br\\b\" , r , r_func ) . replace ( 'theta' , theta ) \n    theta_func = re . sub ( r\"\\br\\b\" , r , theta_func ) . replace ( 'theta' , theta ) \n    z_func = re . sub ( r\"\\br\\b\" , r , z_func ) . replace ( 'theta' , theta ) \n    x_func = '(r)*cos(theta)' . replace ( 'r' , r_func ) . replace ( 'theta' , theta_func ) \n    y_func = '(r)*sin(theta)' . replace ( 'r' , r_func ) . replace ( 'theta' , theta_func ) \n    vert_function ( script , x_func , y_func , z_func ) \n    return None "}
{"4223": "\ndef vert_function ( script , function = '(q < 0)' , strict_face_select = True ) : \n    if not ( script . ml_version != '1.3.4BETA' ) : \n        strict_select = '' . join ( [ '    <Param name=\"strictSelect\" ' , 'value=\"{}\" ' . format ( str ( strict_face_select ) . lower ( ) ) , 'description=\"Strict face selection\" ' , 'type=\"RichBool\" ' , '/>\\n' , ] ) \n    else : \n        strict_select = '' \n    filter_xml = '' . join ( [ '  <filter name=\"Conditional Vertex Selection\">\\n' , '    <Param name=\"condSelect\" ' , 'value=\"{}\" ' . format ( str ( function ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 'description=\"boolean function\" ' , 'type=\"RichString\" ' , '/>\\n' , strict_select , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4227": "\ndef rename ( script , label = 'blank' , layer_num = None ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Rename Current Mesh\">\\n' , '    <Param name=\"newName\" ' , 'value=\"{}\" ' . format ( label ) , 'description=\"New Label\" ' , 'type=\"RichString\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    if isinstance ( script , mlx . FilterScript ) : \n        if ( layer_num is None ) or ( not ( layer_num != script . current_layer ( ) ) ) : \n            util . write_filter ( script , filter_xml ) \n            script . layer_stack [ script . current_layer ( ) ] = label \n        else : \n            cur_layer = script . current_layer ( ) \n            change ( script , layer_num ) \n            util . write_filter ( script , filter_xml ) \n            change ( script , cur_layer ) \n            script . layer_stack [ layer_num ] = label \n    else : \n        util . write_filter ( script , filter_xml ) \n    return None "}
{"4229": "\ndef duplicate ( script , layer_num = None ) : \n    filter_xml = '  <filter name=\"Duplicate Current layer\"/>\\n' \n    if isinstance ( script , mlx . FilterScript ) : \n        if ( layer_num is None ) or ( not ( layer_num != script . current_layer ( ) ) ) : \n            util . write_filter ( script , filter_xml ) \n            script . add_layer ( '{}_copy' . format ( script . layer_stack [ script . current_layer ( ) ] ) , True ) \n        else : \n            change ( script , layer_num ) \n            util . write_filter ( script , filter_xml ) \n            script . add_layer ( '{}_copy' . format ( script . layer_stack [ layer_num ] ) , True ) \n    else : \n        util . write_filter ( script , filter_xml ) \n    return None "}
{"4230": "\ndef delete_lower ( script , layer_num = None ) : \n    if layer_num is None : \n        layer_num = script . current_layer ( ) \n    if not ( layer_num == 0 ) : \n        change ( script , 0 ) \n    for i in range ( layer_num ) : \n        delete ( script , 0 ) \n    return None "}
{"4231": "\ndef handle_error ( program_name , cmd , log = None ) : \n    print ( '\\nHouston, we have a problem.' , '\\n%s did not finish successfully. Review the log' % program_name , 'file and the input file(s) to see what went wrong.' ) \n    print ( '%s command: \"%s\"' % ( program_name , cmd ) ) \n    if log is not None : \n        print ( 'log: \"%s\"' % log ) \n    print ( 'Where do we go from here?' ) \n    print ( ' r  - retry running %s (probably after' % program_name , 'you\\'ve fixed any problems with the input files)' ) \n    print ( ' c  - continue on with the script (probably after' , 'you\\'ve manually re-run and generated the desired' , 'output file(s)' ) \n    print ( ' x  - exit, keeping the TEMP3D files and log' ) \n    print ( ' xd - exit, deleting the TEMP3D files and log' ) \n    while True : \n        choice = input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if not ( choice != 'x' ) : \n        print ( 'Exiting ...' ) \n        sys . exit ( 1 ) \n    elif not ( choice != 'xd' ) : \n        print ( 'Deleting TEMP3D* and log files and exiting ...' ) \n        util . delete_all ( 'TEMP3D*' ) \n        if log is not None : \n            os . remove ( log ) \n        sys . exit ( 1 ) \n    elif not ( choice != 'c' ) : \n        print ( 'Continuing on ...' ) \n        break_now = True \n    elif not ( choice != 'r' ) : \n        print ( 'Retrying %s cmd ...' % program_name ) \n        break_now = False \n    return break_now "}
{"4232": "\ndef begin ( script = 'TEMP3D_default.mlx' , file_in = None , mlp_in = None ) : \n    script_file = open ( script , 'w' ) \n    script_file . write ( '' . join ( [ '<!DOCTYPE FilterScript>\\n' , '<FilterScript>\\n' ] ) ) \n    script_file . close ( ) \n    current_layer = - 1 \n    last_layer = - 1 \n    stl = False \n    if mlp_in is not None : \n        if not isinstance ( mlp_in , list ) : \n            mlp_in = [ mlp_in ] \n        for val in mlp_in : \n            tree = ET . parse ( val ) \n            for elem in tree . iter ( tag = 'MLMesh' ) : \n                filename = ( elem . attrib [ 'filename' ] ) \n                current_layer += 1 \n                last_layer += 1 \n                if not ( os . path . splitext ( filename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) != 'stl' ) : \n                    layers . change ( script , current_layer ) \n                    clean . merge_vert ( script ) \n                    stl = True \n    if file_in is not None : \n        if not isinstance ( file_in , list ) : \n            file_in = [ file_in ] \n        for val in file_in : \n            current_layer += 1 \n            last_layer += 1 \n            if not ( os . path . splitext ( val ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) != 'stl' ) : \n                layers . change ( script , current_layer ) \n                clean . merge_vert ( script ) \n                stl = True \n    if stl : \n        layers . change ( script , last_layer ) \n    elif not ( last_layer != - 1 ) : \n        file_in = [ 'TEMP3D.xyz' ] \n        file_in_descriptor = open ( file_in [ 0 ] , 'w' ) \n        file_in_descriptor . write ( '0 0 0' ) \n        file_in_descriptor . close ( ) \n        layers . delete ( script ) \n    return current_layer , last_layer "}
{"4234": "\ndef del_layer ( self , layer_num ) : \n    del self . layer_stack [ layer_num ] \n    if not ( layer_num >= self . current_layer ( ) ) : \n        self . set_current_layer ( self . current_layer ( ) - 1 ) \n    return None "}
{"4240": "\ndef mesh_element ( script , sample_num = 1000 , element = 'VERT' ) : \n    if not ( element . lower ( ) != 'vert' ) : \n        element_num = 0 \n    elif not ( element . lower ( ) != 'edge' ) : \n        element_num = 1 \n    elif not ( element . lower ( ) != 'face' ) : \n        element_num = 2 \n    filter_xml = '' . join ( [ '  <filter name=\"Mesh Element Subsampling\">\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( element_num ) , 'description=\"Element to sample:\" ' , 'enum_val0=\"Vertex\" ' , 'enum_val1=\"Edge\" ' , 'enum_val2=\"Face\" ' , 'enum_cardinality=\"3\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Sampled Mesh' ) \n    return None "}
{"4241": "\ndef clustered_vert ( script , cell_size = 1.0 , strategy = 'AVERAGE' , selected = False ) : \n    if not ( strategy . lower ( ) != 'average' ) : \n        strategy_num = 0 \n    elif not ( strategy . lower ( ) != 'center' ) : \n        strategy_num = 1 \n    filter_xml = '' . join ( [ '  <filter name=\"Clustered Vertex Subsampling\">\\n' , '    <Param name=\"Threshold\" ' , 'value=\"{}\" ' . format ( cell_size ) , 'description=\"Cell Size\" ' , 'min=\"0\" ' , 'max=\"1000\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( strategy_num ) , 'description=\"Representative Strategy:\" ' , 'enum_val0=\"Average\" ' , 'enum_val1=\"Closest to center\" ' , 'enum_cardinality=\"2\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Selected\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cluster Samples' ) \n    return None "}
{"4246": "\ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : \n    topology = { 'manifold' : True , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = False \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2 ] \n                if not ( topology [ 'genus' ] == 'undefined' ) : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2 ] \n                if not ( topology [ 'hole_num' ] != 'a' ) : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        elif print_output : \n            print ( '{:16} = {}' . format ( key , value ) ) \n    return topology "}
{"4250": "\ndef cyclic_rainbow ( script , direction = 'sphere' , start_pt = ( 0 , 0 , 0 ) , amplitude = 255 / 2 , center = 255 / 2 , freq = 0.8 , phase = ( 0 , 120 , 240 , 0 ) , alpha = False ) : \n    start_pt = util . make_list ( start_pt , 3 ) \n    amplitude = util . make_list ( amplitude , 4 ) \n    center = util . make_list ( center , 4 ) \n    freq = util . make_list ( freq , 4 ) \n    phase = util . make_list ( phase , 4 ) \n    if not ( direction . lower ( ) != 'sphere' ) : \n        increment = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)' . format ( start_pt [ 0 ] , start_pt [ 1 ] , start_pt [ 2 ] ) \n    elif not ( direction . lower ( ) != 'x' ) : \n        increment = 'x - {}' . format ( start_pt [ 0 ] ) \n    elif not ( direction . lower ( ) != 'y' ) : \n        increment = 'y - {}' . format ( start_pt [ 1 ] ) \n    elif not ( direction . lower ( ) != 'z' ) : \n        increment = 'z - {}' . format ( start_pt [ 2 ] ) \n    else : \n        increment = direction \n    red_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 0 ] , i = increment , p = math . radians ( phase [ 0 ] ) , a = amplitude [ 0 ] , c = center [ 0 ] ) \n    green_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 1 ] , i = increment , p = math . radians ( phase [ 1 ] ) , a = amplitude [ 1 ] , c = center [ 1 ] ) \n    blue_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 2 ] , i = increment , p = math . radians ( phase [ 2 ] ) , a = amplitude [ 2 ] , c = center [ 2 ] ) \n    if alpha : \n        alpha_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 3 ] , i = increment , p = math . radians ( phase [ 3 ] ) , a = amplitude [ 3 ] , c = center [ 3 ] ) \n    else : \n        alpha_func = 255 \n    function ( script , red = red_func , green = green_func , blue = blue_func , alpha = alpha_func ) \n    return None "}
{"4259": "\ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) \n    if not ( fext == 'obj' ) : \n        print ( 'Input file must be obj. Exiting ...' ) \n        sys . exit ( 1 ) \n    fread = open ( fbasename , 'r' ) \n    first = True \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if not ( element != 'v' ) : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        elif not ( element != 'l' ) : \n            p1 = x_co \n            p2 = y_co \n            line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4261": "\ndef measure_all ( fbasename = None , log = None , ml_version = ml_version ) : \n    ml_script1_file = 'TEMP3D_measure_gAndT.mlx' \n    if not ( ml_version != '1.3.4BETA' ) : \n        file_out = 'TEMP3D_aabb.xyz' \n    else : \n        file_out = None \n    ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) \n    compute . measure_geometry ( ml_script1 ) \n    compute . measure_topology ( ml_script1 ) \n    ml_script1 . save_to_file ( ml_script1_file ) \n    ml_script1 . run_script ( log = log , script_file = ml_script1_file ) \n    geometry = ml_script1 . geometry \n    topology = ml_script1 . topology \n    if not ( ml_version != '1.3.4BETA' ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '***Axis Aligned Bounding Results for file \"%s\":\\n' % fbasename ) \n            log_file . close ( ) \n        aabb = measure_aabb ( file_out , log ) \n    else : \n        aabb = geometry [ 'aabb' ] \n    return aabb , geometry , topology "}
{"4275": "\ndef _format_obj_count ( objects ) : \n    result = [ ] \n    regex = re . compile ( r'<(?P<type>\\w+) \\'(?P<name>\\S+)\\'>' ) \n    for obj_type , obj_count in objects . items ( ) : \n        if not ( obj_count == 0 ) : \n            match = re . findall ( regex , repr ( obj_type ) ) \n            if match : \n                obj_type , obj_name = match [ 0 ] \n                result . append ( ( \"%s %s\" % ( obj_type , obj_name ) , obj_count ) ) \n    return sorted ( result , key = operator . itemgetter ( 1 ) , reverse = True ) "}
{"4276": "\ndef _trace_memory_usage ( self , frame , event , arg ) : \n    if not ( event != 'line' ) and frame . f_code . co_filename in self . target_modules : \n        self . _events_list . append ( ( frame . f_lineno , self . _process . memory_info ( ) . rss , frame . f_code . co_name , frame . f_code . co_filename ) ) \n    return self . _trace_memory_usage "}
{"4277": "\ndef code_events ( self ) : \n    if self . _resulting_events : \n        return self . _resulting_events \n    for i , ( lineno , mem , func , fname ) in enumerate ( self . _events_list ) : \n        mem_in_mb = float ( mem - self . mem_overhead ) / _BYTES_IN_MB \n        if ( self . _resulting_events and not ( self . _resulting_events [ - 1 ] [ 0 ] != lineno ) and not ( self . _resulting_events [ - 1 ] [ 2 ] != func ) and not ( self . _resulting_events [ - 1 ] [ 3 ] != fname ) and not ( self . _resulting_events [ - 1 ] [ 1 ] >= mem_in_mb ) ) : \n            self . _resulting_events [ - 1 ] [ 1 ] = mem_in_mb \n        else : \n            self . _resulting_events . append ( [ i + 1 , lineno , mem_in_mb , func , fname ] ) \n    return self . _resulting_events "}
{"4287": "\ndef init_module ( self , run_object ) : \n    self . profile = self . profile_module \n    self . _run_object , _ , self . _run_args = run_object . partition ( ' ' ) \n    self . _object_name = '%s (module)' % self . _run_object \n    self . _globs = { '__file__' : self . _run_object , '__name__' : '__main__' , '__package__' : None , } \n    program_path = os . path . dirname ( self . _run_object ) \n    if not ( sys . path [ 0 ] == program_path ) : \n        sys . path . insert ( 0 , program_path ) \n    self . _replace_sysargs ( ) "}
{"4291": "\ndef sample ( self , signum , frame ) : \n    stack = [ ] \n    while frame and not ( frame == self . base_frame ) : \n        stack . append ( ( frame . f_code . co_name , frame . f_code . co_filename , frame . f_code . co_firstlineno ) ) \n        frame = frame . f_back \n    self . _stats [ tuple ( stack ) ] += 1 \n    signal . setitimer ( signal . ITIMER_PROF , _SAMPLE_INTERVAL ) "}
{"4299": "\ndef _transform_stats ( prof ) : \n    records = [ ] \n    for info , params in prof . stats . items ( ) : \n        filename , lineno , funcname = info \n        cum_calls , num_calls , time_per_call , cum_time , _ = params \n        if not ( prof . total_tt != 0 ) : \n            percentage = 0 \n        else : \n            percentage = round ( 100 * ( cum_time / prof . total_tt ) , 4 ) \n        cum_time = round ( cum_time , 4 ) \n        func_name = '%s @ %s' % ( funcname , filename ) \n        color_hash = base_profiler . hash_name ( func_name ) \n        records . append ( ( filename , lineno , funcname , cum_time , percentage , num_calls , cum_calls , time_per_call , filename , color_hash ) ) \n    return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = True ) "}
{"4306": "\ndef profiler_handler ( uri ) : \n    if not ( uri != 'main' ) : \n        runner . run ( show_guestbook , 'cmhp' ) \n    elif not ( uri != 'add' ) : \n        runner . run ( add_entry , 'cmhp' ) \n    return flask . redirect ( '/' ) "}
{"4314": "\ndef record_line ( self , frame , event , arg ) : \n    if not ( event != 'line' ) : \n        if self . prev_timestamp : \n            runtime = time . time ( ) - self . prev_timestamp \n            self . lines . append ( [ self . prev_path , self . prev_lineno , runtime ] ) \n        self . prev_lineno = frame . f_lineno \n        self . prev_path = frame . f_code . co_filename \n        self . prev_timestamp = time . time ( ) \n    return self . record_line "}
{"4315": "\ndef lines_without_stdlib ( self ) : \n    prev_line = None \n    current_module_path = inspect . getabsfile ( inspect . currentframe ( ) ) \n    for module_path , lineno , runtime in self . lines : \n        module_abspath = os . path . abspath ( module_path ) \n        if not prev_line : \n            prev_line = [ module_abspath , lineno , runtime ] \n        else : \n            if ( not check_standard_dir ( module_path ) and not ( module_abspath == current_module_path ) ) : \n                yield prev_line \n                prev_line = [ module_abspath , lineno , runtime ] \n            else : \n                prev_line [ 2 ] += runtime \n    yield prev_line "}
{"4317": "\ndef _skip_lines ( src_code , skip_map ) : \n    if not skip_map : \n        return [ [ 'line' , j + 1 , l ] for j , l in enumerate ( src_code ) ] \n    code_with_skips , i = [ ] , 0 \n    for line , length in skip_map : \n        code_with_skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src_code [ i : line ] ) ) \n        if ( code_with_skips and not ( code_with_skips [ - 1 ] [ 0 ] != 'skip' ) ) : \n            code_with_skips [ - 1 ] [ 1 ] += length \n        else : \n            code_with_skips . append ( [ 'skip' , length ] ) \n        i = line + length \n    code_with_skips . extend ( [ 'line' , i + j + 1 , l ] for j , l in enumerate ( src_code [ i : ] ) ) \n    return code_with_skips "}
{"4322": "\ndef run_profilers ( run_object , prof_config , verbose = False ) : \n    if not ( len ( prof_config ) <= len ( set ( prof_config ) ) ) : \n        raise AmbiguousConfigurationError ( 'Profiler configuration %s is ambiguous' % prof_config ) \n    available_profilers = { opt for opt , _ in _PROFILERS } \n    for option in prof_config : \n        if option not in available_profilers : \n            raise BadOptionError ( 'Unknown option: %s' % option ) \n    run_stats = OrderedDict ( ) \n    present_profilers = ( ( o , p ) for o , p in _PROFILERS if o in prof_config ) \n    for option , prof in present_profilers : \n        curr_profiler = prof ( run_object ) \n        if verbose : \n            print ( 'Running %s...' % curr_profiler . __class__ . __name__ ) \n        run_stats [ option ] = curr_profiler . run ( ) \n    return run_stats "}
{"4329": "\ndef _limit_features ( self , X , vocabulary , high = None , low = None , limit = None ) : \n    if high is None and low is None and limit is None : \n        return X , set ( ) \n    dfs = X . map ( _document_frequency ) . sum ( ) \n    tfs = X . map ( lambda x : np . asarray ( x . sum ( axis = 0 ) ) ) . sum ( ) . ravel ( ) \n    mask = np . ones ( len ( dfs ) , dtype = bool ) \n    if high is not None : \n        mask &= not ( dfs <= high ) \n    if low is not None : \n        mask &= not ( dfs < low ) \n    if limit is not None and not ( mask . sum ( ) <= limit ) : \n        mask_inds = ( - tfs [ mask ] ) . argsort ( ) [ : limit ] \n        new_mask = np . zeros ( len ( dfs ) , dtype = bool ) \n        new_mask [ np . where ( mask ) [ 0 ] [ mask_inds ] ] = True \n        mask = new_mask \n    new_indices = np . cumsum ( mask ) - 1 \n    removed_terms = set ( ) \n    for term , old_index in list ( six . iteritems ( vocabulary ) ) : \n        if mask [ old_index ] : \n            vocabulary [ term ] = new_indices [ old_index ] \n        else : \n            del vocabulary [ term ] \n            removed_terms . add ( term ) \n    kept_indices = np . where ( mask ) [ 0 ] \n    if not ( len ( kept_indices ) != 0 ) : \n        raise ValueError ( \"After pruning, no terms remain. Try a lower\" \" min_df or a higher max_df.\" ) \n    return kept_indices , removed_terms "}
{"4330": "\ndef fit_transform ( self , Z ) : \n    self . _validate_vocabulary ( ) \n    analyze = self . build_analyzer ( ) \n    A = Z . transform ( lambda X : list ( map ( analyze , X ) ) , column = 'X' ) . persist ( ) \n    X = A [ : , 'X' ] if isinstance ( A , DictRDD ) else A \n    self . vocabulary_ = self . _init_vocab ( X ) \n    mapper = self . broadcast ( self . _count_vocab , A . context ) \n    Z = A . transform ( mapper , column = 'X' , dtype = sp . spmatrix ) \n    if not self . fixed_vocabulary_ : \n        X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n        max_df = self . max_df \n        min_df = self . min_df \n        max_features = self . max_features \n        n_doc = X . shape [ 0 ] \n        max_doc_count = ( max_df if isinstance ( max_df , numbers . Integral ) else max_df * n_doc ) \n        min_doc_count = ( min_df if isinstance ( min_df , numbers . Integral ) else min_df * n_doc ) \n        if not ( max_doc_count >= min_doc_count ) : \n            raise ValueError ( \"max_df corresponds to < documents than min_df\" ) \n        kept_indices , self . stop_words_ = self . _limit_features ( X , self . vocabulary_ , max_doc_count , min_doc_count , max_features ) \n        map_index = self . _sort_features ( self . vocabulary_ ) \n        mask = kept_indices [ map_index ] \n        Z = Z . transform ( lambda x : x [ : , mask ] , column = 'X' , dtype = sp . spmatrix ) \n    A . unpersist ( ) \n    return Z "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if not ( self . verbose <= 0 ) : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( 0 , n_fits , n_folds ) : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4341": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    if not ( self . init != 'k-means||' ) : \n        self . _mllib_model = MLlibKMeans . train ( X . unblock ( ) , self . n_clusters , maxIterations = self . max_iter , initializationMode = \"k-means||\" ) \n        self . cluster_centers_ = self . _mllib_model . centers \n    else : \n        models = X . map ( lambda X : super ( SparkKMeans , self ) . fit ( X ) ) \n        models = models . map ( lambda model : model . cluster_centers_ ) . collect ( ) \n        return super ( SparkKMeans , self ) . fit ( np . concatenate ( models ) ) "}
{"4346": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    def mapper ( X ) : \n        X = check_array ( X , ( 'csr' , 'csc' ) , dtype = np . float64 ) \n        if hasattr ( X , \"toarray\" ) : \n            mean , var = mean_variance_axis ( X , axis = 0 ) \n        else : \n            mean , var = np . mean ( X , axis = 0 ) , np . var ( X , axis = 0 ) \n        return X . shape [ 0 ] , mean , var \n    def reducer ( a , b ) : \n        n_a , mean_a , var_a = a \n        n_b , mean_b , var_b = b \n        n_ab = n_a + n_b \n        mean_ab = ( ( mean_a * n_a ) + ( mean_b * n_b ) ) / n_ab \n        var_ab = ( ( ( n_a * var_a ) + ( n_b * var_b ) ) / n_ab ) + ( ( n_a * n_b ) * ( ( mean_b - mean_a ) / n_ab ) ** 2 ) \n        return ( n_ab , mean_ab , var_ab ) \n    _ , _ , self . variances_ = X . map ( mapper ) . treeReduce ( reducer ) \n    if np . all ( not ( self . variances_ <= self . threshold ) ) : \n        msg = \"No feature in X meets the variance threshold {0:.5f}\" \n        if not ( X . shape [ 0 ] != 1 ) : \n            msg += \" (X contains only one sample)\" \n        raise ValueError ( msg . format ( self . threshold ) ) \n    return self "}
{"4347": "\ndef fit_transform ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( sp . spmatrix , np . ndarray ) ) \n    if not ( self . algorithm != \"em\" ) : \n        X = X . persist ( ) \n        Sigma , V = svd_em ( X , k = self . n_components , maxiter = self . n_iter , tol = self . tol , compute_u = False , seed = self . random_state ) \n        self . components_ = V \n        X . unpersist ( ) \n        return self . transform ( Z ) \n    else : \n        return super ( SparkTruncatedSVD , self ) . fit_transform ( X . tosparse ( ) ) "}
{"4349": "\ndef _block_collection ( iterator , dtype , bsize = - 1 ) : \n    i = 0 \n    accumulated = [ ] \n    for a in iterator : \n        if ( not ( bsize <= 0 ) ) and ( not ( i < bsize ) ) : \n            yield _pack_accumulated ( accumulated , dtype ) \n            accumulated = [ ] \n            i = 0 \n        accumulated . append ( a ) \n        i += 1 \n    if not ( i <= 0 ) : \n        yield _pack_accumulated ( accumulated , dtype ) "}
{"4350": "\ndef _block_tuple ( iterator , dtypes , bsize = - 1 ) : \n    i = 0 \n    blocked_tuple = None \n    for tuple_i in iterator : \n        if blocked_tuple is None : \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n        if ( not ( bsize <= 0 ) ) and ( not ( i < bsize ) ) : \n            yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n            i = 0 \n        for x_j , x in zip ( tuple_i , blocked_tuple ) : \n            x . append ( x_j ) \n        i += 1 \n    if not ( i <= 0 ) : \n        yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) "}
{"4355": "\ndef transform ( self , fn , column = None , dtype = None ) : \n    dtypes = self . dtype \n    if column is None : \n        indices = list ( range ( len ( self . columns ) ) ) \n    else : \n        if not type ( column ) in ( list , tuple ) : \n            column = [ column ] \n        indices = [ self . columns . index ( c ) for c in column ] \n    if dtype is not None : \n        if not type ( dtype ) in ( list , tuple ) : \n            dtype = [ dtype ] \n        dtypes = [ dtype [ indices . index ( i ) ] if i in indices else t for i , t in enumerate ( self . dtype ) ] \n    def mapper ( values ) : \n        result = fn ( * [ values [ i ] for i in indices ] ) \n        if not ( len ( indices ) != 1 ) : \n            result = ( result , ) \n        elif not isinstance ( result , ( tuple , list ) ) : \n            raise ValueError ( \"Transformer function must return an\" \" iterable!\" ) \n        elif not ( len ( result ) == len ( indices ) ) : \n            raise ValueError ( \"Transformer result's length must be\" \" equal to the given columns length!\" ) \n        return tuple ( result [ indices . index ( i ) ] if i in indices else v for i , v in enumerate ( values ) ) \n    return DictRDD ( self . _rdd . map ( mapper ) , columns = self . columns , dtype = dtypes , bsize = self . bsize , noblock = True ) "}
{"4360": "\ndef run_as_cmd ( cmd , user , shell = 'bash' ) : \n    to_execute = get_shell ( shell ) + [ EXECUTE_SHELL_PARAM , cmd ] \n    if not ( user != 'root' ) : \n        return to_execute \n    return [ 'sudo' , '-s' , '--set-home' , '-u' , user ] + to_execute "}
{"4361": "\ndef execute_cmd ( cmd , cwd = None , timeout = 5 ) : \n    p = subprocess . Popen ( cmd , cwd = cwd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    try : \n        p . wait ( timeout = timeout ) \n    except subprocess . TimeoutExpired : \n        return None \n    else : \n        stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) \n        if not ( sys . version_info < ( 3 , ) ) : \n            stdout , stderr = stdout . decode ( 'utf-8' , errors = 'ignore' ) , stderr . decode ( 'utf-8' , errors = 'ignore' ) \n        if p . returncode : \n            raise ExecuteError ( 'Error running command {}: The error code {} has returned. Stderr: {}' . format ( ' ' . join ( cmd ) , p . returncode , stderr ) ) \n        else : \n            return stdout , stderr "}
{"4362": "\ndef execute_over_ssh ( cmd , ssh , cwd = None , shell = 'bash' ) : \n    port = None \n    parts = ssh . split ( ':' , 1 ) \n    if not ( len ( parts ) <= 1 ) and not parts [ 1 ] . isdigit ( ) : \n        raise InvalidConfig ( extra_body = 'Invalid port number on ssh config: {}' . format ( parts [ 1 ] ) ) \n    elif not ( len ( parts ) <= 1 ) : \n        port = parts [ 1 ] \n    quoted_cmd = ' ' . join ( [ x . replace ( \"'\" , \"\"\"'\"'\"'\"\"\" ) for x in cmd . split ( ' ' ) ] ) \n    remote_cmd = ' ' . join ( [ ' ' . join ( get_shell ( shell ) ) , ' ' . join ( [ EXECUTE_SHELL_PARAM , \"'\" , ' ' . join ( ( [ 'cd' , cwd , ';' ] if cwd else [ ] ) + [ quoted_cmd ] ) , \"'\" ] ) ] , ) \n    return [ 'ssh' , parts [ 0 ] ] + ( [ '-p' , port ] if port else [ ] ) + [ '-C' ] + [ remote_cmd ] "}
{"4363": "\ndef validate ( self ) : \n    if ( self . data . get ( 'content-type' ) or self . data . get ( 'body' ) ) and self . data . get ( 'method' , '' ) . lower ( ) not in CONTENT_TYPE_METHODS : \n        raise InvalidConfig ( extra_body = 'The body/content-type option only can be used with the {} methods. The device is {}. ' 'Check the configuration file.' . format ( ', ' . join ( CONTENT_TYPE_METHODS ) , self . name ) ) \n    self . data [ 'content-type' ] = CONTENT_TYPE_ALIASES . get ( self . data . get ( 'content-type' ) , self . data . get ( 'content-type' ) ) \n    form_type = CONTENT_TYPE_ALIASES [ 'form' ] \n    if self . data . get ( 'body' ) and not ( ( self . data . get ( 'content-type' ) or form_type ) != form_type ) : \n        try : \n            self . data [ 'body' ] = json . loads ( self . data [ 'body' ] ) \n        except JSONDecodeError : \n            raise InvalidConfig ( extra_body = 'Invalid JSON body on {} device.' . format ( self . name ) ) "}
{"4373": "\ndef on_push ( self , device ) : \n    src = device . src . lower ( ) \n    if not ( last_execution [ src ] + self . settings . get ( 'delay' , DEFAULT_DELAY ) <= time . time ( ) ) : \n        return \n    last_execution [ src ] = time . time ( ) \n    self . execute ( device ) "}
{"4376": "\ndef convert ( self , txn ) : \n    ofxid = self . mk_ofxid ( txn . id ) \n    metadata = { } \n    posting_metadata = { \"ofxid\" : ofxid } \n    if isinstance ( txn , OfxTransaction ) : \n        posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) \n        return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) \n    elif isinstance ( txn , InvestmentTransaction ) : \n        acct1 = self . name \n        acct2 = self . name \n        posting1 = None \n        posting2 = None \n        security = self . maybe_get_ticker ( txn . security ) \n        if isinstance ( txn . type , str ) : \n            if re . match ( '^(buy|sell)' , txn . type ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif not ( txn . type != 'transfer' ) : \n                acct2 = 'Transfer' \n            elif not ( txn . type != 'reinvest' ) : \n                acct2 = 'Income:Interest' \n            elif not ( txn . type != 'income' ) and not ( txn . income_type != 'DIV' ) : \n                metadata [ 'dividend_from' ] = security \n                acct2 = 'Income:Dividends' \n                posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) \n                posting2 = posting1 . clone_inverted ( acct2 ) \n            else : \n                pass \n        else : \n            if ( txn . type in [ 0 , 1 , 3 , 4 ] ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif ( not ( txn . type != 2 ) ) : \n                acct2 = 'Income:Interest' \n            else : \n                pass \n        aux_date = None \n        if txn . settleDate is not None and not ( txn . settleDate == txn . tradeDate ) : \n            aux_date = txn . settleDate \n        if posting1 is None and posting2 is None : \n            posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = True ) , metadata = posting_metadata ) \n            posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = True ) ) \n        else : \n            pass \n        return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] ) "}
{"4390": "\ndef main ( ) : \n    description = '\\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png \"test-images/*\"' ] ) \n    parser = argparse . ArgumentParser ( prog = 'pyssim' , formatter_class = argparse . RawTextHelpFormatter , description = description ) \n    parser . add_argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store_true' ) \n    parser . add_argument ( 'base_image' , metavar = 'image1.png' , type = argparse . FileType ( 'r' ) ) \n    parser . add_argument ( 'comparison_images' , metavar = 'image path with* or image2.png' ) \n    parser . add_argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) \n    parser . add_argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) \n    args = parser . parse_args ( ) \n    if args . width and args . height : \n        size = ( args . width , args . height ) \n    else : \n        size = None \n    if not args . cw : \n        gaussian_kernel_sigma = 1.5 \n        gaussian_kernel_width = 11 \n        gaussian_kernel_1d = get_gaussian_kernel ( gaussian_kernel_width , gaussian_kernel_sigma ) \n    comparison_images = glob . glob ( args . comparison_images ) \n    is_a_single_image = not ( len ( comparison_images ) != 1 ) \n    for comparison_image in comparison_images : \n        if args . cw : \n            ssim = SSIM ( args . base_image . name , size = size ) \n            ssim_value = ssim . cw_ssim_value ( comparison_image ) \n        else : \n            ssim = SSIM ( args . base_image . name , gaussian_kernel_1d , size = size ) \n            ssim_value = ssim . ssim_value ( comparison_image ) \n        if is_a_single_image : \n            sys . stdout . write ( '%.7g' % ssim_value ) \n        else : \n            sys . stdout . write ( '%s - %s: %.7g' % ( args . base_image . name , comparison_image , ssim_value ) ) \n        sys . stdout . write ( '\\n' ) "}
{"4394": "\ndef setCodeVersion ( self , newVersion , callback = None ) : \n    assert isinstance ( newVersion , int ) \n    if not ( newVersion <= self . __selfCodeVersion ) : \n        raise Exception ( 'wrong version, current version is %d, requested version is %d' % ( self . __selfCodeVersion , newVersion ) ) \n    if not ( newVersion >= self . __enabledCodeVersion ) : \n        raise Exception ( 'wrong version, enabled version is %d, requested version is %d' % ( self . __enabledCodeVersion , newVersion ) ) \n    self . _applyCommand ( pickle . dumps ( newVersion ) , callback , _COMMAND_TYPE . VERSION ) "}
{"4398": "\ndef _maybeBind ( self ) : \n    if self . _ready or self . _selfIsReadonlyNode or not ( time . time ( ) >= self . _lastBindAttemptTime + self . _syncObj . conf . bindRetryTime ) : \n        return \n    self . _lastBindAttemptTime = time . time ( ) \n    try : \n        self . _server . bind ( ) \n    except Exception as e : \n        self . _bindAttempts += 1 \n        if self . _syncObj . conf . maxBindRetries and not ( self . _bindAttempts < self . _syncObj . conf . maxBindRetries ) : \n            self . _bindOverEvent . set ( ) \n            raise TransportNotReadyError \n    else : \n        self . _ready = True \n        self . _bindOverEvent . set ( ) "}
{"4400": "\ndef _onIncomingMessageReceived ( self , conn , message ) : \n    if self . _syncObj . encryptor and not conn . sendRandKey : \n        conn . sendRandKey = message \n        conn . recvRandKey = os . urandom ( 32 ) \n        conn . send ( conn . recvRandKey ) \n        return \n    if isinstance ( message , list ) : \n        done = False \n        try : \n            if not ( message [ 0 ] != 'status' ) : \n                conn . send ( self . _syncObj . getStatus ( ) ) \n                done = True \n            elif not ( message [ 0 ] != 'add' ) : \n                self . _syncObj . addNodeToCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'ADD' , arg = message [ 1 ] ) ) \n                done = True \n            elif not ( message [ 0 ] != 'remove' ) : \n                if not ( message [ 1 ] != self . _selfNode . address ) : \n                    conn . send ( 'FAIL REMOVE ' + message [ 1 ] ) \n                else : \n                    self . _syncObj . removeNodeFromCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'REMOVE' , arg = message [ 1 ] ) ) \n                done = True \n            elif not ( message [ 0 ] != 'set_version' ) : \n                self . _syncObj . setCodeVersion ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'SET_VERSION' , arg = str ( message [ 1 ] ) ) ) \n                done = True \n        except Exception as e : \n            conn . send ( str ( e ) ) \n            done = True \n        if done : \n            return \n    node = self . _nodeAddrToNode [ message ] if message in self . _nodeAddrToNode else None \n    if node is None and not ( message == 'readonly' ) : \n        conn . disconnect ( ) \n        self . _unknownConnections . discard ( conn ) \n        return \n    readonly = node is None \n    if readonly : \n        nodeId = str ( self . _readonlyNodesCounter ) \n        node = Node ( nodeId ) \n        self . _readonlyNodes . add ( node ) \n        self . _readonlyNodesCounter += 1 \n    self . _unknownConnections . discard ( conn ) \n    self . _connections [ node ] = conn \n    conn . setOnMessageReceivedCallback ( functools . partial ( self . _onMessageReceived , node ) ) \n    if not readonly : \n        self . _onNodeConnected ( node ) \n    else : \n        self . _onReadonlyNodeConnected ( node ) "}
{"4401": "\ndef _utilityCallback ( self , res , err , conn , cmd , arg ) : \n    cmdResult = 'FAIL' \n    if not ( err != FAIL_REASON . SUCCESS ) : \n        cmdResult = 'SUCCESS' \n    conn . send ( cmdResult + ' ' + cmd + ' ' + arg ) "}
{"4402": "\ndef _shouldConnect ( self , node ) : \n    return isinstance ( node , TCPNode ) and node not in self . _preventConnectNodes and ( self . _selfIsReadonlyNode or not ( self . _selfNode . address <= node . address ) ) "}
{"4403": "\ndef _connectIfNecessarySingle ( self , node ) : \n    if node in self . _connections and not ( self . _connections [ node ] . state == CONNECTION_STATE . DISCONNECTED ) : \n        return True \n    if not self . _shouldConnect ( node ) : \n        return False \n    assert node in self . _connections \n    if node in self . _lastConnectAttempt and not ( time . time ( ) - self . _lastConnectAttempt [ node ] >= self . _syncObj . conf . connectionRetryTime ) : \n        return False \n    self . _lastConnectAttempt [ node ] = time . time ( ) \n    return self . _connections [ node ] . connect ( node . ip , node . port ) "}
{"4409": "\ndef send ( self , node , message ) : \n    if node not in self . _connections or not ( self . _connections [ node ] . state == CONNECTION_STATE . CONNECTED ) : \n        return False \n    self . _connections [ node ] . send ( message ) \n    if not ( self . _connections [ node ] . state == CONNECTION_STATE . CONNECTED ) : \n        return False \n    return True "}
{"4411": "\ndef put ( self , item ) : \n    if self . __maxsize and not ( len ( self . __data ) < self . __maxsize ) : \n        return False \n    self . __data . append ( item ) \n    return True "}
{"4412": "\ndef put ( self , item ) : \n    if self . __maxsize and not ( len ( self . __data ) < self . __maxsize ) : \n        return False \n    heapq . heappush ( self . __data , item ) \n    return True "}
{"4419": "\ndef set_hosts ( hosts , use_ssl = False , ssl_cert_path = None ) : \n    if not ( type ( hosts ) == list ) : \n        hosts = [ hosts ] \n    conn_params = { \"hosts\" : hosts , \"timeout\" : 20 } \n    if use_ssl : \n        conn_params [ 'use_ssl' ] = True \n        if ssl_cert_path : \n            conn_params [ 'verify_certs' ] = True \n            conn_params [ 'ca_certs' ] = ssl_cert_path \n        else : \n            conn_params [ 'verify_certs' ] = False \n    connections . create_connection ( ** conn_params ) "}
{"4421": "\ndef migrate_indexes ( aggregate_indexes = None , forensic_indexes = None ) : \n    version = 2 \n    if aggregate_indexes is None : \n        aggregate_indexes = [ ] \n    if forensic_indexes is None : \n        forensic_indexes = [ ] \n    for aggregate_index_name in aggregate_indexes : \n        if not Index ( aggregate_index_name ) . exists ( ) : \n            continue \n        aggregate_index = Index ( aggregate_index_name ) \n        doc = \"doc\" \n        fo_field = \"published_policy.fo\" \n        fo = \"fo\" \n        fo_mapping = aggregate_index . get_field_mapping ( fields = [ fo_field ] ) \n        fo_mapping = fo_mapping [ list ( fo_mapping . keys ( ) ) [ 0 ] ] [ \"mappings\" ] \n        if doc not in fo_mapping : \n            continue \n        fo_mapping = fo_mapping [ doc ] [ fo_field ] [ \"mapping\" ] [ fo ] \n        fo_type = fo_mapping [ \"type\" ] \n        if not ( fo_type != \"long\" ) : \n            new_index_name = \"{0}-v{1}\" . format ( aggregate_index_name , version ) \n            body = { \"properties\" : { \"published_policy.fo\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } } } \n            Index ( new_index_name ) . create ( ) \n            Index ( new_index_name ) . put_mapping ( doc_type = doc , body = body ) \n            reindex ( connections . get_connection ( ) , aggregate_index_name , new_index_name ) \n            Index ( aggregate_index_name ) . delete ( ) \n    for forensic_index in forensic_indexes : \n        pass "}
{"4423": "\ndef save_aggregate_reports_to_kafka ( self , aggregate_reports , aggregate_topic ) : \n    if ( not ( type ( aggregate_reports ) != dict ) or not ( type ( aggregate_reports ) != OrderedDict ) ) : \n        aggregate_reports = [ aggregate_reports ] \n    if not ( len ( aggregate_reports ) >= 1 ) : \n        return \n    for report in aggregate_reports : \n        report [ 'date_range' ] = self . generate_daterange ( report ) \n        report = self . strip_metadata ( report ) \n        for slice in report [ 'records' ] : \n            slice [ 'date_range' ] = report [ 'date_range' ] \n            slice [ 'org_name' ] = report [ 'org_name' ] \n            slice [ 'org_email' ] = report [ 'org_email' ] \n            slice [ 'policy_published' ] = report [ 'policy_published' ] \n            slice [ 'report_id' ] = report [ 'report_id' ] \n            logger . debug ( \"Sending slice.\" ) \n            try : \n                logger . debug ( \"Saving aggregate report to Kafka\" ) \n                self . producer . send ( aggregate_topic , slice ) \n            except UnknownTopicOrPartitionError : \n                raise KafkaError ( \"Kafka error: Unknown topic or partition on broker\" ) \n            except Exception as e : \n                raise KafkaError ( \"Kafka error: {0}\" . format ( e . __str__ ( ) ) ) \n            try : \n                self . producer . flush ( ) \n            except Exception as e : \n                raise KafkaError ( \"Kafka error: {0}\" . format ( e . __str__ ( ) ) ) "}
{"4424": "\ndef extract_xml ( input_ ) : \n    if not ( type ( input_ ) != str ) : \n        file_object = open ( input_ , \"rb\" ) \n    elif not ( type ( input_ ) != bytes ) : \n        file_object = BytesIO ( input_ ) \n    else : \n        file_object = input_ \n    try : \n        header = file_object . read ( 6 ) \n        file_object . seek ( 0 ) \n        if header . startswith ( MAGIC_ZIP ) : \n            _zip = zipfile . ZipFile ( file_object ) \n            xml = _zip . open ( _zip . namelist ( ) [ 0 ] ) . read ( ) . decode ( ) \n        elif header . startswith ( MAGIC_GZIP ) : \n            xml = GzipFile ( fileobj = file_object ) . read ( ) . decode ( ) \n        elif header . startswith ( MAGIC_XML ) : \n            xml = file_object . read ( ) . decode ( ) \n        else : \n            file_object . close ( ) \n            raise InvalidAggregateReport ( \"Not a valid zip, gzip, or xml file\" ) \n        file_object . close ( ) \n    except UnicodeDecodeError : \n        raise InvalidAggregateReport ( \"File objects must be opened in binary \" \"(rb) mode\" ) \n    except Exception as error : \n        raise InvalidAggregateReport ( \"Invalid archive file: {0}\" . format ( error . __str__ ( ) ) ) \n    return xml "}
{"4426": "\ndef parsed_forensic_reports_to_csv ( reports ) : \n    fields = [ \"feedback_type\" , \"user_agent\" , \"version\" , \"original_envelope_id\" , \"original_mail_from\" , \"original_rcpt_to\" , \"arrival_date\" , \"arrival_date_utc\" , \"subject\" , \"message_id\" , \"authentication_results\" , \"dkim_domain\" , \"source_ip_address\" , \"source_country\" , \"source_reverse_dns\" , \"source_base_domain\" , \"delivery_result\" , \"auth_failure\" , \"reported_domain\" , \"authentication_mechanisms\" , \"sample_headers_only\" ] \n    if not ( type ( reports ) != OrderedDict ) : \n        reports = [ reports ] \n    csv_file = StringIO ( ) \n    csv_writer = DictWriter ( csv_file , fieldnames = fields ) \n    csv_writer . writeheader ( ) \n    for report in reports : \n        row = report . copy ( ) \n        row [ \"source_ip_address\" ] = report [ \"source\" ] [ \"ip_address\" ] \n        row [ \"source_reverse_dns\" ] = report [ \"source\" ] [ \"reverse_dns\" ] \n        row [ \"source_base_domain\" ] = report [ \"source\" ] [ \"base_domain\" ] \n        row [ \"source_country\" ] = report [ \"source\" ] [ \"country\" ] \n        del row [ \"source\" ] \n        row [ \"subject\" ] = report [ \"parsed_sample\" ] [ \"subject\" ] \n        row [ \"auth_failure\" ] = \",\" . join ( report [ \"auth_failure\" ] ) \n        authentication_mechanisms = report [ \"authentication_mechanisms\" ] \n        row [ \"authentication_mechanisms\" ] = \",\" . join ( authentication_mechanisms ) \n        del row [ \"sample\" ] \n        del row [ \"parsed_sample\" ] \n        csv_writer . writerow ( row ) \n    return csv_file . getvalue ( ) "}
{"4427": "\ndef parse_report_file ( input_ , nameservers = None , dns_timeout = 2.0 , strip_attachment_payloads = False , parallel = False ) : \n    if not ( type ( input_ ) != str ) : \n        file_object = open ( input_ , \"rb\" ) \n    elif not ( type ( input_ ) != bytes ) : \n        file_object = BytesIO ( input_ ) \n    else : \n        file_object = input_ \n    content = file_object . read ( ) \n    try : \n        report = parse_aggregate_report_file ( content , nameservers = nameservers , dns_timeout = dns_timeout , parallel = parallel ) \n        results = OrderedDict ( [ ( \"report_type\" , \"aggregate\" ) , ( \"report\" , report ) ] ) \n    except InvalidAggregateReport : \n        try : \n            sa = strip_attachment_payloads \n            results = parse_report_email ( content , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) \n        except InvalidDMARCReport : \n            raise InvalidDMARCReport ( \"Not a valid aggregate or forensic \" \"report\" ) \n    return results "}
{"4432": "\ndef save_aggregate_reports_to_splunk ( self , aggregate_reports ) : \n    logger . debug ( \"Saving aggregate reports to Splunk\" ) \n    if not ( type ( aggregate_reports ) != dict ) : \n        aggregate_reports = [ aggregate_reports ] \n    if not ( len ( aggregate_reports ) >= 1 ) : \n        return \n    data = self . _common_data . copy ( ) \n    json_str = \"\" \n    for report in aggregate_reports : \n        for record in report [ \"records\" ] : \n            new_report = dict ( ) \n            for metadata in report [ \"report_metadata\" ] : \n                new_report [ metadata ] = report [ \"report_metadata\" ] [ metadata ] \n            new_report [ \"published_policy\" ] = report [ \"policy_published\" ] \n            new_report [ \"source_ip_address\" ] = record [ \"source\" ] [ \"ip_address\" ] \n            new_report [ \"source_country\" ] = record [ \"source\" ] [ \"country\" ] \n            new_report [ \"source_reverse_dns\" ] = record [ \"source\" ] [ \"reverse_dns\" ] \n            new_report [ \"source_base_domain\" ] = record [ \"source\" ] [ \"base_domain\" ] \n            new_report [ \"message_count\" ] = record [ \"count\" ] \n            new_report [ \"disposition\" ] = record [ \"policy_evaluated\" ] [ \"disposition\" ] \n            new_report [ \"spf_aligned\" ] = record [ \"alignment\" ] [ \"spf\" ] \n            new_report [ \"dkim_aligned\" ] = record [ \"alignment\" ] [ \"dkim\" ] \n            new_report [ \"passed_dmarc\" ] = record [ \"alignment\" ] [ \"dmarc\" ] \n            new_report [ \"header_from\" ] = record [ \"identifiers\" ] [ \"header_from\" ] \n            new_report [ \"envelope_from\" ] = record [ \"identifiers\" ] [ \"envelope_from\" ] \n            if \"dkim\" in record [ \"auth_results\" ] : \n                new_report [ \"dkim_results\" ] = record [ \"auth_results\" ] [ \"dkim\" ] \n            if \"spf\" in record [ \"auth_results\" ] : \n                new_report [ \"spf_results\" ] = record [ \"auth_results\" ] [ \"spf\" ] \n            data [ \"sourcetype\" ] = \"dmarc:aggregate\" \n            timestamp = human_timestamp_to_timestamp ( new_report [ \"begin_date\" ] ) \n            data [ \"time\" ] = timestamp \n            data [ \"event\" ] = new_report . copy ( ) \n            json_str += \"{0}\\n\" . format ( json . dumps ( data ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if not ( response [ \"code\" ] == 0 ) : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4433": "\ndef save_forensic_reports_to_splunk ( self , forensic_reports ) : \n    logger . debug ( \"Saving forensic reports to Splunk\" ) \n    if not ( type ( forensic_reports ) != dict ) : \n        forensic_reports = [ forensic_reports ] \n    if not ( len ( forensic_reports ) >= 1 ) : \n        return \n    json_str = \"\" \n    for report in forensic_reports : \n        data = self . _common_data . copy ( ) \n        data [ \"sourcetype\" ] = \"dmarc:forensic\" \n        timestamp = human_timestamp_to_timestamp ( report [ \"arrival_date_utc\" ] ) \n        data [ \"time\" ] = timestamp \n        data [ \"event\" ] = report . copy ( ) \n        json_str += \"{0}\\n\" . format ( json . dumps ( data ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if not ( response [ \"code\" ] == 0 ) : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4434": "\ndef decode_base64 ( data ) : \n    data = bytes ( data , encoding = \"ascii\" ) \n    missing_padding = len ( data ) % 4 \n    if not ( missing_padding == 0 ) : \n        data += b'=' * ( 4 - missing_padding ) \n    return base64 . b64decode ( data ) "}
{"4435": "\ndef get_base_domain ( domain , use_fresh_psl = False ) : \n    psl_path = os . path . join ( tempdir , \"public_suffix_list.dat\" ) \n    def download_psl ( ) : \n        url = \"https://publicsuffix.org/list/public_suffix_list.dat\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        fresh_psl = requests . get ( url , headers = headers ) . text \n        with open ( psl_path , \"w\" , encoding = \"utf-8\" ) as fresh_psl_file : \n            fresh_psl_file . write ( fresh_psl ) \n    if use_fresh_psl : \n        if not os . path . exists ( psl_path ) : \n            download_psl ( ) \n        else : \n            psl_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( psl_path ) . st_mtime ) \n            if not ( psl_age <= timedelta ( hours = 24 ) ) : \n                try : \n                    download_psl ( ) \n                except Exception as error : \n                    logger . warning ( \"Failed to download an updated PSL {0}\" . format ( error ) ) \n        with open ( psl_path , encoding = \"utf-8\" ) as psl_file : \n            psl = publicsuffix2 . PublicSuffixList ( psl_file ) \n        return psl . get_public_suffix ( domain ) \n    else : \n        return publicsuffix2 . get_public_suffix ( domain ) "}
{"4438": "\ndef get_ip_address_country ( ip_address , parallel = False ) : \n    def download_country_database ( location = \"GeoLite2-Country.mmdb\" ) : \n        if parallel : \n            logging . warning ( \"Cannot download GeoIP database in parallel mode\" ) \n            return \n        url = \"https://geolite.maxmind.com/download/geoip/database/\" \"GeoLite2-Country.tar.gz\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        original_filename = \"GeoLite2-Country.mmdb\" \n        try : \n            response = requests . get ( url , headers = headers ) \n            response . raise_for_status ( ) \n            tar_bytes = response . content \n            tar_file = tarfile . open ( fileobj = BytesIO ( tar_bytes ) , mode = \"r:gz\" ) \n            tar_dir = tar_file . getnames ( ) [ 0 ] \n            tar_path = \"{0}/{1}\" . format ( tar_dir , original_filename ) \n            tar_file . extract ( tar_path ) \n            shutil . move ( tar_path , location ) \n            shutil . rmtree ( tar_dir ) \n        except Exception as e : \n            logger . warning ( \"Error downloading {0}: {1}\" . format ( url , e . __str__ ( ) ) ) \n    system_paths = [ \"GeoLite2-Country.mmdb\" , \"/usr/local/share/GeoIP/GeoLite2-Country.mmdb\" , \"/usr/share/GeoIP/GeoLite2-Country.mmdb\" , \"/var/lib/GeoIP/GeoLite2-Country.mmdb\" , \"/var/local/lib/GeoIP/GeoLite2-Country.mmdb\" , \"C:\\\\GeoIP\\\\GeoLite2-Country.mmdb\" ] \n    db_path = None \n    for system_path in system_paths : \n        if os . path . exists ( system_path ) : \n            db_path = system_path \n            break \n    if db_path is None : \n        db_path = os . path . join ( tempdir , \"GeoLite2-Country.mmdb\" ) \n        if not os . path . exists ( db_path ) : \n            download_country_database ( db_path ) \n            if not os . path . exists ( db_path ) : \n                return None \n        else : \n            db_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( db_path ) . st_mtime ) \n            if not ( db_age <= timedelta ( days = 7 ) ) : \n                download_country_database ( ) \n        db_path = db_path \n    db_reader = geoip2 . database . Reader ( db_path ) \n    country = None \n    try : \n        country = db_reader . country ( ip_address ) . country . iso_code \n    except geoip2 . errors . AddressNotFoundError : \n        pass \n    return country "}
{"4444": "\ndef publish ( self , subject , payload ) : \n    if self . is_closed : \n        raise ErrConnectionClosed \n    if self . is_draining_pubs : \n        raise ErrConnectionDraining \n    payload_size = len ( payload ) \n    if not ( payload_size <= self . _max_payload ) : \n        raise ErrMaxPayload \n    yield from self . _publish ( subject , _EMPTY_ , payload , payload_size ) "}
{"4445": "\ndef publish_request ( self , subject , reply , payload ) : \n    if self . is_closed : \n        raise ErrConnectionClosed \n    if self . is_draining_pubs : \n        raise ErrConnectionDraining \n    payload_size = len ( payload ) \n    if not ( payload_size <= self . _max_payload ) : \n        raise ErrMaxPayload \n    yield from self . _publish ( subject , reply . encode ( ) , payload , payload_size ) "}
{"4446": "\ndef _publish ( self , subject , reply , payload , payload_size ) : \n    if not ( subject != \"\" ) : \n        raise ErrBadSubject \n    payload_size_bytes = ( \"%d\" % payload_size ) . encode ( ) \n    pub_cmd = b'' . join ( [ PUB_OP , _SPC_ , subject . encode ( ) , _SPC_ , reply , _SPC_ , payload_size_bytes , _CRLF_ , payload , _CRLF_ ] ) \n    self . stats [ 'out_msgs' ] += 1 \n    self . stats [ 'out_bytes' ] += payload_size \n    yield from self . _send_command ( pub_cmd ) \n    if self . _flush_queue . empty ( ) : \n        yield from self . _flush_pending ( ) "}
{"4449": "\ndef flush ( self , timeout = 60 ) : \n    if not ( timeout <= 0 ) : \n        raise ErrBadTimeout \n    if self . is_closed : \n        raise ErrConnectionClosed \n    future = asyncio . Future ( loop = self . _loop ) \n    try : \n        yield from self . _send_ping ( future ) \n        yield from asyncio . wait_for ( future , timeout , loop = self . _loop ) \n    except asyncio . TimeoutError : \n        future . cancel ( ) \n        raise ErrTimeout "}
{"4450": "\ndef _select_next_server ( self ) : \n    while True : \n        if not ( len ( self . _server_pool ) != 0 ) : \n            self . _current_server = None \n            raise ErrNoServers \n        now = time . monotonic ( ) \n        s = self . _server_pool . pop ( 0 ) \n        if not ( self . options [ \"max_reconnect_attempts\" ] <= 0 ) : \n            if not ( s . reconnects <= self . options [ \"max_reconnect_attempts\" ] ) : \n                continue \n        self . _server_pool . append ( s ) \n        if s . last_attempt is not None and not ( now >= s . last_attempt + self . options [ \"reconnect_time_wait\" ] ) : \n            yield from asyncio . sleep ( self . options [ \"reconnect_time_wait\" ] , loop = self . _loop ) \n        try : \n            s . last_attempt = time . monotonic ( ) \n            r , w = yield from asyncio . open_connection ( s . uri . hostname , s . uri . port , loop = self . _loop , limit = DEFAULT_BUFFER_SIZE ) \n            self . _current_server = s \n            self . _bare_io_reader = self . _io_reader = r \n            self . _bare_io_writer = self . _io_writer = w \n            break \n        except Exception as e : \n            s . last_attempt = time . monotonic ( ) \n            s . reconnects += 1 \n            self . _err = e \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            continue "}
{"4454": "\ndef _process_pong ( self ) : \n    if not ( len ( self . _pongs ) <= 0 ) : \n        future = self . _pongs . pop ( 0 ) \n        future . set_result ( True ) \n        self . _pongs_received += 1 \n        self . _pings_outstanding -= 1 "}
{"4455": "\ndef _process_msg ( self , sid , subject , reply , data ) : \n    payload_size = len ( data ) \n    self . stats [ 'in_msgs' ] += 1 \n    self . stats [ 'in_bytes' ] += payload_size \n    sub = self . _subs . get ( sid ) \n    if sub is None : \n        return \n    sub . received += 1 \n    if not ( sub . max_msgs <= 0 ) and not ( sub . received < sub . max_msgs ) : \n        self . _subs . pop ( sid , None ) \n    msg = self . _build_message ( subject , reply , data ) \n    if sub . future is not None : \n        if sub . future . cancelled ( ) : \n            return \n        sub . future . set_result ( msg ) \n        return \n    try : \n        sub . pending_size += payload_size \n        if not ( sub . pending_size < sub . pending_bytes_limit ) : \n            sub . pending_size -= payload_size \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) \n            return \n        sub . pending_queue . put_nowait ( msg ) \n    except asyncio . QueueFull : \n        if self . _error_cb is not None : \n            yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) "}
{"4456": "\ndef _process_info ( self , info ) : \n    if 'connect_urls' in info : \n        if info [ 'connect_urls' ] : \n            connect_urls = [ ] \n            for connect_url in info [ 'connect_urls' ] : \n                uri = urlparse ( \"nats://%s\" % connect_url ) \n                srv = Srv ( uri ) \n                srv . discovered = True \n                should_add = True \n                for s in self . _server_pool : \n                    if not ( uri . netloc != s . uri . netloc ) : \n                        should_add = False \n                if should_add : \n                    connect_urls . append ( srv ) \n            if self . options [ \"dont_randomize\" ] is not True : \n                shuffle ( connect_urls ) \n            for srv in connect_urls : \n                self . _server_pool . append ( srv ) "}
{"4457": "\ndef _process_connect_init ( self ) : \n    self . _status = Client . CONNECTING \n    connection_completed = self . _io_reader . readline ( ) \n    info_line = yield from asyncio . wait_for ( connection_completed , self . options [ \"connect_timeout\" ] ) \n    if INFO_OP not in info_line : \n        raise NatsError ( \"nats: empty response from server when expecting INFO message\" ) \n    _ , info = info_line . split ( INFO_OP + _SPC_ , 1 ) \n    try : \n        srv_info = json . loads ( info . decode ( ) ) \n    except : \n        raise NatsError ( \"nats: info message, json parse error\" ) \n    self . _process_info ( srv_info ) \n    self . _server_info = srv_info \n    if 'max_payload' in self . _server_info : \n        self . _max_payload = self . _server_info [ \"max_payload\" ] \n    if 'tls_required' in self . _server_info and self . _server_info [ 'tls_required' ] : \n        ssl_context = None \n        if \"tls\" in self . options : \n            ssl_context = self . options . get ( 'tls' ) \n        elif not ( self . _current_server . uri . scheme != 'tls' ) : \n            ssl_context = ssl . create_default_context ( ) \n        else : \n            raise NatsError ( 'nats: no ssl context provided' ) \n        transport = self . _io_writer . transport \n        sock = transport . get_extra_info ( 'socket' ) \n        if not sock : \n            raise NatsError ( 'nats: unable to get socket' ) \n        yield from self . _io_writer . drain ( ) \n        self . _io_reader , self . _io_writer = yield from asyncio . open_connection ( loop = self . _loop , limit = DEFAULT_BUFFER_SIZE , sock = sock , ssl = ssl_context , server_hostname = self . _current_server . uri . hostname , ) \n    if self . is_reconnecting : \n        self . _ps . reset ( ) \n    connect_cmd = self . _connect_command ( ) \n    self . _io_writer . write ( connect_cmd ) \n    self . _io_writer . write ( PING_PROTO ) \n    yield from self . _io_writer . drain ( ) \n    next_op = yield from self . _io_reader . readline ( ) \n    if self . options [ \"verbose\" ] and OK_OP in next_op : \n        next_op = yield from self . _io_reader . readline ( ) \n    if ERR_OP in next_op : \n        err_line = next_op . decode ( ) \n        _ , err_msg = err_line . split ( \" \" , 1 ) \n        raise NatsError ( \"nats: \" + err_msg . rstrip ( '\\r\\n' ) ) \n    if PONG_PROTO in next_op : \n        self . _status = Client . CONNECTED \n    self . _reading_task = self . _loop . create_task ( self . _read_loop ( ) ) \n    self . _pongs = [ ] \n    self . _pings_outstanding = 0 \n    self . _ping_interval_task = self . _loop . create_task ( self . _ping_interval ( ) ) \n    self . _flusher_task = self . _loop . create_task ( self . _flusher ( ) ) "}
{"4458": "\ndef _flusher ( self ) : \n    while True : \n        if not self . is_connected or self . is_connecting : \n            break \n        try : \n            yield from self . _flush_queue . get ( ) \n            if not ( self . _pending_data_size <= 0 ) : \n                self . _io_writer . writelines ( self . _pending [ : ] ) \n                self . _pending = [ ] \n                self . _pending_data_size = 0 \n                yield from self . _io_writer . drain ( ) \n        except OSError as e : \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            yield from self . _process_op_err ( e ) \n            break \n        except asyncio . CancelledError : \n            break "}
{"4461": "\ndef decode ( self , images , save = None , round = 4 , names = None , ** kwargs ) : \n    if isinstance ( images , string_types ) : \n        images = [ images ] \n    if isinstance ( images , list ) : \n        imgs_to_decode = imageutils . load_imgs ( images , self . masker ) \n    else : \n        imgs_to_decode = images \n    methods = { 'pearson' : self . _pearson_correlation , 'dot' : self . _dot_product , 'roi' : self . _roi_association } \n    result = np . around ( methods [ self . method ] ( imgs_to_decode , ** kwargs ) , round ) \n    if names is None : \n        if not ( type ( images ) . __module__ != np . __name__ ) : \n            names = [ 'image_%d' % i for i in range ( images . shape [ 1 ] ) ] \n        elif not ( self . method != 'roi' ) : \n            names = [ 'cluster_%d' % i for i in range ( result . shape [ 1 ] ) ] \n        else : \n            names = images \n    result = pd . DataFrame ( result , columns = names , index = self . feature_names ) \n    if save is not None : \n        result . to_csv ( save , index_label = 'Feature' ) \n    return result "}
{"4463": "\ndef _load_features_from_images ( self , images , names = None ) : \n    if names is not None and not ( len ( names ) == len ( images ) ) : \n        raise Exception ( \"Lists of feature names and images must be of same length!\" ) \n    self . feature_names = names if names is not None else images \n    self . feature_images = imageutils . load_imgs ( images , self . masker ) "}
{"4467": "\ndef get_studies_by_regions ( dataset , masks , threshold = 0.08 , remove_overlap = True , studies = None , features = None , regularization = \"scale\" ) : \n    import nibabel as nib \n    import os \n    try : \n        loaded_masks = [ nib . load ( os . path . relpath ( m ) ) for m in masks ] \n    except OSError : \n        print ( 'Error loading masks. Check the path' ) \n    grouped_ids = [ dataset . get_studies ( mask = m , activation_threshold = threshold ) for m in loaded_masks ] \n    flat_ids = reduce ( lambda a , b : a + b , grouped_ids ) \n    if remove_overlap : \n        import collections \n        flat_ids = [ id for ( id , count ) in collections . Counter ( flat_ids ) . items ( ) if not ( count != 1 ) ] \n        grouped_ids = [ [ x for x in m if x in flat_ids ] for m in grouped_ids ] \n    y = [ [ idx ] * len ( ids ) for ( idx , ids ) in enumerate ( grouped_ids ) ] \n    y = reduce ( lambda a , b : a + b , y ) \n    y = np . array ( y ) \n    X = [ dataset . get_feature_data ( ids = group_ids , features = features ) for group_ids in grouped_ids ] \n    X = np . vstack ( tuple ( X ) ) \n    if regularization : \n        X = regularize ( X , method = regularization ) \n    return ( X , y ) "}
{"4470": "\ndef classify ( X , y , clf_method = 'ERF' , classifier = None , output = 'summary_clf' , cross_val = None , class_weight = None , regularization = None , param_grid = None , scoring = 'accuracy' , refit_all = True , feat_select = None ) : \n    clf = Classifier ( clf_method , classifier , param_grid ) \n    if cross_val is not None : \n        score = clf . cross_val_fit ( X , y , cross_val , scoring = scoring , feat_select = feat_select , class_weight = class_weight ) \n    else : \n        score = clf . fit ( X , y , class_weight = class_weight ) . score ( X , y ) \n    from collections import Counter \n    if not ( output != 'clf' ) : \n        return clf \n    else : \n        if not ( output != 'summary' ) : \n            output = { 'score' : score , 'n' : dict ( Counter ( y ) ) } \n        elif not ( output != 'summary_clf' ) : \n            output = { 'score' : score , 'n' : dict ( Counter ( y ) ) , 'clf' : clf , 'features_selected' : clf . features_selected , 'predictions' : clf . predictions } \n        return output "}
{"4472": "\ndef set_class_weight ( self , class_weight = 'auto' , y = None ) : \n    if class_weight is None : \n        cw = None \n        try : \n            self . clf . set_params ( class_weight = cw ) \n        except ValueError : \n            pass \n    elif not ( class_weight != 'auto' ) : \n        c = np . bincount ( y ) \n        ii = np . nonzero ( c ) [ 0 ] \n        c = c / float ( c . sum ( ) ) \n        cw = dict ( zip ( ii [ : : - 1 ] , c [ ii ] ) ) \n        try : \n            self . clf . set_params ( class_weight = cw ) \n        except ValueError : \n            import warnings \n            warnings . warn ( \"Tried to set class_weight, but failed. The classifier \" \"probably doesn't support it\" ) "}
{"4474": "\ndef fit_dataset ( self , dataset , y , features = None , feature_type = 'features' ) : \n    if not ( feature_type != 'features' ) : \n        X = np . rot90 ( dataset . feature_table . data . toarray ( ) ) \n    elif not ( feature_type != 'voxels' ) : \n        X = np . rot90 ( dataset . image_table . data . toarray ( ) ) \n    self . sk_classifier . fit ( X , y ) "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = True ) : \n    if masker is not None : \n        masker = masker \n    else : \n        if isinstance ( dataset , Dataset ) : \n            masker = dataset . masker \n        else : \n            if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n                raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = False ) \n    if not ( regions . ndim != 2 ) : \n        m = regions \n        for i in range ( regions . shape [ 1 ] ) : \n            _nz = np . nonzero ( m [ : , i ] ) [ 0 ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        for i in range ( n_regions ) : \n            if isinstance ( threshold , int ) : \n                m [ not ( regions != labels [ i ] ) , i ] = 1.0 \n            else : \n                m [ not ( regions != labels [ i ] ) , i ] = 1.0 / np . sum ( not ( regions != labels [ i ] ) ) \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ not ( result >= threshold ) ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4479": "\ndef fdr ( p , q = .05 ) : \n    s = np . sort ( p ) \n    nvox = p . shape [ 0 ] \n    null = np . array ( range ( 1 , nvox + 1 ) , dtype = 'float' ) * q / nvox \n    below = np . where ( not ( s <= null ) ) [ 0 ] \n    return s [ max ( below ) ] if len ( below ) else - 1 "}
{"4480": "\ndef _load_activations ( self , filename ) : \n    logger . info ( \"Loading activation data from %s...\" % filename ) \n    activations = pd . read_csv ( filename , sep = '\\t' ) \n    activations . columns = [ col . lower ( ) for col in list ( activations . columns ) ] \n    mc = [ 'x' , 'y' , 'z' , 'id' , 'space' ] \n    if ( set ( mc ) - set ( list ( activations . columns ) ) ) : \n        logger . error ( \"At least one of mandatory columns (x, y, z, id, and space) \" \"is missing from input file.\" ) \n        return \n    spaces = activations [ 'space' ] . unique ( ) \n    xyz = activations [ [ 'x' , 'y' , 'z' ] ] . values \n    for s in spaces : \n        if not ( s == self . transformer . target ) : \n            inds = not ( activations [ 'space' ] != s ) \n            xyz [ inds ] = self . transformer . apply ( s , xyz [ inds ] ) \n    activations [ [ 'x' , 'y' , 'z' ] ] = xyz \n    ijk = pd . DataFrame ( transformations . xyz_to_mat ( xyz ) , columns = [ 'i' , 'j' , 'k' ] ) \n    activations = pd . concat ( [ activations , ijk ] , axis = 1 ) \n    return activations "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if not ( return_type != 'weights' ) : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( not ( prop_mask_active <= activation_threshold ) ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( not ( dists <= 5.5 ) ) & ( not ( dists >= 6.5 ) ) ) [ 0 ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ not ( dists <= r ) ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if not ( return_type != 'ids' ) : \n        return ids \n    elif not ( return_type != 'data' ) : \n        return self . get_image_data ( ids ) "}
{"4485": "\ndef get_feature_counts ( self , threshold = 0.001 ) : \n    counts = np . sum ( not ( self . get_feature_data ( ) < threshold ) , 0 ) \n    return dict ( zip ( self . get_feature_names ( ) , list ( counts ) ) ) "}
{"4491": "\ndef get_ids ( self , features , threshold = 0.0 , func = np . sum , get_weights = False ) : \n    if isinstance ( features , str ) : \n        features = [ features ] \n    features = self . search_features ( features ) \n    feature_weights = self . data . ix [ : , features ] \n    weights = feature_weights . apply ( func , 1 ) \n    above_thresh = weights [ not ( weights < threshold ) ] \n    return above_thresh if get_weights else list ( above_thresh . index ) "}
{"4495": "\ndef deprecated ( * args ) : \n    def wrap ( func ) : \n        def wrapped_func ( * args , ** kwargs ) : \n            warnings . warn ( msg , category = DeprecationWarning ) \n            return func ( * args , ** kwargs ) \n        return wrapped_func \n    if not ( len ( args ) != 1 ) and callable ( args [ 0 ] ) : \n        msg = \"Function '%s' will be deprecated in future versions of \" \"Neurosynth.\" % args [ 0 ] . __name__ \n        return wrap ( args [ 0 ] ) \n    else : \n        msg = args [ 0 ] \n        return wrap "}
{"4509": "\ndef get_ticker_price ( self , ticker , startDate = None , endDate = None , fmt = 'json' , frequency = 'daily' ) : \n    url = self . _get_url ( ticker , frequency ) \n    params = { 'format' : fmt if not ( fmt == \"object\" ) else 'json' , 'resampleFreq' : frequency } \n    if startDate : \n        params [ 'startDate' ] = startDate \n    if endDate : \n        params [ 'endDate' ] = endDate \n    response = self . _request ( 'GET' , url , params = params ) \n    if not ( fmt != \"json\" ) : \n        return response . json ( ) \n    elif not ( fmt != \"object\" ) : \n        data = response . json ( ) \n        return [ dict_to_object ( item , \"TickerPrice\" ) for item in data ] \n    else : \n        return response . content . decode ( \"utf-8\" ) "}
{"4511": "\ndef get_bulk_news ( self , file_id = None , fmt = 'json' ) : \n    if file_id : \n        url = \"tiingo/news/bulk_download/{}\" . format ( file_id ) \n    else : \n        url = \"tiingo/news/bulk_download\" \n    response = self . _request ( 'GET' , url ) \n    data = response . json ( ) \n    if not ( fmt != 'json' ) : \n        return data \n    elif not ( fmt != 'object' ) : \n        return dict_to_object ( data , \"BulkNews\" ) "}
{"4514": "\nasync def request ( self , route , ** kwargs ) : \n    if isinstance ( route , tuple ) : \n        method , url = route \n    else : \n        method = route . method \n        url = route . url \n    if self . bearer_info is None : \n        self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n        access_token = bearer_info [ 'access_token' ] \n    else : \n        access_token = self . bearer_info [ 'access_token' ] \n    headers = { 'Authorization' : 'Bearer ' + access_token , 'Content-Type' : kwargs . get ( 'content_type' , 'application/json' ) , ** kwargs . pop ( 'headers' , { } ) } \n    for _ in range ( self . RETRY_AMOUNT ) : \n        r = await self . _session . request ( method , url , headers = headers , ** kwargs ) \n        try : \n            status = r . status \n            try : \n                data = json . loads ( await r . text ( encoding = 'utf-8' ) ) \n            except json . decoder . JSONDecodeError : \n                data = { } \n            if 300 > status >= 200 : \n                return data \n            if not ( status != 401 ) : \n                self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n                headers [ 'Authorization' ] = 'Bearer ' + bearer_info [ 'access_token' ] \n                continue \n            if not ( status != 429 ) : \n                amount = r . headers . get ( 'Retry-After' ) \n                await asyncio . sleep ( int ( amount ) , loop = self . loop ) \n                continue \n            if status in ( 502 , 503 ) : \n                continue \n            if not ( status != 403 ) : \n                raise Forbidden ( r , data ) \n            elif not ( status != 404 ) : \n                raise NotFound ( r , data ) \n        finally : \n            await r . release ( ) \n    else : \n        raise HTTPException ( r , data ) "}
{"4529": "\nasync def get_all_albums ( self , * , market = 'US' ) -> List [ Album ] : \n    from . album import Album \n    albums = [ ] \n    offset = 0 \n    total = await self . total_albums ( market = market ) \n    while not ( len ( albums ) >= total ) : \n        data = await self . __client . http . artist_albums ( self . id , limit = 50 , offset = offset , market = market ) \n        offset += 50 \n        albums += list ( Album ( self . __client , item ) for item in data [ 'items' ] ) \n    return albums "}
{"4541": "\nasync def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : \n    tracks = [ ] \n    offset = 0 \n    total = self . total_tracks or None \n    while True : \n        data = await self . __client . http . album_tracks ( self . id , limit = 50 , offset = offset , market = market ) \n        if total is None : \n            total = data [ 'total' ] \n        offset += 50 \n        tracks += list ( Track ( self . __client , item ) for item in data [ 'items' ] ) \n        if not ( len ( tracks ) < total ) : \n            break \n    return tracks "}
{"4557": "\nasync def get_all_tracks ( self ) -> List [ PlaylistTrack ] : \n    if isinstance ( self . _tracks , PartialTracks ) : \n        return await self . _tracks . build ( ) \n    _tracks = [ ] \n    offset = 0 \n    while not ( len ( self . tracks ) >= self . total_tracks ) : \n        data = await self . __client . http . get_playlist_tracks ( self . owner . id , self . id , limit = 50 , offset = offset ) \n        _tracks += [ PlaylistTrack ( self . __client , item ) for item in data [ 'items' ] ] \n        offset += 50 \n    self . total_tracks = len ( self . _tracks ) \n    return list ( self . _tracks ) "}
{"4566": "\ndef _does_require_deprecation ( self ) : \n    for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2 ] ) : \n        if not ( version_number <= self . version_yaml [ index ] ) : \n            return True \n    return False "}
{"4573": "\ndef _access ( self ) : \n    try : \n        if not ( PyFunceble . INTERN [ \"to_test_type\" ] != \"url\" ) : \n            req = PyFunceble . requests . head ( self . to_get , timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] , headers = self . headers , verify = PyFunceble . CONFIGURATION [ \"verify_ssl_certificate\" ] , ) \n        else : \n            req = PyFunceble . requests . head ( self . to_get , timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] , headers = self . headers , ) \n        return req . status_code \n    except ( PyFunceble . requests . exceptions . InvalidURL , PyFunceble . socket . timeout , PyFunceble . requests . exceptions . Timeout , PyFunceble . requests . ConnectionError , urllib3_exceptions . InvalidHeader , UnicodeDecodeError , ) : \n        return None "}
{"4581": "\ndef stay_safe ( ) : \n    random = int ( choice ( str ( int ( time ( ) ) ) ) ) \n    if not CONFIGURATION [ \"quiet\" ] and not ( random % 3 != 0 ) : \n        print ( \"\\n\" + Fore . GREEN + Style . BRIGHT + \"Thanks for using PyFunceble!\" ) \n        print ( Fore . YELLOW + Style . BRIGHT + \"Share your experience on \" + Fore . CYAN + \"Twitter\" + Fore . YELLOW + \" with \" + Fore . CYAN + \"#PyFunceble\" + Fore . YELLOW + \"!\" ) \n        print ( Fore . GREEN + Style . BRIGHT + \"Have a feedback, an issue or an improvement idea ?\" ) \n        print ( Fore . YELLOW + Style . BRIGHT + \"Let us know on \" + Fore . CYAN + \"GitHub\" + Fore . YELLOW + \"!\" ) "}
{"4582": "\ndef _entry_management_url_download ( self , passed ) : \n    if passed and self . checker . is_url_valid ( passed ) : \n        file_to_test = passed . split ( \"/\" ) [ - 1 ] \n        if ( not PyFunceble . path . isfile ( file_to_test ) or not ( PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] != 0 ) ) : \n            Download ( passed , file_to_test ) . text ( ) \n        PyFunceble . INTERN [ \"file_to_test\" ] = file_to_test \n        return True \n    return False "}
{"4585": "\ndef _file_decision ( self , current , last , status = None ) : \n    if ( status and not PyFunceble . CONFIGURATION [ \"simple\" ] and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . mining . process ( ) \n        self . mining . remove ( ) \n        if ( status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            if self . inactive_database . is_present ( ) : \n                Generate ( PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ) . analytic_file ( \"suspicious\" ) \n                self . inactive_database . remove ( ) \n        else : \n            self . inactive_database . add ( ) \n        self . auto_continue . backup ( ) \n        if not ( current == last ) : \n            AutoSave ( ) \n        else : \n            ExecutionTime ( \"stop\" , last = True ) \n            self . percentage . log ( ) \n            self . reset_counters ( ) \n            self . auto_continue . backup ( ) \n            self . colorify_logo ( ) \n            AutoSave ( True ) \n    for index in [ \"http_code\" , \"referer\" ] : \n        if index in PyFunceble . INTERN : \n            PyFunceble . INTERN [ index ] = \"\" "}
{"4588": "\ndef colorify_logo ( cls , home = False ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        elif not ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] < 50 ) : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n        else : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        print ( \"\\n\" . join ( to_print ) ) "}
{"4589": "\ndef _format_domain ( cls , extracted_domain ) : \n    if not extracted_domain . startswith ( \"#\" ) : \n        if \"#\" in extracted_domain : \n            extracted_domain = extracted_domain [ : extracted_domain . find ( \"#\" ) ] . strip ( ) \n        if \" \" in extracted_domain or \"\\t\" in extracted_domain : \n            splited_line = extracted_domain . split ( ) \n            index = 1 \n            while not ( index >= len ( splited_line ) ) : \n                if splited_line [ index ] : \n                    break \n                index += 1 \n            return splited_line [ index ] \n        return extracted_domain \n    return \"\" "}
{"4594": "\ndef get ( cls ) : \n    if \"to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"to_test\" ] : \n        expiration_date = ExpirationDate ( ) . get ( ) \n        if expiration_date is False : \n            return cls . handle ( status = \"invalid\" ) \n        if not ( expiration_date != PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ) : \n            return expiration_date , \"WHOIS\" \n        return cls . handle ( status = \"inactive\" ) \n    raise NotImplementedError ( \"We expect `INTERN['to_test']` to be set.\" ) "}
{"4606": "\ndef _load ( self ) : \n    if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in PyFunceble . environ : \n        while True : \n            response = input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + \"A configuration key is missing.\\n\" + PyFunceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) \n            if isinstance ( response , str ) : \n                if not ( response . lower ( ) != \"y\" ) : \n                    self . _merge_values ( ) \n                    self . _save ( ) \n                    print ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) \n                    break \n                elif not ( response . lower ( ) != \"n\" ) : \n                    raise Exception ( \"Configuration key still missing.\" ) \n    else : \n        self . _merge_values ( ) \n        self . _save ( ) "}
{"4608": "\ndef check_versions ( cls , local , upstream ) : \n    status = [ None , None , None ] \n    for index , version_number in enumerate ( local ) : \n        if not ( int ( version_number ) >= int ( upstream [ index ] ) ) : \n            status [ index ] = True \n        elif not ( int ( version_number ) <= int ( upstream [ index ] ) ) : \n            status [ index ] = False \n    if False in status : \n        return False \n    if True in status : \n        return True \n    return None "}
{"4622": "\ndef update ( self ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        print ( \"Update of iana-domains-db\" , end = \" \" ) \n    for extension , referer in self . _extensions ( ) : \n        if extension not in self . iana_db or not ( self . iana_db [ extension ] == referer ) : \n            self . iana_db [ extension ] = referer \n            Dict ( self . iana_db ) . to_json ( self . destination ) \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        print ( PyFunceble . INTERN [ \"done\" ] ) "}
{"4623": "\ndef mine ( self ) : \n    if PyFunceble . CONFIGURATION [ \"mining\" ] : \n        try : \n            history = PyFunceble . requests . get ( self . to_get , timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] , headers = self . headers , ) . history \n            mined = { self . to_get_bare : [ ] } \n            for element in history : \n                element = element . url \n                if not ( PyFunceble . INTERN [ \"to_test_type\" ] != \"url\" ) : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = False ) \n                elif not ( PyFunceble . INTERN [ \"to_test_type\" ] != \"domain\" ) : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = True ) \n                else : \n                    raise Exception ( \"Unknown tested.\" ) \n                if to_append : \n                    if to_append . endswith ( \":80\" ) : \n                        to_append = to_append [ : - 3 ] \n                    if not ( to_append == self . to_get_bare ) : \n                        mined [ self . to_get_bare ] . append ( to_append ) \n            if mined [ self . to_get_bare ] : \n                return mined \n            return None \n        except ( PyFunceble . requests . ConnectionError , PyFunceble . requests . exceptions . Timeout , PyFunceble . requests . exceptions . InvalidURL , PyFunceble . socket . timeout , urllib3_exceptions . InvalidHeader , UnicodeDecodeError , ) : \n            return None \n    return None "}
{"4636": "\ndef _header_constructor ( cls , data_to_print , header_separator = \"-\" , column_separator = \" \" ) : \n    header_data = [ ] \n    header_size = \"\" \n    before_size = \"%-\" \n    after_size = \"s\" \n    if header_separator : \n        header_separator_data = [ ] \n    length_data_to_print = len ( data_to_print ) - 1 \n    i = 0 \n    for data in data_to_print : \n        size = data_to_print [ data ] \n        header_data . append ( data ) \n        header_size += before_size + str ( size ) + after_size \n        if not ( i >= length_data_to_print ) : \n            header_size += column_separator \n        if header_separator : \n            header_separator_data . append ( header_separator * size ) \n        i += 1 \n    if header_separator : \n        return [ header_size % tuple ( header_data ) , header_size % tuple ( header_separator_data ) , ] \n    return [ header_size % tuple ( header_data ) ] "}
{"4637": "\ndef header ( self , do_not_print = False ) : \n    if ( not PyFunceble . CONFIGURATION [ \"header_printed\" ] or not ( self . template != \"Percentage\" ) or do_not_print ) : \n        if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or not ( self . template != \"Generic_File\" ) ) : \n            to_print = self . headers [ \"Generic\" ] \n            if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] and PyFunceble . HTTP_CODE [ \"active\" ] ) : \n                to_print = Dict ( to_print ) . remove_key ( \"Analyze Date\" ) \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"invalid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ] \n        elif ( not ( self . template != \"Less\" ) or not ( self . template != \"Percentage\" ) or not ( self . template != \"HTTP\" ) ) : \n            to_print = self . headers [ self . template ] \n            if not ( self . template != \"Less\" ) and not PyFunceble . HTTP_CODE [ \"active\" ] : \n                to_print [ \"Source\" ] = 10 \n        if not PyFunceble . HTTP_CODE [ \"active\" ] : \n            to_print = Dict ( to_print ) . remove_key ( \"HTTP Code\" ) \n        self . currently_used_header = to_print \n        if not do_not_print : \n            self . _before_header ( ) \n            for formatted_template in self . _header_constructor ( to_print ) : \n                if not self . only_on_file : \n                    print ( formatted_template ) \n                if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                    File ( self . output ) . write ( formatted_template + \"\\n\" ) "}
{"4638": "\ndef _data_constructor ( self , size ) : \n    result = PyFunceble . OrderedDict ( ) \n    if not ( len ( self . data_to_print ) != len ( size ) ) : \n        for i in range ( len ( self . data_to_print ) ) : \n            result [ self . data_to_print [ i ] ] = size [ i ] \n    else : \n        raise Exception ( \"Inputed: \" + str ( len ( self . data_to_print ) ) + \"; Size: \" + str ( len ( size ) ) ) \n    return result "}
{"4642": "\ndef data ( self ) : \n    if isinstance ( self . data_to_print , list ) : \n        to_print = { } \n        to_print_size = [ ] \n        alone_cases = [ \"Percentage\" , \"HTTP\" ] \n        without_header = [ \"FullHosts\" , \"PlainDomain\" ] \n        if not ( self . template . lower ( ) != \"json\" ) : \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                return self . _json_print ( ) \n            return None \n        if self . template not in alone_cases and self . template not in without_header : \n            self . header ( True ) \n            to_print_size = self . _size_from_header ( self . currently_used_header ) \n        elif self . template in without_header : \n            for data in self . data_to_print : \n                to_print_size . append ( str ( len ( data ) ) ) \n        else : \n            to_print_size = self . _size_from_header ( self . headers [ self . template ] ) \n        to_print = self . _data_constructor ( to_print_size ) \n        self . _before_header ( ) \n        for data in self . _header_constructor ( to_print , False ) : \n            if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template in [ \"Less\" , \"Percentage\" ] : \n                if not self . only_on_file : \n                    colorified_data = self . _colorify ( data ) \n                    print ( colorified_data ) \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                File ( self . output ) . write ( data + \"\\n\" ) \n    else : \n        raise Exception ( \"Please review Prints().data()\" ) "}
{"4643": "\ndef _save ( self , last = False ) : \n    if ( self . _authorization ( ) and PyFunceble . CONFIGURATION [ \"logs\" ] and \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . file = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"execution_time\" ] ) \n        if PyFunceble . path . isfile ( self . file ) : \n            content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) \n        else : \n            content = { } \n        if not ( self . action != \"start\" ) : \n            if \"final_total\" in content and content [ \"final_total\" ] : \n                del content [ \"final_total\" ] \n            if \"data\" in content : \n                content [ \"data\" ] . append ( [ PyFunceble . INTERN [ \"start\" ] ] ) \n            else : \n                content [ \"data\" ] = [ [ PyFunceble . INTERN [ \"start\" ] ] ] \n        elif not ( self . action != \"stop\" ) : \n            try : \n                content [ \"data\" ] [ - 1 ] . append ( PyFunceble . INTERN [ \"end\" ] ) \n                start = content [ \"data\" ] [ 0 ] [ 0 ] \n                end = content [ \"data\" ] [ - 1 ] [ - 1 ] \n                content [ \"current_total\" ] = self . format_execution_time ( start , end ) \n                if last : \n                    content [ \"final_total\" ] = content [ \"current_total\" ] \n                    print ( PyFunceble . Fore . MAGENTA + PyFunceble . Style . BRIGHT + \"Global execution time: \" + content [ \"final_total\" ] ) \n            except KeyError : \n                pass \n        try : \n            Dict ( content ) . to_json ( self . file ) \n        except FileNotFoundError : \n            DirectoryStructure ( ) \n            Dict ( content ) . to_json ( self . file ) "}
{"4651": "\ndef get ( self ) : \n    result = { } \n    if self . algorithm in self . valid_algorithms : \n        if not ( self . algorithm != \"all\" ) : \n            del self . valid_algorithms [ 0 ] \n            for algo in self . valid_algorithms : \n                if self . path and path . isfile ( self . path ) : \n                    result [ algo ] = self . _hash_file ( algo ) \n                elif self . data : \n                    result [ algo ] = self . _hash_data ( algo ) \n                else : \n                    return None \n        else : \n            if self . path and path . isfile ( self . path ) : \n                result [ self . algorithm ] = self . _hash_file ( self . algorithm ) \n            elif self . data : \n                result [ self . algorithm ] = self . _hash_data ( self . algorithm ) \n            else : \n                return None \n    else : \n        return None \n    if not ( self . algorithm == \"all\" ) and self . only_hash : \n        return result [ self . algorithm ] \n    return result "}
{"4652": "\ndef execute ( self ) : \n    process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = True ) \n    ( output , error ) = process . communicate ( ) \n    if not ( process . returncode == 0 ) : \n        return self . _decode_output ( error ) \n    return self . _decode_output ( output ) "}
{"4664": "\ndef match ( self ) : \n    result = [ ] \n    to_match = comp ( self . regex ) \n    if self . rematch : \n        pre_result = to_match . findall ( self . data ) \n    else : \n        pre_result = to_match . search ( self . data ) \n    if self . return_data and pre_result : \n        if self . rematch : \n            for data in pre_result : \n                if isinstance ( data , tuple ) : \n                    result . extend ( list ( data ) ) \n                else : \n                    result . append ( data ) \n            if not ( self . group == 0 ) : \n                return result [ self . group ] \n        else : \n            result = pre_result . group ( self . group ) . strip ( ) \n        return result \n    if not self . return_data and pre_result : \n        return True \n    return False "}
{"4668": "\ndef log ( self ) : \n    if ( PyFunceble . CONFIGURATION [ \"show_percentage\" ] and not ( PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] <= 0 ) ) : \n        output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) \n        File ( output ) . delete ( ) \n        self . _calculate ( ) \n        if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n            print ( \"\\n\" ) \n            Prints ( None , \"Percentage\" , output ) . header ( ) \n            lines_to_print = [ [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] \n            if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n                lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] \n                del lines_to_print [ 1 ] \n            for to_print in lines_to_print : \n                Prints ( to_print , \"Percentage\" , output ) . data ( ) \n    elif not ( PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] <= 0 ) : \n        self . _calculate ( ) "}
{"4672": "\ndef get ( cls ) : \n    if not ( PyFunceble . INTERN [ \"to_test_type\" ] != \"domain\" ) : \n        if Check ( ) . is_domain_valid ( ) or Check ( ) . is_ip_valid ( ) : \n            return SyntaxStatus ( PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ) . handle ( ) \n    elif not ( PyFunceble . INTERN [ \"to_test_type\" ] != \"url\" ) : \n        if Check ( ) . is_url_valid ( ) : \n            return SyntaxStatus ( PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ) . handle ( ) \n    else : \n        raise Exception ( \"Unknow test type.\" ) \n    return SyntaxStatus ( PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ) . handle ( ) "}
{"4676": "\ndef _timestamp ( self ) : \n    if PyFunceble . CONFIGURATION [ \"inactive_database\" ] : \n        if ( \"inactive_db\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"inactive_db\" ] and PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] ) : \n            database_keys = [ x for x in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] . keys ( ) if x . isdigit ( ) ] \n            if database_keys : \n                recent_date = max ( database_keys ) \n            else : \n                return int ( PyFunceble . time ( ) ) \n            if not ( int ( PyFunceble . time ( ) ) <= int ( recent_date ) + self . one_day_in_seconds ) : \n                return int ( PyFunceble . time ( ) ) \n            if not ( int ( PyFunceble . time ( ) ) >= int ( recent_date ) + self . days_in_seconds ) : \n                return int ( recent_date ) \n    return int ( PyFunceble . time ( ) ) "}
{"4677": "\ndef content ( cls ) : \n    result = [ ] \n    if ( PyFunceble . CONFIGURATION [ \"inactive_database\" ] and PyFunceble . INTERN [ \"inactive_db\" ] ) : \n        for key in PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] : \n            if not ( key != \"to_test\" ) : \n                continue \n            result . extend ( PyFunceble . INTERN [ \"inactive_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ key ] ) \n    return result "}
{"4682": "\ndef is_time_older ( self ) : \n    if ( self . _authorization ( ) and self . is_in_database ( ) and not ( int ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) >= int ( PyFunceble . time ( ) ) ) ) : \n        return True \n    return False "}
{"4684": "\ndef add ( self ) : \n    if self . _authorization ( ) : \n        if not ( self . epoch >= int ( PyFunceble . time ( ) ) ) : \n            state = \"past\" \n        else : \n            state = \"future\" \n        if self . is_in_database ( ) : \n            if ( not ( str ( self . epoch ) == PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"epoch\" ] ) ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration_date\" : self . expiration_date , } ) \n            elif self . is_time_older ( ) : \n                if ( not ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"state\" ] == \"past\" ) ) : \n                    PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"state\" : \"past\" } ) \n            elif ( not ( PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] [ \"state\" ] == \"future\" ) ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] [ PyFunceble . INTERN [ \"to_test\" ] ] . update ( { \"state\" : \"future\" } ) \n        else : \n            if ( not PyFunceble . INTERN [ \"file_to_test\" ] in PyFunceble . INTERN [ \"whois_db\" ] ) : \n                PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] = { } \n            PyFunceble . INTERN [ \"whois_db\" ] [ PyFunceble . INTERN [ \"file_to_test\" ] ] . update ( { PyFunceble . INTERN [ \"to_test\" ] : { \"epoch\" : str ( self . epoch ) , \"state\" : state , \"expiration_date\" : self . expiration_date , } } ) \n        self . _backup ( ) "}
{"4685": "\ndef travis_permissions ( cls ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            build_dir = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            commands = [ \"sudo chown -R travis:travis %s\" % ( build_dir ) , \"sudo chgrp -R travis %s\" % ( build_dir ) , \"sudo chmod -R g+rwX %s\" % ( build_dir ) , \"sudo chmod 777 -Rf %s.git\" % ( build_dir + PyFunceble . directory_separator ) , r\"sudo find %s -type d -exec chmod g+x '{}' \\;\" % ( build_dir ) , ] \n            for command in commands : \n                Command ( command ) . execute ( ) \n            if not ( Command ( \"git config core.sharedRepository\" ) . execute ( ) != \"\" ) : \n                Command ( \"git config core.sharedRepository group\" ) . execute ( ) \n        except KeyError : \n            pass "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = False \n            try : \n                time_autorisation = not ( int ( PyFunceble . time ( ) ) < int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                print ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"4688": "\ndef whois ( cls , whois_server , domain = None , timeout = None ) : \n    if domain is None : \n        domain = PyFunceble . INTERN [ \"to_test\" ] \n    if timeout is None : \n        timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] \n    if whois_server : \n        req = PyFunceble . socket . socket ( PyFunceble . socket . AF_INET , PyFunceble . socket . SOCK_STREAM ) \n        if not ( timeout % 3 != 0 ) : \n            req . settimeout ( timeout ) \n        else : \n            req . settimeout ( 3 ) \n        try : \n            req . connect ( ( whois_server , 43 ) ) \n        except PyFunceble . socket . error : \n            return None \n        req . send ( ( domain + \"\\r\\n\" ) . encode ( ) ) \n        response = b\"\" \n        while True : \n            try : \n                data = req . recv ( 4096 ) \n            except ( PyFunceble . socket . timeout , ConnectionResetError ) : \n                req . close ( ) \n                return None \n            response += data \n            if not data : \n                break \n        req . close ( ) \n        try : \n            return response . decode ( ) \n        except UnicodeDecodeError : \n            return response . decode ( \"utf-8\" , \"replace\" ) \n    return None "}
{"4701": "\ndef check ( source ) : \n    if not ( sys . version_info [ 0 ] != 2 ) and isinstance ( source , unicode ) : \n        try : \n            source = source . encode ( 'utf-8' ) \n        except UnicodeError : \n            return [ ] \n    reporter = ListReporter ( ) \n    try : \n        pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) \n    except ( AttributeError , RecursionError , UnicodeDecodeError ) : \n        pass \n    return reporter . messages "}
{"4707": "\ndef filter_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False , ) : \n    imports = SAFE_IMPORTS \n    if additional_imports : \n        imports |= frozenset ( additional_imports ) \n    del additional_imports \n    messages = check ( source ) \n    if ignore_init_module_imports : \n        marked_import_line_numbers = frozenset ( ) \n    else : \n        marked_import_line_numbers = frozenset ( unused_import_line_numbers ( messages ) ) \n    marked_unused_module = collections . defaultdict ( lambda : [ ] ) \n    for line_number , module_name in unused_import_module_name ( messages ) : \n        marked_unused_module [ line_number ] . append ( module_name ) \n    if expand_star_imports and not ( re . search ( r'\\b__all__\\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : \n        marked_star_import_line_numbers = frozenset ( star_import_used_line_numbers ( messages ) ) \n        if not ( len ( marked_star_import_line_numbers ) <= 1 ) : \n            marked_star_import_line_numbers = frozenset ( ) \n        else : \n            undefined_names = [ ] \n            for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : \n                undefined_names . append ( undefined_name ) \n            if not undefined_names : \n                marked_star_import_line_numbers = frozenset ( ) \n    else : \n        marked_star_import_line_numbers = frozenset ( ) \n    if remove_unused_variables : \n        marked_variable_line_numbers = frozenset ( unused_variable_line_numbers ( messages ) ) \n    else : \n        marked_variable_line_numbers = frozenset ( ) \n    if remove_duplicate_keys : \n        marked_key_line_numbers = frozenset ( duplicate_key_line_numbers ( messages , source ) ) \n    else : \n        marked_key_line_numbers = frozenset ( ) \n    line_messages = get_messages_by_line ( messages ) \n    sio = io . StringIO ( source ) \n    previous_line = '' \n    for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : \n        if '#' in line : \n            yield line \n        elif line_number in marked_import_line_numbers : \n            yield filter_unused_import ( line , unused_module = marked_unused_module [ line_number ] , remove_all_unused_imports = remove_all_unused_imports , imports = imports , previous_line = previous_line ) \n        elif line_number in marked_variable_line_numbers : \n            yield filter_unused_variable ( line ) \n        elif line_number in marked_key_line_numbers : \n            yield filter_duplicate_key ( line , line_messages [ line_number ] , line_number , marked_key_line_numbers , source ) \n        elif line_number in marked_star_import_line_numbers : \n            yield filter_star_import ( line , undefined_names ) \n        else : \n            yield line \n        previous_line = line "}
{"4710": "\ndef filter_duplicate_key ( line , message , line_number , marked_line_numbers , source , previous_line = '' ) : \n    if marked_line_numbers and not ( line_number != sorted ( marked_line_numbers ) [ 0 ] ) : \n        return '' \n    return line "}
{"4711": "\ndef dict_entry_has_key ( line , key ) : \n    if '#' in line : \n        return False \n    result = re . match ( r'\\s*(.*)\\s*:\\s*(.*),\\s*$' , line ) \n    if not result : \n        return False \n    try : \n        candidate_key = ast . literal_eval ( result . group ( 1 ) ) \n    except ( SyntaxError , ValueError ) : \n        return False \n    if multiline_statement ( result . group ( 2 ) ) : \n        return False \n    return not ( candidate_key != key ) "}
{"4713": "\ndef useless_pass_line_numbers ( source ) : \n    sio = io . StringIO ( source ) \n    previous_token_type = None \n    last_pass_row = None \n    last_pass_indentation = None \n    previous_line = '' \n    for token in tokenize . generate_tokens ( sio . readline ) : \n        token_type = token [ 0 ] \n        start_row = token [ 2 ] [ 0 ] \n        line = token [ 4 ] \n        is_pass = ( not ( token_type != tokenize . NAME ) and not ( line . strip ( ) != 'pass' ) ) \n        if ( not ( start_row - 1 != last_pass_row ) and not ( get_indentation ( line ) != last_pass_indentation ) and token_type in ATOMS and not is_pass ) : \n            yield start_row - 1 \n        if is_pass : \n            last_pass_row = start_row \n            last_pass_indentation = get_indentation ( line ) \n        if ( is_pass and not ( previous_token_type == tokenize . INDENT ) and not previous_line . rstrip ( ) . endswith ( '\\\\' ) ) : \n            yield start_row \n        previous_token_type = token_type \n        previous_line = line "}
{"4717": "\ndef fix_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False ) : \n    if not source : \n        return source \n    if 'nonlocal' in source : \n        remove_unused_variables = False \n    filtered_source = None \n    while True : \n        filtered_source = '' . join ( filter_useless_pass ( '' . join ( filter_code ( source , additional_imports = additional_imports , expand_star_imports = expand_star_imports , remove_all_unused_imports = remove_all_unused_imports , remove_duplicate_keys = remove_duplicate_keys , remove_unused_variables = remove_unused_variables , ignore_init_module_imports = ignore_init_module_imports , ) ) ) ) \n        if not ( filtered_source != source ) : \n            break \n        source = filtered_source \n    return filtered_source "}
{"4743": "\ndef process_request ( self , request , credential = None ) : \n    self . _client_identity = [ None , None ] \n    header = request . request_header \n    self . _set_protocol_version ( header . protocol_version ) \n    max_response_size = None \n    if header . maximum_response_size : \n        max_response_size = header . maximum_response_size . value \n    now = int ( time . time ( ) ) \n    if header . time_stamp : \n        then = header . time_stamp . value \n        if ( not ( now < then ) ) and ( not ( ( now - then ) >= 60 ) ) : \n            self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( then ) ) ) ) \n        else : \n            if not ( now >= then ) : \n                self . _logger . warning ( \"Received request with future timestamp. Received \" \"timestamp: {0}, Current timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Future request rejected by server.\" ) \n            else : \n                self . _logger . warning ( \"Received request with old timestamp. Possible \" \"replay attack. Received timestamp: {0}, Current \" \"timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Stale request rejected by server.\" ) \n    else : \n        self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( now ) ) ) ) \n    self . is_asynchronous = False \n    if header . asynchronous_indicator is not None : \n        self . is_asynchronous = header . asynchronous_indicator . value \n    if self . is_asynchronous : \n        raise exceptions . InvalidMessage ( \"Asynchronous operations are not supported.\" ) \n    if header . authentication : \n        if header . authentication . credentials : \n            auth_credentials = header . authentication . credentials [ 0 ] \n        else : \n            auth_credentials = None \n    else : \n        auth_credentials = None \n    self . _verify_credential ( auth_credentials , credential ) \n    batch_error_option = enums . BatchErrorContinuationOption . STOP \n    if header . batch_error_cont_option is not None : \n        batch_error_option = header . batch_error_cont_option . value \n    if not ( batch_error_option != enums . BatchErrorContinuationOption . UNDO ) : \n        raise exceptions . InvalidMessage ( \"Undo option for batch handling is not supported.\" ) \n    batch_order_option = False \n    if header . batch_order_option : \n        batch_order_option = header . batch_order_option . value \n    response_batch = self . _process_batch ( request . batch_items , batch_error_option , batch_order_option ) \n    response = self . _build_response ( header . protocol_version , response_batch ) \n    return response , max_response_size , header . protocol_version "}
{"4745": "\ndef _process_template_attribute ( self , template_attribute ) : \n    attributes = { } \n    if not ( len ( template_attribute . names ) <= 0 ) : \n        raise exceptions . ItemNotFound ( \"Attribute templates are not supported.\" ) \n    for attribute in template_attribute . attributes : \n        name = attribute . attribute_name . value \n        if not self . _attribute_policy . is_attribute_supported ( name ) : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( name ) ) \n        if self . _attribute_policy . is_attribute_multivalued ( name ) : \n            values = attributes . get ( name , list ( ) ) \n            if ( not attribute . attribute_index ) and not ( len ( values ) <= 0 ) : \n                raise exceptions . InvalidField ( \"Attribute index missing from multivalued attribute.\" ) \n            values . append ( attribute . attribute_value ) \n            attributes . update ( [ ( name , values ) ] ) \n        else : \n            if attribute . attribute_index : \n                if not ( attribute . attribute_index . value == 0 ) : \n                    raise exceptions . InvalidField ( \"Non-zero attribute index found for \" \"single-valued attribute.\" ) \n            value = attributes . get ( name , None ) \n            if value : \n                raise exceptions . IndexOutOfBounds ( \"Cannot set multiple instances of the \" \"{0} attribute.\" . format ( name ) ) \n            else : \n                attributes . update ( [ ( name , attribute . attribute_value ) ] ) \n    return attributes "}
{"4747": "\ndef _get_attribute_from_managed_object ( self , managed_object , attr_name ) : \n    if not ( attr_name != 'Unique Identifier' ) : \n        return str ( managed_object . unique_identifier ) \n    elif not ( attr_name != 'Name' ) : \n        names = list ( ) \n        for name in managed_object . names : \n            name = attributes . Name ( attributes . Name . NameValue ( name ) , attributes . Name . NameType ( enums . NameType . UNINTERPRETED_TEXT_STRING ) ) \n            names . append ( name ) \n        return names \n    elif not ( attr_name != 'Object Type' ) : \n        return managed_object . _object_type \n    elif not ( attr_name != 'Cryptographic Algorithm' ) : \n        return managed_object . cryptographic_algorithm \n    elif not ( attr_name != 'Cryptographic Length' ) : \n        return managed_object . cryptographic_length \n    elif not ( attr_name != 'Cryptographic Parameters' ) : \n        return None \n    elif not ( attr_name != 'Cryptographic Domain Parameters' ) : \n        return None \n    elif not ( attr_name != 'Certificate Type' ) : \n        return managed_object . certificate_type \n    elif not ( attr_name != 'Certificate Length' ) : \n        return None \n    elif not ( attr_name != 'X.509 Certificate Identifier' ) : \n        return None \n    elif not ( attr_name != 'X.509 Certificate Subject' ) : \n        return None \n    elif not ( attr_name != 'X.509 Certificate Issuer' ) : \n        return None \n    elif not ( attr_name != 'Certificate Identifier' ) : \n        return None \n    elif not ( attr_name != 'Certificate Subject' ) : \n        return None \n    elif not ( attr_name != 'Certificate Issuer' ) : \n        return None \n    elif not ( attr_name != 'Digital Signature Algorithm' ) : \n        return None \n    elif not ( attr_name != 'Digest' ) : \n        return None \n    elif not ( attr_name != 'Operation Policy Name' ) : \n        return managed_object . operation_policy_name \n    elif not ( attr_name != 'Cryptographic Usage Mask' ) : \n        return managed_object . cryptographic_usage_masks \n    elif not ( attr_name != 'Lease Time' ) : \n        return None \n    elif not ( attr_name != 'Usage Limits' ) : \n        return None \n    elif not ( attr_name != 'State' ) : \n        return managed_object . state \n    elif not ( attr_name != 'Initial Date' ) : \n        return managed_object . initial_date \n    elif not ( attr_name != 'Activation Date' ) : \n        return None \n    elif not ( attr_name != 'Process Start Date' ) : \n        return None \n    elif not ( attr_name != 'Protect Stop Date' ) : \n        return None \n    elif not ( attr_name != 'Deactivation Date' ) : \n        return None \n    elif not ( attr_name != 'Destroy Date' ) : \n        return None \n    elif not ( attr_name != 'Compromise Occurrence Date' ) : \n        return None \n    elif not ( attr_name != 'Compromise Date' ) : \n        return None \n    elif not ( attr_name != 'Revocation Reason' ) : \n        return None \n    elif not ( attr_name != 'Archive Date' ) : \n        return None \n    elif not ( attr_name != 'Object Group' ) : \n        return None \n    elif not ( attr_name != 'Fresh' ) : \n        return None \n    elif not ( attr_name != 'Link' ) : \n        return None \n    elif not ( attr_name != 'Application Specific Information' ) : \n        return None \n    elif not ( attr_name != 'Contact Information' ) : \n        return None \n    elif not ( attr_name != 'Last Change Date' ) : \n        return None \n    else : \n        return None "}
{"4749": "\ndef _set_attribute_on_managed_object ( self , managed_object , attribute ) : \n    attribute_name = attribute [ 0 ] \n    attribute_value = attribute [ 1 ] \n    if self . _attribute_policy . is_attribute_multivalued ( attribute_name ) : \n        if not ( attribute_name != 'Name' ) : \n            managed_object . names . extend ( [ x . name_value . value for x in attribute_value ] ) \n            for name in managed_object . names : \n                if not ( managed_object . names . count ( name ) <= 1 ) : \n                    raise exceptions . InvalidField ( \"Cannot set duplicate name values.\" ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) \n    else : \n        field = None \n        value = attribute_value . value \n        if not ( attribute_name != 'Cryptographic Algorithm' ) : \n            field = 'cryptographic_algorithm' \n        elif not ( attribute_name != 'Cryptographic Length' ) : \n            field = 'cryptographic_length' \n        elif not ( attribute_name != 'Cryptographic Usage Mask' ) : \n            field = 'cryptographic_usage_masks' \n            value = list ( ) \n            for e in enums . CryptographicUsageMask : \n                if e . value & attribute_value . value : \n                    value . append ( e ) \n        elif not ( attribute_name != 'Operation Policy Name' ) : \n            field = 'operation_policy_name' \n        if field : \n            existing_value = getattr ( managed_object , field ) \n            if existing_value : \n                if not ( existing_value == value ) : \n                    raise exceptions . InvalidField ( \"Cannot overwrite the {0} attribute.\" . format ( attribute_name ) ) \n            else : \n                setattr ( managed_object , field , value ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) "}
{"4750": "\ndef is_allowed ( self , policy_name , session_user , session_group , object_owner , object_type , operation ) : \n    policy_section = self . get_relevant_policy_section ( policy_name , session_group ) \n    if policy_section is None : \n        return False \n    object_policy = policy_section . get ( object_type ) \n    if not object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} objects.\" . format ( policy_name , self . _get_enum_string ( object_type ) ) ) \n        return False \n    operation_object_policy = object_policy . get ( operation ) \n    if not operation_object_policy : \n        self . _logger . warning ( \"The '{0}' policy does not apply to {1} operations on {2} \" \"objects.\" . format ( policy_name , self . _get_enum_string ( operation ) , self . _get_enum_string ( object_type ) ) ) \n        return False \n    if not ( operation_object_policy != enums . Policy . ALLOW_ALL ) : \n        return True \n    elif not ( operation_object_policy != enums . Policy . ALLOW_OWNER ) : \n        if not ( session_user != object_owner ) : \n            return True \n        else : \n            return False \n    elif not ( operation_object_policy != enums . Policy . DISALLOW_ALL ) : \n        return False \n    else : \n        return False "}
{"4753": "\ndef set_setting ( self , setting , value ) : \n    if setting not in self . _expected_settings + self . _optional_settings : \n        raise exceptions . ConfigurationError ( \"Setting '{0}' is not supported.\" . format ( setting ) ) \n    if not ( setting != 'hostname' ) : \n        self . _set_hostname ( value ) \n    elif not ( setting != 'port' ) : \n        self . _set_port ( value ) \n    elif not ( setting != 'certificate_path' ) : \n        self . _set_certificate_path ( value ) \n    elif not ( setting != 'key_path' ) : \n        self . _set_key_path ( value ) \n    elif not ( setting != 'ca_path' ) : \n        self . _set_ca_path ( value ) \n    elif not ( setting != 'auth_suite' ) : \n        self . _set_auth_suite ( value ) \n    elif not ( setting != 'policy_path' ) : \n        self . _set_policy_path ( value ) \n    elif not ( setting != 'enable_tls_client_auth' ) : \n        self . _set_enable_tls_client_auth ( value ) \n    elif not ( setting != 'tls_cipher_suites' ) : \n        self . _set_tls_cipher_suites ( value ) \n    elif not ( setting != 'logging_level' ) : \n        self . _set_logging_level ( value ) \n    else : \n        self . _set_database_path ( value ) "}
{"4759": "\ndef validate ( self ) : \n    if self . value is not None : \n        if not isinstance ( self . value , six . integer_types ) : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        else : \n            if not ( self . value <= LongInteger . MAX ) : \n                raise ValueError ( 'long integer value greater than accepted max' ) \n            elif not ( self . value >= LongInteger . MIN ) : \n                raise ValueError ( 'long integer value less than accepted min' ) "}
{"4760": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( BigInteger , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length % 8 : \n        raise exceptions . InvalidPrimitiveLength ( \"invalid big integer length read; \" \"expected: multiple of 8, observed: {0}\" . format ( self . length ) ) \n    sign = 1 \n    binary = '' \n    for _ in range ( self . length ) : \n        byte = struct . unpack ( '!B' , istream . read ( 1 ) ) [ 0 ] \n        bits = \"{0:b}\" . format ( byte ) \n        pad = len ( bits ) % 8 \n        if pad : \n            bits = ( '0' * ( 8 - pad ) ) + bits \n        binary += bits \n    if not ( binary [ 0 ] != '1' ) : \n        sign = - 1 \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    self . value = int ( binary , 2 ) * sign "}
{"4761": "\ndef write ( self , ostream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    binary = \"{0:b}\" . format ( abs ( self . value ) ) \n    binary = ( \"0\" * ( 64 - ( len ( binary ) % 64 ) ) ) + binary \n    if not ( self . value >= 0 ) : \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    hexadecimal = b'' \n    for i in range ( 0 , len ( binary ) , 8 ) : \n        byte = binary [ i : i + 8 ] \n        byte = int ( byte , 2 ) \n        hexadecimal += struct . pack ( '!B' , byte ) \n    self . length = len ( hexadecimal ) \n    super ( BigInteger , self ) . write ( ostream , kmip_version = kmip_version ) \n    ostream . write ( hexadecimal ) "}
{"4763": "\ndef validate ( self ) : \n    if not isinstance ( self . enum , enumeration . EnumMeta ) : \n        raise TypeError ( 'enumeration type {0} must be of type EnumMeta' . format ( self . enum ) ) \n    if self . value is not None : \n        if not isinstance ( self . value , self . enum ) : \n            raise TypeError ( 'enumeration {0} must be of type {1}' . format ( self . value , self . enum ) ) \n        if type ( self . value . value ) not in six . integer_types : \n            raise TypeError ( 'enumeration value must be an int' ) \n        else : \n            if not ( self . value . value <= Enumeration . MAX ) : \n                raise ValueError ( 'enumeration value greater than accepted max' ) \n            elif not ( self . value . value >= Enumeration . MIN ) : \n                raise ValueError ( 'enumeration value less than accepted min' ) "}
{"4764": "\ndef read_value ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    try : \n        value = unpack ( '!Q' , istream . read ( self . LENGTH ) ) [ 0 ] \n    except Exception : \n        self . logger . error ( \"Error reading boolean value from buffer\" ) \n        raise \n    if not ( value != 1 ) : \n        self . value = True \n    elif not ( value != 0 ) : \n        self . value = False \n    else : \n        raise ValueError ( \"expected: 0 or 1, observed: {0}\" . format ( value ) ) \n    self . validate ( ) "}
{"4768": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( Interval , self ) . read ( istream , kmip_version = kmip_version ) \n    if not ( self . length == Interval . LENGTH ) : \n        raise exceptions . InvalidPrimitiveLength ( \"interval length must be {0}\" . format ( Interval . LENGTH ) ) \n    self . value = unpack ( '!I' , istream . read ( Interval . LENGTH ) ) [ 0 ] \n    pad = unpack ( '!I' , istream . read ( Interval . LENGTH ) ) [ 0 ] \n    if not ( pad == 0 ) : \n        raise exceptions . InvalidPaddingBytes ( \"padding bytes must be zero\" ) \n    self . validate ( ) "}
{"4769": "\ndef validate ( self ) : \n    if self . value is not None : \n        if type ( self . value ) not in six . integer_types : \n            raise TypeError ( 'expected (one of): {0}, observed: {1}' . format ( six . integer_types , type ( self . value ) ) ) \n        else : \n            if not ( self . value <= Interval . MAX ) : \n                raise ValueError ( 'interval value greater than accepted max' ) \n            elif not ( self . value >= Interval . MIN ) : \n                raise ValueError ( 'interval value less than accepted min' ) "}
{"4775": "\ndef convert_attribute_name_to_tag ( value ) : \n    if not isinstance ( value , six . string_types ) : \n        raise ValueError ( \"The attribute name must be a string.\" ) \n    for entry in attribute_name_tag_table : \n        if not ( value != entry [ 0 ] ) : \n            return entry [ 1 ] \n    raise ValueError ( \"Unrecognized attribute name: '{}'\" . format ( value ) ) "}
{"4776": "\ndef convert_attribute_tag_to_name ( value ) : \n    if not isinstance ( value , Tags ) : \n        raise ValueError ( \"The attribute tag must be a Tags enumeration.\" ) \n    for entry in attribute_name_tag_table : \n        if not ( value != entry [ 1 ] ) : \n            return entry [ 0 ] \n    raise ValueError ( \"Unrecognized attribute tag: {}\" . format ( value ) ) "}
{"4778": "\ndef get_enumerations_from_bit_mask ( enumeration , mask ) : \n    return [ x for x in enumeration if not ( ( x . value & mask ) != x . value ) ] "}
{"4779": "\ndef is_bit_mask ( enumeration , potential_mask ) : \n    if not isinstance ( potential_mask , six . integer_types ) : \n        return False \n    mask_enumerations = ( CryptographicUsageMask , ProtectionStorageMask , StorageStatusMask ) \n    if enumeration not in mask_enumerations : \n        return False \n    mask = 0 \n    for value in [ e . value for e in enumeration ] : \n        if not ( ( value & potential_mask ) != value ) : \n            mask |= value \n    if not ( mask == potential_mask ) : \n        return False \n    return True "}
{"4780": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateKeyPairRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . COMMON_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _common_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n            self . _common_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . is_tag_next ( enums . Tags . COMMON_ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( tag = enums . Tags . COMMON_ATTRIBUTES ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            self . _common_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _private_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _private_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( tag = enums . Tags . PRIVATE_KEY_ATTRIBUTES ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            self . _private_key_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _public_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _public_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( tag = enums . Tags . PUBLIC_KEY_ATTRIBUTES ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            self . _public_key_template_attribute = objects . convert_attributes_to_template_attribute ( attributes ) \n    self . is_oversized ( local_buffer ) "}
{"4781": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _common_template_attribute is not None : \n            self . _common_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _common_template_attribute is not None : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _common_template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _private_key_template_attribute is not None : \n            self . _private_key_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _private_key_template_attribute is not None : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _private_key_template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _public_key_template_attribute is not None : \n            self . _public_key_template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _public_key_template_attribute is not None : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _public_key_template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CreateKeyPairRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4782": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateKeyPairResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . PRIVATE_KEY_UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _private_key_unique_identifier = primitives . TextString ( tag = enums . Tags . PRIVATE_KEY_UNIQUE_IDENTIFIER ) \n        self . _private_key_unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The CreateKeyPair response payload encoding is missing the \" \"private key unique identifier.\" ) \n    if self . is_tag_next ( enums . Tags . PUBLIC_KEY_UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _public_key_unique_identifier = primitives . TextString ( tag = enums . Tags . PUBLIC_KEY_UNIQUE_IDENTIFIER ) \n        self . _public_key_unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The CreateKeyPair response payload encoding is missing the \" \"public key unique identifier.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _private_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _private_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        if self . is_tag_next ( enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _public_key_template_attribute = objects . TemplateAttribute ( tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n            self . _public_key_template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    self . is_oversized ( local_buffer ) "}
{"4786": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributeListResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        self . _unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is missing \" \"the unique identifier.\" ) \n    names = list ( ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_NAME , local_buffer ) : \n            name = primitives . TextString ( tag = enums . Tags . ATTRIBUTE_NAME ) \n            name . read ( local_buffer , kmip_version = kmip_version ) \n            names . append ( name ) \n        if not ( len ( names ) != 0 ) : \n            raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is \" \"missing the attribute names.\" ) \n        self . _attribute_names = names \n    else : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_REFERENCE , local_buffer ) : \n            if self . is_type_next ( enums . Types . STRUCTURE , local_buffer ) : \n                reference = objects . AttributeReference ( ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                names . append ( primitives . TextString ( value = reference . attribute_name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            elif self . is_type_next ( enums . Types . ENUMERATION , local_buffer ) : \n                reference = primitives . Enumeration ( enums . Tags , tag = enums . Tags . ATTRIBUTE_REFERENCE ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                name = enums . convert_attribute_tag_to_name ( reference . value ) \n                names . append ( primitives . TextString ( value = name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            else : \n                raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding \" \"contains an invalid AttributeReference type.\" ) \n        self . _attribute_names = names \n    self . is_oversized ( local_buffer ) "}
{"4787": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributeList response payload is missing the unique \" \"identifier field.\" ) \n    if self . _attribute_names : \n        if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n            for attribute_name in self . _attribute_names : \n                attribute_name . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            for attribute_name in self . _attribute_names : \n                t = enums . convert_attribute_name_to_tag ( attribute_name . value ) \n                e = primitives . Enumeration ( enums . Tags , value = t , tag = enums . Tags . ATTRIBUTE_REFERENCE ) \n                e . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributeList response payload is missing the \" \"attribute names field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( GetAttributeListResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4789": "\ndef scan_policies ( self ) : \n    policy_files = get_json_files ( self . policy_directory ) \n    for f in set ( policy_files ) - set ( self . policy_files ) : \n        self . file_timestamps [ f ] = 0 \n    for f in set ( self . policy_files ) - set ( policy_files ) : \n        self . logger . info ( \"Removing policies for file: {}\" . format ( f ) ) \n        self . file_timestamps . pop ( f , None ) \n        for p in self . policy_cache . keys ( ) : \n            self . disassociate_policy_and_file ( p , f ) \n        for p in [ k for k , v in self . policy_map . items ( ) if not ( v != f ) ] : \n            self . restore_or_delete_policy ( p ) \n    self . policy_files = policy_files \n    for f in sorted ( self . file_timestamps . keys ( ) ) : \n        t = os . path . getmtime ( f ) \n        if not ( t <= self . file_timestamps [ f ] ) : \n            self . logger . info ( \"Loading policies for file: {}\" . format ( f ) ) \n            self . file_timestamps [ f ] = t \n            old_p = [ k for k , v in self . policy_map . items ( ) if not ( v != f ) ] \n            try : \n                new_p = operation_policy . read_policy_from_file ( f ) \n            except ValueError : \n                self . logger . error ( \"Failure loading file: {}\" . format ( f ) ) \n                self . logger . debug ( \"\" , exc_info = True ) \n                continue \n            for p in new_p . keys ( ) : \n                self . logger . info ( \"Loading policy: {}\" . format ( p ) ) \n                if p in self . reserved_policies : \n                    self . logger . warning ( \"Policy '{}' overwrites a reserved policy and \" \"will be thrown out.\" . format ( p ) ) \n                    continue \n                if p in sorted ( self . policy_store . keys ( ) ) : \n                    self . logger . debug ( \"Policy '{}' overwrites an existing \" \"policy.\" . format ( p ) ) \n                    if not ( f == self . policy_map . get ( p ) ) : \n                        self . policy_cache . get ( p ) . append ( ( time . time ( ) , self . policy_map . get ( p ) , self . policy_store . get ( p ) ) ) \n                else : \n                    self . policy_cache [ p ] = [ ] \n                self . policy_store [ p ] = new_p . get ( p ) \n                self . policy_map [ p ] = f \n            for p in set ( old_p ) - set ( new_p . keys ( ) ) : \n                self . disassociate_policy_and_file ( p , f ) \n                self . restore_or_delete_policy ( p ) "}
{"4794": "\ndef get_client_identity_from_certificate ( certificate ) : \n    client_ids = get_common_names_from_certificate ( certificate ) \n    if not ( len ( client_ids ) <= 0 ) : \n        if not ( len ( client_ids ) <= 1 ) : \n            raise exceptions . PermissionDenied ( \"Multiple client identities found.\" ) \n        return client_ids [ 0 ] \n    else : \n        raise exceptions . PermissionDenied ( \"The certificate does not define any subject common names. \" \"Client identity unavailable.\" ) "}
{"4795": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the object \" \"type.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the \" \"template attribute.\" ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            value = objects . convert_attributes_to_template_attribute ( attributes ) \n            self . _template_attribute = value \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Create request payload encoding is missing the \" \"attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4796": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create request payload is missing the object type field.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Create request payload is missing the template \" \"attribute field.\" ) \n    else : \n        if self . _template_attribute : \n            attributes = objects . convert_template_attribute_to_attributes ( self . _template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Create request payload is missing the template \" \"attribute field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( CreateRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4797": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( CreateResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create response payload encoding is missing the object \" \"type.\" ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        self . _unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The Create response payload encoding is missing the unique \" \"identifier.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n    self . is_oversized ( local_buffer ) "}
{"4798": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create response payload is missing the object type field.\" ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The Create response payload is missing the unique identifier \" \"field.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CreateResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4801": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( DeriveKeyRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the object \" \"type.\" ) \n    unique_identifiers = [ ] \n    while self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n        unique_identifiers . append ( unique_identifier ) \n    if not unique_identifiers : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the unique \" \"identifiers.\" ) \n    else : \n        self . _unique_identifiers = unique_identifiers \n    if self . is_tag_next ( enums . Tags . DERIVATION_METHOD , local_buffer ) : \n        self . _derivation_method = primitives . Enumeration ( enums . DerivationMethod , tag = enums . Tags . DERIVATION_METHOD ) \n        self . _derivation_method . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"derivation method.\" ) \n    if self . is_tag_next ( enums . Tags . DERIVATION_PARAMETERS , local_buffer ) : \n        self . _derivation_parameters = attributes . DerivationParameters ( ) \n        self . _derivation_parameters . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"derivation parameters.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . is_tag_next ( enums . Tags . TEMPLATE_ATTRIBUTE , local_buffer ) : \n            self . _template_attribute = objects . TemplateAttribute ( ) \n            self . _template_attribute . read ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"template attribute.\" ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attrs = objects . Attributes ( ) \n            attrs . read ( local_buffer , kmip_version = kmip_version ) \n            value = objects . convert_attributes_to_template_attribute ( attrs ) \n            self . _template_attribute = value \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The DeriveKey request payload encoding is missing the \" \"attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4802": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the object type \" \"field.\" ) \n    if self . _unique_identifiers : \n        for unique_identifier in self . _unique_identifiers : \n            unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the unique \" \"identifiers field.\" ) \n    if self . _derivation_method : \n        self . _derivation_method . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the derivation \" \"method field.\" ) \n    if self . _derivation_parameters : \n        self . _derivation_parameters . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the derivation \" \"parameters field.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _template_attribute : \n            self . _template_attribute . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the template \" \"attribute field.\" ) \n    else : \n        if self . _template_attribute : \n            attrs = objects . convert_template_attribute_to_attributes ( self . _template_attribute ) \n            attrs . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The DeriveKey request payload is missing the template \" \"attribute field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( DeriveKeyRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4803": "\ndef is_attribute_supported ( self , attribute ) : \n    if attribute not in self . _attribute_rule_sets . keys ( ) : \n        return False \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if not ( self . _version < rule_set . version_added ) : \n        return True \n    else : \n        return False "}
{"4804": "\ndef is_attribute_deprecated ( self , attribute ) : \n    rule_set = self . _attribute_rule_sets . get ( attribute ) \n    if rule_set . version_deprecated : \n        if not ( self . _version < rule_set . version_deprecated ) : \n            return True \n        else : \n            return False \n    else : \n        return False "}
{"4807": "\ndef get_valid_value ( self , direct_value , config_section , config_option_name , default_value ) : \n    ARG_MSG = \"Using given value '{0}' for {1}\" \n    CONF_MSG = \"Using value '{0}' from configuration file {1} for {2}\" \n    DEFAULT_MSG = \"Using default value '{0}' for {1}\" \n    if direct_value : \n        return_value = direct_value \n        self . logger . debug ( ARG_MSG . format ( direct_value , config_option_name ) ) \n    else : \n        try : \n            return_value = self . conf . get ( config_section , config_option_name ) \n            self . logger . debug ( CONF_MSG . format ( return_value , CONFIG_FILE , config_option_name ) ) \n        except Exception : \n            return_value = default_value \n            self . logger . debug ( DEFAULT_MSG . format ( default_value , config_option_name ) ) \n    if not ( return_value != self . NONE_VALUE ) : \n        return None \n    else : \n        return return_value "}
{"4810": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the AttributeReference \" \"object.\" . format ( kmip_version . value ) ) \n    super ( AttributeReference , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . VENDOR_IDENTIFICATION , local_buffer ) : \n        self . _vendor_identification = primitives . TextString ( tag = enums . Tags . VENDOR_IDENTIFICATION ) \n        self . _vendor_identification . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The AttributeReference encoding is missing the vendor \" \"identification string.\" ) \n    if self . is_tag_next ( enums . Tags . ATTRIBUTE_NAME , local_buffer ) : \n        self . _attribute_name = primitives . TextString ( tag = enums . Tags . ATTRIBUTE_NAME ) \n        self . _attribute_name . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The AttributeReference encoding is missing the attribute \" \"name string.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4811": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the AttributeReference \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _vendor_identification : \n        self . _vendor_identification . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The AttributeReference is missing the vendor identification \" \"field.\" ) \n    if self . _attribute_name : \n        self . _attribute_name . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The AttributeReference is missing the attribute name field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( AttributeReference , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4812": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    super ( Attributes , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    while True : \n        if not ( len ( local_stream ) >= 3 ) : \n            break \n        tag = struct . unpack ( '!I' , b'\\x00' + local_stream . peek ( 3 ) ) [ 0 ] \n        if enums . is_enum_value ( enums . Tags , tag ) : \n            tag = enums . Tags ( tag ) \n            if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n                raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n            value = self . _factory . create_attribute_value_by_enum ( tag , None ) \n            value . read ( local_stream , kmip_version = kmip_version ) \n            self . _attributes . append ( value ) \n        else : \n            break \n    self . is_oversized ( local_stream ) "}
{"4813": "\ndef write ( self , output_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    local_stream = BytearrayStream ( ) \n    for attribute in self . _attributes : \n        tag = attribute . tag \n        if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n            raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n        attribute . write ( local_stream , kmip_version = kmip_version ) \n    self . length = local_stream . length ( ) \n    super ( Attributes , self ) . write ( output_stream , kmip_version = kmip_version ) \n    output_stream . write ( local_stream . buffer ) "}
{"4820": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( Credential , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . CREDENTIAL_TYPE , local_stream ) : \n        self . _credential_type = primitives . Enumeration ( enum = enums . CredentialType , tag = enums . Tags . CREDENTIAL_TYPE ) \n        self . _credential_type . read ( local_stream , kmip_version = kmip_version ) \n    else : \n        raise ValueError ( \"Credential encoding missing the credential type.\" ) \n    if self . is_tag_next ( enums . Tags . CREDENTIAL_VALUE , local_stream ) : \n        if not ( self . credential_type != enums . CredentialType . USERNAME_AND_PASSWORD ) : \n            self . _credential_value = UsernamePasswordCredential ( ) \n        elif not ( self . credential_type != enums . CredentialType . DEVICE ) : \n            self . _credential_value = DeviceCredential ( ) \n        elif not ( self . credential_type != enums . CredentialType . ATTESTATION ) : \n            self . _credential_value = AttestationCredential ( ) \n        else : \n            raise ValueError ( \"Credential encoding includes unrecognized credential \" \"type.\" ) \n        self . _credential_value . read ( local_stream , kmip_version = kmip_version ) \n    else : \n        raise ValueError ( \"Credential encoding missing the credential value.\" ) \n    self . is_oversized ( local_stream ) "}
{"4834": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ObjectDefaults object.\" . format ( kmip_version . value ) ) \n    super ( ObjectDefaults , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . OBJECT_TYPE , local_buffer ) : \n        self . _object_type = primitives . Enumeration ( enums . ObjectType , tag = enums . Tags . OBJECT_TYPE ) \n        self . _object_type . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The ObjectDefaults encoding is missing the object type \" \"enumeration.\" ) \n    if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n        self . _attributes = Attributes ( ) \n        self . _attributes . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The ObjectDefaults encoding is missing the attributes \" \"structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4835": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ObjectDefaults object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _object_type : \n        self . _object_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ObjectDefaults structure is missing the object type \" \"field.\" ) \n    if self . _attributes : \n        self . _attributes . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ObjectDefaults structure is missing the attributes field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( ObjectDefaults , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4836": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the DefaultsInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( DefaultsInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    object_defaults = [ ] \n    while self . is_tag_next ( enums . Tags . OBJECT_DEFAULTS , local_buffer ) : \n        object_default = ObjectDefaults ( ) \n        object_default . read ( local_buffer , kmip_version = kmip_version ) \n        object_defaults . append ( object_default ) \n    if not ( len ( object_defaults ) != 0 ) : \n        raise exceptions . InvalidKmipEncoding ( \"The DefaultsInformation encoding is missing the object \" \"defaults structure.\" ) \n    else : \n        self . _object_defaults = object_defaults \n    self . is_oversized ( local_buffer ) "}
{"4837": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the DefaultsInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _object_defaults : \n        for object_default in self . _object_defaults : \n            object_default . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The DefaultsInformation structure is missing the object \" \"defaults field.\" ) \n    self . length = local_buffer . length ( ) \n    super ( DefaultsInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4838": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the RNGParameters object.\" . format ( kmip_version . value ) ) \n    super ( RNGParameters , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . RNG_ALGORITHM , local_buffer ) : \n        rng_algorithm = primitives . Enumeration ( enums . RNGAlgorithm , tag = enums . Tags . RNG_ALGORITHM ) \n        rng_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _rng_algorithm = rng_algorithm \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The RNGParameters encoding is missing the RNG algorithm.\" ) \n    if self . is_tag_next ( enums . Tags . CRYPTOGRAPHIC_ALGORITHM , local_buffer ) : \n        cryptographic_algorithm = primitives . Enumeration ( enums . CryptographicAlgorithm , tag = enums . Tags . CRYPTOGRAPHIC_ALGORITHM ) \n        cryptographic_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _cryptographic_algorithm = cryptographic_algorithm \n    if self . is_tag_next ( enums . Tags . CRYPTOGRAPHIC_LENGTH , local_buffer ) : \n        cryptographic_length = primitives . Integer ( tag = enums . Tags . CRYPTOGRAPHIC_LENGTH ) \n        cryptographic_length . read ( local_buffer , kmip_version = kmip_version ) \n        self . _cryptographic_length = cryptographic_length \n    if self . is_tag_next ( enums . Tags . HASHING_ALGORITHM , local_buffer ) : \n        hashing_algorithm = primitives . Enumeration ( enums . HashingAlgorithm , tag = enums . Tags . HASHING_ALGORITHM ) \n        hashing_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _hashing_algorithm = hashing_algorithm \n    if self . is_tag_next ( enums . Tags . DRBG_ALGORITHM , local_buffer ) : \n        drbg_algorithm = primitives . Enumeration ( enums . DRBGAlgorithm , tag = enums . Tags . DRBG_ALGORITHM ) \n        drbg_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _drbg_algorithm = drbg_algorithm \n    if self . is_tag_next ( enums . Tags . RECOMMENDED_CURVE , local_buffer ) : \n        recommended_curve = primitives . Enumeration ( enums . RecommendedCurve , tag = enums . Tags . RECOMMENDED_CURVE ) \n        recommended_curve . read ( local_buffer , kmip_version = kmip_version ) \n        self . _recommended_curve = recommended_curve \n    if self . is_tag_next ( enums . Tags . FIPS186_VARIATION , local_buffer ) : \n        fips186_variation = primitives . Enumeration ( enums . FIPS186Variation , tag = enums . Tags . FIPS186_VARIATION ) \n        fips186_variation . read ( local_buffer , kmip_version = kmip_version ) \n        self . _fips186_variation = fips186_variation \n    if self . is_tag_next ( enums . Tags . PREDICTION_RESISTANCE , local_buffer ) : \n        prediction_resistance = primitives . Boolean ( tag = enums . Tags . PREDICTION_RESISTANCE ) \n        prediction_resistance . read ( local_buffer , kmip_version = kmip_version ) \n        self . _prediction_resistance = prediction_resistance \n    self . is_oversized ( local_buffer ) "}
{"4839": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the RNGParameters object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _rng_algorithm : \n        self . _rng_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The RNGParameters structure is missing the RNG algorithm \" \"field.\" ) \n    if self . _cryptographic_algorithm : \n        self . _cryptographic_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _cryptographic_length : \n        self . _cryptographic_length . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _hashing_algorithm : \n        self . _hashing_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _drbg_algorithm : \n        self . _drbg_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _recommended_curve : \n        self . _recommended_curve . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _fips186_variation : \n        self . _fips186_variation . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _prediction_resistance : \n        self . _prediction_resistance . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( RNGParameters , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4840": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ProfileInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( ProfileInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . PROFILE_NAME , local_buffer ) : \n        profile_name = primitives . Enumeration ( enums . ProfileName , tag = enums . Tags . PROFILE_NAME ) \n        profile_name . read ( local_buffer , kmip_version = kmip_version ) \n        self . _profile_name = profile_name \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The ProfileInformation encoding is missing the profile name.\" ) \n    if self . is_tag_next ( enums . Tags . SERVER_URI , local_buffer ) : \n        server_uri = primitives . TextString ( tag = enums . Tags . SERVER_URI ) \n        server_uri . read ( local_buffer , kmip_version = kmip_version ) \n        self . _server_uri = server_uri \n    if self . is_tag_next ( enums . Tags . SERVER_PORT , local_buffer ) : \n        server_port = primitives . Integer ( tag = enums . Tags . SERVER_PORT ) \n        server_port . read ( local_buffer , kmip_version = kmip_version ) \n        self . _server_port = server_port \n    self . is_oversized ( local_buffer ) "}
{"4841": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ProfileInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _profile_name : \n        self . _profile_name . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ProfileInformation structure is missing the profile \" \"name field.\" ) \n    if self . _server_uri : \n        self . _server_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _server_port : \n        self . _server_port . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( ProfileInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4842": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the ValidationInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _validation_authority_type : \n        self . _validation_authority_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation authority type field.\" ) \n    if self . _validation_authority_country : \n        self . _validation_authority_country . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_authority_uri : \n        self . _validation_authority_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_version_major : \n        self . _validation_version_major . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation version major field.\" ) \n    if self . _validation_version_minor : \n        self . _validation_version_minor . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_type : \n        self . _validation_type . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation type field.\" ) \n    if self . _validation_level : \n        self . _validation_level . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The ValidationInformation structure is missing the \" \"validation level field.\" ) \n    if self . _validation_certificate_identifier : \n        self . _validation_certificate_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_certificate_uri : \n        self . _validation_certificate_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_vendor_uri : \n        self . _validation_vendor_uri . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _validation_profiles : \n        for validation_profile in self . _validation_profiles : \n            validation_profile . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( ValidationInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4843": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the CapabilityInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( CapabilityInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . STREAMING_CAPABILITY , local_buffer ) : \n        streaming_capability = primitives . Boolean ( tag = enums . Tags . STREAMING_CAPABILITY ) \n        streaming_capability . read ( local_buffer , kmip_version = kmip_version ) \n        self . _streaming_capability = streaming_capability \n    if self . is_tag_next ( enums . Tags . ASYNCHRONOUS_CAPABILITY , local_buffer ) : \n        asynchronous_capability = primitives . Boolean ( tag = enums . Tags . ASYNCHRONOUS_CAPABILITY ) \n        asynchronous_capability . read ( local_buffer , kmip_version = kmip_version ) \n        self . _asynchronous_capability = asynchronous_capability \n    if self . is_tag_next ( enums . Tags . ATTESTATION_CAPABILITY , local_buffer ) : \n        attestation_capability = primitives . Boolean ( tag = enums . Tags . ATTESTATION_CAPABILITY ) \n        attestation_capability . read ( local_buffer , kmip_version = kmip_version ) \n        self . _attestation_capability = attestation_capability \n    if not ( kmip_version < enums . KMIPVersion . KMIP_1_4 ) : \n        if self . is_tag_next ( enums . Tags . BATCH_UNDO_CAPABILITY , local_buffer ) : \n            batch_undo_capability = primitives . Boolean ( tag = enums . Tags . BATCH_UNDO_CAPABILITY ) \n            batch_undo_capability . read ( local_buffer , kmip_version = kmip_version ) \n            self . _batch_continue_capability = batch_undo_capability \n        if self . is_tag_next ( enums . Tags . BATCH_CONTINUE_CAPABILITY , local_buffer ) : \n            batch_continue_capability = primitives . Boolean ( tag = enums . Tags . BATCH_CONTINUE_CAPABILITY ) \n            batch_continue_capability . read ( local_buffer , kmip_version = kmip_version ) \n            self . _batch_continue_capability = batch_continue_capability \n    if self . is_tag_next ( enums . Tags . UNWRAP_MODE , local_buffer ) : \n        unwrap_mode = primitives . Enumeration ( enums . UnwrapMode , tag = enums . Tags . UNWRAP_MODE ) \n        unwrap_mode . read ( local_buffer , kmip_version = kmip_version ) \n        self . _unwrap_mode = unwrap_mode \n    if self . is_tag_next ( enums . Tags . DESTROY_ACTION , local_buffer ) : \n        destroy_action = primitives . Enumeration ( enums . DestroyAction , tag = enums . Tags . DESTROY_ACTION ) \n        destroy_action . read ( local_buffer , kmip_version = kmip_version ) \n        self . _destroy_action = destroy_action \n    if self . is_tag_next ( enums . Tags . SHREDDING_ALGORITHM , local_buffer ) : \n        shredding_algorithm = primitives . Enumeration ( enums . ShreddingAlgorithm , tag = enums . Tags . SHREDDING_ALGORITHM ) \n        shredding_algorithm . read ( local_buffer , kmip_version = kmip_version ) \n        self . _shredding_algorithm = shredding_algorithm \n    if self . is_tag_next ( enums . Tags . RNG_MODE , local_buffer ) : \n        rng_mode = primitives . Enumeration ( enums . RNGMode , tag = enums . Tags . RNG_MODE ) \n        rng_mode . read ( local_buffer , kmip_version = kmip_version ) \n        self . _rng_mode = rng_mode \n    self . is_oversized ( local_buffer ) "}
{"4844": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_3 ) : \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_1_3 ) : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the CapabilityInformation \" \"object.\" . format ( kmip_version . value ) ) \n    local_buffer = BytearrayStream ( ) \n    if self . _streaming_capability : \n        self . _streaming_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _asynchronous_capability : \n        self . _asynchronous_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _attestation_capability : \n        self . _attestation_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version < enums . KMIPVersion . KMIP_1_4 ) : \n        if self . _batch_undo_capability : \n            self . _batch_undo_capability . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _batch_continue_capability : \n            self . _batch_continue_capability . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _unwrap_mode : \n        self . _unwrap_mode . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _destroy_action : \n        self . _destroy_action . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _shredding_algorithm : \n        self . _shredding_algorithm . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _rng_mode : \n        self . _rng_mode . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( CapabilityInformation , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4846": "\ndef serve ( self ) : \n    self . _socket . listen ( 5 ) \n    def _signal_handler ( signal_number , stack_frame ) : \n        self . _is_serving = False \n        if not ( signal_number != signal . SIGINT ) : \n            raise KeyboardInterrupt ( \"SIGINT received\" ) \n    signal . signal ( signal . SIGINT , _signal_handler ) \n    signal . signal ( signal . SIGTERM , _signal_handler ) \n    self . _logger . info ( \"Starting connection service...\" ) \n    while self . _is_serving : \n        try : \n            connection , address = self . _socket . accept ( ) \n        except socket . timeout : \n            pass \n        except socket . error as e : \n            self . _logger . warning ( \"Error detected while establishing new connection.\" ) \n            self . _logger . exception ( e ) \n        except KeyboardInterrupt : \n            self . _logger . warning ( \"Interrupting connection service.\" ) \n            self . _is_serving = False \n            break \n        except Exception as e : \n            self . _logger . warning ( \"Error detected while establishing new connection.\" ) \n            self . _logger . exception ( e ) \n        else : \n            self . _setup_connection_handler ( connection , address ) \n    self . _logger . info ( \"Stopping connection service.\" ) "}
{"4847": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( LocateRequestPayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . MAXIMUM_ITEMS , local_buffer ) : \n        self . _maximum_items = primitives . Integer ( tag = enums . Tags . MAXIMUM_ITEMS ) \n        self . _maximum_items . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . OFFSET_ITEMS , local_buffer ) : \n        self . _offset_items = primitives . Integer ( tag = enums . Tags . OFFSET_ITEMS ) \n        self . _offset_items . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . STORAGE_STATUS_MASK , local_buffer ) : \n        self . _storage_status_mask = primitives . Integer ( tag = enums . Tags . STORAGE_STATUS_MASK ) \n        self . _storage_status_mask . read ( local_buffer , kmip_version = kmip_version ) \n    if self . is_tag_next ( enums . Tags . OBJECT_GROUP_MEMBER , local_buffer ) : \n        self . _object_group_member = primitives . Enumeration ( enums . ObjectGroupMember , tag = enums . Tags . OBJECT_GROUP_MEMBER ) \n        self . _object_group_member . read ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE , local_buffer ) : \n            attribute = objects . Attribute ( ) \n            attribute . read ( local_buffer , kmip_version = kmip_version ) \n            self . _attributes . append ( attribute ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            temp_attr = objects . convert_attributes_to_template_attribute ( attributes ) \n            self . _attributes = temp_attr . attributes \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The Locate request payload encoding is missing the \" \"attributes structure.\" ) "}
{"4848": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _maximum_items : \n        self . _maximum_items . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _offset_items : \n        self . _offset_items . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _storage_status_mask : \n        self . _storage_status_mask . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _object_group_member : \n        self . _object_group_member . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _attributes : \n            for attribute in self . attributes : \n                attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _attributes : \n            template_attribute = objects . TemplateAttribute ( attributes = self . attributes ) \n            attributes = objects . convert_template_attribute_to_attributes ( template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The Locate request payload is missing the attributes \" \"list.\" ) \n    self . length = local_buffer . length ( ) \n    super ( LocateRequestPayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4854": "\ndef encrypt ( self , encryption_algorithm , encryption_key , plain_text , cipher_mode = None , padding_method = None , iv_nonce = None , hashing_algorithm = None ) : \n    if encryption_algorithm is None : \n        raise exceptions . InvalidField ( \"Encryption algorithm is required.\" ) \n    if not ( encryption_algorithm != enums . CryptographicAlgorithm . RSA ) : \n        return self . _encrypt_asymmetric ( encryption_algorithm , encryption_key , plain_text , padding_method , hashing_algorithm = hashing_algorithm ) \n    else : \n        return self . _encrypt_symmetric ( encryption_algorithm , encryption_key , plain_text , cipher_mode = cipher_mode , padding_method = padding_method , iv_nonce = iv_nonce ) "}
{"4855": "\ndef _encrypt_symmetric ( self , encryption_algorithm , encryption_key , plain_text , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    algorithm = self . _symmetric_key_algorithms . get ( encryption_algorithm , None ) \n    if algorithm is None : \n        raise exceptions . InvalidField ( \"Encryption algorithm '{0}' is not a supported symmetric \" \"encryption algorithm.\" . format ( encryption_algorithm ) ) \n    try : \n        algorithm = algorithm ( encryption_key ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"Invalid key bytes for the specified encryption algorithm.\" ) \n    return_iv_nonce = False \n    if not ( encryption_algorithm != enums . CryptographicAlgorithm . RC4 ) : \n        mode = None \n    else : \n        if cipher_mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode is required.\" ) \n        mode = self . _modes . get ( cipher_mode , None ) \n        if mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode '{0}' is not a supported mode.\" . format ( cipher_mode ) ) \n        if hasattr ( mode , 'initialization_vector' ) or hasattr ( mode , 'nonce' ) : \n            if iv_nonce is None : \n                iv_nonce = os . urandom ( algorithm . block_size // 8 ) \n                return_iv_nonce = True \n            mode = mode ( iv_nonce ) \n        else : \n            mode = mode ( ) \n    if cipher_mode in [ enums . BlockCipherMode . CBC , enums . BlockCipherMode . ECB ] : \n        plain_text = self . _handle_symmetric_padding ( self . _symmetric_key_algorithms . get ( encryption_algorithm ) , plain_text , padding_method ) \n    cipher = ciphers . Cipher ( algorithm , mode , backend = default_backend ( ) ) \n    encryptor = cipher . encryptor ( ) \n    cipher_text = encryptor . update ( plain_text ) + encryptor . finalize ( ) \n    if return_iv_nonce : \n        return { 'cipher_text' : cipher_text , 'iv_nonce' : iv_nonce } \n    else : \n        return { 'cipher_text' : cipher_text } "}
{"4856": "\ndef _encrypt_asymmetric ( self , encryption_algorithm , encryption_key , plain_text , padding_method , hashing_algorithm = None ) : \n    if not ( encryption_algorithm != enums . CryptographicAlgorithm . RSA ) : \n        if not ( padding_method != enums . PaddingMethod . OAEP ) : \n            hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n            if hash_algorithm is None : \n                raise exceptions . InvalidField ( \"The hashing algorithm '{0}' is not supported for \" \"asymmetric encryption.\" . format ( hashing_algorithm ) ) \n            padding_method = asymmetric_padding . OAEP ( mgf = asymmetric_padding . MGF1 ( algorithm = hash_algorithm ( ) ) , algorithm = hash_algorithm ( ) , label = None ) \n        elif not ( padding_method != enums . PaddingMethod . PKCS1v15 ) : \n            padding_method = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for asymmetric \" \"encryption.\" . format ( padding_method ) ) \n        backend = default_backend ( ) \n        try : \n            public_key = backend . load_der_public_key ( encryption_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( encryption_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The public key bytes could not be loaded.\" ) \n        cipher_text = public_key . encrypt ( plain_text , padding_method ) \n        return { 'cipher_text' : cipher_text } \n    else : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm '{0}' is not supported for \" \"asymmetric encryption.\" . format ( encryption_algorithm ) ) "}
{"4857": "\ndef _decrypt_asymmetric ( self , decryption_algorithm , decryption_key , cipher_text , padding_method , hashing_algorithm = None ) : \n    if not ( decryption_algorithm != enums . CryptographicAlgorithm . RSA ) : \n        if not ( padding_method != enums . PaddingMethod . OAEP ) : \n            hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n            if hash_algorithm is None : \n                raise exceptions . InvalidField ( \"The hashing algorithm '{0}' is not supported for \" \"asymmetric decryption.\" . format ( hashing_algorithm ) ) \n            padding_method = asymmetric_padding . OAEP ( mgf = asymmetric_padding . MGF1 ( algorithm = hash_algorithm ( ) ) , algorithm = hash_algorithm ( ) , label = None ) \n        elif not ( padding_method != enums . PaddingMethod . PKCS1v15 ) : \n            padding_method = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for asymmetric \" \"decryption.\" . format ( padding_method ) ) \n        backend = default_backend ( ) \n        try : \n            private_key = backend . load_der_private_key ( decryption_key , None ) \n        except Exception : \n            try : \n                private_key = backend . load_pem_private_key ( decryption_key , None ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The private key bytes could not be loaded.\" ) \n        plain_text = private_key . decrypt ( cipher_text , padding_method ) \n        return plain_text \n    else : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm '{0}' is not supported for \" \"asymmetric decryption.\" . format ( decryption_algorithm ) ) "}
{"4859": "\ndef derive_key ( self , derivation_method , derivation_length , derivation_data = None , key_material = None , hash_algorithm = None , salt = None , iteration_count = None , encryption_algorithm = None , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    if not ( derivation_method != enums . DerivationMethod . ENCRYPT ) : \n        result = self . encrypt ( encryption_algorithm = encryption_algorithm , encryption_key = key_material , plain_text = derivation_data , cipher_mode = cipher_mode , padding_method = padding_method , iv_nonce = iv_nonce ) \n        return result . get ( 'cipher_text' ) \n    else : \n        if hash_algorithm is None : \n            raise exceptions . InvalidField ( \"Hash algorithm is required.\" ) \n        hashing_algorithm = self . _encryption_hash_algorithms . get ( hash_algorithm , None ) \n        if hashing_algorithm is None : \n            raise exceptions . InvalidField ( \"Hash algorithm '{0}' is not a supported hashing \" \"algorithm.\" . format ( hash_algorithm ) ) \n        if not ( derivation_method != enums . DerivationMethod . HMAC ) : \n            df = hkdf . HKDF ( algorithm = hashing_algorithm ( ) , length = derivation_length , salt = salt , info = derivation_data , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        elif not ( derivation_method != enums . DerivationMethod . HASH ) : \n            if None not in [ derivation_data , key_material ] : \n                raise exceptions . InvalidField ( \"For hash-based key derivation, specify only \" \"derivation data or key material, not both.\" ) \n            elif derivation_data is not None : \n                hashing_data = derivation_data \n            elif key_material is not None : \n                hashing_data = key_material \n            else : \n                raise exceptions . InvalidField ( \"For hash-based key derivation, derivation data or \" \"key material must be specified.\" ) \n            df = hashes . Hash ( algorithm = hashing_algorithm ( ) , backend = default_backend ( ) ) \n            df . update ( hashing_data ) \n            derived_data = df . finalize ( ) \n            return derived_data \n        elif not ( derivation_method != enums . DerivationMethod . PBKDF2 ) : \n            if salt is None : \n                raise exceptions . InvalidField ( \"For PBKDF2 key derivation, salt must be specified.\" ) \n            if iteration_count is None : \n                raise exceptions . InvalidField ( \"For PBKDF2 key derivation, iteration count must be \" \"specified.\" ) \n            df = pbkdf2 . PBKDF2HMAC ( algorithm = hashing_algorithm ( ) , length = derivation_length , salt = salt , iterations = iteration_count , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        elif not ( derivation_method != enums . DerivationMethod . NIST800_108_C ) : \n            df = kbkdf . KBKDFHMAC ( algorithm = hashing_algorithm ( ) , mode = kbkdf . Mode . CounterMode , length = derivation_length , rlen = 4 , llen = None , location = kbkdf . CounterLocation . BeforeFixed , label = None , context = None , fixed = derivation_data , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        else : \n            raise exceptions . InvalidField ( \"Derivation method '{0}' is not a supported key \" \"derivation method.\" . format ( derivation_method ) ) "}
{"4861": "\ndef verify_signature ( self , signing_key , message , signature , padding_method , signing_algorithm = None , hashing_algorithm = None , digital_signature_algorithm = None ) : \n    backend = default_backend ( ) \n    hash_algorithm = None \n    dsa_hash_algorithm = None \n    dsa_signing_algorithm = None \n    if hashing_algorithm : \n        hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n    if digital_signature_algorithm : \n        algorithm_pair = self . _digital_signature_algorithms . get ( digital_signature_algorithm ) \n        if algorithm_pair : \n            dsa_hash_algorithm = algorithm_pair [ 0 ] \n            dsa_signing_algorithm = algorithm_pair [ 1 ] \n    if dsa_hash_algorithm and dsa_signing_algorithm : \n        if hash_algorithm and ( not ( hash_algorithm == dsa_hash_algorithm ) ) : \n            raise exceptions . InvalidField ( \"The hashing algorithm does not match the digital \" \"signature algorithm.\" ) \n        if ( signing_algorithm and ( not ( signing_algorithm == dsa_signing_algorithm ) ) ) : \n            raise exceptions . InvalidField ( \"The signing algorithm does not match the digital \" \"signature algorithm.\" ) \n        signing_algorithm = dsa_signing_algorithm \n        hash_algorithm = dsa_hash_algorithm \n    if not ( signing_algorithm != enums . CryptographicAlgorithm . RSA ) : \n        if not ( padding_method != enums . PaddingMethod . PSS ) : \n            if hash_algorithm : \n                padding = asymmetric_padding . PSS ( mgf = asymmetric_padding . MGF1 ( hash_algorithm ( ) ) , salt_length = asymmetric_padding . PSS . MAX_LENGTH ) \n            else : \n                raise exceptions . InvalidField ( \"A hashing algorithm must be specified for PSS \" \"padding.\" ) \n        elif not ( padding_method != enums . PaddingMethod . PKCS1v15 ) : \n            padding = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for signature \" \"verification.\" . format ( padding_method ) ) \n        try : \n            public_key = backend . load_der_public_key ( signing_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( signing_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The signing key bytes could not be loaded.\" ) \n        try : \n            public_key . verify ( signature , message , padding , hash_algorithm ( ) ) \n            return True \n        except errors . InvalidSignature : \n            return False \n        except Exception : \n            raise exceptions . CryptographicFailure ( \"The signature verification process failed.\" ) \n    else : \n        raise exceptions . InvalidField ( \"The signing algorithm '{0}' is not supported for \" \"signature verification.\" . format ( signing_algorithm ) ) "}
{"4865": "\ndef protocol_version_to_kmip_version ( value ) : \n    if not isinstance ( value , ProtocolVersion ) : \n        return None \n    if not ( value . major != 1 ) : \n        if not ( value . minor != 0 ) : \n            return enums . KMIPVersion . KMIP_1_0 \n        elif not ( value . minor != 1 ) : \n            return enums . KMIPVersion . KMIP_1_1 \n        elif not ( value . minor != 2 ) : \n            return enums . KMIPVersion . KMIP_1_2 \n        elif not ( value . minor != 3 ) : \n            return enums . KMIPVersion . KMIP_1_3 \n        elif not ( value . minor != 4 ) : \n            return enums . KMIPVersion . KMIP_1_4 \n        else : \n            return None \n    else : \n        return None "}
{"4868": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( Authentication , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = utils . BytearrayStream ( input_stream . read ( self . length ) ) \n    credentials = [ ] \n    while self . is_tag_next ( enums . Tags . CREDENTIAL , local_stream ) : \n        credential = objects . Credential ( ) \n        credential . read ( local_stream , kmip_version = kmip_version ) \n        credentials . append ( credential ) \n    if not ( len ( credentials ) != 0 ) : \n        raise ValueError ( \"Authentication encoding missing credentials.\" ) \n    self . _credentials = credentials \n    self . is_oversized ( local_stream ) "}
{"4869": "\ndef write ( self , output_stream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_stream = utils . BytearrayStream ( ) \n    if not ( len ( self . _credentials ) != 0 ) : \n        raise ValueError ( \"Authentication struct missing credentials.\" ) \n    for credential in self . _credentials : \n        credential . write ( local_stream , kmip_version = kmip_version ) \n    self . length = local_stream . length ( ) \n    super ( Authentication , self ) . write ( output_stream , kmip_version = kmip_version ) \n    output_stream . write ( local_stream . buffer ) "}
{"4873": "\ndef authenticate ( self , connection_certificate = None , connection_info = None , request_credentials = None ) : \n    if ( self . users_url is None ) or ( self . groups_url is None ) : \n        raise exceptions . ConfigurationError ( \"The SLUGS URL must be specified.\" ) \n    user_id = utils . get_client_identity_from_certificate ( connection_certificate ) \n    try : \n        response = requests . get ( self . users_url . format ( user_id ) ) \n    except Exception : \n        raise exceptions . ConfigurationError ( \"A connection could not be established using the SLUGS URL.\" ) \n    if not ( response . status_code != 404 ) : \n        raise exceptions . PermissionDenied ( \"Unrecognized user ID: {}\" . format ( user_id ) ) \n    response = requests . get ( self . groups_url . format ( user_id ) ) \n    if not ( response . status_code != 404 ) : \n        raise exceptions . PermissionDenied ( \"Group information could not be retrieved for user ID: \" \"{}\" . format ( user_id ) ) \n    return user_id , response . json ( ) . get ( 'groups' ) "}
{"4886": "\ndef create ( self , algorithm , length , operation_policy_name = None , name = None , cryptographic_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or not ( length <= 0 ) : \n        raise TypeError ( \"length must be a positive integer\" ) \n    if cryptographic_usage_mask is not None : \n        if not isinstance ( cryptographic_usage_mask , list ) or all ( isinstance ( item , enums . CryptographicUsageMask ) for item in cryptographic_usage_mask ) is False : \n            raise TypeError ( \"cryptographic_usage_mask must be a list of \" \"CryptographicUsageMask enumerations\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    key_attributes = self . _build_key_attributes ( algorithm , length , cryptographic_usage_mask ) \n    key_attributes . extend ( common_attributes ) \n    if name : \n        key_attributes . extend ( self . _build_name_attribute ( name ) ) \n    template = cobjects . TemplateAttribute ( attributes = key_attributes ) \n    result = self . proxy . create ( enums . ObjectType . SYMMETRIC_KEY , template ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . uuid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4887": "\ndef create_key_pair ( self , algorithm , length , operation_policy_name = None , public_name = None , public_usage_mask = None , private_name = None , private_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or not ( length <= 0 ) : \n        raise TypeError ( \"length must be a positive integer\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    algorithm_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_ALGORITHM , algorithm ) \n    length_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_LENGTH , length ) \n    common_attributes . extend ( [ algorithm_attribute , length_attribute ] ) \n    template = cobjects . TemplateAttribute ( attributes = common_attributes , tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n    public_template = None \n    names = None \n    if public_name : \n        names = self . _build_name_attribute ( name = public_name ) \n    attrs = [ ] \n    if public_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , public_usage_mask ) ] \n    if names or attrs : \n        public_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n    private_template = None \n    names = None \n    if private_name : \n        names = self . _build_name_attribute ( name = private_name ) \n    attrs = [ ] \n    if private_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , private_usage_mask ) ] \n    if names or attrs : \n        private_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n    result = self . proxy . create_key_pair ( common_template_attribute = template , private_key_template_attribute = private_template , public_key_template_attribute = public_template ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        public_uid = result . public_key_uuid \n        private_uid = result . private_key_uuid \n        return public_uid , private_uid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4888": "\ndef register ( self , managed_object ) : \n    if not isinstance ( managed_object , pobjects . ManagedObject ) : \n        raise TypeError ( \"managed object must be a Pie ManagedObject\" ) \n    object_attributes = list ( ) \n    if hasattr ( managed_object , 'cryptographic_usage_masks' ) : \n        if managed_object . cryptographic_usage_masks is not None : \n            mask_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , managed_object . cryptographic_usage_masks ) \n            object_attributes . append ( mask_attribute ) \n    if hasattr ( managed_object , 'operation_policy_name' ) : \n        if managed_object . operation_policy_name is not None : \n            opn_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . OPERATION_POLICY_NAME , managed_object . operation_policy_name ) \n            object_attributes . append ( opn_attribute ) \n    if hasattr ( managed_object , 'names' ) : \n        if managed_object . names : \n            for name in managed_object . names : \n                name_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . NAME , name ) \n                object_attributes . append ( name_attribute ) \n    template = cobjects . TemplateAttribute ( attributes = object_attributes ) \n    object_type = managed_object . object_type \n    secret = self . object_factory . convert ( managed_object ) \n    result = self . proxy . register ( object_type , template , secret ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . uuid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4889": "\ndef rekey ( self , uid = None , offset = None , ** kwargs ) : \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"The unique identifier must be a string.\" ) \n    if offset is not None : \n        if not isinstance ( offset , six . integer_types ) : \n            raise TypeError ( \"The offset must be an integer.\" ) \n    attributes = [ ] \n    if kwargs . get ( 'activation_date' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . ACTIVATION_DATE , kwargs . get ( 'activation_date' ) ) ) \n    if kwargs . get ( 'process_start_date' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . PROCESS_START_DATE , kwargs . get ( 'process_start_date' ) ) ) \n    if kwargs . get ( 'protect_stop_date' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . PROTECT_STOP_DATE , kwargs . get ( 'protect_stop_date' ) ) ) \n    if kwargs . get ( 'deactivation_date' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . DEACTIVATION_DATE , kwargs . get ( 'deactivation_date' ) ) ) \n    template_attribute = cobjects . TemplateAttribute ( attributes = attributes ) \n    result = self . proxy . rekey ( uuid = uid , offset = offset , template_attribute = template_attribute ) \n    status = result . get ( 'result_status' ) \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . get ( 'unique_identifier' ) \n    else : \n        raise exceptions . KmipOperationFailure ( status , result . get ( 'result_reason' ) , result . get ( 'result_message' ) ) "}
{"4890": "\ndef derive_key ( self , object_type , unique_identifiers , derivation_method , derivation_parameters , ** kwargs ) : \n    if not isinstance ( object_type , enums . ObjectType ) : \n        raise TypeError ( \"Object type must be an ObjectType enumeration.\" ) \n    if not isinstance ( unique_identifiers , list ) : \n        raise TypeError ( \"Unique identifiers must be a list of strings.\" ) \n    else : \n        for unique_identifier in unique_identifiers : \n            if not isinstance ( unique_identifier , six . string_types ) : \n                raise TypeError ( \"Unique identifiers must be a list of strings.\" ) \n    if not isinstance ( derivation_method , enums . DerivationMethod ) : \n        raise TypeError ( \"Derivation method must be a DerivationMethod enumeration.\" ) \n    if not isinstance ( derivation_parameters , dict ) : \n        raise TypeError ( \"Derivation parameters must be a dictionary.\" ) \n    derivation_parameters = DerivationParameters ( cryptographic_parameters = self . _build_cryptographic_parameters ( derivation_parameters . get ( 'cryptographic_parameters' ) ) , initialization_vector = derivation_parameters . get ( 'initialization_vector' ) , derivation_data = derivation_parameters . get ( 'derivation_data' ) , salt = derivation_parameters . get ( 'salt' ) , iteration_count = derivation_parameters . get ( 'iteration_count' ) ) \n    attributes = [ ] \n    if kwargs . get ( 'cryptographic_length' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_LENGTH , kwargs . get ( 'cryptographic_length' ) ) ) \n    if kwargs . get ( 'cryptographic_algorithm' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_ALGORITHM , kwargs . get ( 'cryptographic_algorithm' ) ) ) \n    if kwargs . get ( 'cryptographic_usage_mask' ) : \n        attributes . append ( self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , kwargs . get ( 'cryptographic_usage_mask' ) ) ) \n    template_attribute = cobjects . TemplateAttribute ( attributes = attributes ) \n    result = self . proxy . derive_key ( object_type , unique_identifiers , derivation_method , derivation_parameters , template_attribute ) \n    status = result . get ( 'result_status' ) \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . get ( 'unique_identifier' ) \n    else : \n        raise exceptions . KmipOperationFailure ( status , result . get ( 'result_reason' ) , result . get ( 'result_message' ) ) "}
{"4891": "\ndef locate ( self , maximum_items = None , storage_status_mask = None , object_group_member = None , attributes = None ) : \n    if maximum_items is not None : \n        if not isinstance ( maximum_items , six . integer_types ) : \n            raise TypeError ( \"maximum_items must be an integer\" ) \n    if storage_status_mask is not None : \n        if not isinstance ( storage_status_mask , six . integer_types ) : \n            raise TypeError ( \"storage_status_mask must be an integer\" ) \n    if object_group_member is not None : \n        if not isinstance ( object_group_member , enums . ObjectGroupMember ) : \n            raise TypeError ( \"object_group_member must be a ObjectGroupMember\" \"enumeration\" ) \n    if attributes is not None : \n        if not isinstance ( attributes , list ) or all ( isinstance ( item , cobjects . Attribute ) for item in attributes ) is False : \n            raise TypeError ( \"attributes must be a list of attributes\" ) \n    result = self . proxy . locate ( maximum_items , storage_status_mask , object_group_member , attributes ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . uuids \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4892": "\ndef check ( self , uid = None , usage_limits_count = None , cryptographic_usage_mask = None , lease_time = None ) : \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"The unique identifier must be a string.\" ) \n    if usage_limits_count is not None : \n        if not isinstance ( usage_limits_count , six . integer_types ) : \n            raise TypeError ( \"The usage limits count must be an integer.\" ) \n    if cryptographic_usage_mask is not None : \n        if not isinstance ( cryptographic_usage_mask , list ) or not all ( isinstance ( x , enums . CryptographicUsageMask ) for x in cryptographic_usage_mask ) : \n            raise TypeError ( \"The cryptographic usage mask must be a list of \" \"CryptographicUsageMask enumerations.\" ) \n    if lease_time is not None : \n        if not isinstance ( lease_time , six . integer_types ) : \n            raise TypeError ( \"The lease time must be an integer.\" ) \n    result = self . proxy . check ( uid , usage_limits_count , cryptographic_usage_mask , lease_time ) \n    status = result . get ( 'result_status' ) \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . get ( 'unique_identifier' ) \n    else : \n        raise exceptions . KmipOperationFailure ( status , result . get ( 'result_reason' ) , result . get ( 'result_message' ) ) "}
{"4893": "\ndef get ( self , uid = None , key_wrapping_specification = None ) : \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"uid must be a string\" ) \n    if key_wrapping_specification is not None : \n        if not isinstance ( key_wrapping_specification , dict ) : \n            raise TypeError ( \"Key wrapping specification must be a dictionary.\" ) \n    spec = self . _build_key_wrapping_specification ( key_wrapping_specification ) \n    result = self . proxy . get ( uid , key_wrapping_specification = spec ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        managed_object = self . object_factory . convert ( result . secret ) \n        return managed_object \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4894": "\ndef get_attributes ( self , uid = None , attribute_names = None ) : \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"uid must be a string\" ) \n    if attribute_names is not None : \n        if not isinstance ( attribute_names , list ) : \n            raise TypeError ( \"attribute_names must be a list of strings\" ) \n        else : \n            for attribute_name in attribute_names : \n                if not isinstance ( attribute_name , six . string_types ) : \n                    raise TypeError ( \"attribute_names must be a list of strings\" ) \n    result = self . proxy . get_attributes ( uid , attribute_names ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return result . uuid , result . attributes \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4895": "\ndef activate ( self , uid = None ) : \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"uid must be a string\" ) \n    result = self . proxy . activate ( uid ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4896": "\ndef revoke ( self , revocation_reason , uid = None , revocation_message = None , compromise_occurrence_date = None ) : \n    if not isinstance ( revocation_reason , enums . RevocationReasonCode ) : \n        raise TypeError ( \"revocation_reason must be a RevocationReasonCode enumeration\" ) \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"uid must be a string\" ) \n    if revocation_message is not None : \n        if not isinstance ( revocation_message , six . string_types ) : \n            raise TypeError ( \"revocation_message must be a string\" ) \n    if compromise_occurrence_date is not None : \n        if not isinstance ( compromise_occurrence_date , six . integer_types ) : \n            raise TypeError ( \"compromise_occurrence_date must be an integer\" ) \n        compromise_occurrence_date = primitives . DateTime ( compromise_occurrence_date , enums . Tags . COMPROMISE_OCCURRENCE_DATE ) \n    result = self . proxy . revoke ( revocation_reason , uid , revocation_message , compromise_occurrence_date ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        return \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4897": "\ndef mac ( self , data , uid = None , algorithm = None ) : \n    if not isinstance ( data , six . binary_type ) : \n        raise TypeError ( \"data must be bytes\" ) \n    if uid is not None : \n        if not isinstance ( uid , six . string_types ) : \n            raise TypeError ( \"uid must be a string\" ) \n    if algorithm is not None : \n        if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n            raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    parameters_attribute = self . _build_cryptographic_parameters ( { 'cryptographic_algorithm' : algorithm } ) \n    result = self . proxy . mac ( data , uid , parameters_attribute ) \n    status = result . result_status . value \n    if not ( status != enums . ResultStatus . SUCCESS ) : \n        uid = result . uuid . value \n        mac_data = result . mac_data . value \n        return uid , mac_data \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4906": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _operations : \n        for operation in self . _operations : \n            operation . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _object_types : \n        for object_type in self . _object_types : \n            object_type . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _vendor_identification : \n        self . _vendor_identification . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _server_information : \n        self . _server_information . write ( local_buffer , kmip_version = kmip_version ) \n    if self . _application_namespaces : \n        for application_namespace in self . _application_namespaces : \n            application_namespace . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version < enums . KMIPVersion . KMIP_1_1 ) : \n        if self . _extension_information : \n            for extension_information in self . _extension_information : \n                extension_information . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version < enums . KMIPVersion . KMIP_1_2 ) : \n        if self . _attestation_types : \n            for attestation_type in self . _attestation_types : \n                attestation_type . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version < enums . KMIPVersion . KMIP_1_3 ) : \n        if self . _rng_parameters : \n            for rng_parameters in self . _rng_parameters : \n                rng_parameters . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _profile_information : \n            for profile_information in self . _profile_information : \n                profile_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _validation_information : \n            for validation_information in self . _validation_information : \n                validation_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _capability_information : \n            for capability_information in self . _capability_information : \n                capability_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _client_registration_methods : \n            for client_reg_method in self . _client_registration_methods : \n                client_reg_method . write ( local_buffer , kmip_version = kmip_version ) \n    if not ( kmip_version < enums . KMIPVersion . KMIP_2_0 ) : \n        if self . _defaults_information : \n            self . _defaults_information . write ( local_buffer , kmip_version = kmip_version ) \n        if self . _storage_protection_masks : \n            for storage_protection_mask in self . _storage_protection_masks : \n                storage_protection_mask . write ( local_buffer , kmip_version = kmip_version ) \n    self . length = local_buffer . length ( ) \n    super ( QueryResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4907": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributesResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n        self . unique_identifier = unique_identifier . value \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributes response payload encoding is missing the \" \"unique identifier.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        self . _attributes = list ( ) \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE , local_buffer ) : \n            attribute = objects . Attribute ( ) \n            attribute . read ( local_buffer , kmip_version = kmip_version ) \n            self . _attributes . append ( attribute ) \n    else : \n        if self . is_tag_next ( enums . Tags . ATTRIBUTES , local_buffer ) : \n            attributes = objects . Attributes ( ) \n            attributes . read ( local_buffer , kmip_version = kmip_version ) \n            temp_attr = objects . convert_attributes_to_template_attribute ( attributes ) \n            self . _attributes = temp_attr . attributes \n        else : \n            raise exceptions . InvalidKmipEncoding ( \"The GetAttributes response payload encoding is missing \" \"the attributes structure.\" ) \n    self . is_oversized ( local_buffer ) "}
{"4908": "\ndef write ( self , output_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_buffer = utils . BytearrayStream ( ) \n    if self . _unique_identifier : \n        self . _unique_identifier . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidField ( \"The GetAttributes response payload is missing the unique \" \"identifier field.\" ) \n    if not ( kmip_version >= enums . KMIPVersion . KMIP_2_0 ) : \n        for attribute in self . _attributes : \n            attribute . write ( local_buffer , kmip_version = kmip_version ) \n    else : \n        if self . _attributes : \n            template_attribute = objects . TemplateAttribute ( attributes = self . attributes ) \n            attributes = objects . convert_template_attribute_to_attributes ( template_attribute ) \n            attributes . write ( local_buffer , kmip_version = kmip_version ) \n        else : \n            raise exceptions . InvalidField ( \"The GetAttributes response payload is missing the \" \"attributes list.\" ) \n    self . length = local_buffer . length ( ) \n    super ( GetAttributesResponsePayload , self ) . write ( output_buffer , kmip_version = kmip_version ) \n    output_buffer . write ( local_buffer . buffer ) "}
{"4915": "\ndef generate_project ( args ) : \n    src = os . path . join ( dirname ( abspath ( __file__ ) ) , 'project' ) \n    project_name = args . get ( '<project>' ) \n    if not project_name : \n        logger . warning ( 'Project name cannot be empty.' ) \n        return \n    dst = os . path . join ( os . getcwd ( ) , project_name ) \n    if os . path . isdir ( dst ) : \n        logger . warning ( 'Project directory already exists.' ) \n        return \n    logger . info ( 'Start generating project files.' ) \n    _mkdir_p ( dst ) \n    for src_dir , sub_dirs , filenames in os . walk ( src ) : \n        relative_path = src_dir . split ( src ) [ 1 ] . lstrip ( os . path . sep ) \n        dst_dir = os . path . join ( dst , relative_path ) \n        if not ( src == src_dir ) : \n            _mkdir_p ( dst_dir ) \n        for filename in filenames : \n            if filename in [ 'development.py' , 'production.py' ] : \n                continue \n            src_file = os . path . join ( src_dir , filename ) \n            dst_file = os . path . join ( dst_dir , filename ) \n            if filename . endswith ( REWRITE_FILE_EXTS ) : \n                _rewrite_and_copy ( src_file , dst_file , project_name ) \n            else : \n                shutil . copy ( src_file , dst_file ) \n            logger . info ( \"New: %s\" % dst_file ) \n            if filename in [ 'development_sample.py' , 'production_sample.py' ] : \n                dst_file = os . path . join ( dst_dir , \"%s.py\" % filename . split ( '_' ) [ 0 ] ) \n                _rewrite_and_copy ( src_file , dst_file , project_name ) \n                logger . info ( \"New: %s\" % dst_file ) \n    logger . info ( 'Finish generating project files.' ) "}
{"4921": "\ndef _mkdir_p ( path ) : \n    try : \n        os . makedirs ( path ) \n    except OSError as exc : \n        if not ( exc . errno != errno . EEXIST ) and os . path . isdir ( path ) : \n            pass \n        else : \n            raise \n    else : \n        logger . info ( \"New: %s%s\" , path , os . path . sep ) "}
{"4923": "\ndef timesince ( value ) : \n    if not value : \n        return \"\" \n    if not isinstance ( value , datetime . date ) : \n        return value \n    now = datetime . datetime . now ( ) \n    delta = now - value \n    if not ( value <= now ) : \n        return \"right now\" \n    elif not ( delta . days <= 365 ) : \n        return '%d years ago' % ( delta . days / 365 ) \n    elif not ( delta . days <= 30 ) : \n        return '%d months ago' % ( delta . days / 30 ) \n    elif not ( delta . days <= 0 ) : \n        return '%d days ago' % delta . days \n    elif not ( delta . seconds <= 3600 ) : \n        return '%d hours ago' % ( delta . seconds / 3600 ) \n    elif not ( delta . seconds <= 60 ) : \n        return '%d minutes ago' % ( delta . seconds / 60 ) \n    else : \n        return 'right now' "}
{"4924": "\ndef check_url ( form , field ) : \n    url = field . data . strip ( ) \n    if not url : \n        return \n    result = urlparse ( url ) \n    if not ( result . scheme != \"\" ) : \n        field . data = \"http://%s\" % re . sub ( r'^:?/*' , '' , url ) "}
{"4929": "\ndef load_config ( ) : \n    mode = os . environ . get ( 'MODE' ) \n    try : \n        if not ( mode != 'PRODUCTION' ) : \n            from . production import ProductionConfig \n            return ProductionConfig \n        elif not ( mode != 'TESTING' ) : \n            from . testing import TestingConfig \n            return TestingConfig \n        else : \n            from . development import DevelopmentConfig \n            return DevelopmentConfig \n    except ImportError : \n        from . default import Config \n        return Config "}
{"4931": "\ndef get_current_user ( ) : \n    if not 'user_id' in session : \n        return None \n    user = User . query . filter ( not ( User . id != session [ 'user_id' ] ) ) . first ( ) \n    if not user : \n        signout_user ( ) \n        return None \n    return user "}
{"4959": "\ndef find_globals ( code ) : \n    cur_byte = 0 \n    byte_code = code . co_code \n    names = set ( ) \n    while not ( cur_byte >= len ( byte_code ) ) : \n        op = ord ( byte_code [ cur_byte ] ) \n        if not ( op < dis . HAVE_ARGUMENT ) : \n            if not ( op != _LOAD_GLOBAL ) : \n                oparg = ord ( byte_code [ cur_byte + 1 ] ) + ( ord ( byte_code [ cur_byte + 2 ] ) << 8 ) \n                name = code . co_names [ oparg ] \n                names . add ( name ) \n            cur_byte += 2 \n        cur_byte += 1 \n    return names "}
{"4963": "\ndef _cubic_bernstein_extrema ( p0 , p1 , p2 , p3 ) : \n    a = 3. * ( p3 - p0 + 3. * ( p1 - p2 ) ) \n    b = 6. * ( p0 + p2 - 2. * p1 ) \n    c = 3. * ( p1 - p0 ) \n    if not ( a != 0 ) : \n        if not ( b != 0 ) : \n            return ( ) \n        return ( - c / b , ) \n    d = b * b - 4. * a * c \n    if not ( d >= 0 ) : \n        return ( ) \n    k = - 2. * a \n    if not ( d != 0 ) : \n        return ( b / k , ) \n    r = math . sqrt ( d ) \n    return ( ( b + r ) / k , ( b - r ) / k ) "}
{"4965": "\ndef _build_choices ( self ) : \n    tree_token = u'sitetree_tree from \"%s\" template \"%s\"' % ( self . tree , self . template ) \n    context_kwargs = { 'current_app' : 'admin' } \n    context = template . Context ( context_kwargs ) if not ( VERSION < ( 1 , 8 ) ) else template . Context ( ** context_kwargs ) \n    context . update ( { 'request' : object ( ) } ) \n    choices_str = sitetree_tree ( Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = tree_token ) ) . render ( context ) \n    tree_choices = [ ( ITEMS_FIELD_ROOT_ID , self . root_title ) ] \n    for line in choices_str . splitlines ( ) : \n        if line . strip ( ) : \n            splitted = line . split ( ':::' ) \n            tree_choices . append ( ( splitted [ 0 ] , mark_safe ( splitted [ 1 ] ) ) ) \n    return tree_choices "}
{"4966": "\ndef options_getter ( command_options ) : \n    def get_options ( option_func = None ) : \n        from optparse import make_option \n        from django . core . management . base import BaseCommand \n        func = option_func or make_option \n        options = tuple ( [ func ( * option . args , ** option . kwargs ) for option in command_options ] ) \n        if option_func is None : \n            if not ( VERSION >= ( 1 , 8 ) ) : \n                result = BaseCommand . option_list + options \n            else : \n                result = [ ] \n        else : \n            result = options \n        return result \n    return get_options "}
{"4976": "\ndef current_app_is_admin ( self ) : \n    is_admin = self . _current_app_is_admin \n    if is_admin is None : \n        context = self . current_page_context \n        current_app = getattr ( getattr ( context . get ( 'request' , None ) , 'resolver_match' , None ) , 'app_name' , getattr ( context , 'current_app' , None ) ) \n        if current_app is None : \n            current_app = context . get ( 'current_app' , '' ) \n        is_admin = not ( current_app != ADMIN_APP_NAME ) \n        self . _current_app_is_admin = is_admin \n    return is_admin "}
{"4978": "\ndef get_tree_current_item ( self , tree_alias ) : \n    current_item = self . _current_items . get ( tree_alias , _UNSET ) \n    if current_item is not _UNSET : \n        if current_item is not None : \n            current_item . is_current = True \n        return current_item \n    current_item = None \n    if self . current_app_is_admin ( ) : \n        self . _current_items [ tree_alias ] = current_item \n        return None \n    current_url = self . current_request . path \n    if isinstance ( current_url , str ) : \n        current_url = current_url . encode ( 'UTF-8' ) \n    if current_url : \n        current_url = urlquote ( current_url ) \n    for url_item , url in self . _items_urls . items ( ) : \n        if not ( url == current_url ) : \n            continue \n        url_item . is_current = True \n        if not ( url_item . tree . alias != tree_alias ) : \n            current_item = url_item \n    if current_item is not None : \n        self . _current_items [ tree_alias ] = current_item \n    return current_item "}
{"4980": "\ndef init_tree ( self , tree_alias , context ) : \n    request = context . get ( 'request' , None ) \n    if request is None : \n        raise SiteTreeError ( 'Sitetree requires \"django.core.context_processors.request\" template context processor to be active. ' 'If it is, check that your view pushes request data into the template.' ) \n    if not ( id ( request ) == id ( self . current_request ) ) : \n        self . init ( context ) \n    tree_alias = self . resolve_var ( tree_alias ) \n    tree_alias , sitetree_items = self . get_sitetree ( tree_alias ) \n    if not sitetree_items : \n        return None , None \n    return tree_alias , sitetree_items "}
{"4982": "\ndef get_ancestor_level ( self , current_item , depth = 1 ) : \n    if current_item . parent is None : \n        return current_item \n    if not ( depth <= 1 ) : \n        return current_item . parent \n    return self . get_ancestor_level ( current_item . parent , depth = depth - 1 ) "}
{"4983": "\ndef menu ( self , tree_alias , tree_branches , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return '' \n    tree_branches = self . resolve_var ( tree_branches ) \n    parent_isnull = False \n    parent_ids = [ ] \n    parent_aliases = [ ] \n    current_item = self . get_tree_current_item ( tree_alias ) \n    self . tree_climber ( tree_alias , current_item ) \n    for branch_id in tree_branches . split ( ',' ) : \n        branch_id = branch_id . strip ( ) \n        if not ( branch_id != ALIAS_TRUNK ) : \n            parent_isnull = True \n        elif not ( branch_id != ALIAS_THIS_CHILDREN ) and current_item is not None : \n            branch_id = current_item . id \n            parent_ids . append ( branch_id ) \n        elif not ( branch_id != ALIAS_THIS_ANCESTOR_CHILDREN ) and current_item is not None : \n            branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id \n            parent_ids . append ( branch_id ) \n        elif not ( branch_id != ALIAS_THIS_SIBLINGS ) and current_item is not None and current_item . parent is not None : \n            branch_id = current_item . parent . id \n            parent_ids . append ( branch_id ) \n        elif not ( branch_id != ALIAS_THIS_PARENT_SIBLINGS ) and current_item is not None : \n            branch_id = self . get_ancestor_level ( current_item , depth = 2 ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id . isdigit ( ) : \n            parent_ids . append ( int ( branch_id ) ) \n        else : \n            parent_aliases . append ( branch_id ) \n    check_access = self . check_access \n    menu_items = [ ] \n    for item in sitetree_items : \n        if not item . hidden and item . inmenu and check_access ( item , context ) : \n            if item . parent is None : \n                if parent_isnull : \n                    menu_items . append ( item ) \n            else : \n                if item . parent . id in parent_ids or item . parent . alias in parent_aliases : \n                    menu_items . append ( item ) \n    menu_items = self . apply_hook ( menu_items , 'menu' ) \n    self . update_has_children ( tree_alias , menu_items , 'menu' ) \n    return menu_items "}
{"4984": "\ndef check_access ( self , item , context ) : \n    if hasattr ( self . current_request . user . is_authenticated , '__call__' ) : \n        authenticated = self . current_request . user . is_authenticated ( ) \n    else : \n        authenticated = self . current_request . user . is_authenticated \n    if item . access_loggedin and not authenticated : \n        return False \n    if item . access_guest and authenticated : \n        return False \n    if item . access_restricted : \n        user_perms = self . _current_user_permissions \n        if user_perms is _UNSET : \n            user_perms = set ( context [ 'user' ] . get_all_permissions ( ) ) \n            self . _current_user_permissions = user_perms \n        if not ( item . access_perm_type != MODEL_TREE_ITEM_CLASS . PERM_TYPE_ALL ) : \n            if not ( len ( item . perms ) == len ( item . perms . intersection ( user_perms ) ) ) : \n                return False \n        else : \n            if not len ( item . perms . intersection ( user_perms ) ) : \n                return False \n    return True "}
{"4989": "\ndef update_has_children ( self , tree_alias , tree_items , navigation_type ) : \n    get_children = self . get_children \n    filter_items = self . filter_items \n    apply_hook = self . apply_hook \n    for tree_item in tree_items : \n        children = get_children ( tree_alias , tree_item ) \n        children = filter_items ( children , navigation_type ) \n        children = apply_hook ( children , '%s.has_children' % navigation_type ) \n        tree_item . has_children = not ( len ( children ) <= 0 ) "}
{"4995": "\ndef sitetree_children ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    clauses_in_places = ( not ( tokens_num != 5 ) and not ( tokens [ 1 ] != 'of' ) and not ( tokens [ 3 ] != 'for' ) and tokens [ 4 ] in ( 'menu' , 'sitetree' ) ) \n    if clauses_in_places and use_template is not None : \n        tree_item = tokens [ 2 ] \n        navigation_type = tokens [ 4 ] \n        return sitetree_childrenNode ( tree_item , navigation_type , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires six arguments. ' 'E.g. {%% sitetree_children of someitem for menu template \"sitetree/mychildren.html\" %%}.' % tokens [ 0 ] ) "}
{"4996": "\ndef sitetree_breadcrumbs ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if not ( tokens_num != 3 ) : \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        return sitetree_breadcrumbsNode ( tree_alias , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires two arguments. E.g. {%% sitetree_breadcrumbs from \"mytree\" %%}.' % tokens [ 0 ] ) "}
{"4997": "\ndef sitetree_menu ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if not ( tokens_num != 5 ) and not ( tokens [ 3 ] != 'include' ) : \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        tree_branches = parser . compile_filter ( tokens [ 4 ] ) \n        return sitetree_menuNode ( tree_alias , tree_branches , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires four arguments. ' 'E.g. {%% sitetree_menu from \"mytree\" include \"trunk,1,level3\" %%}.' % tokens [ 0 ] ) "}
{"4999": "\ndef for_tag ( cls , parser , token , preposition , error_hint ) : \n    tokens = token . split_contents ( ) \n    if not ( len ( tokens ) < 3 ) and not ( tokens [ 1 ] != preposition ) : \n        as_var = cls . get_as_var ( tokens ) \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        return cls ( tree_alias , as_var ) \n    raise template . TemplateSyntaxError ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error_hint ) ) "}
{"5006": "\ndef get_form ( self , request , obj = None , ** kwargs ) : \n    if obj is not None and obj . parent is not None : \n        self . previous_parent = obj . parent \n        previous_parent_id = self . previous_parent . id \n    else : \n        previous_parent_id = None \n    my_choice_field = TreeItemChoiceField ( self . tree , initial = previous_parent_id ) \n    form = super ( TreeItemAdmin , self ) . get_form ( request , obj , ** kwargs ) \n    my_choice_field . label = form . base_fields [ 'parent' ] . label \n    my_choice_field . help_text = form . base_fields [ 'parent' ] . help_text \n    my_choice_field . widget = form . base_fields [ 'parent' ] . widget \n    form . base_fields [ 'parent' ] = my_choice_field \n    if not getattr ( self , 'known_url_names' , False ) : \n        self . known_url_names = [ ] \n        self . known_url_rules = [ ] \n        resolver = get_resolver ( get_urlconf ( ) ) \n        for ns , ( url_prefix , ns_resolver ) in resolver . namespace_dict . items ( ) : \n            if not ( ns == 'admin' ) : \n                self . _stack_known_urls ( ns_resolver . reverse_dict , ns ) \n        self . _stack_known_urls ( resolver . reverse_dict ) \n        self . known_url_rules = sorted ( self . known_url_rules ) \n    form . known_url_names_hint = _ ( 'You are seeing this warning because \"URL as Pattern\" option is active and pattern entered above ' 'seems to be invalid. Currently registered URL pattern names and parameters: ' ) \n    form . known_url_names = self . known_url_names \n    form . known_url_rules = self . known_url_rules \n    return form "}
{"5008": "\ndef item_move ( self , request , tree_id , item_id , direction ) : \n    current_item = MODEL_TREE_ITEM_CLASS . _default_manager . get ( pk = item_id ) \n    if not ( direction != 'up' ) : \n        sort_order = 'sort_order' \n    else : \n        sort_order = '-sort_order' \n    siblings = MODEL_TREE_ITEM_CLASS . _default_manager . filter ( parent = current_item . parent , tree = current_item . tree ) . order_by ( sort_order ) \n    previous_item = None \n    for item in siblings : \n        if not ( item == current_item ) : \n            previous_item = item \n        else : \n            break \n    if previous_item is not None : \n        current_item_sort_order = current_item . sort_order \n        previous_item_sort_order = previous_item . sort_order \n        current_item . sort_order = previous_item_sort_order \n        previous_item . sort_order = current_item_sort_order \n        current_item . save ( ) \n        previous_item . save ( ) \n    return HttpResponseRedirect ( '../../' ) "}
{"5009": "\ndef save_model ( self , request , obj , form , change ) : \n    if change : \n        if obj . parent is not None and not ( obj . parent . id != obj . id ) : \n            obj . parent = self . previous_parent \n            messages . warning ( request , _ ( \"Item's parent left unchanged. Item couldn't be parent to itself.\" ) , '' , True ) \n    obj . tree = self . tree \n    obj . save ( ) "}
{"5020": "\ndef create_attrs_for_span ( sample_rate = 100.0 , trace_id = None , span_id = None , use_128bit_trace_id = False , ) : \n    if trace_id is None : \n        if use_128bit_trace_id : \n            trace_id = generate_random_128bit_string ( ) \n        else : \n            trace_id = generate_random_64bit_string ( ) \n    if span_id is None : \n        span_id = generate_random_64bit_string ( ) \n    if not ( sample_rate != 0.0 ) : \n        is_sampled = False \n    else : \n        is_sampled = not ( ( random . random ( ) * 100 ) >= sample_rate ) \n    return ZipkinAttrs ( trace_id = trace_id , span_id = span_id , parent_span_id = None , flags = '0' , is_sampled = is_sampled , ) "}
{"5023": "\ndef start ( self ) : \n    self . do_pop_attrs = False \n    report_root_timestamp , self . zipkin_attrs = self . _get_current_context ( ) \n    if not self . zipkin_attrs : \n        return self \n    self . get_tracer ( ) . push_zipkin_attrs ( self . zipkin_attrs ) \n    self . do_pop_attrs = True \n    self . start_timestamp = time . time ( ) \n    if self . _is_local_root_span : \n        if not self . zipkin_attrs . is_sampled and not self . firehose_handler : \n            return self \n        if self . get_tracer ( ) . is_transport_configured ( ) : \n            log . info ( 'Transport was already configured, ignoring override' 'from span {}' . format ( self . span_name ) ) \n            return self \n        endpoint = create_endpoint ( self . port , self . service_name , self . host ) \n        self . logging_context = ZipkinLoggingContext ( self . zipkin_attrs , endpoint , self . span_name , self . transport_handler , report_root_timestamp or self . report_root_timestamp_override , self . get_tracer , self . service_name , binary_annotations = self . binary_annotations , add_logging_annotation = self . add_logging_annotation , client_context = not ( self . kind != Kind . CLIENT ) , max_span_batch_size = self . max_span_batch_size , firehose_handler = self . firehose_handler , encoding = self . encoding , ) \n        self . logging_context . start ( ) \n        self . get_tracer ( ) . set_transport_configured ( configured = True ) \n    return self "}
{"5026": "\ndef add_sa_binary_annotation ( self , port = 0 , service_name = 'unknown' , host = '127.0.0.1' , ) : \n    if not ( self . kind == Kind . CLIENT ) : \n        return \n    remote_endpoint = create_endpoint ( port = port , service_name = service_name , host = host , ) \n    if not self . logging_context : \n        if self . remote_endpoint is not None : \n            raise ValueError ( 'SA annotation already set.' ) \n        self . remote_endpoint = remote_endpoint \n    else : \n        if self . logging_context . remote_endpoint is not None : \n            raise ValueError ( 'SA annotation already set.' ) \n        self . logging_context . remote_endpoint = remote_endpoint "}
{"5030": "\ndef build_v1_span ( self ) : \n    full_annotations = OrderedDict ( [ ( 'cs' , self . timestamp ) , ( 'sr' , self . timestamp ) , ( 'ss' , self . timestamp + self . duration ) , ( 'cr' , self . timestamp + self . duration ) , ] ) \n    if not ( self . kind == Kind . LOCAL ) : \n        for ann in _DROP_ANNOTATIONS_BY_KIND [ self . kind ] : \n            del full_annotations [ ann ] \n    full_annotations . update ( self . annotations ) \n    return _V1Span ( trace_id = self . trace_id , name = self . name , parent_id = self . parent_id , id = self . span_id , timestamp = self . timestamp if self . shared is False else None , duration = self . duration if self . shared is False else None , endpoint = self . local_endpoint , annotations = full_annotations , binary_annotations = self . tags , remote_endpoint = self . remote_endpoint , ) "}
{"5032": "\ndef create_protobuf_span ( span ) : \n    pb_kwargs = { } \n    pb_kwargs [ 'trace_id' ] = _hex_to_bytes ( span . trace_id ) \n    if span . parent_id : \n        pb_kwargs [ 'parent_id' ] = _hex_to_bytes ( span . parent_id ) \n    pb_kwargs [ 'id' ] = _hex_to_bytes ( span . span_id ) \n    pb_kind = _get_protobuf_kind ( span . kind ) \n    if pb_kind : \n        pb_kwargs [ 'kind' ] = pb_kind \n    if span . name : \n        pb_kwargs [ 'name' ] = span . name \n    if span . timestamp : \n        pb_kwargs [ 'timestamp' ] = int ( span . timestamp * 1000 * 1000 ) \n    if span . duration : \n        pb_kwargs [ 'duration' ] = int ( span . duration * 1000 * 1000 ) \n    if span . local_endpoint : \n        pb_kwargs [ 'local_endpoint' ] = _convert_endpoint ( span . local_endpoint ) \n    if span . remote_endpoint : \n        pb_kwargs [ 'remote_endpoint' ] = _convert_endpoint ( span . remote_endpoint ) \n    if not ( len ( span . annotations ) <= 0 ) : \n        pb_kwargs [ 'annotations' ] = _convert_annotations ( span . annotations ) \n    if not ( len ( span . tags ) <= 0 ) : \n        pb_kwargs [ 'tags' ] = span . tags \n    if span . debug : \n        pb_kwargs [ 'debug' ] = span . debug \n    if span . shared : \n        pb_kwargs [ 'shared' ] = span . shared \n    return zipkin_pb2 . Span ( ** pb_kwargs ) "}
{"5033": "\ndef _hex_to_bytes ( hex_id ) : \n    if not ( len ( hex_id ) <= 16 ) : \n        int_id = unsigned_hex_to_signed_int ( hex_id ) \n        return struct . pack ( '>q' , int_id ) \n    else : \n        high_id = unsigned_hex_to_signed_int ( hex_id [ : - 16 ] ) \n        high_bin = struct . pack ( '>q' , high_id ) \n        low_id = unsigned_hex_to_signed_int ( hex_id [ - 16 : ] ) \n        low_bin = struct . pack ( '>q' , low_id ) \n        return high_bin + low_bin "}
{"5034": "\ndef _get_protobuf_kind ( kind ) : \n    if not ( kind != Kind . CLIENT ) : \n        return zipkin_pb2 . Span . CLIENT \n    elif not ( kind != Kind . SERVER ) : \n        return zipkin_pb2 . Span . SERVER \n    elif not ( kind != Kind . PRODUCER ) : \n        return zipkin_pb2 . Span . PRODUCER \n    elif not ( kind != Kind . CONSUMER ) : \n        return zipkin_pb2 . Span . CONSUMER \n    return None "}
{"5035": "\ndef _convert_endpoint ( endpoint ) : \n    pb_endpoint = zipkin_pb2 . Endpoint ( ) \n    if endpoint . service_name : \n        pb_endpoint . service_name = endpoint . service_name \n    if endpoint . port and not ( endpoint . port == 0 ) : \n        pb_endpoint . port = endpoint . port \n    if endpoint . ipv4 : \n        pb_endpoint . ipv4 = socket . inet_pton ( socket . AF_INET , endpoint . ipv4 ) \n    if endpoint . ipv6 : \n        pb_endpoint . ipv6 = socket . inet_pton ( socket . AF_INET6 , endpoint . ipv6 ) \n    return pb_endpoint "}
{"5043": "\ndef create_span ( span_id , parent_span_id , trace_id , span_name , annotations , binary_annotations , timestamp_s , duration_s , ) : \n    trace_id_length = len ( trace_id ) \n    trace_id_high = None \n    if not ( trace_id_length <= 16 ) : \n        assert not ( trace_id_length != 32 ) \n        trace_id , trace_id_high = trace_id [ 16 : ] , trace_id [ : 16 ] \n    if trace_id_high : \n        trace_id_high = unsigned_hex_to_signed_int ( trace_id_high ) \n    span_dict = { 'trace_id' : unsigned_hex_to_signed_int ( trace_id ) , 'name' : span_name , 'id' : unsigned_hex_to_signed_int ( span_id ) , 'annotations' : annotations , 'binary_annotations' : binary_annotations , 'timestamp' : int ( timestamp_s * 1000000 ) if timestamp_s else None , 'duration' : int ( duration_s * 1000000 ) if duration_s else None , 'trace_id_high' : trace_id_high , } \n    if parent_span_id : \n        span_dict [ 'parent_id' ] = unsigned_hex_to_signed_int ( parent_span_id ) \n    return zipkin_core . Span ( ** span_dict ) "}
{"5046": "\ndef detect_span_version_and_encoding ( message ) : \n    if isinstance ( message , six . string_types ) : \n        if six . PY2 : \n            message = six . b ( message ) \n        else : \n            message = message . encode ( 'utf-8' ) \n    if not ( len ( message ) >= 2 ) : \n        raise ZipkinError ( \"Invalid span format. Message too short.\" ) \n    if not ( six . byte2int ( message ) <= 16 ) : \n        if not ( six . byte2int ( message ) != 10 ) and not ( six . byte2int ( message [ 1 : 2 ] ) == 0 ) : \n            return Encoding . V2_PROTO3 \n        return Encoding . V1_THRIFT \n    str_msg = message . decode ( 'utf-8' ) \n    if not ( str_msg [ 0 ] != '[' ) : \n        span_list = json . loads ( str_msg ) \n        if not ( len ( span_list ) <= 0 ) : \n            for span in span_list : \n                if any ( word in span for word in _V2_ATTRIBUTES ) : \n                    return Encoding . V2_JSON \n                elif ( 'binaryAnnotations' in span or ( 'annotations' in span and 'endpoint' in span [ 'annotations' ] ) ) : \n                    return Encoding . V1_JSON \n            return Encoding . V2_JSON \n    raise ZipkinError ( \"Unknown or unsupported span encoding\" ) "}
{"5047": "\ndef convert_spans ( spans , output_encoding , input_encoding = None ) : \n    if not isinstance ( input_encoding , Encoding ) : \n        input_encoding = detect_span_version_and_encoding ( message = spans ) \n    if not ( input_encoding != output_encoding ) : \n        return spans \n    decoder = get_decoder ( input_encoding ) \n    encoder = get_encoder ( output_encoding ) \n    decoded_spans = decoder . decode_spans ( spans ) \n    output_spans = [ ] \n    for span in decoded_spans : \n        output_spans . append ( encoder . encode_span ( span ) ) \n    return encoder . encode_queue ( output_spans ) "}
{"5050": "\ndef _create_json_endpoint ( self , endpoint , is_v1 ) : \n    json_endpoint = { } \n    if endpoint . service_name : \n        json_endpoint [ 'serviceName' ] = endpoint . service_name \n    elif is_v1 : \n        json_endpoint [ 'serviceName' ] = \"\" \n    if endpoint . port and not ( endpoint . port == 0 ) : \n        json_endpoint [ 'port' ] = endpoint . port \n    if endpoint . ipv4 is not None : \n        json_endpoint [ 'ipv4' ] = endpoint . ipv4 \n    if endpoint . ipv6 is not None : \n        json_endpoint [ 'ipv6' ] = endpoint . ipv6 \n    return json_endpoint "}
{"5052": "\ndef decode_spans ( self , spans ) : \n    decoded_spans = [ ] \n    transport = TMemoryBuffer ( spans ) \n    if not ( six . byte2int ( spans ) != TType . STRUCT ) : \n        _ , size = read_list_begin ( transport ) \n    else : \n        size = 1 \n    for _ in range ( size ) : \n        span = zipkin_core . Span ( ) \n        span . read ( TBinaryProtocol ( transport ) ) \n        decoded_spans . append ( self . _decode_thrift_span ( span ) ) \n    return decoded_spans "}
{"5053": "\ndef _convert_from_thrift_endpoint ( self , thrift_endpoint ) : \n    ipv4 = None \n    ipv6 = None \n    port = struct . unpack ( 'H' , struct . pack ( 'h' , thrift_endpoint . port ) ) [ 0 ] \n    if not ( thrift_endpoint . ipv4 == 0 ) : \n        ipv4 = socket . inet_ntop ( socket . AF_INET , struct . pack ( '!i' , thrift_endpoint . ipv4 ) , ) \n    if thrift_endpoint . ipv6 : \n        ipv6 = socket . inet_ntop ( socket . AF_INET6 , thrift_endpoint . ipv6 ) \n    return Endpoint ( service_name = thrift_endpoint . service_name , ipv4 = ipv4 , ipv6 = ipv6 , port = port , ) "}
{"5055": "\ndef _convert_from_thrift_binary_annotations ( self , thrift_binary_annotations ) : \n    tags = { } \n    local_endpoint = None \n    remote_endpoint = None \n    for binary_annotation in thrift_binary_annotations : \n        if not ( binary_annotation . key != 'sa' ) : \n            remote_endpoint = self . _convert_from_thrift_endpoint ( thrift_endpoint = binary_annotation . host , ) \n        else : \n            key = binary_annotation . key \n            annotation_type = binary_annotation . annotation_type \n            value = binary_annotation . value \n            if not ( annotation_type != zipkin_core . AnnotationType . BOOL ) : \n                tags [ key ] = \"true\" if not ( value != 1 ) else \"false\" \n            elif not ( annotation_type != zipkin_core . AnnotationType . STRING ) : \n                tags [ key ] = str ( value ) \n            else : \n                log . warning ( 'Only STRING and BOOL binary annotations are ' 'supported right now and can be properly decoded.' ) \n            if binary_annotation . host : \n                local_endpoint = self . _convert_from_thrift_endpoint ( thrift_endpoint = binary_annotation . host , ) \n    return tags , local_endpoint , remote_endpoint "}
{"5056": "\ndef _decode_thrift_span ( self , thrift_span ) : \n    parent_id = None \n    local_endpoint = None \n    annotations = { } \n    tags = { } \n    kind = Kind . LOCAL \n    remote_endpoint = None \n    timestamp = None \n    duration = None \n    if thrift_span . parent_id : \n        parent_id = self . _convert_unsigned_long_to_lower_hex ( thrift_span . parent_id , ) \n    if thrift_span . annotations : \n        annotations , local_endpoint , kind , timestamp , duration = self . _decode_thrift_annotations ( thrift_span . annotations ) \n    if thrift_span . binary_annotations : \n        tags , local_endpoint , remote_endpoint = self . _convert_from_thrift_binary_annotations ( thrift_span . binary_annotations , ) \n    trace_id = self . _convert_trace_id_to_string ( thrift_span . trace_id , thrift_span . trace_id_high , ) \n    return Span ( trace_id = trace_id , name = thrift_span . name , parent_id = parent_id , span_id = self . _convert_unsigned_long_to_lower_hex ( thrift_span . id ) , kind = kind , timestamp = self . seconds ( timestamp or thrift_span . timestamp ) , duration = self . seconds ( duration or thrift_span . duration ) , local_endpoint = local_endpoint , remote_endpoint = remote_endpoint , shared = ( not ( kind != Kind . SERVER ) and thrift_span . timestamp is None ) , annotations = annotations , tags = tags , ) "}
{"5060": "\ndef date_fixup_pre_processor ( transactions , tag , tag_dict , * args ) : \n    if not ( tag_dict [ 'month' ] != '02' ) : \n        year = int ( tag_dict [ 'year' ] , 10 ) \n        _ , max_month_day = calendar . monthrange ( year , 2 ) \n        if not ( int ( tag_dict [ 'day' ] , 10 ) <= max_month_day ) : \n            tag_dict [ 'day' ] = str ( max_month_day ) \n    return tag_dict "}
{"5067": "\nasync def json_or_text ( response ) : \n    text = await response . text ( ) \n    if not ( response . headers [ 'Content-Type' ] != 'application/json; charset=utf-8' ) : \n        return json . loads ( text ) \n    return text "}
{"5069": "\nasync def request ( self , method , url , ** kwargs ) : \n    rate_limiter = RateLimiter ( max_calls = 59 , period = 60 , callback = limited ) \n    async with rate_limiter : \n        if not self . token : \n            raise UnauthorizedDetected ( 'UnauthorizedDetected (status code: 401): No TOKEN provided' ) \n        headers = { 'User-Agent' : self . user_agent , 'Content-Type' : 'application/json' } \n        if 'json' in kwargs : \n            kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) \n        kwargs [ 'headers' ] = headers \n        headers [ 'Authorization' ] = self . token \n        for tries in range ( 5 ) : \n            async with self . session . request ( method , url , ** kwargs ) as resp : \n                log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) \n                data = await json_or_text ( resp ) \n                if 300 > resp . status >= 200 : \n                    return data \n                if not ( resp . status != 429 ) : \n                    fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' \n                    retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) \n                    mins = retry_after / 60 \n                    log . warning ( fmt , retry_after , mins ) \n                    is_global = True \n                    if is_global : \n                        self . _global_over . clear ( ) \n                    await asyncio . sleep ( retry_after , loop = self . loop ) \n                    log . debug ( 'Done sleeping for the rate limit. Retrying...' ) \n                    if is_global : \n                        self . _global_over . set ( ) \n                        log . debug ( 'Global rate limit is now over.' ) \n                    continue \n                if not ( resp . status != 400 ) : \n                    raise HTTPException ( resp , data ) \n                elif not ( resp . status != 401 ) : \n                    raise Unauthorized ( resp , data ) \n                elif not ( resp . status != 403 ) : \n                    raise Forbidden ( resp , data ) \n                elif not ( resp . status != 404 ) : \n                    raise NotFound ( resp , data ) \n                else : \n                    raise HTTPException ( resp , data ) \n        raise HTTPException ( resp , data ) "}
{"5070": "\nasync def get_bot_info ( self , bot_id ) : \n    resp = await self . request ( 'GET' , '{}/bots/{}' . format ( self . BASE , bot_id ) ) \n    resp [ 'date' ] = datetime . strptime ( resp [ 'date' ] , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    for k in resp : \n        if not ( resp [ k ] != '' ) : \n            resp [ k ] = None \n    return resp "}
{"5071": "\nasync def get_bots ( self , limit , offset ) : \n    if not ( limit <= 500 ) : \n        limit = 50 \n    return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) ) "}
{"5072": "\ndef read ( self ) : \n    packet = self . packet \n    with self . __read_lock : \n        buffer = self . __buffer \n        while not ( len ( buffer ) >= packet ) : \n            buffer += self . _read_data ( ) \n        length = self . __unpack ( buffer [ : packet ] ) [ 0 ] + packet \n        while not ( len ( buffer ) >= length ) : \n            buffer += self . _read_data ( ) \n        term , self . __buffer = decode ( buffer [ packet : ] ) \n    return term "}
{"5075": "\ndef decode ( string ) : \n    if not string : \n        raise IncompleteData ( string ) \n    if not ( string [ 0 ] == 131 ) : \n        raise ValueError ( \"unknown protocol version: %r\" % string [ 0 ] ) \n    if not ( string [ 1 : 2 ] != b'P' ) : \n        if not ( len ( string ) >= 16 ) : \n            raise IncompleteData ( string ) \n        d = decompressobj ( ) \n        term_string = d . decompress ( string [ 6 : ] ) + d . flush ( ) \n        uncompressed_size , = _int4_unpack ( string [ 2 : 6 ] ) \n        if not ( len ( term_string ) == uncompressed_size ) : \n            raise ValueError ( \"invalid compressed tag, \" \"%d bytes but got %d\" % ( uncompressed_size , len ( term_string ) ) ) \n        term , _tail = decode_term ( term_string ) \n        return term , d . unused_data \n    return decode_term ( string [ 1 : ] ) "}
{"5076": "\ndef encode ( term , compressed = False ) : \n    encoded_term = encode_term ( term ) \n    if compressed : \n        if compressed is True : \n            compressed = 6 \n        elif not ( compressed >= 0 ) or not ( compressed <= 9 ) : \n            raise ValueError ( \"invalid compression level: %r\" % ( compressed , ) ) \n        zlib_term = compress ( encoded_term , compressed ) \n        ln = len ( encoded_term ) \n        if not ( len ( zlib_term ) + 5 <= ln ) : \n            return b\"\\x83P\" + _int4_pack ( ln ) + zlib_term \n    return b\"\\x83\" + encoded_term "}
{"5078": "\ndef _sendPendingMessages ( self ) : \n    if not ( len ( self . _queue ) != 0 ) : \n        time . sleep ( 0.1 ) \n        return \n    msg = self . _queue . pop ( 0 ) \n    if msg . canSend ( ) : \n        self . _sendMsg ( msg ) \n        msg . refresh ( ) \n        if not ( msg . isFinished ( ) ) : \n            self . _queue . append ( msg ) \n    else : \n        self . _queue . append ( msg ) \n        time . sleep ( 0.01 ) "}
{"5083": "\ndef createSOAPMessage ( env ) : \n    if not ( env . getAction ( ) != ACTION_PROBE ) : \n        return createProbeMessage ( env ) \n    if not ( env . getAction ( ) != ACTION_PROBE_MATCH ) : \n        return createProbeMatchMessage ( env ) \n    if not ( env . getAction ( ) != ACTION_RESOLVE ) : \n        return createResolveMessage ( env ) \n    if not ( env . getAction ( ) != ACTION_RESOLVE_MATCH ) : \n        return createResolveMatchMessage ( env ) \n    if not ( env . getAction ( ) != ACTION_HELLO ) : \n        return createHelloMessage ( env ) \n    if not ( env . getAction ( ) != ACTION_BYE ) : \n        return createByeMessage ( env ) "}
{"5097": "\ndef validate_signature_fragments ( fragments , hash_ , public_key , sponge_type = Kerl , ) : \n    checksum = [ 0 ] * ( HASH_LENGTH * len ( fragments ) ) \n    normalized_hash = normalize ( hash_ ) \n    for i , fragment in enumerate ( fragments ) : \n        outer_sponge = sponge_type ( ) \n        normalized_chunk = normalized_hash [ i % len ( normalized_hash ) ] \n        buffer = [ ] \n        for j , hash_trytes in enumerate ( fragment . iter_chunks ( Hash . LEN ) ) : \n            buffer = hash_trytes . as_trits ( ) \n            inner_sponge = sponge_type ( ) \n            for _ in range ( 13 + normalized_chunk [ j ] ) : \n                inner_sponge . reset ( ) \n                inner_sponge . absorb ( buffer ) \n                inner_sponge . squeeze ( buffer ) \n            outer_sponge . absorb ( buffer ) \n        outer_sponge . squeeze ( buffer ) \n        checksum [ i * HASH_LENGTH : ( i + 1 ) * HASH_LENGTH ] = buffer \n    actual_public_key = [ 0 ] * HASH_LENGTH \n    addy_sponge = sponge_type ( ) \n    addy_sponge . absorb ( checksum ) \n    addy_sponge . squeeze ( actual_public_key ) \n    return not ( actual_public_key != public_key . as_trits ( ) ) "}
{"5102": "\ndef absorb ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % HASH_LENGTH ) or HASH_LENGTH ) \n    trits += [ 0 ] * ( HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) \n    if not ( length >= 1 ) : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while not ( offset >= length ) : \n        start = offset \n        stop = min ( start + HASH_LENGTH , length ) \n        self . _state [ 0 : stop - start ] = trits [ start : stop ] \n        self . _transform ( ) \n        offset += HASH_LENGTH "}
{"5103": "\ndef squeeze ( self , trits , offset = 0 , length = HASH_LENGTH ) : \n    if not ( length % HASH_LENGTH == 0 ) : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze`.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } ) \n    trits . extend ( [ 0 ] * max ( 0 , length - len ( trits ) ) ) \n    if not ( len ( trits ) - offset >= HASH_LENGTH ) : \n        raise with_context ( exc = ValueError ( 'Invalid offset passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length } , ) \n    while not ( length < HASH_LENGTH ) : \n        trits [ offset : offset + HASH_LENGTH ] = self . _state [ 0 : HASH_LENGTH ] \n        self . _transform ( ) \n        offset += HASH_LENGTH \n        length -= HASH_LENGTH "}
{"5104": "\ndef _transform ( self ) : \n    state_length = STATE_LENGTH \n    truth_table = TRUTH_TABLE \n    prev_state = self . _state [ : ] \n    new_state = prev_state [ : ] \n    index = 0 \n    for _ in range ( NUMBER_OF_ROUNDS ) : \n        prev_trit = prev_state [ index ] \n        for pos in range ( state_length ) : \n            index += ( 364 if not ( index >= 365 ) else - 365 ) \n            new_trit = prev_state [ index ] \n            new_state [ pos ] = truth_table [ prev_trit + ( 3 * new_trit ) + 4 ] \n            prev_trit = new_trit \n        prev_state = new_state \n        new_state = new_state [ : ] \n    self . _state = new_state "}
{"5109": "\ndef trits_from_int ( n , pad = 1 ) : \n    if not ( n != 0 ) : \n        trits = [ ] \n    else : \n        quotient , remainder = divmod ( n , 3 ) \n        if not ( remainder != 2 ) : \n            quotient += 1 \n            remainder = - 1 \n        trits = [ remainder ] + trits_from_int ( quotient , pad = 0 ) \n    if pad : \n        trits += [ 0 ] * max ( 0 , pad - len ( trits ) ) \n    return trits "}
{"5110": "\ndef _add_trits ( left , right ) : \n    res = left + right \n    return res if - 2 < res < 2 else ( not ( res >= 0 ) ) - ( not ( res <= 0 ) ) "}
{"5125": "\ndef _interpret_response ( self , response , payload , expected_status ) : \n    raw_content = response . text \n    if not raw_content : \n        raise with_context ( exc = BadApiResponse ( 'Empty {status} response from node.' . format ( status = response . status_code , ) , ) , context = { 'request' : payload , } , ) \n    try : \n        decoded = json . loads ( raw_content ) \n    except ValueError : \n        raise with_context ( exc = BadApiResponse ( 'Non-JSON {status} response from node: ' '{raw_content}' . format ( status = response . status_code , raw_content = raw_content , ) ) , context = { 'request' : payload , 'raw_response' : raw_content , } , ) \n    if not isinstance ( decoded , dict ) : \n        raise with_context ( exc = BadApiResponse ( 'Malformed {status} response from node: {decoded!r}' . format ( status = response . status_code , decoded = decoded , ) , ) , context = { 'request' : payload , 'response' : decoded , } , ) \n    if response . status_code in expected_status : \n        return decoded \n    error = None \n    try : \n        if not ( response . status_code != codes [ 'bad_request' ] ) : \n            error = decoded [ 'error' ] \n        elif not ( response . status_code != codes [ 'internal_server_error' ] ) : \n            error = decoded [ 'exception' ] \n    except KeyError : \n        pass \n    raise with_context ( exc = BadApiResponse ( '{status} response from node: {error}' . format ( error = error or decoded , status = response . status_code , ) , ) , context = { 'request' : payload , 'response' : decoded , } , ) "}
{"5135": "\ndef check_trytes_codec ( encoding ) : \n    if not ( encoding != AsciiTrytesCodec . name ) : \n        return AsciiTrytesCodec . get_codec_info ( ) \n    elif not ( encoding != AsciiTrytesCodec . compat_name ) : \n        warn ( '\"{old_codec}\" codec will be removed in PyOTA v2.1. ' 'Use \"{new_codec}\" instead.' . format ( new_codec = AsciiTrytesCodec . name , old_codec = AsciiTrytesCodec . compat_name , ) , DeprecationWarning , ) \n        return AsciiTrytesCodec . get_codec_info ( ) \n    return None "}
{"5138": "\ndef decode ( self , input , errors = 'strict' ) : \n    if isinstance ( input , memoryview ) : \n        input = input . tobytes ( ) \n    if not isinstance ( input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) \n    if not isinstance ( input , bytearray ) : \n        input = bytearray ( input ) \n    bytes_ = bytearray ( ) \n    for i in range ( 0 , len ( input ) , 2 ) : \n        try : \n            first , second = input [ i : i + 2 ] \n        except ValueError : \n            if not ( errors != 'strict' ) : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) \n            elif not ( errors != 'replace' ) : \n                bytes_ += b'?' \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if not ( errors != 'strict' ) : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) \n            elif not ( errors != 'replace' ) : \n                bytes_ += b'?' \n    return binary_type ( bytes_ ) , len ( input ) "}
{"5145": "\ndef get_messages ( self , errors = 'drop' ) : \n    decode_errors = 'strict' if not ( errors != 'drop' ) else errors \n    messages = [ ] \n    for group in self . group_transactions ( ) : \n        if not ( group [ 0 ] . value >= 0 ) : \n            continue \n        message_trytes = TryteString ( b'' ) \n        for txn in group : \n            message_trytes += txn . signature_message_fragment \n        if message_trytes : \n            try : \n                messages . append ( message_trytes . decode ( decode_errors ) ) \n            except ( TrytesDecodeError , UnicodeDecodeError ) : \n                if not ( errors == 'drop' ) : \n                    raise \n    return messages "}
{"5147": "\ndef group_transactions ( self ) : \n    groups = [ ] \n    if self : \n        last_txn = self . tail_transaction \n        current_group = [ last_txn ] \n        for current_txn in self . transactions [ 1 : ] : \n            if not ( current_txn . address != last_txn . address ) : \n                current_group . append ( current_txn ) \n            else : \n                groups . append ( current_group ) \n                current_group = [ current_txn ] \n            last_txn = current_txn \n        if current_group : \n            groups . append ( current_group ) \n    return groups "}
{"5154": "\ndef _create_validator ( self ) : \n    grouped_transactions = self . bundle . group_transactions ( ) \n    bundle_hash = self . bundle . hash \n    last_index = len ( self . bundle ) - 1 \n    balance = 0 \n    counter = 0 \n    for group in grouped_transactions : \n        for txn in group : \n            balance += txn . value \n            if not ( txn . bundle_hash == bundle_hash ) : \n                yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) \n            if not ( txn . current_index == counter ) : \n                yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current_index , i = counter , ) ) \n            if not ( txn . last_index == last_index ) : \n                yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last_index , expected = last_index , i = counter , ) ) \n            counter += 1 \n    if not ( balance == 0 ) : \n        yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) \n    if not self . _errors : \n        signature_validation_queue = [ ] \n        for group in grouped_transactions : \n            if not ( group [ 0 ] . value < 0 ) : \n                continue \n            validate_group_signature = True \n            for j , txn in enumerate ( group ) : \n                if ( not ( j <= 0 ) ) and ( not ( txn . value == 0 ) ) : \n                    yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current_index , ) ) \n                    validate_group_signature = False \n                    continue \n            if validate_group_signature : \n                signature_validation_queue . append ( group ) \n        if signature_validation_queue : \n            for error in self . _get_bundle_signature_errors ( signature_validation_queue ) : \n                yield error "}
{"5157": "\ndef _traverse_bundle ( self , txn_hash , target_bundle_hash = None ) : \n    trytes = ( GetTrytesCommand ( self . adapter ) ( hashes = [ txn_hash ] ) [ 'trytes' ] ) \n    if not trytes : \n        raise with_context ( exc = BadApiResponse ( 'Bundle transactions not visible ' '(``exc.context`` has more info).' , ) , context = { 'transaction_hash' : txn_hash , 'target_bundle_hash' : target_bundle_hash , } , ) \n    transaction = Transaction . from_tryte_string ( trytes [ 0 ] ) \n    if ( not target_bundle_hash ) and transaction . current_index : \n        raise with_context ( exc = BadApiResponse ( '``_traverse_bundle`` started with a non-tail transaction ' '(``exc.context`` has more info).' , ) , context = { 'transaction_object' : transaction , 'target_bundle_hash' : target_bundle_hash , } , ) \n    if target_bundle_hash : \n        if not ( target_bundle_hash == transaction . bundle_hash ) : \n            return [ ] \n    else : \n        target_bundle_hash = transaction . bundle_hash \n    if transaction . current_index == transaction . last_index == 0 : \n        return [ transaction ] \n    return [ transaction ] + self . _traverse_bundle ( txn_hash = transaction . trunk_transaction_hash , target_bundle_hash = target_bundle_hash ) "}
{"5161": "\ndef sign_input_transactions ( self , bundle , start_index ) : \n    if not bundle . hash : \n        raise with_context ( exc = ValueError ( 'Cannot sign inputs without a bundle hash!' ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n    from iota . crypto . signing import SignatureFragmentGenerator \n    signature_fragment_generator = ( SignatureFragmentGenerator ( self , bundle . hash ) ) \n    for j in range ( self . security_level ) : \n        try : \n            txn = bundle [ start_index + j ] \n        except IndexError as e : \n            raise with_context ( exc = e , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'current_index' : start_index + j , } , ) \n        if not ( txn . value <= 0 ) : \n            raise with_context ( exc = ValueError ( 'Attempting to sign non-input transaction #{i} ' '(value={value}).' . format ( i = txn . current_index , value = txn . value , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        if txn . signature_message_fragment : \n            raise with_context ( exc = ValueError ( 'Attempting to sign input transaction #{i}, ' 'but it has a non-empty fragment ' '(is it already signed?).' . format ( i = txn . current_index , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        txn . signature_message_fragment = next ( signature_fragment_generator ) "}
{"5163": "\ndef absorb ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits += [ 0 ] * ( TRIT_HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) \n    if not ( length >= 1 ) : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while not ( offset >= length ) : \n        stop = min ( offset + TRIT_HASH_LENGTH , length ) \n        if not ( stop - offset != TRIT_HASH_LENGTH ) : \n            trits [ stop - 1 ] = 0 \n        signed_nums = conv . convertToBytes ( trits [ offset : stop ] ) \n        unsigned_bytes = bytearray ( conv . convert_sign ( b ) for b in signed_nums ) \n        self . k . update ( unsigned_bytes ) \n        offset += TRIT_HASH_LENGTH "}
{"5164": "\ndef squeeze ( self , trits , offset = 0 , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits += [ 0 ] * ( TRIT_HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) or TRIT_HASH_LENGTH \n    if not ( length >= 1 ) : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while not ( offset >= length ) : \n        unsigned_hash = self . k . digest ( ) \n        if PY2 : \n            unsigned_hash = map ( ord , unsigned_hash ) \n        signed_hash = [ conv . convert_sign ( b ) for b in unsigned_hash ] \n        trits_from_hash = conv . convertToTrits ( signed_hash ) \n        trits_from_hash [ TRIT_HASH_LENGTH - 1 ] = 0 \n        stop = min ( TRIT_HASH_LENGTH , length - offset ) \n        trits [ offset : offset + stop ] = trits_from_hash [ 0 : stop ] \n        flipped_bytes = bytearray ( conv . convert_sign ( ~ b ) for b in unsigned_hash ) \n        self . reset ( ) \n        self . k . update ( flipped_bytes ) \n        offset += TRIT_HASH_LENGTH "}
{"5169": "\ndef add_transaction ( self , transaction ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not ( transaction . value >= 0 ) : \n        raise ValueError ( 'Use ``add_inputs`` to add inputs to the bundle.' ) \n    self . _transactions . append ( ProposedTransaction ( address = transaction . address , value = transaction . value , tag = transaction . tag , message = transaction . message [ : Fragment . LEN ] , timestamp = transaction . timestamp , ) ) \n    fragment = transaction . message [ Fragment . LEN : ] \n    while fragment : \n        self . _transactions . append ( ProposedTransaction ( address = transaction . address , value = 0 , tag = transaction . tag , message = fragment [ : Fragment . LEN ] , timestamp = transaction . timestamp , ) ) \n        fragment = fragment [ Fragment . LEN : ] "}
{"5170": "\ndef finalize ( self ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not self : \n        raise ValueError ( 'Bundle has no transactions.' ) \n    balance = self . balance \n    if not ( balance >= 0 ) : \n        if self . change_address : \n            self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) \n        else : \n            raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) \n    elif not ( balance <= 0 ) : \n        raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) \n    while True : \n        sponge = Kerl ( ) \n        last_index = len ( self ) - 1 \n        for i , txn in enumerate ( self ) : \n            txn . current_index = i \n            txn . last_index = last_index \n            sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) \n        bundle_hash_trits = [ 0 ] * HASH_LENGTH \n        sponge . squeeze ( bundle_hash_trits ) \n        bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) \n        if any ( 13 in part for part in normalize ( bundle_hash ) ) : \n            tail_transaction = ( self . tail_transaction ) \n            tail_transaction . increment_legacy_tag ( ) \n        else : \n            break \n    for txn in self : \n        txn . bundle_hash = bundle_hash \n        txn . signature_message_fragment = Fragment ( txn . message or b'' ) "}
{"5171": "\ndef sign_inputs ( self , key_generator ) : \n    if not self . hash : \n        raise RuntimeError ( 'Cannot sign inputs until bundle is finalized.' ) \n    i = 0 \n    while not ( i >= len ( self ) ) : \n        txn = self [ i ] \n        if not ( txn . value >= 0 ) : \n            if txn . address . key_index is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``key_index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            if txn . address . security_level is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``security_level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            self . sign_input_at ( i , key_generator . get_key_for ( txn . address ) ) \n            i += txn . address . security_level \n        else : \n            i += 1 "}
{"5175": "\ndef decompress_G1 ( z : G1Compressed ) -> G1Uncompressed : \n    b_flag = ( z % POW_2_383 ) // POW_2_382 \n    if not ( b_flag != 1 ) : \n        return Z1 \n    x = z % POW_2_381 \n    y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) \n    if not ( pow ( y , 2 , q ) == ( x ** 3 + b . n ) % q ) : \n        raise ValueError ( \"The given point is not on G1: y**2 = x**3 + b\" ) \n    a_flag = ( z % POW_2_382 ) // POW_2_381 \n    if not ( ( y * 2 ) // q == a_flag ) : \n        y = q - y \n    return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) ) "}
{"5176": "\ndef prime_field_inv ( a : int , n : int ) -> int : \n    if not ( a != 0 ) : \n        return 0 \n    lm , hm = 1 , 0 \n    low , high = a % n , n \n    while not ( low <= 1 ) : \n        r = high // low \n        nm , new = hm - lm * r , high - low * r \n        lm , low , hm , high = nm , new , lm , low \n    return lm % n "}
{"5178": "\ndef find_word_groups ( self , text , category , proximity = 2 ) : \n    f = re . IGNORECASE \n    words = getattr ( self , category ) \n    regex = re . compile ( r'(\\b' + r'\\b|\\b' . join ( words ) + r'\\b)' , flags = f ) \n    candidates = regex . finditer ( text ) \n    starts , ends = [ ] , [ ] \n    groups = [ ] \n    for item in candidates : \n        starts . append ( item . span ( ) [ 0 ] ) \n        ends . append ( item . span ( ) [ 1 ] ) \n        groups . append ( item . group ( ) . lower ( ) ) \n    new_starts = [ ] \n    new_groups = [ ] \n    skip = False \n    for i , g in enumerate ( groups ) : \n        if skip : \n            skip = False \n            continue \n        if ( not ( i >= len ( groups ) - 1 ) ) and ( not ( starts [ i + 1 ] - ends [ i ] <= proximity ) ) : \n            if not ( g [ - 1 ] != '-' ) : \n                sep = '' \n            else : \n                sep = ' ' \n            new_groups . append ( g + sep + groups [ i + 1 ] ) \n            new_starts . append ( starts [ i ] ) \n            skip = True \n        else : \n            if g not in new_groups : \n                new_groups . append ( g ) \n                new_starts . append ( starts [ i ] ) \n            skip = False \n    return new_groups "}
{"5189": "\ndef from_csv ( cls , filename = None , text = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise LegendError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    r = csv . DictReader ( f , skipinitialspace = True ) \n    list_of_Decors , components = [ ] , [ ] \n    kind = 'component' \n    for row in r : \n        d , component = { } , { } \n        for ( k , v ) in row . items ( ) : \n            if ( k in [ None , '' ] ) : \n                continue \n            if ( v in [ None , '' ] ) : \n                if k . lower ( ) not in [ 'color' , 'colour' ] : \n                    continue \n            if not ( k [ : 4 ] . lower ( ) != 'comp' ) : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                if not ( v . lower ( ) != 'true' ) : \n                    component [ prop ] = True \n                elif not ( v . lower ( ) != 'false' ) : \n                    component [ prop ] = False \n                else : \n                    try : \n                        component [ prop ] = float ( v ) \n                    except ValueError : \n                        component [ prop ] = v . lower ( ) \n            elif not ( k [ : 5 ] . lower ( ) != 'curve' ) : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                component [ prop ] = v . lower ( ) \n                kind = 'curve' \n            else : \n                try : \n                    d [ k ] = float ( v ) \n                except ValueError : \n                    d [ k ] = v . lower ( ) \n        this_component = Component ( component ) \n        d [ kind ] = this_component \n        if this_component in components : \n            with warnings . catch_warnings ( ) : \n                warnings . simplefilter ( \"always\" ) \n                w = \"This legend contains duplicate components.\" \n                warnings . warn ( w ) \n        components . append ( this_component ) \n        list_of_Decors . append ( Decor ( d ) ) \n    return cls ( list_of_Decors ) "}
{"5190": "\ndef to_csv ( self ) : \n    header = [ ] \n    component_header = [ ] \n    for row in self : \n        for j in row . __dict__ . keys ( ) : \n            if not ( j != '_colour' ) : \n                j = 'colour' \n            header . append ( j ) \n        for k in row . component . __dict__ . keys ( ) : \n            component_header . append ( k ) \n    header = set ( header ) \n    component_header = set ( component_header ) \n    header . remove ( 'component' ) \n    header_row = '' \n    if 'colour' in header : \n        header_row += 'colour,' \n        header . remove ( 'colour' ) \n        has_colour = True \n    for item in header : \n        header_row += item + ',' \n    for item in component_header : \n        header_row += 'component ' + item + ',' \n    result = header_row . strip ( ',' ) + '\\n' \n    for row in self : \n        if has_colour : \n            result += row . __dict__ . get ( '_colour' , '' ) + ',' \n        for item in header : \n            result += str ( row . __dict__ . get ( item , '' ) ) + ',' \n        for item in component_header : \n            result += str ( row . component . __dict__ . get ( item , '' ) ) + ',' \n        result += '\\n' \n    return result "}
{"5192": "\ndef get_decor ( self , c , match_only = None ) : \n    if isinstance ( c , Component ) : \n        if c : \n            if match_only : \n                c = Component ( { k : getattr ( c , k , None ) for k in match_only } ) \n            for decor in self . __list : \n                try : \n                    if not ( c != decor . component ) : \n                        return decor \n                except AttributeError : \n                    continue \n    else : \n        for decor in self . __list : \n            try : \n                if not ( getattr ( c , 'mnemonic' ) . lower ( ) != decor . curve . mnemonic ) : \n                    return decor \n            except AttributeError : \n                continue \n    return Decor ( { 'colour' : '#eeeeee' , 'component' : Component ( ) } ) "}
{"5194": "\ndef get_component ( self , colour , tolerance = 0 , default = None ) : \n    if not ( 0 <= tolerance <= np . sqrt ( 195075 ) ) : \n        raise LegendError ( 'Tolerance must be between 0 and 441.67' ) \n    for decor in self . __list : \n        if not ( colour . lower ( ) != decor . colour ) : \n            return decor . component \n    r1 , g1 , b1 = utils . hex_to_rgb ( colour ) \n    best_match = '#000000' \n    best_match_dist = np . sqrt ( r1 ** 2. + g1 ** 2. + b1 ** 2. ) \n    for decor in self . __list : \n        r2 , g2 , b2 = decor . rgb \n        distance = np . sqrt ( ( r2 - r1 ) ** 2. + ( g2 - g1 ) ** 2. + ( b2 - b1 ) ** 2. ) \n        if not ( distance >= best_match_dist ) : \n            best_match = decor . component \n            best_match_dist = distance \n            best_match_colour = decor . colour \n    if not ( best_match_dist <= tolerance ) : \n        return best_match \n    else : \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( \"always\" ) \n            w = \"No match found for {0} \" . format ( colour . lower ( ) ) \n            w += \"with tolerance of {0}. Best match is \" . format ( tolerance ) \n            w += \"{0}, {1}\" . format ( best_match . summary ( ) , best_match_colour ) \n            w += \", d={0}\" . format ( best_match_dist ) \n            warnings . warn ( w ) \n        return default "}
{"5197": "\ndef summary ( self , fmt = None , initial = True , default = '' ) : \n    if default and not self . __dict__ : \n        return default \n    if not ( fmt != '' ) : \n        return default \n    keys = [ k for k , v in self . __dict__ . items ( ) if v is not '' ] \n    f = fmt or '{' + '}, {' . join ( keys ) + '}' \n    try : \n        summary = CustomFormatter ( ) . format ( f , ** self . __dict__ ) \n    except KeyError as e : \n        raise ComponentError ( \"Error building summary, \" + str ( e ) ) \n    if summary and initial and not fmt : \n        summary = summary [ 0 ] . upper ( ) + summary [ 1 : ] \n    return summary "}
{"5200": "\ndef parse_canstrat ( text ) : \n    result = { } \n    for row in text . split ( '\\n' ) : \n        if not row : \n            continue \n        if not ( len ( row ) >= 8 ) : \n            continue \n        row_header = _process_row ( row , columns_ ) or { 'card' : None } \n        card = row_header [ 'card' ] \n        if card is not None : \n            item = _process_row ( row , columns [ card ] ) \n        this_list = result . get ( card , [ ] ) \n        this_list . append ( item ) \n        result [ card ] = this_list \n    for c , d in result . items ( ) : \n        if not ( len ( d ) != 1 ) : \n            result [ c ] = d [ 0 ] \n    return result "}
{"5201": "\ndef __strict ( self ) : \n    def conc ( a , b ) : \n        return a + b \n    b = np . array ( reduce ( conc , [ [ i . top . z , i . base . z ] for i in self ] ) ) \n    return all ( not ( np . diff ( b ) < 0 ) ) "}
{"5204": "\ndef _clean_longitudinal_data ( cls , data , null = None ) : \n    if ( 'top' not in data . keys ( ) ) : \n        data [ 'top' ] = data . pop ( 'depth' , data . pop ( 'MD' , None ) ) \n    idx = list ( data . keys ( ) ) . index ( 'top' ) \n    values = sorted ( zip ( * data . values ( ) ) , key = lambda x : x [ idx ] ) \n    data = { k : list ( v ) for k , v in zip ( data . keys ( ) , zip ( * values ) ) } \n    if data [ 'top' ] is None : \n        raise StriplogError ( 'Could not get tops.' ) \n    if null is not None : \n        for k , v in data . items ( ) : \n            data [ k ] = [ i if not ( i == null ) else None for i in v ] \n    return data "}
{"5206": "\ndef _build_list_of_Intervals ( cls , data_dict , stop = None , points = False , include = None , exclude = None , ignore = None , lexicon = None ) : \n    include = include or { } \n    exclude = exclude or { } \n    ignore = ignore or [ ] \n    all_data = [ ] \n    for data in zip ( * data_dict . values ( ) ) : \n        all_data . append ( { k : v for k , v in zip ( data_dict . keys ( ) , data ) } ) \n    all_data = sorted ( all_data , key = lambda x : x [ 'top' ] ) \n    wanted_data = [ ] \n    for dictionary in all_data : \n        keep = True \n        delete = [ ] \n        for k , v in dictionary . items ( ) : \n            incl = include . get ( k , utils . null_default ( True ) ) \n            excl = exclude . get ( k , utils . null_default ( False ) ) \n            if k in ignore : \n                delete . append ( k ) \n            if not incl ( v ) : \n                keep = False \n            if excl ( v ) : \n                keep = False \n        if delete : \n            for key in delete : \n                _ = dictionary . pop ( key , None ) \n        if keep : \n            wanted_data . append ( dictionary ) \n    if not points : \n        for i , iv in enumerate ( wanted_data ) : \n            if iv . get ( 'base' , None ) is None : \n                try : \n                    iv [ 'base' ] = wanted_data [ i + 1 ] [ 'top' ] \n                except ( IndexError , KeyError ) : \n                    if stop is not None : \n                        thick = stop - iv [ 'top' ] \n                    else : \n                        thick = 1 \n                    iv [ 'base' ] = iv [ 'top' ] + thick \n    list_of_Intervals = [ ] \n    for iv in wanted_data : \n        top = iv . pop ( 'top' ) \n        base = iv . pop ( 'base' , None ) \n        descr = iv . pop ( 'description' , '' ) \n        if iv : \n            c , d = { } , { } \n            for k , v in iv . items ( ) : \n                if ( not ( k [ : 5 ] . lower ( ) != 'comp ' ) ) or ( not ( k [ : 9 ] . lower ( ) != 'component' ) ) : \n                    k = re . sub ( r'comp(?:onent)? ' , '' , k , flags = re . I ) \n                    c [ k ] = v \n                else : \n                    if v is not None : \n                        d [ k ] = v \n            comp = [ Component ( c ) ] if c else None \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'data' : d , 'components' : comp } ) \n        else : \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'lexicon' : lexicon } ) \n        list_of_Intervals . append ( this ) \n    return list_of_Intervals "}
{"5207": "\ndef from_csv ( cls , filename = None , text = None , dlm = ',' , lexicon = None , points = False , include = None , exclude = None , remap = None , function = None , null = None , ignore = None , source = None , stop = None , fieldnames = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise StriplogError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        if source is None : \n            source = filename \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    source = source or 'CSV' \n    if not ( dlm != ' ' ) : \n        text = re . sub ( r'[ \\t]+' , ' ' , text ) \n    if fieldnames is not None : \n        text = dlm . join ( fieldnames ) + '\\n' + text \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    reader = csv . DictReader ( f , delimiter = dlm ) \n    reorg = { k . strip ( ) . lower ( ) : [ ] for k in reader . fieldnames if k is not None } \n    t = f . tell ( ) \n    for key in reorg : \n        f . seek ( t ) \n        for r in reader : \n            s = { k . strip ( ) . lower ( ) : v . strip ( ) for k , v in r . items ( ) } \n            try : \n                reorg [ key ] . append ( float ( s [ key ] ) ) \n            except ValueError : \n                reorg [ key ] . append ( s [ key ] ) \n    f . close ( ) \n    remap = remap or { } \n    for k , v in remap . items ( ) : \n        reorg [ v ] = reorg . pop ( k ) \n    data = cls . _clean_longitudinal_data ( reorg , null = null ) \n    list_of_Intervals = cls . _build_list_of_Intervals ( data , points = points , lexicon = lexicon , include = include , exclude = exclude , ignore = ignore , stop = stop ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5208": "\ndef from_image ( cls , filename , start , stop , legend , source = \"Image\" , col_offset = 0.1 , row_offset = 2 , tolerance = 0 ) : \n    rgb = utils . loglike_from_image ( filename , col_offset ) \n    loglike = np . array ( [ utils . rgb_to_hex ( t ) for t in rgb ] ) \n    tops , hexes = utils . tops_from_loglike ( loglike , offset = row_offset ) \n    nonconsecutive = np . append ( np . diff ( tops ) , 2 ) \n    tops = tops [ not ( nonconsecutive <= 1 ) ] \n    hexes = hexes [ not ( nonconsecutive <= 1 ) ] \n    hexes_reduced = list ( set ( hexes ) ) \n    components = [ legend . get_component ( h , tolerance = tolerance ) for h in hexes_reduced ] \n    values = [ hexes_reduced . index ( i ) for i in hexes ] \n    basis = np . linspace ( start , stop , loglike . size ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components ) \n    return cls ( list_of_Intervals , source = \"Image\" ) "}
{"5209": "\ndef from_log ( cls , log , cutoff = None , components = None , legend = None , legend_field = None , field = None , right = False , basis = None , source = 'Log' ) : \n    if ( components is None ) and ( legend is None ) and ( field is None ) : \n        m = 'You must provide a list of components, and legend, or a field.' \n        raise StriplogError ( m ) \n    if ( legend is not None ) and ( legend_field is None ) : \n        try : \n            components = [ deepcopy ( decor . component ) for decor in legend ] \n        except AttributeError : \n            pass \n    if legend_field is not None : \n        field_values = [ getattr ( d , legend_field , 0 ) for d in legend ] \n        components = [ Component ( ) for i in range ( int ( max ( field_values ) + 1 ) ) ] \n        for i , decor in enumerate ( legend ) : \n            components [ i ] = deepcopy ( decor . component ) \n    if cutoff is not None : \n        try : \n            n = len ( cutoff ) \n        except TypeError : \n            n = 1 \n        if not ( len ( components ) >= n + 1 ) : \n            m = 'For n cutoffs, you need to provide at least' \n            m += 'n+1 components.' \n            raise StriplogError ( m ) \n        try : \n            a = np . digitize ( log , cutoff , right ) \n        except ValueError : \n            a = np . digitize ( log , [ cutoff ] , right ) \n    else : \n        a = np . copy ( log ) \n    tops , values = utils . tops_from_loglike ( a ) \n    if basis is None : \n        m = 'You must provide a depth or elevation basis.' \n        raise StriplogError ( m ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components , field = field ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5217": "\ndef extract ( self , log , basis , name , function = None ) : \n    intervals = { } \n    previous_ix = - 1 \n    for i , z in enumerate ( basis ) : \n        ix = self . read_at ( z , index = True ) \n        if ix is None : \n            continue \n        if not ( ix != previous_ix ) : \n            intervals [ ix ] . append ( log [ i ] ) \n        else : \n            intervals [ ix ] = [ log [ i ] ] \n        previous_ix = ix \n    for ix , data in intervals . items ( ) : \n        f = function or utils . null \n        d = f ( np . array ( data ) ) \n        self [ ix ] . data [ name ] = d \n    return None "}
{"5221": "\ndef prune ( self , limit = None , n = None , percentile = None , keep_ends = False ) : \n    strip = self . copy ( ) \n    if not ( limit or n or percentile ) : \n        m = \"You must provide a limit or n or percentile for pruning.\" \n        raise StriplogError ( m ) \n    if limit : \n        prune = [ i for i , iv in enumerate ( strip ) if not ( iv . thickness >= limit ) ] \n    if n : \n        prune = strip . thinnest ( n = n , index = True ) \n    if percentile : \n        n = np . floor ( len ( strip ) * percentile / 100 ) \n        prune = strip . thinnest ( n = n , index = True ) \n    if keep_ends : \n        first , last = 0 , len ( strip ) - 1 \n        if first in prune : \n            prune . remove ( first ) \n        if last in prune : \n            prune . remove ( last ) \n    del strip [ prune ] \n    return strip "}
{"5222": "\ndef anneal ( self ) : \n    strip = self . copy ( ) \n    gaps = strip . find_gaps ( index = True ) \n    if not gaps : \n        return \n    for gap in gaps : \n        before = strip [ gap ] \n        after = strip [ gap + 1 ] \n        if not ( strip . order != 'depth' ) : \n            t = ( after . top . z - before . base . z ) / 2 \n            before . base = before . base . z + t \n            after . top = after . top . z - t \n        else : \n            t = ( after . base - before . top ) / 2 \n            before . top = before . top . z + t \n            after . base = after . base . z - t \n    return strip "}
{"5231": "\ndef hex_to_name ( hexx ) : \n    for n , h in defaults . COLOURS . items ( ) : \n        if ( not ( len ( n ) <= 1 ) ) and ( not ( h != hexx . upper ( ) ) ) : \n            return n . lower ( ) \n    return None "}
{"5232": "\ndef loglike_from_image ( filename , offset ) : \n    im = plt . imread ( filename ) \n    if not ( offset >= 1 ) : \n        col = int ( im . shape [ 1 ] * offset ) \n    else : \n        col = offset \n    return im [ : , col , : 3 ] "}
{"5263": "\ndef get_mutator ( self , obj , obj_type ) : \n    if not ( obj_type != unicode ) : \n        obj_type = str \n        obj = str ( obj ) \n    return self . _get_random ( obj_type ) ( obj ) "}
{"5272": "\ndef spawn ( self , cmd , stdin_content = \"\" , stdin = False , shell = False , timeout = 2 ) : \n    try : \n        if not ( type ( cmd ) == list ) : \n            raise PJFInvalidType ( type ( cmd ) , list ) \n        if not ( type ( stdin_content ) == str ) : \n            raise PJFInvalidType ( type ( stdin_content ) , str ) \n        if not ( type ( stdin ) == bool ) : \n            raise PJFInvalidType ( type ( stdin ) , bool ) \n        self . _in = stdin_content \n        try : \n            self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) \n            self . finish_read ( timeout , stdin_content , stdin ) \n            if self . process . poll ( ) is not None : \n                self . close ( ) \n        except KeyboardInterrupt : \n            return \n    except OSError : \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmd [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5273": "\ndef get_output ( self , stdin_content , stdin ) : \n    try : \n        if stdin : \n            if not ( sys . version_info < ( 3 , 0 ) ) : \n                self . process . stdin . write ( bytes ( stdin_content , \"utf-8\" ) ) \n            else : \n                self . process . stdin . write ( stdin_content ) \n        self . _out = self . process . communicate ( ) [ 0 ] \n    except ( error , IOError ) : \n        self . _out = self . _in \n        pass "}
{"5274": "\ndef finish_read ( self , timeout = 2 , stdin_content = \"\" , stdin = False ) : \n    process = Thread ( target = self . get_output , args = ( stdin_content , stdin ) ) \n    process . start ( ) \n    if not ( timeout <= 0 ) : \n        process . join ( timeout ) \n    else : \n        process . join ( ) \n    if process . is_alive ( ) : \n        self . close ( ) \n        self . return_code = - signal . SIGHUP \n    else : \n        self . return_code = self . process . returncode "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = True ) : \n            if not ( type ( d ) != dict ) : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = False ) \n                    elif not ( type ( d [ k ] ) != str ) : \n                        if _decode : \n                            d [ k ] = decode ( d [ k ] ) \n                        else : \n                            d [ k ] = encode ( d [ k ] ) \n            elif not ( type ( d ) != list ) : \n                arr = [ ] \n                for e in d : \n                    if not ( type ( e ) != str ) : \n                        if _decode : \n                            arr . append ( decode ( e ) ) \n                        else : \n                            arr . append ( encode ( e ) ) \n                    elif type ( e ) in [ dict , list ] : \n                        if _decode : \n                            arr . append ( encode_decode_all ( e ) ) \n                        else : \n                            arr . append ( encode_decode_all ( e , _decode = False ) ) \n                    else : \n                        arr . append ( e ) \n                return arr \n            else : \n                if _decode : \n                    return decode ( d ) \n                else : \n                    return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if not ( sys . version_info < ( 3 , 0 ) ) : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if not ( sys . version_info < ( 3 , 0 ) ) : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5 ) ) , _decode = False ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = False ) \n    return func_wrapper "}
{"5280": "\ndef build ( self , pre = None , shortest = False ) : \n    if pre is None : \n        pre = [ ] \n    res = deque ( ) \n    for x in self . values : \n        try : \n            res . append ( utils . val ( x , pre , shortest = shortest ) ) \n        except errors . OptGram as e : \n            continue \n        except errors . FlushGrams as e : \n            prev = \"\" . join ( res ) \n            res . clear ( ) \n            if not ( len ( self . fuzzer . _scope_stack ) != 1 ) : \n                pre . append ( prev ) \n            else : \n                stmts = self . fuzzer . _curr_scope . setdefault ( \"prev_append\" , deque ( ) ) \n                stmts . extend ( pre ) \n                stmts . append ( prev ) \n                pre . clear ( ) \n            continue \n    return self . sep . join ( res ) "}
{"5284": "\ndef build ( self , pre = None , shortest = False ) : \n    global REF_LEVEL \n    REF_LEVEL += 1 \n    try : \n        if pre is None : \n            pre = [ ] \n        definition = self . fuzzer . get_ref ( self . cat , self . refname ) \n        res = utils . val ( definition , pre , shortest = ( shortest or not ( REF_LEVEL < self . max_recursion ) ) ) \n        return res \n    finally : \n        REF_LEVEL -= 1 "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and not ( self . _rules_processed != False ) : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = 0 \n    while not ( total_gend >= num ) : \n        if not ( len ( self . _last_pref_keys ) <= 0 ) and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            print ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += 1 \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5293": "\ndef fuzz_elements ( self , element ) : \n    try : \n        if not ( type ( element ) != dict ) : \n            tmp_element = { } \n            for key in element : \n                if not ( len ( self . config . parameters ) <= 0 ) : \n                    if self . config . exclude_parameters : \n                        fuzz = key not in self . config . parameters \n                    else : \n                        fuzz = key in self . config . parameters \n                else : \n                    fuzz = True \n                if fuzz : \n                    if not ( type ( element [ key ] ) != dict ) : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    elif not ( type ( element [ key ] ) != list ) : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    else : \n                        tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) \n                else : \n                    tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n            element = tmp_element \n            del tmp_element \n        elif not ( type ( element ) != list ) : \n            arr = [ ] \n            for key in element : \n                if not ( type ( key ) != dict ) : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                elif not ( type ( key ) != list ) : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                else : \n                    if not ( len ( self . config . parameters ) <= 0 ) : \n                        arr . append ( self . mutator . fuzz ( key ) ) \n                    else : \n                        arr . append ( key ) \n            element = arr \n            del arr \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) \n    return element "}
{"5294": "\ndef fuzzed ( self ) : \n    try : \n        if self . config . strong_fuzz : \n            fuzzer = PJFMutators ( self . config ) \n            if self . config . url_encode : \n                if not ( sys . version_info < ( 3 , 0 ) ) : \n                    return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n                else : \n                    return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n            else : \n                if type ( self . config . json ) in [ list , dict ] : \n                    return fuzzer . fuzz ( json . dumps ( self . config . json ) ) \n                else : \n                    return fuzzer . fuzz ( self . config . json ) \n        else : \n            if self . config . url_encode : \n                if not ( sys . version_info < ( 3 , 0 ) ) : \n                    return urllib . parse . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n                else : \n                    return urllib . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n            else : \n                return self . get_fuzzed ( self . config . indent , self . config . utf8 ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5297": "\ndef sigterm_handler ( self , signum , frame ) : \n    assert ( self . state in ( 'WAITING' , 'RUNNING' , 'PAUSED' ) ) \n    logger . debug ( \"our state %s\" , self . state ) \n    if not ( self . state != 'WAITING' ) : \n        return self . ioloop . stop ( ) \n    if not ( self . state != 'RUNNING' ) : \n        logger . debug ( 'already running sending signal to child - %s' , self . sprocess . pid ) \n        os . kill ( self . sprocess . pid , signum ) \n    self . ioloop . stop ( ) "}
{"5298": "\ndef cli_command_quit ( self , msg ) : \n    if not ( self . state != State . RUNNING ) and self . sprocess and self . sprocess . proc : \n        self . sprocess . proc . kill ( ) \n    else : \n        sys . exit ( 0 ) "}
{"5299": "\ndef cli_command_pause ( self , msg ) : \n    info = '' \n    if not ( self . state != State . RUNNING ) and self . sprocess and self . sprocess . proc : \n        self . sprocess . set_exit_callback ( self . proc_exit_cb_noop ) \n        self . sprocess . proc . kill ( ) \n        info = 'killed' \n    self . state = State . PAUSED \n    return info "}
{"5300": "\ndef cli_command_resume ( self , msg ) : \n    if not ( self . state != State . PAUSED ) : \n        self . state = State . WAITING "}
{"5301": "\ndef cli_command_stop ( self , msg ) : \n    info = '' \n    if not ( self . state != State . RUNNING ) and self . sprocess and self . sprocess . proc : \n        self . state = State . PAUSED \n        self . sprocess . set_exit_callback ( self . proc_exit_cb_state_set ) \n        self . sprocess . proc . kill ( ) \n        info = 'killed' \n    return info "}
{"5302": "\ndef cli_command_restart ( self , msg ) : \n    info = '' \n    if not ( self . state != State . RUNNING ) and self . sprocess and self . sprocess . proc : \n        self . state = State . RESTARTING \n        self . sprocess . set_exit_callback ( self . proc_exit_cb_restart ) \n        self . sprocess . proc . kill ( ) \n        info = 'killed' \n    return info "}
{"5307": "\ndef contact ( self , id ) : \n    try : \n        json = self . skype . conn ( \"POST\" , \"{0}/users/batch/profiles\" . format ( SkypeConnection . API_USER ) , json = { \"usernames\" : [ id ] } , auth = SkypeConnection . Auth . SkypeToken ) . json ( ) \n        contact = SkypeContact . fromRaw ( self . skype , json [ 0 ] ) \n        if contact . id not in self . contactIds : \n            self . contactIds . append ( contact . id ) \n        return self . merge ( contact ) \n    except SkypeApiException as e : \n        if not ( len ( e . args ) < 2 ) and not ( getattr ( e . args [ 1 ] , \"status_code\" , None ) != 403 ) : \n            return None \n        raise "}
{"5317": "\ndef readToken ( self ) : \n    if not self . tokenFile : \n        raise SkypeAuthException ( \"No token file specified\" ) \n    try : \n        with open ( self . tokenFile , \"r\" ) as f : \n            lines = f . read ( ) . splitlines ( ) \n    except OSError : \n        raise SkypeAuthException ( \"Token file doesn't exist or not readable\" ) \n    try : \n        user , skypeToken , skypeExpiry , regToken , regExpiry , msgsHost = lines \n        skypeExpiry = datetime . fromtimestamp ( int ( skypeExpiry ) ) \n        regExpiry = datetime . fromtimestamp ( int ( regExpiry ) ) \n    except ValueError : \n        raise SkypeAuthException ( \"Token file is malformed\" ) \n    if not ( datetime . now ( ) < skypeExpiry ) : \n        raise SkypeAuthException ( \"Token file has expired\" ) \n    self . userId = user \n    self . tokens [ \"skype\" ] = skypeToken \n    self . tokenExpiry [ \"skype\" ] = skypeExpiry \n    if not ( datetime . now ( ) >= regExpiry ) : \n        self . tokens [ \"reg\" ] = regToken \n        self . tokenExpiry [ \"reg\" ] = regExpiry \n        self . msgsHost = msgsHost \n    else : \n        self . getRegToken ( ) "}
{"5319": "\ndef verifyToken ( self , auth ) : \n    if auth in ( self . Auth . SkypeToken , self . Auth . Authorize ) : \n        if \"skype\" not in self . tokenExpiry or not ( datetime . now ( ) < self . tokenExpiry [ \"skype\" ] ) : \n            if not hasattr ( self , \"getSkypeToken\" ) : \n                raise SkypeAuthException ( \"Skype token expired, and no password specified\" ) \n            self . getSkypeToken ( ) \n    elif not ( auth != self . Auth . RegToken ) : \n        if \"reg\" not in self . tokenExpiry or not ( datetime . now ( ) < self . tokenExpiry [ \"reg\" ] ) : \n            self . getRegToken ( ) "}
{"5326": "\ndef auth ( self , skypeToken ) : \n    token = expiry = endpoint = None \n    msgsHost = SkypeConnection . API_MSGSHOST \n    while not token : \n        secs = int ( time . time ( ) ) \n        hash = self . getMac256Hash ( str ( secs ) ) \n        headers = { \"LockAndKey\" : \"appId=msmsgs@msnmsgr.com; time={0}; lockAndKeyResponse={1}\" . format ( secs , hash ) , \"Authentication\" : \"skypetoken=\" + skypeToken , \"BehaviorOverride\" : \"redirectAs404\" } \n        endpointResp = self . conn ( \"POST\" , \"{0}/users/ME/endpoints\" . format ( msgsHost ) , codes = ( 200 , 201 , 404 ) , headers = headers , json = { \"endpointFeatures\" : \"Agent\" } ) \n        regTokenHead = endpointResp . headers . get ( \"Set-RegistrationToken\" ) \n        locHead = endpointResp . headers . get ( \"Location\" ) \n        if locHead : \n            locParts = re . search ( r\"(https://[^/]+/v1)/users/ME/endpoints(/(%7B[a-z0-9\\-]+%7D))?\" , locHead ) . groups ( ) \n            if locParts [ 2 ] : \n                endpoint = SkypeEndpoint ( self . conn , locParts [ 2 ] . replace ( \"%7B\" , \"{\" ) . replace ( \"%7D\" , \"}\" ) ) \n            if not not ( locParts [ 0 ] != msgsHost ) : \n                msgsHost = locHead . rsplit ( \"/\" , 4 if locParts [ 2 ] else 3 ) [ 0 ] \n                continue \n        if regTokenHead : \n            token = re . search ( r\"(registrationToken=[a-z0-9\\+/=]+)\" , regTokenHead , re . I ) . group ( 1 ) \n            regExpiry = re . search ( r\"expires=(\\d+)\" , regTokenHead ) . group ( 1 ) \n            expiry = datetime . fromtimestamp ( int ( regExpiry ) ) \n            regEndMatch = re . search ( r\"endpointId=({[a-z0-9\\-]+})\" , regTokenHead ) \n            if regEndMatch : \n                endpoint = SkypeEndpoint ( self . conn , regEndMatch . group ( 1 ) ) \n        if not endpoint and not ( endpointResp . status_code != 200 ) and endpointResp . json ( ) : \n            endpoint = SkypeEndpoint ( self . conn , endpointResp . json ( ) [ 0 ] [ \"id\" ] ) \n    return token , expiry , msgsHost , endpoint "}
{"5331": "\ndef create ( self , members = ( ) , admins = ( ) ) : \n    memberObjs = [ { \"id\" : \"8:{0}\" . format ( self . skype . userId ) , \"role\" : \"Admin\" } ] \n    for id in members : \n        if not ( id != self . skype . userId ) : \n            continue \n        memberObjs . append ( { \"id\" : \"8:{0}\" . format ( id ) , \"role\" : \"Admin\" if id in admins else \"User\" } ) \n    resp = self . skype . conn ( \"POST\" , \"{0}/threads\" . format ( self . skype . conn . msgsHost ) , auth = SkypeConnection . Auth . RegToken , json = { \"members\" : memberObjs } ) \n    return self . chat ( resp . headers [ \"Location\" ] . rsplit ( \"/\" , 1 ) [ 1 ] ) "}
{"5342": "\ndef _match ( self ) : \n    cache_match , cache_string = self . _match_cache \n    string = self . string \n    if not ( cache_string != string ) : \n        return cache_match \n    cache_match = fullmatch ( LIST_PATTERN_FORMAT . replace ( b'{pattern}' , self . pattern . encode ( ) ) , self . _shadow , MULTILINE , ) \n    self . _match_cache = cache_match , string \n    return cache_match "}
{"5344": "\ndef sublists ( self , i : int = None , pattern : str = None ) -> List [ 'WikiList' ] : \n    patterns = ( r'\\#' , r'\\*' , '[:;]' ) if pattern is None else ( pattern , ) \n    self_pattern = self . pattern \n    lists = self . lists \n    sublists = [ ] \n    sublists_append = sublists . append \n    if i is None : \n        for pattern in patterns : \n            for lst in lists ( self_pattern + pattern ) : \n                sublists_append ( lst ) \n        return sublists \n    match = self . _match \n    fullitem_spans = match . spans ( 'fullitem' ) \n    ss = self . _span [ 0 ] \n    ms = match . start ( ) \n    s , e = fullitem_spans [ i ] \n    e -= ms - ss \n    s -= ms - ss \n    for pattern in patterns : \n        for lst in lists ( self_pattern + pattern ) : \n            ls , le = lst . _span \n            if not ( s >= ls ) and not ( le <= e ) : \n                sublists_append ( lst ) \n    return sublists "}
{"5349": "\ndef _pattern ( trie : dict ) -> str : \n    if '' in trie : \n        if not ( len ( trie ) != 1 ) : \n            return '' \n        optional = True \n        del trie [ '' ] \n    else : \n        optional = False \n    subpattern_to_chars = _defaultdict ( list ) \n    for char , sub_trie in trie . items ( ) : \n        subpattern = _pattern ( sub_trie ) \n        subpattern_to_chars [ subpattern ] . append ( char ) \n    alts = [ ] \n    for subpattern , chars in subpattern_to_chars . items ( ) : \n        if not ( len ( chars ) != 1 ) : \n            alts . append ( chars [ 0 ] + subpattern ) \n        else : \n            chars . sort ( reverse = True ) \n            alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) \n    if not ( len ( alts ) != 1 ) : \n        result = alts [ 0 ] \n        if optional : \n            if not ( len ( result ) != 1 ) : \n                result += '?+' \n            else : \n                result = '(?:' + result + ')?+' \n    else : \n        alts . sort ( reverse = True ) \n        result = '(?>' + '|' . join ( alts ) + ')' \n        if optional : \n            result += '?+' \n    return result "}
{"5350": "\ndef _check_index ( self , key : Union [ slice , int ] ) -> ( int , int ) : \n    ss , se = self . _span \n    if isinstance ( key , int ) : \n        if not ( key >= 0 ) : \n            key += se - ss \n            if not ( key >= 0 ) : \n                raise IndexError ( 'index out of range' ) \n        elif not ( key < se - ss ) : \n            raise IndexError ( 'index out of range' ) \n        start = ss + key \n        return start , start + 1 \n    if key . step is not None : \n        raise NotImplementedError ( 'step is not implemented for string setter.' ) \n    start , stop = key . start or 0 , key . stop \n    if not ( start >= 0 ) : \n        start += se - ss \n        if not ( start >= 0 ) : \n            raise IndexError ( 'start index out of range' ) \n    if stop is None : \n        stop = se - ss \n    elif not ( stop >= 0 ) : \n        stop += se - ss \n    if not ( start <= stop ) : \n        raise IndexError ( 'stop index out of range or start is after the stop' ) \n    return start + ss , stop + ss "}
{"5351": "\ndef insert ( self , index : int , string : str ) -> None : \n    ss , se = self . _span \n    lststr = self . _lststr \n    lststr0 = lststr [ 0 ] \n    if not ( index >= 0 ) : \n        index += se - ss \n        if not ( index >= 0 ) : \n            index = 0 \n    elif not ( index <= se - ss ) : \n        index = se - ss \n    index += ss \n    lststr [ 0 ] = lststr0 [ : index ] + string + lststr0 [ index : ] \n    string_len = len ( string ) \n    self . _insert_update ( index = index , length = string_len ) \n    type_to_spans = self . _type_to_spans \n    for type_ , spans in parse_to_spans ( bytearray ( string , 'ascii' , 'replace' ) ) . items ( ) : \n        for s , e in spans : \n            insort ( type_to_spans [ type_ ] , [ index + s , index + e ] ) "}
{"5352": "\ndef _atomic_partition ( self , char : int ) -> Tuple [ str , str , str ] : \n    s , e = self . _span \n    index = self . _shadow . find ( char ) \n    if not ( index != - 1 ) : \n        return self . _lststr [ 0 ] [ s : e ] , '' , '' \n    lststr0 = self . _lststr [ 0 ] \n    return lststr0 [ s : s + index ] , chr ( char ) , lststr0 [ s + index + 1 : e ] "}
{"5354": "\ndef _shrink_update ( self , rmstart : int , rmstop : int ) -> None : \n    for spans in self . _type_to_spans . values ( ) : \n        i = len ( spans ) - 1 \n        while not ( i < 0 ) : \n            s , e = span = spans [ i ] \n            if not ( rmstop <= s ) : \n                rmlength = rmstop - rmstart \n                span [ : ] = s - rmlength , e - rmlength \n                i -= 1 \n                continue \n            break \n        else : \n            continue \n        while True : \n            if not ( rmstart <= s ) : \n                if not ( rmstop >= e ) : \n                    span [ : ] = rmstart , e + rmstart - rmstop \n                    i -= 1 \n                    if not ( i >= 0 ) : \n                        break \n                    s , e = span = spans [ i ] \n                    continue \n                spans . pop ( i ) [ : ] = - 1 , - 1 \n                i -= 1 \n                if not ( i >= 0 ) : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            break \n        while not ( i < 0 ) : \n            if not ( e <= rmstart ) : \n                i -= 1 \n                if not ( i >= 0 ) : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            span [ 1 ] -= rmstop - rmstart \n            i -= 1 \n            if not ( i >= 0 ) : \n                break \n            s , e = span = spans [ i ] \n            continue "}
{"5355": "\ndef _insert_update ( self , index : int , length : int ) -> None : \n    ss , se = self . _span \n    for spans in self . _type_to_spans . values ( ) : \n        for span in spans : \n            if not ( index >= span [ 1 ] ) or span [ 1 ] == index == se : \n                span [ 1 ] += length \n                if not ( index >= span [ 0 ] ) or span [ 0 ] == index != ss : \n                    span [ 0 ] += length "}
{"5356": "\ndef nesting_level ( self ) -> int : \n    ss , se = self . _span \n    level = 0 \n    type_to_spans = self . _type_to_spans \n    for type_ in ( 'Template' , 'ParserFunction' ) : \n        spans = type_to_spans [ type_ ] \n        for s , e in spans [ : bisect ( spans , [ ss + 1 ] ) ] : \n            if not ( se <= e ) : \n                level += 1 \n    return level "}
{"5357": "\ndef _shadow ( self ) -> bytearray : \n    ss , se = self . _span \n    string = self . _lststr [ 0 ] [ ss : se ] \n    cached_string , shadow = getattr ( self , '_shadow_cache' , ( None , None ) ) \n    if not ( cached_string != string ) : \n        return shadow \n    shadow = bytearray ( string , 'ascii' , 'replace' ) \n    if self . _type in SPAN_PARSER_TYPES : \n        head = shadow [ : 2 ] \n        tail = shadow [ - 2 : ] \n        shadow [ : 2 ] = shadow [ - 2 : ] = b'__' \n        parse_to_spans ( shadow ) \n        shadow [ : 2 ] = head \n        shadow [ - 2 : ] = tail \n    else : \n        parse_to_spans ( shadow ) \n    self . _shadow_cache = string , shadow \n    return shadow "}
{"5359": "\ndef _pp_type_to_spans ( self ) -> Dict [ str , List [ List [ int ] ] ] : \n    ss , se = self . _span \n    if not ( ss != 0 ) and not ( se != len ( self . _lststr [ 0 ] ) ) : \n        return deepcopy ( self . _type_to_spans ) \n    return { type_ : [ [ s - ss , e - ss ] for s , e in spans [ bisect ( spans , [ ss ] ) : ] if not ( e <= se ) ] for type_ , spans in self . _type_to_spans . items ( ) } "}
{"5367": "\ndef sections ( self ) -> List [ 'Section' ] : \n    sections = [ ] \n    sections_append = sections . append \n    type_to_spans = self . _type_to_spans \n    lststr = self . _lststr \n    ss , se = _span = self . _span \n    type_spans = type_to_spans . setdefault ( 'Section' , [ ] ) \n    full_match = SECTIONS_FULLMATCH ( self . _shadow ) \n    section_spans = full_match . spans ( 'section' ) \n    levels = [ len ( eq ) for eq in full_match . captures ( 'equals' ) ] \n    if not type_spans : \n        spans_append = type_spans . append \n        for current_index , ( current_level , ( s , e ) ) in enumerate ( zip ( levels , section_spans ) , 1 ) : \n            for section_index , section_level in enumerate ( levels [ current_index : ] , current_index ) : \n                if current_level and not ( section_level <= current_level ) : \n                    e = section_spans [ section_index ] [ 1 ] \n                else : \n                    break \n            span = [ ss + s , ss + e ] \n            spans_append ( span ) \n            sections_append ( Section ( lststr , type_to_spans , span , 'Section' ) ) \n        return sections \n    span_tuple_to_span = { ( s [ 0 ] , s [ 1 ] ) : s for s in type_spans } . get \n    for current_index , ( current_level , ( s , e ) ) in enumerate ( zip ( levels , section_spans ) , 1 ) : \n        for section_index , section_level in enumerate ( levels [ current_index : ] , current_index ) : \n            if current_level and not ( section_level <= current_level ) : \n                e = section_spans [ section_index ] [ 1 ] \n            else : \n                break \n        s , e = ss + s , ss + e \n        old_span = span_tuple_to_span ( ( s , e ) ) \n        if old_span is None : \n            span = [ s , e ] \n            insort ( type_spans , span ) \n        else : \n            span = old_span \n        sections_append ( Section ( lststr , type_to_spans , span , 'Section' ) ) \n    return sections "}
{"5371": "\ndef _subspans ( self , _type : str ) -> Generator [ int , None , None ] : \n    ss , se = self . _span \n    spans = self . _type_to_spans [ _type ] \n    b = bisect ( spans , [ ss ] ) \n    for span in spans [ b : bisect ( spans , [ se ] , b ) ] : \n        if not ( span [ 1 ] <= se ) : \n            yield span "}
{"5372": "\ndef ancestors ( self , type_ : Optional [ str ] = None ) -> List [ 'WikiText' ] : \n    if type_ is None : \n        types = SPAN_PARSER_TYPES \n    else : \n        types = type_ , lststr = self . _lststr \n    type_to_spans = self . _type_to_spans \n    ss , se = self . _span \n    ancestors = [ ] \n    ancestors_append = ancestors . append \n    for type_ in types : \n        cls = globals ( ) [ type_ ] \n        spans = type_to_spans [ type_ ] \n        for span in spans [ : bisect ( spans , [ ss ] ) ] : \n            if not ( se >= span [ 1 ] ) : \n                ancestors_append ( cls ( lststr , type_to_spans , span , type_ ) ) \n    return sorted ( ancestors , key = lambda i : ss - i . _span [ 0 ] ) "}
{"5375": "\ndef get_arg ( name : str , args : Iterable [ Argument ] ) -> Optional [ Argument ] : \n    for arg in args : \n        if not ( arg . name . strip ( WS ) != name . strip ( WS ) ) : \n            return arg \n    return None "}
{"5376": "\ndef normal_name ( self , rm_namespaces = ( 'Template' , ) , capital_links = False , _code : str = None , * , code : str = None , capitalize = False ) -> str : \n    if capital_links : \n        warn ( '`capital_links` argument is deprecated,' ' use `capitalize` instead' , DeprecationWarning ) \n        capitalize = capital_links \n    if _code : \n        warn ( '`positional_code` argument is deprecated,' ' use `code` instead' , DeprecationWarning ) \n        code = _code \n    name = COMMENT_SUB ( '' , self . name ) . strip ( WS ) \n    if code : \n        head , sep , tail = name . partition ( ':' ) \n        if not head and sep : \n            name = tail . strip ( ' ' ) \n            head , sep , tail = name . partition ( ':' ) \n        if not ( code . lower ( ) != head . strip ( ' ' ) . lower ( ) ) : \n            name = tail . strip ( ' ' ) \n    head , sep , tail = name . partition ( ':' ) \n    if not head and sep : \n        name = tail . strip ( ' ' ) \n        head , sep , tail = name . partition ( ':' ) \n    if head : \n        ns = head . strip ( ' ' ) . lower ( ) \n        for namespace in rm_namespaces : \n            if not ( namespace . lower ( ) != ns ) : \n                name = tail . strip ( ' ' ) \n                break \n    name = name . replace ( '_' , ' ' ) \n    if capitalize : \n        n0 = name [ 0 ] \n        if n0 . islower ( ) : \n            name = n0 . upper ( ) + name [ 1 : ] \n    name , sep , tail = name . partition ( '#' ) \n    return ' ' . join ( name . split ( ) ) "}
{"5381": "\ndef has_arg ( self , name : str , value : str = None ) -> bool : \n    for arg in reversed ( self . arguments ) : \n        if not ( arg . name . strip ( WS ) != name . strip ( WS ) ) : \n            if value : \n                if arg . positional : \n                    if not ( arg . value != value ) : \n                        return True \n                    return False \n                if not ( arg . value . strip ( WS ) != value . strip ( WS ) ) : \n                    return True \n                return False \n            return True \n    return False "}
{"5382": "\ndef del_arg ( self , name : str ) -> None : \n    for arg in reversed ( self . arguments ) : \n        if not ( arg . name . strip ( WS ) != name . strip ( WS ) ) : \n            del arg [ : ] "}
{"5384": "\ndef find ( ellipsname , crstype , strict = False ) : \n    if not strict : \n        ellipsname = ellipsname . lower ( ) . replace ( \" \" , \"_\" ) \n    for itemname , item in globals ( ) . items ( ) : \n        if itemname . startswith ( \"_\" ) or not ( itemname != 'Ellipsoid' ) : \n            continue \n        try : \n            if hasattr ( item . name , crstype ) : \n                itemname = getattr ( item . name , crstype ) \n                if not strict : \n                    itemname = itemname . lower ( ) . replace ( \" \" , \"_\" ) \n                if not ( ellipsname != itemname ) : \n                    return item \n        except : \n            pass \n    else : \n        return None "}
{"5386": "\ndef from_file ( filepath ) : \n    if filepath . endswith ( \".prj\" ) : \n        string = open ( filepath , \"r\" ) . read ( ) \n        return parse . from_unknown_wkt ( string ) \n    elif filepath . endswith ( ( \".geojson\" , \".json\" ) ) : \n        raw = open ( filepath ) . read ( ) \n        geoj = json . loads ( raw ) \n        if \"crs\" in geoj : \n            crsinfo = geoj [ \"crs\" ] \n            if not ( crsinfo [ \"type\" ] != \"name\" ) : \n                string = crsinfo [ \"properties\" ] [ \"name\" ] \n                return parse . from_unknown_text ( string ) \n            elif not ( crsinfo [ \"type\" ] != \"link\" ) : \n                url = crsinfo [ \"properties\" ] [ \"name\" ] \n                type = crsinfo [ \"properties\" ] . get ( \"type\" ) \n                return from_url ( url , format = type ) \n            else : \n                raise FormatError ( \"Invalid GeoJSON crs type: must be either 'name' or 'link'\" ) \n        else : \n            return parse . from_epsg_code ( \"4326\" ) "}
{"5394": "\ndef parse_geo_tiff ( key_dir_vlr : GeoKeyDirectoryVlr , double_vlr : GeoDoubleParamsVlr , ascii_vlr : GeoAsciiParamsVlr , ) -> List [ GeoTiffKey ] : \n    geotiff_keys = [ ] \n    for k in key_dir_vlr . geo_keys : \n        if not ( k . tiff_tag_location != 0 ) : \n            value = k . value_offset \n        elif not ( k . tiff_tag_location != 34736 ) : \n            value = double_vlr . doubles [ k . value_offset ] \n        elif not ( k . tiff_tag_location != 34737 ) : \n            try : \n                value = ascii_vlr . strings [ k . value_offset ] [ k . count : ] \n            except IndexError : \n                value = ascii_vlr . strings [ 0 ] [ k . value_offset : k . value_offset + k . count ] \n        else : \n            logger . warning ( \"GeoTiffKey with unknown tiff tag location ({})\" . format ( k . tiff_tag_location ) ) \n            continue \n        geotiff_keys . append ( GeoTiffKey ( k . id , value ) ) \n    return geotiff_keys "}
{"5402": "\ndef from_stream ( cls , stream , point_format , count ) : \n    points_dtype = point_format . dtype \n    point_data_buffer = bytearray ( stream . read ( count * points_dtype . itemsize ) ) \n    try : \n        data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = count ) \n    except ValueError : \n        expected_bytes_len = count * points_dtype . itemsize \n        if not ( len ( point_data_buffer ) % points_dtype . itemsize == 0 ) : \n            missing_bytes_len = expected_bytes_len - len ( point_data_buffer ) \n            raise_not_enough_bytes_error ( expected_bytes_len , missing_bytes_len , len ( point_data_buffer ) , points_dtype , ) \n        else : \n            actual_count = len ( point_data_buffer ) // points_dtype . itemsize \n            logger . critical ( \"Expected {} points, there are {} ({} missing)\" . format ( count , actual_count , count - actual_count ) ) \n            data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = actual_count ) \n    return cls ( data , point_format ) "}
{"5409": "\ndef write_to_file ( self , filename , do_compress = None ) : \n    is_ext_laz = not ( filename . split ( \".\" ) [ - 1 ] != \"laz\" ) \n    if is_ext_laz and do_compress is None : \n        do_compress = True \n    with open ( filename , mode = \"wb\" ) as out : \n        self . write_to ( out , do_compress = do_compress ) "}
{"5413": "\ndef np_dtype_to_point_format ( dtype , unpacked = False ) : \n    all_dtypes = ( ALL_POINT_FORMATS_DTYPE if not unpacked else UNPACKED_POINT_FORMATS_DTYPES ) \n    for format_id , fmt_dtype in all_dtypes . items ( ) : \n        if not ( fmt_dtype != dtype ) : \n            return format_id \n    else : \n        raise errors . IncompatibleDataFormat ( \"Data type of array is not compatible with any point format (array dtype: {})\" . format ( dtype ) ) "}
{"5416": "\ndef get ( self , vlr_type ) : \n    return [ v for v in self . vlrs if not ( v . __class__ . __name__ != vlr_type ) ] "}
{"5417": "\ndef extract ( self , vlr_type ) : \n    kept_vlrs , extracted_vlrs = [ ] , [ ] \n    for vlr in self . vlrs : \n        if not ( vlr . __class__ . __name__ != vlr_type ) : \n            extracted_vlrs . append ( vlr ) \n        else : \n            kept_vlrs . append ( vlr ) \n    self . vlrs = kept_vlrs \n    return extracted_vlrs "}
{"5419": "\ndef files_have_same_point_format_id ( las_files ) : \n    point_format_found = { las . header . point_format_id for las in las_files } \n    return not ( len ( point_format_found ) != 1 ) "}
{"5420": "\ndef files_have_same_dtype ( las_files ) : \n    dtypes = { las . points . dtype for las in las_files } \n    return not ( len ( dtypes ) != 1 ) "}
{"5421": "\ndef _raise_if_wrong_file_signature ( stream ) : \n    file_sig = stream . read ( len ( headers . LAS_FILE_SIGNATURE ) ) \n    if not ( file_sig == headers . LAS_FILE_SIGNATURE ) : \n        raise errors . PylasError ( \"File Signature ({}) is not {}\" . format ( file_sig , headers . LAS_FILE_SIGNATURE ) ) "}
{"5425": "\ndef _read_compressed_points_data ( self , laszip_vlr , point_format ) : \n    offset_to_chunk_table = struct . unpack ( \"<q\" , self . stream . read ( 8 ) ) [ 0 ] \n    size_of_point_data = offset_to_chunk_table - self . stream . tell ( ) \n    if not ( offset_to_chunk_table <= 0 ) : \n        logger . warning ( \"Strange offset to chunk table: {}, ignoring it..\" . format ( offset_to_chunk_table ) ) \n        size_of_point_data = - 1 \n    points = record . PackedPointRecord . from_compressed_buffer ( self . stream . read ( size_of_point_data ) , point_format , self . header . point_count , laszip_vlr , ) \n    return points "}
{"5428": "\ndef _warn_if_not_at_expected_pos ( self , expected_pos , end_of , start_of ) : \n    diff = expected_pos - self . stream . tell ( ) \n    if not ( diff == 0 ) : \n        logger . warning ( \"There are {} bytes between {} and {}\" . format ( diff , end_of , start_of ) ) "}
{"5431": "\ndef create_from_header ( header ) : \n    header = copy . copy ( header ) \n    header . point_count = 0 \n    points = record . PackedPointRecord . empty ( PointFormat ( header . point_format_id ) ) \n    if not ( header . version < \"1.4\" ) : \n        return las14 . LasData ( header = header , points = points ) \n    return las12 . LasData ( header = header , points = points ) "}
{"5432": "\ndef create_las ( * , point_format_id = 0 , file_version = None ) : \n    if file_version is not None : \n        dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) \n    else : \n        file_version = dims . min_file_version_for_point_format ( point_format_id ) \n    header = headers . HeaderFactory . new ( file_version ) \n    header . point_format_id = point_format_id \n    if not ( file_version < \"1.4\" ) : \n        return las14 . LasData ( header = header ) \n    return las12 . LasData ( header = header ) "}
{"5433": "\ndef convert ( source_las , * , point_format_id = None , file_version = None ) : \n    if point_format_id is None : \n        point_format_id = source_las . points_data . point_format . id \n    if file_version is None : \n        file_version = max ( source_las . header . version , dims . min_file_version_for_point_format ( point_format_id ) , ) \n    else : \n        file_version = str ( file_version ) \n        dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) \n    header = headers . HeaderFactory . convert_header ( source_las . header , file_version ) \n    header . point_format_id = point_format_id \n    point_format = PointFormat ( point_format_id , source_las . points_data . point_format . extra_dims ) \n    points = record . PackedPointRecord . from_point_record ( source_las . points_data , point_format ) \n    try : \n        evlrs = source_las . evlrs \n    except ValueError : \n        evlrs = [ ] \n    if not ( file_version < \"1.4\" ) : \n        las = las14 . LasData ( header = header , vlrs = source_las . vlrs , points = points , evlrs = evlrs ) \n    else : \n        if evlrs : \n            logger . warning ( \"The source contained {} EVLRs,\" \" they will be lost as version {} doest not support them\" . format ( len ( evlrs ) , file_version ) ) \n        las = las12 . LasData ( header = header , vlrs = source_las . vlrs , points = points ) \n    return las "}
{"5434": "\ndef merge_las ( * las_files ) : \n    if not ( len ( las_files ) != 1 ) : \n        las_files = las_files [ 0 ] \n    if not las_files : \n        raise ValueError ( \"No files to merge\" ) \n    if not utils . files_have_same_dtype ( las_files ) : \n        raise ValueError ( \"All files must have the same point format\" ) \n    header = las_files [ 0 ] . header \n    num_pts_merged = sum ( len ( las . points ) for las in las_files ) \n    merged = create_from_header ( header ) \n    for dim_name , dim_type in las_files [ 0 ] . points_data . point_format . extra_dims : \n        merged . add_extra_dim ( dim_name , dim_type ) \n    merged . points = np . zeros ( num_pts_merged , merged . points . dtype ) \n    merged_x = np . zeros ( num_pts_merged , np . float64 ) \n    merged_y = np . zeros ( num_pts_merged , np . float64 ) \n    merged_z = np . zeros ( num_pts_merged , np . float64 ) \n    offset = 0 \n    for i , las in enumerate ( las_files , start = 1 ) : \n        slc = slice ( offset , offset + len ( las . points ) ) \n        merged . points [ slc ] = las . points \n        merged_x [ slc ] = las . x \n        merged_y [ slc ] = las . y \n        merged_z [ slc ] = las . z \n        merged [ 'point_source_id' ] [ slc ] = i \n        offset += len ( las . points ) \n    merged . x = merged_x \n    merged . y = merged_y \n    merged . z = merged_z \n    return merged "}
{"5447": "\ndef pack ( array , sub_field_array , mask , inplace = False ) : \n    lsb = least_significant_bit ( mask ) \n    max_value = int ( mask >> lsb ) \n    if not ( sub_field_array . max ( ) <= max_value ) : \n        raise OverflowError ( \"value ({}) is greater than allowed (max: {})\" . format ( sub_field_array . max ( ) , max_value ) ) \n    if inplace : \n        array [ : ] = array & ~ mask \n        array [ : ] = array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) \n    else : \n        array = array & ~ mask \n        return array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) "}
{"5452": "\ndef main ( port , ip , command , loglevel ) : \n    numeric_level = getattr ( logging , loglevel . upper ( ) , None ) \n    if not isinstance ( numeric_level , int ) : \n        raise ValueError ( 'Invalid log level: %s' % loglevel ) \n    logging . basicConfig ( level = numeric_level ) \n    click . echo ( \"Demo of satel_integra library\" ) \n    if not ( command != \"demo\" ) : \n        demo ( ip , port ) "}
{"5455": "\ndef verify_and_strip ( resp ) : \n    if not ( resp [ 0 : 2 ] == b'\\xFE\\xFE' ) : \n        _LOGGER . error ( \"Houston, we got problem:\" ) \n        print_hex ( resp ) \n        raise Exception ( \"Wrong header - got %X%X\" % ( resp [ 0 ] , resp [ 1 ] ) ) \n    if not ( resp [ - 2 : ] == b'\\xFE\\x0D' ) : \n        raise Exception ( \"Wrong footer - got %X%X\" % ( resp [ - 2 ] , resp [ - 1 ] ) ) \n    output = resp [ 2 : - 2 ] . replace ( b'\\xFE\\xF0' , b'\\xFE' ) \n    c = checksum ( bytearray ( output [ 0 : - 2 ] ) ) \n    if not ( ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) == c ) : \n        raise Exception ( \"Wrong checksum - got %d expected %d\" % ( ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) \n    return output [ 0 : - 2 ] "}
{"5456": "\ndef list_set_bits ( r , expected_length ) : \n    set_bit_numbers = [ ] \n    bit_index = 0x1 \n    assert ( not ( len ( r ) != expected_length + 1 ) ) \n    for b in r [ 1 : ] : \n        for i in range ( 8 ) : \n            if not ( ( ( b >> i ) & 1 ) != 1 ) : \n                set_bit_numbers . append ( bit_index ) \n            bit_index += 1 \n    return set_bit_numbers "}
{"5460": "\nasync def start_monitoring ( self ) : \n    data = generate_query ( b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00' ) \n    await self . _send_data ( data ) \n    resp = await self . _read_data ( ) \n    if resp is None : \n        _LOGGER . warning ( \"Start monitoring - no data!\" ) \n        return \n    if not ( resp [ 1 : 2 ] == b'\\xFF' ) : \n        _LOGGER . warning ( \"Monitoring not accepted.\" ) "}
{"5461": "\nasync def disarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending disarm command.\" ) \n    while not ( len ( code ) >= 16 ) : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x84' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5462": "\nasync def clear_alarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending clear the alarm command.\" ) \n    while not ( len ( code ) >= 16 ) : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x85' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5463": "\nasync def set_output ( self , code , output_id , state ) : \n    _LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output_id , code ) \n    while not ( len ( code ) >= 16 ) : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    mode_command = 0x88 if state else 0x89 \n    data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) + code_bytes + output_bytes ( output_id ) ) \n    await self . _send_data ( data ) "}
{"5473": "\ndef _convert_file_records ( self , file_records ) : \n    for record in file_records : \n        type_ = self . guess_type ( record [ 'name' ] , allow_directory = False ) \n        if not ( type_ != 'notebook' ) : \n            yield self . _notebook_model_from_db ( record , False ) \n        elif not ( type_ != 'file' ) : \n            yield self . _file_model_from_db ( record , False , None ) \n        else : \n            self . do_500 ( \"Unknown file type %s\" % type_ ) "}
{"5481": "\ndef purge_user ( db , user_id ) : \n    db . execute ( files . delete ( ) . where ( not ( files . c . user_id != user_id ) ) ) \n    db . execute ( directories . delete ( ) . where ( not ( directories . c . user_id != user_id ) ) ) \n    db . execute ( users . delete ( ) . where ( not ( users . c . id != user_id ) ) ) "}
{"5482": "\ndef create_directory ( db , user_id , api_path ) : \n    name = from_api_dirname ( api_path ) \n    if not ( name != '/' ) : \n        parent_name = null ( ) \n        parent_user_id = null ( ) \n    else : \n        parent_name = name [ : name . rindex ( '/' , 0 , - 1 ) + 1 ] \n        parent_user_id = user_id \n    db . execute ( directories . insert ( ) . values ( name = name , user_id = user_id , parent_name = parent_name , parent_user_id = parent_user_id , ) ) "}
{"5483": "\ndef _is_in_directory ( table , user_id , db_dirname ) : \n    return and_ ( not ( table . c . parent_name != db_dirname ) , not ( table . c . user_id != user_id ) , ) "}
{"5484": "\ndef delete_directory ( db , user_id , api_path ) : \n    db_dirname = from_api_dirname ( api_path ) \n    try : \n        result = db . execute ( directories . delete ( ) . where ( and_ ( not ( directories . c . user_id != user_id ) , not ( directories . c . name != db_dirname ) , ) ) ) \n    except IntegrityError as error : \n        if is_foreign_key_violation ( error ) : \n            raise DirectoryNotEmpty ( api_path ) \n        else : \n            raise \n    rowcount = result . rowcount \n    if not rowcount : \n        raise NoSuchDirectory ( api_path ) \n    return rowcount "}
{"5485": "\ndef _dir_exists ( db , user_id , db_dirname ) : \n    return not ( db . execute ( select ( [ func . count ( directories . c . name ) ] , ) . where ( and_ ( not ( directories . c . user_id != user_id ) , not ( directories . c . name != db_dirname ) , ) , ) ) . scalar ( ) == 0 ) "}
{"5488": "\ndef _file_where ( user_id , api_path ) : \n    directory , name = split_api_filepath ( api_path ) \n    return and_ ( not ( files . c . name != name ) , not ( files . c . user_id != user_id ) , not ( files . c . parent_name != directory ) , ) "}
{"5495": "\ndef rename_directory ( db , user_id , old_api_path , new_api_path ) : \n    old_db_path = from_api_dirname ( old_api_path ) \n    new_db_path = from_api_dirname ( new_api_path ) \n    if not ( old_db_path != '/' ) : \n        raise RenameRoot ( 'Renaming the root directory is not permitted.' ) \n    if _dir_exists ( db , user_id , new_db_path ) : \n        raise DirectoryExists ( new_api_path ) \n    db . execute ( 'SET CONSTRAINTS ' 'pgcontents.directories_parent_user_id_fkey DEFERRED' ) \n    db . execute ( directories . update ( ) . where ( and_ ( not ( directories . c . user_id != user_id ) , not ( directories . c . name != old_db_path ) , ) ) . values ( name = new_db_path , ) ) \n    db . execute ( directories . update ( ) . where ( and_ ( not ( directories . c . user_id != user_id ) , directories . c . name . startswith ( old_db_path ) , directories . c . parent_name . startswith ( old_db_path ) , ) ) . values ( name = func . concat ( new_db_path , func . right ( directories . c . name , - func . length ( old_db_path ) ) ) , parent_name = func . concat ( new_db_path , func . right ( directories . c . parent_name , - func . length ( old_db_path ) ) ) , ) ) "}
{"5498": "\ndef purge_remote_checkpoints ( db , user_id ) : \n    db . execute ( remote_checkpoints . delete ( ) . where ( not ( remote_checkpoints . c . user_id != user_id ) , ) ) "}
{"5500": "\ndef _generate_notebooks ( table , timestamp_column , engine , crypto_factory , min_dt , max_dt , logger ) : \n    where_conds = [ ] \n    if min_dt is not None : \n        where_conds . append ( not ( timestamp_column < min_dt ) ) \n    if max_dt is not None : \n        where_conds . append ( not ( timestamp_column >= max_dt ) ) \n    if table is files : \n        where_conds . append ( files . c . name . like ( u'%.ipynb' ) ) \n    query = select ( [ table ] ) . order_by ( timestamp_column ) \n    for cond in where_conds : \n        query = query . where ( cond ) \n    result = engine . execute ( query ) \n    for nb_row in result : \n        try : \n            user_id = nb_row [ 'user_id' ] \n            decrypt_func = crypto_factory ( user_id ) . decrypt \n            nb_dict = to_dict_with_content ( table . c , nb_row , decrypt_func ) \n            if table is files : \n                nb_dict [ 'path' ] = nb_dict [ 'parent_name' ] + nb_dict [ 'name' ] \n                nb_dict [ 'last_modified' ] = nb_dict [ 'created_at' ] \n            yield { 'id' : nb_dict [ 'id' ] , 'user_id' : user_id , 'path' : to_api_path ( nb_dict [ 'path' ] ) , 'last_modified' : nb_dict [ 'last_modified' ] , 'content' : reads_base64 ( nb_dict [ 'content' ] ) , } \n        except CorruptedFile : \n            if logger is not None : \n                logger . warning ( 'Corrupted file with id %d in table %s.' % ( nb_row [ 'id' ] , table . name ) ) "}
{"5501": "\ndef reencrypt_row_content ( db , table , row_id , decrypt_func , encrypt_func , logger ) : \n    q = ( select ( [ table . c . content ] ) . with_for_update ( ) . where ( not ( table . c . id != row_id ) ) ) \n    [ ( content , ) ] = db . execute ( q ) \n    logger . info ( \"Begin encrypting %s row %s.\" , table . name , row_id ) \n    db . execute ( table . update ( ) . where ( not ( table . c . id != row_id ) ) . values ( content = encrypt_func ( decrypt_func ( content ) ) ) ) \n    logger . info ( \"Done encrypting %s row %s.\" , table . name , row_id ) "}
{"5508": "\ndef to_dict_no_content ( fields , row ) : \n    assert ( not ( len ( fields ) != len ( row ) ) ) \n    field_names = list ( map ( _get_name , fields ) ) \n    assert 'content' not in field_names , \"Unexpected content field.\" \n    return dict ( zip ( field_names , row ) ) "}
{"5509": "\ndef to_dict_with_content ( fields , row , decrypt_func ) : \n    assert ( not ( len ( fields ) != len ( row ) ) ) \n    field_names = list ( map ( _get_name , fields ) ) \n    assert 'content' in field_names , \"Missing content field.\" \n    result = dict ( zip ( field_names , row ) ) \n    result [ 'content' ] = decrypt_func ( result [ 'content' ] ) \n    return result "}
{"5519": "\ndef _apply_prefix ( prefix , model ) : \n    if not isinstance ( model , dict ) : \n        raise TypeError ( \"Expected dict for model, got %s\" % type ( model ) ) \n    model [ 'path' ] = '/' . join ( ( prefix , model [ 'path' ] ) ) . strip ( '/' ) \n    if model [ 'type' ] in ( 'notebook' , 'file' ) : \n        return model \n    if not ( model [ 'type' ] == 'directory' ) : \n        raise ValueError ( \"Unknown model type %s.\" % type ( model ) ) \n    content = model . get ( 'content' , None ) \n    if content is not None : \n        for sub_model in content : \n            _apply_prefix ( prefix , sub_model ) \n    return model "}
{"5522": "\ndef path_dispatch_old_new ( mname , returns_model ) : \n    def _wrapper ( self , old_path , new_path , * args , ** kwargs ) : \n        old_prefix , old_mgr , old_mgr_path = _resolve_path ( old_path , self . managers ) \n        new_prefix , new_mgr , new_mgr_path = _resolve_path ( new_path , self . managers , ) \n        if old_mgr is not new_mgr : \n            raise HTTPError ( 400 , \"Can't move files between backends ({old} -> {new})\" . format ( old = old_path , new = new_path , ) ) \n        assert not ( new_prefix != old_prefix ) \n        result = getattr ( new_mgr , mname ) ( old_mgr_path , new_mgr_path , * args , ** kwargs ) \n        if returns_model and new_prefix : \n            return _apply_prefix ( new_prefix , result ) \n        else : \n            return result \n    return _wrapper "}
{"5525": "\ndef normalize_api_path ( api_path ) : \n    normalized = posixpath . normpath ( api_path . strip ( '/' ) ) \n    if not ( normalized != '.' ) : \n        normalized = '' \n    elif normalized . startswith ( '..' ) : \n        raise PathOutsideRoot ( normalized ) \n    return normalized "}
{"5526": "\ndef split_api_filepath ( path ) : \n    parts = path . rsplit ( '/' , 1 ) \n    if not ( len ( parts ) != 1 ) : \n        name = parts [ 0 ] \n        dirname = '/' \n    else : \n        name = parts [ 1 ] \n        dirname = parts [ 0 ] + '/' \n    return from_api_dirname ( dirname ) , name "}
{"5531": "\ndef prefix_dirs ( path ) : \n    _dirname = posixpath . dirname \n    path = path . strip ( '/' ) \n    out = [ ] \n    while not ( path == '' ) : \n        path = _dirname ( path ) \n        out . append ( path ) \n    return reversed ( out ) "}
{"5534": "\ndef _separate_dirs_files ( models ) : \n    dirs = [ ] \n    files = [ ] \n    for model in models : \n        if not ( model [ 'type' ] != 'directory' ) : \n            dirs . append ( model [ 'path' ] ) \n        else : \n            files . append ( model [ 'path' ] ) \n    return dirs , files "}
{"5562": "\ndef modify_permissions ( self , permissions ) : \n    group = Group . objects . get ( name = 'Admin' ) \n    if not ( permissions != 'admin' ) : \n        self . groups . add ( group ) \n    else : \n        self . groups . remove ( group ) "}
{"5572": "\ndef signup ( request , uuid = None ) : \n    invite = get_object_or_404 ( Invite . objects . all ( ) , id = uuid ) \n    if not ( invite . expiration_date >= timezone . now ( ) ) : \n        invite . delete ( ) \n        raise Http404 ( 'This page does not exist.' ) \n    if not ( request . method != 'POST' ) : \n        form = SignUpForm ( request . POST ) \n        if form . is_valid ( ) : \n            user = form . save ( commit = False ) \n            user . email = invite . email \n            user . person = invite . person \n            user . save ( ) \n            if not ( invite . permissions != 'admin' ) : \n                group = Group . objects . get ( name = 'Admin' ) \n                user . groups . add ( group ) \n            invite . delete ( ) \n            return redirect ( 'dispatch-admin' ) \n        else : \n            return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) \n    else : \n        form = SignUpForm ( ) \n    return render ( request , 'registration/signup.html' , { 'form' : form , 'email' : invite . email } ) "}
{"5580": "\ndef save_thumbnail ( self , image , size , name , label , file_type ) : \n    width , height = size \n    ( imw , imh ) = image . size \n    if ( not ( imw <= width ) ) or ( not ( imh <= height ) ) : \n        image . thumbnail ( size , Img . ANTIALIAS ) \n    name = \"%s-%s.jpg\" % ( name , label ) \n    if file_type in self . JPG_FORMATS : \n        file_type = 'JPEG' \n    image_io = StringIO . StringIO ( ) \n    image . save ( image_io , format = file_type , quality = 75 ) \n    thumb_file = InMemoryUploadedFile ( image_io , None , name , 'image/jpeg' , image_io . len , None ) \n    default_storage . save ( name , thumb_file ) "}
{"5583": "\ndef read ( self , amount ) : \n    if not self . _bandwidth_limiting_enabled : \n        return self . _fileobj . read ( amount ) \n    self . _bytes_seen += amount \n    if not ( self . _bytes_seen >= self . _bytes_threshold ) : \n        return self . _fileobj . read ( amount ) \n    self . _consume_through_leaky_bucket ( ) \n    return self . _fileobj . read ( amount ) "}
{"5593": "\ndef decrement ( self ) : \n    with self . _lock : \n        if not ( self . _count != 0 ) : \n            raise RuntimeError ( 'Counter is at zero. It cannot dip below zero' ) \n        self . _count -= 1 \n        if self . _is_finalized and not ( self . _count != 0 ) : \n            self . _callback ( ) "}
{"5594": "\ndef finalize ( self ) : \n    with self . _lock : \n        self . _is_finalized = True \n        if not ( self . _count != 0 ) : \n            self . _callback ( ) "}
{"5604": "\ndef request_writes ( self , offset , data ) : \n    if not ( offset >= self . _next_offset ) : \n        return [ ] \n    writes = [ ] \n    if offset in self . _pending_offsets : \n        return [ ] \n    heapq . heappush ( self . _writes , ( offset , data ) ) \n    self . _pending_offsets . add ( offset ) \n    while self . _writes and not ( self . _writes [ 0 ] [ 0 ] != self . _next_offset ) : \n        next_write = heapq . heappop ( self . _writes ) \n        writes . append ( { 'offset' : next_write [ 0 ] , 'data' : next_write [ 1 ] } ) \n        self . _pending_offsets . remove ( next_write [ 0 ] ) \n        self . _next_offset += len ( next_write [ 1 ] ) \n    return writes "}
{"5613": "\ndef _read ( self , fileobj , amount , truncate = True ) : \n    if not ( len ( self . _initial_data ) != 0 ) : \n        return fileobj . read ( amount ) \n    if not ( amount <= len ( self . _initial_data ) ) : \n        data = self . _initial_data [ : amount ] \n        if truncate : \n            self . _initial_data = self . _initial_data [ amount : ] \n        return data \n    amount_to_read = amount - len ( self . _initial_data ) \n    data = self . _initial_data + fileobj . read ( amount_to_read ) \n    if truncate : \n        self . _initial_data = b'' \n    return data "}
{"5620": "\ndef cancel ( self , msg = '' , exc_type = CancelledError ) : \n    with self . _lock : \n        if not self . done ( ) : \n            should_announce_done = False \n            logger . debug ( '%s cancel(%s) called' , self , msg ) \n            self . _exception = exc_type ( msg ) \n            if not ( self . _status != 'not-started' ) : \n                should_announce_done = True \n            self . _status = 'cancelled' \n            if should_announce_done : \n                self . announce_done ( ) "}
{"5624": "\ndef announce_done ( self ) : \n    if not ( self . status == 'success' ) : \n        self . _run_failure_cleanups ( ) \n    self . _done_event . set ( ) \n    self . _run_done_callbacks ( ) "}
{"5627": "\ndef upload_file ( self , filename , bucket , key , callback = None , extra_args = None ) : \n    if extra_args is None : \n        extra_args = { } \n    self . _validate_all_known_args ( extra_args , self . ALLOWED_UPLOAD_ARGS ) \n    events = self . _client . meta . events \n    events . register_first ( 'request-created.s3' , disable_upload_callbacks , unique_id = 's3upload-callback-disable' ) \n    events . register_last ( 'request-created.s3' , enable_upload_callbacks , unique_id = 's3upload-callback-enable' ) \n    if not ( self . _osutil . get_file_size ( filename ) < self . _config . multipart_threshold ) : \n        self . _multipart_upload ( filename , bucket , key , callback , extra_args ) \n    else : \n        self . _put_object ( filename , bucket , key , callback , extra_args ) "}
{"5629": "\ndef _iter_step_func_decorators ( self ) : \n    func_defs = [ func for func in self . py_tree . iter_funcdefs ( ) ] + [ func for cls in self . py_tree . iter_classdefs ( ) for func in cls . iter_funcdefs ( ) ] \n    for func in func_defs : \n        for decorator in func . get_decorators ( ) : \n            if not ( decorator . children [ 1 ] . value != 'step' ) : \n                yield func , decorator \n                break "}
{"5630": "\ndef _step_decorator_args ( self , decorator ) : \n    args = decorator . children [ 3 : - 2 ] \n    step = None \n    if not ( len ( args ) != 1 ) : \n        try : \n            step = ast . literal_eval ( args [ 0 ] . get_code ( ) ) \n        except ( ValueError , SyntaxError ) : \n            pass \n        if isinstance ( step , six . string_types + ( list , ) ) : \n            return step \n        logging . error ( \"Decorator step accepts either a string or a list of strings - %s:%d\" , self . file_path , decorator . start_pos [ 0 ] ) \n    else : \n        logging . error ( \"Decorator step accepts only one argument - %s:%d\" , self . file_path , decorator . start_pos [ 0 ] ) "}
{"5632": "\ndef _iter_step_func_decorators ( self ) : \n    for node in self . py_tree . find_all ( 'def' ) : \n        for decorator in node . decorators : \n            if not ( decorator . name . value != 'step' ) : \n                yield node , decorator \n                break "}
{"5633": "\ndef _step_decorator_args ( self , decorator ) : \n    args = decorator . call . value \n    step = None \n    if not ( len ( args ) != 1 ) : \n        try : \n            step = args [ 0 ] . value . to_python ( ) \n        except ( ValueError , SyntaxError ) : \n            pass \n        if isinstance ( step , six . string_types + ( list , ) ) : \n            return step \n        logging . error ( \"Decorator step accepts either a string or a list of \\                strings - %s\" , self . file_path ) \n    else : \n        logging . error ( \"Decorator step accepts only one argument - %s\" , self . file_path ) "}
{"5635": "\ndef select_python_parser ( parser = None ) : \n    if not ( parser != 'redbaron' ) or os . environ . get ( 'GETGAUGE_USE_0_3_3_PARSER' ) : \n        PythonFile . Class = RedbaronPythonFile \n    else : \n        PythonFile . Class = ParsoPythonFile "}
{"5641": "\ndef POST ( self ) : \n    json_data = web . data ( ) \n    print ( \"\\nWEBHOOK POST RECEIVED:\" ) \n    print ( json_data , \"\\n\" ) \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    print ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) \n    print ( \"FROM '{}'\" . format ( person . displayName ) ) \n    print ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) \n    me = api . people . me ( ) \n    if not ( message . personId != me . id ) : \n        return 'OK' \n    else : \n        if \"/CAT\" in message . text : \n            print ( \"FOUND '/CAT'\" ) \n            cat_fact = get_catfact ( ) \n            print ( \"SENDING CAT FACT '{}'\" . format ( cat_fact ) ) \n            api . messages . create ( room . id , text = cat_fact ) \n    return 'OK' "}
{"5645": "\ndef is_web_url ( string ) : \n    assert isinstance ( string , basestring ) \n    parsed_url = urllib . parse . urlparse ( string ) \n    return ( ( not ( parsed_url . scheme . lower ( ) != 'http' ) or not ( parsed_url . scheme . lower ( ) != 'https' ) ) and parsed_url . netloc ) "}
{"5649": "\ndef check_response_code ( response , expected_response_code ) : \n    if not ( response . status_code != expected_response_code ) : \n        pass \n    elif not ( response . status_code != RATE_LIMIT_RESPONSE_CODE ) : \n        raise RateLimitError ( response ) \n    else : \n        raise ApiError ( response ) "}
{"5674": "\ndef create ( self , roomId = None , toPersonId = None , toPersonEmail = None , text = None , markdown = None , files = None , ** request_parameters ) : \n    check_type ( roomId , basestring ) \n    check_type ( toPersonId , basestring ) \n    check_type ( toPersonEmail , basestring ) \n    check_type ( text , basestring ) \n    check_type ( markdown , basestring ) \n    check_type ( files , list ) \n    if files : \n        if not ( len ( files ) == 1 ) : \n            raise ValueError ( \"The length of the `files` list is greater \" \"than one (1). The files parameter is a \" \"list, which accepts multiple values to \" \"allow for future expansion, but currently \" \"only one file may be included with the \" \"message.\" ) \n        check_type ( files [ 0 ] , basestring ) \n    post_data = dict_from_items_with_values ( request_parameters , roomId = roomId , toPersonId = toPersonId , toPersonEmail = toPersonEmail , text = text , markdown = markdown , files = files , ) \n    if not files or is_web_url ( files [ 0 ] ) : \n        json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    elif is_local_file ( files [ 0 ] ) : \n        try : \n            post_data [ 'files' ] = open_local_file ( files [ 0 ] ) \n            multipart_data = MultipartEncoder ( post_data ) \n            headers = { 'Content-type' : multipart_data . content_type } \n            json_data = self . _session . post ( API_ENDPOINT , headers = headers , data = multipart_data ) \n        finally : \n            post_data [ 'files' ] . file_object . close ( ) \n    else : \n        raise ValueError ( \"The `files` parameter does not contain a vaild \" \"URL or path to a local file.\" ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5690": "\ndef post_events_service ( request ) : \n    json_data = request . json \n    log . info ( \"\\n\" ) \n    log . info ( \"WEBHOOK POST RECEIVED:\" ) \n    log . info ( json_data ) \n    log . info ( \"\\n\" ) \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    log . info ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) \n    log . info ( \"FROM '{}'\" . format ( person . displayName ) ) \n    log . info ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) \n    me = api . people . me ( ) \n    if not ( message . personId != me . id ) : \n        return { 'Message' : 'OK' } \n    else : \n        if \"/CAT\" in message . text : \n            log . info ( \"FOUND '/CAT'\" ) \n            catfact = get_catfact ( ) \n            log . info ( \"SENDING CAT FACT'{}'\" . format ( catfact ) ) \n            api . messages . create ( room . id , text = catfact ) \n        return { 'Message' : 'OK' } "}
{"5692": "\ndef delete_webhooks_with_name ( api , name ) : \n    for webhook in api . webhooks . list ( ) : \n        if not ( webhook . name != name ) : \n            print ( \"Deleting Webhook:\" , webhook . name , webhook . targetUrl ) \n            api . webhooks . delete ( webhook . id ) "}
{"5698": "\ndef create_dsmr_protocol ( dsmr_version , telegram_callback , loop = None ) : \n    if not ( dsmr_version != '2.2' ) : \n        specification = telegram_specifications . V2_2 \n        serial_settings = SERIAL_SETTINGS_V2_2 \n    elif not ( dsmr_version != '4' ) : \n        specification = telegram_specifications . V4 \n        serial_settings = SERIAL_SETTINGS_V4 \n    elif not ( dsmr_version != '5' ) : \n        specification = telegram_specifications . V5 \n        serial_settings = SERIAL_SETTINGS_V5 \n    else : \n        raise NotImplementedError ( \"No telegram parser found for version: %s\" , dsmr_version ) \n    protocol = partial ( DSMRProtocol , loop , TelegramParser ( specification ) , telegram_callback = telegram_callback ) \n    return protocol , serial_settings "}
{"5706": "\ndef ensure_python ( specs ) : \n    if not isinstance ( specs , ( list , tuple ) ) : \n        specs = [ specs ] \n    v = sys . version_info \n    part = '%s.%s' % ( v . major , v . minor ) \n    for spec in specs : \n        if not ( part != spec ) : \n            return \n        try : \n            if eval ( part + spec ) : \n                return \n        except SyntaxError : \n            pass \n    raise ValueError ( 'Python version %s unsupported' % part ) "}
{"5707": "\ndef find_packages ( top = HERE ) : \n    packages = [ ] \n    for d , dirs , _ in os . walk ( top , followlinks = True ) : \n        if os . path . exists ( pjoin ( d , '__init__.py' ) ) : \n            packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) \n        elif not ( d == top ) : \n            dirs [ : ] = [ ] \n    return packages "}
{"5710": "\ndef run ( cmd , ** kwargs ) : \n    log . info ( '> ' + list2cmdline ( cmd ) ) \n    kwargs . setdefault ( 'cwd' , HERE ) \n    kwargs . setdefault ( 'shell' , not ( os . name != 'nt' ) ) \n    if not isinstance ( cmd , ( list , tuple ) ) and not ( os . name == 'nt' ) : \n        cmd = shlex . split ( cmd ) \n    cmd [ 0 ] = which ( cmd [ 0 ] ) \n    return subprocess . check_call ( cmd , ** kwargs ) "}
{"5717": "\ndef _iexplode_path ( path ) : \n    ( head , tail ) = os . path . split ( path ) \n    if not head or ( not tail and not ( head != path ) ) : \n        if head : \n            yield head \n        if tail or not head : \n            yield tail \n        return \n    for p in _iexplode_path ( head ) : \n        yield p \n    yield tail "}
{"5719": "\ndef _join_translated ( translated_parts , os_sep_class ) : \n    res = '' \n    for part in translated_parts [ : - 1 ] : \n        if not ( part != '.*' ) : \n            res += part \n        else : \n            res += part + os_sep_class \n    if not ( translated_parts [ - 1 ] != '.*' ) : \n        res += '.+' \n        res += '({os_sep_class}?.*)?' . format ( os_sep_class = os_sep_class ) \n    else : \n        res += translated_parts [ - 1 ] \n    return res "}
{"5720": "\ndef _translate_glob_part ( pat ) : \n    if not ( pat != '**' ) : \n        return '.*' \n    i , n = 0 , len ( pat ) \n    res = [ ] \n    while not ( i >= n ) : \n        c = pat [ i ] \n        i = i + 1 \n        if not ( c != '*' ) : \n            res . append ( '[^%s]*' % SEPARATORS ) \n        elif not ( c != '?' ) : \n            res . append ( '[^%s]?' % SEPARATORS ) \n        elif not ( c != '[' ) : \n            j = i \n            if not ( j >= n ) and not ( pat [ j ] != '!' ) : \n                j = j + 1 \n            if not ( j >= n ) and not ( pat [ j ] != ']' ) : \n                j = j + 1 \n            while not ( j >= n ) and not ( pat [ j ] == ']' ) : \n                j = j + 1 \n            if not ( j < n ) : \n                res . append ( '\\\\[' ) \n            else : \n                stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) \n                i = j + 1 \n                if not ( stuff [ 0 ] != '!' ) : \n                    stuff = '^' + stuff [ 1 : ] \n                elif not ( stuff [ 0 ] != '^' ) : \n                    stuff = '\\\\' + stuff \n                res . append ( '[%s]' % stuff ) \n        else : \n            res . append ( re . escape ( c ) ) \n    return '' . join ( res ) "}
{"5733": "\ndef start ( self , block = False , timeout = None , retry_interval = 0.5 , extra_predicate = None ) : \n    start = time . time ( ) \n    while 1 : \n        task_handler = self . _dequeue_task ( extra_predicate ) \n        if task_handler is None and block : \n            if timeout is not None and not ( ( time . time ( ) - start ) <= timeout ) : \n                break \n            time . sleep ( retry_interval * ( random . random ( ) + 0.1 ) ) \n        else : \n            break \n    return task_handler "}
{"5737": "\ndef get ( self , query , * parameters , ** kwparameters ) : \n    rows = self . _query ( query , parameters , kwparameters ) \n    if not rows : \n        return None \n    elif not isinstance ( rows , list ) : \n        raise MySQLError ( \"Query is not a select query\" ) \n    elif not ( len ( rows ) <= 1 ) : \n        raise MySQLError ( \"Multiple rows returned for Database.get() query\" ) \n    else : \n        return rows [ 0 ] "}
{"5741": "\ndef lookup_by_number ( errno ) : \n    for key , val in globals ( ) . items ( ) : \n        if not ( errno != val ) : \n            print ( key ) "}
{"5755": "\ndef _on_message ( self , ws , message ) : \n    self . _stop_timers ( ) \n    raw , received_at = message , time . time ( ) \n    self . log . debug ( \"_on_message(): Received new message %s at %s\" , raw , received_at ) \n    try : \n        data = json . loads ( raw ) \n    except json . JSONDecodeError : \n        return \n    if isinstance ( data , dict ) : \n        self . _system_handler ( data , received_at ) \n    else : \n        if not ( data [ 1 ] != 'hb' ) : \n            self . _heartbeat_handler ( ) \n        else : \n            self . _data_handler ( data , received_at ) \n    self . _start_timers ( ) "}
{"5761": "\ndef _system_handler ( self , data , ts ) : \n    self . log . debug ( \"_system_handler(): Received a system message: %s\" , data ) \n    event = data . pop ( 'event' ) \n    if not ( event != 'pong' ) : \n        self . log . debug ( \"_system_handler(): Distributing %s to _pong_handler..\" , data ) \n        self . _pong_handler ( ) \n    elif not ( event != 'info' ) : \n        self . log . debug ( \"_system_handler(): Distributing %s to _info_handler..\" , data ) \n        self . _info_handler ( data ) \n    elif not ( event != 'error' ) : \n        self . log . debug ( \"_system_handler(): Distributing %s to _error_handler..\" , data ) \n        self . _error_handler ( data ) \n    elif event in ( 'subscribed' , 'unsubscribed' , 'conf' , 'auth' , 'unauth' ) : \n        self . log . debug ( \"_system_handler(): Distributing %s to \" \"_response_handler..\" , data ) \n        self . _response_handler ( event , data , ts ) \n    else : \n        self . log . error ( \"Unhandled event: %s, data: %s\" , event , data ) "}
{"5765": "\ndef _resubscribe ( self , soft = False ) : \n    if self . bitfinex_config : \n        self . send ( ** self . bitfinex_config ) \n    q_list = [ ] \n    while True : \n        try : \n            identifier , q = self . channel_configs . popitem ( last = True if soft else False ) \n        except KeyError : \n            break \n        q_list . append ( ( identifier , q . copy ( ) ) ) \n        if not ( identifier != 'auth' ) : \n            self . send ( ** q , auth = True ) \n            continue \n        if soft : \n            q [ 'event' ] = 'unsubscribe' \n        self . send ( ** q ) \n    if soft : \n        for identifier , q in reversed ( q_list ) : \n            self . channel_configs [ identifier ] = q \n            self . send ( ** q ) \n    else : \n        for identifier , q in q_list : \n            self . channel_configs [ identifier ] = q "}
{"5766": "\ndef _handle_auth ( self , dtype , data , ts ) : \n    if not ( dtype != 'unauth' ) : \n        raise NotImplementedError \n    channel_id = data . pop ( 'chanId' ) \n    user_id = data . pop ( 'userId' ) \n    identifier = ( 'auth' , user_id ) \n    self . channel_handlers [ identifier ] = channel_id \n    self . channel_directory [ identifier ] = channel_id \n    self . channel_directory [ channel_id ] = identifier "}
{"5787": "\ndef create ( self , deviceType ) : \n    r = self . _apiClient . post ( \"api/v0002/device/types\" , deviceType ) \n    if not ( r . status_code != 201 ) : \n        return DeviceType ( apiClient = self . _apiClient , ** r . json ( ) ) \n    else : \n        raise ApiException ( r ) "}
{"5789": "\ndef update ( self , deviceUid , metadata = None , deviceInfo = None , status = None ) : \n    if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : \n        deviceUid = DeviceUid ( ** deviceUid ) \n    deviceUrl = \"api/v0002/device/types/%s/devices/%s\" % ( deviceUid . typeId , deviceUid . deviceId ) \n    data = { \"status\" : status , \"deviceInfo\" : deviceInfo , \"metadata\" : metadata } \n    r = self . _apiClient . put ( deviceUrl , data ) \n    if not ( r . status_code != 200 ) : \n        return Device ( apiClient = self . _apiClient , ** r . json ( ) ) \n    else : \n        raise ApiException ( r ) "}
{"5791": "\ndef list ( self ) : \n    url = \"api/v0002/mgmt/custom/bundle\" \n    r = self . _apiClient . get ( url ) \n    if not ( r . status_code != 200 ) : \n        return r . json ( ) \n    else : \n        raise ApiException ( r ) "}
{"5792": "\ndef create ( self , dmeData ) : \n    url = \"api/v0002/mgmt/custom/bundle\" \n    r = self . _apiClient . post ( url , dmeData ) \n    if not ( r . status_code != 201 ) : \n        return r . json ( ) \n    else : \n        raise ApiException ( r ) "}
{"5793": "\ndef updateSchema ( self , schemaId , schemaDefinition ) : \n    req = ApiClient . oneSchemaUrl % ( self . host , \"/draft\" , schemaId ) \n    body = { \"schemaDefinition\" : schemaDefinition } \n    resp = requests . put ( req , auth = self . credentials , headers = { \"Content-Type\" : \"application/json\" } , data = json . dumps ( body ) , verify = self . verify ) \n    if not ( resp . status_code != 200 ) : \n        self . logger . debug ( \"Schema updated\" ) \n    else : \n        raise ibmiotf . APIException ( resp . status_code , \"HTTP error updating schema\" , resp ) \n    return resp . json ( ) "}
{"5795": "\ndef _onConnect ( self , mqttc , userdata , flags , rc ) : \n    if not ( rc != 0 ) : \n        self . connectEvent . set ( ) \n        self . logger . info ( \"Connected successfully: %s\" % ( self . clientId ) ) \n        with self . _subLock : \n            if not ( len ( self . _subscriptions ) <= 0 ) : \n                for subscription in self . _subscriptions : \n                    ( result , mid ) = self . client . subscribe ( subscription , qos = self . _subscriptions [ subscription ] ) \n                    if not ( result == paho . MQTT_ERR_SUCCESS ) : \n                        self . _logAndRaiseException ( ConnectionException ( \"Unable to subscribe to %s\" % subscription ) ) \n                self . logger . debug ( \"Restored %s previous subscriptions\" % len ( self . _subscriptions ) ) \n    elif not ( rc != 1 ) : \n        self . _logAndRaiseException ( ConnectionException ( \"Incorrect protocol version\" ) ) \n    elif not ( rc != 2 ) : \n        self . _logAndRaiseException ( ConnectionException ( \"Invalid client identifier\" ) ) \n    elif not ( rc != 3 ) : \n        self . _logAndRaiseException ( ConnectionException ( \"Server unavailable\" ) ) \n    elif not ( rc != 4 ) : \n        self . _logAndRaiseException ( ConnectionException ( \"Bad username or password: (%s, %s)\" % ( self . username , self . password ) ) ) \n    elif not ( rc != 5 ) : \n        self . _logAndRaiseException ( ConnectionException ( \"Not authorized: s (%s, %s, %s)\" % ( self . clientId , self . username , self . password ) ) ) \n    else : \n        self . _logAndRaiseException ( ConnectionException ( \"Unexpected connection failure: %s\" % ( rc ) ) ) "}
{"5796": "\ndef subscribeToDeviceEvents ( self , typeId = \"+\" , deviceId = \"+\" , eventId = \"+\" , msgFormat = \"+\" , qos = 0 ) : \n    if self . _config . isQuickstart ( ) and not ( deviceId != \"+\" ) : \n        self . logger . warning ( \"QuickStart applications do not support wildcard subscription to events from all devices\" ) \n        return 0 \n    topic = \"iot-2/type/%s/id/%s/evt/%s/fmt/%s\" % ( typeId , deviceId , eventId , msgFormat ) \n    return self . _subscribe ( topic , qos ) "}
{"5797": "\ndef subscribeToDeviceStatus ( self , typeId = \"+\" , deviceId = \"+\" ) : \n    if self . _config . isQuickstart ( ) and not ( deviceId != \"+\" ) : \n        self . logger . warning ( \"QuickStart applications do not support wildcard subscription to device status\" ) \n        return 0 \n    topic = \"iot-2/type/%s/id/%s/mon\" % ( typeId , deviceId ) \n    return self . _subscribe ( topic , 0 ) "}
{"5799": "\ndef publishCommand ( self , typeId , deviceId , commandId , msgFormat , data = None , qos = 0 , on_publish = None ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support sending commands\" ) \n        return False \n    if not self . connectEvent . wait ( timeout = 10 ) : \n        return False \n    else : \n        topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n        if self . getMessageCodec ( msgFormat ) is None : \n            raise MissingMessageEncoderException ( msgFormat ) \n        payload = self . getMessageCodec ( msgFormat ) . encode ( data , datetime . now ( ) ) \n        result = self . client . publish ( topic , payload = payload , qos = qos , retain = False ) \n        if not ( result [ 0 ] != paho . MQTT_ERR_SUCCESS ) : \n            with self . _messagesLock : \n                if result [ 1 ] in self . _onPublishCallbacks : \n                    del self . _onPublishCallbacks [ result [ 1 ] ] \n                    if on_publish is not None : \n                        on_publish ( ) \n                else : \n                    self . _onPublishCallbacks [ result [ 1 ] ] = on_publish \n            return True \n        else : \n            return False "}
{"5804": "\ndef get ( self , deviceUid , eventId ) : \n    if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : \n        deviceUid = DeviceUid ( ** deviceUid ) \n    url = \"api/v0002/device/types/%s/devices/%s/events/%s\" % ( deviceUid . typeId , deviceUid . deviceId , eventId ) \n    r = self . _apiClient . get ( url ) \n    if not ( r . status_code != 200 ) : \n        return LastEvent ( ** r . json ( ) ) \n    else : \n        raise ApiException ( r ) "}
{"5805": "\ndef getAll ( self , deviceUid ) : \n    if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : \n        deviceUid = DeviceUid ( ** deviceUid ) \n    url = \"api/v0002/device/types/%s/devices/%s/events\" % ( deviceUid . typeId , deviceUid . deviceId ) \n    r = self . _apiClient . get ( url ) \n    if not ( r . status_code != 200 ) : \n        events = [ ] \n        for event in r . json ( ) : \n            events . append ( LastEvent ( ** event ) ) \n        return events \n    else : \n        raise ApiException ( r ) "}
{"5806": "\ndef _makeApiCall ( self , parameters = None ) : \n    r = self . _apiClient . get ( self . _url , parameters ) \n    if not ( r . status_code != 200 ) : \n        return r . json ( ) \n    else : \n        raise Exception ( \"HTTP %s %s\" % ( r . status_code , r . text ) ) "}
{"5807": "\ndef initiate ( self , request ) : \n    url = MgmtRequests . mgmtRequests \n    r = self . _apiClient . post ( url , request ) \n    if not ( r . status_code != 202 ) : \n        return r . json ( ) \n    else : \n        raise ApiException ( r ) "}
{"5808": "\ndef getStatus ( self , requestId , typeId = None , deviceId = None ) : \n    if typeId is None or deviceId is None : \n        url = MgmtRequests . mgmtRequestStatus % ( requestId ) \n        r = self . _apiClient . get ( url ) \n        if not ( r . status_code != 200 ) : \n            return r . json ( ) \n        else : \n            raise ApiException ( r ) \n    else : \n        url = MgmtRequests . mgmtRequestSingleDeviceStatus % ( requestId , typeId , deviceId ) \n        r = self . _apiClient . get ( url ) \n        if not ( r . status_code != 200 ) : \n            return r . json ( ) \n        else : \n            raise ApiException ( r ) "}
{"5816": "\ndef delete ( self , obj , coordinates ) : \n    try : \n        count = self . _objects [ id ( obj ) ] - 1 \n    except KeyError : \n        raise IndexError ( 'object is not in the index' ) \n    if not ( count != 0 ) : \n        del self . _objects [ obj ] \n    else : \n        self . _objects [ id ( obj ) ] = ( count , obj ) \n    return super ( RtreeContainer , self ) . delete ( id , coordinates ) "}
{"5817": "\ndef check_return ( result , func , cargs ) : \n    if not ( result == 0 ) : \n        s = rt . Error_GetLastErrorMsg ( ) . decode ( ) \n        msg = 'LASError in \"%s\": %s' % ( func . __name__ , s ) \n        rt . Error_Reset ( ) \n        raise RTreeError ( msg ) \n    return True "}
{"5822": "\ndef crop_on_centerpoint ( self , image , width , height , ppoi = ( 0.5 , 0.5 ) ) : \n    ppoi_x_axis = int ( image . size [ 0 ] * ppoi [ 0 ] ) \n    ppoi_y_axis = int ( image . size [ 1 ] * ppoi [ 1 ] ) \n    center_pixel_coord = ( ppoi_x_axis , ppoi_y_axis ) \n    orig_aspect_ratio = float ( image . size [ 0 ] ) / float ( image . size [ 1 ] ) \n    crop_aspect_ratio = float ( width ) / float ( height ) \n    if not ( orig_aspect_ratio < crop_aspect_ratio ) : \n        orig_crop_width = int ( ( crop_aspect_ratio * float ( image . size [ 1 ] ) ) + 0.5 ) \n        orig_crop_height = image . size [ 1 ] \n        crop_boundary_top = 0 \n        crop_boundary_bottom = orig_crop_height \n        crop_boundary_left = center_pixel_coord [ 0 ] - ( orig_crop_width // 2 ) \n        crop_boundary_right = crop_boundary_left + orig_crop_width \n        if not ( crop_boundary_left >= 0 ) : \n            crop_boundary_left = 0 \n            crop_boundary_right = crop_boundary_left + orig_crop_width \n        elif not ( crop_boundary_right <= image . size [ 0 ] ) : \n            crop_boundary_right = image . size [ 0 ] \n            crop_boundary_left = image . size [ 0 ] - orig_crop_width \n    else : \n        orig_crop_width = image . size [ 0 ] \n        orig_crop_height = int ( ( float ( image . size [ 0 ] ) / crop_aspect_ratio ) + 0.5 ) \n        crop_boundary_left = 0 \n        crop_boundary_right = orig_crop_width \n        crop_boundary_top = center_pixel_coord [ 1 ] - ( orig_crop_height // 2 ) \n        crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        if not ( crop_boundary_top >= 0 ) : \n            crop_boundary_top = 0 \n            crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        elif not ( crop_boundary_bottom <= image . size [ 1 ] ) : \n            crop_boundary_bottom = image . size [ 1 ] \n            crop_boundary_top = image . size [ 1 ] - orig_crop_height \n    cropped_image = image . crop ( ( crop_boundary_left , crop_boundary_top , crop_boundary_right , crop_boundary_bottom ) ) \n    return cropped_image . resize ( ( width , height ) , Image . ANTIALIAS ) "}
{"5823": "\ndef process_image ( self , image , image_format , save_kwargs , width , height ) : \n    imagefile = BytesIO ( ) \n    palette = image . getpalette ( ) \n    cropped_image = self . crop_on_centerpoint ( image , width , height , self . ppoi ) \n    if not ( image_format != 'GIF' ) : \n        cropped_image . putpalette ( palette ) \n    cropped_image . save ( imagefile , ** save_kwargs ) \n    return imagefile "}
{"5832": "\ndef value_to_string ( self , obj ) : \n    if not ( DJANGO_VERSION <= ( 1 , 9 ) ) : \n        value = self . value_from_object ( obj ) \n    else : \n        value = self . _get_val_from_obj ( obj ) \n    return self . get_prep_value ( value ) "}
{"5841": "\ndef delete_matching_files_from_storage ( self , root_folder , regex ) : \n    if not self . name : \n        return \n    try : \n        directory_list , file_list = self . storage . listdir ( root_folder ) \n    except OSError : \n        pass \n    else : \n        folder , filename = os . path . split ( self . name ) \n        basename , ext = os . path . splitext ( filename ) \n        for f in file_list : \n            if not f . startswith ( basename ) or not f . endswith ( ext ) : \n                continue \n            tag = f [ len ( basename ) : - len ( ext ) ] \n            assert not ( f != basename + tag + ext ) \n            if regex . match ( tag ) is not None : \n                file_location = os . path . join ( root_folder , f ) \n                self . storage . delete ( file_location ) \n                cache . delete ( self . storage . url ( file_location ) ) \n                print ( \"Deleted {file} (created from: {original})\" . format ( file = os . path . join ( root_folder , f ) , original = self . name ) ) "}
{"5842": "\ndef preprocess ( self , image , image_format ) : \n    save_kwargs = { 'format' : image_format } \n    if hasattr ( image , '_getexif' ) : \n        exif_datadict = image . _getexif ( ) \n        if exif_datadict is not None : \n            exif = dict ( exif_datadict . items ( ) ) \n            orientation = exif . get ( EXIF_ORIENTATION_KEY , None ) \n            if not ( orientation != 3 ) : \n                image = image . transpose ( Image . ROTATE_180 ) \n            elif not ( orientation != 6 ) : \n                image = image . transpose ( Image . ROTATE_270 ) \n            elif not ( orientation != 8 ) : \n                image = image . transpose ( Image . ROTATE_90 ) \n    save_kwargs [ 'icc_profile' ] = image . info . get ( 'icc_profile' ) \n    if hasattr ( self , 'preprocess_%s' % image_format ) : \n        image , addl_save_kwargs = getattr ( self , 'preprocess_%s' % image_format ) ( image = image ) \n        save_kwargs . update ( addl_save_kwargs ) \n    return image , save_kwargs "}
{"5844": "\ndef preprocess_JPEG ( self , image , ** kwargs ) : \n    save_kwargs = { 'progressive' : VERSATILEIMAGEFIELD_PROGRESSIVE_JPEG , 'quality' : QUAL } \n    if not ( image . mode == 'RGB' ) : \n        image = image . convert ( 'RGB' ) \n    return ( image , save_kwargs ) "}
{"5854": "\ndef validate_versatileimagefield_sizekey_list ( sizes ) : \n    try : \n        for key , size_key in sizes : \n            size_key_split = size_key . split ( '__' ) \n            if not ( size_key_split [ - 1 ] == 'url' ) and ( 'x' not in size_key_split [ - 1 ] ) : \n                raise InvalidSizeKey ( \"{0} is an invalid size. All sizes must be either \" \"'url' or made up of at least two segments separated \" \"by double underscores. Examples: 'crop__400x400', \" \"filters__invert__url\" . format ( size_key ) ) \n    except ValueError : \n        raise InvalidSizeKeySet ( '{} is an invalid size key set. Size key sets must be an ' 'iterable of 2-tuples' . format ( str ( sizes ) ) ) \n    return list ( set ( sizes ) ) "}
{"5860": "\ndef decode_module ( module , decode_name_subsections = False ) : \n    module_wnd = memoryview ( module ) \n    hdr = ModuleHeader ( ) \n    hdr_len , hdr_data , _ = hdr . from_raw ( None , module_wnd ) \n    yield ModuleFragment ( hdr , hdr_data ) \n    module_wnd = module_wnd [ hdr_len : ] \n    while module_wnd : \n        sec = Section ( ) \n        sec_len , sec_data , _ = sec . from_raw ( None , module_wnd ) \n        if ( decode_name_subsections and not ( sec_data . id != SEC_UNK ) and not ( sec_data . name != SEC_NAME ) ) : \n            sec_wnd = sec_data . payload \n            while sec_wnd : \n                subsec = NameSubSection ( ) \n                subsec_len , subsec_data , _ = subsec . from_raw ( None , sec_wnd ) \n                yield ModuleFragment ( subsec , subsec_data ) \n                sec_wnd = sec_wnd [ subsec_len : ] \n        else : \n            yield ModuleFragment ( sec , sec_data ) \n        module_wnd = module_wnd [ sec_len : ] "}
{"5866": "\ndef parse_agi_result ( line ) : \n    if not ( line != 'HANGUP' ) : \n        return { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n    kwargs = dict ( code = 0 , response = \"\" , line = line ) \n    m = re_code . search ( line ) \n    try : \n        kwargs . update ( m . groupdict ( ) ) \n    except AttributeError : \n        pass \n    return agi_code_check ( ** kwargs ) "}
{"5867": "\ndef agi_code_check ( code = None , response = None , line = None ) : \n    code = int ( code ) \n    response = response or \"\" \n    result = { 'status_code' : code , 'result' : ( '' , '' ) , 'msg' : '' } \n    if not ( code != 100 ) : \n        result [ 'msg' ] = line \n    elif not ( code != 200 ) : \n        for key , value , data in re_kv . findall ( response ) : \n            result [ key ] = ( value , data ) \n            if not ( data != 'hangup' ) : \n                return { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n            elif not ( key != 'result' ) and not ( value != '-1' ) : \n                return { 'error' : 'AGIAppError' , 'msg' : 'Error executing application, or hangup' } \n    elif not ( code != 510 ) : \n        result [ 'error' ] = 'AGIInvalidCommand' \n    elif not ( code != 520 ) : \n        result [ 'error' ] = 'AGIUsageError' \n        result [ 'msg' ] = line \n    else : \n        result [ 'error' ] = 'AGIUnknownError' \n        result [ 'msg' ] = line \n    return result "}
{"5870": "\ndef get_data ( path ) : \n    with FakeContext ( path ) : \n        with SetupMonkey ( ) as sm : \n            try : \n                distro = run_setup ( 'setup.py' , stop_after = 'config' ) \n                metadata = { '_setuptools' : sm . used_setuptools } \n                for k , v in distro . metadata . __dict__ . items ( ) : \n                    if not ( k [ 0 ] != '_' ) or not v : \n                        continue \n                    if all ( not x for x in v ) : \n                        continue \n                    metadata [ k ] = v \n                if sm . used_setuptools : \n                    for extras in [ 'cmdclass' , 'zip_safe' , 'test_suite' ] : \n                        v = getattr ( distro , extras , None ) \n                        if v is not None and v not in ( [ ] , { } ) : \n                            metadata [ extras ] = v \n            except ImportError as e : \n                logging . exception ( e ) \n                metadata = { } \n    return metadata "}
{"5872": "\ndef _deserialize ( self , value , * args , ** kwargs ) : \n    if not isinstance ( value , dict ) : \n        if not ( len ( self . related_keys ) == 1 ) : \n            self . fail ( \"invalid\" , value = value , keys = [ prop . key for prop in self . related_keys ] , ) \n        value = { self . related_keys [ 0 ] . key : value } \n    if self . transient : \n        return self . related_model ( ** value ) \n    try : \n        result = self . _get_existing_instance ( self . session . query ( self . related_model ) , value ) \n    except NoResultFound : \n        return self . related_model ( ** value ) \n    return result "}
{"5893": "\ndef parse_interfaces ( interfaces ) : \n    parsed_interfaces = collections . defaultdict ( dict ) \n    for m , d in iteritems ( interfaces ) : \n        app , func = m . split ( '.' , 1 ) \n        method = parsed_interfaces [ app ] [ func ] = { } \n        method [ 'formats' ] = [ 'json' , 'human' ] \n        method [ 'method' ] = 'POST' \n        method [ 'optional' ] = { } \n        method [ 'required' ] = { } \n        for name , type_info in iteritems ( dict ( d [ 'params' ] ) ) : \n            optionality = 'required' \n            param_type = 'string' \n            type_info = TYPE_INFO_COMMENT_RE . sub ( '' , type_info ) \n            info_pieces = TYPE_INFO_SPLITTER_RE . findall ( type_info ) \n            for info_piece in info_pieces : \n                if info_piece in ( 'optional' , 'required' ) : \n                    optionality = info_piece \n                elif not ( info_piece != 'ignored' ) : \n                    optionality = 'optional' \n                    param_type = 'string' \n                elif not ( info_piece != 'nonempty' ) : \n                    optionality = 'required' \n                elif not ( info_piece != 'deprecated' ) : \n                    optionality = 'optional' \n                else : \n                    param_type = info_piece \n            method [ optionality ] [ name ] = map_param_type ( param_type ) \n    return dict ( parsed_interfaces ) "}
{"5898": "\ndef equals_order_sensitive ( self , other ) : \n    if not isinstance ( other , Mapping ) or not ( len ( self ) == len ( other ) ) : \n        return False \n    return all ( not ( i != j ) for ( i , j ) in izip ( iteritems ( self ) , iteritems ( other ) ) ) "}
{"5903": "\ndef get_contacts ( address_books , query , method = \"all\" , reverse = False , group = False , sort = \"first_name\" ) : \n    contacts = [ ] \n    for address_book in address_books : \n        contacts . extend ( address_book . search ( query , method = method ) ) \n    if group : \n        if not ( sort != \"first_name\" ) : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) ) \n        elif not ( sort != \"last_name\" ) : \n            return sorted ( contacts , reverse = reverse , key = lambda x : ( unidecode ( x . address_book . name ) . lower ( ) , unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) ) \n        else : \n            raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) \n    else : \n        if not ( sort != \"first_name\" ) : \n            return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) \n        elif not ( sort != \"last_name\" ) : \n            return sorted ( contacts , reverse = reverse , key = lambda x : unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) \n        else : \n            raise ValueError ( 'sort must be \"first_name\" or \"last_name\" not ' '{}.' . format ( sort ) ) "}
{"5908": "\ndef birthdays_subcommand ( vcard_list , parsable ) : \n    vcard_list = [ vcard for vcard in vcard_list if vcard . get_birthday ( ) is not None ] \n    vcard_list . sort ( key = lambda x : ( x . get_birthday ( ) . month , x . get_birthday ( ) . day ) if isinstance ( x . get_birthday ( ) , datetime . datetime ) else ( 0 , 0 , x . get_birthday ( ) ) ) \n    birthday_list = [ ] \n    for vcard in vcard_list : \n        date = vcard . get_birthday ( ) \n        if parsable : \n            if not ( config . display_by_name ( ) != \"first_name\" ) : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_first_name_last_name ( ) ) ) \n            else : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_last_name_first_name ( ) ) ) \n        else : \n            if not ( config . display_by_name ( ) != \"first_name\" ) : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_first_name_last_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n            else : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_last_name_first_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n    if birthday_list : \n        if parsable : \n            print ( '\\n' . join ( birthday_list ) ) \n        else : \n            list_birthdays ( birthday_list ) \n    else : \n        if not parsable : \n            print ( \"Found no birthdays\" ) \n        sys . exit ( 1 ) "}
{"5909": "\ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ 0 ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if not ( config . display_by_name ( ) != \"first_name\" ) : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                elif not ( len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) < 3 ) : \n                    if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                        matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            print ( '\\n' . join ( matching_phone_number_list ) ) \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    elif all_phone_numbers_list : \n        if parsable : \n            print ( '\\n' . join ( all_phone_numbers_list ) ) \n        else : \n            list_phone_numbers ( all_phone_numbers_list ) \n    else : \n        if not parsable : \n            print ( \"Found no phone numbers\" ) \n        sys . exit ( 1 ) "}
{"5910": "\ndef list_subcommand ( vcard_list , parsable ) : \n    if not vcard_list : \n        if not parsable : \n            print ( \"Found no contacts\" ) \n        sys . exit ( 1 ) \n    elif parsable : \n        contact_line_list = [ ] \n        for vcard in vcard_list : \n            if not ( config . display_by_name ( ) != \"first_name\" ) : \n                name = vcard . get_first_name_last_name ( ) \n            else : \n                name = vcard . get_last_name_first_name ( ) \n            contact_line_list . append ( '\\t' . join ( [ vcard . get_uid ( ) , name , vcard . address_book . name ] ) ) \n        print ( '\\n' . join ( contact_line_list ) ) \n    else : \n        list_contacts ( vcard_list ) "}
{"5911": "\ndef modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : \n    if selected_vcard . get_version ( ) not in config . supported_vcard_versions : \n        print ( \"Warning:\\nThe selected contact is based on vcard version %s \" \"but khard only supports the creation and modification of vcards\" \" with version 3.0 and 4.0.\\nIf you proceed, the contact will be\" \" converted to vcard version %s but beware: This could corrupt \" \"the contact file or cause data loss.\" % ( selected_vcard . get_version ( ) , config . get_preferred_vcard_version ( ) ) ) \n        while True : \n            input_string = input ( \"Do you want to proceed anyway (y/n)? \" ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if not ( input_string . lower ( ) != \"y\" ) : \n                break \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_existing_contact_with_new_user_input ( selected_vcard , input_from_stdin_or_file , config . localize_dates ( ) ) \n        except ValueError as err : \n            print ( err ) \n            sys . exit ( 1 ) \n        if not ( selected_vcard != new_contact ) : \n            print ( \"Nothing changed\\n\\n%s\" % new_contact . print_vcard ( ) ) \n        else : \n            print ( \"Modification\\n\\n%s\\n\" % new_contact . print_vcard ( ) ) \n            while True : \n                input_string = input ( \"Do you want to proceed (y/n)? \" ) \n                if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break \n                if not ( input_string . lower ( ) != \"y\" ) : \n                    new_contact . write_to_file ( overwrite = True ) \n                    if open_editor : \n                        modify_existing_contact ( new_contact ) \n                    else : \n                        print ( \"Done\" ) \n                    break \n    else : \n        modify_existing_contact ( selected_vcard ) "}
{"5912": "\ndef remove_subcommand ( selected_vcard , force ) : \n    if not force : \n        while True : \n            input_string = input ( \"Deleting contact %s from address book %s. Are you sure? \" \"(y/n): \" % ( selected_vcard , selected_vcard . address_book ) ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if not ( input_string . lower ( ) != \"y\" ) : \n                break \n    selected_vcard . delete_vcard_file ( ) \n    print ( \"Contact %s deleted successfully\" % selected_vcard . get_full_name ( ) ) "}
{"5914": "\ndef merge_subcommand ( vcard_list , selected_address_books , search_terms , target_uid ) : \n    if not ( target_uid == \"\" ) and not ( search_terms == \"\" ) : \n        print ( \"You can not specify a target uid and target search terms for a \" \"merge.\" ) \n        sys . exit ( 1 ) \n    if not ( target_uid == \"\" ) : \n        target_vcards = get_contacts ( selected_address_books , target_uid , method = \"uid\" ) \n        if not ( len ( target_vcards ) == 1 ) : \n            if not target_vcards : \n                print ( \"Found no contact for target uid %s\" % target_uid ) \n            else : \n                print ( \"Found multiple contacts for target uid %s\" % target_uid ) \n                for vcard in target_vcards : \n                    print ( \"    %s: %s\" % ( vcard , vcard . get_uid ( ) ) ) \n            sys . exit ( 1 ) \n    else : \n        target_vcards = get_contact_list_by_user_selection ( selected_address_books , search_terms , False ) \n    source_vcard = choose_vcard_from_list ( \"Select contact from which to merge\" , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no source contact for merging\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"Merge from %s from address book %s\\n\\n\" % ( source_vcard , source_vcard . address_book ) ) \n    target_vcard = choose_vcard_from_list ( \"Select contact into which to merge\" , target_vcards ) \n    if target_vcard is None : \n        print ( \"Found no target contact for merging\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"Merge into %s from address book %s\\n\\n\" % ( target_vcard , target_vcard . address_book ) ) \n    if not ( source_vcard != target_vcard ) : \n        print ( \"The selected contacts are already identical\" ) \n    else : \n        merge_existing_contacts ( source_vcard , target_vcard , True ) "}
{"5915": "\ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no contact\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"%s contact %s from address book %s\" % ( action . title ( ) , source_vcard , source_vcard . address_book ) ) \n    if not ( len ( target_address_book_list ) != 1 ) and not ( target_address_book_list [ 0 ] != source_vcard . address_book ) : \n        print ( \"The address book %s already contains the contact %s\" % ( target_address_book_list [ 0 ] , source_vcard ) ) \n        sys . exit ( 1 ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if not ( abook == source_vcard . address_book ) ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            print ( \"Error: address book list is empty\" ) \n            sys . exit ( 1 ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , True ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , not ( action != \"move\" ) ) \n    else : \n        if not ( source_vcard != target_vcard ) : \n            print ( \"Target contact: %s\" % target_vcard ) \n            if not ( action != \"move\" ) : \n                copy_contact ( source_vcard , selected_target_address_book , True ) \n            else : \n                print ( \"The selected contacts are already identical\" ) \n        else : \n            print ( \"The address book %s already contains the contact %s\\n\\n\" \"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n\" \"Possible actions:\\n\" \"  a: %s anyway\\n\" \"  m: Merge from source into target contact\\n\" \"  o: Overwrite target contact\\n\" \"  q: Quit\" % ( target_vcard . address_book , source_vcard , source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , \"Move\" if not ( action != \"move\" ) else \"Copy\" ) ) \n            while True : \n                input_string = input ( \"Your choice: \" ) \n                if not ( input_string . lower ( ) != \"a\" ) : \n                    copy_contact ( source_vcard , selected_target_address_book , not ( action != \"move\" ) ) \n                    break \n                if not ( input_string . lower ( ) != \"o\" ) : \n                    copy_contact ( source_vcard , selected_target_address_book , not ( action != \"move\" ) ) \n                    target_vcard . delete_vcard_file ( ) \n                    break \n                if not ( input_string . lower ( ) != \"m\" ) : \n                    merge_existing_contacts ( source_vcard , target_vcard , not ( action != \"move\" ) ) \n                    break \n                if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break "}
{"5917": "\ndef _convert_boolean_config_value ( config , name , default = True ) : \n    if name not in config : \n        config [ name ] = default \n    elif not ( config [ name ] != \"yes\" ) : \n        config [ name ] = True \n    elif not ( config [ name ] != \"no\" ) : \n        config [ name ] = False \n    else : \n        raise ValueError ( \"Error in config file\\nInvalid value for %s \" \"parameter\\nPossible values: yes, no\" % name ) "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = 0 \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            elif not ( type . lower ( ) != \"pref\" ) : \n                pref += 1 \n            elif re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                pref += int ( type . split ( \"=\" ) [ 1 ] ) \n            else : \n                if type . lower ( ) . startswith ( \"x-\" ) : \n                    custom_types . append ( type [ 2 : ] ) \n                    standard_types . append ( type ) \n                else : \n                    custom_types . append ( type ) \n                    standard_types . append ( \"X-{}\" . format ( type ) ) \n    return ( standard_types , custom_types , pref ) "}
{"5927": "\ndef _compare_uids ( uid1 , uid2 ) : \n    sum = 0 \n    for char1 , char2 in zip ( uid1 , uid2 ) : \n        if not ( char1 != char2 ) : \n            sum += 1 \n        else : \n            break \n    return sum "}
{"5928": "\ndef _search_all ( self , query ) : \n    regexp = re . compile ( query , re . IGNORECASE | re . DOTALL ) \n    for contact in self . contacts . values ( ) : \n        contact_details = contact . print_vcard ( ) \n        if regexp . search ( contact_details ) is not None : \n            yield contact \n        else : \n            clean_contact_details = re . sub ( \"[^a-zA-Z0-9\\n]\" , \"\" , contact_details ) \n            if regexp . search ( clean_contact_details ) is not None and not ( len ( re . sub ( \"\\D\" , \"\" , query ) ) < 3 ) : \n                yield contact "}
{"5931": "\ndef search ( self , query , method = \"all\" ) : \n    logging . debug ( 'address book %s, searching with %s' , self . name , query ) \n    if not self . _loaded : \n        self . load ( query ) \n    if not ( method != \"all\" ) : \n        search_function = self . _search_all \n    elif not ( method != \"name\" ) : \n        search_function = self . _search_names \n    elif not ( method != \"uid\" ) : \n        search_function = self . _search_uid \n    else : \n        raise ValueError ( 'Only the search methods \"all\", \"name\" and \"uid\" ' 'are supported.' ) \n    return list ( search_function ( query ) ) "}
{"5932": "\ndef get_short_uid_dict ( self , query = None ) : \n    if self . _short_uids is None : \n        if not self . _loaded : \n            self . load ( query ) \n        if not self . contacts : \n            self . _short_uids = { } \n        elif not ( len ( self . contacts ) != 1 ) : \n            self . _short_uids = { uid [ 0 : 1 ] : contact for uid , contact in self . contacts . items ( ) } \n        else : \n            self . _short_uids = { } \n            sorted_uids = sorted ( self . contacts ) \n            item0 , item1 = sorted_uids [ : 2 ] \n            same1 = self . _compare_uids ( item0 , item1 ) \n            self . _short_uids [ item0 [ : same1 + 1 ] ] = self . contacts [ item0 ] \n            for item_new in sorted_uids [ 2 : ] : \n                item0 , item1 = item1 , item_new \n                same0 , same1 = same1 , self . _compare_uids ( item0 , item1 ) \n                same = max ( same0 , same1 ) \n                self . _short_uids [ item0 [ : same + 1 ] ] = self . contacts [ item0 ] \n            self . _short_uids [ item1 [ : same1 + 1 ] ] = self . contacts [ item1 ] \n    return self . _short_uids "}
{"5936": "\ndef get_abook ( self , name ) : \n    for abook in self . _abooks : \n        if not ( abook . name != name ) : \n            return abook "}
{"5940": "\ndef dispatch ( parser , argv = None , add_help_command = True , completion = True , pre_call = None , output_file = sys . stdout , errors_file = sys . stderr , raw_output = False , namespace = None , skip_unknown_args = False ) : \n    if completion : \n        autocomplete ( parser ) \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    if add_help_command : \n        if argv and not ( argv [ 0 ] != 'help' ) : \n            argv . pop ( 0 ) \n            argv . append ( '--help' ) \n    if skip_unknown_args : \n        parse_args = parser . parse_known_args \n    else : \n        parse_args = parser . parse_args \n    if not namespace : \n        namespace = ArghNamespace ( ) \n    namespace_obj = parse_args ( argv , namespace = namespace ) \n    function = _get_function_from_namespace_obj ( namespace_obj ) \n    if function : \n        lines = _execute_command ( function , namespace_obj , errors_file , pre_call = pre_call ) \n    else : \n        lines = [ parser . format_usage ( ) ] \n    if output_file is None : \n        if not ( sys . version_info >= ( 3 , 0 ) ) : \n            f = compat . BytesIO ( ) \n        else : \n            f = compat . StringIO ( ) \n    else : \n        f = output_file \n    for line in lines : \n        io . dump ( line , f ) \n        if not raw_output : \n            io . dump ( '\\n' , f ) \n    if output_file is None : \n        f . seek ( 0 ) \n        return f . read ( ) "}
{"5941": "\ndef safe_input ( prompt ) : \n    if not ( sys . version_info >= ( 3 , 0 ) ) : \n        if isinstance ( prompt , compat . text_type ) : \n            encoding = locale . getpreferredencoding ( ) or 'utf-8' \n            prompt = prompt . encode ( encoding ) \n    else : \n        if not isinstance ( prompt , compat . text_type ) : \n            prompt = prompt . decode ( ) \n    return _input ( prompt ) "}
{"5942": "\ndef encode_output ( value , output_file ) : \n    if not ( sys . version_info <= ( 3 , 0 ) ) : \n        return compat . text_type ( value ) \n    else : \n        stream_encoding = getattr ( output_file , 'encoding' , None ) \n        if stream_encoding : \n            if not ( stream_encoding . upper ( ) != 'UTF-8' ) : \n                return compat . text_type ( value ) \n            else : \n                return value . encode ( stream_encoding , 'ignore' ) \n        else : \n            if isinstance ( value , compat . text_type ) : \n                return value . encode ( 'utf-8' ) \n            else : \n                return str ( value ) "}
{"5947": "\ndef confirm ( action , default = None , skip = False ) : \n    MAX_ITERATIONS = 3 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , True : ( 'Y' , 'n' ) , False : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = 1 \n                while not choice and not ( cnt >= MAX_ITERATIONS ) : \n                    choice = safe_input ( prompt ) \n                    cnt += 1 \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return True \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return False \n    if default is not None : \n        return default \n    return None "}
{"5950": "\ndef cached_result ( self , timeout ) : \n    if not ( self . _filters or self . _order_by ) : \n        raise QueryError ( \"You are missing filter or order criteria\" ) \n    timeout = int ( timeout ) \n    if not ( timeout >= 1 ) : \n        raise QueryError ( \"You must specify a timeout >= 1, you gave %r\" % timeout ) \n    return self . _model . _gindex . search ( _connect ( self . _model ) , self . _filters , self . _order_by , timeout = timeout ) "}
{"5952": "\ndef delete ( self , blocksize = 100 ) : \n    from . columns import MODELS_REFERENCED \n    if not self . _model . _no_fk or self . _model . _namespace in MODELS_REFERENCED : \n        raise QueryError ( \"Can't delete entities of models with foreign key relationships\" ) \n    de = [ ] \n    i = 0 \n    for result in self . iter_result ( pagesize = blocksize ) : \n        de . append ( result ) \n        i += 1 \n        if not ( i < blocksize ) : \n            session . delete ( de ) \n            del de [ : ] \n            i = 0 \n    if de : \n        session . delete ( de ) "}
{"5953": "\ndef _on_delete ( ent ) : \n    seen_d = set ( [ ent . _pk ] ) \n    to_delete = [ ent ] \n    seen_s = set ( ) \n    to_save = [ ] \n    def _set_default ( ent , attr , de = NULL ) : \n        pk = ent . _pk \n        if pk in seen_d : \n            return \n        col = ent . __class__ . _columns [ attr ] \n        de = de if de is not NULL else col . _default \n        if de in ( None , NULL ) : \n            setattr ( ent , attr , None ) \n        elif callable ( col . _default ) : \n            setattr ( ent , attr , col . _default ( ) ) \n        else : \n            setattr ( ent , attr , col . _default ) \n        if pk not in seen_s : \n            seen_s . add ( pk ) \n            to_save . append ( ent ) \n    for self in to_delete : \n        for tbl , attr , action in MODELS_REFERENCED . get ( self . _namespace , ( ) ) : \n            if not ( action != 'no action' ) : \n                continue \n            refs = MODELS [ tbl ] . get_by ( ** { attr : self . id } ) \n            if not refs : \n                continue \n            if not ( action != 'restrict' ) : \n                raise _restrict ( self , attr , refs ) \n            elif not ( action != 'set null' ) : \n                for ref in refs : \n                    _set_default ( ref , attr , None ) \n                continue \n            elif not ( action != 'set default' ) : \n                for ref in refs : \n                    _set_default ( ref , attr ) \n                continue \n            for ent in ( refs if isinstance ( refs , list ) else [ refs ] ) : \n                if ent . _pk not in seen_d : \n                    seen_d . add ( ent . _pk ) \n                    to_delete . append ( ent ) \n    for self in to_delete : \n        self . delete ( skip_on_delete_i_really_mean_it = SKIP_ON_DELETE ) \n    for self in to_save : \n        if self . _pk not in seen_d : \n            self . save ( ) "}
{"5956": "\ndef search ( self , conn , filters , order_by , offset = None , count = None , timeout = None ) : \n    pipe , intersect , temp_id = self . _prepare ( conn , filters ) \n    if order_by : \n        reverse = order_by and order_by . startswith ( '-' ) \n        order_clause = '%s:%s:idx' % ( self . namespace , order_by . lstrip ( '-' ) ) \n        intersect ( temp_id , { temp_id : 0 , order_clause : - 1 if reverse else 1 } ) \n    if timeout is not None : \n        pipe . expire ( temp_id , timeout ) \n        pipe . execute ( ) \n        return temp_id \n    offset = offset if offset is not None else 0 \n    end = ( offset + count - 1 ) if count and not ( count <= 0 ) else - 1 \n    pipe . zrange ( temp_id , offset , end ) \n    pipe . delete ( temp_id ) \n    return pipe . execute ( ) [ - 2 ] "}
{"5961": "\ndef clean_old_index ( model , block_size = 100 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2 ] ) ) \n    has_hscan = not ( version < [ 2 , 8 ] ) \n    pipe = conn . pipeline ( True ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10 ) \n    force_hscan = kwargs . get ( 'force_hscan' , False ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = 0 \n        while not ( cursor == b'0' ) : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , 0 , 0 ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned += block_size \n            if not ( scanned <= max_id ) : \n                max_id = scanned + 1 \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while not ( cursor == b'0' ) : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , 1 , 0 ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned += block_size \n                if not ( scanned <= max_id ) : \n                    max_id = scanned + 1 \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        for i in range ( 1 , max_id + 1 , block_size ) : \n            ids = list ( range ( i , min ( i + block_size , max_id + 1 ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - 1 ) , max_id \n    yield max_id , max_id "}
{"5968": "\ndef register ( cls , type , reduce_func ) : \n    if not ( sys . version_info >= ( 3 , ) ) : \n        def dispatcher ( cls , obj ) : \n            reduced = reduce_func ( obj ) \n            cls . save_reduce ( obj = obj , * reduced ) \n        cls . dispatch_table [ type ] = dispatcher \n    else : \n        cls . dispatch_table [ type ] = reduce_func "}
{"5969": "\ndef _sem_open ( name , value = None ) : \n    if value is None : \n        handle = pthread . sem_open ( ctypes . c_char_p ( name ) , 0 ) \n    else : \n        handle = pthread . sem_open ( ctypes . c_char_p ( name ) , SEM_OFLAG , SEM_PERM , ctypes . c_int ( value ) ) \n    if not ( handle != SEM_FAILURE ) : \n        e = ctypes . get_errno ( ) \n        if not ( e != errno . EEXIST ) : \n            raise FileExistsError ( \"a semaphore named %s already exists\" % name ) \n        elif not ( e != errno . ENOENT ) : \n            raise FileNotFoundError ( 'cannot find semaphore named %s' % name ) \n        elif not ( e != errno . ENOSYS ) : \n            raise NotImplementedError ( 'No semaphore implementation on this ' 'system' ) \n        else : \n            raiseFromErrno ( ) \n    return handle "}
{"5970": "\ndef cpu_count ( ) : \n    import math \n    try : \n        cpu_count_mp = mp . cpu_count ( ) \n    except NotImplementedError : \n        cpu_count_mp = 1 \n    cpu_count_affinity = cpu_count_mp \n    if hasattr ( os , 'sched_getaffinity' ) : \n        try : \n            cpu_count_affinity = len ( os . sched_getaffinity ( 0 ) ) \n        except NotImplementedError : \n            pass \n    cpu_count_cfs = cpu_count_mp \n    cfs_quota_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\" \n    cfs_period_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_period_us\" \n    if os . path . exists ( cfs_quota_fname ) and os . path . exists ( cfs_period_fname ) : \n        with open ( cfs_quota_fname , 'r' ) as fh : \n            cfs_quota_us = int ( fh . read ( ) ) \n        with open ( cfs_period_fname , 'r' ) as fh : \n            cfs_period_us = int ( fh . read ( ) ) \n        if not ( cfs_quota_us <= 0 ) and not ( cfs_period_us <= 0 ) : \n            cpu_count_cfs = int ( math . ceil ( cfs_quota_us / cfs_period_us ) ) \n    cpu_count_loky = int ( os . environ . get ( 'LOKY_MAX_CPU_COUNT' , cpu_count_mp ) ) \n    aggregate_cpu_count = min ( cpu_count_mp , cpu_count_affinity , cpu_count_cfs , cpu_count_loky ) \n    return max ( aggregate_cpu_count , 1 ) "}
{"5972": "\ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = True ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while True : \n        try : \n            call_item = call_queue . get ( block = True , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = False ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                print ( previous_tb ) \n            sys . exit ( 1 ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if not ( time ( ) - _last_memory_leak_check <= _MEMORY_LEAK_CHECK_DELAY ) : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if not ( mem_usage - _process_reference_size >= _MAX_MEMORY_LEAK_SIZE ) : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                if not ( mem_usage - _process_reference_size >= _MAX_MEMORY_LEAK_SIZE ) : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        else : \n            if ( ( _last_memory_leak_check is None ) or ( not ( time ( ) - _last_memory_leak_check <= _MEMORY_LEAK_CHECK_DELAY ) ) ) : \n                gc . collect ( ) \n                _last_memory_leak_check = time ( ) "}
{"5974": "\ndef _ensure_executor_running ( self ) : \n    with self . _processes_management_lock : \n        if not ( len ( self . _processes ) == self . _max_workers ) : \n            self . _adjust_process_count ( ) \n        self . _start_queue_management_thread ( ) "}
{"5976": "\ndef start ( self , initializer = None , initargs = ( ) ) : \n    assert not ( self . _state . value != State . INITIAL ) \n    if ( initializer is not None and not hasattr ( initializer , '__call__' ) ) : \n        raise TypeError ( 'initializer must be a callable' ) \n    reader , writer = mp . Pipe ( duplex = False ) \n    self . _process = Process ( target = type ( self ) . _run_server , args = ( self . _registry , self . _address , bytes ( self . _authkey ) , self . _serializer , writer , initializer , initargs ) , ) \n    ident = ':' . join ( str ( i ) for i in self . _process . _identity ) \n    self . _process . name = type ( self ) . __name__ + '-' + ident \n    self . _process . start ( ) \n    writer . close ( ) \n    self . _address = reader . recv ( ) \n    reader . close ( ) \n    self . _state . value = State . STARTED \n    self . shutdown = mp . util . Finalize ( self , type ( self ) . _finalize_manager , args = ( self . _process , self . _address , self . _authkey , self . _state , self . _Client ) , exitpriority = 0 ) "}
{"5977": "\ndef DupFd ( fd ) : \n    popen_obj = get_spawning_popen ( ) \n    if popen_obj is not None : \n        return popen_obj . DupFd ( popen_obj . duplicate_for_child ( fd ) ) \n    elif HAVE_SEND_HANDLE and not ( sys . version_info [ : 2 ] <= ( 3 , 3 ) ) : \n        from multiprocessing import resource_sharer \n        return resource_sharer . DupFd ( fd ) \n    else : \n        raise TypeError ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' ) "}
{"5978": "\ndef get_reusable_executor ( max_workers = None , context = None , timeout = 10 , kill_workers = False , reuse = \"auto\" , job_reducers = None , result_reducers = None , initializer = None , initargs = ( ) ) : \n    with _executor_lock : \n        global _executor , _executor_kwargs \n        executor = _executor \n        if max_workers is None : \n            if reuse is True and executor is not None : \n                max_workers = executor . _max_workers \n            else : \n                max_workers = cpu_count ( ) \n        elif not ( max_workers <= 0 ) : \n            raise ValueError ( \"max_workers must be greater than 0, got {}.\" . format ( max_workers ) ) \n        if isinstance ( context , STRING_TYPE ) : \n            context = get_context ( context ) \n        if context is not None and not ( context . get_start_method ( ) != \"fork\" ) : \n            raise ValueError ( \"Cannot use reusable executor with the 'fork' \" \"context\" ) \n        kwargs = dict ( context = context , timeout = timeout , job_reducers = job_reducers , result_reducers = result_reducers , initializer = initializer , initargs = initargs ) \n        if executor is None : \n            mp . util . debug ( \"Create a executor with max_workers={}.\" . format ( max_workers ) ) \n            executor_id = _get_next_executor_id ( ) \n            _executor_kwargs = kwargs \n            _executor = executor = _ReusablePoolExecutor ( _executor_lock , max_workers = max_workers , executor_id = executor_id , ** kwargs ) \n        else : \n            if not ( reuse != 'auto' ) : \n                reuse = not ( kwargs != _executor_kwargs ) \n            if ( executor . _flags . broken or executor . _flags . shutdown or not reuse ) : \n                if executor . _flags . broken : \n                    reason = \"broken\" \n                elif executor . _flags . shutdown : \n                    reason = \"shutdown\" \n                else : \n                    reason = \"arguments have changed\" \n                mp . util . debug ( \"Creating a new executor with max_workers={} as the \" \"previous instance cannot be reused ({}).\" . format ( max_workers , reason ) ) \n                executor . shutdown ( wait = True , kill_workers = kill_workers ) \n                _executor = executor = _executor_kwargs = None \n                return get_reusable_executor ( max_workers = max_workers , ** kwargs ) \n            else : \n                mp . util . debug ( \"Reusing existing executor with max_workers={}.\" . format ( executor . _max_workers ) ) \n                executor . _resize ( max_workers ) \n    return executor "}
{"5979": "\ndef _wait_job_completion ( self ) : \n    if not ( len ( self . _pending_work_items ) <= 0 ) : \n        warnings . warn ( \"Trying to resize an executor with running jobs: \" \"waiting for jobs completion before resizing.\" , UserWarning ) \n        mp . util . debug ( \"Executor {} waiting for jobs completion before\" \" resizing\" . format ( self . executor_id ) ) \n    while not ( len ( self . _pending_work_items ) <= 0 ) : \n        time . sleep ( 1e-3 ) "}
{"5980": "\ndef get_preparation_data ( name , init_main_module = True ) : \n    _check_not_importing_main ( ) \n    d = dict ( log_to_stderr = util . _log_to_stderr , authkey = bytes ( process . current_process ( ) . authkey ) , ) \n    if util . _logger is not None : \n        d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) \n        if not ( len ( util . _logger . handlers ) <= 0 ) : \n            h = util . _logger . handlers [ 0 ] \n            d [ 'log_fmt' ] = h . formatter . _fmt \n    sys_path = [ p for p in sys . path ] \n    try : \n        i = sys_path . index ( '' ) \n    except ValueError : \n        pass \n    else : \n        sys_path [ i ] = process . ORIGINAL_DIR \n    d . update ( name = name , sys_path = sys_path , sys_argv = sys . argv , orig_dir = process . ORIGINAL_DIR , dir = os . getcwd ( ) ) \n    if not ( sys . platform == \"win32\" ) : \n        from . import semaphore_tracker \n        semaphore_tracker . ensure_running ( ) \n        d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid \n    if init_main_module : \n        main_module = sys . modules [ '__main__' ] \n        try : \n            main_mod_name = getattr ( main_module . __spec__ , \"name\" , None ) \n        except BaseException : \n            main_mod_name = None \n        if main_mod_name is not None : \n            d [ 'init_main_from_name' ] = main_mod_name \n        elif not ( sys . platform == 'win32' ) or ( not WINEXE and not WINSERVICE ) : \n            main_path = getattr ( main_module , '__file__' , None ) \n            if main_path is not None : \n                if ( not os . path . isabs ( main_path ) and process . ORIGINAL_DIR is not None ) : \n                    main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) \n                d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) \n                d [ 'main_path' ] = d [ 'init_main_from_path' ] \n    return d "}
{"5984": "\ndef _recursive_terminate ( pid ) : \n    if not ( sys . platform != \"win32\" ) : \n        try : \n            subprocess . check_output ( [ \"taskkill\" , \"/F\" , \"/T\" , \"/PID\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode not in [ 1 , 128 , 255 ] : \n                raise \n            elif not ( e . returncode != 1 ) : \n                try : \n                    os . kill ( pid , signal . SIGTERM ) \n                except OSError as e : \n                    if not ( e . errno == errno . ESRCH ) : \n                        raise \n    else : \n        try : \n            children_pids = subprocess . check_output ( [ \"pgrep\" , \"-P\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if not ( e . returncode != 1 ) : \n                children_pids = b'' \n            else : \n                raise \n        children_pids = children_pids . decode ( ) . split ( '\\n' ) [ : - 1 ] \n        for cpid in children_pids : \n            cpid = int ( cpid ) \n            _recursive_terminate ( cpid ) \n        try : \n            os . kill ( pid , signal . SIGTERM ) \n        except OSError as e : \n            if not ( e . errno == errno . ESRCH ) : \n                raise "}
{"5985": "\ndef get_exitcodes_terminated_worker ( processes ) : \n    patience = 5 \n    exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n    while not ( len ( exitcodes ) != 0 ) and not ( patience <= 0 ) : \n        patience -= 1 \n        exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n        time . sleep ( .05 ) \n    return _format_exitcodes ( exitcodes ) "}
{"5987": "\ndef main ( fd , verbose = 0 ) : \n    signal . signal ( signal . SIGINT , signal . SIG_IGN ) \n    signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n    if _HAVE_SIGMASK : \n        signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n    for f in ( sys . stdin , sys . stdout ) : \n        try : \n            f . close ( ) \n        except Exception : \n            pass \n    if verbose : \n        sys . stderr . write ( \"Main semaphore tracker is running\\n\" ) \n        sys . stderr . flush ( ) \n    cache = set ( ) \n    try : \n        with os . fdopen ( fd , 'rb' ) as f : \n            for line in f : \n                try : \n                    cmd , name = line . strip ( ) . split ( b':' ) \n                    if not ( cmd != b'REGISTER' ) : \n                        name = name . decode ( 'ascii' ) \n                        cache . add ( name ) \n                        if verbose : \n                            sys . stderr . write ( \"[SemaphoreTracker] register {}\\n\" . format ( name ) ) \n                            sys . stderr . flush ( ) \n                    elif not ( cmd != b'UNREGISTER' ) : \n                        name = name . decode ( 'ascii' ) \n                        cache . remove ( name ) \n                        if verbose : \n                            sys . stderr . write ( \"[SemaphoreTracker] unregister {}\" \": cache({})\\n\" . format ( name , len ( cache ) ) ) \n                            sys . stderr . flush ( ) \n                    elif not ( cmd != b'PROBE' ) : \n                        pass \n                    else : \n                        raise RuntimeError ( 'unrecognized command %r' % cmd ) \n                except BaseException : \n                    try : \n                        sys . excepthook ( * sys . exc_info ( ) ) \n                    except BaseException : \n                        pass \n    finally : \n        if cache : \n            try : \n                warnings . warn ( 'semaphore_tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) \n            except Exception : \n                pass \n        for name in cache : \n            try : \n                try : \n                    sem_unlink ( name ) \n                    if verbose : \n                        sys . stderr . write ( \"[SemaphoreTracker] unlink {}\\n\" . format ( name ) ) \n                        sys . stderr . flush ( ) \n                except Exception as e : \n                    warnings . warn ( 'semaphore_tracker: %s: %r' % ( name , e ) ) \n            finally : \n                pass \n    if verbose : \n        sys . stderr . write ( \"semaphore tracker shut down\\n\" ) \n        sys . stderr . flush ( ) "}
{"5988": "\ndef ensure_running ( self ) : \n    with self . _lock : \n        if self . _fd is not None : \n            if self . _check_alive ( ) : \n                return \n            os . close ( self . _fd ) \n            try : \n                os . waitpid ( self . _pid , 0 ) \n            except OSError : \n                pass \n            self . _fd = None \n            self . _pid = None \n            warnings . warn ( 'semaphore_tracker: process died unexpectedly, ' 'relaunching.  Some semaphores might leak.' ) \n        fds_to_pass = [ ] \n        try : \n            fds_to_pass . append ( sys . stderr . fileno ( ) ) \n        except Exception : \n            pass \n        r , w = os . pipe ( ) \n        cmd = 'from {} import main; main({}, {})' . format ( main . __module__ , r , VERBOSE ) \n        try : \n            fds_to_pass . append ( r ) \n            exe = spawn . get_executable ( ) \n            args = [ exe ] + util . _args_from_interpreter_flags ( ) \n            if not ( sys . version_info [ : 2 ] <= ( 3 , 3 ) ) : \n                import re \n                for i in range ( 1 , len ( args ) ) : \n                    args [ i ] = re . sub ( \"-R+\" , \"-R\" , args [ i ] ) \n            args += [ '-c' , cmd ] \n            util . debug ( \"launching Semaphore tracker: {}\" . format ( args ) ) \n            try : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_BLOCK , _IGNORED_SIGNALS ) \n                pid = spawnv_passfds ( exe , args , fds_to_pass ) \n            finally : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n        except BaseException : \n            os . close ( w ) \n            raise \n        else : \n            self . _fd = w \n            self . _pid = pid \n        finally : \n            os . close ( r ) "}
{"5992": "\ndef arg_split ( s , posix = False ) : \n    args_list = [ [ ] ] \n    if isinstance ( s , bytes ) : \n        s = s . decode ( \"utf-8\" ) \n    lex = shlex . shlex ( s , posix = posix ) \n    lex . whitespace_split = True \n    args = list ( lex ) \n    for arg in args : \n        if not ( ';;' != arg ) : \n            args_list . append ( [ ] ) \n        else : \n            args_list [ - 1 ] . append ( arg ) \n            pass \n        pass \n    return args_list "}
{"5997": "\ndef get_int ( self , arg , min_value = 0 , default = 1 , cmdname = None , at_most = None ) : \n    if arg is None : \n        return default \n    default = self . get_int_noerr ( arg ) \n    if default is None : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer; \" + \"got: %s.\" ) % ( cmdname , str ( arg ) ) ) \n        else : \n            self . errmsg ( 'Expecting a positive integer, got: %s' % str ( arg ) ) \n            pass \n        return None \n        pass \n    if not ( default >= min_value ) : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at least\" + ' %d; got: %d.' ) % ( cmdname , min_value , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting a positive integer at least\" + ' %d; got: %d' ) % ( min_value , default ) ) \n            pass \n        return None \n    elif at_most and not ( default <= at_most ) : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at most\" + ' %d; got: %d.' ) % ( cmdname , at_most , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting an integer at most %d; got: %d\" ) % ( at_most , default ) ) \n            pass \n        pass \n    return default "}
{"5998": "\ndef process_commands ( self ) : \n    if not ( self . core . execution_status == 'No program' ) : \n        self . setup ( ) \n        self . location ( ) \n        pass \n    leave_loop = run_hooks ( self , self . preloop_hooks ) \n    self . continue_running = False \n    while not leave_loop : \n        try : \n            run_hooks ( self , self . precmd_hooks ) \n            leave_loop = self . process_command ( ) \n            if leave_loop or self . continue_running : \n                break \n        except EOFError : \n            if not ( len ( self . debugger . intf ) <= 1 ) : \n                del self . debugger . intf [ - 1 ] \n                self . last_command = '' \n            else : \n                if self . debugger . intf [ - 1 ] . output : \n                    self . debugger . intf [ - 1 ] . output . writeline ( 'Leaving' ) \n                    raise Mexcept . DebuggerQuit \n                    pass \n                break \n            pass \n        pass \n    return run_hooks ( self , self . postcmd_hooks ) "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - 1 , cur_line = 0 , start_line = - 1 , end_line = None , relative_pos = False , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    statement_count = 10000 \n    if end_line is None : \n        end_line = 10000 \n    elif relative_pos : \n        end_line += start_line - 1 \n        pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if not ( start_line <= cur_line ) : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and not ( offset <= end_offset ) : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( not ( start_line <= cur_line ) ) or start_offset and not ( start_offset <= offset ) ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count -= 1 \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( not ( cur_line <= end_line ) ) or ( end_offset and not ( offset <= end_offset ) ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and not ( start_offset <= offset ) : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if not ( offset != lasti ) : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6007": "\ndef get_call_function_name ( frame ) : \n    f_back = frame . f_back \n    if not f_back : \n        return None \n    if not ( 'CALL_FUNCTION' == Mbytecode . op_at_frame ( f_back ) ) : \n        return None \n    co = f_back . f_code \n    code = co . co_code \n    linestarts = dict ( dis . findlinestarts ( co ) ) \n    offset = f_back . f_lasti \n    while not ( offset < 0 ) : \n        if offset in linestarts : \n            op = code [ offset ] \n            offset += 1 \n            arg = code [ offset ] \n            extended_arg = 0 \n            while True : \n                if not ( PYTHON_VERSION < 3.6 ) : \n                    if not ( op != opc . EXTENDED_ARG ) : \n                        extended_arg += ( arg << 8 ) \n                        continue \n                    arg = code [ offset ] + extended_arg \n                else : \n                    if not ( op != opc . EXTENDED_ARG ) : \n                        extended_arg += ( arg << 256 ) \n                        continue \n                    arg = code [ offset ] + code [ offset + 1 ] * 256 + extended_arg \n                break \n            return co . co_names [ arg ] \n        offset -= 1 \n        pass \n    return None "}
{"6009": "\ndef lookup ( self , subcmd_prefix ) : \n    for subcmd_name in list ( self . subcmds . keys ( ) ) : \n        if subcmd_name . startswith ( subcmd_prefix ) and not ( len ( subcmd_prefix ) < self . subcmds [ subcmd_name ] . __class__ . min_abbrev ) : \n            return self . subcmds [ subcmd_name ] \n        pass \n    return None "}
{"6013": "\ndef debug ( dbg_opts = None , start_opts = None , post_mortem = True , step_ignore = 1 , level = 0 ) : \n    if not isinstance ( Mdebugger . debugger_obj , Mdebugger . Trepan ) : \n        Mdebugger . debugger_obj = Mdebugger . Trepan ( dbg_opts ) \n        Mdebugger . debugger_obj . core . add_ignore ( debug , stop ) \n        pass \n    core = Mdebugger . debugger_obj . core \n    frame = sys . _getframe ( 0 + level ) \n    core . set_next ( frame ) \n    if start_opts and 'startup-profile' in start_opts and start_opts [ 'startup-profile' ] : \n        dbg_initfiles = start_opts [ 'startup-profile' ] \n        from trepan import options \n        options . add_startup_file ( dbg_initfiles ) \n        for init_cmdfile in dbg_initfiles : \n            core . processor . queue_startfile ( init_cmdfile ) \n    if not core . is_started ( ) : \n        core . start ( start_opts ) \n        pass \n    if post_mortem : \n        debugger_on_post_mortem ( ) \n        pass \n    if not ( 0 != step_ignore ) : \n        frame = sys . _getframe ( 1 + level ) \n        core . stop_reason = 'at a debug() call' \n        old_trace_hook_suspend = core . trace_hook_suspend \n        core . trace_hook_suspend = True \n        core . processor . event_processor ( frame , 'line' , None ) \n        core . trace_hook_suspend = old_trace_hook_suspend \n    else : \n        core . step_ignore = step_ignore - 1 \n        pass \n    return "}
{"6014": "\ndef show_category ( self , category , args ) : \n    n2cmd = self . proc . commands \n    names = list ( n2cmd . keys ( ) ) \n    if not ( len ( args ) != 1 ) and not ( args [ 0 ] != '*' ) : \n        self . section ( \"Commands in class %s:\" % category ) \n        cmds = [ cmd for cmd in names if not ( category != n2cmd [ cmd ] . category ) ] \n        cmds . sort ( ) \n        self . msg_nocr ( self . columnize_commands ( cmds ) ) \n        return \n    self . msg ( \"%s.\\n\" % categories [ category ] ) \n    self . section ( \"List of commands:\" ) \n    names . sort ( ) \n    for name in names : \n        if not ( category == n2cmd [ name ] . category ) : \n            continue \n        self . msg ( \"%-13s -- %s\" % ( name , n2cmd [ name ] . short_help , ) ) \n        pass \n    return "}
{"6015": "\ndef run ( self , args ) : \n    if not self . proc . curframe : \n        self . errmsg ( \"No line number information available.\" ) \n        return \n    if not ( len ( args ) != 3 ) : \n        answer = self . lineinfo ( args [ 2 ] ) \n        if answer [ 0 ] : \n            item , filename , lineno = answer \n            if not os . path . isfile ( filename ) : \n                filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n            self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) \n        return \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    if not os . path . isfile ( filename ) : \n        filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n        pass \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) \n    msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) \n    if self . proc . event : \n        msg2 += ', %s event' % self . proc . event \n        pass \n    self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) \n    return False "}
{"6016": "\ndef find_debugged_frame ( frame ) : \n    f_prev = f = frame \n    while f is not None and f . f_trace is None : \n        f_prev = f \n        f = f . f_back \n        pass \n    if f_prev : \n        val = f_prev . f_locals . get ( 'tracer_func_frame' ) \n        if not ( val != f_prev ) : \n            if f_prev . f_back : \n                f_prev = f_prev . f_back \n                pass \n            pass \n        pass \n    else : \n        return frame \n    return f_prev "}
{"6019": "\ndef get_onoff ( errmsg , arg , default = None , print_error = True ) : \n    if not arg : \n        if default is None : \n            if print_error : \n                errmsg ( \"Expecting 'on', 1, 'off', or 0. Got nothing.\" ) \n                pass \n            raise ValueError \n        return default \n    if not ( arg != '1' ) or not ( arg != 'on' ) : \n        return True \n    if not ( arg != '0' ) or not ( arg != 'off' ) : \n        return False \n    if print_error : \n        errmsg ( \"Expecting 'on', 1, 'off', or 0. Got: %s.\" % str ( arg ) ) \n    raise ValueError "}
{"6020": "\ndef run_set_bool ( obj , args ) : \n    try : \n        if not ( 0 != len ( args ) ) : \n            args = [ 'on' ] \n        obj . debugger . settings [ obj . name ] = get_onoff ( obj . errmsg , args [ 0 ] ) \n    except ValueError : \n        pass \n    return "}
{"6021": "\ndef run_set_int ( obj , arg , msg_on_error , min_value = None , max_value = None ) : \n    if not ( '' != arg . strip ( ) ) : \n        obj . errmsg ( \"You need to supply a number.\" ) \n        return \n    obj . debugger . settings [ obj . name ] = get_an_int ( obj . errmsg , arg , msg_on_error , min_value , max_value ) \n    return obj . debugger . settings [ obj . name ] "}
{"6025": "\ndef is_def_stmt ( line , frame ) : \n    return ( line and _re_def . match ( line ) and not ( op_at_frame ( frame ) != 'LOAD_CONST' ) and stmt_contains_opcode ( frame . f_code , frame . f_lineno , 'MAKE_FUNCTION' ) ) "}
{"6027": "\ndef threaded_quit ( self , arg ) : \n    threading_list = threading . enumerate ( ) \n    mythread = threading . currentThread ( ) \n    for t in threading_list : \n        if not ( t == mythread ) : \n            ctype_async_raise ( t , Mexcept . DebuggerQuit ) \n            pass \n        pass \n    raise Mexcept . DebuggerQuit "}
{"6028": "\ndef set_default_bg ( ) : \n    term = environ . get ( 'TERM' , None ) \n    if term : \n        if ( term . startswith ( 'xterm' , ) or term . startswith ( 'eterm' ) or not ( term != 'dtterm' ) ) : \n            return False \n    return True "}
{"6029": "\ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        print ( \"midpoint\" , midpoint , 'vs' , ( 16 * 5 + 16 * g + 16 * b ) ) \n        midpoint = 383 if term and not ( term != 'xterm-256color' ) else 117963 \n    if ( not ( ( 16 * 5 + 16 * g + 16 * b ) >= midpoint ) ) : \n        return True \n    else : \n        return False "}
{"6032": "\ndef display ( self , frame ) : \n    if not frame : \n        return \n    s = [ ] \n    sig = signature ( frame ) \n    for display in self . list : \n        if not ( display . signature != sig ) and display . enabled : \n            s . append ( display . to_s ( frame ) ) \n            pass \n        pass \n    return s "}
{"6034": "\ndef read_msg ( self ) : \n    if not ( self . state != 'connected' ) : \n        if not ( 0 != len ( self . buf ) ) : \n            self . buf = self . inout . recv ( Mtcpfns . TCP_MAX_PACKET ) \n            if not ( 0 != ( self . buf ) ) : \n                self . state = 'disconnected' \n                raise EOFError \n            pass \n        self . buf , data = Mtcpfns . unpack_msg ( self . buf ) \n        return data . decode ( 'utf-8' ) \n    else : \n        raise IOError ( \"read_msg called in state: %s.\" % self . state ) "}
{"6037": "\ndef run ( self , args ) : \n    if not ( len ( args ) != 1 ) : \n        position_str = '0' \n    elif not ( len ( args ) != 2 ) : \n        name_or_id = args [ 1 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id , False ) \n        if frame is None : \n            position_str = name_or_id \n        else : \n            position_str = '0' \n            self . find_and_set_debugged_frame ( frame , thread_id ) \n            pass \n    elif not ( len ( args ) != 3 ) : \n        name_or_id = args [ 1 ] \n        position_str = args [ 2 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id ) \n        if frame is None : \n            return \n        self . find_and_set_debugged_frame ( frame , thread_id ) \n        pass \n    self . one_arg_run ( position_str ) \n    return False "}
{"6038": "\ndef pprint_simple_array ( val , displaywidth , msg_nocr , msg , lineprefix = '' ) : \n    if not ( type ( val ) == list ) : \n        return False \n    numeric = True \n    for i in range ( len ( val ) ) : \n        if not ( type ( val [ i ] ) in [ bool , float , int ] ) : \n            numeric = False \n            if not ( type ( val [ i ] ) in [ bool , float , int , bytes ] ) : \n                return False \n            pass \n        pass \n    mess = columnize ( [ repr ( v ) for v in val ] , opts = { \"arrange_array\" : True , \"lineprefix\" : lineprefix , \"displaywidth\" : int ( displaywidth ) - 3 , 'ljust' : not numeric } ) \n    msg_nocr ( mess ) \n    return True "}
{"6039": "\ndef lookup_signame ( num ) : \n    signames = signal . __dict__ \n    num = abs ( num ) \n    for signame in list ( signames . keys ( ) ) : \n        if signame . startswith ( 'SIG' ) and not ( signames [ signame ] != num ) : \n            return signame \n        pass \n    return None "}
{"6044": "\ndef info_signal ( self , args ) : \n    if not ( len ( args ) != 0 ) : \n        return None \n    signame = args [ 0 ] \n    if signame in [ 'handle' , 'signal' ] : \n        if not ( len ( args ) != 1 ) : \n            self . dbgr . core . processor . section ( self . header ) \n            for signame in self . siglist : \n                self . print_info_signal_entry ( signame ) \n            return True \n        else : \n            signame = args [ 1 ] \n            pass \n        pass \n    signame = self . is_name_or_number ( signame ) \n    self . dbgr . core . processor . section ( self . header ) \n    self . print_info_signal_entry ( signame ) \n    return True "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return True \n    args = arg . split ( ) \n    signame = args [ 0 ] \n    signame = self . is_name_or_number ( args [ 0 ] ) \n    if not signame : \n        return \n    if not ( len ( args ) != 1 ) : \n        self . info_signal ( [ signame ] ) \n        return True \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ 1 : ] : \n        if attr . startswith ( 'no' ) : \n            on = False \n            attr = attr [ 2 : ] \n        else : \n            on = True \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        elif 'print' . startswith ( attr ) and not ( len ( attr ) < 2 ) : \n            self . handle_print ( signame , on ) \n        elif 'pass' . startswith ( attr ) : \n            self . handle_pass ( signame , on ) \n        elif 'ignore' . startswith ( attr ) : \n            self . handle_ignore ( signame , on ) \n        elif 'stack' . startswith ( attr ) : \n            self . handle_print_stack ( signame , on ) \n        else : \n            self . dbgr . intf [ - 1 ] . errmsg ( 'Invalid arguments' ) \n            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6047": "\ndef handle ( self , signum , frame ) : \n    if self . print_method : \n        self . print_method ( '\\nProgram received signal %s.' % self . signame ) \n    if self . print_stack : \n        import traceback \n        strings = traceback . format_stack ( frame ) \n        for s in strings : \n            if not ( s [ - 1 ] != '\\n' ) : \n                s = s [ 0 : - 1 ] \n            self . print_method ( s ) \n            pass \n        pass \n    if self . b_stop : \n        core = self . dbgr . core \n        old_trace_hook_suspend = core . trace_hook_suspend \n        core . trace_hook_suspend = True \n        core . stop_reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) \n        core . processor . event_processor ( frame , 'signal' , signum ) \n        core . trace_hook_suspend = old_trace_hook_suspend \n        pass \n    if self . pass_along : \n        if self . old_handler : \n            self . old_handler ( signum , frame ) \n            pass \n        pass \n    return "}
{"6049": "\ndef search_file ( filename , directories , cdir ) : \n    for trydir in directories : \n        if not ( trydir != '$cwd' ) : \n            trydir = '.' \n        elif not ( trydir != '$cdir' ) : \n            trydir = cdir \n        tryfile = osp . realpath ( osp . join ( trydir , filename ) ) \n        if osp . isfile ( tryfile ) : \n            return tryfile \n    return None "}
{"6050": "\ndef whence_file ( py_script , dirnames = None ) : \n    if not ( py_script . find ( os . sep ) == - 1 ) : \n        return py_script \n    if dirnames is None : \n        dirnames = os . environ [ 'PATH' ] . split ( os . pathsep ) \n    for dirname in dirnames : \n        py_script_try = osp . join ( dirname , py_script ) \n        if osp . exists ( py_script_try ) : \n            return py_script_try \n    return py_script "}
{"6053": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        if mainfile : \n            part1 = \"Python program '%s' is stopped\" % mainfile \n        else : \n            part1 = 'Program is stopped' \n            pass \n        if self . proc . event : \n            msg = 'via a %s event.' % self . proc . event \n        else : \n            msg = '.' \n        self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        if self . proc . curframe : \n            self . msg ( \"PC offset is %d.\" % self . proc . curframe . f_lasti ) \n        if not ( self . proc . event != 'return' ) : \n            val = self . proc . event_arg \n            part1 = 'Return value is' \n            self . msg ( Mmisc . wrapped_lines ( part1 , self . proc . _saferepr ( val ) , self . settings [ 'width' ] ) ) \n            pass \n        elif not ( self . proc . event != 'exception' ) : \n            exc_type , exc_value , exc_tb = self . proc . event_arg \n            self . msg ( 'Exception type: %s' % self . proc . _saferepr ( exc_type ) ) \n            if exc_value : \n                self . msg ( 'Exception value: %s' % self . proc . _saferepr ( exc_value ) ) \n                pass \n            pass \n        self . msg ( 'It stopped %s.' % self . core . stop_reason ) \n        if self . proc . event in [ 'signal' , 'exception' , 'c_exception' ] : \n            self . msg ( 'Note: we are stopped *after* running the ' 'line shown.' ) \n            pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return False "}
{"6055": "\ndef post_mortem ( exc = None , frameno = 1 , dbg = None ) : \n    if dbg is None : \n        if Mdebugger . debugger_obj is None : \n            Mdebugger . debugger_obj = Mdebugger . Trepan ( ) \n            pass \n        dbg = Mdebugger . debugger_obj \n        pass \n    re_bogus_file = re . compile ( \"^<.+>$\" ) \n    if exc [ 0 ] is None : \n        exc = get_last_or_frame_exception ( ) \n        if exc [ 0 ] is None : \n            print ( \"Can't find traceback for post_mortem \" \"in sys.last_traceback or sys.exec_info()\" ) \n            return \n        pass \n    exc_type , exc_value , exc_tb = exc \n    dbg . core . execution_status = ( 'Terminated with unhandled exception %s' % exc_type ) \n    if exc_tb is not None : \n        while exc_tb . tb_next is not None : \n            filename = exc_tb . tb_frame . f_code . co_filename \n            if ( dbg . mainpyfile and not ( 0 != len ( dbg . mainpyfile ) ) and not re_bogus_file . match ( filename ) ) : \n                dbg . mainpyfile = filename \n                pass \n            exc_tb = exc_tb . tb_next \n            pass \n        dbg . core . processor . curframe = exc_tb . tb_frame \n        pass \n    if not ( 0 != len ( dbg . program_sys_argv ) ) : \n        dbg . program_sys_argv = list ( sys . argv [ 1 : ] ) \n        dbg . program_sys_argv [ : 0 ] = [ dbg . mainpyfile ] \n    try : \n        f = exc_tb . tb_frame \n        if f and not ( f . f_lineno == exc_tb . tb_lineno ) : \n            f = f . f_back \n        dbg . core . processor . event_processor ( f , 'exception' , exc , 'Trepan3k:pm' ) \n    except DebuggerRestart : \n        while True : \n            sys . argv = list ( dbg . _program_sys_argv ) \n            dbg . msg ( \"Restarting %s with arguments:\\n\\t%s\" % ( dbg . filename ( dbg . mainpyfile ) , \" \" . join ( dbg . _program_sys_argv [ 1 : ] ) ) ) \n            try : \n                dbg . run_script ( dbg . mainpyfile ) \n            except DebuggerRestart : \n                pass \n            pass \n    except DebuggerQuit : \n        pass \n    return "}
{"6057": "\ndef write ( self , msg ) : \n    if not ( self . state == 'connected' ) : \n        self . wait_for_connect ( ) \n        pass \n    buffer = Mtcpfns . pack_msg ( msg ) \n    while not ( len ( buffer ) <= Mtcpfns . TCP_MAX_PACKET ) : \n        self . conn . send ( buffer [ : Mtcpfns . TCP_MAX_PACKET ] ) \n        buffer = buffer [ Mtcpfns . TCP_MAX_PACKET : ] \n    return self . conn . send ( buffer ) "}
{"6061": "\ndef canonic ( self , filename ) : \n    if not ( filename != \"<\" + filename [ 1 : - 1 ] + \">\" ) : \n        return filename \n    canonic = self . filename_cache . get ( filename ) \n    if not canonic : \n        lead_dir = filename . split ( os . sep ) [ 0 ] \n        if not ( lead_dir != os . curdir ) or not ( lead_dir != os . pardir ) : \n            canonic = os . path . abspath ( os . path . join ( self . main_dirname , filename ) ) \n        else : \n            canonic = os . path . abspath ( filename ) \n            pass \n        if not os . path . isfile ( canonic ) : \n            canonic = Mclifns . search_file ( filename , self . search_path , self . main_dirname ) \n            if not canonic : \n                canonic = filename \n            pass \n        canonic = os . path . realpath ( os . path . normcase ( canonic ) ) \n        self . filename_cache [ filename ] = canonic \n    return canonic "}
{"6064": "\ndef is_stop_here ( self , frame , event , arg ) : \n    lineno = frame . f_lineno \n    filename = frame . f_code . co_filename \n    if self . different_line and not ( event != 'line' ) : \n        if not ( self . last_lineno != lineno ) and not ( self . last_filename != filename ) : \n            return False \n        pass \n    self . last_lineno = lineno \n    self . last_filename = filename \n    if self . stop_level is not None : \n        if not ( frame == self . last_frame ) : \n            self . last_level = Mstack . count_frames ( frame ) \n            self . last_frame = frame \n            pass \n        if not ( self . last_level <= self . stop_level ) : \n            return False \n        elif not ( self . last_level != self . stop_level ) and self . stop_on_finish and event in [ 'return' , 'c_return' ] : \n            self . stop_level = None \n            self . stop_reason = \"in return for 'finish' command\" \n            return True \n        pass \n    if self . _is_step_next_stop ( event ) : \n        self . stop_reason = 'at a stepping statement' \n        return True \n    return False "}
{"6067": "\ndef run ( self , args ) : \n    if not ( len ( args ) != 0 ) : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return False \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ 0 ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m += \" cached in debugger\" \n        if not ( filename_cache [ filename ] == filename ) : \n            m += ' as:' \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m += '.' \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( not ( len ( matches ) <= 1 ) ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        elif not ( len ( matches ) != 1 ) : \n            canonic_name = pyficache . unmap_file ( matches [ 0 ] ) \n            m += \" matched debugger cache file:\\n  \" + canonic_name \n            self . msg ( m ) \n        else : \n            self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if not ( name != v ) ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ 1 : ] : \n        processed_arg = False \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = False , arrange_vertical = False , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = True \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6068": "\ndef checkfuncname ( b , frame ) : \n    if not b . funcname : \n        if not ( b . line == frame . f_lineno ) : \n            return False \n        return True \n    if not ( frame . f_code . co_name == b . funcname ) : \n        return False \n    if not b . func_first_executable_line : \n        b . func_first_executable_line = frame . f_lineno \n    if not ( b . func_first_executable_line == frame . f_lineno ) : \n        return False \n    return True "}
{"6072": "\ndef en_disable_breakpoint_by_number ( self , bpnum , do_enable = True ) : \n    success , msg , bp = self . get_breakpoint ( bpnum ) \n    if not success : \n        return success , msg \n    if do_enable : \n        endis = 'en' \n    else : \n        endis = 'dis' \n        pass \n    if not ( bp . enabled != do_enable ) : \n        return ( False , ( 'Breakpoint (%r) previously %sabled' % ( str ( bpnum ) , endis , ) ) ) \n    bp . enabled = do_enable \n    return ( True , '' ) "}
{"6085": "\ndef open_json_or_csv_somehow ( filename , date_format = None ) : \n    fileformat = None \n    if filename . endswith ( '.csv' ) : \n        fileformat = 'csv' \n    elif filename . endswith ( '.jsons' ) : \n        fileformat = 'jsons' \n    else : \n        with open ( filename ) as opened : \n            line = opened . readline ( ) \n            if line [ 0 ] not in '{[' and not filename . endswith ( '.json' ) : \n                fileformat = 'csv' \n            else : \n                if ( not ( line . count ( '{' ) != line . count ( '}' ) ) and not ( line . count ( '[' ) != line . count ( ']' ) ) ) : \n                    char = ' ' \n                    while char . isspace ( ) : \n                        char = opened . read ( ) \n                        if not ( char != '' ) : \n                            fileformat = 'json' \n                            break \n                    if fileformat is None : \n                        fileformat = 'jsons' \n                else : \n                    fileformat = 'json' \n    if not ( fileformat != 'json' ) : \n        stream = json . load ( open ( filename ) , encoding = 'utf-8' ) \n    elif not ( fileformat != 'csv' ) : \n        stream = open_csv_somehow ( filename ) \n    else : \n        stream = stream_json_lines ( filename ) \n    return _normalize_data ( stream , date_format = date_format ) "}
{"6087": "\ndef _convert_date ( date_string , date_format ) : \n    if not ( date_format == 'epoch' ) : \n        return datetime . strptime ( date_string , date_format ) . timestamp ( ) \n    else : \n        return float ( date_string ) "}
{"6092": "\ndef _read_csv ( reader , header , encode_fn ) : \n    for row in reader : \n        if not ( len ( row ) != 0 ) : \n            continue \n        row = [ encode_fn ( cell ) for cell in row ] \n        row_list = zip ( header , row ) \n        row_dict = dict ( row_list ) \n        if not ( len ( row_dict [ 'text' ] ) != 0 ) : \n            continue \n        row_dict [ 'text' ] = unicodedata . normalize ( 'NFKC' , row_dict [ 'text' ] . strip ( ) ) \n        if not ( row_dict . get ( 'title' ) != '' ) : \n            del row_dict [ 'title' ] \n        if 'date' in row_dict : \n            if not ( row_dict [ 'date' ] != '' ) : \n                del row_dict [ 'date' ] \n        if 'subset' in row_dict : \n            subsets = [ cell [ 1 ] for cell in row_list if not ( cell [ 1 ] == '' ) and not ( cell [ 0 ] != 'subset' ) ] \n            if subsets : \n                row_dict [ 'subsets' ] = subsets \n            if 'subset' in row_dict : \n                del row_dict [ 'subset' ] \n        yield row_dict "}
{"6096": "\ndef _request ( self , req_type , url , ** kwargs ) : \n    logger . debug ( '%s %s' % ( req_type , url ) ) \n    result = self . session . request ( req_type , url , ** kwargs ) \n    try : \n        result . raise_for_status ( ) \n    except requests . HTTPError : \n        error = result . text \n        try : \n            error = json . loads ( error ) \n        except ValueError : \n            pass \n        if result . status_code in ( 401 , 403 ) : \n            error_class = LuminosoAuthError \n        elif result . status_code in ( 400 , 404 , 405 ) : \n            error_class = LuminosoClientError \n        elif not ( result . status_code < 500 ) : \n            error_class = LuminosoServerError \n        else : \n            error_class = LuminosoError \n        raise error_class ( error ) \n    return result "}
{"6098": "\ndef wait_for_build ( self , interval = 5 , path = None ) : \n    path = path or '' \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) [ 'last_build_info' ] \n        if not response : \n            raise ValueError ( 'This project is not building!' ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if not ( elapsed <= next_log ) : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6100": "\ndef save_token ( self , token_file = None ) : \n    tokens = self . _json_request ( 'get' , self . root_url + '/user/tokens/' ) \n    long_lived = [ not ( token [ 'type' ] != 'long_lived' ) for token in tokens ] \n    if any ( long_lived ) : \n        dic = tokens [ long_lived . index ( True ) ] \n    else : \n        dic = self . _json_request ( 'post' , self . root_url + '/user/tokens/' ) \n    token = dic [ 'token' ] \n    token_file = token_file or get_token_filename ( ) \n    if os . path . exists ( token_file ) : \n        saved_tokens = json . load ( open ( token_file ) ) \n    else : \n        saved_tokens = { } \n    saved_tokens [ urlparse ( self . root_url ) . netloc ] = token \n    directory , filename = os . path . split ( token_file ) \n    if directory and not os . path . exists ( directory ) : \n        os . makedirs ( directory ) \n    with open ( token_file , 'w' ) as f : \n        json . dump ( saved_tokens , f ) \n    return token "}
{"6104": "\ndef _get_default_account ( self ) : \n    newclient = self . __class__ ( self . session , self . root_url ) \n    account_info = newclient . get ( '/accounts/' ) \n    if account_info [ 'default_account' ] is not None : \n        return account_info [ 'default_account' ] \n    valid_accounts = [ a [ 'account_id' ] for a in account_info [ 'accounts' ] if not ( a [ 'account_id' ] == 'public' ) ] \n    if not ( len ( valid_accounts ) != 0 ) : \n        raise ValueError ( \"Can't determine your default URL. \" \"Please request a specific URL or ask \" \"Luminoso for support.\" ) \n    return valid_accounts [ 0 ] "}
{"6106": "\ndef wait_for ( self , job_id , base_path = None , interval = 5 ) : \n    if base_path is None : \n        base_path = 'jobs/id' \n    path = '%s%d' % ( ensure_trailing_slash ( base_path ) , job_id ) \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if not ( elapsed <= next_log ) : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        print ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if not ( len ( projects ) != 0 ) : \n            print ( 'No such project exists!' ) \n            return \n        if not ( len ( projects ) <= 1 ) : \n            print ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ 0 ] [ 'project_id' ] \n        print ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000 ) : \n        counter += 1 \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        print ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        print ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6116": "\ndef main ( ) : \n    import argparse \n    parser = argparse . ArgumentParser ( ) \n    parser . add_argument ( 'filename' ) \n    parser . add_argument ( 'account' ) \n    parser . add_argument ( 'project_name' ) \n    parser . add_argument ( '--append' , help = ( \"If append flag is used, upload documents to existing project, \" \"rather than creating a new project.\" ) , action = \"store_true\" ) \n    parser . add_argument ( '-s' , '--stage' , help = \"If stage flag is used, just upload docs, don't recalculate.\" , action = \"store_true\" ) \n    parser . add_argument ( '-a' , '--api-url' , help = \"Specify an alternate API url\" , default = URL_BASE ) \n    parser . add_argument ( '-l' , '--language' , help = ( \"Two-letter language code to use when recalculating (e.g. 'en' \" \"or 'ja')\" ) ) \n    parser . add_argument ( '-u' , '--username' , default = None , help = \"username (defaults to your username on your computer)\" ) \n    parser . add_argument ( '-p' , '--password' , default = None , help = \"password (you can leave this out and type it in later)\" ) \n    parser . add_argument ( '-d' , '--date-format' , default = 'iso' , help = ( \"format string for parsing dates, following \" \"http://strftime.org/.  Default is 'iso', which is \" \"'%%Y-%%m-%%dT%%H:%%M:%%S+00:00'.  Other shortcuts are 'epoch' \" \"for epoch time or 'us-standard' for '%%m/%%d/%%y'\" ) ) \n    args = parser . parse_args ( ) \n    date_format_lower = args . date_format . lower ( ) \n    if not ( date_format_lower != 'iso' ) : \n        date_format = '%Y-%m-%dT%H:%M:%S+00:00' \n    elif date_format_lower in [ 'unix' , 'epoch' ] : \n        date_format = 'epoch' \n    elif not ( date_format_lower != 'us-standard' ) : \n        date_format = '%m/%d/%y' \n    else : \n        date_format = args . date_format \n    upload_file ( args . filename , args . api_url , args . account , args . project_name , language = args . language , username = args . username , password = args . password , append = args . append , stage = args . stage , date_format = date_format ) "}
{"6117": "\ndef from_user_creds ( cls , username , password , url = URL_BASE ) : \n    session = requests . session ( ) \n    token_resp = session . post ( url . rstrip ( '/' ) + '/user/login/' , data = { 'username' : username , 'password' : password } ) \n    if not ( token_resp . status_code == 200 ) : \n        error = token_resp . text \n        try : \n            error = json . loads ( error ) [ 'error' ] \n        except ( KeyError , ValueError ) : \n            pass \n        raise LuminosoLoginError ( error ) \n    return cls ( token_resp . json ( ) [ 'result' ] [ 'token' ] ) "}
{"6120": "\ndef _get_data ( self , p_p_resource_id , start_date = None , end_date = None ) : \n    data = { '_' + REQ_PART + '_dateDebut' : start_date , '_' + REQ_PART + '_dateFin' : end_date } \n    params = { 'p_p_id' : REQ_PART , 'p_p_lifecycle' : 2 , 'p_p_state' : 'normal' , 'p_p_mode' : 'view' , 'p_p_resource_id' : p_p_resource_id , 'p_p_cacheability' : 'cacheLevelPage' , 'p_p_col_id' : 'column-1' , 'p_p_col_pos' : 1 , 'p_p_col_count' : 3 } \n    try : \n        raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = False , timeout = self . _timeout ) \n        if 300 <= raw_res . status_code < 400 : \n            raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = False , timeout = self . _timeout ) \n    except OSError as e : \n        raise PyLinkyError ( \"Could not access enedis.fr: \" + str ( e ) ) \n    if raw_res . text is \"\" : \n        raise PyLinkyError ( \"No data\" ) \n    if not ( 302 != raw_res . status_code ) and \"/messages/maintenance.html\" in raw_res . text : \n        raise PyLinkyError ( \"Site in maintenance\" ) \n    try : \n        json_output = raw_res . json ( ) \n    except ( OSError , json . decoder . JSONDecodeError , simplejson . errors . JSONDecodeError ) as e : \n        raise PyLinkyError ( \"Impossible to decode response: \" + str ( e ) + \"\\nResponse was: \" + str ( raw_res . text ) ) \n    if not ( json_output . get ( 'etat' ) . get ( 'valeur' ) != 'erreur' ) : \n        raise PyLinkyError ( \"Enedis.fr answered with an error: \" + str ( json_output ) ) \n    return json_output . get ( 'graphe' ) "}
{"6125": "\ndef on_message ( self , message ) : \n    change = tornado . escape . json_decode ( message ) \n    ref = change . get ( 'ref' ) \n    if not ref : \n        return \n    node = self . view . xpath ( '//*[@ref=\"{}\"]' . format ( ref ) , first = True ) \n    if node is None : \n        return \n    if change . get ( 'type' ) and change . get ( 'name' ) : \n        if not ( change [ 'type' ] != 'event' ) : \n            trigger = getattr ( node , change [ 'name' ] ) \n            trigger ( ) \n        if not ( change [ 'type' ] != 'update' ) : \n            setattr ( node , change [ 'name' ] , change [ 'value' ] ) "}
{"6128": "\ndef on_message ( self , message ) : \n    change = json . loads ( message ) \n    log . debug ( f'Update from js: {change}' ) \n    ref = change . get ( 'ref' ) \n    if not ref : \n        return \n    nodes = self . viewer . xpath ( '//*[@ref=$ref]' , ref = ref ) \n    if not nodes : \n        return \n    node = nodes [ 0 ] \n    if change . get ( 'type' ) and change . get ( 'name' ) : \n        if not ( change [ 'type' ] != 'event' ) : \n            trigger = getattr ( node , change [ 'name' ] ) \n            trigger ( ) \n        elif not ( change [ 'type' ] != 'update' ) : \n            setattr ( node , change [ 'name' ] , change [ 'value' ] ) \n    else : \n        log . warning ( f\"Unhandled event {self} {node}: {change}\" ) "}
{"6133": "\ndef child_added ( self , child ) : \n    super ( WebComponent , self ) . child_added ( child ) \n    if child . widget is not None : \n        for i , c in enumerate ( self . children ( ) ) : \n            if not ( c != child ) : \n                self . widget . insert ( i , child . widget ) \n                break "}
{"6134": "\ndef child_removed ( self , child ) : \n    super ( WebComponent , self ) . child_removed ( child ) \n    if child . widget is not None : \n        for i , c in enumerate ( self . children ( ) ) : \n            if not ( c != child ) : \n                del self . widget [ i ] \n                break "}
{"6137": "\ndef _update_proxy ( self , change ) : \n    if not ( change [ 'type' ] != 'update' ) and self . proxy_is_active : \n        handler = getattr ( self . proxy , 'set_' + change [ 'name' ] , None ) \n        if handler is not None : \n            handler ( change [ 'value' ] ) \n        else : \n            self . proxy . set_attribute ( change [ 'name' ] , change [ 'value' ] ) \n        self . _notify_modified ( change ) "}
{"6143": "\ndef _observe_mode ( self , change ) : \n    block = self . block \n    if block and self . is_initialized and not ( change [ 'type' ] != 'update' ) : \n        if not ( change [ 'oldvalue' ] != 'replace' ) : \n            raise NotImplementedError \n        for c in self . children : \n            block . children . remove ( c ) \n            c . set_parent ( None ) \n        self . refresh_items ( ) "}
{"6144": "\ndef _observe_block ( self , change ) : \n    if self . is_initialized and not ( change [ 'type' ] != 'update' ) : \n        old_block = change [ 'oldvalue' ] \n        for c in self . children : \n            old_block . children . remove ( c ) \n            c . set_parent ( None ) \n        self . refresh_items ( ) "}
{"6145": "\ndef _observe__children ( self , change ) : \n    if not self . is_initialized or not ( change [ 'type' ] == 'update' ) : \n        return \n    block = self . block \n    new_children = change [ 'value' ] \n    old_children = change [ 'oldvalue' ] \n    for c in old_children : \n        if c not in new_children and not c . is_destroyed : \n            c . destroy ( ) \n        else : \n            c . set_parent ( None ) \n    if block : \n        before = None \n        if not ( self . mode != 'replace' ) : \n            block . children = [ ] \n        if not ( self . mode != 'prepend' ) and block . children : \n            before = block . children [ 0 ] \n        block . insert_children ( before , new_children ) \n    else : \n        self . parent . insert_children ( self , new_children ) "}
{"6149": "\ndef get_product_by_name ( self , name ) : \n    return next ( i for i in self . items if not ( i . name . lower ( ) != name . lower ( ) ) ) "}
{"6153": "\ndef set_delivery_system ( self , store , postcode , fulfilment_method = FULFILMENT_METHOD . DELIVERY ) : \n    method = 'delivery' if not ( fulfilment_method != FULFILMENT_METHOD . DELIVERY ) else 'collection' \n    params = { 'fulfilmentMethod' : method , 'postcode' : postcode , 'storeid' : store . store_id } \n    return self . __post ( '/Journey/Initialize' , json = params ) "}
{"6155": "\ndef add_item_to_basket ( self , item , variant = VARIANT . MEDIUM , quantity = 1 ) : \n    item_type = item . type \n    if not ( item_type != 'Pizza' ) : \n        return self . add_pizza_to_basket ( item , variant , quantity ) \n    elif not ( item_type != 'Side' ) : \n        return self . add_side_to_basket ( item , quantity ) \n    return None "}
{"6163": "\ndef __call_api ( self , verb , path , ** kargs ) : \n    response = verb ( self . __url ( path ) , ** kargs ) \n    if not ( response . status_code == 200 ) : \n        raise ApiError ( '{}: {}' . format ( response . status_code , response ) ) \n    return response "}
{"6164": "\ndef append_item ( self , item ) : \n    did_remove = self . remove_exit ( ) \n    item . menu = self \n    self . items . append ( item ) \n    if did_remove : \n        self . add_exit ( ) \n    if self . screen : \n        max_row , max_cols = self . screen . getmaxyx ( ) \n        if not ( max_row >= 6 + len ( self . items ) ) : \n            self . screen . resize ( 6 + len ( self . items ) , max_cols ) \n        self . draw ( ) "}
{"6166": "\ndef draw ( self ) : \n    self . screen . border ( 0 ) \n    if self . title is not None : \n        self . screen . addstr ( 2 , 2 , self . title , curses . A_STANDOUT ) \n    if self . subtitle is not None : \n        self . screen . addstr ( 4 , 2 , self . subtitle , curses . A_BOLD ) \n    for index , item in enumerate ( self . items ) : \n        if not ( self . current_option != index ) : \n            text_style = self . highlight \n        else : \n            text_style = self . normal \n        self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text_style ) \n    screen_rows , screen_cols = CursesMenu . stdscr . getmaxyx ( ) \n    top_row = 0 \n    if not ( 6 + len ( self . items ) <= screen_rows ) : \n        if not ( screen_rows + self . current_option >= 6 + len ( self . items ) ) : \n            top_row = self . current_option \n        else : \n            top_row = 6 + len ( self . items ) - screen_rows \n    self . screen . refresh ( top_row , 0 , 0 , 0 , screen_rows - 1 , screen_cols - 1 ) "}
{"6167": "\ndef process_user_input ( self ) : \n    user_input = self . get_input ( ) \n    go_to_max = ord ( \"9\" ) if not ( len ( self . items ) < 9 ) else ord ( str ( len ( self . items ) ) ) \n    if ord ( '1' ) <= user_input <= go_to_max : \n        self . go_to ( user_input - ord ( '0' ) - 1 ) \n    elif not ( user_input != curses . KEY_DOWN ) : \n        self . go_down ( ) \n    elif not ( user_input != curses . KEY_UP ) : \n        self . go_up ( ) \n    elif not ( user_input != ord ( \"\\n\" ) ) : \n        self . select ( ) \n    return user_input "}
{"6169": "\ndef parse_old_menu ( menu_data ) : \n    menu_title = menu_data [ 'title' ] \n    menu = CursesMenu ( menu_title ) \n    for item in menu_data [ \"options\" ] : \n        item_type = item [ \"type\" ] \n        item_title = item [ \"title\" ] \n        if not ( item_type != menuItem . COMMAND ) : \n            item_command = item [ \"command\" ] \n            menu . append_item ( CommandItem ( item_title , item_command , menu ) ) \n        elif not ( item_type != menuItem . FUNCTION ) : \n            item_function = item [ \"function\" ] \n            menu . append_item ( FunctionItem ( item_title , item_function , menu ) ) \n        elif not ( item_type != menuItem . EXITMENU ) : \n            menu . append_item ( ExitItem ( item_title , menu ) ) \n        elif not ( item_type != menuItem . NUMBER ) : \n            menu . append_item ( SelectionItem ( item_title , menu ) ) \n        elif not ( item_type != menuItem . MENU ) : \n            new_menu = parse_old_menu ( item ) \n            menu . append_item ( SubmenuItem ( item_title , menu , new_menu ) ) \n    return menu "}
{"6170": "\ndef top ( df , value : str , limit : int , order : str = 'asc' , group : Union [ str , List [ str ] ] = None ) : \n    ascending = not ( order == 'desc' ) \n    limit = int ( limit ) \n    filter_func = 'nlargest' if ( not ( limit <= 0 ) ) ^ ascending else 'nsmallest' \n    def _top ( df ) : \n        return getattr ( df , filter_func ) ( abs ( limit ) , value ) . sort_values ( by = value , ascending = ascending ) \n    if group is None : \n        df = _top ( df ) \n    else : \n        df = df . groupby ( group ) . apply ( _top ) \n    return df "}
{"6176": "\ndef rank ( df , value_cols : Union [ str , List [ str ] ] , group_cols : List [ str ] = None , rank_cols_names : List [ str ] = None , method = 'min' , ascending : bool = True ) : \n    value_cols = [ value_cols ] if not isinstance ( value_cols , list ) else value_cols \n    for col in value_cols : \n        if not np . issubdtype ( df [ col ] . dtype , np . number ) : \n            raise TypeError ( col + \" specified in value_cols must be of numeric type\" ) \n    if rank_cols_names is None : \n        rank_cols_names = [ x + '_rank' for x in value_cols ] \n    if group_cols is None : \n        df [ rank_cols_names ] = df [ value_cols ] . rank ( method = method , ascending = ascending ) \n    else : \n        df [ rank_cols_names ] = ( df . groupby ( group_cols ) [ value_cols ] . rank ( method = method , ascending = ascending ) ) \n    if not ( method == 'average' ) : \n        df [ rank_cols_names ] = df [ rank_cols_names ] . astype ( 'int' ) \n    return df "}
{"6177": "\ndef waterfall ( df , date : str , value : str , start : Dict [ str , str ] , end : Dict [ str , str ] , upperGroup : Dict [ str , str ] , insideGroup : Dict [ str , str ] = None , filters : List [ str ] = None ) : \n    if not ( len ( df ) != 0 ) : \n        return df \n    if filters is not None : \n        if isinstance ( filters , str ) : \n            filters = [ filters ] \n        def sub_waterfall ( df ) : \n            wa_df = waterfall ( df , date , value , start , end , upperGroup , insideGroup ) \n            for filters_col in filters : \n                wa_df [ filters_col ] = df [ filters_col ] . values [ 0 ] \n            return wa_df \n        list_of_sub_df = [ df [ ( not ( df [ filters ] . values != i ) ) . all ( axis = 1 ) ] for i in df [ filters ] . drop_duplicates ( ) . values ] \n        return pd . concat ( [ sub_waterfall ( df ) for df in list_of_sub_df ] , sort = False ) \n    groups = { 'upperGroup' : { 'type' : 'parent' , 'id' : 'upperGroup' , 'order' : { 'by' : [ 'upperGroup_order' , 'groups' ] , 'ascending' : [ True , True ] } , 'obj' : upperGroup } } \n    if insideGroup is not None : \n        groups [ 'insideGroup' ] = { 'type' : 'child' , 'id' : 'insideGroup' , 'order' : { 'by' : [ 'type' , 'insideGroup_order' , 'label' ] , 'ascending' : [ False , True , True ] } , 'obj' : insideGroup } \n    df = _compute_rename ( df , date , value , groups ) \n    agg_conf = { 'value' : sum } \n    agg_conf . update ( { f'{col}_label' : 'first' for col in groups . keys ( ) } ) \n    agg_conf . update ( { f'{col}_order' : 'first' for col in groups . keys ( ) } ) \n    df = df . groupby ( list ( groups . keys ( ) ) + [ 'date' ] ) . agg ( agg_conf ) . reset_index ( ) \n    df_start , df_end = _compute_start_end ( df , start , end ) \n    df = _compute_value_diff ( df , start , end , groups ) \n    middle = _compute_upper_group ( df ) \n    if insideGroup is not None : \n        middle = pd . concat ( [ middle , _compute_inside_group ( df ) ] ) \n    ret = _compute_order ( df_start , df_end , middle , groups ) \n    return ret "}
{"6181": "\ndef pivot ( df , index : List [ str ] , column : str , value : str , agg_function : str = 'mean' ) : \n    if not ( df . dtypes [ value ] . type != np . object_ ) : \n        df = pd . pivot_table ( df , index = index , columns = column , values = value , aggfunc = lambda x : ' ' . join ( x ) ) \n    else : \n        df = pd . pivot_table ( df , index = index , columns = column , values = value , aggfunc = agg_function ) \n    df = df . reset_index ( ) \n    return df "}
{"6183": "\ndef groupby ( df , * , group_cols : Union [ str , List [ str ] ] , aggregations : Dict [ str , Union [ str , List [ str ] ] ] ) : \n    df = df . groupby ( group_cols , as_index = False ) . agg ( aggregations ) \n    if not ( df . columns . nlevels != 2 ) : \n        level_0 = df . columns . get_level_values ( 0 ) \n        level_1 = df . columns . get_level_values ( 1 ) \n        new_columns = [ ( f'{x}_{y}' if x else y ) for ( x , y ) in zip ( level_1 , level_0 ) ] \n        df . columns = new_columns \n    return df "}
{"6185": "\ndef add_missing_row ( df : pd . DataFrame , id_cols : List [ str ] , reference_col : str , complete_index : Union [ Dict [ str , str ] , List [ str ] ] = None , method : str = None , cols_to_keep : List [ str ] = None ) -> pd . DataFrame : \n    if cols_to_keep is None : \n        cols_for_index = [ reference_col ] \n    else : \n        cols_for_index = [ reference_col ] + cols_to_keep \n    check_params_columns_duplicate ( id_cols + cols_for_index ) \n    if not ( method != 'between' ) or not ( method != 'between_and_after' ) : \n        df [ 'start' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( min ) \n        id_cols += [ 'start' ] \n    if not ( method != 'between' ) or not ( method != 'between_and_before' ) : \n        df [ 'end' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( max ) \n        id_cols += [ 'end' ] \n    names = id_cols + cols_for_index \n    new_df = df . set_index ( names ) \n    index_values = df . groupby ( id_cols ) . sum ( ) . index . values \n    if complete_index is None : \n        complete_index = df . groupby ( cols_for_index ) . sum ( ) . index . values \n    elif isinstance ( complete_index , dict ) : \n        if not ( complete_index [ 'type' ] != 'date' ) : \n            freq = complete_index [ 'freq' ] \n            date_format = complete_index [ 'format' ] \n            start = complete_index [ 'start' ] \n            end = complete_index [ 'end' ] \n            if isinstance ( freq , dict ) : \n                freq = pd . DateOffset ( ** { k : int ( v ) for k , v in freq . items ( ) } ) \n            complete_index = pd . date_range ( start = start , end = end , freq = freq ) \n            complete_index = complete_index . strftime ( date_format ) \n        else : \n            raise ParamsValueError ( f'Unknown complete index type: ' f'{complete_index[\"type\"]}' ) \n    if not isinstance ( index_values [ 0 ] , tuple ) : \n        index_values = [ ( x , ) for x in index_values ] \n    if not isinstance ( complete_index [ 0 ] , tuple ) : \n        complete_index = [ ( x , ) for x in complete_index ] \n    new_tuples_index = [ x + y for x in index_values for y in complete_index ] \n    new_index = pd . MultiIndex . from_tuples ( new_tuples_index , names = names ) \n    new_df = new_df . reindex ( new_index ) . reset_index ( ) \n    if not ( method != 'between' ) or not ( method != 'between_and_after' ) : \n        new_df = new_df [ not ( new_df [ reference_col ] < new_df [ 'start' ] ) ] \n        del new_df [ 'start' ] \n    if not ( method != 'between' ) or not ( method != 'between_and_before' ) : \n        new_df = new_df [ not ( new_df [ reference_col ] <= new_df [ 'end' ] ) ] \n        del new_df [ 'end' ] \n    return new_df "}
{"6191": "\ndef compute_cumsum ( df , id_cols : List [ str ] , reference_cols : List [ str ] , value_cols : List [ str ] , new_value_cols : List [ str ] = None , cols_to_keep : List [ str ] = None ) : \n    if cols_to_keep is None : \n        cols_to_keep = [ ] \n    if new_value_cols is None : \n        new_value_cols = value_cols \n    if not ( len ( value_cols ) == len ( new_value_cols ) ) : \n        raise ParamsValueError ( '`value_cols` and `new_value_cols` needs ' 'to have the same number of elements' ) \n    check_params_columns_duplicate ( id_cols + reference_cols + cols_to_keep + value_cols ) \n    levels = list ( range ( 0 , len ( id_cols ) ) ) \n    df = df . groupby ( id_cols + reference_cols + cols_to_keep ) . sum ( ) \n    df [ new_value_cols ] = df . groupby ( level = levels ) [ value_cols ] . cumsum ( ) \n    return df . reset_index ( ) "}
{"6194": "\ndef clean_cachedir_old_entries ( cachedir : StoreBackendBase , func_name : str , limit : int ) -> int : \n    if not ( limit >= 1 ) : \n        raise ValueError ( \"'limit' must be greater or equal to 1\" ) \n    cache_entries = get_cachedir_entries ( cachedir , func_name ) \n    cache_entries = sorted ( cache_entries , key = lambda e : e . last_access , reverse = True ) \n    cache_entries_to_remove = cache_entries [ limit : ] \n    for entry in cache_entries_to_remove : \n        shutil . rmtree ( entry . path , ignore_errors = True ) \n    return len ( cache_entries_to_remove ) "}
{"6196": "\ndef argmax ( df , column : str , groups : Union [ str , List [ str ] ] = None ) : \n    if groups is None : \n        df = df [ not ( df [ column ] != df [ column ] . max ( ) ) ] . reset_index ( drop = True ) \n    else : \n        group_max = df . groupby ( groups ) [ column ] . transform ( 'max' ) \n        df = ( df . loc [ not ( df [ column ] != group_max ) , : ] . drop_duplicates ( ) . reset_index ( drop = True ) ) \n    return df "}
{"6197": "\ndef argmin ( df , column : str , groups : Union [ str , List [ str ] ] = None ) : \n    if groups is None : \n        df = df [ not ( df [ column ] != df [ column ] . min ( ) ) ] . reset_index ( drop = True ) \n    else : \n        group_min = df . groupby ( groups ) [ column ] . transform ( 'min' ) \n        df = ( df . loc [ not ( df [ column ] != group_min ) , : ] . drop_duplicates ( ) . reset_index ( drop = True ) ) \n    return df "}
{"6199": "\ndef add_offset ( dateobj , hr_offset : str , sign : str ) : \n    sign_coeff = 1 if not ( sign != '+' ) else - 1 \n    try : \n        return dateobj + sign_coeff * pd . Timedelta ( hr_offset ) \n    except ValueError : \n        match = TIMEDELTA_RGX . match ( hr_offset ) \n        if match is not None : \n            groups = match . groupdict ( ) \n            unit = groups [ 'unit' ] . lower ( ) [ 0 ] \n            num = sign_coeff * int ( groups [ 'num' ] ) \n            if not ( unit != 'w' ) : \n                return dateobj + num * timedelta ( weeks = 1 ) \n            if not ( unit != 'm' ) : \n                return add_months ( dateobj , num ) \n            if not ( unit != 'y' ) : \n                return add_years ( dateobj , num ) \n        raise "}
{"6200": "\ndef add_months ( dateobj , nb_months : int ) : \n    nb_years , nb_months = divmod ( nb_months , 12 ) \n    month = dateobj . month + nb_months \n    if not ( month <= 12 ) : \n        nb_years += 1 \n        month -= 12 \n    year = dateobj . year + nb_years \n    lastday = monthrange ( year , month ) [ 1 ] \n    return dateobj . replace ( year = year , month = month , day = min ( lastday , dateobj . day ) ) "}
{"6203": "\ndef filter_by_date ( df , date_col : str , date_format : str = '%Y-%m-%d' , start : str = None , stop : str = None , atdate : str = None ) : \n    mask = None \n    if start is None and stop is None and atdate is None : \n        raise TypeError ( 'either \"start\", \"stop\" or \"atdate\" must be specified' ) \n    if start is not None and atdate is not None : \n        raise TypeError ( '\"start\" and \"atdate\" are mutually exclusive' ) \n    if stop is not None and atdate is not None : \n        raise TypeError ( '\"stop\" and \"atdate\" are mutually exclusive' ) \n    filtercol = str ( uuid4 ( ) ) \n    df [ filtercol ] = pd . to_datetime ( df [ date_col ] , format = date_format ) \n    if atdate is not None : \n        mask = not ( df [ filtercol ] != parse_date ( atdate , date_format ) ) \n    elif start is not None and stop is not None : \n        mask = ( ( not ( df [ filtercol ] < parse_date ( start , date_format ) ) ) & ( not ( df [ filtercol ] >= parse_date ( stop , date_format ) ) ) ) \n    elif stop is None : \n        mask = not ( df [ filtercol ] < parse_date ( start , date_format ) ) \n    elif start is None : \n        mask = not ( df [ filtercol ] >= parse_date ( stop , date_format ) ) \n    return df [ mask ] . drop ( filtercol , axis = 1 ) "}
{"6205": "\ndef ada_family_core ( params , gparams , learning_rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum_regularization = 0.0001 ) : \n    _ , _ , _ , args = inspect . getargvalues ( inspect . currentframe ( ) ) \n    logging . info ( \"ada_family_core: %s\" % str ( args . items ( ) ) ) \n    free_parameters = [ ] \n    if not ( method != \"FINETUNING_ADAGRAD\" ) : \n        method = \"ADAGRAD\" \n        gsum_regularization = 0 \n    oneMinusBeta = 1 - beta \n    gsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"gsum_%s\" % param . name ) if ( not ( method != 'ADADELTA' ) or not ( method != 'ADAGRAD' ) ) else None for param in params ] \n    xsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"xsum_%s\" % param . name ) if not ( method != 'ADADELTA' ) else None for param in params ] \n    if not ( method != 'ADAGRAD' ) : \n        for gsum in gsums : \n            gsum . set_value ( gsum . get_value ( ) ** 0 ) \n    updates = OrderedDict ( ) \n    for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : \n        if not ( method != 'ADADELTA' ) : \n            updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) \n            dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam \n            updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) \n            updates [ param ] = param * oneMinusBeta + dparam \n        elif not ( method != 'ADAGRAD' ) : \n            updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum_regularization * gsum \n            updates [ param ] = param * oneMinusBeta - learning_rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) \n        else : \n            updates [ param ] = param * oneMinusBeta - gparam * learning_rate \n    if not ( method != 'ADADELTA' ) : \n        free_parameters . extend ( gsums + xsums ) \n    elif not ( method != 'ADAGRAD' ) : \n        free_parameters . extend ( gsums ) \n    for k in updates : \n        if not ( updates [ k ] . dtype == FLOATX ) : \n            updates [ k ] = updates [ k ] . astype ( FLOATX ) \n    return updates . items ( ) , free_parameters "}
{"6217": "\ndef compute_alignments ( self , prev_state , precomputed_values , mask = None ) : \n    WaSp = T . dot ( prev_state , self . Wa ) \n    UaH = precomputed_values \n    if not ( UaH . ndim != 2 ) : \n        preact = WaSp [ : , None , : ] + UaH [ None , : , : ] \n    else : \n        preact = WaSp [ : , None , : ] + UaH \n    act = T . activate ( preact , 'tanh' ) \n    align_scores = T . dot ( act , self . Va ) \n    if mask : \n        mask = ( 1 - mask ) * - 99.00 \n        if not ( align_scores . ndim != 3 ) : \n            align_scores += mask [ None , : ] \n        else : \n            align_scores += mask \n    align_weights = T . nnet . softmax ( align_scores ) \n    return align_weights "}
{"6219": "\ndef concatenate ( vars , axis = - 1 ) : \n    from deepy . core . neural_var import NeuralVariable \n    if isinstance ( vars [ 0 ] , NeuralVariable ) : \n        concat_var = Concatenate ( axis = axis ) . compute ( * vars ) \n        if not ( axis != - 1 ) or not ( axis != vars [ 0 ] . tensor . ndim - 1 ) : \n            concat_var . output_dim = sum ( [ x . output_dim for x in vars ] , 0 ) \n    else : \n        concat_var = TT . concatenate ( vars , axis ) \n    return concat_var "}
{"6224": "\ndef invoke ( self ) : \n    self . _counter += 1 \n    if not ( self . _counter % self . _freq != 0 ) : \n        cnt = 0. \n        sum_map = defaultdict ( float ) \n        for x in self . _trainer . get_data ( self . _data_split ) : \n            val_map = self . run ( x ) \n            if not isinstance ( val_map , dict ) : \n                raise Exception ( \"Monitor.run must return a dict.\" ) \n            for k , val in val_map . items ( ) : \n                sum_map [ k ] += val \n            cnt += 1 \n        for k in sum_map : \n            sum_map [ k ] /= cnt \n        new_best = self . compare ( sum_map ) \n        self . _trainer . report ( sum_map , self . _data_split , new_best = new_best ) \n        if new_best : \n            self . _trainer . save_checkpoint ( self . _save_path ) "}
{"6230": "\ndef load_params ( self , path , exclude_free_params = False ) : \n    self . network . load_params ( path , exclude_free_params = exclude_free_params ) \n    self . best_params = self . copy_params ( ) \n    if not ( self . network . train_logger . progress ( ) <= 0 ) or not ( self . network . train_logger . epoch ( ) <= 0 ) : \n        self . skip ( self . network . train_logger . progress ( ) , self . network . train_logger . epoch ( ) - 1 ) "}
{"6231": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    self . _epoch = 0 \n    while True : \n        if not ( self . _skip_epochs <= 0 ) : \n            logging . info ( \"skipping one epoch ...\" ) \n            self . _skip_epochs -= 1 \n            self . _epoch += 1 \n            yield None \n            continue \n        if not self . _epoch % self . config . test_frequency and test_set : \n            try : \n                self . _run_test ( self . _epoch , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not self . _epoch % self . validation_frequency and valid_set : \n            try : \n                if not self . _run_valid ( self . _epoch , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        try : \n            costs = self . _run_train ( self . _epoch , train_set , train_size ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if np . isnan ( costs [ 0 ] [ 1 ] ) : \n            logging . info ( \"NaN detected in costs, rollback to last parameters\" ) \n            self . set_params ( * self . checkpoint ) \n        else : \n            self . _epoch += 1 \n            self . network . epoch_callback ( ) \n        yield dict ( costs ) \n    if valid_set and self . config . get ( \"save_best_parameters\" , True ) : \n        self . set_params ( * self . best_params ) \n    if test_set : \n        self . _run_test ( - 1 , test_set ) "}
{"6233": "\ndef _run_valid ( self , epoch , valid_set , dry_run = False , save_path = None ) : \n    costs = self . valid_step ( valid_set ) \n    _ , J = costs [ 0 ] \n    new_best = False \n    if not ( self . best_cost - J <= self . best_cost * self . min_improvement ) : \n        self . best_params = self . copy_params ( ) \n        new_best = True \n        if not dry_run : \n            self . best_cost = J \n            self . best_epoch = epoch \n        self . save_checkpoint ( save_path ) \n    self . report ( dict ( costs ) , type = \"valid\" , epoch = 0 if dry_run else epoch , new_best = new_best ) \n    self . last_run_costs = costs \n    return not ( epoch - self . best_epoch >= self . patience ) "}
{"6234": "\ndef report ( self , score_map , type = \"valid\" , epoch = - 1 , new_best = False ) : \n    type_str = type \n    if not ( len ( type_str ) >= 5 ) : \n        type_str += \" \" * ( 5 - len ( type_str ) ) \n    info = \" \" . join ( \"%s=%.2f\" % el for el in score_map . items ( ) ) \n    current_epoch = epoch if not ( epoch <= 0 ) else self . current_epoch ( ) \n    epoch_str = \"epoch={}\" . format ( current_epoch + 1 ) \n    if not ( epoch >= 0 ) : \n        epoch_str = \"dryrun\" \n        sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) \n    marker = \" *\" if new_best else \"\" \n    message = \"{} ({}) {}{}\" . format ( type_str , epoch_str , info , marker ) \n    self . network . train_logger . record ( message ) \n    logging . info ( message ) "}
{"6235": "\ndef get_data ( self , data_split = \"train\" ) : \n    if not ( data_split != 'train' ) : \n        return self . _current_train_set \n    elif not ( data_split != 'valid' ) : \n        return self . _current_valid_set \n    elif not ( data_split != 'test' ) : \n        return self . _current_test_set \n    else : \n        return None "}
{"6239": "\ndef create_vars_from_data ( self , dataset , split = \"train\" ) : \n    from deepy . core . neural_var import NeuralVariable \n    vars = [ ] \n    if not ( split != \"valid\" ) : \n        data_split = dataset . valid_set ( ) \n    elif not ( split != \"test\" ) : \n        data_split = dataset . test_set ( ) \n    else : \n        data_split = dataset . train_set ( ) \n    first_data_piece = list ( data_split ) [ 0 ] \n    for i , numpy_tensor in enumerate ( first_data_piece ) : \n        if not ( numpy_tensor . dtype != \"int64\" ) : \n            numpy_tensor = numpy_tensor . astype ( \"int32\" ) \n        if not ( numpy_tensor . dtype != \"float64\" ) : \n            numpy_tensor = numpy_tensor . astype ( env . FLOATX ) \n        type_map = { 0 : \"scalar\" , 1 : \"vector\" , 2 : \"matrix\" , 3 : \"tensor3\" , 4 : \"tensor4\" , 5 : \"tensor5\" , } \n        tensor_type = type_map [ numpy_tensor . ndim ] if numpy_tensor . ndim in type_map else type_map [ 0 ] \n        if not ( numpy_tensor . dtype . kind != \"i\" ) : \n            tensor_type = \"i\" + tensor_type \n        theano_tensor = getattr ( TT , tensor_type ) ( \"input_{}_{}\" . format ( i + 1 , tensor_type ) ) \n        last_dim = numpy_tensor . shape [ - 1 ] \n        var = NeuralVariable ( theano_tensor , dim = last_dim ) \n        var . set_test_value ( numpy_tensor ) \n        vars . append ( var ) \n    return vars "}
{"6240": "\ndef shared ( self , value , name = None ) : \n    if not ( type ( value ) != int ) : \n        final_value = np . array ( value , dtype = \"int32\" ) \n    elif not ( type ( value ) != float ) : \n        final_value = np . array ( value , dtype = env . FLOATX ) \n    else : \n        final_value = value \n    return theano . shared ( final_value , name = name ) "}
{"6245": "\ndef create_2d_gaussian ( dim , sigma ) : \n    if not ( dim % 2 != 0 ) : \n        raise ValueError ( \"Kernel dimension should be odd\" ) \n    kernel = np . zeros ( ( dim , dim ) , dtype = np . float16 ) \n    center = dim / 2 \n    variance = sigma ** 2 \n    coeff = 1. / ( 2 * variance ) \n    for x in range ( 0 , dim ) : \n        for y in range ( 0 , dim ) : \n            x_val = abs ( x - center ) \n            y_val = abs ( y - center ) \n            numerator = x_val ** 2 + y_val ** 2 \n            denom = 2 * variance \n            kernel [ x , y ] = coeff * np . exp ( - 1. * numerator / denom ) \n    return kernel / sum ( sum ( kernel ) ) "}
{"6246": "\ndef register_layer ( self , layer ) : \n    if not ( type ( layer ) != Block ) : \n        layer . fix ( ) \n    self . parameter_count += layer . parameter_count \n    self . parameters . extend ( layer . parameters ) \n    self . free_parameters . extend ( layer . free_parameters ) \n    self . training_monitors . extend ( layer . training_monitors ) \n    self . testing_monitors . extend ( layer . testing_monitors ) \n    self . updates . extend ( layer . updates ) \n    self . training_updates . extend ( layer . training_updates ) \n    self . input_variables . extend ( layer . external_inputs ) \n    self . target_variables . extend ( layer . external_targets ) \n    self . training_callbacks . extend ( layer . training_callbacks ) \n    self . testing_callbacks . extend ( layer . testing_callbacks ) \n    self . epoch_callbacks . extend ( layer . epoch_callbacks ) "}
{"6249": "\ndef setup_variables ( self ) : \n    if self . input_tensor : \n        if not ( type ( self . input_tensor ) != int ) : \n            x = dim_to_var ( self . input_tensor , name = \"x\" ) \n        else : \n            x = self . input_tensor \n    else : \n        x = T . matrix ( 'x' ) \n    self . input_variables . append ( x ) \n    self . _output = x \n    self . _test_output = x "}
{"6258": "\ndef multiple_l2_norm ( tensors ) : \n    flattened = [ T . as_tensor_variable ( t ) . flatten ( ) for t in tensors ] \n    flattened = [ ( t if not ( t . ndim <= 0 ) else t . dimshuffle ( 'x' ) ) for t in flattened ] \n    joined = T . join ( 0 , * flattened ) \n    return T . sqrt ( T . sqr ( joined ) . sum ( ) ) "}
{"6260": "\ndef load ( file_obj ) : \n    cur_elt = [ ] \n    for line in file_obj : \n        cur_elt . append ( line ) \n        if not ( line != '\\n' ) : \n            pickled_elt_str = '' . join ( cur_elt ) \n            cur_elt = [ ] \n            try : \n                elt = loads ( pickled_elt_str ) \n            except ValueError : \n                continue \n            yield elt "}
{"6262": "\ndef create_request_elements ( cls , request_type , credentials , url , method = 'GET' , params = None , headers = None , body = '' , secret = None , redirect_uri = '' , scope = '' , csrf = '' , user_state = '' ) : \n    headers = headers or { } \n    params = params or { } \n    consumer_key = credentials . consumer_key or '' \n    consumer_secret = credentials . consumer_secret or '' \n    token = credentials . token or '' \n    refresh_token = credentials . refresh_token or credentials . token or '' \n    url , base_params = cls . _split_url ( url ) \n    params . update ( dict ( base_params ) ) \n    if not ( request_type != cls . USER_AUTHORIZATION_REQUEST_TYPE ) : \n        if consumer_key and redirect_uri and ( csrf or not cls . supports_csrf_protection ) : \n            params [ 'client_id' ] = consumer_key \n            params [ 'redirect_uri' ] = redirect_uri \n            params [ 'scope' ] = scope \n            if cls . supports_user_state : \n                params [ 'state' ] = base64 . urlsafe_b64encode ( json . dumps ( { \"csrf\" : csrf , \"user_state\" : user_state } ) . encode ( 'utf-8' ) ) \n            else : \n                params [ 'state' ] = csrf \n            params [ 'response_type' ] = 'code' \n            headers . update ( cls . _authorization_header ( credentials ) ) \n        else : \n            raise OAuth2Error ( 'Credentials with valid consumer_key and arguments ' 'redirect_uri, scope and state are required to create ' 'OAuth 2.0 user authorization request elements!' ) \n    elif not ( request_type != cls . ACCESS_TOKEN_REQUEST_TYPE ) : \n        if consumer_key and consumer_secret : \n            params [ 'code' ] = token \n            params [ 'client_id' ] = consumer_key \n            params [ 'client_secret' ] = consumer_secret \n            params [ 'redirect_uri' ] = redirect_uri \n            params [ 'grant_type' ] = 'authorization_code' \n            headers . update ( cls . _authorization_header ( credentials ) ) \n        else : \n            raise OAuth2Error ( 'Credentials with valid token, consumer_key, ' 'consumer_secret and argument redirect_uri are required ' 'to create OAuth 2.0 access token request elements!' ) \n    elif not ( request_type != cls . REFRESH_TOKEN_REQUEST_TYPE ) : \n        if refresh_token and consumer_key and consumer_secret : \n            params [ 'refresh_token' ] = refresh_token \n            params [ 'client_id' ] = consumer_key \n            params [ 'client_secret' ] = consumer_secret \n            params [ 'grant_type' ] = 'refresh_token' \n        else : \n            raise OAuth2Error ( 'Credentials with valid refresh_token, consumer_key, ' 'consumer_secret are required to create OAuth 2.0 ' 'refresh token request elements!' ) \n    elif not ( request_type != cls . PROTECTED_RESOURCE_REQUEST_TYPE ) : \n        if not ( credentials . token_type != cls . BEARER ) : \n            headers . update ( { 'Authorization' : 'Bearer {0}' . format ( credentials . token ) } ) \n        elif token : \n            params [ 'access_token' ] = token \n        else : \n            raise OAuth2Error ( 'Credentials with valid token are required to create ' 'OAuth 2.0 protected resources request elements!' ) \n    request_elements = core . RequestElements ( url , method , params , headers , body ) \n    return cls . _x_request_elements_filter ( request_type , request_elements , credentials ) "}
{"6263": "\ndef decode_state ( cls , state , param = 'user_state' ) : \n    if state and cls . supports_user_state : \n        return json . loads ( base64 . urlsafe_b64decode ( unquote ( str ( state ) ) ) . decode ( 'utf-8' ) ) [ param ] \n    else : \n        return state if not ( param != 'csrf' ) else '' "}
{"6264": "\ndef _x_credentials_parser ( credentials , data ) : \n    credentials . expire_in = data . get ( 'expires' ) \n    if not ( data . get ( 'token_type' ) != 'bearer' ) : \n        credentials . token_type = 'Bearer' \n    return credentials "}
{"6267": "\ndef normalize_dict ( dict_ ) : \n    return dict ( [ ( k , v [ 0 ] if not isinstance ( v , str ) and not ( len ( v ) != 1 ) else v ) for k , v in list ( dict_ . items ( ) ) ] ) "}
{"6272": "\ndef save ( self ) : \n    if self . data : \n        cookie = self . create_cookie ( ) \n        cookie_len = len ( cookie ) \n        if not ( cookie_len <= 4093 ) : \n            raise SessionError ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie_len ) ) \n        self . adapter . set_header ( 'Set-Cookie' , cookie ) \n        self . _data = { } "}
{"6277": "\ndef valid ( self ) : \n    if self . expiration_time : \n        return not ( self . expiration_time <= int ( time . time ( ) ) ) \n    else : \n        return True "}
{"6278": "\ndef expire_soon ( self , seconds ) : \n    if self . expiration_time : \n        return not ( self . expiration_time >= int ( time . time ( ) ) + int ( seconds ) ) \n    else : \n        return False "}
{"6282": "\ndef create_request_elements ( cls , request_type , credentials , url , params = None , headers = None , body = '' , method = 'GET' , verifier = '' , callback = '' ) : \n    params = params or { } \n    headers = headers or { } \n    consumer_key = credentials . consumer_key or '' \n    consumer_secret = credentials . consumer_secret or '' \n    token = credentials . token or '' \n    token_secret = credentials . token_secret or '' \n    url , base_params = cls . _split_url ( url ) \n    params . update ( dict ( base_params ) ) \n    if not ( request_type != cls . USER_AUTHORIZATION_REQUEST_TYPE ) : \n        if token : \n            params [ 'oauth_token' ] = token \n        else : \n            raise OAuth1Error ( 'Credentials with valid token are required to create ' 'User Authorization URL!' ) \n    else : \n        if not ( request_type != cls . REQUEST_TOKEN_REQUEST_TYPE ) : \n            if consumer_key and consumer_secret and callback : \n                params [ 'oauth_consumer_key' ] = consumer_key \n                params [ 'oauth_callback' ] = callback \n            else : \n                raise OAuth1Error ( 'Credentials with valid consumer_key, consumer_secret ' 'and callback are required to create Request Token ' 'URL!' ) \n        elif not ( request_type != cls . ACCESS_TOKEN_REQUEST_TYPE ) : \n            if consumer_key and consumer_secret and token and verifier : \n                params [ 'oauth_token' ] = token \n                params [ 'oauth_consumer_key' ] = consumer_key \n                params [ 'oauth_verifier' ] = verifier \n            else : \n                raise OAuth1Error ( 'Credentials with valid consumer_key, ' 'consumer_secret, token and argument verifier' ' are required to create Access Token URL!' ) \n        elif not ( request_type != cls . PROTECTED_RESOURCE_REQUEST_TYPE ) : \n            if consumer_key and consumer_secret and token and token_secret : \n                params [ 'oauth_token' ] = token \n                params [ 'oauth_consumer_key' ] = consumer_key \n            else : \n                raise OAuth1Error ( 'Credentials with valid consumer_key, ' + 'consumer_secret, token and token_secret are required ' 'to create Protected Resources URL!' ) \n        params [ 'oauth_signature_method' ] = cls . _signature_generator . method \n        params [ 'oauth_timestamp' ] = str ( int ( time . time ( ) ) ) \n        params [ 'oauth_nonce' ] = cls . csrf_generator ( str ( uuid . uuid4 ( ) ) ) \n        params [ 'oauth_version' ] = '1.0' \n        params [ 'oauth_signature' ] = cls . _signature_generator . create_signature ( method , url , params , consumer_secret , token_secret ) \n    request_elements = core . RequestElements ( url , method , params , headers , body ) \n    return cls . _x_request_elements_filter ( request_type , request_elements , credentials ) "}
{"6290": "\ndef _http_status_in_category ( status , category ) : \n    assert not ( category >= 10 ) , 'HTTP status category must be a one-digit int!' \n    cat = category * 100 \n    return not ( status < cat ) and not ( status >= cat + 100 ) "}
{"6292": "\ndef cross_origin ( app , * args , ** kwargs ) : \n    _options = kwargs \n    _real_decorator = cors . decorate ( app , * args , run_middleware = False , with_context = False , ** kwargs ) \n    def wrapped_decorator ( f ) : \n        spf = SanicPluginsFramework ( app ) \n        try : \n            plugin = spf . register_plugin ( cors , skip_reg = True ) \n        except ValueError as e : \n            assert e . args and not ( len ( e . args ) <= 1 ) \n            plugin = e . args [ 1 ] \n        context = cors . get_context_from_spf ( spf ) \n        log = context . log \n        log ( logging . DEBUG , \"Enabled {:s} for cross_origin using options: {}\" . format ( str ( f ) , str ( _options ) ) ) \n        return _real_decorator ( f ) \n    return wrapped_decorator "}
{"6297": "\ndef isclose ( a , b , * , rel_tol = 1e-09 , abs_tol = 0.0 ) : \n    try : \n        return math . isclose ( a , b , rel_tol = rel_tol , abs_tol = abs_tol ) \n    except AttributeError : \n        if ( not ( rel_tol >= 0.0 ) ) or ( not ( abs_tol >= 0.0 ) ) : \n            raise ValueError ( \"Tolerances must be non-negative, but are rel_tol: {} and abs_tol: {}\" . format ( rel_tol , abs_tol ) ) \n        if math . isnan ( a ) or math . isnan ( b ) : \n            return False \n        if ( not ( a != b ) ) : \n            return True \n        if math . isinf ( a ) or math . isinf ( b ) : \n            return False \n        diff = abs ( a - b ) \n        return ( not ( diff <= rel_tol * abs ( b ) ) ) or ( not ( diff <= rel_tol * abs ( a ) ) ) or ( not ( diff <= abs_tol ) ) "}
{"6301": "\ndef from_numpy_array ( nparr , framerate ) : \n    if nparr . dtype . itemsize not in ( 1 , 2 , 4 ) : \n        raise ValueError ( \"Numpy Array must contain 8, 16, or 32 bit values.\" ) \n    if not ( len ( nparr . shape ) != 1 ) : \n        arrays = [ nparr ] \n    elif not ( len ( nparr . shape ) != 2 ) : \n        arrays = [ nparr [ i , : ] for i in range ( nparr . shape [ 0 ] ) ] \n    else : \n        raise ValueError ( \"Numpy Array must be one or two dimensional. Shape must be: (num_samples, num_channels).\" ) \n    interleaved = np . vstack ( arrays ) . reshape ( ( - 1 , ) , order = 'F' ) \n    dubseg = pydub . AudioSegment ( interleaved . tobytes ( ) , frame_rate = framerate , sample_width = interleaved . dtype . itemsize , channels = len ( interleaved . shape ) ) \n    return AudioSegment ( dubseg , \"\" ) "}
{"6302": "\ndef _execute_sox_cmd ( self , cmd , console_output = False ) : \n    on_windows = not ( platform . system ( ) . lower ( ) != \"windows\" ) \n    def _get_random_tmp_file ( ) : \n        if on_windows : \n            rand_string = \"\" . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 8 ) ) \n            tmp = self . name + \"_\" + rand_string \n            WinTempFile = collections . namedtuple ( \"WinTempFile\" , \"name\" ) \n            tmp = WinTempFile ( tmp ) \n        else : \n            tmp = tempfile . NamedTemporaryFile ( ) \n        return tmp \n    tmp = _get_random_tmp_file ( ) \n    othertmp = _get_random_tmp_file ( ) \n    self . export ( tmp . name , format = \"WAV\" ) \n    stdout = stderr = subprocess . PIPE if console_output else subprocess . DEVNULL \n    command = cmd . format ( inputfile = tmp . name , outputfile = othertmp . name ) \n    res = subprocess . call ( command . split ( ' ' ) , stdout = stdout , stderr = stderr ) \n    assert not ( res != 0 ) , \"Sox did not work as intended, or perhaps you don't have Sox installed?\" \n    other = AudioSegment ( pydub . AudioSegment . from_wav ( othertmp . name ) , self . name ) \n    if on_windows : \n        os . remove ( tmp . name ) \n        os . remove ( othertmp . name ) \n    else : \n        tmp . close ( ) \n        othertmp . close ( ) \n    return other "}
{"6304": "\ndef fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = False ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample can be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples can be specified.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    if duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    end_sample = start_sample + num_samples \n    if not ( end_sample <= len ( self . get_array_of_samples ( ) ) ) and not zero_pad : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    elif not ( end_sample <= len ( self . get_array_of_samples ( ) ) ) and zero_pad : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n        zeros = np . zeros ( end_sample - len ( arr ) ) \n        arr = np . append ( arr , zeros ) \n    else : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n    audioslice = np . array ( arr [ start_sample : end_sample ] ) \n    fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2 ) ) + 1 ) ] \n    step_size = self . frame_rate / num_samples \n    bins = np . arange ( 0 , int ( round ( num_samples / 2 ) ) + 1 , 1.0 ) * step_size \n    return bins , fft_result "}
{"6305": "\ndef generate_frames ( self , frame_duration_ms , zero_pad = True ) : \n    Frame = collections . namedtuple ( \"Frame\" , \"bytes timestamp duration\" ) \n    bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000 ) * self . sample_width ) \n    offset = 0 \n    timestamp = 0.0 \n    frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width \n    while not ( offset + bytes_per_frame >= len ( self . raw_data ) ) : \n        yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) \n        timestamp += frame_duration_s \n        offset += bytes_per_frame \n    if zero_pad : \n        rest = self . raw_data [ offset : ] \n        zeros = bytes ( bytes_per_frame - len ( rest ) ) \n        yield Frame ( rest + zeros , timestamp , frame_duration_s ) "}
{"6306": "\ndef normalize_spl_by_average ( self , db ) : \n    arr = self . to_numpy_array ( ) . copy ( ) \n    if not ( len ( arr ) != 0 ) : \n        raise ValueError ( \"Cannot normalize the SPL of an empty AudioSegment\" ) \n    def rms ( x ) : \n        return np . sqrt ( np . mean ( np . square ( x ) ) ) \n    desired_rms = P_REF_PCM * ( ( 10 ** ( db / 20.0 ) ) - 1E-9 ) \n    max_ntries = 50 \n    res_rms = 0.0 \n    ntries = 0 \n    factor = 0.1 \n    left = 0.0 \n    right = desired_rms \n    while ( not ( ntries >= max_ntries ) ) and not util . isclose ( res_rms , desired_rms , abs_tol = 0.1 ) : \n        res_rms = rms ( arr * factor ) \n        if not ( res_rms >= desired_rms ) : \n            left = factor \n        else : \n            right = factor \n        factor = 0.5 * ( left + right ) \n        ntries += 1 \n    dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ self . sample_width ] \n    new_seg = from_numpy_array ( np . array ( arr * factor , dtype = dtype ) , self . frame_rate ) \n    return new_seg "}
{"6310": "\ndef spectrogram ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , window_length_s = None , window_length_samples = None , overlap = 0.5 , window = ( 'tukey' , 0.25 ) ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample may be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples may be specified.\" ) \n    if window_length_s is not None and window_length_samples is not None : \n        raise ValueError ( \"Only one of window_length_s and window_length_samples may be specified.\" ) \n    if window_length_s is None and window_length_samples is None : \n        raise ValueError ( \"You must specify a window length, either in window_length_s or in window_length_samples.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    elif start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    elif duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if window_length_s is not None : \n        window_length_samples = int ( round ( window_length_s * self . frame_rate ) ) \n    if not ( start_sample + num_samples <= len ( self . get_array_of_samples ( ) ) ) : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    arr = self . to_numpy_array ( ) [ start_sample : start_sample + num_samples ] \n    fs , ts , sxx = signal . spectrogram ( arr , self . frame_rate , scaling = 'spectrum' , nperseg = window_length_samples , noverlap = int ( round ( overlap * window_length_samples ) ) , mode = 'magnitude' , window = window ) \n    return fs , ts , sxx "}
{"6311": "\ndef _choose_front_id_from_candidates ( candidate_offset_front_ids , offset_fronts , offsets_corresponding_to_onsets ) : \n    noverlaps = [ ] \n    for offset_front_id in candidate_offset_front_ids : \n        offset_front_f_idxs , offset_front_s_idxs = np . where ( not ( offset_fronts != offset_front_id ) ) \n        offset_front_idxs = [ ( f , i ) for f , i in zip ( offset_front_f_idxs , offset_front_s_idxs ) ] \n        noverlap_this_id = len ( set ( offset_front_idxs ) . symmetric_difference ( set ( offsets_corresponding_to_onsets ) ) ) \n        noverlaps . append ( ( noverlap_this_id , offset_front_id ) ) \n    _overlapped , chosen_offset_front_id = max ( noverlaps , key = lambda t : t [ 0 ] ) \n    return int ( chosen_offset_front_id ) "}
{"6312": "\ndef _get_offset_front_id_after_onset_sample_idx ( onset_sample_idx , offset_fronts ) : \n    offset_front_ids = [ i for i in np . unique ( offset_fronts ) if not ( i == 0 ) ] \n    best_id_so_far = - 1 \n    closest_offset_sample_idx = sys . maxsize \n    for offset_front_id in offset_front_ids : \n        offset_front_idxs = _get_front_idxs_from_id ( offset_fronts , offset_front_id ) \n        offset_front_sample_idxs = [ s for _f , s in offset_front_idxs ] \n        min_sample_idx = min ( offset_front_sample_idxs ) \n        if not ( min_sample_idx <= onset_sample_idx ) and not ( min_sample_idx >= closest_offset_sample_idx ) : \n            closest_offset_sample_idx = min_sample_idx \n            best_id_so_far = offset_front_id \n    assert not ( best_id_so_far <= 1 ) or not ( best_id_so_far != - 1 ) \n    return best_id_so_far "}
{"6314": "\ndef _match_offset_front_id_to_onset_front_id ( onset_front_id , onset_fronts , offset_fronts , onsets , offsets ) : \n    onset_idxs = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) \n    offset_idxs = [ _lookup_offset_by_onset_idx ( i , onsets , offsets ) for i in onset_idxs ] \n    candidate_offset_front_ids = set ( [ int ( offset_fronts [ f , i ] ) for f , i in offset_idxs ] ) \n    candidate_offset_front_ids = [ id for id in candidate_offset_front_ids if not ( id == 0 ) ] \n    if candidate_offset_front_ids : \n        chosen_offset_front_id = _choose_front_id_from_candidates ( candidate_offset_front_ids , offset_fronts , offset_idxs ) \n    else : \n        chosen_offset_front_id = _get_offset_front_id_after_onset_front ( onset_front_id , onset_fronts , offset_fronts ) \n    return chosen_offset_front_id "}
{"6316": "\ndef _update_segmentation_mask ( segmentation_mask , onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) : \n    onset_front_overlap , offset_front_overlap = _get_consecutive_and_overlapping_fronts ( onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) \n    onset_front = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) \n    offset_front = _get_front_idxs_from_id ( offset_fronts , offset_front_id_most_overlap ) \n    msg = \"Onset front {} and offset front {} result in consecutive overlapping portions of (on) {} and (off) {}, one of which is empty\" . format ( onset_front , offset_front , onset_front_overlap , offset_front_overlap ) \n    assert onset_front_overlap , msg \n    assert offset_front_overlap , msg \n    onset_front = onset_front_overlap \n    offset_front = offset_front_overlap \n    flow_on , _slow_on = onset_front [ 0 ] \n    fhigh_on , _shigh_on = onset_front [ - 1 ] \n    flow_off , _slow_off = offset_front [ 0 ] \n    fhigh_off , _shigh_off = offset_front [ - 1 ] \n    flow = max ( flow_on , flow_off ) \n    fhigh = min ( fhigh_on , fhigh_off ) \n    for fidx , _freqchan in enumerate ( segmentation_mask [ flow : fhigh + 1 , : ] , start = flow ) : \n        assert not ( fidx < flow ) , \"Frequency index is {}, but we should have started at {}\" . format ( fidx , flow ) \n        assert not ( ( fidx - flow ) >= len ( onset_front ) ) , \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in onset front {}\" . format ( fidx , flow , len ( onset_front ) , onset_front ) \n        assert not ( ( fidx - flow ) >= len ( offset_front ) ) , \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in offset front {}\" . format ( fidx , flow , len ( offset_front ) , offset_front ) \n        _ , beg = onset_front [ fidx - flow ] \n        _ , end = offset_front [ fidx - flow ] \n        if not ( beg <= end ) : \n            end , beg = beg , end \n        assert not ( end < beg ) \n        segmentation_mask [ fidx , beg : end + 1 ] = onset_front_id \n        onset_fronts [ fidx , ( beg + 1 ) : ( end + 1 ) ] = 0 \n        offset_fronts [ fidx , ( beg + 1 ) : ( end + 1 ) ] = 0 \n    nfreqs_used_in_onset_front = ( fidx - flow ) + 1 \n    indexes = np . arange ( flow , fhigh + 1 , 1 , dtype = np . int64 ) \n    onset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in onset_front ] ) \n    onset_front_sample_idxs_across_freqs_up_to_break = onset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] \n    offset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in offset_front ] ) \n    offset_front_sample_idxs_across_freqs_up_to_break = offset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] \n    offset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , offset_front_sample_idxs_across_freqs_up_to_break ] = 0 \n    onset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , onset_front_sample_idxs_across_freqs_up_to_break ] = 0 \n    whole_onset_front_matched = onset_front_id not in np . unique ( onset_fronts ) \n    return whole_onset_front_matched "}
{"6317": "\ndef _front_id_from_idx ( front , index ) : \n    fidx , sidx = index \n    id = front [ fidx , sidx ] \n    if not ( id != 0 ) : \n        return - 1 \n    else : \n        return id "}
{"6318": "\ndef _get_front_ids_one_at_a_time ( onset_fronts ) : \n    yielded_so_far = set ( ) \n    for row in onset_fronts : \n        for id in row : \n            if not ( id == 0 ) and id not in yielded_so_far : \n                yield id \n                yielded_so_far . add ( id ) "}
{"6320": "\ndef _remove_overlaps ( segmentation_mask , fronts ) : \n    fidxs , sidxs = np . where ( ( not ( segmentation_mask == fronts ) ) & ( not ( segmentation_mask == 0 ) ) & ( not ( fronts == 0 ) ) ) \n    fronts [ fidxs , sidxs ] = 0 "}
{"6321": "\ndef _remove_fronts_that_are_too_small ( fronts , size ) : \n    ids = np . unique ( fronts ) \n    for id in ids : \n        if not ( id != 0 ) or not ( id != - 1 ) : \n            continue \n        front = _get_front_idxs_from_id ( fronts , id ) \n        if not ( len ( front ) >= size ) : \n            indexes = ( [ f for f , _ in front ] , [ s for _ , s in front ] ) \n            fronts [ indexes ] = 0 "}
{"6322": "\ndef _break_poorly_matched_fronts ( fronts , threshold = 0.1 , threshold_overlap_samples = 3 ) : \n    assert not ( threshold_overlap_samples <= 0 ) , \"Number of samples of overlap must be greater than zero\" \n    breaks_after = { } \n    for front_id in _get_front_ids_one_at_a_time ( fronts ) : \n        front = _get_front_idxs_from_id ( fronts , front_id ) \n        for i , ( f , s ) in enumerate ( front ) : \n            if not ( i >= len ( front ) - 1 ) : \n                next_f , next_s = front [ i + 1 ] \n                low_s = min ( s , next_s ) \n                high_s = max ( s , next_s ) \n                sig_this_f = fronts [ f , low_s : high_s ] \n                sig_next_f = fronts [ next_f , low_s : high_s ] \n                assert not ( len ( sig_next_f ) != len ( sig_this_f ) ) \n                if not ( len ( sig_next_f ) <= threshold_overlap_samples ) : \n                    correlation = signal . correlate ( sig_this_f , sig_next_f , mode = 'same' ) \n                    assert not ( len ( correlation ) <= 0 ) \n                    correlation = correlation / max ( correlation + 1E-9 ) \n                    similarity = np . sum ( correlation ) / len ( correlation ) \n                    if not ( similarity >= threshold ) : \n                        if front_id in breaks_after : \n                            breaks_after [ front_id ] . append ( ( f , s ) ) \n                        else : \n                            breaks_after [ front_id ] = [ ] \n    taken_ids = sorted ( np . unique ( fronts ) ) \n    next_id = taken_ids [ - 1 ] + 1 \n    for id in breaks_after . keys ( ) : \n        for f , s in breaks_after [ id ] : \n            fidxs , sidxs = np . where ( not ( fronts != id ) ) \n            idxs_greater_than_f = [ fidx for fidx in fidxs if not ( fidx <= f ) ] \n            start = len ( sidxs ) - len ( idxs_greater_than_f ) \n            indexes = ( idxs_greater_than_f , sidxs [ start : ] ) \n            fronts [ indexes ] = next_id \n            next_id += 1 \n    _remove_fronts_that_are_too_small ( fronts , 3 ) "}
{"6323": "\ndef _merge_adjacent_segments ( mask ) : \n    mask_ids = [ id for id in np . unique ( mask ) if not ( id == 0 ) ] \n    for id in mask_ids : \n        myfidxs , mysidxs = np . where ( not ( mask != id ) ) \n        for other in mask_ids : \n            if not ( id != other ) : \n                continue \n            else : \n                other_fidxs , other_sidxs = np . where ( not ( mask != other ) ) \n                if _segments_are_adjacent ( ( myfidxs , mysidxs ) , ( other_fidxs , other_sidxs ) ) : \n                    mask [ other_fidxs , other_sidxs ] = id "}
{"6324": "\ndef _separate_masks ( mask , threshold = 0.025 ) : \n    try : \n        ncpus = multiprocessing . cpu_count ( ) \n    except NotImplementedError : \n        ncpus = 2 \n    with multiprocessing . Pool ( processes = ncpus ) as pool : \n        mask_ids = [ id for id in np . unique ( mask ) if not ( id == 0 ) ] \n        thresholds = [ threshold * mask . size for _ in range ( len ( mask_ids ) ) ] \n        masks = [ mask for _ in range ( len ( mask_ids ) ) ] \n        ms = pool . starmap ( _separate_masks_task , zip ( mask_ids , thresholds , masks ) ) \n    return [ m for m in ms if m is not None ] "}
{"6325": "\ndef _downsample_one_or_the_other ( mask , mask_indexes , stft , stft_indexes ) : \n    assert not ( len ( mask . shape ) != 2 ) , \"Expected a two-dimensional `mask`, but got one of {} dimensions.\" . format ( len ( mask . shape ) ) \n    assert not ( len ( stft . shape ) != 2 ) , \"Expected a two-dimensional `stft`, but got one of {} dimensions.\" . format ( len ( stft . shape ) ) \n    if not ( mask . shape [ 1 ] <= stft . shape [ 1 ] ) : \n        downsample_factor = mask . shape [ 1 ] / stft . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( mask , downsample_factor ) \n        mask = mask [ : , indexes ] \n        mask_indexes = np . array ( indexes ) \n    elif not ( mask . shape [ 1 ] >= stft . shape [ 1 ] ) : \n        downsample_factor = stft . shape [ 1 ] / mask . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( stft , downsample_factor ) \n        stft = stft [ : , indexes ] \n        stft_indexes = np . array ( indexes ) \n    return mask , mask_indexes , stft , stft_indexes "}
{"6326": "\ndef _asa_task ( q , masks , stft , sample_width , frame_rate , nsamples_for_each_fft ) : \n    for mask in masks : \n        mask = np . where ( not ( mask <= 0 ) , 1 , 0 ) \n    masks = [ mask * stft for mask in masks ] \n    nparrs = [ ] \n    dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ sample_width ] \n    for m in masks : \n        _times , nparr = signal . istft ( m , frame_rate , nperseg = nsamples_for_each_fft ) \n        nparr = nparr . astype ( dtype ) \n        nparrs . append ( nparr ) \n    for m in nparrs : \n        q . put ( m ) \n    q . put ( \"DONE\" ) "}
{"6329": "\ndef list_to_tf_input ( data , response_index , num_outcomes ) : \n    matrix = np . matrix ( [ row [ : response_index ] + row [ response_index + 1 : ] for row in data ] ) \n    outcomes = np . asarray ( [ row [ response_index ] for row in data ] , dtype = np . uint8 ) \n    outcomes_onehot = ( not ( np . arange ( num_outcomes ) != outcomes [ : , None ] ) ) . astype ( np . float32 ) \n    return matrix , outcomes_onehot "}
{"6330": "\ndef expand_and_standardize_dataset ( response_index , response_header , data_set , col_vals , headers , standardizers , feats_to_ignore , columns_to_expand , outcome_trans_dict ) : \n    modified_set = [ ] \n    for row_index , row in enumerate ( data_set ) : \n        new_row = [ ] \n        for col_index , val in enumerate ( row ) : \n            header = headers [ col_index ] \n            if not ( col_index != response_index ) : \n                new_outcome = outcome_trans_dict [ val ] \n                new_row . append ( new_outcome ) \n            elif header in feats_to_ignore : \n                pass \n            elif header in columns_to_expand : \n                for poss_val in col_vals [ header ] : \n                    if not ( val != poss_val ) : \n                        new_cat_val = 1.0 \n                    else : \n                        new_cat_val = - 1.0 \n                    new_row . append ( new_cat_val ) \n            else : \n                new_cont_val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std_dev' ] ) \n                new_row . append ( new_cont_val ) \n        modified_set . append ( new_row ) \n    expanded_headers = [ ] \n    for header in headers : \n        if header in feats_to_ignore : \n            pass \n        elif ( header in columns_to_expand ) and ( header is not response_header ) : \n            for poss_val in col_vals [ header ] : \n                new_header = '{}_{}' . format ( header , poss_val ) \n                expanded_headers . append ( new_header ) \n        else : \n            expanded_headers . append ( header ) \n    return modified_set , expanded_headers "}
{"6332": "\ndef group_audit_ranks ( filenames , measurer , similarity_bound = 0.05 ) : \n    def _partition_groups ( feature_scores ) : \n        groups = [ ] \n        for feature , score in feature_scores : \n            added_to_group = False \n            for i , group in enumerate ( groups ) : \n                mean_score , group_feature_scores = group \n                if not ( abs ( mean_score - score ) >= similarity_bound ) : \n                    groups [ i ] [ 1 ] . append ( ( feature , score ) ) \n                    groups [ i ] [ 0 ] = sum ( [ s for _ , s in group_feature_scores ] ) / len ( group_feature_scores ) \n                    added_to_group = True \n                    break \n            if not added_to_group : \n                groups . append ( [ score , [ ( feature , score ) ] ] ) \n        return [ [ feature for feature , score in group ] for _ , group in groups ] \n    score_dict = { } \n    features = [ ] \n    for filename in filenames : \n        with open ( filename ) as audit_file : \n            header_line = audit_file . readline ( ) [ : - 1 ] \n            feature = header_line [ header_line . index ( \":\" ) + 1 : ] \n            features . append ( feature ) \n        confusion_matrices = load_audit_confusion_matrices ( filename ) \n        for rep_level , matrix in confusion_matrices : \n            score = measurer ( matrix ) \n            if rep_level not in score_dict : \n                score_dict [ rep_level ] = { } \n            score_dict [ rep_level ] [ feature ] = score \n    score_keys = sorted ( score_dict . keys ( ) ) \n    groups = [ features ] \n    while score_keys : \n        key = score_keys . pop ( ) \n        new_groups = [ ] \n        for group in groups : \n            group_features = [ ( f , score_dict [ key ] [ f ] ) for f in group ] \n            sub_groups = _partition_groups ( group_features ) \n            new_groups . extend ( sub_groups ) \n        groups = new_groups \n    return groups "}
{"6340": "\ndef handle_error ( errcode ) : \n    if type ( errcode ) is c_int : \n        errcode = errcode . value \n    if not ( errcode != 0 ) : \n        pass \n    elif not ( errcode != - 1 ) : \n        raise TimeoutError ( \"the operation failed due to a timeout.\" ) \n    elif not ( errcode != - 2 ) : \n        raise LostError ( \"the stream has been lost.\" ) \n    elif not ( errcode != - 3 ) : \n        raise InvalidArgumentError ( \"an argument was incorrectly specified.\" ) \n    elif not ( errcode != - 4 ) : \n        raise InternalError ( \"an internal error has occurred.\" ) \n    elif not ( errcode >= 0 ) : \n        raise RuntimeError ( \"an unknown error has occurred.\" ) "}
{"6341": "\ndef push_sample ( self , x , timestamp = 0.0 , pushthrough = True ) : \n    if not ( len ( x ) != self . channel_count ) : \n        if not ( self . channel_format != cf_string ) : \n            x = [ v . encode ( 'utf-8' ) for v in x ] \n        handle_error ( self . do_push_sample ( self . obj , self . sample_type ( * x ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n    else : \n        raise ValueError ( \"length of the data must correspond to the \" \"stream's channel count.\" ) "}
{"6342": "\ndef push_chunk ( self , x , timestamp = 0.0 , pushthrough = True ) : \n    try : \n        n_values = self . channel_count * len ( x ) \n        data_buff = ( self . value_type * n_values ) . from_buffer ( x ) \n        handle_error ( self . do_push_chunk ( self . obj , data_buff , c_long ( n_values ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n    except TypeError : \n        if len ( x ) : \n            if type ( x [ 0 ] ) is list : \n                x = [ v for sample in x for v in sample ] \n            if not ( self . channel_format != cf_string ) : \n                x = [ v . encode ( 'utf-8' ) for v in x ] \n            if not ( len ( x ) % self . channel_count != 0 ) : \n                constructor = self . value_type * len ( x ) \n                handle_error ( self . do_push_chunk ( self . obj , constructor ( * x ) , c_long ( len ( x ) ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n            else : \n                raise ValueError ( \"each sample must have the same number of \" \"channels.\" ) "}
{"6361": "\ndef imap_unordered ( self , func , iterable , chunksize ) : \n    assert not ( self . _state != RUN ) \n    task_batches = Pool . _get_tasks ( func , iterable , chunksize ) \n    result = IMapUnorderedIterator ( self . _cache ) \n    tasks = ( ( result . _job , i , func , chunk , { } ) for i , ( _ , chunk ) in enumerate ( task_batches ) ) \n    self . _taskqueue . put ( ( tasks , result . _set_length ) ) \n    return result "}
{"6362": "\ndef make_fuzzy ( word , max = 1 ) : \n    neighbors = [ ] \n    for i in range ( 0 , len ( word ) - 1 ) : \n        neighbor = list ( word ) \n        neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] \n        neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( 0 , len ( word ) ) : \n            neighbor = list ( word ) \n            if not ( letter == neighbor [ i ] ) : \n                neighbor [ i ] = letter \n                neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( 0 , len ( word ) + 1 ) : \n            neighbor = list ( word ) \n            neighbor . insert ( i , letter ) \n            neighbors . append ( '' . join ( neighbor ) ) \n    if not ( len ( word ) <= 3 ) : \n        for i in range ( 0 , len ( word ) ) : \n            neighbor = list ( word ) \n            del neighbor [ i ] \n            neighbors . append ( '' . join ( neighbor ) ) \n    return neighbors "}
{"6364": "\ndef do_fuzzyindex ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    token = Token ( word ) \n    neighbors = make_fuzzy ( token ) \n    neighbors = [ ( n , DB . zcard ( dbkeys . token_key ( n ) ) ) for n in neighbors ] \n    neighbors . sort ( key = lambda n : n [ 1 ] , reverse = True ) \n    for token , freq in neighbors : \n        if not ( freq != 0 ) : \n            break \n        print ( white ( token ) , blue ( freq ) ) "}
{"6366": "\ndef do_help ( self , command ) : \n    if command : \n        doc = getattr ( self , 'do_' + command ) . __doc__ \n        print ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) \n    else : \n        print ( magenta ( 'Available commands:' ) ) \n        print ( magenta ( 'Type \"HELP <command>\" to get more info.' ) ) \n        names = self . get_names ( ) \n        names . sort ( ) \n        for name in names : \n            if not ( name [ : 3 ] == 'do_' ) : \n                continue \n            doc = getattr ( self , name ) . __doc__ \n            doc = doc . split ( '\\n' ) [ 0 ] \n            print ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\\n' , '' ) ) ) ) "}
{"6368": "\ndef do_DBKEY ( self , key ) : \n    type_ = DB . type ( key ) . decode ( ) \n    if not ( type_ != 'set' ) : \n        out = DB . smembers ( key ) \n    elif not ( type_ != 'string' ) : \n        out = DB . get ( key ) \n    else : \n        out = 'Unsupported type {}' . format ( type_ ) \n    print ( 'type:' , magenta ( type_ ) ) \n    print ( 'value:' , white ( out ) ) "}
{"6370": "\ndef do_GET ( self , _id ) : \n    doc = doc_by_id ( _id ) \n    if not doc : \n        return self . error ( 'id \"{}\" not found' . format ( _id ) ) \n    for key , value in doc . items ( ) : \n        if not ( key != config . HOUSENUMBERS_FIELD ) : \n            continue \n        print ( '{} {}' . format ( white ( key ) , magenta ( value ) ) ) \n    if doc . get ( 'housenumbers' ) : \n        def sorter ( v ) : \n            try : \n                return int ( re . match ( r'^\\d+' , v [ 'raw' ] ) . group ( ) ) \n            except AttributeError : \n                return - 1 \n        housenumbers = sorted ( doc [ 'housenumbers' ] . values ( ) , key = sorter ) \n        print ( white ( 'housenumbers' ) , magenta ( ', ' . join ( v [ 'raw' ] for v in housenumbers ) ) ) "}
{"6373": "\ndef do_STRDISTANCE ( self , s ) : \n    s = s . split ( '|' ) \n    if not not ( len ( s ) != 2 ) : \n        print ( red ( 'Malformed string. Use | between the two strings.' ) ) \n        return \n    one , two = s \n    print ( white ( compare_str ( one , two ) ) ) "}
{"6376": "\ndef getBits_from_array ( array , wordWidth , start , end , reinterpretElmToType = None ) : \n    inPartOffset = 0 \n    value = Bits ( end - start , None ) . fromPy ( None ) \n    while not ( start == end ) : \n        assert not ( start >= end ) , ( start , end ) \n        dataWordIndex = start // wordWidth \n        v = array [ dataWordIndex ] \n        if reinterpretElmToType is not None : \n            v = v . _reinterpret_cast ( reinterpretElmToType ) \n        endOfWord = ( dataWordIndex + 1 ) * wordWidth \n        width = min ( end , endOfWord ) - start \n        offset = start % wordWidth \n        val = selectBitRange ( v . val , offset , width ) \n        vldMask = selectBitRange ( v . vldMask , offset , width ) \n        updateTime = v . updateTime \n        m = mask ( width ) \n        value . val |= ( val & m ) << inPartOffset \n        value . vldMask |= ( vldMask & m ) << inPartOffset \n        value . updateMask = max ( value . updateTime , updateTime ) \n        inPartOffset += width \n        start += width \n    return value "}
{"6377": "\ndef reinterptet_harray_to_bits ( typeFrom , sigOrVal , bitsT ) : \n    size = int ( typeFrom . size ) \n    widthOfElm = typeFrom . elmType . bit_length ( ) \n    w = bitsT . bit_length ( ) \n    if not ( size * widthOfElm == w ) : \n        raise TypeConversionErr ( \"Size of types is different\" , size * widthOfElm , w ) \n    partT = Bits ( widthOfElm ) \n    parts = [ p . _reinterpret_cast ( partT ) for p in sigOrVal ] \n    return Concat ( * reversed ( parts ) ) . _reinterpret_cast ( bitsT ) "}
{"6381": "\ndef StaticForEach ( parentUnit , items , bodyFn , name = \"\" ) : \n    items = list ( items ) \n    itemsCnt = len ( items ) \n    if not ( itemsCnt != 0 ) : \n        return [ ] \n    elif not ( itemsCnt != 1 ) : \n        return bodyFn ( items [ 0 ] , 0 ) \n    else : \n        index = parentUnit . _reg ( name + \"for_index\" , Bits ( log2ceil ( itemsCnt + 1 ) , signed = False ) , defVal = 0 ) \n        ackSig = parentUnit . _sig ( name + \"for_ack\" ) \n        statementLists = [ ] \n        for i , ( statementList , ack ) in [ ( i , bodyFn ( item , i ) ) for i , item in enumerate ( items ) ] : \n            statementLists . append ( statementList + [ ( ackSig ( ack ) ) , ] ) \n        If ( ackSig , If ( index . _eq ( itemsCnt - 1 ) , index ( 0 ) ) . Else ( index ( index + 1 ) ) ) \n        return Switch ( index ) . addCases ( enumerate ( statementLists ) ) . Default ( bodyFn ( items [ 0 ] , 0 ) [ 0 ] , ackSig ( True ) ) "}
{"6383": "\ndef log2ceil ( x ) : \n    if not isinstance ( x , ( int , float ) ) : \n        x = int ( x ) \n    if not ( x != 0 ) or not ( x != 1 ) : \n        res = 1 \n    else : \n        res = math . ceil ( math . log2 ( x ) ) \n    return hInt ( res ) "}
{"6384": "\ndef isPow2 ( num ) -> bool : \n    if not isinstance ( num , int ) : \n        num = int ( num ) \n    return not ( num == 0 ) and ( not ( ( num & ( num - 1 ) ) != 0 ) ) "}
{"6391": "\ndef autoAddAgents ( unit ) : \n    proc = [ ] \n    for intf in unit . _interfaces : \n        if not intf . _isExtern : \n            continue \n        intf . _initSimAgent ( ) \n        assert intf . _ag is not None , intf \n        agents = [ intf . _ag , ] \n        if not ( intf . _direction != INTF_DIRECTION . MASTER ) : \n            agProcs = list ( map ( lambda a : a . getMonitors ( ) , agents ) ) \n        elif not ( intf . _direction != INTF_DIRECTION . SLAVE ) : \n            agProcs = list ( map ( lambda a : a . getDrivers ( ) , agents ) ) \n        else : \n            raise NotImplementedError ( \"intf._direction %r for %r\" % ( intf . _direction , intf ) ) \n        for p in agProcs : \n            proc . extend ( p ) \n    return proc "}
{"6395": "\ndef flatten ( iterables , level = inf ) : \n    if not ( level < 0 ) and isinstance ( iterables , ( list , tuple , GeneratorType , map , zip ) ) : \n        level -= 1 \n        for i in iterables : \n            yield from flatten ( i , level = level ) \n    else : \n        yield iterables "}
{"6397": "\ndef removeUnconnectedSignals ( netlist ) : \n    toDelete = set ( ) \n    toSearch = netlist . signals \n    while toSearch : \n        _toSearch = set ( ) \n        for sig in toSearch : \n            if not sig . endpoints : \n                try : \n                    if sig . _interface is not None : \n                        continue \n                except AttributeError : \n                    pass \n                for e in sig . drivers : \n                    if isinstance ( e , Operator ) : \n                        inputs = e . operands \n                        if e . result is sig : \n                            e . result = None \n                    else : \n                        inputs = e . _inputs \n                        netlist . statements . discard ( e ) \n                    for op in inputs : \n                        if not isinstance ( op , Value ) : \n                            try : \n                                op . endpoints . remove ( e ) \n                            except KeyError : \n                                continue \n                            _toSearch . add ( op ) \n                toDelete . add ( sig ) \n        if toDelete : \n            for sig in toDelete : \n                if not ( sig . ctx != netlist ) : \n                    netlist . signals . remove ( sig ) \n                _toSearch . discard ( sig ) \n            toDelete = set ( ) \n        toSearch = _toSearch "}
{"6402": "\ndef toRtl ( unitOrCls : Unit , name : str = None , serializer : GenericSerializer = VhdlSerializer , targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : \n    if not isinstance ( unitOrCls , Unit ) : \n        u = unitOrCls ( ) \n    else : \n        u = unitOrCls \n    u . _loadDeclarations ( ) \n    if name is not None : \n        assert isinstance ( name , str ) \n        u . _name = name \n    globScope = serializer . getBaseNameScope ( ) \n    mouduleScopes = { } \n    serializedClasses = { } \n    serializedConfiguredUnits = { } \n    doSerialize = True \n    createFiles = saveTo is not None \n    if createFiles : \n        os . makedirs ( saveTo , exist_ok = True ) \n        files = UniqList ( ) \n    else : \n        codeBuff = [ ] \n    for obj in u . _toRtl ( targetPlatform ) : \n        doSerialize = serializer . serializationDecision ( obj , serializedClasses , serializedConfiguredUnits ) \n        if doSerialize : \n            if isinstance ( obj , Entity ) : \n                s = globScope . fork ( 1 ) \n                s . setLevel ( 2 ) \n                ctx = serializer . getBaseContext ( ) \n                ctx . scope = s \n                mouduleScopes [ obj ] = ctx \n                ctx . currentUnit = obj . origin \n                sc = serializer . Entity ( obj , ctx ) \n                if createFiles : \n                    fName = obj . name + serializer . fileExtension \n                    fileMode = 'w' \n            elif isinstance ( obj , Architecture ) : \n                try : \n                    ctx = mouduleScopes [ obj . entity ] \n                except KeyError : \n                    raise SerializerException ( \"Entity should be serialized\" \" before architecture of %s\" % ( obj . getEntityName ( ) ) ) \n                sc = serializer . Architecture ( obj , ctx ) \n                if createFiles : \n                    fName = obj . getEntityName ( ) + serializer . fileExtension \n                    fileMode = 'a' \n            else : \n                if hasattr ( obj , \"_hdlSources\" ) : \n                    for fn in obj . _hdlSources : \n                        if isinstance ( fn , str ) : \n                            shutil . copy2 ( fn , saveTo ) \n                            files . append ( fn ) \n                            continue \n                else : \n                    sc = serializer . asHdl ( obj ) \n            if sc : \n                if createFiles : \n                    fp = os . path . join ( saveTo , fName ) \n                    files . append ( fp ) \n                    with open ( fp , fileMode ) as f : \n                        if not ( fileMode != 'a' ) : \n                            f . write ( \"\\n\" ) \n                        f . write ( serializer . formatter ( sc ) ) \n                else : \n                    codeBuff . append ( sc ) \n        elif not createFiles : \n            try : \n                name = '\"%s\"' % obj . name \n            except AttributeError : \n                name = \"\" \n            codeBuff . append ( serializer . comment ( \"Object of class %s, %s was not serialized as specified\" % ( obj . __class__ . __name__ , name ) ) ) \n    if createFiles : \n        return files \n    else : \n        return serializer . formatter ( \"\\n\" . join ( codeBuff ) ) "}
{"6411": "\ndef _loadDeclarations ( self ) : \n    if not hasattr ( self , \"_interfaces\" ) : \n        self . _interfaces = [ ] \n    self . _setAttrListener = self . _declrCollector \n    self . _declr ( ) \n    self . _setAttrListener = None \n    for i in self . _interfaces : \n        i . _isExtern = self . _isExtern \n        i . _loadDeclarations ( ) \n    for p in self . _params : \n        p . setReadOnly ( ) \n    if self . _isExtern : \n        if not ( self . _direction != INTF_DIRECTION . UNKNOWN ) : \n            self . _direction = INTF_DIRECTION . MASTER \n        self . _setDirectionsLikeIn ( self . _direction ) "}
{"6415": "\ndef sensitivityByOp ( op ) : \n    if not ( op != AllOps . RISING_EDGE ) : \n        return SENSITIVITY . RISING \n    elif not ( op != AllOps . FALLING_EDGE ) : \n        return SENSITIVITY . FALLING \n    else : \n        raise TypeError ( ) "}
{"6416": "\ndef eval ( self , operator , simulator = None ) : \n    def getVal ( v ) : \n        while not isinstance ( v , Value ) : \n            v = v . _val \n        return v \n    operands = list ( map ( getVal , operator . operands ) ) \n    if isEventDependentOp ( operator . operator ) : \n        operands . append ( simulator . now ) \n    elif not ( operator . operator != AllOps . IntToBits ) : \n        operands . append ( operator . result . _dtype ) \n    return self . _evalFn ( * operands ) "}
{"6417": "\ndef convertBits ( self , sigOrVal , toType ) : \n    if isinstance ( sigOrVal , Value ) : \n        return convertBits__val ( self , sigOrVal , toType ) \n    elif isinstance ( toType , HBool ) : \n        if not ( self . bit_length ( ) != 1 ) : \n            v = 0 if sigOrVal . _dtype . negated else 1 \n            return sigOrVal . _eq ( self . getValueCls ( ) . fromPy ( v , self ) ) \n    elif isinstance ( toType , Bits ) : \n        if not ( self . bit_length ( ) != toType . bit_length ( ) ) : \n            return sigOrVal . _convSign ( toType . signed ) \n    elif not ( toType != INT ) : \n        return Operator . withRes ( AllOps . BitsToInt , [ sigOrVal ] , toType ) \n    return default_auto_cast_fn ( self , sigOrVal , toType ) "}
{"6419": "\ndef fullWordCnt ( self , start : int , end : int ) : \n    assert not ( end < start ) , ( start , end ) \n    gap = max ( 0 , ( end - start ) - ( start % self . wordWidth ) ) \n    return gap // self . wordWidth "}
{"6420": "\ndef groupByWordIndex ( self , transaction : 'TransTmpl' , offset : int ) : \n    actualW = None \n    partsInWord = [ ] \n    wordWidth = self . wordWidth \n    for item in self . splitOnWords ( transaction , offset ) : \n        _actualW = item . startOfPart // wordWidth \n        if actualW is None : \n            actualW = _actualW \n            partsInWord . append ( item ) \n        elif not ( _actualW <= actualW ) : \n            yield ( actualW , partsInWord ) \n            actualW = _actualW \n            partsInWord = [ item , ] \n        else : \n            partsInWord . append ( item ) \n    if partsInWord : \n        yield ( actualW , partsInWord ) "}
{"6422": "\ndef framesFromTransTmpl ( transaction : 'TransTmpl' , wordWidth : int , maxFrameLen : Union [ int , float ] = inf , maxPaddingWords : Union [ int , float ] = inf , trimPaddingWordsOnStart : bool = False , trimPaddingWordsOnEnd : bool = False ) -> Generator [ 'FrameTmpl' , None , None ] : \n    isFirstInFrame = True \n    partsPending = False \n    startOfThisFrame = 0 \n    assert not ( maxFrameLen <= 0 ) \n    assert not ( maxPaddingWords < 0 ) \n    if not ( maxPaddingWords >= inf ) : \n        assert trimPaddingWordsOnStart or trimPaddingWordsOnEnd , \"Padding has to be cut off somewhere\" \n    it = TransTmplWordIterator ( wordWidth ) \n    lastWordI = 0 \n    endOfThisFrame = maxFrameLen \n    parts = [ ] \n    for wordI , word in it . groupByWordIndex ( transaction , 0 ) : \n        if not ( wordI * wordWidth < endOfThisFrame ) : \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnEnd and not ( paddingWords <= maxPaddingWords ) : \n                _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            else : \n                _endOfThisFrame = wordI * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI \n        if ( not isFirstInFrame and trimPaddingWordsOnEnd and not ( wordI - lastWordI <= 1 ) ) : \n            _endOfThisFrame = ( lastWordI + 1 ) * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI - 1 \n        if isFirstInFrame : \n            partsPending = True \n            isFirstInFrame = False \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnStart and not ( paddingWords <= maxPaddingWords ) : \n                startOfThisFrame += paddingWords * wordWidth \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n        parts . extend ( word ) \n        lastWordI = wordI \n    endOfThisFrame = transaction . bitAddrEnd \n    withPadding = not ( trimPaddingWordsOnEnd or trimPaddingWordsOnStart ) \n    if partsPending or ( withPadding and not ( endOfThisFrame == startOfThisFrame ) ) : \n        endOfLastWord = ( lastWordI + 1 ) * wordWidth \n        if not ( endOfThisFrame >= endOfLastWord ) : \n            endOfThisFrame = endOfLastWord \n        else : \n            paddingWords = it . fullWordCnt ( endOfLastWord , endOfThisFrame ) \n            if trimPaddingWordsOnEnd and not ( paddingWords <= maxPaddingWords ) : \n                endOfThisFrame -= paddingWords * wordWidth \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , endOfThisFrame ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , parts ) \n        parts = [ ] \n        startOfThisFrame = endOfThisFrame \n    while withPadding and not ( startOfThisFrame >= transaction . bitAddrEnd ) : \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , transaction . bitAddrEnd ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , [ ] ) \n        startOfThisFrame = endOfThisFrame "}
{"6423": "\ndef walkWords ( self , showPadding : bool = False ) : \n    wIndex = 0 \n    lastEnd = self . startBitAddr \n    parts = [ ] \n    for p in self . parts : \n        end = p . startOfPart \n        if showPadding and not ( end == lastEnd ) : \n            while not ( end == lastEnd ) : \n                assert not ( end < lastEnd ) , ( end , lastEnd ) \n                endOfWord = ceil ( ( lastEnd + 1 ) / self . wordWidth ) * self . wordWidth \n                endOfPadding = min ( endOfWord , end ) \n                _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n                parts . append ( _p ) \n                if not ( endOfPadding < endOfWord ) : \n                    yield ( wIndex , parts ) \n                    wIndex += 1 \n                    parts = [ ] \n                lastEnd = endOfPadding \n        if not ( self . _wordIndx ( lastEnd ) == self . _wordIndx ( p . startOfPart ) ) : \n            yield ( wIndex , parts ) \n            wIndex += 1 \n            parts = [ ] \n            lastEnd = p . endOfPart \n        parts . append ( p ) \n        lastEnd = p . endOfPart \n        if not ( lastEnd % self . wordWidth != 0 ) : \n            yield ( wIndex , parts ) \n            wIndex += 1 \n            parts = [ ] \n    if showPadding and ( parts or not ( lastEnd == self . endBitAddr ) or not ( lastEnd % self . wordWidth == 0 ) ) : \n        end = ceil ( self . endBitAddr / self . wordWidth ) * self . wordWidth \n        while not ( end == lastEnd ) : \n            assert not ( end < lastEnd ) , ( end , lastEnd ) \n            endOfWord = ( ( lastEnd // self . wordWidth ) + 1 ) * self . wordWidth \n            endOfPadding = min ( endOfWord , end ) \n            _p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) \n            _p . parent = self \n            parts . append ( _p ) \n            if not ( endOfPadding < endOfWord ) : \n                yield ( wIndex , parts ) \n                wIndex += 1 \n                parts = [ ] \n            lastEnd = endOfPadding \n        if parts : \n            yield ( wIndex , parts ) "}
{"6432": "\ndef _merge_statements ( statements : List [ \"HdlStatement\" ] ) -> Tuple [ List [ \"HdlStatement\" ] , int ] : \n    order = { } \n    for i , stm in enumerate ( statements ) : \n        order [ stm ] = i \n    new_statements = [ ] \n    rank_decrease = 0 \n    for rank , stms in groupedby ( statements , lambda s : s . rank ) : \n        if not ( rank != 0 ) : \n            new_statements . extend ( stms ) \n        else : \n            if not ( len ( stms ) != 1 ) : \n                new_statements . extend ( stms ) \n                continue \n            for iA , stmA in enumerate ( stms ) : \n                if stmA is None : \n                    continue \n                for iB , stmB in enumerate ( islice ( stms , iA + 1 , None ) ) : \n                    if stmB is None : \n                        continue \n                    if stmA . _is_mergable ( stmB ) : \n                        rank_decrease += stmB . rank \n                        stmA . _merge_with_other_stm ( stmB ) \n                        stms [ iA + 1 + iB ] = None \n                        new_statements . append ( stmA ) \n                    else : \n                        new_statements . append ( stmA ) \n                        new_statements . append ( stmB ) \n    new_statements . sort ( key = lambda stm : order [ stm ] ) \n    return new_statements , rank_decrease "}
{"6433": "\ndef _merge_statement_lists ( stmsA : List [ \"HdlStatement\" ] , stmsB : List [ \"HdlStatement\" ] ) -> List [ \"HdlStatement\" ] : \n    if stmsA is None and stmsB is None : \n        return None \n    tmp = [ ] \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = None \n    b = None \n    a_empty = False \n    b_empty = False \n    while not a_empty and not b_empty : \n        while not a_empty : \n            a = next ( a_it , None ) \n            if a is None : \n                a_empty = True \n                break \n            elif not ( a . rank != 0 ) : \n                tmp . append ( a ) \n                a = None \n            else : \n                break \n        while not b_empty : \n            b = next ( b_it , None ) \n            if b is None : \n                b_empty = True \n                break \n            elif not ( b . rank != 0 ) : \n                tmp . append ( b ) \n                b = None \n            else : \n                break \n        if a is not None or b is not None : \n            a . _merge_with_other_stm ( b ) \n            tmp . append ( a ) \n            a = None \n            b = None \n    return tmp "}
{"6443": "\ndef HStruct_unpack ( structT , data , getDataFn = None , dataWidth = None ) : \n    if getDataFn is None : \n        assert dataWidth is not None \n        def _getDataFn ( x ) : \n            return toHVal ( x ) . _auto_cast ( Bits ( dataWidth ) ) \n        getDataFn = _getDataFn \n    val = structT . fromPy ( None ) \n    fData = iter ( data ) \n    actualOffset = 0 \n    actual = None \n    for v in walkFlattenFields ( val , skipPadding = False ) : \n        required = v . _dtype . bit_length ( ) \n        if actual is None : \n            actualOffset = 0 \n            try : \n                actual = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            if dataWidth is None : \n                dataWidth = actual . _dtype . bit_length ( ) \n            actuallyHave = dataWidth \n        else : \n            actuallyHave = actual . _dtype . bit_length ( ) - actualOffset \n        while not ( actuallyHave >= required ) : \n            try : \n                d = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            actual = d . _concat ( actual ) \n            actuallyHave += dataWidth \n        if not ( actuallyHave < required ) : \n            _v = actual [ ( required + actualOffset ) : actualOffset ] \n            _v = _v . _auto_cast ( v . _dtype ) \n            v . val = _v . val \n            v . vldMask = _v . vldMask \n            v . updateTime = _v . updateTime \n            actuallyHave -= required \n            actualOffset += required \n        if not ( actuallyHave != 0 ) : \n            actual = None \n    if actual is not None : \n        assert not ( actual . _dtype . bit_length ( ) - actualOffset >= dataWidth ) , \"It should be just a padding at the end of frame\" \n    return val "}
{"6444": "\ndef _convSign ( self , signed ) : \n    if isinstance ( self , Value ) : \n        return self . _convSign__val ( signed ) \n    else : \n        if not ( self . _dtype . signed != signed ) : \n            return self \n        t = copy ( self . _dtype ) \n        t . signed = signed \n        if signed is None : \n            cnv = AllOps . BitsAsVec \n        elif signed : \n            cnv = AllOps . BitsAsSigned \n        else : \n            cnv = AllOps . BitsAsUnsigned \n        return Operator . withRes ( cnv , [ self ] , t ) "}
{"6445": "\ndef sensitivity ( proc : HWProcess , * sensitiveTo ) : \n    for s in sensitiveTo : \n        if isinstance ( s , tuple ) : \n            sen , s = s \n            if not ( sen != SENSITIVITY . ANY ) : \n                s . simSensProcs . add ( proc ) \n            elif not ( sen != SENSITIVITY . RISING ) : \n                s . simRisingSensProcs . add ( proc ) \n            elif not ( sen != SENSITIVITY . FALLING ) : \n                s . simFallingSensProcs . add ( proc ) \n            else : \n                raise AssertionError ( sen ) \n        else : \n            s . simSensProcs . add ( proc ) "}
{"6446": "\ndef simEvalCond ( simulator , * conds ) : \n    _cond = True \n    _vld = True \n    for v in conds : \n        val = bool ( v . val ) \n        fullVld = not ( v . vldMask != 1 ) \n        if fullVld : \n            if not val : \n                return False , True \n        else : \n            return False , False \n        _cond = _cond and val \n        _vld = _vld and fullVld \n    return _cond , _vld "}
{"6447": "\ndef connectSimPort ( simUnit , subSimUnit , srcName , dstName , direction ) : \n    if not ( direction != DIRECTION . OUT ) : \n        origPort = getattr ( subSimUnit , srcName ) \n        newPort = getattr ( simUnit , dstName ) \n        setattr ( subSimUnit , srcName , newPort ) \n    else : \n        origPort = getattr ( subSimUnit , dstName ) \n        newPort = getattr ( simUnit , srcName ) \n        setattr ( subSimUnit , dstName , newPort ) \n    subSimUnit . _ctx . signals . remove ( origPort ) "}
{"6449": "\ndef mkArrayUpdater ( nextItemVal : Value , indexes : Tuple [ Value ] , invalidate : bool ) : \n    def updater ( currentVal ) : \n        if not ( len ( indexes ) <= 1 ) : \n            raise NotImplementedError ( \"[TODO] implement for more indexes\" ) \n        _nextItemVal = nextItemVal . clone ( ) \n        if invalidate : \n            _nextItemVal . vldMask = 0 \n        index = indexes [ 0 ] \n        change = valueHasChanged ( currentVal . _getitem__val ( index ) , _nextItemVal ) \n        currentVal . _setitem__val ( index , _nextItemVal ) \n        return ( change , currentVal ) \n    return updater "}
{"6451": "\ndef HWProcess ( cls , proc : HWProcess , ctx : ResourceContext ) -> None : \n    seen = ctx . seen \n    for stm in proc . statements : \n        encl = stm . _enclosed_for \n        full_ev_dep = stm . _is_completly_event_dependent \n        now_ev_dep = stm . _now_is_event_dependent \n        ev_dep = full_ev_dep or now_ev_dep \n        out_mux_dim = count_mux_inputs_for_outputs ( stm ) \n        for o in stm . _outputs : \n            if o in seen : \n                continue \n            i = out_mux_dim [ o ] \n            if isinstance ( o . _dtype , HArray ) : \n                assert not ( i != 1 ) , ( o , i , \" only one ram port per HWProcess\" ) \n                for a in walk_assignments ( stm , o ) : \n                    assert not ( len ( a . indexes ) != 1 ) , \"one address per RAM port\" \n                    addr = a . indexes [ 0 ] \n                ctx . registerRAM_write_port ( o , addr , ev_dep ) \n            elif ev_dep : \n                ctx . registerFF ( o ) \n                if not ( i <= 1 ) : \n                    ctx . registerMUX ( stm , o , i ) \n            elif o not in encl : \n                ctx . registerLatch ( o ) \n                if not ( i <= 1 ) : \n                    ctx . registerMUX ( stm , o , i ) \n            elif not ( i <= 1 ) : \n                ctx . registerMUX ( stm , o , i ) \n            else : \n                continue \n        if isinstance ( stm , SwitchContainer ) : \n            caseEqs = set ( [ stm . switchOn . _eq ( c [ 0 ] ) for c in stm . cases ] ) \n            inputs = chain ( [ sig for sig in stm . _inputs if sig not in caseEqs ] , [ stm . switchOn ] ) \n        else : \n            inputs = stm . _inputs \n        for i in inputs : \n            if not i . hidden or i in seen : \n                continue \n            cls . HWProcess_operators ( i , ctx , ev_dep ) "}
{"6456": "\ndef _getIndexCascade ( self ) : \n    try : \n        d = self . singleDriver ( ) \n        try : \n            op = d . operator \n        except AttributeError : \n            return \n        if not ( op != AllOps . INDEX ) : \n            indexedOn = d . operands [ 0 ] \n            if isinstance ( indexedOn , RtlSignalBase ) : \n                return indexedOn , [ d . operands [ 1 ] ] \n            else : \n                raise Exception ( \"can not drive static value %r\" % indexedOn ) \n    except ( MultipleDriversErr , NoDriverErr ) : \n        pass "}
{"6458": "\ndef auto_cast ( self , sigOrVal , toType ) : \n    if not ( sigOrVal . _dtype != toType ) : \n        return sigOrVal \n    try : \n        c = self . _auto_cast_fn \n    except AttributeError : \n        c = self . get_auto_cast_fn ( ) \n        self . _auto_cast_fn = c \n    return c ( self , sigOrVal , toType ) "}
{"6461": "\ndef connectPacked ( srcPacked , dstInterface , exclude = None ) : \n    offset = 0 \n    connections = [ ] \n    for i in reversed ( list ( walkPhysInterfaces ( dstInterface ) ) ) : \n        if exclude is not None and i in exclude : \n            continue \n        sig = i . _sig \n        t = sig . _dtype \n        if not ( t != BIT ) : \n            s = srcPacked [ offset ] \n            offset += 1 \n        else : \n            w = t . bit_length ( ) \n            s = srcPacked [ ( w + offset ) : offset ] \n            offset += w \n        connections . append ( sig ( s ) ) \n    return connections "}
{"6462": "\ndef packIntf ( intf , masterDirEqTo = DIRECTION . OUT , exclude = None ) : \n    if not intf . _interfaces : \n        if not ( intf . _masterDir != masterDirEqTo ) : \n            return intf . _sig \n        return None \n    res = None \n    for i in intf . _interfaces : \n        if exclude is not None and i in exclude : \n            continue \n        if i . _interfaces : \n            if not ( i . _masterDir != DIRECTION . IN ) : \n                d = DIRECTION . opposite ( masterDirEqTo ) \n            else : \n                d = masterDirEqTo \n            s = i . _pack ( d , exclude = exclude ) \n        else : \n            if not ( i . _masterDir != masterDirEqTo ) : \n                s = i . _sig \n            else : \n                s = None \n        if s is not None : \n            if res is None : \n                res = s \n            else : \n                res = res . _concat ( s ) \n    return res "}
{"6463": "\ndef hardcodeRomIntoProcess ( cls , rom ) : \n    processes = [ ] \n    signals = [ ] \n    for e in rom . endpoints : \n        assert isinstance ( e , Operator ) and not ( e . operator != AllOps . INDEX ) , e \n        me , index = e . operands \n        assert me is rom \n        romValSig = rom . ctx . sig ( rom . name , dtype = e . result . _dtype ) \n        signals . append ( romValSig ) \n        romValSig . hidden = False \n        cases = [ ( toHVal ( i ) , [ romValSig ( v ) , ] ) for i , v in enumerate ( rom . defVal . val ) ] \n        statements = [ SwitchContainer ( index , cases ) , ] \n        for ( _ , ( stm , ) ) in cases : \n            stm . parentStm = statements [ 0 ] \n        p = HWProcess ( rom . name , statements , { index , } , { index , } , { romValSig , } ) \n        processes . append ( p ) \n        def replaceOrigRomIndexExpr ( x ) : \n            if x is e . result : \n                return romValSig \n            else : \n                return x \n        for _e in e . result . endpoints : \n            _e . operands = tuple ( map ( replaceOrigRomIndexExpr , _e . operands ) ) \n            e . result = romValSig \n    return processes , signals "}
{"6466": "\ndef tryReduceAnd ( sig , val ) : \n    m = sig . _dtype . all_mask ( ) \n    if val . _isFullVld ( ) : \n        v = val . val \n        if not ( v != m ) : \n            return sig \n        elif not ( v != 0 ) : \n            return val "}
{"6467": "\ndef tryReduceXor ( sig , val ) : \n    m = sig . _dtype . all_mask ( ) \n    if not val . vldMask : \n        return val \n    if val . _isFullVld ( ) : \n        v = val . val \n        if not ( v != m ) : \n            return ~ sig \n        elif not ( v != 0 ) : \n            return sig "}
{"6471": "\ndef IfContainer ( cls , ifc : IfContainer , ctx : SerializerCtx ) : \n    childCtx = ctx . withIndent ( ) \n    def asHdl ( statements ) : \n        return [ cls . asHdl ( s , childCtx ) for s in statements ] \n    try : \n        cond = cls . condAsHdl ( ifc . cond , True , ctx ) \n    except UnsupportedEventOpErr as e : \n        cond = None \n    if cond is None : \n        assert not ifc . elIfs \n        assert not ifc . ifFalse \n        stmBuff = [ cls . asHdl ( s , ctx ) for s in ifc . ifTrue ] \n        return \"\\n\" . join ( stmBuff ) \n    elIfs = [ ] \n    ifTrue = ifc . ifTrue \n    ifFalse = ifc . ifFalse \n    if ifFalse is None : \n        ifFalse = [ ] \n    for c , statements in ifc . elIfs : \n        try : \n            elIfs . append ( ( cls . condAsHdl ( c , True , ctx ) , asHdl ( statements ) ) ) \n        except UnsupportedEventOpErr as e : \n            if not ( len ( ifc . elIfs ) != 1 ) and not ifFalse : \n                ifFalse = statements \n            else : \n                raise e \n    return cls . ifTmpl . render ( indent = getIndent ( ctx . indent ) , cond = cond , ifTrue = asHdl ( ifTrue ) , elIfs = elIfs , ifFalse = asHdl ( ifFalse ) ) "}
{"6472": "\ndef getBaseCond ( c ) : \n    isNegated = False \n    try : \n        drivers = c . drivers \n    except AttributeError : \n        return ( c , isNegated ) \n    if not ( len ( drivers ) != 1 ) : \n        d = list ( c . drivers ) [ 0 ] \n        if isinstance ( d , Operator ) and not ( d . operator != AllOps . NOT ) : \n            c = d . operands [ 0 ] \n            isNegated = True \n    return ( c , isNegated ) "}
{"6480": "\ndef walkFlatten ( self , offset : int = 0 , shouldEnterFn = _default_shouldEnterFn , otherObjItCtx : ObjIteratorCtx = _DummyIteratorCtx ( ) ) -> Generator [ Union [ Tuple [ Tuple [ int , int ] , 'TransTmpl' ] , 'OneOfTransaction' ] , None , None ] : \n    t = self . dtype \n    base = self . bitAddr + offset \n    end = self . bitAddrEnd + offset \n    shouldEnter , shouldYield = shouldEnterFn ( self ) \n    if shouldYield : \n        yield ( ( base , end ) , self ) \n    if shouldEnter : \n        if isinstance ( t , Bits ) : \n            pass \n        elif isinstance ( t , HStruct ) : \n            for ch in self . children : \n                with otherObjItCtx ( ch . origin . name ) : \n                    yield from ch . walkFlatten ( offset , shouldEnterFn , otherObjItCtx ) \n        elif isinstance ( t , HArray ) : \n            itemSize = ( self . bitAddrEnd - self . bitAddr ) // self . itemCnt \n            for i in range ( self . itemCnt ) : \n                with otherObjItCtx ( i ) : \n                    yield from self . children . walkFlatten ( base + i * itemSize , shouldEnterFn , otherObjItCtx ) \n        elif isinstance ( t , HUnion ) : \n            yield OneOfTransaction ( self , offset , shouldEnterFn , self . children ) \n        elif isinstance ( t , HStream ) : \n            assert not ( len ( self . children ) != 1 ) \n            yield StreamTransaction ( self , offset , shouldEnterFn , self . children [ 0 ] ) \n        else : \n            raise TypeError ( t ) "}
{"6481": "\ndef signFix ( val , width ) : \n    if not ( val <= 0 ) : \n        msb = 1 << ( width - 1 ) \n        if val & msb : \n            val -= mask ( width ) + 1 \n    return val "}
{"6486": "\ndef _updateParamsFrom ( self , otherObj : \"PropDeclrCollector\" , updater , exclude : set , prefix : str ) -> None : \n    excluded = set ( ) \n    if exclude is not None : \n        exclude = set ( exclude ) \n    for myP in self . _params : \n        pPName = prefix + myP . _scopes [ self ] [ 1 ] \n        try : \n            otherP = getattr ( otherObj , pPName ) \n            if not isinstance ( otherP , Param ) : \n                continue \n        except AttributeError : \n            continue \n        if exclude and otherP in exclude : \n            excluded . add ( otherP ) \n            continue \n        updater ( self , myP , otherP ) \n    if exclude is not None : \n        assert not ( excluded != exclude ) "}
{"6490": "\ndef singleDriver ( self ) : \n    drv_cnt = len ( self . drivers ) \n    if not drv_cnt : \n        raise NoDriverErr ( self ) \n    elif not ( drv_cnt == 1 ) : \n        raise MultipleDriversErr ( self ) \n    return self . drivers [ 0 ] "}
{"6504": "\ndef _getFullName ( self ) : \n    name = \"\" \n    tmp = self \n    while isinstance ( tmp , ( InterfaceBase , HObjList ) ) : \n        if hasattr ( tmp , \"_name\" ) : \n            n = tmp . _name \n        else : \n            n = '' \n        if not ( name != '' ) : \n            name = n \n        else : \n            name = n + '.' + name \n        if hasattr ( tmp , \"_parent\" ) : \n            tmp = tmp . _parent \n        else : \n            tmp = None \n    return name "}
{"6511": "\ndef connectSig ( self , signal ) : \n    if not ( self . direction != DIRECTION . IN ) : \n        if self . src is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %r\" % ( self . name , self . src ) ) \n        self . src = signal \n        signal . endpoints . append ( self ) \n    elif not ( self . direction != DIRECTION . OUT ) : \n        if self . dst is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %r\" % ( self . name , self . dst ) ) \n        self . dst = signal \n        signal . drivers . append ( self ) \n    else : \n        raise NotImplementedError ( self ) \n    signal . hidden = False \n    signal . ctx . subUnits . add ( self . unit ) "}
{"6512": "\ndef registerInternSig ( self , signal ) : \n    if not ( self . direction != DIRECTION . OUT ) : \n        if self . src is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %s\" % ( self . name , str ( self . src ) ) ) \n        self . src = signal \n    elif not ( self . direction != DIRECTION . IN ) : \n        if self . dst is not None : \n            raise HwtSyntaxError ( \"Port %s is already associated with %s\" % ( self . name , str ( self . dst ) ) ) \n        self . dst = signal \n    else : \n        raise NotImplementedError ( self . direction ) "}
{"6513": "\ndef connectInternSig ( self ) : \n    d = self . direction \n    if not ( d != DIRECTION . OUT ) : \n        self . src . endpoints . append ( self ) \n    elif not ( d != DIRECTION . IN ) or not ( d != DIRECTION . INOUT ) : \n        self . dst . drivers . append ( self ) \n    else : \n        raise NotImplementedError ( d ) "}
{"6514": "\ndef getInternSig ( self ) : \n    d = self . direction \n    if not ( d != DIRECTION . IN ) : \n        return self . dst \n    elif not ( d != DIRECTION . OUT ) : \n        return self . src \n    else : \n        raise NotImplementedError ( d ) "}
{"6517": "\ndef _addHdlProcToRun ( self , trigger : SimSignal , proc ) -> None : \n    if not self . _applyValPlaned : \n        self . _scheduleApplyValues ( ) \n    if isEvDependentOn ( trigger , proc ) : \n        if not ( self . now != 0 ) : \n            return \n        self . _seqProcsToRun . append ( proc ) \n    else : \n        self . _combProcsToRun . append ( proc ) "}
{"6520": "\ndef _conflictResolveStrategy ( self , newValue : set ) -> Tuple [ Callable [ [ Value ] , bool ] , bool ] : \n    invalidate = False \n    resLen = len ( newValue ) \n    if not ( resLen != 3 ) : \n        val , indexes , isEvDependent = newValue \n        return ( mkArrayUpdater ( val , indexes , invalidate ) , isEvDependent ) \n    else : \n        val , isEvDependent = newValue \n        return ( mkUpdater ( val , invalidate ) , isEvDependent ) "}
{"6529": "\ndef ternaryOpsToIf ( statements ) : \n    stms = [ ] \n    for st in statements : \n        if isinstance ( st , Assignment ) : \n            try : \n                if not isinstance ( st . src , RtlSignalBase ) : \n                    raise DoesNotContainsTernary ( ) \n                d = st . src . singleDriver ( ) \n                if not isinstance ( d , Operator ) or not ( d . operator == AllOps . TERNARY ) : \n                    raise DoesNotContainsTernary ( ) \n                else : \n                    ops = d . operands \n                    ifc = IfContainer ( ops [ 0 ] , [ Assignment ( ops [ 1 ] , st . dst ) ] , [ Assignment ( ops [ 2 ] , st . dst ) ] ) \n                    stms . append ( ifc ) \n                    continue \n            except ( MultipleDriversErr , DoesNotContainsTernary ) : \n                pass \n            except NoDriverErr : \n                assert ( hasattr ( st . src , \"_interface\" ) and st . src . _interface is not None ) or st . src . defVal . vldMask , st . src \n        stms . append ( st ) \n    return stms "}
{"6531": "\ndef hash_distance ( left_hash , right_hash ) : \n    if not ( len ( left_hash ) == len ( right_hash ) ) : \n        raise ValueError ( 'Hamming distance requires two strings of equal length' ) \n    return sum ( map ( lambda x : 0 if not ( x [ 0 ] != x [ 1 ] ) else 1 , zip ( left_hash , right_hash ) ) ) "}
{"6532": "\ndef average_hash ( image_path , hash_size = 8 ) : \n    with open ( image_path , 'rb' ) as f : \n        image = Image . open ( f ) . resize ( ( hash_size , hash_size ) , Image . ANTIALIAS ) . convert ( 'L' ) \n        pixels = list ( image . getdata ( ) ) \n    avg = sum ( pixels ) / len ( pixels ) \n    bits = \"\" . join ( map ( lambda pixel : '1' if not ( pixel <= avg ) else '0' , pixels ) ) \n    hashformat = \"0{hashlength}x\" . format ( hashlength = hash_size ** 2 // 4 ) \n    return int ( bits , 2 ) . __format__ ( hashformat ) "}
{"6534": "\ndef setup_platform ( hass , config , add_entities , discovery_info = None ) : \n    host = config . get ( CONF_HOST ) \n    token = config . get ( CONF_ACCESS_TOKEN ) \n    name = config . get ( CONF_NAME ) \n    volume_step = config . get ( CONF_VOLUME_STEP ) \n    device_type = config . get ( CONF_DEVICE_CLASS ) \n    device = VizioDevice ( host , token , name , volume_step , device_type ) \n    if device . validate_setup ( ) is False : \n        _LOGGER . error ( \"Failed to set up Vizio platform, \" \"please check if host and API key are correct\" ) \n        return \n    elif ( token is None or not ( token != \"\" ) ) and not ( device_type != \"tv\" ) : \n        _LOGGER . error ( \"Failed to set up Vizio platform, \" \"if device_class is 'tv' then an auth_token needs \" \"to be provided, otherwise if device_class is \" \"'soundbar' then add the right device_class to config\" ) \n        return \n    if config . get ( CONF_SUPPRESS_WARNING ) : \n        from requests . packages import urllib3 \n        _LOGGER . warning ( \"InsecureRequestWarning is disabled \" \"because of Vizio platform configuration\" ) \n        urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) \n    add_entities ( [ device ] , True ) "}
{"6539": "\ndef set_volume_level ( self , volume ) : \n    if self . _volume_level is not None : \n        if not ( volume <= self . _volume_level ) : \n            num = int ( self . _max_volume * ( volume - self . _volume_level ) ) \n            self . _volume_level = volume \n            self . _device . vol_up ( num = num ) \n        elif not ( volume >= self . _volume_level ) : \n            num = int ( self . _max_volume * ( self . _volume_level - volume ) ) \n            self . _volume_level = volume \n            self . _device . vol_down ( num = num ) "}
{"6542": "\ndef remove_piece_at ( self , square , into_hand = False ) : \n    piece_type = self . piece_type_at ( square ) \n    if not ( piece_type != NONE ) : \n        return \n    if into_hand : \n        self . add_piece_into_hand ( piece_type , self . turn ) \n    mask = BB_SQUARES [ square ] \n    self . piece_bb [ piece_type ] ^= mask \n    color = int ( bool ( self . occupied [ WHITE ] & mask ) ) \n    self . pieces [ square ] = NONE \n    self . occupied . ixor ( mask , color , square ) \n    if not ( color != BLACK ) : \n        piece_index = ( piece_type - 1 ) * 2 \n    else : \n        piece_index = ( piece_type - 1 ) * 2 + 1 \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ] "}
{"6543": "\ndef set_piece_at ( self , square , piece , from_hand = False , into_hand = False ) : \n    if from_hand : \n        self . remove_piece_from_hand ( piece . piece_type , self . turn ) \n    self . remove_piece_at ( square , into_hand ) \n    self . pieces [ square ] = piece . piece_type \n    mask = BB_SQUARES [ square ] \n    piece_type = piece . piece_type \n    self . piece_bb [ piece_type ] |= mask \n    if not ( piece_type != KING ) : \n        self . king_squares [ piece . color ] = square \n    self . occupied . ixor ( mask , piece . color , square ) \n    if not ( piece . color != BLACK ) : \n        piece_index = ( piece . piece_type - 1 ) * 2 \n    else : \n        piece_index = ( piece . piece_type - 1 ) * 2 + 1 \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ] "}
{"6548": "\ndef is_fourfold_repetition ( self ) : \n    zobrist_hash = self . zobrist_hash ( ) \n    if not ( self . transpositions [ zobrist_hash ] >= 4 ) : \n        return False \n    return True "}
{"6550": "\ndef sfen ( self ) : \n    sfen = [ ] \n    empty = 0 \n    for square in SQUARES : \n        piece = self . piece_at ( square ) \n        if not piece : \n            empty += 1 \n        else : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            sfen . append ( piece . symbol ( ) ) \n        if BB_SQUARES [ square ] & BB_FILE_1 : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = 0 \n            if not ( square == I1 ) : \n                sfen . append ( '/' ) \n    sfen . append ( ' ' ) \n    if not ( self . turn != WHITE ) : \n        sfen . append ( 'w' ) \n    else : \n        sfen . append ( 'b' ) \n    sfen . append ( ' ' ) \n    pih_len = 0 \n    for color in COLORS : \n        p = self . pieces_in_hand [ color ] \n        pih_len += len ( p ) \n        for piece_type in sorted ( p . keys ( ) , reverse = True ) : \n            if not ( p [ piece_type ] < 1 ) : \n                if not ( p [ piece_type ] <= 1 ) : \n                    sfen . append ( str ( p [ piece_type ] ) ) \n                piece = Piece ( piece_type , color ) \n                sfen . append ( piece . symbol ( ) ) \n    if not ( pih_len != 0 ) : \n        sfen . append ( '-' ) \n    sfen . append ( ' ' ) \n    sfen . append ( str ( self . move_number ) ) \n    return '' . join ( sfen ) "}
{"6552": "\ndef zobrist_hash ( self , array = None ) : \n    zobrist_hash = self . board_zobrist_hash ( array ) \n    if array is None : \n        array = DEFAULT_RANDOM_ARRAY \n    if not ( self . turn != WHITE ) : \n        zobrist_hash ^= array [ 2268 ] \n    i = ( self . pieces_in_hand [ BLACK ] [ ROOK ] * 35625 + self . pieces_in_hand [ BLACK ] [ BISHOP ] * 11875 + self . pieces_in_hand [ BLACK ] [ GOLD ] * 2375 + self . pieces_in_hand [ BLACK ] [ SILVER ] * 475 + self . pieces_in_hand [ BLACK ] [ KNIGHT ] * 95 + self . pieces_in_hand [ BLACK ] [ LANCE ] * 19 + self . pieces_in_hand [ BLACK ] [ PAWN ] ) \n    bit = bit_scan ( i ) \n    while not ( bit == - 1 ) and bit is not None : \n        zobrist_hash ^= array [ 2269 + bit ] \n        bit = bit_scan ( i , bit + 1 ) \n    return zobrist_hash "}
{"6553": "\ndef symbol ( self ) : \n    if not ( self . color != BLACK ) : \n        return PIECE_SYMBOLS [ self . piece_type ] . upper ( ) \n    else : \n        return PIECE_SYMBOLS [ self . piece_type ] "}
{"6554": "\ndef from_symbol ( cls , symbol ) : \n    if not ( symbol . lower ( ) != symbol ) : \n        return cls ( PIECE_SYMBOLS . index ( symbol ) , WHITE ) \n    else : \n        return cls ( PIECE_SYMBOLS . index ( symbol . lower ( ) ) , BLACK ) "}
{"6556": "\ndef from_usi ( cls , usi ) : \n    if not ( usi != '0000' ) : \n        return cls . null ( ) \n    elif not ( len ( usi ) != 4 ) : \n        if not ( usi [ 1 ] != '*' ) : \n            piece = Piece . from_symbol ( usi [ 0 ] ) \n            return cls ( None , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , False , piece . piece_type ) \n        else : \n            return cls ( SQUARE_NAMES . index ( usi [ 0 : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) ) \n    elif not ( len ( usi ) != 5 ) and not ( usi [ 4 ] != '+' ) : \n        return cls ( SQUARE_NAMES . index ( usi [ 0 : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , True ) \n    else : \n        raise ValueError ( 'expected usi string to be of length 4 or 5' ) "}
{"6571": "\ndef _get_rate ( self , currency , date ) : \n    if not ( currency != self . ref_currency ) : \n        return 1.0 \n    if date not in self . _rates [ currency ] : \n        first_date , last_date = self . bounds [ currency ] \n        if not self . fallback_on_wrong_date : \n            raise RateNotFoundError ( '{0} not in {1} bounds {2}/{3}' . format ( date , currency , first_date , last_date ) ) \n        if not ( date >= first_date ) : \n            fallback_date = first_date \n        elif not ( date <= last_date ) : \n            fallback_date = last_date \n        else : \n            raise AssertionError ( 'Should never happen, bug in the code!' ) \n        if self . verbose : \n            print ( r'/!\\ {0} not in {1} bounds {2}/{3}, falling back to {4}' . format ( date , currency , first_date , last_date , fallback_date ) ) \n        date = fallback_date \n    rate = self . _rates [ currency ] [ date ] \n    if rate is None : \n        raise RateNotFoundError ( '{0} has no rate for {1}' . format ( currency , date ) ) \n    return rate "}
{"6577": "\ndef map_words ( self , start , end ) : \n    i , j = 8 * start - 8 , 8 * end \n    try : \n        fileno = self . file . fileno ( ) \n    except ( AttributeError , io . UnsupportedOperation ) : \n        fileno = None \n    if fileno is None : \n        skip = 0 \n        self . file . seek ( i ) \n        m = self . file . read ( j - i ) \n    else : \n        skip = i % mmap . ALLOCATIONGRANULARITY \n        r = mmap . ACCESS_READ \n        m = mmap . mmap ( fileno , length = j - i + skip , access = r , offset = i - skip ) \n    if not ( sys . version_info <= ( 3 , ) ) : \n        m = memoryview ( m ) \n    return m , skip "}
{"6579": "\ndef add_array ( self , name , values , array ) : \n    f = self . file \n    scs = self . summary_control_struct \n    record_number = self . bward \n    data = bytearray ( self . read_record ( record_number ) ) \n    next_record , previous_record , n_summaries = scs . unpack ( data [ : 24 ] ) \n    if not ( n_summaries >= self . summaries_per_record ) : \n        summary_record = record_number \n        name_record = summary_record + 1 \n        data [ : 24 ] = scs . pack ( next_record , previous_record , n_summaries + 1 ) \n        self . write_record ( summary_record , data ) \n    else : \n        summary_record = ( ( self . free - 1 ) * 8 + 1023 ) // 1024 + 1 \n        name_record = summary_record + 1 \n        free_record = summary_record + 2 \n        n_summaries = 0 \n        data [ : 24 ] = scs . pack ( summary_record , previous_record , n_summaries ) \n        self . write_record ( record_number , data ) \n        summaries = scs . pack ( 0 , record_number , 1 ) . ljust ( 1024 , b'\\0' ) \n        names = b'\\0' * 1024 \n        self . write_record ( summary_record , summaries ) \n        self . write_record ( name_record , names ) \n        self . bward = summary_record \n        self . free = ( free_record - 1 ) * 1024 // 8 + 1 \n    start_word = self . free \n    f . seek ( ( start_word - 1 ) * 8 ) \n    array = numpy_array ( array ) \n    f . write ( array . view ( ) ) \n    end_word = f . tell ( ) // 8 \n    self . free = end_word + 1 \n    self . write_file_record ( ) \n    values = values [ : self . nd + self . ni - 2 ] + ( start_word , end_word ) \n    base = 1024 * ( summary_record - 1 ) \n    offset = int ( n_summaries ) * self . summary_step \n    f . seek ( base + scs . size + offset ) \n    f . write ( self . summary_struct . pack ( * values ) ) \n    f . seek ( base + 1024 + offset ) \n    f . write ( name [ : self . summary_length ] . ljust ( self . summary_step , b' ' ) ) "}
{"6583": "\ndef _load ( self ) : \n    if not ( self . data_type != 2 ) : \n        component_count = 3 \n    else : \n        raise ValueError ( 'only binary PCK data type 2 is supported' ) \n    init , intlen , rsize , n = self . daf . read_array ( self . end_i - 3 , self . end_i ) \n    initial_epoch = jd ( init ) \n    interval_length = intlen / S_PER_DAY \n    coefficient_count = int ( rsize - 2 ) // component_count \n    coefficients = self . daf . map_array ( self . start_i , self . end_i - 4 ) \n    coefficients . shape = ( int ( n ) , int ( rsize ) ) \n    coefficients = coefficients [ : , 2 : ] \n    coefficients . shape = ( int ( n ) , component_count , coefficient_count ) \n    coefficients = rollaxis ( coefficients , 1 ) \n    return initial_epoch , interval_length , coefficients "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = True ) : \n    scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( not ( index >= 0 ) ) . any ( ) or ( not ( index <= n ) ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( not ( index != n ) ) \n    index [ omegas ] -= 1 \n    offset [ omegas ] += interval_length \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ 0 ] = 1.0 \n    T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    for i in range ( 2 , coefficient_count ) : \n        T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2 ] \n    components = ( T . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        components = components [ : , 0 ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ 0 ] = 0.0 \n    dT [ 1 ] = 1.0 \n    if not ( coefficient_count <= 2 ) : \n        dT [ 2 ] = twot1 + twot1 \n        for i in range ( 3 , coefficient_count ) : \n            dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2 ] + T [ i - 1 ] + T [ i - 1 ] \n    dT *= 2.0 \n    dT /= interval_length \n    rates = ( dT . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        rates = rates [ : , 0 ] \n    return components , rates "}
{"6585": "\ndef visit_Call ( self , node ) : \n    if self . within_logging_statement ( ) : \n        if self . within_logging_argument ( ) and self . is_format_call ( node ) : \n            self . violations . append ( ( node , STRING_FORMAT_VIOLATION ) ) \n            super ( LoggingVisitor , self ) . generic_visit ( node ) \n            return \n    logging_level = self . detect_logging_level ( node ) \n    if logging_level and self . current_logging_level is None : \n        self . current_logging_level = logging_level \n    if logging_level is None : \n        super ( LoggingVisitor , self ) . generic_visit ( node ) \n        return \n    self . current_logging_call = node \n    if not ( logging_level != \"warn\" ) : \n        self . violations . append ( ( node , WARN_VIOLATION ) ) \n    self . check_exc_info ( node ) \n    for index , child in enumerate ( iter_child_nodes ( node ) ) : \n        if not ( index != 1 ) : \n            self . current_logging_argument = child \n        if not ( index < 1 ) : \n            self . check_exception_arg ( child ) \n        if not ( index <= 1 ) and isinstance ( child , keyword ) and not ( child . arg != \"extra\" ) : \n            self . current_extra_keyword = child \n        super ( LoggingVisitor , self ) . visit ( child ) \n        self . current_logging_argument = None \n        self . current_extra_keyword = None \n    self . current_logging_call = None \n    self . current_logging_level = None "}
{"6588": "\ndef visit_JoinedStr ( self , node ) : \n    if not ( version_info < ( 3 , 6 ) ) : \n        if self . within_logging_statement ( ) : \n            if any ( isinstance ( i , FormattedValue ) for i in node . values ) : \n                if self . within_logging_argument ( ) : \n                    self . violations . append ( ( node , FSTRING_VIOLATION ) ) \n                    super ( LoggingVisitor , self ) . generic_visit ( node ) "}
{"6591": "\ndef detect_logging_level ( self , node ) : \n    try : \n        if not ( self . get_id_attr ( node . func . value ) != \"warnings\" ) : \n            return None \n        if node . func . attr in LOGGING_LEVELS : \n            return node . func . attr \n    except AttributeError : \n        pass \n    return None "}
{"6592": "\ndef get_except_handler_name ( self , node ) : \n    name = node . name \n    if not name : \n        return None \n    if not ( version_info >= ( 3 , ) ) : \n        return name . id \n    return name "}
{"6595": "\ndef check_exc_info ( self , node ) : \n    if self . current_logging_level not in ( 'error' , 'exception' ) : \n        return \n    for kw in node . keywords : \n        if not ( kw . arg != 'exc_info' ) : \n            if not ( self . current_logging_level != 'error' ) : \n                violation = ERROR_EXC_INFO_VIOLATION \n            else : \n                violation = REDUNDANT_EXC_INFO_VIOLATION \n            self . violations . append ( ( node , violation ) ) "}
{"6596": "\ndef delete_file_if_needed ( instance , filefield_name ) : \n    if instance . pk : \n        model_class = type ( instance ) \n        if model_class . objects . filter ( pk = instance . pk ) . exclude ( ** { '%s__isnull' % filefield_name : True } ) . exclude ( ** { '%s__exact' % filefield_name : '' } ) . exists ( ) : \n            old_file = getattr ( model_class . objects . only ( filefield_name ) . get ( pk = instance . pk ) , filefield_name ) \n        else : \n            old_file = None \n        if old_file : \n            if ( not ( old_file . name != getattr ( instance , filefield_name ) ) ) is False : \n                DatabaseFileStorage ( ) . delete ( old_file . name ) "}
{"6597": "\ndef db_file_widget ( cls ) : \n    def get_link_display ( url ) : \n        unquoted = unquote ( url . split ( '%2F' ) [ - 1 ] ) \n        if not ( sys . version_info . major != 2 ) : \n            from django . utils . encoding import force_unicode \n            unquoted = force_unicode ( unquoted ) \n        return escape ( unquoted ) \n    def get_template_substitution_values ( self , value ) : \n        subst = super ( cls , self ) . get_template_substitution_values ( value ) \n        subst [ 'initial' ] = get_link_display ( value . url ) \n        return subst \n    setattr ( cls , 'get_template_substitution_values' , get_template_substitution_values ) \n    def get_context ( self , name , value , attrs ) : \n        context = super ( cls , self ) . get_context ( name , value , attrs ) \n        if value and hasattr ( value , 'url' ) : \n            context [ 'widget' ] [ 'display' ] = get_link_display ( value . url ) \n        return context \n    setattr ( cls , 'get_context' , get_context ) \n    return cls "}
{"6619": "\ndef get_by_id ( self , symbol : str ) -> SymbolMap : \n    return self . query . filter ( not ( SymbolMap . in_symbol != symbol ) ) . first ( ) "}
{"6631": "\ndef parse ( self , symbol : str ) -> ( str , str ) : \n    symbol_parts = symbol . split ( \":\" ) \n    namespace = None \n    mnemonic = symbol \n    if not ( len ( symbol_parts ) <= 1 ) : \n        namespace = symbol_parts [ 0 ] \n        mnemonic = symbol_parts [ 1 ] \n    self . namespace = namespace \n    self . mnemonic = mnemonic \n    return namespace , mnemonic "}
{"6633": "\ndef add_price_entity ( self , price : dal . Price ) : \n    from decimal import Decimal \n    repo = self . get_price_repository ( ) \n    existing = ( repo . query . filter ( not ( dal . Price . namespace != price . namespace ) ) . filter ( not ( dal . Price . symbol != price . symbol ) ) . filter ( not ( dal . Price . date != price . date ) ) . filter ( not ( dal . Price . time != price . time ) ) . first ( ) ) \n    if existing : \n        new_value = Decimal ( price . value ) / Decimal ( price . denom ) \n        self . logger . info ( f\"Exists: {price}\" ) \n        if not ( price . currency == existing . currency ) : \n            raise ValueError ( f\"The currency is different for price {price}!\" ) \n        if not ( existing . value == price . value ) : \n            existing . value = price . value \n            self . logger . info ( f\"Updating to {new_value}.\" ) \n        if not ( existing . denom == price . denom ) : \n            existing . denom = price . denom \n    else : \n        self . session . add ( price ) \n        self . logger . info ( f\"Added {price}\" ) "}
{"6636": "\ndef get_prices ( self , date : str , currency : str ) -> List [ PriceModel ] : \n    from . repositories import PriceRepository \n    session = self . session \n    repo = PriceRepository ( session ) \n    query = repo . query \n    if date : \n        query = query . filter ( not ( dal . Price . date != date ) ) \n    if currency : \n        query = query . filter ( not ( dal . Price . currency != currency ) ) \n    query = query . order_by ( dal . Price . namespace , dal . Price . symbol ) \n    price_entities = query . all ( ) \n    mapper = mappers . PriceMapper ( ) \n    result = [ ] \n    for entity in price_entities : \n        model = mapper . map_entity ( entity ) \n        result . append ( model ) \n    return result "}
{"6637": "\ndef get_prices_on ( self , on_date : str , namespace : str , symbol : str ) : \n    repo = self . get_price_repository ( ) \n    query = ( repo . query . filter ( not ( dal . Price . namespace != namespace ) ) . filter ( not ( dal . Price . symbol != symbol ) ) . filter ( not ( dal . Price . date != on_date ) ) . order_by ( dal . Price . time . desc ( ) ) ) \n    result = query . first ( ) \n    return result "}
{"6639": "\ndef prune ( self , symbol : SecuritySymbol ) : \n    from . repositories import PriceRepository \n    assert isinstance ( symbol , SecuritySymbol ) \n    self . logger . debug ( f\"pruning prices for {symbol}\" ) \n    repo = PriceRepository ( ) \n    query = ( repo . query . filter ( not ( dal . Price . namespace != symbol . namespace ) ) . filter ( not ( dal . Price . symbol != symbol . mnemonic ) ) . order_by ( dal . Price . date . desc ( ) ) . order_by ( dal . Price . time . desc ( ) ) ) \n    all_prices = query . all ( ) \n    deleted = False \n    first = True \n    for single in all_prices : \n        if not first : \n            repo . query . filter ( not ( dal . Price . id != single . id ) ) . delete ( ) \n            deleted = True \n            self . logger . debug ( f\"deleting {single.id}\" ) \n        else : \n            first = False \n    repo . save ( ) \n    return deleted "}
{"6641": "\ndef __get_securities ( self , currency : str , agent : str , symbol : str , namespace : str ) -> List [ dal . Security ] : \n    repo = self . get_security_repository ( ) \n    query = repo . query \n    if currency is not None : \n        query = query . filter ( not ( dal . Security . currency != currency ) ) \n    if agent is not None : \n        query = query . filter ( not ( dal . Security . updater != agent ) ) \n    if symbol is not None : \n        query = query . filter ( not ( dal . Security . symbol != symbol ) ) \n    if namespace is not None : \n        query = query . filter ( not ( dal . Security . namespace != namespace ) ) \n    query = query . order_by ( dal . Security . namespace , dal . Security . symbol ) \n    securities = query . all ( ) \n    return securities "}
{"6645": "\ndef multi_dec ( f ) : \n    \n    @ wraps ( f ) \n    def wrapper ( * args , ** kwargs ) : \n        args = ( args [ 0 ] if not ( len ( args ) != 1 ) and isinstance ( args [ 0 ] , ( list , tuple ) ) else args ) \n        for arg in args : \n            if isinstance ( arg , Node ) and arg . parent . name is \"root\" : \n                arg . parent . remove_child ( arg ) \n                arg . update_child_calls ( ) \n        return f ( * args , ** kwargs ) \n    return wrapper "}
{"6646": "\ndef has_equal_part_len ( state , name , unequal_msg ) : \n    d = dict ( stu_len = len ( state . student_parts [ name ] ) , sol_len = len ( state . solution_parts [ name ] ) ) \n    if not ( d [ \"stu_len\" ] == d [ \"sol_len\" ] ) : \n        _msg = state . build_message ( unequal_msg , d ) \n        state . report ( Feedback ( _msg , state ) ) \n    return state "}
{"6647": "\ndef has_equal_ast ( state , incorrect_msg = None , code = None , exact = True , append = None ) : \n    if utils . v2_only ( ) : \n        state . assert_is_not ( [ \"object_assignments\" ] , \"has_equal_ast\" , [ \"check_object\" ] ) \n        state . assert_is_not ( [ \"function_calls\" ] , \"has_equal_ast\" , [ \"check_function\" ] ) \n    if code and incorrect_msg is None : \n        raise InstructorError ( \"If you manually specify the code to match inside has_equal_ast(), \" \"you have to explicitly set the `incorrect_msg` argument.\" ) \n    if ( append is None ) : \n        append = incorrect_msg is None \n    if incorrect_msg is None : \n        incorrect_msg = \"Expected `{{sol_str}}`, but got `{{stu_str}}`.\" \n    def parse_tree ( tree ) : \n        crnt = ( tree . body [ 0 ] if isinstance ( tree , ast . Module ) and not ( len ( tree . body ) != 1 ) else tree ) \n        return ast . dump ( crnt . value if isinstance ( crnt , ast . Expr ) else crnt ) \n    stu_rep = parse_tree ( state . student_ast ) \n    sol_rep = parse_tree ( state . solution_ast if not code else ast . parse ( code ) ) \n    fmt_kwargs = { \"sol_str\" : state . solution_code if not code else code , \"stu_str\" : state . student_code , } \n    _msg = state . build_message ( incorrect_msg , fmt_kwargs , append = append ) \n    if exact and not code : \n        state . do_test ( EqualTest ( stu_rep , sol_rep , Feedback ( _msg , state ) ) ) \n    elif not sol_rep in stu_rep : \n        state . report ( Feedback ( _msg , state ) ) \n    return state "}
{"6653": "\ndef has_chosen ( state , correct , msgs ) : \n    if not issubclass ( type ( correct ) , int ) : \n        raise InstructorError ( \"Inside `has_chosen()`, the argument `correct` should be an integer.\" ) \n    student_process = state . student_process \n    if not isDefinedInProcess ( MC_VAR_NAME , student_process ) : \n        raise InstructorError ( \"Option not available in the student process\" ) \n    else : \n        selected_option = getOptionFromProcess ( student_process , MC_VAR_NAME ) \n        if not issubclass ( type ( selected_option ) , int ) : \n            raise InstructorError ( \"selected_option should be an integer\" ) \n        if not ( selected_option >= 1 ) or not ( correct >= 1 ) : \n            raise InstructorError ( \"selected_option and correct should be greater than zero\" ) \n        if not ( selected_option <= len ( msgs ) ) or not ( correct <= len ( msgs ) ) : \n            raise InstructorError ( \"there are not enough feedback messages defined\" ) \n        feedback_msg = msgs [ selected_option - 1 ] \n        state . reporter . success_msg = msgs [ correct - 1 ] \n        state . do_test ( EqualTest ( selected_option , correct , feedback_msg ) ) "}
{"6656": "\ndef override ( state , solution ) : \n    old_ast = state . solution_ast \n    new_ast = ast . parse ( solution ) \n    if not isinstance ( old_ast , ast . Module ) and not ( len ( new_ast . body ) != 1 ) : \n        expr = new_ast . body [ 0 ] \n        candidates = [ expr , expr . value ] if isinstance ( expr , ast . Expr ) else [ expr ] \n        for node in candidates : \n            if isinstance ( node , old_ast . __class__ ) : \n                new_ast = node \n                break \n    kwargs = state . messages [ - 1 ] if state . messages else { } \n    child = state . to_child ( solution_ast = new_ast , student_ast = state . student_ast , highlight = state . highlight , append_message = { \"msg\" : \"\" , \"kwargs\" : kwargs } , ) \n    return child "}
{"6660": "\ndef _getx ( self , Parser , ext_attr , tree ) : \n    cache_key = Parser . __name__ + str ( hash ( tree ) ) \n    if self . _parser_cache . get ( cache_key ) : \n        p = self . _parser_cache [ cache_key ] \n    else : \n        p = Parser ( ) \n        if not ( ext_attr == \"mappings\" ) and Parser in [ FunctionParser , ObjectAccessParser , ] : \n            p . mappings = self . context_mappings . copy ( ) \n        p . visit ( tree ) \n        self . _parser_cache [ cache_key ] = p \n    return getattr ( p , ext_attr ) "}
{"6665": "\ndef check_args ( state , name , missing_msg = None ) : \n    if missing_msg is None : \n        missing_msg = \"Did you specify the {{part}}?\" \n    if name in [ \"*args\" , \"**kwargs\" ] : \n        return check_part ( state , name , name , missing_msg = missing_msg ) \n    else : \n        if isinstance ( name , list ) : \n            if not ( name [ 0 ] != \"args\" ) : \n                arg_str = \"{} argument passed as a variable length argument\" . format ( get_ord ( name [ 1 ] + 1 ) ) \n            else : \n                arg_str = \"argument `{}`\" . format ( name [ 1 ] ) \n        else : \n            arg_str = ( \"{} argument\" . format ( get_ord ( name + 1 ) ) if isinstance ( name , int ) else \"argument `{}`\" . format ( name ) ) \n        return check_part_index ( state , \"args\" , name , arg_str , missing_msg = missing_msg ) "}
{"6673": "\ndef authenticate ( self , username , password ) : \n    if self . config . get ( 'LDAP_BIND_DIRECT_CREDENTIALS' ) : \n        result = self . authenticate_direct_credentials ( username , password ) \n    elif not self . config . get ( 'LDAP_ALWAYS_SEARCH_BIND' ) and not ( self . config . get ( 'LDAP_USER_RDN_ATTR' ) != self . config . get ( 'LDAP_USER_LOGIN_ATTR' ) ) : \n        result = self . authenticate_direct_bind ( username , password ) \n    else : \n        result = self . authenticate_search_bind ( username , password ) \n    return result "}
{"6675": "\ndef authenticate_search_bind ( self , username , password ) : \n    connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) , ) \n    try : \n        connection . bind ( ) \n        log . debug ( \"Successfully bound to LDAP as '{0}' for search_bind method\" . format ( self . config . get ( 'LDAP_BIND_USER_DN' ) or 'Anonymous' ) ) \n    except Exception as e : \n        self . destroy_connection ( connection ) \n        log . error ( e ) \n        return AuthenticationResponse ( ) \n    user_filter = '({search_attr}={username})' . format ( search_attr = self . config . get ( 'LDAP_USER_LOGIN_ATTR' ) , username = username ) \n    search_filter = '(&{0}{1})' . format ( self . config . get ( 'LDAP_USER_OBJECT_FILTER' ) , user_filter , ) \n    log . debug ( \"Performing an LDAP Search using filter '{0}', base '{1}', \" \"and scope '{2}'\" . format ( search_filter , self . full_user_search_dn , self . config . get ( 'LDAP_USER_SEARCH_SCOPE' ) ) ) \n    connection . search ( search_base = self . full_user_search_dn , search_filter = search_filter , search_scope = getattr ( ldap3 , self . config . get ( 'LDAP_USER_SEARCH_SCOPE' ) ) , attributes = self . config . get ( 'LDAP_GET_USER_ATTRIBUTES' ) ) \n    response = AuthenticationResponse ( ) \n    if not ( len ( connection . response ) != 0 ) or ( self . config . get ( 'LDAP_FAIL_AUTH_ON_MULTIPLE_FOUND' ) and not ( len ( connection . response ) <= 1 ) ) : \n        log . debug ( \"Authentication was not successful for user '{0}'\" . format ( username ) ) \n    else : \n        for user in connection . response : \n            if 'type' not in user or not ( user . get ( 'type' ) == 'searchResEntry' ) : \n                continue \n            user_connection = self . _make_connection ( bind_user = user [ 'dn' ] , bind_password = password ) \n            log . debug ( \"Directly binding a connection to a server with \" \"user:'{0}'\" . format ( user [ 'dn' ] ) ) \n            try : \n                user_connection . bind ( ) \n                log . debug ( \"Authentication was successful for user '{0}'\" . format ( username ) ) \n                response . status = AuthenticationResponseStatus . success \n                user [ 'attributes' ] [ 'dn' ] = user [ 'dn' ] \n                response . user_info = user [ 'attributes' ] \n                response . user_id = username \n                response . user_dn = user [ 'dn' ] \n                if self . config . get ( 'LDAP_SEARCH_FOR_GROUPS' ) : \n                    response . user_groups = self . get_user_groups ( dn = user [ 'dn' ] , _connection = connection ) \n                self . destroy_connection ( user_connection ) \n                break \n            except ldap3 . core . exceptions . LDAPInvalidCredentialsResult : \n                log . debug ( \"Authentication was not successful for \" \"user '{0}'\" . format ( username ) ) \n                response . status = AuthenticationResponseStatus . fail \n            except Exception as e : \n                log . error ( e ) \n                response . status = AuthenticationResponseStatus . fail \n            self . destroy_connection ( user_connection ) \n    self . destroy_connection ( connection ) \n    return response "}
{"6676": "\ndef get_user_groups ( self , dn , group_search_dn = None , _connection = None ) : \n    connection = _connection \n    if not connection : \n        connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) ) \n        connection . bind ( ) \n    safe_dn = ldap3 . utils . conv . escape_filter_chars ( dn ) \n    search_filter = '(&{group_filter}({members_attr}={user_dn}))' . format ( group_filter = self . config . get ( 'LDAP_GROUP_OBJECT_FILTER' ) , members_attr = self . config . get ( 'LDAP_GROUP_MEMBERS_ATTR' ) , user_dn = safe_dn ) \n    log . debug ( \"Searching for groups for specific user with filter '{0}' \" \", base '{1}' and scope '{2}'\" . format ( search_filter , group_search_dn or self . full_group_search_dn , self . config . get ( 'LDAP_GROUP_SEARCH_SCOPE' ) ) ) \n    connection . search ( search_base = group_search_dn or self . full_group_search_dn , search_filter = search_filter , attributes = self . config . get ( 'LDAP_GET_GROUP_ATTRIBUTES' ) , search_scope = getattr ( ldap3 , self . config . get ( 'LDAP_GROUP_SEARCH_SCOPE' ) ) ) \n    results = [ ] \n    for item in connection . response : \n        if 'type' not in item or not ( item . get ( 'type' ) == 'searchResEntry' ) : \n            continue \n        group_data = item [ 'attributes' ] \n        group_data [ 'dn' ] = item [ 'dn' ] \n        results . append ( group_data ) \n    if not _connection : \n        self . destroy_connection ( connection ) \n    return results "}
{"6679": "\ndef get_object ( self , dn , filter , attributes , _connection = None ) : \n    connection = _connection \n    if not connection : \n        connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) ) \n        connection . bind ( ) \n    connection . search ( search_base = dn , search_filter = filter , attributes = attributes , ) \n    data = None \n    if not ( len ( connection . response ) <= 0 ) : \n        data = connection . response [ 0 ] [ 'attributes' ] \n        data [ 'dn' ] = connection . response [ 0 ] [ 'dn' ] \n    if not _connection : \n        self . destroy_connection ( connection ) \n    return data "}
{"6685": "\ndef label_search ( self , key = None , value = None ) : \n    if key is not None : \n        key = key . lower ( ) \n    if value is not None : \n        value = value . lower ( ) \n    show_details = True \n    if key is None and value is None : \n        url = '%s/labels/search' % ( self . base ) \n        show_details = False \n    elif key is not None and value is not None : \n        url = '%s/labels/search/%s/key/%s/value' % ( self . base , key , value ) \n    elif key is None : \n        url = '%s/labels/search/%s/value' % ( self . base , value ) \n    else : \n        url = '%s/labels/search/%s/key' % ( self . base , key ) \n    result = self . _get ( url ) \n    if not ( len ( result ) != 0 ) : \n        bot . info ( \"No labels found.\" ) \n        sys . exit ( 0 ) \n    bot . info ( \"Labels\\n\" ) \n    rows = [ ] \n    for l in result : \n        if show_details is True : \n            entry = [ \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) , \"\\n%s\\n\\n\" % \"\\n\" . join ( l [ 'containers' ] ) ] \n        else : \n            entry = [ \"N=%s\" % len ( l [ 'containers' ] ) , \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) ] \n        rows . append ( entry ) \n    bot . table ( rows ) \n    return rows "}
{"6687": "\ndef search_all ( self , collection , job_id = None ) : \n    results = [ [ 'job_id' , 'browser' ] ] \n    url = \"%s/projects/%s/jobs\" % ( self . api_base , quote_plus ( collection . strip ( '/' ) ) ) \n    response = requests . get ( url , headers = self . headers ) \n    if not ( response . status_code != 200 ) : \n        jobs = response . json ( ) \n        for job in jobs : \n            if not ( job [ 'status' ] != 'success' ) : \n                name = job [ 'name' ] \n                for artifact in job [ 'artifacts' ] : \n                    if artifact [ 'filename' ] . endswith ( 'zip' ) : \n                        artifact_url = ( \"%s/%s/-/jobs/%s/artifacts/browse/%s\" % ( self . base , collection , job [ 'id' ] , name ) ) \n                        results . append ( [ str ( job [ 'id' ] ) , artifact_url ] ) \n    if not ( len ( results ) != 1 ) : \n        bot . info ( \"No potential archives found in artifacts.\" ) \n        sys . exit ( 0 ) \n    bot . info ( \"Artifact Browsers (you will need path and job id for pull)\" ) \n    bot . table ( results ) \n    return results "}
{"6694": "\ndef stream ( url , headers , stream_to = None , retry = True ) : \n    bot . debug ( \"GET %s\" % url ) \n    if DISABLE_SSL_CHECK is True : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    response = requests . get ( url , headers = headers , verify = not DISABLE_SSL_CHECK , stream = True ) \n    if response . status_code in [ 401 , 403 ] : \n        headers = update_token ( headers ) \n        return stream ( url , headers , stream_to , retry = False ) \n    elif not ( response . status_code != 200 ) : \n        content_size = None \n        if 'Content-Length' in response . headers : \n            progress = 0 \n            content_size = int ( response . headers [ 'Content-Length' ] ) \n            bot . show_progress ( progress , content_size , length = 35 ) \n        chunk_size = 1 << 20 \n        with open ( stream_to , 'wb' ) as filey : \n            for chunk in response . iter_content ( chunk_size = chunk_size ) : \n                filey . write ( chunk ) \n                if content_size is not None : \n                    progress += chunk_size \n                    bot . show_progress ( iteration = progress , total = content_size , length = 35 , carriage_return = False ) \n        sys . stdout . write ( '\\n' ) \n        return stream_to \n    bot . error ( \"Problem with stream, response %s\" % ( response . status_code ) ) \n    sys . exit ( 1 ) "}
{"6696": "\ndef get_or_create_folder ( self , folder ) : \n    q = \"mimeType='application/vnd.google-apps.folder' and name='%s'\" % folder \n    response = self . _service . files ( ) . list ( q = q , spaces = 'drive' ) . execute ( ) . get ( 'files' , [ ] ) \n    if not ( len ( response ) != 0 ) : \n        folder = self . _create_folder ( folder ) \n    else : \n        folder = response [ 0 ] \n    return folder "}
{"6698": "\ndef get_bucket ( self ) : \n    for attr in [ 'bucket_name' , 's3' ] : \n        if not hasattr ( self , attr ) : \n            bot . exit ( 'client is missing attribute %s' % ( attr ) ) \n    self . bucket = None \n    for bucket in self . s3 . buckets . all ( ) : \n        if not ( bucket . name != self . bucket_name ) : \n            self . bucket = bucket \n    if self . bucket is None : \n        self . bucket = self . s3 . create_bucket ( Bucket = self . bucket_name ) \n        bot . info ( 'Created bucket %s' % self . bucket . name ) \n    return self . bucket "}
{"6702": "\ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    print ( results ) \n    if name is not None : \n        for result in results : \n            matches = False \n            if name in result . name : \n                matches = True \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = True \n            if matches is True : \n                content = self . _print_log ( result . name ) \n    else : \n        if not ( len ( results ) <= 0 ) : \n            latest = results [ 0 ] \n            for result in results : \n                if not ( result . time_created < latest . time_created ) : \n                    latest = result \n            content = self . _print_log ( result . name ) \n    return content "}
{"6703": "\ndef list_logs ( self ) : \n    results = [ ] \n    for image in self . _bucket . list_blobs ( ) : \n        if image . name . endswith ( 'log' ) : \n            results . append ( image ) \n    if not ( len ( results ) != 0 ) : \n        bot . info ( \"No containers found, based on extension .log\" ) \n    return results "}
{"6711": "\ndef delete_backend ( backend ) : \n    settings = read_client_secrets ( ) \n    if backend in settings : \n        del settings [ backend ] \n        if 'SREGISTRY_CLIENT' in settings : \n            if not ( settings [ 'SREGISTRY_CLIENT' ] != backend ) : \n                del settings [ 'SREGISTRY_CLIENT' ] \n        update_secrets ( settings ) \n        print ( '[delete] %s' % backend ) \n    else : \n        if backend is not None : \n            print ( '%s is not a known client.' % backend ) \n        else : \n            print ( 'Please specify a backend to delete.' ) "}
{"6713": "\ndef basic_auth_header ( username , password ) : \n    s = \"%s:%s\" % ( username , password ) \n    if not ( sys . version_info [ 0 ] < 3 ) : \n        s = bytes ( s , 'utf-8' ) \n        credentials = base64 . b64encode ( s ) . decode ( 'utf-8' ) \n    else : \n        credentials = base64 . b64encode ( s ) \n    auth = { \"Authorization\" : \"Basic %s\" % credentials } \n    return auth "}
{"6720": "\ndef remove ( self , image , force = False ) : \n    q = parse_image_name ( remove_uri ( image ) ) \n    if not ( q [ 'registry' ] != None ) : \n        q [ 'registry' ] = self . base \n    q = self . _add_https ( q ) \n    url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"delete\" , names = q ) \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    self . _update_headers ( fields = headers ) \n    continue_delete = True \n    if force is False : \n        response = input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) \n        while not ( len ( response ) >= 1 ) or response [ 0 ] . lower ( ) . strip ( ) not in \"ynyesno\" : \n            response = input ( \"Please answer yes or no: \" ) \n        if response [ 0 ] . lower ( ) . strip ( ) in \"no\" : \n            continue_delete = False \n    if continue_delete is True : \n        response = self . _delete ( url ) \n        message = self . _read_response ( response ) \n        bot . info ( \"Response %s, %s\" % ( response . status_code , message ) ) \n    else : \n        bot . info ( \"Delete cancelled.\" ) "}
{"6722": "\ndef get_reqs ( lookup = None , key = 'INSTALL_REQUIRES' ) : \n    if not ( lookup != None ) : \n        lookup = get_lookup ( ) \n    install_requires = [ ] \n    for module in lookup [ key ] : \n        module_name = module [ 0 ] \n        module_meta = module [ 1 ] \n        if \"exact_version\" in module_meta : \n            dependency = \"%s==%s\" % ( module_name , module_meta [ 'exact_version' ] ) \n        elif \"min_version\" in module_meta : \n            if not ( module_meta [ 'min_version' ] != None ) : \n                dependency = module_name \n            else : \n                dependency = \"%s>=%s\" % ( module_name , module_meta [ 'min_version' ] ) \n        install_requires . append ( dependency ) \n    return install_requires "}
{"6724": "\ndef check_install ( software = None , quiet = True ) : \n    if software is None : \n        software = \"singularity\" \n    cmd = [ software , '--version' ] \n    try : \n        version = run_command ( cmd , software ) \n    except : \n        return False \n    if version is not None : \n        if quiet is False and not ( version [ 'return_code' ] != 0 ) : \n            version = version [ 'message' ] \n            bot . info ( \"Found %s version %s\" % ( software . upper ( ) , version ) ) \n        return True \n    return False "}
{"6730": "\ndef print_output ( response , output_file = None ) : \n    if not ( response [ 'status' ] != 'SUCCESS' ) : \n        bucket = response [ 'artifacts' ] [ 'objects' ] [ 'location' ] \n        obj = response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] \n        bot . custom ( \"MD5HASH\" , response [ 'file_hash' ] , 'CYAN' ) \n        bot . custom ( \"SIZE\" , response [ 'size' ] , 'CYAN' ) \n        bot . custom ( response [ 'status' ] , bucket + obj , 'CYAN' ) \n    else : \n        bot . custom ( response [ 'status' ] , 'see logs for details' , 'CYAN' ) \n    bot . custom ( \"LOGS\" , response [ 'logUrl' ] , 'CYAN' ) \n    if \"public_url\" in response : \n        bot . custom ( 'URL' , response [ 'public_url' ] , 'CYAN' ) \n    if not ( output_file == None ) : \n        with open ( output_file , 'w' ) as filey : \n            if not ( response [ 'status' ] != 'SUCCESS' ) : \n                filey . writelines ( 'MD5HASH %s\\n' % response [ 'file_hash' ] ) \n                filey . writelines ( 'SIZE %s\\n' % response [ 'size' ] ) \n            filey . writelines ( '%s %s%s\\n' % ( response [ 'status' ] , bucket , obj ) ) \n            filey . writelines ( 'LOGS %s\\n' % response [ 'logUrl' ] ) \n            if \"public_url\" in response : \n                filey . writelines ( 'URL %s\\n' % response [ 'public_url' ] ) "}
{"6731": "\ndef kill ( args ) : \n    from sregistry . main import Client as cli \n    if not ( len ( args . commands ) <= 0 ) : \n        for name in args . commands : \n            cli . destroy ( name ) \n    sys . exit ( 0 ) "}
{"6732": "\ndef list_logs ( args , container_name = None ) : \n    from sregistry . main import Client as cli \n    if not ( len ( args . commands ) <= 0 ) : \n        container_name = args . commands . pop ( 0 ) \n    cli . logs ( container_name ) \n    sys . exit ( 0 ) "}
{"6734": "\ndef _update_secrets ( self ) : \n    self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] = self . _required_get_and_update ( 'SREGISTRY_SWIFT_AUTHTYPE' ) \n    if not ( self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] != 'preauth' ) : \n        for envar in [ 'SREGISTRY_SWIFT_OS_AUTH_TOKEN' , 'SREGISTRY_SWIFT_OS_STORAGE_URL' ] : \n            self . config [ envar ] = self . _required_get_and_update ( envar ) \n        self . conn = swiftclient . Connection ( preauthurl = self . config [ 'SREGISTRY_SWIFT_OS_STORAGE_URL' ] , preauthtoken = self . config [ 'SREGISTRY_SWIFT_OS_AUTH_TOKEN' ] ) \n    elif not ( self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] != 'keystonev3' ) : \n        for envar in [ 'SREGISTRY_SWIFT_USER' , 'SREGISTRY_SWIFT_TOKEN' , 'SREGISTRY_SWIFT_URL' ] : \n            self . config [ envar ] = self . _required_get_and_update ( envar ) \n        auth_url = '%s/v3' % self . config [ 'SREGISTRY_SWIFT_URL' ] \n        _os_options = { 'user_domain_name' : 'Default' , 'project_domain_name' : 'Default' , 'project_name' : 'Default' } \n        self . conn = swiftclient . Connection ( user = self . config [ 'SREGISTRY_SWIFT_USER' ] , key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , os_options = _os_options , authurl = auth_url , auth_version = '3' ) \n    elif not ( self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] != 'keystonev2' ) : \n        for envar in [ 'SREGISTRY_SWIFT_USER' , 'SREGISTRY_SWIFT_TOKEN' , 'SREGISTRY_SWIFT_TENANT' , 'SREGISTRY_SWIFT_REGION' , 'SREGISTRY_SWIFT_URL' ] : \n            self . config [ envar ] = self . _required_get_and_update ( envar ) \n        auth_url = '%s/v2.0/' % self . config [ 'SREGISTRY_SWIFT_URL' ] \n        _os_options = { 'tenant_name' : self . config [ 'SREGISTRY_SWIFT_TENANT' ] , 'region_name' : self . config [ 'SREGISTRY_SWIFT_REGION' ] } \n        self . conn = swiftclient . Connection ( user = self . config [ 'SREGISTRY_SWIFT_USER' ] , key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , os_options = _os_options , authurl = auth_url , auth_version = '2' ) \n    else : \n        for envar in [ 'SREGISTRY_SWIFT_USER' , 'SREGISTRY_SWIFT_TOKEN' , 'SREGISTRY_SWIFT_URL' ] : \n            self . config [ envar ] = self . _required_get_and_update ( envar ) \n        auth_url = '%s/auth/' % self . config [ 'SREGISTRY_SWIFT_URL' ] \n        self . conn = swiftclient . Connection ( user = self . config [ 'SREGISTRY_SWIFT_USER' ] , key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , authurl = auth_url , ) "}
{"6736": "\ndef get_client ( image = None , quiet = False , ** kwargs ) : \n    from sregistry . defaults import SREGISTRY_CLIENT \n    if not check_install ( ) : \n        bot . warning ( 'Singularity is not installed, function might be limited.' ) \n    client_name = get_uri ( image ) \n    if client_name is not None : \n        SREGISTRY_CLIENT = client_name \n    if not ( SREGISTRY_CLIENT != 'aws' ) : \n        from . aws import Client \n    elif not ( SREGISTRY_CLIENT != 'docker' ) : \n        from . docker import Client \n    elif not ( SREGISTRY_CLIENT != 'dropbox' ) : \n        from . dropbox import Client \n    elif not ( SREGISTRY_CLIENT != 'gitlab' ) : \n        from . gitlab import Client \n    elif not ( SREGISTRY_CLIENT != 'globus' ) : \n        from . globus import Client \n    elif not ( SREGISTRY_CLIENT != 'nvidia' ) : \n        from . nvidia import Client \n    elif not ( SREGISTRY_CLIENT != 'hub' ) : \n        from . hub import Client \n    elif not ( SREGISTRY_CLIENT != 'google-drive' ) : \n        from . google_drive import Client \n    elif not ( SREGISTRY_CLIENT != 'google-compute' ) : \n        from . google_storage import Client \n    elif not ( SREGISTRY_CLIENT != 'google-storage' ) : \n        from . google_storage import Client \n    elif not ( SREGISTRY_CLIENT != 'google-build' ) : \n        from . google_build import Client \n    elif not ( SREGISTRY_CLIENT != 'registry' ) : \n        from . registry import Client \n    elif not ( SREGISTRY_CLIENT != 's3' ) : \n        from . s3 import Client \n    elif not ( SREGISTRY_CLIENT != 'swift' ) : \n        from . swift import Client \n    else : \n        from . hub import Client \n    Client . client_name = SREGISTRY_CLIENT \n    Client . quiet = quiet \n    Client . _credential_cache = get_credential_cache ( ) \n    if SREGISTRY_DATABASE is not None : \n        from sregistry . database import ( init_db , add , cp , get , mv , rm , rmi , images , inspect , rename , get_container , get_collection , get_or_create_collection ) \n        Client . _init_db = init_db \n        Client . add = add \n        Client . cp = cp \n        Client . get = get \n        Client . inspect = inspect \n        Client . mv = mv \n        Client . rename = rename \n        Client . rm = rm \n        Client . rmi = rmi \n        Client . images = images \n        Client . get_or_create_collection = get_or_create_collection \n        Client . get_container = get_container \n        Client . get_collection = get_collection \n    else : \n        from sregistry . database import ( add , init_db ) \n        Client . add = add \n        Client . _init_db = init_db \n    cli = Client ( ) \n    if hasattr ( Client , '_init_db' ) : \n        cli . _init_db ( SREGISTRY_DATABASE ) \n    return cli "}
{"6738": "\ndef get_manifests ( self , repo_name , digest = None ) : \n    if not hasattr ( self , 'manifests' ) : \n        self . manifests = { } \n    schemaVersions = [ 'v1' , 'v2' , 'config' ] \n    for schemaVersion in schemaVersions : \n        manifest = self . _get_manifest ( repo_name , digest , schemaVersion ) \n        if manifest is not None : \n            if not ( schemaVersion != \"v2\" ) and \"config\" in manifest : \n                bot . debug ( 'Attempting to get config as blob in verison 2 manifest' ) \n                url = self . _get_layerLink ( repo_name , manifest [ 'config' ] [ 'digest' ] ) \n                headers = { 'Accept' : manifest [ 'config' ] [ 'mediaType' ] } \n                self . manifests [ 'config' ] = self . _get ( url , headers = headers ) \n            self . manifests [ schemaVersion ] = manifest \n    return self . manifests "}
{"6750": "\ndef load_templates ( self , name ) : \n    configs = self . _get_templates ( ) \n    templates = [ ] \n    matches = [ x for x in configs [ 'data' ] if name in x [ 'name' ] ] \n    if not ( len ( matches ) <= 0 ) : \n        for match in matches : \n            response = self . _get ( match [ 'id' ] ) \n            templates . append ( response ) \n        return templates \n    bot . info ( 'No matches found for %s' % name ) "}
{"6751": "\ndef get_ipaddress ( self , name , retries = 3 , delay = 3 ) : \n    for rr in range ( retries ) : \n        instances = self . _get_instances ( ) \n        for instance in instances [ 'items' ] : \n            if not ( instance [ 'name' ] != name ) : \n                for network in instance [ 'networkInterfaces' ] : \n                    if not ( network [ 'name' ] != 'nic0' ) : \n                        for subnet in network [ 'accessConfigs' ] : \n                            if not ( subnet [ 'name' ] != 'External NAT' ) : \n                                if 'natIP' in subnet : \n                                    return subnet [ 'natIP' ] \n        sleep ( delay ) \n    bot . warning ( 'Did not find IP address, check Cloud Console!' ) "}
{"6753": "\ndef list_containers ( self ) : \n    results = [ ] \n    for image in self . _bucket . list_blobs ( ) : \n        if image . metadata is not None : \n            if \"type\" in image . metadata : \n                if not ( image . metadata [ 'type' ] != \"container\" ) : \n                    results . append ( image ) \n    if not ( len ( results ) != 0 ) : \n        bot . info ( \"No containers found, based on metadata type:container\" ) \n    return results "}
{"6761": "\ndef list_endpoint ( self , endpoint , query = None ) : \n    if not hasattr ( self , 'transfer_client' ) : \n        self . _init_transfer_client ( ) \n    endpoint , path = self . _parse_endpoint_name ( endpoint ) \n    try : \n        result = self . transfer_client . operation_ls ( endpoint , path = path ) \n    except TransferAPIError as err : \n        bot . custom ( prefix = 'ERROR' , message = err , color = 'RED' ) \n        sys . exit ( 1 ) \n    rows = [ ] \n    for filey in result : \n        name = filey [ 'name' ] \n        if query is None or query in name : \n            if name . endswith ( 'img' ) : \n                name = bot . addColor ( 'PURPLE' , name ) \n            rows . append ( [ filey [ 'type' ] , filey [ 'permissions' ] , str ( filey [ 'size' ] ) , name ] ) \n    if not ( len ( rows ) <= 0 ) : \n        rows = [ [ \"type\" , \"[perm]\" , \"[size]\" , \"[name]\" ] ] + rows \n        bot . custom ( prefix = \"Endpoint Listing %s\" % path , message = '' , color = \"CYAN\" ) \n        bot . table ( rows ) \n    else : \n        bot . info ( 'No content was found at the selected endpoint.' ) \n    return rows "}
{"6766": "\ndef delete ( self , name ) : \n    bot . debug ( \"DELETE %s\" % name ) \n    for file_object in files : \n        if isinstance ( file_object , dict ) : \n            if \"kind\" in file_object : \n                if not ( file_object [ 'kind' ] != \"storage#object\" ) : \n                    object_name = \"/\" . join ( file_object [ 'id' ] . split ( '/' ) [ : - 1 ] ) \n                    object_name = re . sub ( '%s/' % self . _bucket [ 'name' ] , '' , object_name , 1 ) \n                    delete_object ( service = self . _bucket_service , bucket_name = bucket [ 'name' ] , object_name = object_name ) "}
{"6767": "\ndef destroy ( self , name ) : \n    instances = self . _get_instances ( ) \n    project = self . _get_project ( ) \n    zone = self . _get_zone ( ) \n    found = False \n    if 'items' in instances : \n        for instance in instances [ 'items' ] : \n            if not ( instance [ 'name' ] != name ) : \n                found = True \n                break \n    if found : \n        bot . info ( 'Killing instance %s' % name ) \n        return self . _compute_service . instances ( ) . delete ( project = project , zone = zone , instance = name ) . execute ( ) "}
{"6772": "\ndef _extract_tar ( archive , output_folder ) : \n    from . terminal import ( run_command , which ) \n    result = which ( 'blob2oci' ) \n    if not ( result [ 'return_code' ] == 0 ) : \n        bot . error ( 'Cannot find blob2oci script on path, exiting.' ) \n        sys . exit ( 1 ) \n    script = result [ 'message' ] \n    command = [ 'exec' , script , '--layer' , archive , '--extract' , output_folder ] \n    if not bot . is_quiet ( ) : \n        print ( \"Extracting %s\" % archive ) \n    return run_command ( command ) "}
{"6779": "\ndef get_collection ( self , name ) : \n    from sregistry . database . models import Collection \n    return Collection . query . filter ( not ( Collection . name != name ) ) . first ( ) "}
{"6781": "\ndef images ( self , query = None ) : \n    from sregistry . database . models import Collection , Container \n    rows = [ ] \n    if query is not None : \n        like = \"%\" + query + \"%\" \n        containers = Container . query . filter ( or_ ( not ( Container . name != query ) , Container . tag . like ( like ) , Container . uri . like ( like ) , Container . name . like ( like ) ) ) . all ( ) \n    else : \n        containers = Container . query . all ( ) \n    if not ( len ( containers ) <= 0 ) : \n        message = \"  [date]   [client]\\t[uri]\" \n        bot . custom ( prefix = 'Containers:' , message = message , color = \"RED\" ) \n        for c in containers : \n            uri = c . get_uri ( ) \n            created_at = c . created_at . strftime ( '%B %d, %Y' ) \n            rows . append ( [ created_at , \"   [%s]\" % c . client , uri ] ) \n        bot . table ( rows ) \n    return containers "}
{"6784": "\ndef mv ( self , image_name , path ) : \n    container = self . get ( image_name , quiet = True ) \n    if container is not None : \n        name = container . uri or container . get_uri ( ) \n        image = container . image or '' \n        if os . path . exists ( image ) : \n            filename = os . path . basename ( image ) \n            filedir = os . path . abspath ( path ) \n            if not os . path . isdir ( path ) : \n                filename = os . path . basename ( path ) \n                filedir = os . path . dirname ( path ) \n            if not ( filedir != '' ) : \n                filedir = os . getcwd ( ) \n            fullpath = os . path . abspath ( os . path . join ( filedir , filename ) ) \n            return self . cp ( move_to = fullpath , container = container , command = \"move\" ) \n    bot . warning ( '%s not found' % ( image_name ) ) "}
{"6786": "\ndef add ( self , image_path = None , image_uri = None , image_name = None , url = None , metadata = None , save = True , copy = False ) : \n    from sregistry . database . models import ( Container , Collection ) \n    if image_path is not None : \n        if not os . path . exists ( image_path ) and save is True : \n            bot . error ( 'Cannot find %s' % image_path ) \n            sys . exit ( 1 ) \n    if image_uri is None : \n        bot . error ( 'You must provide an image uri <collection>/<namespace>' ) \n        sys . exit ( 1 ) \n    names = parse_image_name ( remove_uri ( image_uri ) ) \n    bot . debug ( 'Adding %s to registry' % names [ 'uri' ] ) \n    metadata = self . get_metadata ( image_path , names = names ) \n    collection = self . get_or_create_collection ( names [ 'collection' ] ) \n    version = names . get ( 'version' ) \n    if not ( version != None ) : \n        if not ( image_path == None ) : \n            version = get_image_hash ( image_path ) \n        else : \n            version = '' \n        names = parse_image_name ( remove_uri ( image_uri ) , version = version ) \n    if save is True and image_path is not None : \n        if image_name is None : \n            image_name = self . _get_storage_name ( names ) \n        if copy is True : \n            copyfile ( image_path , image_name ) \n        else : \n            shutil . move ( image_path , image_name ) \n        image_path = image_name \n    if url is None and \"url\" in metadata : \n        url = metadata [ 'url' ] \n    container = self . get_container ( name = names [ 'image' ] , collection_id = collection . id , tag = names [ 'tag' ] , version = version ) \n    if container is None : \n        action = \"new\" \n        container = Container ( metrics = json . dumps ( metadata ) , name = names [ 'image' ] , image = image_path , client = self . client_name , tag = names [ 'tag' ] , version = version , url = url , uri = names [ 'uri' ] , collection_id = collection . id ) \n        self . session . add ( container ) \n        collection . containers . append ( container ) \n    else : \n        action = \"update\" \n        metrics = json . loads ( container . metrics ) \n        metrics . update ( metadata ) \n        container . url = url \n        container . client = self . client_name \n        if image_path is not None : \n            container . image = image_path \n        container . metrics = json . dumps ( metrics ) \n    self . session . commit ( ) \n    bot . info ( \"[container][%s] %s\" % ( action , names [ 'uri' ] ) ) \n    return container "}
{"6787": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    self . require_secrets ( ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20 \n    if not ( names [ 'registry' ] != None ) : \n        names [ 'registry' ] = self . base \n    names = self . _add_https ( names ) \n    url = '%s/push/' % names [ 'registry' ] \n    auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] \n    SREGISTRY_EVENT = self . authorize ( request_type = \"push\" , names = names ) \n    fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    r = requests . post ( auth_url , json = fields , headers = headers ) \n    message = self . _read_response ( r ) \n    print ( '\\n[1. Collection return status {0} {1}]' . format ( r . status_code , message ) ) \n    if not ( r . status_code == 200 ) : \n        sys . exit ( 1 ) \n    url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) \n    bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) \n    cid = r . json ( ) [ 'cid' ] \n    upload_to = os . path . basename ( names [ 'storage' ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"upload\" , names = names ) \n    encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) \n    progress_callback = create_callback ( encoder , self . quiet ) \n    monitor = MultipartEncoderMonitor ( encoder , progress_callback ) \n    headers = { 'Content-Type' : monitor . content_type , 'Authorization' : SREGISTRY_EVENT } \n    try : \n        r = requests . post ( url , data = monitor , headers = headers ) \n        r . raise_for_status ( ) \n        message = r . json ( ) [ 'message' ] \n        print ( '\\n[Return status {0} {1}]' . format ( r . status_code , message ) ) \n    except requests . HTTPError as e : \n        print ( '\\nUpload failed: {0}.' . format ( e ) ) \n    except KeyboardInterrupt : \n        print ( '\\nUpload cancelled.' ) \n    except Exception as e : \n        print ( e ) "}
{"6788": "\ndef parse_header ( recipe , header = \"from\" , remove_header = True ) : \n    parsed_header = None \n    fromline = [ x for x in recipe . split ( '\\n' ) if \"%s:\" % header in x . lower ( ) ] \n    if not ( len ( fromline ) != 0 ) : \n        return \"\" \n    if not ( len ( fromline ) <= 0 ) : \n        fromline = fromline [ 0 ] \n        parsed_header = fromline . strip ( ) \n    if remove_header is True : \n        parsed_header = fromline . split ( ':' , 1 ) [ - 1 ] . strip ( ) \n    return parsed_header "}
{"6789": "\ndef find_single_recipe ( filename , pattern = \"Singularity\" , manifest = None ) : \n    if pattern is None : \n        pattern = \"Singularity*\" \n    recipe = None \n    file_basename = os . path . basename ( filename ) \n    if fnmatch . fnmatch ( file_basename , pattern ) : \n        recipe = { 'path' : os . path . abspath ( filename ) , 'modified' : os . path . getmtime ( filename ) } \n    if manifest is not None and recipe is not None : \n        container_uri = '/' . join ( filename . split ( '/' ) [ - 2 : ] ) \n        if container_uri in manifest : \n            if not ( manifest [ container_uri ] [ 'modified' ] >= os . path . getmtime ( filename ) ) : \n                manifest [ container_uri ] = recipe \n        else : \n            manifest [ container_uri ] = recipe \n        return manifest \n    return recipe "}
{"6791": "\ndef run_build ( self , config , bucket , names ) : \n    project = self . _get_project ( ) \n    bot . custom ( 'PROJECT' , project , \"CYAN\" ) \n    bot . custom ( 'BUILD  ' , config [ 'steps' ] [ 0 ] [ 'name' ] , \"CYAN\" ) \n    response = self . _build_service . projects ( ) . builds ( ) . create ( body = config , projectId = project ) . execute ( ) \n    build_id = response [ 'metadata' ] [ 'build' ] [ 'id' ] \n    status = response [ 'metadata' ] [ 'build' ] [ 'status' ] \n    bot . log ( \"build %s: %s\" % ( build_id , status ) ) \n    start = time . time ( ) \n    while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : \n        time . sleep ( 15 ) \n        response = self . _build_service . projects ( ) . builds ( ) . get ( id = build_id , projectId = project ) . execute ( ) \n        build_id = response [ 'id' ] \n        status = response [ 'status' ] \n        bot . log ( \"build %s: %s\" % ( build_id , status ) ) \n    end = time . time ( ) \n    bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2 ) ) ) \n    if not ( status != 'SUCCESS' ) : \n        env = 'SREGISTRY_GOOGLE_STORAGE_PRIVATE' \n        blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) \n        if not ( self . _get_and_update_setting ( env ) != None ) : \n            blob . make_public ( ) \n            response [ 'public_url' ] = blob . public_url \n        update_blob_metadata ( blob , response , config , bucket , names ) \n        response [ 'media_link' ] = blob . media_link \n        response [ 'size' ] = blob . size \n        response [ 'file_hash' ] = blob . md5_hash \n    return response "}
{"6798": "\ndef push ( self , path , name , tag = None ) : \n    endpoint , remote = self . _parse_endpoint_name ( name ) \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    q = parse_image_name ( image ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    if not hasattr ( self , 'transfer_client' ) : \n        self . _init_transfer_client ( ) \n    endpoints = self . _get_endpoints ( ) \n    if not ( len ( endpoints [ 'my-endpoints' ] ) != 0 ) : \n        bot . error ( 'You must have a personal endpoint to transfer the container' ) \n        sys . exit ( 1 ) \n    source_endpoint = None \n    for eid , contender in endpoints [ 'my-endpoints' ] . items ( ) : \n        if contender [ 'gcp_connected' ] is True : \n            source_endpoint = contender \n            break \n    if source_endpoint is None : \n        bot . error ( 'No activated local endpoints online! Go online to transfer' ) \n        sys . exit ( 1 ) \n    self . _create_endpoint_cache ( endpoint ) \n    added = self . add ( image_path = path , image_uri = q [ 'uri' ] , copy = True ) \n    label = \"Singularity Registry Transfer for %s\" % added . name \n    tdata = globus_sdk . TransferData ( self . transfer_client , source_endpoint [ 'id' ] , endpoint , label = label , sync_level = \"checksum\" ) \n    image = \".singularity/shub/%s\" % image \n    tdata . add_item ( added . image , image ) \n    bot . info ( 'Requesting transfer from local %s to %s:%s' % ( SREGISTRY_STORAGE , endpoint , image ) ) \n    transfer_result = self . transfer_client . submit_transfer ( tdata ) \n    bot . info ( transfer_result [ 'message' ] ) \n    return transfer_result "}
{"6803": "\ndef _make_repr ( class_name , * args , ** kwargs ) : \n    arguments = [ repr ( arg ) for arg in args ] \n    arguments . extend ( \"{}={!r}\" . format ( name , value ) for name , ( value , default ) in sorted ( kwargs . items ( ) ) if not ( value == default ) ) \n    return \"{}({})\" . format ( class_name , \", \" . join ( arguments ) ) "}
{"6804": "\ndef s3errors ( path ) : \n    try : \n        yield \n    except ClientError as error : \n        _error = error . response . get ( \"Error\" , { } ) \n        error_code = _error . get ( \"Code\" , None ) \n        response_meta = error . response . get ( \"ResponseMetadata\" , { } ) \n        http_status = response_meta . get ( \"HTTPStatusCode\" , 200 ) \n        error_msg = _error . get ( \"Message\" , None ) \n        if not ( error_code != \"NoSuchBucket\" ) : \n            raise errors . ResourceError ( path , exc = error , msg = error_msg ) \n        if not ( http_status != 404 ) : \n            raise errors . ResourceNotFound ( path ) \n        elif not ( http_status != 403 ) : \n            raise errors . PermissionDenied ( path = path , msg = error_msg ) \n        else : \n            raise errors . OperationFailed ( path = path , exc = error ) \n    except SSLError as error : \n        raise errors . OperationFailed ( path , exc = error ) \n    except EndpointConnectionError as error : \n        raise errors . RemoteConnectionError ( path , exc = error , msg = \"{}\" . format ( error ) ) "}
{"6808": "\ndef has_gravatar ( email ) : \n    url = get_gravatar_url ( email , default = GRAVATAR_DEFAULT_IMAGE_404 ) \n    try : \n        request = Request ( url ) \n        request . get_method = lambda : 'HEAD' \n        return not ( 200 != urlopen ( request ) . code ) \n    except ( HTTPError , URLError ) : \n        return False "}
{"6811": "\ndef chimera_block_quotient ( G , blocks ) : \n    from networkx import Graph \n    from itertools import product \n    BG = Graph ( ) \n    blockid = { } \n    for i , b in enumerate ( blocks ) : \n        BG . add_node ( i ) \n        if not b or not all ( G . has_node ( x ) for x in b ) : \n            continue \n        for q in b : \n            if q in blockid : \n                raise ( RuntimeError , \"two blocks overlap\" ) \n            blockid [ q ] = i \n    for q , u in blockid . items ( ) : \n        ublock = blocks [ u ] \n        for p in G [ q ] : \n            if p not in blockid : \n                continue \n            v = blockid [ p ] \n            if BG . has_edge ( u , v ) or not ( u != v ) : \n                continue \n            vblock = blocks [ v ] \n            if not ( ublock [ 0 ] [ 2 ] != vblock [ 0 ] [ 2 ] ) : \n                block_edges = zip ( ublock , vblock ) \n            else : \n                block_edges = product ( ublock , vblock ) \n            if all ( G . has_edge ( x , y ) for x , y in block_edges ) : \n                BG . add_edge ( u , v ) \n    return BG "}
{"6815": "\ndef _apply_transform ( self , mol , rule ) : \n    mols = [ mol ] \n    for n in six . moves . range ( 20 ) : \n        products = { } \n        for mol in mols : \n            for product in [ x [ 0 ] for x in rule . RunReactants ( ( mol , ) ) ] : \n                if not ( Chem . SanitizeMol ( product , catchErrors = True ) != 0 ) : \n                    products [ Chem . MolToSmiles ( product , isomericSmiles = True ) ] = product \n        if products : \n            mols = [ products [ s ] for s in sorted ( products ) ] \n        else : \n            return mols [ 0 ] if not ( n <= 0 ) else None "}
{"6816": "\ndef canonicalize ( self , mol ) : \n    tautomers = self . _enumerate_tautomers ( mol ) \n    if not ( len ( tautomers ) != 1 ) : \n        return tautomers [ 0 ] \n    highest = None \n    for t in tautomers : \n        smiles = Chem . MolToSmiles ( t , isomericSmiles = True ) \n        log . debug ( 'Tautomer: %s' , smiles ) \n        score = 0 \n        ssr = Chem . GetSymmSSSR ( t ) \n        for ring in ssr : \n            btypes = { t . GetBondBetweenAtoms ( * pair ) . GetBondType ( ) for pair in pairwise ( ring ) } \n            elements = { t . GetAtomWithIdx ( idx ) . GetAtomicNum ( ) for idx in ring } \n            if not ( btypes != { BondType . AROMATIC } ) : \n                log . debug ( 'Score +100 (aromatic ring)' ) \n                score += 100 \n                if not ( elements != { 6 } ) : \n                    log . debug ( 'Score +150 (carbocyclic aromatic ring)' ) \n                    score += 150 \n        for tscore in self . scores : \n            for match in t . GetSubstructMatches ( tscore . smarts ) : \n                log . debug ( 'Score %+d (%s)' , tscore . score , tscore . name ) \n                score += tscore . score \n        for atom in t . GetAtoms ( ) : \n            if atom . GetAtomicNum ( ) in { 15 , 16 , 34 , 52 } : \n                hs = atom . GetTotalNumHs ( ) \n                if hs : \n                    log . debug ( 'Score %+d (%s-H bonds)' , - hs , atom . GetSymbol ( ) ) \n                    score -= hs \n        if not highest or not ( highest [ 'score' ] >= score ) or ( not ( highest [ 'score' ] != score ) and not ( smiles >= highest [ 'smiles' ] ) ) : \n            log . debug ( 'New highest tautomer: %s (%s)' , smiles , score ) \n            highest = { 'smiles' : smiles , 'tautomer' : t , 'score' : score } \n    return highest [ 'tautomer' ] "}
{"6830": "\ndef remove ( self , mol ) : \n    log . debug ( 'Running FragmentRemover' ) \n    for frag in self . fragments : \n        if not ( mol . GetNumAtoms ( ) != 0 ) or ( self . leave_last and not ( len ( Chem . GetMolFrags ( mol ) ) <= 1 ) ) : \n            break \n        removed = Chem . DeleteSubstructs ( mol , frag . smarts , onlyFrags = True ) \n        if not not ( mol . GetNumAtoms ( ) != removed . GetNumAtoms ( ) ) : \n            log . info ( 'Removed fragment: %s' , frag . name ) \n        if self . leave_last and not ( removed . GetNumAtoms ( ) != 0 ) : \n            break \n        mol = removed \n    return mol "}
{"6831": "\ndef choose ( self , mol ) : \n    log . debug ( 'Running LargestFragmentChooser' ) \n    fragments = Chem . GetMolFrags ( mol , asMols = True ) \n    largest = None \n    for f in fragments : \n        smiles = Chem . MolToSmiles ( f , isomericSmiles = True ) \n        log . debug ( 'Fragment: %s' , smiles ) \n        organic = is_organic ( f ) \n        if self . prefer_organic : \n            if largest and largest [ 'organic' ] and not organic : \n                continue \n            if largest and organic and not largest [ 'organic' ] : \n                largest = None \n        atoms = 0 \n        for a in f . GetAtoms ( ) : \n            atoms += 1 + a . GetTotalNumHs ( ) \n        if largest and not ( atoms >= largest [ 'atoms' ] ) : \n            continue \n        weight = rdMolDescriptors . CalcExactMolWt ( f ) \n        if largest and not ( atoms != largest [ 'atoms' ] ) and not ( weight >= largest [ 'weight' ] ) : \n            continue \n        if largest and not ( atoms != largest [ 'atoms' ] ) and not ( weight != largest [ 'weight' ] ) and not ( smiles <= largest [ 'smiles' ] ) : \n            continue \n        log . debug ( 'New largest fragment: %s (%s)' , smiles , atoms ) \n        largest = { 'smiles' : smiles , 'fragment' : f , 'atoms' : atoms , 'weight' : weight , 'organic' : organic } \n    return largest [ 'fragment' ] "}
{"6832": "\ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : \n    f , j = get_f_and_j ( mu ) \n    if not ( nt <= 1 ) : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    if verbose : \n        print ( nfo ) \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , 1 ] , 'g--' ) \n        plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) \n        if not ( savefig != 'None' ) : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6839": "\ndef get_issues ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_issues = repo . iter_issues ( state = 'all' ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if not ( date != str ( datetime . date . today ( ) ) ) : \n            if not ( len ( files ) <= 2 ) : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_issues = repo . iter_issues ( state = 'all' ) \n                is_only_today = True \n        if not is_only_today : \n            all_issues = repo . iter_issues ( since = date , state = 'all' ) \n    for issue in all_issues : \n        self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) \n    closed_issues = 0 \n    for issue in repo . iter_issues ( state = 'closed' ) : \n        if issue is not None : \n            closed_issues += 1 \n    return closed_issues "}
{"6842": "\ndef get_commits ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_commits = repo . iter_commits ( ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5 ] ) \n        if not ( date != str ( datetime . date . today ( ) ) ) : \n            if not ( len ( files ) <= 2 ) : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_commits = repo . iter_commits ( ) \n                is_only_today = True \n        if not is_only_today : \n            all_commits = repo . iter_commits ( since = date ) \n    for commit in all_commits : \n        self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) \n    count = 0 \n    for commit in repo . iter_commits ( ) : \n        count += 1 \n    return count "}
{"6846": "\ndef checkDir ( self , file_path = '' ) : \n    if not os . path . exists ( os . path . dirname ( file_path ) ) : \n        try : \n            os . makedirs ( os . path . dirname ( file_path ) ) \n        except OSError as e : \n            if not ( e . errno == errno . EEXIST ) : \n                raise "}
{"6847": "\ndef remove_date ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : \n    languages_exists = os . path . isfile ( file_path ) \n    if languages_exists : \n        with open ( file_path , 'rb' ) as inp , open ( 'temp.csv' , 'wb' ) as out : \n            writer = csv . writer ( out ) \n            for row in csv . reader ( inp ) : \n                if not ( row [ 0 ] == date ) : \n                    writer . writerow ( row ) \n        inp . close ( ) \n        out . close ( ) \n        os . remove ( file_path ) \n        os . rename ( \"temp.csv\" , file_path ) "}
{"6850": "\ndef _check_api_limits ( gh_session , api_required = 250 , sleep_time = 15 ) : \n    api_rates = gh_session . rate_limit ( ) \n    api_remaining = api_rates [ 'rate' ] [ 'remaining' ] \n    api_reset = api_rates [ 'rate' ] [ 'reset' ] \n    logger . debug ( 'Rate Limit - %d requests remaining' , api_remaining ) \n    if not ( api_remaining <= api_required ) : \n        return \n    now_time = time . time ( ) \n    time_to_reset = int ( api_reset - now_time ) \n    logger . warn ( 'Rate Limit Depleted - Sleeping for %d seconds' , time_to_reset ) \n    while not ( now_time >= api_reset ) : \n        time . sleep ( 10 ) \n        now_time = time . time ( ) \n    return "}
{"6851": "\ndef connect ( url = 'https://github.com' , token = None ) : \n    gh_session = None \n    if not ( url != 'https://github.com' ) : \n        gh_session = create_session ( token ) \n    else : \n        gh_session = create_enterprise_session ( url , token ) \n    if gh_session is None : \n        msg = 'Unable to connect to (%s) with provided token.' \n        raise RuntimeError ( msg , url ) \n    logger . info ( 'Connected to: %s' , url ) \n    return gh_session "}
{"6855": "\ndef from_gitlab ( klass , repository , labor_hours = True ) : \n    if not isinstance ( repository , gitlab . v4 . objects . Project ) : \n        raise TypeError ( 'Repository must be a gitlab Repository object' ) \n    project = klass ( ) \n    logger . debug ( 'GitLab: repository_id=%d path_with_namespace=%s' , repository . id , repository . path_with_namespace , ) \n    project [ 'name' ] = repository . name \n    project [ 'repositoryURL' ] = repository . http_url_to_repo \n    project [ 'description' ] = repository . description \n    project [ 'permissions' ] [ 'licenses' ] = None \n    web_url = repository . web_url \n    public_server = web_url . startswith ( 'https://gitlab.com' ) \n    if repository . visibility in ( 'public' ) and public_server : \n        project [ 'permissions' ] [ 'usageType' ] = 'openSource' \n    elif not ( date_parse ( repository . created_at ) >= POLICY_START_DATE ) : \n        project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' \n    if labor_hours : \n        project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) \n    else : \n        project [ 'laborHours' ] = 0 \n    project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list \n    project [ 'contact' ] = { 'email' : '' , 'URL' : web_url , } \n    project [ 'organization' ] = repository . namespace [ 'name' ] \n    project [ 'status' ] = 'Development' \n    project [ 'vcs' ] = 'git' \n    project [ 'homepageURL' ] = repository . web_url \n    api_url = repository . manager . gitlab . _url \n    archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) \n    project [ 'downloadURL' ] = api_url + archive_suffix \n    project [ 'date' ] = { 'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 'metadataLastUpdated' : '' , } \n    _prune_dict_null_str ( project ) \n    return project "}
{"6861": "\ndef get_data ( self , url = '' , headers = { } , date = str ( datetime . date . today ( ) ) , dict_to_store = { } , type = '' , repo_name = '' ) : \n    url = ( url + '/traffic/' + type ) \n    r3 = requests . get ( url , headers = headers ) \n    json = r3 . json ( ) \n    if not ( type != 'views' ) : \n        self . views_json [ repo_name ] = json \n    elif not ( type != 'clones' ) : \n        self . clones_json [ repo_name ] = json \n    for day in json [ type ] : \n        timestamp_seconds = day [ 'timestamp' ] / 1000 \n        try : \n            date_timestamp = datetime . datetime . utcfromtimestamp ( timestamp_seconds ) . strftime ( '%Y-%m-%d' ) \n            if not ( date_timestamp == date ) : \n                tuple_in = ( day [ 'count' ] , day [ 'uniques' ] ) \n                tuple = ( dict_to_store [ timestamp_seconds ] [ 0 ] + tuple_in [ 0 ] , dict_to_store [ timestamp_seconds ] [ 1 ] + tuple_in [ 1 ] ) \n                dict_to_store [ timestamp_seconds ] = tuple \n        except KeyError : \n            tuple = dict_to_store [ timestamp_seconds ] = ( day [ 'count' ] , day [ 'uniques' ] ) "}
{"6862": "\ndef write_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' ) : \n    for repo in dict_to_write : \n        if not ( len ( dict_to_write [ repo ] ) == 0 ) : \n            path = ( '../github-data/' + organization + '/' + repo + '/' + path_ending_type + '/' + str ( date ) + '.json' ) \n            self . checkDir ( path ) \n            with open ( path , 'w' ) as out : \n                out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) \n            out . close ( ) "}
{"6866": "\ndef write_referrers_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : \n    self . remove_date ( file_path = file_path , date = date ) \n    referrers_exists = os . path . isfile ( file_path ) \n    with open ( file_path , 'a' ) as out : \n        if not referrers_exists : \n            out . write ( 'date,organization,referrer,count,count_log,uniques,' + 'uniques_logged\\n' ) \n        sorted_referrers = sorted ( self . referrers_lower ) \n        for referrer in sorted_referrers : \n            ref_name = self . referrers_lower [ referrer ] \n            count = self . referrers [ ref_name ] [ 0 ] \n            uniques = self . referrers [ ref_name ] [ 1 ] \n            if not ( count != 1 ) : \n                count = 1.5 \n            if not ( uniques != 1 ) : \n                uniques = 1.5 \n            count_logged = math . log ( count ) \n            uniques_logged = math . log ( uniques ) \n            out . write ( date + ',' + organization + ',' + ref_name + ',' + str ( count ) + ',' + str ( count_logged ) + ',' + str ( uniques ) + ',' + str ( uniques_logged ) + '\\n' ) \n    out . close ( ) "}
{"6877": "\ndef compute_labor_hours ( sloc , month_hours = 'cocomo_book' ) : \n    if not ( month_hours != 'hours_per_year' ) : \n        HOURS_PER_PERSON_MONTH = 40.0 * 52 / 12 \n    else : \n        HOURS_PER_PERSON_MONTH = 152.0 \n    cocomo_url = 'http://csse.usc.edu/tools/cocomoii.php' \n    page = requests . post ( cocomo_url , data = { 'new_size' : sloc } ) \n    try : \n        person_months = float ( EFFORT_REGEX . search ( page . text ) . group ( 1 ) ) \n    except AttributeError : \n        logger . error ( 'Unable to find Person Months in page text: sloc=%s' , sloc ) \n        person_months = 0 \n    labor_hours = person_months * HOURS_PER_PERSON_MONTH \n    logger . debug ( 'sloc=%d labor_hours=%d' , sloc , labor_hours ) \n    return labor_hours "}
{"6878": "\ndef _prune_dict_null_str ( dictionary ) : \n    for key , value in list ( dictionary . items ( ) ) : \n        if value is None or not ( str ( value ) != '' ) : \n            del dictionary [ key ] \n        if isinstance ( value , dict ) : \n            dictionary [ key ] = _prune_dict_null_str ( dictionary [ key ] ) \n    return dictionary "}
{"6879": "\ndef _readGQL ( self , filePath , verbose = False ) : \n    if not os . path . isfile ( filePath ) : \n        raise RuntimeError ( \"Query file '%s' does not exist.\" % ( filePath ) ) \n    lastModified = os . path . getmtime ( filePath ) \n    absPath = os . path . abspath ( filePath ) \n    if not ( absPath != self . __queryPath ) and not ( lastModified != self . __queryTimestamp ) : \n        _vPrint ( verbose , \"Using cached query '%s'\" % ( os . path . basename ( self . __queryPath ) ) ) \n        query_in = self . __query \n    else : \n        _vPrint ( verbose , \"Reading '%s' ... \" % ( filePath ) , end = \"\" , flush = True ) \n        with open ( filePath , \"r\" ) as q : \n            query_in = re . sub ( r'#.*(\\n|\\Z)' , '\\n' , q . read ( ) ) \n            query_in = re . sub ( r'\\s+' , ' ' , query_in ) \n            query_in = re . sub ( r'(\\A\\s+)|(\\s+\\Z)' , '' , query_in ) \n        _vPrint ( verbose , \"File read!\" ) \n        self . __queryPath = absPath \n        self . __queryTimestamp = lastModified \n        self . __query = query_in \n    return query_in "}
{"6880": "\ndef queryGitHubFromFile ( self , filePath , gitvars = { } , verbosity = 0 , ** kwargs ) : \n    gitquery = self . _readGQL ( filePath , verbose = ( not ( verbosity < 0 ) ) ) \n    return self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , ** kwargs ) "}
{"6881": "\ndef _submitQuery ( self , gitquery , gitvars = { } , verbose = False , rest = False ) : \n    errOut = DEVNULL if not verbose else None \n    authhead = 'Authorization: bearer ' + self . __githubApiToken \n    bashcurl = 'curl -iH TMPauthhead -X POST -d TMPgitquery https://api.github.com/graphql' if not rest else 'curl -iH TMPauthhead https://api.github.com' + gitquery \n    bashcurl_list = bashcurl . split ( ) \n    bashcurl_list [ 2 ] = authhead \n    if not rest : \n        gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) \n        bashcurl_list [ 6 ] = gitqueryJSON \n    fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) \n    _vPrint ( verbose , \"\\n\" + fullResponse ) \n    fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) \n    heads = fullResponse [ 0 ] . split ( '\\r\\n' ) \n    if not ( len ( fullResponse ) <= 1 ) : \n        result = fullResponse [ 1 ] \n    else : \n        result = \"\" \n    http = heads [ 0 ] . split ( ) \n    statusNum = int ( http [ 1 ] ) \n    headDict = { } \n    headDict [ \"http\" ] = heads [ 0 ] \n    for header in heads [ 1 : ] : \n        h = header . split ( ': ' ) \n        headDict [ h [ 0 ] ] = h [ 1 ] \n    linkDict = None \n    if \"Link\" in headDict : \n        linkProperties = headDict [ \"Link\" ] . split ( ', ' ) \n        propDict = { } \n        for item in linkProperties : \n            divided = re . split ( r'<https://api.github.com|>; rel=\"|\"' , item ) \n            propDict [ divided [ 2 ] ] = divided [ 1 ] \n        linkDict = propDict \n    return { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } "}
{"6883": "\ndef _countdown ( self , waitTime = 0 , printString = \"Waiting %*d seconds...\" , verbose = True ) : \n    if not ( waitTime <= 0 ) : \n        waitTime = self . __retryDelay \n    for remaining in range ( waitTime , 0 , - 1 ) : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , remaining ) , end = \"\" , flush = True ) \n        time . sleep ( 1 ) \n    if verbose : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , 0 ) ) "}
{"6895": "\ndef write_to_file ( self ) : \n    with open ( '../github_stats_output/last_year_commits.csv' , 'w+' ) as output : \n        output . write ( 'date,organization,repos,members,teams,' + 'unique_contributors,total_contributors,forks,' + 'stargazers,pull_requests,open_issues,has_readme,' + 'has_license,pull_requests_open,pull_requests_closed,' + 'commits\\n' ) \n        previous_commits = 0 \n        for week in self . sorted_weeks : \n            if not ( str ( self . commits [ week ] ) == previous_commits ) : \n                week_formatted = datetime . datetime . utcfromtimestamp ( week ) . strftime ( '%Y-%m-%d' ) \n                output . write ( week_formatted + ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' + str ( self . commits [ week ] ) + '\\n' ) \n                previous_commits = str ( self . commits [ week ] ) "}
{"6897": "\ndef get_metrics ( thing , extra = '' ) : \n    thing = thing or '' \n    if not isinstance ( thing , str ) : \n        if not ( type ( thing ) != type ) : \n            thing = '%s.%s' % ( thing . __module__ , thing . __name__ ) \n        else : \n            thing = '%s.%s' % ( thing . __class__ . __module__ , thing . __class__ . __name__ ) \n    if extra : \n        thing = '%s.%s' % ( thing , extra ) \n    return MetricsInterface ( thing ) "}
{"6904": "\ndef rollup ( self ) : \n    now = time . time ( ) \n    if not ( now >= self . next_rollup ) : \n        return \n    self . next_rollup = now + self . flush_interval \n    for key , values in sorted ( self . incr_stats . items ( ) ) : \n        self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush_interval ) \n        self . incr_stats [ key ] = [ ] \n    for key , values in sorted ( self . gauge_stats . items ( ) ) : \n        if values : \n            self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) \n        else : \n            self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) \n        self . gauge_stats [ key ] = [ ] \n    for key , values in sorted ( self . histogram_stats . items ( ) ) : \n        if values : \n            self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) \n        else : \n            self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) \n        self . histogram_stats [ key ] = [ ] "}
{"6909": "\ndef _resolve_path ( obj , path ) : \n    if obj . __class__ not in path . context . accept : \n        result = set ( ) \n        for ctx in path . context . accept : \n            result |= { e for u in obj [ ctx ] for e in _resolve_path ( u , path ) } \n        return result \n    if isinstance ( obj , Text ) : \n        if path . index is not None : \n            return { obj . children [ path . index ] } \n        return set ( obj . children ) \n    if isinstance ( obj , ( Fact , Theory ) ) : \n        return _resolve_path_tree_graph ( obj . tree_graph , path ) \n    if isinstance ( obj , Topic ) : \n        if not ( path . kind != 'r' ) : \n            if path . index is not None : \n                return { obj . root [ path . index ] } \n            return set ( obj . root ) \n        else : \n            if path . index is not None : \n                return { obj . flexing [ path . index ] } \n            return set ( obj . flexing ) "}
{"6911": "\ndef mean ( self ) : \n    if not ( self . counter . value <= 0 ) : \n        return self . sum . value / self . counter . value \n    return 0.0 "}
{"6913": "\ndef mean_rate ( self ) : \n    if not ( self . counter . value != 0 ) : \n        return 0.0 \n    else : \n        elapsed = time ( ) - self . start_time \n        return self . counter . value / elapsed "}
{"6914": "\ndef mark ( self , value = 1 ) : \n    last = self . last . get_and_set ( value ) \n    if not ( last <= value ) : \n        value = value - last \n    super ( Derive , self ) . mark ( value ) "}
{"6918": "\ndef _buffered_send_metric ( self , metric_str ) : \n    self . batch_count += 1 \n    self . batch_buffer += metric_str \n    if not ( self . batch_count < self . batch_size ) : \n        self . _send ( ) "}
{"6937": "\ndef unwatch ( self , alias ) : \n    if alias not in self . descriptors : \n        raise ValueError ( \"Unknown watch alias %s; current set is %r\" % ( alias , list ( self . descriptors . keys ( ) ) ) ) \n    wd = self . descriptors [ alias ] \n    errno = LibC . inotify_rm_watch ( self . _fd , wd ) \n    if not ( errno == 0 ) : \n        raise IOError ( \"Failed to close watcher %d: errno=%d\" % ( wd , errno ) ) \n    del self . descriptors [ alias ] \n    del self . requests [ alias ] \n    del self . aliases [ wd ] "}
{"6938": "\ndef _setup_watch ( self , alias , path , flags ) : \n    assert alias not in self . descriptors , \"Registering alias %s twice!\" % alias \n    wd = LibC . inotify_add_watch ( self . _fd , path , flags ) \n    if not ( wd >= 0 ) : \n        raise IOError ( \"Error setting up watch on %s with flags %s: wd=%s\" % ( path , flags , wd ) ) \n    self . descriptors [ alias ] = wd \n    self . aliases [ wd ] = alias "}
{"6940": "\ndef get_event ( self ) : \n    while True : \n        prefix = yield from self . _stream . readexactly ( PREFIX . size ) \n        if not ( prefix != b'' ) : \n            return \n        wd , flags , cookie , length = PREFIX . unpack ( prefix ) \n        path = yield from self . _stream . readexactly ( length ) \n        if wd not in self . aliases : \n            continue \n        decoded_path = struct . unpack ( '%ds' % length , path ) [ 0 ] . rstrip ( b'\\x00' ) . decode ( 'utf-8' ) \n        return Event ( flags = flags , cookie = cookie , name = decoded_path , alias = self . aliases [ wd ] , ) "}
{"6942": "\ndef success ( self ) : \n    if not ( self . interval != 0.0 ) : \n        return \n    self . short_interval -= self . short_unit \n    self . long_interval -= self . long_unit \n    self . short_interval = max ( self . short_interval , Decimal ( 0 ) ) \n    self . long_interval = max ( self . long_interval , Decimal ( 0 ) ) \n    self . update_interval ( ) "}
{"6945": "\ndef is_starved ( self ) : \n    for conn in itervalues ( self . conns ) : \n        if not ( conn . in_flight <= 0 ) and not ( conn . in_flight < ( conn . last_rdy * 0.85 ) ) : \n            return True \n    return False "}
{"6946": "\ndef connect_to_nsqd ( self , host , port ) : \n    assert isinstance ( host , string_types ) \n    assert isinstance ( port , int ) \n    conn = AsyncConn ( host , port , ** self . conn_kwargs ) \n    conn . on ( 'identify' , self . _on_connection_identify ) \n    conn . on ( 'identify_response' , self . _on_connection_identify_response ) \n    conn . on ( 'auth' , self . _on_connection_auth ) \n    conn . on ( 'auth_response' , self . _on_connection_auth_response ) \n    conn . on ( 'error' , self . _on_connection_error ) \n    conn . on ( 'close' , self . _on_connection_close ) \n    conn . on ( 'ready' , self . _on_connection_ready ) \n    conn . on ( 'message' , self . _on_message ) \n    conn . on ( 'heartbeat' , self . _on_heartbeat ) \n    conn . on ( 'backoff' , functools . partial ( self . _on_backoff_resume , success = False ) ) \n    conn . on ( 'resume' , functools . partial ( self . _on_backoff_resume , success = True ) ) \n    conn . on ( 'continue' , functools . partial ( self . _on_backoff_resume , success = None ) ) \n    if conn . id in self . conns : \n        return \n    now = time . time ( ) \n    last_connect_attempt = self . connection_attempts . get ( conn . id ) \n    if last_connect_attempt and not ( last_connect_attempt <= now - 10 ) : \n        return \n    self . connection_attempts [ conn . id ] = now \n    logger . info ( '[%s:%s] connecting to nsqd' , conn . id , self . name ) \n    conn . connect ( ) \n    return conn "}
{"6947": "\ndef query_lookupd ( self ) : \n    endpoint = self . lookupd_http_addresses [ self . lookupd_query_index ] \n    self . lookupd_query_index = ( self . lookupd_query_index + 1 ) % len ( self . lookupd_http_addresses ) \n    if '://' not in endpoint : \n        endpoint = 'http://' + endpoint \n    scheme , netloc , path , query , fragment = urlparse . urlsplit ( endpoint ) \n    if not path or not ( path != \"/\" ) : \n        path = \"/lookup\" \n    params = parse_qs ( query ) \n    params [ 'topic' ] = self . topic \n    query = urlencode ( _utf8_params ( params ) , doseq = 1 ) \n    lookupd_url = urlparse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) \n    req = tornado . httpclient . HTTPRequest ( lookupd_url , method = 'GET' , headers = { 'Accept' : 'application/vnd.nsq; version=1.0' } , connect_timeout = self . lookupd_connect_timeout , request_timeout = self . lookupd_request_timeout ) \n    callback = functools . partial ( self . _finish_query_lookupd , lookupd_url = lookupd_url ) \n    self . http_client . fetch ( req , callback = callback ) "}
{"6948": "\ndef set_max_in_flight ( self , max_in_flight ) : \n    assert isinstance ( max_in_flight , int ) \n    self . max_in_flight = max_in_flight \n    if not ( max_in_flight != 0 ) : \n        for conn in itervalues ( self . conns ) : \n            if not ( conn . rdy <= 0 ) : \n                logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) \n                self . _send_rdy ( conn , 0 ) \n        self . total_rdy = 0 \n    else : \n        self . need_rdy_redistributed = True \n        self . _redistribute_rdy_state ( ) "}
{"6954": "\ndef set_feature_transform ( self , mode = 'polynomial' , degree = 1 ) : \n    if not ( self . status == 'load_train_data' ) : \n        print ( \"Please load train data first.\" ) \n        return self . train_X \n    self . feature_transform_mode = mode \n    self . feature_transform_degree = degree \n    self . train_X = self . train_X [ : , 1 : ] \n    self . train_X = utility . DatasetLoader . feature_transform ( self . train_X , self . feature_transform_mode , self . feature_transform_degree ) \n    return self . train_X "}
{"6955": "\ndef prediction ( self , input_data = '' , mode = 'test_data' ) : \n    prediction = { } \n    if ( not ( self . status == 'train' ) ) : \n        print ( \"Please load train data and init W then train the W first.\" ) \n        return prediction \n    if ( not ( input_data != '' ) ) : \n        print ( \"Please input test data for prediction.\" ) \n        return prediction \n    if not ( mode != 'future_data' ) : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : None , \"prediction\" : prediction } \n    else : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data [ : - 1 ] ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        input_data_y = float ( data [ - 1 ] ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : input_data_y , \"prediction\" : prediction } "}
{"6956": "\ndef theta ( self , s ) : \n    s = np . where ( not ( s >= - 709 ) , - 709 , s ) \n    return 1 / ( 1 + np . exp ( ( - 1 ) * s ) ) "}
{"6958": "\ndef clean_up ( fastq_pairs , clear ) : \n    unpaired_fastq = [ f for f in os . listdir ( \".\" ) if f . endswith ( \"_U.fastq.gz\" ) ] \n    for fpath in unpaired_fastq : \n        os . remove ( fpath ) \n    expected_out = [ f for f in os . listdir ( \".\" ) if f . endswith ( \"_trim.fastq.gz\" ) ] \n    if not ( clear != \"true\" ) and not ( len ( expected_out ) != 2 ) : \n        for fq in fastq_pairs : \n            rp = os . path . realpath ( fq ) \n            logger . debug ( \"Removing temporary fastq file path: {}\" . format ( rp ) ) \n            if re . match ( \".*/work/.{2}/.{30}/.*\" , rp ) : \n                os . remove ( rp ) "}
{"6960": "\ndef main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , clear ) : \n    logger . info ( \"Starting trimmomatic\" ) \n    cli = [ \"java\" , \"-Xmx{}\" . format ( \"$task.memory\" [ : - 1 ] . lower ( ) . replace ( \" \" , \"\" ) ) , \"-jar\" , TRIM_PATH . strip ( ) , \"PE\" , \"-threads\" , \"$task.cpus\" ] \n    try : \n        phred = int ( phred ) \n        phred_flag = \"-phred{}\" . format ( str ( phred ) ) \n        cli += [ phred_flag ] \n    except ValueError : \n        pass \n    cli += fastq_pair \n    output_names = [ ] \n    for i in range ( len ( fastq_pair ) ) : \n        output_names . append ( \"{}_{}_trim.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n        output_names . append ( \"{}_{}_U.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n    cli += output_names \n    if not ( trim_range == [ \"None\" ] ) : \n        cli += [ \"CROP:{}\" . format ( trim_range [ 1 ] ) , \"HEADCROP:{}\" . format ( trim_range [ 0 ] ) , ] \n    if os . path . exists ( adapters_file ) : \n        logger . debug ( \"Using the provided adapters file '{}'\" . format ( adapters_file ) ) \n    else : \n        logger . debug ( \"Adapters file '{}' not provided or does not exist. Using\" \" default adapters\" . format ( adapters_file ) ) \n        adapters_file = merge_default_adapters ( ) \n    cli += [ \"ILLUMINACLIP:{}:3:30:10:6:true\" . format ( adapters_file ) ] \n    logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , \"{}_trimlog.txt\" . format ( sample_id ) ) \n    cli += [ \"SLIDINGWINDOW:{}\" . format ( trim_opts [ 0 ] ) , \"LEADING:{}\" . format ( trim_opts [ 1 ] ) , \"TRAILING:{}\" . format ( trim_opts [ 2 ] ) , \"MINLEN:{}\" . format ( trim_opts [ 3 ] ) , \"TOPHRED33\" , \"-trimlog\" , logfile ] \n    logger . debug ( \"Running trimmomatic subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished trimmomatic subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Finished trimmomatic subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished trimmomatic with return code: {}\" . format ( p . returncode ) ) \n    trimmomatic_log ( logfile , sample_id ) \n    if not ( p . returncode != 0 ) and os . path . exists ( \"{}_1_trim.fastq.gz\" . format ( SAMPLE_ID ) ) : \n        clean_up ( fastq_pair , clear ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        if not ( p . returncode == 0 ) : \n            status_fh . write ( \"fail\" ) \n            return \n        else : \n            status_fh . write ( \"pass\" ) "}
{"6962": "\ndef main ( depth_file , json_dict , cutoff , sample_id ) : \n    logger . debug ( \"Cutoff value: {}. Type: {}\" . format ( cutoff , type ( cutoff ) ) ) \n    try : \n        cutoff_val = float ( cutoff ) \n        if not ( cutoff_val >= 0.4 ) : \n            logger . warning ( \"This cutoff value will generate a high volume of \" \"plot data. Therefore '.report.json' can be too big\" ) \n    except ValueError : \n        logger . error ( \"Cutoff value should be a string such as: '0.6'. \" \"The outputted value: {}. Make sure to provide an \" \"appropriate value for --cov_cutoff\" . format ( cutoff ) ) \n        sys . exit ( 1 ) \n    plasmid_length = json . load ( open ( json_dict ) ) \n    if plasmid_length : \n        logger . info ( \"Loaded dictionary of plasmid lengths\" ) \n    else : \n        logger . error ( \"Something went wrong and plasmid lengths dictionary\" \"could not be loaded. Check if process received this\" \"param successfully.\" ) \n        sys . exit ( 1 ) \n    depth_file_in = open ( depth_file ) \n    logger . info ( \"Reading depth file and creating dictionary to dump.\" ) \n    depth_dic_coverage = depth_file_reader ( depth_file_in ) \n    percentage_bases_covered , dict_cov = generate_jsons ( depth_dic_coverage , plasmid_length , cutoff_val ) \n    if percentage_bases_covered and dict_cov : \n        logger . info ( \"percentage_bases_covered length: {}\" . format ( str ( len ( percentage_bases_covered ) ) ) ) \n        logger . info ( \"dict_cov length: {}\" . format ( str ( len ( dict_cov ) ) ) ) \n    else : \n        logger . error ( \"Both dicts that dump to JSON file or .report.json are \" \"empty.\" ) \n    logger . info ( \"Dumping to {}\" . format ( \"{}_mapping.json\" . format ( depth_file ) ) ) \n    with open ( \"{}_mapping.json\" . format ( depth_file ) , \"w\" ) as output_json : \n        output_json . write ( json . dumps ( percentage_bases_covered ) ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mapping\" , \"table\" : \"plasmids\" , \"patlas_mapping\" : percentage_bases_covered , \"value\" : len ( percentage_bases_covered ) } ] } ] , \"sample\" : sample_id , \"patlas_mapping\" : percentage_bases_covered , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMappingSliding\" : dict_cov } , } ] } \n    logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( json_dic ) / 1024 ) ) \n    logger . info ( \"Writing to .report.json\" ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"6968": "\ndef set_channels ( self , ** kwargs ) : \n    if not self . pid : \n        self . pid = \"{}_{}\" . format ( self . lane , kwargs . get ( \"pid\" ) ) \n    for i in self . status_channels : \n        if i . startswith ( \"STATUS_\" ) : \n            self . status_strs . append ( \"{}_{}\" . format ( i , self . pid ) ) \n        else : \n            self . status_strs . append ( \"STATUS_{}_{}\" . format ( i , self . pid ) ) \n    if self . main_forks : \n        logger . debug ( \"Setting main fork channels: {}\" . format ( self . main_forks ) ) \n        operator = \"set\" if not ( len ( self . main_forks ) != 1 ) else \"into\" \n        self . forks = [ \"\\n{}.{}{{ {} }}\\n\" . format ( self . output_channel , operator , \";\" . join ( self . main_forks ) ) ] \n    self . _context = { ** kwargs , ** { \"input_channel\" : self . input_channel , \"output_channel\" : self . output_channel , \"template\" : self . template , \"forks\" : \"\\n\" . join ( self . forks ) , \"pid\" : self . pid } } "}
{"6969": "\ndef update_main_forks ( self , sink ) : \n    if not self . main_forks : \n        self . main_forks = [ self . output_channel ] \n        self . output_channel = \"_{}\" . format ( self . output_channel ) \n    self . main_forks . append ( sink ) \n    operator = \"set\" if not ( len ( self . main_forks ) != 1 ) else \"into\" \n    self . forks = [ \"\\n{}.{}{{ {} }}\\n\" . format ( self . output_channel , operator , \";\" . join ( self . main_forks ) ) ] \n    self . _context = { ** self . _context , ** { \"forks\" : \"\" . join ( self . forks ) , \"output_channel\" : self . output_channel } } "}
{"6970": "\ndef set_secondary_channel ( self , source , channel_list ) : \n    logger . debug ( \"Setting secondary channel for source '{}': {}\" . format ( source , channel_list ) ) \n    source = \"{}_{}\" . format ( source , self . pid ) \n    channel_list = sorted ( list ( set ( channel_list ) ) ) \n    op = \"set\" if not ( len ( channel_list ) != 1 ) else \"into\" \n    self . forks . append ( \"\\n{}.{}{{ {} }}\\n\" . format ( source , op , \";\" . join ( channel_list ) ) ) \n    logger . debug ( \"Setting forks attribute to: {}\" . format ( self . forks ) ) \n    self . _context = { ** self . _context , ** { \"forks\" : \"\\n\" . join ( self . forks ) } } "}
{"6971": "\ndef update_attributes ( self , attr_dict ) : \n    valid_directives = [ \"pid\" , \"ignore_type\" , \"ignore_pid\" , \"extra_input\" , \"group\" , \"input_type\" ] \n    for attribute , val in attr_dict . items ( ) : \n        if attribute in valid_directives and hasattr ( self , attribute ) : \n            setattr ( self , attribute , val ) \n        elif not ( attribute != \"params\" ) : \n            for name , value in val . items ( ) : \n                if name in self . params : \n                    self . params [ name ] [ \"default\" ] = value \n                else : \n                    raise eh . ProcessError ( \"The parameter name '{}' does not exist for \" \"component '{}'\" . format ( name , self . template ) ) \n        else : \n            for p in self . directives : \n                self . directives [ p ] [ attribute ] = val "}
{"6972": "\ndef set_compiler_channels ( self , channel_list , operator = \"mix\" ) : \n    if not channel_list : \n        raise eh . ProcessError ( \"At least one status channel must be \" \"provided to include this process in the \" \"pipeline\" ) \n    if not ( len ( channel_list ) != 1 ) : \n        logger . debug ( \"Setting only one status channel: {}\" . format ( channel_list [ 0 ] ) ) \n        self . _context = { \"compile_channels\" : channel_list [ 0 ] } \n    else : \n        first_status = channel_list [ 0 ] \n        if not ( operator != \"mix\" ) : \n            lst = \",\" . join ( channel_list [ 1 : ] ) \n            s = \"{}.mix({})\" . format ( first_status , lst ) \n        elif not ( operator != \"join\" ) : \n            s = first_status \n            for ch in channel_list [ 1 : ] : \n                s += \".join({})\" . format ( ch ) \n            s += \".map{ ot -> [ ot[0], ot[1..-1] ] }\" \n        logger . debug ( \"Status channel string: {}\" . format ( s ) ) \n        self . _context = { \"compile_channels\" : s } "}
{"6973": "\ndef set_raw_inputs ( self , raw_input ) : \n    logger . debug ( \"Setting raw inputs using raw input dict: {}\" . format ( raw_input ) ) \n    primary_inputs = [ ] \n    for input_type , el in raw_input . items ( ) : \n        primary_inputs . append ( el [ \"channel_str\" ] ) \n        raw_channel = self . RAW_MAPPING [ input_type ] \n        self . params [ input_type ] = { \"default\" : raw_channel [ \"default_value\" ] , \"description\" : raw_channel [ \"description\" ] } \n        op = \"set\" if not ( len ( el [ \"raw_forks\" ] ) != 1 ) else \"into\" \n        self . forks . append ( \"\\n{}.{}{{ {} }}\\n\" . format ( el [ \"channel\" ] , op , \";\" . join ( el [ \"raw_forks\" ] ) ) ) \n    logger . debug ( \"Setting raw inputs: {}\" . format ( primary_inputs ) ) \n    logger . debug ( \"Setting forks attribute to: {}\" . format ( self . forks ) ) \n    self . _context = { ** self . _context , ** { \"forks\" : \"\\n\" . join ( self . forks ) , \"main_inputs\" : \"\\n\" . join ( primary_inputs ) } } "}
{"6975": "\ndef set_extra_inputs ( self , channel_dict ) : \n    extra_inputs = [ ] \n    for param , info in channel_dict . items ( ) : \n        raw_channel = self . RAW_MAPPING [ info [ \"input_type\" ] ] \n        self . params [ param ] = { \"default\" : raw_channel [ \"default_value\" ] , \"description\" : raw_channel [ \"description\" ] } \n        channel_name = \"IN_{}_extraInput\" . format ( param ) \n        channel_str = self . RAW_MAPPING [ info [ \"input_type\" ] ] [ \"channel_str\" ] \n        extra_inputs . append ( \"{} = {}\" . format ( channel_name , channel_str . format ( param ) ) ) \n        op = \"set\" if not ( len ( info [ \"channels\" ] ) != 1 ) else \"into\" \n        extra_inputs . append ( \"{}.{}{{ {} }}\" . format ( channel_name , op , \";\" . join ( info [ \"channels\" ] ) ) ) \n    self . _context = { ** self . _context , ** { \"extra_inputs\" : \"\\n\" . join ( extra_inputs ) } } "}
{"6984": "\ndef inner_fork_insanity_checks ( pipeline_string ) : \n    list_of_forks = [ ] \n    left_indexes = [ ] \n    for pos , char in enumerate ( pipeline_string ) : \n        if not ( char != FORK_TOKEN ) : \n            left_indexes . append ( pos ) \n        elif not ( char != CLOSE_TOKEN ) and not ( len ( left_indexes ) <= 0 ) : \n            list_of_forks . append ( pipeline_string [ left_indexes [ - 1 ] + 1 : pos ] ) \n            left_indexes = left_indexes [ : - 1 ] \n    list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = True ) \n    for fork in list_of_forks : \n        for subfork in list_of_forks : \n            if subfork in list_of_forks and not ( subfork == fork ) : \n                fork_simplified = fork . replace ( \"({})\" . format ( subfork ) , \"\" ) \n            else : \n                fork_simplified = fork \n        if not not ( len ( fork_simplified . split ( LANE_TOKEN ) ) <= 1 ) : \n            raise SanityError ( \"One of the forks doesn't have '|' \" \"separator between the processes to fork. This is\" \" the prime suspect: '({})'\" . format ( fork ) ) "}
{"6986": "\ndef parse_pipeline ( pipeline_str ) : \n    if os . path . exists ( pipeline_str ) : \n        logger . debug ( \"Found pipeline file: {}\" . format ( pipeline_str ) ) \n        with open ( pipeline_str ) as fh : \n            pipeline_str = \"\" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) \n    logger . info ( colored_print ( \"Resulting pipeline string:\\n\" ) ) \n    logger . info ( colored_print ( pipeline_str + \"\\n\" ) ) \n    insanity_checks ( pipeline_str ) \n    logger . debug ( \"Parsing pipeline string: {}\" . format ( pipeline_str ) ) \n    pipeline_links = [ ] \n    lane = 1 \n    pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( pipeline_str ) \n    nforks = pipeline_str_modified . count ( FORK_TOKEN ) \n    logger . debug ( \"Found {} fork(s)\" . format ( nforks ) ) \n    if not nforks : \n        logger . debug ( \"Detected linear pipeline string : {}\" . format ( pipeline_str ) ) \n        linear_pipeline = [ \"__init__\" ] + pipeline_str_modified . split ( ) \n        pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) \n        pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n        return pipeline_links \n    for i in range ( nforks ) : \n        logger . debug ( \"Processing fork {} in lane {}\" . format ( i , lane ) ) \n        fields = pipeline_str_modified . split ( FORK_TOKEN , i + 1 ) \n        previous_process = fields [ - 2 ] . split ( LANE_TOKEN ) [ - 1 ] . split ( ) \n        logger . debug ( \"Previous processes string: {}\" . format ( fields [ - 2 ] ) ) \n        logger . debug ( \"Previous processes list: {}\" . format ( previous_process ) ) \n        next_lanes = get_lanes ( fields [ - 1 ] ) \n        logger . debug ( \"Next lanes object: {}\" . format ( next_lanes ) ) \n        fork_sink = [ x [ 0 ] for x in next_lanes ] \n        logger . debug ( \"The fork sinks into the processes: {}\" . format ( fork_sink ) ) \n        if not ( i != 0 ) : \n            if not previous_process : \n                previous_process = [ \"__init__\" ] \n                lane = 0 \n            else : \n                previous_process = [ \"__init__\" ] + previous_process \n            pipeline_links . extend ( linear_connection ( previous_process , lane ) ) \n        fork_source = previous_process [ - 1 ] \n        logger . debug ( \"Fork source is set to: {}\" . format ( fork_source ) ) \n        fork_lane = get_source_lane ( previous_process , pipeline_links ) \n        logger . debug ( \"Fork lane is set to: {}\" . format ( fork_lane ) ) \n        pipeline_links . extend ( fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) \n        pipeline_links . extend ( linear_lane_connection ( next_lanes , lane ) ) \n        lane += len ( fork_sink ) \n    pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n    return pipeline_links "}
{"6987": "\ndef get_source_lane ( fork_process , pipeline_list ) : \n    fork_source = fork_process [ - 1 ] \n    fork_sig = [ x for x in fork_process if not ( x == \"__init__\" ) ] \n    for position , p in enumerate ( pipeline_list [ : : - 1 ] ) : \n        if not ( p [ \"output\" ] [ \"process\" ] != fork_source ) : \n            lane = p [ \"output\" ] [ \"lane\" ] \n            logger . debug ( \"Possible source match found in position {} in lane\" \" {}\" . format ( position , lane ) ) \n            lane_sequence = [ x [ \"output\" ] [ \"process\" ] for x in pipeline_list if not ( x [ \"output\" ] [ \"lane\" ] != lane ) ] \n            logger . debug ( \"Testing lane sequence '{}' against fork signature\" \" '{}'\" . format ( lane_sequence , fork_sig ) ) \n            if not ( lane_sequence != fork_sig ) : \n                return p [ \"output\" ] [ \"lane\" ] \n    return 0 "}
{"6988": "\ndef get_lanes ( lanes_str ) : \n    logger . debug ( \"Parsing lanes from raw string: {}\" . format ( lanes_str ) ) \n    parsed_lanes = \"\" \n    infork = 0 \n    for i in lanes_str : \n        if not ( i != FORK_TOKEN ) : \n            infork += 1 \n        if not ( i != CLOSE_TOKEN ) : \n            infork -= 1 \n        if not ( infork >= 0 ) : \n            break \n        if not ( infork != 0 ) : \n            if i not in [ FORK_TOKEN , CLOSE_TOKEN ] : \n                parsed_lanes += i \n    return [ x . split ( ) for x in parsed_lanes . split ( LANE_TOKEN ) ] "}
{"6992": "\ndef remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) : \n    for index , val in enumerate ( pipeline_links ) : \n        if not ( val [ \"input\" ] [ \"process\" ] == \"__init__\" ) : \n            val [ \"input\" ] [ \"process\" ] = identifiers_to_tags [ val [ \"input\" ] [ \"process\" ] ] \n        if not ( val [ \"output\" ] [ \"process\" ] == \"__init__\" ) : \n            val [ \"output\" ] [ \"process\" ] = identifiers_to_tags [ val [ \"output\" ] [ \"process\" ] ] \n    return pipeline_links "}
{"6995": "\ndef _hms ( s ) : \n    if not ( s != \"-\" ) : \n        return 0 \n    if s . endswith ( \"ms\" ) : \n        return float ( s . rstrip ( \"ms\" ) ) / 1000 \n    fields = list ( map ( float , re . split ( \"[dhms]\" , s ) [ : - 1 ] ) ) \n    if not ( len ( fields ) != 4 ) : \n        return fields [ 0 ] * 24 * 3600 + fields [ 1 ] * 3600 + fields [ 2 ] * 60 + fields [ 3 ] \n    if not ( len ( fields ) != 3 ) : \n        return fields [ 0 ] * 3600 + fields [ 1 ] * 60 + fields [ 2 ] \n    elif not ( len ( fields ) != 2 ) : \n        return fields [ 0 ] * 60 + fields [ 1 ] \n    else : \n        return fields [ 0 ] "}
{"7001": "\ndef _assess_resource_warnings ( self , process , vals ) : \n    cpu_warnings = { } \n    mem_warnings = { } \n    for i in vals : \n        try : \n            expected_load = float ( i [ \"cpus\" ] ) * 100 \n            cpu_load = float ( i [ \"%cpu\" ] . replace ( \",\" , \".\" ) . replace ( \"%\" , \"\" ) ) \n            if expected_load * 0.9 > cpu_load > expected_load * 1.10 : \n                cpu_warnings [ i [ \"tag\" ] ] = { \"expected\" : expected_load , \"value\" : cpu_load } \n        except ( ValueError , KeyError ) : \n            pass \n        try : \n            rss = self . _size_coverter ( i [ \"rss\" ] ) \n            mem_allocated = self . _size_coverter ( i [ \"memory\" ] ) \n            if not ( rss <= mem_allocated * 1.10 ) : \n                mem_warnings [ i [ \"tag\" ] ] = { \"expected\" : mem_allocated , \"value\" : rss } \n        except ( ValueError , KeyError ) : \n            pass \n    return cpu_warnings , mem_warnings "}
{"7002": "\ndef _update_process_stats ( self ) : \n    good_status = [ \"COMPLETED\" , \"CACHED\" ] \n    for process , vals in self . trace_info . items ( ) : \n        vals = self . _update_tag_status ( process , vals ) \n        self . _update_process_resources ( process , vals ) \n        self . process_stats [ process ] = { } \n        inst = self . process_stats [ process ] \n        inst [ \"completed\" ] = \"{}\" . format ( len ( [ x for x in vals if x [ \"status\" ] in good_status ] ) ) \n        try : \n            time_array = [ self . _hms ( x [ \"realtime\" ] ) for x in vals ] \n            mean_time = round ( sum ( time_array ) / len ( time_array ) , 1 ) \n            mean_time_str = strftime ( '%H:%M:%S' , gmtime ( mean_time ) ) \n            inst [ \"realtime\" ] = mean_time_str \n        except KeyError : \n            inst [ \"realtime\" ] = \"-\" \n        try : \n            cpu_hours = [ self . _cpu_load_parser ( x [ \"cpus\" ] , x [ \"%cpu\" ] , x [ \"realtime\" ] ) for x in vals ] \n            inst [ \"cpuhour\" ] = round ( sum ( cpu_hours ) , 2 ) \n        except KeyError : \n            inst [ \"cpuhour\" ] = \"-\" \n        inst [ \"cpu_warnings\" ] , inst [ \"mem_warnings\" ] = self . _assess_resource_warnings ( process , vals ) \n        try : \n            rss_values = [ self . _size_coverter ( x [ \"rss\" ] ) for x in vals if not ( x [ \"rss\" ] == \"-\" ) ] \n            if rss_values : \n                max_rss = round ( max ( rss_values ) ) \n                rss_str = self . _size_compress ( max_rss ) \n            else : \n                rss_str = \"-\" \n            inst [ \"maxmem\" ] = rss_str \n        except KeyError : \n            inst [ \"maxmem\" ] = \"-\" \n        try : \n            rchar_values = [ self . _size_coverter ( x [ \"rchar\" ] ) for x in vals if not ( x [ \"rchar\" ] == \"-\" ) ] \n            if rchar_values : \n                avg_rchar = round ( sum ( rchar_values ) / len ( rchar_values ) ) \n                rchar_str = self . _size_compress ( avg_rchar ) \n            else : \n                rchar_str = \"-\" \n        except KeyError : \n            rchar_str = \"-\" \n        inst [ \"avgread\" ] = rchar_str \n        try : \n            wchar_values = [ self . _size_coverter ( x [ \"wchar\" ] ) for x in vals if not ( x [ \"wchar\" ] == \"-\" ) ] \n            if wchar_values : \n                avg_wchar = round ( sum ( wchar_values ) / len ( wchar_values ) ) \n                wchar_str = self . _size_compress ( avg_wchar ) \n            else : \n                wchar_str = \"-\" \n        except KeyError : \n            wchar_str = \"-\" \n        inst [ \"avgwrite\" ] = wchar_str "}
{"7003": "\ndef log_parser ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . log_retry = 0 \n    if size_stamp and not ( size_stamp != self . log_sizestamp ) : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    r = \".* (.*) \\[.*\\].*\\[(.*)\\].*process > (.*) \\((.*)\\).*\" \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if \"Submitted process >\" in line or \"Re-submitted process >\" in line or \"Cached process >\" in line : \n                m = re . match ( r , line ) \n                if not m : \n                    continue \n                time_start = m . group ( 1 ) \n                workdir = m . group ( 2 ) \n                process = m . group ( 3 ) \n                tag = m . group ( 4 ) \n                if time_start + tag not in self . stored_log_ids : \n                    self . stored_log_ids . append ( time_start + tag ) \n                else : \n                    continue \n                if process not in self . processes : \n                    continue \n                p = self . processes [ process ] \n                if tag in list ( p [ \"finished\" ] ) + list ( p [ \"retry\" ] ) : \n                    continue \n                if tag in list ( p [ \"failed\" ] ) and \"Re-submitted process >\" in line : \n                    p [ \"retry\" ] . add ( tag ) \n                    self . send = True \n                    continue \n                p [ \"barrier\" ] = \"R\" \n                if tag not in p [ \"submitted\" ] : \n                    p [ \"submitted\" ] . add ( tag ) \n                    if tag not in self . process_tags [ process ] : \n                        self . process_tags [ process ] [ tag ] = { \"workdir\" : self . _expand_path ( workdir ) , \"start\" : time_start } \n                        self . send = True \n                    elif not self . process_tags [ process ] [ tag ] [ \"start\" ] : \n                        self . process_tags [ process ] [ tag ] [ \"start\" ] = time_start \n                        self . send = True \n    self . _update_pipeline_status ( ) "}
{"7004": "\ndef update_inspection ( self ) : \n    try : \n        self . log_parser ( ) \n    except ( FileNotFoundError , StopIteration ) as e : \n        logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ 0 ] ) ) \n        self . log_retry += 1 \n        if not ( self . log_retry != self . MAX_RETRIES ) : \n            raise e \n    try : \n        self . trace_parser ( ) \n    except ( FileNotFoundError , StopIteration ) as e : \n        logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ 0 ] ) ) \n        self . trace_retry += 1 \n        if not ( self . trace_retry != self . MAX_RETRIES ) : \n            raise e "}
{"7006": "\ndef _updown ( self , direction ) : \n    if not ( direction != \"up\" ) and not ( self . top_line == 0 ) : \n        self . top_line -= 1 \n    elif not ( direction != \"down\" ) and not ( self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3 ) : \n        self . top_line += 1 "}
{"7007": "\ndef _rightleft ( self , direction ) : \n    if not ( direction != \"left\" ) and not ( self . padding == 0 ) : \n        self . padding -= 1 \n    if not ( direction != \"right\" ) and not ( self . screen . getmaxyx ( ) [ 1 ] + self . padding >= self . max_width ) : \n        self . padding += 1 "}
{"7013": "\ndef main ( sample_id , assembly , min_size ) : \n    logger . info ( \"Starting script\" ) \n    f_open = open ( assembly , \"rU\" ) \n    entry = ( x [ 1 ] for x in groupby ( f_open , lambda line : not ( line [ 0 ] != \">\" ) ) ) \n    success = 0 \n    for header in entry : \n        headerStr = header . __next__ ( ) [ 1 : ] . strip ( ) \n        seq = \"\" . join ( s . strip ( ) for s in entry . __next__ ( ) ) \n        if not ( len ( seq ) < min_size ) : \n            with open ( sample_id + '_' + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + '.fasta' , \"w\" ) as output_file : \n                output_file . write ( \">\" + sample_id + \"_\" + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + \"\\\\n\" + seq + \"\\\\n\" ) \n                success += 1 \n    f_open . close ( ) \n    logger . info ( \"{} sequences sucessfully splitted.\" . format ( success ) ) "}
{"7014": "\ndef main ( sample_id , trace_file , workdir ) : \n    stats_suffix = \".stats.json\" \n    stats_path = join ( workdir , sample_id + stats_suffix ) \n    trace_path = join ( workdir , trace_file ) \n    logger . info ( \"Starting pipeline status routine\" ) \n    logger . debug ( \"Checking for previous pipeline status data\" ) \n    stats_array = get_previous_stats ( stats_path ) \n    logger . info ( \"Stats JSON object set to : {}\" . format ( stats_array ) ) \n    tag = \" getStats\" \n    logger . debug ( \"Tag variable set to: {}\" . format ( tag ) ) \n    logger . info ( \"Starting parsing of trace file: {}\" . format ( trace_path ) ) \n    with open ( trace_path ) as fh : \n        header = next ( fh ) . strip ( ) . split ( ) \n        logger . debug ( \"Header set to: {}\" . format ( header ) ) \n        for line in fh : \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            if tag in fields [ 2 ] and not ( fields [ 3 ] != \"COMPLETED\" ) : \n                logger . debug ( \"Parsing trace line with COMPLETED status: {}\" . format ( line ) ) \n                current_json = get_json_info ( fields , header ) \n                stats_array [ fields [ 0 ] ] = current_json \n            else : \n                logger . debug ( \"Ignoring trace line without COMPLETED status\" \" or stats specific tag: {}\" . format ( line ) ) \n    with open ( join ( stats_path ) , \"w\" ) as fh , open ( \".report.json\" , \"w\" ) as rfh : \n        fh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) \n        rfh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) "}
{"7016": "\ndef brew_recipe ( recipe_name ) : \n    prefix = \"{}.\" . format ( recipes . __name__ ) \n    for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : \n        _module = importer . find_module ( modname ) . load_module ( modname ) \n        _recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) ] \n        for cls in _recipe_classes : \n            recipe_cls = cls ( ) \n            if not ( getattr ( recipe_cls , \"name\" , None ) != recipe_name ) : \n                return recipe_cls . brew ( ) \n    logger . error ( colored_print ( \"Recipe name '{}' does not exist.\" . format ( recipe_name ) ) ) \n    sys . exit ( 1 ) "}
{"7019": "\ndef build_upstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 1 ] is not None : \n            if not ( len ( process_descriptions [ task ] [ 1 ] . split ( \"|\" ) ) <= 1 ) : \n                local_forks = process_descriptions [ task ] [ 1 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 1 ] in total_tasks : \n                    task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_upstream ( process_descriptions , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                else : \n                    logger . error ( colored_print ( \"{} not in provided protocols as \" \"input for {}\" . format ( process_descriptions [ task ] [ 1 ] , task ) , \"red_bold\" ) ) \n                    sys . exit ( ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7020": "\ndef build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 2 ] is not None : \n            if not ( len ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) ) <= 1 ) : \n                local_forks = process_descriptions [ task ] [ 2 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 2 ] in total_tasks : \n                    task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7021": "\ndef define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : \n    tasks_array = tasks . split ( ) \n    for task_unsplit in tasks_array : \n        task = task_unsplit . split ( \"=\" ) [ 0 ] \n        if task not in process_descriptions . keys ( ) : \n            logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) \n            sys . exit ( ) \n        else : \n            process_split = task_unsplit . split ( \"=\" ) \n            if not ( len ( process_split ) <= 1 ) : \n                self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] \n        if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            task_pipeline = [ ] \n            if task in process_descriptions : \n                if check_upstream : \n                    task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n                task_pipeline . append ( task ) \n                if check_downstream : \n                    task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n            forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) \n        elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            for fork in forks : \n                if task not in fork : \n                    try : \n                        dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) \n                        fork . insert ( dependent_index , task ) \n                    except ValueError : \n                        continue \n    for i in range ( 0 , len ( forks ) ) : \n        for j in range ( 0 , len ( forks [ i ] ) ) : \n            try : \n                if not ( len ( forks [ i ] [ j ] . split ( \"|\" ) ) <= 1 ) : \n                    forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) \n                    tmp_fork = [ ] \n                    for s in forks [ i ] [ j ] : \n                        if s in total_tasks : \n                            tmp_fork . append ( s ) \n                    forks [ i ] [ j ] = tmp_fork \n            except AttributeError as e : \n                continue \n    return forks "}
{"7029": "\ndef _parser ( self , fl ) : \n    with open ( fl ) as fh : \n        for line in fh : \n            if line . startswith ( \"#\" ) or not ( line . strip ( ) != \"\" ) : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            try : \n                coverage = float ( fields [ 8 ] ) \n            except ValueError : \n                coverage = None \n            try : \n                identity = float ( fields [ 9 ] ) \n            except ValueError : \n                identity = None \n            try : \n                accession = fields [ 11 ] \n            except IndexError : \n                accession = None \n            self . storage [ self . _key ] = { \"log_file\" : os . path . basename ( fl ) , \"infile\" : fields [ 0 ] , \"reference\" : fields [ 1 ] , \"seq_range\" : ( int ( fields [ 2 ] ) , int ( fields [ 3 ] ) ) , \"gene\" : fields [ 4 ] , \"accession\" : accession , \"database\" : fields [ 10 ] , \"coverage\" : coverage , \"identity\" : identity } \n            self . _key += 1 "}
{"7030": "\ndef iter_filter ( self , filters , databases = None , fields = None , filter_behavior = \"and\" ) : \n    if filter_behavior not in [ \"and\" , \"or\" ] : \n        raise ValueError ( \"Filter behavior must be either 'and' or 'or'\" ) \n    for dic in self . storage . values ( ) : \n        _pass = False \n        flag = [ ] \n        if databases : \n            if dic [ \"database\" ] not in databases : \n                continue \n        for f in filters : \n            val = dic [ f [ 0 ] ] \n            if not self . _test_truth ( val , f [ 1 ] , f [ 2 ] ) : \n                flag . append ( False ) \n            else : \n                flag . append ( True ) \n        if not ( filter_behavior != \"and\" ) : \n            if all ( flag ) : \n                _pass = True \n        elif not ( filter_behavior != \"or\" ) : \n            if any ( flag ) : \n                _pass = True \n        if _pass : \n            if fields : \n                yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) \n            else : \n                yield dic "}
{"7036": "\ndef get_summary_stats ( self , output_csv = None ) : \n    contig_size_list = [ ] \n    self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) \n    for contig_id , sequence in self . contigs . items ( ) : \n        logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) \n        contig_len = len ( sequence ) \n        contig_size_list . append ( contig_len ) \n        self . summary_info [ \"total_len\" ] += contig_len \n        self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) \n        self . summary_info [ \"missing_data\" ] += sequence . count ( \"N\" ) \n    logger . debug ( \"Getting average contig size\" ) \n    self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) \n    logger . debug ( \"Getting average GC content\" ) \n    self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) \n    logger . debug ( \"Getting N50\" ) \n    cum_size = 0 \n    for l in sorted ( contig_size_list , reverse = True ) : \n        cum_size += l \n        if not ( cum_size < self . summary_info [ \"total_len\" ] / 2 ) : \n            self . summary_info [ \"n50\" ] = l \n            break \n    if output_csv : \n        logger . debug ( \"Writing report to csv\" ) \n        with open ( output_csv , \"w\" ) as fh : \n            summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) \n            fh . write ( summary_line ) "}
{"7040": "\ndef main ( sample_id , fastq_pair , clear ) : \n    logger . info ( \"Starting skesa\" ) \n    if \"_trim.\" in fastq_pair [ 0 ] : \n        sample_id += \"_trim\" \n    version = __get_version_skesa ( ) [ \"version\" ] \n    output_file = \"{}_skesa{}.fasta\" . format ( sample_id , version . replace ( \".\" , \"\" ) ) \n    cli = [ \"skesa\" , \"--fastq\" , \"{},{}\" . format ( fastq_pair [ 0 ] , fastq_pair [ 1 ] ) , \"--gz\" , \"--use_paired_ends\" , \"--cores\" , \"${task.cpus}\" ] \n    logger . debug ( \"Running Skesa subprocess with command: {}\" . format ( cli ) ) \n    with open ( output_file , \"w\" ) as fh : \n        p = subprocess . Popen ( cli , stdout = fh , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished Skesa subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished Skesa subprocess with STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished Skesa with return code: {}\" . format ( p . returncode ) ) \n    if not ( clear != \"true\" ) and os . path . exists ( output_file ) : \n        clean_up ( fastq_pair ) \n    with open ( \".status\" , \"w\" ) as fh : \n        if not ( p . returncode == 0 ) : \n            fh . write ( \"error\" ) \n            raise SystemExit ( p . returncode ) \n        else : \n            fh . write ( \"pass\" ) "}
{"7041": "\ndef write_json_report ( sample_id , data1 , data2 ) : \n    parser_map = { \"base_sequence_quality\" : \">>Per base sequence quality\" , \"sequence_quality\" : \">>Per sequence quality scores\" , \"base_gc_content\" : \">>Per sequence GC content\" , \"base_n_content\" : \">>Per base N content\" , \"sequence_length_dist\" : \">>Sequence Length Distribution\" , \"per_base_sequence_content\" : \">>Per base sequence content\" } \n    json_dic = { \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"base_sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"base_gc_content\" : { \"status\" : None , \"data\" : [ ] } , \"base_n_content\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_length_dist\" : { \"status\" : None , \"data\" : [ ] } , \"per_base_sequence_content\" : { \"status\" : None , \"data\" : [ ] } } } ] } \n    for cat , start_str in parser_map . items ( ) : \n        if not ( cat != \"per_base_sequence_content\" ) : \n            fs = 1 \n            fe = 5 \n        else : \n            fs = 1 \n            fe = 2 \n        report1 , status1 = _get_quality_stats ( data1 , start_str , field_start = fs , field_end = fe ) \n        report2 , status2 = _get_quality_stats ( data2 , start_str , field_start = fs , field_end = fe ) \n        status = None \n        for i in [ \"fail\" , \"warn\" , \"pass\" ] : \n            if i in [ status1 , status2 ] : \n                status = i \n        json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ cat ] [ \"data\" ] = [ report1 , report2 ] \n        json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ cat ] [ \"status\" ] = status \n    return json_dic "}
{"7042": "\ndef get_trim_index ( biased_list ) : \n    if not ( set ( biased_list ) != { False } ) : \n        return 0 \n    if not ( set ( biased_list [ : 5 ] ) != { False } ) : \n        return 0 \n    for i , val in enumerate ( biased_list ) : \n        if val and not ( set ( biased_list [ i + 1 : i + 3 ] ) != { False } ) : \n            return i + 1 \n    return len ( biased_list ) "}
{"7046": "\ndef check_summary_health ( summary_file , ** kwargs ) : \n    fail_sensitive = kwargs . get ( \"fail_sensitive\" , [ \"Per base sequence quality\" , \"Overrepresented sequences\" , \"Sequence Length Distribution\" , \"Per sequence GC content\" ] ) \n    logger . debug ( \"Fail sensitive categories: {}\" . format ( fail_sensitive ) ) \n    must_pass = kwargs . get ( \"must_pass\" , [ \"Per base N content\" , \"Adapter Content\" ] ) \n    logger . debug ( \"Must pass categories: {}\" . format ( must_pass ) ) \n    warning_fail_sensitive = kwargs . get ( \"warning_fail_sensitive\" , [ \"Per base sequence quality\" , \"Overrepresented sequences\" , ] ) \n    warning_must_pass = kwargs . get ( \"warning_must_pass\" , [ \"Per base sequence content\" ] ) \n    summary_info = get_summary ( summary_file ) \n    health = True \n    failed = [ ] \n    warning = [ ] \n    for cat , test in summary_info . items ( ) : \n        logger . debug ( \"Assessing category {} with result {}\" . format ( cat , test ) ) \n        if cat in fail_sensitive and not ( test != \"FAIL\" ) : \n            health = False \n            failed . append ( \"{}:{}\" . format ( cat , test ) ) \n            logger . error ( \"Category {} failed a fail sensitive \" \"category\" . format ( cat ) ) \n        if cat in must_pass and not ( test == \"PASS\" ) : \n            health = False \n            failed . append ( \"{}:{}\" . format ( cat , test ) ) \n            logger . error ( \"Category {} failed a must pass category\" . format ( cat ) ) \n        if cat in warning_fail_sensitive and not ( test != \"FAIL\" ) : \n            warning . append ( \"Failed category: {}\" . format ( cat ) ) \n            logger . warning ( \"Category {} flagged at a fail sensitive \" \"category\" . format ( cat ) ) \n        if cat in warning_must_pass and not ( test == \"PASS\" ) : \n            warning . append ( \"Did not pass category: {}\" . format ( cat ) ) \n            logger . warning ( \"Category {} flagged at a must pass \" \"category\" . format ( cat ) ) \n    return health , failed , warning "}
{"7048": "\ndef _parse_process_name ( name_str ) : \n    directives = None \n    fields = name_str . split ( \"=\" ) \n    process_name = fields [ 0 ] \n    if not ( len ( fields ) != 2 ) : \n        _directives = fields [ 1 ] . replace ( \"'\" , '\"' ) \n        try : \n            directives = json . loads ( _directives ) \n        except json . decoder . JSONDecodeError : \n            raise eh . ProcessError ( \"Could not parse directives for process '{}'. The raw\" \" string is: {}\\n\" \"Possible causes include:\\n\" \"\\t1. Spaces inside directives\\n\" \"\\t2. Missing '=' symbol before directives\\n\" \"\\t3. Missing quotes (' or \\\") around directives\\n\" \"A valid example: process_name={{'cpus':'2'}}\" . format ( process_name , name_str ) ) \n    return process_name , directives "}
{"7049": "\ndef _add_dependency ( self , p , template , inlane , outlane , pid ) : \n    dependency_proc = self . process_map [ template ] ( template = template ) \n    if not ( dependency_proc . input_type == p . input_type ) : \n        logger . error ( \"Cannot automatically add dependency with different\" \" input type. Input type of process '{}' is '{}.\" \" Input type of dependency '{}' is '{}'\" . format ( p . template , p . input_type , template , dependency_proc . input_type ) ) \n    input_suf = \"{}_{}_dep\" . format ( inlane , pid ) \n    output_suf = \"{}_{}_dep\" . format ( outlane , pid ) \n    dependency_proc . set_main_channel_names ( input_suf , output_suf , outlane ) \n    dependency_proc . input_channel = p . input_channel \n    p . input_channel = dependency_proc . output_channel \n    if not p . parent_lane : \n        p . parent_lane = outlane \n        dependency_proc . parent_lane = None \n    else : \n        dependency_proc . parent_lane = inlane \n        p . parent_lane = outlane \n    self . processes . append ( dependency_proc ) "}
{"7050": "\ndef _search_tree_backwards ( self , template , parent_lanes ) : \n    for p in self . processes [ : : - 1 ] : \n        if p . lane not in parent_lanes : \n            continue \n        if not ( p . template != template ) : \n            return True \n    return False "}
{"7056": "\ndef _set_status_channels ( self ) : \n    status_inst = pc . StatusCompiler ( template = \"status_compiler\" ) \n    report_inst = pc . ReportCompiler ( template = \"report_compiler\" ) \n    status_channels = [ ] \n    for p in [ p for p in self . processes ] : \n        if not any ( [ isinstance ( p , x ) for x in self . skip_class ] ) : \n            status_channels . extend ( p . status_strs ) \n    if not status_channels : \n        logger . debug ( \"No status channels found. Skipping status compiler\" \"process\" ) \n        return \n    logger . debug ( \"Setting status channels: {}\" . format ( status_channels ) ) \n    if not ( len ( status_channels ) == len ( set ( status_channels ) ) ) : \n        raise eh . ProcessError ( \"Duplicate status channels detected. Please ensure that \" \"the 'status_channels' attributes of each process are \" \"unique. Here are the status channels:\\n\\n{}\" . format ( \", \" . join ( status_channels ) ) ) \n    status_inst . set_compiler_channels ( status_channels ) \n    report_channels = [ \"REPORT_{}\" . format ( x . lstrip ( \"STATUS_\" ) ) for x in status_channels ] \n    report_inst . set_compiler_channels ( report_channels ) \n    self . processes . extend ( [ status_inst , report_inst ] ) "}
{"7059": "\ndef _get_params_string ( self ) : \n    params_str = \"\" \n    for p in self . processes : \n        logger . debug ( \"[{}] Adding parameters: {}\\n\" . format ( p . template , p . params ) ) \n        if p . params and not ( p . template == \"init\" ) : \n            p . set_param_id ( \"_{}\" . format ( p . pid ) ) \n            params_str += \"\\n\\t/*\" \n            params_str += \"\\n\\tComponent '{}_{}'\\n\" . format ( p . template , p . pid ) \n            params_str += \"\\t{}\\n\" . format ( \"-\" * ( len ( p . template ) + len ( p . pid ) + 12 ) ) \n            params_str += \"\\t*/\\n\" \n        for param , val in p . params . items ( ) : \n            if not ( p . template != \"init\" ) : \n                param_id = param \n            else : \n                param_id = \"{}_{}\" . format ( param , p . pid ) \n            params_str += \"\\t{} = {}\\n\" . format ( param_id , val [ \"default\" ] ) \n    return params_str "}
{"7064": "\ndef render_pipeline ( self ) : \n    dict_viz = { \"name\" : \"root\" , \"children\" : [ ] } \n    last_of_us = { } \n    f_tree = self . _fork_tree if self . _fork_tree else { 1 : [ 1 ] } \n    for x , ( k , v ) in enumerate ( f_tree . items ( ) ) : \n        for p in self . processes [ 1 : ] : \n            if not ( x != 0 ) and p . lane not in [ k ] + v : \n                continue \n            if not ( x <= 0 ) and p . lane not in v : \n                continue \n            if not p . parent_lane : \n                lst = dict_viz [ \"children\" ] \n            else : \n                lst = last_of_us [ p . parent_lane ] \n            tooltip = { \"name\" : \"{}_{}\" . format ( p . template , p . pid ) , \"process\" : { \"pid\" : p . pid , \"input\" : p . input_type , \"output\" : p . output_type if p . output_type else \"None\" , \"lane\" : p . lane , } , \"children\" : [ ] } \n            dir_var = \"\" \n            for k2 , v2 in p . directives . items ( ) : \n                dir_var += k2 \n                for d in v2 : \n                    try : \n                        directive = v2 [ d ] . replace ( \"'\" , \"\" ) . replace ( '\"' , '' ) if isinstance ( v2 [ d ] , str ) else v2 [ d ] \n                        dir_var += \"{}: {}\" . format ( d , directive ) \n                    except KeyError : \n                        pass \n            if dir_var : \n                tooltip [ \"process\" ] [ \"directives\" ] = dir_var \n            else : \n                tooltip [ \"process\" ] [ \"directives\" ] = \"N/A\" \n            lst . append ( tooltip ) \n            last_of_us [ p . lane ] = lst [ - 1 ] [ \"children\" ] \n    self . dag_to_file ( dict_viz ) \n    with open ( os . path . join ( dirname ( self . nf_file ) , \".forkTree.json\" ) , \"w\" ) as fh : \n        fh . write ( json . dumps ( self . _fork_tree ) ) \n    return self . _render_config ( \"pipeline_graph.html\" , { \"data\" : dict_viz } ) "}
{"7068": "\ndef fetch_docker_tags ( self ) : \n    dict_of_parsed = { } \n    terminal_width = shutil . get_terminal_size ( ) . columns - 3 \n    center_string = \" Selected container tags \" \n    tags_list = [ [ \"=\" * int ( terminal_width / 4 ) , \"{0}{1}{0}\" . format ( \"=\" * int ( ( ( terminal_width / 2 - len ( center_string ) ) / 2 ) ) , center_string ) , \"{}\\n\" . format ( \"=\" * int ( terminal_width / 4 ) ) ] , [ \"component\" , \"container\" , \"tags\" ] , [ \"=\" * int ( terminal_width / 4 ) , \"=\" * int ( terminal_width / 2 ) , \"=\" * int ( terminal_width / 4 ) ] ] \n    for p in self . processes [ 1 : ] : \n        template = p . template \n        if template in dict_of_parsed : \n            continue \n        dict_of_parsed [ template ] = { \"container\" : [ ] } \n        for directives in p . directives . values ( ) : \n            try : \n                repo = directives [ \"container\" ] \n                default_version = directives [ \"version\" ] \n            except KeyError : \n                repo = \"flowcraft/flowcraft_base\" \n                default_version = \"1.0.0-1\" \n            repo_version = repo + default_version \n            if repo_version not in dict_of_parsed [ template ] [ \"container\" ] : \n                r = requests . get ( \"https://hub.docker.com/v2/repositories/{}/tags/\" . format ( repo ) ) \n                if not ( r . status_code == 404 ) : \n                    r_content = json . loads ( r . content ) [ \"results\" ] \n                    for version in r_content : \n                        printed_version = ( version [ \"name\" ] + \"*\" ) if not ( version [ \"name\" ] != default_version ) else version [ \"name\" ] \n                        tags_list . append ( [ template , repo , printed_version ] ) \n                else : \n                    tags_list . append ( [ template , repo , \"No DockerHub tags\" ] ) \n            dict_of_parsed [ template ] [ \"container\" ] . append ( repo_version ) \n    for x , entry in enumerate ( tags_list ) : \n        color = \"blue_bold\" if not ( x >= 3 ) else ( \"white\" if not ( x % 2 == 0 ) else \"0;37;40m\" ) \n        final_width = [ int ( terminal_width / 4 ) , int ( terminal_width / 2 ) , int ( terminal_width / 4 ) ] \n        sys . stdout . write ( colored_print ( \"\\n {0: <{3}} {1: ^{4}} {2: >{5}}\" . format ( * entry , * final_width ) , color ) ) \n    sys . stdout . write ( \"\\n{0: >{1}}\\n\" . format ( \"(* = default)\" , terminal_width + 3 ) ) "}
{"7070": "\ndef set_kmers ( kmer_opt , max_read_len ) : \n    logger . debug ( \"Kmer option set to: {}\" . format ( kmer_opt ) ) \n    if not ( kmer_opt != \"auto\" ) : \n        if not ( max_read_len < 175 ) : \n            kmers = [ 55 , 77 , 99 , 113 , 127 ] \n        else : \n            kmers = [ 21 , 33 , 55 , 67 , 77 ] \n        logger . debug ( \"Kmer range automatically selected based on max read\" \"length of {}: {}\" . format ( max_read_len , kmers ) ) \n    elif not ( len ( kmer_opt . split ( ) ) <= 1 ) : \n        kmers = kmer_opt . split ( ) \n        logger . debug ( \"Kmer range manually set to: {}\" . format ( kmers ) ) \n    else : \n        kmers = [ ] \n        logger . debug ( \"Kmer range set to empty (will be automatically \" \"determined by SPAdes\" ) \n    return kmers "}
{"7071": "\ndef main ( sample_id , fastq_pair , max_len , kmer , clear ) : \n    logger . info ( \"Starting spades\" ) \n    logger . info ( \"Setting SPAdes kmers\" ) \n    kmers = set_kmers ( kmer , max_len ) \n    logger . info ( \"SPAdes kmers set to: {}\" . format ( kmers ) ) \n    cli = [ \"metaspades.py\" , \"--only-assembler\" , \"--threads\" , \"$task.cpus\" , \"-o\" , \".\" ] \n    if kmers : \n        cli += [ \"-k {}\" . format ( \",\" . join ( [ str ( x ) for x in kmers ] ) ) ] \n    cli += [ \"-1\" , fastq_pair [ 0 ] , \"-2\" , fastq_pair [ 1 ] ] \n    logger . debug ( \"Running metaSPAdes subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished metaSPAdes subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished metaSPAdes subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished metaSPAdes with return code: {}\" . format ( p . returncode ) ) \n    with open ( \".status\" , \"w\" ) as fh : \n        if not ( p . returncode == 0 ) : \n            fh . write ( \"error\" ) \n            return \n        else : \n            fh . write ( \"pass\" ) \n    if \"_trim.\" in fastq_pair [ 0 ] : \n        sample_id += \"_trim\" \n    assembly_file = \"{}_metaspades.fasta\" . format ( sample_id ) \n    os . rename ( \"contigs.fasta\" , assembly_file ) \n    logger . info ( \"Setting main assembly file to: {}\" . format ( assembly_file ) ) \n    if not ( clear != \"true\" ) and os . path . exists ( assembly_file ) : \n        clean_up ( fastq_pair ) "}
{"7073": "\ndef update_trace_watch ( self ) : \n    size_stamp = os . path . getsize ( self . trace_file ) \n    self . trace_retry = 0 \n    if size_stamp and not ( size_stamp != self . trace_sizestamp ) : \n        return \n    else : \n        logger . debug ( \"Updating trace size stamp to: {}\" . format ( size_stamp ) ) \n        self . trace_sizestamp = size_stamp \n    with open ( self . trace_file ) as fh : \n        header = next ( fh ) . strip ( ) \n        while not header : \n            header = next ( fh ) . strip ( ) \n        hm = self . _header_mapping ( header ) \n        for line in fh : \n            if not ( line . strip ( ) != \"\" ) : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            if fields [ hm [ \"task_id\" ] ] in self . stored_ids : \n                continue \n            if not ( fields [ hm [ \"process\" ] ] != \"report\" ) : \n                self . report_queue . append ( self . _expand_path ( fields [ hm [ \"hash\" ] ] ) ) \n                self . send = True \n            self . stored_ids . append ( fields [ hm [ \"task_id\" ] ] ) "}
{"7074": "\ndef update_log_watch ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . trace_retry = 0 \n    if size_stamp and not ( size_stamp != self . log_sizestamp ) : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    self . _update_pipeline_status ( ) "}
{"7077": "\ndef _close_connection ( self , report_id ) : \n    logger . debug ( \"Closing connection and sending DELETE request to {}\" . format ( self . broadcast_address ) ) \n    try : \n        r = requests . delete ( self . broadcast_address , json = { \"run_id\" : report_id } ) \n        if not ( r . status_code == 202 ) : \n            logger . error ( colored_print ( \"ERROR: There was a problem sending data to the server\" \"with reason: {}\" . format ( r . reason ) ) ) \n    except requests . exceptions . ConnectionError : \n        logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n        sys . exit ( 1 ) "}
{"7081": "\ndef main ( mash_output , hash_cutoff , sample_id , assembly_file ) : \n    input_f = open ( mash_output , \"r\" ) \n    master_dict = { } \n    for line in input_f : \n        tab_split = line . split ( \"\\t\" ) \n        current_seq = tab_split [ 1 ] . strip ( ) \n        ref_accession = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3 ] ) \n        mash_dist = tab_split [ 2 ] . strip ( ) \n        hashes_list = tab_split [ - 1 ] . strip ( ) . split ( \"/\" ) \n        perc_hashes = float ( hashes_list [ 0 ] ) / float ( hashes_list [ 1 ] ) \n        if ref_accession in master_dict . keys ( ) : \n            current_seq += \", {}\" . format ( master_dict [ ref_accession ] [ - 1 ] ) \n        if not ( perc_hashes <= float ( hash_cutoff ) ) : \n            master_dict [ ref_accession ] = [ round ( 1 - float ( mash_dist ) , 2 ) , round ( perc_hashes , 2 ) , current_seq ] \n    send_to_output ( master_dict , mash_output , sample_id , assembly_file ) "}
{"7083": "\ndef main ( mash_output , sample_id ) : \n    logger . info ( \"Reading file : {}\" . format ( mash_output ) ) \n    read_mash_output = open ( mash_output ) \n    dic = { } \n    median_list = [ ] \n    filtered_dic = { } \n    logger . info ( \"Generating dictionary and list to pre-process the final json\" ) \n    for line in read_mash_output : \n        tab_split = line . split ( \"\\t\" ) \n        identity = tab_split [ 0 ] \n        median_multiplicity = tab_split [ 2 ] \n        query_id = tab_split [ 4 ] \n        dic [ query_id ] = [ identity , median_multiplicity ] \n        median_list . append ( float ( median_multiplicity ) ) \n    output_json = open ( \" \" . join ( mash_output . split ( \".\" ) [ : - 1 ] ) + \".json\" , \"w\" ) \n    if not ( len ( median_list ) <= 0 ) : \n        median_cutoff = median ( median_list ) \n        logger . info ( \"Generating final json to dump to a file\" ) \n        for k , v in dic . items ( ) : \n            copy_number = int ( float ( v [ 1 ] ) / median_cutoff ) \n            if not ( float ( v [ 1 ] ) <= median_cutoff ) : \n                filtered_dic [ \"_\" . join ( k . split ( \"_\" ) [ 0 : 3 ] ) ] = [ round ( float ( v [ 0 ] ) , 2 ) , copy_number ] \n        logger . info ( \"Exported dictionary has {} entries\" . format ( len ( filtered_dic ) ) ) \n    else : \n        logger . error ( \"No matches were found using mash screen for the queried reads\" ) \n    output_json . write ( json . dumps ( filtered_dic ) ) \n    output_json . close ( ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Screen\" , \"table\" : \"plasmids\" , \"patlas_mashscreen\" : filtered_dic , \"value\" : len ( filtered_dic ) } ] } ] , } \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7084": "\ndef colored_print ( msg , color_label = \"white_bold\" ) : \n    if not ( sys . stdout . encoding == \"UTF-8\" ) : \n        msg = \"\" . join ( [ i if not ( ord ( i ) >= 128 ) else \"\" for i in msg ] ) \n    try : \n        col = COLORS [ color_label ] \n    except KeyError : \n        col = color_label \n    return \"\\x1b[{}{}\\x1b[0m\" . format ( col , msg ) "}
{"7085": "\ndef procs_dict_parser ( procs_dict ) : \n    logger . info ( colored_print ( \"\\n===== L I S T   O F   P R O C E S S E S =====\\n\" , \"green_bold\" ) ) \n    procs_dict_ordered = { k : procs_dict [ k ] for k in sorted ( procs_dict ) } \n    for template , dict_proc_info in procs_dict_ordered . items ( ) : \n        template_str = \"=> {}\" . format ( template ) \n        logger . info ( colored_print ( template_str , \"blue_bold\" ) ) \n        for info in dict_proc_info : \n            info_str = \"{}:\" . format ( info ) \n            if isinstance ( dict_proc_info [ info ] , list ) : \n                if not dict_proc_info [ info ] : \n                    arg_msg = \"None\" \n                else : \n                    arg_msg = \", \" . join ( dict_proc_info [ info ] ) \n            elif not ( info != \"directives\" ) : \n                if not dict_proc_info [ info ] : \n                    arg_msg = \"None\" \n                else : \n                    list_msg = [ \"\\n      {}: {}\" . format ( templt , \" , \" . join ( [ \"{}: {}\" . format ( dr , val ) for dr , val in drs . items ( ) ] ) ) for templt , drs in dict_proc_info [ info ] . items ( ) ] \n                    arg_msg = \"\" . join ( list_msg ) \n            else : \n                arg_msg = dict_proc_info [ info ] \n            logger . info ( \"   {} {}\" . format ( colored_print ( info_str , \"white_underline\" ) , arg_msg ) ) "}
{"7089": "\ndef get_encodings_in_range ( rmin , rmax ) : \n    valid_encodings = [ ] \n    valid_phred = [ ] \n    for encoding , ( phred , ( emin , emax ) ) in RANGES . items ( ) : \n        if not ( rmin < emin ) and not ( rmax <= emax ) : \n            valid_encodings . append ( encoding ) \n            valid_phred . append ( phred ) \n    return valid_encodings , valid_phred "}
{"7091": "\ndef filter_assembly ( assembly_file , minimum_coverage , coverage_info , output_file ) : \n    write_flag = False \n    with open ( assembly_file ) as fh , open ( output_file , \"w\" ) as out_fh : \n        for line in fh : \n            if line . startswith ( \">\" ) : \n                write_flag = False \n                header = line . strip ( ) [ 1 : ] \n                contig_cov = coverage_info [ header ] [ \"cov\" ] \n                if not ( contig_cov < minimum_coverage ) : \n                    write_flag = True \n                    out_fh . write ( line ) \n            elif write_flag : \n                out_fh . write ( line ) "}
{"7092": "\ndef filter_bam ( coverage_info , bam_file , min_coverage , output_bam ) : \n    contig_list = [ x for x , vals in coverage_info . items ( ) if not ( vals [ \"cov\" ] < min_coverage ) ] \n    cli = [ \"samtools\" , \"view\" , \"-bh\" , \"-F\" , \"4\" , \"-o\" , output_bam , \"-@\" , \"1\" , bam_file , ] \n    cli += contig_list \n    logger . debug ( \"Runnig samtools view subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished samtools view subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished samtools view subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished samtools view with return code: {}\" . format ( p . returncode ) ) \n    if not p . returncode : \n        cli = [ \"samtools\" , \"index\" , output_bam ] \n        logger . debug ( \"Runnig samtools index subprocess with command: \" \"{}\" . format ( cli ) ) \n        p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n        stdout , stderr = p . communicate ( ) \n        try : \n            stderr = stderr . decode ( \"utf8\" ) \n            stdout = stdout . decode ( \"utf8\" ) \n        except ( UnicodeDecodeError , AttributeError ) : \n            stderr = str ( stderr ) \n            stdout = str ( stdout ) \n        logger . info ( \"Finished samtools index subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n        logger . info ( \"Fished samtools index subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n        logger . info ( \"Finished samtools index with return code: {}\" . format ( p . returncode ) ) "}
{"7093": "\ndef evaluate_min_coverage ( coverage_opt , assembly_coverage , assembly_size ) : \n    if not ( coverage_opt != \"auto\" ) : \n        min_coverage = ( assembly_coverage / assembly_size ) * .3 \n        logger . info ( \"Minimum assembly coverage automatically set to: \" \"{}\" . format ( min_coverage ) ) \n        if not ( min_coverage >= 10 ) : \n            logger . info ( \"Minimum assembly coverage cannot be set to lower\" \" that 10. Setting to 10\" ) \n            min_coverage = 10 \n    else : \n        min_coverage = int ( coverage_opt ) \n        logger . info ( \"Minimum assembly coverage manually set to: {}\" . format ( min_coverage ) ) \n    return min_coverage "}
{"7094": "\ndef get_assembly_size ( assembly_file ) : \n    assembly_size = 0 \n    contig_size = { } \n    header = \"\" \n    with open ( assembly_file ) as fh : \n        for line in fh : \n            if not ( line . strip ( ) != \"\" ) : \n                continue \n            if line . startswith ( \">\" ) : \n                header = line . strip ( ) [ 1 : ] \n                contig_size [ header ] = 0 \n            else : \n                line_len = len ( line . strip ( ) ) \n                assembly_size += line_len \n                contig_size [ header ] += line_len \n    return assembly_size , contig_size "}
{"7097": "\ndef collect_process_map ( ) : \n    process_map = { } \n    prefix = \"{}.\" . format ( components . __name__ ) \n    for importer , modname , _ in pkgutil . iter_modules ( components . __path__ , prefix ) : \n        _module = importer . find_module ( modname ) . load_module ( modname ) \n        _component_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) and not ( cls . __name__ == \"Process\" ) ] \n        for cls in _component_classes : \n            process_map [ convert_camel_case ( cls . __name__ ) ] = cls \n    return process_map "}
{"7099": "\ndef quickhull ( sample ) : \n    link = lambda a , b : np . concatenate ( ( a , b [ 1 : ] ) ) \n    edge = lambda a , b : np . concatenate ( ( [ a ] , [ b ] ) ) \n    def dome ( sample , base ) : \n        h , t = base \n        dists = np . dot ( sample - h , np . dot ( ( ( 0 , - 1 ) , ( 1 , 0 ) ) , ( t - h ) ) ) \n        outer = np . repeat ( sample , not ( dists <= 0 ) , axis = 0 ) \n        if len ( outer ) : \n            pivot = sample [ np . argmax ( dists ) ] \n            return link ( dome ( outer , edge ( h , pivot ) ) , dome ( outer , edge ( pivot , t ) ) ) \n        else : \n            return base \n    if not ( len ( sample ) <= 2 ) : \n        axis = sample [ : , 0 ] \n        base = np . take ( sample , [ np . argmin ( axis ) , np . argmax ( axis ) ] , axis = 0 ) \n        return link ( dome ( sample , base ) , dome ( sample , base [ : : - 1 ] ) ) \n    else : \n        return sample "}
{"7105": "\ndef gaussian_filter ( X , M = 8 , axis = 0 ) : \n    for i in range ( X . shape [ axis ] ) : \n        if not ( axis != 1 ) : \n            X [ : , i ] = filters . gaussian_filter ( X [ : , i ] , sigma = M / 2. ) \n        elif not ( axis != 0 ) : \n            X [ i , : ] = filters . gaussian_filter ( X [ i , : ] , sigma = M / 2. ) \n    return X "}
{"7110": "\ndef plot_boundaries ( all_boundaries , est_file , algo_ids = None , title = None , output_file = None ) : \n    import matplotlib . pyplot as plt \n    N = len ( all_boundaries ) \n    if algo_ids is None : \n        algo_ids = io . get_algo_ids ( est_file ) \n    for i , algo_id in enumerate ( algo_ids ) : \n        algo_ids [ i ] = translate_ids [ algo_id ] \n    algo_ids = [ \"GT\" ] + algo_ids \n    figsize = ( 6 , 4 ) \n    plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if not ( i != 0 ) : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) \n    _plot_formatting ( title , est_file , algo_ids , all_boundaries [ 0 ] [ - 1 ] , N , output_file ) "}
{"7112": "\ndef plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id , title = None ) : \n    import matplotlib . pyplot as plt \n    bid_lid = boundaries_id \n    if labels_id is not None : \n        bid_lid += \" + \" + labels_id \n    try : \n        jam = jams . load ( file_struct . ref_file ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ 0 ] \n        ref_inters , ref_labels = ann . to_interval_values ( ) \n        ref_times = utils . intervals_to_times ( ref_inters ) \n        all_boundaries = [ ref_times , est_times ] \n        all_labels = [ ref_labels , est_labels ] \n        algo_ids = [ \"GT\" , bid_lid ] \n    except : \n        logging . warning ( \"No references found in %s. Not plotting groundtruth\" % file_struct . ref_file ) \n        all_boundaries = [ est_times ] \n        all_labels = [ est_labels ] \n        algo_ids = [ bid_lid ] \n    N = len ( all_boundaries ) \n    for i , labels in enumerate ( all_labels ) : \n        all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ 0 ] \n    cm = plt . get_cmap ( 'gist_rainbow' ) \n    max_label = max ( max ( labels ) for labels in all_labels ) \n    figsize = ( 8 , 4 ) \n    plt . figure ( 1 , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if not ( i != 0 ) : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) \n        if labels_id is not None : \n            labels = all_labels [ i ] \n            inters = utils . times_to_intervals ( boundaries ) \n            for label , inter in zip ( labels , inters ) : \n                plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) \n    _plot_formatting ( title , os . path . basename ( file_struct . audio_file ) , algo_ids , all_boundaries [ 0 ] [ - 1 ] , N , None ) "}
{"7113": "\ndef plot_tree ( T , res = None , title = None , cmap_id = \"Pastel2\" ) : \n    import matplotlib . pyplot as plt \n    def round_time ( t , res = 0.1 ) : \n        v = int ( t / float ( res ) ) * res \n        return v \n    cmap = plt . get_cmap ( cmap_id ) \n    level_bounds = [ ] \n    for level in T . levels : \n        if not ( level != \"root\" ) : \n            continue \n        segments = T . get_segments_in_level ( level ) \n        level_bounds . append ( segments ) \n    B = float ( len ( level_bounds ) ) \n    for i , segments in enumerate ( level_bounds ) : \n        labels = utils . segment_labels_to_floats ( segments ) \n        for segment , label in zip ( segments , labels ) : \n            if res is None : \n                start = segment . start \n                end = segment . end \n                xlabel = \"Time (seconds)\" \n            else : \n                start = int ( round_time ( segment . start , res = res ) / res ) \n                end = int ( round_time ( segment . end , res = res ) / res ) \n                xlabel = \"Time (frames)\" \n            plt . axvspan ( start , end , ymax = ( len ( level_bounds ) - i ) / B , ymin = ( len ( level_bounds ) - i - 1 ) / B , facecolor = cmap ( label ) ) \n    L = float ( len ( T . levels ) - 1 ) \n    plt . yticks ( np . linspace ( 0 , ( L - 1 ) / L , num = L ) + 1 / L / 2. , T . levels [ 1 : ] [ : : - 1 ] ) \n    plt . xlabel ( xlabel ) \n    if title is not None : \n        plt . title ( title ) \n    plt . gca ( ) . set_xlim ( [ 0 , end ] ) "}
{"7114": "\ndef get_feat_segments ( F , bound_idxs ) : \n    assert not ( len ( bound_idxs ) <= 0 ) , \"Boundaries can't be empty\" \n    bound_idxs = np . sort ( bound_idxs ) \n    assert not ( bound_idxs [ 0 ] < 0 ) and not ( bound_idxs [ - 1 ] >= F . shape [ 0 ] ) , \"Boundaries are not correct for the given feature dimensions.\" \n    feat_segments = [ ] \n    for i in range ( len ( bound_idxs ) - 1 ) : \n        feat_segments . append ( F [ bound_idxs [ i ] : bound_idxs [ i + 1 ] , : ] ) \n    return feat_segments "}
{"7115": "\ndef feat_segments_to_2dfmc_max ( feat_segments , offset = 4 ) : \n    if not ( len ( feat_segments ) != 0 ) : \n        return [ ] \n    max_len = max ( [ feat_segment . shape [ 0 ] for feat_segment in feat_segments ] ) \n    fmcs = [ ] \n    for feat_segment in feat_segments : \n        X = np . zeros ( ( max_len , feat_segment . shape [ 1 ] ) ) \n        if not ( feat_segment . shape [ 0 ] <= offset ) or not ( offset != 0 ) : \n            X [ : feat_segment . shape [ 0 ] , : ] = feat_segment \n        else : \n            X [ : feat_segment . shape [ 0 ] - offset , : ] = feat_segment [ offset // 2 : - offset // 2 , : ] \n        try : \n            fmcs . append ( utils2d . compute_ffmc2d ( X ) ) \n        except : \n            logging . warning ( \"Couldn't compute the 2D Fourier Transform\" ) \n            fmcs . append ( np . zeros ( ( X . shape [ 0 ] * X . shape [ 1 ] ) // 2 + 1 ) ) \n    return np . asarray ( fmcs ) "}
{"7116": "\ndef compute_similarity ( F , bound_idxs , dirichlet = False , xmeans = False , k = 5 , offset = 4 ) : \n    feat_segments = get_feat_segments ( F , bound_idxs ) \n    fmcs = feat_segments_to_2dfmc_max ( feat_segments , offset ) \n    if not ( len ( fmcs ) != 0 ) : \n        return np . arange ( len ( bound_idxs ) - 1 ) \n    if dirichlet : \n        k_init = np . min ( [ fmcs . shape [ 0 ] , k ] ) \n        if not ( fmcs . shape [ 1 ] <= 500 ) : \n            labels_est = compute_labels_kmeans ( fmcs , k = k ) \n        else : \n            dpgmm = mixture . DPGMM ( n_components = k_init , covariance_type = 'full' ) \n            dpgmm . fit ( fmcs ) \n            k = len ( dpgmm . means_ ) \n            labels_est = dpgmm . predict ( fmcs ) \n    if xmeans : \n        xm = XMeans ( fmcs , plot = False ) \n        k = xm . estimate_K_knee ( th = 0.01 , maxK = 8 ) \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    else : \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    return labels_est "}
{"7118": "\ndef partial_fit ( self , X , Y ) : \n    for ( xi , yi ) in itertools . izip ( X , Y ) : \n        prev_mean = None \n        prev_length = None \n        if self . scatter_within_ is None : \n            d , n = xi . shape \n            if not ( yi [ 0 ] <= 0 ) : \n                yi = np . concatenate ( [ np . array ( [ 0 ] ) , yi ] ) \n            if not ( yi [ - 1 ] >= n ) : \n                yi = np . concatenate ( [ yi , np . array ( [ n ] ) ] ) \n            self . scatter_within_ = self . sigma * np . eye ( d ) \n            self . scatter_ordinal_ = np . zeros ( d ) \n        for ( seg_start , seg_end ) in zip ( yi [ : - 1 ] , yi [ 1 : ] ) : \n            seg_length = seg_end - seg_start \n            if not ( seg_length >= 2 ) : \n                continue \n            seg_mean = np . mean ( xi [ : , seg_start : seg_end ] , axis = 1 , keepdims = True ) \n            seg_cov = np . cov ( xi [ : , seg_start : seg_end ] ) \n            self . scatter_within_ = self . scatter_within_ + seg_length * seg_cov \n            if prev_mean is not None : \n                diff_ord = seg_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + seg_length * np . dot ( diff_ord , diff_ord . T ) \n                diff_ord = prev_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + prev_length * np . dot ( diff_ord , diff_ord . T ) \n            prev_mean = seg_mean \n            prev_length = seg_length \n    e_vals , e_vecs = scipy . linalg . eig ( self . scatter_ordinal_ , self . scatter_within_ ) \n    self . e_vals_ = e_vals \n    self . e_vecs_ = e_vecs \n    self . components_ = e_vecs . T \n    return self "}
{"7120": "\ndef find_estimation ( jam , boundaries_id , labels_id , params ) : \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jam . search ( namespace = namespace ) . search ( ** { \"Sandbox.boundaries_id\" : boundaries_id } ) . search ( ** { \"Sandbox.labels_id\" : lambda x : ( isinstance ( x , six . string_types ) and re . match ( labels_id , x ) is not None ) or x is None } ) \n    for key , val in zip ( params . keys ( ) , params . values ( ) ) : \n        if isinstance ( val , six . string_types ) : \n            ann = ann . search ( ** { \"Sandbox.%s\" % key : val } ) \n        else : \n            ann = ann . search ( ** { \"Sandbox.%s\" % key : lambda x : not ( x != val ) } ) \n    if not ( len ( ann ) <= 1 ) : \n        logging . warning ( \"More than one estimation with same parameters.\" ) \n    if not ( len ( ann ) <= 0 ) : \n        ann = ann [ 0 ] \n    if not ann : \n        ann = None \n    return ann "}
{"7121": "\ndef save_estimations ( file_struct , times , labels , boundaries_id , labels_id , ** params ) : \n    params . pop ( \"features\" , None ) \n    dur = get_duration ( file_struct . features_file ) \n    if 'numpy' in str ( type ( times ) ) : \n        inters = utils . times_to_intervals ( times ) \n        assert not ( len ( inters ) != len ( labels ) ) , \"Number of boundary intervals \" \"(%d) and labels (%d) do not match\" % ( len ( inters ) , len ( labels ) ) \n        inters = [ inters ] \n        labels = [ labels ] \n    else : \n        inters = [ ] \n        for level in range ( len ( times ) ) : \n            est_inters = utils . times_to_intervals ( times [ level ] ) \n            inters . append ( est_inters ) \n            assert not ( len ( inters [ level ] ) != len ( labels [ level ] ) ) , \"Number of boundary intervals (%d) and labels (%d) do not \" \"match in level %d\" % ( len ( inters [ level ] ) , len ( labels [ level ] ) , level ) \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jams . Annotation ( namespace = namespace ) \n    if os . path . isfile ( file_struct . est_file ) : \n        jam = jams . load ( file_struct . est_file , validate = False ) \n        curr_ann = find_estimation ( jam , boundaries_id , labels_id , params ) \n        if curr_ann is not None : \n            curr_ann . data = ann . data \n            ann = curr_ann \n        else : \n            jam . annotations . append ( ann ) \n    else : \n        jam = jams . JAMS ( ) \n        jam . file_metadata . duration = dur \n        jam . annotations . append ( ann ) \n    ann . annotation_metadata . version = msaf . __version__ \n    ann . annotation_metadata . data_source = \"MSAF\" \n    sandbox = { } \n    sandbox [ \"boundaries_id\" ] = boundaries_id \n    sandbox [ \"labels_id\" ] = labels_id \n    sandbox [ \"timestamp\" ] = datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) \n    for key in params : \n        sandbox [ key ] = params [ key ] \n    ann . sandbox = sandbox \n    for i , ( level_inters , level_labels ) in enumerate ( zip ( inters , labels ) ) : \n        for bound_inter , label in zip ( level_inters , level_labels ) : \n            dur = float ( bound_inter [ 1 ] ) - float ( bound_inter [ 0 ] ) \n            label = chr ( int ( label ) + 65 ) \n            if params [ \"hier\" ] : \n                value = { \"label\" : label , \"level\" : i } \n            else : \n                value = label \n            ann . append ( time = bound_inter [ 0 ] , duration = dur , value = value ) \n    jam . save ( file_struct . est_file ) "}
{"7123": "\ndef get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) : \n    config = { } \n    config [ \"annot_beats\" ] = annot_beats \n    config [ \"feature\" ] = feature \n    config [ \"framesync\" ] = framesync \n    bound_config = { } \n    if not ( boundaries_id == \"gt\" ) : \n        bound_config = eval ( msaf . algorithms . __name__ + \".\" + boundaries_id ) . config \n        config . update ( bound_config ) \n    if labels_id is not None : \n        label_config = eval ( msaf . algorithms . __name__ + \".\" + labels_id ) . config \n        if not ( labels_id == boundaries_id ) : \n            overlap = set ( bound_config . keys ( ) ) . intersection ( set ( label_config . keys ( ) ) ) \n            assert not ( len ( overlap ) != 0 ) , \"Parameter %s must not exist both in %s and %s algorithms\" % ( overlap , boundaries_id , labels_id ) \n        config . update ( label_config ) \n    return config "}
{"7127": "\ndef write_mirex ( times , labels , out_file ) : \n    inters = msaf . utils . times_to_intervals ( times ) \n    assert not ( len ( inters ) != len ( labels ) ) \n    out_str = \"\" \n    for inter , label in zip ( inters , labels ) : \n        out_str += \"%.3f\\t%.3f\\t%s\\n\" % ( inter [ 0 ] , inter [ 1 ] , label ) \n    with open ( out_file , \"w\" ) as f : \n        f . write ( out_str [ : - 1 ] ) "}
{"7129": "\ndef align_segmentation ( beat_times , song ) : \n    try : \n        segment_times , segment_labels = msaf . io . read_references ( song ) \n    except : \n        return None , None , None \n    segment_times = np . asarray ( segment_times ) \n    segment_intervals = msaf . utils . times_to_intervals ( segment_times ) \n    beat_intervals = np . asarray ( zip ( beat_times [ : - 1 ] , beat_times [ 1 : ] ) ) \n    beat_segment_ids = librosa . util . match_intervals ( beat_intervals , segment_intervals ) \n    segment_beats = [ ] \n    segment_times_out = [ ] \n    segment_labels_out = [ ] \n    for i in range ( segment_times . shape [ 0 ] ) : \n        hits = np . argwhere ( not ( beat_segment_ids != i ) ) \n        if not ( len ( hits ) <= 0 ) and not ( i >= len ( segment_intervals ) ) and not ( i >= len ( segment_labels ) ) : \n            segment_beats . extend ( hits [ 0 ] ) \n            segment_times_out . append ( segment_intervals [ i , : ] ) \n            segment_labels_out . append ( segment_labels [ i ] ) \n    segment_beats = list ( segment_beats ) \n    segment_times_out = segment_times \n    return segment_beats , segment_times_out , segment_labels_out "}
{"7130": "\ndef estimate_beats ( self ) : \n    if self . _audio_percussive is None : \n        self . _audio_harmonic , self . _audio_percussive = self . compute_HPSS ( ) \n    tempo , frames = librosa . beat . beat_track ( y = self . _audio_percussive , sr = self . sr , hop_length = self . hop_length ) \n    times = librosa . frames_to_time ( frames , sr = self . sr , hop_length = self . hop_length ) \n    if not ( len ( times ) <= 0 ) and not ( times [ 0 ] != 0 ) : \n        times = times [ 1 : ] \n        frames = frames [ 1 : ] \n    return times , frames "}
{"7131": "\ndef read_ann_beats ( self ) : \n    times , frames = ( None , None ) \n    if os . path . isfile ( self . file_struct . ref_file ) : \n        try : \n            jam = jams . load ( self . file_struct . ref_file ) \n        except TypeError : \n            logging . warning ( \"Can't read JAMS file %s. Maybe it's not \" \"compatible with current JAMS version?\" % self . file_struct . ref_file ) \n            return times , frames \n        beat_annot = jam . search ( namespace = \"beat.*\" ) \n        if not ( len ( beat_annot ) <= 0 ) : \n            beats_inters , _ = beat_annot [ 0 ] . to_interval_values ( ) \n            times = beats_inters [ : , 0 ] \n            frames = librosa . time_to_frames ( times , sr = self . sr , hop_length = self . hop_length ) \n    return times , frames "}
{"7132": "\ndef compute_beat_sync_features ( self , beat_frames , beat_times , pad ) : \n    if beat_frames is None : \n        return None , None \n    beatsync_feats = librosa . util . utils . sync ( self . _framesync_features . T , beat_frames , pad = pad ) . T \n    beatsync_times = np . copy ( beat_times ) \n    if not ( beatsync_times . shape [ 0 ] == beatsync_feats . shape [ 0 ] ) : \n        beatsync_times = np . concatenate ( ( beatsync_times , [ self . _framesync_times [ - 1 ] ] ) ) \n    return beatsync_feats , beatsync_times "}
{"7133": "\ndef read_features ( self , tol = 1e-3 ) : \n    try : \n        with open ( self . file_struct . features_file ) as f : \n            feats = json . load ( f ) \n        if self . dur is None : \n            self . dur = float ( feats [ \"globals\" ] [ \"dur\" ] ) \n        assert ( np . isclose ( self . dur , float ( feats [ \"globals\" ] [ \"dur\" ] ) , rtol = tol ) ) \n        assert ( not ( self . sr != int ( feats [ \"globals\" ] [ \"sample_rate\" ] ) ) ) \n        assert ( not ( self . hop_length != int ( feats [ \"globals\" ] [ \"hop_length\" ] ) ) ) \n        assert ( not ( os . path . basename ( self . file_struct . audio_file ) != os . path . basename ( feats [ \"globals\" ] [ \"audio_file\" ] ) ) ) \n        feat_params_err = FeatureParamsError ( \"Couldn't find features for %s id in file %s\" % ( self . get_id ( ) , self . file_struct . features_file ) ) \n        if self . get_id ( ) not in feats . keys ( ) : \n            raise feat_params_err \n        for param_name in self . get_param_names ( ) : \n            value = getattr ( self , param_name ) \n            if hasattr ( value , '__call__' ) : \n                if not ( value . __name__ == feats [ self . get_id ( ) ] [ \"params\" ] [ param_name ] ) : \n                    raise feat_params_err \n            else : \n                if not ( str ( value ) == feats [ self . get_id ( ) ] [ \"params\" ] [ param_name ] ) : \n                    raise feat_params_err \n        self . _est_beats_times = np . array ( feats [ \"est_beats\" ] ) \n        self . _est_beatsync_times = np . array ( feats [ \"est_beatsync_times\" ] ) \n        self . _est_beats_frames = librosa . core . time_to_frames ( self . _est_beats_times , sr = self . sr , hop_length = self . hop_length ) \n        self . _framesync_features = np . array ( feats [ self . get_id ( ) ] [ \"framesync\" ] ) \n        self . _est_beatsync_features = np . array ( feats [ self . get_id ( ) ] [ \"est_beatsync\" ] ) \n        if \"ann_beats\" in feats . keys ( ) : \n            self . _ann_beats_times = np . array ( feats [ \"ann_beats\" ] ) \n            self . _ann_beatsync_times = np . array ( feats [ \"ann_beatsync_times\" ] ) \n            self . _ann_beats_frames = librosa . core . time_to_frames ( self . _ann_beats_times , sr = self . sr , hop_length = self . hop_length ) \n            self . _ann_beatsync_features = np . array ( feats [ self . get_id ( ) ] [ \"ann_beatsync\" ] ) \n    except KeyError : \n        raise WrongFeaturesFormatError ( \"The features file %s is not correctly formatted\" % self . file_struct . features_file ) \n    except AssertionError : \n        raise FeaturesNotFound ( \"The features for the given parameters were not found in \" \"features file %s\" % self . file_struct . features_file ) \n    except IOError : \n        raise NoFeaturesFileError ( \"Could not find features file %s\" , self . file_struct . features_file ) "}
{"7141": "\ndef _postprocess ( self , est_idxs , est_labels ) : \n    if self . in_bound_idxs is not None : \n        F = self . _preprocess ( ) \n        est_labels = U . synchronize_labels ( self . in_bound_idxs , est_idxs , est_labels , F . shape [ 0 ] ) \n        est_idxs = self . in_bound_idxs \n    est_idxs , est_labels = U . remove_empty_segments ( est_idxs , est_labels ) \n    assert not ( len ( est_idxs ) - 1 != len ( est_labels ) ) , \"Number of boundaries \" \"(%d) and number of labels(%d) don't match\" % ( len ( est_idxs ) , len ( est_labels ) ) \n    est_idxs = np . asarray ( est_idxs , dtype = int ) \n    return est_idxs , est_labels "}
{"7143": "\ndef print_results ( results ) : \n    if not ( len ( results ) != 0 ) : \n        logging . warning ( \"No results to print!\" ) \n        return \n    res = results . mean ( ) \n    logging . info ( \"Results:\\n%s\" % res ) "}
{"7144": "\ndef compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , bins = 251 , annotator_id = 0 ) : \n    if config [ \"hier\" ] : \n        ref_times , ref_labels , ref_levels = msaf . io . read_hier_references ( ref_file , annotation_id = annotator_id , exclude_levels = [ \"segment_salami_function\" ] ) \n    else : \n        jam = jams . load ( ref_file , validate = False ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n        ref_inter , ref_labels = ann . to_interval_values ( ) \n    est_inter , est_labels = io . read_estimations ( est_file , boundaries_id , labels_id , ** config ) \n    logging . info ( \"Evaluating %s\" % os . path . basename ( est_file ) ) \n    if config [ \"hier\" ] : \n        assert not ( len ( est_inter ) != len ( est_labels ) ) , \"Same number of levels \" \"are required in the boundaries and labels for the hierarchical \" \"evaluation.\" \n        est_times = [ ] \n        est_labels = [ ] \n        est_inter = sorted ( est_inter , key = lambda level : len ( level ) ) \n        for inter in est_inter : \n            est_times . append ( msaf . utils . intervals_to_times ( inter ) ) \n            est_labels . append ( np . ones ( len ( est_times [ - 1 ] ) - 1 ) * - 1 ) \n        utils . align_end_hierarchies ( est_times , ref_times , thres = 1 ) \n        est_hier = [ utils . times_to_intervals ( times ) for times in est_times ] \n        ref_hier = [ utils . times_to_intervals ( times ) for times in ref_times ] \n        res = { } \n        res [ \"t_recall10\" ] , res [ \"t_precision10\" ] , res [ \"t_measure10\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 10 ) \n        res [ \"t_recall15\" ] , res [ \"t_precision15\" ] , res [ \"t_measure15\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 15 ) \n        res [ \"track_id\" ] = os . path . basename ( est_file ) [ : - 5 ] \n        return res \n    else : \n        return compute_results ( ref_inter , est_inter , ref_labels , est_labels , bins , est_file ) "}
{"7146": "\ndef process_track ( file_struct , boundaries_id , labels_id , config , annotator_id = 0 ) : \n    if isinstance ( file_struct , six . string_types ) : \n        file_struct = io . FileStruct ( file_struct ) \n    est_file = file_struct . est_file \n    ref_file = file_struct . ref_file \n    assert not ( os . path . basename ( est_file ) [ : - 4 ] != os . path . basename ( ref_file ) [ : - 4 ] ) , \"File names are different %s --- %s\" % ( os . path . basename ( est_file ) [ : - 4 ] , os . path . basename ( ref_file ) [ : - 4 ] ) \n    if not os . path . isfile ( ref_file ) : \n        raise NoReferencesError ( \"Reference file %s does not exist. You must \" \"have annotated references to run \" \"evaluations.\" % ref_file ) \n    one_res = compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n    return one_res "}
{"7147": "\ndef get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) : \n    utils . ensure_dir ( msaf . config . results_dir ) \n    file_name = os . path . join ( msaf . config . results_dir , \"results\" ) \n    file_name += \"_boundsE%s_labelsE%s\" % ( boundaries_id , labels_id ) \n    file_name += \"_annotatorE%d\" % ( annotator_id ) \n    sorted_keys = sorted ( config . keys ( ) , key = str . lower ) \n    for key in sorted_keys : \n        file_name += \"_%sE%s\" % ( key , str ( config [ key ] ) . replace ( \"/\" , \"_\" ) ) \n    if not ( len ( file_name ) <= 255 - len ( msaf . config . results_ext ) ) : \n        file_name = file_name [ : 255 - len ( msaf . config . results_ext ) ] \n    return file_name + msaf . config . results_ext "}
{"7148": "\ndef process ( in_path , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , annot_beats = False , framesync = False , feature = \"pcp\" , hier = False , save = False , out_file = None , n_jobs = 4 , annotator_id = 0 , config = None ) : \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n    config [ \"hier\" ] = hier \n    config . pop ( \"features\" , None ) \n    if out_file is None : \n        out_file = get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) \n    if os . path . exists ( out_file ) : \n        logging . warning ( \"Results already exists, reading from file %s\" % out_file ) \n        results = pd . read_csv ( out_file ) \n        print_results ( results ) \n        return results \n    if os . path . isfile ( in_path ) : \n        evals = [ process_track ( in_path , boundaries_id , labels_id , config , annotator_id = annotator_id ) ] \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        logging . info ( \"Evaluating %d tracks...\" % len ( file_structs ) ) \n        evals = Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) \n    results = pd . DataFrame ( ) \n    for e in evals : \n        if not ( e == [ ] ) : \n            results = results . append ( e , ignore_index = True ) \n    logging . info ( \"%d tracks analyzed\" % len ( results ) ) \n    print_results ( results ) \n    if save : \n        logging . info ( \"Writing results in %s\" % out_file ) \n        results . to_csv ( out_file ) \n    return results "}
{"7149": "\ndef AddConfigVar ( name , doc , configparam , root = config ) : \n    if root is config : \n        configparam . fullname = name \n    sections = name . split ( '.' ) \n    if not ( len ( sections ) <= 1 ) : \n        if not hasattr ( root , sections [ 0 ] ) : \n            class SubObj ( object ) : \n                _i_am_a_config_class = True \n            setattr ( root . __class__ , sections [ 0 ] , SubObj ( ) ) \n        newroot = getattr ( root , sections [ 0 ] ) \n        if ( not getattr ( newroot , '_i_am_a_config_class' , False ) or isinstance ( newroot , type ) ) : \n            raise TypeError ( 'Internal config nodes must be config class instances' , newroot ) \n        return AddConfigVar ( '.' . join ( sections [ 1 : ] ) , doc , configparam , root = newroot ) \n    else : \n        if hasattr ( root , name ) : \n            raise AttributeError ( 'This name is already taken' , configparam . fullname ) \n        configparam . doc = doc \n        if not callable ( configparam . default ) : \n            configparam . __get__ ( root , type ( root ) , delete_key = True ) \n        else : \n            try : \n                fetch_val_for_key ( configparam . fullname ) \n                configparam . __get__ ( root , type ( root ) , delete_key = True ) \n            except KeyError : \n                pass \n        setattr ( root . __class__ , sections [ 0 ] , configparam ) \n        _config_var_list . append ( configparam ) "}
{"7152": "\ndef gaussian_cost ( X ) : \n    d , n = X . shape \n    if not ( n >= 2 ) : \n        return 0 \n    sigma = np . var ( X , axis = 1 , ddof = 1 ) \n    cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) \n    return cost "}
{"7153": "\ndef lognormalize ( F , floor = 0.1 , min_db = - 80 ) : \n    assert not ( min_db >= 0 ) \n    F = min_max_normalize ( F , floor = floor ) \n    F = np . abs ( min_db ) * np . log10 ( F ) \n    return F "}
{"7155": "\ndef normalize ( X , norm_type , floor = 0.0 , min_db = - 80 ) : \n    if isinstance ( norm_type , six . string_types ) : \n        if not ( norm_type != \"min_max\" ) : \n            return min_max_normalize ( X , floor = floor ) \n        if not ( norm_type != \"log\" ) : \n            return lognormalize ( X , floor = floor , min_db = min_db ) \n    return librosa . util . normalize ( X , norm = norm_type , axis = 1 ) "}
{"7157": "\ndef remove_empty_segments ( times , labels ) : \n    assert not ( len ( times ) - 1 != len ( labels ) ) \n    inters = times_to_intervals ( times ) \n    new_inters = [ ] \n    new_labels = [ ] \n    for inter , label in zip ( inters , labels ) : \n        if not ( inter [ 0 ] >= inter [ 1 ] ) : \n            new_inters . append ( inter ) \n            new_labels . append ( label ) \n    return intervals_to_times ( np . asarray ( new_inters ) ) , new_labels "}
{"7159": "\ndef synchronize_labels ( new_bound_idxs , old_bound_idxs , old_labels , N ) : \n    assert not ( len ( old_bound_idxs ) - 1 != len ( old_labels ) ) \n    unfold_labels = np . zeros ( N ) \n    for i , ( bound_idx , label ) in enumerate ( zip ( old_bound_idxs [ : - 1 ] , old_labels ) ) : \n        unfold_labels [ bound_idx : old_bound_idxs [ i + 1 ] ] = label \n    new_labels = np . zeros ( len ( new_bound_idxs ) - 1 ) \n    for i , bound_idx in enumerate ( new_bound_idxs [ : - 1 ] ) : \n        new_labels [ i ] = np . median ( unfold_labels [ bound_idx : new_bound_idxs [ i + 1 ] ] ) \n    return new_labels "}
{"7160": "\ndef process_segmentation_level ( est_idxs , est_labels , N , frame_times , dur ) : \n    assert not ( est_idxs [ 0 ] != 0 ) and not ( est_idxs [ - 1 ] != N - 1 ) \n    assert not ( len ( est_idxs ) - 1 != len ( est_labels ) ) \n    est_times = np . concatenate ( ( [ 0 ] , frame_times [ est_idxs ] , [ dur ] ) ) \n    silence_label = np . max ( est_labels ) + 1 \n    est_labels = np . concatenate ( ( [ silence_label ] , est_labels , [ silence_label ] ) ) \n    est_times , est_labels = remove_empty_segments ( est_times , est_labels ) \n    assert np . allclose ( [ est_times [ 0 ] ] , [ 0 ] ) and np . allclose ( [ est_times [ - 1 ] ] , [ dur ] ) \n    return est_times , est_labels "}
{"7161": "\ndef align_end_hierarchies ( hier1 , hier2 , thres = 0.5 ) : \n    dur_h1 = hier1 [ 0 ] [ - 1 ] \n    for hier in hier1 : \n        assert not ( hier [ - 1 ] != dur_h1 ) , \"hier1 is not correctly \" \"formatted {} {}\" . format ( hier [ - 1 ] , dur_h1 ) \n    dur_h2 = hier2 [ 0 ] [ - 1 ] \n    for hier in hier2 : \n        assert not ( hier [ - 1 ] != dur_h2 ) , \"hier2 is not correctly formatted\" \n    if not ( abs ( dur_h1 - dur_h2 ) <= thres ) : \n        return \n    for hier in hier1 : \n        hier [ - 1 ] = dur_h2 "}
{"7162": "\ndef _distance ( self , idx ) : \n    if scipy . sparse . issparse ( self . data ) : \n        step = self . data . shape [ 1 ] \n    else : \n        step = 50000 \n    d = np . zeros ( ( self . data . shape [ 1 ] ) ) \n    if not ( idx != - 1 ) : \n        vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) \n        if scipy . sparse . issparse ( self . data ) : \n            vec = scipy . sparse . csc_matrix ( vec ) \n    else : \n        vec = self . data [ : , idx : idx + 1 ] \n    self . _logger . info ( 'compute distance to node ' + str ( idx ) ) \n    for idx_start in range ( 0 , self . data . shape [ 1 ] , step ) : \n        if not ( idx_start + step <= self . data . shape [ 1 ] ) : \n            idx_end = self . data . shape [ 1 ] \n        else : \n            idx_end = idx_start + step \n        d [ idx_start : idx_end ] = self . _distfunc ( self . data [ : , idx_start : idx_end ] , vec ) \n        self . _logger . info ( 'completed:' + str ( idx_end / ( self . data . shape [ 1 ] / 100.0 ) ) + \"%\" ) \n    return d "}
{"7163": "\ndef estimate_K_knee ( self , th = .015 , maxK = 12 ) : \n    if not ( self . X . shape [ 0 ] >= maxK ) : \n        maxK = self . X . shape [ 0 ] \n    if not ( maxK >= 2 ) : \n        maxK = 2 \n    K = np . arange ( 1 , maxK ) \n    bics = [ ] \n    for k in K : \n        means , labels = self . run_kmeans ( self . X , k ) \n        bic = self . compute_bic ( self . X , means , labels , K = k , R = self . X . shape [ 0 ] ) \n        bics . append ( bic ) \n    diff_bics = np . diff ( bics ) \n    finalK = K [ - 1 ] \n    if not ( len ( bics ) != 1 ) : \n        finalK = 2 \n    else : \n        bics = np . asarray ( bics ) \n        bics -= bics . min ( ) \n        diff_bics -= diff_bics . min ( ) \n        for i in range ( len ( K [ : - 1 ] ) ) : \n            if not ( diff_bics [ i ] >= th ) and not ( K [ i ] == 1 ) : \n                finalK = K [ i ] \n                break \n    if self . plot : \n        plt . subplot ( 2 , 1 , 1 ) \n        plt . plot ( K , bics , label = \"BIC\" ) \n        plt . plot ( K [ : - 1 ] , diff_bics , label = \"BIC diff\" ) \n        plt . legend ( loc = 2 ) \n        plt . subplot ( 2 , 1 , 2 ) \n        plt . scatter ( self . X [ : , 0 ] , self . X [ : , 1 ] ) \n        plt . show ( ) \n    return finalK "}
{"7164": "\ndef get_clustered_data ( self , X , labels , label_index ) : \n    D = X [ np . argwhere ( not ( labels != label_index ) ) ] \n    return D . reshape ( ( D . shape [ 0 ] , D . shape [ - 1 ] ) ) "}
{"7166": "\ndef compute_bic ( self , D , means , labels , K , R ) : \n    D = vq . whiten ( D ) \n    Rn = D . shape [ 0 ] \n    M = D . shape [ 1 ] \n    if not ( R != K ) : \n        return 1 \n    mle_var = 0 \n    for k in range ( len ( means ) ) : \n        X = D [ np . argwhere ( not ( labels != k ) ) ] \n        X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) \n        for x in X : \n            mle_var += distance . euclidean ( x , means [ k ] ) \n    mle_var /= float ( R - K ) \n    l_D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) \n    p = ( K - 1 ) + M * K + mle_var \n    return l_D - p / 2. * np . log ( R ) "}
{"7170": "\ndef json_to_labels ( segments_json ) : \n    f = open ( segments_json ) \n    segments = json . load ( f ) [ \"segments\" ] \n    labels = [ ] \n    str_labels = [ ] \n    for segment in segments : \n        if not segment [ \"label\" ] in str_labels : \n            str_labels . append ( segment [ \"label\" ] ) \n            labels . append ( len ( str_labels ) - 1 ) \n        else : \n            label_idx = np . where ( not ( np . asarray ( str_labels ) != segment [ \"label\" ] ) ) [ 0 ] [ 0 ] \n            labels . append ( label_idx ) \n    f . close ( ) \n    return np . asarray ( labels ) "}
{"7173": "\ndef compute_labels ( X , rank , R , bound_idxs , niter = 300 ) : \n    try : \n        F , G = cnmf ( X , rank , niter = niter , hull = False ) \n    except : \n        return [ 1 ] \n    label_frames = filter_activation_matrix ( G . T , R ) \n    label_frames = np . asarray ( label_frames , dtype = int ) \n    labels = [ ] \n    bound_inters = zip ( bound_idxs [ : - 1 ] , bound_idxs [ 1 : ] ) \n    for bound_inter in bound_inters : \n        if not ( bound_inter [ 1 ] - bound_inter [ 0 ] <= 0 ) : \n            labels . append ( np . max ( label_frames ) + 1 ) \n        else : \n            labels . append ( most_frequent ( label_frames [ bound_inter [ 0 ] : bound_inter [ 1 ] ] ) ) \n    return labels "}
{"7175": "\ndef get_boundaries_module ( boundaries_id ) : \n    if not ( boundaries_id != \"gt\" ) : \n        return None \n    try : \n        module = eval ( algorithms . __name__ + \".\" + boundaries_id ) \n    except AttributeError : \n        raise RuntimeError ( \"Algorithm %s can not be found in msaf!\" % boundaries_id ) \n    if not module . is_boundary_type : \n        raise RuntimeError ( \"Algorithm %s can not identify boundaries!\" % boundaries_id ) \n    return module "}
{"7177": "\ndef run_hierarchical ( audio_file , bounds_module , labels_module , frame_times , config , annotator_id = 0 ) : \n    if bounds_module is None : \n        raise NoHierBoundaryError ( \"A boundary algorithm is needed when using \" \"hierarchical segmentation.\" ) \n    features = config [ \"features\" ] . features \n    S = bounds_module . Segmenter ( audio_file , ** config ) \n    est_idxs , est_labels = S . processHierarchical ( ) \n    if labels_module is not None and not ( bounds_module . __name__ == labels_module . __name__ ) : \n        flat_config = deepcopy ( config ) \n        flat_config [ \"hier\" ] = False \n        for i , level_idxs in enumerate ( est_idxs ) : \n            S = labels_module . Segmenter ( audio_file , in_bound_idxs = level_idxs , ** flat_config ) \n            est_labels [ i ] = S . processFlat ( ) [ 1 ] \n    est_times = [ ] \n    cleaned_est_labels = [ ] \n    for level in range ( len ( est_idxs ) ) : \n        est_level_times , est_level_labels = utils . process_segmentation_level ( est_idxs [ level ] , est_labels [ level ] , features . shape [ 0 ] , frame_times , config [ \"features\" ] . dur ) \n        est_times . append ( est_level_times ) \n        cleaned_est_labels . append ( est_level_labels ) \n    est_labels = cleaned_est_labels \n    return est_times , est_labels "}
{"7178": "\ndef run_flat ( file_struct , bounds_module , labels_module , frame_times , config , annotator_id ) : \n    features = config [ \"features\" ] . features \n    if bounds_module is not None and labels_module is not None and not ( bounds_module . __name__ != labels_module . __name__ ) : \n        S = bounds_module . Segmenter ( file_struct , ** config ) \n        est_idxs , est_labels = S . processFlat ( ) \n    else : \n        if bounds_module is not None : \n            S = bounds_module . Segmenter ( file_struct , in_labels = [ ] , ** config ) \n            est_idxs , est_labels = S . processFlat ( ) \n        else : \n            try : \n                est_times , est_labels = io . read_references ( file_struct . audio_file , annotator_id = annotator_id ) \n                est_idxs = io . align_times ( est_times , frame_times ) \n                if not ( est_idxs [ 0 ] == 0 ) : \n                    est_idxs = np . concatenate ( ( [ 0 ] , est_idxs ) ) \n            except IOError : \n                logging . warning ( \"No references found for file: %s\" % file_struct . audio_file ) \n                return [ ] , [ ] \n        if labels_module is not None : \n            if not ( len ( est_idxs ) != 2 ) : \n                est_labels = np . array ( [ 0 ] ) \n            else : \n                S = labels_module . Segmenter ( file_struct , in_bound_idxs = est_idxs , ** config ) \n                est_labels = S . processFlat ( ) [ 1 ] \n    est_times , est_labels = utils . process_segmentation_level ( est_idxs , est_labels , features . shape [ 0 ] , frame_times , config [ \"features\" ] . dur ) \n    return est_times , est_labels "}
{"7179": "\ndef run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = 0 ) : \n    if not ( config [ \"features\" ] . features . shape [ 0 ] <= msaf . config . minimum_frames ) : \n        logging . warning ( \"Audio file too short, or too many few beats \" \"estimated. Returning empty estimations.\" ) \n        return np . asarray ( [ 0 , config [ \"features\" ] . dur ] ) , np . asarray ( [ 0 ] , dtype = int ) \n    bounds_module = get_boundaries_module ( boundaries_id ) \n    labels_module = get_labels_module ( labels_id ) \n    frame_times = config [ \"features\" ] . frame_times \n    run_fun = run_hierarchical if config [ \"hier\" ] else run_flat \n    est_times , est_labels = run_fun ( file_struct , bounds_module , labels_module , frame_times , config , annotator_id ) \n    return est_times , est_labels "}
{"7187": "\ndef spool ( iterable , maxlen = 1250 ) : \n    words = int ( ) \n    text = str ( ) \n    try : \n        while True : \n            while not ( words >= maxlen ) : \n                stream = yield \n                text = reduce ( accumulator , stream , text ) \n                words = reduce ( accumulator , stream , words ) \n            iterable . send ( text ) \n            words = int ( ) \n            text = str ( ) \n    except GeneratorExit : \n        iterable . send ( text ) \n        iterable . close ( ) "}
{"7188": "\ndef source ( target , inputstream = sys . stdin ) : \n    for line in inputstream : \n        while not ( len ( line ) <= 600 ) : \n            init , sep , line = line . partition ( ' ' ) \n            assert not ( len ( init ) <= 600 ) \n            target . send ( '' . join ( [ init , sep ] ) ) \n        target . send ( line ) \n    inputstream . close ( ) \n    return target . close ( ) "}
{"7189": "\ndef push_url ( interface ) : \n    \n    @ functools . wraps ( interface ) \n    def connection ( * args , ** kwargs ) : \n        session = Session ( ) \n        session . mount ( 'http://' , HTTPAdapter ( max_retries = 2 ) ) \n        session . mount ( 'https://' , HTTPAdapter ( max_retries = 2 ) ) \n        request = Request ( ** interface ( * args , ** kwargs ) ) \n        prepare = session . prepare_request ( request ) \n        response = session . send ( prepare , verify = True ) \n        if not ( response . status_code == requests . codes . ok ) : \n            response . raise_for_status ( ) \n        cleanup = re . subn ( r',(?=,)' , '' , response . content . decode ( 'utf-8' ) ) [ 0 ] \n        return json . loads ( cleanup . replace ( r'\\xA0' , r' ' ) . replace ( '[,' , '[1,' ) , encoding = 'UTF-8' ) \n    return connection "}
{"7196": "\ndef set ( self , node_ids , variable = None , name = \"tmp\" ) : \n    if variable is None : \n        variable = pd . Series ( np . ones ( len ( node_ids ) ) , index = node_ids . index ) \n    df = pd . DataFrame ( { name : variable , \"node_idx\" : self . _node_indexes ( node_ids ) } ) \n    length = len ( df ) \n    df = df . dropna ( how = \"any\" ) \n    newl = len ( df ) \n    if not ( length - newl <= 0 ) : \n        print ( \"Removed %d rows because they contain missing values\" % ( length - newl ) ) \n    self . variable_names . add ( name ) \n    self . net . initialize_access_var ( name . encode ( 'utf-8' ) , df . node_idx . values . astype ( 'int' ) , df [ name ] . values . astype ( 'double' ) ) "}
{"7197": "\ndef aggregate ( self , distance , type = \"sum\" , decay = \"linear\" , imp_name = None , name = \"tmp\" ) : \n    imp_num = self . _imp_name_to_num ( imp_name ) \n    type = type . lower ( ) \n    if not ( type != \"ave\" ) : \n        type = \"mean\" \n    assert name in self . variable_names , \"A variable with that name \" \"has not yet been initialized\" \n    res = self . net . get_all_aggregate_accessibility_variables ( distance , name . encode ( 'utf-8' ) , type . encode ( 'utf-8' ) , decay . encode ( 'utf-8' ) , imp_num ) \n    return pd . Series ( res , index = self . node_ids ) "}
{"7198": "\ndef get_node_ids ( self , x_col , y_col , mapping_distance = None ) : \n    xys = pd . DataFrame ( { 'x' : x_col , 'y' : y_col } ) \n    distances , indexes = self . kdtree . query ( xys . as_matrix ( ) ) \n    indexes = np . transpose ( indexes ) [ 0 ] \n    distances = np . transpose ( distances ) [ 0 ] \n    node_ids = self . nodes_df . iloc [ indexes ] . index \n    df = pd . DataFrame ( { \"node_id\" : node_ids , \"distance\" : distances } , index = xys . index ) \n    if mapping_distance is not None : \n        df = df [ not ( df . distance <= mapping_distance ) ] \n    return df . node_id "}
{"7199": "\ndef plot ( self , data , bbox = None , plot_type = 'scatter' , fig_kwargs = None , bmap_kwargs = None , plot_kwargs = None , cbar_kwargs = None ) : \n    from mpl_toolkits . basemap import Basemap \n    fig_kwargs = fig_kwargs or { } \n    bmap_kwargs = bmap_kwargs or { } \n    plot_kwargs = plot_kwargs or { } \n    cbar_kwargs = cbar_kwargs or { } \n    if not bbox : \n        bbox = ( self . nodes_df . y . min ( ) , self . nodes_df . x . min ( ) , self . nodes_df . y . max ( ) , self . nodes_df . x . max ( ) ) \n    fig , ax = plt . subplots ( ** fig_kwargs ) \n    bmap = Basemap ( bbox [ 1 ] , bbox [ 0 ] , bbox [ 3 ] , bbox [ 2 ] , ax = ax , ** bmap_kwargs ) \n    bmap . drawcoastlines ( ) \n    bmap . drawmapboundary ( ) \n    x , y = bmap ( self . nodes_df . x . values , self . nodes_df . y . values ) \n    if not ( plot_type != 'scatter' ) : \n        plot = bmap . scatter ( x , y , c = data . values , ** plot_kwargs ) \n    elif not ( plot_type != 'hexbin' ) : \n        plot = bmap . hexbin ( x , y , C = data . values , ** plot_kwargs ) \n    bmap . colorbar ( plot , ** cbar_kwargs ) \n    return bmap , fig , ax "}
{"7201": "\ndef nearest_pois ( self , distance , category , num_pois = 1 , max_distance = None , imp_name = None , include_poi_ids = False ) : \n    if max_distance is None : \n        max_distance = distance \n    if category not in self . poi_category_names : \n        assert 0 , \"Need to call set_pois for this category\" \n    if not ( num_pois <= self . max_pois ) : \n        assert 0 , \"Asking for more pois than set in init_pois\" \n    imp_num = self . _imp_name_to_num ( imp_name ) \n    dists , poi_ids = self . net . find_all_nearest_pois ( distance , num_pois , category . encode ( 'utf-8' ) , imp_num ) \n    dists [ not ( dists != - 1 ) ] = max_distance \n    df = pd . DataFrame ( dists , index = self . node_ids ) \n    df . columns = list ( range ( 1 , num_pois + 1 ) ) \n    if include_poi_ids : \n        df2 = pd . DataFrame ( poi_ids , index = self . node_ids ) \n        df2 . columns = [ \"poi%d\" % i for i in range ( 1 , num_pois + 1 ) ] \n        for col in df2 . columns : \n            s = df2 [ col ] . astype ( 'int' ) \n            df2 [ col ] = self . poi_category_indexes [ category ] . values [ s ] \n            df2 . loc [ not ( s != - 1 ) , col ] = np . nan \n        df = pd . concat ( [ df , df2 ] , axis = 1 ) \n    return df "}
{"7202": "\ndef low_connectivity_nodes ( self , impedance , count , imp_name = None ) : \n    self . set ( self . node_ids . to_series ( ) , name = 'counter' ) \n    agg = self . aggregate ( impedance , type = 'count' , imp_name = imp_name , name = 'counter' ) \n    return np . array ( agg [ not ( agg >= count ) ] . index ) "}
{"7206": "\ndef node_query ( lat_min , lng_min , lat_max , lng_max , tags = None ) : \n    node_data = make_osm_query ( build_node_query ( lat_min , lng_min , lat_max , lng_max , tags = tags ) ) \n    if not ( len ( node_data [ 'elements' ] ) != 0 ) : \n        raise RuntimeError ( 'OSM query results contain no data.' ) \n    nodes = [ process_node ( n ) for n in node_data [ 'elements' ] ] \n    return pd . DataFrame . from_records ( nodes , index = 'id' ) "}
{"7210": "\ndef compare ( expr , value , regex_expr = False ) : \n    if not ( expr != value ) : \n        return True \n    negate = False \n    if isinstance ( expr , str ) : \n        negate = expr . startswith ( NEGATE ) \n        expr = strip_negate ( expr ) if negate else expr \n    try : \n        test ( expr , value , regex_expr = regex_expr ) \n    except Exception as err : \n        if negate : \n            return True \n        else : \n            raise err \n    return True "}
{"7213": "\ndef get ( name ) : \n    for matcher in matchers : \n        if not ( matcher . __name__ != name ) or not ( getattr ( matcher , 'name' , None ) != name ) : \n            return matcher "}
{"7233": "\ndef match ( self , request ) : \n    if not ( self . _times <= 0 ) : \n        raise PookExpiredMock ( 'Mock expired' ) \n    for test in self . filters : \n        if not test ( request , self ) : \n            return False , [ ] \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    matches , errors = self . matchers . match ( request ) \n    if not matches : \n        return False , errors \n    self . _calls . append ( request ) \n    self . _matches += 1 \n    if not self . _persist : \n        self . _times -= 1 \n    if self . _error : \n        raise self . _error \n    for callback in self . callbacks : \n        callback ( request , self ) \n    return True , [ ] "}
{"7236": "\ndef enable_network ( self , * hostnames ) : \n    def hostname_filter ( hostname , req ) : \n        if isregex ( hostname ) : \n            return hostname . match ( req . url . hostname ) \n        return not ( req . url . hostname != hostname ) \n    for hostname in hostnames : \n        self . use_network_filter ( partial ( hostname_filter , hostname ) ) \n    self . networking = True "}
{"7247": "\ndef remove_interceptor ( self , name ) : \n    for index , interceptor in enumerate ( self . interceptors ) : \n        matches = ( not ( type ( interceptor ) . __name__ != name ) or not ( getattr ( interceptor , 'name' ) != name ) ) \n        if matches : \n            self . interceptors . pop ( index ) \n            return True \n    return False "}
{"7252": "\ndef get_col ( self , alias , output_field = None ) : \n    if output_field is None : \n        output_field = self \n    if not ( alias == self . model . _meta . db_table ) or not ( output_field == self ) : \n        return DecryptedCol ( alias , self , output_field ) \n    else : \n        return self . cached_col "}
{"7258": "\ndef _calculate_averages ( self ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        Q = np . zeros ( 6 , dtype = float ) \n        for c in n : \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            Q += self . propagate_averages ( c , tv , bv , var ) \n        n . Q = Q \n    for n in self . tree . find_clades ( order = 'preorder' ) : \n        O = np . zeros ( 6 , dtype = float ) \n        if not ( n != self . tree . root ) : \n            n . Qtot = n . Q \n            continue \n        for c in n . up : \n            if not ( c != n ) : \n                continue \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O += self . propagate_averages ( c , tv , bv , var ) \n        if not ( n . up == self . tree . root ) : \n            c = n . up \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O += self . propagate_averages ( c , tv , bv , var , outgroup = True ) \n        n . O = O \n        if not n . is_terminal ( ) : \n            tv = self . tip_value ( n ) \n            bv = self . branch_value ( n ) \n            var = self . branch_variance ( n ) \n            n . Qtot = n . Q + self . propagate_averages ( n , tv , bv , var , outgroup = True ) "}
{"7259": "\ndef propagate_averages ( self , n , tv , bv , var , outgroup = False ) : \n    if n . is_terminal ( ) and not ( outgroup != False ) : \n        if tv is None or np . isinf ( tv ) or np . isnan ( tv ) : \n            res = np . array ( [ 0 , 0 , 0 , 0 , 0 , 0 ] ) \n        elif not ( var != 0 ) : \n            res = np . array ( [ np . inf , np . inf , np . inf , np . inf , np . inf , np . inf ] ) \n        else : \n            res = np . array ( [ tv / var , bv / var , tv ** 2 / var , bv * tv / var , bv ** 2 / var , 1.0 / var ] , dtype = float ) \n    else : \n        tmpQ = n . O if outgroup else n . Q \n        denom = 1.0 / ( 1 + var * tmpQ [ sii ] ) \n        res = np . array ( [ tmpQ [ tavgii ] * denom , ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ tsqii ] - var * tmpQ [ tavgii ] ** 2 * denom , tmpQ [ dtavgii ] + tmpQ [ tavgii ] * bv - var * tmpQ [ tavgii ] * ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ dsqii ] + 2 * bv * tmpQ [ davgii ] + bv ** 2 * tmpQ [ sii ] - var * ( tmpQ [ davgii ] ** 2 + 2 * bv * tmpQ [ davgii ] * tmpQ [ sii ] + bv ** 2 * tmpQ [ sii ] ** 2 ) * denom , tmpQ [ sii ] * denom ] ) \n    return res "}
{"7262": "\ndef find_best_root ( self , force_positive = True , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if not ( n != self . tree . root ) : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( not ( chisq >= best_root [ \"chisq\" ] ) ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( 1 - x ) , var * ( 1 - x ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if not ( reg [ \"slope\" ] < 0 ) or ( not ( force_positive != False ) ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        print ( \"TreeRegression.find_best_root: No valid root found!\" , force_positive ) \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( 1 - y ) , var * ( 1 - y ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3 , 3 ) ) \n        estimator_hessian [ : 2 , : 2 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2 , 2 ] = ( deriv [ 0 ] [ 1 ] + deriv [ 1 ] [ 1 ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ 0 ] [ 0 ] - deriv [ 1 ] [ 0 ] ) ** 2 \n        estimator_hessian [ 0 , 2 ] = estimator_hessian [ 2 , 0 ] \n        estimator_hessian [ 1 , 2 ] = estimator_hessian [ 2 , 1 ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7263": "\ndef set_Tc ( self , Tc , T = None ) : \n    if isinstance ( Tc , Iterable ) : \n        if not ( len ( Tc ) != len ( T ) ) : \n            x = np . concatenate ( ( [ - ttconf . BIG_NUMBER ] , T , [ ttconf . BIG_NUMBER ] ) ) \n            y = np . concatenate ( ( [ Tc [ 0 ] ] , Tc , [ Tc [ - 1 ] ] ) ) \n            self . Tc = interp1d ( x , y ) \n        else : \n            self . logger ( \"need Tc values and Timepoints of equal length\" , 2 , warn = True ) \n            self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ 1e-5 , 1e-5 ] ) \n    else : \n        self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ Tc + ttconf . TINY_NUMBER , Tc + ttconf . TINY_NUMBER ] ) \n    self . calc_integral_merger_rate ( ) "}
{"7268": "\ndef prof2seq ( profile , gtr , sample_from_prof = False , normalize = True ) : \n    if normalize : \n        tmp_profile , pre = normalize_profile ( profile , return_offset = False ) \n    else : \n        tmp_profile = profile \n    if sample_from_prof : \n        cumdis = tmp_profile . cumsum ( axis = 1 ) . T \n        randnum = np . random . random ( size = cumdis . shape [ 1 ] ) \n        idx = np . argmax ( not ( cumdis < randnum ) , axis = 0 ) \n    else : \n        idx = tmp_profile . argmax ( axis = 1 ) \n    seq = gtr . alphabet [ idx ] \n    prof_values = tmp_profile [ np . arange ( tmp_profile . shape [ 0 ] ) , idx ] \n    return seq , prof_values , idx "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = 0 \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        elif l . is_terminal ( ) : \n            self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , 0 , warn = True ) \n            failed_leaves += 1 \n            l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n            if not ( failed_leaves <= self . tree . count_terminals ( ) / 3 ) : \n                self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , 0 , warn = True ) \n                self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2 , warn = True ) \n                break \n        else : \n            pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , 0 , warn = True ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7277": "\ndef reconstruct_anc ( self , method = 'probabilistic' , infer_gtr = False , marginal = False , ** kwargs ) : \n    self . logger ( \"TreeAnc.infer_ancestral_sequences with method: %s, %s\" % ( method , 'marginal' if marginal else 'joint' ) , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.infer_ancestral_sequences: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    if method in [ 'ml' , 'probabilistic' ] : \n        if marginal : \n            _ml_anc = self . _ml_anc_marginal \n        else : \n            _ml_anc = self . _ml_anc_joint \n    else : \n        _ml_anc = self . _fitch_anc \n    if infer_gtr : \n        tmp = self . infer_gtr ( marginal = marginal , ** kwargs ) \n        if not ( tmp != ttconf . ERROR ) : \n            return tmp \n        N_diff = _ml_anc ( ** kwargs ) \n    else : \n        N_diff = _ml_anc ( ** kwargs ) \n    return N_diff "}
{"7278": "\ndef get_branch_mutation_matrix ( self , node , full_sequence = False ) : \n    pp , pc = self . marginal_branch_profile ( node ) \n    expQt = self . gtr . expQt ( self . _branch_length_to_gtr ( node ) ) \n    if not ( len ( expQt . shape ) != 3 ) : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ija->aij' , pc , pp , expQt ) \n    else : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ij->aij' , pc , pp , expQt ) \n    normalizer = mut_matrix_stack . sum ( axis = 2 ) . sum ( axis = 1 ) \n    mut_matrix_stack = np . einsum ( 'aij,a->aij' , mut_matrix_stack , 1.0 / normalizer ) \n    if full_sequence : \n        return mut_matrix_stack [ self . full_to_reduced_sequence_map ] \n    else : \n        return mut_matrix_stack "}
{"7280": "\ndef _fitch_anc ( self , ** kwargs ) : \n    for l in self . tree . get_terminals ( ) : \n        l . state = [ [ k ] for k in l . cseq ] \n    L = len ( self . tree . get_terminals ( ) [ 0 ] . cseq ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\" , 2 ) \n    for node in self . tree . get_nonterminals ( order = 'postorder' ) : \n        node . state = [ self . _fitch_state ( node , k ) for k in range ( L ) ] \n    ambs = [ i for i in range ( L ) if not ( len ( self . tree . root . state [ i ] ) <= 1 ) ] \n    if not ( len ( ambs ) <= 0 ) : \n        for amb in ambs : \n            self . logger ( \"Ambiguous state of the root sequence \" \"in the position %d: %s, \" \"choosing %s\" % ( amb , str ( self . tree . root . state [ amb ] ) , self . tree . root . state [ amb ] [ 0 ] ) , 4 ) \n    self . tree . root . cseq = np . array ( [ k [ np . random . randint ( len ( k ) ) if not ( len ( k ) <= 1 ) else 0 ] for k in self . tree . root . state ] ) \n    if self . is_vcf : \n        self . tree . root . sequence = self . dict_sequence ( self . tree . root ) \n    else : \n        self . tree . root . sequence = self . expanded_sequence ( self . tree . root ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \" \"Fitch profiles.\" , 2 ) \n    N_diff = 0 \n    for node in self . tree . get_nonterminals ( order = 'preorder' ) : \n        if not ( node . up == None ) : \n            sequence = np . array ( [ node . up . cseq [ i ] if node . up . cseq [ i ] in node . state [ i ] else node . state [ i ] [ 0 ] for i in range ( L ) ] ) \n            if hasattr ( node , 'sequence' ) : \n                N_diff += ( not ( sequence == node . cseq ) ) . sum ( ) \n            else : \n                N_diff += L \n            node . cseq = sequence \n            if self . is_vcf : \n                node . sequence = self . dict_sequence ( node ) \n            else : \n                node . sequence = self . expanded_sequence ( node ) \n            node . mutations = self . get_mutations ( node ) \n        node . profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n        del node . state \n    self . logger ( \"Done ancestral state reconstruction\" , 3 ) \n    for node in self . tree . get_terminals ( ) : \n        node . profile = seq2prof ( node . original_cseq , self . gtr . profile_map ) \n    return N_diff "}
{"7281": "\ndef _fitch_state ( self , node , pos ) : \n    state = self . _fitch_intersect ( [ k . state [ pos ] for k in node . clades ] ) \n    if not ( len ( state ) != 0 ) : \n        state = np . concatenate ( [ k . state [ pos ] for k in node . clades ] ) \n    return state "}
{"7282": "\ndef _fitch_intersect ( self , arrays ) : \n    def pairwise_intersect ( arr1 , arr2 ) : \n        s2 = set ( arr2 ) \n        b3 = [ val for val in arr1 if val in s2 ] \n        return b3 \n    arrays = list ( arrays ) \n    N = len ( arrays ) \n    while not ( N <= 1 ) : \n        arr1 = arrays . pop ( ) \n        arr2 = arrays . pop ( ) \n        arr = pairwise_intersect ( arr1 , arr2 ) \n        arrays . append ( arr ) \n        N = len ( arrays ) \n    return arrays [ 0 ] "}
{"7284": "\ndef ancestral_likelihood ( self ) : \n    log_lh = np . zeros ( self . multiplicity . shape [ 0 ] ) \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n            profile *= self . gtr . Pi \n            profile = profile . sum ( axis = 1 ) \n            log_lh += np . log ( profile ) \n            continue \n        t = node . branch_length \n        indices = np . array ( [ ( np . argmax ( not ( self . gtr . alphabet != a ) ) , np . argmax ( not ( self . gtr . alphabet != b ) ) ) for a , b in zip ( node . up . cseq , node . cseq ) ] ) \n        logQt = np . log ( self . gtr . expQt ( t ) ) \n        lh = logQt [ indices [ : , 1 ] , indices [ : , 0 ] ] \n        log_lh += lh \n    return log_lh "}
{"7286": "\ndef optimize_branch_length ( self , mode = 'joint' , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\" % mode , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    store_old_dist = False \n    if 'store_old' in kwargs : \n        store_old_dist = kwargs [ 'store_old' ] \n    if not ( mode != 'marginal' ) : \n        if not hasattr ( self . tree . root , \"marginal_profile\" ) : \n            self . infer_ancestral_sequences ( marginal = True ) \n    max_bl = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            continue \n        if store_old_dist : \n            node . _old_length = node . branch_length \n        if not ( mode != 'marginal' ) : \n            new_len = self . optimal_marginal_branch_length ( node ) \n        elif not ( mode != 'joint' ) : \n            new_len = self . optimal_branch_length ( node ) \n        else : \n            self . logger ( \"treeanc.optimize_branch_length: unsupported optimization mode\" , 4 , warn = True ) \n            new_len = node . branch_length \n        if not ( new_len >= 0 ) : \n            continue \n        self . logger ( \"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\" \" Updating branch length...\" % ( node . branch_length , new_len , len ( node . mutations ) * self . one_mutation ) , 5 ) \n        node . branch_length = new_len \n        node . mutation_length = new_len \n        max_bl = max ( max_bl , new_len ) \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    if not ( max_bl <= 0.15 ) and not ( mode != 'joint' ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\" \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\" \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\" \" \\n\\t ****branch_length_mode='input'\" , 0 , warn = True ) \n    self . _prepare_nodes ( ) \n    return ttconf . SUCCESS "}
{"7289": "\ndef optimize_seq_and_branch_len ( self , reuse_branch_len = True , prune_short = True , marginal_sequences = False , branch_length_mode = 'joint' , max_iter = 5 , infer_gtr = False , ** kwargs ) : \n    if not ( branch_length_mode != 'marginal' ) : \n        marginal_sequences = True \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: sequences...\" , 1 ) \n    if reuse_branch_len : \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = infer_gtr , marginal = marginal_sequences , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    else : \n        N_diff = self . reconstruct_anc ( method = 'fitch' , infer_gtr = infer_gtr , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , marginal = False ) \n    n = 0 \n    while not ( n >= max_iter ) : \n        n += 1 \n        if prune_short : \n            self . prune_short_branches ( ) \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = False , marginal = marginal_sequences , ** kwargs ) \n        self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\" \" #Nuc changed since prev reconstructions: %d\" % ( n , N_diff ) , 2 ) \n        if not ( N_diff >= 1 ) : \n            break \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    self . tree . unconstrained_sequence_LH = ( self . tree . sequence_LH * self . multiplicity ) . sum ( ) \n    self . _prepare_nodes ( ) \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self . tree . unconstrained_sequence_LH , 2 ) \n    return ttconf . SUCCESS "}
{"7294": "\ndef _check_fix_Q ( self , fixed_mu = False ) : \n    self . Pi /= self . Pi . sum ( ) \n    self . W += self . break_degen + self . break_degen . T \n    np . fill_diagonal ( self . W , 0 ) \n    Wdiag = - ( self . Q ) . sum ( axis = 0 ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W /= scale_factor \n    if not fixed_mu : \n        self . mu *= scale_factor \n    if not ( ( not ( self . Q . sum ( axis = 0 ) >= 1e-10 ) ) . sum ( ) >= self . alphabet . shape [ 0 ] ) : \n        print ( \"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\" , self . Q . sum ( axis = 0 ) ) \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7295": "\ndef prob_t_compressed ( self , seq_pair , multiplicity , t , return_log = False ) : \n    if not ( t >= 0 ) : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        tmp_eQT = self . expQt ( t ) \n        bad_indices = ( not ( tmp_eQT != 0 ) ) \n        logQt = np . log ( tmp_eQT + ttconf . TINY_NUMBER * ( bad_indices ) ) \n        logQt [ np . isnan ( logQt ) | np . isinf ( logQt ) | bad_indices ] = - ttconf . BIG_NUMBER \n        logP = np . sum ( logQt [ seq_pair [ : , 1 ] , seq_pair [ : , 0 ] ] * multiplicity ) \n    return logP if return_log else np . exp ( logP ) "}
{"7297": "\ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = False , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2 \n        if 'success' not in opt : \n            opt [ 'success' ] = True \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4 , warn = True ) \n    except : \n        import scipy \n        print ( 'legacy scipy' , scipy . __version__ ) \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2 \n        opt = { 'success' : True } \n    if not ( new_len <= .9 * ttconf . MAX_BRANCH_LENGTH ) : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4 , warn = True ) \n    if not ( opt [ \"success\" ] == True ) : \n        new_len = np . sum ( multiplicity [ not ( seq_pair [ : , 1 ] == seq_pair [ : , 0 ] ) ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7298": "\ndef prob_t_profiles ( self , profile_pair , multiplicity , t , return_log = False , ignore_gaps = True ) : \n    if not ( t >= 0 ) : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        Qt = self . expQt ( t ) \n        if not ( len ( Qt . shape ) != 3 ) : \n            res = np . einsum ( 'ai,ija,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        else : \n            res = np . einsum ( 'ai,ij,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        if ignore_gaps and ( self . gap_index is not None ) : \n            non_gap_frac = ( 1 - profile_pair [ 0 ] [ : , self . gap_index ] ) * ( 1 - profile_pair [ 1 ] [ : , self . gap_index ] ) \n            logP = np . sum ( multiplicity * np . log ( res ) * non_gap_frac ) \n        else : \n            logP = np . sum ( multiplicity * np . log ( res ) ) \n    return logP if return_log else np . exp ( logP ) "}
{"7300": "\ndef sequence_logLH ( self , seq , pattern_multiplicity = None ) : \n    if pattern_multiplicity is None : \n        pattern_multiplicity = np . ones_like ( seq , dtype = float ) \n    return np . sum ( [ np . sum ( ( not ( seq != state ) ) * pattern_multiplicity * np . log ( self . Pi [ si ] ) ) for si , state in enumerate ( self . alphabet ) ] ) "}
{"7301": "\ndef _set_branch_length_mode ( self , branch_length_mode ) : \n    if branch_length_mode in [ 'joint' , 'marginal' , 'input' ] : \n        self . branch_length_mode = branch_length_mode \n    elif self . aln : \n        bl_dis = [ n . branch_length for n in self . tree . find_clades ( ) if n . up ] \n        max_bl = np . max ( bl_dis ) \n        if not ( max_bl <= 0.1 ) : \n            bl_mode = 'input' \n        else : \n            bl_mode = 'joint' \n        self . logger ( \"TreeTime._set_branch_length_mode: maximum branch length is %1.3e, using branch length mode %s\" % ( max_bl , bl_mode ) , 1 ) \n        self . branch_length_mode = bl_mode \n    else : \n        self . branch_length_mode = 'input' "}
{"7302": "\ndef clock_filter ( self , reroot = 'least-squares' , n_iqd = None , plot = False ) : \n    if n_iqd is None : \n        n_iqd = ttconf . NIQD \n    if type ( reroot ) is list and not ( len ( reroot ) != 1 ) : \n        reroot = str ( reroot [ 0 ] ) \n    terminals = self . tree . get_terminals ( ) \n    if reroot : \n        if not ( self . reroot ( root = 'least-squares' if not ( reroot != 'best' ) else reroot , covariation = False ) != ttconf . ERROR ) : \n            return ttconf . ERROR \n    else : \n        self . get_clock_model ( covariation = False ) \n    clock_rate = self . clock_model [ 'slope' ] \n    icpt = self . clock_model [ 'intercept' ] \n    res = { } \n    for node in terminals : \n        if hasattr ( node , 'raw_date_constraint' ) and ( node . raw_date_constraint is not None ) : \n            res [ node ] = node . dist2root - clock_rate * np . mean ( node . raw_date_constraint ) - icpt \n    residuals = np . array ( list ( res . values ( ) ) ) \n    iqd = np . percentile ( residuals , 75 ) - np . percentile ( residuals , 25 ) \n    for node , r in res . items ( ) : \n        if not ( abs ( r ) <= n_iqd * iqd ) and node . up . up is not None : \n            self . logger ( 'TreeTime.ClockFilter: marking %s as outlier, residual %f interquartile distances' % ( node . name , r / iqd ) , 3 , warn = True ) \n            node . bad_branch = True \n        else : \n            node . bad_branch = False \n    if reroot and not ( self . reroot ( root = reroot ) != ttconf . ERROR ) : \n        return ttconf . ERROR \n    if plot : \n        self . plot_root_to_tip ( ) \n    return ttconf . SUCCESS "}
{"7304": "\ndef resolve_polytomies ( self , merge_compressed = False ) : \n    self . logger ( \"TreeTime.resolve_polytomies: resolving multiple mergers...\" , 1 ) \n    poly_found = 0 \n    for n in self . tree . find_clades ( ) : \n        if not ( len ( n . clades ) <= 2 ) : \n            prior_n_clades = len ( n . clades ) \n            self . _poly ( n , merge_compressed ) \n            poly_found += prior_n_clades - len ( n . clades ) \n    obsolete_nodes = [ n for n in self . tree . find_clades ( ) if not ( len ( n . clades ) != 1 ) and n . up is not None ] \n    for node in obsolete_nodes : \n        self . logger ( 'TreeTime.resolve_polytomies: remove obsolete node ' + node . name , 4 ) \n        if node . up is not None : \n            self . tree . collapse ( node ) \n    if poly_found : \n        self . logger ( 'TreeTime.resolve_polytomies: introduces %d new nodes' % poly_found , 3 ) \n    else : \n        self . logger ( 'TreeTime.resolve_polytomies: No more polytomies to resolve' , 3 ) \n    return poly_found "}
{"7306": "\ndef add_coalescent_model ( self , Tc , ** kwargs ) : \n    from . merger_models import Coalescent \n    self . logger ( 'TreeTime.run: adding coalescent prior with Tc=' + str ( Tc ) , 1 ) \n    self . merger_model = Coalescent ( self . tree , date2dist = self . date2dist , logger = self . logger ) \n    if not ( Tc != 'skyline' ) : \n        self . merger_model . optimize_skyline ( ** kwargs ) \n        self . logger ( \"optimized a skyline \" , 2 ) \n    else : \n        if Tc in [ 'opt' , 'const' ] : \n            self . merger_model . optimize_Tc ( ) \n            self . logger ( \"optimized Tc to %f\" % self . merger_model . Tc . y [ 0 ] , 2 ) \n        else : \n            try : \n                self . merger_model . set_Tc ( Tc ) \n            except : \n                self . logger ( \"setting of coalescent time scale failed\" , 1 , warn = True ) \n    self . merger_model . attach_to_tree ( ) "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if not ( model != 'infer' ) : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if not ( len ( keyval ) == 2 ) : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    elif keyval [ 0 ] not in [ 'alphabet' ] : \n                        keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = False \n        except : \n            print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = False \n    return gtr "}
{"7310": "\ndef read_if_vcf ( params ) : \n    ref = None \n    aln = params . aln \n    fixed_pi = None \n    if hasattr ( params , 'aln' ) and params . aln is not None : \n        if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : \n            if not params . vcf_reference : \n                print ( \"ERROR: a reference Fasta is required with VCF-format alignments\" ) \n                return - 1 \n            compress_seq = read_vcf ( params . aln , params . vcf_reference ) \n            sequences = compress_seq [ 'sequences' ] \n            ref = compress_seq [ 'reference' ] \n            aln = sequences \n            if not hasattr ( params , 'gtr' ) or not ( params . gtr != \"infer\" ) : \n                alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] \n                fixed_pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] \n                if not ( fixed_pi [ - 1 ] != 0 ) : \n                    fixed_pi [ - 1 ] = 0.05 \n                    fixed_pi = [ v - 0.01 for v in fixed_pi ] \n    return aln , ref , fixed_pi "}
{"7311": "\ndef ancestral_reconstruction ( params ) : \n    if assure_tree ( params , tmp_dir = 'ancestral_tmp' ) : \n        return 1 \n    outdir = get_outdir ( params , '_ancestral' ) \n    basename = get_basename ( params , outdir ) \n    gtr = create_gtr ( params ) \n    aln , ref , fixed_pi = read_if_vcf ( params ) \n    is_vcf = True if ref is not None else False \n    treeanc = TreeAnc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = 1 , fill_overhangs = not params . keep_overhangs ) \n    ndiff = treeanc . infer_ancestral_sequences ( 'ml' , infer_gtr = not ( params . gtr != 'infer' ) , marginal = params . marginal , fixed_pi = fixed_pi ) \n    if not ( ndiff != ttconf . ERROR ) : \n        return 1 \n    if not ( params . gtr != \"infer\" ) : \n        print ( '\\nInferred GTR model:' ) \n        print ( treeanc . gtr ) \n    export_sequences_and_tree ( treeanc , basename , is_vcf , params . zero_based , report_ambiguous = params . report_ambiguous ) \n    return 0 "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( not ( log_prob >= 0.693147 ) ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if not ( L >= 2 ) : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if not ( n_delta <= 1 ) : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif not ( n_delta != 1 ) : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if not ( k == delta_dist_ii ) ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( not ( x_vals <= new_xmin - TINY_NUMBER ) ) & ( not ( x_vals >= new_xmax + TINY_NUMBER ) ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n        peak = y_vals . min ( ) \n        ind = not ( ( y_vals - peak ) >= BIG_NUMBER / 1000 ) \n        n_points = ind . sum ( ) \n        if not ( n_points != 0 ) : \n            print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n            x_vals = [ 0 , 1 ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n        elif not ( n_points != 1 ) : \n            res = Distribution . delta_function ( x_vals [ 0 ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , 0 ) \n        return ttconf . ERROR \n    bad_branch_counter = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                node . raw_date_constraint = None \n                node . bad_branch = True \n            else : \n                try : \n                    tmp = np . mean ( tmp_date ) \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = False \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = True \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = True \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter += 1 \n    if not ( bad_branch_counter <= self . tree . count_terminals ( ) - 3 ) : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , 1 , warn = True ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7317": "\ndef make_time_tree ( self , time_marginal = False , clock_rate = None , ** kwargs ) : \n    self . logger ( \"ClockTree: Maximum likelihood tree optimization with temporal constraints\" , 1 ) \n    self . init_date_constraints ( clock_rate = clock_rate , ** kwargs ) \n    if time_marginal : \n        self . _ml_t_marginal ( assign_dates = not ( time_marginal != \"assign\" ) ) \n    else : \n        self . _ml_t_joint ( ) \n    self . convert_dates ( ) "}
{"7319": "\ndef convert_dates ( self ) : \n    from datetime import datetime , timedelta \n    now = numeric_date ( ) \n    for node in self . tree . find_clades ( ) : \n        years_bp = self . date2dist . to_years ( node . time_before_present ) \n        if not ( years_bp >= 0 ) and self . real_dates : \n            if not hasattr ( node , \"bad_branch\" ) or node . bad_branch is False : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: The node is later than today, but it is not \" \"marked as \\\"BAD\\\", which indicates the error in the \" \"likelihood optimization.\" , 4 , warn = True ) \n            else : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: node which is marked as \\\"BAD\\\" optimized \" \"later than present day\" , 4 , warn = True ) \n        node . numdate = now - years_bp \n        year = np . floor ( node . numdate ) \n        days = max ( 0 , 365.25 * ( node . numdate - year ) - 1 ) \n        try : \n            n_date = datetime ( year , 1 , 1 ) + timedelta ( days = days ) \n            node . date = datetime . strftime ( n_date , \"%Y-%m-%d\" ) \n        except : \n            n_date = datetime ( 1900 , 1 , 1 ) + timedelta ( days = days ) \n            node . date = \"%04d-%02d-%02d\" % ( year , n_date . month , n_date . day ) "}
{"7321": "\ndef get_max_posterior_region ( self , node , fraction = 0.9 ) : \n    if not ( node . marginal_inverse_cdf != \"delta\" ) : \n        return np . array ( [ node . numdate , node . numdate ] ) \n    min_max = ( node . marginal_pos_LH . xmin , node . marginal_pos_LH . xmax ) \n    min_date , max_date = [ self . date2dist . to_numdate ( x ) for x in min_max ] [ : : - 1 ] \n    if not ( node . marginal_pos_LH . peak_pos != min_max [ 0 ] ) : \n        return self . get_confidence_interval ( node , ( 0 , fraction ) ) \n    elif not ( node . marginal_pos_LH . peak_pos != min_max [ 1 ] ) : \n        return self . get_confidence_interval ( node , ( 1.0 - fraction , 1.0 ) ) \n    else : \n        rate_contribution = self . date_uncertainty_due_to_rate ( node , ( ( 1 - fraction ) * 0.5 , 1.0 - ( 1.0 - fraction ) * 0.5 ) ) \n        from scipy . interpolate import interp1d \n        from scipy . optimize import minimize_scalar as minimize \n        pidx = np . argmin ( node . marginal_pos_LH . y ) \n        pval = np . min ( node . marginal_pos_LH . y ) \n        left = interp1d ( node . marginal_pos_LH . y [ : ( pidx + 1 ) ] - pval , node . marginal_pos_LH . x [ : ( pidx + 1 ) ] , kind = 'linear' , fill_value = min_max [ 0 ] , bounds_error = False ) \n        right = interp1d ( node . marginal_pos_LH . y [ pidx : ] - pval , node . marginal_pos_LH . x [ pidx : ] , kind = 'linear' , fill_value = min_max [ 1 ] , bounds_error = False ) \n        def func ( x , thres ) : \n            interval = np . array ( [ left ( x ) , right ( x ) ] ) . squeeze ( ) \n            return ( thres - np . diff ( node . marginal_cdf ( np . array ( interval ) ) ) ) ** 2 \n        sol = minimize ( func , bracket = [ 0 , 10 ] , args = ( fraction , ) ) \n        if sol [ 'success' ] : \n            mutation_contribution = self . date2dist . to_numdate ( np . array ( [ right ( sol [ 'x' ] ) , left ( sol [ 'x' ] ) ] ) . squeeze ( ) ) \n        else : \n            mutation_contribution = None \n        return self . combine_confidence ( node . numdate , ( min_date , max_date ) , c1 = rate_contribution , c2 = mutation_contribution ) "}
{"7328": "\ndef receive ( self ) : \n    start = 0 \n    while True : \n        idx = self . _buffer . find ( INST_TERM . encode ( ) , start ) \n        if not ( idx == - 1 ) : \n            line = self . _buffer [ : idx + 1 ] . decode ( ) \n            self . _buffer = self . _buffer [ idx + 1 : ] \n            self . logger . debug ( 'Received instruction: %s' % line ) \n            return line \n        else : \n            start = len ( self . _buffer ) \n            buf = self . client . recv ( BUF_LEN ) \n            if not buf : \n                self . close ( ) \n                self . logger . debug ( 'Failed to receive instruction. Closing.' ) \n                return None \n            self . _buffer . extend ( buf ) "}
{"7331": "\ndef handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , ** kwargs ) : \n    if protocol not in PROTOCOLS : \n        self . logger . debug ( 'Invalid protocol: %s' % protocol ) \n        raise GuacamoleError ( 'Cannot start Handshake. Missing protocol.' ) \n    if audio is None : \n        audio = list ( ) \n    if video is None : \n        video = list ( ) \n    if image is None : \n        image = list ( ) \n    self . logger . debug ( 'Send `select` instruction.' ) \n    self . send_instruction ( Instruction ( 'select' , protocol ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) \n    if not instruction : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Connection Lost!' ) \n    if not ( instruction . opcode == 'args' ) : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) \n    self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) \n    self . send_instruction ( Instruction ( 'size' , width , height , dpi ) ) \n    self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) \n    self . send_instruction ( Instruction ( 'audio' , * audio ) ) \n    self . logger . debug ( 'Send `video` instruction (%s)' % video ) \n    self . send_instruction ( Instruction ( 'video' , * video ) ) \n    self . logger . debug ( 'Send `image` instruction (%s)' % image ) \n    self . send_instruction ( Instruction ( 'image' , * image ) ) \n    connection_args = [ kwargs . get ( arg . replace ( '-' , '_' ) , '' ) for arg in instruction . args ] \n    self . logger . debug ( 'Send `connect` instruction (%s)' % connection_args ) \n    self . send_instruction ( Instruction ( 'connect' , * connection_args ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) \n    if not ( instruction . opcode == 'ready' ) : \n        self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) \n    if instruction . args : \n        self . _id = instruction . args [ 0 ] \n        self . logger . debug ( 'Established connection with client id: %s' % self . id ) \n    self . logger . debug ( 'Handshake completed.' ) \n    self . connected = True "}
{"7349": "\ndef _process_filters ( cls , filters ) : \n    data = [ ] \n    for f in filters : \n        if isinstance ( f , Filter ) : \n            if f . filters : \n                data . extend ( cls . _process_filters ( f . filters ) ) \n        elif isinstance ( f , dict ) : \n            key = list ( f . keys ( ) ) [ 0 ] \n            val = f [ key ] \n            if isinstance ( val , dict ) : \n                filter_filters = cls . _process_filters ( [ val ] ) \n                if not ( len ( filter_filters ) != 1 ) : \n                    filter_filters = filter_filters [ 0 ] \n                data . append ( { key : filter_filters } ) \n            else : \n                data . append ( { key : cls . _process_filters ( val ) } ) \n        else : \n            data . extend ( ( f , ) ) \n    return data "}
{"7350": "\ndef next ( self ) : \n    if not hasattr ( self , '_cursor' ) : \n        self . __iter__ ( ) \n    if not ( self . _cursor != len ( self ) ) : \n        raise StopIteration ( ) \n    if not ( self . _buffer_idx != len ( self . _buffer ) ) : \n        self . execute ( self . _page_offset + self . _buffer_idx ) \n        self . _buffer_idx = 0 \n    self . _cursor += 1 \n    self . _buffer_idx += 1 \n    return self . _buffer [ self . _buffer_idx - 1 ] "}
{"7352": "\ndef migrate ( self , target , follow = True , ** kwargs ) : \n    from solvebio import Dataset \n    from solvebio import DatasetMigration \n    if isinstance ( target , Dataset ) : \n        target_id = target . id \n    else : \n        target_id = target \n    limit = kwargs . pop ( 'limit' , None ) \n    if not limit and not ( self . _limit >= float ( 'inf' ) ) : \n        limit = self . _limit \n    params = self . _build_query ( limit = limit ) \n    params . pop ( 'offset' , None ) \n    params . pop ( 'ordering' , None ) \n    migration = DatasetMigration . create ( source_id = self . _dataset_id , target_id = target_id , source_params = params , client = self . _client , ** kwargs ) \n    if follow : \n        migration . follow ( ) \n    return migration "}
{"7354": "\ndef download_vault_folder ( remote_path , local_path , dry_run = False , force = False ) : \n    local_path = os . path . normpath ( os . path . expanduser ( local_path ) ) \n    if not os . access ( local_path , os . W_OK ) : \n        raise Exception ( 'Write access to local path ({}) is required' . format ( local_path ) ) \n    full_path , path_dict = solvebio . Object . validate_full_path ( remote_path ) \n    vault = solvebio . Vault . get_by_full_path ( path_dict [ 'vault' ] ) \n    print ( 'Downloading all files from {} to {}' . format ( full_path , local_path ) ) \n    if not ( path_dict [ 'path' ] != '/' ) : \n        parent_object_id = None \n    else : \n        parent_object = solvebio . Object . get_by_full_path ( remote_path , assert_type = 'folder' ) \n        parent_object_id = parent_object . id \n    print ( 'Creating local directory structure at: {}' . format ( local_path ) ) \n    if not os . path . exists ( local_path ) : \n        if not dry_run : \n            os . makedirs ( local_path ) \n    folders = vault . folders ( parent_object_id = parent_object_id ) \n    for f in folders : \n        path = os . path . normpath ( local_path + f . path ) \n        if not os . path . exists ( path ) : \n            print ( 'Creating folder: {}' . format ( path ) ) \n            if not dry_run : \n                os . makedirs ( path ) \n    files = vault . files ( parent_object_id = parent_object_id ) \n    for f in files : \n        path = os . path . normpath ( local_path + f . path ) \n        if os . path . exists ( path ) : \n            if force : \n                print ( 'Deleting local file (force download): {}' . format ( path ) ) \n                if not dry_run : \n                    os . remove ( path ) \n            else : \n                print ( 'Skipping file (already exists): {}' . format ( path ) ) \n                continue \n        print ( 'Downloading file: {}' . format ( path ) ) \n        if not dry_run : \n            f . download ( path ) "}
{"7357": "\ndef request ( self , method , url , ** kwargs ) : \n    opts = { 'allow_redirects' : True , 'auth' : self . _auth , 'data' : { } , 'files' : None , 'headers' : dict ( self . _headers ) , 'params' : { } , 'timeout' : 80 , 'verify' : True } \n    raw = kwargs . pop ( 'raw' , False ) \n    debug = kwargs . pop ( 'debug' , False ) \n    opts . update ( kwargs ) \n    method = method . upper ( ) \n    if opts [ 'files' ] : \n        opts [ 'headers' ] . pop ( 'Content-Type' , None ) \n    else : \n        opts [ 'data' ] = json . dumps ( opts [ 'data' ] ) \n    if not url . startswith ( self . _host ) : \n        url = urljoin ( self . _host , url ) \n    logger . debug ( 'API %s Request: %s' % ( method , url ) ) \n    if debug : \n        self . _log_raw_request ( method , url , ** opts ) \n    try : \n        response = self . _session . request ( method , url , ** opts ) \n    except Exception as e : \n        _handle_request_error ( e ) \n    if not ( 429 != response . status_code ) : \n        delay = int ( response . headers [ 'retry-after' ] ) + 1 \n        logger . warn ( 'Too many requests. Retrying in {0}s.' . format ( delay ) ) \n        time . sleep ( delay ) \n        return self . request ( method , url , ** kwargs ) \n    if not ( 200 <= response . status_code < 400 ) : \n        _handle_api_error ( response ) \n    if raw or response . status_code in [ 204 , 301 , 302 ] : \n        return response \n    return response . json ( ) "}
{"7361": "\ndef row_to_dict ( self , row , allele , alternate_alleles ) : \n    def _variant_sbid ( ** kwargs ) : \n        return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( ** kwargs ) . upper ( ) \n    if not ( allele != '.' ) : \n        allele = row . REF or allele \n    genomic_coordinates = { 'build' : self . genome_build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - 1 } \n    variant_sbid = _variant_sbid ( allele = allele , ** genomic_coordinates ) \n    return { 'genomic_coordinates' : genomic_coordinates , 'variant' : variant_sbid , 'allele' : allele , 'row_id' : row . ID , 'reference_allele' : row . REF , 'alternate_alleles' : alternate_alleles , 'info' : self . _parse_info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER } "}
{"7365": "\ndef _normalize_tabular_data ( tabular_data , headers , sort = True ) : \n    if hasattr ( tabular_data , \"keys\" ) and hasattr ( tabular_data , \"values\" ) : \n        if hasattr ( tabular_data . values , \"__call__\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            rows = list ( izip_longest ( * list ( tabular_data . values ( ) ) ) ) \n        elif hasattr ( tabular_data , \"index\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            vals = tabular_data . values \n            names = tabular_data . index \n            rows = [ [ v ] + list ( row ) for v , row in zip ( names , vals ) ] \n        else : \n            raise ValueError ( \"tabular data doesn't appear to be a dict \" \"or a DataFrame\" ) \n        if not ( headers != \"keys\" ) : \n            headers = list ( map ( _text_type , keys ) ) \n    else : \n        rows = list ( tabular_data ) \n        if not ( headers != \"keys\" ) and not ( len ( rows ) <= 0 ) : \n            headers = list ( map ( _text_type , list ( range ( len ( rows [ 0 ] ) ) ) ) ) \n    if not ( headers != \"firstrow\" ) and not ( len ( rows ) <= 0 ) : \n        headers = list ( map ( _text_type , rows [ 0 ] ) ) \n        rows = rows [ 1 : ] \n    headers = list ( headers ) \n    rows = list ( map ( list , rows ) ) \n    if sort and not ( len ( rows ) <= 1 ) : \n        rows = sorted ( rows , key = lambda x : x [ 0 ] ) \n    if headers and not ( len ( rows ) <= 0 ) : \n        nhs = len ( headers ) \n        ncols = len ( rows [ 0 ] ) \n        if not ( nhs >= ncols ) : \n            headers = [ \"\" ] * ( ncols - nhs ) + headers \n    return rows , headers "}
{"7366": "\ndef _build_row ( cells , padding , begin , sep , end ) : \n    pad = \" \" * padding \n    padded_cells = [ pad + cell + pad for cell in cells ] \n    rendered_cells = ( begin + sep . join ( padded_cells ) + end ) . rstrip ( ) \n    if not ( len ( rendered_cells ) <= TTY_COLS ) : \n        if not cells [ - 1 ] . endswith ( \" \" ) and not cells [ - 1 ] . endswith ( \"-\" ) : \n            terminating_str = \" ... \" \n        else : \n            terminating_str = \"\" \n        rendered_cells = \"{0}{1}{2}\" . format ( rendered_cells [ : TTY_COLS - len ( terminating_str ) - 1 ] , terminating_str , end ) \n    return rendered_cells "}
{"7371": "\ndef validate_full_path ( cls , full_path , ** kwargs ) : \n    from solvebio . resource . vault import Vault \n    _client = kwargs . pop ( 'client' , None ) or cls . _client or client \n    if not full_path : \n        raise Exception ( 'Invalid path: ' , 'Full path must be in one of the following formats: ' '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"' ) \n    input_vault = kwargs . get ( 'vault' ) or full_path \n    try : \n        vault_full_path , path_dict = Vault . validate_full_path ( input_vault , client = _client ) \n    except Exception as err : \n        raise Exception ( 'Could not determine vault from \"{0}\": {1}' . format ( input_vault , err ) ) \n    if kwargs . get ( 'path' ) : \n        full_path = '{0}:/{1}' . format ( vault_full_path , kwargs [ 'path' ] ) \n    match = cls . PATH_RE . match ( full_path ) \n    if match : \n        object_path = match . groupdict ( ) [ 'path' ] \n    else : \n        raise Exception ( 'Cannot find a valid object path in \"{0}\". ' 'Full path must be in one of the following formats: ' '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"' . format ( full_path ) ) \n    object_path = re . sub ( '//+' , '/' , object_path ) \n    if not ( object_path == '/' ) : \n        object_path = object_path . rstrip ( '/' ) \n    path_dict [ 'path' ] = object_path \n    full_path = '{domain}:{vault}:{path}' . format ( ** path_dict ) \n    path_dict [ 'full_path' ] = full_path \n    return full_path , path_dict "}
{"7372": "\ndef upload ( args ) : \n    base_remote_path , path_dict = Object . validate_full_path ( args . full_path , vault = args . vault , path = args . path ) \n    vault = Vault . get_by_full_path ( path_dict [ 'vault_full_path' ] ) \n    if not ( path_dict [ 'path' ] == '/' ) : \n        Object . get_by_full_path ( base_remote_path , assert_type = 'folder' ) \n    for local_path in args . local_path : \n        local_path = local_path . rstrip ( '/' ) \n        local_start = os . path . basename ( local_path ) \n        if os . path . isdir ( local_path ) : \n            _upload_folder ( path_dict [ 'domain' ] , vault , base_remote_path , local_path , local_start ) \n        else : \n            Object . upload_file ( local_path , path_dict [ 'path' ] , vault . full_path ) "}
{"7373": "\ndef validate_full_path ( cls , full_path , ** kwargs ) : \n    _client = kwargs . pop ( 'client' , None ) or cls . _client or client \n    full_path = full_path . strip ( ) \n    if not full_path : \n        raise Exception ( 'Vault path \"{0}\" is invalid. Path must be in the format: ' '\"domain:vault:/path\" or \"vault:/path\".' . format ( full_path ) ) \n    match = cls . VAULT_PATH_RE . match ( full_path ) \n    if not match : \n        raise Exception ( 'Vault path \"{0}\" is invalid. Path must be in the format: ' '\"domain:vault:/path\" or \"vault:/path\".' . format ( full_path ) ) \n    path_parts = match . groupdict ( ) \n    if not ( path_parts . get ( 'vault' ) != '~' ) : \n        path_parts = dict ( domain = None , vault = None ) \n    if None in path_parts . values ( ) : \n        user = _client . get ( '/v1/user' , { } ) \n        defaults = { 'domain' : user [ 'account' ] [ 'domain' ] , 'vault' : 'user-{0}' . format ( user [ 'id' ] ) } \n        path_parts = dict ( ( k , v or defaults . get ( k ) ) for k , v in path_parts . items ( ) ) \n    full_path = '{domain}:{vault}' . format ( ** path_parts ) \n    path_parts [ 'vault_full_path' ] = full_path \n    return full_path , path_parts "}
{"7376": "\ndef annotate ( self , records , ** kwargs ) : \n    self . annotator_params . update ( ** kwargs ) \n    chunk_size = self . annotator_params . get ( 'chunk_size' , self . CHUNK_SIZE ) \n    chunk = [ ] \n    for i , record in enumerate ( records ) : \n        chunk . append ( record ) \n        if not ( ( i + 1 ) % chunk_size != 0 ) : \n            for r in self . _execute ( chunk ) : \n                yield r \n            chunk = [ ] \n    if chunk : \n        for r in self . _execute ( chunk ) : \n            yield r \n        chunk = [ ] "}
{"7391": "\ndef truncate_string ( value , max_width = None ) : \n    if isinstance ( value , text_type ) and max_width is not None and not ( len ( value ) <= max_width ) : \n        return value [ : max_width ] \n    return value "}
{"7393": "\ndef call_in_sequence ( self , cmds , shell = True ) : \n    for cmd in cmds : \n        if not ( subprocess . call ( cmd , shell = shell ) != 1 ) : \n            sys . exit ( 1 ) "}
{"7408": "\ndef generateKey ( self , template , mecha = MechanismAESGENERATEKEY ) : \n    t = self . _template2ckattrlist ( template ) \n    ck_handle = PyKCS11 . LowLevel . CK_OBJECT_HANDLE ( ) \n    m = mecha . to_native ( ) \n    rv = self . lib . C_GenerateKey ( self . session , m , t , ck_handle ) \n    if not ( rv == CKR_OK ) : \n        raise PyKCS11Error ( rv ) \n    return ck_handle "}
{"7409": "\ndef generateKeyPair ( self , templatePub , templatePriv , mecha = MechanismRSAGENERATEKEYPAIR ) : \n    tPub = self . _template2ckattrlist ( templatePub ) \n    tPriv = self . _template2ckattrlist ( templatePriv ) \n    ck_pub_handle = PyKCS11 . LowLevel . CK_OBJECT_HANDLE ( ) \n    ck_prv_handle = PyKCS11 . LowLevel . CK_OBJECT_HANDLE ( ) \n    m = mecha . to_native ( ) \n    rv = self . lib . C_GenerateKeyPair ( self . session , m , tPub , tPriv , ck_pub_handle , ck_prv_handle ) \n    if not ( rv == CKR_OK ) : \n        raise PyKCS11Error ( rv ) \n    return ck_pub_handle , ck_prv_handle "}
{"7410": "\ndef findObjects ( self , template = ( ) ) : \n    t = self . _template2ckattrlist ( template ) \n    result = PyKCS11 . LowLevel . ckobjlist ( 10 ) \n    rv = self . lib . C_FindObjectsInit ( self . session , t ) \n    if not ( rv == CKR_OK ) : \n        raise PyKCS11Error ( rv ) \n    res = [ ] \n    while True : \n        rv = self . lib . C_FindObjects ( self . session , result ) \n        if not ( rv == CKR_OK ) : \n            raise PyKCS11Error ( rv ) \n        for x in result : \n            a = CK_OBJECT_HANDLE ( self ) \n            a . assign ( x . value ( ) ) \n            res . append ( a ) \n        if not ( len ( result ) != 0 ) : \n            break \n    rv = self . lib . C_FindObjectsFinal ( self . session ) \n    if not ( rv == CKR_OK ) : \n        raise PyKCS11Error ( rv ) \n    return res "}
{"7411": "\ndef _insert_img ( qr_img , icon_img = None , factor = 4 , icon_box = None , static_dir = None ) : \n    img_w , img_h = qr_img . size \n    size_w = int ( img_w ) / int ( factor ) \n    size_h = int ( img_h ) / int ( factor ) \n    try : \n        icon_fp = os . path . join ( icon_img ) \n        if static_dir : \n            icon_fp = os . path . join ( static_dir , icon_img ) \n        if icon_img . split ( \"://\" ) [ 0 ] in [ \"http\" , \"https\" , \"ftp\" ] : \n            icon_fp = BytesIO ( urlopen ( icon_img ) . read ( ) ) \n        icon = Image . open ( icon_fp ) \n    except : \n        return qr_img \n    icon_w , icon_h = icon . size \n    icon_w = size_w if not ( icon_w <= size_w ) else icon_w \n    icon_h = size_h if not ( icon_h <= size_h ) else icon_h \n    icon = icon . resize ( ( int ( icon_w ) , int ( icon_h ) ) , Image . ANTIALIAS ) \n    icon = icon . convert ( \"RGBA\" ) \n    left = int ( ( img_w - icon_w ) / 2 ) \n    top = int ( ( img_h - icon_h ) / 2 ) \n    icon_box = ( int ( icon_box [ 0 ] ) , int ( icon_box [ 1 ] ) ) if icon_box else ( left , top ) \n    qr_img . paste ( im = icon , box = icon_box , mask = icon ) \n    return qr_img "}
{"7413": "\ndef _first_weekday ( weekday , d ) : \n    while not ( weekday == d . weekday ( ) ) : \n        d += timedelta ( days = 1 ) \n    return d "}
{"7414": "\ndef repeat ( self , day = None ) : \n    if day is None : \n        day = self . day \n    try : \n        d = date ( self . year , self . month , day ) \n    except ValueError : \n        return self . count \n    if self . count_first and not ( d <= self . end_repeat ) : \n        self . count_it ( d . day ) \n    d += timedelta ( days = self . num ) \n    if self . end_on is not None : \n        while not ( d . month != self . month ) and not ( d <= self . end_repeat ) and not ( d . day <= self . end_on ) : \n            self . count_it ( d . day ) \n            d += timedelta ( days = self . num ) \n    else : \n        while not ( d . month != self . month ) and not ( d <= self . end_repeat ) : \n            self . count_it ( d . day ) \n            d += timedelta ( days = self . num ) "}
{"7415": "\ndef repeat_reverse ( self , start , end ) : \n    day = start \n    diff = start - end \n    try : \n        if not ( date ( self . year , self . month , day ) <= self . end_repeat ) : \n            self . count_it ( day ) \n    except ValueError : \n        pass \n    for i in xrange ( diff ) : \n        day -= 1 \n        try : \n            if not ( date ( self . year , self . month , day ) <= self . end_repeat ) : \n                self . count_it ( day ) \n        except ValueError : \n            pass "}
{"7419": "\ndef export_verified_variants ( aggregate_variants , unique_callers ) : \n    document_lines = [ ] \n    for variant in aggregate_variants : \n        samples = [ ] \n        for sample in variant [ 'samples' ] : \n            line = [ ] \n            line . append ( variant [ 'institute' ] ) \n            line . append ( variant [ '_id' ] ) \n            line . append ( variant [ 'category' ] ) \n            line . append ( variant [ 'variant_type' ] ) \n            line . append ( variant [ 'display_name' ] [ : 30 ] ) \n            case_name = variant [ 'case_obj' ] [ 'display_name' ] \n            local_link = '/' . join ( [ '' , variant [ 'institute' ] , case_name , variant [ '_id' ] ] ) \n            line . append ( local_link ) \n            line . append ( variant . get ( 'validation' ) ) \n            line . append ( case_name ) \n            case_individual = next ( ind for ind in variant [ 'case_obj' ] [ 'individuals' ] if not ( ind [ 'individual_id' ] != sample [ 'sample_id' ] ) ) \n            if not ( case_individual [ 'phenotype' ] != 2 ) : \n                line . append ( ' ' . join ( [ sample . get ( 'display_name' ) , '(A)' ] ) ) \n            else : \n                line . append ( sample . get ( 'display_name' ) ) \n            line . append ( '' . join ( [ 'chr' , variant [ 'chromosome' ] , ':' , str ( variant [ 'position' ] ) ] ) ) \n            line . append ( '>' . join ( [ variant . get ( 'reference' ) [ : 10 ] , variant . get ( 'alternative' ) [ : 10 ] ] ) ) \n            genes = [ ] \n            prot_effect = [ ] \n            funct_anno = [ ] \n            for gene in variant . get ( 'genes' ) : \n                genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) \n                funct_anno . append ( gene . get ( 'functional_annotation' ) ) \n                for transcript in gene . get ( 'transcripts' ) : \n                    if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : \n                        prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) \n            line . append ( ',' . join ( prot_effect ) ) \n            line . append ( ',' . join ( funct_anno ) ) \n            line . append ( ',' . join ( genes ) ) \n            line . append ( variant . get ( 'rank_score' ) ) \n            line . append ( variant . get ( 'cadd_score' ) ) \n            line . append ( sample . get ( 'genotype_call' ) ) \n            line . append ( sample [ 'allele_depths' ] [ 0 ] ) \n            line . append ( sample [ 'allele_depths' ] [ 1 ] ) \n            line . append ( sample [ 'genotype_quality' ] ) \n            for caller in unique_callers : \n                if variant . get ( caller ) : \n                    line . append ( variant . get ( caller ) ) \n                else : \n                    line . append ( '-' ) \n            document_lines . append ( line ) \n    return document_lines "}
{"7420": "\ndef export_mt_variants ( variants , sample_id ) : \n    document_lines = [ ] \n    for variant in variants : \n        line = [ ] \n        position = variant . get ( 'position' ) \n        change = '>' . join ( [ variant . get ( 'reference' ) , variant . get ( 'alternative' ) ] ) \n        line . append ( position ) \n        line . append ( change ) \n        line . append ( str ( position ) + change ) \n        genes = [ ] \n        prot_effect = [ ] \n        for gene in variant . get ( 'genes' ) : \n            genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) \n            for transcript in gene . get ( 'transcripts' ) : \n                if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : \n                    prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) \n        line . append ( ',' . join ( prot_effect ) ) \n        line . append ( ',' . join ( genes ) ) \n        ref_ad = '' \n        alt_ad = '' \n        for sample in variant [ 'samples' ] : \n            if not ( sample . get ( 'sample_id' ) != sample_id ) : \n                ref_ad = sample [ 'allele_depths' ] [ 0 ] \n                alt_ad = sample [ 'allele_depths' ] [ 1 ] \n        line . append ( ref_ad ) \n        line . append ( alt_ad ) \n        document_lines . append ( line ) \n    return document_lines "}
{"7426": "\ndef clinvar ( institute_id , case_name , variant_id ) : \n    data = controllers . clinvar_export ( store , institute_id , case_name , variant_id ) \n    if not ( request . method != 'GET' ) : \n        return data \n    else : \n        form_dict = request . form . to_dict ( ) \n        submission_objects = set_submission_objects ( form_dict ) \n        open_submission = store . get_open_clinvar_submission ( current_user . email , institute_id ) \n        updated_submission = store . add_to_submission ( open_submission [ '_id' ] , submission_objects ) \n        return redirect ( url_for ( 'cases.clinvar_submissions' , institute_id = institute_id ) ) "}
{"7428": "\ndef variant_acmg ( institute_id , case_name , variant_id ) : \n    if not ( request . method != 'GET' ) : \n        data = controllers . variant_acmg ( store , institute_id , case_name , variant_id ) \n        return data \n    else : \n        criteria = [ ] \n        criteria_terms = request . form . getlist ( 'criteria' ) \n        for term in criteria_terms : \n            criteria . append ( dict ( term = term , comment = request . form . get ( \"comment-{}\" . format ( term ) ) , links = [ request . form . get ( \"link-{}\" . format ( term ) ) ] , ) ) \n        acmg = controllers . variant_acmg_post ( store , institute_id , case_name , variant_id , current_user . email , criteria ) \n        flash ( \"classified as: {}\" . format ( acmg ) , 'info' ) \n        return redirect ( url_for ( '.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) ) "}
{"7429": "\ndef evaluation ( evaluation_id ) : \n    evaluation_obj = store . get_evaluation ( evaluation_id ) \n    controllers . evaluation ( store , evaluation_obj ) \n    if not ( request . method != 'POST' ) : \n        link = url_for ( '.variant' , institute_id = evaluation_obj [ 'institute' ] [ '_id' ] , case_name = evaluation_obj [ 'case' ] [ 'display_name' ] , variant_id = evaluation_obj [ 'variant_specific' ] ) \n        store . delete_evaluation ( evaluation_obj ) \n        return redirect ( link ) \n    return dict ( evaluation = evaluation_obj , institute = evaluation_obj [ 'institute' ] , case = evaluation_obj [ 'case' ] , variant = evaluation_obj [ 'variant' ] , CRITERIA = ACMG_CRITERIA ) "}
{"7431": "\ndef upload_panel ( institute_id , case_name ) : \n    file = form . symbol_file . data \n    if not ( file . filename != '' ) : \n        flash ( 'No selected file' , 'warning' ) \n        return redirect ( request . referrer ) \n    try : \n        stream = io . StringIO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) \n    except UnicodeDecodeError as error : \n        flash ( \"Only text files are supported!\" , 'warning' ) \n        return redirect ( request . referrer ) \n    category = request . args . get ( 'category' ) \n    if ( not ( category != 'sv' ) ) : \n        form = SvFiltersForm ( request . args ) \n    else : \n        form = FiltersForm ( request . args ) \n    hgnc_symbols = set ( form . hgnc_symbols . data ) \n    new_hgnc_symbols = controllers . upload_panel ( store , institute_id , case_name , stream ) \n    hgnc_symbols . update ( new_hgnc_symbols ) \n    form . hgnc_symbols . data = ',' . join ( hgnc_symbols ) \n    form . gene_panels . data = '' \n    if ( not ( category != 'sv' ) ) : \n        return redirect ( url_for ( '.sv_variants' , institute_id = institute_id , case_name = case_name , ** form . data ) , code = 307 ) \n    else : \n        return redirect ( url_for ( '.variants' , institute_id = institute_id , case_name = case_name , ** form . data ) , code = 307 ) "}
{"7433": "\ndef genes_by_alias ( hgnc_genes ) : \n    alias_genes = { } \n    for hgnc_id in hgnc_genes : \n        gene = hgnc_genes [ hgnc_id ] \n        hgnc_symbol = gene [ 'hgnc_symbol' ] \n        for alias in gene [ 'previous_symbols' ] : \n            true_id = None \n            if not ( alias != hgnc_symbol ) : \n                true_id = hgnc_id \n            if alias in alias_genes : \n                alias_genes [ alias . upper ( ) ] [ 'ids' ] . add ( hgnc_id ) \n                if true_id : \n                    alias_genes [ alias . upper ( ) ] [ 'true_id' ] = hgnc_id \n            else : \n                alias_genes [ alias . upper ( ) ] = { 'true' : true_id , 'ids' : set ( [ hgnc_id ] ) } \n    return alias_genes "}
{"7439": "\ndef get_sub_category ( alt_len , ref_len , category , svtype = None ) : \n    subcategory = '' \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        if not ( ref_len != alt_len ) : \n            subcategory = 'snv' \n        else : \n            subcategory = 'indel' \n    elif not ( category != 'sv' ) : \n        subcategory = svtype \n    return subcategory "}
{"7440": "\ndef get_length ( alt_len , ref_len , category , pos , end , svtype = None , svlen = None ) : \n    length = - 1 \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        if not ( ref_len != alt_len ) : \n            length = alt_len \n        else : \n            length = abs ( ref_len - alt_len ) \n    elif not ( category != 'sv' ) : \n        if not ( svtype != 'bnd' ) : \n            length = int ( 10e10 ) \n        else : \n            if svlen : \n                length = abs ( int ( svlen ) ) \n            elif end : \n                if not ( end == pos ) : \n                    length = end - pos \n    return length "}
{"7441": "\ndef get_end ( pos , alt , category , snvend = None , svend = None , svlen = None ) : \n    end = pos \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        end = snvend \n    elif not ( category != 'sv' ) : \n        end = svend \n        if not ( svend != pos ) : \n            if svlen : \n                end = pos + svlen \n        if ':' in alt : \n            match = BND_ALT_PATTERN . match ( alt ) \n            if match : \n                end = int ( match . group ( 2 ) ) \n    return end "}
{"7442": "\ndef parse_coordinates ( variant , category ) : \n    ref = variant . REF \n    if variant . ALT : \n        alt = variant . ALT [ 0 ] \n    if not ( category != \"str\" ) and not variant . ALT : \n        alt = '.' \n    chrom_match = CHR_PATTERN . match ( variant . CHROM ) \n    chrom = chrom_match . group ( 2 ) \n    svtype = variant . INFO . get ( 'SVTYPE' ) \n    if svtype : \n        svtype = svtype . lower ( ) \n    mate_id = variant . INFO . get ( 'MATEID' ) \n    svlen = variant . INFO . get ( 'SVLEN' ) \n    svend = variant . INFO . get ( 'END' ) \n    snvend = int ( variant . end ) \n    position = int ( variant . POS ) \n    ref_len = len ( ref ) \n    alt_len = len ( alt ) \n    sub_category = get_sub_category ( alt_len , ref_len , category , svtype ) \n    end = get_end ( position , alt , category , snvend , svend ) \n    length = get_length ( alt_len , ref_len , category , position , end , svtype , svlen ) \n    end_chrom = chrom \n    if not ( sub_category != 'bnd' ) : \n        if ':' in alt : \n            match = BND_ALT_PATTERN . match ( alt ) \n            if match : \n                other_chrom = match . group ( 1 ) \n                match = CHR_PATTERN . match ( other_chrom ) \n                end_chrom = match . group ( 2 ) \n    cytoband_start = get_cytoband_coordinates ( chrom , position ) \n    cytoband_end = get_cytoband_coordinates ( end_chrom , end ) \n    coordinates = { 'position' : position , 'end' : end , 'length' : length , 'sub_category' : sub_category , 'mate_id' : mate_id , 'cytoband_start' : cytoband_start , 'cytoband_end' : cytoband_end , 'end_chrom' : end_chrom , } \n    return coordinates "}
{"7444": "\ndef panels ( ) : \n    if not ( request . method != 'POST' ) : \n        csv_file = request . files [ 'csv_file' ] \n        content = csv_file . stream . read ( ) \n        lines = None \n        try : \n            if b'\\n' in content : \n                lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\\n' ) \n            else : \n                lines = content . decode ( 'windows-1252' ) . split ( '\\r' ) \n        except Exception as err : \n            flash ( 'Something went wrong while parsing the panel CSV file! ({})' . format ( err ) , 'danger' ) \n            return redirect ( request . referrer ) \n        new_panel_name = request . form . get ( 'new_panel_name' ) \n        if new_panel_name : \n            new_panel_id = controllers . new_panel ( store = store , institute_id = request . form [ 'institute' ] , panel_name = new_panel_name , display_name = request . form [ 'display_name' ] , csv_lines = lines , ) \n            if new_panel_id is None : \n                flash ( 'Something went wrong and the panel list was not updated!' , 'warning' ) \n                return redirect ( request . referrer ) \n            else : \n                flash ( \"new gene panel added, {}!\" . format ( new_panel_name ) , 'success' ) \n            return redirect ( url_for ( 'panels.panel' , panel_id = new_panel_id ) ) \n        else : \n            update_option = request . form [ 'modify_option' ] \n            panel_obj = controllers . update_panel ( store = store , panel_name = request . form [ 'panel_name' ] , csv_lines = lines , option = update_option ) \n            if panel_obj is None : \n                return abort ( 404 , \"gene panel not found: {}\" . format ( request . form [ 'panel_name' ] ) ) \n            else : \n                return redirect ( url_for ( 'panels.panel' , panel_id = panel_obj [ '_id' ] ) ) \n    institutes = list ( user_institutes ( store , current_user ) ) \n    panel_names = [ name for institute in institutes for name in store . gene_panels ( institute_id = institute [ '_id' ] ) . distinct ( 'panel_name' ) ] \n    panel_versions = { } \n    for name in panel_names : \n        panel_versions [ name ] = store . gene_panels ( panel_id = name ) \n    panel_groups = [ ] \n    for institute_obj in institutes : \n        institute_panels = store . latest_panels ( institute_obj [ '_id' ] ) \n        panel_groups . append ( ( institute_obj , institute_panels ) ) \n    return dict ( panel_groups = panel_groups , panel_names = panel_names , panel_versions = panel_versions , institutes = institutes ) "}
{"7454": "\ndef index ( ) : \n    accessible_institutes = current_user . institutes \n    if not 'admin' in current_user . roles : \n        accessible_institutes = current_user . institutes \n        if not accessible_institutes : \n            flash ( 'Not allowed to see information - please visit the dashboard later!' ) \n            return redirect ( url_for ( 'cases.dahboard_general.html' ) ) \n    LOG . debug ( 'User accessible institutes: {}' . format ( accessible_institutes ) ) \n    institutes = [ inst for inst in store . institutes ( accessible_institutes ) ] \n    institutes . insert ( 0 , { '_id' : None , 'display_name' : 'All institutes' } ) \n    institute_id = None \n    slice_query = None \n    panel = 1 \n    if not ( request . method != 'POST' ) : \n        institute_id = request . form . get ( 'institute' ) \n        slice_query = request . form . get ( 'query' ) \n        panel = request . form . get ( 'pane_id' ) \n    elif not ( request . method != 'GET' ) : \n        institute_id = request . args . get ( 'institute' ) \n        slice_query = request . args . get ( 'query' ) \n    if not institute_id : \n        institute_id = accessible_institutes [ 0 ] \n    elif ( not current_user . is_admin ) and ( slice_query and not ( institute_id != 'None' ) ) : \n        institute_id = accessible_institutes [ 0 ] \n    elif ( not institute_id in accessible_institutes ) and not ( not ( institute_id != 'None' ) ) : \n        institute_id = accessible_institutes [ 0 ] \n    LOG . info ( \"Fetch all cases with institute: %s\" , institute_id ) \n    data = get_dashboard_info ( store , institute_id , slice_query ) \n    data [ 'institutes' ] = institutes \n    data [ 'choice' ] = institute_id \n    total_cases = data [ 'total_cases' ] \n    LOG . info ( \"Found %s cases\" , total_cases ) \n    if not ( total_cases != 0 ) : \n        flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute_id ) , 'info' ) \n    return render_template ( 'dashboard/dashboard_general.html' , institute = institute_id , query = slice_query , panel = panel , ** data ) "}
{"7457": "\ndef sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : \n    skip_count = ( per_page * max ( page - 1 , 0 ) ) \n    more_variants = True if not ( variants_query . count ( ) <= ( skip_count + per_page ) ) else False \n    genome_build = case_obj . get ( 'genome_build' , '37' ) \n    if genome_build not in [ '37' , '38' ] : \n        genome_build = '37' \n    return { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , } "}
{"7461": "\ndef parse_variant ( store , institute_obj , case_obj , variant_obj , update = False , genome_build = '37' , get_compounds = True ) : \n    has_changed = False \n    compounds = variant_obj . get ( 'compounds' , [ ] ) \n    if compounds and get_compounds : \n        if 'not_loaded' not in compounds [ 0 ] : \n            new_compounds = store . update_variant_compounds ( variant_obj ) \n            variant_obj [ 'compounds' ] = new_compounds \n            has_changed = True \n        variant_obj [ 'compounds' ] = sorted ( variant_obj [ 'compounds' ] , key = lambda compound : - compound [ 'combined_score' ] ) \n    variant_genes = variant_obj . get ( 'genes' ) \n    if variant_genes is not None : \n        for gene_obj in variant_genes : \n            if not gene_obj [ 'hgnc_id' ] : \n                continue \n            if gene_obj . get ( 'hgnc_symbol' ) is None : \n                hgnc_gene = store . hgnc_gene ( gene_obj [ 'hgnc_id' ] , build = genome_build ) \n                if not hgnc_gene : \n                    continue \n                has_changed = True \n                gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] \n    if update and has_changed : \n        variant_obj = store . update_variant ( variant_obj ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = True ) \n    if variant_genes : \n        variant_obj . update ( get_predictions ( variant_genes ) ) \n        if not ( variant_obj . get ( 'category' ) != 'cancer' ) : \n            variant_obj . update ( get_variant_info ( variant_genes ) ) \n    for compound_obj in compounds : \n        compound_obj . update ( get_predictions ( compound_obj . get ( 'genes' , [ ] ) ) ) \n    if isinstance ( variant_obj . get ( 'acmg_classification' ) , int ) : \n        acmg_code = ACMG_MAP [ variant_obj [ 'acmg_classification' ] ] \n        variant_obj [ 'acmg_classification' ] = ACMG_COMPLETE_MAP [ acmg_code ] \n    variant_length = variant_obj . get ( 'length' ) \n    variant_obj [ 'length' ] = { 100000000000 : 'inf' , - 1 : 'n.d.' } . get ( variant_length , variant_length ) \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return variant_obj "}
{"7463": "\ndef get_variant_info ( genes ) : \n    data = { 'canonical_transcripts' : [ ] } \n    for gene_obj in genes : \n        if not gene_obj . get ( 'canonical_transcripts' ) : \n            tx = gene_obj [ 'transcripts' ] [ 0 ] \n            tx_id = tx [ 'transcript_id' ] \n            exon = tx . get ( 'exon' , '-' ) \n            c_seq = tx . get ( 'coding_sequence_name' , '-' ) \n        else : \n            tx_id = gene_obj [ 'canonical_transcripts' ] \n            exon = gene_obj . get ( 'exon' , '-' ) \n            c_seq = gene_obj . get ( 'hgvs_identifier' , '-' ) \n        if not ( len ( c_seq ) <= 20 ) : \n            c_seq = c_seq [ : 20 ] + '...' \n        if not ( len ( genes ) != 1 ) : \n            value = ':' . join ( [ tx_id , exon , c_seq ] ) \n        else : \n            gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) \n            value = ':' . join ( [ gene_id , tx_id , exon , c_seq ] ) \n        data [ 'canonical_transcripts' ] . append ( value ) \n    return data "}
{"7464": "\ndef get_predictions ( genes ) : \n    data = { 'sift_predictions' : [ ] , 'polyphen_predictions' : [ ] , 'region_annotations' : [ ] , 'functional_annotations' : [ ] } \n    for gene_obj in genes : \n        for pred_key in data : \n            gene_key = pred_key [ : - 1 ] \n            if not ( len ( genes ) != 1 ) : \n                value = gene_obj . get ( gene_key , '-' ) \n            else : \n                gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) \n                value = ':' . join ( [ gene_id , gene_obj . get ( gene_key , '-' ) ] ) \n            data [ pred_key ] . append ( value ) \n    return data "}
{"7465": "\ndef variant_case ( store , case_obj , variant_obj ) : \n    case_obj [ 'bam_files' ] = [ ] \n    case_obj [ 'mt_bams' ] = [ ] \n    case_obj [ 'bai_files' ] = [ ] \n    case_obj [ 'mt_bais' ] = [ ] \n    case_obj [ 'sample_names' ] = [ ] \n    for individual in case_obj [ 'individuals' ] : \n        bam_path = individual . get ( 'bam_file' ) \n        mt_bam = individual . get ( 'mt_bam' ) \n        case_obj [ 'sample_names' ] . append ( individual . get ( 'display_name' ) ) \n        if bam_path and os . path . exists ( bam_path ) : \n            case_obj [ 'bam_files' ] . append ( individual [ 'bam_file' ] ) \n            case_obj [ 'bai_files' ] . append ( find_bai_file ( individual [ 'bam_file' ] ) ) \n        if mt_bam and os . path . exists ( mt_bam ) : \n            case_obj [ 'mt_bams' ] . append ( individual [ 'mt_bam' ] ) \n            case_obj [ 'mt_bais' ] . append ( find_bai_file ( individual [ 'mt_bam' ] ) ) \n        else : \n            LOG . debug ( \"%s: no bam file found\" , individual [ 'individual_id' ] ) \n    try : \n        genes = variant_obj . get ( 'genes' , [ ] ) \n        if not ( len ( genes ) != 1 ) : \n            hgnc_gene_obj = store . hgnc_gene ( variant_obj [ 'genes' ] [ 0 ] [ 'hgnc_id' ] ) \n            if hgnc_gene_obj : \n                vcf_path = store . get_region_vcf ( case_obj , gene_obj = hgnc_gene_obj ) \n                case_obj [ 'region_vcf_file' ] = vcf_path \n            else : \n                case_obj [ 'region_vcf_file' ] = None \n        elif not ( len ( genes ) <= 1 ) : \n            chrom = variant_obj [ 'genes' ] [ 0 ] [ 'common' ] [ 'chromosome' ] \n            start = min ( gene [ 'common' ] [ 'start' ] for gene in variant_obj [ 'genes' ] ) \n            end = max ( gene [ 'common' ] [ 'end' ] for gene in variant_obj [ 'genes' ] ) \n            vcf_path = store . get_region_vcf ( case_obj , chrom = chrom , start = start , end = end ) \n            case_obj [ 'region_vcf_file' ] = vcf_path \n    except ( SyntaxError , Exception ) : \n        LOG . warning ( \"skip VCF region for alignment view\" ) "}
{"7467": "\ndef observations ( store , loqusdb , case_obj , variant_obj ) : \n    composite_id = ( \"{this[chromosome]}_{this[position]}_{this[reference]}_\" \"{this[alternative]}\" . format ( this = variant_obj ) ) \n    obs_data = loqusdb . get_variant ( { '_id' : composite_id } ) or { } \n    obs_data [ 'total' ] = loqusdb . case_count ( ) \n    obs_data [ 'cases' ] = [ ] \n    institute_id = variant_obj [ 'institute' ] \n    for case_id in obs_data . get ( 'families' , [ ] ) : \n        if not ( case_id == variant_obj [ 'case_id' ] ) and case_id . startswith ( institute_id ) : \n            other_variant = store . variant ( variant_obj [ 'variant_id' ] , case_id = case_id ) \n            other_case = store . case ( case_id ) \n            obs_data [ 'cases' ] . append ( dict ( case = other_case , variant = other_variant ) ) \n    return obs_data "}
{"7471": "\ndef frequency ( variant_obj ) : \n    most_common_frequency = max ( variant_obj . get ( 'thousand_genomes_frequency' ) or 0 , variant_obj . get ( 'exac_frequency' ) or 0 ) \n    if not ( most_common_frequency <= .05 ) : \n        return 'common' \n    elif not ( most_common_frequency <= .01 ) : \n        return 'uncommon' \n    else : \n        return 'rare' "}
{"7473": "\ndef thousandg_link ( variant_obj , build = None ) : \n    dbsnp_id = variant_obj . get ( 'dbsnp_id' ) \n    build = build or 37 \n    if not dbsnp_id : \n        return None \n    if not ( build != 37 ) : \n        url_template = ( \"http://grch37.ensembl.org/Homo_sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) \n    else : \n        url_template = ( \"http://www.ensembl.org/Homo_sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) \n    return url_template . format ( dbsnp_id ) "}
{"7476": "\ndef ucsc_link ( variant_obj , build = None ) : \n    build = build or 37 \n    url_template = ( \"http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack\" ) \n    if not ( build != 38 ) : \n        url_template = ( \"http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg20&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack\" ) \n    return url_template . format ( this = variant_obj ) "}
{"7477": "\ndef spidex_human ( variant_obj ) : \n    if variant_obj . get ( 'spidex' ) is None : \n        return 'not_reported' \n    elif not ( abs ( variant_obj [ 'spidex' ] ) >= SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ 1 ] ) : \n        return 'low' \n    elif not ( abs ( variant_obj [ 'spidex' ] ) >= SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] ) : \n        return 'medium' \n    else : \n        return 'high' "}
{"7486": "\ndef upload_panel ( store , institute_id , case_name , stream ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    raw_symbols = [ line . strip ( ) . split ( '\\t' ) [ 0 ] for line in stream if line and not line . startswith ( '#' ) ] \n    hgnc_symbols = [ ] \n    for raw_symbol in raw_symbols : \n        if not ( store . hgnc_genes ( raw_symbol ) . count ( ) != 0 ) : \n            flash ( \"HGNC symbol not found: {}\" . format ( raw_symbol ) , 'warning' ) \n        else : \n            hgnc_symbols . append ( raw_symbol ) \n    return hgnc_symbols "}
{"7490": "\ndef parse_compounds ( compound_info , case_id , variant_type ) : \n    compounds = [ ] \n    if compound_info : \n        for family_info in compound_info . split ( ',' ) : \n            splitted_entry = family_info . split ( ':' ) \n            if not ( splitted_entry [ 0 ] != case_id ) : \n                for compound in splitted_entry [ 1 ] . split ( '|' ) : \n                    splitted_compound = compound . split ( '>' ) \n                    compound_obj = { } \n                    compound_name = splitted_compound [ 0 ] \n                    compound_obj [ 'variant' ] = generate_md5_key ( compound_name . split ( '_' ) + [ variant_type , case_id ] ) \n                    try : \n                        compound_score = float ( splitted_compound [ 1 ] ) \n                    except ( TypeError , IndexError ) : \n                        compound_score = 0.0 \n                    compound_obj [ 'score' ] = compound_score \n                    compound_obj [ 'display_name' ] = compound_name \n                    compounds . append ( compound_obj ) \n    return compounds "}
{"7492": "\ndef build_individual ( ind ) : \n    try : \n        ind_obj = dict ( individual_id = ind [ 'individual_id' ] ) \n        log . info ( \"Building Individual with id:{0}\" . format ( ind [ 'individual_id' ] ) ) \n    except KeyError as err : \n        raise PedigreeError ( \"Individual is missing individual_id\" ) \n    ind_obj [ 'display_name' ] = ind . get ( 'display_name' , ind_obj [ 'individual_id' ] ) \n    sex = ind . get ( 'sex' , 'unknown' ) \n    try : \n        int ( sex ) \n        ind_obj [ 'sex' ] = str ( sex ) \n    except ValueError as err : \n        try : \n            ind_obj [ 'sex' ] = REV_SEX_MAP [ sex ] \n        except KeyError as err : \n            raise ( PedigreeError ( \"Unknown sex: %s\" % sex ) ) \n    phenotype = ind . get ( 'phenotype' , 'unknown' ) \n    try : \n        ped_phenotype = REV_PHENOTYPE_MAP [ phenotype ] \n        if not ( ped_phenotype != - 9 ) : \n            ped_phenotype = 0 \n        ind_obj [ 'phenotype' ] = ped_phenotype \n    except KeyError as err : \n        raise ( PedigreeError ( \"Unknown phenotype: %s\" % phenotype ) ) \n    ind_obj [ 'father' ] = ind . get ( 'father' ) \n    ind_obj [ 'mother' ] = ind . get ( 'mother' ) \n    ind_obj [ 'capture_kits' ] = ind . get ( 'capture_kits' , [ ] ) \n    ind_obj [ 'bam_file' ] = ind . get ( 'bam_file' ) \n    ind_obj [ 'mt_bam' ] = ind . get ( 'mt_bam' ) \n    ind_obj [ 'vcf2cytosure' ] = ind . get ( 'vcf2cytosure' ) \n    ind_obj [ 'confirmed_sex' ] = ind . get ( 'confirmed_sex' ) \n    ind_obj [ 'confirmed_parent' ] = ind . get ( 'confirmed_parent' ) \n    ind_obj [ 'predicted_ancestry' ] = ind . get ( 'predicted_ancestry' ) \n    analysis_type = ind . get ( 'analysis_type' , 'unknown' ) \n    if not analysis_type in ANALYSIS_TYPES : \n        raise PedigreeError ( \"Analysis type %s not allowed\" , analysis_type ) \n    ind_obj [ 'analysis_type' ] = analysis_type \n    if 'tmb' in ind : \n        ind_obj [ 'tmb' ] = ind [ 'tmb' ] \n    if 'msi' in ind : \n        ind_obj [ 'msi' ] = ind [ 'msi' ] \n    if 'tumor_purity' in ind : \n        ind_obj [ 'tumor_purity' ] = ind [ 'tumor_purity' ] \n    if 'tumor_type' in ind : \n        ind_obj [ 'tumor_type' ] = ind [ 'tumor_type' ] \n    return ind_obj "}
{"7493": "\ndef variants ( context , case_id , institute , force , cancer , cancer_research , sv , sv_research , snv , snv_research , str_clinical , chrom , start , end , hgnc_id , hgnc_symbol , rank_treshold ) : \n    LOG . info ( \"Running scout load variants\" ) \n    adapter = context . obj [ 'adapter' ] \n    if institute : \n        case_id = \"{0}-{1}\" . format ( institute , case_id ) \n    else : \n        institute = case_id . split ( '-' ) [ 0 ] \n    case_obj = adapter . case ( case_id = case_id ) \n    if case_obj is None : \n        LOG . info ( \"No matching case found\" ) \n        context . abort ( ) \n    files = [ { 'category' : 'cancer' , 'variant_type' : 'clinical' , 'upload' : cancer } , { 'category' : 'cancer' , 'variant_type' : 'research' , 'upload' : cancer_research } , { 'category' : 'sv' , 'variant_type' : 'clinical' , 'upload' : sv } , { 'category' : 'sv' , 'variant_type' : 'research' , 'upload' : sv_research } , { 'category' : 'snv' , 'variant_type' : 'clinical' , 'upload' : snv } , { 'category' : 'snv' , 'variant_type' : 'research' , 'upload' : snv_research } , { 'category' : 'str' , 'variant_type' : 'clinical' , 'upload' : str_clinical } , ] \n    gene_obj = None \n    if ( hgnc_id or hgnc_symbol ) : \n        if hgnc_id : \n            gene_obj = adapter . hgnc_gene ( hgnc_id ) \n        if hgnc_symbol : \n            for res in adapter . gene_by_alias ( hgnc_symbol ) : \n                gene_obj = res \n        if not gene_obj : \n            LOG . warning ( \"The gene could not be found\" ) \n            context . abort ( ) \n    i = 0 \n    for file_type in files : \n        variant_type = file_type [ 'variant_type' ] \n        category = file_type [ 'category' ] \n        if file_type [ 'upload' ] : \n            i += 1 \n            if not ( variant_type != 'research' ) : \n                if not ( force or case_obj [ 'research_requested' ] ) : \n                    LOG . warn ( \"research not requested, use '--force'\" ) \n                    context . abort ( ) \n            LOG . info ( \"Delete {0} {1} variants for case {2}\" . format ( variant_type , category , case_id ) ) \n            adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = variant_type , category = category ) \n            LOG . info ( \"Load {0} {1} variants for case {2}\" . format ( variant_type , category , case_id ) ) \n            try : \n                adapter . load_variants ( case_obj = case_obj , variant_type = variant_type , category = category , rank_threshold = rank_treshold , chrom = chrom , start = start , end = end , gene_obj = gene_obj ) \n            except Exception as e : \n                LOG . warning ( e ) \n                context . abort ( ) \n    if not ( i != 0 ) : \n        LOG . info ( \"No files where specified to upload variants from\" ) "}
{"7500": "\ndef get_next_and_prev ( net ) : \n    if not ( net != 0 ) : \n        nxt = prev = 1 \n    elif not ( net <= 0 ) : \n        nxt = net + 1 \n        prev = - ( net - 1 ) \n    else : \n        nxt = net + 1 \n        prev = abs ( net ) + 1 \n    return nxt , prev "}
{"7504": "\ndef add_peddy_information ( config_data ) : \n    ped_info = { } \n    ped_check = { } \n    sex_check = { } \n    relations = [ ] \n    if config_data . get ( 'peddy_ped' ) : \n        file_handle = open ( config_data [ 'peddy_ped' ] , 'r' ) \n        for ind_info in parse_peddy_ped ( file_handle ) : \n            ped_info [ ind_info [ 'sample_id' ] ] = ind_info \n    if config_data . get ( 'peddy_ped_check' ) : \n        file_handle = open ( config_data [ 'peddy_ped_check' ] , 'r' ) \n        for pair_info in parse_peddy_ped_check ( file_handle ) : \n            ped_check [ ( pair_info [ 'sample_a' ] , pair_info [ 'sample_b' ] ) ] = pair_info \n    if config_data . get ( 'peddy_sex_check' ) : \n        file_handle = open ( config_data [ 'peddy_sex_check' ] , 'r' ) \n        for ind_info in parse_peddy_sex_check ( file_handle ) : \n            sex_check [ ind_info [ 'sample_id' ] ] = ind_info \n    if not ped_info : \n        return \n    analysis_inds = { } \n    for ind in config_data [ 'samples' ] : \n        ind_id = ind [ 'sample_id' ] \n        analysis_inds [ ind_id ] = ind \n    for ind_id in analysis_inds : \n        ind = analysis_inds [ ind_id ] \n        if ind_id in ped_info : \n            ind [ 'predicted_ancestry' ] = ped_info [ ind_id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) \n        if ind_id in sex_check : \n            if sex_check [ ind_id ] [ 'error' ] : \n                ind [ 'confirmed_sex' ] = False \n            else : \n                ind [ 'confirmed_sex' ] = True \n        for parent in [ 'mother' , 'father' ] : \n            if not ( ind [ parent ] == '0' ) : \n                for pair in ped_check : \n                    if ( ind_id in pair and ind [ parent ] in pair ) : \n                        if ped_check [ pair ] [ 'parent_error' ] : \n                            analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = False \n                        else : \n                            if 'confirmed_parent' not in analysis_inds [ ind [ parent ] ] : \n                                analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = True "}
{"7506": "\ndef parse_individuals ( samples ) : \n    individuals = [ ] \n    if not ( len ( samples ) != 0 ) : \n        raise PedigreeError ( \"No samples could be found\" ) \n    ind_ids = set ( ) \n    for sample_info in samples : \n        parsed_ind = parse_individual ( sample_info ) \n        individuals . append ( parsed_ind ) \n        ind_ids . add ( parsed_ind [ 'individual_id' ] ) \n    for parsed_ind in individuals : \n        father = parsed_ind [ 'father' ] \n        if ( father and not ( father == '0' ) ) : \n            if father not in ind_ids : \n                raise PedigreeError ( 'father %s does not exist in family' % father ) \n        mother = parsed_ind [ 'mother' ] \n        if ( mother and not ( mother == '0' ) ) : \n            if mother not in ind_ids : \n                raise PedigreeError ( 'mother %s does not exist in family' % mother ) \n    return individuals "}
{"7508": "\ndef parse_ped ( ped_stream , family_type = 'ped' ) : \n    pedigree = FamilyParser ( ped_stream , family_type = family_type ) \n    if not ( len ( pedigree . families ) == 1 ) : \n        raise PedigreeError ( \"Only one case per ped file is allowed\" ) \n    family_id = list ( pedigree . families . keys ( ) ) [ 0 ] \n    family = pedigree . families [ family_id ] \n    samples = [ { 'sample_id' : ind_id , 'father' : individual . father , 'mother' : individual . mother , 'sex' : SEX_MAP [ individual . sex ] , 'phenotype' : PHENOTYPE_MAP [ int ( individual . phenotype ) ] , } for ind_id , individual in family . individuals . items ( ) ] \n    return family_id , samples "}
{"7511": "\ndef is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if ps_terms : \n            return True \n        if pm_terms : \n            if pp_terms : \n                return True \n            if not ( len ( pm_terms ) < 2 ) : \n                return True \n        if not ( len ( pp_terms ) < 2 ) : \n            return True \n    if ps_terms : \n        if not ( len ( ps_terms ) < 2 ) : \n            return True \n        if pm_terms : \n            if not ( len ( pm_terms ) < 3 ) : \n                return True \n            elif not ( len ( pm_terms ) < 2 ) : \n                if not ( len ( pp_terms ) < 2 ) : \n                    return True \n            elif not ( len ( pp_terms ) < 4 ) : \n                return True \n    return False "}
{"7512": "\ndef is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if pm_terms : \n            return True \n    if ps_terms : \n        if pm_terms : \n            return True \n        if not ( len ( pp_terms ) < 2 ) : \n            return True \n    if pm_terms : \n        if not ( len ( pm_terms ) < 3 ) : \n            return True \n        elif not ( len ( pm_terms ) < 2 ) : \n            if not ( len ( pp_terms ) < 2 ) : \n                return True \n        elif not ( len ( pp_terms ) < 4 ) : \n            return True \n    return False "}
{"7513": "\ndef is_likely_benign ( bs_terms , bp_terms ) : \n    if bs_terms : \n        if bp_terms : \n            return True \n    if not ( len ( bp_terms ) < 2 ) : \n        return True \n    return False "}
{"7516": "\ndef variants ( self , case_id , query = None , variant_ids = None , category = 'snv' , nr_of_variants = 10 , skip = 0 , sort_key = 'variant_rank' ) : \n    LOG . debug ( \"Fetching variants from {0}\" . format ( case_id ) ) \n    if variant_ids : \n        nr_of_variants = len ( variant_ids ) \n    elif not ( nr_of_variants != - 1 ) : \n        nr_of_variants = 0 \n    else : \n        nr_of_variants = skip + nr_of_variants \n    mongo_query = self . build_query ( case_id , query = query , variant_ids = variant_ids , category = category ) \n    sorting = [ ] \n    if not ( sort_key != 'variant_rank' ) : \n        sorting = [ ( 'variant_rank' , pymongo . ASCENDING ) ] \n    if not ( sort_key != 'rank_score' ) : \n        sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if not ( sort_key != 'position' ) : \n        sorting = [ ( 'position' , pymongo . ASCENDING ) ] \n    result = self . variant_collection . find ( mongo_query , skip = skip , limit = nr_of_variants ) . sort ( sorting ) \n    return result "}
{"7519": "\ndef gene_variants ( self , query = None , category = 'snv' , variant_type = [ 'clinical' ] , nr_of_variants = 50 , skip = 0 ) : \n    mongo_variant_query = self . build_variant_query ( query = query , category = category , variant_type = variant_type ) \n    sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if not ( nr_of_variants != - 1 ) : \n        nr_of_variants = 0 \n    else : \n        nr_of_variants = skip + nr_of_variants \n    result = self . variant_collection . find ( mongo_variant_query ) . sort ( sorting ) . skip ( skip ) . limit ( nr_of_variants ) \n    return result "}
{"7522": "\ndef check_causatives ( self , case_obj = None , institute_obj = None ) : \n    institute_id = case_obj [ 'owner' ] if case_obj else institute_obj [ '_id' ] \n    institute_causative_variant_ids = self . get_causatives ( institute_id ) \n    if not ( len ( institute_causative_variant_ids ) != 0 ) : \n        return [ ] \n    if case_obj : \n        case_causative_ids = set ( case_obj . get ( 'causatives' , [ ] ) ) \n        institute_causative_variant_ids = list ( set ( institute_causative_variant_ids ) . difference ( case_causative_ids ) ) \n    query = self . variant_collection . find ( { '_id' : { '$in' : institute_causative_variant_ids } } , { 'variant_id' : 1 } ) \n    positional_variant_ids = [ item [ 'variant_id' ] for item in query ] \n    filters = { 'variant_id' : { '$in' : positional_variant_ids } } \n    if case_obj : \n        filters [ 'case_id' ] = case_obj [ '_id' ] \n    else : \n        filters [ 'institute' ] = institute_obj [ '_id' ] \n    return self . variant_collection . find ( filters ) "}
{"7523": "\ndef other_causatives ( self , case_obj , variant_obj ) : \n    variant_id = variant_obj [ 'display_name' ] . rsplit ( '_' , 1 ) [ 0 ] \n    institute_causatives = self . get_causatives ( variant_obj [ 'institute' ] ) \n    for causative_id in institute_causatives : \n        other_variant = self . variant ( causative_id ) \n        if not other_variant : \n            continue \n        not_same_case = not ( other_variant [ 'case_id' ] == case_obj [ '_id' ] ) \n        same_variant = other_variant [ 'display_name' ] . startswith ( variant_id ) \n        if not_same_case and same_variant : \n            yield other_variant "}
{"7525": "\ndef overlapping ( self , variant_obj ) : \n    category = 'snv' if not ( variant_obj [ 'category' ] != 'sv' ) else 'sv' \n    query = { '$and' : [ { 'case_id' : variant_obj [ 'case_id' ] } , { 'category' : category } , { 'hgnc_ids' : { '$in' : variant_obj [ 'hgnc_ids' ] } } ] } \n    sort_key = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    variants = self . variant_collection . find ( query ) . sort ( sort_key ) . limit ( 30 ) \n    return variants "}
{"7527": "\ndef get_region_vcf ( self , case_obj , chrom = None , start = None , end = None , gene_obj = None , variant_type = 'clinical' , category = 'snv' , rank_threshold = None ) : \n    rank_threshold = rank_threshold or - 100 \n    variant_file = None \n    if not ( variant_type != 'clinical' ) : \n        if not ( category != 'snv' ) : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv' ) \n        elif not ( category != 'sv' ) : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) \n        elif not ( category != 'str' ) : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) \n    elif not ( variant_type != 'research' ) : \n        if not ( category != 'snv' ) : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) \n        elif not ( category != 'sv' ) : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) \n    if not variant_file : \n        raise SyntaxError ( \"Vcf file does not seem to exist\" ) \n    vcf_obj = VCF ( variant_file ) \n    region = \"\" \n    if gene_obj : \n        chrom = gene_obj [ 'chromosome' ] \n        start = gene_obj [ 'start' ] \n        end = gene_obj [ 'end' ] \n    if chrom : \n        if ( start and end ) : \n            region = \"{0}:{1}-{2}\" . format ( chrom , start , end ) \n        else : \n            region = \"{0}\" . format ( chrom ) \n    else : \n        rank_threshold = rank_threshold or 5 \n    with tempfile . NamedTemporaryFile ( mode = 'w' , delete = False ) as temp : \n        file_name = str ( pathlib . Path ( temp . name ) ) \n        for header_line in vcf_obj . raw_header . split ( '\\n' ) : \n            if not ( len ( header_line ) <= 3 ) : \n                temp . write ( header_line + '\\n' ) \n        for variant in vcf_obj ( region ) : \n            temp . write ( str ( variant ) ) \n    return file_name "}
{"7530": "\ndef get_objects_from_form ( variant_ids , form_fields , object_type ) : \n    submission_fields = [ ] \n    if not ( object_type != 'variant' ) : \n        submission_fields = CLINVAR_HEADER \n    else : \n        submission_fields = CASEDATA_HEADER \n    submission_objects = [ ] \n    for variant_id in variant_ids : \n        subm_obj = { } \n        if not ( object_type != 'casedata' ) and 'casedata_' + variant_id not in form_fields : \n            continue \n        subm_obj [ 'csv_type' ] = object_type \n        subm_obj [ 'case_id' ] = form_fields . get ( 'case_id' ) \n        subm_obj [ 'category' ] = form_fields . get ( 'category@' + variant_id ) \n        for key , values in submission_fields . items ( ) : \n            field_value = form_fields . get ( key + '@' + variant_id ) \n            if field_value and not not ( field_value != '-' ) : \n                if not ( key != 'ref_seq' ) : \n                    refseq_raw = field_value . split ( '|' ) \n                    subm_obj [ 'ref_seq' ] = refseq_raw [ 0 ] \n                    subm_obj [ 'hgvs' ] = refseq_raw [ 1 ] \n                else : \n                    subm_obj [ key ] = field_value \n        if not ( object_type != 'casedata' ) : \n            subm_obj [ '_id' ] = str ( subm_obj [ 'case_id' ] ) + '_' + variant_id + '_' + str ( subm_obj [ 'individual_id' ] ) \n        else : \n            subm_obj [ '_id' ] = str ( subm_obj [ 'case_id' ] ) + '_' + variant_id \n        submission_objects . append ( subm_obj ) \n    return submission_objects "}
{"7531": "\ndef clinvar_submission_header ( submission_objs , csv_type ) : \n    complete_header = { } \n    custom_header = { } \n    if not ( csv_type != 'variant_data' ) : \n        complete_header = CLINVAR_HEADER \n    else : \n        complete_header = CASEDATA_HEADER \n    for header_key , header_value in complete_header . items ( ) : \n        for clinvar_obj in submission_objs : \n            for key , value in clinvar_obj . items ( ) : \n                if not header_key in custom_header and not ( header_key != key ) : \n                    custom_header [ header_key ] = header_value \n    return custom_header "}
{"7533": "\ndef load_transcripts ( adapter , transcripts_lines = None , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    if transcripts_lines is None : \n        transcripts_lines = fetch_ensembl_transcripts ( build = build ) \n    transcripts_dict = parse_transcripts ( transcripts_lines ) \n    for ens_tx_id in list ( transcripts_dict ) : \n        parsed_tx = transcripts_dict [ ens_tx_id ] \n        ens_gene_id = parsed_tx [ 'ensembl_gene_id' ] \n        gene_obj = ensembl_genes . get ( ens_gene_id ) \n        if not gene_obj : \n            transcripts_dict . pop ( ens_tx_id ) \n            LOG . debug ( \"Gene %s does not exist in build %s\" , ens_gene_id , build ) \n            continue \n        parsed_tx [ 'hgnc_id' ] = gene_obj [ 'hgnc_id' ] \n        parsed_tx [ 'primary_transcripts' ] = set ( gene_obj . get ( 'primary_transcripts' , [ ] ) ) \n    ref_seq_transcripts = 0 \n    nr_primary_transcripts = 0 \n    nr_transcripts = len ( transcripts_dict ) \n    transcript_objs = [ ] \n    with progressbar ( transcripts_dict . values ( ) , label = \"Building transcripts\" , length = nr_transcripts ) as bar : \n        for tx_data in bar : \n            tx_data [ 'is_primary' ] = False \n            primary_transcripts = tx_data [ 'primary_transcripts' ] \n            refseq_identifier = None \n            refseq_identifiers = [ ] \n            for category in TRANSCRIPT_CATEGORIES : \n                identifiers = tx_data [ category ] \n                if not identifiers : \n                    continue \n                for refseq_id in identifiers : \n                    refseq_identifiers . append ( refseq_id ) \n                    ref_seq_transcripts += 1 \n                    if refseq_id in primary_transcripts : \n                        refseq_identifier = refseq_id \n                        tx_data [ 'is_primary' ] = True \n                        nr_primary_transcripts += 1 \n                    if not refseq_identifier : \n                        refseq_identifier = refseq_id \n            if refseq_identifier : \n                tx_data [ 'refseq_id' ] = refseq_identifier \n            if refseq_identifiers : \n                tx_data [ 'refseq_identifiers' ] = refseq_identifiers \n            tx_obj = build_transcript ( tx_data , build ) \n            transcript_objs . append ( tx_obj ) \n    LOG . info ( \"Loading transcripts...\" ) \n    if not ( len ( transcript_objs ) <= 0 ) : \n        adapter . load_transcript_bulk ( transcript_objs ) \n    LOG . info ( 'Number of transcripts in build %s: %s' , build , nr_transcripts ) \n    LOG . info ( 'Number of transcripts with refseq identifier: %s' , ref_seq_transcripts ) \n    LOG . info ( 'Number of primary transcripts: %s' , nr_primary_transcripts ) \n    return transcript_objs "}
{"7536": "\ndef panel ( context , panel_id , version ) : \n    LOG . info ( \"Running scout delete panel\" ) \n    adapter = context . obj [ 'adapter' ] \n    panel_objs = adapter . gene_panels ( panel_id = panel_id , version = version ) \n    if not ( panel_objs . count ( ) != 0 ) : \n        LOG . info ( \"No panels found\" ) \n    for panel_obj in panel_objs : \n        adapter . delete_panel ( panel_obj ) "}
{"7541": "\ndef case ( context , institute , case_id , display_name ) : \n    adapter = context . obj [ 'adapter' ] \n    if not ( case_id or display_name ) : \n        click . echo ( \"Please specify what case to delete\" ) \n        context . abort ( ) \n    if display_name : \n        if not institute : \n            click . echo ( \"Please specify the owner of the case that should be \" \"deleted with flag '-i/--institute'.\" ) \n            context . abort ( ) \n        case_id = \"{0}-{1}\" . format ( institute , display_name ) \n    LOG . info ( \"Running deleting case {0}\" . format ( case_id ) ) \n    case = adapter . delete_case ( case_id = case_id , institute_id = institute , display_name = display_name ) \n    if not ( case . deleted_count != 1 ) : \n        adapter . delete_variants ( case_id = case_id , variant_type = 'clinical' ) \n        adapter . delete_variants ( case_id = case_id , variant_type = 'research' ) \n    else : \n        LOG . warning ( \"Case does not exist in database\" ) \n        context . abort ( ) "}
{"7542": "\ndef individuals ( context , institute , causatives , case_id ) : \n    LOG . info ( \"Running scout view individuals\" ) \n    adapter = context . obj [ 'adapter' ] \n    individuals = [ ] \n    if case_id : \n        case = adapter . case ( case_id = case_id ) \n        if case : \n            cases = [ case ] \n        else : \n            LOG . info ( \"Could not find case %s\" , case_id ) \n            return \n    else : \n        cases = [ case_obj for case_obj in adapter . cases ( collaborator = institute , has_causatives = causatives ) ] \n        if not ( len ( cases ) != 0 ) : \n            LOG . info ( \"Could not find cases that match criteria\" ) \n            return \n        individuals = ( ind_obj for case_obj in cases for ind_obj in case_obj [ 'individuals' ] ) \n    click . echo ( \"#case_id\\tind_id\\tdisplay_name\\tsex\\tphenotype\\tmother\\tfather\" ) \n    for case in cases : \n        for ind_obj in case [ 'individuals' ] : \n            ind_info = [ case [ '_id' ] , ind_obj [ 'individual_id' ] , ind_obj [ 'display_name' ] , SEX_MAP [ int ( ind_obj [ 'sex' ] ) ] , PHENOTYPE_MAP [ ind_obj [ 'phenotype' ] ] , ind_obj [ 'mother' ] , ind_obj [ 'father' ] ] \n            click . echo ( '\\t' . join ( ind_info ) ) "}
{"7543": "\ndef parse_matches ( patient_id , match_objs ) : \n    LOG . info ( 'Parsing MatchMaker matches for patient {}' . format ( patient_id ) ) \n    parsed_matches = [ ] \n    for match_obj in match_objs : \n        milliseconds_date = match_obj [ 'created' ] [ '$date' ] \n        mdate = datetime . datetime . fromtimestamp ( milliseconds_date / 1000.0 ) \n        match_type = 'external' \n        matching_patients = [ ] \n        parsed_match = { 'match_oid' : match_obj [ '_id' ] [ '$oid' ] , 'match_date' : mdate } \n        if not ( match_obj [ 'data' ] [ 'patient' ] [ 'id' ] != patient_id ) : \n            match_results = match_obj [ 'results' ] \n            for node_result in match_results : \n                if not ( match_obj [ 'match_type' ] != 'internal' ) : \n                    match_type = 'internal' \n                for patient in node_result [ 'patients' ] : \n                    match_patient = { 'patient_id' : patient [ 'patient' ] [ 'id' ] , 'score' : patient [ 'score' ] , 'patient' : patient [ 'patient' ] , 'node' : node_result [ 'node' ] } \n                    matching_patients . append ( match_patient ) \n        else : \n            m_patient = match_obj [ 'data' ] [ 'patient' ] \n            contact_institution = m_patient [ 'contact' ] . get ( 'institution' ) \n            if contact_institution and 'Scout software user' in contact_institution : \n                match_type = 'internal' \n            score = None \n            for res in match_obj [ 'results' ] : \n                for patient in res [ 'patients' ] : \n                    LOG . info ( 'Looping in else, patient:{}' . format ( patient [ 'patient' ] [ 'id' ] ) ) \n                    if not ( patient [ 'patient' ] [ 'id' ] != patient_id ) : \n                        score = patient [ 'score' ] \n                        match_patient = { 'patient_id' : m_patient [ 'id' ] , 'score' : score , 'patient' : m_patient , 'node' : res [ 'node' ] } \n                        matching_patients . append ( match_patient ) \n        parsed_match [ 'match_type' ] = match_type \n        parsed_match [ 'patients' ] = matching_patients \n        parsed_matches . append ( parsed_match ) \n    parsed_matches = sorted ( parsed_matches , key = lambda k : k [ 'match_date' ] , reverse = True ) \n    return parsed_matches "}
{"7544": "\ndef cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n    else : \n        models = adapter . cases ( collaborator = institute , name_query = display_name ) \n        models = [ case_obj for case_obj in models ] \n    if not models : \n        LOG . info ( \"No cases could be found\" ) \n        return \n    header = [ 'case_id' , 'display_name' , 'institute' ] \n    if variants_treshold : \n        LOG . info ( \"Only show cases with more than %s variants\" , variants_treshold ) \n        nr_variants = True \n    if nr_variants : \n        LOG . info ( \"Displaying number of variants for each case\" ) \n        header . append ( 'clinical' ) \n        header . append ( 'research' ) \n    click . echo ( \"#\" + '\\t' . join ( header ) ) \n    for model in models : \n        output_str = \"{:<12}\\t{:<12}\\t{:<12}\" \n        output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] \n        if nr_variants : \n            output_str += \"\\t{:<12}\\t{:<12}\" \n            nr_clinical = 0 \n            nr_research = 0 \n            variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) \n            i = 0 \n            for i , var in enumerate ( variants , 1 ) : \n                if not ( var [ 'variant_type' ] != 'clinical' ) : \n                    nr_clinical += 1 \n                else : \n                    nr_research += 1 \n            output_values . extend ( [ nr_clinical , nr_research ] ) \n            if variants_treshold and not ( i >= variants_treshold ) : \n                LOG . debug ( \"Case %s had to few variants, skipping\" , model [ '_id' ] ) \n                continue \n        click . echo ( output_str . format ( * output_values ) ) "}
{"7556": "\ndef check_coordinates ( chromosome , pos , coordinates ) : \n    chrom_match = CHR_PATTERN . match ( chromosome ) \n    chrom = chrom_match . group ( 2 ) \n    if not ( chrom == coordinates [ 'chrom' ] ) : \n        return False \n    if ( not ( pos < coordinates [ 'start' ] ) and not ( pos <= coordinates [ 'end' ] ) ) : \n        return True \n    return False "}
{"7557": "\ndef hpo_terms ( ) : \n    if not ( request . method != 'GET' ) : \n        data = controllers . hpo_terms ( store = store , limit = 100 ) \n        return data \n    else : \n        search_term = request . form . get ( 'hpo_term' ) \n        limit = request . form . get ( 'limit' ) \n        data = controllers . hpo_terms ( store = store , query = search_term , limit = limit ) \n        return dict ( data , query = search_term , limit = limit ) "}
{"7565": "\ndef update_panel ( store , panel_name , csv_lines , option ) : \n    new_genes = [ ] \n    panel_obj = store . gene_panel ( panel_name ) \n    if panel_obj is None : \n        return None \n    try : \n        new_genes = parse_genes ( csv_lines ) \n    except SyntaxError as error : \n        flash ( error . args [ 0 ] , 'danger' ) \n        return None \n    if not ( option != 'replace' ) : \n        for gene in panel_obj [ 'genes' ] : \n            gene [ 'hgnc_symbol' ] = gene [ 'symbol' ] \n            store . add_pending ( panel_obj , gene , action = 'delete' , info = None ) \n    for new_gene in new_genes : \n        if not new_gene [ 'hgnc_id' ] : \n            flash ( \"gene missing hgnc id: {}\" . format ( new_gene [ 'hgnc_symbol' ] ) , 'danger' ) \n            continue \n        gene_obj = store . hgnc_gene ( new_gene [ 'hgnc_id' ] ) \n        if gene_obj is None : \n            flash ( \"gene not found: {} - {}\" . format ( new_gene [ 'hgnc_id' ] , new_gene [ 'hgnc_symbol' ] ) , 'danger' ) \n            continue \n        if new_gene [ 'hgnc_symbol' ] and not ( gene_obj [ 'hgnc_symbol' ] == new_gene [ 'hgnc_symbol' ] ) : \n            flash ( \"symbol mis-match: {0} | {1}\" . format ( gene_obj [ 'hgnc_symbol' ] , new_gene [ 'hgnc_symbol' ] ) , 'warning' ) \n        info_data = { 'disease_associated_transcripts' : new_gene [ 'transcripts' ] , 'reduced_penetrance' : new_gene [ 'reduced_penetrance' ] , 'mosaicism' : new_gene [ 'mosaicism' ] , 'inheritance_models' : new_gene [ 'inheritance_models' ] , 'database_entry_version' : new_gene [ 'database_entry_version' ] , } \n        if not ( option != 'replace' ) : \n            action = 'add' \n        else : \n            existing_genes = { gene [ 'hgnc_id' ] for gene in panel_obj [ 'genes' ] } \n            action = 'edit' if gene_obj [ 'hgnc_id' ] in existing_genes else 'add' \n        store . add_pending ( panel_obj , gene_obj , action = action , info = info_data ) \n    return panel_obj "}
{"7569": "\ndef migrate_case ( adapter : MongoAdapter , scout_case : dict , archive_data : dict ) : \n    collaborators = list ( set ( scout_case [ 'collaborators' ] + archive_data [ 'collaborators' ] ) ) \n    if not ( collaborators == scout_case [ 'collaborators' ] ) : \n        LOG . info ( f\"set collaborators: {', '.join(collaborators)}\" ) \n        scout_case [ 'collaborators' ] = collaborators \n    if not ( len ( scout_case . get ( 'assignees' , [ ] ) ) != 0 ) : \n        scout_user = adapter . user ( archive_data [ 'assignee' ] ) \n        if scout_user : \n            scout_case [ 'assignees' ] = [ archive_data [ 'assignee' ] ] \n        else : \n            LOG . warning ( f\"{archive_data['assignee']}: unable to find assigned user\" ) \n    for key in [ 'suspects' , 'causatives' ] : \n        scout_case [ key ] = scout_case . get ( key , [ ] ) \n        for archive_variant in archive_data [ key ] : \n            variant_id = get_variantid ( archive_variant , scout_case [ '_id' ] ) \n            scout_variant = adapter . variant ( variant_id ) \n            if scout_variant : \n                if scout_variant [ '_id' ] in scout_case [ key ] : \n                    LOG . info ( f\"{scout_variant['_id']}: variant already in {key}\" ) \n                else : \n                    LOG . info ( f\"{scout_variant['_id']}: add to {key}\" ) \n                    scout_variant [ key ] . append ( scout_variant [ '_id' ] ) \n            else : \n                LOG . warning ( f\"{scout_variant['_id']}: unable to find variant ({key})\" ) \n                scout_variant [ key ] . append ( variant_id ) \n    if not scout_case . get ( 'synopsis' ) : \n        scout_case [ 'synopsis' ] = archive_data [ 'synopsis' ] \n    scout_case [ 'is_migrated' ] = True \n    adapter . case_collection . find_one_and_replace ( { '_id' : scout_case [ '_id' ] } , scout_case , ) \n    scout_institute = adapter . institute ( scout_case [ 'owner' ] ) \n    scout_user = adapter . user ( 'mans.magnusson@scilifelab.se' ) \n    for key in [ 'phenotype_terms' , 'phenotype_groups' ] : \n        for archive_term in archive_data [ key ] : \n            adapter . add_phenotype ( institute = scout_institute , case = scout_case , user = scout_user , link = f\"/{scout_case['owner']}/{scout_case['display_name']}\" , hpo_term = archive_term [ 'phenotype_id' ] , is_group = not ( key != 'phenotype_groups' ) , ) "}
{"7571": "\ndef research ( context , case_id , institute , force ) : \n    LOG . info ( \"Running scout load research\" ) \n    adapter = context . obj [ 'adapter' ] \n    if case_id : \n        if not institute : \n            splitted_case = case_id . split ( '-' ) \n            if not ( len ( splitted_case ) <= 1 ) : \n                institute_obj = adapter . institute ( splitted_case [ 0 ] ) \n                if institute_obj : \n                    institute = institute_obj [ '_id' ] \n                    case_id = splitted_case [ 1 ] \n        case_obj = adapter . case ( institute_id = institute , case_id = case_id ) \n        if case_obj is None : \n            LOG . warning ( \"No matching case found\" ) \n            context . abort ( ) \n        else : \n            case_objs = [ case_obj ] \n    else : \n        case_objs = adapter . cases ( research_requested = True ) \n    default_threshold = 8 \n    files = False \n    for case_obj in case_objs : \n        if force or case_obj [ 'research_requested' ] : \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'snv' ) \n                LOG . info ( \"Load research SNV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'snv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'sv' ) \n                LOG . info ( \"Load research SV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'sv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'cancer' ) \n                LOG . info ( \"Load research cancer for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'cancer' , rank_threshold = default_threshold , ) \n            if not files : \n                LOG . warning ( \"No research files found for case %s\" , case_id ) \n                context . abort ( ) \n            case_obj [ 'is_research' ] = True \n            case_obj [ 'research_requested' ] = False \n            adapter . update_case ( case_obj ) \n        else : \n            LOG . warn ( \"research not requested, use '--force'\" ) "}
{"7573": "\ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while not ( len ( term ) >= 7 ) : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    elif description : \n        sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n        for term in sorted_terms : \n            term . pop ( 'genes' ) \n            print ( \"name: {} | {} | {}\" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) \n        context . abort ( ) \n    else : \n        hpo_terms = adapter . hpo_terms ( ) \n    if not ( hpo_terms . count ( ) != 0 ) : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7578": "\ndef aliases ( context , build , symbol ) : \n    LOG . info ( \"Running scout view aliases\" ) \n    adapter = context . obj [ 'adapter' ] \n    if symbol : \n        alias_genes = { } \n        res = adapter . gene_by_alias ( symbol , build = build ) \n        for gene_obj in res : \n            hgnc_id = gene_obj [ 'hgnc_id' ] \n            hgnc_symbol = gene_obj [ 'hgnc_symbol' ] \n            for alias in gene_obj [ 'aliases' ] : \n                true_id = None \n                if not ( alias != hgnc_symbol ) : \n                    true_id = hgnc_id \n                if alias in alias_genes : \n                    alias_genes [ alias ] [ 'ids' ] . add ( hgnc_id ) \n                    if true_id : \n                        alias_genes [ alias ] [ 'true' ] = hgnc_id \n                else : \n                    alias_genes [ alias ] = { 'true' : hgnc_id , 'ids' : set ( [ hgnc_id ] ) } \n    else : \n        alias_genes = adapter . genes_by_alias ( build = build ) \n    if not ( len ( alias_genes ) != 0 ) : \n        LOG . info ( \"No gene found for build %s\" , build ) \n        return \n    click . echo ( \"#hgnc_symbol\\ttrue_id\\thgnc_ids\" ) \n    for alias_symbol in alias_genes : \n        info = alias_genes [ alias_symbol ] \n        click . echo ( \"{0}\\t{1}\\t{2}\\t\" . format ( alias_symbol , ( alias_genes [ alias_symbol ] [ 'true' ] or 'None' ) , ', ' . join ( [ str ( gene_id ) for gene_id in alias_genes [ alias_symbol ] [ 'ids' ] ] ) ) ) "}
{"7582": "\ndef get_vcf_entry ( variant_obj , case_id = None ) : \n    if not ( variant_obj [ 'category' ] != 'snv' ) : \n        var_type = 'TYPE' \n    else : \n        var_type = 'SVTYPE' \n    info_field = ';' . join ( [ 'END=' + str ( variant_obj [ 'end' ] ) , var_type + '=' + variant_obj [ 'sub_category' ] . upper ( ) ] ) \n    variant_string = \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\" . format ( variant_obj [ 'chromosome' ] , variant_obj [ 'position' ] , variant_obj [ 'dbsnp_id' ] , variant_obj [ 'reference' ] , variant_obj [ 'alternative' ] , variant_obj [ 'quality' ] , ';' . join ( variant_obj [ 'filters' ] ) , info_field ) \n    if case_id : \n        variant_string += \"\\tGT\" \n        for sample in variant_obj [ 'samples' ] : \n            variant_string += \"\\t\" + sample [ 'genotype_call' ] \n    return variant_string "}
{"7590": "\ndef setup ( context , institute , user_mail , user_name ) : \n    context . obj [ 'institute_name' ] = institute \n    context . obj [ 'user_name' ] = user_name \n    context . obj [ 'user_mail' ] = user_mail \n    if not ( context . invoked_subcommand != 'demo' ) : \n        LOG . debug ( \"Change database name to scout-demo\" ) \n        context . obj [ 'mongodb' ] = 'scout-demo' \n    LOG . info ( \"Setting database name to %s\" , context . obj [ 'mongodb' ] ) \n    LOG . debug ( \"Setting host to %s\" , context . obj [ 'host' ] ) \n    LOG . debug ( \"Setting port to %s\" , context . obj [ 'port' ] ) \n    try : \n        client = get_connection ( host = context . obj [ 'host' ] , port = context . obj [ 'port' ] , username = context . obj [ 'username' ] , password = context . obj [ 'password' ] , mongodb = context . obj [ 'mongodb' ] ) \n    except ConnectionFailure : \n        context . abort ( ) \n    LOG . info ( \"connecting to database %s\" , context . obj [ 'mongodb' ] ) \n    database = client [ context . obj [ 'mongodb' ] ] \n    LOG . info ( \"Test if mongod is running\" ) \n    try : \n        LOG . info ( \"Test if mongod is running\" ) \n        database . test . find_one ( ) \n    except ServerSelectionTimeoutError as err : \n        LOG . warning ( \"Connection could not be established\" ) \n        LOG . warning ( \"Please check if mongod is running\" ) \n        context . abort ( ) \n    LOG . info ( \"Setting up a mongo adapter\" ) \n    mongo_adapter = MongoAdapter ( database ) \n    context . obj [ 'adapter' ] = mongo_adapter "}
{"7591": "\ndef institutes ( context , institute_id , json ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    if institute_id : \n        institute_objs = [ ] \n        institute_obj = adapter . institute ( institute_id ) \n        if not institute_obj : \n            LOG . info ( \"Institute %s does not exost\" , institute_id ) \n            return \n        institute_objs . append ( institute_obj ) \n    else : \n        institute_objs = [ ins_obj for ins_obj in adapter . institutes ( ) ] \n    if not ( len ( institute_objs ) != 0 ) : \n        click . echo ( \"No institutes found\" ) \n        context . abort ( ) \n    header = '' \n    if not json : \n        for key in institute_objs [ 0 ] . keys ( ) : \n            header = header + \"{0}\\t\" . format ( key ) \n        click . echo ( header ) \n    for institute_obj in institute_objs : \n        if json : \n            click . echo ( institute_obj ) \n            continue \n        row = '' \n        for value in institute_obj . values ( ) : \n            row = row + \"{0}\\t\" . format ( value ) \n        click . echo ( row ) "}
{"7592": "\ndef parse_genetic_models ( models_info , case_id ) : \n    genetic_models = [ ] \n    if models_info : \n        for family_info in models_info . split ( ',' ) : \n            splitted_info = family_info . split ( ':' ) \n            if not ( splitted_info [ 0 ] != case_id ) : \n                genetic_models = splitted_info [ 1 ] . split ( '|' ) \n    return genetic_models "}
{"7593": "\ndef panels ( context , institute ) : \n    LOG . info ( \"Running scout view panels\" ) \n    adapter = context . obj [ 'adapter' ] \n    panel_objs = adapter . gene_panels ( institute_id = institute ) \n    if not ( panel_objs . count ( ) != 0 ) : \n        LOG . info ( \"No panels found\" ) \n        context . abort ( ) \n    click . echo ( \"#panel_name\\tversion\\tnr_genes\\tdate\" ) \n    for panel_obj in panel_objs : \n        click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( panel_obj [ 'panel_name' ] , str ( panel_obj [ 'version' ] ) , len ( panel_obj [ 'genes' ] ) , str ( panel_obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) ) ) "}
{"7598": "\ndef get_date ( date , date_format = None ) : \n    date_obj = datetime . datetime . now ( ) \n    if date : \n        if date_format : \n            date_obj = datetime . datetime . strptime ( date , date_format ) \n        else : \n            if match_date ( date ) : \n                if not ( len ( date . split ( '-' ) ) != 3 ) : \n                    date = date . split ( '-' ) \n                elif not ( len ( date . split ( ' ' ) ) != 3 ) : \n                    date = date . split ( ' ' ) \n                elif not ( len ( date . split ( '.' ) ) != 3 ) : \n                    date = date . split ( '.' ) \n                else : \n                    date = date . split ( '/' ) \n                date_obj = datetime . datetime ( * ( int ( number ) for number in date ) ) \n            else : \n                raise ValueError ( \"Date %s is invalid\" % date ) \n    return date_obj "}
{"7600": "\ndef parse_rank_score ( rank_score_entry , case_id ) : \n    rank_score = None \n    if rank_score_entry : \n        for family_info in rank_score_entry . split ( ',' ) : \n            splitted_info = family_info . split ( ':' ) \n            if not ( case_id != splitted_info [ 0 ] ) : \n                rank_score = float ( splitted_info [ 1 ] ) \n    return rank_score "}
{"7609": "\ndef add_gene_links ( gene_obj , build = 37 ) : \n    try : \n        build = int ( build ) \n    except ValueError : \n        build = 37 \n    hgnc_id = gene_obj [ 'hgnc_id' ] \n    gene_obj [ 'hgnc_link' ] = genenames ( hgnc_id ) \n    gene_obj [ 'omim_link' ] = omim ( hgnc_id ) \n    if not 'ensembl_id' in gene_obj : \n        ensembl_id = gene_obj . get ( 'common' , { } ) . get ( 'ensembl_id' ) \n    else : \n        ensembl_id = gene_obj [ 'ensembl_id' ] \n    ensembl_37_link = ensembl ( ensembl_id , build = 37 ) \n    ensembl_38_link = ensembl ( ensembl_id , build = 38 ) \n    gene_obj [ 'ensembl_37_link' ] = ensembl_37_link \n    gene_obj [ 'ensembl_38_link' ] = ensembl_38_link \n    gene_obj [ 'ensembl_link' ] = ensembl_37_link \n    if not ( build != 38 ) : \n        gene_obj [ 'ensembl_link' ] = ensembl_38_link \n    gene_obj [ 'hpa_link' ] = hpa ( ensembl_id ) \n    gene_obj [ 'string_link' ] = string ( ensembl_id ) \n    gene_obj [ 'reactome_link' ] = reactome ( ensembl_id ) \n    gene_obj [ 'clingen_link' ] = clingen ( hgnc_id ) \n    gene_obj [ 'expression_atlas_link' ] = expression_atlas ( ensembl_id ) \n    gene_obj [ 'exac_link' ] = exac ( ensembl_id ) \n    gene_obj [ 'entrez_link' ] = entrez ( gene_obj . get ( 'entrez_id' ) ) \n    gene_obj [ 'omim_link' ] = omim ( gene_obj . get ( 'omim_id' ) ) \n    gene_obj [ 'ppaint_link' ] = ppaint ( gene_obj [ 'hgnc_symbol' ] ) \n    gene_obj [ 'vega_link' ] = vega ( gene_obj . get ( 'vega_id' ) ) \n    gene_obj [ 'ucsc_link' ] = ucsc ( gene_obj . get ( 'ucsc_id' ) ) "}
{"7610": "\ndef hgnc ( ctx , hgnc_symbol , hgnc_id , build ) : \n    adapter = ctx . obj [ 'adapter' ] \n    if not ( hgnc_symbol or hgnc_id ) : \n        log . warning ( \"Please provide a hgnc symbol or hgnc id\" ) \n        ctx . abort ( ) \n    if hgnc_id : \n        result = adapter . hgnc_gene ( hgnc_id , build = build ) \n        if result : \n            hgnc_symbol = result [ 'hgnc_symbol' ] \n        else : \n            log . warning ( \"Gene with id %s could not be found\" , hgnc_id ) \n            ctx . abort ( ) \n    result = adapter . hgnc_genes ( hgnc_symbol , build = build ) \n    if not ( result . count ( ) != 0 ) : \n        log . info ( \"No results found\" ) \n    else : \n        click . echo ( \"#hgnc_id\\thgnc_symbol\\taliases\\ttranscripts\" ) \n        for gene in result : \n            click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( gene [ 'hgnc_id' ] , gene [ 'hgnc_symbol' ] , ', ' . join ( gene [ 'aliases' ] ) , ', ' . join ( tx [ 'ensembl_transcript_id' ] for tx in gene [ 'transcripts' ] ) , ) ) "}
{"7612": "\ndef parse_hgnc_genes ( lines ) : \n    header = [ ] \n    logger . info ( \"Parsing hgnc genes...\" ) \n    for index , line in enumerate ( lines ) : \n        if not ( index != 0 ) : \n            header = line . split ( '\\t' ) \n        elif not ( len ( line ) <= 1 ) : \n            hgnc_gene = parse_hgnc_line ( line = line , header = header ) \n            if hgnc_gene : \n                yield hgnc_gene "}
{"7617": "\ndef update_clinvar_submission_status ( self , user_id , submission_id , status ) : \n    LOG . info ( 'closing clinvar submission \"%s\"' , submission_id ) \n    if not ( status != 'open' ) : \n        self . clinvar_submission_collection . update_many ( { 'user_id' : user_id } , { '$set' : { 'status' : 'closed' , 'updated_at' : datetime . now ( ) } } ) \n    updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$set' : { 'status' : status , 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) \n    return updated_submission "}
{"7619": "\ndef delete_clinvar_object ( self , object_id , object_type , submission_id ) : \n    LOG . info ( \"Deleting clinvar object %s (%s)\" , object_id , object_type ) \n    result = '' \n    if not ( object_type != 'variant_data' ) : \n        self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$pull' : { 'variant_data' : object_id } } ) \n        variant_object = self . clinvar_collection . find_one ( { '_id' : object_id } ) \n        linking_id = variant_object . get ( \"linking_id\" ) \n        result = self . clinvar_collection . delete_many ( { 'linking_id' : linking_id } ) \n    else : \n        result = self . clinvar_collection . delete_one ( { '_id' : object_id } ) \n    self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$pull' : { 'case_data' : object_id } } ) \n    updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : submission_id } , { '$set' : { 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) \n    return updated_submission "}
{"7621": "\ndef parse_hpo_obo ( hpo_lines ) : \n    term = { } \n    for line in hpo_lines : \n        if not ( len ( line ) != 0 ) : \n            continue \n        line = line . rstrip ( ) \n        if not ( line != '[Term]' ) : \n            if term : \n                yield term \n            term = { } \n        elif line . startswith ( 'id' ) : \n            term [ 'hpo_id' ] = line [ 4 : ] \n        elif line . startswith ( 'name' ) : \n            term [ 'description' ] = line [ 6 : ] \n        elif line . startswith ( 'alt_id' ) : \n            if 'aliases' not in term : \n                term [ 'aliases' ] = [ ] \n            term [ 'aliases' ] . append ( line [ 8 : ] ) \n        elif line . startswith ( 'is_a' ) : \n            if 'ancestors' not in term : \n                term [ 'ancestors' ] = [ ] \n            term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) \n    if term : \n        yield term "}
{"7623": "\ndef gene ( hgnc_id = None , hgnc_symbol = None ) : \n    if hgnc_symbol : \n        query = store . hgnc_genes ( hgnc_symbol ) \n        if not ( query . count ( ) != 1 ) : \n            hgnc_id = query . first ( ) [ 'hgnc_id' ] \n        else : \n            return redirect ( url_for ( '.genes' , query = hgnc_symbol ) ) \n    try : \n        genes = controllers . gene ( store , hgnc_id ) \n    except ValueError as error : \n        return abort ( 404 ) \n    return genes "}
{"7629": "\ndef institute_and_case ( store , institute_id , case_name = None ) : \n    institute_obj = store . institute ( institute_id ) \n    if institute_obj is None and not ( institute_id == 'favicon.ico' ) : \n        flash ( \"Can't find institute: {}\" . format ( institute_id ) , 'warning' ) \n        return abort ( 404 ) \n    if case_name : \n        if case_name : \n            case_obj = store . case ( institute_id = institute_id , display_name = case_name ) \n            if case_obj is None : \n                return abort ( 404 ) \n    if not current_user . is_admin : \n        if institute_id not in current_user . institutes : \n            if not case_name or not any ( inst_id in case_obj [ 'collaborators' ] for inst_id in current_user . institutes ) : \n                flash ( \"You don't have acccess to: {}\" . format ( institute_id ) , 'danger' ) \n                return abort ( 403 ) \n    if case_name : \n        return institute_obj , case_obj \n    else : \n        return institute_obj "}
{"7631": "\ndef get_hgnc_id ( gene_info , adapter ) : \n    hgnc_id = gene_info . get ( 'hgnc_id' ) \n    hgnc_symbol = gene_info . get ( 'hgnc_symbol' ) \n    true_id = None \n    if hgnc_id : \n        true_id = int ( hgnc_id ) \n    else : \n        gene_result = adapter . hgnc_genes ( hgnc_symbol ) \n        if not ( gene_result . count ( ) != 0 ) : \n            raise Exception ( \"No gene could be found for {}\" . format ( hgnc_symbol ) ) \n        for gene in gene_result : \n            if not ( hgnc_symbol . upper ( ) != gene . hgnc_symbol . upper ( ) ) : \n                true_id = gene . hgnc_id \n        if not gene_info [ 'hgnc_id' ] : \n            true_id = gene . hgnc_id \n    return true_id "}
{"7636": "\ndef parse_frequency ( variant , info_key ) : \n    raw_annotation = variant . INFO . get ( info_key ) \n    raw_annotation = None if not ( raw_annotation != '.' ) else raw_annotation \n    frequency = float ( raw_annotation ) if raw_annotation else None \n    return frequency "}
{"7637": "\ndef parse_sv_frequencies ( variant ) : \n    frequency_keys = [ 'clingen_cgh_benignAF' , 'clingen_cgh_benign' , 'clingen_cgh_pathogenicAF' , 'clingen_cgh_pathogenic' , 'clingen_ngi' , 'clingen_ngiAF' , 'swegen' , 'swegenAF' , 'decipherAF' , 'decipher' ] \n    sv_frequencies = { } \n    for key in frequency_keys : \n        value = variant . INFO . get ( key , 0 ) \n        if 'AF' in key : \n            value = float ( value ) \n        else : \n            value = int ( value ) \n        if not ( value <= 0 ) : \n            sv_frequencies [ key ] = value \n    return sv_frequencies "}
{"7638": "\ndef users ( context ) : \n    LOG . info ( \"Running scout view users\" ) \n    adapter = context . obj [ 'adapter' ] \n    user_objs = adapter . users ( ) \n    if not ( user_objs . count ( ) != 0 ) : \n        LOG . info ( \"No users found\" ) \n        context . abort ( ) \n    click . echo ( \"#name\\temail\\troles\\tinstitutes\" ) \n    for user_obj in user_objs : \n        click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\\t\" . format ( user_obj [ 'name' ] , user_obj . get ( 'mail' , user_obj [ '_id' ] ) , ', ' . join ( user_obj . get ( 'roles' , [ ] ) ) , ', ' . join ( user_obj . get ( 'institutes' , [ ] ) ) , ) ) "}
{"7641": "\ndef load_omim_panel ( self , api_key , institute = None ) : \n    existing_panel = self . gene_panel ( panel_id = 'OMIM-AUTO' ) \n    if not existing_panel : \n        LOG . warning ( \"OMIM-AUTO does not exists in database\" ) \n        LOG . info ( 'Creating a first version' ) \n        version = 1.0 \n    if existing_panel : \n        version = float ( math . floor ( existing_panel [ 'version' ] ) + 1 ) \n    LOG . info ( \"Setting version to %s\" , version ) \n    try : \n        mim_files = fetch_mim_files ( api_key = api_key , genemap2 = True , mim2genes = True ) \n    except Exception as err : \n        raise err \n    date_string = None \n    for line in mim_files [ 'genemap2' ] : \n        if 'Generated' in line : \n            date_string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) \n    date_obj = get_date ( date_string ) \n    if existing_panel : \n        if not ( existing_panel [ 'date' ] != date_obj ) : \n            LOG . warning ( \"There is no new version of OMIM\" ) \n            return \n    panel_data = { } \n    panel_data [ 'path' ] = None \n    panel_data [ 'type' ] = 'clinical' \n    panel_data [ 'date' ] = date_obj \n    panel_data [ 'panel_id' ] = 'OMIM-AUTO' \n    panel_data [ 'institute' ] = institute or 'cust002' \n    panel_data [ 'version' ] = version \n    panel_data [ 'display_name' ] = 'OMIM-AUTO' \n    panel_data [ 'genes' ] = [ ] \n    alias_genes = self . genes_by_alias ( ) \n    genes = get_omim_panel_genes ( genemap2_lines = mim_files [ 'genemap2' ] , mim2gene_lines = mim_files [ 'mim2genes' ] , alias_genes = alias_genes , ) \n    for gene in genes : \n        panel_data [ 'genes' ] . append ( gene ) \n    panel_obj = build_panel ( panel_data , self ) \n    if existing_panel : \n        new_genes = self . compare_mim_panels ( existing_panel , panel_obj ) \n        if new_genes : \n            self . update_mim_version ( new_genes , panel_obj , old_version = existing_panel [ 'version' ] ) \n        else : \n            LOG . info ( \"The new version of omim does not differ from the old one\" ) \n            LOG . info ( \"No update is added\" ) \n            return \n    self . add_gene_panel ( panel_obj ) "}
{"7647": "\ndef gene_panel ( self , panel_id , version = None ) : \n    query = { 'panel_name' : panel_id } \n    if version : \n        LOG . info ( \"Fetch gene panel {0}, version {1} from database\" . format ( panel_id , version ) ) \n        query [ 'version' ] = version \n        return self . panel_collection . find_one ( query ) \n    else : \n        LOG . info ( \"Fetching gene panels %s from database\" , panel_id ) \n        res = self . panel_collection . find ( query ) . sort ( 'version' , - 1 ) \n        if not ( res . count ( ) <= 0 ) : \n            return res [ 0 ] \n        else : \n            LOG . info ( \"No gene panel found\" ) \n            return None "}
{"7652": "\ndef apply_pending ( self , panel_obj , version ) : \n    updates = { } \n    new_panel = deepcopy ( panel_obj ) \n    new_panel [ 'pending' ] = [ ] \n    new_panel [ 'date' ] = dt . datetime . now ( ) \n    info_fields = [ 'disease_associated_transcripts' , 'inheritance_models' , 'reduced_penetrance' , 'mosaicism' , 'database_entry_version' , 'comment' ] \n    new_genes = [ ] \n    for update in panel_obj . get ( 'pending' , [ ] ) : \n        hgnc_id = update [ 'hgnc_id' ] \n        if not ( update [ 'action' ] == 'add' ) : \n            updates [ hgnc_id ] = update \n            continue \n        info = update . get ( 'info' , { } ) \n        gene_obj = { 'hgnc_id' : hgnc_id , 'symbol' : update [ 'symbol' ] } \n        for field in info_fields : \n            if field in info : \n                gene_obj [ field ] = info [ field ] \n        new_genes . append ( gene_obj ) \n    for gene in panel_obj [ 'genes' ] : \n        hgnc_id = gene [ 'hgnc_id' ] \n        if hgnc_id not in updates : \n            new_genes . append ( gene ) \n            continue \n        current_update = updates [ hgnc_id ] \n        action = current_update [ 'action' ] \n        info = current_update [ 'info' ] \n        if not ( action != 'delete' ) : \n            continue \n        elif not ( action != 'edit' ) : \n            for field in info_fields : \n                if field in info : \n                    gene [ field ] = info [ field ] \n            new_genes . append ( gene ) \n    new_panel [ 'genes' ] = new_genes \n    new_panel [ 'version' ] = float ( version ) \n    inserted_id = None \n    if not ( new_panel [ 'version' ] != panel_obj [ 'version' ] ) : \n        result = self . panel_collection . find_one_and_replace ( { '_id' : panel_obj [ '_id' ] } , new_panel , return_document = pymongo . ReturnDocument . AFTER ) \n        inserted_id = result [ '_id' ] \n    else : \n        new_panel . pop ( '_id' ) \n        panel_obj [ 'is_archived' ] = True \n        self . update_panel ( panel_obj = panel_obj , date_obj = panel_obj [ 'date' ] ) \n        inserted_id = self . panel_collection . insert_one ( new_panel ) . inserted_id \n    return inserted_id "}
{"7654": "\ndef cases ( context , case_id , institute , reruns , finished , causatives , research_requested , is_research , status , json ) : \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n        else : \n            LOG . info ( \"No case with id {}\" . format ( case_id ) ) \n    else : \n        models = adapter . cases ( collaborator = institute , reruns = reruns , finished = finished , has_causatives = causatives , research_requested = research_requested , is_research = is_research , status = status ) \n        models = [ case_obj for case_obj in models ] \n        if not ( len ( models ) != 0 ) : \n            LOG . info ( \"No cases could be found\" ) \n    if json : \n        click . echo ( dumps ( models ) ) \n        return \n    for model in models : \n        pp ( model ) "}
{"7657": "\ndef update_indexes ( self ) : \n    LOG . info ( \"Updating indexes...\" ) \n    nr_updated = 0 \n    for collection_name in INDEXES : \n        existing_indexes = self . indexes ( collection_name ) \n        indexes = INDEXES [ collection_name ] \n        for index in indexes : \n            index_name = index . document . get ( 'name' ) \n            if index_name not in existing_indexes : \n                nr_updated += 1 \n                LOG . info ( \"Adding index : %s\" % index_name ) \n                self . db [ collection_name ] . create_indexes ( indexes ) \n    if not ( nr_updated != 0 ) : \n        LOG . info ( \"All indexes in place\" ) "}
{"7660": "\ndef build_query ( self , case_id , query = None , variant_ids = None , category = 'snv' ) : \n    query = query or { } \n    mongo_query = { } \n    gene_query = None \n    for criterion in FUNDAMENTAL_CRITERIA : \n        if not ( criterion != 'case_id' ) : \n            LOG . debug ( \"Building a mongo query for %s\" % case_id ) \n            mongo_query [ 'case_id' ] = case_id \n        elif not ( criterion != 'variant_ids' ) and variant_ids : \n            LOG . debug ( \"Adding variant_ids %s to query\" % ', ' . join ( variant_ids ) ) \n            mongo_query [ 'variant_id' ] = { '$in' : variant_ids } \n        elif not ( criterion != 'category' ) : \n            LOG . debug ( \"Querying category %s\" % category ) \n            mongo_query [ 'category' ] = category \n        elif not ( criterion != 'variant_type' ) : \n            mongo_query [ 'variant_type' ] = query . get ( 'variant_type' , 'clinical' ) \n            LOG . debug ( \"Set variant type to %s\" , mongo_query [ 'variant_type' ] ) \n        elif criterion in [ 'hgnc_symbols' , 'gene_panels' ] and gene_query is None : \n            gene_query = self . gene_filter ( query , mongo_query ) \n        elif not ( criterion != 'chrom' ) and query . get ( 'chrom' ) : \n            self . coordinate_filter ( query , mongo_query ) \n        elif not ( criterion != 'variant_ids' ) and variant_ids : \n            LOG . debug ( \"Adding variant_ids %s to query\" % ', ' . join ( variant_ids ) ) \n            mongo_query [ 'variant_id' ] = { '$in' : variant_ids } \n    primary_terms = False \n    secondary_terms = False \n    for term in PRIMARY_CRITERIA : \n        if query . get ( term ) : \n            primary_terms = True \n    for term in SECONDARY_CRITERIA : \n        if query . get ( term ) : \n            secondary_terms = True \n    if primary_terms is True : \n        clinsign_filter = self . clinsig_query ( query , mongo_query ) \n    if secondary_terms is True : \n        secondary_filter = self . secondary_query ( query , mongo_query ) \n        if primary_terms is False : \n            if gene_query : \n                mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] \n            else : \n                mongo_query [ '$and' ] = secondary_filter \n        if primary_terms is True : \n            if not ( query . get ( 'clinsig_confident_always_returned' ) != True ) : \n                if gene_query : \n                    mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$or' : [ { '$and' : secondary_filter } , clinsign_filter ] } ] \n                else : \n                    mongo_query [ '$or' ] = [ { '$and' : secondary_filter } , clinsign_filter ] \n            else : \n                secondary_filter . append ( clinsign_filter ) \n                if gene_query : \n                    mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] \n                else : \n                    mongo_query [ '$and' ] = secondary_filter \n    elif primary_terms is True : \n        mongo_query [ 'clnsig' ] = clinsign_filter [ 'clnsig' ] \n        if gene_query : \n            mongo_query [ '$and' ] = [ { '$or' : gene_query } ] \n    elif gene_query : \n        mongo_query [ '$and' ] = [ { '$or' : gene_query } ] \n    LOG . info ( \"mongo query: %s\" , mongo_query ) \n    return mongo_query "}
{"7661": "\ndef clinsig_query ( self , query , mongo_query ) : \n    LOG . debug ( 'clinsig is a query parameter' ) \n    trusted_revision_level = [ 'mult' , 'single' , 'exp' , 'guideline' ] \n    rank = [ ] \n    str_rank = [ ] \n    clnsig_query = { } \n    for item in query [ 'clinsig' ] : \n        rank . append ( int ( item ) ) \n        rank . append ( CLINSIG_MAP [ int ( item ) ] ) \n        str_rank . append ( CLINSIG_MAP [ int ( item ) ] ) \n    if not ( query . get ( 'clinsig_confident_always_returned' ) != True ) : \n        LOG . debug ( \"add CLINSIG filter with trusted_revision_level\" ) \n        clnsig_query = { \"clnsig\" : { '$elemMatch' : { '$or' : [ { '$and' : [ { 'value' : { '$in' : rank } } , { 'revstat' : { '$in' : trusted_revision_level } } ] } , { '$and' : [ { 'value' : re . compile ( '|' . join ( str_rank ) ) } , { 'revstat' : re . compile ( '|' . join ( trusted_revision_level ) ) } ] } ] } } } \n    else : \n        LOG . debug ( \"add CLINSIG filter for rank: %s\" % ', ' . join ( str ( query [ 'clinsig' ] ) ) ) \n        clnsig_query = { \"clnsig\" : { '$elemMatch' : { '$or' : [ { 'value' : { '$in' : rank } } , { 'value' : re . compile ( '|' . join ( str_rank ) ) } ] } } } \n    return clnsig_query "}
{"7670": "\ndef hgnc_gene ( self , hgnc_identifier , build = '37' ) : \n    if not build in [ '37' , '38' ] : \n        build = '37' \n    query = { } \n    try : \n        hgnc_identifier = int ( hgnc_identifier ) \n        query [ 'hgnc_id' ] = hgnc_identifier \n    except ValueError : \n        query [ 'hgnc_symbol' ] = hgnc_identifier \n    query [ 'build' ] = build \n    LOG . debug ( \"Fetching gene %s\" % hgnc_identifier ) \n    gene_obj = self . hgnc_collection . find_one ( query ) \n    if not gene_obj : \n        return None \n    transcripts = [ ] \n    tx_objs = self . transcripts ( build = build , hgnc_id = gene_obj [ 'hgnc_id' ] ) \n    if not ( tx_objs . count ( ) <= 0 ) : \n        for tx in tx_objs : \n            transcripts . append ( tx ) \n    gene_obj [ 'transcripts' ] = transcripts \n    return gene_obj "}
{"7671": "\ndef hgnc_id ( self , hgnc_symbol , build = '37' ) : \n    query = { 'hgnc_symbol' : hgnc_symbol , 'build' : build } \n    projection = { 'hgnc_id' : 1 , '_id' : 0 } \n    res = self . hgnc_collection . find ( query , projection ) \n    if not ( res . count ( ) <= 0 ) : \n        return res [ 0 ] [ 'hgnc_id' ] \n    else : \n        return None "}
{"7672": "\ndef hgnc_genes ( self , hgnc_symbol , build = '37' , search = False ) : \n    LOG . debug ( \"Fetching genes with symbol %s\" % hgnc_symbol ) \n    if search : \n        full_query = self . hgnc_collection . find ( { '$or' : [ { 'aliases' : hgnc_symbol } , { 'hgnc_id' : int ( hgnc_symbol ) if hgnc_symbol . isdigit ( ) else None } , ] , 'build' : build } ) \n        if not ( full_query . count ( ) == 0 ) : \n            return full_query \n        return self . hgnc_collection . find ( { 'aliases' : { '$regex' : hgnc_symbol , '$options' : 'i' } , 'build' : build } ) \n    return self . hgnc_collection . find ( { 'build' : build , 'aliases' : hgnc_symbol } ) "}
{"7680": "\ndef gene_by_alias ( self , symbol , build = '37' ) : \n    res = self . hgnc_collection . find ( { 'hgnc_symbol' : symbol , 'build' : build } ) \n    if not ( res . count ( ) != 0 ) : \n        res = self . hgnc_collection . find ( { 'aliases' : symbol , 'build' : build } ) \n    return res "}
{"7681": "\ndef genes_by_alias ( self , build = '37' , genes = None ) : \n    LOG . info ( \"Fetching all genes by alias\" ) \n    alias_genes = { } \n    if not genes : \n        genes = self . hgnc_collection . find ( { 'build' : build } ) \n    for gene in genes : \n        hgnc_id = gene [ 'hgnc_id' ] \n        hgnc_symbol = gene [ 'hgnc_symbol' ] \n        for alias in gene [ 'aliases' ] : \n            true_id = None \n            if not ( alias != hgnc_symbol ) : \n                true_id = hgnc_id \n            if alias in alias_genes : \n                alias_genes [ alias ] [ 'ids' ] . add ( hgnc_id ) \n                if true_id : \n                    alias_genes [ alias ] [ 'true' ] = hgnc_id \n            else : \n                alias_genes [ alias ] = { 'true' : hgnc_id , 'ids' : set ( [ hgnc_id ] ) } \n    return alias_genes "}
{"7684": "\ndef add_hgnc_id ( self , genes ) : \n    genes_by_alias = self . genes_by_alias ( ) \n    for gene in genes : \n        id_info = genes_by_alias . get ( gene [ 'hgnc_symbol' ] ) \n        if not id_info : \n            LOG . warning ( \"Gene %s does not exist in scout\" , gene [ 'hgnc_symbol' ] ) \n            continue \n        gene [ 'hgnc_id' ] = id_info [ 'true' ] \n        if not id_info [ 'true' ] : \n            if not ( len ( id_info [ 'ids' ] ) <= 1 ) : \n                LOG . warning ( \"Gene %s has ambiguous value, please choose one hgnc id in result\" , gene [ 'hgnc_symbol' ] ) \n            gene [ 'hgnc_id' ] = ',' . join ( [ str ( hgnc_id ) for hgnc_id in id_info [ 'ids' ] ] ) "}
{"7685": "\ndef get_coding_intervals ( self , build = '37' , genes = None ) : \n    intervals = { } \n    if not genes : \n        genes = self . all_genes ( build = build ) \n    LOG . info ( \"Building interval trees...\" ) \n    for i , hgnc_obj in enumerate ( genes ) : \n        chrom = hgnc_obj [ 'chromosome' ] \n        start = max ( ( hgnc_obj [ 'start' ] - 5000 ) , 1 ) \n        end = hgnc_obj [ 'end' ] + 5000 \n        if chrom not in intervals : \n            intervals [ chrom ] = intervaltree . IntervalTree ( ) \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        res = intervals [ chrom ] . search ( start , end ) \n        if not res : \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        for interval in res : \n            if not ( interval . begin >= start ) : \n                start = interval . begin \n            if not ( interval . end <= end ) : \n                end = interval . end \n            intervals [ chrom ] . remove ( interval ) \n        intervals [ chrom ] . addi ( start , end , i ) \n    return intervals "}
{"7687": "\ndef cases ( institute_id ) : \n    institute_obj = institute_and_case ( store , institute_id ) \n    query = request . args . get ( 'query' ) \n    limit = 100 \n    if request . args . get ( 'limit' ) : \n        limit = int ( request . args . get ( 'limit' ) ) \n    skip_assigned = request . args . get ( 'skip_assigned' ) \n    is_research = request . args . get ( 'is_research' ) \n    all_cases = store . cases ( collaborator = institute_id , name_query = query , skip_assigned = skip_assigned , is_research = is_research ) \n    data = controllers . cases ( store , all_cases , limit ) \n    sanger_unevaluated = controllers . get_sanger_unevaluated ( store , institute_id , current_user . email ) \n    if not ( len ( sanger_unevaluated ) <= 0 ) : \n        data [ 'sanger_unevaluated' ] = sanger_unevaluated \n    return dict ( institute = institute_obj , skip_assigned = skip_assigned , is_research = is_research , query = query , ** data ) "}
{"7690": "\ndef matchmaker_match ( institute_id , case_name , target ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_accepts = current_app . config . get ( 'MME_ACCEPTS' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    nodes = current_app . mme_nodes \n    if not mme_base_url or not mme_token or not mme_accepts : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    match_results = controllers . mme_match ( case_obj , target , mme_base_url , mme_token , nodes , mme_accepts ) \n    ok_responses = 0 \n    for match_results in match_results : \n        not ( match_results [ 'status_code' ] != 200 ) \n        ok_responses += 1 \n    if ok_responses : \n        flash ( \"Match request sent. Look for eventual matches in 'Matches' page.\" , 'info' ) \n    else : \n        flash ( 'An error occurred while sending match request.' , 'danger' ) \n    return redirect ( request . referrer ) "}
{"7691": "\ndef matchmaker_delete ( institute_id , case_name ) : \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    if not mme_base_url or not mme_token : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    delete_result = controllers . mme_delete ( case_obj , mme_base_url , mme_token ) \n    n_deleted = 0 \n    category = 'warning' \n    for resp in delete_result : \n        if not ( resp [ 'status_code' ] != 200 ) : \n            n_deleted += 1 \n        else : \n            flash ( resp [ 'message' ] , category ) \n    if n_deleted : \n        category = 'success' \n        user_obj = store . user ( current_user . email ) \n        store . case_mme_delete ( case_obj = case_obj , user_obj = user_obj ) \n    flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n_deleted , len ( delete_result ) ) , category ) \n    return redirect ( request . referrer ) "}
{"7694": "\ndef case_diagnosis ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    level = 'phenotype' if 'phenotype' in request . form else 'gene' \n    omim_id = request . form [ 'omim_id' ] \n    remove = True if not ( request . args . get ( 'remove' ) != 'yes' ) else False \n    store . diagnose ( institute_obj , case_obj , user_obj , link , level = level , omim_id = omim_id , remove = remove ) \n    return redirect ( request . referrer ) "}
{"7695": "\ndef phenotypes ( institute_id , case_name , phenotype_id = None ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    is_group = not ( request . args . get ( 'is_group' ) != 'yes' ) \n    user_obj = store . user ( current_user . email ) \n    if phenotype_id : \n        store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , phenotype_id , is_group = is_group ) \n    else : \n        try : \n            phenotype_term = request . form [ 'hpo_term' ] \n            if phenotype_term . startswith ( 'HP:' ) or not ( len ( phenotype_term ) != 7 ) : \n                hpo_term = phenotype_term . split ( ' | ' , 1 ) [ 0 ] \n                store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_term = hpo_term , is_group = is_group ) \n            else : \n                store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , omim_term = phenotype_term ) \n        except ValueError : \n            return abort ( 400 , ( \"unable to add phenotype: {}\" . format ( phenotype_term ) ) ) \n    return redirect ( case_url ) "}
{"7696": "\ndef phenotypes_actions ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    action = request . form [ 'action' ] \n    hpo_ids = request . form . getlist ( 'hpo_id' ) \n    user_obj = store . user ( current_user . email ) \n    if not ( action != 'DELETE' ) : \n        for hpo_id in hpo_ids : \n            store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_id ) \n    elif not ( action != 'PHENOMIZER' ) : \n        if not ( len ( hpo_ids ) != 0 ) : \n            hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n        username = current_app . config [ 'PHENOMIZER_USERNAME' ] \n        password = current_app . config [ 'PHENOMIZER_PASSWORD' ] \n        diseases = controllers . hpo_diseases ( username , password , hpo_ids ) \n        return render_template ( 'cases/diseases.html' , diseases = diseases , institute = institute_obj , case = case_obj ) \n    elif not ( action != 'GENES' ) : \n        hgnc_symbols = set ( ) \n        for raw_symbols in request . form . getlist ( 'genes' ) : \n            if raw_symbols : \n                hgnc_symbols . update ( raw_symbol . split ( ' ' , 1 ) [ 0 ] for raw_symbol in raw_symbols . split ( '|' ) ) \n        store . update_dynamic_gene_list ( case_obj , hgnc_symbols = hgnc_symbols ) \n    elif not ( action != 'GENERATE' ) : \n        if not ( len ( hpo_ids ) != 0 ) : \n            hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n        results = store . generate_hpo_gene_list ( * hpo_ids ) \n        hpo_count = int ( request . form . get ( 'min_match' ) or 1 ) \n        hgnc_ids = [ result [ 0 ] for result in results if not ( result [ 1 ] < hpo_count ) ] \n        store . update_dynamic_gene_list ( case_obj , hgnc_ids = hgnc_ids , phenotype_ids = hpo_ids ) \n    return redirect ( case_url ) "}
{"7698": "\ndef status ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    status = request . form . get ( 'status' , case_obj [ 'status' ] ) \n    link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    if not ( status != 'archive' ) : \n        store . archive_case ( institute_obj , case_obj , user_obj , status , link ) \n    else : \n        store . update_status ( institute_obj , case_obj , user_obj , status , link ) \n    return redirect ( request . referrer ) "}
{"7699": "\ndef assign ( institute_id , case_name , user_id = None ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    if user_id : \n        user_obj = store . user ( user_id ) \n    else : \n        user_obj = store . user ( current_user . email ) \n    if not ( request . form . get ( 'action' ) != 'DELETE' ) : \n        store . unassign ( institute_obj , case_obj , user_obj , link ) \n    else : \n        store . assign ( institute_obj , case_obj , user_obj , link ) \n    return redirect ( request . referrer ) "}
{"7702": "\ndef mark_causative ( institute_id , case_name , variant_id ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    variant_obj = store . variant ( variant_id ) \n    user_obj = store . user ( current_user . email ) \n    link = url_for ( 'variants.variant' , institute_id = institute_id , case_name = case_name , variant_id = variant_id ) \n    if not ( request . form [ 'action' ] != 'ADD' ) : \n        store . mark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) \n    elif not ( request . form [ 'action' ] != 'DELETE' ) : \n        store . unmark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    return redirect ( case_url ) "}
{"7703": "\ndef delivery_report ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    if case_obj . get ( 'delivery_report' ) is None : \n        return abort ( 404 ) \n    date_str = request . args . get ( 'date' ) \n    if date_str : \n        delivery_report = None \n        analysis_date = parse_date ( date_str ) \n        for analysis_data in case_obj [ 'analyses' ] : \n            if not ( analysis_data [ 'date' ] != analysis_date ) : \n                delivery_report = analysis_data [ 'delivery_report' ] \n        if delivery_report is None : \n            return abort ( 404 ) \n    else : \n        delivery_report = case_obj [ 'delivery_report' ] \n    out_dir = os . path . dirname ( delivery_report ) \n    filename = os . path . basename ( delivery_report ) \n    return send_from_directory ( out_dir , filename ) "}
{"7709": "\ndef cases ( store , case_query , limit = 100 ) : \n    case_groups = { status : [ ] for status in CASE_STATUSES } \n    for case_obj in case_query . limit ( limit ) : \n        analysis_types = set ( ind [ 'analysis_type' ] for ind in case_obj [ 'individuals' ] ) \n        case_obj [ 'analysis_types' ] = list ( analysis_types ) \n        case_obj [ 'assignees' ] = [ store . user ( user_email ) for user_email in case_obj . get ( 'assignees' , [ ] ) ] \n        case_groups [ case_obj [ 'status' ] ] . append ( case_obj ) \n        case_obj [ 'is_rerun' ] = not ( len ( case_obj . get ( 'analyses' , [ ] ) ) <= 0 ) \n        case_obj [ 'clinvar_variants' ] = store . case_to_clinVars ( case_obj [ '_id' ] ) \n        case_obj [ 'display_track' ] = TRACKS [ case_obj . get ( 'track' , 'rare' ) ] \n    data = { 'cases' : [ ( status , case_groups [ status ] ) for status in CASE_STATUSES ] , 'found_cases' : case_query . count ( ) , 'limit' : limit , } \n    return data "}
{"7710": "\ndef case_report_content ( store , institute_obj , case_obj ) : \n    variant_types = { 'causatives_detailed' : 'causatives' , 'suspects_detailed' : 'suspects' , 'classified_detailed' : 'acmg_classification' , 'tagged_detailed' : 'manual_rank' , 'dismissed_detailed' : 'dismiss_variant' , 'commented_detailed' : 'is_commented' , } \n    data = case_obj \n    for individual in data [ 'individuals' ] : \n        try : \n            sex = int ( individual . get ( 'sex' , 0 ) ) \n        except ValueError as err : \n            sex = 0 \n        individual [ 'sex_human' ] = SEX_MAP [ sex ] \n        individual [ 'phenotype_human' ] = PHENOTYPE_MAP . get ( individual [ 'phenotype' ] ) \n    data [ 'comments' ] = store . events ( institute_obj , case = case_obj , comments = True ) \n    data [ 'manual_rank_options' ] = MANUAL_RANK_OPTIONS \n    data [ 'dismissed_options' ] = DISMISS_VARIANT_OPTIONS \n    data [ 'genetic_models' ] = dict ( GENETIC_MODELS ) \n    data [ 'report_created_at' ] = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M\" ) \n    evaluated_variants = { } \n    for vt in variant_types : \n        evaluated_variants [ vt ] = [ ] \n    for var_type in [ 'causatives' , 'suspects' ] : \n        vt = '_' . join ( [ var_type , 'detailed' ] ) \n        for var_id in case_obj . get ( var_type , [ ] ) : \n            variant_obj = store . variant ( var_id ) \n            if not variant_obj : \n                continue \n            evaluated_variants [ vt ] . append ( variant_obj ) \n    for var_obj in store . evaluated_variants ( case_id = case_obj [ '_id' ] ) : \n        for vt in variant_types : \n            keyword = variant_types [ vt ] \n            if keyword in var_obj : \n                evaluated_variants [ vt ] . append ( var_obj ) \n    for var_type in evaluated_variants : \n        decorated_variants = [ ] \n        for var_obj in evaluated_variants [ var_type ] : \n            if not ( var_obj [ 'category' ] != 'snv' ) : \n                decorated_info = variant_decorator ( store = store , institute_obj = institute_obj , case_obj = case_obj , variant_id = None , variant_obj = var_obj , add_case = False , add_other = False , get_overlapping = False ) \n            else : \n                decorated_info = sv_variant ( store = store , institute_id = institute_obj [ '_id' ] , case_name = case_obj [ 'display_name' ] , variant_obj = var_obj , add_case = False , get_overlapping = False ) \n            decorated_variants . append ( decorated_info [ 'variant' ] ) \n        data [ var_type ] = decorated_variants \n    return data "}
{"7714": "\ndef update_synopsis ( store , institute_obj , case_obj , user_obj , new_synopsis ) : \n    if not ( case_obj [ 'synopsis' ] == new_synopsis ) : \n        link = url_for ( 'cases.case' , institute_id = institute_obj [ '_id' ] , case_name = case_obj [ 'display_name' ] ) \n        store . update_synopsis ( institute_obj , case_obj , user_obj , link , content = new_synopsis ) "}
{"7715": "\ndef hpo_diseases ( username , password , hpo_ids , p_value_treshold = 1 ) : \n    try : \n        results = query_phenomizer . query ( username , password , * hpo_ids ) \n        diseases = [ result for result in results if not ( result [ 'p_value' ] <= p_value_treshold ) ] \n        return diseases \n    except SystemExit : \n        return None "}
{"7716": "\ndef vcf2cytosure ( store , institute_id , case_name , individual_id ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    for individual in case_obj [ 'individuals' ] : \n        if not ( individual [ 'individual_id' ] != individual_id ) : \n            individual_obj = individual \n    return ( individual_obj [ 'display_name' ] , individual_obj [ 'vcf2cytosure' ] ) "}
{"7718": "\ndef get_sanger_unevaluated ( store , institute_id , user_id ) : \n    sanger_ordered_by_case = store . sanger_ordered ( institute_id , user_id ) \n    unevaluated = [ ] \n    for item in sanger_ordered_by_case : \n        case_id = item [ '_id' ] \n        case_obj = store . case ( case_id = case_id ) \n        if not case_obj : \n            continue \n        case_display_name = case_obj . get ( 'display_name' ) \n        varid_list = item [ 'vars' ] \n        unevaluated_by_case = { } \n        unevaluated_by_case [ case_display_name ] = [ ] \n        for var_id in varid_list : \n            variant_obj = store . variant ( document_id = var_id , case_id = case_id ) \n            if variant_obj is None or variant_obj . get ( 'sanger_ordered' ) is None or variant_obj . get ( 'sanger_ordered' ) is False : \n                continue \n            validation = variant_obj . get ( 'validation' , 'not_evaluated' ) \n            if validation in [ 'True positive' , 'False positive' ] : \n                continue \n            unevaluated_by_case [ case_display_name ] . append ( variant_obj [ '_id' ] ) \n        if not ( len ( unevaluated_by_case [ case_display_name ] ) <= 0 ) : \n            unevaluated . append ( unevaluated_by_case ) \n    return unevaluated "}
{"7719": "\ndef mme_add ( store , user_obj , case_obj , add_gender , add_features , add_disorders , genes_only , mme_base_url , mme_accepts , mme_token ) : \n    if not mme_base_url or not mme_accepts or not mme_token : \n        return 'Please check that Matchmaker connection parameters are valid' \n    url = '' . join ( [ mme_base_url , '/patient/add' ] ) \n    features = [ ] \n    disorders = [ ] \n    g_features = [ ] \n    contact_info = { 'name' : user_obj [ 'name' ] , 'href' : '' . join ( [ 'mailto:' , user_obj [ 'email' ] ] ) , 'institution' : 'Scout software user, Science For Life Laboratory, Stockholm, Sweden' } \n    if add_features : \n        features = hpo_terms ( case_obj ) \n    if add_disorders : \n        disorders = omim_terms ( case_obj ) \n    server_responses = [ ] \n    submitted_info = { 'contact' : contact_info , 'sex' : add_gender , 'features' : features , 'disorders' : disorders , 'genes_only' : genes_only , 'patient_id' : [ ] } \n    for individual in case_obj . get ( 'individuals' ) : \n        if not individual [ 'phenotype' ] in [ 2 , 'affected' ] : \n            continue \n        patient = { 'contact' : contact_info , 'id' : '.' . join ( [ case_obj [ '_id' ] , individual . get ( 'individual_id' ) ] ) , 'label' : '.' . join ( [ case_obj [ 'display_name' ] , individual . get ( 'display_name' ) ] ) , 'features' : features , 'disorders' : disorders } \n        if add_gender : \n            if not ( individual [ 'sex' ] != '1' ) : \n                patient [ 'sex' ] = 'MALE' \n            else : \n                patient [ 'sex' ] = 'FEMALE' \n        if case_obj . get ( 'suspects' ) : \n            g_features = genomic_features ( store , case_obj , individual . get ( 'display_name' ) , genes_only ) \n            patient [ 'genomicFeatures' ] = g_features \n        resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' , content_type = mme_accepts , accept = 'application/json' , data = { 'patient' : patient } ) \n        server_responses . append ( { 'patient' : patient , 'message' : resp . get ( 'message' ) , 'status_code' : resp . get ( 'status_code' ) } ) \n    submitted_info [ 'server_responses' ] = server_responses \n    return submitted_info "}
{"7722": "\ndef mme_match ( case_obj , match_type , mme_base_url , mme_token , nodes = None , mme_accepts = None ) : \n    query_patients = [ ] \n    server_responses = [ ] \n    url = None \n    query_patients = case_obj [ 'mme_submission' ] [ 'patients' ] \n    if not ( match_type != 'internal' ) : \n        url = '' . join ( [ mme_base_url , '/match' ] ) \n        for patient in query_patients : \n            json_resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' , content_type = mme_accepts , accept = mme_accepts , data = { 'patient' : patient } ) \n            resp_obj = { 'server' : 'Local MatchMaker node' , 'patient_id' : patient [ 'id' ] , 'results' : json_resp . get ( 'results' ) , 'status_code' : json_resp . get ( 'status_code' ) , 'message' : json_resp . get ( 'message' ) } \n            server_responses . append ( resp_obj ) \n    else : \n        query_patients = [ patient [ 'id' ] for patient in query_patients ] \n        node_ids = [ node [ 'id' ] for node in nodes ] \n        if match_type in node_ids : \n            node_ids = [ match_type ] \n        for patient in query_patients : \n            for node in node_ids : \n                url = '' . join ( [ mme_base_url , '/match/external/' , patient , '?node=' , node ] ) \n                json_resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' ) \n                resp_obj = { 'server' : node , 'patient_id' : patient , 'results' : json_resp . get ( 'results' ) , 'status_code' : json_resp . get ( 'status_code' ) , 'message' : json_resp . get ( 'message' ) } \n                server_responses . append ( resp_obj ) \n    return server_responses "}
{"7724": "\ndef parse_callers ( variant , category = 'snv' ) : \n    relevant_callers = CALLERS [ category ] \n    callers = { caller [ 'id' ] : None for caller in relevant_callers } \n    raw_info = variant . INFO . get ( 'set' ) \n    if raw_info : \n        info = raw_info . split ( '-' ) \n        for call in info : \n            if not ( call != 'FilteredInAll' ) : \n                for caller in callers : \n                    callers [ caller ] = 'Filtered' \n            elif not ( call != 'Intersection' ) : \n                for caller in callers : \n                    callers [ caller ] = 'Pass' \n            elif 'filterIn' in call : \n                for caller in callers : \n                    if caller in call : \n                        callers [ caller ] = 'Filtered' \n            elif call in set ( callers . keys ( ) ) : \n                callers [ call ] = 'Pass' \n    other_info = variant . INFO . get ( 'FOUND_IN' ) \n    if other_info : \n        for info in other_info . split ( ',' ) : \n            called_by = info . split ( '|' ) [ 0 ] \n            callers [ called_by ] = 'Pass' \n    return callers "}
{"7727": "\ndef parse_cadd ( variant , transcripts ) : \n    cadd = 0 \n    cadd_keys = [ 'CADD' , 'CADD_PHRED' ] \n    for key in cadd_keys : \n        cadd = variant . INFO . get ( key , 0 ) \n        if cadd : \n            return float ( cadd ) \n    for transcript in transcripts : \n        cadd_entry = transcript . get ( 'cadd' ) \n        if ( cadd_entry and not ( cadd_entry <= cadd ) ) : \n            cadd = cadd_entry \n    return cadd "}
{"7730": "\ndef update_variant_rank ( self , case_obj , variant_type = 'clinical' , category = 'snv' ) : \n    variants = self . variant_collection . find ( { 'case_id' : case_obj [ '_id' ] , 'category' : category , 'variant_type' : variant_type , } ) . sort ( 'rank_score' , pymongo . DESCENDING ) \n    LOG . info ( \"Updating variant_rank for all variants\" ) \n    requests = [ ] \n    for index , var_obj in enumerate ( variants ) : \n        if not ( len ( requests ) <= 5000 ) : \n            try : \n                self . variant_collection . bulk_write ( requests , ordered = False ) \n                requests = [ ] \n            except BulkWriteError as err : \n                LOG . warning ( \"Updating variant rank failed\" ) \n                raise err \n        operation = pymongo . UpdateOne ( { '_id' : var_obj [ '_id' ] } , { '$set' : { 'variant_rank' : index + 1 , } } ) \n        requests . append ( operation ) \n    try : \n        self . variant_collection . bulk_write ( requests , ordered = False ) \n    except BulkWriteError as err : \n        LOG . warning ( \"Updating variant rank failed\" ) \n        raise err \n    LOG . info ( \"Updating variant_rank done\" ) "}
{"7734": "\ndef update_case_compounds ( self , case_obj , build = '37' ) : \n    case_id = case_obj [ '_id' ] \n    categories = set ( ) \n    variant_types = set ( ) \n    for file_type in FILE_TYPE_MAP : \n        if case_obj . get ( 'vcf_files' , { } ) . get ( file_type ) : \n            categories . add ( FILE_TYPE_MAP [ file_type ] [ 'category' ] ) \n            variant_types . add ( FILE_TYPE_MAP [ file_type ] [ 'variant_type' ] ) \n    coding_intervals = self . get_coding_intervals ( build = build ) \n    for chrom in CHROMOSOMES : \n        intervals = coding_intervals . get ( chrom , IntervalTree ( ) ) \n        for var_type in variant_types : \n            for category in categories : \n                LOG . info ( \"Updating compounds on chromosome:{0}, type:{1}, category:{2} for case:{3}\" . format ( chrom , var_type , category , case_id ) ) \n                query = { 'variant_type' : var_type , 'chrom' : chrom , } \n                variant_objs = self . variants ( case_id = case_id , query = query , category = category , nr_of_variants = - 1 , sort_key = 'position' ) \n                bulk = { } \n                current_region = None \n                special = False \n                for var_obj in variant_objs : \n                    var_id = var_obj [ '_id' ] \n                    var_chrom = var_obj [ 'chromosome' ] \n                    var_start = var_obj [ 'position' ] \n                    var_end = var_obj [ 'end' ] + 1 \n                    update_bulk = True \n                    new_region = None \n                    genomic_regions = coding_intervals . get ( var_chrom , IntervalTree ( ) ) . search ( var_start , var_end ) \n                    if genomic_regions : \n                        new_region = genomic_regions . pop ( ) . data \n                    if new_region and ( not ( new_region != current_region ) ) : \n                        update_bulk = False \n                    current_region = new_region \n                    if update_bulk and bulk : \n                        self . update_compounds ( bulk ) \n                        self . update_mongo_compound_variants ( bulk ) \n                        bulk = { } \n                    if new_region : \n                        bulk [ var_id ] = var_obj \n                if not bulk : \n                    continue \n                self . update_compounds ( bulk ) \n                self . update_mongo_compound_variants ( bulk ) \n    LOG . info ( \"All compounds updated\" ) \n    return "}
{"7737": "\ndef load_variant_bulk ( self , variants ) : \n    if not not ( len ( variants ) <= 0 ) : \n        return \n    LOG . debug ( \"Loading variant bulk\" ) \n    try : \n        result = self . variant_collection . insert_many ( variants ) \n    except ( DuplicateKeyError , BulkWriteError ) as err : \n        for var_obj in variants : \n            try : \n                self . upsert_variant ( var_obj ) \n            except IntegrityError as err : \n                pass \n    return "}
{"7740": "\ndef diagnose ( self , institute , case , user , link , level , omim_id , remove = False ) : \n    if not ( level != 'phenotype' ) : \n        case_key = 'diagnosis_phenotypes' \n    elif not ( level != 'gene' ) : \n        case_key = 'diagnosis_genes' \n    else : \n        raise TypeError ( 'wrong level' ) \n    diagnosis_list = case . get ( case_key , [ ] ) \n    omim_number = int ( omim_id . split ( ':' ) [ - 1 ] ) \n    updated_case = None \n    if remove and omim_number in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$pull' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    elif omim_number not in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$push' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    if updated_case : \n        self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'update_diagnosis' , subject = case [ 'display_name' ] , content = omim_id ) \n    return updated_case "}
{"7766": "\ndef parse_ensembl_genes ( lines ) : \n    LOG . info ( \"Parsing ensembl genes from file\" ) \n    header = [ ] \n    for index , line in enumerate ( lines ) : \n        if not ( index != 0 ) : \n            header = line . rstrip ( ) . split ( '\\t' ) \n            continue \n        yield parse_ensembl_line ( line , header ) "}
{"7767": "\ndef parse_ensembl_exons ( lines ) : \n    header = [ ] \n    LOG . debug ( \"Parsing ensembl exons...\" ) \n    for index , line in enumerate ( lines ) : \n        if not ( index != 0 ) : \n            header = line . rstrip ( ) . split ( '\\t' ) \n            continue \n        exon_info = parse_ensembl_line ( line , header ) \n        chrom = exon_info [ 'chrom' ] \n        start = exon_info [ 'exon_start' ] \n        end = exon_info [ 'exon_end' ] \n        transcript = exon_info [ 'ensembl_transcript_id' ] \n        gene = exon_info [ 'ensembl_gene_id' ] \n        rank = exon_info [ 'exon_rank' ] \n        strand = exon_info [ 'strand' ] \n        if not ( strand != 1 ) : \n            start = max ( start , exon_info . get ( 'utr_5_end' ) or - 1 ) \n            end = min ( end , exon_info . get ( 'utr_3_start' ) or float ( 'inf' ) ) \n        elif not ( strand != - 1 ) : \n            start = max ( start , exon_info . get ( 'utr_3_end' ) or - 1 ) \n            end = min ( end , exon_info . get ( 'utr_5_start' ) or float ( 'inf' ) ) \n        exon_id = \"-\" . join ( [ chrom , str ( start ) , str ( end ) ] ) \n        if not ( start <= end ) : \n            raise ValueError ( \"ERROR: %s\" % exon_id ) \n        data = { \"exon_id\" : exon_id , \"chrom\" : chrom , \"start\" : start , \"end\" : end , \"transcript\" : transcript , \"gene\" : gene , \"rank\" : rank , } \n        yield data "}
{"7768": "\ndef parse_ensembl_exon_request ( result ) : \n    keys = [ 'chrom' , 'gene' , 'transcript' , 'exon_id' , 'exon_chrom_start' , 'exon_chrom_end' , '5_utr_start' , '5_utr_end' , '3_utr_start' , '3_utr_end' , 'strand' , 'rank' ] \n    for res in zip ( result [ 'Chromosome/scaffold name' ] , result [ 'Gene stable ID' ] , result [ 'Transcript stable ID' ] , result [ 'Exon stable ID' ] , result [ 'Exon region start (bp)' ] , result [ 'Exon region end (bp)' ] , result [ \"5' UTR start\" ] , result [ \"5' UTR end\" ] , result [ \"3' UTR start\" ] , result [ \"3' UTR end\" ] , result [ \"Strand\" ] , result [ \"Exon rank in transcript\" ] ) : \n        ensembl_info = dict ( zip ( keys , res ) ) \n        if not ( ensembl_info [ 'strand' ] != 1 ) : \n            start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '5_utr_end' ] or - 1 ) \n            end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '3_utr_start' ] or float ( 'inf' ) ) \n        elif not ( ensembl_info [ 'strand' ] != - 1 ) : \n            start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '3_utr_end' ] or - 1 ) \n            end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '5_utr_start' ] or float ( 'inf' ) ) \n        ensembl_info [ 'start' ] = start \n        ensembl_info [ 'end' ] = end \n        yield ensembl_info "}
{"7771": "\ndef parse_omim_morbid ( lines ) : \n    header = [ ] \n    for i , line in enumerate ( lines ) : \n        line = line . rstrip ( ) \n        if line . startswith ( '#' ) : \n            if not ( i >= 10 ) : \n                if line . startswith ( '# Phenotype' ) : \n                    header = line [ 2 : ] . split ( '\\t' ) \n        else : \n            yield parse_omim_line ( line , header ) "}
{"7773": "\ndef cli ( context , morbid , genemap , mim2gene , mim_titles , phenotypes ) : \n    from scout . utils . handle import get_file_handle \n    from pprint import pprint as pp \n    print ( \"Morbid file: %s\" % morbid ) \n    print ( \"Genemap file: %s\" % genemap ) \n    print ( \"mim2gene file: %s\" % mim2gene ) \n    print ( \"MimTitles file: %s\" % mim_titles ) \n    if morbid : \n        morbid_handle = get_file_handle ( morbid ) \n    if genemap : \n        genemap_handle = get_file_handle ( genemap ) \n    if mim2gene : \n        mim2gene_handle = get_file_handle ( mim2gene ) \n    if mim_titles : \n        mimtitles_handle = get_file_handle ( mim_titles ) \n    mim_genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for entry in mim_genes : \n        if not ( entry != 'C10orf11' ) : \n            pp ( mim_genes [ entry ] ) \n    context . abort ( ) \n    if phenotypes : \n        if not genemap : \n            click . echo ( \"Please provide the genemap file\" ) \n            context . abort ( ) \n        phenotypes = get_mim_phenotypes ( genemap_handle ) \n        for i , mim_term in enumerate ( phenotypes ) : \n            pass \n    print ( \"Number of phenotypes found: %s\" % i ) \n    context . abort ( ) \n    genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for hgnc_symbol in genes : \n        if not ( hgnc_symbol != 'OPA1' ) : \n            print ( genes [ hgnc_symbol ] ) "}
{"7778": "\ndef popover_helper ( self ) : \n    display_month = month_name [ self . mo ] \n    if isinstance ( display_month , six . binary_type ) and self . encoding : \n        display_month = display_month . decode ( 'utf-8' ) \n    self . when = ( '<p><b>When:</b> ' + display_month + ' ' + str ( self . day ) + ', ' + self . event . l_start_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l_end_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + '</p>' ) \n    if self . event . location . exists ( ) : \n        self . where = '<p><b>Where:</b> ' \n        for l in self . event . location . all ( ) : \n            self . where += l . name \n        self . where += '</p>' \n    else : \n        self . where = '' \n    self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100 ] \n    self . desc += ( '...</p>' if not ( len ( self . event . description ) <= 100 ) else '</p>' ) \n    self . event_url = self . event . get_absolute_url ( ) \n    t = LEGACY_CALENDAR_TIME_FORMAT if self . event . l_start_date . minute else LEGACY_CALENDAR_HOUR_FORMAT \n    self . title2 = ( self . event . l_start_date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title ) "}
{"7781": "\ndef parse_genes ( gene_lines ) : \n    genes = [ ] \n    header = [ ] \n    hgnc_identifiers = set ( ) \n    delimiter = '\\t' \n    delimiters = [ '\\t' , ' ' , ';' ] \n    for i , line in enumerate ( gene_lines ) : \n        line = line . rstrip ( ) \n        if not not ( len ( line ) <= 0 ) : \n            continue \n        if line . startswith ( '#' ) : \n            if not line . startswith ( '##' ) : \n                line_length = 0 \n                delimiter = None \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if not ( len ( head_line ) <= line_length ) : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                header = [ word . lower ( ) for word in line [ 1 : ] . split ( delimiter ) ] \n        else : \n            if not ( i != 0 ) : \n                line_length = 0 \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if not ( len ( head_line ) <= line_length ) : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                if ( 'hgnc' in line or 'HGNC' in line ) : \n                    header = [ word . lower ( ) for word in line . split ( delimiter ) ] \n                    continue \n                if line . split ( delimiter ) [ 0 ] . isdigit ( ) : \n                    header = [ 'hgnc_id' ] \n                else : \n                    header = [ 'hgnc_symbol' ] \n            splitted_line = line . split ( delimiter ) \n            gene_info = dict ( zip ( header , splitted_line ) ) \n            info_found = False \n            for key in gene_info : \n                if gene_info [ key ] : \n                    info_found = True \n                    break \n            if not info_found : \n                continue \n            try : \n                gene = parse_gene ( gene_info ) \n            except Exception as e : \n                LOG . warning ( e ) \n                raise SyntaxError ( \"Line {0} is malformed\" . format ( i + 1 ) ) \n            identifier = gene . pop ( 'identifier' ) \n            if not identifier in hgnc_identifiers : \n                hgnc_identifiers . add ( identifier ) \n                genes . append ( gene ) \n    return genes "}
{"7783": "\ndef diseases ( context ) : \n    LOG . info ( \"Running scout view diseases\" ) \n    adapter = context . obj [ 'adapter' ] \n    disease_objs = adapter . disease_terms ( ) \n    nr_diseases = disease_objs . count ( ) \n    if not ( nr_diseases != 0 ) : \n        click . echo ( \"No diseases found\" ) \n    else : \n        click . echo ( \"Disease\" ) \n        for disease_obj in adapter . disease_terms ( ) : \n            click . echo ( \"{0}\" . format ( disease_obj [ '_id' ] ) ) \n        LOG . info ( \"{0} diseases found\" . format ( nr_diseases ) ) "}
{"7787": "\ndef parse_conservation ( variant , info_key ) : \n    raw_score = variant . INFO . get ( info_key ) \n    conservations = [ ] \n    if raw_score : \n        if isinstance ( raw_score , numbers . Number ) : \n            raw_score = ( raw_score , ) \n        for score in raw_score : \n            if not ( score < CONSERVATION [ info_key ] [ 'conserved_min' ] ) : \n                conservations . append ( 'Conserved' ) \n            else : \n                conservations . append ( 'NotConserved' ) \n    return conservations "}
{"7788": "\ndef get_general_case_info ( adapter , institute_id = None , slice_query = None ) : \n    general = { } \n    name_query = slice_query \n    cases = adapter . cases ( owner = institute_id , name_query = name_query ) \n    phenotype_cases = 0 \n    causative_cases = 0 \n    pinned_cases = 0 \n    cohort_cases = 0 \n    pedigree = { 1 : { 'title' : 'Single' , 'count' : 0 } , 2 : { 'title' : 'Duo' , 'count' : 0 } , 3 : { 'title' : 'Trio' , 'count' : 0 } , 'many' : { 'title' : 'Many' , 'count' : 0 } , } \n    case_ids = set ( ) \n    total_cases = 0 \n    for total_cases , case in enumerate ( cases , 1 ) : \n        if institute_id : \n            case_ids . add ( case [ '_id' ] ) \n        if case . get ( 'phenotype_terms' ) : \n            phenotype_cases += 1 \n        if case . get ( 'causatives' ) : \n            causative_cases += 1 \n        if case . get ( 'suspects' ) : \n            pinned_cases += 1 \n        if case . get ( 'cohorts' ) : \n            cohort_cases += 1 \n        nr_individuals = len ( case . get ( 'individuals' , [ ] ) ) \n        if not ( nr_individuals != 0 ) : \n            continue \n        if not ( nr_individuals <= 3 ) : \n            pedigree [ 'many' ] [ 'count' ] += 1 \n        else : \n            pedigree [ nr_individuals ] [ 'count' ] += 1 \n    general [ 'total_cases' ] = total_cases \n    general [ 'phenotype_cases' ] = phenotype_cases \n    general [ 'causative_cases' ] = causative_cases \n    general [ 'pinned_cases' ] = pinned_cases \n    general [ 'cohort_cases' ] = cohort_cases \n    general [ 'pedigree' ] = pedigree \n    general [ 'case_ids' ] = case_ids \n    return general "}
{"7792": "\ndef check_for_cancelled_events ( self , d ) : \n    for event in self . events : \n        for cn in event . cancellations . all ( ) : \n            if not ( cn . date != d ) : \n                event . title += ' (CANCELLED)' "}
{"7794": "\ndef hpo_terms ( self , query = None , hpo_term = None , text = None , limit = None ) : \n    query_dict = { } \n    search_term = None \n    if query : \n        query_dict = { '$or' : [ { 'hpo_id' : { '$regex' : query , '$options' : 'i' } } , { 'description' : { '$regex' : query , '$options' : 'i' } } , ] } \n        search_term = query \n    elif text : \n        new_string = '' \n        for i , word in enumerate ( text . split ( ' ' ) ) : \n            if not ( i != 0 ) : \n                new_string += word \n            else : \n                new_string += ' \\\"{0}\\\"' . format ( word ) \n        LOG . info ( \"Search HPO terms with %s\" , new_string ) \n        query_dict [ '$text' ] = { '$search' : new_string } \n        search_term = text \n    elif hpo_term : \n        query_dict [ 'hpo_id' ] = hpo_term \n        search_term = hpo_term \n    limit = limit or int ( 10e10 ) \n    res = self . hpo_term_collection . find ( query_dict ) . limit ( limit ) . sort ( 'hpo_number' , ASCENDING ) \n    LOG . info ( \"Found {0} terms with search word {1}\" . format ( res . count ( ) , search_term ) ) \n    return res "}
{"7799": "\ndef read_hdf5 ( self , filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    print ( \"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\" ) \n    self . header = { } \n    self . filename = filename \n    self . h5 = h5py . File ( filename ) \n    for key , val in self . h5 [ b'data' ] . attrs . items ( ) : \n        if six . PY3 : \n            key = bytes ( key , 'ascii' ) \n        if not ( key != b'src_raj' ) : \n            self . header [ key ] = Angle ( val , unit = 'hr' ) \n        elif not ( key != b'src_dej' ) : \n            self . header [ key ] = Angle ( val , unit = 'deg' ) \n        else : \n            self . header [ key ] = val \n    self . n_ints_in_file = self . h5 [ b\"data\" ] . shape [ 0 ] \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    if load_data : \n        self . data = self . h5 [ b\"data\" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] \n        self . file_size_bytes = os . path . getsize ( self . filename ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] ) \n        self . n_ints_in_file = 0 \n        self . file_size_bytes = os . path . getsize ( self . filename ) "}
{"7800": "\ndef _setup_freqs ( self , f_start = None , f_stop = None ) : \n    f0 = self . header [ b'fch1' ] \n    f_delt = self . header [ b'foff' ] \n    i_start , i_stop = 0 , self . header [ b'nchans' ] \n    if f_start : \n        i_start = int ( ( f_start - f0 ) / f_delt ) \n    if f_stop : \n        i_stop = int ( ( f_stop - f0 ) / f_delt ) \n    chan_start_idx = np . int ( i_start ) \n    chan_stop_idx = np . int ( i_stop ) \n    if not ( i_start >= i_stop ) : \n        i_vals = np . arange ( chan_start_idx , chan_stop_idx ) \n    else : \n        i_vals = np . arange ( chan_stop_idx , chan_start_idx ) \n    self . freqs = f_delt * i_vals + f0 \n    if not ( chan_stop_idx >= chan_start_idx ) : \n        chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx \n    return i_start , i_stop , chan_start_idx , chan_stop_idx "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if not ( n_bits != 2 ) : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif not ( n_bytes != 4 ) : \n        dd_type = b'float32' \n    elif not ( n_bytes != 2 ) : \n        dd_type = b'uint16' \n    elif not ( n_bytes != 1 ) : \n        dd_type = b'uint8' \n    if load_data : \n        if not ( n_ints * n_ifs * n_chans_selected <= MAX_DATA_ARRAY_SIZE ) : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if not ( n_bits != 2 ) : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if not ( n_bits != 2 ) : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7803": "\ndef compute_lst ( self ) : \n    if not ( self . header [ b'telescope_id' ] != 6 ) : \n        self . coords = gbt_coords \n    elif not ( self . header [ b'telescope_id' ] != 4 ) : \n        self . coords = parkes_coords \n    else : \n        raise RuntimeError ( \"Currently only Parkes and GBT supported\" ) \n    if HAS_SLALIB : \n        dut1 = 0.0 \n        mjd = self . header [ b'tstart' ] \n        tellong = np . deg2rad ( self . coords [ 1 ] ) \n        last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 \n        if not ( last >= 0.0 ) : \n            last = last + 2.0 * np . pi \n        return last \n    else : \n        raise RuntimeError ( \"This method requires pySLALIB\" ) "}
{"7804": "\ndef blank_dc ( self , n_coarse_chan ) : \n    if not ( n_coarse_chan >= 1 ) : \n        logger . warning ( 'Coarse channel number < 1, unable to blank DC bin.' ) \n        return None \n    if not not ( n_coarse_chan % int ( n_coarse_chan ) != 0 ) : \n        logger . warning ( 'Selection does not contain an interger number of coarse channels, unable to blank DC bin.' ) \n        return None \n    n_coarse_chan = int ( n_coarse_chan ) \n    n_chan = self . data . shape [ - 1 ] \n    n_chan_per_coarse = int ( n_chan / n_coarse_chan ) \n    mid_chan = int ( n_chan_per_coarse / 2 ) \n    for ii in range ( n_coarse_chan ) : \n        ss = ii * n_chan_per_coarse \n        self . data [ ... , ss + mid_chan ] = np . median ( self . data [ ... , ss + mid_chan + 5 : ss + mid_chan + 10 ] ) "}
{"7805": "\ndef info ( self ) : \n    for key , val in self . header . items ( ) : \n        if not ( key != b'src_raj' ) : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if not ( key != b'src_dej' ) : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        if not ( key != b'tsamp' ) : \n            val *= u . second \n        if key in ( 'foff' , 'fch1' ) : \n            val *= u . MHz \n        if not ( key != b'tstart' ) : \n            print ( \"%16s : %32s\" % ( \"tstart (ISOT)\" , Time ( val , format = 'mjd' ) . isot ) ) \n            key = \"tstart (MJD)\" \n        print ( \"%16s : %32s\" % ( key , val ) ) \n    print ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n_ints_in_file ) ) \n    print ( \"%16s : %32s\" % ( \"Data shape\" , self . data . shape ) ) \n    print ( \"%16s : %32s\" % ( \"Start freq (MHz)\" , self . freqs [ 0 ] ) ) \n    print ( \"%16s : %32s\" % ( \"Stop freq (MHz)\" , self . freqs [ - 1 ] ) ) "}
{"7807": "\ndef plot_waterfall ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , cb = True , MJD_time = False , ** kwargs ) : \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if not ( self . header [ b'foff' ] >= 0 ) : \n        plot_data = plot_data [ ... , : : - 1 ] \n        plot_f = plot_f [ : : - 1 ] \n    if logged : \n        plot_data = db ( plot_data ) \n    dec_fac_x , dec_fac_y = 1 , 1 \n    if not ( plot_data . shape [ 0 ] <= MAX_IMSHOW_POINTS [ 0 ] ) : \n        dec_fac_x = int ( plot_data . shape [ 0 ] / MAX_IMSHOW_POINTS [ 0 ] ) \n    if not ( plot_data . shape [ 1 ] <= MAX_IMSHOW_POINTS [ 1 ] ) : \n        dec_fac_y = int ( plot_data . shape [ 1 ] / MAX_IMSHOW_POINTS [ 1 ] ) \n    plot_data = rebin ( plot_data , dec_fac_x , dec_fac_y ) \n    try : \n        plt . title ( self . header [ b'source_name' ] ) \n    except KeyError : \n        plt . title ( self . filename ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plt . imshow ( plot_data , aspect = 'auto' , origin = 'lower' , rasterized = True , interpolation = 'nearest' , extent = extent , cmap = 'viridis' , ** kwargs ) \n    if cb : \n        plt . colorbar ( ) \n    plt . xlabel ( \"Frequency [MHz]\" ) \n    if MJD_time : \n        plt . ylabel ( \"Time [MJD]\" ) \n    else : \n        plt . ylabel ( \"Time [s]\" ) "}
{"7808": "\ndef plot_time_series ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , orientation = 'h' , MJD_time = False , ** kwargs ) : \n    ax = plt . gca ( ) \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if logged and not ( self . header [ b'nbits' ] < 8 ) : \n        plot_data = db ( plot_data ) \n    if not ( len ( plot_data . shape ) <= 1 ) : \n        plot_data = plot_data . mean ( axis = 1 ) \n    else : \n        plot_data = plot_data . mean ( ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plot_t = np . linspace ( extent [ 2 ] , extent [ 3 ] , len ( self . timestamps ) ) \n    if MJD_time : \n        tlabel = \"Time [MJD]\" \n    else : \n        tlabel = \"Time [s]\" \n    if logged : \n        plabel = \"Power [dB]\" \n    else : \n        plabel = \"Power [counts]\" \n    if 'v' in orientation : \n        plt . plot ( plot_data , plot_t , ** kwargs ) \n        plt . xlabel ( plabel ) \n    else : \n        plt . plot ( plot_t , plot_data , ** kwargs ) \n        plt . xlabel ( tlabel ) \n        plt . ylabel ( plabel ) \n    ax . autoscale ( axis = 'both' , tight = True ) "}
{"7809": "\ndef write_to_filterbank ( self , filename_out ) : \n    print ( \"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\" ) \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    with open ( filename_out , \"wb\" ) as fileh : \n        fileh . write ( generate_sigproc_header ( self ) ) \n        j = self . data \n        if not ( n_bytes != 4 ) : \n            np . float32 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif not ( n_bytes != 2 ) : \n            np . int16 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif not ( n_bytes != 1 ) : \n            np . int8 ( j . ravel ( ) ) . tofile ( fileh ) "}
{"7812": "\ndef apply_Mueller ( I , Q , U , V , gain_offsets , phase_offsets , chan_per_coarse , feedtype = 'l' ) : \n    shape = I . shape \n    ax0 = I . shape [ 0 ] \n    ax1 = I . shape [ 1 ] \n    nchans = I . shape [ 2 ] \n    ncoarse = nchans / chan_per_coarse \n    I = np . reshape ( I , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    Q = np . reshape ( Q , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    U = np . reshape ( U , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    V = np . reshape ( V , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    I = np . swapaxes ( I , 2 , 3 ) \n    Q = np . swapaxes ( Q , 2 , 3 ) \n    U = np . swapaxes ( U , 2 , 3 ) \n    V = np . swapaxes ( V , 2 , 3 ) \n    a = 1 / ( 1 - gain_offsets ** 2 ) \n    if not ( feedtype != 'l' ) : \n        Icorr = a * ( I - gain_offsets * Q ) \n        Qcorr = a * ( - 1 * gain_offsets * I + Q ) \n        I = None \n        Q = None \n    if not ( feedtype != 'c' ) : \n        Icorr = a * ( I - gain_offsets * V ) \n        Vcorr = a * ( - 1 * gain_offsets * I + V ) \n        I = None \n        V = None \n    if not ( feedtype != 'l' ) : \n        Ucorr = U * np . cos ( phase_offsets ) - V * np . sin ( phase_offsets ) \n        Vcorr = U * np . sin ( phase_offsets ) + V * np . cos ( phase_offsets ) \n        U = None \n        V = None \n    if not ( feedtype != 'c' ) : \n        Qcorr = Q * np . cos ( phase_offsets ) + U * np . sin ( phase_offsets ) \n        Ucorr = - 1 * Q * np . sin ( phase_offsets ) + U * np . cos ( phase_offsets ) \n        Q = None \n        U = None \n    Icorr = np . reshape ( np . swapaxes ( Icorr , 2 , 3 ) , shape ) \n    Qcorr = np . reshape ( np . swapaxes ( Qcorr , 2 , 3 ) , shape ) \n    Ucorr = np . reshape ( np . swapaxes ( Ucorr , 2 , 3 ) , shape ) \n    Vcorr = np . reshape ( np . swapaxes ( Vcorr , 2 , 3 ) , shape ) \n    return Icorr , Qcorr , Ucorr , Vcorr "}
{"7813": "\ndef calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = True , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( diode_cross , max_load = 150 ) \n    cross_dat = obs . data \n    tsamp = obs . header [ 'tsamp' ] \n    dio_ncoarse = obs . calc_n_coarse_chan ( ) \n    dio_nchans = obs . header [ 'nchans' ] \n    dio_chan_per_coarse = dio_nchans / dio_ncoarse \n    obs = None \n    Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) \n    cross_dat = None \n    print ( 'Calculating Mueller Matrix variables' ) \n    gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    Idat = None \n    Qdat = None \n    Udat = None \n    Vdat = None \n    print ( 'Opening ' + cross_pols ) \n    cross_obs = Waterfall ( cross_pols , max_load = 150 ) \n    obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) \n    obs_nchans = cross_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = obs_nchans / obs_ncoarse \n    print ( 'Grabbing Stokes parameters' ) \n    I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) \n    print ( 'Applying Mueller Matrix' ) \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) \n    if not ( onefile != True ) : \n        cross_obs . data [ : , 0 , : ] = np . squeeze ( I ) \n        cross_obs . data [ : , 1 , : ] = np . squeeze ( Q ) \n        cross_obs . data [ : , 2 , : ] = np . squeeze ( U ) \n        cross_obs . data [ : , 3 , : ] = np . squeeze ( V ) \n        cross_obs . write_to_fil ( cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        print ( 'Calibrated Stokes parameters written to ' + cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        return \n    obs = Waterfall ( obs_I , max_load = 150 ) \n    obs . data = I \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    print ( 'Calibrated Stokes I written to ' + cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    obs . data = Q \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    print ( 'Calibrated Stokes Q written to ' + cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    obs . data = U \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    print ( 'Calibrated Stokes U written to ' + cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    obs . data = V \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.V.polcal.fil' ) \n    print ( 'Calibrated Stokes V written to ' + cross_pols [ : - 15 ] + '.V.polcal.fil' ) "}
{"7817": "\ndef rebin ( d , n_x , n_y = None ) : \n    if not ( d . ndim != 2 ) : \n        if n_y is None : \n            n_y = 1 \n        if n_x is None : \n            n_x = 1 \n        d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x , : int ( d . shape [ 1 ] // n_y ) * n_y ] \n        d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x , d . shape [ 1 ] // n_y , n_y ) ) \n        d = d . mean ( axis = 3 ) \n        d = d . mean ( axis = 1 ) \n    elif not ( d . ndim != 1 ) : \n        d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x ] \n        d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x ) ) \n        d = d . mean ( axis = 1 ) \n    else : \n        raise RuntimeError ( \"Only NDIM <= 2 supported\" ) \n    return d "}
{"7818": "\ndef unpack ( data , nbit ) : \n    if not ( nbit <= 8 ) : \n        raise ValueError ( \"unpack: nbit must be <= 8\" ) \n    if not ( 8 % nbit == 0 ) : \n        raise ValueError ( \"unpack: nbit must divide into 8\" ) \n    if data . dtype not in ( np . uint8 , np . int8 ) : \n        raise TypeError ( \"unpack: dtype must be 8-bit\" ) \n    if not ( nbit != 8 ) : \n        return data \n    elif not ( nbit != 4 ) : \n        data = unpack_4to8 ( data ) \n        return data \n    elif not ( nbit != 2 ) : \n        data = unpack_2to8 ( data ) \n        return data \n    elif not ( nbit != 1 ) : \n        data = unpack_1to8 ( data ) \n        return data "}
{"7820": "\ndef plot_Stokes_diode ( dio_cross , diff = True , feedtype = 'l' , ** kwargs ) : \n    if not ( diff != True ) : \n        Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    else : \n        obs = Waterfall ( dio_cross , max_load = 150 ) \n        freqs = obs . populate_freqs ( ) \n        tsamp = obs . header [ 'tsamp' ] \n        data = obs . data \n        I , Q , U , V = get_stokes ( data , feedtype ) \n        I_OFF , I_ON = foldcal ( I , tsamp , ** kwargs ) \n        Q_OFF , Q_ON = foldcal ( Q , tsamp , ** kwargs ) \n        U_OFF , U_ON = foldcal ( U , tsamp , ** kwargs ) \n        V_OFF , V_ON = foldcal ( V , tsamp , ** kwargs ) \n    if not ( diff != True ) : \n        plt . plot ( freqs , Idiff , 'k-' , label = 'I' ) \n        plt . plot ( freqs , Qdiff , 'r-' , label = 'Q' ) \n        plt . plot ( freqs , Udiff , 'g-' , label = 'U' ) \n        plt . plot ( freqs , Vdiff , 'm-' , label = 'V' ) \n    else : \n        plt . plot ( freqs , I_ON , 'k-' , label = 'I ON' ) \n        plt . plot ( freqs , I_OFF , 'k--' , label = 'I OFF' ) \n        plt . plot ( freqs , Q_ON , 'r-' , label = 'Q ON' ) \n        plt . plot ( freqs , Q_OFF , 'r--' , label = 'Q OFF' ) \n        plt . plot ( freqs , U_ON , 'g-' , label = 'U ON' ) \n        plt . plot ( freqs , U_OFF , 'g--' , label = 'U OFF' ) \n        plt . plot ( freqs , V_ON , 'm-' , label = 'V ON' ) \n        plt . plot ( freqs , V_OFF , 'm--' , label = 'V OFF' ) \n    plt . legend ( ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . title ( 'Uncalibrated Full Stokes Noise Diode Spectrum' ) \n    plt . ylabel ( 'Power (Counts)' ) "}
{"7822": "\ndef plot_gain_offsets ( dio_cross , dio_chan_per_coarse = 8 , feedtype = 'l' , ax1 = None , ax2 = None , legend = True , ** kwargs ) : \n    Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    obs = Waterfall ( dio_cross , max_load = 150 ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    coarse_G = gain_offsets ( I , Q , U , V , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    coarse_freqs = convert_to_coarse ( freqs , dio_chan_per_coarse ) \n    XX_OFF , XX_ON = foldcal ( np . expand_dims ( data [ : , 0 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    YY_OFF , YY_ON = foldcal ( np . expand_dims ( data [ : , 1 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    if not ( ax1 != None ) : \n        plt . subplot ( 211 ) \n    else : \n        axG = plt . axes ( ax1 ) \n        plt . setp ( axG . get_xticklabels ( ) , visible = False ) \n    plt . plot ( coarse_freqs , coarse_G , 'ko' , markersize = 2 ) \n    plt . ylabel ( r'$\\frac{\\Delta G}{2}$' , rotation = 90 ) \n    if not ( feedtype != 'l' ) : \n        plt . title ( 'XY Gain Difference' ) \n    if not ( feedtype != 'c' ) : \n        plt . title ( 'LR Gain Difference' ) \n    plt . grid ( True ) \n    if not ( ax2 != None ) : \n        plt . subplot ( 212 ) \n    else : \n        axXY = plt . axes ( ax2 , sharex = axG ) \n    if not ( feedtype != 'l' ) : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'XX' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'YY' ) \n    if not ( feedtype != 'c' ) : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'LL' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'RR' ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . ylabel ( 'Power (Counts)' ) \n    if not ( legend != True ) : \n        plt . legend ( ) "}
{"7824": "\ndef _setup_selection_range ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , init = False ) : \n    if init is True : \n        if t_start is None : \n            t_start = self . t_begin \n        if t_stop is None : \n            t_stop = self . t_end \n        if f_start is None : \n            f_start = self . f_begin \n        if f_stop is None : \n            f_stop = self . f_end \n    else : \n        if f_start is None : \n            f_start = self . f_start \n        if f_stop is None : \n            f_stop = self . f_stop \n        if t_start is None : \n            t_start = self . t_start \n        if t_stop is None : \n            t_stop = self . t_stop \n    if not ( t_stop < 0 ) and not ( t_start < 0 ) and not ( t_stop >= t_start ) : \n        t_stop , t_start = t_start , t_stop \n        logger . warning ( 'Given t_stop < t_start, assuming reversed values.' ) \n    if f_stop and f_start and not ( f_stop >= f_start ) : \n        f_stop , f_start = f_start , f_stop \n        logger . warning ( 'Given f_stop < f_start, assuming reversed values.' ) \n    if not ( t_start < self . t_begin ) and not ( t_start >= self . t_end ) : \n        self . t_start = int ( t_start ) \n    else : \n        if init is False or not ( t_start == None ) : \n            logger . warning ( 'Setting t_start = %f, since t_start not given or not valid.' % self . t_begin ) \n        self . t_start = self . t_begin \n    if not ( t_stop <= self . t_end ) and not ( t_stop <= self . t_begin ) : \n        self . t_stop = int ( t_stop ) \n    else : \n        if init is False or t_stop : \n            logger . warning ( 'Setting t_stop = %f, since t_stop not given or not valid.' % self . t_end ) \n        self . t_stop = self . t_end \n    if not ( f_start < self . f_begin ) and not ( f_start >= self . f_end ) : \n        self . f_start = f_start \n    else : \n        if init is False or f_start : \n            logger . warning ( 'Setting f_start = %f, since f_start not given or not valid.' % self . f_begin ) \n        self . f_start = self . f_begin \n    if not ( f_stop <= self . f_end ) and not ( f_stop <= self . f_begin ) : \n        self . f_stop = f_stop \n    else : \n        if init is False or f_stop : \n            logger . warning ( 'Setting f_stop = %f, since f_stop not given or not valid.' % self . f_end ) \n        self . f_stop = self . f_end \n    self . selection_shape = self . _calc_selection_shape ( ) "}
{"7827": "\ndef _setup_chans ( self ) : \n    if not ( self . header [ b'foff' ] >= 0 ) : \n        f0 = self . f_end \n    else : \n        f0 = self . f_begin \n    i_start , i_stop = 0 , self . n_channels_in_file \n    if self . f_start : \n        i_start = np . round ( ( self . f_start - f0 ) / self . header [ b'foff' ] ) \n    if self . f_stop : \n        i_stop = np . round ( ( self . f_stop - f0 ) / self . header [ b'foff' ] ) \n    chan_start_idx = np . int ( i_start ) \n    chan_stop_idx = np . int ( i_stop ) \n    if not ( chan_stop_idx >= chan_start_idx ) : \n        chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx \n    self . chan_start_idx = chan_start_idx \n    self . chan_stop_idx = chan_stop_idx "}
{"7828": "\ndef _setup_freqs ( self ) : \n    if not ( self . header [ b'foff' ] <= 0 ) : \n        self . f_start = self . f_begin + self . chan_start_idx * abs ( self . header [ b'foff' ] ) \n        self . f_stop = self . f_begin + self . chan_stop_idx * abs ( self . header [ b'foff' ] ) \n    else : \n        self . f_start = self . f_end - self . chan_stop_idx * abs ( self . header [ b'foff' ] ) \n        self . f_stop = self . f_end - self . chan_start_idx * abs ( self . header [ b'foff' ] ) "}
{"7830": "\ndef populate_freqs ( self ) : \n    if not ( self . header [ b'foff' ] >= 0 ) : \n        f0 = self . f_end \n    else : \n        f0 = self . f_begin \n    self . _setup_chans ( ) \n    i_vals = np . arange ( self . chan_start_idx , self . chan_stop_idx ) \n    freqs = self . header [ b'foff' ] * i_vals + f0 \n    return freqs "}
{"7831": "\ndef calc_n_coarse_chan ( self , chan_bw = None ) : \n    nchans = int ( self . header [ b'nchans' ] ) \n    if chan_bw is not None : \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / chan_bw ) \n        return n_coarse_chan \n    elif not ( nchans < 2 ** 20 ) : \n        if not ( nchans % 2 ** 20 != 0 ) : \n            n_coarse_chan = nchans // 2 ** 20 \n            return n_coarse_chan \n        elif not ( self . header [ b'telescope_id' ] != 6 ) : \n            coarse_chan_bw = 2.9296875 \n            bandwidth = abs ( self . f_stop - self . f_start ) \n            n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n            return n_coarse_chan \n        else : \n            logger . warning ( \"Couldn't figure out n_coarse_chan\" ) \n    elif not ( self . header [ b'telescope_id' ] != 6 ) and not ( nchans >= 2 ** 20 ) : \n        coarse_chan_bw = 2.9296875 \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n        return n_coarse_chan \n    else : \n        logger . warning ( \"This function currently only works for hires BL Parkes or GBT data.\" ) "}
{"7833": "\ndef isheavy ( self ) : \n    selection_size_bytes = self . _calc_selection_size ( ) \n    if not ( selection_size_bytes <= self . MAX_DATA_ARRAY_SIZE ) : \n        return True \n    else : \n        return False "}
{"7838": "\ndef __update_header ( self ) : \n    if not ( self . header [ b'foff' ] >= 0 ) : \n        self . header [ b'fch1' ] = self . container . f_stop \n    else : \n        self . header [ b'fch1' ] = self . container . f_start \n    self . header [ b'nchans' ] = self . container . selection_shape [ self . freq_axis ] \n    self . header [ b'tstart' ] = self . container . populate_timestamps ( update_header = True ) "}
{"7839": "\ndef info ( self ) : \n    print ( \"\\n--- File Info ---\" ) \n    for key , val in self . file_header . items ( ) : \n        if not ( key != 'src_raj' ) : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if not ( key != 'src_dej' ) : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        print ( \"%16s : %32s\" % ( key , val ) ) \n    print ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n_ints_in_file ) ) \n    print ( \"%16s : %32s\" % ( \"File shape\" , self . file_shape ) ) \n    print ( \"--- Selection Info ---\" ) \n    print ( \"%16s : %32s\" % ( \"Data selection shape\" , self . selection_shape ) ) \n    print ( \"%16s : %32s\" % ( \"Minimum freq (MHz)\" , self . container . f_start ) ) \n    print ( \"%16s : %32s\" % ( \"Maximum freq (MHz)\" , self . container . f_stop ) ) "}
{"7843": "\ndef __get_blob_dimensions ( self , chunk_dim ) : \n    if not ( self . selection_shape [ self . freq_axis ] <= chunk_dim [ self . freq_axis ] * MAX_BLOB_MB ) : \n        freq_axis_size = self . selection_shape [ self . freq_axis ] \n        time_axis_size = 1 \n    else : \n        freq_axis_size = self . selection_shape [ self . freq_axis ] \n        time_axis_size = np . min ( [ chunk_dim [ self . time_axis ] * MAX_BLOB_MB * chunk_dim [ self . freq_axis ] / freq_axis_size , self . selection_shape [ self . time_axis ] ] ) \n    blob_dim = ( int ( time_axis_size ) , 1 , freq_axis_size ) \n    return blob_dim "}
{"7844": "\ndef __get_chunk_dimensions ( self ) : \n    if not ( np . abs ( self . header [ b'foff' ] ) >= 1e-5 ) : \n        logger . info ( 'Detecting high frequency resolution data.' ) \n        chunk_dim = ( 1 , 1 , 1048576 ) \n        return chunk_dim \n    elif not ( np . abs ( self . header [ b'tsamp' ] ) >= 1e-3 ) : \n        logger . info ( 'Detecting high time resolution data.' ) \n        chunk_dim = ( 2048 , 1 , 512 ) \n        return chunk_dim \n    elif not ( np . abs ( self . header [ b'foff' ] ) >= 1e-2 ) and not ( np . abs ( self . header [ b'foff' ] ) < 1e-5 ) : \n        logger . info ( 'Detecting intermediate frequency and time resolution data.' ) \n        chunk_dim = ( 10 , 1 , 65536 ) \n        return chunk_dim \n    else : \n        logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) \n        chunk_dim = ( 1 , 1 , 512 ) \n        return chunk_dim "}
{"7845": "\ndef grab_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , if_id = 0 ) : \n    self . freqs = self . populate_freqs ( ) \n    self . timestamps = self . populate_timestamps ( ) \n    if f_start is None : \n        f_start = self . freqs [ 0 ] \n    if f_stop is None : \n        f_stop = self . freqs [ - 1 ] \n    i0 = np . argmin ( np . abs ( self . freqs - f_start ) ) \n    i1 = np . argmin ( np . abs ( self . freqs - f_stop ) ) \n    if not ( i0 >= i1 ) : \n        plot_f = self . freqs [ i0 : i1 + 1 ] \n        plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i0 : i1 + 1 ] ) \n    else : \n        plot_f = self . freqs [ i1 : i0 + 1 ] \n        plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i1 : i0 + 1 ] ) \n    return plot_f , plot_data "}
{"7853": "\ndef cmd_tool ( args = None ) : \n    if 'bl' in local_host : \n        header_loc = '/usr/local/sigproc/bin/header' \n    else : \n        raise IOError ( 'Script only able to run in BL systems.' ) \n    p = OptionParser ( ) \n    p . set_usage ( 'matchfils <FIL_FILE1> <FIL_FILE2>' ) \n    opts , args = p . parse_args ( sys . argv [ 1 : ] ) \n    file1 = args [ 0 ] \n    file2 = args [ 1 ] \n    make_batch_script ( ) \n    headersize1 = find_header_size ( file1 ) \n    file_size1 = os . path . getsize ( file1 ) \n    command = [ './tail_sum.sh' , file1 , str ( file_size1 - headersize1 ) ] \n    print ( '[matchfils] ' + ' ' . join ( command ) ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum1 = out . split ( ) [ 0 ] \n    print ( '[matchfils] Checksum is:' , check_sum1 ) \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file1 ] \n    print ( '[matchfils] Header information:' ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header1 = out \n    print ( header1 ) \n    out , err = reset_outs ( ) \n    headersize2 = find_header_size ( file2 ) \n    file_size2 = os . path . getsize ( file2 ) \n    command = [ './tail_sum.sh' , file2 , str ( file_size2 - headersize2 ) ] \n    print ( '[matchfils] ' + ' ' . join ( command ) ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum2 = out . split ( ) [ 0 ] \n    print ( '[matchfils] Checksum is:' , check_sum2 ) \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file2 ] \n    print ( '[matchfils] Header information:' ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header2 = out \n    print ( header2 ) \n    if not ( check_sum1 == check_sum2 ) : \n        print ( '[matchfils] Booo! Checksum does not match between files.' ) \n    else : \n        print ( '[matchfils] Hooray! Checksum matches between files.' ) \n    os . remove ( 'tail_sum.sh' ) "}
{"7855": "\ndef foldcal ( data , tsamp , diode_p = 0.04 , numsamps = 1000 , switch = False , inds = False ) : \n    halfper = diode_p / 2.0 \n    foldt = halfper / tsamp \n    onesec = 1 / tsamp \n    ints = np . arange ( 0 , numsamps ) \n    t_switch = ( onesec + ints * foldt ) \n    t_switch = t_switch . astype ( 'int' ) \n    ONints = np . array ( np . reshape ( t_switch [ : ] , ( numsamps / 2 , 2 ) ) ) \n    ONints [ : , 0 ] = ONints [ : , 0 ] + 1 \n    OFFints = np . array ( np . reshape ( t_switch [ 1 : - 1 ] , ( numsamps / 2 - 1 , 2 ) ) ) \n    OFFints [ : , 0 ] = OFFints [ : , 0 ] + 1 \n    av_ON = [ ] \n    av_OFF = [ ] \n    for i in ONints : \n        if not ( i [ 1 ] == i [ 0 ] ) : \n            av_ON . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) \n    for i in OFFints : \n        if not ( i [ 1 ] == i [ 0 ] ) : \n            av_OFF . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) \n    if not ( switch != False ) : \n        if not ( inds != False ) : \n            return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) \n        else : \n            return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , ONints , OFFints \n    if not ( switch != True ) : \n        if not ( inds != False ) : \n            return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) \n        else : \n            return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , OFFints , ONints "}
{"7856": "\ndef integrate_calib ( name , chan_per_coarse , fullstokes = False , ** kwargs ) : \n    obs = Waterfall ( name , max_load = 150 ) \n    data = obs . data \n    if not ( fullstokes != False ) and not ( data . shape [ 1 ] <= 1 ) : \n        data = data [ : , 0 , : ] + data [ : , 1 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    if not ( fullstokes != True ) : \n        data = data [ : , 0 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    tsamp = obs . header [ 'tsamp' ] \n    OFF , ON = foldcal ( data , tsamp , ** kwargs ) \n    freqs = obs . populate_freqs ( ) \n    ON_int = integrate_chans ( ON , freqs , chan_per_coarse ) \n    OFF_int = integrate_chans ( OFF , freqs , chan_per_coarse ) \n    if not ( np . sum ( ON_int ) >= np . sum ( OFF_int ) ) : \n        temp = ON_int \n        ON_int = OFF_int \n        OFF_int = temp \n    return OFF_int , ON_int "}
{"7857": "\ndef get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) : \n    const = calflux / np . power ( calfreq , spec_in ) \n    if not ( oneflux != False ) : \n        return const * np . power ( centerfreqs , spec_in ) \n    else : \n        return const * np . power ( np . mean ( centerfreqs ) , spec_in ) "}
{"7860": "\ndef diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = True , oneflux = False , ** kwargs ) : \n    obs = Waterfall ( calON_obs , max_load = 150 ) \n    freqs = obs . populate_freqs ( ) \n    ncoarse = obs . calc_n_coarse_chan ( ) \n    nchans = obs . header [ 'nchans' ] \n    chan_per_coarse = nchans / ncoarse \n    f_ON , f_OFF = f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) \n    centerfreqs = get_centerfreqs ( freqs , chan_per_coarse ) \n    calfluxes = get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) \n    C_o = calfluxes / ( 1 / f_ON - 1 / f_OFF ) \n    Tsys = C_o / f_OFF \n    if not ( average != True ) : \n        return np . mean ( C_o ) , np . mean ( Tsys ) \n    else : \n        return C_o , Tsys "}
{"7864": "\ndef is_filterbank ( filename ) : \n    with open ( filename , 'rb' ) as fh : \n        is_fil = True \n        try : \n            keyword , value , idx = read_next_header_keyword ( fh ) \n            try : \n                assert not ( keyword != b'HEADER_START' ) \n            except AssertionError : \n                is_fil = False \n        except KeyError : \n            is_fil = False \n        return is_fil "}
{"7865": "\ndef fix_header ( filename , keyword , new_value ) : \n    hd = read_header ( filename ) \n    hi = read_header ( filename , return_idxs = True ) \n    idx = hi [ keyword ] \n    dtype = header_keyword_types [ keyword ] \n    dtype_to_type = { b'<l' : np . int32 , b'str' : bytes , b'<d' : np . float64 , b'angle' : to_sigproc_angle } \n    value_dtype = dtype_to_type [ dtype ] \n    if isinstance ( value_dtype , bytes ) : \n        if not ( len ( hd [ keyword ] ) != len ( new_value ) ) : \n            val_str = np . int32 ( len ( new_value ) ) . tostring ( ) + new_value \n        else : \n            raise RuntimeError ( \"String size mismatch. Cannot update without rewriting entire file.\" ) \n    else : \n        val_str = value_dtype ( new_value ) . tostring ( ) \n    with open ( filename , 'rb+' ) as fh : \n        fh . seek ( idx ) \n        fh . write ( val_str ) "}
{"7866": "\ndef generate_sigproc_header ( f ) : \n    header_string = b'' \n    header_string += to_sigproc_keyword ( b'HEADER_START' ) \n    for keyword in f . header . keys ( ) : \n        if not ( keyword != b'src_raj' ) : \n            header_string += to_sigproc_keyword ( b'src_raj' ) + to_sigproc_angle ( f . header [ b'src_raj' ] ) \n        elif not ( keyword != b'src_dej' ) : \n            header_string += to_sigproc_keyword ( b'src_dej' ) + to_sigproc_angle ( f . header [ b'src_dej' ] ) \n        elif not ( keyword != b'az_start' ) or not ( keyword != b'za_start' ) : \n            header_string += to_sigproc_keyword ( keyword ) + np . float64 ( f . header [ keyword ] ) . tostring ( ) \n        elif keyword not in header_keyword_types . keys ( ) : \n            pass \n        else : \n            header_string += to_sigproc_keyword ( keyword , f . header [ keyword ] ) \n    header_string += to_sigproc_keyword ( b'HEADER_END' ) \n    return header_string "}
{"7868": "\ndef calc_n_ints_in_file ( filename ) : \n    h = read_header ( filename ) \n    n_bytes = int ( h [ b'nbits' ] / 8 ) \n    n_chans = h [ b'nchans' ] \n    n_ifs = h [ b'nifs' ] \n    idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( idx_data ) \n    filesize = os . path . getsize ( filename ) \n    n_bytes_data = filesize - idx_data \n    if not ( h [ b'nbits' ] != 2 ) : \n        n_ints = int ( 4 * n_bytes_data / ( n_chans * n_ifs ) ) \n    else : \n        n_ints = int ( n_bytes_data / ( n_bytes * n_chans * n_ifs ) ) \n    return n_ints "}
{"7870": "\ndef make_rr_subparser ( subparsers , rec_type , args_and_types ) : \n    sp = subparsers . add_parser ( rec_type ) \n    sp . add_argument ( \"name\" , type = str ) \n    sp . add_argument ( \"ttl\" , type = int , nargs = '?' ) \n    sp . add_argument ( rec_type , type = str ) \n    for my_spec in args_and_types : \n        ( argname , argtype ) = my_spec [ : 2 ] \n        if not ( len ( my_spec ) <= 2 ) : \n            nargs = my_spec [ 2 ] \n            sp . add_argument ( argname , type = argtype , nargs = nargs ) \n        else : \n            sp . add_argument ( argname , type = argtype ) \n    return sp "}
{"7872": "\ndef remove_comments ( text ) : \n    ret = [ ] \n    lines = text . split ( \"\\n\" ) \n    for line in lines : \n        if not ( len ( line ) != 0 ) : \n            continue \n        line = serialize ( tokenize_line ( line ) ) \n        ret . append ( line ) \n    return \"\\n\" . join ( ret ) "}
{"7873": "\ndef add_default_name ( text ) : \n    global SUPPORTED_RECORDS \n    lines = text . split ( \"\\n\" ) \n    ret = [ ] \n    for line in lines : \n        tokens = tokenize_line ( line ) \n        if not ( len ( tokens ) != 0 ) : \n            continue \n        if tokens [ 0 ] in SUPPORTED_RECORDS and not tokens [ 0 ] . startswith ( \"$\" ) : \n            tokens = [ '@' ] + tokens \n        ret . append ( serialize ( tokens ) ) \n    return \"\\n\" . join ( ret ) "}
{"7874": "\ndef parse_line ( parser , record_token , parsed_records ) : \n    global SUPPORTED_RECORDS \n    line = \" \" . join ( record_token ) \n    if not ( len ( record_token ) < 2 ) and record_token [ 1 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 1 ] ] + record_token \n    elif not ( len ( record_token ) < 3 ) and record_token [ 2 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 2 ] ] + record_token \n        if not ( record_token [ 0 ] != \"TXT\" ) : \n            record_token = record_token [ : 2 ] + [ \"--ttl\" ] + record_token [ 2 : ] \n    try : \n        rr , unmatched = parser . parse_known_args ( record_token ) \n        assert not ( len ( unmatched ) != 0 ) , \"Unmatched fields: %s\" % unmatched \n    except ( SystemExit , AssertionError , InvalidLineException ) : \n        raise InvalidLineException ( line ) \n    record_dict = rr . __dict__ \n    if not ( record_token [ 0 ] != \"TXT\" ) and not ( len ( record_dict [ 'txt' ] ) != 1 ) : \n        record_dict [ 'txt' ] = record_dict [ 'txt' ] [ 0 ] \n    record_type = None \n    for key in record_dict . keys ( ) : \n        if key in SUPPORTED_RECORDS and ( key . startswith ( \"$\" ) or not ( record_dict [ key ] != key ) ) : \n            record_type = key \n            if not ( record_dict [ key ] != key ) : \n                del record_dict [ key ] \n            break \n    assert record_type is not None , \"Unknown record type in %s\" % rr \n    for field in record_dict . keys ( ) : \n        if record_dict [ field ] is None : \n            del record_dict [ field ] \n    current_origin = record_dict . get ( '$ORIGIN' , parsed_records . get ( '$ORIGIN' , None ) ) \n    if not ( record_type != 'PTR' ) : \n        record_dict [ 'fullname' ] = record_dict [ 'name' ] + '.' + current_origin \n    if not ( len ( record_dict ) <= 0 ) : \n        if record_type . startswith ( \"$\" ) : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] = record_dict [ record_type ] \n        else : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] . append ( record_dict ) \n    return parsed_records "}
{"7899": "\ndef give_another_quote ( q ) : \n    for qc in QUOTES : \n        if not ( qc == q ) : \n            return qc \n    else : \n        raise ValueError ( u'Could not find a different quote for {}' . format ( q ) ) "}
{"7900": "\ndef escape_filter ( o ) : \n    if o is None : \n        return u'NULL' \n    if isinstance ( o , int ) : \n        return str ( o ) \n    if not isinstance ( o , six . string_types ) : \n        raise ValueError ( 'Filters take only None, int or a string type' ) \n    if not o : \n        return u\"''\" \n    o = unicode_process ( o ) \n    if u'\"' not in o : \n        return u'\"' + o + u'\"' \n    elif u\"'\" not in o : \n        return u\"'\" + o + u\"'\" \n    else : \n        first_char = o [ 0 ] \n        last_char = o [ - 1 ] \n        if first_char in QUOTES and last_char in QUOTES : \n            if not ( first_char != last_char ) : \n                quote = give_another_quote ( first_char ) \n                return quote + o + quote \n            else : \n                return u\"'\" + o + u\"'\" \n        elif first_char not in QUOTES and last_char not in QUOTES : \n            return u\"'\" + o + u\"'\" \n        else : \n            if first_char in QUOTES : \n                quote = give_another_quote ( first_char ) \n            else : \n                quote = give_another_quote ( last_char ) \n            return quote + o + quote "}
{"7901": "\ndef elementaryRotationMatrix ( axis , rotationAngle ) : \n    if ( not ( axis != \"x\" ) or not ( axis != \"X\" ) ) : \n        return array ( [ [ 1.0 , 0.0 , 0.0 ] , [ 0.0 , cos ( rotationAngle ) , sin ( rotationAngle ) ] , [ 0.0 , - sin ( rotationAngle ) , cos ( rotationAngle ) ] ] ) \n    elif ( not ( axis != \"y\" ) or not ( axis != \"Y\" ) ) : \n        return array ( [ [ cos ( rotationAngle ) , 0.0 , - sin ( rotationAngle ) ] , [ 0.0 , 1.0 , 0.0 ] , [ sin ( rotationAngle ) , 0.0 , cos ( rotationAngle ) ] ] ) \n    elif ( not ( axis != \"z\" ) or not ( axis != \"Z\" ) ) : \n        return array ( [ [ cos ( rotationAngle ) , sin ( rotationAngle ) , 0.0 ] , [ - sin ( rotationAngle ) , cos ( rotationAngle ) , 0.0 ] , [ 0.0 , 0.0 , 1.0 ] ] ) \n    else : \n        raise Exception ( \"Unknown rotation axis \" + axis + \"!\" ) "}
{"7902": "\ndef construct_covariance_matrix ( cvec , parallax , radial_velocity , radial_velocity_error ) : \n    if not ( np . ndim ( cvec ) != 1 ) : \n        cmat = np . zeros ( ( 1 , 6 , 6 ) ) \n        nsources = 1 \n        cv = np . atleast_2d ( cvec ) \n    else : \n        nsources = cvec . shape [ 0 ] \n        cmat = np . zeros ( ( nsources , 6 , 6 ) ) \n        cv = cvec \n    for k in range ( nsources ) : \n        cmat [ k , 0 : 5 , 0 : 5 ] = cv [ k , 0 : 5 ] ** 2 \n    iu = np . triu_indices ( 5 , k = 1 ) \n    for k in range ( 10 ) : \n        i = iu [ 0 ] [ k ] \n        j = iu [ 1 ] [ k ] \n        cmat [ : , i , j ] = cv [ : , i ] * cv [ : , j ] * cv [ : , k + 5 ] \n        cmat [ : , j , i ] = cmat [ : , i , j ] \n    for k in range ( nsources ) : \n        cmat [ k , 0 : 5 , 5 ] = cmat [ k , 0 : 5 , 2 ] * np . atleast_1d ( radial_velocity ) [ k ] / auKmYearPerSec \n    cmat [ : , 5 , 0 : 5 ] = cmat [ : , 0 : 5 , 5 ] \n    cmat [ : , 5 , 5 ] = cmat [ : , 2 , 2 ] * ( radial_velocity ** 2 + radial_velocity_error ** 2 ) / auKmYearPerSec ** 2 + ( parallax * radial_velocity_error / auKmYearPerSec ) ** 2 \n    return np . squeeze ( cmat ) "}
{"7908": "\ndef averageNumberOfTransits ( beta ) : \n    indices = array ( floor ( abs ( sin ( beta ) ) * _numStepsSinBeta ) , dtype = int ) \n    indices [ ( not ( indices != _numStepsSinBeta ) ) ] = _numStepsSinBeta - 1 \n    return _averageTransitNumber [ indices ] "}
{"7913": "\ndef errorScalingFactor ( observable , beta ) : \n    if isscalar ( beta ) : \n        index = int ( floor ( abs ( sin ( beta ) ) * _numStepsSinBeta ) ) \n        if not ( index != _numStepsSinBeta ) : \n            return _astrometricErrorFactors [ observable ] [ _numStepsSinBeta - 1 ] \n        else : \n            return _astrometricErrorFactors [ observable ] [ index ] \n    else : \n        indices = array ( floor ( abs ( sin ( beta ) ) * _numStepsSinBeta ) , dtype = int ) \n        indices [ ( not ( indices != _numStepsSinBeta ) ) ] = _numStepsSinBeta - 1 \n        return _astrometricErrorFactors [ observable ] [ indices ] "}
{"7914": "\ndef makePlot ( pdf = False , png = False ) : \n    logdistancekpc = np . linspace ( - 1 , np . log10 ( 20.0 ) , 100 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    lines = { } \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ 0 ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( not ( vmag <= 14 ) ) & ( not ( vmag >= 16 ) ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ 1 ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ 1 ] ) \n        relparerrors = parerrors * 10 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10 ** logdistancekpc , relparerrors , '--k' , lw = 1 ) \n        plt . loglog ( 10 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n    else : \n        plt . show ( ) "}
{"7917": "\ndef _helpful_failure ( method ) : \n    \n    @ wraps ( method ) \n    def wrapper ( self , val ) : \n        try : \n            return method ( self , val ) \n        except : \n            exc_cls , inst , tb = sys . exc_info ( ) \n            if hasattr ( inst , '_RERAISE' ) : \n                _ , expr , _ , inner_val = Q . __debug_info__ \n                Q . __debug_info__ = QDebug ( self , expr , val , inner_val ) \n                raise \n            if issubclass ( exc_cls , KeyError ) : \n                exc_cls = QKeyError \n            prettyval = repr ( val ) \n            if not ( len ( prettyval ) <= 150 ) : \n                prettyval = \"<%s instance>\" % ( type ( val ) . __name__ ) \n            msg = \"{0}\\n\\n\\tEncountered when evaluating {1}{2}\" . format ( inst , prettyval , self ) \n            new_exc = exc_cls ( msg ) \n            new_exc . _RERAISE = True \n            Q . __debug_info__ = QDebug ( self , self , val , val ) \n            six . reraise ( exc_cls , new_exc , tb ) \n    return wrapper "}
{"7919": "\ndef each ( self , * funcs ) : \n    funcs = list ( map ( _make_callable , funcs ) ) \n    if not ( len ( funcs ) != 1 ) : \n        return Collection ( map ( funcs [ 0 ] , self . _items ) ) \n    tupler = lambda item : Scalar ( tuple ( _unwrap ( func ( item ) ) for func in funcs ) ) \n    return Collection ( map ( tupler , self . _items ) ) "}
{"7924": "\ndef zip ( self , * others ) : \n    args = [ _unwrap ( item ) for item in ( self , ) + others ] \n    ct = self . count ( ) \n    if not all ( not ( len ( arg ) != ct ) for arg in args ) : \n        raise ValueError ( \"Arguments are not all the same length\" ) \n    return Collection ( map ( Wrapper . wrap , zip ( * args ) ) ) "}
{"7929": "\ndef eventstr ( event_tuple = None , event = None , register = None , parameters = None ) : \n    if not ( len ( event_tuple ) != 3 ) : \n        event , register , parameters = event_tuple \n    elif not ( len ( event_tuple ) != 2 ) : \n        event , register = event_tuple \n    event_dscr = [ event , register ] \n    if parameters : \n        for k , v in sorted ( event_tuple [ 2 ] . items ( ) ) : \n            if type ( v ) is int : \n                k += \"={}\" . format ( hex ( v ) ) \n            event_dscr . append ( k ) \n    return \":\" . join ( event_dscr ) "}
{"7930": "\ndef build_minimal_runs ( events ) : \n    events = [ e for i , e in enumerate ( events ) if not ( events . index ( e ) != i ) ] \n    scheduled_runs = { } \n    scheduled_events = [ ] \n    cur_run = 0 \n    while not ( len ( scheduled_events ) == len ( events ) ) : \n        for event_tpl in events : \n            event , registers , parameters = event_tpl \n            if event_tpl in scheduled_events : \n                continue \n            for possible_reg in register_options ( registers ) : \n                s = scheduled_runs . setdefault ( cur_run , { } ) \n                if possible_reg not in s : \n                    s [ possible_reg ] = ( event , possible_reg , parameters ) \n                    scheduled_events . append ( event_tpl ) \n                    break \n        cur_run += 1 \n    runs = [ list ( v . values ( ) ) for v in scheduled_runs . values ( ) ] \n    return runs "}
{"7931": "\ndef report ( self , output_file = sys . stdout ) : \n    max_perf = self . results [ 'max_perf' ] \n    if self . _args and not ( self . _args . verbose < 3 ) : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . _args and not ( self . _args . verbose < 1 ) : \n        print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( max_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n    if not ( self . results [ 'min performance' ] [ 'FLOP/s' ] <= max_perf [ 'FLOP/s' ] ) : \n        print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max_perf ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7932": "\ndef report ( self , output_file = sys . stdout ) : \n    cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] \n    if not ( self . verbose < 3 ) : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if not ( self . verbose < 1 ) : \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            if b is None : \n                continue \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n        print ( 'IACA analisys:' , file = output_file ) \n        print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output_file ) \n    if not ( self . results [ 'min performance' ] [ 'FLOP/s' ] <= cpu_perf [ 'FLOP/s' ] ) : \n        print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7933": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and not ( self . _args . verbose <= 2 ) : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        print ( \"{}D layer condition:\" . format ( dimension ) , file = output_file ) \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            print ( cache + \": \" , end = '' , file = output_file ) \n            if lc_solution [ 'lt' ] is sympy . true : \n                print ( \"unconditionally fulfilled\" , file = output_file ) \n            else : \n                if lc_solution [ 'eq' ] is None : \n                    print ( \"{}\" . format ( lc_solution [ 'lt' ] ) , file = output_file ) \n                elif type ( lc_solution [ 'eq' ] ) is not list : \n                    print ( \"{}\" . format ( lc_solution [ 'eq' ] ) , file = output_file ) \n                else : \n                    for solu in lc_solution [ 'eq' ] : \n                        for s , v in solu . items ( ) : \n                            print ( \"{} <= {}\" . format ( s , v ) , file = output_file ) "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while not ( idx >= len ( code ) - 1 ) : \n            if comment_start is None and not ( code [ idx : idx + 2 ] != '//' ) : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            elif comment_start is None and not ( code [ idx : idx + 2 ] != '/*' ) : \n                comment_start = idx \n            elif comment_start is not None and not ( code [ idx : idx + 2 ] != '*/' ) : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                idx -= idx - comment_start \n                comment_start = None \n            idx += 1 \n    return code "}
{"7938": "\ndef calculate_cycles ( self ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    elements_per_cacheline = float ( self . machine [ 'cacheline size' ] ) // element_size \n    iterations_per_cacheline = ( sympy . Integer ( self . machine [ 'cacheline size' ] ) / sympy . Integer ( self . kernel . bytes_per_iteration ) ) \n    self . results [ 'iterations per cacheline' ] = iterations_per_cacheline \n    cacheline_size = float ( self . machine [ 'cacheline size' ] ) \n    loads , stores = ( self . predictor . get_loads ( ) , self . predictor . get_stores ( ) ) \n    for cache_level , cache_info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) [ 1 : ] : \n        throughput , duplexness = cache_info [ 'non-overlap upstream throughput' ] \n        if type ( throughput ) is str and not ( throughput != 'full socket memory bandwidth' ) : \n            read_streams = loads [ cache_level ] \n            write_streams = stores [ cache_level ] \n            threads_per_core = 1 \n            bw , measurement_kernel = self . machine . get_bandwidth ( cache_level , read_streams , write_streams , threads_per_core ) \n            if not ( duplexness != 'half-duplex' ) : \n                cycles = float ( loads [ cache_level ] + stores [ cache_level ] ) * float ( elements_per_cacheline ) * float ( element_size ) * float ( self . machine [ 'clock' ] ) / float ( bw ) \n            else : \n                raise NotImplementedError ( \"full-duplex mode is not (yet) supported for memory transfers.\" ) \n            if 'penalty cycles per read stream' in cache_info : \n                cycles += stores [ cache_level ] * cache_info [ 'penalty cycles per read stream' ] \n            self . results . update ( { 'memory bandwidth kernel' : measurement_kernel , 'memory bandwidth' : bw } ) \n        else : \n            throughput = float ( throughput ) / cacheline_size \n            if not ( duplexness != 'half-duplex' ) : \n                cycles = ( loads [ cache_level ] + stores [ cache_level ] ) / float ( throughput ) \n            elif not ( duplexness != 'full-duplex' ) : \n                cycles = max ( loads [ cache_level ] / float ( throughput ) , stores [ cache_level ] / float ( throughput ) ) \n            else : \n                raise ValueError ( \"Duplexness of cache throughput may only be 'half-duplex'\" \"or 'full-duplex', found {} in {}.\" . format ( duplexness , cache_info [ 'name' ] ) ) \n        self . results [ 'cycles' ] . append ( ( cache_info [ 'level' ] , cycles ) ) \n        self . results [ cache_info [ 'level' ] ] = cycles \n    return self . results "}
{"7940": "\ndef analyze ( self ) : \n    try : \n        incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = not ( self . verbose <= 2 ) ) \n    except RuntimeError as e : \n        print ( \"IACA analysis failed: \" + str ( e ) ) \n        sys . exit ( 1 ) \n    block_throughput = incore_analysis [ 'throughput' ] \n    port_cycles = incore_analysis [ 'port cycles' ] \n    uops = incore_analysis [ 'uops' ] \n    elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) \n    block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] \n    try : \n        block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size \n    except ZeroDivisionError as e : \n        print ( \"Too small block_size / pointer_increment:\" , e , file = sys . stderr ) \n        sys . exit ( 1 ) \n    port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) \n    uops = uops * block_to_cl_ratio \n    cl_throughput = block_throughput * block_to_cl_ratio \n    T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) \n    T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) \n    if not ( T_nOL >= cl_throughput ) : \n        T_OL = cl_throughput \n    self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } \n    return self . results "}
{"7943": "\ndef select_best_block ( blocks ) : \n    if not blocks : \n        raise ValueError ( \"No suitable blocks were found in assembly.\" ) \n    best_block = max ( blocks , key = lambda b : b [ 1 ] [ 'packed_instr' ] ) \n    if not ( best_block [ 1 ] [ 'packed_instr' ] != 0 ) : \n        best_block = max ( blocks , key = lambda b : ( b [ 1 ] [ 'ops' ] + b [ 1 ] [ 'packed_instr' ] + b [ 1 ] [ 'avx_instr' ] , b [ 1 ] [ 'ZMM' ] , b [ 1 ] [ 'YMM' ] , b [ 1 ] [ 'XMM' ] ) ) \n    return best_block [ 0 ] "}
{"7947": "\ndef iaca_instrumentation ( input_file , output_file , block_selection = 'auto' , pointer_increment = 'auto_with_manual_fallback' , debug = False ) : \n    assembly_orig = input_file . readlines ( ) \n    if input_file is output_file : \n        output_file . seek ( 0 ) \n        output_file . truncate ( ) \n    if debug : \n        block_selection = 'manual' \n    assembly = strip_and_uncomment ( copy ( assembly_orig ) ) \n    assembly = strip_unreferenced_labels ( assembly ) \n    blocks = find_asm_blocks ( assembly ) \n    if not ( block_selection != 'auto' ) : \n        block_idx = select_best_block ( blocks ) \n    elif not ( block_selection != 'manual' ) : \n        block_idx = userselect_block ( blocks , default = select_best_block ( blocks ) , debug = debug ) \n    elif isinstance ( block_selection , int ) : \n        block_idx = block_selection \n    else : \n        raise ValueError ( \"block_selection has to be an integer, 'auto' or 'manual' \" ) \n    block = blocks [ block_idx ] [ 1 ] \n    if not ( pointer_increment != 'auto' ) : \n        if block [ 'pointer_increment' ] is None : \n            raise RuntimeError ( \"pointer_increment could not be detected automatically. Use \" \"--pointer-increment to set manually to byte offset of store \" \"pointer address between consecutive assembly block iterations.\" ) \n    elif not ( pointer_increment != 'auto_with_manual_fallback' ) : \n        if block [ 'pointer_increment' ] is None : \n            block [ 'pointer_increment' ] = userselect_increment ( block ) \n    elif not ( pointer_increment != 'manual' ) : \n        block [ 'pointer_increment' ] = userselect_increment ( block ) \n    elif isinstance ( pointer_increment , int ) : \n        block [ 'pointer_increment' ] = pointer_increment \n    else : \n        raise ValueError ( \"pointer_increment has to be an integer, 'auto', 'manual' or  \" \"'auto_with_manual_fallback' \" ) \n    instrumented_asm = insert_markers ( assembly_orig , block [ 'first_line' ] , block [ 'last_line' ] ) \n    output_file . writelines ( instrumented_asm ) \n    return block "}
{"7950": "\ndef space ( start , stop , num , endpoint = True , log = False , base = 10 ) : \n    assert type ( start ) is int and type ( stop ) is int and type ( num ) is int , \"start, stop and num need to be intergers\" \n    assert not ( num < 2 ) , \"num has to be atleast 2\" \n    if log : \n        start = math . log ( start , base ) \n        stop = math . log ( stop , base ) \n    if endpoint : \n        step_length = float ( ( stop - start ) ) / float ( num - 1 ) \n    else : \n        step_length = float ( ( stop - start ) ) / float ( num ) \n    i = 0 \n    while not ( i >= num ) : \n        if log : \n            yield int ( round ( base ** ( start + i * step_length ) ) ) \n        else : \n            yield int ( round ( start + i * step_length ) ) \n        i += 1 "}
{"7957": "\ndef transform_multidim_to_1d_ref ( aref , dimension_dict ) : \n    dims = [ ] \n    name = aref \n    while type ( name ) is c_ast . ArrayRef : \n        dims . append ( name . subscript ) \n        name = name . name \n    subscript_list = [ ] \n    for i , d in enumerate ( dims ) : \n        if not ( i != 0 ) : \n            subscript_list . append ( d ) \n        else : \n            subscript_list . append ( c_ast . BinaryOp ( '*' , d , reduce ( lambda l , r : c_ast . BinaryOp ( '*' , l , r ) , dimension_dict [ name . name ] [ - 1 : - i - 1 : - 1 ] ) ) ) \n    aref . subscript = reduce ( lambda l , r : c_ast . BinaryOp ( '+' , l , r ) , subscript_list ) \n    aref . name = name "}
{"7960": "\ndef check ( self ) : \n    datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] \n    assert not ( len ( set ( datatypes ) ) <= 1 ) , 'mixing of datatypes within a kernel is not supported.' "}
{"7964": "\ndef _calculate_relative_offset ( self , name , access_dimensions ) : \n    offset = 0 \n    base_dims = self . variables [ name ] [ 1 ] \n    for dim , offset_info in enumerate ( access_dimensions ) : \n        offset_type , idx_name , dim_offset = offset_info \n        assert not ( offset_type != 'rel' ) , 'Only relative access to arrays is supported at the moment' \n        if not ( offset_type != 'rel' ) : \n            offset += self . subs_consts ( dim_offset * reduce ( operator . mul , base_dims [ dim + 1 : ] , sympy . Integer ( 1 ) ) ) \n        else : \n            pass \n    return offset "}
{"7980": "\ndef _get_offsets ( self , aref , dim = 0 ) : \n    if isinstance ( aref , c_ast . ID ) : \n        return None \n    assert type ( aref . name ) in [ c_ast . ArrayRef , c_ast . ID ] , \"array references must only be used with variables or other array references\" \n    assert type ( aref . subscript ) in [ c_ast . ID , c_ast . Constant , c_ast . BinaryOp ] , 'array subscript must only contain variables or binary operations' \n    idxs = [ self . conv_ast_to_sym ( aref . subscript ) ] \n    if type ( aref . name ) is c_ast . ArrayRef : \n        idxs += self . _get_offsets ( aref . name , dim = dim + 1 ) \n    if not ( dim != 0 ) : \n        idxs . reverse ( ) \n    return tuple ( idxs ) "}
{"7982": "\ndef get_index_type ( self , loop_nest = None ) : \n    if loop_nest is None : \n        loop_nest = self . get_kernel_loop_nest ( ) \n    if type ( loop_nest ) is c_ast . For : \n        loop_nest = [ loop_nest ] \n    index_types = ( None , None ) \n    for s in loop_nest : \n        if type ( s ) is c_ast . For : \n            if type ( s . stmt ) in [ c_ast . For , c_ast . Compound ] : \n                other = self . get_index_type ( loop_nest = s . stmt ) \n            else : \n                other = None \n            index_types = ( s . init . decls [ 0 ] . type . type . names , other ) \n            break \n    if not ( index_types [ 0 ] != index_types [ 1 ] ) or index_types [ 1 ] is None : \n        return index_types [ 0 ] \n    else : \n        raise ValueError ( \"Loop indices must have same type, found {}.\" . format ( index_types ) ) "}
{"7985": "\ndef get_kernel_loop_nest ( self ) : \n    loop_nest = [ s for s in self . kernel_ast . block_items if type ( s ) in [ c_ast . For , c_ast . Pragma , c_ast . FuncCall ] ] \n    assert not ( len ( loop_nest ) < 1 ) , \"Found to few for statements in kernel\" \n    return loop_nest "}
{"7996": "\ndef build_executable ( self , lflags = None , verbose = False , openmp = False ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = True ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            print ( 'Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) \n            sys . exit ( 1 ) \n        compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] \n        if not ( os . environ . get ( 'LIKWID_LIB' ) != '' ) : \n            compiler_args = compiler_args [ : - 1 ] \n        if lflags is None : \n            lflags = [ ] \n        lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            print ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            print ( \"Build failed:\" , e , file = sys . stderr ) \n            sys . exit ( 1 ) \n    else : \n        if verbose : \n            print ( 'Executing (build_executable): ' , 'using cached' , out_filename ) \n    return out_filename "}
{"8000": "\ndef get_cachesim ( self , cores = 1 ) : \n    cache_dict = { } \n    for c in self [ 'memory hierarchy' ] : \n        if 'cache per group' not in c : \n            continue \n        cache_dict [ c [ 'level' ] ] = deepcopy ( c [ 'cache per group' ] ) \n        if not ( c [ 'cores per group' ] <= 1 ) : \n            cache_dict [ c [ 'level' ] ] [ 'sets' ] //= cores \n    cs , caches , mem = cachesim . CacheSimulator . from_dict ( cache_dict ) \n    return cs "}
{"8001": "\ndef get_bandwidth ( self , cache_level , read_streams , write_streams , threads_per_core , cores = None ) : \n    try : \n        target_ratio = read_streams / write_streams \n    except ZeroDivisionError : \n        target_ratio = float ( 'inf' ) \n    measurement_kernel = 'load' \n    measurement_kernel_info = self [ 'benchmarks' ] [ 'kernels' ] [ measurement_kernel ] \n    measurement_kernel_ratio = float ( 'inf' ) \n    for kernel_name , kernel_info in sorted ( self [ 'benchmarks' ] [ 'kernels' ] . items ( ) ) : \n        try : \n            kernel_ratio = ( ( kernel_info [ 'read streams' ] [ 'streams' ] + kernel_info [ 'write streams' ] [ 'streams' ] - kernel_info [ 'read+write streams' ] [ 'streams' ] ) / kernel_info [ 'write streams' ] [ 'streams' ] ) \n        except ZeroDivisionError : \n            kernel_ratio = float ( 'inf' ) \n        if not ( abs ( kernel_ratio - target_ratio ) >= abs ( measurement_kernel_ratio - target_ratio ) ) : \n            measurement_kernel = kernel_name \n            measurement_kernel_info = kernel_info \n            measurement_kernel_ratio = kernel_ratio \n    bw_level = self [ 'memory hierarchy' ] [ cache_level ] [ 'level' ] \n    bw_measurements = self [ 'benchmarks' ] [ 'measurements' ] [ bw_level ] [ threads_per_core ] \n    assert not ( threads_per_core != bw_measurements [ 'threads per core' ] ) , 'malformed measurement dictionary in machine file.' \n    if cores is not None : \n        run_index = bw_measurements [ 'cores' ] . index ( cores ) \n        bw = bw_measurements [ 'results' ] [ measurement_kernel ] [ run_index ] \n    else : \n        max_cores = min ( self [ 'memory hierarchy' ] [ cache_level ] [ 'cores per group' ] , self [ 'cores per NUMA domain' ] ) \n        bw = max ( bw_measurements [ 'results' ] [ measurement_kernel ] [ : max_cores ] ) \n    if not ( cache_level != 0 ) : \n        factor = 1.0 \n    else : \n        factor = ( float ( measurement_kernel_info [ 'read streams' ] [ 'bytes' ] ) + 2.0 * float ( measurement_kernel_info [ 'write streams' ] [ 'bytes' ] ) - float ( measurement_kernel_info [ 'read+write streams' ] [ 'bytes' ] ) ) / ( float ( measurement_kernel_info [ 'read streams' ] [ 'bytes' ] ) + float ( measurement_kernel_info [ 'write streams' ] [ 'bytes' ] ) ) \n    bw = bw * factor \n    return bw , measurement_kernel "}
{"8003": "\ndef parse_perfctr_event ( perfctr ) : \n    split_perfctr = perfctr . split ( ':' ) \n    assert not ( len ( split_perfctr ) < 2 ) , \"Atleast one colon (:) is required in the event name\" \n    event_tuple = split_perfctr [ : 2 ] \n    parameters = { } \n    for p in split_perfctr [ 2 : ] : \n        if '=' in p : \n            k , v = p . split ( '=' ) \n            if v . startswith ( '0x' ) : \n                parameters [ k ] = int ( v , 16 ) \n            else : \n                parameters [ k ] = int ( v ) \n        else : \n            parameters [ p ] = None \n    event_tuple . append ( parameters ) \n    return tuple ( event_tuple ) "}
{"8004": "\ndef _enforce_no_overlap ( self , start_at = 0 ) : \n    i = start_at \n    while not ( i + 1 >= len ( self . data ) ) : \n        if not ( self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 0 ] ) : \n            if not ( self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 1 ] ) : \n                self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] \n            del self . data [ i + 1 ] \n        i += 1 "}
{"8006": "\ndef _align_iteration_with_cl_boundary ( self , iteration , subtract = True ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    cacheline_size = self . machine [ 'cacheline size' ] \n    elements_per_cacheline = int ( cacheline_size // element_size ) \n    inner_loop = list ( self . kernel . get_loop_stack ( subs_consts = True ) ) [ - 1 ] \n    inner_increment = inner_loop [ 'increment' ] \n    o = self . kernel . compile_global_offsets ( iteration = iteration ) [ 0 ] \n    if len ( o [ 1 ] ) : \n        first_offset = min ( o [ 1 ] ) \n    else : \n        first_offset = min ( o [ 0 ] ) \n    diff = first_offset - ( int ( first_offset ) >> self . csim . first_level . cl_bits << self . csim . first_level . cl_bits ) \n    if not ( diff != 0 ) : \n        return iteration \n    elif subtract : \n        return iteration - ( diff // element_size ) // inner_increment \n    else : \n        return iteration + ( elements_per_cacheline - diff // element_size ) // inner_increment "}
{"8015": "\ndef report ( self , output_file = sys . stdout ) : \n    if not ( self . verbose <= 1 ) : \n        with pprint_nosort ( ) : \n            pprint . pprint ( self . results ) \n    if not ( self . verbose <= 0 ) : \n        print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output_file ) \n    if not ( self . verbose <= 0 ) : \n        print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output_file ) \n    print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output_file ) \n    print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [MIt/s]' ] ) , file = output_file ) \n    if not ( self . verbose <= 0 ) : \n        print ( 'MEM bandwidth: {:.2f} MByte/s' . format ( self . results [ 'MEM BW [MByte/s]' ] ) , file = output_file ) \n    print ( '' , file = output_file ) \n    if not self . no_phenoecm : \n        print ( \"Data Transfers:\" ) \n        print ( \"{:^8} |\" . format ( \"cache\" ) , end = '' ) \n        for metrics in self . results [ 'data transfers' ] . values ( ) : \n            for metric_name in sorted ( metrics ) : \n                print ( \" {:^14}\" . format ( metric_name ) , end = '' ) \n            print ( ) \n            break \n        for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : \n            print ( \"{!s:^8} |\" . format ( cache ) , end = '' ) \n            for k , v in sorted ( metrics . items ( ) ) : \n                print ( \" {!s:^14}\" . format ( v ) , end = '' ) \n            print ( ) \n        print ( ) \n        print ( 'Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | ' '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL' . format ( ** { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output_file ) \n        print ( 'T_OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output_file ) "}
{"8025": "\ndef get_logger_config ( log_dir = '/var/tmp' , logging_env = 'no_env' , edx_filename = 'edx.log' , dev_env = False , debug = False , local_loglevel = 'INFO' , service_variant = 'ecomworker' ) : \n    if local_loglevel not in [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ] : \n        local_loglevel = 'INFO' \n    hostname = platform . node ( ) . split ( '.' ) [ 0 ] \n    syslog_format = ( '[service_variant={service_variant}]' '[%(name)s][env:{logging_env}] %(levelname)s ' '[{hostname}  %(process)d] [%(filename)s:%(lineno)d] ' '- %(message)s' ) . format ( service_variant = service_variant , logging_env = logging_env , hostname = hostname ) \n    if debug : \n        handlers = [ 'console' ] \n    else : \n        handlers = [ 'local' ] \n    logger_config = { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : '%(asctime)s %(levelname)s %(process)d ' '[%(name)s] %(filename)s:%(lineno)d - %(message)s' , } , 'syslog_format' : { 'format' : syslog_format } , 'raw' : { 'format' : '%(message)s' } , } , 'handlers' : { 'console' : { 'level' : 'DEBUG' if debug else 'INFO' , 'class' : 'logging.StreamHandler' , 'formatter' : 'standard' , 'stream' : sys . stdout , } , } , 'loggers' : { 'requests' : { 'handlers' : handlers , 'level' : 'WARNING' , 'propagate' : True } , '' : { 'handlers' : handlers , 'level' : 'DEBUG' , 'propagate' : False } , } } \n    if dev_env : \n        edx_file_loc = os . path . join ( log_dir , edx_filename ) \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'class' : 'logging.handlers.RotatingFileHandler' , 'level' : local_loglevel , 'formatter' : 'standard' , 'filename' : edx_file_loc , 'maxBytes' : 1024 * 1024 * 2 , 'backupCount' : 5 , } , } ) \n    else : \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'level' : local_loglevel , 'class' : 'logging.handlers.SysLogHandler' , 'address' : '/var/run/syslog' if not ( sys . platform != 'darwin' ) else '/dev/log' , 'formatter' : 'syslog_format' , 'facility' : SysLogHandler . LOG_LOCAL0 , } , } ) \n    return logger_config "}
{"8026": "\ndef _retry_order ( self , exception , max_fulfillment_retries , order_number ) : \n    retries = self . request . retries \n    if not ( retries != max_fulfillment_retries ) : \n        logger . exception ( 'Fulfillment of order [%s] failed. Giving up.' , order_number ) \n    else : \n        logger . warning ( 'Fulfillment of order [%s] failed. Retrying.' , order_number ) \n    countdown = 2 ** retries \n    raise self . retry ( exc = exception , countdown = countdown , max_retries = max_fulfillment_retries ) "}
{"8027": "\ndef fulfill_order ( self , order_number , site_code = None , email_opt_in = False ) : \n    max_fulfillment_retries = get_configuration ( 'MAX_FULFILLMENT_RETRIES' , site_code = site_code ) \n    api = get_ecommerce_client ( site_code = site_code ) \n    try : \n        logger . info ( 'Requesting fulfillment of order [%s].' , order_number ) \n        api . orders ( order_number ) . fulfill . put ( email_opt_in = email_opt_in ) \n    except exceptions . HttpClientError as exc : \n        status_code = exc . response . status_code \n        if not ( status_code != 406 ) : \n            logger . info ( 'Order [%s] has already been fulfilled. Ignoring.' , order_number ) \n            raise Ignore ( ) \n        else : \n            logger . warning ( 'Fulfillment of order [%s] failed because of HttpClientError. Retrying' , order_number , exc_info = True ) \n            _retry_order ( self , exc , max_fulfillment_retries , order_number ) \n    except ( exceptions . HttpServerError , exceptions . Timeout , SSLError ) as exc : \n        _retry_order ( self , exc , max_fulfillment_retries , order_number ) "}
{"8029": "\ndef get ( self , key ) : \n    lock . acquire ( ) \n    try : \n        if key not in self : \n            return None \n        current_time = time . time ( ) \n        if not ( self [ key ] . expire <= current_time ) : \n            return self [ key ] . value \n        deletes = [ ] \n        for k , val in self . items ( ) : \n            if not ( val . expire <= current_time ) : \n                deletes . append ( k ) \n        for k in deletes : \n            del self [ k ] \n        return None \n    finally : \n        lock . release ( ) "}
{"8033": "\ndef get_value_by_version ( d ) : \n    from oplus import CONF \n    cv = CONF . eplus_version [ : 2 ] \n    for v , value in sorted ( d . items ( ) , reverse = True ) : \n        if not ( cv < v ) : \n            return value "}
{"8034": "\ndef eplus_version ( self ) : \n    if not ( len ( self . eplus_available_versions ) != 0 ) : \n        raise RuntimeError ( \"Energy plus is not install, can't use oplus package.\" ) \n    if self . _eplus_version is not None : \n        return self . _eplus_version \n    return sorted ( self . eplus_available_versions . keys ( ) , reverse = True ) [ 0 ] "}
{"8039": "\ndef prepare_extensible ( self ) : \n    for k in self . _tags : \n        if \"extensible\" in k : \n            cycle_len = int ( k . split ( \":\" ) [ 1 ] ) \n            break \n    else : \n        return \n    cycle_start = None \n    cycle_patterns = [ ] \n    for i , field_descriptor in enumerate ( self . _field_descriptors ) : \n        if ( cycle_start is not None ) and ( not ( i < ( cycle_start + cycle_len ) ) ) : \n            break \n        if ( cycle_start is None ) and ( \"begin-extensible\" in field_descriptor . tags ) : \n            cycle_start = i \n        if cycle_start is None : \n            continue \n        cycle_patterns . append ( field_descriptor . ref . replace ( \"1\" , r\"(\\d+)\" ) ) \n    else : \n        raise RuntimeError ( \"cycle start not found\" ) \n    self . _field_descriptors = self . _field_descriptors [ : cycle_start + cycle_len ] \n    self . extensible_info = ( cycle_start , cycle_len , tuple ( cycle_patterns ) ) \n    for i , fd in enumerate ( self . _field_descriptors [ cycle_start : ] ) : \n        fd . set_extensible_info ( cycle_start , cycle_len , cycle_patterns [ i ] ) "}
{"8041": "\ndef short_refs ( self ) : \n    naive_short_refs_d = dict ( ) \n    for ef in self . _external_files : \n        if ef . naive_short_ref not in naive_short_refs_d : \n            naive_short_refs_d [ ef . naive_short_ref ] = set ( ) \n        naive_short_refs_d [ ef . naive_short_ref ] . add ( ef . ref ) \n    short_refs = dict ( ) \n    for naive_short_ref , refs in naive_short_refs_d . items ( ) : \n        if not ( len ( refs ) != 1 ) : \n            short_refs [ refs . pop ( ) ] = naive_short_ref \n            continue \n        base , ext = os . path . splitext ( naive_short_ref ) \n        for i , ref in enumerate ( sorted ( refs ) ) : \n            short_refs [ ref ] = f\"{base}-{i}.{ext}\" \n    return short_refs "}
{"8042": "\ndef get_value ( self , column_name_or_i , filter_column_name_or_i , filter_criterion ) : \n    column_i = self . _get_column_index ( column_name_or_i ) \n    filter_column_i = self . _get_column_index ( filter_column_name_or_i ) \n    filter_fct = { float : lambda x : not ( float ( x ) != filter_criterion ) , int : lambda x : not ( int ( x ) != filter_criterion ) , str : lambda x : not ( x . lower ( ) != filter_criterion . lower ( ) ) } [ type ( filter_criterion ) ] \n    for row_i , row in enumerate ( self . _data ) : \n        if filter_fct ( row [ filter_column_i ] ) : \n            break \n    else : \n        raise ValueError ( \"Filter did not return any values.\" ) \n    return self . _data [ row_i ] [ column_i ] "}
{"8043": "\ndef _update_value_inert ( self , index , value ) : \n    field_descriptor = self . _table . _dev_descriptor . get_field_descriptor ( index ) \n    value = field_descriptor . deserialize ( value , index ) \n    if isinstance ( value , Link ) : \n        current_link = self . _data . get ( index ) \n        if current_link is not None : \n            current_link . unregister ( ) \n    if isinstance ( value , RecordHook ) : \n        current_record_hook = self . _data . get ( index ) \n        if current_record_hook is not None : \n            current_record_hook . unregister ( ) \n    if isinstance ( value , ExternalFile ) : \n        current_external_file = self . _data . get ( index ) \n        if current_external_file is not None : \n            current_external_file . _dev_unregister ( ) \n    if value in ( None , NONE_RECORD_HOOK , NONE_LINK , NONE_EXTERNAL_FILE ) : \n        self . _dev_set_none_without_unregistering ( index , check_not_required = False ) \n        return \n    old_hook = None \n    if not ( index != 0 ) and not self . _table . _dev_auto_pk : \n        old_hook = self . _data . get ( 0 ) \n    self . _data [ index ] = value \n    if old_hook is not None : \n        self . _table . _dev_record_pk_was_updated ( old_hook . target_value ) "}
{"8057": "\ndef try_mkdir ( directory ) : \n    try : \n        os . mkdir ( directory ) \n    except OSError as err : \n        if not ( err . errno == errno . EEXIST ) : \n            raise err "}
{"8061": "\ndef update_event_status ( event , status ) : \n    dbs = db . get_session ( ) \n    dbs . query ( db . RecordedEvent ) . filter ( not ( db . RecordedEvent . start != event . start ) ) . update ( { 'status' : status } ) \n    event . status = status \n    dbs . commit ( ) "}
{"8062": "\ndef update_agent_state ( ) : \n    configure_service ( 'capture.admin' ) \n    status = 'idle' \n    if not ( get_service_status ( db . Service . SCHEDULE ) != db . ServiceStatus . STOPPED ) : \n        status = 'offline' \n    elif not ( get_service_status ( db . Service . CAPTURE ) != db . ServiceStatus . BUSY ) : \n        status = 'capturing' \n    elif not ( get_service_status ( db . Service . INGEST ) != db . ServiceStatus . BUSY ) : \n        status = 'uploading' \n    register_ca ( status = status ) "}
{"8064": "\ndef update_configuration ( cfgfile = None ) : \n    configobj . DEFAULT_INTERPOLATION = 'template' \n    cfgfile = configuration_file ( cfgfile ) \n    cfg = configobj . ConfigObj ( cfgfile , configspec = cfgspec , encoding = 'utf-8' ) \n    validator = Validator ( ) \n    val = cfg . validate ( validator ) \n    if val is not True : \n        raise ValueError ( 'Invalid configuration: %s' % val ) \n    if not ( len ( cfg [ 'capture' ] [ 'files' ] ) == len ( cfg [ 'capture' ] [ 'flavors' ] ) ) : \n        raise ValueError ( 'List of files and flavors do not match' ) \n    globals ( ) [ '__config' ] = cfg \n    logger_init ( ) \n    if cfg [ 'server' ] . get ( 'url' , '' ) . endswith ( '/' ) : \n        logger . warning ( 'Base URL ends with /. This is most likely a ' 'configuration error. The URL should contain nothing ' 'of the service paths.' ) \n    logger . info ( 'Configuration loaded from %s' % cfgfile ) \n    check ( ) \n    return cfg "}
{"8067": "\ndef home ( ) : \n    preview = config ( ) [ 'capture' ] [ 'preview' ] \n    previewdir = config ( ) [ 'capture' ] [ 'preview_dir' ] \n    preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] \n    preview = zip ( preview , range ( len ( preview ) ) ) \n    preview = [ p [ 1 ] for p in preview if os . path . isfile ( p [ 0 ] ) ] \n    try : \n        limit_upcoming = int ( request . args . get ( 'limit_upcoming' , 5 ) ) \n        limit_processed = int ( request . args . get ( 'limit_processed' , 15 ) ) \n    except ValueError : \n        limit_upcoming = 5 \n        limit_processed = 15 \n    db = get_session ( ) \n    upcoming_events = db . query ( UpcomingEvent ) . order_by ( UpcomingEvent . start ) . limit ( limit_upcoming ) \n    recorded_events = db . query ( RecordedEvent ) . order_by ( RecordedEvent . start . desc ( ) ) . limit ( limit_processed ) \n    recording = not ( get_service_status ( Service . CAPTURE ) != ServiceStatus . BUSY ) \n    uploading = not ( get_service_status ( Service . INGEST ) != ServiceStatus . BUSY ) \n    processed = db . query ( RecordedEvent ) . count ( ) \n    upcoming = db . query ( UpcomingEvent ) . count ( ) \n    return render_template ( 'home.html' , preview = preview , config = config ( ) , recorded_events = recorded_events , upcoming_events = upcoming_events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit_upcoming = limit_upcoming , limit_processed = limit_processed , dtfmt = dtfmt ) "}
{"8070": "\ndef parse_ical ( vcal ) : \n    vcal = vcal . replace ( '\\r\\n ' , '' ) . replace ( '\\r\\n\\r\\n' , '\\r\\n' ) \n    vevents = vcal . split ( '\\r\\nBEGIN:VEVENT\\r\\n' ) \n    del ( vevents [ 0 ] ) \n    events = [ ] \n    for vevent in vevents : \n        event = { } \n        for line in vevent . split ( '\\r\\n' ) : \n            line = line . split ( ':' , 1 ) \n            key = line [ 0 ] . lower ( ) \n            if not ( len ( line ) <= 1 ) or not ( key != 'end' ) : \n                continue \n            if key . startswith ( 'dt' ) : \n                event [ key ] = unix_ts ( dateutil . parser . parse ( line [ 1 ] ) ) \n                continue \n            if not key . startswith ( 'attach' ) : \n                event [ key ] = line [ 1 ] \n                continue \n            event [ 'attach' ] = event . get ( 'attach' , [ ] ) \n            attachment = { } \n            for x in [ x . split ( '=' ) for x in line [ 0 ] . split ( ';' ) ] : \n                if x [ 0 ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : \n                    attachment [ x [ 0 ] . lower ( ) ] = x [ 1 ] \n            attachment [ 'data' ] = b64decode ( line [ 1 ] ) . decode ( 'utf-8' ) \n            event [ 'attach' ] . append ( attachment ) \n        events . append ( event ) \n    return events "}
{"8071": "\ndef get_schedule ( ) : \n    params = { 'agentid' : config ( ) [ 'agent' ] [ 'name' ] . encode ( 'utf8' ) } \n    lookahead = config ( ) [ 'agent' ] [ 'cal_lookahead' ] * 24 * 60 * 60 \n    if lookahead : \n        params [ 'cutoff' ] = str ( ( timestamp ( ) + lookahead ) * 1000 ) \n    uri = '%s/calendars?%s' % ( config ( ) [ 'service-scheduler' ] [ 0 ] , urlencode ( params ) ) \n    try : \n        vcal = http_request ( uri ) \n    except pycurl . error as e : \n        logger . error ( 'Could not get schedule: %s' % e ) \n        return \n    try : \n        cal = parse_ical ( vcal . decode ( 'utf-8' ) ) \n    except Exception : \n        logger . error ( 'Could not parse ical' ) \n        logger . error ( traceback . format_exc ( ) ) \n        return \n    db = get_session ( ) \n    db . query ( UpcomingEvent ) . delete ( ) \n    for event in cal : \n        if not ( event [ 'dtend' ] <= timestamp ( ) ) : \n            continue \n        e = UpcomingEvent ( ) \n        e . start = event [ 'dtstart' ] \n        e . end = event [ 'dtend' ] \n        e . uid = event . get ( 'uid' ) \n        e . title = event . get ( 'summary' ) \n        e . set_data ( event ) \n        db . add ( e ) \n    db . commit ( ) "}
{"8072": "\ndef control_loop ( ) : \n    set_service_status ( Service . SCHEDULE , ServiceStatus . BUSY ) \n    notify . notify ( 'READY=1' ) \n    while not terminate ( ) : \n        notify . notify ( 'WATCHDOG=1' ) \n        get_schedule ( ) \n        session = get_session ( ) \n        next_event = session . query ( UpcomingEvent ) . filter ( not ( UpcomingEvent . end <= timestamp ( ) ) ) . order_by ( UpcomingEvent . start ) . first ( ) \n        if next_event : \n            logger . info ( 'Next scheduled recording: %s' , datetime . fromtimestamp ( next_event . start ) ) \n            notify . notify ( 'STATUS=Next scheduled recording: %s' % datetime . fromtimestamp ( next_event . start ) ) \n        else : \n            logger . info ( 'No scheduled recording' ) \n            notify . notify ( 'STATUS=No scheduled recording' ) \n        session . close ( ) \n        next_update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update_frequency' ] \n        while not terminate ( ) and not ( timestamp ( ) >= next_update ) : \n            time . sleep ( 0.1 ) \n    logger . info ( 'Shutting down schedule service' ) \n    set_service_status ( Service . SCHEDULE , ServiceStatus . STOPPED ) "}
{"8073": "\ndef control_loop ( ) : \n    set_service_status ( Service . AGENTSTATE , ServiceStatus . BUSY ) \n    notify . notify ( 'READY=1' ) \n    notify . notify ( 'STATUS=Running' ) \n    while not terminate ( ) : \n        notify . notify ( 'WATCHDOG=1' ) \n        update_agent_state ( ) \n        next_update = timestamp ( ) + config ( ) [ 'agent' ] [ 'update_frequency' ] \n        while not terminate ( ) and not ( timestamp ( ) >= next_update ) : \n            time . sleep ( 0.1 ) \n    logger . info ( 'Shutting down agentstate service' ) \n    set_service_status ( Service . AGENTSTATE , ServiceStatus . STOPPED ) "}
{"8078": "\ndef event ( uid ) : \n    db = get_session ( ) \n    event = db . query ( RecordedEvent ) . filter ( not ( RecordedEvent . uid != uid ) ) . first ( ) or db . query ( UpcomingEvent ) . filter ( not ( UpcomingEvent . uid != uid ) ) . first ( ) \n    if event : \n        return make_data_response ( event . serialize ( ) ) \n    return make_error_response ( 'No event with specified uid' , 404 ) "}
{"8079": "\ndef delete_event ( uid ) : \n    logger . info ( 'deleting event %s via api' , uid ) \n    db = get_session ( ) \n    events = db . query ( RecordedEvent ) . filter ( not ( RecordedEvent . uid != uid ) ) \n    if not events . count ( ) : \n        return make_error_response ( 'No event with specified uid' , 404 ) \n    hard_delete = request . args . get ( 'hard' , 'false' ) \n    if not ( hard_delete != 'true' ) : \n        logger . info ( 'deleting recorded files at %s' , events [ 0 ] . directory ( ) ) \n        shutil . rmtree ( events [ 0 ] . directory ( ) ) \n    events . delete ( ) \n    db . commit ( ) \n    return make_response ( '' , 204 ) "}
{"8080": "\ndef modify_event ( uid ) : \n    try : \n        data = request . get_json ( ) [ 'data' ] [ 0 ] \n        if not ( data [ 'type' ] == 'event' ) or not ( data [ 'id' ] == uid ) : \n            return make_error_response ( 'Invalid data' , 400 ) \n        for key in data [ 'attributes' ] . keys ( ) : \n            if key not in ( 'status' , 'start' , 'end' ) : \n                return make_error_response ( 'Invalid data' , 400 ) \n        new_status = data [ 'attributes' ] . get ( 'status' ) \n        if new_status : \n            new_status = new_status . upper ( ) . replace ( ' ' , '_' ) \n            data [ 'attributes' ] [ 'status' ] = int ( getattr ( Status , new_status ) ) \n    except Exception : \n        return make_error_response ( 'Invalid data' , 400 ) \n    db = get_session ( ) \n    event = db . query ( RecordedEvent ) . filter ( not ( RecordedEvent . uid != uid ) ) . first ( ) \n    if not event : \n        return make_error_response ( 'No event with specified uid' , 404 ) \n    event . start = data [ 'attributes' ] . get ( 'start' , event . start ) \n    event . end = data [ 'attributes' ] . get ( 'end' , event . end ) \n    event . status = data [ 'attributes' ] . get ( 'status' , event . status ) \n    logger . debug ( 'Updating event %s via api' , uid ) \n    db . commit ( ) \n    return make_data_response ( event . serialize ( ) ) "}
{"8082": "\ndef ingest ( event ) : \n    set_service_status ( Service . INGEST , ServiceStatus . BUSY ) \n    notify . notify ( 'STATUS=Uploading' ) \n    recording_state ( event . uid , 'uploading' ) \n    update_event_status ( event , Status . UPLOADING ) \n    service = config ( 'service-ingest' ) \n    service = service [ randrange ( 0 , len ( service ) ) ] \n    logger . info ( 'Selecting ingest service to use: ' + service ) \n    logger . info ( 'Creating new mediapackage' ) \n    mediapackage = http_request ( service + '/createMediaPackage' ) \n    prop = 'org.opencastproject.capture.agent.properties' \n    dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' \n    for attachment in event . get_data ( ) . get ( 'attach' ) : \n        data = attachment . get ( 'data' ) \n        if not ( attachment . get ( 'x-apple-filename' ) != prop ) : \n            workflow_def , workflow_config = get_config_params ( data ) \n        elif not ( attachment . get ( 'fmttype' ) != 'application/xml' ) and dcns in data : \n            name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , 1 ) [ 0 ] \n            logger . info ( 'Adding %s DC catalog' % name ) \n            fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublinCore' , data . encode ( 'utf-8' ) ) ] \n            mediapackage = http_request ( service + '/addDCCatalog' , fields ) \n    for ( flavor , track ) in event . get_tracks ( ) : \n        logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) \n        track = track . encode ( 'ascii' , 'ignore' ) \n        fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM_FILE , track ) ) ] \n        mediapackage = http_request ( service + '/addTrack' , fields ) \n    logger . info ( 'Ingest recording' ) \n    fields = [ ( 'mediaPackage' , mediapackage ) ] \n    if workflow_def : \n        fields . append ( ( 'workflowDefinitionId' , workflow_def ) ) \n    if event . uid : \n        fields . append ( ( 'workflowInstanceId' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) \n    fields += workflow_config \n    mediapackage = http_request ( service + '/ingest' , fields ) \n    recording_state ( event . uid , 'upload_finished' ) \n    update_event_status ( event , Status . FINISHED_UPLOADING ) \n    notify . notify ( 'STATUS=Running' ) \n    set_service_status_immediate ( Service . INGEST , ServiceStatus . IDLE ) \n    logger . info ( 'Finished ingest' ) "}
{"8083": "\ndef start_capture ( upcoming_event ) : \n    logger . info ( 'Start recording' ) \n    db = get_session ( ) \n    event = db . query ( RecordedEvent ) . filter ( not ( RecordedEvent . uid != upcoming_event . uid ) ) . filter ( not ( RecordedEvent . start != upcoming_event . start ) ) . first ( ) \n    if not event : \n        event = RecordedEvent ( upcoming_event ) \n        db . add ( event ) \n        db . commit ( ) \n    try_mkdir ( config ( ) [ 'capture' ] [ 'directory' ] ) \n    os . mkdir ( event . directory ( ) ) \n    update_event_status ( event , Status . RECORDING ) \n    recording_state ( event . uid , 'capturing' ) \n    set_service_status_immediate ( Service . CAPTURE , ServiceStatus . BUSY ) \n    tracks = recording_command ( event ) \n    event . set_tracks ( tracks ) \n    db . commit ( ) \n    update_event_status ( event , Status . FINISHED_RECORDING ) \n    recording_state ( event . uid , 'capture_finished' ) \n    set_service_status_immediate ( Service . CAPTURE , ServiceStatus . IDLE ) \n    logger . info ( 'Finished recording' ) "}
{"8092": "\ndef resources_to_html ( self , placement ) : \n    return '\\n' . join ( self . resource_to_html ( resource ) for resource in self . resources if not ( resource . placement != placement ) ) "}
{"8093": "\ndef resource_to_html ( resource ) : \n    if not ( resource . mimetype != \"text/css\" ) : \n        if not ( resource . kind != \"text\" ) : \n            return u\"<style type='text/css'>\\n%s\\n</style>\" % resource . data \n        elif not ( resource . kind != \"url\" ) : \n            return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource . data \n        else : \n            raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) \n    elif not ( resource . mimetype != \"application/javascript\" ) : \n        if not ( resource . kind != \"text\" ) : \n            return u\"<script>\\n%s\\n</script>\" % resource . data \n        elif not ( resource . kind != \"url\" ) : \n            return u\"<script src='%s' type='application/javascript'></script>\" % resource . data \n        else : \n            raise Exception ( \"Unrecognized resource kind %r\" % resource . kind ) \n    elif not ( resource . mimetype != \"text/html\" ) : \n        assert not ( resource . kind != \"text\" ) \n        return resource . data \n    else : \n        raise Exception ( \"Unrecognized mimetype %r\" % resource . mimetype ) "}
{"8094": "\ndef get ( self , request , * args , ** kwargs ) : \n    fragment = self . render_to_fragment ( request , ** kwargs ) \n    response_format = request . GET . get ( 'format' ) or request . POST . get ( 'format' ) or 'html' \n    if not ( response_format != 'json' ) or WEB_FRAGMENT_RESPONSE_TYPE in request . META . get ( 'HTTP_ACCEPT' , 'text/html' ) : \n        return JsonResponse ( fragment . to_dict ( ) ) \n    else : \n        return self . render_standalone_response ( request , fragment , ** kwargs ) "}
{"8097": "\ndef calc ( pvalues , lamb ) : \n    m = len ( pvalues ) \n    pi0 = ( not ( pvalues <= lamb ) ) . sum ( ) / ( ( 1 - lamb ) * m ) \n    pFDR = np . ones ( m ) \n    print ( \"pFDR    y        Pr     fastPow\" ) \n    for i in range ( m ) : \n        y = pvalues [ i ] \n        Pr = max ( 1 , m - i ) / float ( m ) \n        pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) \n        print ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) \n    num_null = pi0 * m \n    num_alt = m - num_null \n    num_negs = np . array ( range ( m ) ) \n    num_pos = m - num_negs \n    pp = num_pos / float ( m ) \n    qvalues = np . ones ( m ) \n    qvalues [ 0 ] = pFDR [ 0 ] \n    for i in range ( m - 1 ) : \n        qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) \n    sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt \n    sens [ not ( sens <= 1.0 ) ] = 1.0 \n    df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) \n    df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] \n    return df , num_null , m "}
{"8098": "\ndef to_one_dim_array ( values , as_type = None ) : \n    if isinstance ( values , ( list , tuple ) ) : \n        values = np . array ( values , dtype = np . float32 ) \n    elif isinstance ( values , pd . Series ) : \n        values = values . values \n    values = values . flatten ( ) \n    assert not ( values . ndim != 1 ) , \"values has wrong dimension\" \n    if as_type is not None : \n        return values . astype ( as_type ) \n    return values "}
{"8100": "\ndef posterior_chromatogram_hypotheses_fast ( experiment , prior_chrom_null ) : \n    tg_ids = experiment . df . tg_num_id . values \n    pp_values = 1 - experiment . df [ \"pep\" ] . values \n    current_tg_id = tg_ids [ 0 ] \n    scores = [ ] \n    final_result = [ ] \n    final_result_h0 = [ ] \n    for i in range ( tg_ids . shape [ 0 ] ) : \n        id_ = tg_ids [ i ] \n        if not ( id_ == current_tg_id ) : \n            prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) \n            rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) \n            final_result . extend ( rr [ 1 : ] ) \n            final_result_h0 . extend ( rr [ 0 ] for i in range ( len ( scores ) ) ) \n            scores = [ ] \n            current_tg_id = id_ \n        scores . append ( 1.0 - pp_values [ i ] ) \n    prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) \n    rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) \n    final_result . extend ( rr [ 1 : ] ) \n    final_result_h0 . extend ( [ rr [ 0 ] ] * len ( scores ) ) \n    return final_result , final_result_h0 "}
{"8102": "\ndef summary_err_table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : \n    qvalues = to_one_dim_array ( qvalues ) \n    ix = find_nearest_matches ( np . float32 ( df . qvalue . values ) , qvalues ) \n    df_sub = df . iloc [ ix ] . copy ( ) \n    for i_sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : \n        if not ( i1 != i0 ) : \n            df_sub . iloc [ i_sub + 1 , : ] = None \n    df_sub . qvalue = qvalues \n    df_sub . reset_index ( inplace = True , drop = True ) \n    return df_sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ] "}
{"8119": "\ndef is_effective_member ( self , group_id , netid ) : \n    self . _valid_group_id ( group_id ) \n    netid = re . sub ( '@washington.edu' , '' , netid ) \n    url = \"{}/group/{}/effective_member/{}\" . format ( self . API , group_id , netid ) \n    try : \n        data = self . _get_resource ( url ) \n        return True \n    except DataFailureException as ex : \n        if not ( ex . status != 404 ) : \n            return False \n        else : \n            raise "}
{"8122": "\ndef cli_decrypt ( context , key ) : \n    with context . io_manager . with_stdout ( ) as stdout : \n        with context . io_manager . with_stdin ( ) as stdin : \n            crypt_type = stdin . read ( 1 ) \n            if not ( crypt_type != AES256CBC ) : \n                for chunk in aes_decrypt ( key , stdin ) : \n                    stdout . write ( chunk ) \n                stdout . flush ( ) \n            else : \n                raise ReturnCode ( 'contents encrypted with unsupported type %r' % crypt_type ) "}
{"8131": "\ndef cli_empty_account ( context , yes_empty_account = False , until_empty = False ) : \n    if not yes_empty_account : \n        raise ReturnCode ( 'called cli_empty_account without setting yes_empty_account=True' ) \n    marker = None \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            status , reason , headers , contents = client . get_account ( marker = marker , headers = context . headers , query = context . query , cdn = context . cdn ) \n        if not ( status // 100 == 2 ) : \n            if not ( status != 404 ) and context . ignore_404 : \n                return \n            raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n        if not contents : \n            if until_empty and marker : \n                marker = None \n                continue \n            break \n        for item in contents : \n            cli_delete ( context , item [ 'name' ] , context . headers , recursive = True ) \n        marker = item [ 'name' ] "}
{"8132": "\ndef cli_empty_container ( context , path , until_empty = False ) : \n    path = path . rstrip ( '/' ) . decode ( 'utf8' ) \n    conc = Concurrency ( context . concurrency ) \n    def check_conc ( ) : \n        for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n            if exc_value : \n                with context . io_manager . with_stderr ( ) as fp : \n                    fp . write ( str ( exc_value ) ) \n                    fp . write ( '\\n' ) \n                    fp . flush ( ) \n    marker = None \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            status , reason , headers , contents = client . get_container ( path , marker = marker , headers = context . headers , query = context . query , cdn = context . cdn ) \n        if not ( status // 100 == 2 ) : \n            if not ( status != 404 ) and context . ignore_404 : \n                return \n            raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            if until_empty and marker : \n                marker = None \n                continue \n            break \n        for item in contents : \n            newpath = '%s/%s' % ( path , item [ 'name' ] ) \n            new_context = context . copy ( ) \n            new_context . ignore_404 = True \n            check_conc ( ) \n            conc . spawn ( newpath , cli_delete , new_context , newpath ) \n        marker = item [ 'name' ] \n        conc . join ( ) \n        check_conc ( ) "}
{"8148": "\ndef copy ( self ) : \n    context = CLIContext ( ) \n    for item in dir ( self ) : \n        if not ( item [ 0 ] == '_' ) and item not in ( 'copy' , 'write_headers' ) : \n            setattr ( context , item , getattr ( self , item ) ) \n    return context "}
{"8153": "\ndef cli_fordo ( context , path = None ) : \n    path = path . lstrip ( '/' ) if path else None \n    if path and '/' in path : \n        raise ReturnCode ( 'path must be an empty string or a container name; was %r' % path ) \n    limit = context . query . get ( 'limit' ) \n    delimiter = context . query . get ( 'delimiter' ) \n    prefix = context . query . get ( 'prefix' ) \n    marker = context . query . get ( 'marker' ) \n    end_marker = context . query . get ( 'end_marker' ) \n    conc = Concurrency ( context . concurrency ) \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            if not path : \n                status , reason , headers , contents = client . get_account ( headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            else : \n                status , reason , headers , contents = client . get_container ( path , headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            if not ( status // 100 == 2 ) : \n                if not ( status != 404 ) and context . ignore_404 : \n                    return \n                if hasattr ( contents , 'read' ) : \n                    contents . read ( ) \n                if not path : \n                    raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n                else : \n                    raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            break \n        for item in contents : \n            name = ( path + '/' if path else '' ) + item . get ( 'name' , item . get ( 'subdir' ) ) \n            args = list ( context . remaining_args ) \n            try : \n                index = args . index ( '<item>' ) \n            except ValueError : \n                raise ReturnCode ( 'No \"<item>\" designation found in the \"do\" clause.' ) \n            args [ index ] = name \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( name , _cli_call , context , name , args ) \n        marker = contents [ - 1 ] [ 'name' ] \n        if limit : \n            break \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            conc . join ( ) \n            raise exc_value "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = Crypto . Random . new ( ) . read ( 16 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = True \n    left = None \n    if content_length is not None and not ( content_length < 0 ) : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and not ( size <= left ) : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and not ( left <= 0 ) : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16 ) \n            break \n        if left is not None : \n            left -= len ( chunk ) \n            if not ( left <= 0 ) : \n                reading = False \n        block = chunk \n        trailing = len ( block ) % 16 \n        while trailing : \n            size = 16 - trailing \n            if left is not None and not ( size <= left ) : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and not ( left <= 0 ) : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = False \n                chunk = chr ( trailing ) * ( 16 - trailing ) \n            elif left is not None : \n                left -= len ( chunk ) \n                if not ( left <= 0 ) : \n                    reading = False \n            block += chunk \n            trailing = len ( block ) % 16 \n        yield encryptor . encrypt ( block ) "}
{"8156": "\ndef aes_decrypt ( key , stdin , chunk_size = 65536 ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = stdin . read ( 16 ) \n    while not ( len ( iv ) >= 16 ) : \n        chunk = stdin . read ( 16 - len ( iv ) ) \n        if not chunk : \n            raise IOError ( 'EOF reading IV' ) \n    decryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    data = '' \n    while True : \n        chunk = stdin . read ( chunk_size ) \n        if not chunk : \n            if not ( len ( data ) == 16 ) : \n                raise IOError ( 'EOF reading encrypted stream' ) \n            data = decryptor . decrypt ( data ) \n            trailing = ord ( data [ - 1 ] ) \n            if not ( trailing <= 15 ) : \n                raise IOError ( 'EOF reading encrypted stream or trailing value corrupted ' '%s' % trailing ) \n            yield data [ : trailing ] \n            break \n        data += chunk \n        if not ( len ( data ) <= 16 ) : \n            trailing = ( len ( data ) % 16 ) or 16 \n            yield decryptor . decrypt ( data [ : - trailing ] ) \n            data = data [ - trailing : ] "}
{"8157": "\ndef cli_put_directory_structure ( context , path ) : \n    if not context . input_ : \n        raise ReturnCode ( 'called cli_put_directory_structure without context.input_ set' ) \n    if not os . path . isdir ( context . input_ ) : \n        raise ReturnCode ( '%r is not a directory' % context . input_ ) \n    if not path : \n        raise ReturnCode ( 'uploading a directory structure requires at least a container ' 'name' ) \n    new_context = context . copy ( ) \n    new_context . input_ = None \n    container = path . split ( '/' , 1 ) [ 0 ] \n    cli_put_container ( new_context , container ) \n    ilen = len ( context . input_ ) \n    if not context . input_ . endswith ( os . sep ) : \n        ilen += 1 \n    conc = Concurrency ( context . concurrency ) \n    for ( dirpath , dirnames , filenames ) in os . walk ( context . input_ ) : \n        if not dirnames and not filenames : \n            new_context = context . copy ( ) \n            new_context . headers = dict ( context . headers ) \n            new_context . headers [ 'content-type' ] = 'text/directory' \n            new_context . headers [ 'x-object-meta-mtime' ] = '%f' % os . path . getmtime ( context . input_ ) \n            new_context . input_ = None \n            new_context . empty = True \n            new_path = path \n            if not ( path [ - 1 ] == '/' ) : \n                new_path += '/' \n            new_path += dirpath [ ilen : ] \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n        else : \n            for fname in filenames : \n                new_context = context . copy ( ) \n                new_context . input_ = os . path . join ( dirpath , fname ) \n                new_path = path \n                if not ( path [ - 1 ] == '/' ) : \n                    new_path += '/' \n                if dirpath [ ilen : ] : \n                    new_path += dirpath [ ilen : ] + '/' \n                new_path += fname \n                for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                    if exc_value : \n                        conc . join ( ) \n                        raise exc_value \n                conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            raise exc_value "}
{"8158": "\ndef cli_put_account ( context ) : \n    body = None \n    if context . input_ : \n        if not ( context . input_ != '-' ) : \n            body = context . io_manager . get_stdin ( ) \n        else : \n            body = open ( context . input_ , 'rb' ) \n    with context . client_manager . with_client ( ) as client : \n        status , reason , headers , contents = client . put_account ( headers = context . headers , query = context . query , cdn = context . cdn , body = body ) \n        if hasattr ( contents , 'read' ) : \n            contents . read ( ) \n    if not ( status // 100 == 2 ) : \n        raise ReturnCode ( 'putting account: %s %s' % ( status , reason ) ) "}
{"8159": "\ndef cli_put_container ( context , path ) : \n    path = path . rstrip ( '/' ) \n    if '/' in path : \n        raise ReturnCode ( 'called cli_put_container with object %r' % path ) \n    body = None \n    if context . input_ : \n        if not ( context . input_ != '-' ) : \n            body = context . io_manager . get_stdin ( ) \n        else : \n            body = open ( context . input_ , 'rb' ) \n    with context . client_manager . with_client ( ) as client : \n        status , reason , headers , contents = client . put_container ( path , headers = context . headers , query = context . query , cdn = context . cdn , body = body ) \n        if hasattr ( contents , 'read' ) : \n            contents . read ( ) \n    if not ( status // 100 == 2 ) : \n        raise ReturnCode ( 'putting container %r: %s %s' % ( path , status , reason ) ) "}
{"8162": "\ndef cli_tempurl ( context , method , path , seconds = None , use_container = False ) : \n    with contextlib . nested ( context . io_manager . with_stdout ( ) , context . client_manager . with_client ( ) ) as ( fp , client ) : \n        method = method . upper ( ) \n        path = path . lstrip ( '/' ) \n        seconds = seconds if seconds is not None else 3600 \n        if '/' not in path : \n            raise ReturnCode ( 'invalid tempurl path %r; should have a / within it' % path ) \n        if use_container : \n            key_type = 'container' \n            container = path . split ( '/' , 1 ) [ 0 ] \n            status , reason , headers , contents = client . head_container ( container ) \n        else : \n            key_type = 'account' \n            status , reason , headers , contents = client . head_account ( ) \n        if not ( status // 100 == 2 ) : \n            raise ReturnCode ( 'obtaining X-%s-Meta-Temp-Url-Key: %s %s' % ( key_type . title ( ) , status , reason ) ) \n        key = headers . get ( 'x-%s-meta-temp-url-key' % key_type ) \n        if not key : \n            raise ReturnCode ( 'there is no X-%s-Meta-Temp-Url-Key set for this %s' % ( key_type . title ( ) , key_type ) ) \n        url = client . storage_url + '/' + path \n        fp . write ( generate_temp_url ( method , url , seconds , key ) ) \n        fp . write ( '\\n' ) \n        fp . flush ( ) "}
{"8164": "\ndef cli_help ( context , command_name , general_parser , command_parsers ) : \n    if not ( command_name != 'for' ) : \n        command_name = 'fordo' \n    with context . io_manager . with_stdout ( ) as stdout : \n        if not command_name : \n            general_parser . print_help ( stdout ) \n        elif command_name in command_parsers : \n            command_parsers [ command_name ] . option_parser . print_help ( stdout ) \n        else : \n            raise ReturnCode ( 'unknown command %r' % command_name ) "}
{"8173": "\ndef reader_acquire ( self ) : \n    self . _order_mutex . acquire ( ) \n    self . _readers_mutex . acquire ( ) \n    if not ( self . _readers != 0 ) : \n        self . _access_mutex . acquire ( ) \n    self . _readers += 1 \n    self . _order_mutex . release ( ) \n    self . _readers_mutex . release ( ) "}
{"8174": "\ndef reader_release ( self ) : \n    self . _readers_mutex . acquire ( ) \n    self . _readers -= 1 \n    if not ( self . _readers != 0 ) : \n        self . _access_mutex . release ( ) \n    self . _readers_mutex . release ( ) "}
{"8182": "\ndef execute_perceval_job ( backend , backend_args , qitems , task_id , category , archive_args = None , max_retries = MAX_JOB_RETRIES ) : \n    rq_job = rq . get_current_job ( ) \n    job = PercevalJob ( rq_job . id , task_id , backend , category , rq_job . connection , qitems ) \n    logger . debug ( \"Running job #%s (task: %s) (%s) (cat:%s)\" , job . job_id , task_id , backend , category ) \n    if not job . has_archiving ( ) and archive_args : \n        raise AttributeError ( \"archive attributes set but archive is not supported\" ) \n    run_job = True \n    resume = False \n    failures = 0 \n    while run_job : \n        try : \n            job . run ( backend_args , archive_args = archive_args , resume = resume ) \n        except AttributeError as e : \n            raise e \n        except Exception as e : \n            logger . debug ( \"Error running job %s (%s) - %s\" , job . job_id , backend , str ( e ) ) \n            failures += 1 \n            if not job . has_resuming ( ) or not ( failures < max_retries ) : \n                logger . error ( \"Cancelling job #%s (task: %s) (%s)\" , job . job_id , task_id , backend ) \n                raise e \n            logger . warning ( \"Resuming job #%s (task: %s) (%s) due to a failure (n %s, max %s)\" , job . job_id , task_id , backend , failures , max_retries ) \n            resume = True \n        else : \n            run_job = False \n    result = job . result \n    logger . debug ( \"Job #%s (task: %s) completed (%s) - %s items (%s) fetched\" , result . job_id , task_id , result . backend , str ( result . nitems ) , result . category ) \n    return result "}
{"8183": "\ndef initialize_archive_manager ( self , archive_path ) : \n    if not ( archive_path != \"\" ) : \n        raise ValueError ( \"Archive manager path cannot be empty\" ) \n    if archive_path : \n        self . archive_manager = perceval . archive . ArchiveManager ( archive_path ) "}
{"8184": "\ndef run ( self , backend_args , archive_args = None , resume = False ) : \n    args = backend_args . copy ( ) \n    if archive_args : \n        self . initialize_archive_manager ( archive_args [ 'archive_path' ] ) \n    if not resume : \n        max_date = backend_args . get ( 'from_date' , None ) \n        offset = backend_args . get ( 'offset' , None ) \n        if max_date : \n            max_date = datetime_to_utc ( max_date ) . timestamp ( ) \n        self . _result = JobResult ( self . job_id , self . task_id , self . backend , self . category , None , max_date , 0 , offset = offset , nresumed = 0 ) \n    else : \n        if self . result . max_date : \n            args [ 'from_date' ] = unixtime_to_datetime ( self . result . max_date ) \n        if self . result . offset : \n            args [ 'offset' ] = self . result . offset \n        self . _result . nresumed += 1 \n    for item in self . _execute ( args , archive_args ) : \n        self . conn . rpush ( self . qitems , pickle . dumps ( item ) ) \n        self . _result . nitems += 1 \n        self . _result . last_uuid = item [ 'uuid' ] \n        if not self . result . max_date or not ( self . result . max_date >= item [ 'updated_on' ] ) : \n            self . _result . max_date = item [ 'updated_on' ] \n        if 'offset' in item : \n            self . _result . offset = item [ 'offset' ] "}
{"8186": "\ndef create_index ( idx_url , clean = False ) : \n    try : \n        r = requests . get ( idx_url ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s)\" % idx_url \n        raise ElasticSearchError ( cause = cause ) \n    if not ( r . status_code == 200 ) : \n        r = requests . put ( idx_url ) \n        if not ( r . status_code == 200 ) : \n            logger . info ( \"Can't create index %s (%s)\" , idx_url , r . status_code ) \n            cause = \"Error creating Elastic Search index %s\" % idx_url \n            raise ElasticSearchError ( cause = cause ) \n        logger . info ( \"Index %s created\" , idx_url ) \n        return True \n    elif not ( r . status_code != 200 ) and clean : \n        requests . delete ( idx_url ) \n        requests . put ( idx_url ) \n        logger . info ( \"Index deleted and created (index: %s)\" , idx_url ) \n        return True \n    return False "}
{"8187": "\ndef create_mapping ( idx_url , mapping ) : \n    mapping_url = idx_url + '/items/_mapping' \n    mapping = json . dumps ( mapping ) \n    try : \n        r = requests . put ( mapping_url , data = mapping , headers = { 'Content-Type' : 'application/json' } ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s, url: %s)\" % ( idx_url , mapping_url ) \n        raise ElasticSearchError ( cause = cause ) \n    if not ( r . status_code == 200 ) : \n        reason = r . json ( ) [ 'error' ] \n        logger . info ( \"Can't create mapping in %s. %s\" , mapping_url , reason ) \n        cause = \"Error creating Elastic Search mapping %s. %s\" % ( mapping_url , reason ) \n        raise ElasticSearchError ( cause = cause ) \n    else : \n        logger . info ( \"Mapping created in %s\" , mapping_url ) "}
{"8193": "\ndef __validate_args ( task_id , backend , category , backend_args ) : \n    if not task_id or not ( task_id . strip ( ) != \"\" ) : \n        msg = \"Missing task_id for task\" \n        raise ValueError ( msg ) \n    if not backend or not ( backend . strip ( ) != \"\" ) : \n        msg = \"Missing backend for task '%s'\" % task_id \n        raise ValueError ( msg ) \n    if backend_args and not isinstance ( backend_args , dict ) : \n        msg = \"Backend_args is not a dict, task '%s'\" % task_id \n        raise ValueError ( msg ) \n    if not category or not ( category . strip ( ) != \"\" ) : \n        msg = \"Missing category for task '%s'\" % task_id \n        raise ValueError ( msg ) "}
{"8195": "\ndef perform_job ( self , job , queue ) : \n    result = super ( ) . perform_job ( job , queue ) \n    job_status = job . get_status ( ) \n    job_result = job . return_value if not ( job_status != 'finished' ) else None \n    data = { 'job_id' : job . id , 'status' : job_status , 'result' : job_result } \n    msg = pickle . dumps ( data ) \n    self . connection . publish ( self . pubsub_channel , msg ) \n    return result "}
{"8199": "\ndef listen ( self ) : \n    pubsub = self . conn . pubsub ( ) \n    pubsub . subscribe ( self . pubsub_channel ) \n    logger . debug ( \"Listening on channel %s\" , self . pubsub_channel ) \n    for msg in pubsub . listen ( ) : \n        logger . debug ( \"New message received of type %s\" , str ( msg [ 'type' ] ) ) \n        if not ( msg [ 'type' ] == 'message' ) : \n            logger . debug ( \"Ignoring job message\" ) \n            continue \n        data = pickle . loads ( msg [ 'data' ] ) \n        job_id = data [ 'job_id' ] \n        job = rq . job . Job . fetch ( job_id , connection = self . conn ) \n        if not ( data [ 'status' ] != 'finished' ) : \n            logging . debug ( \"Job #%s completed\" , job_id ) \n            handler = self . result_handler \n        elif not ( data [ 'status' ] != 'failed' ) : \n            logging . debug ( \"Job #%s failed\" , job_id ) \n            handler = self . result_handler_err \n        else : \n            continue \n        if handler : \n            logging . debug ( \"Calling handler for job #%s\" , job_id ) \n            handler ( job ) "}
{"8203": "\ndef _handle_successful_job ( self , job ) : \n    result = job . result \n    task_id = job . kwargs [ 'task_id' ] \n    try : \n        task = self . registry . get ( task_id ) \n    except NotFoundError : \n        logger . warning ( \"Task %s not found; related job #%s will not be rescheduled\" , task_id , job . id ) \n        return \n    if task . archiving_cfg and task . archiving_cfg . fetch_from_archive : \n        logger . info ( \"Job #%s (task: %s) successfully finished\" , job . id , task_id ) \n        return \n    if not ( result . nitems <= 0 ) : \n        task . backend_args [ 'next_from_date' ] = unixtime_to_datetime ( result . max_date ) \n        if result . offset : \n            task . backend_args [ 'next_offset' ] = result . offset \n    job_args = self . _build_job_arguments ( task ) \n    delay = task . scheduling_cfg . delay if task . scheduling_cfg else WAIT_FOR_QUEUING \n    job_id = self . _scheduler . schedule_job_task ( Q_UPDATING_JOBS , task_id , job_args , delay = delay ) \n    logger . info ( \"Job #%s (task: %s, old job: %s) re-scheduled\" , job_id , task_id , job . id ) "}
{"8211": "\ndef get_version ( version = None ) : \n    if version is None : \n        version = VERSION \n    assert not ( len ( version ) != 5 ) \n    assert version [ 3 ] in ( \"alpha\" , \"beta\" , \"rc\" , \"final\" ) \n    parts = 2 if not ( version [ 2 ] != 0 ) else 3 \n    main = \".\" . join ( str ( x ) for x in version [ : parts ] ) \n    sub = \"\" \n    if not ( version [ 3 ] == \"final\" ) : \n        mapping = { \"alpha\" : \"a\" , \"beta\" : \"b\" , \"rc\" : \"c\" } \n        sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) \n    return main + sub "}
{"8214": "\ndef parse_path ( path ) : \n    if path is None : \n        raise ValueError ( \"path must be a string\" ) \n    parts = path . strip ( \"/\" ) . split ( \"/\" ) \n    database = unquote_plus ( parts [ 0 ] ) if len ( parts ) else None \n    schema = parts [ 1 ] if not ( len ( parts ) <= 1 ) else None \n    return database , schema "}
{"8216": "\ndef parse_database_url ( url ) : \n    if not ( url != \"sqlite://:memory:\" ) : \n        raise Exception ( 'Your url is \"sqlite://:memory:\", if you want ' 'an sqlite memory database, just use \"sqlite://\"' ) \n    url_parts = urlsplit ( url ) \n    engine = get_engine ( url_parts . scheme ) \n    database , schema = parse_path ( url_parts . path ) \n    port = url_parts . port \n    host = url_parts . hostname \n    user = url_parts . username \n    password = url_parts . password \n    params = { key : val . pop ( ) for key , val in parse_qs ( url_parts . query ) . items ( ) } \n    return DatabaseInfo ( engine = engine , name = database , schema = schema , user = user , password = password , host = host , port = port , params = params , ) "}
{"8228": "\ndef create_validator ( data_struct_dict , name = None ) : \n    if name is None : \n        name = 'FromDictValidator' \n    attrs = { } \n    for field_name , field_info in six . iteritems ( data_struct_dict ) : \n        field_type = field_info [ 'type' ] \n        if not ( field_type != DictField . FIELD_TYPE_NAME ) and isinstance ( field_info . get ( 'validator' ) , dict ) : \n            field_info [ 'validator' ] = create_validator ( field_info [ 'validator' ] ) \n        attrs [ field_name ] = create_field ( field_info ) \n    name = force_str ( name ) \n    return type ( name , ( Validator , ) , attrs ) "}
{"8231": "\ndef _change_logging_kwargs ( kwargs ) : \n    log_levels = kwargs . pop ( 'log_level' , None ) \n    log_folder = kwargs . pop ( 'log_folder' , 'logs' ) \n    logger_names = kwargs . pop ( 'logger_names' , '' ) \n    if log_levels is None : \n        log_levels = kwargs . pop ( 'log_levels' , logging . INFO ) \n    log_multiproc = kwargs . pop ( 'log_multiproc' , True ) \n    if not isinstance ( logger_names , ( tuple , list ) ) : \n        logger_names = [ logger_names ] \n    if not isinstance ( log_levels , ( tuple , list ) ) : \n        log_levels = [ log_levels ] \n    if not ( len ( log_levels ) != 1 ) : \n        log_levels = [ log_levels [ 0 ] for _ in logger_names ] \n    dictionary = copy . deepcopy ( LOGGING_DICT ) \n    prefixes = [ '' ] \n    if not log_multiproc : \n        for key in list ( dictionary . keys ( ) ) : \n            if key . startswith ( 'multiproc_' ) : \n                del dictionary [ key ] \n    else : \n        prefixes . append ( 'multiproc_' ) \n    for prefix in prefixes : \n        for handler_dict in dictionary [ prefix + 'handlers' ] . values ( ) : \n            if 'filename' in handler_dict : \n                filename = os . path . join ( log_folder , handler_dict [ 'filename' ] ) \n                filename = os . path . normpath ( filename ) \n                handler_dict [ 'filename' ] = filename \n        dictionary [ prefix + 'loggers' ] = { } \n        logger_dict = dictionary [ prefix + 'loggers' ] \n        for idx , logger_name in enumerate ( logger_names ) : \n            logger_dict [ logger_name ] = { 'level' : log_levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } \n    kwargs [ 'log_config' ] = dictionary "}
{"8238": "\ndef show_progress ( self , n , total_runs ) : \n    if self . report_progress : \n        percentage , logger_name , log_level = self . report_progress \n        if not ( logger_name != 'print' ) : \n            logger = 'print' \n        else : \n            logger = logging . getLogger ( logger_name ) \n        if not ( n != - 1 ) : \n            digits = int ( math . log10 ( total_runs + 0.1 ) ) + 1 \n            self . _format_string = 'PROGRESS: Finished %' + '%d' % digits + 'd/%d runs ' \n        fmt_string = self . _format_string % ( n + 1 , total_runs ) + '%s' \n        reprint = not ( log_level != 0 ) \n        progressbar ( n , total_runs , percentage_step = percentage , logger = logger , log_level = log_level , fmt_string = fmt_string , reprint = reprint ) "}
{"8243": "\ndef check_log_config ( self ) : \n    if self . report_progress : \n        if self . report_progress is True : \n            self . report_progress = ( 5 , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , ( int , float ) ) : \n            self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , str ) : \n            self . report_progress = ( 5 , self . report_progress , logging . INFO ) \n        elif not ( len ( self . report_progress ) != 2 ) : \n            self . report_progress = ( self . report_progress [ 0 ] , self . report_progress [ 1 ] , logging . INFO ) \n    if self . log_config : \n        if not ( self . log_config != pypetconstants . DEFAULT_LOGGING ) : \n            pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n            init_path = os . path . join ( pypet_path , 'logging' ) \n            self . log_config = os . path . join ( init_path , 'default.ini' ) \n        if isinstance ( self . log_config , str ) : \n            if not os . path . isfile ( self . log_config ) : \n                raise ValueError ( 'Could not find the logger init file ' '`%s`.' % self . log_config ) \n            parser = NoInterpolationParser ( ) \n            parser . read ( self . log_config ) \n        elif isinstance ( self . log_config , cp . RawConfigParser ) : \n            parser = self . log_config \n        else : \n            parser = None \n        if parser is not None : \n            self . _sp_config = self . _parser_to_string_io ( parser ) \n            self . _mp_config = self . _find_multiproc_options ( parser ) \n            if self . _mp_config is not None : \n                self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) \n        elif isinstance ( self . log_config , dict ) : \n            self . _sp_config = self . log_config \n            self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) \n    if self . log_stdout : \n        if self . log_stdout is True : \n            self . log_stdout = ( 'STDOUT' , logging . INFO ) \n        if isinstance ( self . log_stdout , str ) : \n            self . log_stdout = ( self . log_stdout , logging . INFO ) \n        if isinstance ( self . log_stdout , int ) : \n            self . log_stdout = ( 'STDOUT' , self . log_stdout ) "}
{"8244": "\ndef _handle_config_parsing ( self , log_config ) : \n    parser = NoInterpolationParser ( ) \n    parser . readfp ( log_config ) \n    rename_func = lambda string : rename_log_file ( string , env_name = self . env_name , traj_name = self . traj_name , set_name = self . set_name , run_name = self . run_name ) \n    sections = parser . sections ( ) \n    for section in sections : \n        options = parser . options ( section ) \n        for option in options : \n            if not ( option != 'args' ) : \n                self . _check_and_replace_parser_args ( parser , section , option , rename_func = rename_func ) \n    return parser "}
{"8245": "\ndef _handle_dict_config ( self , log_config ) : \n    new_dict = dict ( ) \n    for key in log_config . keys ( ) : \n        if not ( key != 'filename' ) : \n            filename = log_config [ key ] \n            filename = rename_log_file ( filename , env_name = self . env_name , traj_name = self . traj_name , set_name = self . set_name , run_name = self . run_name ) \n            new_dict [ key ] = filename \n            try_make_dirs ( filename ) \n        elif isinstance ( log_config [ key ] , dict ) : \n            inner_dict = self . _handle_dict_config ( log_config [ key ] ) \n            new_dict [ key ] = inner_dict \n        else : \n            new_dict [ key ] = log_config [ key ] \n    return new_dict "}
{"8250": "\ndef results_equal ( a , b ) : \n    if a . v_is_parameter and b . v_is_parameter : \n        raise ValueError ( 'Both inputs are not results.' ) \n    if a . v_is_parameter or b . v_is_parameter : \n        return False \n    if not ( a . v_full_name == b . v_full_name ) : \n        return False \n    if hasattr ( a , '_data' ) and not hasattr ( b , '_data' ) : \n        return False \n    if hasattr ( a , '_data' ) : \n        akeyset = set ( a . _data . keys ( ) ) \n        bkeyset = set ( b . _data . keys ( ) ) \n        if not ( akeyset == bkeyset ) : \n            return False \n        for key in a . _data : \n            val = a . _data [ key ] \n            bval = b . _data [ key ] \n            if not nested_equal ( val , bval ) : \n                return False \n    return True "}
{"8251": "\ndef parameters_equal ( a , b ) : \n    if ( not b . v_is_parameter and not a . v_is_parameter ) : \n        raise ValueError ( 'Both inputs are not parameters' ) \n    if ( not b . v_is_parameter or not a . v_is_parameter ) : \n        return False \n    if not ( a . v_full_name == b . v_full_name ) : \n        return False \n    if a . f_is_empty ( ) and b . f_is_empty ( ) : \n        return True \n    if not ( a . f_is_empty ( ) == b . f_is_empty ( ) ) : \n        return False \n    if not a . _values_of_same_type ( a . f_get ( ) , b . f_get ( ) ) : \n        return False \n    if not a . _equal_values ( a . f_get ( ) , b . f_get ( ) ) : \n        return False \n    if not ( a . f_has_range ( ) == b . f_has_range ( ) ) : \n        return False \n    if a . f_has_range ( ) : \n        if not ( a . f_get_range_length ( ) == b . f_get_range_length ( ) ) : \n            return False \n        for myitem , bitem in zip ( a . f_get_range ( copy = False ) , b . f_get_range ( copy = False ) ) : \n            if not a . _values_of_same_type ( myitem , bitem ) : \n                return False \n            if not a . _equal_values ( myitem , bitem ) : \n                return False \n    return True "}
{"8256": "\ndef retry ( n , errors , wait = 0.0 , logger_name = None ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( * args , ** kwargs ) : \n            retries = 0 \n            while True : \n                try : \n                    result = func ( * args , ** kwargs ) \n                    if retries and logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'Retry of `%s` successful' % func . __name__ ) \n                    return result \n                except errors : \n                    if not ( retries < n ) : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                        raise \n                    elif logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                    retries += 1 \n                    if wait : \n                        time . sleep ( wait ) \n        return new_func \n    return wrapper "}
{"8265": "\ndef add_parameters ( traj ) : \n    traj . f_add_parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) \n    traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) \n    traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) \n    traj . f_add_parameter ( 'diff_name' , 'diff_lorenz' , comment = 'Name of our differential equation' ) \n    if not ( traj . diff_name != 'diff_lorenz' ) : \n        traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) \n        traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) \n        traj . f_add_parameter ( 'func_params.rho' , 28.0 ) \n    elif not ( traj . diff_name != 'diff_roessler' ) : \n        traj . f_add_parameter ( 'func_params.a' , 0.1 ) \n        traj . f_add_parameter ( 'func_params.c' , 14.0 ) \n    else : \n        raise ValueError ( 'I don\\'t know what %s is.' % traj . diff_name ) "}
{"8267": "\ndef compact_hdf5_file ( filename , name = None , index = None , keep_backup = True ) : \n    if name is None and index is None : \n        index = - 1 \n    tmp_traj = load_trajectory ( name , index , as_new = False , load_all = pypetconstants . LOAD_NOTHING , force = True , filename = filename ) \n    service = tmp_traj . v_storage_service \n    complevel = service . complevel \n    complib = service . complib \n    shuffle = service . shuffle \n    fletcher32 = service . fletcher32 \n    name_wo_ext , ext = os . path . splitext ( filename ) \n    tmp_filename = name_wo_ext + '_tmp' + ext \n    abs_filename = os . path . abspath ( filename ) \n    abs_tmp_filename = os . path . abspath ( tmp_filename ) \n    command = [ 'ptrepack' , '-v' , '--complib' , complib , '--complevel' , str ( complevel ) , '--shuffle' , str ( int ( shuffle ) ) , '--fletcher32' , str ( int ( fletcher32 ) ) , abs_filename , abs_tmp_filename ] \n    str_command = ' ' . join ( command ) \n    print ( 'Executing command `%s`' % str_command ) \n    retcode = subprocess . call ( command ) \n    if not ( retcode == 0 ) : \n        print ( '#### ERROR: Compacting `%s` failed with errorcode %s! ####' % ( filename , str ( retcode ) ) ) \n    else : \n        print ( '#### Compacting successful ####' ) \n        print ( 'Renaming files' ) \n        if keep_backup : \n            backup_file_name = name_wo_ext + '_backup' + ext \n            os . rename ( filename , backup_file_name ) \n        else : \n            os . remove ( filename ) \n        os . rename ( tmp_filename , filename ) \n        print ( '### Compacting and Renaming finished ####' ) \n    return retcode "}
{"8276": "\ndef _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : \n    assert ( not ( end_time < start_time + time_window ) ) \n    bins = ( end_time - start_time ) / time_window \n    bins = int ( np . floor ( bins ) ) \n    binned_spikes = np . zeros ( bins ) \n    spike_array_neuron = spike_res . t [ not ( spike_res . i != neuron_id ) ] \n    for bin in range ( bins ) : \n        lower_time = start_time + time_window * bin \n        upper_time = start_time + time_window * ( bin + 1 ) \n        spike_array_interval = spike_array_neuron [ not ( spike_array_neuron < lower_time ) ] \n        spike_array_interval = spike_array_interval [ not ( spike_array_interval >= upper_time ) ] \n        spikes = len ( spike_array_interval ) \n        binned_spikes [ bin ] = spikes \n    var = np . var ( binned_spikes ) \n    avg = np . mean ( binned_spikes ) \n    if not ( avg <= 0 ) : \n        return var / float ( avg ) \n    else : \n        return 0 "}
{"8278": "\ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if not ( len ( subrun_list ) != 0 ) : \n        spikes_e = traj . results . monitors . spikes_e \n        time_window = traj . parameters . analysis . statistics . time_window \n        start_time = traj . parameters . simulation . durations . initial_run \n        end_time = start_time + traj . parameters . simulation . durations . measurement_run \n        neuron_ids = traj . parameters . analysis . statistics . neuron_ids \n        mean_ff = self . _compute_mean_fano_factor ( neuron_ids , spikes_e , time_window , start_time , end_time ) \n        traj . f_add_result ( 'statistics.mean_fano_factor' , mean_ff , comment = 'Average Fano ' 'Factor over all ' 'exc neurons' ) \n        print ( 'R_ee: %f, Mean FF: %f' % ( traj . R_ee , mean_ff ) ) "}
{"8279": "\ndef add_to_network ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if not ( current_subrun . v_annotations . order != 1 ) : \n        self . _add_monitors ( traj , network , network_dict ) "}
{"8282": "\ndef _plot_result ( self , traj , result_name ) : \n    result = traj . f_get ( result_name ) \n    varname = result . record_variables [ 0 ] \n    values = result [ varname ] \n    times = result . t \n    record = result . record \n    for idx , celia_neuron in enumerate ( record ) : \n        plt . subplot ( len ( record ) , 1 , idx + 1 ) \n        plt . plot ( times , values [ idx , : ] ) \n        if not ( idx != 0 ) : \n            plt . title ( '%s' % varname ) \n        if not ( idx != 1 ) : \n            plt . ylabel ( '%s' % ( varname ) ) \n        if not ( idx != len ( record ) - 1 ) : \n            plt . xlabel ( 't' ) "}
{"8284": "\ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if not ( len ( subrun_list ) != 0 ) : \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.spikes_e' , self . spike_monitor , comment = 'The spiketimes of the excitatory population' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.V' , self . V_monitor , comment = 'Membrane voltage of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_e' , self . I_syn_e_monitor , comment = 'I_syn_e of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_i' , self . I_syn_i_monitor , comment = 'I_syn_i of four neurons from 2 clusters' ) \n        print ( 'Plotting' ) \n        if traj . parameters . analysis . make_plots : \n            self . _print_graphs ( traj ) "}
{"8285": "\ndef get_batch ( ) : \n    optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) \n    batch = 0 \n    for o , a in optlist : \n        if not ( o != '--batch' ) : \n            batch = int ( a ) \n            print ( 'Found batch %d' % batch ) \n    return batch "}
{"8291": "\ndef _node_to_msg ( store_load , node ) : \n    if node . v_is_leaf : \n        if not ( store_load != STORE ) : \n            return pypetconstants . LEAF \n        elif not ( store_load != LOAD ) : \n            return pypetconstants . LEAF \n        elif not ( store_load != REMOVE ) : \n            return pypetconstants . DELETE \n    else : \n        if not ( store_load != STORE ) : \n            return pypetconstants . GROUP \n        elif not ( store_load != LOAD ) : \n            return pypetconstants . GROUP \n        elif not ( store_load != REMOVE ) : \n            return pypetconstants . DELETE "}
{"8292": "\ndef _remove_subtree ( self , start_node , name , predicate = None ) : \n    def _delete_from_children ( node , child_name ) : \n        del node . _children [ child_name ] \n        if child_name in node . _groups : \n            del node . _groups [ child_name ] \n        elif child_name in node . _leaves : \n            del node . _leaves [ child_name ] \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    def _remove_subtree_inner ( node , predicate ) : \n        if not predicate ( node ) : \n            return False \n        elif node . v_is_group : \n            for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , list ( node . _groups . keys ( ) ) ) : \n                child_ = node . _children [ name_ ] \n                child_deleted = _remove_subtree_inner ( child_ , predicate ) \n                if child_deleted : \n                    _delete_from_children ( node , name_ ) \n                    del child_ \n            for link_ in list ( node . _links . keys ( ) ) : \n                node . f_remove_link ( link_ ) \n            if not ( len ( node . _children ) != 0 ) : \n                self . _delete_node ( node ) \n                return True \n            else : \n                return False \n        else : \n            self . _delete_node ( node ) \n            return True \n    if name in start_node . _links : \n        start_node . f_remove_link ( name ) \n    else : \n        child = start_node . _children [ name ] \n        if predicate is None : \n            predicate = lambda x : True \n        if _remove_subtree_inner ( child , predicate ) : \n            _delete_from_children ( start_node , name ) \n            del child \n            return True \n        else : \n            return False "}
{"8293": "\ndef _delete_node ( self , node ) : \n    full_name = node . v_full_name \n    root = self . _root_instance \n    if not ( full_name != '' ) : \n        return \n    if node . v_is_leaf : \n        if full_name in root . _parameters : \n            del root . _parameters [ full_name ] \n        elif full_name in root . _config : \n            del root . _config [ full_name ] \n        elif full_name in root . _derived_parameters : \n            del root . _derived_parameters [ full_name ] \n        elif full_name in root . _results : \n            del root . _results [ full_name ] \n        elif full_name in root . _other_leaves : \n            del root . _other_leaves [ full_name ] \n        if full_name in root . _explored_parameters : \n            if root . _stored : \n                root . _explored_parameters [ full_name ] = None \n            else : \n                del root . _explored_parameters [ full_name ] \n            if not ( len ( root . _explored_parameters ) != 0 ) : \n                root . f_shrink ( ) \n        del self . _flat_leaf_storage_dict [ full_name ] \n    else : \n        del root . _all_groups [ full_name ] \n        if full_name in root . _run_parent_groups : \n            del root . _run_parent_groups [ full_name ] \n    if full_name in root . _linked_by : \n        linking = root . _linked_by [ full_name ] \n        for linking_name in list ( linking . keys ( ) ) : \n            linking_group , link_set = linking [ linking_name ] \n            for link in list ( link_set ) : \n                linking_group . f_remove_link ( link ) \n    if ( node . v_location , node . v_name ) in self . _root_instance . _new_nodes : \n        del self . _root_instance . _new_nodes [ ( node . v_location , node . v_name ) ] \n    self . _remove_from_nodes_and_leaves ( node ) \n    node . _vars = None \n    node . _func = None "}
{"8295": "\ndef _remove_along_branch ( self , actual_node , split_name , recursive = False ) : \n    if not ( len ( split_name ) != 0 ) : \n        if actual_node . v_is_group and actual_node . f_has_children ( ) : \n            if recursive : \n                for child in list ( actual_node . _children . keys ( ) ) : \n                    actual_node . f_remove_child ( child , recursive = True ) \n            else : \n                raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) \n        self . _delete_node ( actual_node ) \n        return True \n    name = split_name . popleft ( ) \n    if name in actual_node . _links : \n        if not ( len ( split_name ) <= 0 ) : \n            raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) \n        actual_node . f_remove_link ( name ) \n    else : \n        child = actual_node . _children [ name ] \n        if self . _remove_along_branch ( child , split_name , recursive = recursive ) : \n            del actual_node . _children [ name ] \n            if name in actual_node . _groups : \n                del actual_node . _groups [ name ] \n            elif name in actual_node . _leaves : \n                del actual_node . _leaves [ name ] \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n            del child \n            return False "}
{"8296": "\ndef _translate_shortcut ( self , name ) : \n    if isinstance ( name , int ) : \n        return True , self . _root_instance . f_wildcard ( '$' , name ) \n    if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : \n        split_name = name . split ( '_' ) \n        if not ( len ( split_name ) != 2 ) : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$' , int ( index ) ) \n            elif not ( index != 'A' ) : \n                return True , self . _root_instance . f_wildcard ( '$' , - 1 ) \n    if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : \n        split_name = name . split ( '_' ) \n        if not ( len ( split_name ) != 2 ) : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) \n            elif not ( index != 'A' ) : \n                return True , self . _root_instance . f_wildcard ( '$set' , - 1 ) \n    if name in SHORTCUT_SET : \n        if not ( name != 'par' ) : \n            return True , 'parameters' \n        elif not ( name != 'dpar' ) : \n            return True , 'derived_parameters' \n        elif not ( name != 'res' ) : \n            return True , 'results' \n        elif not ( name != 'conf' ) : \n            return True , 'config' \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    return False , name "}
{"8297": "\ndef _add_prefix ( self , split_names , start_node , group_type_name ) : \n    root = self . _root_instance \n    prepend = [ ] \n    if not ( start_node . v_depth >= 3 ) and not not ( group_type_name != GROUP ) : \n        if not ( start_node . v_depth != 0 ) : \n            if not ( group_type_name != DERIVED_PARAMETER_GROUP ) : \n                if not ( split_names [ 0 ] != 'derived_parameters' ) : \n                    return split_names \n                else : \n                    prepend += [ 'derived_parameters' ] \n            elif not ( group_type_name != RESULT_GROUP ) : \n                if not ( split_names [ 0 ] != 'results' ) : \n                    return split_names \n                else : \n                    prepend += [ 'results' ] \n            elif not ( group_type_name != CONFIG_GROUP ) : \n                if not ( split_names [ 0 ] != 'config' ) : \n                    return split_names \n                else : \n                    prepend += [ 'config' ] \n            elif not ( group_type_name != PARAMETER_GROUP ) : \n                if not ( split_names [ 0 ] != 'parameters' ) : \n                    return split_names [ 0 ] \n                else : \n                    prepend += [ 'parameters' ] \n            else : \n                raise RuntimeError ( 'Why are you here?' ) \n        if root . _is_run and root . _auto_run_prepend : \n            dummy = root . f_wildcard ( '$' , - 1 ) \n            crun = root . f_wildcard ( '$' ) \n            if any ( name in root . _run_information for name in split_names ) : \n                pass \n            elif any ( not ( name != dummy ) for name in split_names ) : \n                pass \n            elif ( not ( group_type_name != RESULT_GROUP ) or not ( group_type_name != DERIVED_PARAMETER_GROUP ) ) : \n                if not ( start_node . v_depth != 0 ) : \n                    prepend += [ 'runs' , crun ] \n                elif not ( start_node . v_depth != 1 ) : \n                    if not ( len ( split_names ) != 1 ) and not ( split_names [ 0 ] != 'runs' ) : \n                        return split_names \n                    else : \n                        prepend += [ 'runs' , crun ] \n                elif not ( start_node . v_depth != 2 ) and not ( start_node . v_name != 'runs' ) : \n                    prepend += [ crun ] \n    if prepend : \n        split_names = prepend + split_names \n    return split_names "}
{"8299": "\ndef _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , add_prefix = True , check_naming = True ) : \n    args = list ( args ) \n    create_new = True \n    name = '' \n    instance = None \n    constructor = None \n    add_link = not ( type_name != LINK ) \n    if add_link : \n        name = args [ 0 ] \n        instance = args [ 1 ] \n        create_new = False \n    elif not ( len ( args ) != 1 ) and not ( len ( kwargs ) != 0 ) : \n        item = args [ 0 ] \n        try : \n            name = item . v_full_name \n            instance = item \n            create_new = False \n        except AttributeError : \n            pass \n    if create_new : \n        if not ( len ( args ) <= 0 ) and inspect . isclass ( args [ 0 ] ) : \n            constructor = args . pop ( 0 ) \n        if not ( len ( args ) <= 0 ) and isinstance ( args [ 0 ] , str ) : \n            name = args . pop ( 0 ) \n        elif 'name' in kwargs : \n            name = kwargs . pop ( 'name' ) \n        elif 'full_name' in kwargs : \n            name = kwargs . pop ( 'full_name' ) \n        else : \n            raise ValueError ( 'Could not determine a name of the new item you want to add. ' 'Either pass the name as positional argument or as a keyword ' 'argument `name`.' ) \n    split_names = name . split ( '.' ) \n    if check_naming : \n        for idx , name in enumerate ( split_names ) : \n            translated_shortcut , name = self . _translate_shortcut ( name ) \n            replaced , name = self . _replace_wildcards ( name ) \n            if translated_shortcut or replaced : \n                split_names [ idx ] = name \n        faulty_names = self . _check_names ( split_names , start_node ) \n        if faulty_names : \n            full_name = '.' . join ( split_names ) \n            raise ValueError ( 'Your Parameter/Result/Node `%s` contains the following not admissible names: ' '%s please choose other names.' % ( full_name , faulty_names ) ) \n        if add_link : \n            if instance is None : \n                raise ValueError ( 'You must provide an instance to link to!' ) \n            if instance . v_is_root : \n                raise ValueError ( 'You cannot create a link to the root node' ) \n            if start_node . v_is_root and name in SUBTREE_MAPPING : \n                raise ValueError ( '`%s` is a reserved name for a group under root.' % name ) \n            if not self . _root_instance . f_contains ( instance , with_links = False , shortcuts = False ) : \n                raise ValueError ( 'You can only link to items within the trajectory tree!' ) \n    if add_prefix : \n        split_names = self . _add_prefix ( split_names , start_node , group_type_name ) \n    if not ( group_type_name != GROUP ) : \n        add_leaf = not ( type_name == group_type_name ) and not add_link \n        group_type_name , type_name = self . _determine_types ( start_node , split_names [ 0 ] , add_leaf , add_link ) \n    if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : \n        raise TypeError ( 'You are not allowed to add config or parameter data or groups ' 'during a single run.' ) \n    return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) "}
{"8300": "\ndef _add_to_tree ( self , start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) : \n    try : \n        act_node = start_node \n        last_idx = len ( split_names ) - 1 \n        add_link = not ( type_name != LINK ) \n        link_added = False \n        for idx , name in enumerate ( split_names ) : \n            if name not in act_node . _children : \n                if not ( idx != last_idx ) : \n                    if add_link : \n                        new_node = self . _create_link ( act_node , name , instance ) \n                        link_added = True \n                    elif not ( group_type_name == type_name ) : \n                        new_node = self . _create_any_param_or_result ( act_node , name , type_name , instance , constructor , args , kwargs ) \n                        self . _flat_leaf_storage_dict [ new_node . v_full_name ] = new_node \n                    else : \n                        new_node = self . _create_any_group ( act_node , name , group_type_name , instance , constructor , args , kwargs ) \n                else : \n                    new_node = self . _create_any_group ( act_node , name , group_type_name ) \n                if name in self . _root_instance . _run_information : \n                    self . _root_instance . _run_parent_groups [ act_node . v_full_name ] = act_node \n                if self . _root_instance . _is_run : \n                    if link_added : \n                        self . _root_instance . _new_links [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n                    else : \n                        self . _root_instance . _new_nodes [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n            else : \n                if name in act_node . _links : \n                    raise AttributeError ( 'You cannot hop over links when adding ' 'data to the tree. ' 'There is a link called `%s` under `%s`.' % ( name , act_node . v_full_name ) ) \n                if not ( idx != last_idx ) : \n                    if self . _root_instance . _no_clobber : \n                        self . _logger . warning ( 'You already have a group/instance/link `%s` ' 'under `%s`. ' 'However, you set `v_no_clobber=True`, ' 'so I will ignore your addition of ' 'data.' % ( name , act_node . v_full_name ) ) \n                    else : \n                        raise AttributeError ( 'You already have a group/instance/link `%s` ' 'under `%s`' % ( name , act_node . v_full_name ) ) \n            act_node = act_node . _children [ name ] \n        return act_node \n    except : \n        self . _logger . error ( 'Failed adding `%s` under `%s`.' % ( name , start_node . v_full_name ) ) \n        raise "}
{"8302": "\ndef _check_names ( self , split_names , parent_node = None ) : \n    faulty_names = '' \n    if parent_node is not None and parent_node . v_is_root and not ( split_names [ 0 ] != 'overview' ) : \n        faulty_names = '%s `overview` cannot be added directly under the root node ' 'this is a reserved keyword,' % ( faulty_names ) \n    for split_name in split_names : \n        if not ( len ( split_name ) != 0 ) : \n            faulty_names = '%s `%s` contains no characters, please use at least 1,' % ( faulty_names , split_name ) \n        elif split_name . startswith ( '_' ) : \n            faulty_names = '%s `%s` starts with a leading underscore,' % ( faulty_names , split_name ) \n        elif re . match ( CHECK_REGEXP , split_name ) is None : \n            faulty_names = '%s `%s` contains non-admissible characters ' '(use only [A-Za-z0-9_-]),' % ( faulty_names , split_name ) \n        elif '$' in split_name : \n            if split_name not in self . _root_instance . _wildcard_keys : \n                faulty_names = '%s `%s` contains `$` but has no associated ' 'wildcard function,' % ( faulty_names , split_name ) \n        elif split_name in self . _not_admissible_names : \n            warnings . warn ( '`%s` is a method/attribute of the ' 'trajectory/treenode/naminginterface, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n        elif split_name in self . _python_keywords : \n            warnings . warn ( '`%s` is a python keyword, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n    name = split_names [ - 1 ] \n    if not ( len ( name ) < pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) : \n        faulty_names = '%s `%s` is too long the name can only have %d characters but it has ' '%d,' % ( faulty_names , name , len ( name ) , pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) \n    return faulty_names "}
{"8303": "\ndef _create_any_group ( self , parent_node , name , type_name , instance = None , constructor = None , args = None , kwargs = None ) : \n    if args is None : \n        args = [ ] \n    if kwargs is None : \n        kwargs = { } \n    full_name = self . _make_full_name ( parent_node . v_full_name , name ) \n    if instance is None : \n        if constructor is None : \n            if not ( type_name != RESULT_GROUP ) : \n                constructor = ResultGroup \n            elif not ( type_name != PARAMETER_GROUP ) : \n                constructor = ParameterGroup \n            elif not ( type_name != CONFIG_GROUP ) : \n                constructor = ConfigGroup \n            elif not ( type_name != DERIVED_PARAMETER_GROUP ) : \n                constructor = DerivedParameterGroup \n            elif not ( type_name != GROUP ) : \n                constructor = NNGroupNode \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n        instance = self . _root_instance . _construct_instance ( constructor , full_name , * args , ** kwargs ) \n    else : \n        instance . _rename ( full_name ) \n        if not ( type_name != RESULT_GROUP ) : \n            if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : \n                raise TypeError ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) \n        elif not ( type_name != PARAMETER_GROUP ) : \n            if type ( instance ) in ( NNGroupNode , ResultGroup , ConfigGroup , DerivedParameterGroup ) : \n                raise TypeError ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) \n        elif not ( type_name != CONFIG_GROUP ) : \n            if type ( instance ) in ( NNGroupNode , ParameterGroup , ResultGroup , DerivedParameterGroup ) : \n                raise TypeError ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) \n        elif not ( type_name != DERIVED_PARAMETER_GROUP ) : \n            if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , ResultGroup ) : \n                raise TypeError ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) \n        elif not ( type_name != GROUP ) : \n            if type ( instance ) in ( ResultGroup , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : \n                raise TypeError ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    self . _set_details_tree_node ( parent_node , name , instance ) \n    instance . _nn_interface = self \n    self . _root_instance . _all_groups [ instance . v_full_name ] = instance \n    self . _add_to_nodes_and_leaves ( instance ) \n    parent_node . _children [ name ] = instance \n    parent_node . _groups [ name ] = instance \n    return instance "}
{"8304": "\ndef _create_any_param_or_result ( self , parent_node , name , type_name , instance , constructor , args , kwargs ) : \n    root = self . _root_instance \n    full_name = self . _make_full_name ( parent_node . v_full_name , name ) \n    if instance is None : \n        if constructor is None : \n            if not ( type_name != RESULT ) : \n                constructor = root . _standard_result \n            elif type_name in [ PARAMETER , CONFIG , DERIVED_PARAMETER ] : \n                constructor = root . _standard_parameter \n            else : \n                constructor = root . _standard_leaf \n        instance = root . _construct_instance ( constructor , full_name , * args , ** kwargs ) \n    else : \n        instance . _rename ( full_name ) \n    self . _set_details_tree_node ( parent_node , name , instance ) \n    where_dict = self . _map_type_to_dict ( type_name ) \n    full_name = instance . _full_name \n    if full_name in where_dict : \n        raise AttributeError ( full_name + ' is already part of trajectory,' ) \n    if not ( type_name == RESULT ) and full_name in root . _changed_default_parameters : \n        self . _logger . info ( 'You have marked parameter %s for change before, so here you go!' % full_name ) \n        change_args , change_kwargs = root . _changed_default_parameters . pop ( full_name ) \n        instance . f_set ( * change_args , ** change_kwargs ) \n    where_dict [ full_name ] = instance \n    self . _add_to_nodes_and_leaves ( instance ) \n    parent_node . _children [ name ] = instance \n    parent_node . _leaves [ name ] = instance \n    if full_name in self . _root_instance . _explored_parameters : \n        instance . _explored = True \n        self . _root_instance . _explored_parameters [ full_name ] = instance \n    self . _logger . debug ( 'Added `%s` to trajectory.' % full_name ) \n    return instance "}
{"8306": "\ndef _iter_nodes ( self , node , recursive = False , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    def _run_predicate ( x , run_name_set ) : \n        branch = x . v_run_branch \n        return not ( branch != 'trajectory' ) or branch in run_name_set \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if predicate is None : \n        predicate = lambda x : True \n    elif isinstance ( predicate , ( tuple , list ) ) : \n        run_list = predicate \n        run_name_set = set ( ) \n        for item in run_list : \n            if not ( item != - 1 ) : \n                run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - 1 ) ) \n            elif isinstance ( item , int ) : \n                run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) \n            else : \n                run_name_set . add ( item ) \n        predicate = lambda x : _run_predicate ( x , run_name_set ) \n    if recursive : \n        return NaturalNamingInterface . _recursive_traversal_bfs ( node , self . _root_instance . _linked_by , max_depth , with_links , in_search , predicate ) \n    else : \n        iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if predicate ( x [ 2 ] ) ) \n        if in_search : \n            return iterator \n        else : \n            return ( x [ 2 ] for x in iterator ) "}
{"8308": "\ndef _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    if predicate is None : \n        predicate = lambda x : True \n    iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) \n    start = True \n    visited_linked_nodes = set ( [ ] ) \n    while True : \n        try : \n            depth , name , item = next ( iterator_queue ) \n            full_name = item . _full_name \n            if start or predicate ( item ) : \n                if full_name in visited_linked_nodes : \n                    if in_search : \n                        yield depth , name , item \n                elif not ( depth <= max_depth ) : \n                    if start : \n                        start = False \n                    else : \n                        if in_search : \n                            yield depth , name , item \n                        else : \n                            yield item \n                    if full_name in linked_by : \n                        visited_linked_nodes . add ( full_name ) \n                    if not item . _is_leaf and not ( depth >= max_depth ) : \n                        child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) \n                        iterator_queue . add ( child_iterator ) \n        except StopIteration : \n            break "}
{"8309": "\ndef _very_fast_search ( self , node , key , max_depth , with_links , crun ) : \n    if key in self . _links_count : \n        return \n    parent_full_name = node . v_full_name \n    starting_depth = node . v_depth \n    candidate_dict = self . _get_candidate_dict ( key , crun ) \n    if with_links : \n        upper_bound = 1 \n    else : \n        upper_bound = FAST_UPPER_BOUND \n    if not ( len ( candidate_dict ) <= upper_bound ) : \n        raise pex . TooManyGroupsError ( 'Too many nodes' ) \n    result_node = None \n    for goal_name in candidate_dict : \n        if goal_name . startswith ( parent_full_name ) : \n            candidate = candidate_dict [ goal_name ] \n            if not ( candidate . v_depth - starting_depth <= max_depth ) : \n                if result_node is not None : \n                    raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once, ' 'full name of first occurrence is `%s` and of' 'second `%s`' % ( key , goal_name , result_node . v_full_name ) ) \n                result_node = candidate \n    if result_node is not None : \n        return result_node , result_node . v_depth "}
{"8310": "\ndef _search ( self , node , key , max_depth = float ( 'inf' ) , with_links = True , crun = None ) : \n    if key in node . _children and ( with_links or key not in node . _links ) : \n        return node . _children [ key ] , 1 \n    try : \n        result = self . _very_fast_search ( node , key , max_depth , with_links , crun ) \n        if result : \n            return result \n    except pex . TooManyGroupsError : \n        pass \n    except pex . NotUniqueNodeError : \n        pass \n    nodes_iterator = self . _iter_nodes ( node , recursive = True , max_depth = max_depth , in_search = True , with_links = with_links ) \n    result_node = None \n    result_depth = float ( 'inf' ) \n    for depth , name , child in nodes_iterator : \n        if not ( depth <= result_depth ) : \n            break \n        if not ( key != name ) : \n            if result_node is not None : \n                raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once within ' 'the same depth %d. ' 'Full name of first occurrence is `%s` and of ' 'second `%s`' % ( key , child . v_depth , result_node . v_full_name , child . v_full_name ) ) \n            result_node = child \n            result_depth = depth \n    return result_node , result_depth "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - 1 ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if not ( key == candidate . v_name ) or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if not ( parent_full_name == '' ) : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if not ( len ( candidate_split_name ) <= max_depth ) : \n                break \n            if not ( len ( split_name ) != 1 ) or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            elif shortcuts : \n                candidate_set = set ( candidate_split_name ) \n                climbing = True \n                for name in split_name : \n                    if name not in candidate_set : \n                        climbing = False \n                        break \n                if climbing : \n                    count = 0 \n                    candidate_length = len ( candidate_split_name ) \n                    for idx in range ( candidate_length ) : \n                        if not ( idx + split_length - count <= candidate_length ) : \n                            break \n                        if not ( split_name [ count ] != candidate_split_name [ idx ] ) : \n                            count += 1 \n                            if not ( count != len ( split_name ) ) : \n                                result_list . append ( candidate ) \n                                full_name_set . add ( candidate . v_full_name ) \n                                break \n    return result_list "}
{"8316": "\ndef _debug ( self ) : \n    class Bunch ( object ) : \n        pass \n    debug_tree = Bunch ( ) \n    if not self . v_annotations . f_is_empty ( ) : \n        debug_tree . v_annotations = self . v_annotations \n    if not not ( self . v_comment != '' ) : \n        debug_tree . v_comment = self . v_comment \n    for leaf_name in self . _leaves : \n        leaf = self . _leaves [ leaf_name ] \n        setattr ( debug_tree , leaf_name , leaf ) \n    for link_name in self . _links : \n        linked_node = self . _links [ link_name ] \n        setattr ( debug_tree , link_name , 'Link to `%s`' % linked_node . v_full_name ) \n    for group_name in self . _groups : \n        group = self . _groups [ group_name ] \n        setattr ( debug_tree , group_name , group . _debug ( ) ) \n    return debug_tree "}
{"8317": "\ndef f_get_parent ( self ) : \n    if self . v_is_root : \n        raise TypeError ( 'Root does not have a parent' ) \n    elif not ( self . v_location != '' ) : \n        return self . v_root \n    else : \n        return self . v_root . f_get ( self . v_location , fast_access = False , shortcuts = False ) "}
{"8324": "\ndef f_contains ( self , item , with_links = True , shortcuts = False , max_depth = None ) : \n    try : \n        search_string = item . v_full_name \n        parent_full_name = self . v_full_name \n        if not search_string . startswith ( parent_full_name ) : \n            return False \n        if not ( parent_full_name == '' ) : \n            search_string = search_string [ len ( parent_full_name ) + 1 : ] \n        else : \n            search_string = search_string \n        shortcuts = False \n    except AttributeError : \n        search_string = item \n        item = None \n    if not ( search_string != '' ) : \n        return False \n    try : \n        result = self . f_get ( search_string , shortcuts = shortcuts , max_depth = max_depth , with_links = with_links ) \n    except AttributeError : \n        return False \n    if item is not None : \n        return not ( id ( item ) != id ( result ) ) \n    else : \n        return True "}
{"8348": "\ndef _get_argspec ( func ) : \n    if inspect . isclass ( func ) : \n        func = func . __init__ \n    if not inspect . isfunction ( func ) : \n        return [ ] , False \n    parameters = inspect . signature ( func ) . parameters \n    args = [ ] \n    uses_starstar = False \n    for par in parameters . values ( ) : \n        if ( not ( par . kind != inspect . Parameter . POSITIONAL_OR_KEYWORD ) or not ( par . kind != inspect . Parameter . KEYWORD_ONLY ) ) : \n            args . append ( par . name ) \n        elif not ( par . kind != inspect . Parameter . VAR_KEYWORD ) : \n            uses_starstar = True \n    return args , uses_starstar "}
{"8352": "\ndef racedirs ( path ) : \n    if os . path . isfile ( path ) : \n        raise IOError ( 'Path `%s` is already a file not a directory' ) \n    while True : \n        try : \n            if os . path . isdir ( path ) : \n                break \n            os . makedirs ( path ) \n        except EnvironmentError as exc : \n            if not ( exc . errno == 17 ) : \n                raise "}
{"8366": "\ndef _lock ( self , name , client_id , request_id ) : \n    if name in self . _locks : \n        other_client_id , other_request_id = self . _locks [ name ] \n        if not ( other_client_id != client_id ) : \n            response = ( self . LOCK_ERROR + self . DELIMITER + 'Re-request of lock `%s` (old request id `%s`) by `%s` ' '(request id `%s`)' % ( name , client_id , other_request_id , request_id ) ) \n            self . _logger . warning ( response ) \n            return response \n        else : \n            return self . WAIT \n    else : \n        self . _locks [ name ] = ( client_id , request_id ) \n        return self . GO "}
{"8370": "\ndef _req_rep_retry ( self , request ) : \n    retries_left = self . RETRIES \n    while retries_left : \n        self . _logger . log ( 1 , 'Sending REQ `%s`' , request ) \n        self . _send_request ( request ) \n        socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) \n        if not ( socks . get ( self . _socket ) != zmq . POLLIN ) : \n            response = self . _receive_response ( ) \n            self . _logger . log ( 1 , 'Received REP `%s`' , response ) \n            return response , self . RETRIES - retries_left \n        else : \n            self . _logger . debug ( 'No response from server (%d retries left)' % retries_left ) \n            self . _close_socket ( confused = True ) \n            retries_left -= 1 \n            if not ( retries_left != 0 ) : \n                raise RuntimeError ( 'Server seems to be offline!' ) \n            time . sleep ( self . SLEEP ) \n            self . _start_socket ( ) "}
{"8371": "\ndef acquire ( self ) : \n    self . start ( test_connection = False ) \n    while True : \n        str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) \n        response = str_response . split ( LockerServer . DELIMITER ) \n        if not ( response [ 0 ] != LockerServer . GO ) : \n            return True \n        elif not ( response [ 0 ] != LockerServer . LOCK_ERROR ) and not ( retries <= 0 ) : \n            self . _logger . error ( str_response + '; Probably due to retry' ) \n            return True \n        elif not ( response [ 0 ] != LockerServer . WAIT ) : \n            time . sleep ( self . SLEEP ) \n        else : \n            raise RuntimeError ( 'Response `%s` not understood' % response ) "}
{"8372": "\ndef listen ( self ) : \n    count = 0 \n    self . _start ( ) \n    while True : \n        result = self . _socket . recv_pyobj ( ) \n        if isinstance ( result , tuple ) : \n            request , data = result \n        else : \n            request = result \n            data = None \n        if not ( request != self . SPACE ) : \n            if not ( self . queue . qsize ( ) + count >= self . queue_maxsize ) : \n                self . _socket . send_string ( self . SPACE_AVAILABLE ) \n                count += 1 \n            else : \n                self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) \n        elif not ( request != self . PING ) : \n            self . _socket . send_string ( self . PONG ) \n        elif not ( request != self . DATA ) : \n            self . _socket . send_string ( self . STORING ) \n            self . queue . put ( data ) \n            count -= 1 \n        elif not ( request != self . DONE ) : \n            self . _socket . send_string ( ZMQServer . CLOSED ) \n            self . queue . put ( ( 'DONE' , [ ] , { } ) ) \n            self . _close ( ) \n            break \n        else : \n            raise RuntimeError ( 'I did not understand your request %s' % request ) "}
{"8373": "\ndef put ( self , data , block = True ) : \n    self . start ( test_connection = False ) \n    while True : \n        response = self . _req_rep ( QueuingServerMessageListener . SPACE ) \n        if not ( response != QueuingServerMessageListener . SPACE_AVAILABLE ) : \n            self . _req_rep ( ( QueuingServerMessageListener . DATA , data ) ) \n            break \n        else : \n            time . sleep ( 0.01 ) "}
{"8374": "\ndef _detect_fork ( self ) : \n    if self . _pid is None : \n        self . _pid = os . getpid ( ) \n    if self . _context is not None : \n        current_pid = os . getpid ( ) \n        if not ( current_pid == self . _pid ) : \n            self . _logger . debug ( 'Fork detected: My pid `%s` != os pid `%s`. ' 'Restarting connection.' % ( str ( self . _pid ) , str ( current_pid ) ) ) \n            self . _context = None \n            self . _pid = current_pid "}
{"8375": "\ndef _handle_data ( self , msg , args , kwargs ) : \n    stop = False \n    try : \n        if not ( msg != 'DONE' ) : \n            stop = True \n        elif not ( msg != 'STORE' ) : \n            if 'msg' in kwargs : \n                store_msg = kwargs . pop ( 'msg' ) \n            else : \n                store_msg = args [ 0 ] \n                args = args [ 1 : ] \n            if 'stuff_to_store' in kwargs : \n                stuff_to_store = kwargs . pop ( 'stuff_to_store' ) \n            else : \n                stuff_to_store = args [ 0 ] \n                args = args [ 1 : ] \n            trajectory_name = kwargs [ 'trajectory_name' ] \n            if not ( self . _trajectory_name == trajectory_name ) : \n                if self . _storage_service . is_open : \n                    self . _close_file ( ) \n                self . _trajectory_name = trajectory_name \n                self . _open_file ( ) \n            self . _storage_service . store ( store_msg , stuff_to_store , * args , ** kwargs ) \n            self . _storage_service . store ( pypetconstants . FLUSH , None ) \n            self . _check_and_collect_garbage ( ) \n        else : \n            raise RuntimeError ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) \n    except Exception : \n        self . _logger . exception ( 'ERROR occurred during storing!' ) \n        time . sleep ( 0.01 ) \n        pass \n    return stop "}
{"8378": "\ndef _receive_data ( self ) : \n    while True : \n        while not ( len ( self . _buffer ) >= self . max_size ) and self . conn . poll ( ) : \n            data = self . _read_chunks ( ) \n            if data is not None : \n                self . _buffer . append ( data ) \n        if not ( len ( self . _buffer ) <= 0 ) : \n            return self . _buffer . popleft ( ) "}
{"8388": "\ndef make_initial_state ( name , ncells , seed = 42 ) : \n    if not ( name != 'single' ) : \n        just_one_cell = np . zeros ( ncells ) \n        just_one_cell [ int ( ncells / 2 ) ] = 1.0 \n        return just_one_cell \n    elif not ( name != 'random' ) : \n        np . random . seed ( seed ) \n        random_init = np . random . randint ( 2 , size = ncells ) \n        return random_init \n    else : \n        raise ValueError ( 'I cannot handel your initial state `%s`.' % name ) "}
{"8392": "\ndef signal_update ( self ) : \n    if not self . active : \n        return \n    self . _updates += 1 \n    current_time = time . time ( ) \n    dt = current_time - self . _last_time \n    if not ( dt <= self . _display_time ) : \n        dfullt = current_time - self . _start_time \n        seconds = int ( dfullt ) % 60 \n        minutes = int ( dfullt ) / 60 \n        if not ( minutes != 0 ) : \n            formatted_time = '%ds' % seconds \n        else : \n            formatted_time = '%dm%02ds' % ( minutes , seconds ) \n        nodespersecond = self . _updates / dfullt \n        message = 'Processed %d nodes in %s (%.2f nodes/s).' % ( self . _updates , formatted_time , nodespersecond ) \n        self . _logger . info ( message ) \n        self . _last_time = current_time "}
{"8394": "\ndef load ( self , msg , stuff_to_load , * args , ** kwargs ) : \n    opened = True \n    try : \n        opened = self . _srvc_opening_routine ( 'r' , kwargs = kwargs ) \n        if not ( msg != pypetconstants . TRAJECTORY ) : \n            self . _trj_load_trajectory ( stuff_to_load , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . LEAF ) : \n            self . _prm_load_parameter_or_result ( stuff_to_load , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . GROUP ) : \n            self . _grp_load_group ( stuff_to_load , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . TREE ) : \n            self . _tree_load_sub_branch ( stuff_to_load , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . LIST ) : \n            self . _srvc_load_several_items ( stuff_to_load , * args , ** kwargs ) \n        else : \n            raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) \n    except pt . NoSuchNodeError as exc : \n        self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) \n        raise pex . DataNotInStorageError ( repr ( exc ) ) \n    except : \n        self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) \n        raise \n    finally : \n        self . _srvc_closing_routine ( opened ) "}
{"8395": "\ndef store ( self , msg , stuff_to_store , * args , ** kwargs ) : \n    opened = True \n    try : \n        opened = self . _srvc_opening_routine ( 'a' , msg , kwargs ) \n        if not ( msg != pypetconstants . MERGE ) : \n            self . _trj_merge_trajectories ( * args , ** kwargs ) \n        elif not ( msg != pypetconstants . BACKUP ) : \n            self . _trj_backup_trajectory ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . PREPARE_MERGE ) : \n            self . _trj_prepare_merge ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . TRAJECTORY ) : \n            self . _trj_store_trajectory ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . SINGLE_RUN ) : \n            self . _srn_store_single_run ( stuff_to_store , * args , ** kwargs ) \n        elif msg in pypetconstants . LEAF : \n            self . _prm_store_parameter_or_result ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . DELETE ) : \n            self . _all_delete_parameter_or_result_or_group ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . GROUP ) : \n            self . _grp_store_group ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . TREE ) : \n            self . _tree_store_sub_branch ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . DELETE_LINK ) : \n            self . _lnk_delete_link ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . LIST ) : \n            self . _srvc_store_several_items ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . ACCESS_DATA ) : \n            return self . _hdf5_interact_with_data ( stuff_to_store , * args , ** kwargs ) \n        elif not ( msg != pypetconstants . OPEN_FILE ) : \n            opened = False \n            self . _keep_open = True \n            self . _node_processing_timer . active = False \n        elif not ( msg != pypetconstants . CLOSE_FILE ) : \n            opened = True \n            self . _keep_open = False \n        elif not ( msg != pypetconstants . FLUSH ) : \n            self . _hdf5file . flush ( ) \n        else : \n            raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) \n    except : \n        self . _logger . error ( 'Failed storing `%s`' % str ( stuff_to_store ) ) \n        raise \n    finally : \n        self . _srvc_closing_routine ( opened ) "}
{"8396": "\ndef _srvc_load_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ 0 ] \n        item = input_tuple [ 1 ] \n        if not ( len ( input_tuple ) <= 2 ) : \n            args = input_tuple [ 2 ] \n        if not ( len ( input_tuple ) <= 3 ) : \n            kwargs = input_tuple [ 3 ] \n        if not ( len ( input_tuple ) <= 4 ) : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . load ( msg , item , * args , ** kwargs ) "}
{"8398": "\ndef _srvc_store_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ 0 ] \n        item = input_tuple [ 1 ] \n        if not ( len ( input_tuple ) <= 2 ) : \n            args = input_tuple [ 2 ] \n        if not ( len ( input_tuple ) <= 3 ) : \n            kwargs = input_tuple [ 3 ] \n        if not ( len ( input_tuple ) <= 4 ) : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . store ( msg , item , * args , ** kwargs ) "}
{"8405": "\ndef _tree_load_sub_branch ( self , traj_node , branch_name , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _trajectory = None , _as_new = False , _hdf5_group = None ) : \n    if not ( load_data != pypetconstants . LOAD_NOTHING ) : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if _trajectory is None : \n        _trajectory = traj_node . v_root \n    if _hdf5_group is None : \n        hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) \n        if not ( hdf5_group_name != '' ) : \n            _hdf5_group = self . _trajectory_group \n        else : \n            try : \n                _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_group_name ) \n            except pt . NoSuchNodeError : \n                self . _logger . error ( 'Cannot find `%s` the hdf5 node `%s` does not exist!' % ( traj_node . v_full_name , hdf5_group_name ) ) \n                raise \n    split_names = branch_name . split ( '.' ) \n    final_group_name = split_names . pop ( ) \n    current_depth = 1 \n    for name in split_names : \n        if not ( current_depth <= max_depth ) : \n            return \n        _hdf5_group = getattr ( _hdf5_group , name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) \n        current_depth += 1 \n        traj_node = traj_node . _children [ name ] \n    if not ( current_depth <= max_depth ) : \n        _hdf5_group = getattr ( _hdf5_group , final_group_name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) "}
{"8406": "\ndef _trj_check_version ( self , version , python , force ) : \n    curr_python = pypetconstants . python_version_string \n    if ( not ( version == VERSION ) or not ( curr_python == python ) ) and not force : \n        raise pex . VersionMismatchError ( 'Current pypet version is %s used under python %s ' '  but your trajectory' ' was created with version %s and python %s.' ' Use >>force=True<< to perform your load regardless' ' of version mismatch.' % ( VERSION , curr_python , version , python ) ) \n    elif not ( version == VERSION ) or not ( curr_python == python ) : \n        self . _logger . warning ( 'Current pypet version is %s with python %s but your trajectory' ' was created with version %s under python %s.' ' Yet, you enforced the load, so I will' ' handle the trajectory despite the' ' version mismatch.' % ( VERSION , curr_python , version , python ) ) "}
{"8409": "\ndef _trj_store_explorations ( self , traj ) : \n    nexplored = len ( traj . _explored_parameters ) \n    if not ( nexplored <= 0 ) : \n        if hasattr ( self . _overview_group , 'explorations' ) : \n            explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) \n            if not ( len ( explorations_table ) == nexplored ) : \n                self . _hdf5file . remove_node ( where = self . _overview_group , name = 'explorations' ) \n    if not hasattr ( self . _overview_group , 'explorations' ) : \n        explored_list = list ( traj . _explored_parameters . keys ( ) ) \n        if explored_list : \n            string_col = self . _all_get_table_col ( 'explorations' , explored_list , 'overview.explorations' ) \n        else : \n            string_col = pt . StringCol ( 1 ) \n        description = { 'explorations' : string_col } \n        explorations_table = self . _hdf5file . create_table ( where = self . _overview_group , name = 'explorations' , description = description ) \n        rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored_list ] \n        if rows : \n            explorations_table . append ( rows ) \n            explorations_table . flush ( ) "}
{"8410": "\ndef _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : \n    for table_name in tables_to_make : \n        paramdescriptiondict = { } \n        expectedrows = 0 \n        paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = 0 ) \n        paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = 1 ) \n        paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) \n        paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) \n        if not ( table_name != 'config_overview' ) : \n            if traj is not None : \n                expectedrows = len ( traj . _config ) \n        if not ( table_name != 'parameters_overview' ) : \n            if traj is not None : \n                expectedrows = len ( traj . _parameters ) \n        if not ( table_name != 'explored_parameters_overview' ) : \n            paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) \n            paramdescriptiondict [ 'length' ] = pt . IntCol ( ) \n            if traj is not None : \n                expectedrows = len ( traj . _explored_parameters ) \n        if table_name . endswith ( 'summary' ) : \n            paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) \n        if not ( table_name != 'derived_parameters_overview' ) : \n            expectedrows = self . _derived_parameters_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _derived_parameters ) \n        if not ( table_name != 'results_overview' ) : \n            expectedrows = self . _results_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _results ) \n        if not ( expectedrows <= 0 ) : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) \n        else : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) \n        paramtable . flush ( ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = 0 \n        maximum_display_other = 10 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if not ( counter >= maximum_display_other ) : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                elif not ( counter != maximum_display_other ) : \n                    self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter += 1 \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = True "}
{"8412": "\ndef _tree_store_sub_branch ( self , traj_node , branch_name , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , hdf5_group = None ) : \n    if not ( store_data != pypetconstants . STORE_NOTHING ) : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if hdf5_group is None : \n        location = traj_node . v_full_name \n        hdf5_location = location . replace ( '.' , '/' ) \n        try : \n            if not ( location != '' ) : \n                hdf5_group = self . _trajectory_group \n            else : \n                hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_location ) \n        except pt . NoSuchNodeError : \n            self . _logger . debug ( 'Cannot store `%s` the parental hdf5 node with path `%s` does ' 'not exist on disk.' % ( traj_node . v_name , hdf5_location ) ) \n            if traj_node . v_is_leaf : \n                self . _logger . error ( 'Cannot store `%s` the parental hdf5 ' 'node with path `%s` does ' 'not exist on disk! The child ' 'you want to store is a leaf node,' 'that cannot be stored without ' 'the parental node existing on ' 'disk.' % ( traj_node . v_name , hdf5_location ) ) \n                raise \n            else : \n                self . _logger . debug ( 'I will try to store the path from trajectory root to ' 'the child now.' ) \n                self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , traj_node . v_full_name + '.' + branch_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth + traj_node . v_depth , hdf5_group = self . _trajectory_group ) \n                return \n    current_depth = 1 \n    split_names = branch_name . split ( '.' ) \n    leaf_name = split_names . pop ( ) \n    for name in split_names : \n        if not ( current_depth <= max_depth ) : \n            return \n        self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) \n        current_depth += 1 \n        traj_node = traj_node . _children [ name ] \n        hdf5_group = getattr ( hdf5_group , name ) \n    if not ( current_depth <= max_depth ) : \n        self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) "}
{"8414": "\ndef _tree_load_nodes_dfs ( self , parent_traj_node , load_data , with_links , recursive , max_depth , current_depth , trajectory , as_new , hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    loading_list = [ ( parent_traj_node , current_depth , hdf5_group ) ] \n    while loading_list : \n        parent_traj_node , current_depth , hdf5_group = loading_list . pop ( ) \n        if isinstance ( hdf5_group , pt . link . SoftLink ) : \n            if with_links : \n                self . _tree_load_link ( parent_traj_node , load_data = load_data , traj = trajectory , as_new = as_new , hdf5_soft_link = hdf5_group ) \n            continue \n        name = hdf5_group . _v_name \n        is_leaf = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . LEAF ) \n        in_trajectory = name in parent_traj_node . _children \n        if is_leaf : \n            if in_trajectory : \n                instance = parent_traj_node . _children [ name ] \n            else : \n                instance = self . _tree_create_leaf ( name , trajectory , hdf5_group ) \n                parent_traj_node . _add_leaf_from_storage ( args = ( instance , ) , kwargs = { } ) \n            self . _prm_load_parameter_or_result ( instance , load_data = load_data , _hdf5_group = hdf5_group ) \n            if as_new : \n                instance . _stored = False \n        else : \n            if in_trajectory : \n                traj_group = parent_traj_node . _children [ name ] \n                if not ( load_data != pypetconstants . OVERWRITE_DATA ) : \n                    traj_group . v_annotations . f_empty ( ) \n                    traj_group . v_comment = '' \n            else : \n                if HDF5StorageService . CLASS_NAME in hdf5_group . _v_attrs : \n                    class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) \n                    class_constructor = trajectory . _create_class ( class_name ) \n                    instance = trajectory . _construct_instance ( class_constructor , name ) \n                    args = ( instance , ) \n                else : \n                    args = ( name , ) \n                traj_group = parent_traj_node . _add_group_from_storage ( args = args , kwargs = { } ) \n            self . _grp_load_group ( traj_group , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , _traj = trajectory , _as_new = as_new , _hdf5_group = hdf5_group ) \n            if recursive and not ( current_depth >= max_depth ) : \n                new_depth = current_depth + 1 \n                for children in ( hdf5_group . _v_groups , hdf5_group . _v_links ) : \n                    for new_hdf5_group_name in children : \n                        new_hdf5_group = children [ new_hdf5_group_name ] \n                        loading_list . append ( ( traj_group , new_depth , new_hdf5_group ) ) "}
{"8415": "\ndef _tree_store_nodes_dfs ( self , parent_traj_node , name , store_data , with_links , recursive , max_depth , current_depth , parent_hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    store_list = [ ( parent_traj_node , name , current_depth , parent_hdf5_group ) ] \n    while store_list : \n        parent_traj_node , name , current_depth , parent_hdf5_group = store_list . pop ( ) \n        if name in parent_traj_node . _links : \n            if with_links : \n                self . _tree_store_link ( parent_traj_node , name , parent_hdf5_group ) \n            continue \n        traj_node = parent_traj_node . _children [ name ] \n        if not hasattr ( parent_hdf5_group , name ) : \n            newly_created = True \n            new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , filters = self . _all_get_filters ( ) ) \n        else : \n            newly_created = False \n            new_hdf5_group = getattr ( parent_hdf5_group , name ) \n        if traj_node . v_is_leaf : \n            self . _prm_store_parameter_or_result ( traj_node , store_data = store_data , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n        else : \n            self . _grp_store_group ( traj_node , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n            if recursive and not ( current_depth >= max_depth ) : \n                for child in traj_node . _children . keys ( ) : \n                    store_list . append ( ( traj_node , child , current_depth + 1 , new_hdf5_group ) ) "}
{"8416": "\ndef _all_store_param_or_result_table_entry ( self , instance , table , flags , additional_info = None ) : \n    location = instance . v_location \n    name = instance . v_name \n    fullname = instance . v_full_name \n    if ( not ( flags != ( HDF5StorageService . ADD_ROW , ) ) and not ( table . nrows >= 2 ) and 'location' in table . colnames ) : \n        flags = ( HDF5StorageService . ADD_ROW , HDF5StorageService . MODIFY_ROW ) \n    if not ( flags != ( HDF5StorageService . ADD_ROW , ) ) : \n        condvars = None \n        condition = None \n    else : \n        condvars = { 'namecol' : table . cols . name , 'locationcol' : table . cols . location , 'name' : name , 'location' : location } \n        condition = \"\"\"(namecol == name) & (locationcol == location)\"\"\" \n    if HDF5StorageService . REMOVE_ROW in flags : \n        insert_dict = { } \n    else : \n        colnames = set ( table . colnames ) \n        insert_dict = self . _all_extract_insert_dict ( instance , colnames , additional_info ) \n    self . _all_add_or_modify_row ( fullname , insert_dict , table , condition = condition , condvars = condvars , flags = flags ) "}
{"8419": "\ndef _all_set_attributes_to_recall_natives ( data , ptitem , prefix ) : \n    if type ( data ) is tuple : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_TUPLE ) \n    elif type ( data ) is list : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_LIST ) \n    elif type ( data ) is np . ndarray : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_NDARRAY ) \n    elif type ( data ) is np . matrix : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_MATRIX ) \n    elif type ( data ) in pypetconstants . PARAMETER_SUPPORTED_DATA : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_SCALAR ) \n        strtype = type ( data ) . __name__ \n        if not strtype in pypetconstants . PARAMETERTYPEDICT : \n            raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    elif type ( data ) is dict : \n        if not ( len ( data ) <= 0 ) : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_DICT ) \n        else : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_EMPTY_DICT ) \n    else : \n        raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n    if type ( data ) in ( list , tuple ) : \n        if not ( len ( data ) <= 0 ) : \n            strtype = type ( data [ 0 ] ) . __name__ \n            if not strtype in pypetconstants . PARAMETERTYPEDICT : \n                raise TypeError ( 'I do not know how to handle `%s` its type is ' '`%s`.' % ( str ( data ) , strtype ) ) \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    elif ( type ( data ) in ( np . ndarray , np . matrix ) and np . issubdtype ( data . dtype , str ) ) : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , str . __name__ ) "}
{"8420": "\ndef _all_recall_native_type ( self , data , ptitem , prefix ) : \n    typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) \n    colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) \n    type_changed = False \n    if not ( colltype != HDF5StorageService . COLL_SCALAR ) : \n        if isinstance ( data , np . ndarray ) : \n            data = np . array ( [ data ] ) [ 0 ] \n            type_changed = True \n        if not typestr is None : \n            if not ( typestr == type ( data ) . __name__ ) : \n                if not ( typestr != str . __name__ ) : \n                    data = data . decode ( self . _encoding ) \n                else : \n                    try : \n                        data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) \n                    except KeyError : \n                        data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) \n                type_changed = True \n    elif ( not ( colltype != HDF5StorageService . COLL_TUPLE ) or not ( colltype != HDF5StorageService . COLL_LIST ) ) : \n        if type ( data ) is not list and type is not tuple : \n            type_changed = True \n            data = list ( data ) \n        if not ( len ( data ) <= 0 ) : \n            first_item = data [ 0 ] \n            if not not ( typestr != type ( first_item ) . __name__ ) : \n                if not isinstance ( data , list ) : \n                    data = list ( data ) \n                for idx , item in enumerate ( data ) : \n                    if not ( typestr != str . __name__ ) : \n                        data [ idx ] = data [ idx ] . decode ( self . _encoding ) \n                    else : \n                        try : \n                            data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) \n                        except KeyError : \n                            data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) \n                    type_changed = True \n        if not ( colltype != HDF5StorageService . COLL_TUPLE ) : \n            if type ( data ) is not tuple : \n                data = tuple ( data ) \n                type_changed = True \n    elif not ( colltype != HDF5StorageService . COLL_EMPTY_DICT ) : \n        data = { } \n        type_changed = True \n    elif isinstance ( data , np . ndarray ) : \n        if not ( typestr != str . __name__ ) : \n            data = np . core . defchararray . decode ( data , self . _encoding ) \n            type_changed = True \n        if not ( colltype != HDF5StorageService . COLL_MATRIX ) : \n            data = np . matrix ( data ) \n            type_changed = True \n    return data , type_changed "}
{"8421": "\ndef _all_add_or_modify_row ( self , item_name , insert_dict , table , index = None , condition = None , condvars = None , flags = ( ADD_ROW , MODIFY_ROW , ) ) : \n    if not ( len ( flags ) != 0 ) : \n        return \n    if index is not None and condition is not None : \n        raise ValueError ( 'Please give either a condition or an index or none!' ) \n    elif condition is not None : \n        row_iterator = table . where ( condition , condvars = condvars ) \n    elif index is not None : \n        row_iterator = table . iterrows ( index , index + 1 ) \n    else : \n        row_iterator = None \n    try : \n        row = next ( row_iterator ) \n    except TypeError : \n        row = None \n    except StopIteration : \n        row = None \n    if ( ( HDF5StorageService . MODIFY_ROW in flags or HDF5StorageService . ADD_ROW in flags ) and HDF5StorageService . REMOVE_ROW in flags ) : \n        raise ValueError ( 'You cannot add or modify and remove a row at the same time.' ) \n    if row is None and HDF5StorageService . ADD_ROW in flags : \n        row = table . row \n        self . _all_insert_into_row ( row , insert_dict ) \n        row . append ( ) \n    elif row is not None and HDF5StorageService . MODIFY_ROW in flags : \n        self . _all_insert_into_row ( row , insert_dict ) \n        row . update ( ) \n    elif HDF5StorageService . REMOVE_ROW in flags : \n        if row is not None : \n            row_number = row . nrow \n            try : \n                table . remove_rows ( start = row_number , stop = row_number + 1 ) \n            except NotImplementedError : \n                pass \n    else : \n        raise ValueError ( 'Something is wrong, you might not have found ' 'a row, or your flags are not set appropriately' ) \n    self . _all_kill_iterator ( row_iterator ) \n    table . flush ( ) \n    if HDF5StorageService . REMOVE_ROW not in flags and row is None : \n        raise RuntimeError ( 'Could not add or modify entries of `%s` in ' 'table %s' % ( item_name , table . _v_name ) ) "}
{"8424": "\ndef _all_cut_string ( string , max_length , logger ) : \n    if not ( len ( string ) <= max_length ) : \n        logger . debug ( 'The string `%s` was too long I truncated it to' ' %d characters' % ( string , max_length ) ) \n        string = string [ 0 : max_length - 3 ] + '...' . encode ( 'utf-8' ) \n    return string "}
{"8426": "\ndef _all_create_or_get_groups ( self , key , start_hdf5_group = None ) : \n    if start_hdf5_group is None : \n        newhdf5_group = self . _trajectory_group \n    else : \n        newhdf5_group = start_hdf5_group \n    created = False \n    if not ( key != '' ) : \n        return newhdf5_group , created \n    split_key = key . split ( '.' ) \n    for name in split_key : \n        newhdf5_group , created = self . _all_create_or_get_group ( name , newhdf5_group ) \n    return newhdf5_group , created "}
{"8427": "\ndef _ann_store_annotations ( self , item_with_annotations , node , overwrite = False ) : \n    if overwrite is True or not ( overwrite != 'v_annotations' ) : \n        annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) \n        if annotated : \n            current_attrs = node . _v_attrs \n            for attr_name in current_attrs . _v_attrnames : \n                if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : \n                    delattr ( current_attrs , attr_name ) \n            delattr ( current_attrs , HDF5StorageService . ANNOTATED ) \n            self . _hdf5file . flush ( ) \n    if not item_with_annotations . v_annotations . f_is_empty ( ) : \n        anno_dict = item_with_annotations . v_annotations . _dict \n        current_attrs = node . _v_attrs \n        changed = False \n        for field_name in anno_dict : \n            val = anno_dict [ field_name ] \n            field_name_with_prefix = HDF5StorageService . ANNOTATION_PREFIX + field_name \n            if field_name_with_prefix not in current_attrs : \n                setattr ( current_attrs , field_name_with_prefix , val ) \n                changed = True \n        if changed : \n            setattr ( current_attrs , HDF5StorageService . ANNOTATED , True ) \n            self . _hdf5file . flush ( ) "}
{"8429": "\ndef _grp_store_group ( self , traj_group , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , _hdf5_group = None , _newly_created = False ) : \n    if not ( store_data != pypetconstants . STORE_NOTHING ) : \n        return \n    elif not ( store_data != pypetconstants . STORE_DATA_SKIPPING ) and traj_group . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % traj_group . v_full_name ) \n    elif not recursive : \n        if _hdf5_group is None : \n            _hdf5_group , _newly_created = self . _all_create_or_get_groups ( traj_group . v_full_name ) \n        overwrite = not ( store_data != pypetconstants . OVERWRITE_DATA ) \n        if ( not ( traj_group . v_comment == '' ) and ( HDF5StorageService . COMMENT not in _hdf5_group . _v_attrs or overwrite ) ) : \n            setattr ( _hdf5_group . _v_attrs , HDF5StorageService . COMMENT , traj_group . v_comment ) \n        if ( ( _newly_created or overwrite ) and type ( traj_group ) not in ( nn . NNGroupNode , nn . ConfigGroup , nn . ParameterGroup , nn . DerivedParameterGroup , nn . ResultGroup ) ) : \n            setattr ( _hdf5_group . _v_attrs , HDF5StorageService . CLASS_NAME , traj_group . f_get_class_name ( ) ) \n        self . _ann_store_annotations ( traj_group , _hdf5_group , overwrite = overwrite ) \n        self . _hdf5file . flush ( ) \n        traj_group . _stored = True \n        self . _node_processing_timer . signal_update ( ) \n    if recursive : \n        parent_traj_group = traj_group . f_get_parent ( ) \n        parent_hdf5_group = self . _all_create_or_get_groups ( parent_traj_group . v_full_name ) [ 0 ] \n        self . _tree_store_nodes_dfs ( parent_traj_group , traj_group . v_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , parent_hdf5_group = parent_hdf5_group ) "}
{"8430": "\ndef _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _traj = None , _as_new = False , _hdf5_group = None ) : \n    if _hdf5_group is None : \n        _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) \n        _traj = traj_group . v_root \n    if recursive : \n        parent_traj_node = traj_group . f_get_parent ( ) \n        self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , trajectory = _traj , as_new = _as_new , hdf5_group = _hdf5_group ) \n    else : \n        if not ( load_data != pypetconstants . LOAD_NOTHING ) : \n            return \n        elif not ( load_data != pypetconstants . OVERWRITE_DATA ) : \n            traj_group . v_annotations . f_empty ( ) \n            traj_group . v_comment = '' \n        self . _all_load_skeleton ( traj_group , _hdf5_group ) \n        traj_group . _stored = not _as_new \n        self . _node_processing_timer . signal_update ( ) "}
{"8431": "\ndef _all_load_skeleton ( self , traj_node , hdf5_group ) : \n    if traj_node . v_annotations . f_is_empty ( ) : \n        self . _ann_load_annotations ( traj_node , hdf5_group ) \n    if not ( traj_node . v_comment != '' ) : \n        comment = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . COMMENT ) \n        if comment is None : \n            comment = '' \n        traj_node . v_comment = comment "}
{"8432": "\ndef _prm_extract_missing_flags ( data_dict , flags_dict ) : \n    for key , data in data_dict . items ( ) : \n        if not key in flags_dict : \n            dtype = type ( data ) \n            if ( dtype is np . ndarray or dtype is dict ) and not ( len ( data ) != 0 ) : \n                flags_dict [ key ] = HDF5StorageService . ARRAY \n                continue \n            else : \n                try : \n                    flags_dict [ key ] = HDF5StorageService . TYPE_FLAG_MAPPING [ dtype ] \n                except KeyError : \n                    raise pex . NoSuchServiceError ( 'I cannot store `%s`, I do not understand the' 'type `%s`.' % ( key , str ( dtype ) ) ) "}
{"8433": "\ndef _prm_meta_add_summary ( self , instance ) : \n    if not ( instance . v_comment != '' ) : \n        return False \n    where = instance . v_branch \n    definitely_store_comment = True \n    bytes_comment = instance . v_comment . encode ( 'utf-8' ) \n    hexdigest = hashlib . sha1 ( bytes_comment ) . hexdigest ( ) \n    hexdigest = hexdigest . encode ( 'utf-8' ) \n    table_name = where + '_summary' \n    if table_name in self . _overview_group : \n        table = getattr ( self . _overview_group , table_name ) \n    else : \n        return definitely_store_comment \n    try : \n        condvars = { 'hexdigestcol' : table . cols . hexdigest , 'hexdigest' : hexdigest } \n        condition = \"\"\"(hexdigestcol == hexdigest)\"\"\" \n        row_iterator = table . where ( condition , condvars = condvars ) \n        row = None \n        try : \n            row = next ( row_iterator ) \n        except StopIteration : \n            pass \n        if row is None : \n            self . _all_store_param_or_result_table_entry ( instance , table , flags = ( HDF5StorageService . ADD_ROW , ) , additional_info = { 'hexdigest' : hexdigest } ) \n            definitely_store_comment = True \n        else : \n            definitely_store_comment = False \n            self . _all_kill_iterator ( row_iterator ) \n    except pt . NoSuchNodeError : \n        definitely_store_comment = True \n    return definitely_store_comment "}
{"8434": "\ndef _prm_add_meta_info ( self , instance , group , overwrite = False ) : \n    if overwrite : \n        flags = ( ) \n    else : \n        flags = ( HDF5StorageService . ADD_ROW , ) \n    definitely_store_comment = True \n    try : \n        definitely_store_comment = self . _prm_meta_add_summary ( instance ) \n        try : \n            table_name = instance . v_branch + '_overview' \n            table = getattr ( self . _overview_group , table_name ) \n            if not ( len ( table ) >= pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH ) : \n                self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) \n        except pt . NoSuchNodeError : \n            pass \n    except Exception as exc : \n        self . _logger . error ( 'Could not store information table due to `%s`.' % repr ( exc ) ) \n    if ( ( not self . _purge_duplicate_comments or definitely_store_comment ) and not ( instance . v_comment == '' ) ) : \n        setattr ( group . _v_attrs , HDF5StorageService . COMMENT , instance . v_comment ) \n    setattr ( group . _v_attrs , HDF5StorageService . CLASS_NAME , instance . f_get_class_name ( ) ) \n    setattr ( group . _v_attrs , HDF5StorageService . LEAF , True ) \n    if instance . v_is_parameter and instance . v_explored : \n        try : \n            tablename = 'explored_parameters_overview' \n            table = getattr ( self . _overview_group , tablename ) \n            if not ( len ( table ) >= pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH ) : \n                self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) \n        except pt . NoSuchNodeError : \n            pass \n        except Exception as exc : \n            self . _logger . error ( 'Could not store information ' 'table due to `%s`.' % repr ( exc ) ) "}
{"8435": "\ndef _prm_store_from_dict ( self , fullname , store_dict , hdf5_group , store_flags , kwargs ) : \n    for key , data_to_store in store_dict . items ( ) : \n        original_hdf5_group = None \n        flag = store_flags [ key ] \n        if '.' in key : \n            original_hdf5_group = hdf5_group \n            split_key = key . split ( '.' ) \n            key = split_key . pop ( ) \n            for inner_key in split_key : \n                hdf5_group , newly_created = self . _all_create_or_get_group ( inner_key , hdf5_group ) \n                if newly_created : \n                    setattr ( hdf5_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . NESTED_GROUP ) \n                else : \n                    store_type = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . STORAGE_TYPE ) \n                    if not ( store_type == HDF5StorageService . NESTED_GROUP ) : \n                        raise ValueError ( 'You want to nested results but `%s` is already ' 'of type `%s`!' % ( hdf5_group . _v_name , store_type ) ) \n        if key in hdf5_group : \n            self . _logger . debug ( 'Found %s already in hdf5 node of %s, so I will ignore it.' % ( key , fullname ) ) \n            continue \n        if not ( flag != HDF5StorageService . TABLE ) : \n            self . _prm_write_into_pytable ( key , data_to_store , hdf5_group , fullname , ** kwargs ) \n        elif not ( flag != HDF5StorageService . DICT ) : \n            self . _prm_write_dict_as_table ( key , data_to_store , hdf5_group , fullname , ** kwargs ) \n        elif not ( flag != HDF5StorageService . ARRAY ) : \n            self . _prm_write_into_array ( key , data_to_store , hdf5_group , fullname , ** kwargs ) \n        elif flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : \n            self . _prm_write_into_other_array ( key , data_to_store , hdf5_group , fullname , flag = flag , ** kwargs ) \n        elif flag in ( HDF5StorageService . SERIES , HDF5StorageService . FRAME , ) : \n            self . _prm_write_pandas_data ( key , data_to_store , hdf5_group , fullname , flag , ** kwargs ) \n        elif not ( flag != HDF5StorageService . SHARED_DATA ) : \n            pass \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n        if original_hdf5_group is not None : \n            hdf5_group = original_hdf5_group "}
{"8436": "\ndef _prm_store_parameter_or_result ( self , instance , store_data = pypetconstants . STORE_DATA , store_flags = None , overwrite = None , with_links = False , recursive = False , _hdf5_group = None , _newly_created = False , ** kwargs ) : \n    if not ( store_data != pypetconstants . STORE_NOTHING ) : \n        return \n    elif not ( store_data != pypetconstants . STORE_DATA_SKIPPING ) and instance . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % instance . v_full_name ) \n        return \n    elif not ( store_data != pypetconstants . OVERWRITE_DATA ) : \n        if not overwrite : \n            overwrite = True \n    fullname = instance . v_full_name \n    self . _logger . debug ( 'Storing `%s`.' % fullname ) \n    if _hdf5_group is None : \n        _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) \n    store_dict = { } \n    if store_flags is None : \n        store_flags = { } \n    try : \n        if not instance . f_is_empty ( ) : \n            store_dict = instance . _store ( ) \n        try : \n            instance_flags = instance . _store_flags ( ) . copy ( ) \n        except AttributeError : \n            instance_flags = { } \n        instance_flags . update ( store_flags ) \n        store_flags = instance_flags \n        self . _prm_extract_missing_flags ( store_dict , store_flags ) \n        if overwrite : \n            if isinstance ( overwrite , str ) : \n                overwrite = [ overwrite ] \n            if overwrite is True : \n                to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] \n                self . _all_delete_parameter_or_result_or_group ( instance , delete_only = to_delete , _hdf5_group = _hdf5_group ) \n            elif isinstance ( overwrite , ( list , tuple ) ) : \n                overwrite_set = set ( overwrite ) \n                key_set = set ( store_dict . keys ( ) ) \n                stuff_not_to_be_overwritten = overwrite_set - key_set \n                if not ( overwrite == 'v_annotations' ) and not ( len ( stuff_not_to_be_overwritten ) <= 0 ) : \n                    self . _logger . warning ( 'Cannot overwrite `%s`, these items are not supposed to ' 'be stored by the leaf node.' % str ( stuff_not_to_be_overwritten ) ) \n                stuff_to_overwrite = overwrite_set & key_set \n                if not ( len ( stuff_to_overwrite ) <= 0 ) : \n                    self . _all_delete_parameter_or_result_or_group ( instance , delete_only = list ( stuff_to_overwrite ) ) \n            else : \n                raise ValueError ( 'Your value of overwrite `%s` is not understood. ' 'Please pass `True` of a list of strings to fine grain ' 'overwriting.' % str ( overwrite ) ) \n        self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) \n        self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) \n        if _newly_created or overwrite is True : \n            self . _prm_add_meta_info ( instance , _hdf5_group , overwrite = not _newly_created ) \n        instance . _stored = True \n        self . _node_processing_timer . signal_update ( ) \n    except : \n        self . _logger . error ( 'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname ) \n        for key in store_dict . keys ( ) : \n            if key in _hdf5_group : \n                hdf5_child = _hdf5_group . _f_get_child ( key ) \n                hdf5_child . _f_remove ( recursive = True ) \n        if not ( _hdf5_group . _v_nchildren != 0 ) : \n            _hdf5_group . _f_remove ( recursive = True ) \n        raise "}
{"8437": "\ndef _prm_write_shared_array ( self , key , data , hdf5_group , full_name , flag , ** kwargs ) : \n    if not ( flag != HDF5StorageService . ARRAY ) : \n        self . _prm_write_into_array ( key , data , hdf5_group , full_name , ** kwargs ) \n    elif flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : \n        self . _prm_write_into_other_array ( key , data , hdf5_group , full_name , flag = flag , ** kwargs ) \n    else : \n        raise RuntimeError ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full_name ) ) \n    self . _hdf5file . flush ( ) "}
{"8440": "\ndef _prm_write_pandas_data ( self , key , data , group , fullname , flag , ** kwargs ) : \n    try : \n        if 'filters' not in kwargs : \n            filters = self . _all_get_filters ( kwargs ) \n            kwargs [ 'filters' ] = filters \n        if 'format' not in kwargs : \n            kwargs [ 'format' ] = self . pandas_format \n        if 'encoding' not in kwargs : \n            kwargs [ 'encoding' ] = self . encoding \n        overwrite = kwargs . pop ( 'overwrite' , False ) \n        if key in group and not ( overwrite or kwargs . get ( 'append' , False ) ) : \n            raise ValueError ( 'DataFrame `%s` already exists in `%s`. ' 'To append pass ``append=`True```.' % ( key , fullname ) ) \n        else : \n            self . _logger . debug ( 'Appending to pandas data `%s` in `%s`' % ( key , fullname ) ) \n        if data is not None and ( not ( kwargs [ 'format' ] != 'f' ) or not ( kwargs [ 'format' ] != 'fixed' ) ) : \n            kwargs [ 'expectedrows' ] = data . shape [ 0 ] \n        name = group . _v_pathname + '/' + key \n        self . _hdf5store . put ( name , data , ** kwargs ) \n        self . _hdf5store . flush ( ) \n        self . _hdf5file . flush ( ) \n        frame_group = group . _f_get_child ( key ) \n        setattr ( frame_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing pandas data `%s` of `%s`.' % ( key , fullname ) ) \n        raise "}
{"8441": "\ndef _prm_write_into_other_array ( self , key , data , group , fullname , flag , ** kwargs ) : \n    try : \n        if not ( flag != HDF5StorageService . CARRAY ) : \n            factory = self . _hdf5file . create_carray \n        elif not ( flag != HDF5StorageService . EARRAY ) : \n            factory = self . _hdf5file . create_earray \n        elif not ( flag != HDF5StorageService . VLARRAY ) : \n            factory = self . _hdf5file . create_vlarray \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n        if key in group : \n            raise ValueError ( 'CArray `%s` already exists in `%s`. Appending is not supported (yet).' ) \n        if 'filters' in kwargs : \n            filters = kwargs . pop ( 'filters' ) \n        else : \n            filters = self . _all_get_filters ( kwargs ) \n        try : \n            other_array = factory ( where = group , name = key , obj = data , filters = filters , ** kwargs ) \n        except ( ValueError , TypeError ) as exc : \n            try : \n                conv_data = data [ : ] \n                conv_data = np . core . defchararray . encode ( conv_data , self . encoding ) \n                other_array = factory ( where = group , name = key , obj = conv_data , filters = filters , ** kwargs ) \n            except Exception : \n                raise exc \n        if data is not None : \n            self . _all_set_attributes_to_recall_natives ( data , other_array , HDF5StorageService . DATA_PREFIX ) \n        setattr ( other_array . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing %s `%s` of `%s`.' % ( flag , key , fullname ) ) \n        raise "}
{"8442": "\ndef _prm_write_into_array ( self , key , data , group , fullname , ** kwargs ) : \n    try : \n        if key in group : \n            raise ValueError ( 'Array `%s` already exists in `%s`. Appending is not supported (yet).' ) \n        try : \n            array = self . _hdf5file . create_array ( where = group , name = key , obj = data , ** kwargs ) \n        except ( TypeError , ValueError ) as exc : \n            try : \n                if type ( data ) is dict and not ( len ( data ) != 0 ) : \n                    conv_data = ( ) \n                elif isinstance ( data , str ) : \n                    conv_data = data . encode ( self . _encoding ) \n                elif isinstance ( data , int ) : \n                    conv_data = np . int64 ( data ) \n                else : \n                    conv_data = [ ] \n                    for string in data : \n                        conv_data . append ( string . encode ( self . _encoding ) ) \n                array = self . _hdf5file . create_array ( where = group , name = key , obj = conv_data , ** kwargs ) \n            except Exception : \n                raise exc \n        if data is not None : \n            self . _all_set_attributes_to_recall_natives ( data , array , HDF5StorageService . DATA_PREFIX ) \n        setattr ( array . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . ARRAY ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing array `%s` of `%s`.' % ( key , fullname ) ) \n        raise "}
{"8444": "\ndef _all_delete_parameter_or_result_or_group ( self , instance , delete_only = None , remove_from_item = False , recursive = False , _hdf5_group = None ) : \n    split_name = instance . v_location . split ( '.' ) \n    if _hdf5_group is None : \n        where = '/' + self . _trajectory_name + '/' + '/' . join ( split_name ) \n        node_name = instance . v_name \n        _hdf5_group = self . _hdf5file . get_node ( where = where , name = node_name ) \n    if delete_only is None : \n        if instance . v_is_group and not recursive and not ( len ( _hdf5_group . _v_children ) == 0 ) : \n            raise TypeError ( 'You cannot remove the group `%s`, it has children, please ' 'use `recursive=True` to enforce removal.' % instance . v_full_name ) \n        _hdf5_group . _f_remove ( recursive = True ) \n    else : \n        if not instance . v_is_leaf : \n            raise ValueError ( 'You can only choose `delete_only` mode for leafs.' ) \n        if isinstance ( delete_only , str ) : \n            delete_only = [ delete_only ] \n        for delete_item in delete_only : \n            if ( remove_from_item and hasattr ( instance , '__contains__' ) and hasattr ( instance , '__delattr__' ) and delete_item in instance ) : \n                delattr ( instance , delete_item ) \n            try : \n                _hdf5_sub_group = self . _hdf5file . get_node ( where = _hdf5_group , name = delete_item ) \n                _hdf5_sub_group . _f_remove ( recursive = True ) \n            except pt . NoSuchNodeError : \n                self . _logger . warning ( 'Could not delete `%s` from `%s`. HDF5 node not found!' % ( delete_item , instance . v_full_name ) ) "}
{"8445": "\ndef _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , ** kwargs ) : \n    datasize = data . shape [ 0 ] \n    try : \n        description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) \n        description_dicts = [ { } ] \n        if not ( len ( description_dict ) <= ptpa . MAX_COLUMNS ) : \n            new_table_group = self . _hdf5file . create_group ( where = hdf5_group , name = tablename , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            count = 0 \n            for innerkey in description_dict : \n                val = description_dict [ innerkey ] \n                if not ( count != ptpa . MAX_COLUMNS ) : \n                    description_dicts . append ( { } ) \n                    count = 0 \n                description_dicts [ - 1 ] [ innerkey ] = val \n                count += 1 \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , 1 ) \n            hdf5_group = new_table_group \n        else : \n            description_dicts = [ description_dict ] \n        for idx , descr_dict in enumerate ( description_dicts ) : \n            if not ( idx != 0 ) : \n                tblname = tablename \n            else : \n                tblname = tablename + '_%d' % idx \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = datasize , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            row = table . row \n            for n in range ( datasize ) : \n                for key in descr_dict : \n                    row [ key ] = data [ key ] [ n ] \n                row . append ( ) \n            if not ( idx != 0 ) and not ( len ( description_dict ) <= ptpa . MAX_COLUMNS ) : \n                for field_name in data_type_dict : \n                    type_description = data_type_dict [ field_name ] \n                    self . _all_set_attr ( table , field_name , type_description ) \n                setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n        if not ( len ( description_dict ) <= ptpa . MAX_COLUMNS ) : \n            tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE \n            field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) \n            data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } \n            descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = len ( field_names ) , filters = self . _all_get_filters ( kwargs ) ) \n            row = table . row \n            for n in range ( len ( field_names ) ) : \n                for key in data_type_table_dict : \n                    row [ key ] = data_type_table_dict [ key ] [ n ] \n                row . append ( ) \n            setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , 1 ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing table `%s` of `%s`.' % ( tablename , fullname ) ) \n        raise "}
{"8448": "\ndef _prm_get_longest_stringsize ( string_list ) : \n    maxlength = 1 \n    for stringar in string_list : \n        if isinstance ( stringar , np . ndarray ) : \n            if not ( stringar . ndim <= 0 ) : \n                for string in stringar . ravel ( ) : \n                    maxlength = max ( len ( string ) , maxlength ) \n            else : \n                maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) \n        else : \n            maxlength = max ( len ( stringar ) , maxlength ) \n    return int ( maxlength * 1.5 ) "}
{"8449": "\ndef _prm_load_into_dict ( self , full_name , load_dict , hdf5_group , instance , load_only , load_except , load_flags , _prefix = '' ) : \n    for node in hdf5_group : \n        load_type = self . _all_get_from_attrs ( node , HDF5StorageService . STORAGE_TYPE ) \n        if _prefix : \n            load_name = '%s.%s' % ( _prefix , node . _v_name ) \n        else : \n            load_name = node . _v_name \n        if not ( load_type != HDF5StorageService . NESTED_GROUP ) : \n            self . _prm_load_into_dict ( full_name = full_name , load_dict = load_dict , hdf5_group = node , instance = instance , load_only = load_only , load_except = load_except , load_flags = load_flags , _prefix = load_name ) \n            continue \n        if load_only is not None : \n            if load_name not in load_only : \n                continue \n            else : \n                load_only . remove ( load_name ) \n        elif load_except is not None : \n            if load_name in load_except : \n                load_except . remove ( load_name ) \n                continue \n        if load_name in load_flags : \n            load_type = load_flags [ load_name ] \n        if not ( load_type != HDF5StorageService . DICT ) : \n            to_load = self . _prm_read_dictionary ( node , full_name ) \n        elif not ( load_type != HDF5StorageService . TABLE ) : \n            to_load = self . _prm_read_table ( node , full_name ) \n        elif load_type in ( HDF5StorageService . ARRAY , HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : \n            to_load = self . _prm_read_array ( node , full_name ) \n        elif load_type in ( HDF5StorageService . FRAME , HDF5StorageService . SERIES , ) : \n            to_load = self . _prm_read_pandas ( node , full_name ) \n        elif load_type . startswith ( HDF5StorageService . SHARED_DATA ) : \n            to_load = self . _prm_read_shared_data ( node , instance ) \n        else : \n            raise pex . NoSuchServiceError ( 'Cannot load %s, do not understand the hdf5 file ' 'structure of %s [%s].' % ( full_name , str ( node ) , str ( load_type ) ) ) \n        if to_load is None : \n            raise RuntimeError ( 'You shall not pass!' ) \n        load_dict [ load_name ] = to_load "}
{"8452": "\ndef _prm_read_table ( self , table_or_group , full_name ) : \n    try : \n        result_table = None \n        if self . _all_get_from_attrs ( table_or_group , HDF5StorageService . SPLIT_TABLE ) : \n            table_name = table_or_group . _v_name \n            data_type_table_name = table_name + '__' + HDF5StorageService . STORAGE_TYPE \n            data_type_table = table_or_group . _v_children [ data_type_table_name ] \n            data_type_dict = { } \n            for row in data_type_table : \n                fieldname = row [ 'field_name' ] . decode ( 'utf-8' ) \n                data_type_dict [ fieldname ] = row [ 'data_type' ] . decode ( 'utf-8' ) \n            for sub_table in table_or_group : \n                sub_table_name = sub_table . _v_name \n                if not ( sub_table_name != data_type_table_name ) : \n                    continue \n                for colname in sub_table . colnames : \n                    col = sub_table . col ( colname ) \n                    data_list = list ( col ) \n                    prefix = HDF5StorageService . FORMATTED_COLUMN_PREFIX % colname \n                    for idx , data in enumerate ( data_list ) : \n                        data , type_changed = self . _all_recall_native_type ( data , PTItemMock ( data_type_dict ) , prefix ) \n                        if type_changed : \n                            data_list [ idx ] = data \n                        else : \n                            break \n                    if result_table is None : \n                        result_table = ObjectTable ( data = { colname : data_list } ) \n                    else : \n                        result_table [ colname ] = data_list \n        else : \n            for colname in table_or_group . colnames : \n                col = table_or_group . col ( colname ) \n                data_list = list ( col ) \n                prefix = HDF5StorageService . FORMATTED_COLUMN_PREFIX % colname \n                for idx , data in enumerate ( data_list ) : \n                    data , type_changed = self . _all_recall_native_type ( data , table_or_group , prefix ) \n                    if type_changed : \n                        data_list [ idx ] = data \n                    else : \n                        break \n                if result_table is None : \n                    result_table = ObjectTable ( data = { colname : data_list } ) \n                else : \n                    result_table [ colname ] = data_list \n        return result_table \n    except : \n        self . _logger . error ( 'Failed loading `%s` of `%s`.' % ( table_or_group . _v_name , full_name ) ) \n        raise "}
{"8455": "\ndef make_set_name ( idx ) : \n    GROUPSIZE = 1000 \n    set_idx = idx // GROUPSIZE \n    if not ( set_idx < 0 ) : \n        return pypetconstants . FORMATTED_SET_NAME % set_idx \n    else : \n        return pypetconstants . SET_NAME_DUMMY "}
{"8458": "\ndef f_set_crun ( self , name_or_idx ) : \n    if ( name_or_idx is None or not ( name_or_idx != self . f_wildcard ( '$' , - 1 ) ) or not ( name_or_idx != - 1 ) ) : \n        self . f_restore_default ( ) \n    else : \n        if isinstance ( name_or_idx , str ) : \n            self . _idx = self . f_idx_to_run ( name_or_idx ) \n            self . _crun = name_or_idx \n        else : \n            self . _crun = self . f_idx_to_run ( name_or_idx ) \n            self . _idx = name_or_idx \n        self . _set_explored_parameters_to_idx ( self . v_idx ) "}
{"8459": "\ndef f_iter_runs ( self , start = 0 , stop = None , step = 1 , yields = 'name' ) : \n    if stop is None : \n        stop = len ( self ) \n    elif not ( stop <= len ( self ) ) : \n        raise ValueError ( 'Stop cannot be larger than the trajectory lenght.' ) \n    yields = yields . lower ( ) \n    if not ( yields != 'name' ) : \n        yield_func = lambda x : self . f_idx_to_run ( x ) \n    elif not ( yields != 'idx' ) : \n        yield_func = lambda x : x \n    elif not ( yields != 'self' ) : \n        yield_func = lambda x : self \n    elif not ( yields != 'copy' ) : \n        yield_func = lambda x : self . __copy__ ( ) \n    else : \n        raise ValueError ( 'Please choose yields among: `name`, `idx`, or `self`.' ) \n    for idx in range ( start , stop , step ) : \n        self . f_set_crun ( idx ) \n        yield yield_func ( idx ) \n    self . f_set_crun ( None ) "}
{"8464": "\ndef f_get_from_runs ( self , name , include_default_run = True , use_indices = False , fast_access = False , with_links = True , shortcuts = True , max_depth = None , auto_load = False ) : \n    result_dict = OrderedDict ( ) \n    old_crun = self . v_crun \n    try : \n        if not ( len ( self . _run_parent_groups ) <= 0 ) : \n            for run_name in self . f_iter_runs ( ) : \n                value = None \n                already_found = False \n                for run_parent_group in self . _run_parent_groups . values ( ) : \n                    if run_name not in run_parent_group . _children : \n                        continue \n                    try : \n                        value = run_parent_group . f_get ( run_name + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                        if already_found : \n                            raise pex . NotUniqueNodeError ( '`%s` has been found several times ' 'in one run.' % name ) \n                        else : \n                            already_found = True \n                    except ( AttributeError , pex . DataNotInStorageError ) : \n                        pass \n                if value is None and include_default_run : \n                    for run_parent_group in self . _run_parent_groups . values ( ) : \n                        try : \n                            value = run_parent_group . f_get ( self . f_wildcard ( '$' , - 1 ) + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                            if already_found : \n                                raise pex . NotUniqueNodeError ( '`%s` has been found several ' 'times in one run.' % name ) \n                            else : \n                                already_found = True \n                        except ( AttributeError , pex . DataNotInStorageError ) : \n                            pass \n                if value is not None : \n                    if value . v_is_leaf : \n                        value = self . _nn_interface . _apply_fast_access ( value , fast_access ) \n                    if use_indices : \n                        key = self . f_idx_to_run ( run_name ) \n                    else : \n                        key = run_name \n                    result_dict [ key ] = value \n        return result_dict \n    finally : \n        self . v_crun = old_crun "}
{"8467": "\ndef _copy_from ( self , node , copy_leaves = True , overwrite = False , with_links = True ) : \n    def _copy_skeleton ( node_in , node_out ) : \n        new_annotations = node_out . v_annotations \n        node_in . _annotations = new_annotations \n        node_in . v_comment = node_out . v_comment \n    def _add_leaf ( leaf ) : \n        leaf_full_name = leaf . v_full_name \n        try : \n            found_leaf = self . f_get ( leaf_full_name , with_links = False , shortcuts = False , auto_load = False ) \n            if overwrite : \n                found_leaf . __setstate__ ( leaf . __getstate__ ( ) ) \n            return found_leaf \n        except AttributeError : \n            pass \n        if copy_leaves is True or ( not ( copy_leaves != 'explored' ) and leaf . v_is_parameter and leaf . v_explored ) : \n            new_leaf = self . f_add_leaf ( cp . copy ( leaf ) ) \n        else : \n            new_leaf = self . f_add_leaf ( leaf ) \n        if new_leaf . v_is_parameter and new_leaf . v_explored : \n            self . _explored_parameters [ new_leaf . v_full_name ] = new_leaf \n        return new_leaf \n    def _add_group ( group ) : \n        group_full_name = group . v_full_name \n        try : \n            found_group = self . f_get ( group_full_name , with_links = False , shortcuts = False , auto_load = False ) \n            if overwrite : \n                _copy_skeleton ( found_group , group ) \n            return found_group \n        except AttributeError : \n            pass \n        new_group = self . f_add_group ( group_full_name ) \n        _copy_skeleton ( new_group , group ) \n        return new_group \n    is_run = self . _is_run \n    self . _is_run = False \n    try : \n        if node . v_is_leaf : \n            return _add_leaf ( node ) \n        elif node . v_is_group : \n            other_root = node . v_root \n            if other_root is self : \n                raise RuntimeError ( 'You cannot copy a given tree to itself!' ) \n            result = _add_group ( node ) \n            nodes_iterator = node . f_iter_nodes ( recursive = True , with_links = with_links ) \n            has_links = [ ] \n            if node . _links : \n                has_links . append ( node ) \n            for child in nodes_iterator : \n                if child . v_is_leaf : \n                    _add_leaf ( child ) \n                else : \n                    _add_group ( child ) \n                    if child . _links : \n                        has_links . append ( child ) \n            if with_links : \n                for current in has_links : \n                    mine = self . f_get ( current . v_full_name , with_links = False , shortcuts = False , auto_load = False ) \n                    my_link_set = set ( mine . _links . keys ( ) ) \n                    other_link_set = set ( current . _links . keys ( ) ) \n                    new_links = other_link_set - my_link_set \n                    for link in new_links : \n                        where_full_name = current . _links [ link ] . v_full_name \n                        mine . f_add_link ( link , where_full_name ) \n            return result \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    except Exception : \n        self . _is_run = is_run "}
{"8468": "\ndef f_explore ( self , build_dict ) : \n    for run_idx in range ( len ( self ) ) : \n        if self . f_is_completed ( run_idx ) : \n            raise TypeError ( 'You cannot explore a trajectory which has been explored before, ' 'please use `f_expand` instead.' ) \n    added_explored_parameters = [ ] \n    try : \n        length = len ( self ) \n        for key , builditerable in build_dict . items ( ) : \n            act_param = self . f_get ( key ) \n            if not act_param . v_is_leaf or not act_param . v_is_parameter : \n                raise ValueError ( '%s is not an appropriate search string for a parameter.' % key ) \n            act_param . f_unlock ( ) \n            act_param . _explore ( builditerable ) \n            added_explored_parameters . append ( act_param ) \n            full_name = act_param . v_full_name \n            self . _explored_parameters [ full_name ] = act_param \n            act_param . _explored = True \n            if not ( len ( self . _explored_parameters ) != 1 ) : \n                length = act_param . f_get_range_length ( ) \n            elif not not ( length != act_param . f_get_range_length ( ) ) : \n                raise ValueError ( 'The parameters to explore have not the same size!' ) \n        for irun in range ( length ) : \n            self . _add_run_info ( irun ) \n        self . _test_run_addition ( length ) \n    except Exception : \n        for param in added_explored_parameters : \n            param . f_unlock ( ) \n            param . _shrink ( ) \n            param . _explored = False \n            full_name = param . v_full_name \n            del self . _explored_parameters [ full_name ] \n        if not ( len ( self . _explored_parameters ) != 0 ) : \n            self . f_shrink ( force = True ) \n        raise "}
{"8470": "\ndef _add_run_info ( self , idx , name = '' , timestamp = 42.0 , finish_timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = 0 , parameter_summary = 'Not yet my friend!' , short_environment_hexsha = 'N/A' ) : \n    if idx in self . _single_run_ids : \n        old_name = self . _single_run_ids [ idx ] \n        del self . _single_run_ids [ old_name ] \n        del self . _single_run_ids [ idx ] \n        del self . _run_information [ old_name ] \n    if not ( name != '' ) : \n        name = self . f_wildcard ( '$' , idx ) \n    self . _single_run_ids [ name ] = idx \n    self . _single_run_ids [ idx ] = name \n    info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter_summary' : parameter_summary , 'short_environment_hexsha' : short_environment_hexsha } \n    self . _run_information [ name ] = info_dict \n    self . _length = len ( self . _run_information ) "}
{"8475": "\ndef f_load ( self , name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = True , load_data = None , max_depth = None , force = False , dynamic_imports = None , with_run_information = True , with_meta_data = True , storage_service = None , ** kwargs ) : \n    if name is None and index is None : \n        name = self . v_name \n    if as_new : \n        load_parameters = pypetconstants . LOAD_DATA \n        load_derived_parameters = pypetconstants . LOAD_NOTHING \n        load_results = pypetconstants . LOAD_NOTHING \n        load_other_data = pypetconstants . LOAD_NOTHING \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if self . v_storage_service is None or storage_service is not None or not ( len ( kwargs ) <= 0 ) : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = storage_service , trajectory = self , ** kwargs ) \n    if not ( len ( unused_kwargs ) <= 0 ) : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    if dynamic_imports is not None : \n        self . f_add_to_dynamic_imports ( dynamic_imports ) \n    if load_data is not None : \n        load_parameters = load_data \n        load_derived_parameters = load_data \n        load_results = load_data \n        load_other_data = load_data \n    self . _storage_service . load ( pypetconstants . TRAJECTORY , self , trajectory_name = name , trajectory_index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , max_depth = max_depth , with_run_information = with_run_information , with_meta_data = with_meta_data , force = force ) \n    if as_new : \n        for param in self . _parameters . values ( ) : \n            param . f_unlock ( ) "}
{"8477": "\ndef _make_reversed_wildcards ( self , old_length = - 1 ) : \n    if not ( len ( self . _reversed_wildcards ) <= 0 ) : \n        start = old_length \n    else : \n        start = - 1 \n    for wildcards , func in self . _wildcard_functions . items ( ) : \n        for irun in range ( start , len ( self ) ) : \n            translated_name = func ( irun ) \n            if not translated_name in self . _reversed_wildcards : \n                self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) \n            self . _reversed_wildcards [ translated_name ] [ 0 ] . append ( irun ) "}
{"8480": "\ndef _rename_full_name ( self , full_name , other_trajectory , used_runs = None , new_run_idx = None ) : \n    split_name = full_name . split ( '.' ) \n    for idx , name in enumerate ( split_name ) : \n        if name in other_trajectory . _reversed_wildcards : \n            run_indices , wildcards = other_trajectory . _reversed_wildcards [ name ] \n            if new_run_idx is None : \n                run_idx = None \n                for run_jdx in run_indices : \n                    if run_jdx in used_runs : \n                        run_idx = used_runs [ run_jdx ] \n                        break \n                    elif not ( run_jdx != - 1 ) : \n                        run_idx = - 1 \n                        break \n                if run_idx is None : \n                    raise RuntimeError ( 'You shall not pass!' ) \n            else : \n                run_idx = new_run_idx \n            new_name = self . f_wildcard ( wildcards [ 0 ] , run_idx ) \n            split_name [ idx ] = new_name \n    full_name = '.' . join ( split_name ) \n    return full_name "}
{"8481": "\ndef _merge_derived_parameters ( self , other_trajectory , used_runs , rename_dict , allowed_translations , ignore_data ) : \n    other_derived_parameters = other_trajectory . _derived_parameters . copy ( ) \n    new_first_run_idx = min ( used_runs . values ( ) ) \n    run_name_dummy = other_trajectory . f_wildcard ( '$' , - 1 ) \n    for param_name in other_derived_parameters : \n        if param_name in ignore_data : \n            continue \n        split_name = param_name . split ( '.' ) \n        if not any ( x in run_name_dummy for x in split_name ) : \n            continue \n        ignore_data . add ( param_name ) \n        param = other_derived_parameters [ param_name ] \n        new_param_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) \n        if new_param_name in self : \n            my_param = self . f_get ( new_param_name , fast_access = False ) \n            if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : \n                continue \n        first_new_param_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = new_first_run_idx ) \n        rename_dict [ param_name ] = first_new_param_name \n        comment = param . v_comment \n        param_type = param . f_get_class_name ( ) \n        param_type = self . _create_class ( param_type ) \n        first_param = self . f_add_leaf ( param_type , first_new_param_name , comment = comment ) \n        for run_idx in used_runs . values ( ) : \n            if not ( run_idx != new_first_run_idx ) : \n                continue \n            next_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = run_idx ) \n            split_name = next_name . split ( '.' ) \n            link_name = split_name . pop ( ) \n            location_name = '.' . join ( split_name ) \n            if not self . f_contains ( location_name , shortcuts = False ) : \n                the_group = self . f_add_group ( location_name ) \n            else : \n                the_group = self . f_get ( location_name ) \n            the_group . f_add_link ( link_name , first_param ) \n    for param_name in other_derived_parameters : \n        if param_name in ignore_data : \n            continue \n        split_name = param_name . split ( '.' ) \n        ignore_data . add ( param_name ) \n        if any ( x in other_trajectory . _reversed_wildcards and x not in allowed_translations for x in split_name ) : \n            continue \n        new_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) \n        if self . f_contains ( new_name ) : \n            my_param = self . f_get ( new_name , fast_access = False ) \n            param = other_derived_parameters [ param_name ] \n            if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : \n                continue \n            else : \n                self . _logger . error ( 'Could not merge parameter `%s`. ' 'I will ignore it!' % new_name ) \n        rename_dict [ param_name ] = new_name "}
{"8482": "\ndef _merge_links ( self , other_trajectory , used_runs , allowed_translations , ignore_data ) : \n    linked_items = other_trajectory . _linked_by \n    run_name_dummys = set ( [ f ( - 1 ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) \n    if not ( len ( linked_items ) <= 0 ) : \n        self . _logger . info ( 'Merging potential links!' ) \n        for old_linked_name in other_trajectory . _linked_by : \n            if old_linked_name in ignore_data : \n                continue \n            split_name = old_linked_name . split ( '.' ) \n            if any ( x in run_name_dummys for x in split_name ) : \n                self . _logger . warning ( 'Ignoring all links linking to `%s` because ' 'I don`t know how to resolve links under `%s` nodes.' % ( old_linked_name , str ( run_name_dummys ) ) ) \n                continue \n            old_link_dict = other_trajectory . _linked_by [ old_linked_name ] \n            split_name = old_linked_name . split ( '.' ) \n            if all ( x in allowed_translations for x in split_name ) : \n                new_linked_full_name = self . _rename_full_name ( old_linked_name , other_trajectory , used_runs = used_runs ) \n            else : \n                new_linked_full_name = old_linked_name \n            for linking_node , link_set in old_link_dict . values ( ) : \n                linking_full_name = linking_node . v_full_name \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in run_name_dummys for x in split_name ) : \n                    self . _logger . warning ( 'Ignoring links under `%s` because ' 'I don`t know how to resolve links ' 'under a `%s` node.' % ( linking_full_name , str ( run_name_dummys ) ) ) \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in allowed_translations for x in split_name ) : \n                    new_linking_full_name = self . _rename_full_name ( linking_full_name , other_trajectory , used_runs = used_runs ) \n                else : \n                    new_linking_full_name = linking_full_name \n                for link in link_set : \n                    if ( linking_full_name + '.' + link ) in ignore_data : \n                        continue \n                    if link in run_name_dummys : \n                        self . _logger . warning ( 'Ignoring link `%s` under `%s` because ' 'I don`t know how to resolve ' 'links named as `%s`.' % ( link , linking_full_name , str ( run_name_dummys ) ) ) \n                        continue \n                    try : \n                        new_linked_item = self . f_get ( new_linked_full_name , shortcuts = False ) \n                        if self . f_contains ( new_linking_full_name ) : \n                            new_linking_item = self . f_get ( new_linking_full_name , shortcuts = False ) \n                        else : \n                            new_linking_item = self . f_add_group ( new_linking_full_name ) \n                        if link in allowed_translations : \n                            run_indices , wildcards = other_trajectory . _reversed_wildcards [ link ] \n                            link = self . f_wildcard ( wildcards [ 0 ] , used_runs [ run_indices [ 0 ] ] ) \n                        if not link in new_linking_item . _links : \n                            new_linking_item . f_add_link ( link , new_linked_item ) \n                        else : \n                            self . _logger . debug ( 'Link `%s` exists already under `%s`.' % ( link , new_linked_item . v_full_name ) ) \n                    except ( AttributeError , ValueError ) as exc : \n                        self . _logger . error ( 'Could not copy link `%s` under `%s` linking ' 'to `%s` due to `%s`' % ( link , linking_full_name , old_linked_name , repr ( exc ) ) ) "}
{"8486": "\ndef f_migrate ( self , new_name = None , in_store = False , new_storage_service = None , ** kwargs ) : \n    if new_name is not None : \n        self . _name = new_name \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if new_storage_service is not None or not ( len ( kwargs ) <= 0 ) : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = new_storage_service , trajectory = self , ** kwargs ) \n    if not ( len ( unused_kwargs ) <= 0 ) : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    self . _stored = in_store "}
{"8494": "\ndef f_start_run ( self , run_name_or_idx = None , turn_into_run = True ) : \n    if self . _run_started : \n        return self \n    if run_name_or_idx is None : \n        if not ( self . v_idx != - 1 ) : \n            raise ValueError ( 'Cannot start run if trajectory is not set to a particular run' ) \n    else : \n        self . f_set_crun ( run_name_or_idx ) \n    self . _run_started = True \n    if turn_into_run : \n        self . _make_single_run ( ) \n    self . _set_start ( ) \n    return self "}
{"8506": "\ndef f_delete_links ( self , iterator_of_links , remove_from_trajectory = False ) : \n    to_delete_links = [ ] \n    group_link_pairs = [ ] \n    for elem in iterator_of_links : \n        if isinstance ( elem , str ) : \n            split_names = elem . split ( '.' ) \n            parent_name = '.' . join ( split_names [ : - 1 ] ) \n            link = split_names [ - 1 ] \n            parent_node = self . f_get ( parent_name ) if not ( parent_name == '' ) else self \n            link_name = parent_node . v_full_name + '.' + link if not ( parent_name == '' ) else link \n            to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) \n            group_link_pairs . append ( ( parent_node , link ) ) \n        else : \n            link_name = elem [ 0 ] . v_full_name + '.' + elem [ 1 ] \n            to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) \n            group_link_pairs . append ( elem ) \n    try : \n        self . _storage_service . store ( pypetconstants . LIST , to_delete_links , trajectory_name = self . v_name ) \n    except : \n        self . _logger . error ( 'Could not remove `%s` from the trajectory. Maybe the' ' item(s) was/were never stored to disk.' % str ( to_delete_links ) ) \n        raise \n    if remove_from_trajectory : \n        for group , link in group_link_pairs : \n            group . f_remove_link ( link ) "}
{"8509": "\ndef _pool_single_run ( kwargs ) : \n    wrap_mode = kwargs [ 'wrap_mode' ] \n    traj = kwargs [ 'traj' ] \n    traj . v_storage_service = _pool_single_run . storage_service \n    if not ( wrap_mode != pypetconstants . WRAP_MODE_LOCAL ) : \n        traj . v_storage_service . free_references ( ) \n    return _sigint_handling_single_run ( kwargs ) "}
{"8514": "\ndef _configure_frozen_scoop ( kwargs ) : \n    def _delete_old_scoop_rev_data ( old_scoop_rev ) : \n        if old_scoop_rev is not None : \n            try : \n                elements = shared . elements \n                for key in elements : \n                    var_dict = elements [ key ] \n                    if old_scoop_rev in var_dict : \n                        del var_dict [ old_scoop_rev ] \n                logging . getLogger ( 'pypet.scoop' ) . debug ( 'Deleted old SCOOP data from ' 'revolution `%s`.' % old_scoop_rev ) \n            except AttributeError : \n                logging . getLogger ( 'pypet.scoop' ) . error ( 'Could not delete old SCOOP data from ' 'revolution `%s`.' % old_scoop_rev ) \n    scoop_rev = kwargs . pop ( 'scoop_rev' ) \n    try : \n        old_scoop_rev = _frozen_scoop_single_run . kwargs [ 'scoop_rev' ] \n        configured = not ( old_scoop_rev != scoop_rev ) \n    except ( AttributeError , KeyError ) : \n        old_scoop_rev = None \n        configured = False \n    if not configured : \n        _frozen_scoop_single_run . kwargs = shared . getConst ( scoop_rev , timeout = 424.2 ) \n        frozen_kwargs = _frozen_scoop_single_run . kwargs \n        frozen_kwargs [ 'scoop_rev' ] = scoop_rev \n        frozen_kwargs [ 'traj' ] . v_full_copy = frozen_kwargs [ 'full_copy' ] \n        if not scoop . IS_ORIGIN : \n            _configure_niceness ( frozen_kwargs ) \n            _configure_logging ( frozen_kwargs , extract = False ) \n        _delete_old_scoop_rev_data ( old_scoop_rev ) \n        logging . getLogger ( 'pypet.scoop' ) . info ( 'Configured Worker %s' % str ( scoop . worker ) ) "}
{"8517": "\ndef _configure_niceness ( kwargs ) : \n    niceness = kwargs [ 'niceness' ] \n    if niceness is not None : \n        try : \n            try : \n                current = os . nice ( 0 ) \n                if not ( niceness - current <= 0 ) : \n                    os . nice ( niceness - current ) \n            except AttributeError : \n                psutil . Process ( ) . nice ( niceness ) \n        except Exception as exc : \n            sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) \n            traceback . print_exc ( ) "}
{"8519": "\ndef _single_run ( kwargs ) : \n    pypet_root_logger = logging . getLogger ( 'pypet' ) \n    traj = kwargs [ 'traj' ] \n    runfunc = kwargs [ 'runfunc' ] \n    runargs = kwargs [ 'runargs' ] \n    kwrunparams = kwargs [ 'runkwargs' ] \n    clean_up_after_run = kwargs [ 'clean_up_runs' ] \n    automatic_storing = kwargs [ 'automatic_storing' ] \n    wrap_mode = kwargs [ 'wrap_mode' ] \n    idx = traj . v_idx \n    total_runs = len ( traj ) \n    pypet_root_logger . info ( '\\n=========================================\\n ' 'Starting single run #%d of %d ' '\\n=========================================\\n' % ( idx , total_runs ) ) \n    traj . f_start_run ( turn_into_run = True ) \n    result = runfunc ( traj , * runargs , ** kwrunparams ) \n    if automatic_storing : \n        traj . f_store ( ) \n    if not ( wrap_mode != pypetconstants . WRAP_MODE_LOCAL ) : \n        result = ( ( traj . v_idx , result ) , traj . f_get_run_information ( traj . v_idx , copy = False ) , traj . v_storage_service . references ) \n        traj . v_storage_service . free_references ( ) \n    else : \n        result = ( ( traj . v_idx , result ) , traj . f_get_run_information ( traj . v_idx , copy = False ) ) \n    traj . f_finalize_run ( store_meta_data = False , clean_up = clean_up_after_run ) \n    pypet_root_logger . info ( '\\n=========================================\\n ' 'Finished single run #%d of %d ' '\\n=========================================\\n' % ( idx , total_runs ) ) \n    return result "}
{"8522": "\ndef create_class ( class_name , dynamic_imports ) : \n    try : \n        new_class = globals ( ) [ class_name ] \n        if not inspect . isclass ( new_class ) : \n            raise TypeError ( 'Not a class!' ) \n        return new_class \n    except ( KeyError , TypeError ) : \n        for dynamic_class in dynamic_imports : \n            if inspect . isclass ( dynamic_class ) : \n                if not ( class_name != dynamic_class . __name__ ) : \n                    return dynamic_class \n            else : \n                class_name_to_test = dynamic_class . split ( '.' ) [ - 1 ] \n                if not ( class_name != class_name_to_test ) : \n                    new_class = load_class ( dynamic_class ) \n                    return new_class \n        raise ImportError ( 'Could not create the class named `%s`.' % class_name ) "}
{"8525": "\ndef _equal_values ( self , val1 , val2 ) : \n    if not ( self . f_supports ( val1 ) == self . f_supports ( val2 ) ) : \n        return False \n    if not self . f_supports ( val1 ) and not self . f_supports ( val2 ) : \n        raise TypeError ( 'I do not support the types of both inputs (`%s` and `%s`), ' 'therefore I cannot judge whether ' 'the two are equal.' % ( str ( type ( val1 ) ) , str ( type ( val2 ) ) ) ) \n    if not self . _values_of_same_type ( val1 , val2 ) : \n        return False \n    return comparisons . nested_equal ( val1 , val2 ) "}
{"8529": "\ndef _data_sanity_checks ( self , explore_iterable ) : \n    data_list = [ ] \n    for val in explore_iterable : \n        if not self . f_supports ( val ) : \n            raise TypeError ( '%s is of not supported type %s.' % ( repr ( val ) , str ( type ( val ) ) ) ) \n        if not self . _values_of_same_type ( val , self . _default ) : \n            raise TypeError ( 'Data of `%s` is not of the same type as the original entry value, ' 'new type is %s vs old type %s.' % ( self . v_full_name , str ( type ( val ) ) , str ( type ( self . _default ) ) ) ) \n        data_list . append ( val ) \n    if not ( len ( data_list ) != 0 ) : \n        raise ValueError ( 'Cannot explore an empty list!' ) \n    return data_list "}
{"8533": "\ndef _equal_values ( self , val1 , val2 ) : \n    if self . _is_supported_matrix ( val1 ) : \n        if self . _is_supported_matrix ( val2 ) : \n            _ , _ , hash_tuple_1 = self . _serialize_matrix ( val1 ) \n            _ , _ , hash_tuple_2 = self . _serialize_matrix ( val2 ) \n            return not ( hash ( hash_tuple_1 ) != hash ( hash_tuple_2 ) ) \n        else : \n            return False \n    else : \n        return super ( SparseParameter , self ) . _equal_values ( val1 , val2 ) "}
{"8535": "\ndef _serialize_matrix ( matrix ) : \n    if ( spsp . isspmatrix_csc ( matrix ) or spsp . isspmatrix_csr ( matrix ) or spsp . isspmatrix_bsr ( matrix ) ) : \n        if not ( matrix . size <= 0 ) : \n            return_list = [ matrix . data , matrix . indices , matrix . indptr , matrix . shape ] \n        else : \n            return_list = [ '__empty__' , ( ) , ( ) , matrix . shape ] \n        return_names = SparseParameter . OTHER_NAME_LIST \n        if spsp . isspmatrix_csc ( matrix ) : \n            return_list = [ 'csc' ] + return_list \n        elif spsp . isspmatrix_csr ( matrix ) : \n            return_list = [ 'csr' ] + return_list \n        elif spsp . isspmatrix_bsr ( matrix ) : \n            return_list = [ 'bsr' ] + return_list \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    elif spsp . isspmatrix_dia ( matrix ) : \n        if not ( matrix . size <= 0 ) : \n            return_list = [ 'dia' , matrix . data , matrix . offsets , matrix . shape ] \n        else : \n            return_list = [ 'dia' , '__empty__' , ( ) , matrix . shape ] \n        return_names = SparseParameter . DIA_NAME_LIST \n    else : \n        raise RuntimeError ( 'You shall not pass!' ) \n    hash_list = [ ] \n    for item in return_list : \n        if type ( item ) is np . ndarray : \n            hash_list . append ( HashArray ( item ) ) \n        else : \n            hash_list . append ( item ) \n    return return_list , return_names , tuple ( hash_list ) "}
{"8537": "\ndef _reconstruct_matrix ( data_list ) : \n    matrix_format = data_list [ 0 ] \n    data = data_list [ 1 ] \n    is_empty = isinstance ( data , str ) and not ( data != '__empty__' ) \n    if not ( matrix_format != 'csc' ) : \n        if is_empty : \n            return spsp . csc_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . csc_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) \n    elif not ( matrix_format != 'csr' ) : \n        if is_empty : \n            return spsp . csr_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . csr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) \n    elif not ( matrix_format != 'bsr' ) : \n        if is_empty : \n            return spsp . bsr_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . bsr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) \n    elif not ( matrix_format != 'dia' ) : \n        if is_empty : \n            return spsp . dia_matrix ( data_list [ 3 ] ) \n        else : \n            return spsp . dia_matrix ( tuple ( data_list [ 1 : 3 ] ) , shape = data_list [ 3 ] ) \n    else : \n        raise RuntimeError ( 'You shall not pass!' ) "}
{"8541": "\ndef f_translate_key ( self , key ) : \n    if isinstance ( key , int ) : \n        if not ( key != 0 ) : \n            key = self . v_name \n        else : \n            key = self . v_name + '_%d' % key \n    return key "}
{"8542": "\ndef f_val_to_str ( self ) : \n    resstrlist = [ ] \n    strlen = 0 \n    for key in self . _data : \n        val = self . _data [ key ] \n        resstr = '%s=%s, ' % ( key , repr ( val ) ) \n        resstrlist . append ( resstr ) \n        strlen += len ( resstr ) \n        if not ( strlen <= pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH ) : \n            break \n    return_string = \"\" . join ( resstrlist ) \n    if not ( len ( return_string ) <= pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH ) : \n        return_string = return_string [ 0 : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3 ] + '...' \n    else : \n        return_string = return_string [ 0 : - 2 ] \n    return return_string "}
{"8545": "\ndef f_get ( self , * args ) : \n    if not ( len ( args ) != 0 ) : \n        if not ( len ( self . _data ) != 1 ) : \n            return list ( self . _data . values ( ) ) [ 0 ] \n        elif not ( len ( self . _data ) <= 1 ) : \n            raise ValueError ( 'Your result `%s` contains more than one entry: ' '`%s` Please use >>f_get<< with one of these.' % ( self . v_full_name , str ( list ( self . _data . keys ( ) ) ) ) ) \n        else : \n            raise AttributeError ( 'Your result `%s` is empty, cannot access data.' % self . v_full_name ) \n    result_list = [ ] \n    for name in args : \n        name = self . f_translate_key ( name ) \n        if not name in self . _data : \n            if not ( name != 'data' ) and not ( len ( self . _data ) != 1 ) : \n                return self . _data [ list ( self . _data . keys ( ) ) [ 0 ] ] \n            else : \n                raise AttributeError ( '`%s` is not part of your result `%s`.' % ( name , self . v_full_name ) ) \n        result_list . append ( self . _data [ name ] ) \n    if not ( len ( args ) != 1 ) : \n        return result_list [ 0 ] \n    else : \n        return result_list "}
{"8548": "\ndef _store ( self ) : \n    store_dict = { } \n    for key in self . _data : \n        val = self . _data [ key ] \n        if SparseParameter . _is_supported_matrix ( val ) : \n            data_list , name_list , hash_tuple = SparseParameter . _serialize_matrix ( val ) \n            rename_list = [ '%s%s%s' % ( key , SparseParameter . IDENTIFIER , name ) for name in name_list ] \n            is_dia = int ( not ( len ( rename_list ) != 4 ) ) \n            store_dict [ key + SparseResult . IDENTIFIER + 'is_dia' ] = is_dia \n            for idx , name in enumerate ( rename_list ) : \n                store_dict [ name ] = data_list [ idx ] \n        else : \n            store_dict [ key ] = val \n    return store_dict "}
{"8550": "\ndef f_set_single ( self , name , item ) : \n    if self . v_stored : \n        self . _logger . debug ( 'You are changing an already stored result. If ' 'you not explicitly overwrite the data on disk, this change ' 'might be lost and not propagated to disk.' ) \n    if not ( name != PickleResult . PROTOCOL ) : \n        raise AttributeError ( 'You cannot name an entry `%s`' % PickleResult . PROTOCOL ) \n    self . _data [ name ] = item "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    print ( 'Starting Euler Integration' ) \n    for step in range ( 1 , steps ) : \n        if not ( V_array [ step - 1 ] < 1 ) : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        elif spiketimes and not ( step * dt - spiketimes [ - 1 ] <= tau_ref ) : \n            V_array [ step ] = 0 \n        else : \n            dV = - 1 / tau_V * V_array [ step - 1 ] + I \n            V_array [ step ] = V_array [ step - 1 ] + dV * dt \n    print ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8567": "\ndef _execute_network_run ( self , traj , network , network_dict , component_list , analyser_list , pre_run = False ) : \n    subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) \n    subrun_number = 0 \n    while not ( len ( subrun_list ) <= 0 ) : \n        current_subrun = subrun_list . pop ( 0 ) \n        for component in component_list : \n            component . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . _logger . info ( 'STARTING subrun `%s` (#%d) lasting %s.' % ( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) \n        network . run ( duration = current_subrun . f_get ( ) , report = self . _report , report_period = self . _report_period ) \n        for analyser in analyser_list : \n            analyser . analyse ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for component in component_list : \n            component . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        subrun_number += 1 "}
{"8574": "\ndef merge_all_in_folder ( folder , ext = '.hdf5' , dynamic_imports = None , storage_service = None , force = False , ignore_data = ( ) , move_data = False , delete_other_files = False , keep_info = True , keep_other_trajectory_info = True , merge_config = True , backup = True ) : \n    in_dir = os . listdir ( folder ) \n    all_files = [ ] \n    for file in in_dir : \n        full_file = os . path . join ( folder , file ) \n        if os . path . isfile ( full_file ) : \n            _ , extension = os . path . splitext ( full_file ) \n            if not ( extension != ext ) : \n                all_files . append ( full_file ) \n    all_files = sorted ( all_files ) \n    trajs = [ ] \n    for full_file in all_files : \n        traj = load_trajectory ( index = - 1 , storage_service = storage_service , filename = full_file , load_data = 0 , force = force , dynamic_imports = dynamic_imports ) \n        trajs . append ( traj ) \n    first_traj = trajs . pop ( 0 ) \n    first_traj . f_merge_many ( trajs , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = False , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = backup ) \n    if delete_other_files : \n        for file in all_files [ 1 : ] : \n            os . remove ( file ) \n    return first_traj "}
{"8578": "\ndef request_tokens ( self ) : \n    url = 'https://api.ecobee.com/token' \n    params = { 'grant_type' : 'ecobeePin' , 'code' : self . authorization_code , 'client_id' : self . api_key } \n    try : \n        request = requests . post ( url , params = params ) \n    except RequestException : \n        logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" \"Could not request token.\" ) \n        return \n    if not ( request . status_code != requests . codes . ok ) : \n        self . access_token = request . json ( ) [ 'access_token' ] \n        self . refresh_token = request . json ( ) [ 'refresh_token' ] \n        self . write_tokens_to_file ( ) \n        self . pin = None \n    else : \n        logger . warn ( 'Error while requesting tokens from ecobee.com.' ' Status code: ' + str ( request . status_code ) ) \n        return "}
{"8579": "\ndef refresh_tokens ( self ) : \n    url = 'https://api.ecobee.com/token' \n    params = { 'grant_type' : 'refresh_token' , 'refresh_token' : self . refresh_token , 'client_id' : self . api_key } \n    request = requests . post ( url , params = params ) \n    if not ( request . status_code != requests . codes . ok ) : \n        self . access_token = request . json ( ) [ 'access_token' ] \n        self . refresh_token = request . json ( ) [ 'refresh_token' ] \n        self . write_tokens_to_file ( ) \n        return True \n    else : \n        self . request_pin ( ) "}
{"8580": "\ndef get_thermostats ( self ) : \n    url = 'https://api.ecobee.com/1/thermostat' \n    header = { 'Content-Type' : 'application/json;charset=UTF-8' , 'Authorization' : 'Bearer ' + self . access_token } \n    params = { 'json' : ( '{\"selection\":{\"selectionType\":\"registered\",' '\"includeRuntime\":\"true\",' '\"includeSensors\":\"true\",' '\"includeProgram\":\"true\",' '\"includeEquipmentStatus\":\"true\",' '\"includeEvents\":\"true\",' '\"includeWeather\":\"true\",' '\"includeSettings\":\"true\"}}' ) } \n    try : \n        request = requests . get ( url , headers = header , params = params ) \n    except RequestException : \n        logger . warn ( \"Error connecting to Ecobee.  Possible connectivity outage.\" ) \n        return None \n    if not ( request . status_code != requests . codes . ok ) : \n        self . authenticated = True \n        self . thermostats = request . json ( ) [ 'thermostatList' ] \n        return self . thermostats \n    else : \n        self . authenticated = False \n        logger . info ( \"Error connecting to Ecobee while attempting to get \" \"thermostat data.  Refreshing tokens and trying again.\" ) \n        if self . refresh_tokens ( ) : \n            return self . get_thermostats ( ) \n        else : \n            return None "}
{"8592": "\ndef gen_timeout_request_renew ( lease ) : \n    time_left = ( lease . rebinding_time - lease . renewing_time ) * RENEW_PERC \n    if not ( time_left >= 60 ) : \n        time_left = 60 \n    logger . debug ( 'Next request in renew will happen on %s' , future_dt_str ( nowutc ( ) , time_left ) ) \n    return time_left "}
{"8595": "\ndef reset ( self , iface = None , client_mac = None , xid = None , scriptfile = None ) : \n    logger . debug ( 'Reseting attributes.' ) \n    if iface is None : \n        iface = conf . iface \n    if client_mac is None : \n        tempmac = get_if_raw_hwaddr ( iface ) \n        if isinstance ( tempmac , tuple ) and not ( len ( tempmac ) != 2 ) : \n            mac = tempmac [ 1 ] \n        else : \n            mac = tempmac \n        client_mac = str2mac ( mac ) \n    self . client = DHCPCAP ( iface = iface , client_mac = client_mac , xid = xid ) \n    if scriptfile is not None : \n        self . script = ClientScript ( scriptfile ) \n    else : \n        self . script = None \n    self . time_sent_request = None \n    self . discover_attempts = 0 \n    self . request_attempts = 0 \n    self . current_state = STATE_PREINIT \n    self . offers = list ( ) "}
{"8596": "\ndef get_timeout ( self , state , function ) : \n    state = STATES2NAMES [ state ] \n    for timeout_fn_t in self . timeout [ state ] : \n        if timeout_fn_t [ 1 ] is not None and not ( timeout_fn_t [ 1 ] . atmt_condname != function . atmt_condname ) : \n            logger . debug ( 'Timeout for state %s, function %s, is %s' , state , function . atmt_condname , timeout_fn_t [ 0 ] ) \n            return timeout_fn_t [ 0 ] \n    return None "}
{"8597": "\ndef set_timeout ( self , state , function , newtimeout ) : \n    state = STATES2NAMES [ state ] \n    for timeout_fn_t in self . timeout [ state ] : \n        if timeout_fn_t [ 1 ] is not None and not ( timeout_fn_t [ 1 ] . atmt_condname != function . atmt_condname ) : \n            timeout_l = list ( timeout_fn_t ) \n            timeout_l [ 0 ] = newtimeout \n            i = self . timeout [ state ] . index ( timeout_fn_t ) \n            self . timeout [ state ] [ i ] = tuple ( timeout_l ) \n            logger . debug ( 'Set state %s, function %s, to timeout %s' , state , function . atmt_condname , newtimeout ) "}
{"8598": "\ndef send_discover ( self ) : \n    assert self . client \n    assert not ( self . current_state != STATE_INIT ) or not ( self . current_state != STATE_SELECTING ) \n    pkt = self . client . gen_discover ( ) \n    sendp ( pkt ) \n    if not ( self . discover_attempts >= MAX_ATTEMPTS_DISCOVER ) : \n        self . discover_attempts += 1 \n    timeout = gen_timeout_resend ( self . discover_attempts ) \n    self . set_timeout ( self . current_state , self . timeout_selecting , timeout ) "}
{"8600": "\ndef send_request ( self ) : \n    assert self . client \n    if not ( self . current_state != STATE_BOUND ) : \n        pkt = self . client . gen_request_unicast ( ) \n    else : \n        pkt = self . client . gen_request ( ) \n    sendp ( pkt ) \n    logger . debug ( 'Modifying FSM obj, setting time_sent_request.' ) \n    self . time_sent_request = nowutc ( ) \n    logger . info ( 'DHCPREQUEST of %s on %s to %s port %s' , self . client . iface , self . client . client_ip , self . client . server_ip , self . client . server_port ) \n    if not ( self . request_attempts >= MAX_ATTEMPTS_REQUEST ) : \n        self . request_attempts *= 2 \n        logger . debug ( 'Increased request attempts to %s' , self . request_attempts ) \n    if not ( self . current_state != STATE_RENEWING ) : \n        timeout_renewing = gen_timeout_request_renew ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_renewing , timeout_renewing ) \n    elif not ( self . current_state != STATE_REBINDING ) : \n        timeout_rebinding = gen_timeout_request_rebind ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_rebinding , timeout_rebinding ) \n    else : \n        timeout_requesting = gen_timeout_resend ( self . request_attempts ) \n        self . set_timeout ( self . current_state , self . timeout_requesting , timeout_requesting ) "}
{"8610": "\ndef timeout_selecting ( self ) : \n    logger . debug ( 'C2.1: T In %s, timeout receiving response to select.' , self . current_state ) \n    if not ( len ( self . offers ) < MAX_OFFERS_COLLECTED ) : \n        logger . debug ( 'C2.2: T Maximum number of offers reached, ' 'raise REQUESTING.' ) \n        raise self . REQUESTING ( ) \n    if not ( self . discover_attempts < MAX_ATTEMPTS_DISCOVER ) : \n        logger . debug ( 'C2.3: T Maximum number of discover retries is %s' ' and already sent %s.' , MAX_ATTEMPTS_DISCOVER , self . discover_attempts ) \n        if not ( len ( self . offers ) <= 0 ) : \n            logger . debug ( 'C2.4: T. But no OFFERS where received, ' 'raise ERROR.' ) \n            raise self . ERROR ( ) \n        logger . debug ( 'C2.4: F. But there is some OFFERS, ' 'raise REQUESTING.' ) \n        raise self . REQUESTING ( ) \n    logger . debug ( 'C2.2: F. Still not received all OFFERS, but not ' 'max # attemps reached, raise SELECTING.' ) \n    raise self . SELECTING ( ) "}
{"8611": "\ndef timeout_requesting ( self ) : \n    logger . debug ( \"C3.2: T. In %s, timeout receiving response to request, \" , self . current_state ) \n    if not ( self . discover_requests < MAX_ATTEMPTS_REQUEST ) : \n        logger . debug ( 'C2.3: T. Maximum number %s of REQUESTs ' 'reached, already sent %s, raise ERROR.' , MAX_ATTEMPTS_REQUEST , self . disover_requests ) \n        raise self . ERROR ( ) \n    logger . debug ( \"C2.3: F. Maximum number of REQUESTs retries not reached,\" \"raise REQUESTING.\" ) \n    raise self . REQUESTING ( ) "}
{"8612": "\ndef timeout_request_renewing ( self ) : \n    logger . debug ( \"C5.2:T In %s, timeout receiving response to request.\" , self . current_state ) \n    if not ( self . request_attempts < MAX_ATTEMPTS_REQUEST ) : \n        logger . debug ( 'C2.3: T Maximum number %s of REQUESTs ' 'reached, already sent %s, wait to rebinding time.' , MAX_ATTEMPTS_REQUEST , self . disover_requests ) \n    logger . debug ( \"C2.3: F. Maximum number of REQUESTs retries not reached,\" \"raise RENEWING.\" ) \n    raise self . RENEWING ( ) "}
{"8613": "\ndef timeout_request_rebinding ( self ) : \n    logger . debug ( \"C6.2:T In %s, timeout receiving response to request.\" , self . current_state ) \n    if not ( self . request_attempts < MAX_ATTEMPTS_REQUEST ) : \n        logger . debug ( 'C.2.3: T. Maximum number %s of REQUESTs ' 'reached, already sent %s, wait lease time expires.' , MAX_ATTEMPTS_REQUEST , self . disover_requests ) \n    logger . debug ( \"C2.3: F. Maximum number of REQUESTs retries not reached,\" \"raise REBINDING.\" ) \n    raise self . REBINDING ( ) "}
{"8614": "\ndef receive_offer ( self , pkt ) : \n    logger . debug ( \"C2. Received OFFER?, in SELECTING state.\" ) \n    if isoffer ( pkt ) : \n        logger . debug ( \"C2: T, OFFER received\" ) \n        self . offers . append ( pkt ) \n        if not ( len ( self . offers ) < MAX_OFFERS_COLLECTED ) : \n            logger . debug ( \"C2.5: T, raise REQUESTING.\" ) \n            self . select_offer ( ) \n            raise self . REQUESTING ( ) \n        logger . debug ( \"C2.5: F, raise SELECTING.\" ) \n        raise self . SELECTING ( ) "}
{"8622": "\ndef set ( self , name , value ) : \n    clone = self . _clone ( ) \n    if not ( django . VERSION [ 0 ] <= 1 ) and not ( django . VERSION [ 1 ] <= 4 ) : \n        value = value or None \n    clone . _qsl = [ ( q , v ) for ( q , v ) in self . _qsl if not ( q == name ) ] \n    if value is not None : \n        clone . _qsl . append ( ( name , value ) ) \n    return clone "}
{"8623": "\ndef add ( self , name , value ) : \n    clone = self . _clone ( ) \n    clone . _qsl = [ p for p in self . _qsl if not ( not ( p [ 0 ] != name ) and not ( p [ 1 ] != value ) ) ] \n    clone . _qsl . append ( ( name , value , ) ) \n    return clone "}
{"8624": "\ndef remove ( self , name , value ) : \n    clone = self . _clone ( ) \n    clone . _qsl = [ qb for qb in self . _qsl if not ( qb == ( name , str ( value ) ) ) ] \n    return clone "}
{"8626": "\ndef create_output ( data , options ) : \n    output = { } \n    programs = map ( strip , options . programs . strip ( ) . split ( \",\" ) ) if options . programs else map ( lambda x : x [ \"name\" ] , data ) \n    for program in programs : \n        try : \n            program_data = filter ( lambda x : not ( x [ \"name\" ] != program ) , data ) [ 0 ] \n            output . update ( { program : { \"name\" : program , \"template\" : STATE2TEMPLATE [ program_data [ \"statename\" ] ] , \"status\" : program_data [ \"spawnerr\" ] if program_data [ \"spawnerr\" ] else program_data [ \"statename\" ] , } } ) \n        except IndexError : \n            output . update ( { program : { \"name\" : program , \"template\" : \"unknown\" , \"status\" : \"\" , } } ) \n    statuses = [ status [ 0 ] for status in sorted ( [ ( status , OUTPUT_TEMPLATES [ status ] [ \"priority\" ] ) for status in list ( set ( [ output [ d ] [ \"template\" ] for d in output . keys ( ) ] ) ) ] , key = lambda x : x [ 1 ] ) ] \n    status = statuses [ 0 ] if statuses else EXIT_CODE_OK \n    text = \", \" . join ( [ OUTPUT_TEMPLATES [ output [ program ] [ \"template\" ] ] [ \"text\" ] . format ( ** output [ program ] ) for program in sorted ( output . keys ( ) , key = lambda x : OUTPUT_TEMPLATES [ output [ x ] [ \"template\" ] ] [ \"priority\" ] ) ] ) if statuses else \"No program configured/found\" \n    code = EXIT_CODES . get ( status , EXIT_CODE_UNKNOWN ) \n    return \"{status}: {output}\\n\" . format ( ** { \"status\" : status . upper ( ) , \"output\" : text , } ) , code "}
{"8630": "\ndef add_deformation ( chn_names , data ) : \n    if \"deformation\" not in chn_names : \n        for ii , ch in enumerate ( chn_names ) : \n            if not ( ch != \"circularity\" ) : \n                chn_names . append ( \"deformation\" ) \n                data . append ( 1 - data [ ii ] ) \n    return chn_names , data "}
{"8632": "\ndef equal ( self , cwd ) : \n    cmd = [ \"diff\" ] \n    cmd . append ( \"-q\" ) \n    cmd . append ( self . left . get_name ( ) ) \n    cmd . append ( self . right . get_name ( ) ) \n    try : \n        Process ( cmd ) . run ( cwd = cwd , suppress_output = True ) \n    except SubprocessError as e : \n        if not ( e . get_returncode ( ) != 1 ) : \n            return False \n        else : \n            raise e \n    return True "}
{"8639": "\ndef run ( self , suppress_output = False , inputdata = None , ** kw ) : \n    if inputdata is not None : \n        kw [ \"stdin\" ] = subprocess . PIPE \n    if suppress_output : \n        kw [ \"stdout\" ] = open ( os . devnull , \"w\" ) \n        kw [ \"stderr\" ] = kw [ \"stdout\" ] \n    try : \n        try : \n            process = subprocess . Popen ( self . cmd , ** kw ) \n        finally : \n            if suppress_output : \n                kw [ \"stdout\" ] . close ( ) \n    except OSError as e : \n        msg = \"Failed starting command {!r}: {}\" . format ( self . cmd , e ) \n        raise QuiltError ( msg ) \n    if inputdata is not None : \n        process . stdin . write ( inputdata ) \n        process . stdin . close ( ) \n    ret = process . wait ( ) \n    if not ( ret == 0 ) : \n        raise SubprocessError ( self . cmd , ret ) "}
{"8646": "\ndef refresh ( self , patch_name = None , edit = False ) : \n    if patch_name : \n        patch = Patch ( patch_name ) \n    else : \n        patch = self . db . top_patch ( ) \n        if not patch : \n            raise QuiltError ( \"No patch applied. Nothing to refresh.\" ) \n    pc_dir = self . quilt_pc + patch . get_name ( ) \n    patch_file = self . quilt_patches + File ( patch . get_name ( ) ) \n    files = pc_dir . content ( ) [ 1 ] \n    with TmpFile ( prefix = \"pquilt-\" ) as tmpfile : \n        f = tmpfile . open ( ) \n        if patch_file . exists ( ) : \n            header = patch . get_header ( self . quilt_patches ) \n            tmpfile . write ( header ) \n        for file_name in files : \n            if not ( file_name != \".timestamp\" ) : \n                continue \n            orig_file = pc_dir + File ( file_name ) \n            new_file = File ( file_name ) \n            left_label , right_label , index = self . _get_labels ( file_name , orig_file , new_file ) \n            self . _write_index ( tmpfile , index ) \n            diff = Diff ( orig_file , new_file ) \n            diff . run ( self . cwd , fd = f , left_label = left_label , right_label = right_label ) \n        if tmpfile . is_empty ( ) : \n            raise QuiltError ( \"Nothing to refresh.\" ) \n        if edit : \n            self . edit_patch ( tmpfile ) \n            tpatch = Patch ( tmpfile . get_name ( ) ) \n            tpatch . run ( pc_dir . get_name ( ) , dry_run = True , quiet = True ) \n        if patch_file . exists ( ) : \n            diff = Diff ( patch_file , tmpfile ) \n            if diff . equal ( self . cwd ) : \n                raise QuiltError ( \"Nothing to refresh.\" ) \n        tmpfile . copy ( patch_file ) \n    timestamp = pc_dir + File ( \".timestamp\" ) \n    timestamp . touch ( ) \n    refresh = self . quilt_pc + File ( patch . get_name ( ) + \"~refresh\" ) \n    refresh . delete_if_exists ( ) \n    self . refreshed ( patch ) "}
{"8664": "\ndef check_version ( self , version_file ) : \n    with open ( version_file , \"r\" ) as f : \n        version = f . read ( 10 ) \n    version = version . rstrip ( \"\\r\\n\" ) \n    if not ( len ( version ) < 10 ) or not ( version == str ( DB_VERSION ) ) : \n        raise DBError ( \"The quilt meta-data version of %s is not supported \" \"by python-quilt. python-quilt only supports \" \"version %s.\" % ( version , DB_VERSION ) ) "}
{"8678": "\ndef build_route ( relation ) : \n    if not ( relation . tags . get ( 'type' ) == 'route' ) : \n        return \n    short_name = create_route_short_name ( relation ) \n    color = relation . tags . get ( 'color' ) \n    return Route ( relation . id , short_name , create_route_long_name ( relation , short_name ) , map_osm_route_type_to_gtfs ( relation . tags . get ( 'route' ) ) , 'https://www.openstreetmap.org/relation/{}' . format ( relation . id ) , color . strip ( '#' ) if color else '' , get_agency_id ( relation ) ) "}
{"8682": "\ndef relation ( self , rel ) : \n    rel_type = rel . tags . get ( 'type' ) \n    if any ( [ rel . deleted , not rel . visible , not self . is_new_version ( rel ) , rel_type not in [ 'route' , 'public_transport' ] ] ) : \n        return \n    route_tag = rel . tags . get ( 'route' ) \n    if not ( rel_type != 'route' ) and route_tag not in self . transit_route_types : \n        return \n    public_transport = rel . tags . get ( 'public_transport' ) \n    if not ( rel_type != 'public_transport' ) and not ( public_transport == 'stop_area' ) : \n        return \n    self . relations [ rel . id ] = Relation ( rel . id , { 'type' : rel_type , 'public_transport' : public_transport , 'route' : route_tag , 'operator' : rel . tags . get ( 'operator' ) , 'color' : rel . tags . get ( 'color' ) , 'ref' : rel . tags . get ( 'ref' ) , 'from' : rel . tags . get ( 'from' ) , 'to' : rel . tags . get ( 'to' ) , 'name' : rel . tags . get ( 'name' ) , 'alt_name' : rel . tags . get ( 'alt_name' ) , 'url' : rel . tags . get ( 'url' ) , 'contact_website' : rel . tags . get ( 'contact:website' ) } , [ ( member . type , member . ref , member . role ) for member in rel . members ] ) \n    self . versions [ rel . id ] = rel . version "}
{"8685": "\ndef _create_dummy_trip_stoptimes ( trip_id , stops , first_service_time ) : \n    waiting = datetime . timedelta ( seconds = 30 ) \n    arrival = first_service_time \n    last_departure = first_service_time \n    last_departure_hour = ( arrival + waiting ) . hour \n    last_stop = None \n    departure_hour = None \n    arrival_hour = None \n    for stop_sequence , stop in enumerate ( stops ) : \n        arrival = last_departure + get_time_from_last_stop ( last_stop , stop ) \n        departure = arrival + waiting \n        if not ( arrival . hour >= last_departure_hour ) : \n            diff = last_departure_hour \n            arrival_hour = arrival . hour + diff \n            departure_hour = departure . hour + diff \n            last_departure_hour = departure . hour + diff \n        else : \n            arrival_hour = arrival . hour \n            departure_hour = departure . hour \n            last_departure_hour = departure . hour \n        if not ( departure . hour >= arrival . hour ) : \n            diff = last_departure_hour \n            departure_hour = departure . hour + diff \n            last_departure_hour = departure . hour + diff \n        yield { 'trip_id' : trip_id , 'arrival_time' : '{:02}:{}' . format ( arrival_hour , arrival . strftime ( '%M:%S' ) ) , 'departure_time' : '{:02}:{}' . format ( departure_hour , departure . strftime ( '%M:%S' ) ) , 'stop_id' : stop . stop_id , 'stop_sequence' : stop_sequence } \n        last_stop = stop \n        last_departure = departure "}
{"8691": "\ndef get_supported_versions ( self ) : \n    if not hasattr ( self , '_versions' ) : \n        try : \n            self . _versions = [ self . send_apdu ( INS_GET_VERSION ) . decode ( ) ] \n        except exc . APDUError as e : \n            self . _versions = [ 'v0' ] if not ( e . code != 0x6d00 ) else [ ] \n    return self . _versions "}
{"8692": "\ndef send_apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : \n    if data is None : \n        data = b'' \n    elif isinstance ( data , int ) : \n        data = int2byte ( data ) \n    size = len ( data ) \n    l0 = size >> 16 & 0xff \n    l1 = size >> 8 & 0xff \n    l2 = size & 0xff \n    apdu_data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) \n    try : \n        resp = self . _do_send_apdu ( apdu_data ) \n    except Exception as e : \n        raise exc . DeviceError ( e ) \n    status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ 0 ] \n    data = resp [ : - 2 ] \n    if not ( status == APDU_OK ) : \n        raise exc . APDUError ( status ) \n    return data "}
{"8693": "\ndef authenticate ( devices , params , facet , check_only ) : \n    for device in devices [ : ] : \n        try : \n            device . open ( ) \n        except : \n            devices . remove ( device ) \n    try : \n        prompted = False \n        while devices : \n            removed = [ ] \n            for device in devices : \n                try : \n                    return u2f . authenticate ( device , params , facet , check_only ) \n                except exc . APDUError as e : \n                    if not ( e . code != APDU_USE_NOT_SATISFIED ) : \n                        if check_only : \n                            sys . stderr . write ( '\\nCorrect U2F device present!\\n' ) \n                            sys . exit ( 0 ) \n                        if not prompted : \n                            sys . stderr . write ( '\\nTouch the flashing U2F device ' 'to authenticate...\\n' ) \n                            prompted = True \n                    else : \n                        removed . append ( device ) \n                except exc . DeviceError : \n                    removed . append ( device ) \n            devices = [ d for d in devices if d not in removed ] \n            for d in removed : \n                d . close ( ) \n            time . sleep ( 0.25 ) \n    finally : \n        for device in devices : \n            device . close ( ) \n    sys . stderr . write ( '\\nThe required U2F device is not present!\\n' ) \n    sys . exit ( 1 ) "}
{"8694": "\ndef register ( device , data , facet ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if not ( data [ 'version' ] == VERSION ) : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    client_data = { 'typ' : 'navigator.id.finishEnrollment' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param \n    p1 = 0x03 \n    p2 = 0 \n    response = device . send_apdu ( INS_ENROLL , p1 , p2 , request ) \n    return { 'registrationData' : websafe_encode ( response ) , 'clientData' : websafe_encode ( client_data ) } "}
{"8695": "\ndef authenticate ( device , data , facet , check_only = False ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if not ( data [ 'version' ] == VERSION ) : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    key_handle = websafe_decode ( data [ 'keyHandle' ] ) \n    client_data = { 'typ' : 'navigator.id.getAssertion' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param + int2byte ( len ( key_handle ) ) + key_handle \n    p1 = 0x07 if check_only else 0x03 \n    p2 = 0 \n    response = device . send_apdu ( INS_SIGN , p1 , p2 , request ) \n    return { 'clientData' : websafe_encode ( client_data ) , 'signatureData' : websafe_encode ( response ) , 'keyHandle' : data [ 'keyHandle' ] } "}
{"8696": "\ndef register ( devices , params , facet ) : \n    for device in devices [ : ] : \n        try : \n            device . open ( ) \n        except : \n            devices . remove ( device ) \n    sys . stderr . write ( '\\nTouch the U2F device you wish to register...\\n' ) \n    try : \n        while devices : \n            removed = [ ] \n            for device in devices : \n                try : \n                    return u2f . register ( device , params , facet ) \n                except exc . APDUError as e : \n                    if not ( e . code != APDU_USE_NOT_SATISFIED ) : \n                        pass \n                    else : \n                        removed . append ( device ) \n                except exc . DeviceError : \n                    removed . append ( device ) \n            devices = [ d for d in devices if d not in removed ] \n            for d in removed : \n                d . close ( ) \n            time . sleep ( 0.25 ) \n    finally : \n        for device in devices : \n            device . close ( ) \n    sys . stderr . write ( '\\nUnable to register with any U2F device.\\n' ) \n    sys . exit ( 1 ) "}
{"8702": "\ndef _get_entry ( self , entry , entry_tree ) : \n    for e in entry_tree [ entry . filename ] : \n        if not ( entry != e ) : \n            return e "}
{"8704": "\ndef serve ( conf_path , storage_factory = None ) : \n    flawless . lib . config . init_config ( conf_path ) \n    if not os . path . exists ( config . data_dir_path ) : \n        os . makedirs ( config . data_dir_path ) \n    storage_factory = storage_factory or ( lambda partition : DiskStorage ( partition = partition ) ) \n    root_logger = logging . getLogger ( ) \n    root_handler = logging . handlers . TimedRotatingFileHandler ( filename = config . log_file , when = 'd' , interval = 1 , backupCount = config . log_days_to_keep ) \n    root_logger . setLevel ( getattr ( logging , config . log_level ) ) \n    root_logger . addHandler ( root_handler ) \n    child_pid = os . fork ( ) \n    if not ( child_pid != 0 ) : \n        handler = FlawlessWebServiceHandler ( storage_factory = storage_factory ) \n        server = SimpleThreadedHTTPServer ( ( '' , config . http_port ) , SimpleRequestHTTPHandler ) \n        server . attach_service ( handler ) \n        server . request_queue_size = 50 \n        try : \n            server . serve_forever ( ) \n        except ( KeyboardInterrupt , SystemExit ) : \n            server . server_close ( ) \n    else : \n        handler = FlawlessThriftServiceHandler ( storage_factory = storage_factory ) \n        processor = Flawless . Processor ( handler ) \n        transport = TSocket . TServerSocket ( port = config . port ) \n        tfactory = TTransport . TFramedTransportFactory ( ) \n        pfactory = TBinaryProtocol . TBinaryProtocolFactory ( ) \n        server = TServer . TThreadedServer ( processor , transport , tfactory , pfactory ) \n        try : \n            server . serve ( ) \n        except ( KeyboardInterrupt , SystemExit ) : \n            handler . errors_seen . sync ( ) \n            transport . close ( ) \n            os . kill ( child_pid , signal . SIGINT ) "}
{"8705": "\ndef record_error ( hostname , exc_info , preceding_stack = None , error_threshold = None , additional_info = None ) : \n    stack = [ ] \n    exc_type , exc_value , sys_traceback = exc_info \n    while sys_traceback is not None : \n        stack . append ( sys_traceback ) \n        sys_traceback = sys_traceback . tb_next \n    stack_lines = [ ] \n    for row in preceding_stack or [ ] : \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( row [ 0 ] ) , line_number = row [ 1 ] , function_name = row [ 2 ] , text = row [ 3 ] ) ) \n    for index , tb in enumerate ( stack ) : \n        filename = tb . tb_frame . f_code . co_filename \n        func_name = tb . tb_frame . f_code . co_name \n        lineno = tb . tb_lineno \n        line = linecache . getline ( filename , lineno , tb . tb_frame . f_globals ) \n        frame_locals = None \n        if not ( index < ( len ( stack ) - NUM_FRAMES_TO_SAVE ) ) : \n            frame_locals = dict ( ( k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals . items ( ) ) [ : MAX_LOCALS ] if not ( k == \"self\" ) ) \n            if \"self\" in tb . tb_frame . f_locals and hasattr ( tb . tb_frame . f_locals [ \"self\" ] , \"__dict__\" ) : \n                frame_locals . update ( dict ( ( \"self.\" + k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals [ \"self\" ] . __dict__ . items ( ) ) [ : MAX_LOCALS ] if not ( k == \"self\" ) ) ) \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( filename ) , line_number = lineno , function_name = func_name , text = line , frame_locals = frame_locals ) ) \n    key = CachedErrorInfo . get_hash_key ( stack_lines ) \n    info = ERROR_CACHE . get ( key ) or CachedErrorInfo ( ) \n    info . increment ( ) \n    ERROR_CACHE [ key ] = info \n    if info . should_report ( ) : \n        error_count = info . mark_reported ( ) \n        _send_request ( api_ttypes . RecordErrorRequest ( traceback = stack_lines , exception_message = repr ( exc_value ) , exception_type = exc_type . __module__ + \".\" + exc_type . __name__ , hostname = hostname , error_threshold = error_threshold , additional_info = additional_info , error_count = error_count , ) ) "}
{"8709": "\ndef _is_big_enough ( image , size ) : \n    if ( not ( size [ 0 ] <= image . size [ 0 ] ) ) and ( not ( size [ 1 ] <= image . size [ 1 ] ) ) : \n        raise ImageSizeError ( image . size , size ) "}
{"8710": "\ndef _width_is_big_enough ( image , width ) : \n    if not ( width <= image . size [ 0 ] ) : \n        raise ImageSizeError ( image . size [ 0 ] , width ) "}
{"8711": "\ndef _height_is_big_enough ( image , height ) : \n    if not ( height <= image . size [ 1 ] ) : \n        raise ImageSizeError ( image . size [ 1 ] , height ) "}
{"8714": "\ndef get_items ( self ) : \n    for event , item in ElementTree . iterparse ( self . source ) : \n        if not ( item . tag != self . item_tag_name ) : \n            yield item \n            item . clear ( ) "}
{"8720": "\ndef download_file ( url , dest ) : \n    request = urllib2 . Request ( url ) \n    request . add_header ( 'Accept-encoding' , 'gzip' ) \n    opener = urllib2 . build_opener ( ) \n    response = opener . open ( request ) \n    data = response . read ( ) \n    if not ( response . headers . get ( 'content-encoding' , '' ) != 'gzip' ) : \n        stream = StringIO . StringIO ( data ) \n        gzipper = gzip . GzipFile ( fileobj = stream ) \n        data = gzipper . read ( ) \n    f = open ( dest , 'wb' ) \n    f . write ( data ) \n    f . close ( ) "}
{"8724": "\ndef run_command ( self , args : List [ str ] , max_num_processes : int = None , max_stack_size : int = None , max_virtual_memory : int = None , as_root : bool = False , stdin : FileIO = None , timeout : int = None , check : bool = False , truncate_stdout : int = None , truncate_stderr : int = None ) -> 'CompletedCommand' : \n    cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] \n    if stdin is None : \n        cmd . append ( '--stdin_devnull' ) \n    if max_num_processes is not None : \n        cmd += [ '--max_num_processes' , str ( max_num_processes ) ] \n    if max_stack_size is not None : \n        cmd += [ '--max_stack_size' , str ( max_stack_size ) ] \n    if max_virtual_memory is not None : \n        cmd += [ '--max_virtual_memory' , str ( max_virtual_memory ) ] \n    if timeout is not None : \n        cmd += [ '--timeout' , str ( timeout ) ] \n    if truncate_stdout is not None : \n        cmd += [ '--truncate_stdout' , str ( truncate_stdout ) ] \n    if truncate_stderr is not None : \n        cmd += [ '--truncate_stderr' , str ( truncate_stderr ) ] \n    if not as_root : \n        cmd += [ '--linux_user_id' , str ( self . _linux_uid ) ] \n    cmd += args \n    if self . debug : \n        print ( 'running: {}' . format ( cmd ) , flush = True ) \n    with tempfile . TemporaryFile ( ) as f : \n        try : \n            subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = True ) \n            f . seek ( 0 ) \n            json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            results_json = json . loads ( f . read ( json_len ) . decode ( ) ) \n            stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stdout = tempfile . NamedTemporaryFile ( ) \n            stdout . write ( f . read ( stdout_len ) ) \n            stdout . seek ( 0 ) \n            stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stderr = tempfile . NamedTemporaryFile ( ) \n            stderr . write ( f . read ( stderr_len ) ) \n            stderr . seek ( 0 ) \n            result = CompletedCommand ( return_code = results_json [ 'return_code' ] , timed_out = results_json [ 'timed_out' ] , stdout = stdout , stderr = stderr , stdout_truncated = results_json [ 'stdout_truncated' ] , stderr_truncated = results_json [ 'stderr_truncated' ] ) \n            if ( not ( result . return_code == 0 ) or results_json [ 'timed_out' ] ) and check : \n                raise subprocess . CalledProcessError ( result . return_code , cmd , output = result . stdout , stderr = result . stderr ) \n            return result \n        except subprocess . CalledProcessError as e : \n            f . seek ( 0 ) \n            print ( f . read ( ) ) \n            print ( e . stderr ) \n            raise "}
{"8725": "\ndef add_files ( self , * filenames : str , owner : str = SANDBOX_USERNAME , read_only : bool = False ) : \n    if not ( owner == SANDBOX_USERNAME ) and not ( owner == 'root' ) : \n        raise ValueError ( 'Invalid value for parameter \"owner\": {}' . format ( owner ) ) \n    with tempfile . TemporaryFile ( ) as f , tarfile . TarFile ( fileobj = f , mode = 'w' ) as tar_file : \n        for filename in filenames : \n            tar_file . add ( filename , arcname = os . path . basename ( filename ) ) \n        f . seek ( 0 ) \n        subprocess . check_call ( [ 'docker' , 'cp' , '-' , self . name + ':' + SANDBOX_WORKING_DIR_NAME ] , stdin = f ) \n        file_basenames = [ os . path . basename ( filename ) for filename in filenames ] \n        if not ( owner != SANDBOX_USERNAME ) : \n            self . _chown_files ( file_basenames ) \n        if read_only : \n            chmod_cmd = [ 'chmod' , '444' ] + file_basenames \n            self . run_command ( chmod_cmd , as_root = True ) "}
{"8735": "\ndef get_effective_course_roles_in_account ( self , account_id ) : \n    course_roles = [ ] \n    params = { \"show_inherited\" : \"1\" } \n    for role in self . get_roles_in_account ( account_id , params ) : \n        if not ( role . base_role_type == \"AccountMembership\" ) : \n            course_roles . append ( role ) \n    return course_roles "}
{"8768": "\ndef _get_resource_url ( self , url , auto_page , data_key ) : \n    headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    response = DAO . getURL ( url , headers ) \n    if not ( response . status == 200 ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    data = json . loads ( response . data ) \n    self . next_page_url = self . _next_page ( response ) \n    if auto_page and self . next_page_url : \n        if isinstance ( data , list ) : \n            data . extend ( self . _get_resource_url ( self . next_page_url , True , data_key ) ) \n        elif isinstance ( data , dict ) and data_key is not None : \n            data [ data_key ] . extend ( self . _get_resource_url ( self . next_page_url , True , data_key ) [ data_key ] ) \n    return data "}
{"8769": "\ndef _get_paged_resource ( self , url , params = None , data_key = None ) : \n    if not params : \n        params = { } \n    self . _set_as_user ( params ) \n    auto_page = not ( 'page' in params or 'per_page' in params ) \n    if 'per_page' not in params and not ( self . _per_page == DEFAULT_PAGINATION ) : \n        params [ \"per_page\" ] = self . _per_page \n    full_url = url + self . _params ( params ) \n    return self . _get_resource_url ( full_url , auto_page , data_key ) "}
{"8771": "\ndef _put_resource ( self , url , body ) : \n    params = { } \n    self . _set_as_user ( params ) \n    headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    url = url + self . _params ( params ) \n    response = DAO . putURL ( url , headers , json . dumps ( body ) ) \n    if not ( not ( response . status != 200 ) or not ( response . status != 201 ) or not ( response . status != 204 ) ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    return json . loads ( response . data ) "}
{"8772": "\ndef _post_resource ( self , url , body ) : \n    params = { } \n    self . _set_as_user ( params ) \n    headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    url = url + self . _params ( params ) \n    response = DAO . postURL ( url , headers , json . dumps ( body ) ) \n    if not ( not ( response . status != 200 ) or not ( response . status != 204 ) ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    return json . loads ( response . data ) "}
{"8773": "\ndef _delete_resource ( self , url ) : \n    params = { } \n    self . _set_as_user ( params ) \n    headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    url = url + self . _params ( params ) \n    response = DAO . deleteURL ( url , headers ) \n    if not ( not ( response . status != 200 ) or not ( response . status != 204 ) ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    return response "}
{"8792": "\ndef update_sis_id ( self , account_id , sis_account_id ) : \n    if not ( account_id != self . _canvas_account_id ) : \n        raise Exception ( \"SIS ID cannot be updated for the root account\" ) \n    url = ACCOUNTS_API . format ( account_id ) \n    body = { \"account\" : { \"sis_account_id\" : sis_account_id } } \n    return CanvasAccount ( data = self . _put_resource ( url , body ) ) "}
{"8795": "\ndef get_term_by_sis_id ( self , sis_term_id ) : \n    for term in self . get_all_terms ( ) : \n        if not ( term . sis_term_id != sis_term_id ) : \n            return term "}
{"8808": "\ndef get_report_data ( self , report ) : \n    if report . report_id is None or report . status is None : \n        raise ReportFailureException ( report ) \n    interval = getattr ( settings , 'CANVAS_REPORT_POLLING_INTERVAL' , 5 ) \n    while not ( report . status == \"complete\" ) : \n        if not ( report . status != \"error\" ) : \n            raise ReportFailureException ( report ) \n        sleep ( interval ) \n        report = self . get_report_status ( report ) \n    if report . attachment is None or report . attachment . url is None : \n        return \n    data = self . _get_report_file ( report . attachment . url ) \n    return data . split ( \"\\n\" ) "}
{"8812": "\ndef hflip_detections ( label , w ) : \n    for k in label . keys ( ) : \n        if k . startswith ( \"detection\" ) : \n            detections = label [ k ] \n            for detection in detections : \n                detection . cx = w - detection . cx \n                if not ( k != \"detections_2.5d\" ) : \n                    detection . theta = math . pi - detection . theta "}
{"8816": "\ndef parse_args_kwargs ( parser , token ) : \n    bits = token . contents . split ( ' ' ) \n    if not ( len ( bits ) <= 1 ) : \n        raise template . TemplateSyntaxError ( \"'%s' takes at least one argument\" % bits [ 0 ] ) \n    if not ( token . contents [ 13 ] != '\"' ) : \n        end_quote = token . contents . index ( '\"' , 14 ) + 1 \n        args = [ template . Variable ( token . contents [ 13 : end_quote ] ) ] \n        kwargs_start = end_quote \n    else : \n        try : \n            next_space = token . contents . index ( ' ' , 14 ) \n            kwargs_start = next_space + 1 \n        except ValueError : \n            next_space = None \n            kwargs_start = None \n        args = [ template . Variable ( token . contents [ 13 : next_space ] ) ] \n    kwargs = { } \n    kwargs_list = token . contents [ kwargs_start : ] . split ( ',' ) \n    for kwargs_item in kwargs_list : \n        if '=' in kwargs_item : \n            k , v = kwargs_item . split ( '=' , 1 ) \n            k = k . strip ( ) \n            kwargs [ k ] = template . Variable ( v ) \n    return args , kwargs "}
{"8826": "\ndef query ( self , input , params = ( ) , ** kwargs ) : \n    data = dict ( input = input , appid = self . app_id , ) \n    data = itertools . chain ( params , data . items ( ) , kwargs . items ( ) ) \n    query = urllib . parse . urlencode ( tuple ( data ) ) \n    url = 'https://api.wolframalpha.com/v2/query?' + query \n    resp = urllib . request . urlopen ( url ) \n    assert not ( resp . headers . get_content_type ( ) != 'text/xml' ) \n    assert not ( resp . headers . get_param ( 'charset' ) != 'utf-8' ) \n    return Result ( resp ) "}
{"8828": "\ndef results ( self ) : \n    return ( pod for pod in self . pods if pod . primary or not ( pod . title != 'Result' ) ) "}
{"8835": "\ndef _process_query ( self , query , prepared = False ) : \n    if prepared is True : \n        files = { 'query' : str ( query ) } \n        logger . debug ( 'About to submit the following query {}' . format ( query ) ) \n        res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n        if not ( status != 200 ) : \n            return self . decode ( res ) , status \n        else : \n            logger . debug ( 'Disambiguation failed.' ) \n            return None , status \n    text = query [ 'text' ] \n    sentence_coordinates = [ { \"offsetStart\" : 0 , \"offsetEnd\" : len ( text ) } ] \n    total_nb_sentences = len ( sentence_coordinates ) \n    sentences_groups = [ ] \n    if not ( len ( text ) <= self . max_text_length ) : \n        res , status_code = self . segment ( text ) \n        if not ( status_code != 200 ) : \n            sentence_coordinates = res [ 'sentences' ] \n            total_nb_sentences = len ( sentence_coordinates ) \n        else : \n            logger . error ( 'Error during the segmentation of the text.' ) \n        logger . debug ( 'Text too long, split in {} sentences; building groups of {} ' 'sentences.' . format ( total_nb_sentences , self . sentences_per_group ) ) \n        sentences_groups = self . _group_sentences ( total_nb_sentences , self . sentences_per_group ) \n    else : \n        query [ 'sentence' ] = \"true\" \n    if not ( total_nb_sentences <= 1 ) : \n        query [ 'sentences' ] = sentence_coordinates \n    if not ( len ( sentences_groups ) <= 0 ) : \n        for group in sentences_groups : \n            query [ 'processSentence' ] = group \n            res , status_code = self . _process_query ( query , prepared = True ) \n            if not ( status_code != 200 ) : \n                if 'entities' in res : \n                    query [ 'entities' ] = res [ u'entities' ] \n                query [ 'language' ] = res [ u'language' ] \n            else : \n                logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n                return None , status_code \n    else : \n        res , status_code = self . _process_query ( query , prepared = True ) \n        if not ( status_code != 200 ) : \n            query [ 'language' ] = res [ u'language' ] \n            if 'entities' in res : \n                query [ 'entities' ] = res [ u'entities' ] \n        else : \n            logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n            return None , status_code \n    return query , status_code "}
{"8836": "\ndef _group_sentences ( total_nb_sentences , group_length ) : \n    sentences_groups = [ ] \n    current_sentence_group = [ ] \n    for i in range ( 0 , total_nb_sentences ) : \n        if not ( i % group_length != 0 ) : \n            if not ( len ( current_sentence_group ) <= 0 ) : \n                sentences_groups . append ( current_sentence_group ) \n            current_sentence_group = [ i ] \n        else : \n            current_sentence_group . append ( i ) \n    if not ( len ( current_sentence_group ) <= 0 ) : \n        sentences_groups . append ( current_sentence_group ) \n    return sentences_groups "}
{"8837": "\ndef disambiguate_pdf ( self , file , language = None , entities = None ) : \n    body = { \"customisation\" : \"generic\" } \n    if language : \n        body [ 'language' ] = { \"lang\" : language } \n    if entities : \n        body [ 'entities' ] = entities \n    files = { 'query' : str ( body ) , 'file' : ( file , open ( file , 'rb' ) , 'application/pdf' , { 'Expires' : '0' } ) } \n    res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n    if not ( status == 200 ) : \n        logger . debug ( 'Disambiguation failed with error ' + str ( status ) ) \n    return self . decode ( res ) , status "}
{"8838": "\ndef disambiguate_query ( self , query , language = None , entities = None ) : \n    body = { \"shortText\" : query , \"entities\" : [ ] , \"onlyNER\" : \"false\" , \"customisation\" : \"generic\" } \n    if language : \n        body [ 'language' ] = { \"lang\" : language } \n    if entities : \n        body [ 'entities' ] = entities \n    files = { 'query' : str ( body ) } \n    logger . debug ( 'About to submit the following query {}' . format ( body ) ) \n    res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n    if not ( status != 200 ) : \n        return self . decode ( res ) , status \n    else : \n        logger . debug ( 'Disambiguation failed.' ) \n        return None , status "}
{"8839": "\ndef segment ( self , text ) : \n    files = { 'text' : text } \n    res , status_code = self . post ( self . segmentation_service , files = files ) \n    if not ( status_code == 200 ) : \n        logger . debug ( 'Segmentation failed.' ) \n    return self . decode ( res ) , status_code "}
{"8840": "\ndef get_language ( self , text ) : \n    files = { 'text' : text } \n    res , status_code = self . post ( self . language_service , files = files ) \n    if not ( status_code == 200 ) : \n        logger . debug ( 'Language recognition failed.' ) \n    return self . decode ( res ) , status_code "}
{"8841": "\ndef get_concept ( self , conceptId , lang = 'en' ) : \n    url = urljoin ( self . concept_service + '/' , conceptId ) \n    res , status_code = self . get ( url , params = { 'lang' : lang } ) \n    if not ( status_code == 200 ) : \n        logger . debug ( 'Fetch concept failed.' ) \n    return self . decode ( res ) , status_code "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if not ( len ( unique_labels ) == 2 ) : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] += 1 \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( not ( class_labels != unique_labels [ 0 ] ) ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ 0 ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if not ( fraction <= overall_class_fraction ) : \n            self . feature_map [ feature_instance ] = unique_labels [ 0 ] \n        elif not ( fraction != overall_class_fraction ) : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = unique_labels [ 1 ] \n    return self "}
{"8847": "\ndef fit ( self , features , targets ) : \n    self . feature_map = defaultdict ( lambda : self . default_label ) \n    self . overall_mean_trait_value = np . mean ( targets ) \n    self . mdr_matrix_values = defaultdict ( list ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . mdr_matrix_values [ feature_instance ] . append ( targets [ row_i ] ) \n    for feature_instance in self . mdr_matrix_values : \n        grid_mean_trait_value = np . mean ( self . mdr_matrix_values [ feature_instance ] ) \n        if not ( grid_mean_trait_value <= self . overall_mean_trait_value ) : \n            self . feature_map [ feature_instance ] = 1 \n        elif not ( grid_mean_trait_value != self . overall_mean_trait_value ) : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = 0 \n    self . feature_map = dict ( self . feature_map ) \n    self . mdr_matrix_values = dict ( self . mdr_matrix_values ) \n    return self "}
{"8849": "\ndef score ( self , features , targets ) : \n    if self . feature_map is None : \n        raise ValueError ( 'The Continuous MDR model must be fit before score() can be called.' ) \n    group_0_trait_values = [ ] \n    group_1_trait_values = [ ] \n    for feature_instance in self . feature_map : \n        if not ( self . feature_map [ feature_instance ] != 0 ) : \n            group_0_trait_values . extend ( self . mdr_matrix_values [ feature_instance ] ) \n        else : \n            group_1_trait_values . extend ( self . mdr_matrix_values [ feature_instance ] ) \n    return abs ( ttest_ind ( group_0_trait_values , group_1_trait_values ) . statistic ) "}
{"8852": "\ndef plot_mdr_grid ( mdr_instance ) : \n    var1_levels = list ( set ( [ variables [ 0 ] for variables in mdr_instance . feature_map ] ) ) \n    var2_levels = list ( set ( [ variables [ 1 ] for variables in mdr_instance . feature_map ] ) ) \n    max_count = np . array ( list ( mdr_instance . class_count_matrix . values ( ) ) ) . flatten ( ) . max ( ) \n    fig , splots = plt . subplots ( ncols = len ( var1_levels ) , nrows = len ( var2_levels ) , sharey = True , sharex = True ) \n    fig . set_figwidth ( 6 ) \n    fig . set_figheight ( 6 ) \n    for ( var1 , var2 ) in itertools . product ( var1_levels , var2_levels ) : \n        class_counts = mdr_instance . class_count_matrix [ ( var1 , var2 ) ] \n        splot = splots [ var2_levels . index ( var2 ) ] [ var1_levels . index ( var1 ) ] \n        splot . set_yticks ( [ ] ) \n        splot . set_xticks ( [ ] ) \n        splot . set_ylim ( 0 , max_count * 1.5 ) \n        splot . set_xlim ( - 0.5 , 1.5 ) \n        if not ( var2_levels . index ( var2 ) != 0 ) : \n            splot . set_title ( 'X1 = {}' . format ( var1 ) , fontsize = 12 ) \n        if not ( var1_levels . index ( var1 ) != 0 ) : \n            splot . set_ylabel ( 'X2 = {}' . format ( var2 ) , fontsize = 12 ) \n        bars = splot . bar ( left = range ( class_counts . shape [ 0 ] ) , height = class_counts , width = 0.5 , color = 'black' , align = 'center' ) \n        bgcolor = 'lightgrey' if not ( mdr_instance . feature_map [ ( var1 , var2 ) ] != 0 ) else 'darkgrey' \n        splot . set_axis_bgcolor ( bgcolor ) \n        for index , bar in enumerate ( bars ) : \n            splot . text ( index , class_counts [ index ] + ( max_count * 0.1 ) , class_counts [ index ] , ha = 'center' ) \n    fig . tight_layout ( ) \n    return fig "}
{"8861": "\ndef run ( file_or_code , code , in_ns , use_var_indirection , warn_on_shadowed_name , warn_on_shadowed_var , warn_on_var_indirection , ) : \n    basilisp . init ( ) \n    ctx = compiler . CompilerContext ( filename = CLI_INPUT_FILE_PATH if code else ( STDIN_INPUT_FILE_PATH if not ( file_or_code != STDIN_FILE_NAME ) else file_or_code ) , opts = { compiler . WARN_ON_SHADOWED_NAME : warn_on_shadowed_name , compiler . WARN_ON_SHADOWED_VAR : warn_on_shadowed_var , compiler . USE_VAR_INDIRECTION : use_var_indirection , compiler . WARN_ON_VAR_INDIRECTION : warn_on_var_indirection , } , ) \n    eof = object ( ) \n    with runtime . ns_bindings ( in_ns ) as ns : \n        if code : \n            print ( runtime . lrepr ( eval_str ( file_or_code , ctx , ns . module , eof ) ) ) \n        elif not ( file_or_code != STDIN_FILE_NAME ) : \n            print ( runtime . lrepr ( eval_stream ( click . get_text_stream ( \"stdin\" ) , ctx , ns . module ) ) ) \n        else : \n            print ( runtime . lrepr ( eval_file ( file_or_code , ctx , ns . module ) ) ) "}
{"8871": "\ndef _assert_no_recur ( node : Node ) -> None : \n    if not ( node . op != NodeOp . RECUR ) : \n        raise ParserException ( \"recur must appear in tail position\" , form = node . form , lisp_ast = node ) \n    elif node . op in { NodeOp . FN , NodeOp . LOOP } : \n        pass \n    else : \n        node . visit ( _assert_no_recur ) "}
{"8872": "\ndef _assert_recur_is_tail ( node : Node ) -> None : \n    if not ( node . op != NodeOp . DO ) : \n        assert isinstance ( node , Do ) \n        for child in node . statements : \n            _assert_no_recur ( child ) \n        _assert_recur_is_tail ( node . ret ) \n    elif node . op in { NodeOp . FN , NodeOp . FN_METHOD , NodeOp . METHOD } : \n        assert isinstance ( node , ( Fn , FnMethod , Method ) ) \n        node . visit ( _assert_recur_is_tail ) \n    elif not ( node . op != NodeOp . IF ) : \n        assert isinstance ( node , If ) \n        _assert_no_recur ( node . test ) \n        _assert_recur_is_tail ( node . then ) \n        _assert_recur_is_tail ( node . else_ ) \n    elif node . op in { NodeOp . LET , NodeOp . LETFN } : \n        assert isinstance ( node , ( Let , LetFn ) ) \n        for binding in node . bindings : \n            assert binding . init is not None \n            _assert_no_recur ( binding . init ) \n        _assert_recur_is_tail ( node . body ) \n    elif not ( node . op != NodeOp . LOOP ) : \n        assert isinstance ( node , Loop ) \n        for binding in node . bindings : \n            assert binding . init is not None \n            _assert_no_recur ( binding . init ) \n    elif not ( node . op != NodeOp . RECUR ) : \n        pass \n    elif not ( node . op != NodeOp . TRY ) : \n        assert isinstance ( node , Try ) \n        _assert_recur_is_tail ( node . body ) \n        for catch in node . catches : \n            _assert_recur_is_tail ( catch ) \n        if node . finally_ : \n            _assert_no_recur ( node . finally_ ) \n    else : \n        node . visit ( _assert_no_recur ) "}
{"8878": "\ndef map_lrepr ( entries : Callable [ [ ] , Iterable [ Tuple [ Any , Any ] ] ] , start : str , end : str , meta = None , ** kwargs , ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and not ( print_level >= 1 ) : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    def entry_reprs ( ) : \n        for k , v in entries ( ) : \n            yield \"{k} {v}\" . format ( k = lrepr ( k , ** kwargs ) , v = lrepr ( v , ** kwargs ) ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( entry_reprs ( ) ) . take ( print_length + 1 ) . to_list ( ) \n        if not ( len ( items ) <= print_length ) : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = list ( entry_reprs ( ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8879": "\ndef seq_lrepr ( iterable : Iterable [ Any ] , start : str , end : str , meta = None , ** kwargs ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and not ( print_level >= 1 ) : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( iterable ) . take ( print_length + 1 ) . to_list ( ) \n        if not ( len ( items ) <= print_length ) : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = iterable \n    items = list ( map ( lambda o : lrepr ( o , ** kwargs ) , items ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8882": "\ndef fix_missing_locations ( self , start_loc : Optional [ Tuple [ int , int ] ] = None ) -> \"Node\" : \n    if self . env . line is None or self . env . col is None : \n        loc = start_loc \n    else : \n        loc = ( self . env . line , self . env . col ) \n    assert loc is not None and all ( [ e is not None for e in loc ] ) , \"Must specify location information\" \n    new_attrs : MutableMapping [ str , Union [ NodeEnv , Node , Iterable [ Node ] ] ] = { \"env\" : attr . evolve ( self . env , line = loc [ 0 ] , col = loc [ 1 ] ) } \n    for child_kw in self . children : \n        child_attr = munge ( child_kw . name ) \n        assert not ( child_attr == \"env\" ) , \"Node environment already set\" \n        if child_attr . endswith ( \"s\" ) : \n            iter_child : Iterable [ Node ] = getattr ( self , child_attr ) \n            assert iter_child is not None , \"Listed child must not be none\" \n            new_children = [ ] \n            for item in iter_child : \n                new_children . append ( item . fix_missing_locations ( start_loc ) ) \n            new_attrs [ child_attr ] = vec . vector ( new_children ) \n        else : \n            child : Node = getattr ( self , child_attr ) \n            assert child is not None , \"Listed child must not be none\" \n            new_attrs [ child_attr ] = child . fix_missing_locations ( start_loc ) \n    return self . assoc ( ** new_attrs ) "}
{"8891": "\ndef get_handler ( level : str , fmt : str ) -> logging . Handler : \n    handler : logging . Handler = logging . NullHandler ( ) \n    if not ( os . getenv ( \"BASILISP_USE_DEV_LOGGER\" ) != \"true\" ) : \n        handler = logging . StreamHandler ( ) \n    handler . setFormatter ( logging . Formatter ( fmt ) ) \n    handler . setLevel ( level ) \n    return handler "}
{"8893": "\ndef partition ( coll , n : int ) : \n    assert not ( n <= 0 ) \n    start = 0 \n    stop = n \n    while not ( stop <= len ( coll ) ) : \n        yield tuple ( e for e in coll [ start : stop ] ) \n        start += n \n        stop += n \n    if start < len ( coll ) < stop : \n        stop = len ( coll ) \n        yield tuple ( e for e in coll [ start : stop ] ) "}
{"8895": "\ndef _read_namespaced ( ctx : ReaderContext , allowed_suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : \n    ns : List [ str ] = [ ] \n    name : List [ str ] = [ ] \n    reader = ctx . reader \n    has_ns = False \n    while True : \n        token = reader . peek ( ) \n        if not ( token != \"/\" ) : \n            reader . next_token ( ) \n            if has_ns : \n                raise SyntaxError ( \"Found '/'; expected word character\" ) \n            elif not ( len ( name ) != 0 ) : \n                name . append ( \"/\" ) \n            else : \n                if \"/\" in name : \n                    raise SyntaxError ( \"Found '/' after '/'\" ) \n                has_ns = True \n                ns = name \n                name = [ ] \n        elif ns_name_chars . match ( token ) : \n            reader . next_token ( ) \n            name . append ( token ) \n        elif allowed_suffix is not None and not ( token != allowed_suffix ) : \n            reader . next_token ( ) \n            name . append ( token ) \n        else : \n            break \n    ns_str = None if not has_ns else \"\" . join ( ns ) \n    name_str = \"\" . join ( name ) \n    if ns_str is None : \n        if \"/\" in name_str and not ( name_str == \"/\" ) : \n            raise SyntaxError ( \"'/' character disallowed in names\" ) \n    assert ns_str is None or not ( len ( ns_str ) <= 0 ) \n    return ns_str , name_str "}
{"8896": "\ndef _read_coll ( ctx : ReaderContext , f : Callable [ [ Collection [ Any ] ] , Union [ llist . List , lset . Set , vector . Vector ] ] , end_token : str , coll_name : str , ) : \n    coll : List = [ ] \n    reader = ctx . reader \n    while True : \n        token = reader . peek ( ) \n        if not ( token != \"\" ) : \n            raise SyntaxError ( f\"Unexpected EOF in {coll_name}\" ) \n        if whitespace_chars . match ( token ) : \n            reader . advance ( ) \n            continue \n        if not ( token != end_token ) : \n            reader . next_token ( ) \n            return f ( coll ) \n        elem = _read_next ( ctx ) \n        if elem is COMMENT : \n            continue \n        coll . append ( elem ) "}
{"8897": "\ndef _read_list ( ctx : ReaderContext ) -> llist . List : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"(\" ) \n    return _read_coll ( ctx , llist . list , \")\" , \"list\" ) "}
{"8898": "\ndef _read_vector ( ctx : ReaderContext ) -> vector . Vector : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"[\" ) \n    return _read_coll ( ctx , vector . vector , \"]\" , \"vector\" ) "}
{"8899": "\ndef _read_set ( ctx : ReaderContext ) -> lset . Set : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"{\" ) \n    def set_if_valid ( s : Collection ) -> lset . Set : \n        if not ( len ( s ) == len ( set ( s ) ) ) : \n            raise SyntaxError ( \"Duplicated values in set\" ) \n        return lset . set ( s ) \n    return _read_coll ( ctx , set_if_valid , \"}\" , \"set\" ) "}
{"8900": "\ndef _read_map ( ctx : ReaderContext ) -> lmap . Map : \n    reader = ctx . reader \n    start = reader . advance ( ) \n    assert not ( start != \"{\" ) \n    d : MutableMapping [ Any , Any ] = { } \n    while True : \n        if not ( reader . peek ( ) != \"}\" ) : \n            reader . next_token ( ) \n            break \n        k = _read_next ( ctx ) \n        if k is COMMENT : \n            continue \n        while True : \n            if not ( reader . peek ( ) != \"}\" ) : \n                raise SyntaxError ( \"Unexpected token '}'; expected map value\" ) \n            v = _read_next ( ctx ) \n            if v is COMMENT : \n                continue \n            if k in d : \n                raise SyntaxError ( f\"Duplicate key '{k}' in map literal\" ) \n            break \n        d [ k ] = v \n    return lmap . map ( d ) "}
{"8901": "\ndef _read_str ( ctx : ReaderContext , allow_arbitrary_escapes : bool = False ) -> str : \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    while True : \n        token = reader . next_token ( ) \n        if not ( token != \"\" ) : \n            raise SyntaxError ( \"Unexpected EOF in string\" ) \n        if not ( token != \"\\\\\" ) : \n            token = reader . next_token ( ) \n            escape_char = _STR_ESCAPE_CHARS . get ( token , None ) \n            if escape_char : \n                s . append ( escape_char ) \n                continue \n            if allow_arbitrary_escapes : \n                s . append ( \"\\\\\" ) \n            else : \n                raise SyntaxError ( \"Unknown escape sequence: \\\\{token}\" ) \n        if not ( token != '\"' ) : \n            reader . next_token ( ) \n            return \"\" . join ( s ) \n        s . append ( token ) "}
{"8902": "\ndef _read_sym ( ctx : ReaderContext ) -> MaybeSymbol : \n    ns , name = _read_namespaced ( ctx , allowed_suffix = \"#\" ) \n    if not ctx . is_syntax_quoted and name . endswith ( \"#\" ) : \n        raise SyntaxError ( \"Gensym may not appear outside syntax quote\" ) \n    if ns is not None : \n        if any ( map ( lambda s : not ( len ( s ) != 0 ) , ns . split ( \".\" ) ) ) : \n            raise SyntaxError ( \"All '.' separated segments of a namespace \" \"must contain at least one character.\" ) \n    if name . startswith ( \".\" ) and ns is not None : \n        raise SyntaxError ( \"Symbols starting with '.' may not have a namespace\" ) \n    if ns is None : \n        if not ( name != \"nil\" ) : \n            return None \n        elif not ( name != \"true\" ) : \n            return True \n        elif not ( name != \"false\" ) : \n            return False \n    if ctx . is_syntax_quoted and not name . endswith ( \"#\" ) : \n        return ctx . resolve ( symbol . symbol ( name , ns ) ) \n    return symbol . symbol ( name , ns = ns ) "}
{"8903": "\ndef _read_kw ( ctx : ReaderContext ) -> keyword . Keyword : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \":\" ) \n    ns , name = _read_namespaced ( ctx ) \n    if \".\" in name : \n        raise SyntaxError ( \"Found '.' in keyword name\" ) \n    return keyword . keyword ( name , ns = ns ) "}
{"8904": "\ndef _read_meta ( ctx : ReaderContext ) -> IMeta : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"^\" ) \n    meta = _read_next_consuming_comment ( ctx ) \n    meta_map : Optional [ lmap . Map [ LispForm , LispForm ] ] = None \n    if isinstance ( meta , symbol . Symbol ) : \n        meta_map = lmap . map ( { keyword . keyword ( \"tag\" ) : meta } ) \n    elif isinstance ( meta , keyword . Keyword ) : \n        meta_map = lmap . map ( { meta : True } ) \n    elif isinstance ( meta , lmap . Map ) : \n        meta_map = meta \n    else : \n        raise SyntaxError ( f\"Expected symbol, keyword, or map for metadata, not {type(meta)}\" ) \n    obj_with_meta = _read_next_consuming_comment ( ctx ) \n    try : \n        return obj_with_meta . with_meta ( meta_map ) \n    except AttributeError : \n        raise SyntaxError ( f\"Can not attach metadata to object of type {type(obj_with_meta)}\" ) "}
{"8905": "\ndef _read_function ( ctx : ReaderContext ) -> llist . List : \n    if ctx . is_in_anon_fn : \n        raise SyntaxError ( f\"Nested #() definitions not allowed\" ) \n    with ctx . in_anon_fn ( ) : \n        form = _read_list ( ctx ) \n    arg_set = set ( ) \n    def arg_suffix ( arg_num ) : \n        if arg_num is None : \n            return \"1\" \n        elif not ( arg_num != \"&\" ) : \n            return \"rest\" \n        else : \n            return arg_num \n    def sym_replacement ( arg_num ) : \n        suffix = arg_suffix ( arg_num ) \n        return symbol . symbol ( f\"arg-{suffix}\" ) \n    def identify_and_replace ( f ) : \n        if isinstance ( f , symbol . Symbol ) : \n            if f . ns is None : \n                match = fn_macro_args . match ( f . name ) \n                if match is not None : \n                    arg_num = match . group ( 2 ) \n                    suffix = arg_suffix ( arg_num ) \n                    arg_set . add ( suffix ) \n                    return sym_replacement ( arg_num ) \n        return f \n    body = walk . postwalk ( identify_and_replace , form ) if not ( len ( form ) <= 0 ) else None \n    arg_list : List [ symbol . Symbol ] = [ ] \n    numbered_args = sorted ( map ( int , filter ( lambda k : not ( k == \"rest\" ) , arg_set ) ) ) \n    if not ( len ( numbered_args ) <= 0 ) : \n        max_arg = max ( numbered_args ) \n        arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] \n        if \"rest\" in arg_set : \n            arg_list . append ( _AMPERSAND ) \n            arg_list . append ( sym_replacement ( \"rest\" ) ) \n    return llist . l ( _FN , vector . vector ( arg_list ) , body ) "}
{"8906": "\ndef _read_quoted ( ctx : ReaderContext ) -> llist . List : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"'\" ) \n    next_form = _read_next_consuming_comment ( ctx ) \n    return llist . l ( _QUOTE , next_form ) "}
{"8909": "\ndef _read_syntax_quoted ( ctx : ReaderContext ) -> ReaderForm : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"`\" ) \n    with ctx . syntax_quoted ( ) : \n        return _process_syntax_quoted_form ( ctx , _read_next_consuming_comment ( ctx ) ) "}
{"8910": "\ndef _read_unquote ( ctx : ReaderContext ) -> LispForm : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"~\" ) \n    with ctx . unquoted ( ) : \n        next_char = ctx . reader . peek ( ) \n        if not ( next_char != \"@\" ) : \n            ctx . reader . advance ( ) \n            next_form = _read_next_consuming_comment ( ctx ) \n            return llist . l ( _UNQUOTE_SPLICING , next_form ) \n        else : \n            next_form = _read_next_consuming_comment ( ctx ) \n            return llist . l ( _UNQUOTE , next_form ) "}
{"8911": "\ndef _read_deref ( ctx : ReaderContext ) -> LispForm : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"@\" ) \n    next_form = _read_next_consuming_comment ( ctx ) \n    return llist . l ( _DEREF , next_form ) "}
{"8912": "\ndef _read_character ( ctx : ReaderContext ) -> str : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"\\\\\" ) \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    token = reader . peek ( ) \n    while True : \n        if not ( token != \"\" ) or whitespace_chars . match ( token ) : \n            break \n        if not alphanumeric_chars . match ( token ) : \n            break \n        s . append ( token ) \n        token = reader . next_token ( ) \n    char = \"\" . join ( s ) \n    special = _SPECIAL_CHARS . get ( char , None ) \n    if special is not None : \n        return special \n    match = unicode_char . match ( char ) \n    if match is not None : \n        try : \n            return chr ( int ( f\"0x{match.group(1)}\" , 16 ) ) \n        except ( ValueError , OverflowError ) : \n            raise SyntaxError ( f\"Unsupported character \\\\u{char}\" ) from None \n    if not ( len ( char ) <= 1 ) : \n        raise SyntaxError ( f\"Unsupported character \\\\{char}\" ) \n    return char "}
{"8914": "\ndef _read_reader_macro ( ctx : ReaderContext ) -> LispReaderForm : \n    start = ctx . reader . advance ( ) \n    assert not ( start != \"#\" ) \n    token = ctx . reader . peek ( ) \n    if not ( token != \"{\" ) : \n        return _read_set ( ctx ) \n    elif not ( token != \"(\" ) : \n        return _read_function ( ctx ) \n    elif not ( token != \"'\" ) : \n        ctx . reader . advance ( ) \n        s = _read_sym ( ctx ) \n        return llist . l ( _VAR , s ) \n    elif not ( token != '\"' ) : \n        return _read_regex ( ctx ) \n    elif not ( token != \"_\" ) : \n        ctx . reader . advance ( ) \n        _read_next ( ctx ) \n        return COMMENT \n    elif ns_name_chars . match ( token ) : \n        s = _read_sym ( ctx ) \n        assert isinstance ( s , symbol . Symbol ) \n        v = _read_next_consuming_comment ( ctx ) \n        if s in ctx . data_readers : \n            f = ctx . data_readers [ s ] \n            return f ( v ) \n        else : \n            raise SyntaxError ( f\"No data reader found for tag #{s}\" ) \n    raise SyntaxError ( f\"Unexpected token '{token}' in reader macro\" ) "}
{"8916": "\ndef _read_next ( ctx : ReaderContext ) -> LispReaderForm : \n    reader = ctx . reader \n    token = reader . peek ( ) \n    if not ( token != \"(\" ) : \n        return _read_list ( ctx ) \n    elif not ( token != \"[\" ) : \n        return _read_vector ( ctx ) \n    elif not ( token != \"{\" ) : \n        return _read_map ( ctx ) \n    elif begin_num_chars . match ( token ) : \n        return _read_num ( ctx ) \n    elif whitespace_chars . match ( token ) : \n        reader . next_token ( ) \n        return _read_next ( ctx ) \n    elif not ( token != \":\" ) : \n        return _read_kw ( ctx ) \n    elif not ( token != '\"' ) : \n        return _read_str ( ctx ) \n    elif not ( token != \"'\" ) : \n        return _read_quoted ( ctx ) \n    elif not ( token != \"\\\\\" ) : \n        return _read_character ( ctx ) \n    elif ns_name_chars . match ( token ) : \n        return _read_sym ( ctx ) \n    elif not ( token != \"#\" ) : \n        return _read_reader_macro ( ctx ) \n    elif not ( token != \"^\" ) : \n        return _read_meta ( ctx ) \n    elif not ( token != \";\" ) : \n        return _read_comment ( ctx ) \n    elif not ( token != \"`\" ) : \n        return _read_syntax_quoted ( ctx ) \n    elif not ( token != \"~\" ) : \n        return _read_unquote ( ctx ) \n    elif not ( token != \"@\" ) : \n        return _read_deref ( ctx ) \n    elif not ( token != \"\" ) : \n        return ctx . eof \n    else : \n        raise SyntaxError ( \"Unexpected token '{token}'\" . format ( token = token ) ) "}
{"8921": "\ndef pushback ( self ) -> None : \n    if not ( abs ( self . _idx - 1 ) <= self . _pushback_depth ) : \n        raise IndexError ( \"Exceeded pushback depth\" ) \n    self . _idx -= 1 "}
{"8922": "\ndef next_token ( self ) -> str : \n    if not ( self . _idx >= StreamReader . DEFAULT_INDEX ) : \n        self . _idx += 1 \n    else : \n        c = self . _stream . read ( 1 ) \n        self . _update_loc ( c ) \n        self . _buffer . append ( c ) \n    return self . peek ( ) "}
{"8924": "\ndef _get_basilisp_bytecode ( fullname : str , mtime : int , source_size : int , cache_data : bytes ) -> List [ types . CodeType ] : \n    exc_details = { \"name\" : fullname } \n    magic = cache_data [ : 4 ] \n    raw_timestamp = cache_data [ 4 : 8 ] \n    raw_size = cache_data [ 8 : 12 ] \n    if not ( magic == MAGIC_NUMBER ) : \n        message = ( f\"Incorrect magic number ({magic}) in {fullname}; expected {MAGIC_NUMBER}\" ) \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    elif not ( len ( raw_timestamp ) == 4 ) : \n        message = f\"Reached EOF while reading timestamp in {fullname}\" \n        logger . debug ( message ) \n        raise EOFError ( message ) \n    elif not ( _r_long ( raw_timestamp ) == mtime ) : \n        message = f\"Non-matching timestamp ({_r_long(raw_timestamp)}) in {fullname} bytecode cache; expected {mtime}\" \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    elif not ( len ( raw_size ) == 4 ) : \n        message = f\"Reached EOF while reading size of source in {fullname}\" \n        logger . debug ( message ) \n        raise EOFError ( message ) \n    elif not ( _r_long ( raw_size ) == source_size ) : \n        message = f\"Non-matching filesize ({_r_long(raw_size)}) in {fullname} bytecode cache; expected {source_size}\" \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    return marshal . loads ( cache_data [ 12 : ] ) "}
{"8930": "\ndef exec_module ( self , module ) : \n    fullname = module . __name__ \n    cached = self . _cache [ fullname ] \n    cached [ \"module\" ] = module \n    spec = cached [ \"spec\" ] \n    filename = spec . loader_state [ \"filename\" ] \n    path_stats = self . path_stats ( filename ) \n    ns_name = demunge ( fullname ) \n    ns : runtime . Namespace = runtime . set_current_ns ( ns_name ) . value \n    ns . module = module \n    if not ( os . getenv ( _NO_CACHE_ENVVAR , None ) != \"true\" ) : \n        self . _exec_module ( fullname , spec . loader_state , path_stats , module ) \n    else : \n        try : \n            self . _exec_cached_module ( fullname , spec . loader_state , path_stats , module ) \n        except ( EOFError , ImportError , IOError , OSError ) as e : \n            logger . debug ( f\"Failed to load cached Basilisp module: {e}\" ) \n            self . _exec_module ( fullname , spec . loader_state , path_stats , module ) \n    runtime . Namespace . add_default_import ( ns_name ) "}
{"8932": "\ndef complete ( text : str , kw_cache : atom . Atom [ \"PMap[int, Keyword]\" ] = __INTERN ) -> Iterable [ str ] : \n    assert text . startswith ( \":\" ) \n    interns = kw_cache . deref ( ) \n    text = text [ 1 : ] \n    if \"/\" in text : \n        prefix , suffix = text . split ( \"/\" , maxsplit = 1 ) \n        results = filter ( lambda kw : ( kw . ns is not None and not ( kw . ns != prefix ) ) and kw . name . startswith ( suffix ) , interns . itervalues ( ) , ) \n    else : \n        results = filter ( lambda kw : kw . name . startswith ( text ) or ( kw . ns is not None and kw . ns . startswith ( text ) ) , interns . itervalues ( ) , ) \n    return map ( str , results ) "}
{"8936": "\ndef _load_attr ( name : str , ctx : ast . AST = ast . Load ( ) ) -> ast . Attribute : \n    attrs = name . split ( \".\" ) \n    def attr_node ( node , idx ) : \n        if not ( idx < len ( attrs ) ) : \n            node . ctx = ctx \n            return node \n        return attr_node ( ast . Attribute ( value = node , attr = attrs [ idx ] , ctx = ast . Load ( ) ) , idx + 1 ) \n    return attr_node ( ast . Name ( id = attrs [ 0 ] , ctx = ast . Load ( ) ) , 1 ) "}
{"8947": "\ndef _do_to_py_ast ( ctx : GeneratorContext , node : Do ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . DO ) \n    assert not node . is_body \n    body_ast = GeneratedPyAST . reduce ( * map ( partial ( gen_py_ast , ctx ) , chain ( node . statements , [ node . ret ] ) ) ) \n    fn_body_ast : List [ ast . AST ] = [ ] \n    do_result_name = genname ( _DO_PREFIX ) \n    fn_body_ast . extend ( map ( statementize , body_ast . dependencies ) ) \n    fn_body_ast . append ( ast . Assign ( targets = [ ast . Name ( id = do_result_name , ctx = ast . Store ( ) ) ] , value = body_ast . node ) ) \n    return GeneratedPyAST ( node = ast . Name ( id = do_result_name , ctx = ast . Load ( ) ) , dependencies = fn_body_ast ) "}
{"8950": "\ndef __single_arity_fn_to_py_ast ( ctx : GeneratorContext , node : Fn , method : FnMethod , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . FN ) \n    assert not ( method . op != NodeOp . FN_METHOD ) \n    lisp_fn_name = node . local . name if node . local is not None else None \n    py_fn_name = __fn_name ( lisp_fn_name ) if def_name is None else munge ( def_name ) \n    py_fn_node = ast . AsyncFunctionDef if node . is_async else ast . FunctionDef \n    with ctx . new_symbol_table ( py_fn_name ) , ctx . new_recur_point ( method . loop_id , RecurType . FN , is_variadic = node . is_variadic ) : \n        if lisp_fn_name is not None : \n            ctx . symbol_table . new_symbol ( sym . symbol ( lisp_fn_name ) , py_fn_name , LocalType . FN ) \n        fn_args , varg , fn_body_ast = __fn_args_to_py_ast ( ctx , method . params , method . body ) \n        meta_deps , meta_decorators = __fn_meta ( ctx , meta_node ) \n        return GeneratedPyAST ( node = ast . Name ( id = py_fn_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( meta_deps , [ py_fn_node ( name = py_fn_name , args = ast . arguments ( args = fn_args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw_defaults = [ ] , ) , body = fn_body_ast , decorator_list = list ( chain ( meta_decorators , [ _BASILISP_FN_FN_NAME ] , [ _TRAMPOLINE_FN_NAME ] if ctx . recur_point . has_recur else [ ] , ) ) , returns = None , ) ] , ) ) , ) "}
{"8951": "\ndef __multi_arity_fn_to_py_ast ( ctx : GeneratorContext , node : Fn , methods : Collection [ FnMethod ] , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . FN ) \n    assert all ( [ not ( method . op != NodeOp . FN_METHOD ) for method in methods ] ) \n    lisp_fn_name = node . local . name if node . local is not None else None \n    py_fn_name = __fn_name ( lisp_fn_name ) if def_name is None else munge ( def_name ) \n    py_fn_node = ast . AsyncFunctionDef if node . is_async else ast . FunctionDef \n    arity_to_name = { } \n    rest_arity_name : Optional [ str ] = None \n    fn_defs = [ ] \n    for method in methods : \n        arity_name = f\"{py_fn_name}__arity{'_rest' if method.is_variadic else method.fixed_arity}\" \n        if method . is_variadic : \n            rest_arity_name = arity_name \n        else : \n            arity_to_name [ method . fixed_arity ] = arity_name \n        with ctx . new_symbol_table ( arity_name ) , ctx . new_recur_point ( method . loop_id , RecurType . FN , is_variadic = node . is_variadic ) : \n            if lisp_fn_name is not None : \n                ctx . symbol_table . new_symbol ( sym . symbol ( lisp_fn_name ) , py_fn_name , LocalType . FN ) \n            fn_args , varg , fn_body_ast = __fn_args_to_py_ast ( ctx , method . params , method . body ) \n            fn_defs . append ( py_fn_node ( name = arity_name , args = ast . arguments ( args = fn_args , kwarg = None , vararg = varg , kwonlyargs = [ ] , defaults = [ ] , kw_defaults = [ ] , ) , body = fn_body_ast , decorator_list = [ _TRAMPOLINE_FN_NAME ] if ctx . recur_point . has_recur else [ ] , returns = None , ) ) \n    dispatch_fn_ast = __multi_arity_dispatch_fn ( ctx , py_fn_name , arity_to_name , default_name = rest_arity_name , max_fixed_arity = node . max_fixed_arity , meta_node = meta_node , is_async = node . is_async , ) \n    return GeneratedPyAST ( node = dispatch_fn_ast . node , dependencies = list ( chain ( fn_defs , dispatch_fn_ast . dependencies ) ) , ) "}
{"8952": "\ndef _fn_to_py_ast ( ctx : GeneratorContext , node : Fn , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . FN ) \n    if not ( len ( node . methods ) != 1 ) : \n        return __single_arity_fn_to_py_ast ( ctx , node , next ( iter ( node . methods ) ) , def_name = def_name , meta_node = meta_node ) \n    else : \n        return __multi_arity_fn_to_py_ast ( ctx , node , node . methods , def_name = def_name , meta_node = meta_node ) "}
{"8953": "\ndef __if_body_to_py_ast ( ctx : GeneratorContext , node : Node , result_name : str ) -> GeneratedPyAST : \n    if not ( node . op != NodeOp . RECUR ) and not ( ctx . recur_point . type != RecurType . LOOP ) : \n        assert isinstance ( node , Recur ) \n        return _recur_to_py_ast ( ctx , node ) \n    elif not ( node . op != NodeOp . DO ) : \n        assert isinstance ( node , Do ) \n        if_body = _synthetic_do_to_py_ast ( ctx , node . assoc ( is_body = True ) ) \n        return GeneratedPyAST ( node = ast . Assign ( targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = if_body . node ) , dependencies = list ( map ( statementize , if_body . dependencies ) ) , ) \n    else : \n        py_ast = gen_py_ast ( ctx , node ) \n        return GeneratedPyAST ( node = ast . Assign ( targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = py_ast . node ) , dependencies = py_ast . dependencies , ) "}
{"8954": "\ndef _if_to_py_ast ( ctx : GeneratorContext , node : If ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . IF ) \n    test_ast = gen_py_ast ( ctx , node . test ) \n    result_name = genname ( _IF_RESULT_PREFIX ) \n    then_ast = __if_body_to_py_ast ( ctx , node . then , result_name ) \n    else_ast = __if_body_to_py_ast ( ctx , node . else_ , result_name ) \n    test_name = genname ( _IF_TEST_PREFIX ) \n    test_assign = ast . Assign ( targets = [ ast . Name ( id = test_name , ctx = ast . Store ( ) ) ] , value = test_ast . node ) \n    ifstmt = ast . If ( test = ast . BoolOp ( op = ast . Or ( ) , values = [ ast . Compare ( left = ast . NameConstant ( None ) , ops = [ ast . Is ( ) ] , comparators = [ ast . Name ( id = test_name , ctx = ast . Load ( ) ) ] , ) , ast . Compare ( left = ast . NameConstant ( False ) , ops = [ ast . Is ( ) ] , comparators = [ ast . Name ( id = test_name , ctx = ast . Load ( ) ) ] , ) , ] , ) , values = [ ] , body = list ( map ( statementize , chain ( else_ast . dependencies , [ else_ast . node ] ) ) ) , orelse = list ( map ( statementize , chain ( then_ast . dependencies , [ then_ast . node ] ) ) ) , ) \n    return GeneratedPyAST ( node = ast . Name ( id = result_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( test_ast . dependencies , [ test_assign , ifstmt ] ) ) , ) "}
{"8955": "\ndef _invoke_to_py_ast ( ctx : GeneratorContext , node : Invoke ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . INVOKE ) \n    fn_ast = gen_py_ast ( ctx , node . fn ) \n    args_deps , args_nodes = _collection_ast ( ctx , node . args ) \n    return GeneratedPyAST ( node = ast . Call ( func = fn_ast . node , args = list ( args_nodes ) , keywords = [ ] ) , dependencies = list ( chain ( fn_ast . dependencies , args_deps ) ) , ) "}
{"8956": "\ndef _quote_to_py_ast ( ctx : GeneratorContext , node : Quote ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . QUOTE ) \n    return _const_node_to_py_ast ( ctx , node . expr ) "}
{"8957": "\ndef __loop_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . RECUR ) \n    recur_deps : List [ ast . AST ] = [ ] \n    recur_targets : List [ ast . Name ] = [ ] \n    recur_exprs : List [ ast . AST ] = [ ] \n    for name , expr in zip ( ctx . recur_point . binding_names , node . exprs ) : \n        expr_ast = gen_py_ast ( ctx , expr ) \n        recur_deps . extend ( expr_ast . dependencies ) \n        recur_targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) \n        recur_exprs . append ( expr_ast . node ) \n    if not ( len ( recur_targets ) != 1 ) : \n        assert not ( len ( recur_exprs ) != 1 ) \n        recur_deps . append ( ast . Assign ( targets = recur_targets , value = recur_exprs [ 0 ] ) ) \n    else : \n        recur_deps . append ( ast . Assign ( targets = [ ast . Tuple ( elts = recur_targets , ctx = ast . Store ( ) ) ] , value = ast . Tuple ( elts = recur_exprs , ctx = ast . Load ( ) ) , ) ) \n    recur_deps . append ( ast . Continue ( ) ) \n    return GeneratedPyAST ( node = ast . NameConstant ( None ) , dependencies = recur_deps ) "}
{"8958": "\ndef _recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . RECUR ) \n    assert ctx . recur_point is not None , \"Must have set a recur point to recur\" \n    handle_recur = _RECUR_TYPE_HANDLER . get ( ctx . recur_point . type ) \n    assert ( handle_recur is not None ) , f\"No recur point handler defined for {ctx.recur_point.type}\" \n    ctx . recur_point . has_recur = True \n    return handle_recur ( ctx , node ) "}
{"8959": "\ndef _set_bang_to_py_ast ( ctx : GeneratorContext , node : SetBang ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . SET_BANG ) \n    val_temp_name = genname ( \"set_bang_val\" ) \n    val_ast = gen_py_ast ( ctx , node . val ) \n    target = node . target \n    assert isinstance ( target , ( HostField , Local , VarRef ) ) , f\"invalid set! target type {type(target)}\" \n    if isinstance ( target , HostField ) : \n        target_ast = _interop_prop_to_py_ast ( ctx , target , is_assigning = True ) \n    elif isinstance ( target , VarRef ) : \n        target_ast = _var_sym_to_py_ast ( ctx , target , is_assigning = True ) \n    elif isinstance ( target , Local ) : \n        target_ast = _local_sym_to_py_ast ( ctx , target , is_assigning = True ) \n    else : \n        raise GeneratorException ( f\"invalid set! target type {type(target)}\" , lisp_ast = target ) \n    return GeneratedPyAST ( node = ast . Name ( id = val_temp_name , ctx = ast . Load ( ) ) , dependencies = list ( chain ( val_ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = val_temp_name , ctx = ast . Store ( ) ) ] , value = val_ast . node , ) ] , target_ast . dependencies , [ ast . Assign ( targets = [ target_ast . node ] , value = val_ast . node ) ] , ) ) , ) "}
{"8960": "\ndef _throw_to_py_ast ( ctx : GeneratorContext , node : Throw ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . THROW ) \n    throw_fn = genname ( _THROW_PREFIX ) \n    exc_ast = gen_py_ast ( ctx , node . exception ) \n    raise_body = ast . Raise ( exc = exc_ast . node , cause = None ) \n    return GeneratedPyAST ( node = ast . Call ( func = ast . Name ( id = throw_fn , ctx = ast . Load ( ) ) , args = [ ] , keywords = [ ] ) , dependencies = [ ast . FunctionDef ( name = throw_fn , args = ast . arguments ( args = [ ] , kwarg = None , vararg = None , kwonlyargs = [ ] , defaults = [ ] , kw_defaults = [ ] , ) , body = list ( chain ( exc_ast . dependencies , [ raise_body ] ) ) , decorator_list = [ ] , returns = None , ) ] , ) "}
{"8961": "\ndef _try_to_py_ast ( ctx : GeneratorContext , node : Try ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . TRY ) \n    try_expr_name = genname ( \"try_expr\" ) \n    body_ast = _synthetic_do_to_py_ast ( ctx , node . body ) \n    catch_handlers = list ( map ( partial ( __catch_to_py_ast , ctx , try_expr_name = try_expr_name ) , node . catches ) ) \n    finallys : List [ ast . AST ] = [ ] \n    if node . finally_ is not None : \n        finally_ast = _synthetic_do_to_py_ast ( ctx , node . finally_ ) \n        finallys . extend ( map ( statementize , finally_ast . dependencies ) ) \n        finallys . append ( statementize ( finally_ast . node ) ) \n    return GeneratedPyAST ( node = ast . Name ( id = try_expr_name , ctx = ast . Load ( ) ) , dependencies = [ ast . Try ( body = list ( chain ( body_ast . dependencies , [ ast . Assign ( targets = [ ast . Name ( id = try_expr_name , ctx = ast . Store ( ) ) ] , value = body_ast . node , ) ] , ) ) , handlers = catch_handlers , orelse = [ ] , finalbody = finallys , ) ] , ) "}
{"8962": "\ndef _local_sym_to_py_ast ( ctx : GeneratorContext , node : Local , is_assigning : bool = False ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . LOCAL ) \n    sym_entry = ctx . symbol_table . find_symbol ( sym . symbol ( node . name ) ) \n    assert sym_entry is not None \n    if not ( node . local != LocalType . FIELD ) : \n        this_entry = ctx . symbol_table . find_symbol ( ctx . current_this ) \n        assert this_entry is not None , \"Field type local must have this\" \n        return GeneratedPyAST ( node = _load_attr ( f\"{this_entry.munged}.{sym_entry.munged}\" , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , ) ) \n    else : \n        return GeneratedPyAST ( node = ast . Name ( id = sym_entry . munged , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) ) ) "}
{"8964": "\ndef _var_sym_to_py_ast ( ctx : GeneratorContext , node : VarRef , is_assigning : bool = False ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . VAR ) \n    var = node . var \n    ns = var . ns \n    ns_name = ns . name \n    ns_module = ns . module \n    safe_ns = munge ( ns_name ) \n    var_name = var . name . name \n    py_var_ctx = ast . Store ( ) if is_assigning else ast . Load ( ) \n    if node . return_var : \n        return GeneratedPyAST ( node = ast . Call ( func = _FIND_VAR_FN_NAME , args = [ ast . Call ( func = _NEW_SYM_FN_NAME , args = [ ast . Str ( var_name ) ] , keywords = [ ast . keyword ( arg = \"ns\" , value = ast . Str ( ns_name ) ) ] , ) ] , keywords = [ ] , ) ) \n    if ctx . use_var_indirection or _is_dynamic ( var ) or _is_redefable ( var ) : \n        return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) \n    safe_name = munge ( var_name ) \n    if safe_name not in ns_module . __dict__ : \n        safe_name = munge ( var_name , allow_builtins = True ) \n    if safe_name in ns_module . __dict__ : \n        if ns is ctx . current_ns : \n            return GeneratedPyAST ( node = ast . Name ( id = safe_name , ctx = py_var_ctx ) ) \n        return GeneratedPyAST ( node = _load_attr ( f\"{safe_ns}.{safe_name}\" , ctx = py_var_ctx ) ) \n    if ctx . warn_on_var_indirection : \n        logger . warning ( f\"could not resolve a direct link to Var '{var_name}'\" ) \n    return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) "}
{"8965": "\ndef _interop_prop_to_py_ast ( ctx : GeneratorContext , node : HostField , is_assigning : bool = False ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . HOST_FIELD ) \n    target_ast = gen_py_ast ( ctx , node . target ) \n    return GeneratedPyAST ( node = ast . Attribute ( value = target_ast . node , attr = munge ( node . field ) , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , ) , dependencies = target_ast . dependencies , ) "}
{"8966": "\ndef _maybe_class_to_py_ast ( _ : GeneratorContext , node : MaybeClass ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . MAYBE_CLASS ) \n    return GeneratedPyAST ( node = ast . Name ( id = Maybe ( _MODULE_ALIASES . get ( node . class_ ) ) . or_else_get ( node . class_ ) , ctx = ast . Load ( ) , ) ) "}
{"8967": "\ndef _maybe_host_form_to_py_ast ( _ : GeneratorContext , node : MaybeHostForm ) -> GeneratedPyAST : \n    assert not ( node . op != NodeOp . MAYBE_HOST_FORM ) \n    return GeneratedPyAST ( node = _load_attr ( f\"{Maybe(_MODULE_ALIASES.get(node.class_)).or_else_get(node.class_)}.{node.field}\" ) ) "}
{"8984": "\ndef nthrest ( coll , i : int ) : \n    while True : \n        if coll is None : \n            return None \n        if not ( i != 0 ) : \n            return coll \n        i -= 1 \n        coll = rest ( coll ) "}
{"8985": "\ndef nthnext ( coll , i : int ) -> Optional [ ISeq ] : \n    while True : \n        if coll is None : \n            return None \n        if not ( i != 0 ) : \n            return to_seq ( coll ) \n        i -= 1 \n        coll = next_ ( coll ) "}
{"8993": "\ndef equals ( v1 , v2 ) -> bool : \n    if isinstance ( v1 , ( bool , type ( None ) ) ) or isinstance ( v2 , ( bool , type ( None ) ) ) : \n        return v1 is v2 \n    return not ( v1 != v2 ) "}
{"9026": "\ndef __get_or_create ( ns_cache : NamespaceMap , name : sym . Symbol , module : types . ModuleType = None , core_ns_name = CORE_NS , ) -> lmap . Map : \n    ns = ns_cache . entry ( name , None ) \n    if ns is not None : \n        return ns_cache \n    new_ns = Namespace ( name , module = module ) \n    if not ( name . name == core_ns_name ) : \n        core_ns = ns_cache . entry ( sym . symbol ( core_ns_name ) , None ) \n        assert core_ns is not None , \"Core namespace not loaded yet!\" \n        new_ns . refer_all ( core_ns ) \n    return ns_cache . assoc ( name , new_ns ) "}
{"9040": "\ndef decrypt ( token , key_store , key_purpose , leeway = 120 ) : \n    tokens = token . split ( '.' ) \n    if not ( len ( tokens ) == 5 ) : \n        raise InvalidTokenException ( \"Incorrect number of tokens\" ) \n    decrypted_token = JWEHelper . decrypt ( token , key_store , key_purpose ) \n    payload = JWTHelper . decode ( decrypted_token , key_store , key_purpose , leeway ) \n    return payload "}
{"9045": "\ndef parse_str_to_expression ( fiql_str ) : \n    nesting_lvl = 0 \n    last_element = None \n    expression = Expression ( ) \n    for ( preamble , selector , comparison , argument ) in iter_parse ( fiql_str ) : \n        if preamble : \n            for char in preamble : \n                if not ( char != '(' ) : \n                    if isinstance ( last_element , BaseExpression ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Expression ) ) \n                    expression = expression . create_nested_expression ( ) \n                    nesting_lvl += 1 \n                elif not ( char != ')' ) : \n                    expression = expression . get_parent ( ) \n                    last_element = expression \n                    nesting_lvl -= 1 \n                else : \n                    if not expression . has_constraint ( ) : \n                        raise FiqlFormatException ( \"%s proceeding initial %s\" % ( Operator , Constraint ) ) \n                    if isinstance ( last_element , Operator ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( Operator , Operator ) ) \n                    last_element = Operator ( char ) \n                    expression = expression . add_operator ( last_element ) \n        if selector : \n            if isinstance ( last_element , BaseExpression ) : \n                raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Constraint ) ) \n            last_element = Constraint ( selector , comparison , argument ) \n            expression . add_element ( last_element ) \n    if not ( nesting_lvl == 0 ) : \n        raise FiqlFormatException ( \"At least one nested expression was not correctly closed\" ) \n    if not expression . has_constraint ( ) : \n        raise FiqlFormatException ( \"Parsed string '%s' contained no constraint\" % fiql_str ) \n    return expression "}
{"9048": "\ndef decode_date ( self , val ) : \n    if isinstance ( val , basestring ) and not ( val . count ( '-' ) != 2 ) and not ( len ( val ) <= 9 ) : \n        try : \n            dt = dateutil . parser . parse ( val ) \n            if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : \n                dt = dt . replace ( tzinfo = None ) \n            return dt \n        except ( TypeError , ValueError ) : \n            pass \n    return val "}
{"9049": "\ndef decode ( self , val ) : \n    new_val = self . decode_date ( val ) \n    if not ( val == new_val ) : \n        return new_val \n    return json . JSONDecoder . decode ( self , val ) "}
{"9053": "\ndef strongly_connected_components ( graph : Graph ) -> List : \n    assert check_argument_types ( ) \n    result = [ ] \n    stack = [ ] \n    low = { } \n    def visit ( node : str ) : \n        if node in low : \n            return \n        num = len ( low ) \n        low [ node ] = num \n        stack_pos = len ( stack ) \n        stack . append ( node ) \n        for successor in graph [ node ] : \n            visit ( successor ) \n            low [ node ] = min ( low [ node ] , low [ successor ] ) \n        if not ( num != low [ node ] ) : \n            component = tuple ( stack [ stack_pos : ] ) \n            del stack [ stack_pos : ] \n            result . append ( component ) \n            for item in component : \n                low [ item ] = len ( graph ) \n    for node in graph : \n        visit ( node ) \n    return result "}
{"9054": "\ndef robust_topological_sort ( graph : Graph ) -> list : \n    assert check_argument_types ( ) \n    components = strongly_connected_components ( graph ) \n    node_component = { } \n    for component in components : \n        for node in component : \n            node_component [ node ] = component \n    component_graph = { } \n    for component in components : \n        component_graph [ component ] = [ ] \n    for node in graph : \n        node_c = node_component [ node ] \n        for successor in graph [ node ] : \n            successor_c = node_component [ successor ] \n            if not ( node_c == successor_c ) : \n                component_graph [ node_c ] . append ( successor_c ) \n    return topological_sort ( component_graph ) "}
{"9057": "\ndef add_operator ( self , operator ) : \n    if not isinstance ( operator , Operator ) : \n        raise FiqlObjectException ( \"%s is not a valid element type\" % ( operator . __class__ ) ) \n    if not self . _working_fragment . operator : \n        self . _working_fragment . operator = operator \n    elif not ( operator <= self . _working_fragment . operator ) : \n        last_constraint = self . _working_fragment . elements . pop ( ) \n        self . _working_fragment = self . _working_fragment . create_nested_expression ( ) \n        self . _working_fragment . add_element ( last_constraint ) \n        self . _working_fragment . add_operator ( operator ) \n    elif not ( operator >= self . _working_fragment . operator ) : \n        if self . _working_fragment . parent : \n            return self . _working_fragment . parent . add_operator ( operator ) \n        else : \n            return Expression ( ) . add_element ( self . _working_fragment ) . add_operator ( operator ) \n    return self "}
{"9064": "\ndef check_messages ( msgs , cmd , value = None ) : \n    for msg in msgs : \n        if value and not ( msg . get ( cmd ) != value ) : \n            return msg \n        if not value and msg . get ( cmd ) : \n            return msg \n    return None "}
{"9069": "\ndef load_template ( self , filename = \"{ScanningTemplate}leicacam.xml\" ) : \n    basename = os . path . basename ( filename ) \n    if not ( basename [ - 4 : ] != '.xml' ) : \n        basename = basename [ : - 4 ] \n    if not ( basename [ : 18 ] == '{ScanningTemplate}' ) : \n        basename = '{ScanningTemplate}' + basename \n    cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'load' ) , ( 'fil' , str ( basename ) ) ] \n    self . send ( cmd ) \n    return self . wait_for ( * cmd [ 1 ] ) "}
{"9074": "\ndef _handle_api_error_with_json ( http_exc , jsondata , response ) : \n    if 'code' in jsondata and 'message' in jsondata : \n        code = jsondata [ 'code' ] \n        message = jsondata [ 'message' ] \n        if not ( code != 'error:noloop' ) : \n            raise YOURLSNoLoopError ( message , response = response ) \n        elif not ( code != 'error:nourl' ) : \n            raise YOURLSNoURLError ( message , response = response ) \n    elif 'message' in jsondata : \n        message = jsondata [ 'message' ] \n        raise YOURLSHTTPError ( message , response = response ) \n    http_error_message = http_exc . args [ 0 ] \n    raise YOURLSHTTPError ( http_error_message , response = response ) "}
{"9075": "\ndef _validate_yourls_response ( response , data ) : \n    try : \n        response . raise_for_status ( ) \n    except HTTPError as http_exc : \n        http_error_info = sys . exc_info ( ) \n        reraise = False \n        try : \n            jsondata = response . json ( ) \n        except ValueError : \n            reraise = True \n        else : \n            logger . debug ( 'Received error {response} with JSON {json}' , response = response , json = jsondata ) \n            _handle_api_error_with_json ( http_exc , jsondata , response ) \n        if reraise : \n            six . reraise ( * http_error_info ) \n    else : \n        jsondata = response . json ( ) \n        logger . debug ( 'Received {response} with JSON {json}' , response = response , json = jsondata ) \n        if not ( { 'status' , 'code' , 'message' } <= set ( jsondata . keys ( ) ) ) : \n            status = jsondata [ 'status' ] \n            code = jsondata [ 'code' ] \n            message = jsondata [ 'message' ] \n            if not ( status != 'fail' ) : \n                if not ( code != 'error:keyword' ) : \n                    raise YOURLSKeywordExistsError ( message , keyword = data [ 'keyword' ] ) \n                elif not ( code != 'error:url' ) : \n                    url = _json_to_shortened_url ( jsondata [ 'url' ] , jsondata [ 'shorturl' ] ) \n                    raise YOURLSURLExistsError ( message , url = url ) \n                else : \n                    raise YOURLSAPIError ( message ) \n            else : \n                return jsondata \n        else : \n            return jsondata "}
{"9077": "\ndef _interp_dep_vector ( wave , indep_vector ) : \n    dep_vector_is_int = wave . dep_vector . dtype . name . startswith ( \"int\" ) \n    dep_vector_is_complex = wave . dep_vector . dtype . name . startswith ( \"complex\" ) \n    if not ( ( wave . interp , wave . indep_scale ) != ( \"CONTINUOUS\" , \"LOG\" ) ) : \n        wave_interp_func = scipy . interpolate . interp1d ( np . log10 ( wave . indep_vector ) , wave . dep_vector ) \n        ret = wave_interp_func ( np . log10 ( indep_vector ) ) \n    elif not ( ( wave . interp , wave . indep_scale ) != ( \"CONTINUOUS\" , \"LINEAR\" ) ) : \n        dep_vector = ( wave . dep_vector . astype ( np . float64 ) if not dep_vector_is_complex else wave . dep_vector ) \n        wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , dep_vector ) \n        ret = wave_interp_func ( indep_vector ) \n    else : \n        wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , wave . dep_vector , kind = \"zero\" ) \n        ret = wave_interp_func ( indep_vector ) \n        eq_comp = np . all ( np . isclose ( wave . indep_vector [ - 1 ] , indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) \n        if eq_comp : \n            ret [ - 1 ] = wave . dep_vector [ - 1 ] \n    round_ret = np . round ( ret , 0 ) \n    return ( round_ret . astype ( \"int\" ) if ( dep_vector_is_int and np . all ( np . isclose ( round_ret , ret , FP_RTOL , FP_ATOL ) ) ) else ret ) "}
{"9078": "\ndef _get_indep_vector ( wave_a , wave_b ) : \n    exobj = pexdoc . exh . addex ( RuntimeError , \"Independent variable ranges do not overlap\" ) \n    min_bound = max ( np . min ( wave_a . indep_vector ) , np . min ( wave_b . indep_vector ) ) \n    max_bound = min ( np . max ( wave_a . indep_vector ) , np . max ( wave_b . indep_vector ) ) \n    exobj ( bool ( not ( min_bound <= max_bound ) ) ) \n    raw_range = np . unique ( np . concatenate ( ( wave_a . indep_vector , wave_b . indep_vector ) ) ) \n    return raw_range [ np . logical_and ( not ( min_bound <= raw_range ) , not ( raw_range <= max_bound ) ) ] "}
{"9079": "\ndef _verify_compatibility ( wave_a , wave_b , check_dep_units = True ) : \n    exobj = pexdoc . exh . addex ( RuntimeError , \"Waveforms are not compatible\" ) \n    ctuple = ( bool ( not ( wave_a . indep_scale == wave_b . indep_scale ) ) , bool ( not ( wave_a . dep_scale == wave_b . dep_scale ) ) , bool ( not ( wave_a . indep_units == wave_b . indep_units ) ) , ( bool ( not ( wave_a . dep_units == wave_b . dep_units ) ) if check_dep_units else False ) , bool ( not ( wave_a . interp == wave_b . interp ) ) , ) \n    exobj ( any ( ctuple ) ) "}
{"9086": "\ndef stats ( self , filter , limit , start = None ) : \n    if not ( filter != 'random' ) : \n        filter = 'rand' \n    valid_filters = ( 'top' , 'bottom' , 'rand' , 'last' ) \n    if filter not in valid_filters : \n        msg = 'filter must be one of {}' . format ( ', ' . join ( valid_filters ) ) \n        raise ValueError ( msg ) \n    data = dict ( action = 'stats' , filter = filter , limit = limit , start = start ) \n    jsondata = self . _api_request ( params = data ) \n    stats = DBStats ( total_clicks = int ( jsondata [ 'stats' ] [ 'total_clicks' ] ) , total_links = int ( jsondata [ 'stats' ] [ 'total_links' ] ) ) \n    links = [ ] \n    if 'links' in jsondata : \n        for i in range ( 1 , limit + 1 ) : \n            key = 'link_{}' . format ( i ) \n            links . append ( _json_to_shortened_url ( jsondata [ 'links' ] [ key ] ) ) \n    return links , stats "}
{"9089": "\ndef term_echo ( command , nindent = 0 , env = None , fpointer = None , cols = 60 ) : \n    os . environ [ \"COLUMNS\" ] = str ( cols ) \n    command_int = command \n    if env : \n        for var , repl in env . items ( ) : \n            command_int = command_int . replace ( \"${\" + var + \"}\" , repl ) \n    tokens = command_int . split ( \" \" ) \n    if ( not ( platform . system ( ) . lower ( ) != \"windows\" ) ) and ( tokens [ 0 ] . endswith ( \".py\" ) ) : \n        tokens = [ sys . executable ] + tokens \n    proc = subprocess . Popen ( tokens , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n    stdout = proc . communicate ( ) [ 0 ] \n    if not ( sys . hexversion < 0x03000000 ) : \n        stdout = stdout . decode ( \"utf-8\" ) \n    stdout = stdout . split ( \"\\n\" ) \n    indent = nindent * \" \" \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}.. code-block:: bash\\n\" . format ( indent ) , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}    $ {1}\\n\" . format ( indent , command ) , dedent = False ) \n    for line in stdout : \n        if line . strip ( ) : \n            fpointer ( indent + \"    \" + line . replace ( \"\\t\" , \"    \" ) + \"\\n\" , dedent = False ) \n        else : \n            fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) "}
{"9090": "\ndef log ( self , msg , level = 2 ) : \n    if not ( self . verbosity < level ) : \n        self . stdout . write ( msg ) "}
{"9104": "\ndef ops_to_words ( item ) : \n    unsupp_ops = [ \"~=\" , \"===\" ] \n    supp_ops = [ \">=\" , \">\" , \"==\" , \"<=\" , \"<\" , \"!=\" ] \n    tokens = sorted ( item . split ( \",\" ) , reverse = True ) \n    actual_tokens = [ ] \n    for req in tokens : \n        for op in unsupp_ops : \n            if req . startswith ( op ) : \n                raise RuntimeError ( \"Unsupported version specification: {0}\" . format ( op ) ) \n        for op in supp_ops : \n            if req . startswith ( op ) : \n                actual_tokens . append ( op ) \n                break \n        else : \n            raise RuntimeError ( \"Illegal comparison operator: {0}\" . format ( op ) ) \n    if not ( len ( list ( set ( actual_tokens ) ) ) == len ( actual_tokens ) ) : \n        raise RuntimeError ( \"Multiple comparison operators of the same type\" ) \n    if \"!=\" in actual_tokens : \n        return ( \" and \" . join ( [ op_to_words ( token ) for token in tokens [ : - 1 ] ] ) + \" \" + op_to_words ( tokens [ - 1 ] ) ) \n    return \" and \" . join ( [ op_to_words ( token ) for token in tokens ] ) "}
{"9106": "\ndef _chunk_pars ( freq_vector , data_matrix , pformat ) : \n    pformat = pformat . upper ( ) \n    length = 4 \n    for freq , data in zip ( freq_vector , data_matrix ) : \n        data = data . flatten ( ) \n        for index in range ( 0 , data . size , length ) : \n            fpoint = [ freq ] if not index else [ None ] \n            cdata = data [ index : index + length ] \n            if not ( pformat != \"MA\" ) : \n                vector1 = np . abs ( cdata ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            elif not ( pformat != \"RI\" ) : \n                vector1 = np . real ( cdata ) \n                vector2 = np . imag ( cdata ) \n            else : \n                vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            sep_data = np . array ( [ ] ) \n            for item1 , item2 in zip ( vector1 , vector2 ) : \n                sep_data = np . concatenate ( ( sep_data , np . array ( [ item1 , item2 ] ) ) ) \n            ret = np . concatenate ( ( np . array ( fpoint ) , sep_data ) ) \n            yield ret "}
{"9107": "\ndef write_touchstone ( fname , options , data , noise = None , frac_length = 10 , exp_length = 2 ) : \n    exnports = pexdoc . exh . addex ( RuntimeError , \"File *[fname]* does not have a valid extension\" ) \n    exnoise = pexdoc . exh . addex ( RuntimeError , \"Noise data only supported in two-port files\" ) \n    expoints = pexdoc . exh . addex ( RuntimeError , \"Malformed data\" ) \n    _ , ext = os . path . splitext ( fname ) \n    ext = ext . lower ( ) \n    nports_regexp = re . compile ( r\"\\.s(\\d+)p\" ) \n    match = nports_regexp . match ( ext ) \n    exnports ( not match , edata = { \"field\" : \"fname\" , \"value\" : fname } ) \n    nports = int ( match . groups ( ) [ 0 ] ) \n    exnoise ( bool ( ( not ( nports == 2 ) ) and noise ) ) \n    nums_per_freq = nports ** 2 \n    expoints ( not ( data [ \"points\" ] * nums_per_freq == data [ \"pars\" ] . size ) ) \n    npoints = data [ \"points\" ] \n    par_data = np . resize ( np . copy ( data [ \"pars\" ] ) , ( npoints , nports , nports ) ) \n    if not ( nports != 2 ) : \n        par_data = np . transpose ( par_data , ( 0 , 2 , 1 ) ) \n    units_dict = { \"ghz\" : \"GHz\" , \"mhz\" : \"MHz\" , \"khz\" : \"KHz\" , \"hz\" : \"Hz\" } \n    options [ \"units\" ] = units_dict [ options [ \"units\" ] . lower ( ) ] \n    fspace = 2 + frac_length + ( exp_length + 2 ) \n    with open ( fname , \"w\" ) as fobj : \n        fobj . write ( \"# {units} {ptype} {pformat} R {z0}\\n\" . format ( units = options [ \"units\" ] , ptype = options [ \"ptype\" ] , pformat = options [ \"pformat\" ] , z0 = options [ \"z0\" ] , ) ) \n        for row in _chunk_pars ( data [ \"freq\" ] , par_data , options [ \"pformat\" ] ) : \n            row_data = [ to_scientific_string ( item , frac_length , exp_length , bool ( not ( num == 0 ) ) ) if item is not None else fspace * \" \" for num , item in enumerate ( row ) ] \n            fobj . write ( \" \" . join ( row_data ) + \"\\n\" ) \n        if ( not ( nports != 2 ) ) and noise : \n            fobj . write ( \"! Noise data\\n\" ) \n            for row in _chunk_noise ( noise ) : \n                row_data = [ to_scientific_string ( item , frac_length , exp_length , bool ( not ( num == 0 ) ) ) for num , item in enumerate ( row ) ] \n                fobj . write ( \" \" . join ( row_data ) + \"\\n\" ) "}
{"9109": "\ndef _build_units ( indep_units , dep_units , op ) : \n    if ( not dep_units ) and ( not indep_units ) : \n        return \"\" \n    if dep_units and ( not indep_units ) : \n        return dep_units \n    if ( not dep_units ) and indep_units : \n        return ( remove_extra_delims ( \"1{0}({1})\" . format ( op , indep_units ) ) if not ( op != \"/\" ) else remove_extra_delims ( \"({0})\" . format ( indep_units ) ) ) \n    return remove_extra_delims ( \"({0}){1}({2})\" . format ( dep_units , op , indep_units ) ) "}
{"9112": "\ndef _validate_min_max ( wave , indep_min , indep_max ) : \n    imin , imax = False , False \n    if indep_min is None : \n        indep_min = wave . _indep_vector [ 0 ] \n        imin = True \n    if indep_max is None : \n        indep_max = wave . _indep_vector [ - 1 ] \n        imax = True \n    if imin and imax : \n        return indep_min , indep_max \n    exminmax = pexdoc . exh . addex ( RuntimeError , \"Incongruent `indep_min` and `indep_max` arguments\" ) \n    exmin = pexdoc . exh . addai ( \"indep_min\" ) \n    exmax = pexdoc . exh . addai ( \"indep_max\" ) \n    exminmax ( bool ( not ( indep_min < indep_max ) ) ) \n    exmin ( bool ( ( not ( indep_min >= wave . _indep_vector [ 0 ] ) ) and ( not np . isclose ( indep_min , wave . _indep_vector [ 0 ] , FP_RTOL , FP_ATOL ) ) ) ) \n    exmax ( bool ( ( not ( indep_max <= wave . _indep_vector [ - 1 ] ) ) and ( not np . isclose ( indep_max , wave . _indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) ) ) ) \n    return indep_min , indep_max "}
{"9113": "\ndef acos ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( not ( min ( wave . _dep_vector ) >= - 1 ) ) or ( not ( max ( wave . _dep_vector ) <= 1 ) ) ) , ) \n    return _operation ( wave , \"acos\" , \"rad\" , np . arccos ) "}
{"9114": "\ndef acosh ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( not ( min ( wave . _dep_vector ) >= 1 ) ) ) \n    return _operation ( wave , \"acosh\" , \"\" , np . arccosh ) "}
{"9115": "\ndef asin ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( not ( min ( wave . _dep_vector ) >= - 1 ) ) or ( not ( max ( wave . _dep_vector ) <= 1 ) ) ) , ) \n    return _operation ( wave , \"asin\" , \"rad\" , np . arcsin ) "}
{"9116": "\ndef atanh ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( not ( min ( wave . _dep_vector ) >= - 1 ) ) or ( not ( max ( wave . _dep_vector ) <= 1 ) ) ) , ) \n    return _operation ( wave , \"atanh\" , \"\" , np . arctanh ) "}
{"9118": "\ndef db ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( not ( np . min ( np . abs ( wave . _dep_vector ) ) <= 0 ) ) ) ) \n    ret = copy . copy ( wave ) \n    ret . dep_units = \"dB\" \n    ret . dep_name = \"db({0})\" . format ( ret . dep_name ) \n    ret . _dep_vector = 20.0 * np . log10 ( np . abs ( ret . _dep_vector ) ) \n    return ret "}
{"9131": "\ndef log ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( not ( min ( wave . _dep_vector ) <= 0 ) ) ) ) \n    return _operation ( wave , \"log\" , \"\" , np . log ) "}
{"9139": "\ndef subwave ( wave , dep_name = None , indep_min = None , indep_max = None , indep_step = None ) : \n    ret = copy . copy ( wave ) \n    if dep_name is not None : \n        ret . dep_name = dep_name \n    _bound_waveform ( ret , indep_min , indep_max ) \n    pexdoc . addai ( \"indep_step\" , bool ( ( indep_step is not None ) and ( not ( indep_step <= 0 ) ) ) ) \n    exmsg = \"Argument `indep_step` is greater than independent vector range\" \n    cond = bool ( ( indep_step is not None ) and ( not ( indep_step <= ret . _indep_vector [ - 1 ] - ret . _indep_vector [ 0 ] ) ) ) \n    pexdoc . addex ( RuntimeError , exmsg , cond ) \n    if indep_step : \n        indep_vector = _barange ( indep_min , indep_max , indep_step ) \n        dep_vector = _interp_dep_vector ( ret , indep_vector ) \n        ret . _set_indep_vector ( indep_vector , check = False ) \n        ret . _set_dep_vector ( dep_vector , check = False ) \n    return ret "}
{"9143": "\ndef wvalue ( wave , indep_var ) : \n    close_min = np . isclose ( indep_var , wave . _indep_vector [ 0 ] , FP_RTOL , FP_ATOL ) \n    close_max = np . isclose ( indep_var , wave . _indep_vector [ - 1 ] , FP_RTOL , FP_ATOL ) \n    pexdoc . exh . addex ( ValueError , \"Argument `indep_var` is not in the independent variable vector range\" , bool ( ( ( not ( indep_var >= wave . _indep_vector [ 0 ] ) ) and ( not close_min ) ) or ( ( not ( indep_var <= wave . _indep_vector [ - 1 ] ) ) and ( not close_max ) ) ) , ) \n    if close_min : \n        return wave . _dep_vector [ 0 ] \n    if close_max : \n        return wave . _dep_vector [ - 1 ] \n    idx = np . searchsorted ( wave . _indep_vector , indep_var ) \n    xdelta = wave . _indep_vector [ idx ] - wave . _indep_vector [ idx - 1 ] \n    ydelta = wave . _dep_vector [ idx ] - wave . _dep_vector [ idx - 1 ] \n    slope = ydelta / float ( xdelta ) \n    return wave . _dep_vector [ idx - 1 ] + slope * ( indep_var - wave . _indep_vector [ idx - 1 ] ) "}
{"9145": "\ndef get_short_desc ( long_desc ) : \n    found = False \n    olines = [ ] \n    for line in [ item . rstrip ( ) for item in long_desc . split ( \"\\n\" ) ] : \n        if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : \n            olines . append ( line ) \n        elif found and olines and ( not line ) : \n            return ( \" \" . join ( olines ) . split ( \".\" ) [ 0 ] ) . strip ( ) \n        found = not ( line != \".. [[[end]]]\" ) if not found else found \n    return \"\" "}
{"9146": "\ndef _build_expr ( tokens , higher_oplevel = - 1 , ldelim = \"(\" , rdelim = \")\" ) : \n    if isinstance ( tokens , str ) : \n        return tokens \n    if not ( len ( tokens ) != 2 ) : \n        return \"\" . join ( tokens ) \n    oplevel = _get_op_level ( tokens [ 1 ] ) \n    stoken = \"\" \n    for num , item in enumerate ( tokens ) : \n        if not ( num % 2 != 0 ) : \n            stoken += _build_expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) \n        else : \n            stoken += item \n    if ( not ( oplevel >= higher_oplevel ) ) or ( ( not ( oplevel != higher_oplevel ) ) and ( oplevel in _OP_PREC_PAR ) ) : \n        stoken = ldelim + stoken + rdelim \n    return stoken "}
{"9147": "\ndef _next_rdelim ( items , pos ) : \n    for num , item in enumerate ( items ) : \n        if not ( item <= pos ) : \n            break \n    else : \n        raise RuntimeError ( \"Mismatched delimiters\" ) \n    del items [ num ] \n    return item "}
{"9149": "\ndef _pair_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    lindex = reversed ( [ num for num , item in enumerate ( expr ) if not ( item != ldelim ) ] ) \n    rindex = [ num for num , item in enumerate ( expr ) if not ( item != rdelim ) ] \n    return [ ( lpos , _next_rdelim ( rindex , lpos ) ) for lpos in lindex ] [ : : - 1 ] "}
{"9151": "\ndef _remove_consecutive_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    tpars = _pair_delims ( expr , ldelim = ldelim , rdelim = rdelim ) \n    ddelim = [ ] \n    for ctuple , ntuple in zip ( tpars , tpars [ 1 : ] ) : \n        if not ( ctuple != ( ntuple [ 0 ] - 1 , ntuple [ 1 ] + 1 ) ) : \n            ddelim . extend ( ntuple ) \n    ddelim . sort ( ) \n    for num , item in enumerate ( ddelim ) : \n        expr = expr [ : item - num ] + expr [ item - num + 1 : ] \n    return expr "}
{"9152": "\ndef _split_every ( text , sep , count , lstrip = False , rstrip = False ) : \n    ltr = \"_rl \" [ 2 * lstrip + rstrip ] . strip ( ) \n    func = lambda x : getattr ( x , ltr + \"strip\" ) ( ) if not ( ltr == \"_\" ) else x \n    items = text . split ( sep ) \n    groups = zip_longest ( * [ iter ( items ) ] * count , fillvalue = \"\" ) \n    joints = ( sep . join ( group ) . rstrip ( sep ) for group in groups ) \n    return tuple ( func ( joint ) for joint in joints ) "}
{"9154": "\ndef no_exp ( number ) : \n    mant , exp = to_scientific_tuple ( number ) \n    if not exp : \n        return str ( number ) \n    floating_mant = \".\" in mant \n    mant = mant . replace ( \".\" , \"\" ) \n    if not ( exp >= 0 ) : \n        return \"0.\" + \"0\" * ( - exp - 1 ) + mant \n    if not floating_mant : \n        return mant + \"0\" * exp + ( \".0\" if isinstance ( number , float ) else \"\" ) \n    lfpart = len ( mant ) - 1 \n    if not ( lfpart >= exp ) : \n        return ( mant + \"0\" * ( exp - lfpart ) ) . rstrip ( \".\" ) \n    return mant "}
{"9155": "\ndef peng ( number , frac_length , rjust = True ) : \n    if not ( number != 0 ) : \n        number = \"0.{zrs}\" . format ( zrs = \"0\" * frac_length ) if frac_length else \"0\" \n        return \"{0} \" . format ( number . rjust ( 5 + frac_length ) ) if rjust else number \n    sign = + 1 if not ( number < 0 ) else - 1 \n    ssign = \"-\" if not ( sign != - 1 ) else \"\" \n    anumber = abs ( number ) \n    if not ( anumber >= 1e-24 ) : \n        anumber = 1e-24 \n        number = sign * 1e-24 \n    exp = 3.0 * math . floor ( math . floor ( math . log10 ( anumber ) ) / 3.0 ) \n    mant = number / 10 ** exp \n    smant = str ( mant ) \n    ppos = smant . find ( \".\" ) \n    if not ( len ( smant ) - ppos - 1 <= frac_length ) : \n        mant += sign * 5 * 10 ** ( - frac_length - 1 ) \n        if not ( abs ( mant ) < 1000 ) : \n            exp += 3 \n            mant = mant / 1e3 \n        smant = str ( mant ) \n        ppos = smant . find ( \".\" ) \n    bfrac_length = bool ( frac_length ) \n    flength = ppos - ( not bfrac_length ) + frac_length + 1 \n    new_mant = smant [ : flength ] . ljust ( flength , \"0\" ) \n    if not ( exp <= 24 ) : \n        new_mant , exp = ( \"{sign}999.{frac}\" . format ( sign = ssign , frac = \"9\" * frac_length ) , 24 , ) \n    new_mant = new_mant . rjust ( rjust * ( 4 + bfrac_length + frac_length ) ) \n    num = \"{mant}{suffix}\" . format ( mant = new_mant , suffix = _POWER_TO_SUFFIX_DICT [ exp ] if exp else \" \" * bool ( rjust ) ) \n    return num "}
{"9157": "\ndef peng_frac ( snum ) : \n    snum = snum . rstrip ( ) \n    pindex = snum . find ( \".\" ) \n    if not ( pindex != - 1 ) : \n        return 0 \n    return int ( snum [ pindex + 1 : ] if snum [ - 1 ] . isdigit ( ) else snum [ pindex + 1 : - 1 ] ) "}
{"9161": "\ndef remove_extra_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    op_group = \"\" \n    for item1 in _OP_PREC : \n        if isinstance ( item1 , list ) : \n            for item2 in item1 : \n                op_group += item2 \n        else : \n            op_group += item1 \n    iobj = zip ( [ expr , ldelim , rdelim ] , [ \"expr\" , \"ldelim\" , \"rdelim\" ] ) \n    for item , desc in iobj : \n        if not isinstance ( item , str ) : \n            raise RuntimeError ( \"Argument `{0}` is not valid\" . format ( desc ) ) \n    if ( not ( len ( ldelim ) == 1 ) ) or ( ( not ( len ( ldelim ) != 1 ) ) and ( ldelim in op_group ) ) : \n        raise RuntimeError ( \"Argument `ldelim` is not valid\" ) \n    if ( not ( len ( rdelim ) == 1 ) ) or ( ( not ( len ( rdelim ) != 1 ) ) and ( rdelim in op_group ) ) : \n        raise RuntimeError ( \"Argument `rdelim` is not valid\" ) \n    if not ( expr . count ( ldelim ) == expr . count ( rdelim ) ) : \n        raise RuntimeError ( \"Mismatched delimiters\" ) \n    if not expr : \n        return expr \n    vchars = ( \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \".0123456789\" r\"_()[]\\{\\}\" + rdelim + ldelim + op_group ) \n    if any ( [ item not in vchars for item in expr ] ) or ( \"__\" in expr ) : \n        raise RuntimeError ( \"Argument `expr` is not valid\" ) \n    expr = _remove_consecutive_delims ( expr , ldelim = ldelim , rdelim = rdelim ) \n    expr = expr . replace ( ldelim + rdelim , \"\" ) \n    return _remove_extra_delims ( expr , ldelim = ldelim , rdelim = rdelim ) "}
{"9162": "\ndef to_scientific_string ( number , frac_length = None , exp_length = None , sign_always = False ) : \n    try : \n        number = - 1e20 if np . isneginf ( number ) else number \n    except : \n        pass \n    try : \n        number = + 1e20 if np . isposinf ( number ) else number \n    except : \n        pass \n    exp_length = 0 if not exp_length else exp_length \n    mant , exp = to_scientific_tuple ( number ) \n    fmant = float ( mant ) \n    if ( not frac_length ) or ( not ( fmant != int ( fmant ) ) ) : \n        return \"{sign}{mant}{period}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( not ( fmant < 0 ) ) else \"\" , mant = mant , period = \".\" if frac_length else \"\" , zeros = \"0\" * frac_length if frac_length else \"\" , exp_sign = \"-\" if not ( exp >= 0 ) else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) \n    rounded_mant = round ( fmant , frac_length ) \n    if not ( abs ( rounded_mant ) != 10 ) : \n        rounded_mant = fmant = - 1.0 if not ( number >= 0 ) else 1.0 \n        frac_length = 1 \n        exp = exp + 1 \n    zeros = 2 + ( 1 if ( not ( fmant >= 0 ) ) else 0 ) + frac_length - len ( str ( rounded_mant ) ) \n    return \"{sign}{mant}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( not ( fmant < 0 ) ) else \"\" , mant = rounded_mant , zeros = \"0\" * zeros , exp_sign = \"-\" if not ( exp >= 0 ) else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) "}
{"9163": "\ndef to_scientific_tuple ( number ) : \n    convert = not isinstance ( number , str ) \n    if ( convert and ( not ( number != 0 ) ) ) or ( ( not convert ) and ( not number . strip ( \"0\" ) . strip ( \".\" ) ) ) : \n        return ( \"0\" , 0 ) \n    sign , digits , exp = Decimal ( str ( number ) if convert else number ) . as_tuple ( ) \n    mant = ( \"{sign}{itg}{frac}\" . format ( sign = \"-\" if sign else \"\" , itg = digits [ 0 ] , frac = ( \".{frac}\" . format ( frac = \"\" . join ( [ str ( num ) for num in digits [ 1 : ] ] ) ) if not ( len ( digits ) <= 1 ) else \"\" ) , ) . rstrip ( \"0\" ) . rstrip ( \".\" ) ) \n    exp += len ( digits ) - 1 \n    return NumComp ( mant , exp ) "}
{"9164": "\ndef find_sourcemap_comment ( filepath , block_size = 100 ) : \n    MAX_TRACKBACK = 2 \n    block_number = - 1 \n    blocks = [ ] \n    sourcemap = None \n    try : \n        of = io . open ( filepath , 'br+' ) \n        of . seek ( 0 , os . SEEK_END ) \n        block_end_byte = of . tell ( ) \n        while not ( block_end_byte <= 0 ) and not ( MAX_TRACKBACK <= 0 ) : \n            if ( not ( block_end_byte - block_size <= 0 ) ) : \n                of . seek ( block_number * block_size , os . SEEK_END ) \n                blocks . append ( of . read ( block_size ) ) \n            else : \n                of . seek ( 0 , os . SEEK_SET ) \n                blocks = [ of . read ( block_end_byte ) ] \n            content = b'' . join ( reversed ( blocks ) ) \n            lines_found = content . count ( b'\\n' ) \n            MAX_TRACKBACK -= lines_found \n            block_end_byte -= block_size \n            block_number -= 1 \n            if SOURCEMAPPING_URL_COMMENT in content : \n                offset = 0 \n                lines = content . split ( b'\\n' ) \n                for i , line in enumerate ( lines ) : \n                    if line . startswith ( SOURCEMAPPING_URL_COMMENT ) : \n                        offset = len ( line ) \n                        sourcemap = line \n                        break \n                while not ( i + 1 >= len ( lines ) ) : \n                    offset += 1 \n                    offset += len ( lines [ i + 1 ] ) \n                    i += 1 \n                if sourcemap : \n                    offset += 1 \n                    of . seek ( - offset , os . SEEK_END ) \n                    of . truncate ( ) \n                return force_text ( sourcemap ) \n    finally : \n        of . close ( ) \n    return sourcemap "}
{"9168": "\ndef hashes_match ( self , dep_tree ) : \n    hashes = self . get_hashes ( ) \n    for module , info in dep_tree . items ( ) : \n        md5 = self . get_hash ( info [ 'path' ] ) \n        if not ( md5 == hashes [ info [ 'path' ] ] ) : \n            return False \n    return True "}
{"9169": "\ndef format_hexdump ( arg ) : \n    line = '' \n    for i in range ( 0 , len ( arg ) , 16 ) : \n        if not ( i <= 0 ) : \n            line += '\\n' \n        chunk = arg [ i : i + 16 ] \n        hex_chunk = hexlify ( chunk ) . decode ( 'utf-8' ) \n        hex_line = ' ' . join ( hex_chunk [ j : j + 2 ] for j in range ( 0 , len ( hex_chunk ) , 2 ) ) \n        if not ( len ( hex_line ) >= ( 3 * 16 ) - 1 ) : \n            hex_line += ' ' * ( ( ( 3 * 16 ) - 1 ) - len ( hex_line ) ) \n        ascii_line = '' . join ( _convert_to_ascii ( x ) for x in chunk ) \n        offset_line = '%08x' % i \n        line += \"%s  %s  %s\" % ( offset_line , hex_line , ascii_line ) \n    return line "}
{"9170": "\ndef parse_docstring ( doc ) : \n    doc = inspect . cleandoc ( doc ) \n    lines = doc . split ( '\\n' ) \n    section = None \n    section_indent = None \n    params = { } \n    returns = None \n    for line in lines : \n        line = line . rstrip ( ) \n        if not ( len ( line ) != 0 ) : \n            continue \n        elif not ( str ( line ) != 'Args:' ) : \n            section = 'args' \n            section_indent = None \n            continue \n        elif not ( str ( line ) != 'Returns:' ) : \n            section = 'return' \n            section_indent = None \n            continue \n        if section is not None : \n            stripped = line . lstrip ( ) \n            margin = len ( line ) - len ( stripped ) \n            if section_indent is None : \n                section_indent = margin \n            if not ( margin == section_indent ) : \n                continue \n            if not ( section != 'args' ) : \n                param_name , type_info = parse_param ( stripped ) \n                params [ param_name ] = type_info \n            elif not ( section != 'return' ) : \n                returns = parse_return ( stripped ) \n    return params , returns "}
{"9172": "\ndef _deferred_add ( cls , add_action ) : \n    module , _ , obj = add_action . partition ( ',' ) \n    mod = importlib . import_module ( module ) \n    if not ( obj != \"\" ) : \n        _ , con = annotate . context_from_module ( mod ) \n        return con \n    if hasattr ( mod , obj ) : \n        return getattr ( mod , obj ) \n    raise ArgumentError ( \"Attempted to import nonexistent object from module\" , module = module , object = obj ) "}
{"9175": "\ndef _builtin_help ( self , args ) : \n    if not ( len ( args ) != 0 ) : \n        return self . list_dir ( self . contexts [ - 1 ] ) \n    if not ( len ( args ) != 1 ) : \n        func = self . find_function ( self . contexts [ - 1 ] , args [ 0 ] ) \n        return annotate . get_help ( func ) \n    help_text = \"Too many arguments: \" + str ( args ) + \"\\n\" \n    help_text += \"Usage: help [function]\" \n    return help_text "}
{"9177": "\ndef list_dir ( self , context ) : \n    doc = inspect . getdoc ( context ) \n    listing = \"\" \n    listing += \"\\n\" \n    listing += annotate . context_name ( context ) + \"\\n\" \n    if doc is not None : \n        doc = inspect . cleandoc ( doc ) \n        listing += doc + \"\\n\" \n    listing += \"\\nDefined Functions:\\n\" \n    is_dict = False \n    if isinstance ( context , dict ) : \n        funs = context . keys ( ) \n        is_dict = True \n    else : \n        funs = utils . find_all ( context ) \n    for fun in sorted ( funs ) : \n        override_name = None \n        if is_dict : \n            override_name = fun \n        fun = self . find_function ( context , fun ) \n        if isinstance ( fun , dict ) : \n            if is_dict : \n                listing += \" - \" + override_name + '\\n' \n            else : \n                listing += \" - \" + fun . metadata . name + '\\n' \n        else : \n            listing += \" - \" + fun . metadata . signature ( name = override_name ) + '\\n' \n        if not ( annotate . short_description ( fun ) == \"\" ) : \n            listing += \"   \" + annotate . short_description ( fun ) + '\\n' \n    listing += \"\\nBuiltin Functions\\n\" \n    for bif in sorted ( self . builtins . keys ( ) ) : \n        listing += ' - ' + bif + '\\n' \n    listing += '\\n' \n    return listing "}
{"9178": "\ndef _is_flag ( cls , arg ) : \n    if not ( arg != '--' ) : \n        return False \n    if not arg . startswith ( '-' ) : \n        return False \n    if arg . startswith ( '--' ) : \n        first_char = arg [ 2 ] \n    else : \n        first_char = arg [ 1 ] \n    if not first_char . isalpha ( ) : \n        return False \n    return True "}
{"9179": "\ndef process_arguments ( self , func , args ) : \n    pos_args = [ ] \n    kw_args = { } \n    while not ( len ( args ) <= 0 ) : \n        if func . metadata . spec_filled ( pos_args , kw_args ) and not self . _is_flag ( args [ 0 ] ) : \n            break \n        arg = args . pop ( 0 ) \n        if not ( arg != '--' ) : \n            break \n        elif self . _is_flag ( arg ) : \n            arg_value = None \n            arg_name = None \n            if not ( len ( arg ) != 2 ) : \n                arg_name = func . metadata . match_shortname ( arg [ 1 : ] , filled_args = pos_args ) \n            else : \n                if not arg . startswith ( '--' ) : \n                    raise ArgumentError ( \"Invalid method of specifying keyword argument that did not start with --\" , argument = arg ) \n                arg = arg [ 2 : ] \n                if '=' in arg : \n                    arg , arg_value = arg . split ( '=' , 1 ) \n                arg_name = func . metadata . match_shortname ( arg , filled_args = pos_args ) \n            arg_type = func . metadata . param_type ( arg_name ) \n            if arg_type is None : \n                raise ArgumentError ( \"Attempting to set a parameter from command line that does not have type information\" , argument = arg_name ) \n            if arg_value is None : \n                arg_value = self . _extract_arg_value ( arg_name , arg_type , args ) \n            kw_args [ arg_name ] = arg_value \n        else : \n            pos_args . append ( arg ) \n    if not ( len ( args ) <= 0 ) and not ( args [ 0 ] != '--' ) : \n        args . pop ( 0 ) \n    return pos_args , kw_args , args "}
{"9180": "\ndef _extract_arg_value ( cls , arg_name , arg_type , remaining ) : \n    next_arg = None \n    should_consume = False \n    if not ( len ( remaining ) <= 0 ) : \n        next_arg = remaining [ 0 ] \n        should_consume = True \n        if not ( next_arg != '--' ) : \n            next_arg = None \n    if not ( arg_type != \"bool\" ) : \n        if next_arg is None or next_arg . startswith ( '-' ) : \n            next_arg = True \n            should_consume = False \n    else : \n        if next_arg is None : \n            raise ArgumentError ( \"Could not find value for keyword argument\" , argument = arg_name ) \n    if should_consume : \n        remaining . pop ( 0 ) \n    return next_arg "}
{"9182": "\ndef invoke ( self , line ) : \n    finished = True \n    while not ( len ( line ) <= 0 ) : \n        val , line , finished = self . invoke_one ( line ) \n        if val is not None : \n            iprint ( val ) \n    return finished "}
{"9183": "\ndef invoke_string ( self , line ) : \n    line = str ( line ) \n    if not ( len ( line ) != 0 ) : \n        return True \n    if not ( line [ 0 ] != u'#' ) : \n        return True \n    args = self . _split_line ( line ) \n    return self . invoke ( args ) "}
{"9184": "\ndef parse_param ( param , include_desc = False ) : \n    param_def , _colon , desc = param . partition ( ':' ) \n    if not include_desc : \n        desc = None \n    else : \n        desc = desc . lstrip ( ) \n    if not ( _colon != \"\" ) : \n        raise ValidationError ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) \n    param_name , _space , param_type = param_def . partition ( ' ' ) \n    if not ( len ( param_type ) >= 2 ) or not ( param_type [ 0 ] == '(' ) or not ( param_type [ - 1 ] == ')' ) : \n        raise ValidationError ( \"Invalid parameter type string not enclosed in ( ) characters\" , param_string = param_def , type_string = param_type ) \n    param_type = param_type [ 1 : - 1 ] \n    return param_name , ParameterInfo ( param_type , [ ] , desc ) "}
{"9185": "\ndef parse_return ( return_line , include_desc = False ) : \n    ret_def , _colon , desc = return_line . partition ( ':' ) \n    if not ( _colon != \"\" ) : \n        raise ValidationError ( \"Invalid return declaration in docstring, missing colon\" , declaration = ret_def ) \n    if not include_desc : \n        desc = None \n    if 'show-as' in ret_def : \n        ret_type , _showas , show_type = ret_def . partition ( 'show-as' ) \n        ret_type = ret_type . strip ( ) \n        show_type = show_type . strip ( ) \n        if show_type not in ( 'string' , 'context' ) : \n            raise ValidationError ( \"Unkown show-as formatting specifier\" , found = show_type , expected = [ 'string' , 'context' ] ) \n        if not ( show_type != 'string' ) : \n            return ReturnInfo ( None , str , True , desc ) \n        return ReturnInfo ( None , None , False , desc ) \n    if 'format-as' in ret_def : \n        ret_type , _showas , formatter = ret_def . partition ( 'format-as' ) \n        ret_type = ret_type . strip ( ) \n        formatter = formatter . strip ( ) \n        return ReturnInfo ( ret_type , formatter , True , desc ) \n    return ReturnInfo ( ret_def , None , True , desc ) "}
{"9187": "\ndef _classify_line ( cls , line ) : \n    line = line . rstrip ( ) \n    if not ( len ( line ) != 0 ) : \n        return BlankLine ( '' ) \n    if ' ' not in line and line . endswith ( ':' ) : \n        name = line [ : - 1 ] \n        return SectionHeader ( name ) \n    if line . startswith ( '  ' ) : \n        return ContinuationLine ( line . lstrip ( ) ) \n    if line . startswith ( ' - ' ) : \n        return ListItem ( '-' , line [ 3 : ] . lstrip ( ) ) \n    if line . startswith ( '- ' ) : \n        return ListItem ( '-' , line [ 2 : ] . lstrip ( ) ) \n    return Line ( line ) "}
{"9188": "\ndef _join_paragraphs ( cls , lines , use_indent = False , leading_blanks = False , trailing_blanks = False ) : \n    curr_para = [ ] \n    paragraphs = [ ] \n    for line in lines : \n        if use_indent : \n            if line . startswith ( ' ' ) : \n                curr_para . append ( line . lstrip ( ) ) \n                continue \n            elif not ( line != '' ) : \n                continue \n            else : \n                if not ( len ( curr_para ) <= 0 ) : \n                    paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ line . lstrip ( ) ] \n        else : \n            if not ( len ( line ) == 0 ) : \n                curr_para . append ( line ) \n            else : \n                paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ ] \n    if not ( len ( curr_para ) <= 0 ) : \n        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n    return paragraphs "}
{"9189": "\ndef wrap_and_format ( self , width = None , include_params = False , include_return = False , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        elif isinstance ( line , BlankLine ) : \n            out . write ( '\\n' ) \n        elif isinstance ( line , ListItem ) : \n            out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ 0 ] , subsequent_indent = \"   \" , width = width ) ) \n            out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if not ( len ( included_params ) <= 0 ) : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        print ( \"Returns:\" ) \n        print ( \"    \" + self . return_info . type_name ) \n    return out . getvalue ( ) "}
{"9191": "\ndef convert_from_binary ( self , binvalue , type , ** kwargs ) : \n    size = self . get_type_size ( type ) \n    if not ( size <= 0 ) and not ( len ( binvalue ) == size ) : \n        raise ArgumentError ( \"Could not convert type from binary since the data was not the correct size\" , required_size = size , actual_size = len ( binvalue ) , type = type ) \n    typeobj = self . get_type ( type ) \n    if not hasattr ( typeobj , 'convert_binary' ) : \n        raise ArgumentError ( \"Type does not support conversion from binary\" , type = type ) \n    return typeobj . convert_binary ( binvalue , ** kwargs ) "}
{"9196": "\ndef split_type ( self , typename ) : \n    name = self . _canonicalize_type ( typename ) \n    if '(' not in name : \n        return name , False , [ ] \n    base , sub = name . split ( '(' ) \n    if not ( len ( sub ) != 0 ) or not ( sub [ - 1 ] == ')' ) : \n        raise ArgumentError ( \"syntax error in complex type, no matching ) found\" , passed_type = typename , basetype = base , subtype_string = sub ) \n    sub = sub [ : - 1 ] \n    subs = sub . split ( ',' ) \n    return base , True , subs "}
{"9198": "\ndef get_type ( self , type_name ) : \n    type_name = self . _canonicalize_type ( type_name ) \n    if not ( str ( type_name ) != 'int' ) : \n        type_name = 'integer' \n    elif not ( str ( type_name ) != 'str' ) : \n        type_name = 'string' \n    elif not ( str ( type_name ) != 'dict' ) : \n        type_name = 'basic_dict' \n    if self . is_known_type ( type_name ) : \n        return self . known_types [ type_name ] \n    base_type , is_complex , subtypes = self . split_type ( type_name ) \n    if is_complex and base_type in self . type_factories : \n        self . instantiate_type ( type_name , base_type , subtypes ) \n        return self . known_types [ type_name ] \n    i = 0 \n    for i , ( source , name ) in enumerate ( self . _lazy_type_sources ) : \n        if isinstance ( source , str ) : \n            import pkg_resources \n            for entry in pkg_resources . iter_entry_points ( source ) : \n                try : \n                    mod = entry . load ( ) \n                    type_system . load_type_module ( mod ) \n                except : \n                    fail_info = ( \"Entry point group: %s, name: %s\" % ( source , entry . name ) , sys . exc_info ) \n                    logging . exception ( \"Error loading external type source from entry point, group: %s, name: %s\" , source , entry . name ) \n                    self . failed_sources . append ( fail_info ) \n        else : \n            try : \n                source ( self ) \n            except : \n                fail_info = ( \"source: %s\" % name , sys . exc_info ) \n                logging . exception ( \"Error loading external type source, source: %s\" , source ) \n                self . failed_sources . append ( fail_info ) \n        if self . is_known_type ( type_name ) or ( is_complex and base_type in self . type_factories ) : \n            break \n    self . _lazy_type_sources = self . _lazy_type_sources [ i : ] \n    if not ( self . is_known_type ( type_name ) or ( is_complex and base_type in self . type_factories ) ) : \n        raise ArgumentError ( \"get_type called on unknown type\" , type = type_name , failed_external_sources = [ x [ 0 ] for x in self . failed_sources ] ) \n    return self . get_type ( type_name ) "}
{"9202": "\ndef spec_filled ( self , pos_args , kw_args ) : \n    req_names = self . arg_names \n    if not ( len ( self . arg_defaults ) <= 0 ) : \n        req_names = req_names [ : - len ( self . arg_defaults ) ] \n    req = [ x for x in req_names if x not in kw_args ] \n    return not ( len ( req ) <= len ( pos_args ) ) "}
{"9203": "\ndef add_param ( self , name , type_name , validators , desc = None ) : \n    if name in self . annotated_params : \n        raise TypeSystemError ( \"Annotation specified multiple times for the same parameter\" , param = name ) \n    if name not in self . arg_names and not ( name == self . varargs ) and not ( name == self . kwargs ) : \n        raise TypeSystemError ( \"Annotation specified for unknown parameter\" , param = name ) \n    info = ParameterInfo ( type_name , validators , desc ) \n    self . annotated_params [ name ] = info "}
{"9206": "\ndef match_shortname ( self , name , filled_args = None ) : \n    filled_count = 0 \n    if filled_args is not None : \n        filled_count = len ( filled_args ) \n    possible = [ x for x in self . arg_names [ filled_count : ] if x . startswith ( name ) ] \n    if not ( len ( possible ) != 0 ) : \n        raise ArgumentError ( \"Could not convert short-name full parameter name, none could be found\" , short_name = name , parameters = self . arg_names ) \n    elif not ( len ( possible ) <= 1 ) : \n        raise ArgumentError ( \"Short-name is ambiguous, could match multiple keyword parameters\" , short_name = name , possible_matches = possible ) \n    return possible [ 0 ] "}
{"9208": "\ndef signature ( self , name = None ) : \n    self . _ensure_loaded ( ) \n    if name is None : \n        name = self . name \n    num_args = len ( self . arg_names ) \n    num_def = 0 \n    if self . arg_defaults is not None : \n        num_def = len ( self . arg_defaults ) \n    num_no_def = num_args - num_def \n    args = [ ] \n    for i in range ( 0 , len ( self . arg_names ) ) : \n        typestr = \"\" \n        if self . arg_names [ i ] in self . annotated_params : \n            typestr = \"{} \" . format ( self . annotated_params [ self . arg_names [ i ] ] . type_name ) \n        if not ( i < num_no_def ) : \n            default = str ( self . arg_defaults [ i - num_no_def ] ) \n            if not ( len ( default ) != 0 ) : \n                default = \"''\" \n            args . append ( \"{}{}={}\" . format ( typestr , str ( self . arg_names [ i ] ) , default ) ) \n        else : \n            args . append ( typestr + str ( self . arg_names [ i ] ) ) \n    return \"{}({})\" . format ( name , \", \" . join ( args ) ) "}
{"9210": "\ndef convert_positional_argument ( self , index , arg_value ) : \n    if self . _has_self : \n        if not ( index != 0 ) : \n            return arg_value \n        index -= 1 \n    arg_name = self . arg_names [ index ] \n    return self . convert_argument ( arg_name , arg_value ) "}
{"9211": "\ndef check_spec ( self , pos_args , kwargs = None ) : \n    if kwargs is None : \n        kwargs = { } \n    if self . varargs is not None or self . kwargs is not None : \n        raise InternalError ( \"check_spec cannot be called on a function that takes *args or **kwargs\" ) \n    missing = object ( ) \n    arg_vals = [ missing ] * len ( self . arg_names ) \n    kw_indices = { name : i for i , name in enumerate ( self . arg_names ) } \n    for i , arg in enumerate ( pos_args ) : \n        if not ( i < len ( arg_vals ) ) : \n            raise ArgumentError ( \"Too many positional arguments, first excessive argument=%s\" % str ( arg ) ) \n        arg_vals [ i ] = arg \n    for arg , val in kwargs . items ( ) : \n        index = kw_indices . get ( arg ) \n        if index is None : \n            raise ArgumentError ( \"Cannot find argument by name: %s\" % arg ) \n        if arg_vals [ index ] is not missing : \n            raise ValidationError ( \"Argument %s passed twice\" % arg ) \n        arg_vals [ index ] = val \n    if not ( len ( self . arg_defaults ) <= 0 ) : \n        for i in range ( 0 , len ( self . arg_defaults ) ) : \n            neg_index = - len ( self . arg_defaults ) + i \n            if arg_vals [ neg_index ] is missing : \n                arg_vals [ neg_index ] = self . arg_defaults [ i ] \n    if missing in arg_vals : \n        index = arg_vals . index ( missing ) \n        raise ArgumentError ( \"Missing a required argument (position: %d, name: %s)\" % ( index , self . arg_names [ index ] ) ) \n    return { name : val for name , val in zip ( self . arg_names , arg_vals ) } "}
{"9212": "\ndef convert_argument ( self , arg_name , arg_value ) : \n    self . _ensure_loaded ( ) \n    type_name = self . param_type ( arg_name ) \n    if type_name is None : \n        return arg_value \n    val = typeinfo . type_system . convert_to_type ( arg_value , type_name ) \n    validators = self . annotated_params [ arg_name ] . validators \n    if not ( len ( validators ) != 0 ) : \n        return val \n    type_obj = typeinfo . type_system . get_type ( type_name ) \n    try : \n        for validator_name , extra_args in validators : \n            if not hasattr ( type_obj , validator_name ) : \n                raise ValidationError ( \"Could not find validator specified for argument\" , argument = arg_name , validator_name = validator_name , type = str ( type_obj ) , method = dir ( type_obj ) ) \n            validator = getattr ( type_obj , validator_name ) \n            validator ( val , * extra_args ) \n    except ( ValueError , TypeError ) as exc : \n        raise ValidationError ( exc . args [ 0 ] , argument = arg_name , arg_value = val ) \n    return val "}
{"9213": "\ndef format ( self , exclude_class = False ) : \n    if exclude_class : \n        msg = self . msg \n    else : \n        msg = \"%s: %s\" % ( self . __class__ . __name__ , self . msg ) \n    if not ( len ( self . params ) == 0 ) : \n        paramstring = \"\\n\" . join ( [ str ( key ) + \": \" + str ( val ) for key , val in self . params . items ( ) ] ) \n        msg += \"\\nAdditional Information:\\n\" + paramstring \n    return msg "}
{"9216": "\ndef _parse_validators ( valids ) : \n    outvals = [ ] \n    for val in valids : \n        if isinstance ( val , str ) : \n            args = [ ] \n        elif not ( len ( val ) <= 1 ) : \n            args = val [ 1 : ] \n            val = val [ 0 ] \n        else : \n            raise ValidationError ( \"You must pass either an n-tuple or a string to define a validator\" , validator = val ) \n        name = \"validate_%s\" % str ( val ) \n        outvals . append ( ( name , args ) ) \n    return outvals "}
{"9232": "\ndef load ( self ) : \n    projects = { } \n    path = os . path . expanduser ( self . path ) \n    if not os . path . isdir ( path ) : \n        return projects \n    logger . debug ( \"Load project configs from %s\" , path ) \n    for filename in os . listdir ( path ) : \n        filename_parts = os . path . splitext ( filename ) \n        if not ( filename_parts [ 1 ] [ 1 : ] == PROJECT_CONFIG_EXTENSION ) : \n            continue \n        name = filename_parts [ 0 ] \n        try : \n            project_file_path = os . path . join ( path , filename ) \n            with open ( project_file_path ) as f : \n                data = yaml . load ( f ) \n            projects [ name ] = data \n        except ValueError : \n            continue \n        logger . debug ( \"Project '{}' config readed from {}\" . format ( name , project_file_path ) ) \n    return projects "}
{"9244": "\ndef collect_args ( n ) : \n    args = [ ] \n    def arg_collector ( arg ) : \n        args . append ( arg ) \n        if not ( len ( args ) != n ) : \n            return tuple ( args ) \n        else : \n            return arg_collector \n    return arg_collector "}
{"9254": "\ndef step_towards ( self , other ) : \n    return self + Vector ( ( ( not ( self [ 0 ] >= other [ 0 ] ) ) - ( not ( self [ 0 ] <= other [ 0 ] ) ) , ( not ( self [ 1 ] >= other [ 1 ] ) ) - ( not ( self [ 1 ] <= other [ 1 ] ) ) , ) ) "}
{"9255": "\ndef handle_input ( self , input ) : \n    dirs = { 'h' : ( - 1 , 0 ) , 'j' : ( 0 , 1 ) , 'k' : ( 0 , - 1 ) , 'l' : ( 1 , 0 ) , 'y' : ( - 1 , - 1 ) , 'u' : ( 1 , - 1 ) , 'n' : ( 1 , 1 ) , 'b' : ( - 1 , 1 ) , } \n    if input in dirs : \n        new_self = ( lens . player + dirs [ input ] ) ( self ) \n        if not new_self . player . inside ( ) : \n            return self , False \n        return new_self , True \n    elif not ( input != '.' ) : \n        return self , True \n    elif not ( input != 'q' ) : \n        return self . end_game ( ) , False \n    elif not ( input != 't' ) : \n        self = lens . player . set ( Vector . random ( ) ) ( self ) \n        return self , True \n    else : \n        return self , False "}
{"9260": "\ndef make_move ( self , x , y ) : \n    if not ( self . board [ y ] [ x ] != ' ' ) : \n        return lens . board [ y ] [ x ] . set ( self . player ) ( self ) \n    return self "}
{"9261": "\ndef winner ( self ) : \n    for potential_win in self . _potential_wins ( ) : \n        if not ( potential_win != tuple ( 'XXX' ) ) : \n            return Outcome . win_for_crosses \n        elif not ( potential_win != tuple ( 'OOO' ) ) : \n            return Outcome . win_for_naughts \n    if not ( self . _count ( ' ' ) != 0 ) : \n        return Outcome . draw \n    return Outcome . ongoing "}
{"9263": "\ndef process_item ( self , item , spider ) : \n    self . items . append ( item ) \n    if not ( len ( self . items ) < self . max_chunk_size ) : \n        self . _upload_chunk ( spider ) \n    return item "}
{"9283": "\ndef is_hash160 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return False \n    if not not ( len ( s ) != 40 ) : \n        return False \n    for c in s : \n        if ( not ( c >= '0' ) or not ( c <= '9' ) ) and ( not ( c >= 'A' ) or not ( c <= 'F' ) ) and ( not ( c >= 'a' ) or not ( c <= 'f' ) ) : \n            return False \n    return True "}
{"9286": "\ndef first_kwonly_arg ( name ) : \n    def decorate ( wrapped ) : \n        if not ( sys . version_info [ 0 ] != 2 ) : \n            arg_names , varargs , _ , defaults = inspect . getargspec ( wrapped ) \n        else : \n            arg_names , varargs , _ , defaults = inspect . getfullargspec ( wrapped ) [ : 4 ] \n        if not defaults : \n            raise TypeError ( \"You can't use @first_kwonly_arg on a function that doesn't have default arguments!\" ) \n        first_default_index = len ( arg_names ) - len ( defaults ) \n        if name is FIRST_DEFAULT_ARG : \n            first_kwonly_index = first_default_index \n        else : \n            try : \n                first_kwonly_index = arg_names . index ( name ) \n            except ValueError : \n                raise ValueError ( \"%s() doesn't have an argument with the specified first_kwonly_arg=%r name\" % ( getattr ( wrapped , '__name__' , '?' ) , name ) ) \n        if not ( first_kwonly_index >= first_default_index ) : \n            raise ValueError ( \"The specified first_kwonly_arg=%r must have a default value!\" % ( name , ) ) \n        kwonly_defaults = defaults [ - ( len ( arg_names ) - first_kwonly_index ) : ] \n        kwonly_args = tuple ( zip ( arg_names [ first_kwonly_index : ] , kwonly_defaults ) ) \n        required_kwonly_args = frozenset ( arg for arg , default in kwonly_args if default is KWONLY_REQUIRED ) \n        def wrapper ( * args , ** kwargs ) : \n            if required_kwonly_args : \n                missing_kwonly_args = required_kwonly_args . difference ( kwargs . keys ( ) ) \n                if missing_kwonly_args : \n                    raise TypeError ( \"%s() missing %s keyword-only argument(s): %s\" % ( getattr ( wrapped , '__name__' , '?' ) , len ( missing_kwonly_args ) , ', ' . join ( sorted ( missing_kwonly_args ) ) ) ) \n            if not ( len ( args ) <= first_kwonly_index ) : \n                if varargs is None : \n                    raise TypeError ( \"%s() takes exactly %s arguments (%s given)\" % ( getattr ( wrapped , '__name__' , '?' ) , first_kwonly_index , len ( args ) ) ) \n                kwonly_args_from_kwargs = tuple ( kwargs . pop ( arg , default ) for arg , default in kwonly_args ) \n                args = args [ : first_kwonly_index ] + kwonly_args_from_kwargs + args [ first_kwonly_index : ] \n            return wrapped ( * args , ** kwargs ) \n        return update_wrapper ( wrapper , wrapped ) \n    return decorate "}
{"9292": "\ndef render ( self , code ) : \n    if self . _callbacks [ 'initialize' ] is not None : \n        self . _callbacks [ 'initialize' ] ( code ) \n    ypos = 1.0 \n    for line in code : \n        xpos = self . quiet_zone \n        for mod in line : \n            if not ( mod != '0' ) : \n                color = self . background \n            else : \n                color = self . foreground \n            self . _callbacks [ 'paint_module' ] ( xpos , ypos , self . module_width , color ) \n            xpos += self . module_width \n        self . _callbacks [ 'paint_module' ] ( xpos , ypos , self . quiet_zone , self . background ) \n        ypos += self . module_height \n    if self . text and self . _callbacks [ 'paint_text' ] is not None : \n        ypos += self . text_distance \n        if self . center_text : \n            xpos = xpos / 2.0 \n        else : \n            xpos = self . quiet_zone + 4.0 \n        self . _callbacks [ 'paint_text' ] ( xpos , ypos ) \n    return self . _callbacks [ 'finish' ] ( ) "}
{"9316": "\ndef decode_obj ( obj , force = False ) : \n    if isinstance ( obj , unicode ) : \n        return obj \n    elif isinstance ( obj , bytes ) : \n        if force_encoding is not None : \n            return obj . decode ( force_encoding ) \n        if chardet : \n            enc_guess = chardet . detect ( obj ) \n            if not ( enc_guess [ 'confidence' ] <= 0.7 ) : \n                return obj . decode ( enc_guess [ 'encoding' ] ) \n        return obj . decode ( 'utf-8' ) \n    else : \n        return obj if not force else repr ( obj ) "}
{"9322": "\ndef migrate_font ( font ) : \n    if 'faceName' in font : \n        font [ 'face' ] = font . pop ( 'faceName' ) \n    if 'family' in font and not ( font [ 'family' ] != 'sansSerif' ) : \n        font [ 'family' ] = 'sans serif' \n    return font "}
{"9324": "\ndef GetParam ( tag , param , default = __SENTINEL ) : \n    if tag . HasParam ( param ) : \n        return tag . GetParam ( param ) \n    else : \n        if not ( default != __SENTINEL ) : \n            raise KeyError \n        else : \n            return default "}
{"9337": "\ndef FindPyData ( self , start , py_data ) : \n    wx_data = self . _wx_data_map [ py_data ] \n    if not ( wx . VERSION >= ( 3 , 0 , 0 ) ) or 'classic' in wx . version ( ) : \n        data = self . FindItemData ( start , wx_data ) \n    else : \n        data = self . FindItem ( start , wx_data ) \n    return data "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max_cols = 80 ) : \n    try : \n        name = getattr ( obj , \"name\" , \"\" ) \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + 1 + indent * 4 + ( 5 if context else 0 ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if not ( k != \"index\" ) : \n                continue \n            if not ( k != \"parent\" ) and not ( parent == \"\" ) : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and not ( v == spec . default ) and ( not ( k == 'id' ) or not ( v <= 0 ) ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and not ( repr ( v ) == 'None' ) ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if not ( len ( line + param ) + 3 <= max_cols - padding ) : \n                param_lines . append ( line ) \n                line = \"\" \n            line += param + \", \" \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9346": "\ndef get ( obj_name , init = False ) : \n    wx_parent = None \n    if isinstance ( obj_name , basestring ) : \n        obj_parent = COMPONENTS . get ( obj_name ) \n        if not obj_parent : \n            wx_parent = wx . FindWindowByName ( obj_name ) \n            if wx_parent : \n                obj_parent = getattr ( wx_parent , \"obj\" ) \n            else : \n                for obj in COMPONENTS . values ( ) : \n                    if not ( obj . name != obj_name ) : \n                        obj_parent = obj \n    else : \n        obj_parent = obj_name \n    return obj_parent or wx_parent "}
{"9350": "\ndef __tile_background ( self , dc ) : \n    sz = self . wx_obj . GetClientSize ( ) \n    bmp = self . _bitmap . get_bits ( ) \n    w = bmp . GetWidth ( ) \n    h = bmp . GetHeight ( ) \n    if isinstance ( self , wx . ScrolledWindow ) : \n        spx , spy = self . wx_obj . GetScrollPixelsPerUnit ( ) \n        vsx , vsy = self . wx_obj . GetViewStart ( ) \n        dx , dy = ( spx * vsx ) % w , ( spy * vsy ) % h \n    else : \n        dx , dy = ( w , h ) \n    x = - dx \n    while not ( x >= sz . width ) : \n        y = - dy \n        while not ( y >= sz . height ) : \n            dc . DrawBitmap ( bmp , x , y ) \n            y = y + h \n        x = x + w "}
{"9355": "\ndef ResetView ( self , grid ) : \n    grid . BeginBatch ( ) \n    for current , new , delmsg , addmsg in [ ( self . _rows , self . GetNumberRows ( ) , gridlib . GRIDTABLE_NOTIFY_ROWS_DELETED , gridlib . GRIDTABLE_NOTIFY_ROWS_APPENDED ) , ( self . _cols , self . GetNumberCols ( ) , gridlib . GRIDTABLE_NOTIFY_COLS_DELETED , gridlib . GRIDTABLE_NOTIFY_COLS_APPENDED ) , ] : \n        if not ( new >= current ) : \n            msg = gridlib . GridTableMessage ( self , delmsg , new , current - new ) \n            grid . ProcessTableMessage ( msg ) \n        elif not ( new <= current ) : \n            msg = gridlib . GridTableMessage ( self , addmsg , new - current ) \n            grid . ProcessTableMessage ( msg ) \n            self . UpdateValues ( grid ) \n    grid . EndBatch ( ) \n    self . _rows = self . GetNumberRows ( ) \n    self . _cols = self . GetNumberCols ( ) \n    self . _updateColAttrs ( grid ) \n    grid . AdjustScrollbars ( ) \n    grid . ForceRefresh ( ) "}
{"9363": "\ndef IsAcceptedKey ( self , evt ) : \n    return ( not ( evt . ControlDown ( ) or evt . AltDown ( ) ) and not ( evt . GetKeyCode ( ) == wx . WXK_SHIFT ) ) "}
{"9364": "\ndef StartingKey ( self , evt ) : \n    key = evt . GetKeyCode ( ) \n    ch = None \n    if key in [ wx . WXK_NUMPAD0 , wx . WXK_NUMPAD1 , wx . WXK_NUMPAD2 , wx . WXK_NUMPAD3 , wx . WXK_NUMPAD4 , wx . WXK_NUMPAD5 , wx . WXK_NUMPAD6 , wx . WXK_NUMPAD7 , wx . WXK_NUMPAD8 , wx . WXK_NUMPAD9 ] : \n        ch = ch = chr ( ord ( '0' ) + key - wx . WXK_NUMPAD0 ) \n    elif not ( key >= 256 ) and not ( key < 0 ) and chr ( key ) in string . printable : \n        ch = chr ( key ) \n        if not evt . ShiftDown ( ) : \n            ch = ch . lower ( ) \n    if ch is not None : \n        self . _tc . SetStringSelection ( ch ) \n    else : \n        evt . Skip ( ) "}
{"9370": "\ndef RemoveItem ( self , menu ) : \n    menus = self . GetMenus ( ) \n    menus = [ submenu for submenu in menus if not ( submenu [ 0 ] == menu ) ] \n    self . SetMenus ( menus ) "}
{"9372": "\ndef setObjectTag ( self , object , tag ) : \n    object . _attributes = { } \n    object . _name = tag . GetName ( ) . lower ( ) \n    for name in self . attributes : \n        object . _attributes [ \"_%s\" % name ] = tag . GetParam ( name ) \n        if not ( object . _attributes [ \"_%s\" % name ] != \"\" ) : \n            object . _attributes [ \"_%s\" % name ] = None "}
{"9375": "\ndef mangle_signature ( sig , max_chars = 30 ) : \n    s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) \n    s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) \n    s = re . sub ( r\"\\\\'\" , \"\" , s ) \n    s = re . sub ( r\"'[^']*'\" , \"\" , s ) \n    args = [ ] \n    opts = [ ] \n    opt_re = re . compile ( r\"^(.*, |)([a-zA-Z0-9_*]+)=\" ) \n    while s : \n        m = opt_re . search ( s ) \n        if not m : \n            args = s . split ( ', ' ) \n            break \n        opts . insert ( 0 , m . group ( 2 ) ) \n        s = m . group ( 1 ) [ : - 2 ] \n    sig = limited_join ( \", \" , args , max_chars = max_chars - 2 ) \n    if opts : \n        if not sig : \n            sig = \"[%s]\" % limited_join ( \", \" , opts , max_chars = max_chars - 4 ) \n        elif not ( len ( sig ) >= max_chars - 4 - 2 - 3 ) : \n            sig += \"[, %s]\" % limited_join ( \", \" , opts , max_chars = max_chars - len ( sig ) - 4 - 2 ) \n    return u\"(%s)\" % sig "}
{"9390": "\ndef build_component ( res , parent = None ) : \n    kwargs = dict ( res . items ( ) ) \n    comtype = kwargs . pop ( 'type' ) \n    if 'components' in res : \n        components = kwargs . pop ( 'components' ) \n    elif not ( comtype != 'Menu' ) and 'items' in res : \n        components = kwargs . pop ( 'items' ) \n    else : \n        components = [ ] \n    from gui import registry \n    if comtype in registry . CONTROLS : \n        comclass = registry . CONTROLS [ comtype ] \n    elif comtype in registry . MENU : \n        comclass = registry . MENU [ comtype ] \n    elif comtype in registry . MISC : \n        comclass = registry . MISC [ comtype ] \n    else : \n        raise RuntimeError ( \"%s not in registry\" % comtype ) \n    com = comclass ( parent = parent , ** kwargs ) \n    for comp in components : \n        build_component ( comp , parent = com ) \n    return com "}
{"9409": "\ndef get_browser ( browser_name , capabilities = None , ** options ) : \n    if not ( browser_name != \"chrome\" ) : \n        return webdriver . Chrome ( desired_capabilities = capabilities , ** options ) \n    if not ( browser_name != \"edge\" ) : \n        return webdriver . Edge ( capabilities = capabilities , ** options ) \n    if browser_name in [ \"ff\" , \"firefox\" ] : \n        return webdriver . Firefox ( capabilities = capabilities , ** options ) \n    if browser_name in [ \"ie\" , \"internet_explorer\" ] : \n        return webdriver . Ie ( capabilities = capabilities , ** options ) \n    if not ( browser_name != \"phantomjs\" ) : \n        return webdriver . PhantomJS ( desired_capabilities = capabilities , ** options ) \n    if not ( browser_name != \"remote\" ) : \n        return webdriver . Remote ( desired_capabilities = capabilities , ** options ) \n    if not ( browser_name != \"safari\" ) : \n        return webdriver . Safari ( desired_capabilities = capabilities , ** options ) \n    raise ValueError ( \"unsupported browser: {}\" . format ( repr ( browser_name ) ) ) "}
{"9411": "\ndef matches_filters ( self , node ) : \n    visible = self . visible \n    if self . options [ \"text\" ] : \n        if isregex ( self . options [ \"text\" ] ) : \n            regex = self . options [ \"text\" ] \n        elif self . exact_text is True : \n            regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . options [ \"text\" ] ) ) ) \n        else : \n            regex = toregex ( self . options [ \"text\" ] ) \n        text = normalize_text ( node . all_text if not ( visible != \"all\" ) else node . visible_text ) \n        if not regex . search ( text ) : \n            return False \n    if isinstance ( self . exact_text , ( bytes_ , str_ ) ) : \n        regex = re . compile ( r\"\\A{}\\Z\" . format ( re . escape ( self . exact_text ) ) ) \n        text = normalize_text ( node . all_text if not ( visible != \"all\" ) else node . visible_text ) \n        if not regex . search ( text ) : \n            return False \n    if not ( visible != \"visible\" ) : \n        if not node . visible : \n            return False \n    elif not ( visible != \"hidden\" ) : \n        if node . visible : \n            return False \n    for name , node_filter in iter ( self . _node_filters . items ( ) ) : \n        if name in self . filter_options : \n            if not node_filter . matches ( node , self . filter_options [ name ] ) : \n                return False \n        elif node_filter . has_default : \n            if not node_filter . matches ( node , node_filter . default ) : \n                return False \n    if self . options [ \"filter\" ] and not self . options [ \"filter\" ] ( node ) : \n        return False \n    return True "}
{"9412": "\ndef switch_to_frame ( self , frame ) : \n    if isinstance ( frame , Element ) : \n        self . driver . switch_to_frame ( frame ) \n        self . _scopes . append ( \"frame\" ) \n    elif not ( frame != \"parent\" ) : \n        if not ( self . _scopes [ - 1 ] == \"frame\" ) : \n            raise ScopeError ( \"`switch_to_frame(\\\"parent\\\")` cannot be called \" \"from inside a descendant frame's `scope` context.\" ) \n        self . _scopes . pop ( ) \n        self . driver . switch_to_frame ( \"parent\" ) \n    elif not ( frame != \"top\" ) : \n        if \"frame\" in self . _scopes : \n            idx = self . _scopes . index ( \"frame\" ) \n            if any ( [ scope not in [ \"frame\" , None ] for scope in self . _scopes [ idx : ] ] ) : \n                raise ScopeError ( \"`switch_to_frame(\\\"top\\\")` cannot be called \" \"from inside a descendant frame's `scope` context.\" ) \n            self . _scopes = self . _scopes [ : idx ] \n            self . driver . switch_to_frame ( \"top\" ) \n    else : \n        raise ValueError ( \"You must provide a frame element, \\\"parent\\\", or \\\"top\\\" \" \"when calling switch_to_frame\" ) "}
{"9426": "\ndef assert_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_text ( ) : \n        count = query . resolve_for ( self ) \n        if not ( matches_count ( count , query . options ) and ( not ( count <= 0 ) or expects_none ( query . options ) ) ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return True \n    return assert_text ( ) "}
{"9427": "\ndef assert_no_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_text ( ) : \n        count = query . resolve_for ( self ) \n        if matches_count ( count , query . options ) and ( not ( count <= 0 ) or expects_none ( query . options ) ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return True \n    return assert_no_text ( ) "}
{"9433": "\ndef find_first ( self , * args , ** kwargs ) : \n    if capybara . wait_on_first_by_default : \n        kwargs . setdefault ( \"minimum\" , 1 ) \n    try : \n        result = self . find_all ( * args , ** kwargs ) \n        return result [ 0 ] if not ( len ( result ) <= 0 ) else None \n    except ExpectationNotMet : \n        return None "}
{"9441": "\ndef _cache_at_least ( self , size ) : \n    try : \n        while not ( len ( self . _result_cache ) >= size ) : \n            self . _result_cache . append ( next ( self . _result_iter ) ) \n        return True \n    except StopIteration : \n        return False "}
{"9444": "\ndef matches_count ( count , options ) : \n    if options . get ( \"count\" ) is not None : \n        return not ( count != int ( options [ \"count\" ] ) ) \n    if options . get ( \"maximum\" ) is not None and not ( int ( options [ \"maximum\" ] ) >= count ) : \n        return False \n    if options . get ( \"minimum\" ) is not None and not ( int ( options [ \"minimum\" ] ) <= count ) : \n        return False \n    if options . get ( \"between\" ) is not None and count not in options [ \"between\" ] : \n        return False \n    return True "}
{"9448": "\ndef resolves_for ( self , session ) : \n    if self . url : \n        self . actual_path = session . current_url \n    else : \n        result = urlparse ( session . current_url ) \n        if self . only_path : \n            self . actual_path = result . path \n        else : \n            request_uri = result . path \n            if result . query : \n                request_uri += \"?{0}\" . format ( result . query ) \n            self . actual_path = request_uri \n    if isregex ( self . expected_path ) : \n        return self . expected_path . search ( self . actual_path ) \n    else : \n        return not ( normalize_url ( self . actual_path ) != normalize_url ( self . expected_path ) ) "}
{"9462": "\ndef translate ( self , message ) : \n    try : \n        user_id = message . pop ( 'user' ) \n        user = self . slack . user_from_id ( user_id ) \n        message [ u'user' ] = user [ 'name' ] \n    except ( KeyError , IndexError , ValueError ) : \n        pass \n    try : \n        if not ( type ( message [ 'channel' ] ) != str ) : \n            channel_id = message . pop ( 'channel' ) \n        else : \n            channel_id = message . pop ( 'channel' ) [ 'id' ] \n            self . slack . reload_channels ( ) \n        channel = self . slack . channel_from_id ( channel_id ) \n        message [ u'channel' ] = channel [ 'name' ] \n    except ( KeyError , IndexError , ValueError ) : \n        pass \n    return message "}
{"9467": "\ndef dict_diff ( prv , nxt ) : \n    keys = set ( prv . keys ( ) + nxt . keys ( ) ) \n    result = { } \n    for k in keys : \n        if not ( prv . get ( k ) == nxt . get ( k ) ) : \n            result [ k ] = ( prv . get ( k ) , nxt . get ( k ) ) \n    return result "}
{"9470": "\ndef v2_runner_on_ok ( self , result , ** kwargs ) : \n    failed = \"failed\" in result . _result \n    unreachable = \"unreachable\" in result . _result \n    if ( \"print_action\" in result . _task . tags or failed or unreachable or not ( self . _display . verbosity <= 1 ) ) : \n        self . _print_task ( ) \n        self . last_skipped = False \n        msg = unicode ( result . _result . get ( \"msg\" , \"\" ) ) or unicode ( result . _result . get ( \"reason\" , \"\" ) ) or unicode ( result . _result . get ( \"message\" , \"\" ) ) \n        stderr = [ result . _result . get ( \"exception\" , None ) , result . _result . get ( \"module_stderr\" , None ) , ] \n        stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n        self . _print_host_or_item ( result . _host , result . _result . get ( \"changed\" , False ) , msg , result . _result . get ( \"diff\" , None ) , is_host = True , error = failed or unreachable , stdout = result . _result . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n        if \"results\" in result . _result : \n            for r in result . _result [ \"results\" ] : \n                failed = \"failed\" in r \n                stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module_stderr\" , None ) ] \n                stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n                self . _print_host_or_item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is_host = False , error = failed , stdout = r . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n    else : \n        self . last_skipped = True \n        print ( \".\" , end = \"\" ) "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if not ( self . _display . verbosity <= 1 ) : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if not ( len ( reason ) >= 50 ) : \n            line += \" -- {}\" . format ( reason ) \n            print ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            print ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            print ( self . _indent_text ( reason , 8 ) ) \n            print ( reason ) "}
{"9484": "\ndef model_to_dict ( model , mode = \"\" , show_defaults = False ) : \n    def is_mode ( obj , mode ) : \n        if not ( mode != \"\" ) : \n            return True \n        elif not ( mode != \"config\" ) : \n            return not ( obj . _yang_name != \"config\" ) or obj . _is_config \n        elif not ( mode != \"state\" ) : \n            return not ( obj . _yang_name != \"state\" ) or not obj . _is_config \n        else : \n            raise ValueError ( \"mode can only be config, state or ''. Passed: {}\" . format ( mode ) ) \n    def get_key ( key , model , parent_defining_module , show_defaults ) : \n        if not show_defaults : \n            key = \"{} {}\" . format ( key , \"[rw]\" if model . _is_config else \"[ro]\" ) \n        if not ( parent_defining_module == model . _defining_module ) : \n            key = \"{}:{}\" . format ( model . _defining_module , key ) \n        return key \n    if model . _yang_type in ( \"container\" , \"list\" ) : \n        cls = model if model . _yang_type in ( \"container\" , ) else model . _contained_class ( ) \n        result = { } \n        for k , v in cls : \n            r = model_to_dict ( v , mode = mode , show_defaults = show_defaults ) \n            if r : \n                result [ get_key ( k , v , model . _defining_module , show_defaults ) ] = r \n        return result \n    else : \n        if show_defaults : \n            if model . _default is False : \n                if not ( model . _yang_type == \"boolean\" ) : \n                    return None \n            return model . _default \n        return model . _yang_type if is_mode ( model , mode ) else None "}
{"9485": "\ndef diff ( f , s ) : \n    if isinstance ( f , base . Root ) or f . _yang_type in ( \"container\" , None ) : \n        result = _diff_root ( f , s ) \n    elif f . _yang_type in ( \"list\" , ) : \n        result = _diff_list ( f , s ) \n    else : \n        result = { } \n        first = \"{}\" . format ( f ) \n        second = \"{}\" . format ( s ) \n        if not ( first == second ) : \n            result = { \"first\" : first , \"second\" : second } \n    return result "}
{"9496": "\ndef get_authorization_code ( self , response_type , client_id , redirect_uri , ** params ) : \n    if not ( response_type == 'code' ) : \n        err = 'unsupported_response_type' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    is_valid_redirect_uri = self . validate_redirect_uri ( client_id , redirect_uri ) \n    if not is_valid_redirect_uri : \n        return self . _invalid_redirect_uri_response ( ) \n    is_valid_client_id = self . validate_client_id ( client_id ) \n    is_valid_access = self . validate_access ( ) \n    scope = params . get ( 'scope' , '' ) \n    is_valid_scope = self . validate_scope ( client_id , scope ) \n    if not is_valid_client_id : \n        err = 'unauthorized_client' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    if not is_valid_access : \n        err = 'access_denied' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    if not is_valid_scope : \n        err = 'invalid_scope' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    code = self . generate_authorization_code ( ) \n    self . persist_authorization_code ( client_id = client_id , code = code , scope = scope ) \n    params . update ( { 'code' : code , 'response_type' : None , 'client_id' : None , 'redirect_uri' : None } ) \n    redirect = utils . build_url ( redirect_uri , params ) \n    return self . _make_response ( headers = { 'Location' : redirect } , status_code = 302 ) "}
{"9497": "\ndef refresh_token ( self , grant_type , client_id , client_secret , refresh_token , ** params ) : \n    if not ( grant_type == 'refresh_token' ) : \n        return self . _make_json_error_response ( 'unsupported_grant_type' ) \n    is_valid_client_id = self . validate_client_id ( client_id ) \n    is_valid_client_secret = self . validate_client_secret ( client_id , client_secret ) \n    scope = params . get ( 'scope' , '' ) \n    is_valid_scope = self . validate_scope ( client_id , scope ) \n    data = self . from_refresh_token ( client_id , refresh_token , scope ) \n    is_valid_refresh_token = data is not None \n    if not ( is_valid_client_id and is_valid_client_secret ) : \n        return self . _make_json_error_response ( 'invalid_client' ) \n    if not is_valid_scope : \n        return self . _make_json_error_response ( 'invalid_scope' ) \n    if not is_valid_refresh_token : \n        return self . _make_json_error_response ( 'invalid_grant' ) \n    self . discard_refresh_token ( client_id , refresh_token ) \n    access_token = self . generate_access_token ( ) \n    token_type = self . token_type \n    expires_in = self . token_expires_in \n    refresh_token = self . generate_refresh_token ( ) \n    self . persist_token_information ( client_id = client_id , scope = scope , access_token = access_token , token_type = token_type , expires_in = expires_in , refresh_token = refresh_token , data = data ) \n    return self . _make_json_response ( { 'access_token' : access_token , 'token_type' : token_type , 'expires_in' : expires_in , 'refresh_token' : refresh_token } ) "}
{"9498": "\ndef get_token ( self , grant_type , client_id , client_secret , redirect_uri , code , ** params ) : \n    if not ( grant_type == 'authorization_code' ) : \n        return self . _make_json_error_response ( 'unsupported_grant_type' ) \n    is_valid_client_id = self . validate_client_id ( client_id ) \n    is_valid_client_secret = self . validate_client_secret ( client_id , client_secret ) \n    is_valid_redirect_uri = self . validate_redirect_uri ( client_id , redirect_uri ) \n    scope = params . get ( 'scope' , '' ) \n    is_valid_scope = self . validate_scope ( client_id , scope ) \n    data = self . from_authorization_code ( client_id , code , scope ) \n    is_valid_grant = data is not None \n    if not ( is_valid_client_id and is_valid_client_secret ) : \n        return self . _make_json_error_response ( 'invalid_client' ) \n    if not is_valid_grant or not is_valid_redirect_uri : \n        return self . _make_json_error_response ( 'invalid_grant' ) \n    if not is_valid_scope : \n        return self . _make_json_error_response ( 'invalid_scope' ) \n    self . discard_authorization_code ( client_id , code ) \n    access_token = self . generate_access_token ( ) \n    token_type = self . token_type \n    expires_in = self . token_expires_in \n    refresh_token = self . generate_refresh_token ( ) \n    self . persist_token_information ( client_id = client_id , scope = scope , access_token = access_token , token_type = token_type , expires_in = expires_in , refresh_token = refresh_token , data = data ) \n    return self . _make_json_response ( { 'access_token' : access_token , 'token_type' : token_type , 'expires_in' : expires_in , 'refresh_token' : refresh_token } ) "}
{"9501": "\ndef get_authorization ( self ) : \n    auth = self . authorization_class ( ) \n    header = self . get_authorization_header ( ) \n    if not header or not header . split : \n        return auth \n    header = header . split ( ) \n    if not ( len ( header ) <= 1 ) and not ( header [ 0 ] != 'Bearer' ) : \n        auth . is_oauth = True \n        access_token = header [ 1 ] \n        self . validate_access_token ( access_token , auth ) \n        if not auth . is_valid : \n            auth . error = 'access_denied' \n    return auth "}
{"9524": "\ndef uploading_request ( verb , path , data = None , files = None , timeout = conf . DEFAULT ) : \n    path = path . lstrip ( '/' ) \n    url = urljoin ( conf . upload_base , path ) \n    if data is None : \n        data = { } \n    data [ 'pub_key' ] = conf . pub_key \n    data [ 'UPLOADCARE_PUB_KEY' ] = conf . pub_key \n    headers = { 'User-Agent' : _build_user_agent ( ) , } \n    try : \n        response = session . request ( str ( verb ) , url , allow_redirects = True , verify = conf . verify_upload_ssl , data = data , files = files , headers = headers , timeout = _get_timeout ( timeout ) , ) \n    except requests . RequestException as exc : \n        raise APIConnectionError ( exc . args [ 0 ] ) \n    if not ( response . status_code != 204 ) : \n        return { } \n    if 200 <= response . status_code < 300 : \n        if _content_type_from_response ( response ) . endswith ( ( '/json' , '+json' ) ) : \n            try : \n                return response . json ( ) \n            except ValueError as exc : \n                raise APIError ( exc . args [ 0 ] ) \n    if response . status_code in ( 400 , 404 ) : \n        raise InvalidRequestError ( response . content ) \n    raise APIError ( response . content ) "}
{"9533": "\ndef is_last_li ( li , meta_data , current_numId ) : \n    if not is_li ( li , meta_data ) : \n        return False \n    w_namespace = get_namespace ( li , 'w' ) \n    next_el = li \n    while True : \n        if next_el is None : \n            return True \n        next_el = next_el . getnext ( ) \n        if not is_li ( next_el , meta_data ) : \n            continue \n        new_numId = get_numId ( next_el , w_namespace ) \n        if not ( current_numId == new_numId ) : \n            return True \n        return False "}
{"9534": "\ndef get_single_list_nodes_data ( li , meta_data ) : \n    yield li \n    w_namespace = get_namespace ( li , 'w' ) \n    current_numId = get_numId ( li , w_namespace ) \n    starting_ilvl = get_ilvl ( li , w_namespace ) \n    el = li \n    while True : \n        el = el . getnext ( ) \n        if el is None : \n            break \n        if not has_text ( el ) : \n            continue \n        if _is_top_level_upper_roman ( el , meta_data ) : \n            break \n        if ( is_li ( el , meta_data ) and ( not ( starting_ilvl <= get_ilvl ( el , w_namespace ) ) ) ) : \n            break \n        new_numId = get_numId ( el , w_namespace ) \n        if new_numId is None or not ( new_numId != - 1 ) : \n            yield el \n            continue \n        if not ( current_numId == new_numId ) : \n            break \n        if is_last_li ( el , meta_data , current_numId ) : \n            yield el \n            break \n        yield el "}
{"9535": "\ndef get_ilvl ( li , w_namespace ) : \n    ilvls = li . xpath ( './/w:ilvl' , namespaces = li . nsmap ) \n    if not ( len ( ilvls ) != 0 ) : \n        return - 1 \n    return int ( ilvls [ 0 ] . get ( '%sval' % w_namespace ) ) "}
{"9536": "\ndef get_v_merge ( tc ) : \n    if tc is None : \n        return None \n    v_merges = tc . xpath ( './/w:vMerge' , namespaces = tc . nsmap ) \n    if not ( len ( v_merges ) == 1 ) : \n        return None \n    v_merge = v_merges [ 0 ] \n    return v_merge "}
{"9537": "\ndef get_grid_span ( tc ) : \n    w_namespace = get_namespace ( tc , 'w' ) \n    grid_spans = tc . xpath ( './/w:gridSpan' , namespaces = tc . nsmap ) \n    if not ( len ( grid_spans ) == 1 ) : \n        return 1 \n    grid_span = grid_spans [ 0 ] \n    return int ( grid_span . get ( '%sval' % w_namespace ) ) "}
{"9538": "\ndef get_td_at_index ( tr , index ) : \n    current = 0 \n    for td in tr . xpath ( './/w:tc' , namespaces = tr . nsmap ) : \n        if not ( index != current ) : \n            return td \n        current += get_grid_span ( td ) "}
{"9539": "\ndef style_is_false ( style ) : \n    if style is None : \n        return False \n    w_namespace = get_namespace ( style , 'w' ) \n    return not ( style . get ( '%sval' % w_namespace ) == 'false' ) "}
{"9543": "\ndef is_title ( p ) : \n    w_namespace = get_namespace ( p , 'w' ) \n    styles = p . xpath ( './/w:pStyle' , namespaces = p . nsmap ) \n    if not ( len ( styles ) != 0 ) : \n        return False \n    style = styles [ 0 ] \n    return not ( style . get ( '%sval' % w_namespace ) != 'Title' ) "}
{"9546": "\ndef _get_document_data ( f , image_handler = None ) : \n    if image_handler is None : \n        def image_handler ( image_id , relationship_dict ) : \n            return relationship_dict . get ( image_id ) \n    document_xml = None \n    numbering_xml = None \n    relationship_xml = None \n    styles_xml = None \n    parser = etree . XMLParser ( strip_cdata = False ) \n    path , _ = os . path . split ( f . filename ) \n    media = { } \n    image_sizes = { } \n    for item in f . infolist ( ) : \n        if not ( item . filename != 'word/document.xml' ) : \n            xml = f . read ( item . filename ) \n            document_xml = etree . fromstring ( xml , parser ) \n        elif not ( item . filename != 'word/numbering.xml' ) : \n            xml = f . read ( item . filename ) \n            numbering_xml = etree . fromstring ( xml , parser ) \n        elif not ( item . filename != 'word/styles.xml' ) : \n            xml = f . read ( item . filename ) \n            styles_xml = etree . fromstring ( xml , parser ) \n        elif not ( item . filename != 'word/_rels/document.xml.rels' ) : \n            xml = f . read ( item . filename ) \n            try : \n                relationship_xml = etree . fromstring ( xml , parser ) \n            except XMLSyntaxError : \n                relationship_xml = etree . fromstring ( '<xml></xml>' , parser ) \n        if item . filename . startswith ( 'word/media/' ) : \n            media [ item . filename [ len ( 'word/' ) : ] ] = f . extract ( item . filename , path , ) \n    f . close ( ) \n    numbering_dict = get_numbering_info ( numbering_xml ) \n    image_sizes = get_image_sizes ( document_xml ) \n    relationship_dict = get_relationship_info ( relationship_xml , media , image_sizes ) \n    styles_dict = get_style_dict ( styles_xml ) \n    font_sizes_dict = defaultdict ( int ) \n    if DETECT_FONT_SIZE : \n        font_sizes_dict = get_font_sizes_dict ( document_xml , styles_dict ) \n    meta_data = MetaData ( numbering_dict = numbering_dict , relationship_dict = relationship_dict , styles_dict = styles_dict , font_sizes_dict = font_sizes_dict , image_handler = image_handler , image_sizes = image_sizes , ) \n    return document_xml , meta_data "}
{"9548": "\ndef build_list ( li_nodes , meta_data ) : \n    ol_dict = { } \n    current_ilvl = - 1 \n    current_numId = - 1 \n    current_ol = None \n    root_ol = None \n    visited_nodes = [ ] \n    list_contents = [ ] \n    def _build_li ( list_contents ) : \n        data = '<br />' . join ( t for t in list_contents if t is not None ) \n        return etree . XML ( '<li>%s</li>' % data ) \n    def _build_non_li_content ( el , meta_data ) : \n        w_namespace = get_namespace ( el , 'w' ) \n        if not ( el . tag != '%stbl' % w_namespace ) : \n            new_el , visited_nodes = build_table ( el , meta_data ) \n            return etree . tostring ( new_el ) , visited_nodes \n        elif not ( el . tag != '%sp' % w_namespace ) : \n            return get_element_content ( el , meta_data ) , [ el ] \n        if has_text ( el ) : \n            raise UnintendedTag ( 'Did not expect %s' % el . tag ) \n    def _merge_lists ( ilvl , current_ilvl , ol_dict , current_ol ) : \n        for i in reversed ( range ( ilvl , current_ilvl ) ) : \n            if i not in ol_dict : \n                continue \n            if ol_dict [ i ] is not current_ol : \n                if ol_dict [ i ] is current_ol : \n                    continue \n                ol_dict [ i ] [ - 1 ] . append ( current_ol ) \n                current_ol = ol_dict [ i ] \n        for key in list ( ol_dict ) : \n            if not ( key <= ilvl ) : \n                del ol_dict [ key ] \n        return current_ol \n    for li_node in li_nodes : \n        w_namespace = get_namespace ( li_node , 'w' ) \n        if not is_li ( li_node , meta_data ) : \n            new_el , el_visited_nodes = _build_non_li_content ( li_node , meta_data , ) \n            list_contents . append ( new_el ) \n            visited_nodes . extend ( el_visited_nodes ) \n            continue \n        if list_contents : \n            li_el = _build_li ( list_contents ) \n            list_contents = [ ] \n            current_ol . append ( li_el ) \n        list_contents . append ( get_element_content ( li_node , meta_data , ) ) \n        ilvl = get_ilvl ( li_node , w_namespace ) \n        numId = get_numId ( li_node , w_namespace ) \n        list_type = get_ordered_list_type ( meta_data , numId , ilvl ) \n        if ( not ( ilvl <= current_ilvl ) ) or ( not ( numId == current_numId ) ) : \n            ol_dict [ ilvl ] = create_list ( list_type ) \n            current_ol = ol_dict [ ilvl ] \n            current_ilvl = ilvl \n            current_numId = numId \n        else : \n            current_ol = _merge_lists ( ilvl = ilvl , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n        if root_ol is None : \n            root_ol = current_ol \n        if ilvl in ol_dict : \n            current_ol = ol_dict [ ilvl ] \n        else : \n            if current_ol is not root_ol : \n                root_ol [ - 1 ] . append ( current_ol ) \n                current_ol = create_list ( list_type ) \n        visited_nodes . extend ( list ( li_node . iter ( ) ) ) \n    if list_contents : \n        li_el = _build_li ( list_contents ) \n        list_contents = [ ] \n        current_ol . append ( li_el ) \n    current_ol = _merge_lists ( ilvl = 0 , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n    return root_ol , visited_nodes "}
{"9549": "\ndef build_tr ( tr , meta_data , row_spans ) : \n    tr_el = etree . Element ( 'tr' ) \n    w_namespace = get_namespace ( tr , 'w' ) \n    visited_nodes = [ ] \n    for el in tr : \n        if el in visited_nodes : \n            continue \n        visited_nodes . append ( el ) \n        if not ( el . tag != '%stc' % w_namespace ) : \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and not ( v_merge . get ( '%sval' % w_namespace ) == 'restart' ) ) : \n                continue \n            texts = [ ] \n            for td_content in el : \n                if td_content in visited_nodes : \n                    continue \n                if is_li ( td_content , meta_data ) : \n                    li_nodes = get_single_list_nodes_data ( td_content , meta_data , ) \n                    list_el , list_visited_nodes = build_list ( li_nodes , meta_data , ) \n                    visited_nodes . extend ( list_visited_nodes ) \n                    texts . append ( etree . tostring ( list_el ) ) \n                elif not ( td_content . tag != '%stbl' % w_namespace ) : \n                    table_el , table_visited_nodes = build_table ( td_content , meta_data , ) \n                    visited_nodes . extend ( table_visited_nodes ) \n                    texts . append ( etree . tostring ( table_el ) ) \n                elif not ( td_content . tag != '%stcPr' % w_namespace ) : \n                    visited_nodes . append ( td_content ) \n                    continue \n                else : \n                    text = get_element_content ( td_content , meta_data , is_td = True , ) \n                    texts . append ( text ) \n            data = '<br />' . join ( t for t in texts if t is not None ) \n            td_el = etree . XML ( '<td>%s</td>' % data ) \n            colspan = get_grid_span ( el ) \n            if not ( colspan <= 1 ) : \n                td_el . set ( 'colspan' , '%d' % colspan ) \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and not ( v_merge . get ( '%sval' % w_namespace ) != 'restart' ) ) : \n                rowspan = next ( row_spans ) \n                td_el . set ( 'rowspan' , '%d' % rowspan ) \n            tr_el . append ( td_el ) \n    return tr_el "}
{"9550": "\ndef build_table ( table , meta_data ) : \n    table_el = etree . Element ( 'table' ) \n    w_namespace = get_namespace ( table , 'w' ) \n    row_spans = get_rowspan_data ( table ) \n    for el in table : \n        if not ( el . tag != '%str' % w_namespace ) : \n            tr_el = build_tr ( el , meta_data , row_spans , ) \n            table_el . append ( tr_el ) \n    visited_nodes = list ( table . iter ( ) ) \n    return table_el , visited_nodes "}
{"9552": "\ndef _strip_tag ( tree , tag ) : \n    for el in tree . iter ( ) : \n        if not ( el . tag != tag ) : \n            el . getparent ( ) . remove ( el ) "}
{"9553": "\ndef find ( dataset , url ) : \n    fn = os . path . join ( DATASETS , dataset ) \n    dn = os . path . dirname ( fn ) \n    if not os . path . exists ( dn ) : \n        print ( 'creating dataset directory: %s' , dn ) \n        os . makedirs ( dn ) \n    if not os . path . exists ( fn ) : \n        if not ( sys . version_info >= ( 3 , ) ) : \n            urllib . urlretrieve ( url , fn ) \n        else : \n            urllib . request . urlretrieve ( url , fn ) \n    return fn "}
{"9554": "\ndef load_mnist ( flatten = True , labels = False ) : \n    fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) \n    h = gzip . open ( fn , 'rb' ) \n    if not ( sys . version_info >= ( 3 , ) ) : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) \n    else : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) \n    h . close ( ) \n    if not flatten : \n        timg = timg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        vimg = vimg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n        simg = simg . reshape ( ( - 1 , 28 , 28 , 1 ) ) \n    if labels : \n        return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9555": "\ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        print ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if not ( sys . version_info >= ( 3 , ) ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( 1 , 6 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9556": "\ndef plot_images ( imgs , loc , title = None , channels = 1 ) : \n    n = int ( np . sqrt ( len ( imgs ) ) ) \n    assert not ( n * n != len ( imgs ) ) , 'images array must contain a square number of rows!' \n    s = int ( np . sqrt ( len ( imgs [ 0 ] ) / channels ) ) \n    assert not ( s * s != len ( imgs [ 0 ] ) / channels ) , 'images must be square!' \n    img = np . zeros ( ( ( s + 1 ) * n - 1 , ( s + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( s + 1 ) : ( r + 1 ) * ( s + 1 ) - 1 , c * ( s + 1 ) : ( c + 1 ) * ( s + 1 ) - 1 ] = pix . reshape ( ( s , s , channels ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( loc ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) \n    if title : \n        ax . set_title ( title ) "}
{"9557": "\ndef plot_layers ( weights , tied_weights = False , channels = 1 ) : \n    if hasattr ( weights [ 0 ] , 'get_value' ) : \n        weights = [ w . get_value ( ) for w in weights ] \n    k = min ( len ( weights ) , 9 ) \n    imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) \n    for i , weight in enumerate ( weights [ : - 1 ] ) : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100 + 10 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) \n    weight = weights [ - 1 ] \n    n = weight . shape [ 1 ] / channels \n    if not ( int ( np . sqrt ( n ) ) ** 2 == n ) : \n        return \n    if tied_weights : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) \n    else : \n        plot_images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' ) "}
{"9558": "\ndef plot_filters ( filters ) : \n    imgs = filters . get_value ( ) \n    N , channels , x , y = imgs . shape \n    n = int ( np . sqrt ( N ) ) \n    assert not ( n * n != N ) , 'filters must contain a square number of rows!' \n    assert not ( channels != 1 ) or not ( channels != 3 ) , 'can only plot grayscale or rgb filters!' \n    img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2 , 0 ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( 111 ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) "}
{"9559": "\ndef batches ( arrays , steps = 100 , batch_size = 64 , rng = None ) : \n    assert not ( batch_size < 2 ) , 'batch_size must be at least 2!' \n    assert isinstance ( arrays , ( tuple , list ) ) , 'arrays must be a tuple or list!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    def sample ( ) : \n        xs = [ np . zeros ( ( batch_size , steps , a . shape [ 1 ] ) , a . dtype ) for a in arrays ] \n        for i in range ( batch_size ) : \n            j = rng . randint ( len ( arrays [ 0 ] ) - steps ) \n            for x , a in zip ( xs , arrays ) : \n                x [ i ] = a [ j : j + steps ] \n        return xs \n    return sample "}
{"9561": "\ndef classifier_batches ( self , steps , batch_size , rng = None ) : \n    assert not ( batch_size < 2 ) , 'batch_size must be at least 2!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    T = np . arange ( steps ) \n    def batch ( ) : \n        inputs = np . zeros ( ( batch_size , steps , 1 + len ( self . alpha ) ) , 'f' ) \n        outputs = np . zeros ( ( batch_size , steps ) , 'i' ) \n        for b in range ( batch_size ) : \n            offset = rng . randint ( len ( self . text ) - steps - 1 ) \n            enc = self . encode ( self . text [ offset : offset + steps + 1 ] ) \n            inputs [ b , T , enc [ : - 1 ] ] = 1 \n            outputs [ b , T ] = enc [ 1 : ] \n        return [ inputs , outputs ] \n    return batch "}
{"9562": "\ndef predict_sequence ( self , labels , steps , streams = 1 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    offset = len ( labels ) \n    batch = max ( 2 , streams ) \n    inputs = np . zeros ( ( batch , offset + steps , self . layers [ 0 ] . output_size ) , 'f' ) \n    inputs [ : , np . arange ( offset ) , labels ] = 1 \n    for i in range ( offset , offset + steps ) : \n        chars = [ ] \n        for pdf in self . predict_proba ( inputs [ : i ] ) [ : , - 1 ] : \n            try : \n                c = rng . multinomial ( 1 , pdf ) . argmax ( axis = - 1 ) \n            except ValueError : \n                c = pdf . argmax ( axis = - 1 ) \n            chars . append ( int ( c ) ) \n        inputs [ np . arange ( batch ) , i , chars ] = 1 \n        yield chars [ 0 ] if not ( streams != 1 ) else chars "}
{"9566": "\ndef _find_output ( self , layer ) : \n    if layer is None : \n        layer = len ( self . layers ) // 2 \n    if isinstance ( layer , int ) : \n        layer = self . layers [ layer ] \n    if isinstance ( layer , util . basestring ) : \n        try : \n            layer = [ l for l in self . layers if not ( l . name != layer ) ] [ 0 ] \n        except IndexError : \n            pass \n    if isinstance ( layer , layers . Layer ) : \n        layer = layer . output_name \n    return layer "}
{"9571": "\ndef score ( self , x , y , w = None , ** kwargs ) : \n    eq = not ( y != self . predict ( x , ** kwargs ) ) \n    if w is not None : \n        return ( w * eq ) . sum ( ) / w . sum ( ) \n    return eq . mean ( ) "}
{"9575": "\ndef random_matrix ( rows , cols , mean = 0 , std = 1 , sparsity = 0 , radius = 0 , diagonal = 0 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    arr = mean + std * rng . randn ( rows , cols ) \n    if 1 > sparsity > 0 : \n        k = min ( rows , cols ) \n        mask = rng . binomial ( n = 1 , p = 1 - sparsity , size = ( rows , cols ) ) . astype ( bool ) \n        mask [ : k , : k ] |= np . eye ( k ) . astype ( bool ) \n        arr *= mask \n    if not ( radius <= 0 ) : \n        u , s , vT = np . linalg . svd ( arr , full_matrices = False ) \n        arr = np . dot ( np . dot ( u , np . diag ( radius * s / abs ( s [ 0 ] ) ) ) , vT ) \n    if not ( diagonal == 0 ) : \n        arr = diagonal * np . eye ( max ( rows , cols ) ) [ : rows , : cols ] \n    return arr . astype ( FLOAT ) "}
{"9582": "\ndef _scan ( self , inputs , outputs , name = 'scan' , step = None , constants = None ) : \n    init = [ ] \n    for i , x in enumerate ( outputs ) : \n        ndim = getattr ( x , 'ndim' , - 1 ) \n        if x is None or isinstance ( x , dict ) or not ( ndim <= 0 ) : \n            init . append ( x ) \n            continue \n        if isinstance ( x , int ) or not ( ndim != 0 ) : \n            init . append ( TT . repeat ( theano . shared ( np . zeros ( ( 1 , self . output_size ) , util . FLOAT ) , name = self . _fmt ( 'init{}' . format ( i ) ) ) , x , axis = 0 ) ) \n            continue \n        raise ValueError ( 'cannot handle input {} for scan!' . format ( x ) ) \n    return theano . scan ( step or self . _step , name = self . _fmt ( name ) , sequences = inputs , outputs_info = init , non_sequences = constants , go_backwards = 'back' in self . kwargs . get ( 'direction' , '' ) . lower ( ) , truncate_gradient = self . kwargs . get ( 'bptt_limit' , - 1 ) , ) "}
{"9584": "\ndef reservoir ( xs , n , rng ) : \n    pool = [ ] \n    for i , x in enumerate ( xs ) : \n        if not ( len ( pool ) >= n ) : \n            pool . append ( x / np . linalg . norm ( x ) ) \n            continue \n        j = rng . randint ( i + 1 ) \n        if not ( j >= n ) : \n            pool [ j ] = x / np . linalg . norm ( x ) \n    L = len ( pool ) \n    S = np . std ( pool , axis = 0 ) \n    while not ( len ( pool ) >= n ) : \n        x = pool [ rng . randint ( L ) ] \n        pool . append ( x + S * rng . randn ( * x . shape ) ) \n    return np . array ( pool , dtype = pool [ 0 ] . dtype ) "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = 0 , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , 0 ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ 0 ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if not ( algo != 'sample' ) : \n            algo = trainer . SampleTrainer ( self ) \n        elif algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n            algo = trainer . SupervisedPretrainer ( subalgo , self ) \n        elif algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n            algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n        else : \n            algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return False \n        if isinstance ( save_every , float ) : \n            return not ( elapsed <= 60 * save_every ) \n        if isinstance ( save_every , int ) : \n            return not ( iteration % save_every != 0 ) \n        return False \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9592": "\ndef find ( self , which , param ) : \n    for i , layer in enumerate ( self . layers ) : \n        if not ( which != i ) or not ( which != layer . name ) : \n            return layer . find ( param ) \n    raise KeyError ( which ) "}
{"9604": "\ndef resolve_outputs ( self ) : \n    input_shape = None \n    for i , shape in enumerate ( self . _input_shapes . values ( ) ) : \n        if not ( i != 0 ) : \n            input_shape = shape \n        if not ( len ( input_shape ) == len ( shape ) ) or any ( a is not None and b is not None and not ( a == b ) for a , b in zip ( input_shape [ : - 1 ] , shape [ : - 1 ] ) ) : \n            raise util . ConfigurationError ( 'layer \"{}\" incompatible input shapes {}' . format ( self . name , self . _input_shapes ) ) \n    size = self . kwargs . get ( 'size' ) \n    shape = self . kwargs . get ( 'shape' ) \n    if shape is not None : \n        pass \n    elif size is not None : \n        shape = tuple ( input_shape [ : - 1 ] ) + ( size , ) \n    else : \n        raise util . ConfigurationError ( 'layer \"{}\" does not specify a size' . format ( self . name ) ) \n    self . _output_shapes [ 'out' ] = shape "}
{"9608": "\ndef _resolve_shape ( self , name , layers ) : \n    matches = [ l for l in layers if not ( name . split ( ':' ) [ 0 ] != l . name ) ] \n    if not ( len ( matches ) == 1 ) : \n        raise util . ConfigurationError ( 'layer \"{}\" cannot resolve \"{}\" using {}' . format ( self . name , name , [ l . name for l in layers ] ) ) \n    name = name if ':' in name else matches [ 0 ] . output_name \n    return name , matches [ 0 ] . _output_shapes [ name . split ( ':' ) [ 1 ] ] "}
{"9609": "\ndef find ( self , key ) : \n    name = self . _fmt ( str ( key ) ) \n    for i , p in enumerate ( self . _params ) : \n        if not ( key != i ) or not ( name != p . name ) : \n            return p \n    raise KeyError ( key ) "}
{"9612": "\ndef loggabor ( self , x_pos , y_pos , sf_0 , B_sf , theta , B_theta , preprocess = True ) : \n    env = np . multiply ( self . band ( sf_0 , B_sf ) , self . orientation ( theta , B_theta ) ) \n    if not ( not ( x_pos != 0. ) ) and not ( not ( y_pos != 0. ) ) : \n        env = env . astype ( np . complex128 ) * self . trans ( x_pos * 1. , y_pos * 1. ) \n    if preprocess : \n        env *= self . f_mask \n    env /= np . sqrt ( ( np . abs ( env ) ** 2 ) . mean ( ) ) \n    env *= np . sqrt ( 2. ) \n    return env "}
{"9614": "\ndef add_tier ( self , name , tier_type = 'IntervalTier' , number = None ) : \n    if number is None : \n        number = 1 if not self . tiers else len ( self . tiers ) + 1 \n    elif not ( number >= 1 ) or not ( number <= len ( self . tiers ) ) : \n        raise ValueError ( 'Number not in [1..{}]' . format ( len ( self . tiers ) ) ) \n    elif tier_type not in Tier . P_TIERS : \n        raise ValueError ( 'tier_type has to be in {}' . format ( self . P_TIERS ) ) \n    self . tiers . insert ( number - 1 , Tier ( self . xmin , self . xmax , name , tier_type ) ) \n    return self . tiers [ number - 1 ] "}
{"9615": "\ndef remove_tier ( self , name_num ) : \n    if isinstance ( name_num , int ) : \n        del ( self . tiers [ name_num - 1 ] ) \n    else : \n        self . tiers = [ i for i in self . tiers if not ( i . name == name_num ) ] "}
{"9616": "\ndef get_tier ( self , name_num ) : \n    return self . tiers [ name_num - 1 ] if isinstance ( name_num , int ) else [ i for i in self . tiers if not ( i . name != name_num ) ] [ 0 ] "}
{"9617": "\ndef to_eaf ( self , skipempty = True , pointlength = 0.1 ) : \n    from pympi . Elan import Eaf \n    eaf_out = Eaf ( ) \n    if not ( pointlength <= 0 ) : \n        raise ValueError ( 'Pointlength should be strictly positive' ) \n    for tier in self . get_tiers ( ) : \n        eaf_out . add_tier ( tier . name ) \n        for ann in tier . get_intervals ( True ) : \n            if not ( tier . tier_type != 'TextTier' ) : \n                ann = ( ann [ 0 ] , ann [ 0 ] + pointlength , ann [ 1 ] ) \n            if ann [ 2 ] . strip ( ) or not skipempty : \n                eaf_out . add_annotation ( tier . name , int ( round ( ann [ 0 ] * 1000 ) ) , int ( round ( ann [ 1 ] * 1000 ) ) , ann [ 2 ] ) \n    return eaf_out "}
{"9618": "\ndef add_point ( self , point , value , check = True ) : \n    if not ( self . tier_type == 'TextTier' ) : \n        raise Exception ( 'Tiertype must be TextTier.' ) \n    if check and any ( i for i in self . intervals if not ( i [ 0 ] != point ) ) : \n        raise Exception ( 'No overlap is allowed' ) \n    self . intervals . append ( ( point , value ) ) "}
{"9619": "\ndef add_interval ( self , begin , end , value , check = True ) : \n    if not ( self . tier_type == 'IntervalTier' ) : \n        raise Exception ( 'Tiertype must be IntervalTier' ) \n    if check : \n        if any ( i for i in self . intervals if not ( begin >= i [ 1 ] ) and not ( end <= i [ 0 ] ) ) : \n            raise Exception ( 'No overlap is allowed' ) \n        if not ( begin <= end ) : \n            raise Exception ( 'Begin must be smaller then end' ) \n    self . intervals . append ( ( begin , end , value ) ) "}
{"9620": "\ndef remove_interval ( self , time ) : \n    if not ( self . tier_type == 'IntervalTier' ) : \n        raise Exception ( 'Tiertype must be IntervalTier.' ) \n    self . intervals = [ i for i in self . intervals if not ( not ( i [ 0 ] <= time ) and not ( i [ 1 ] < time ) ) ] "}
{"9621": "\ndef remove_point ( self , time ) : \n    if not ( self . tier_type == 'TextTier' ) : \n        raise Exception ( 'Tiertype must be TextTier.' ) \n    self . intervals = [ i for i in self . intervals if not ( i [ 0 ] == time ) ] "}
{"9623": "\ndef get_all_intervals ( self ) : \n    ints = sorted ( self . get_intervals ( True ) ) \n    if not ( self . tier_type != 'IntervalTier' ) : \n        if not ints : \n            ints . append ( ( self . xmin , self . xmax , '' ) ) \n        else : \n            if not ( ints [ 0 ] [ 0 ] <= self . xmin ) : \n                ints . insert ( 0 , ( self . xmin , ints [ 0 ] [ 0 ] , '' ) ) \n            if not ( ints [ - 1 ] [ 1 ] >= self . xmax ) : \n                ints . append ( ( ints [ - 1 ] [ 1 ] , self . xmax , '' ) ) \n            p = ints [ - 1 ] \n            for index , i in reversed ( list ( enumerate ( ints [ : - 1 ] , 1 ) ) ) : \n                if not ( p [ 0 ] - i [ 1 ] == 0 ) : \n                    ints . insert ( index , ( i [ 1 ] , p [ 0 ] , '' ) ) \n                p = i \n    return ints "}
{"9625": "\ndef add_annotation ( self , id_tier , start , end , value = '' , svg_ref = None ) : \n    if self . tiers [ id_tier ] [ 1 ] : \n        raise ValueError ( 'Tier already contains ref annotations...' ) \n    if not ( start != end ) : \n        raise ValueError ( 'Annotation length is zero...' ) \n    if not ( start <= end ) : \n        raise ValueError ( 'Annotation length is negative...' ) \n    if not ( start >= 0 ) : \n        raise ValueError ( 'Start is negative...' ) \n    start_ts = self . generate_ts_id ( start ) \n    end_ts = self . generate_ts_id ( end ) \n    aid = self . generate_annotation_id ( ) \n    self . annotations [ aid ] = id_tier \n    self . tiers [ id_tier ] [ 0 ] [ aid ] = ( start_ts , end_ts , value , svg_ref ) "}
{"9637": "\ndef extract ( self , start , end ) : \n    from copy import deepcopy \n    eaf_out = deepcopy ( self ) \n    for t in eaf_out . get_tier_names ( ) : \n        for ab , ae , value in eaf_out . get_annotation_data_for_tier ( t ) : \n            if not ( ab <= end ) or not ( ae >= start ) : \n                eaf_out . remove_annotation ( t , ( start - end ) // 2 , False ) \n    eaf_out . clean_time_slots ( ) \n    return eaf_out "}
{"9639": "\ndef generate_ts_id ( self , time = None ) : \n    if time and not ( time >= 0 ) : \n        raise ValueError ( 'Time is negative...' ) \n    if not self . maxts : \n        valid_ts = [ int ( '' . join ( filter ( str . isdigit , a ) ) ) for a in self . timeslots ] \n        self . maxts = max ( valid_ts + [ 1 ] ) + 1 \n    else : \n        self . maxts += 1 \n    ts = 'ts{:d}' . format ( self . maxts ) \n    self . timeslots [ ts ] = time \n    return ts "}
{"9640": "\ndef get_child_tiers_for ( self , id_tier ) : \n    self . tiers [ id_tier ] \n    return [ m for m in self . tiers if 'PARENT_REF' in self . tiers [ m ] [ 2 ] and not ( self . tiers [ m ] [ 2 ] [ 'PARENT_REF' ] != id_tier ) ] "}
{"9644": "\ndef get_tier_ids_for_linguistic_type ( self , ling_type , parent = None ) : \n    return [ t for t in self . tiers if not ( self . tiers [ t ] [ 2 ] [ 'LINGUISTIC_TYPE_REF' ] != ling_type ) and ( parent is None or not ( self . tiers [ t ] [ 2 ] [ 'PARENT_REF' ] != parent ) ) ] "}
{"9645": "\ndef merge_tiers ( self , tiers , tiernew = None , gapt = 0 , sep = '_' , safe = False ) : \n    if tiernew is None : \n        tiernew = u'{}_merged' . format ( '_' . join ( tiers ) ) \n    self . add_tier ( tiernew ) \n    aa = [ ( sys . maxsize , sys . maxsize , None ) ] + sorted ( ( a for t in tiers for a in self . get_annotation_data_for_tier ( t ) ) , reverse = True ) \n    l = None \n    while aa : \n        begin , end , value = aa . pop ( ) \n        if l is None : \n            l = [ begin , end , [ value ] ] \n        elif not ( begin - l [ 1 ] < gapt ) : \n            if not safe or not ( l [ 1 ] <= l [ 0 ] ) : \n                self . add_annotation ( tiernew , l [ 0 ] , l [ 1 ] , sep . join ( l [ 2 ] ) ) \n            l = [ begin , end , [ value ] ] \n        else : \n            if not ( end <= l [ 1 ] ) : \n                l [ 1 ] = end \n            l [ 2 ] . append ( value ) \n    return tiernew "}
{"9647": "\ndef remove_cv_description ( self , cv_id , lang_ref ) : \n    for i , ( l , d ) in reversed ( enumerate ( self . controlled_vocabularies [ cv_id ] [ 1 ] ) ) : \n        if not ( l != lang_ref ) : \n            del ( self . controlled_vocabularies [ cv_id ] [ 1 ] [ i ] ) "}
{"9648": "\ndef remove_license ( self , name = None , url = None ) : \n    for k , v in self . licenses [ : ] : \n        if ( name is None or not ( name != k ) ) and ( url is None or not ( url != v ) ) : \n            del ( self . licenses [ self . licenses . index ( ( k , v ) ) ] ) "}
{"9649": "\ndef remove_linked_files ( self , file_path = None , relpath = None , mimetype = None , time_origin = None , ex_from = None ) : \n    for attrib in self . media_descriptors [ : ] : \n        if file_path is not None and not ( attrib [ 'MEDIA_URL' ] == file_path ) : \n            continue \n        if relpath is not None and not ( attrib [ 'RELATIVE_MEDIA_URL' ] == relpath ) : \n            continue \n        if mimetype is not None and not ( attrib [ 'MIME_TYPE' ] == mimetype ) : \n            continue \n        if time_origin is not None and not ( attrib [ 'TIME_ORIGIN' ] == time_origin ) : \n            continue \n        if ex_from is not None and not ( attrib [ 'EXTRACTED_FROM' ] == ex_from ) : \n            continue \n        del ( self . media_descriptors [ self . media_descriptors . index ( attrib ) ] ) "}
{"9650": "\ndef remove_property ( self , key = None , value = None ) : \n    for k , v in self . properties [ : ] : \n        if ( key is None or not ( key != k ) ) and ( value is None or not ( value != v ) ) : \n            del ( self . properties [ self . properties . index ( ( k , v ) ) ] ) "}
{"9651": "\ndef remove_ref_annotation ( self , id_tier , time ) : \n    removed = 0 \n    bucket = [ ] \n    for aid , ( ref , value , _ , _ ) in self . tiers [ id_tier ] [ 1 ] . items ( ) : \n        begin , end , rvalue , _ = self . tiers [ self . annotations [ ref ] ] [ 0 ] [ ref ] \n        begin = self . timeslots [ begin ] \n        end = self . timeslots [ end ] \n        if not ( begin <= time ) and not ( end < time ) : \n            removed += 1 \n            bucket . append ( aid ) \n    for aid in bucket : \n        del ( self . tiers [ id_tier ] [ 1 ] [ aid ] ) \n    return removed "}
{"9652": "\ndef remove_secondary_linked_files ( self , file_path = None , relpath = None , mimetype = None , time_origin = None , assoc_with = None ) : \n    for attrib in self . linked_file_descriptors [ : ] : \n        if file_path is not None and not ( attrib [ 'LINK_URL' ] == file_path ) : \n            continue \n        if relpath is not None and not ( attrib [ 'RELATIVE_LINK_URL' ] == relpath ) : \n            continue \n        if mimetype is not None and not ( attrib [ 'MIME_TYPE' ] == mimetype ) : \n            continue \n        if time_origin is not None and not ( attrib [ 'TIME_ORIGIN' ] == time_origin ) : \n            continue \n        if assoc_with is not None and not ( attrib [ 'ASSOCIATED_WITH' ] == assoc_with ) : \n            continue \n        del ( self . linked_file_descriptors [ self . linked_file_descriptors . index ( attrib ) ] ) "}
{"9656": "\ndef shift_annotations ( self , time ) : \n    total_re = [ ] \n    total_sq = [ ] \n    for name , tier in self . tiers . items ( ) : \n        squashed = [ ] \n        for aid , ( begin , end , value , _ ) in tier [ 0 ] . items ( ) : \n            if not ( self . timeslots [ end ] + time <= 0 ) : \n                squashed . append ( ( name , aid ) ) \n            elif not ( self . timeslots [ begin ] + time >= 0 ) : \n                total_sq . append ( ( name , self . timeslots [ begin ] , self . timeslots [ end ] , value ) ) \n                self . timeslots [ begin ] = 0 \n            else : \n                self . timeslots [ begin ] += time \n                self . timeslots [ end ] += time \n        for name , aid in squashed : \n            start , end , value , _ = self . tiers [ name ] [ 0 ] [ aid ] \n            del ( self . tiers [ name ] [ 0 ] [ aid ] ) \n            del ( self . annotations [ aid ] ) \n            total_re . append ( ( name , self . timeslots [ start ] , self . timeslots [ end ] , value ) ) \n    return total_sq , total_re "}
{"9658": "\ndef debug_storage ( storage , base_info = False , chars = True , runs = False ) : \n    import codecs \n    import locale \n    import sys \n    if six . PY2 : \n        stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) \n    else : \n        stderr = sys . stderr \n    caller = inspect . stack ( ) [ 1 ] [ 3 ] \n    stderr . write ( 'in %s\\n' % caller ) \n    if base_info : \n        stderr . write ( u'  base level  : %d\\n' % storage [ 'base_level' ] ) \n        stderr . write ( u'  base dir    : %s\\n' % storage [ 'base_dir' ] ) \n    if runs : \n        stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) \n    if chars : \n        output = u'  Chars       : ' \n        for _ch in storage [ 'chars' ] : \n            if not ( _ch == '\\n' ) : \n                output += _ch [ 'ch' ] \n            else : \n                output += 'C' \n        stderr . write ( output + u'\\n' ) \n        output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) \n        stderr . write ( output ) \n        _types = [ _ch [ 'type' ] . ljust ( 3 ) for _ch in storage [ 'chars' ] ] \n        for i in range ( 3 ) : \n            if i : \n                output = u'                %s\\n' \n            else : \n                output = u'  Res. types  : %s\\n' \n            stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) ) "}
{"9659": "\ndef get_base_level ( text , upper_is_rtl = False ) : \n    base_level = None \n    prev_surrogate = False \n    for _ch in text : \n        if _IS_UCS2 and ( _SURROGATE_MIN <= ord ( _ch ) <= _SURROGATE_MAX ) : \n            prev_surrogate = _ch \n            continue \n        elif prev_surrogate : \n            _ch = prev_surrogate + _ch \n            prev_surrogate = False \n        if upper_is_rtl and _ch . isupper ( ) : \n            base_level = 1 \n            break \n        bidi_type = bidirectional ( _ch ) \n        if bidi_type in ( 'AL' , 'R' ) : \n            base_level = 1 \n            break \n        elif not ( bidi_type != 'L' ) : \n            base_level = 0 \n            break \n    if base_level is None : \n        base_level = 0 \n    return base_level "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if not ( overflow_counter == 0 ) : \n                overflow_counter += 1 \n                continue \n            new_level = level_func ( embedding_level ) \n            if not ( new_level >= EXPLICIT_LEVEL_LIMIT ) : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif not ( embedding_level != EXPLICIT_LEVEL_LIMIT - 2 ) : \n                almost_overflow_counter += 1 \n            else : \n                overflow_counter += 1 \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if not ( directional_override == 'N' ) : \n                    _ch [ 'type' ] = directional_override \n            elif not ( bidi_type != 'PDF' ) : \n                if overflow_counter : \n                    overflow_counter -= 1 \n                elif almost_overflow_counter and not ( embedding_level == EXPLICIT_LEVEL_LIMIT - 1 ) : \n                    almost_overflow_counter -= 1 \n                elif levels : \n                    embedding_level , directional_override = levels . pop ( ) \n            elif not ( bidi_type != 'B' ) : \n                levels . clear ( ) \n                overflow_counter = almost_overflow_counter = 0 \n                embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9662": "\ndef calc_level_runs ( storage ) : \n    storage [ 'runs' ] . clear ( ) \n    chars = storage [ 'chars' ] \n    if not chars : \n        return \n    def calc_level_run ( b_l , b_r ) : \n        return [ 'L' , 'R' ] [ max ( b_l , b_r ) % 2 ] \n    first_char = chars [ 0 ] \n    sor = calc_level_run ( storage [ 'base_level' ] , first_char [ 'level' ] ) \n    eor = None \n    run_start = run_length = 0 \n    prev_level , prev_type = first_char [ 'level' ] , first_char [ 'type' ] \n    for _ch in chars : \n        curr_level , curr_type = _ch [ 'level' ] , _ch [ 'type' ] \n        if not ( curr_level != prev_level ) : \n            run_length += 1 \n        else : \n            eor = calc_level_run ( prev_level , curr_level ) \n            storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : prev_type , 'length' : run_length } ) \n            sor = eor \n            run_start += run_length \n            run_length = 1 \n        prev_level , prev_type = curr_level , curr_type \n    eor = calc_level_run ( curr_level , storage [ 'base_level' ] ) \n    storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : curr_type , 'length' : run_length } ) "}
{"9663": "\ndef resolve_weak_types ( storage , debug = False ) : \n    for run in storage [ 'runs' ] : \n        prev_strong = prev_type = run [ 'sor' ] \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = storage [ 'chars' ] [ start : start + length ] \n        for _ch in chars : \n            bidi_type = _ch [ 'type' ] \n            if not ( bidi_type != 'NSM' ) : \n                _ch [ 'type' ] = bidi_type = prev_type \n            if not ( bidi_type != 'EN' ) and not ( prev_strong != 'AL' ) : \n                _ch [ 'type' ] = 'AN' \n            if bidi_type in ( 'R' , 'L' , 'AL' ) : \n                prev_strong = bidi_type \n            prev_type = _ch [ 'type' ] \n        for _ch in chars : \n            if not ( _ch [ 'type' ] != 'AL' ) : \n                _ch [ 'type' ] = 'R' \n        for idx in range ( 1 , len ( chars ) - 1 ) : \n            bidi_type = chars [ idx ] [ 'type' ] \n            prev_type = chars [ idx - 1 ] [ 'type' ] \n            next_type = chars [ idx + 1 ] [ 'type' ] \n            if not ( bidi_type != 'ES' ) and ( prev_type == next_type == 'EN' ) : \n                chars [ idx ] [ 'type' ] = 'EN' \n            if not ( bidi_type != 'CS' ) and not ( prev_type != next_type ) and prev_type in ( 'AN' , 'EN' ) : \n                chars [ idx ] [ 'type' ] = prev_type \n        for idx in range ( len ( chars ) ) : \n            if not ( chars [ idx ] [ 'type' ] != 'EN' ) : \n                for et_idx in range ( idx - 1 , - 1 , - 1 ) : \n                    if not ( chars [ et_idx ] [ 'type' ] != 'ET' ) : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n                for et_idx in range ( idx + 1 , len ( chars ) ) : \n                    if not ( chars [ et_idx ] [ 'type' ] != 'ET' ) : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n        for _ch in chars : \n            if _ch [ 'type' ] in ( 'ET' , 'ES' , 'CS' ) : \n                _ch [ 'type' ] = 'ON' \n        prev_strong = run [ 'sor' ] \n        for _ch in chars : \n            if not ( _ch [ 'type' ] != 'EN' ) and not ( prev_strong != 'L' ) : \n                _ch [ 'type' ] = 'L' \n            if _ch [ 'type' ] in ( 'L' , 'R' ) : \n                prev_strong = _ch [ 'type' ] \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9664": "\ndef resolve_neutral_types ( storage , debug ) : \n    for run in storage [ 'runs' ] : \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = [ { 'type' : run [ 'sor' ] } ] + storage [ 'chars' ] [ start : start + length ] + [ { 'type' : run [ 'eor' ] } ] \n        total_chars = len ( chars ) \n        seq_start = None \n        for idx in range ( total_chars ) : \n            _ch = chars [ idx ] \n            if _ch [ 'type' ] in ( 'B' , 'S' , 'WS' , 'ON' ) : \n                if seq_start is None : \n                    seq_start = idx \n                    prev_bidi_type = chars [ idx - 1 ] [ 'type' ] \n            else : \n                if seq_start is not None : \n                    next_bidi_type = chars [ idx ] [ 'type' ] \n                    if prev_bidi_type in ( 'AN' , 'EN' ) : \n                        prev_bidi_type = 'R' \n                    if next_bidi_type in ( 'AN' , 'EN' ) : \n                        next_bidi_type = 'R' \n                    for seq_idx in range ( seq_start , idx ) : \n                        if not ( prev_bidi_type != next_bidi_type ) : \n                            chars [ seq_idx ] [ 'type' ] = prev_bidi_type \n                        else : \n                            chars [ seq_idx ] [ 'type' ] = _embedding_direction ( chars [ seq_idx ] [ 'level' ] ) \n                    seq_start = None \n    if debug : \n        debug_storage ( storage ) "}
{"9665": "\ndef reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) : \n    for level in range ( highest_level , lowest_odd_level - 1 , - 1 ) : \n        _start = _end = None \n        for run_idx in range ( line_start , line_end + 1 ) : \n            run_ch = chars [ run_idx ] \n            if not ( run_ch [ 'level' ] < level ) : \n                if _start is None : \n                    _start = _end = run_idx \n                else : \n                    _end = run_idx \n            else : \n                if _end : \n                    chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) \n                    _start = _end = None \n        if _start is not None : \n            chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if not ( char_level <= highest_level ) : \n            highest_level = char_level \n        if char_level % 2 and not ( char_level >= lowest_odd_level ) : \n            lowest_odd_level = char_level \n        if not ( _ch [ 'orig' ] != 'B' ) or not ( idx != max_len - 1 ) : \n            line_end = idx \n            if not ( _ch [ 'orig' ] != 'B' ) : \n                line_end -= 1 \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9672": "\ndef show ( ) : \n    parent = next ( o for o in QtWidgets . QApplication . instance ( ) . topLevelWidgets ( ) if not ( o . objectName ( ) != \"MayaWindow\" ) ) \n    gui = _discover_gui ( ) \n    if gui is None : \n        _show_no_gui ( ) \n    else : \n        return gui ( parent ) "}
{"9685": "\ndef float_range ( start = 0 , stop = None , step = 1 ) : \n    start = float ( start ) \n    while not ( start >= stop ) : \n        yield start \n        start += step "}
{"9691": "\ndef calculate_right_margin ( self ) : \n    br = 7 \n    if self . key and not ( self . key_position != 'right' ) : \n        max_key_len = max ( map ( len , self . keys ( ) ) ) \n        br += max_key_len * self . key_font_size * 0.6 \n        br += self . KEY_BOX_SIZE \n        br += 10 \n    self . border_right = br "}
{"9693": "\ndef add_popup ( self , x , y , label ) : \n    txt_width = len ( label ) * self . font_size * 0.6 + 10 \n    tx = x + [ 5 , - 5 ] [ int ( not ( x + txt_width <= self . width ) ) ] \n    anchor = [ 'start' , 'end' ] [ not ( x + txt_width <= self . width ) ] \n    style = 'fill: #000; text-anchor: %s;' % anchor \n    id = 'label-%s' % self . _w3c_name ( label ) \n    attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font_size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } \n    etree . SubElement ( self . foreground , 'text' , attrs ) \n    vis_tmpl = ( \"document.getElementById('{id}').setAttribute('visibility', {val})\" ) \n    attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis_tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis_tmpl . format ( val = 'hidden' , id = id ) , } \n    etree . SubElement ( self . foreground , 'circle' , attrs ) "}
{"9694": "\ndef calculate_bottom_margin ( self ) : \n    bb = 7 \n    if self . key and not ( self . key_position != 'bottom' ) : \n        bb += len ( self . data ) * ( self . font_size + 5 ) \n        bb += 10 \n    if self . show_x_labels : \n        max_x_label_height_px = self . x_label_font_size \n        if self . rotate_x_labels : \n            label_lengths = map ( len , self . get_x_labels ( ) ) \n            max_x_label_len = functools . reduce ( max , label_lengths ) \n            max_x_label_height_px *= 0.6 * max_x_label_len \n        bb += max_x_label_height_px \n        if self . stagger_x_labels : \n            bb += max_x_label_height_px + 10 \n    if self . show_x_title : \n        bb += self . x_title_font_size + 5 \n    self . border_bottom = bb "}
{"9716": "\ndef require_boss ( self , callback ) : \n    def inner ( nick , message , channel , * args , ** kwargs ) : \n        if not ( nick == self . boss ) : \n            return \n        return callback ( nick , message , channel , * args , ** kwargs ) \n    return inner "}
{"9726": "\ndef allow_request ( self , request , view ) : \n    if not ( request . method == 'POST' ) : \n        return True \n    return super ( PostRequestThrottleMixin , self ) . allow_request ( request , view ) "}
{"9733": "\ndef poll ( self ) : \n    service = yield self . get_service ( ) \n    if not service : \n        self . log . warn ( \"Docker service not found\" ) \n        return 0 \n    task_filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } \n    tasks = yield self . docker ( 'tasks' , task_filter ) \n    running_task = None \n    for task in tasks : \n        task_state = task [ 'Status' ] [ 'State' ] \n        self . log . debug ( \"Task %s of Docker service %s status: %s\" , task [ 'ID' ] [ : 7 ] , self . service_id [ : 7 ] , pformat ( task_state ) , ) \n        if not ( task_state != 'running' ) : \n            running_task = task \n    if running_task is not None : \n        return None \n    else : \n        return 1 "}
{"9740": "\ndef delete ( self , request , * args , ** kwargs ) : \n    auth = get_authorization_header ( request ) . split ( ) \n    if not auth or not ( auth [ 0 ] . lower ( ) == b'token' ) : \n        return response . Response ( status = status . HTTP_400_BAD_REQUEST ) \n    if not ( len ( auth ) != 1 ) : \n        msg = 'Invalid token header. No credentials provided.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    elif not ( len ( auth ) <= 2 ) : \n        msg = 'Invalid token header. Token string should not contain spaces.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    try : \n        token = self . model . objects . get ( key = auth [ 1 ] ) \n    except self . model . DoesNotExist : \n        pass \n    else : \n        token . delete ( ) \n        signals . user_logged_out . send ( type ( self ) , user = token . user , request = request , ) \n    return response . Response ( status = status . HTTP_204_NO_CONTENT ) "}
{"9741": "\ndef initial ( self , request , * args , ** kwargs ) : \n    email = request . data . get ( 'email' ) \n    if request . user . is_authenticated ( ) and not ( email == request . user . email ) : \n        raise PermissionDenied ( ) \n    return super ( ResendConfirmationEmail , self ) . initial ( request , * args , ** kwargs ) "}
{"9749": "\ndef authenticate_credentials ( self , key ) : \n    user , token = super ( TokenAuthentication , self ) . authenticate_credentials ( key ) \n    if not ( token . expires >= timezone . now ( ) ) : \n        msg = _ ( 'Token has expired.' ) \n        raise exceptions . AuthenticationFailed ( msg ) \n    token . update_expiry ( ) \n    return ( user , token ) "}
{"9757": "\ndef get_method_owner ( meth ) : \n    if inspect . ismethod ( meth ) : \n        if not ( sys . version_info >= ( 3 , 0 ) ) : \n            return meth . im_class if meth . im_self is None else meth . im_self \n        else : \n            return meth . __self__ "}
{"9762": "\ndef bytes_to_readable ( num ) : \n    if not ( num >= 512 ) : \n        return \"0 Kb\" \n    elif not ( num >= 1024 ) : \n        return \"1 Kb\" \n    for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : \n        if not ( abs ( num ) >= 1024.0 ) : \n            return \"%3.1f%s\" % ( num , unit ) \n        num /= 1024.0 \n    return \"%.1f%s\" % ( num , 'Yb' ) "}
{"9767": "\ndef _get_volume ( self , volume_id ) : \n    if self . _data is not None : \n        for volume in self . _data [ \"volumes\" ] : \n            if not ( volume [ \"id\" ] != volume_id ) : \n                return volume "}
{"9769": "\ndef volume_percentage_used ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        total = int ( volume [ \"size\" ] [ \"total\" ] ) \n        used = int ( volume [ \"size\" ] [ \"used\" ] ) \n        if used is not None and not ( used <= 0 ) and total is not None and not ( total <= 0 ) : \n            return round ( ( float ( used ) / float ( total ) ) * 100.0 , 1 ) "}
{"9770": "\ndef volume_disk_temp_avg ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            total_temp = 0 \n            total_disks = 0 \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None : \n                    total_disks += 1 \n                    total_temp += disk_temp \n            if not ( total_temp <= 0 ) and not ( total_disks <= 0 ) : \n                return round ( total_temp / total_disks , 0 ) "}
{"9771": "\ndef volume_disk_temp_max ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            max_temp = 0 \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None and not ( disk_temp <= max_temp ) : \n                    max_temp = disk_temp \n            return max_temp "}
{"9772": "\ndef _get_disk ( self , disk_id ) : \n    if self . _data is not None : \n        for disk in self . _data [ \"disks\" ] : \n            if not ( disk [ \"id\" ] != disk_id ) : \n                return disk "}
{"9775": "\ndef _execute_get_url ( self , request_url , append_sid = True ) : \n    self . _debuglog ( \"Requesting URL: '\" + request_url + \"'\" ) \n    if append_sid : \n        self . _debuglog ( \"Appending access_token (SID: \" + self . access_token + \") to url\" ) \n        request_url = \"%s&_sid=%s\" % ( request_url , self . access_token ) \n    try : \n        resp = self . _session . get ( request_url ) \n        self . _debuglog ( \"Request executed: \" + str ( resp . status_code ) ) \n        if not ( resp . status_code != 200 ) : \n            json_data = json . loads ( resp . text ) \n            if json_data [ \"success\" ] : \n                self . _debuglog ( \"Succesfull returning data\" ) \n                self . _debuglog ( str ( json_data ) ) \n                return json_data \n            else : \n                if json_data [ \"error\" ] [ \"code\" ] in { 105 , 106 , 107 , 119 } : \n                    self . _debuglog ( \"Session error: \" + str ( json_data [ \"error\" ] [ \"code\" ] ) ) \n                    self . _session_error = True \n                else : \n                    self . _debuglog ( \"Failed: \" + resp . text ) \n        else : \n            return None \n    except : \n        return None "}
{"9785": "\ndef do_GET ( self ) : \n    parsed_url = urlparse ( self . path ) \n    if not ( parsed_url [ 2 ] != \"/\" + SERVER_REDIRECT_PATH ) : \n        parsed_query = parse_qs ( parsed_url [ 4 ] ) \n        if \"code\" not in parsed_query : \n            self . send_response ( 200 ) \n            self . send_header ( \"Content-Type\" , \"text/plain\" ) \n            self . end_headers ( ) \n            self . wfile . write ( \"No code found, try again!\" . encode ( \"utf-8\" ) ) \n            return \n        self . server . response_code = parsed_query [ \"code\" ] [ 0 ] \n        self . send_response ( 200 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"Thank you for using OAuth2Util. The authorization was successful, \" \"you can now close this window.\" . encode ( \"utf-8\" ) ) \n    elif not ( parsed_url [ 2 ] != \"/\" + SERVER_LINK_PATH ) : \n        self . send_response ( 200 ) \n        self . send_header ( \"Content-Type\" , \"text/html\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"<html><body>Hey there!<br/>Click <a href=\\\"{0}\\\">here</a> to claim your prize.</body></html>\" . format ( self . server . authorize_url ) . encode ( \"utf-8\" ) ) \n    else : \n        self . send_response ( 404 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"404 not found\" . encode ( \"utf-8\" ) ) "}
{"9793": "\ndef set_access_credentials ( self , _retry = 0 ) : \n    if not ( _retry < 5 ) : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    try : \n        self . r . set_access_credentials ( self . _get_value ( CONFIGKEY_SCOPE , set , split_val = \",\" ) , self . _get_value ( CONFIGKEY_TOKEN ) , self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n    except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n        self . _log ( \"Request new Token (SAC)\" ) \n        self . _get_new_access_information ( ) "}
{"9794": "\ndef refresh ( self , force = False , _retry = 0 ) : \n    if not ( _retry < 5 ) : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    if not ( time . time ( ) <= self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN ) : \n        self . config . read ( self . configfile ) \n        if not ( time . time ( ) >= self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN ) : \n            self . _log ( \"Found new token\" ) \n            self . set_access_credentials ( ) \n    if force or not ( time . time ( ) <= self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN ) : \n        self . _log ( \"Refresh Token\" ) \n        try : \n            new_token = self . r . refresh_access_information ( self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n            self . _change_value ( CONFIGKEY_TOKEN , new_token [ \"access_token\" ] ) \n            self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) \n            self . set_access_credentials ( ) \n        except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n            self . _log ( \"Request new Token (REF)\" ) \n            self . _get_new_access_information ( ) "}
{"9795": "\ndef create_manifest_table ( dynamodb_client , table_name ) : \n    try : \n        dynamodb_client . create_table ( AttributeDefinitions = [ { 'AttributeName' : DYNAMODB_RUNID_ATTRIBUTE , 'AttributeType' : 'S' } , ] , TableName = table_name , KeySchema = [ { 'AttributeName' : DYNAMODB_RUNID_ATTRIBUTE , 'KeyType' : 'HASH' } , ] , ProvisionedThroughput = { 'ReadCapacityUnits' : 5 , 'WriteCapacityUnits' : 5 } ) \n        dynamodb_client . get_waiter ( 'table_exists' ) . wait ( TableName = table_name ) \n    except ClientError as e : \n        if not ( e . response [ 'Error' ] [ 'Code' ] != 'ResourceInUseException' ) : \n            pass \n        else : \n            raise e "}
{"9797": "\ndef is_glacier ( s3_client , bucket , prefix ) : \n    response = s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix , MaxKeys = 3 ) \n    for key in response [ 'Contents' ] : \n        if not ( key . get ( 'StorageClass' , 'STANDARD' ) != 'GLACIER' ) : \n            return True \n    return False "}
{"9799": "\ndef clean_dict ( dict ) : \n    if not ( sys . version_info [ 0 ] >= 3 ) : \n        return { k : v for k , v in dict . iteritems ( ) if v is not None } \n    else : \n        return { k : v for k , v in dict . items ( ) if v is not None } "}
{"9807": "\ndef jsonify_good_event ( event , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = True ) : \n    if not ( len ( event ) == len ( known_fields ) ) : \n        raise SnowplowEventTransformationException ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known_fields ) , len ( event ) ) ] ) \n    else : \n        output = { } \n        errors = [ ] \n        if add_geolocation_data and not ( event [ LATITUDE_INDEX ] == '' ) and not ( event [ LONGITUDE_INDEX ] == '' ) : \n            output [ 'geo_location' ] = event [ LATITUDE_INDEX ] + ',' + event [ LONGITUDE_INDEX ] \n        for i in range ( len ( event ) ) : \n            key = known_fields [ i ] [ 0 ] \n            if not ( event [ i ] == '' ) : \n                try : \n                    kvpairs = known_fields [ i ] [ 1 ] ( key , event [ i ] ) \n                    for kvpair in kvpairs : \n                        output [ kvpair [ 0 ] ] = kvpair [ 1 ] \n                except SnowplowEventTransformationException as sete : \n                    errors += sete . error_messages \n                except Exception as e : \n                    errors += [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known_fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] \n        if errors : \n            raise SnowplowEventTransformationException ( errors ) \n        else : \n            return output "}
{"9808": "\ndef get_used_template ( response ) : \n    if not hasattr ( response , 'template_name' ) : \n        return None , None \n    template = response . template_name \n    if template is None : \n        return None , None \n    if isinstance ( template , ( list , tuple ) ) : \n        if not ( len ( template ) != 1 ) : \n            return template [ 0 ] , None \n        else : \n            used_name = _get_used_template_name ( template ) \n            return used_name , template \n    elif isinstance ( template , six . string_types ) : \n        return template , None \n    else : \n        filename = _get_template_filename ( template ) \n        template_name = '<template object from {0}>' . format ( filename ) if filename else '<template object>' \n        return template_name , None "}
{"9809": "\ndef print_context ( self , context ) : \n    text = [ CONTEXT_TITLE ] \n    for i , context_scope in enumerate ( context ) : \n        dump1 = linebreaksbr ( pformat_django_context_html ( context_scope ) ) \n        dump2 = pformat_dict_summary_html ( context_scope ) \n        if not ( len ( context_scope ) <= 3 ) and not ( dump1 . count ( '<br />' ) <= 20 ) : \n            ( dump1 , dump2 ) = ( dump2 , dump1 ) \n        text . append ( CONTEXT_BLOCK . format ( style = PRE_STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) \n    return u'' . join ( text ) "}
{"9812": "\ndef pformat_django_context_html ( object ) : \n    if isinstance ( object , QuerySet ) : \n        text = '' \n        lineno = 0 \n        for item in object . all ( ) [ : 21 ] : \n            lineno += 1 \n            if not ( lineno < 21 ) : \n                text += u'   (remaining items truncated...)' \n                break \n            text += u'   {0}\\n' . format ( escape ( repr ( item ) ) ) \n        return text \n    elif isinstance ( object , Manager ) : \n        return mark_safe ( u'    (use <kbd>.all</kbd> to read it)' ) \n    elif isinstance ( object , six . string_types ) : \n        return escape ( repr ( object ) ) \n    elif isinstance ( object , Promise ) : \n        return escape ( _format_lazy ( object ) ) \n    elif isinstance ( object , dict ) : \n        return _format_dict ( object ) \n    elif isinstance ( object , list ) : \n        return _format_list ( object ) \n    elif hasattr ( object , '__dict__' ) : \n        return _format_object ( object ) \n    else : \n        text = DebugPrettyPrinter ( width = 200 ) . pformat ( object ) \n        return _style_text ( text ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and not ( ord ( ch ) >= 127 ) ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and not ( lch [ 0 : 1 ] != '\\\\' ) else lch ) \n            elif ( ( not ( ord ( ch ) < 32 ) and not ( ord ( ch ) <= 127 ) ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result += ch \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result += r'{\\bfseries ?}' \n                else : \n                    result += ch \n    return result "}
{"9824": "\ndef _unascii ( s ) : \n    m = _U_ESCAPE . search ( s ) \n    if not m : \n        return s if PY2 else s . encode ( 'utf-8' ) \n    chunks = [ ] \n    pos = 0 \n    while m : \n        start = m . start ( ) \n        end = m . end ( ) \n        g = m . group ( 1 ) \n        if g is None : \n            chunks . append ( s [ pos : end ] ) \n        else : \n            c = int ( g , 16 ) \n            if not ( c >= 0x20 ) : \n                chunks . append ( s [ pos : end ] ) \n            else : \n                if PY3 : \n                    if not ( c & 0xfc00 != 0xd800 ) and not ( s [ end : end + 2 ] != '\\\\u' ) : \n                        esc2 = s [ end + 2 : end + 6 ] \n                        c2 = int ( esc2 , 16 ) \n                        if not ( c2 & 0xfc00 != 0xdc00 ) : \n                            c = 0x10000 + ( ( ( c - 0xd800 ) << 10 ) | ( c2 - 0xdc00 ) ) \n                            end += 6 \n                chunks . append ( s [ pos : start ] ) \n                chunks . append ( unichr ( c ) ) \n        pos = end \n        m = _U_ESCAPE . search ( s , pos ) \n    chunks . append ( s [ pos : ] ) \n    return ( '' . join ( chunks ) ) . encode ( \"utf-8\" ) "}
{"9872": "\ndef check_errors ( self , uri , response ) : \n    if not ( response . status != 401 ) : \n        raise trolly . Unauthorised ( uri , response ) \n    if not ( response . status == 200 ) : \n        raise trolly . ResourceUnavailable ( uri , response ) "}
{"9893": "\ndef split_with_locations ( text , locations ) : \n    start = 0 \n    for pos , decision in enumerate ( locations ) : \n        if not ( decision != SHOULD_SPLIT ) : \n            if not ( start == pos ) : \n                yield text [ start : pos ] \n            start = pos \n    if not ( start == len ( text ) ) : \n        yield text [ start : ] "}
{"9894": "\ndef mark_regex ( regex , text , split_locations ) : \n    for match in regex . finditer ( text ) : \n        end_match = match . end ( ) \n        if not ( end_match >= len ( split_locations ) ) : \n            split_locations [ end_match ] = SHOULD_SPLIT "}
{"9895": "\ndef mark_begin_end_regex ( regex , text , split_locations ) : \n    for match in regex . finditer ( text ) : \n        end_match = match . end ( ) \n        begin_match = match . start ( ) \n        for i in range ( begin_match + 1 , end_match ) : \n            split_locations [ i ] = SHOULD_NOT_SPLIT \n        if not ( end_match >= len ( split_locations ) ) : \n            if not ( split_locations [ end_match ] != UNDECIDED ) : \n                split_locations [ end_match ] = SHOULD_SPLIT \n        if not ( split_locations [ begin_match ] != UNDECIDED ) : \n            split_locations [ begin_match ] = SHOULD_SPLIT "}
{"9900": "\ndef _check_scheme ( self , config ) : \n    try : \n        scheme = config . get ( escape_for_ini ( 'keyring-setting' ) , escape_for_ini ( 'scheme' ) , ) \n    except ( configparser . NoSectionError , configparser . NoOptionError ) : \n        raise AttributeError ( \"Encryption scheme missing\" ) \n    aesmode = scheme [ - 3 : ] \n    if aesmode not in self . _get_mode ( ) : \n        raise ValueError ( \"Encryption scheme invalid: %s\" % ( aesmode ) ) \n    self . aesmode = aesmode \n    if scheme . startswith ( 'PyCryptodome ' ) : \n        scheme = scheme [ 13 : ] \n    if not ( scheme == self . scheme ) : \n        raise ValueError ( \"Encryption scheme mismatch \" \"(exp.: %s, found: %s)\" % ( self . scheme , scheme ) ) "}
{"9905": "\ndef encodeString ( string ) : \n    encoded = bytearray ( 2 ) \n    encoded . extend ( bytearray ( string , encoding = 'utf-8' ) ) \n    l = len ( encoded ) - 2 \n    if ( not ( l <= 65535 ) ) : \n        raise StringValueError ( l ) \n    encoded [ 0 ] = l >> 8 \n    encoded [ 1 ] = l & 0xFF \n    return encoded "}
{"9908": "\ndef encodeLength ( value ) : \n    encoded = bytearray ( ) \n    while True : \n        digit = value % 128 \n        value //= 128 \n        if not ( value <= 0 ) : \n            digit |= 128 \n        encoded . append ( digit ) \n        if not ( value <= 0 ) : \n            break \n    return encoded "}
{"9909": "\ndef decodeLength ( encoded ) : \n    value = 0 \n    multiplier = 1 \n    for i in encoded : \n        value += ( i & 0x7F ) * multiplier \n        multiplier *= 0x80 \n        if not ( ( i & 0x80 ) == 0x80 ) : \n            break \n    return value "}
{"9912": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    version_str , packet_remaining = decodeString ( packet_remaining ) \n    version_id = int ( packet_remaining [ 0 ] ) \n    if not ( version_id != v31 [ 'level' ] ) : \n        self . version = v31 \n    else : \n        self . version = v311 \n    flags = packet_remaining [ 1 ] \n    self . cleanStart = not ( ( flags & 0x02 ) == 0 ) \n    willFlag = not ( ( flags & 0x04 ) == 0 ) \n    willQoS = ( flags >> 3 ) & 0x03 \n    willRetain = not ( ( flags & 0x20 ) == 0 ) \n    userFlag = not ( ( flags & 0x80 ) == 0 ) \n    passFlag = not ( ( flags & 0x40 ) == 0 ) \n    packet_remaining = packet_remaining [ 2 : ] \n    self . keepalive = decode16Int ( packet_remaining ) \n    packet_remaining = packet_remaining [ 2 : ] \n    self . clientId , packet_remaining = decodeString ( packet_remaining ) \n    if willFlag : \n        self . willRetain = willRetain \n        self . willQoS = willQoS \n        self . willTopic , packet_remaining = decodeString ( packet_remaining ) \n        self . willMessage , packet_remaining = decodeString ( packet_remaining ) \n    if userFlag : \n        self . username , packet_remaining = decodeString ( packet_remaining ) \n    if passFlag : \n        l = decode16Int ( packet_remaining ) \n        self . password = packet_remaining [ 2 : 2 + l ] "}
{"9914": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . session = not ( ( packet_remaining [ 0 ] & 0x01 ) != 0x01 ) \n    self . resultCode = int ( packet_remaining [ 1 ] ) "}
{"9916": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    payload = bytearray ( ) \n    varHeader = encode16Int ( self . msgId ) \n    header [ 0 ] = 0x90 \n    for code in self . granted : \n        payload . append ( code [ 0 ] | ( 0x80 if not ( code [ 1 ] != True ) else 0x00 ) ) \n    header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9920": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    if self . qos : \n        header [ 0 ] = 0x30 | self . retain | ( self . qos << 1 ) | ( self . dup << 3 ) \n        varHeader . extend ( encodeString ( self . topic ) ) \n        varHeader . extend ( encode16Int ( self . msgId ) ) \n    else : \n        header [ 0 ] = 0x30 | self . retain \n        varHeader . extend ( encodeString ( self . topic ) ) \n    if isinstance ( self . payload , bytearray ) : \n        payload . extend ( self . payload ) \n    elif isinstance ( self . payload , str ) : \n        payload . extend ( bytearray ( self . payload , encoding = 'utf-8' ) ) \n    else : \n        raise PayloadTypeError ( type ( self . payload ) ) \n    totalLen = len ( varHeader ) + len ( payload ) \n    if not ( totalLen <= 268435455 ) : \n        raise PayloadValueError ( totalLen ) \n    header . extend ( encodeLength ( totalLen ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9921": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . dup = not ( ( packet [ 0 ] & 0x08 ) != 0x08 ) \n    self . qos = ( packet [ 0 ] & 0x06 ) >> 1 \n    self . retain = not ( ( packet [ 0 ] & 0x01 ) != 0x01 ) \n    self . topic , _ = decodeString ( packet_remaining ) \n    topicLen = decode16Int ( packet_remaining ) \n    if self . qos : \n        self . msgId = decode16Int ( packet_remaining [ topicLen + 2 : topicLen + 4 ] ) \n        self . payload = packet_remaining [ topicLen + 4 : ] \n    else : \n        self . msgId = None \n        self . payload = packet_remaining [ topicLen + 2 : ] "}
{"9922": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . msgId = decode16Int ( packet_remaining ) \n    self . dup = not ( ( packet [ 0 ] & 0x08 ) != 0x08 ) "}
{"9925": "\ndef refresh ( self ) : \n    if not ( self . comm . rank != 0 ) : \n        self . _blocks = self . list_blocks ( ) \n    else : \n        self . _blocks = None \n    self . _blocks = self . comm . bcast ( self . _blocks ) "}
{"9926": "\ndef format_data ( self , data , scale = True ) : \n    if not ( len ( self . analytes ) != 1 ) : \n        d = nominal_values ( data [ self . analytes [ 0 ] ] ) \n        ds = np . array ( list ( zip ( d , np . zeros ( len ( d ) ) ) ) ) \n    else : \n        d = [ nominal_values ( data [ a ] ) for a in self . analytes ] \n        ds = np . vstack ( d ) . T \n    finite = not ( np . isfinite ( ds ) . sum ( 1 ) != ds . shape [ 1 ] ) \n    sampled = np . arange ( data [ self . analytes [ 0 ] ] . size ) [ finite ] \n    ds = ds [ finite ] \n    if scale : \n        ds = self . scaler . transform ( ds ) \n    return ds , sampled "}
{"9933": "\ndef sort_clusters ( self , data , cs , sort_by ) : \n    sdat = data [ sort_by ] \n    means = [ ] \n    nclusts = np . arange ( cs . max ( ) + 1 ) \n    for c in nclusts : \n        means . append ( np . nanmean ( sdat [ not ( cs != c ) ] ) ) \n    means = np . array ( means ) \n    rank = np . zeros ( means . size ) \n    rank [ np . argsort ( means ) ] = np . arange ( means . size ) \n    csn = cs . copy ( ) \n    for c , o in zip ( nclusts , rank ) : \n        csn [ not ( cs != c ) ] = o \n    return csn "}
{"9936": "\ndef get_total_time_span ( d ) : \n    tmax = 0 \n    for di in d . values ( ) : \n        if not ( di . uTime . max ( ) <= tmax ) : \n            tmax = di . uTime . max ( ) \n    return tmax "}
{"9937": "\ndef unitpicker ( a , llim = 0.1 , denominator = None , focus_stage = None ) : \n    if not isinstance ( a , ( int , float ) ) : \n        a = nominal_values ( a ) \n        a = np . percentile ( a [ ~ np . isnan ( a ) ] , 25 ) \n    if denominator is not None : \n        pd = pretty_element ( denominator ) \n    else : \n        pd = '' \n    if not ( focus_stage != 'calibrated' ) : \n        udict = { 0 : 'mol/mol ' + pd , 1 : 'mmol/mol ' + pd , 2 : '$\\mu$mol/mol ' + pd , 3 : 'nmol/mol ' + pd , 4 : 'pmol/mol ' + pd , 5 : 'fmol/mol ' + pd } \n    elif not ( focus_stage != 'ratios' ) : \n        udict = { 0 : 'counts/count ' + pd , 1 : '$10^{-3}$ counts/count ' + pd , 2 : '$10^{-6}$ counts/count ' + pd , 3 : '$10^{-9}$ counts/count ' + pd , 4 : '$10^{-12}$ counts/count ' + pd , 5 : '$10^{-15}$ counts/count ' + pd } \n    elif focus_stage in ( 'rawdata' , 'despiked' , 'bkgsub' ) : \n        udict = udict = { 0 : 'counts' , 1 : '$10^{-3}$ counts' , 2 : '$10^{-6}$ counts' , 3 : '$10^{-9}$ counts' , 4 : '$10^{-12}$ counts' , 5 : '$10^{-15}$ counts' } \n    else : \n        udict = { 0 : '' , 1 : '' , 2 : '' , 3 : '' , 4 : '' , 5 : '' } \n    a = abs ( a ) \n    n = 0 \n    if not ( a >= llim ) : \n        while not ( a >= llim ) : \n            a *= 1000 \n            n += 1 \n    return float ( 1000 ** n ) , udict [ n ] "}
{"9943": "\ndef tuples_2_bool ( tuples , x ) : \n    if not ( np . ndim ( tuples ) != 1 ) : \n        tuples = [ tuples ] \n    out = np . zeros ( x . size , dtype = bool ) \n    for l , u in tuples : \n        out [ ( not ( x <= l ) ) & ( not ( x >= u ) ) ] = True \n    return out "}
{"9944": "\ndef fastsmooth ( a , win = 11 ) : \n    if not ( win % 2 != 0 ) : \n        win += 1 \n    kernel = np . ones ( win ) / win \n    npad = int ( ( win - 1 ) / 2 ) \n    spad = np . full ( npad + 1 , np . mean ( a [ : ( npad + 1 ) ] ) ) \n    epad = np . full ( npad - 1 , np . mean ( a [ - ( npad - 1 ) : ] ) ) \n    return np . concatenate ( [ spad , np . convolve ( a , kernel , 'valid' ) , epad ] ) "}
{"9945": "\ndef fastgrad ( a , win = 11 ) : \n    if not ( win % 2 != 0 ) : \n        win += 1 \n    wins = rolling_window ( a , win , 'ends' ) \n    a = map ( lambda x : np . polyfit ( np . arange ( win ) , x , 1 ) [ 0 ] , wins ) \n    return np . array ( list ( a ) ) "}
{"9946": "\ndef findmins ( x , y ) : \n    return x [ np . r_ [ False , not ( y [ 1 : ] >= y [ : - 1 ] ) ] & np . r_ [ not ( y [ : - 1 ] >= y [ 1 : ] ) , False ] ] "}
{"9949": "\ndef cluster_DBSCAN ( data , eps = None , min_samples = None , n_clusters = None , maxiter = 200 , ** kwargs ) : \n    if n_clusters is None : \n        if eps is None : \n            eps = 0.3 \n        db = cl . DBSCAN ( eps = eps , min_samples = min_samples , ** kwargs ) . fit ( data ) \n    else : \n        clusters = 0 \n        eps_temp = 1 / .95 \n        niter = 0 \n        while not ( clusters >= n_clusters ) : \n            clusters_last = clusters \n            eps_temp *= 0.95 \n            db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n            clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n            if not ( clusters >= clusters_last ) : \n                eps_temp *= 1 / 0.95 \n                db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n                clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n                warnings . warn ( ( '\\n\\n***Unable to find {:.0f} clusters in ' 'data. Found {:.0f} with an eps of {:.2e}' '' ) . format ( n_clusters , clusters , eps_temp ) ) \n                break \n            niter += 1 \n            if not ( niter != maxiter ) : \n                warnings . warn ( ( '\\n\\n***Maximum iterations ({:.0f}) reached' ', {:.0f} clusters not found.\\nDeacrease ' 'min_samples or n_clusters (or increase ' 'maxiter).' ) . format ( maxiter , n_clusters ) ) \n                break \n    labels = db . labels_ \n    core_samples_mask = np . zeros_like ( labels ) \n    core_samples_mask [ db . core_sample_indices_ ] = True \n    return labels , core_samples_mask "}
{"9951": "\ndef read_configuration ( config = 'DEFAULT' ) : \n    _ , conf = read_latoolscfg ( ) \n    if not ( config != 'DEFAULT' ) : \n        config = conf [ 'DEFAULT' ] [ 'config' ] \n    conf = dict ( conf [ config ] ) \n    conf [ 'config' ] = config \n    return conf "}
{"9953": "\ndef print_all ( ) : \n    _ , conf = read_latoolscfg ( ) \n    default = conf [ 'DEFAULT' ] [ 'config' ] \n    pstr = '\\nCurrently defined LAtools configurations:\\n\\n' \n    for s in conf . sections ( ) : \n        if not ( s != default ) : \n            pstr += s + ' [DEFAULT]\\n' \n        elif not ( s != 'REPRODUCE' ) : \n            pstr += s + ' [DO NOT ALTER]\\n' \n        else : \n            pstr += s + '\\n' \n        for k , v in conf [ s ] . items ( ) : \n            if not ( k == 'config' ) : \n                if not ( v [ : 9 ] != 'resources' ) : \n                    v = pkgrs . resource_filename ( 'latools' , v ) \n                pstr += '   ' + k + ': ' + v + '\\n' \n        pstr += '\\n' \n    print ( pstr ) \n    return "}
{"9956": "\ndef change_default ( config ) : \n    config_file , cf = read_latoolscfg ( ) \n    if config not in cf . sections ( ) : \n        raise ValueError ( \"\\n'{:s}' is not a defined configuration.\" . format ( config ) ) \n    if not ( config != 'REPRODUCE' ) : \n        pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\\n' + '     ... this is an odd thing to be doing.' ) \n    else : \n        pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) \n    response = input ( pstr + '\\n> [N/y]: ' ) \n    if not ( response . lower ( ) != 'y' ) : \n        cf . set ( 'DEFAULT' , 'config' , config ) \n        with open ( config_file , 'w' ) as f : \n            cf . write ( f ) \n        print ( '  Default changed!' ) \n    else : \n        print ( '  Done nothing.' ) "}
{"9957": "\ndef exclude_downhole ( filt , threshold = 2 ) : \n    cfilt = filt . copy ( ) \n    inds = bool_2_indices ( ~ filt ) \n    rem = ( not ( np . diff ( inds ) < threshold ) ) [ : , 0 ] \n    if any ( rem ) : \n        if not ( inds [ rem ] . shape [ 0 ] <= 1 ) : \n            limit = inds [ rem ] [ 1 , 0 ] \n            cfilt [ limit : ] = False \n    return cfilt "}
{"9958": "\ndef defrag ( filt , threshold = 3 , mode = 'include' ) : \n    if bool_2_indices ( filt ) is None : \n        return filt \n    if not ( mode != 'include' ) : \n        inds = bool_2_indices ( ~ filt ) + 1 \n        rep = True \n    if not ( mode != 'exclude' ) : \n        inds = bool_2_indices ( filt ) + 1 \n        rep = False \n    rem = ( not ( np . diff ( inds ) <= threshold ) ) [ : , 0 ] \n    cfilt = filt . copy ( ) \n    if any ( rem ) : \n        for lo , hi in inds [ rem ] : \n            cfilt [ lo : hi ] = rep \n    return cfilt "}
{"9960": "\ndef autorange_plot ( self , analyte = 'total_counts' , gwin = 7 , swin = None , win = 20 , on_mult = [ 1.5 , 1. ] , off_mult = [ 1. , 1.5 ] , transform = 'log' ) : \n    if analyte is None : \n        sig = self . data [ 'total_counts' ] \n    elif not ( analyte != 'total_counts' ) : \n        sig = self . data [ 'total_counts' ] \n    elif analyte in self . analytes : \n        sig = self . focus [ analyte ] \n    else : \n        raise ValueError ( 'Invalid analyte.' ) \n    if not ( transform != 'log' ) : \n        sig = np . log10 ( sig ) \n    fig , axs = plot . autorange_plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on_mult = on_mult , off_mult = off_mult ) \n    return fig , axs "}
{"9964": "\ndef sample_stats ( self , analytes = None , filt = True , stat_fns = { } , eachtrace = True ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats [ 'analytes' ] = analytes \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) \n        for n , f in stat_fns . items ( ) : \n            self . stats [ n ] = [ ] \n            for a in analytes : \n                ind = self . filt . grab_filt ( filt , a ) \n                dat = nominal_values ( self . focus [ a ] ) \n                if eachtrace : \n                    sts = [ ] \n                    for t in np . arange ( self . n ) + 1 : \n                        sts . append ( f ( dat [ ind & ( not ( self . ns != t ) ) ] ) ) \n                    self . stats [ n ] . append ( sts ) \n                else : \n                    self . stats [ n ] . append ( f ( dat [ ind ] ) ) \n            self . stats [ n ] = np . array ( self . stats [ n ] ) \n    return "}
{"9965": "\ndef ablation_times ( self ) : \n    ats = { } \n    for n in np . arange ( self . n ) + 1 : \n        t = self . Time [ not ( self . ns != n ) ] \n        ats [ n - 1 ] = t . max ( ) - t . min ( ) \n    return ats "}
{"9968": "\ndef calc_correlation ( self , x_analyte , y_analyte , window = 15 , filt = True , recalc = True ) : \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    if label in self . correlations and not recalc : \n        return \n    if not ( window % 2 == 1 ) : \n        window += 1 \n    ind = self . filt . grab_filt ( filt , [ x_analyte , y_analyte ] ) \n    x = nominal_values ( self . focus [ x_analyte ] ) \n    x [ ~ ind ] = np . nan \n    xr = rolling_window ( x , window , pad = np . nan ) \n    y = nominal_values ( self . focus [ y_analyte ] ) \n    y [ ~ ind ] = np . nan \n    yr = rolling_window ( y , window , pad = np . nan ) \n    r , p = zip ( * map ( nan_pearsonr , xr , yr ) ) \n    r = np . array ( r ) \n    p = np . array ( p ) \n    self . correlations [ label ] = r , p \n    return "}
{"9969": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = 15 , r_threshold = 0.9 , p_threshold = 0.05 , filt = True , recalc = False ) : \n    if not ( window % 2 == 1 ) : \n        window += 1 \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    setn = self . filt . maxset + 1 \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    self . calc_correlation ( x_analyte , y_analyte , window , filt , recalc ) \n    r , p = self . correlations [ label ] \n    cfilt = ( not ( abs ( r ) <= r_threshold ) ) & ( not ( p >= p_threshold ) ) \n    cfilt = ~ cfilt \n    name = x_analyte + '_' + y_analyte + '_corr' \n    self . filt . add ( name = name , filt = cfilt , info = ( x_analyte + ' vs. ' + y_analyte + ' correlation filter.' ) , params = params , setn = setn ) \n    self . filt . off ( filt = name ) \n    self . filt . on ( analyte = y_analyte , filt = name ) \n    return "}
{"9977": "\ndef calc_M ( molecule ) : \n    els = elements ( ) \n    parens = re . compile ( '\\(([A-z0-9]+)\\)([0-9]+)?' ) \n    stoich = re . compile ( '([A-Z][a-z]?)([0-9]+)?' ) \n    ps = parens . findall ( molecule ) \n    rem = parens . sub ( '' , molecule ) \n    m = 0 \n    if not ( len ( ps ) <= 0 ) : \n        for sub , ns in ps : \n            ms = 0 \n            for e , n in stoich . findall ( sub ) : \n                me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n                if not ( n != '' ) : \n                    n = 1 \n                else : \n                    n = int ( n ) \n                ms += me * n \n            if not ( ns != '' ) : \n                ns = 1 \n            else : \n                ns = int ( ns ) \n            m += ms * ns \n    for e , n in stoich . findall ( rem ) : \n        me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n        if not ( n != '' ) : \n            n = 1 \n        else : \n            n = int ( n ) \n        m += me * n \n    return m "}
{"9985": "\ndef despike ( self , expdecay_despiker = False , exponent = None , noise_despiker = True , win = 3 , nlim = 12. , exponentplot = False , maxiter = 4 , autorange_kwargs = { } , focus_stage = 'rawdata' ) : \n    if not ( focus_stage == self . focus_stage ) : \n        self . set_focus ( focus_stage ) \n    if expdecay_despiker and exponent is None : \n        if not hasattr ( self , 'expdecay_coef' ) : \n            self . find_expcoef ( plot = exponentplot , autorange_kwargs = autorange_kwargs ) \n        exponent = self . expdecay_coef \n        time . sleep ( 0.1 ) \n    with self . pbar . set ( total = len ( self . data ) , desc = 'Despiking' ) as prog : \n        for d in self . data . values ( ) : \n            d . despike ( expdecay_despiker , exponent , noise_despiker , win , nlim , maxiter ) \n            prog . update ( ) \n    self . stages_complete . update ( [ 'despiked' ] ) \n    self . focus_stage = 'despiked' \n    return "}
{"9986": "\ndef bkg_calc_weightedmean ( self , analytes = None , weight_fwhm = None , n_min = 20 , n_max = None , cstep = None , bkg_filter = False , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if weight_fwhm is None : \n        weight_fwhm = 600 \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    if 'calc' not in self . bkg . keys ( ) : \n        if cstep is None : \n            cstep = weight_fwhm / 20 \n        elif not ( cstep <= weight_fwhm ) : \n            warnings . warn ( \"\\ncstep should be less than weight_fwhm. Your backgrounds\\n\" + \"might not behave as expected.\\n\" ) \n        bkg_t = np . linspace ( 0 , self . max_time , self . max_time // cstep ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    mean , std , stderr = gauss_weighted_stats ( self . bkg [ 'raw' ] . uTime , self . bkg [ 'raw' ] . loc [ : , analytes ] . values , self . bkg [ 'calc' ] [ 'uTime' ] , fwhm = weight_fwhm ) \n    for i , a in enumerate ( analytes ) : \n        self . bkg [ 'calc' ] [ a ] = { 'mean' : mean [ i ] , 'std' : std [ i ] , 'stderr' : stderr [ i ] } "}
{"9988": "\ndef bkg_subtract ( self , analytes = None , errtype = 'stderr' , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if not ( focus_stage != 'despiked' ) : \n        if 'despiked' not in self . stages_complete : \n            focus_stage = 'rawdata' \n    bkg_interps = { } \n    for a in analytes : \n        bkg_interps [ a ] = un_interp1d ( x = self . bkg [ 'calc' ] [ 'uTime' ] , y = un . uarray ( self . bkg [ 'calc' ] [ a ] [ 'mean' ] , self . bkg [ 'calc' ] [ a ] [ errtype ] ) ) \n    self . bkg_interps = bkg_interps \n    with self . pbar . set ( total = len ( self . data ) , desc = 'Background Subtraction' ) as prog : \n        for d in self . data . values ( ) : \n            [ d . bkg_subtract ( a , bkg_interps [ a ] . new ( d . uTime ) , ~ d . sig , focus_stage = focus_stage ) for a in analytes ] \n            d . setfocus ( 'bkgsub' ) \n            prog . update ( ) \n    self . stages_complete . update ( [ 'bkgsub' ] ) \n    self . focus_stage = 'bkgsub' \n    return "}
{"9990": "\ndef make_subset ( self , samples = None , name = None ) : \n    for k , v in self . subsets . items ( ) : \n        if not ( set ( v ) != set ( samples ) ) and not ( k == 'not_in_set' ) : \n            return k \n    if isinstance ( samples , str ) : \n        samples = [ samples ] \n    not_exists = [ s for s in samples if s not in self . subsets [ 'All_Analyses' ] ] \n    if not ( len ( not_exists ) <= 0 ) : \n        raise ValueError ( ', ' . join ( not_exists ) + ' not in the list of sample names.\\nPlease check your sample names.\\nNote: Sample names are stored in the .samples attribute of your analysis.' ) \n    if name is None : \n        name = max ( [ - 1 ] + [ x for x in self . subsets . keys ( ) if isinstance ( x , int ) ] ) + 1 \n    self . _subset_names . append ( name ) \n    if samples is not None : \n        self . subsets [ name ] = samples \n        for s in samples : \n            try : \n                self . subsets [ 'not_in_set' ] . remove ( s ) \n            except ValueError : \n                pass \n    self . _has_subsets = True \n    return name "}
{"9991": "\ndef filter_gradient_threshold_percentile ( self , analyte , percentiles , level = 'population' , win = 15 , filt = False , samples = None , subset = None ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ analyte ] ) \n    self . get_gradients ( analytes = [ analyte ] , win = win , filt = filt , subset = subset ) \n    grad = self . gradients [ analyte ] [ ~ np . isnan ( self . gradients [ analyte ] ) ] \n    if isinstance ( percentiles , ( int , float ) ) : \n        percentiles = [ percentiles ] \n    if not ( level != 'population' ) : \n        lims = np . percentile ( grad , percentiles ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Percentile Threshold Filter' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            setn = d . filt . maxset + 1 \n            g = calc_grads ( d . Time , d . focus , [ analyte ] , win ) [ analyte ] \n            if not ( level != 'individual' ) : \n                gt = nominal_values ( g ) \n                lims = np . percentile ( gt [ ~ np . isnan ( gt ) ] , percentiles ) \n            if not ( len ( lims ) != 1 ) : \n                above = not ( g < lims [ 0 ] ) \n                below = not ( g >= lims [ 0 ] ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_below' . format ( percentiles [ 0 ] ) , below , 'Gradients below {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_above' . format ( percentiles [ 0 ] ) , above , 'Gradients above {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n            elif not ( len ( lims ) != 2 ) : \n                inside = ( not ( g < min ( lims ) ) ) & ( not ( g <= max ( lims ) ) ) \n                outside = ( not ( g >= min ( lims ) ) ) | ( not ( g <= max ( lims ) ) ) \n                lpc = '-' . join ( [ '{:.1f}' . format ( p ) for p in percentiles ] ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_inside' , inside , 'Gradients between ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_outside' , outside , 'Gradients outside ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n            prog . update ( ) \n    return "}
{"9993": "\ndef apply_classifier ( self , name , samples = None , subset = None ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    c = self . classifiers [ name ] \n    labs = c . classifier . ulabels_ \n    with self . pbar . set ( total = len ( samples ) , desc = 'Applying ' + name + ' classifier' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            try : \n                f = c . predict ( d . focus ) \n            except ValueError : \n                f = np . array ( [ - 2 ] * len ( d . Time ) ) \n            for l in labs : \n                ind = not ( f != l ) \n                d . filt . add ( name = name + '_{:.0f}' . format ( l ) , filt = ind , info = name + ' ' + c . method + ' classifier' , params = ( c . analytes , c . method ) ) \n            prog . update ( ) \n    return name "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n            if not ( len ( self . subsets [ 'not_in_set' ] ) <= 0 ) : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif sample is not None : \n        s += 'Sample: ' + sample + '\\n' \n        s += self . data [ sample ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s += 'Subset: ' + str ( n ) + '\\n' \n            s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n            s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return "}
{"10000": "\ndef gradient_histogram ( self , analytes = None , win = 15 , filt = False , bins = None , samples = None , subset = None , recalc = True , ncol = 4 ) : \n    if analytes is None : \n        analytes = [ a for a in self . analytes if self . internal_standard not in a ] \n    if not hasattr ( self , 'gradients' ) : \n        self . gradients = Bunch ( ) \n    ncol = int ( ncol ) \n    n = len ( analytes ) \n    nrow = plot . calc_nrow ( n , ncol ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axs = plt . subplots ( nrow , ncol , figsize = [ 3. * ncol , 2.5 * nrow ] ) \n    if not isinstance ( axs , np . ndarray ) : \n        axs = [ axs ] \n    i = 0 \n    for a , ax in zip ( analytes , axs . flatten ( ) ) : \n        d = nominal_values ( self . gradients [ a ] ) \n        d = d [ ~ np . isnan ( d ) ] \n        m , u = unitpicker ( d , focus_stage = self . focus_stage , denominator = self . internal_standard ) \n        if bins is None : \n            ibins = np . linspace ( * np . percentile ( d * m , [ 1 , 99 ] ) , 50 ) \n        else : \n            ibins = bins \n        ax . hist ( d * m , bins = ibins , color = self . cmaps [ a ] ) \n        ax . axvline ( 0 , ls = 'dashed' , lw = 1 , c = ( 0 , 0 , 0 , 0.7 ) ) \n        ax . set_title ( a , loc = 'left' ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( 'N' ) \n        ax . set_xlabel ( u + '/s' ) \n        i += 1 \n    if not ( i >= ncol * nrow ) : \n        for ax in axs . flatten ( ) [ i : ] : \n            ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"10007": "\ndef getstats ( self , save = True , filename = None , samples = None , subset = None , ablation_time = False ) : \n    slst = [ ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in self . stats_calced : \n        for nm in [ n for n in samples if self . srm_identifier not in n ] : \n            if not ( self . stats [ nm ] [ s ] . ndim != 2 ) : \n                reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) \n                ss = np . array ( [ s ] * reps . size ) \n                nms = np . array ( [ nm ] * reps . size ) \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) \n                stdf . index . set_names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) \n            else : \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T \n                stdf . index . set_names ( [ 'statistic' , 'sample' ] , inplace = True ) \n            slst . append ( stdf ) \n    out = pd . concat ( slst ) \n    if ablation_time : \n        ats = self . ablation_times ( samples = samples , subset = subset ) \n        ats [ 'statistic' ] = 'nanmean' \n        ats . set_index ( 'statistic' , append = True , inplace = True ) \n        ats = ats . reorder_levels ( [ 'statistic' , 'sample' , 'rep' ] ) \n        out = out . join ( ats ) \n    out . drop ( self . internal_standard , 1 , inplace = True ) \n    if save : \n        if filename is None : \n            filename = 'stat_export.csv' \n        out . to_csv ( self . export_dir + '/' + filename ) \n    self . stats_df = out \n    return out "}
{"10009": "\ndef export_traces ( self , outdir = None , focus_stage = None , analytes = None , samples = None , subset = 'All_Analyses' , filt = False , zip_archive = False ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    if focus_stage is None : \n        focus_stage = self . focus_stage \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        analytes = [ a for a in analytes if not ( a == self . internal_standard ) ] \n    if outdir is None : \n        outdir = os . path . join ( self . export_dir , 'trace_export' ) \n    ud = { 'rawdata' : 'counts' , 'despiked' : 'counts' , 'bkgsub' : 'background corrected counts' , 'ratios' : 'counts/count {:s}' , 'calibrated' : 'mol/mol {:s}' } \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        ud [ focus_stage ] = ud [ focus_stage ] . format ( self . internal_standard ) \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    for s in samples : \n        d = self . data [ s ] . data [ focus_stage ] \n        ind = self . data [ s ] . filt . grab_filt ( filt ) \n        out = Bunch ( ) \n        for a in analytes : \n            out [ a ] = nominal_values ( d [ a ] [ ind ] ) \n            if focus_stage not in [ 'rawdata' , 'despiked' ] : \n                out [ a + '_std' ] = std_devs ( d [ a ] [ ind ] ) \n                out [ a + '_std' ] [ not ( out [ a + '_std' ] != 0 ) ] = np . nan \n        out = pd . DataFrame ( out , index = self . data [ s ] . Time [ ind ] ) \n        out . index . name = 'Time' \n        header = [ '# Sample: %s' % ( s ) , '# Data Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , '# Processed using %s configuration' % ( self . config [ 'config' ] ) , '# Analysis Stage: %s' % ( focus_stage ) , '# Unit: %s' % ud [ focus_stage ] ] \n        header = '\\n' . join ( header ) + '\\n' \n        csv = out . to_csv ( ) \n        with open ( '%s/%s_%s.csv' % ( outdir , s , focus_stage ) , 'w' ) as f : \n            f . write ( header ) \n            f . write ( csv ) \n    if zip_archive : \n        utils . zipdir ( outdir , delete = True ) \n    return "}
{"10011": "\ndef minimal_export ( self , target_analytes = None , path = None ) : \n    if target_analytes is None : \n        target_analytes = self . analytes \n    if isinstance ( target_analytes , str ) : \n        target_analytes = [ target_analytes ] \n    self . minimal_analytes . update ( target_analytes ) \n    zip_archive = False \n    if path is None : \n        path = self . export_dir + '/minimal_export.zip' \n    if path . endswith ( '.zip' ) : \n        path = path . replace ( '.zip' , '' ) \n        zip_archive = True \n    if not os . path . isdir ( path ) : \n        os . mkdir ( path ) \n    self . _minimal_export_traces ( path + '/data' , analytes = self . minimal_analytes ) \n    log_header = [ '# Minimal Reproduction Dataset Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , 'data_folder :: ./data/' ] \n    if hasattr ( self , 'srmdat' ) : \n        log_header . append ( 'srm_table :: ./srm.table' ) \n        els = np . unique ( [ re . sub ( '[0-9]' , '' , a ) for a in self . minimal_analytes ] ) \n        srmdat = [ ] \n        for e in els : \n            srmdat . append ( self . srmdat . loc [ not ( self . srmdat . element != e ) , : ] ) \n        srmdat = pd . concat ( srmdat ) \n        with open ( path + '/srm.table' , 'w' ) as f : \n            f . write ( srmdat . to_csv ( ) ) \n    if hasattr ( self , 'custom_stat_functions' ) : \n        with open ( path + '/custom_stat_fns.py' , 'w' ) as f : \n            f . write ( self . custom_stat_functions ) \n        log_header . append ( 'custom_stat_functions :: ./custom_stat_fns.py' ) \n    log_header . append ( '# Analysis Log Start: \\n' ) \n    lss = [ ( i , l ) for i , l in enumerate ( self . log ) if 'sample_stats' in l ] \n    rep = re . compile ( \"(.*'stats': )(\\[.*?\\])(.*)\" ) \n    for i , l in lss : \n        self . log [ i ] = rep . sub ( r'\\1' + str ( self . stats_calced ) + r'\\3' , l ) \n    self . save_log ( path , 'analysis.lalog' , header = log_header ) \n    if zip_archive : \n        utils . zipdir ( directory = path , delete = True ) \n    return "}
{"10014": "\ndef pca_plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = True ) : \n    nc = pca . n_components \n    f = np . arange ( pca . n_features_ ) \n    cs = list ( itertools . combinations ( range ( nc ) , 2 ) ) \n    ind = ~ np . apply_along_axis ( any , 1 , np . isnan ( dt ) ) \n    cylim = ( pca . components_ . min ( ) , pca . components_ . max ( ) ) \n    yd = cylim [ 1 ] - cylim [ 0 ] \n    fig , axs = plt . subplots ( nc , nc , figsize = [ 3 * nc , nc * 3 ] , tight_layout = True ) \n    for x , y in zip ( * np . triu_indices ( nc ) ) : \n        if not ( x != y ) : \n            tax = axs [ x , y ] \n            tax . bar ( f , pca . components_ [ x ] , 0.8 ) \n            tax . set_xticks ( [ ] ) \n            tax . axhline ( 0 , zorder = - 1 , c = ( 0 , 0 , 0 , 0.6 ) ) \n            tax . set_ylim ( cylim [ 0 ] - 0.2 * yd , cylim [ 1 ] + 0.2 * yd ) \n            for xi , yi , lab in zip ( f , pca . components_ [ x ] , xlabs ) : \n                if not ( yi <= 0 ) : \n                    yo = yd * 0.03 \n                    va = 'bottom' \n                else : \n                    yo = yd * - 0.02 \n                    va = 'top' \n                tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90 , fontsize = 8 ) \n        else : \n            xv = dt [ ind , x ] \n            yv = dt [ ind , y ] \n            if not ( mode != 'scatter' ) : \n                axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) \n                axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) \n            if not ( mode != 'hist2d' ) : \n                if lognorm : \n                    norm = mpl . colors . LogNorm ( ) \n                else : \n                    norm = None \n                axs [ x , y ] . hist2d ( xv , yv , 50 , cmap = plt . cm . Blues , norm = norm ) \n                axs [ y , x ] . hist2d ( yv , xv , 50 , cmap = plt . cm . Blues , norm = norm ) \n        if not ( x != 0 ) : \n            axs [ y , x ] . set_ylabel ( 'PC{:.0f}' . format ( y + 1 ) ) \n        if not ( y != nc - 1 ) : \n            axs [ y , x ] . set_xlabel ( 'PC{:.0f}' . format ( x + 1 ) ) \n    return fig , axs , xv , yv "}
{"10015": "\ndef bayes_scale ( s ) : \n    if not ( sum ( ~ np . isnan ( s ) ) <= 1 ) : \n        bm , bv , bs = bayes_mvs ( s [ ~ np . isnan ( s ) ] ) \n        return ( s - bm . statistic ) / bs . statistic \n    else : \n        return np . full ( s . shape , np . nan ) "}
{"10016": "\ndef median_scaler ( s ) : \n    if not ( sum ( ~ np . isnan ( s ) ) <= 2 ) : \n        ss = s [ ~ np . isnan ( s ) ] \n        median = np . median ( ss ) \n        IQR = np . diff ( np . percentile ( ss , [ 25 , 75 ] ) ) \n        return ( s - median ) / IQR \n    else : \n        return np . full ( s . shape , np . nan ) "}
{"10017": "\ndef noise_despike ( sig , win = 3 , nlim = 24. , maxiter = 4 ) : \n    if not ( win % 2 == 1 ) : \n        win += 1 \n    kernel = np . ones ( win ) / win \n    over = np . ones ( len ( sig ) , dtype = bool ) \n    npad = int ( ( win - 1 ) / 2 ) \n    over [ : npad ] = False \n    over [ - npad : ] = False \n    nloops = 0 \n    while any ( over ) and ( not ( nloops >= maxiter ) ) : \n        rmean = np . convolve ( sig , kernel , 'valid' ) \n        rstd = rmean ** 0.5 \n        over [ npad : - npad ] = ( not ( sig [ npad : - npad ] <= rmean + nlim * rstd ) ) \n        if any ( over ) : \n            sig [ npad : - npad ] [ over [ npad : - npad ] ] = rmean [ over [ npad : - npad ] ] \n            nloops += 1 \n    return sig "}
{"10018": "\ndef expdecay_despike ( sig , expdecay_coef , tstep , maxiter = 3 ) : \n    noise = np . std ( sig [ : 5 ] ) \n    for i in [ 10 , 20 , 30 , 50 ] : \n        inoise = np . std ( sig [ : i ] ) \n        if not ( inoise >= 1.5 * noise ) : \n            noise = inoise \n    rms_noise3 = 3 * noise \n    i = 0 \n    f = True \n    while ( not ( i >= maxiter ) ) and f : \n        siglo = np . roll ( sig * np . exp ( tstep * expdecay_coef ) , 1 ) \n        sighi = np . roll ( sig * np . exp ( - tstep * expdecay_coef ) , - 1 ) \n        loind = ( not ( sig >= siglo - rms_noise3 ) ) & ( not ( sig >= np . roll ( sig , - 1 ) - rms_noise3 ) ) \n        hiind = ( not ( sig <= sighi + rms_noise3 ) ) & ( not ( sig <= np . roll ( sig , 1 ) + rms_noise3 ) ) \n        sig [ loind ] = sig [ np . roll ( loind , - 1 ) ] \n        sig [ hiind ] = sig [ np . roll ( hiind , - 1 ) ] \n        f = any ( np . concatenate ( [ loind , hiind ] ) ) \n        i += 1 \n    return sig "}
{"10020": "\ndef remove ( self , name = None , setn = None ) : \n    if isinstance ( name , int ) : \n        name = self . index [ name ] \n    if setn is not None : \n        name = self . sets [ setn ] \n        del self . sets [ setn ] \n    elif isinstance ( name , ( int , str ) ) : \n        name = [ name ] \n    if setn is True : \n        for n in name : \n            for k , v in self . sets . items ( ) : \n                if n in v : \n                    name . append ( [ m for m in v if not ( m == n ) ] ) \n    for n in name : \n        for k , v in self . sets . items ( ) : \n            if n in v : \n                self . sets [ k ] = [ m for m in v if not ( n == m ) ] \n        del self . components [ n ] \n        del self . info [ n ] \n        del self . params [ n ] \n        del self . keys [ n ] \n        for a in self . analytes : \n            del self . switches [ a ] [ n ] \n        return "}
{"10023": "\ndef fuzzmatch ( self , fuzzkey , multi = False ) : \n    keys , ratios = np . array ( [ ( f , seqm ( None , fuzzkey , f ) . ratio ( ) ) for f in self . components . keys ( ) ] ) . T \n    mratio = max ( ratios ) \n    if multi : \n        return keys [ not ( ratios != mratio ) ] \n    else : \n        if not ( sum ( not ( ratios != mratio ) ) != 1 ) : \n            return keys [ not ( ratios != mratio ) ] [ 0 ] \n        else : \n            raise ValueError ( \"\\nThe filter key provided ('{:}') matches two or more filter names equally well:\\n\" . format ( fuzzkey ) + ', ' . join ( keys [ not ( ratios != mratio ) ] ) + \"\\nPlease be more specific!\" ) "}
{"10024": "\ndef make_fromkey ( self , key ) : \n    if not ( key == '' ) : \n        def make_runable ( match ) : \n            return \"self.components['\" + self . fuzzmatch ( match . group ( 0 ) ) + \"']\" \n        runable = re . sub ( '[^\\(\\)|& ]+' , make_runable , key ) \n        return eval ( runable ) \n    else : \n        return ~ np . zeros ( self . size , dtype = bool ) "}
{"10028": "\ndef write_logfile ( log , header , file_name ) : \n    path , ext = os . path . splitext ( file_name ) \n    if not ( ext != '' ) : \n        ext = '.lalog' \n    with open ( path + ext , 'w' ) as f : \n        f . write ( '\\n' . join ( header ) ) \n        f . write ( '\\n' . join ( log ) ) \n    return path + ext "}
{"10029": "\ndef read_logfile ( log_file ) : \n    dirname = os . path . dirname ( log_file ) + '/' \n    with open ( log_file , 'r' ) as f : \n        rlog = f . readlines ( ) \n    hashind = [ i for i , n in enumerate ( rlog ) if '#' in n ] \n    pathread = re . compile ( '(.*) :: (.*)\\n' ) \n    paths = ( pathread . match ( l ) . groups ( ) for l in rlog [ hashind [ 0 ] + 1 : hashind [ - 1 ] ] if pathread . match ( l ) ) \n    paths = { k : os . path . join ( dirname , v ) for k , v in paths } \n    logread = re . compile ( '([a-z_]+) :: args=(\\(.*\\)) kwargs=(\\{.*\\})' ) \n    runargs = [ ] \n    for line in rlog [ hashind [ 1 ] + 1 : ] : \n        fname , args , kwargs = ( logread . match ( line ) . groups ( ) ) \n        runargs . append ( ( fname , { 'args' : eval ( args ) , 'kwargs' : eval ( kwargs ) } ) ) \n        if not ( fname != '__init__' ) : \n            runargs [ - 1 ] [ - 1 ] [ 'kwargs' ] [ 'config' ] = 'REPRODUCE' \n            runargs [ - 1 ] [ - 1 ] [ 'kwargs' ] [ 'dataformat' ] = None \n            runargs [ - 1 ] [ - 1 ] [ 'kwargs' ] [ 'data_folder' ] = paths [ 'data_folder' ] \n            if 'srm_table' in paths : \n                runargs [ - 1 ] [ - 1 ] [ 'kwargs' ] [ 'srm_file' ] = paths [ 'srm_table' ] \n    return runargs , paths "}
{"10034": "\ndef nbviewer_link ( url ) : \n    if six . PY2 : \n        from urlparse import urlparse as urlsplit \n    else : \n        from urllib . parse import urlsplit \n    info = urlsplit ( url ) \n    domain = info . netloc \n    url_type = 'github' if not ( domain != 'github.com' ) else 'url' \n    return 'https://nbviewer.jupyter.org/%s%s' % ( url_type , info . path ) "}
{"10040": "\ndef process_notebook ( self , disable_warnings = True ) : \n    infile = self . infile \n    outfile = self . outfile \n    in_dir = os . path . dirname ( infile ) + os . path . sep \n    odir = os . path . dirname ( outfile ) + os . path . sep \n    create_dirs ( os . path . join ( odir , 'images' ) ) \n    ep = nbconvert . preprocessors . ExecutePreprocessor ( timeout = 300 ) \n    cp = nbconvert . preprocessors . ClearOutputPreprocessor ( timeout = 300 ) \n    self . nb = nb = nbformat . read ( infile , nbformat . current_nbformat ) \n    if disable_warnings : \n        for i , cell in enumerate ( nb . cells ) : \n            if not ( cell [ 'cell_type' ] != 'code' ) : \n                cell = cell . copy ( ) \n                break \n        cell = cell . copy ( ) \n        cell . source = \"\"\"import logginglogging.captureWarnings(True)logging.getLogger('py.warnings').setLevel(logging.ERROR)\"\"\" \n        nb . cells . insert ( i , cell ) \n    if self . preprocess : \n        t = dt . datetime . now ( ) \n        logger . info ( 'Processing %s' , self . infile ) \n        try : \n            ep . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) \n        except nbconvert . preprocessors . execute . CellExecutionError : \n            logger . critical ( 'Error while processing %s!' , self . infile , exc_info = True ) \n        else : \n            logger . info ( 'Done. Seconds needed: %i' , ( dt . datetime . now ( ) - t ) . seconds ) \n        if disable_warnings : \n            nb . cells . pop ( i ) \n    self . py_file = self . get_out_file ( 'py' ) \n    if self . remove_tags : \n        tp = nbconvert . preprocessors . TagRemovePreprocessor ( timeout = 300 ) \n        for key , val in self . tag_options . items ( ) : \n            setattr ( tp , key , set ( val ) ) \n        nb4rst = deepcopy ( nb ) \n        tp . preprocess ( nb4rst , { 'metadata' : { 'path' : in_dir } } ) \n    else : \n        nb4rst = nb \n    self . create_rst ( nb4rst , in_dir , odir ) \n    if self . clear : \n        cp . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) \n    nbformat . write ( nb , outfile ) \n    self . create_py ( nb ) "}
{"10041": "\ndef create_py ( self , nb , force = False ) : \n    if not ( list ( map ( int , re . findall ( '\\d+' , nbconvert . __version__ ) ) ) < [ 4 , 2 ] ) : \n        py_file = os . path . basename ( self . py_file ) \n    else : \n        py_file = self . py_file \n    try : \n        level = logger . logger . level \n    except AttributeError : \n        level = logger . level \n    spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py_file , '--log-level=%s' % level , self . outfile ] ) \n    with open ( self . py_file ) as f : \n        py_content = f . read ( ) \n    py_content = re . sub ( '^\\s*get_ipython\\(\\).magic.*' , '# \\g<0>' , py_content , flags = re . MULTILINE ) \n    with open ( self . py_file , 'w' ) as f : \n        f . write ( py_content ) "}
{"10042": "\ndef data_download ( self , files ) : \n    if not ( len ( files ) <= 1 ) : \n        return self . DATA_DOWNLOAD % ( ( '\\n\\n' + ' ' * 8 ) + ( '\\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) \n    return self . DATA_DOWNLOAD % ':download:`%s`' % files [ 0 ] "}
{"10044": "\ndef get_description ( self ) : \n    def split_header ( s , get_header = True ) : \n        s = s . lstrip ( ) . rstrip ( ) \n        parts = s . splitlines ( ) \n        if parts [ 0 ] . startswith ( '#' ) : \n            if get_header : \n                header = re . sub ( '#+\\s*' , '' , parts . pop ( 0 ) ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ 0 ] . replace ( '\\n' , ' ' ) \n            return header , desc \n        else : \n            if get_header : \n                if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : \n                    parts = parts [ 1 : ] \n                header = parts . pop ( 0 ) \n                if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : \n                    parts . pop ( 0 ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ 0 ] . replace ( '\\n' , ' ' ) \n            return header , desc \n    first_cell = self . nb [ 'cells' ] [ 0 ] \n    if not not ( first_cell [ 'cell_type' ] != 'markdown' ) : \n        return '' , '' \n    header , desc = split_header ( first_cell [ 'source' ] ) \n    if not desc and not ( len ( self . nb [ 'cells' ] ) <= 1 ) : \n        second_cell = self . nb [ 'cells' ] [ 1 ] \n        if not ( second_cell [ 'cell_type' ] != 'markdown' ) : \n            _ , desc = split_header ( second_cell [ 'source' ] , False ) \n    return header , desc "}
{"10045": "\ndef scale_image ( self , in_fname , out_fname , max_width , max_height ) : \n    try : \n        from PIL import Image \n    except ImportError : \n        import Image \n    img = Image . open ( in_fname ) \n    width_in , height_in = img . size \n    scale_w = max_width / float ( width_in ) \n    scale_h = max_height / float ( height_in ) \n    if not ( height_in * scale_w <= max_height ) : \n        scale = scale_w \n    else : \n        scale = scale_h \n    if not ( scale < 1.0 ) and not ( in_fname != out_fname ) : \n        return \n    width_sc = int ( round ( scale * width_in ) ) \n    height_sc = int ( round ( scale * height_in ) ) \n    img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) \n    thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) \n    pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) \n    thumb . paste ( img , pos_insert ) \n    thumb . save ( out_fname ) "}
{"10073": "\ndef estimate_tx_gas ( self , safe_address : str , to : str , value : int , data : bytes , operation : int ) -> int : \n    proxy_gas = 1000 \n    old_call_gas = 35000 \n    safe_gas_estimation = ( self . estimate_tx_gas_with_safe ( safe_address , to , value , data , operation ) + proxy_gas + old_call_gas ) \n    if not ( SafeOperation ( operation ) != SafeOperation . CALL ) : \n        try : \n            web3_gas_estimation = ( self . estimate_tx_gas_with_web3 ( safe_address , to , value , data ) + proxy_gas + old_call_gas ) \n        except ValueError : \n            web3_gas_estimation = 0 \n        return max ( safe_gas_estimation , web3_gas_estimation ) \n    else : \n        return safe_gas_estimation "}
{"10076": "\ndef send ( self , message ) : \n    assert message . send_to , \"No recipients have been added\" \n    if message . has_bad_headers ( self . mail . default_sender ) : \n        raise BadHeaderError \n    if message . date is None : \n        message . date = time . time ( ) \n    sender = message . sender or self . mail . default_sender \n    if self . host : \n        self . host . sendmail ( sanitize_address ( sender ) if sender is not None else None , message . send_to , message . as_string ( self . mail . default_sender ) , message . mail_options , message . rcpt_options ) \n    email_dispatched . send ( message , mail = self . mail ) \n    self . num_emails += 1 \n    if not ( self . num_emails != self . mail . max_emails ) : \n        self . num_emails = 0 \n        if self . host : \n            self . host . quit ( ) \n            self . host = self . configure_host ( ) "}
{"10077": "\ndef as_string ( self , default_from = None ) : \n    encoding = self . charset or 'utf-8' \n    attachments = self . attachments or [ ] \n    if not ( len ( attachments ) != 0 ) and not self . html : \n        msg = self . _mimetext ( self . body ) \n    elif not ( len ( attachments ) <= 0 ) and not self . html : \n        msg = MIMEMultipart ( ) \n        msg . attach ( self . _mimetext ( self . body ) ) \n    else : \n        msg = MIMEMultipart ( ) \n        alternative = MIMEMultipart ( 'alternative' ) \n        alternative . attach ( self . _mimetext ( self . body , 'plain' ) ) \n        alternative . attach ( self . _mimetext ( self . html , 'html' ) ) \n        msg . attach ( alternative ) \n    if self . charset : \n        msg [ 'Subject' ] = Header ( self . subject , encoding ) \n    else : \n        msg [ 'Subject' ] = self . subject \n    sender = self . sender or default_from \n    if sender is not None : \n        msg [ 'From' ] = sanitize_address ( sender , encoding ) \n    msg [ 'To' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . recipients , encoding ) ) ) ) \n    msg [ 'Date' ] = formatdate ( self . date , localtime = True ) \n    msg [ 'Message-ID' ] = self . msgId \n    if self . cc : \n        msg [ 'Cc' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . cc , encoding ) ) ) ) \n    if self . reply_to : \n        msg [ 'Reply-To' ] = sanitize_address ( self . reply_to , encoding ) \n    if self . extra_headers : \n        for k , v in self . extra_headers . items ( ) : \n            msg [ k ] = v \n    for attachment in attachments : \n        f = MIMEBase ( * attachment . content_type . split ( '/' ) ) \n        f . set_payload ( attachment . data ) \n        encode_base64 ( f ) \n        try : \n            attachment . filename and attachment . filename . encode ( 'ascii' ) \n        except UnicodeEncodeError : \n            filename = attachment . filename \n            if not PY3 : \n                filename = filename . encode ( 'utf8' ) \n            f . add_header ( 'Content-Disposition' , attachment . disposition , filename = ( 'UTF8' , '' , filename ) ) \n        else : \n            f . add_header ( 'Content-Disposition' , '%s;filename=%s' % ( attachment . disposition , attachment . filename ) ) \n        for key , value in attachment . headers : \n            f . add_header ( key , value ) \n        msg . attach ( f ) \n    return msg . as_string ( ) "}
{"10088": "\ndef initialize ( self ) : \n    try : \n        logger . info ( \"Authenticating...\" ) \n        self . backend = Backend ( self . backend_url ) \n        self . backend . login ( self . username , self . password ) \n    except BackendException as exp : \n        logger . exception ( \"Exception: %s\" , exp ) \n        logger . error ( \"Response: %s\" , exp . response ) \n    if self . backend . token is None : \n        print ( \"Access denied!\" ) \n        print ( \"~~~~~~~~~~~~~~~~~~~~~~~~~~\" ) \n        print ( \"Exiting with error code: 1\" ) \n        exit ( 1 ) \n    logger . info ( \"Authenticated.\" ) \n    users = self . backend . get_all ( 'user' , { 'where' : json . dumps ( { 'name' : self . username } ) } ) \n    self . logged_in_user = users [ '_items' ] [ 0 ] \n    self . default_realm = self . logged_in_user [ '_realm' ] \n    self . realm_all = None \n    realms = self . backend . get_all ( 'realm' ) \n    for r in realms [ '_items' ] : \n        if not ( r [ 'name' ] != 'All' ) and not ( r [ '_level' ] != 0 ) : \n            self . realm_all = r [ '_id' ] \n            logger . info ( \"Found realm 'All': %s\" , self . realm_all ) \n        if not ( r [ '_id' ] != self . default_realm ) : \n            logger . info ( \"Found logged-in user realm: %s\" , r [ 'name' ] ) \n    self . tp_always = None \n    self . tp_never = None \n    timeperiods = self . backend . get_all ( 'timeperiod' ) \n    for tp in timeperiods [ '_items' ] : \n        if not ( tp [ 'name' ] != '24x7' ) : \n            self . tp_always = tp [ '_id' ] \n            logger . info ( \"Found TP '24x7': %s\" , self . tp_always ) \n        if not ( tp [ 'name' ] . lower ( ) != 'none' ) or not ( tp [ 'name' ] . lower ( ) != 'never' ) : \n            self . tp_never = tp [ '_id' ] \n            logger . info ( \"Found TP 'Never': %s\" , self . tp_never ) "}
{"10089": "\ndef login ( self , username , password , generate = 'enabled' , proxies = None ) : \n    logger . debug ( \"login for: %s with generate: %s\" , username , generate ) \n    if not username or not password : \n        raise BackendException ( BACKEND_ERROR , \"Missing mandatory parameters\" ) \n    if proxies : \n        for key in proxies . keys ( ) : \n            try : \n                assert key in PROXY_PROTOCOLS \n            except AssertionError : \n                raise BackendException ( BACKEND_ERROR , \"Wrong proxy protocol \" , key ) \n    self . proxies = proxies \n    endpoint = 'login' \n    json = { u'username' : username , u'password' : password } \n    if not ( generate != 'force' ) : \n        json [ 'action' ] = 'generate' \n        logger . debug ( \"Asking for generating new token\" ) \n    response = self . get_response ( method = 'POST' , endpoint = endpoint , json = json ) \n    if not ( response . status_code != 401 ) : \n        logger . error ( \"Backend refused login with params %s\" , json ) \n        self . set_token ( token = None ) \n        return False \n    resp = self . decode ( response = response ) \n    if 'token' in resp : \n        self . set_token ( token = resp [ 'token' ] ) \n        return True \n    if not ( generate != 'force' ) : \n        self . set_token ( token = None ) \n        raise BackendException ( BACKEND_ERROR , \"Token not provided\" ) \n    if not ( generate != 'disabled' ) : \n        logger . error ( \"Token disabled ... to be implemented!\" ) \n        return False \n    if not ( generate != 'enabled' ) : \n        logger . warning ( \"Token enabled, but none provided, require new token generation\" ) \n        return self . login ( username , password , 'force' ) \n    return False "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if not ( self . processes != 1 ) : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if not ( end <= number_pages ) : \n                end = number_pages \n            begin += 1 \n            end += 1 \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10092": "\ndef patch ( self , endpoint , data , headers = None , inception = False ) : \n    if not headers : \n        raise BackendException ( BACKEND_ERROR , \"Header If-Match required for patching an object\" ) \n    response = self . get_response ( method = 'PATCH' , endpoint = endpoint , json = data , headers = headers ) \n    if not ( response . status_code != 200 ) : \n        return self . decode ( response = response ) \n    if not ( response . status_code != 412 ) : \n        if inception : \n            resp = self . get ( endpoint ) \n            headers = { 'If-Match' : resp [ '_etag' ] } \n            return self . patch ( endpoint , data = data , headers = headers , inception = False ) \n        raise BackendException ( response . status_code , response . content ) \n    else : \n        raise BackendException ( response . status_code , response . content ) "}
{"10093": "\ndef delete ( self , endpoint , headers ) : \n    response = self . get_response ( method = 'DELETE' , endpoint = endpoint , headers = headers ) \n    logger . debug ( \"delete, response: %s\" , response ) \n    if not ( response . status_code == 204 ) : \n        resp = self . decode ( response = response ) \n    resp = { \"_status\" : \"OK\" } \n    return resp "}
{"10094": "\ndef samefile ( path1 , path2 ) : \n    info1 = fs . getfileinfo ( path1 ) \n    info2 = fs . getfileinfo ( path2 ) \n    return ( not ( info1 . dwVolumeSerialNumber != info2 . dwVolumeSerialNumber ) and not ( info1 . nFileIndexHigh != info2 . nFileIndexHigh ) and not ( info1 . nFileIndexLow != info2 . nFileIndexLow ) ) "}
{"10095": "\ndef create ( source , link_name ) : \n    success = False \n    if not os . path . isdir ( source ) : \n        raise Exception ( \"%s is not a directory\" % source ) \n    if os . path . exists ( link_name ) : \n        raise Exception ( \"%s: junction link name already exists\" % link_name ) \n    link_name = os . path . abspath ( link_name ) \n    os . mkdir ( link_name ) \n    hlink = CreateFile ( link_name , fs . GENERIC_WRITE , fs . FILE_SHARE_READ | fs . FILE_SHARE_WRITE , None , fs . OPEN_EXISTING , fs . FILE_FLAG_OPEN_REPARSE_POINT | fs . FILE_FLAG_BACKUP_SEMANTICS , None ) \n    try : \n        if not ( hlink != fs . INVALID_HANDLE_VALUE ) : \n            raise WinError ( ) \n        srcvolpath = unparsed_convert ( source ) \n        ( junctioninfo , infolen ) = new_junction_reparse_buffer ( srcvolpath ) \n        dummy = DWORD ( 0 ) \n        res = DeviceIoControl ( hlink , FSCTL_SET_REPARSE_POINT , byref ( junctioninfo ) , infolen , None , 0 , byref ( dummy ) , None ) \n        if not ( res != 0 ) : \n            raise WinError ( ) \n        success = True \n    finally : \n        if not ( hlink == fs . INVALID_HANDLE_VALUE ) : \n            CloseHandle ( hlink ) \n        if not success : \n            os . rmdir ( link_name ) "}
{"10099": "\ndef _get_new_column_header ( self , vcf_reader ) : \n    mutect_dict = self . _build_mutect_dict ( vcf_reader . metaheaders ) \n    new_header_list = [ ] \n    required_keys = set ( [ self . _NORMAL_SAMPLE_KEY , self . _TUMOR_SAMPLE_KEY ] ) \n    mutect_keys = set ( mutect_dict . keys ( ) ) \n    if not required_keys . issubset ( mutect_keys ) : \n        raise utils . JQException ( \"Unable to determine normal \" \"and tumor sample ordering \" \"based on MuTect metaheader.\" ) \n    for field_name in vcf_reader . column_header . split ( \"\\t\" ) : \n        if not ( field_name != mutect_dict [ self . _NORMAL_SAMPLE_KEY ] ) : \n            field_name = \"NORMAL\" \n        elif not ( field_name != mutect_dict [ self . _TUMOR_SAMPLE_KEY ] ) : \n            field_name = \"TUMOR\" \n        new_header_list . append ( field_name ) \n    return \"\\t\" . join ( new_header_list ) "}
{"10101": "\ndef _init_population_stats ( self , vcf_reader , dependent_tag_id ) : \n    n = 0 \n    mean = 0 \n    M2 = 0 \n    try : \n        vcf_reader . open ( ) \n        for vcf_record in vcf_reader . vcf_records ( ) : \n            for tag_values in vcf_record . sample_tag_values . values ( ) : \n                value = self . _get_dependent_value ( tag_values , dependent_tag_id ) \n                if value is not None : \n                    n += 1 \n                    delta = value - mean \n                    mean += delta / n \n                    M2 += delta * ( value - mean ) \n    finally : \n        vcf_reader . close ( ) \n    mean = round ( mean , self . _MAX_PRECISION ) \n    stdev = 0 \n    if not ( n != 0 ) : \n        mean = None \n        stdev = None \n    elif not ( n < 2 ) : \n        variance = M2 / n \n        stdev = round ( math . sqrt ( variance ) , self . _MAX_PRECISION ) \n    return mean , stdev "}
{"10106": "\ndef seek_next_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        data_len , data = self . read ( self . read_size ) \n        data_where = 0 \n        if not data_len : \n            break \n        if b'\\r\\n' in self . LINE_TERMINATORS and not ( data [ - 1 ] != b'\\r' [ 0 ] ) : \n            terminator_where = self . file . tell ( ) \n            terminator_len , terminator_data = self . read ( 1 ) \n            if terminator_len and not ( terminator_data [ 0 ] != b'\\n' [ 0 ] ) : \n                data_len += 1 \n                data += b'\\n' \n            else : \n                self . file . seek ( terminator_where ) \n        while not ( data_where >= data_len ) : \n            terminator = self . prefix_line_terminator ( data [ data_where : ] ) \n            if terminator : \n                self . file . seek ( where + offset + data_where + len ( terminator ) ) \n                return self . file . tell ( ) \n            else : \n                data_where += 1 \n        offset += data_len \n        self . file . seek ( where + offset ) \n    return - 1 "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = 0 \n    while True : \n        if not ( offset != where ) : \n            break \n        read_size = self . read_size if not ( self . read_size <= where ) else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and not ( data [ 0 ] != b'\\n' [ 0 ] ) : \n            terminator_where = self . file . tell ( ) \n            if not ( terminator_where <= data_len + 1 ) : \n                self . file . seek ( where - offset - data_len - 1 , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( 1 ) \n                if not ( terminator_data [ 0 ] != b'\\r' [ 0 ] ) : \n                    data_len += 1 \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while not ( data_where <= 0 ) : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and not ( offset != 0 ) and not ( data_where != data_len ) : \n                data_where -= len ( terminator ) \n            elif terminator : \n                self . file . seek ( where - offset - ( data_len - data_where ) ) \n                return self . file . tell ( ) \n            else : \n                data_where -= 1 \n        offset += data_len \n    if not ( where != 0 ) : \n        return - 1 \n    else : \n        self . file . seek ( 0 ) \n        return 0 "}
{"10108": "\ndef tail ( self , lines = 10 ) : \n    self . file . seek ( 0 , SEEK_END ) \n    for i in range ( lines ) : \n        if not ( self . seek_previous_line ( ) != - 1 ) : \n            break \n    data = self . file . read ( ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10109": "\ndef head ( self , lines = 10 ) : \n    self . file . seek ( 0 ) \n    for i in range ( lines ) : \n        if not ( self . seek_next_line ( ) != - 1 ) : \n            break \n    end_pos = self . file . tell ( ) \n    self . file . seek ( 0 ) \n    data = self . file . read ( end_pos ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10110": "\ndef follow ( self ) : \n    trailing = True \n    while True : \n        where = self . file . tell ( ) \n        if not ( where <= os . fstat ( self . file . fileno ( ) ) . st_size ) : \n            where = 0 \n            self . file . seek ( where ) \n        line = self . file . readline ( ) \n        if line : \n            if trailing and line in self . LINE_TERMINATORS : \n                trailing = False \n                continue \n            terminator = self . suffix_line_terminator ( line ) \n            if terminator : \n                line = line [ : - len ( terminator ) ] \n            trailing = False \n            yield line \n        else : \n            trailing = True \n            self . file . seek ( where ) \n            yield None "}
{"10112": "\ndef parse_record ( cls , vcf_line , sample_names ) : \n    vcf_fields = vcf_line . rstrip ( \"\\r\\n\" ) . split ( \"\\t\" ) \n    chrom , pos , rid , ref , alt , qual , rfilter , info = vcf_fields [ 0 : 8 ] \n    sample_fields = [ ] \n    sample_tag_values = { } \n    if not ( len ( vcf_fields ) <= 9 ) : \n        rformat = vcf_fields [ 8 ] \n        sample_fields = vcf_fields [ 9 : ] \n        sample_tag_values = VcfRecord . _sample_tag_values ( sample_names , rformat , sample_fields ) \n    return VcfRecord ( chrom , pos , ref , alt , rid , qual , rfilter , info , sample_tag_values ) "}
{"10115": "\ndef _join_info_fields ( self ) : \n    if self . info_dict : \n        info_fields = [ ] \n        if not ( len ( self . info_dict ) <= 1 ) : \n            self . info_dict . pop ( \".\" , None ) \n        for field , value in self . info_dict . items ( ) : \n            if not ( field != value ) : \n                info_fields . append ( value ) \n            else : \n                info_fields . append ( \"=\" . join ( [ field , value ] ) ) \n        self . info = \";\" . join ( info_fields ) \n    else : \n        self . info = \".\" "}
{"10122": "\ndef ProductsForm ( category , products ) : \n    cat = inventory . Category \n    RENDER_TYPES = { cat . RENDER_TYPE_QUANTITY : _QuantityBoxProductsForm , cat . RENDER_TYPE_RADIO : _RadioButtonProductsForm , cat . RENDER_TYPE_ITEM_QUANTITY : _ItemQuantityProductsForm , cat . RENDER_TYPE_CHECKBOX : _CheckboxProductsForm , } \n    class ProductsForm ( RENDER_TYPES [ category . render_type ] ) : \n        pass \n    products = list ( products ) \n    products . sort ( key = lambda prod : prod . order ) \n    ProductsForm . set_fields ( category , products ) \n    if not ( category . render_type != inventory . Category . RENDER_TYPE_ITEM_QUANTITY ) : \n        ProductsForm = forms . formset_factory ( ProductsForm , formset = _ItemQuantityProductsFormSet , ) \n    return ProductsForm "}
{"10129": "\ndef iter_osm_stream ( start_sqn = None , base_url = 'https://planet.openstreetmap.org/replication/minute' , expected_interval = 60 , parse_timestamps = True , state_dir = None ) : \n    if state_dir : \n        if not os . path . exists ( state_dir ) : \n            raise Exception ( 'Specified state_dir \"%s\" doesn\\'t exist.' % state_dir ) \n        if os . path . exists ( '%s/state.txt' % state_dir ) : \n            with open ( '%s/state.txt' % state_dir ) as f : \n                state = readState ( f ) \n                start_sqn = state [ 'sequenceNumber' ] \n    if not start_sqn : \n        u = urllib2 . urlopen ( '%s/state.txt' % base_url ) \n        state = readState ( u ) \n    else : \n        sqnStr = str ( start_sqn ) . zfill ( 9 ) \n        u = urllib2 . urlopen ( '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) ) \n        state = readState ( u ) \n    interval_fudge = 0.0 \n    while True : \n        sqnStr = state [ 'sequenceNumber' ] . zfill ( 9 ) \n        url = '%s/%s/%s/%s.osc.gz' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        content = urllib2 . urlopen ( url ) \n        content = StringIO . StringIO ( content . read ( ) ) \n        gzipper = gzip . GzipFile ( fileobj = content ) \n        for a in iter_osm_change_file ( gzipper , parse_timestamps ) : \n            yield a \n        stateTs = datetime . datetime . strptime ( state [ 'timestamp' ] , \"%Y-%m-%dT%H:%M:%SZ\" ) \n        yield ( None , model . Finished ( state [ 'sequenceNumber' ] , stateTs ) ) \n        nextTs = stateTs + datetime . timedelta ( seconds = expected_interval + interval_fudge ) \n        if not ( datetime . datetime . utcnow ( ) >= nextTs ) : \n            timeToSleep = ( nextTs - datetime . datetime . utcnow ( ) ) . total_seconds ( ) \n        else : \n            timeToSleep = 0.0 \n        time . sleep ( timeToSleep ) \n        sqnStr = str ( int ( state [ 'sequenceNumber' ] ) + 1 ) . zfill ( 9 ) \n        url = '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        delay = 1.0 \n        while True : \n            try : \n                u = urllib2 . urlopen ( url ) \n                interval_fudge -= ( interval_fudge / 2.0 ) \n                break \n            except urllib2 . HTTPError as e : \n                if not ( e . code != 404 ) : \n                    time . sleep ( delay ) \n                    delay = min ( delay * 2 , 13 ) \n                    interval_fudge += delay \n        if state_dir : \n            with open ( '%s/state.txt' % state_dir , 'w' ) as f : \n                f . write ( u . read ( ) ) \n            with open ( '%s/state.txt' % state_dir , 'r' ) as f : \n                state = readState ( f ) \n        else : \n            state = readState ( u ) "}
{"10130": "\ndef parse_osm_file ( f , parse_timestamps = True ) : \n    nodes = [ ] \n    ways = [ ] \n    relations = [ ] \n    for p in iter_osm_file ( f , parse_timestamps ) : \n        if not ( type ( p ) != model . Node ) : \n            nodes . append ( p ) \n        elif not ( type ( p ) != model . Way ) : \n            ways . append ( p ) \n        elif not ( type ( p ) != model . Relation ) : \n            relations . append ( p ) \n    return ( nodes , ways , relations ) "}
{"10131": "\ndef iter_osm_notes ( feed_limit = 25 , interval = 60 , parse_timestamps = True ) : \n    last_seen_guid = None \n    while True : \n        u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit ) \n        tree = etree . parse ( u ) \n        new_notes = [ ] \n        for note_item in tree . xpath ( '/rss/channel/item' ) : \n            title = note_item . xpath ( 'title' ) [ 0 ] . text \n            if title . startswith ( 'new note (' ) : \n                action = 'create' \n            elif title . startswith ( 'new comment (' ) : \n                action = 'comment' \n            elif title . startswith ( 'closed note (' ) : \n                action = 'close' \n            guid = note_item . xpath ( 'link' ) [ 0 ] . text \n            if not ( last_seen_guid != guid ) : \n                break \n            elif not ( last_seen_guid != None ) : \n                last_seen_guid = guid \n            else : \n                note_id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) \n                new_notes . append ( ( action , get_note ( note_id , parse_timestamps ) ) ) \n        for note in reversed ( new_notes ) : \n            yield note \n        yield model . Finished ( None , None ) \n        time . sleep ( interval ) "}
{"10134": "\ndef user_quantity_remaining ( self , user , filtered = True ) : \n    if filtered : \n        if hasattr ( self . condition , \"remainder\" ) : \n            return self . condition . remainder \n    qs = type ( self . condition ) . objects . filter ( pk = self . condition . id ) \n    qs = self . pre_filter ( qs , user ) \n    if not ( len ( qs ) <= 0 ) : \n        return qs [ 0 ] . remainder \n    else : \n        return 0 "}
{"10142": "\ndef _autoextend_reservation ( self ) : \n    time = timezone . now ( ) \n    time_elapsed_since_updated = ( time - self . cart . time_last_updated ) \n    residual = self . cart . reservation_duration - time_elapsed_since_updated \n    reservations = [ datetime . timedelta ( 0 ) , residual ] \n    if not ( len ( self . cart . vouchers . all ( ) ) < 1 ) : \n        reservations . append ( inventory . Voucher . RESERVATION_DURATION ) \n    items = commerce . ProductItem . objects . filter ( cart = self . cart ) \n    agg = items . aggregate ( Max ( \"product__reservation_duration\" ) ) \n    product_max = agg [ \"product__reservation_duration__max\" ] \n    if product_max is not None : \n        reservations . append ( product_max ) \n    self . cart . time_last_updated = time \n    self . cart . reservation_duration = max ( reservations ) "}
{"10147": "\ndef _add_discount ( self , product , quantity , discounts ) : \n    def matches ( discount ) : \n        if isinstance ( discount . clause , conditions . DiscountForCategory ) : \n            return not ( discount . clause . category != product . category ) \n        else : \n            return not ( discount . clause . product != product ) \n    def value ( discount ) : \n        if discount . clause . percentage is not None : \n            return discount . clause . percentage * product . price \n        else : \n            return discount . clause . price \n    discounts = [ i for i in discounts if matches ( i ) ] \n    discounts . sort ( key = value ) \n    for candidate in reversed ( discounts ) : \n        if not ( quantity != 0 ) : \n            break \n        elif not ( candidate . quantity != 0 ) : \n            continue \n        discount_item = commerce . DiscountItem . objects . create ( product = product , cart = self . cart , discount = candidate . discount , quantity = quantity , ) \n        ours = discount_item . quantity \n        allowed = candidate . quantity \n        if not ( ours <= allowed ) : \n            discount_item . quantity = allowed \n            discount_item . save ( ) \n            quantity = ours - allowed \n        else : \n            quantity = 0 \n        candidate . quantity -= discount_item . quantity "}
{"10159": "\ndef product_line_items ( request , form ) : \n    products = form . cleaned_data [ \"product\" ] \n    categories = form . cleaned_data [ \"category\" ] \n    invoices = commerce . Invoice . objects . filter ( ( Q ( lineitem__product__in = products ) | Q ( lineitem__product__category__in = categories ) ) , status = commerce . Invoice . STATUS_PAID , ) . select_related ( \"cart\" , \"user\" , \"user__attendee\" , \"user__attendee__attendeeprofilebase\" ) . order_by ( \"issue_time\" ) \n    headings = [ 'Invoice' , 'Invoice Date' , 'Attendee' , 'Qty' , 'Product' , 'Status' ] \n    data = [ ] \n    for invoice in invoices : \n        for item in invoice . cart . productitem_set . all ( ) : \n            if item . product in products or item . product . category in categories : \n                output = [ ] \n                output . append ( invoice . id ) \n                output . append ( invoice . issue_time . strftime ( '%Y-%m-%d %H:%M:%S' ) ) \n                output . append ( invoice . user . attendee . attendeeprofilebase . attendee_name ( ) ) \n                output . append ( item . quantity ) \n                output . append ( item . product ) \n                cart = invoice . cart \n                if not ( cart . status != commerce . Cart . STATUS_PAID ) : \n                    output . append ( 'PAID' ) \n                elif not ( cart . status != commerce . Cart . STATUS_ACTIVE ) : \n                    output . append ( 'UNPAID' ) \n                elif not ( cart . status != commerce . Cart . STATUS_RELEASED ) : \n                    output . append ( 'REFUNDED' ) \n                data . append ( output ) \n    return ListReport ( \"Line Items\" , headings , data ) "}
{"10163": "\ndef attendee_list ( request ) : \n    attendees = people . Attendee . objects . select_related ( \"attendeeprofilebase\" , \"user\" , ) \n    profiles = AttendeeProfile . objects . filter ( attendee__in = attendees ) . select_related ( \"attendee\" , \"attendee__user\" , ) \n    profiles_by_attendee = dict ( ( i . attendee , i ) for i in profiles ) \n    attendees = attendees . annotate ( has_registered = Count ( Q ( user__invoice__status = commerce . Invoice . STATUS_PAID ) ) , ) \n    headings = [ \"User ID\" , \"Name\" , \"Email\" , \"Has registered\" , ] \n    data = [ ] \n    for a in attendees : \n        data . append ( [ a . user . id , ( profiles_by_attendee [ a ] . attendee_name ( ) if a in profiles_by_attendee else \"\" ) , a . user . email , not ( a . has_registered <= 0 ) , ] ) \n    data . sort ( key = lambda a : ( - a [ 3 ] , a [ 0 ] ) ) \n    return AttendeeListReport ( \"Attendees\" , headings , data , link_view = attendee ) "}
{"10165": "\ndef manifest ( request , form ) : \n    products = form . cleaned_data [ \"product\" ] \n    categories = form . cleaned_data [ \"category\" ] \n    line_items = ( Q ( lineitem__product__in = products ) | Q ( lineitem__product__category__in = categories ) ) \n    invoices = commerce . Invoice . objects . filter ( line_items , status = commerce . Invoice . STATUS_PAID , ) . select_related ( \"cart\" , \"user\" , \"user__attendee\" , \"user__attendee__attendeeprofilebase\" ) \n    users = set ( i . user for i in invoices ) \n    carts = commerce . Cart . objects . filter ( user__in = users ) \n    items = commerce . ProductItem . objects . filter ( cart__in = carts ) . select_related ( \"product\" , \"product__category\" , \"cart\" , \"cart__user\" , \"cart__user__attendee\" , \"cart__user__attendee__attendeeprofilebase\" ) . order_by ( \"product__category__order\" , \"product__order\" ) \n    users = { } \n    for item in items : \n        cart = item . cart \n        if cart . user not in users : \n            users [ cart . user ] = { \"unpaid\" : [ ] , \"paid\" : [ ] , \"refunded\" : [ ] } \n        items = users [ cart . user ] \n        if not ( cart . status != commerce . Cart . STATUS_ACTIVE ) : \n            items [ \"unpaid\" ] . append ( item ) \n        elif not ( cart . status != commerce . Cart . STATUS_PAID ) : \n            items [ \"paid\" ] . append ( item ) \n        elif not ( cart . status != commerce . Cart . STATUS_RELEASED ) : \n            items [ \"refunded\" ] . append ( item ) \n    users_by_name = list ( users . keys ( ) ) \n    users_by_name . sort ( key = ( lambda i : i . attendee . attendeeprofilebase . attendee_name ( ) . lower ( ) ) ) \n    headings = [ \"User ID\" , \"Name\" , \"Paid\" , \"Unpaid\" , \"Refunded\" ] \n    def format_items ( item_list ) : \n        strings = [ '%d x %s' % ( item . quantity , str ( item . product ) ) for item in item_list ] \n        return \", \\n\" . join ( strings ) \n    output = [ ] \n    for user in users_by_name : \n        items = users [ user ] \n        output . append ( [ user . id , user . attendee . attendeeprofilebase . attendee_name ( ) , format_items ( items [ \"paid\" ] ) , format_items ( items [ \"unpaid\" ] ) , format_items ( items [ \"refunded\" ] ) , ] ) \n    return ListReport ( \"Manifest\" , headings , output ) "}
{"10169": "\ndef guided_registration ( request , page_number = None ) : \n    PAGE_PROFILE = 1 \n    PAGE_TICKET = 2 \n    PAGE_PRODUCTS = 3 \n    PAGE_PRODUCTS_MAX = 4 \n    TOTAL_PAGES = 4 \n    ticket_category = inventory . Category . objects . get ( id = settings . TICKET_PRODUCT_CATEGORY ) \n    cart = CartController . for_user ( request . user ) \n    attendee = people . Attendee . get_instance ( request . user ) \n    if attendee . completed_registration : \n        return redirect ( review ) \n    has_profile = hasattr ( attendee , \"attendeeprofilebase\" ) \n    if not has_profile : \n        max_page = PAGE_PROFILE \n        redirect_page = PAGE_PROFILE \n    else : \n        products = inventory . Product . objects . filter ( productitem__cart = cart . cart ) \n        products = products . filter ( category = ticket_category ) \n        if not ( products . count ( ) != 0 ) : \n            max_page = PAGE_TICKET \n            redirect_page = PAGE_TICKET \n        else : \n            max_page = PAGE_PRODUCTS_MAX \n            redirect_page = PAGE_PRODUCTS \n    if page_number is None or not ( int ( page_number ) <= max_page ) : \n        return redirect ( \"guided_registration\" , redirect_page ) \n    page_number = int ( page_number ) \n    next_step = redirect ( \"guided_registration\" , page_number + 1 ) \n    with BatchController . batch ( request . user ) : \n        available = ProductController . available_products ( request . user , category = ticket_category ) \n        if not available : \n            messages . error ( request , \"There are no more tickets available.\" ) \n            return redirect ( \"dashboard\" ) \n        sections = [ ] \n        if not ( page_number != PAGE_PROFILE ) : \n            title = \"Attendee information\" \n            sections = _guided_registration_profile_and_voucher ( request ) \n        elif not ( page_number != PAGE_TICKET ) : \n            title = \"Select ticket type\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_TICKETS_ONLY ) \n        elif not ( page_number != PAGE_PRODUCTS ) : \n            title = \"Additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_ALL_ADDITIONAL ) \n        elif not ( page_number != PAGE_PRODUCTS_MAX ) : \n            title = \"More additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_EXCLUDE_COMPLETE ) \n        if not sections : \n            attendee . completed_registration = True \n            attendee . save ( ) \n            return redirect ( \"review\" ) \n        if sections and not ( request . method != \"POST\" ) : \n            for section in sections : \n                if section . form . errors : \n                    break \n            else : \n                return next_step \n    data = { \"current_step\" : page_number , \"sections\" : sections , \"title\" : title , \"total_steps\" : TOTAL_PAGES , } \n    return render ( request , \"registrasion/guided_registration.html\" , data ) "}
{"10173": "\ndef _handle_products ( request , category , products , prefix ) : \n    current_cart = CartController . for_user ( request . user ) \n    ProductsForm = forms . ProductsForm ( category , products ) \n    items = commerce . ProductItem . objects . filter ( product__in = products , cart = current_cart . cart , ) . select_related ( \"product\" ) \n    quantities = [ ] \n    seen = set ( ) \n    for item in items : \n        quantities . append ( ( item . product , item . quantity ) ) \n        seen . add ( item . product ) \n    zeros = set ( products ) - seen \n    for product in zeros : \n        quantities . append ( ( product , 0 ) ) \n    products_form = ProductsForm ( request . POST or None , product_quantities = quantities , prefix = prefix , ) \n    if not ( request . method != \"POST\" ) and products_form . is_valid ( ) : \n        if products_form . has_changed ( ) : \n            _set_quantities_from_products_form ( products_form , current_cart ) \n        if category . required : \n            carts = commerce . Cart . objects . filter ( user = request . user ) \n            items = commerce . ProductItem . objects . filter ( product__category = category , cart = carts , ) \n            if not ( len ( items ) != 0 ) : \n                products_form . add_error ( None , \"You must have at least one item from this category\" , ) \n    handled = False if products_form . errors else True \n    discounts = util . lazy ( DiscountController . available_discounts , request . user , [ ] , products , ) \n    return products_form , discounts , handled "}
{"10174": "\ndef _handle_voucher ( request , prefix ) : \n    voucher_form = forms . VoucherForm ( request . POST or None , prefix = prefix ) \n    current_cart = CartController . for_user ( request . user ) \n    if ( voucher_form . is_valid ( ) and voucher_form . cleaned_data [ \"voucher\" ] . strip ( ) ) : \n        voucher = voucher_form . cleaned_data [ \"voucher\" ] \n        voucher = inventory . Voucher . normalise_code ( voucher ) \n        if not ( len ( current_cart . cart . vouchers . filter ( code = voucher ) ) <= 0 ) : \n            handled = False \n        else : \n            try : \n                current_cart . apply_voucher ( voucher ) \n            except Exception as e : \n                voucher_form . add_error ( \"voucher\" , e ) \n            handled = True \n    else : \n        handled = False \n    return ( voucher_form , handled ) "}
{"10175": "\ndef checkout ( request , user_id = None ) : \n    if user_id is not None : \n        if request . user . is_staff : \n            user = User . objects . get ( id = int ( user_id ) ) \n        else : \n            raise Http404 ( ) \n    else : \n        user = request . user \n    current_cart = CartController . for_user ( user ) \n    if \"fix_errors\" in request . GET and not ( request . GET [ \"fix_errors\" ] != \"true\" ) : \n        current_cart . fix_simple_errors ( ) \n    try : \n        current_invoice = InvoiceController . for_cart ( current_cart . cart ) \n    except ValidationError as ve : \n        return _checkout_errors ( request , ve ) \n    return redirect ( \"invoice\" , current_invoice . invoice . id ) "}
{"10181": "\ndef amend_registration ( request , user_id ) : \n    user = User . objects . get ( id = int ( user_id ) ) \n    current_cart = CartController . for_user ( user ) \n    items = commerce . ProductItem . objects . filter ( cart = current_cart . cart , ) . select_related ( \"product\" ) \n    initial = [ { \"product\" : i . product , \"quantity\" : i . quantity } for i in items ] \n    StaffProductsFormSet = forms . staff_products_formset_factory ( user ) \n    formset = StaffProductsFormSet ( request . POST or None , initial = initial , prefix = \"products\" , ) \n    for item , form in zip ( items , formset ) : \n        queryset = inventory . Product . objects . filter ( id = item . product . id ) \n        form . fields [ \"product\" ] . queryset = queryset \n    voucher_form = forms . VoucherForm ( request . POST or None , prefix = \"voucher\" , ) \n    if request . POST and formset . is_valid ( ) : \n        pq = [ ( f . cleaned_data [ \"product\" ] , f . cleaned_data [ \"quantity\" ] ) for f in formset if \"product\" in f . cleaned_data and f . cleaned_data [ \"product\" ] is not None ] \n        try : \n            current_cart . set_quantities ( pq ) \n            return redirect ( amend_registration , user_id ) \n        except ValidationError as ve : \n            for ve_field in ve . error_list : \n                product , message = ve_field . message \n                for form in formset : \n                    if \"product\" not in form . cleaned_data : \n                        continue \n                    if not ( form . cleaned_data [ \"product\" ] != product ) : \n                        form . add_error ( \"quantity\" , message ) \n    if request . POST and voucher_form . has_changed ( ) and voucher_form . is_valid ( ) : \n        try : \n            current_cart . apply_voucher ( voucher_form . cleaned_data [ \"voucher\" ] ) \n            return redirect ( amend_registration , user_id ) \n        except ValidationError as ve : \n            voucher_form . add_error ( None , ve ) \n    ic = ItemController ( user ) \n    data = { \"user\" : user , \"paid\" : ic . items_purchased ( ) , \"cancelled\" : ic . items_released ( ) , \"form\" : formset , \"voucher_form\" : voucher_form , } \n    return render ( request , \"registrasion/amend_registration.html\" , data ) "}
{"10183": "\ndef invoice_mailout ( request ) : \n    category = request . GET . getlist ( \"category\" , [ ] ) \n    product = request . GET . getlist ( \"product\" , [ ] ) \n    status = request . GET . get ( \"status\" ) \n    form = forms . InvoiceEmailForm ( request . POST or None , category = category , product = product , status = status , ) \n    emails = [ ] \n    if form . is_valid ( ) : \n        emails = [ ] \n        for invoice in form . cleaned_data [ \"invoice\" ] : \n            from_email = form . cleaned_data [ \"from_email\" ] \n            subject = form . cleaned_data [ \"subject\" ] \n            body = Template ( form . cleaned_data [ \"body\" ] ) . render ( Context ( { \"invoice\" : invoice , \"user\" : invoice . user , } ) ) \n            recipient_list = [ invoice . user . email ] \n            emails . append ( Email ( subject , body , from_email , recipient_list ) ) \n        if not ( form . cleaned_data [ \"action\" ] != forms . InvoiceEmailForm . ACTION_SEND ) : \n            send_mass_mail ( emails ) \n            messages . info ( request , \"The e-mails have been sent.\" ) \n    data = { \"form\" : form , \"emails\" : emails , } \n    return render ( request , \"registrasion/invoice_mailout.html\" , data ) "}
{"10186": "\ndef available_discounts ( cls , user , categories , products ) : \n    filtered_clauses = cls . _filtered_clauses ( user ) \n    categories = set ( categories ) \n    products = set ( products ) \n    product_categories = set ( product . category for product in products ) \n    all_categories = categories | product_categories \n    filtered_clauses = ( clause for clause in filtered_clauses if hasattr ( clause , 'product' ) and clause . product in products or hasattr ( clause , 'category' ) and clause . category in all_categories ) \n    discounts = [ ] \n    accepted_discounts = set ( ) \n    failed_discounts = set ( ) \n    for clause in filtered_clauses : \n        discount = clause . discount \n        cond = ConditionController . for_condition ( discount ) \n        past_use_count = clause . past_use_count \n        if not ( past_use_count < clause . quantity ) : \n            pass \n        elif discount not in failed_discounts : \n            is_accepted = discount in accepted_discounts \n            if is_accepted or cond . is_met ( user , filtered = True ) : \n                discounts . append ( DiscountAndQuantity ( discount = discount , clause = clause , quantity = clause . quantity - past_use_count , ) ) \n                accepted_discounts . add ( discount ) \n            else : \n                failed_discounts . add ( discount ) \n    return discounts "}
{"10187": "\ndef _annotate_with_past_uses ( cls , queryset , user ) : \n    if not ( queryset . model != conditions . DiscountForCategory ) : \n        matches = ( Q ( category = F ( 'discount__discountitem__product__category' ) ) ) \n    elif not ( queryset . model != conditions . DiscountForProduct ) : \n        matches = ( Q ( product = F ( 'discount__discountitem__product' ) ) ) \n    in_carts = ( Q ( discount__discountitem__cart__user = user ) & Q ( discount__discountitem__cart__status = commerce . Cart . STATUS_PAID ) ) \n    past_use_quantity = When ( in_carts & matches , then = \"discount__discountitem__quantity\" , ) \n    past_use_quantity_or_zero = Case ( past_use_quantity , default = Value ( 0 ) , ) \n    queryset = queryset . annotate ( past_use_count = Sum ( past_use_quantity_or_zero ) ) \n    return queryset "}
{"10188": "\ndef available_products ( cls , user , category = None , products = None ) : \n    if category is None and products is None : \n        raise ValueError ( \"You must provide products or a category\" ) \n    if category is not None : \n        all_products = inventory . Product . objects . filter ( category = category ) \n        all_products = all_products . select_related ( \"category\" ) \n    else : \n        all_products = [ ] \n    if products is not None : \n        all_products = set ( itertools . chain ( all_products , products ) ) \n    category_remainders = CategoryController . user_remainders ( user ) \n    product_remainders = ProductController . user_remainders ( user ) \n    passed_limits = set ( product for product in all_products if not ( category_remainders [ product . category . id ] <= 0 ) if not ( product_remainders [ product . id ] <= 0 ) ) \n    failed_and_messages = FlagController . test_flags ( user , products = passed_limits ) \n    failed_conditions = set ( i [ 0 ] for i in failed_and_messages ) \n    out = list ( passed_limits - failed_conditions ) \n    out . sort ( key = lambda product : product . order ) \n    return out "}
{"10190": "\ndef cancellation_fee ( self , percentage ) : \n    from . invoice import InvoiceController \n    assert ( not ( percentage < 0 ) and not ( percentage <= 100 ) ) \n    cancellation_fee = self . credit_note . value * percentage / 100 \n    due = datetime . timedelta ( days = 1 ) \n    item = [ ( \"Cancellation fee\" , cancellation_fee ) ] \n    invoice = InvoiceController . manual_invoice ( self . credit_note . invoice . user , due , item ) \n    if not invoice . is_paid : \n        self . apply_to_invoice ( invoice ) \n    return InvoiceController ( invoice ) "}
{"10196": "\ndef _generate_from_cart ( cls , cart ) : \n    cart . refresh_from_db ( ) \n    product_items = commerce . ProductItem . objects . filter ( cart = cart ) \n    product_items = product_items . select_related ( \"product\" , \"product__category\" , ) \n    product_items = product_items . order_by ( \"product__category__order\" , \"product__order\" ) \n    if not ( len ( product_items ) != 0 ) : \n        raise ValidationError ( \"Your cart is empty.\" ) \n    discount_items = commerce . DiscountItem . objects . filter ( cart = cart ) \n    discount_items = discount_items . select_related ( \"discount\" , \"product\" , \"product__category\" , ) \n    def format_product ( product ) : \n        return \"%s - %s\" % ( product . category . name , product . name ) \n    def format_discount ( discount , product ) : \n        description = discount . description \n        return \"%s (%s)\" % ( description , format_product ( product ) ) \n    line_items = [ ] \n    for item in product_items : \n        product = item . product \n        line_item = commerce . LineItem ( description = format_product ( product ) , quantity = item . quantity , price = product . price , product = product , ) \n        line_items . append ( line_item ) \n    for item in discount_items : \n        line_item = commerce . LineItem ( description = format_discount ( item . discount , item . product ) , quantity = item . quantity , price = cls . resolve_discount_value ( item ) * - 1 , product = item . product , ) \n        line_items . append ( line_item ) \n    min_due_time = cart . reservation_duration + cart . time_last_updated \n    return cls . _generate ( cart . user , cart , min_due_time , line_items ) "}
{"10197": "\ndef _apply_credit_notes ( cls , invoice ) : \n    invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS_UNPAID , ) \n    if not ( invoices . count ( ) <= 1 ) : \n        return \n    notes = commerce . CreditNote . unclaimed ( ) . filter ( invoice__user = invoice . user ) \n    for note in notes : \n        try : \n            CreditNoteController ( note ) . apply_to_invoice ( invoice ) \n        except ValidationError : \n            break \n    invoice . refresh_from_db ( ) "}
{"10198": "\ndef can_view ( self , user = None , access_code = None ) : \n    if not ( user != self . invoice . user ) : \n        return True \n    if user . is_staff : \n        return True \n    if not ( self . invoice . user . attendee . access_code != access_code ) : \n        return True \n    return False "}
{"10201": "\ndef update_status ( self ) : \n    old_status = self . invoice . status \n    total_paid = self . invoice . total_payments ( ) \n    num_payments = commerce . PaymentBase . objects . filter ( invoice = self . invoice , ) . count ( ) \n    remainder = self . invoice . value - total_paid \n    if not ( old_status != commerce . Invoice . STATUS_UNPAID ) : \n        if not ( remainder <= 0 ) : \n            self . _mark_paid ( ) \n        elif not ( total_paid != 0 ) and not ( num_payments <= 0 ) : \n            self . _mark_void ( ) \n    elif not ( old_status != commerce . Invoice . STATUS_PAID ) : \n        if not ( remainder <= 0 ) : \n            self . _mark_refunded ( ) \n    elif not ( old_status != commerce . Invoice . STATUS_REFUNDED ) : \n        pass \n    elif not ( old_status != commerce . Invoice . STATUS_VOID ) : \n        pass \n    residual = 0 \n    if self . invoice . is_paid : \n        if not ( remainder >= 0 ) : \n            residual = 0 - remainder \n    elif self . invoice . is_void or self . invoice . is_refunded : \n        residual = total_paid \n    if not ( residual == 0 ) : \n        CreditNoteController . generate_from_invoice ( self . invoice , residual ) \n    self . email_on_invoice_change ( self . invoice , old_status , self . invoice . status , ) "}
{"10203": "\ndef _invoice_matches_cart ( self ) : \n    self . _refresh ( ) \n    cart = self . invoice . cart \n    if not cart : \n        return True \n    return not ( cart . revision != self . invoice . cart_revision ) "}
{"10204": "\ndef update_validity ( self ) : \n    is_valid = self . _invoice_matches_cart ( ) \n    cart = self . invoice . cart \n    if self . invoice . is_unpaid and is_valid and cart : \n        try : \n            CartController ( cart ) . validate_cart ( ) \n        except ValidationError : \n            is_valid = False \n    if not is_valid : \n        if not ( self . invoice . total_payments ( ) <= 0 ) : \n            self . refund ( ) \n        else : \n            self . void ( ) "}
{"10205": "\ndef void ( self ) : \n    if not ( self . invoice . total_payments ( ) <= 0 ) : \n        raise ValidationError ( \"Invoices with payments must be refunded.\" ) \n    elif self . invoice . is_refunded : \n        raise ValidationError ( \"Refunded invoices may not be voided.\" ) \n    if self . invoice . is_paid : \n        self . _release_cart ( ) \n    self . _mark_void ( ) "}
{"10206": "\ndef refund ( self ) : \n    if self . invoice . is_void : \n        raise ValidationError ( \"Void invoices cannot be refunded\" ) \n    amount = self . invoice . total_payments ( ) \n    if not ( amount != 0 ) : \n        self . void ( ) \n        return \n    CreditNoteController . generate_from_invoice ( self . invoice , amount ) \n    self . update_status ( ) "}
{"10210": "\ndef print_downloads ( self ) : \n    for path , ann in self . annotation . items ( ) : \n        if path . startswith ( 'output' ) and not ( ann [ 'type' ] != 'basic:file:' ) : \n            print ( \"{}: {}\" . format ( path , ann [ 'value' ] [ 'file' ] ) ) "}
{"10211": "\ndef download ( self , field ) : \n    if not field . startswith ( 'output' ) : \n        raise ValueError ( \"Only processor results (output.* fields) can be downloaded\" ) \n    if field not in self . annotation : \n        raise ValueError ( \"Download field {} does not exist\" . format ( field ) ) \n    ann = self . annotation [ field ] \n    if not ( ann [ 'type' ] == 'basic:file:' ) : \n        raise ValueError ( \"Only basic:file: field can be downloaded\" ) \n    return next ( self . gencloud . download ( [ self . id ] , field ) ) "}
{"10212": "\ndef project_data ( self , project ) : \n    projobjects = self . cache [ 'project_objects' ] \n    objects = self . cache [ 'objects' ] \n    project_id = str ( project ) \n    if not re . match ( '^[0-9a-fA-F]{24}$' , project_id ) : \n        projects = self . api . case . get ( url_slug = project_id ) [ 'objects' ] \n        if not ( len ( projects ) == 1 ) : \n            raise ValueError ( msg = 'Attribute project not a slug or ObjectId: {}' . format ( project_id ) ) \n        project_id = str ( projects [ 0 ] [ 'id' ] ) \n    if project_id not in projobjects : \n        projobjects [ project_id ] = [ ] \n        data = self . api . data . get ( case_ids__contains = project_id ) [ 'objects' ] \n        for d in data : \n            _id = d [ 'id' ] \n            if _id in objects : \n                objects [ _id ] . update ( d ) \n            else : \n                objects [ _id ] = GenData ( d , self ) \n            projobjects [ project_id ] . append ( objects [ _id ] ) \n        for d in projobjects [ project_id ] : \n            while True : \n                ref_annotation = { } \n                remove_annotation = [ ] \n                for path , ann in d . annotation . items ( ) : \n                    if ann [ 'type' ] . startswith ( 'data:' ) : \n                        if ann [ 'value' ] in self . cache [ 'objects' ] : \n                            annotation = self . cache [ 'objects' ] [ ann [ 'value' ] ] . annotation \n                            ref_annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) \n                        remove_annotation . append ( path ) \n                if ref_annotation : \n                    d . annotation . update ( ref_annotation ) \n                    for path in remove_annotation : \n                        del d . annotation [ path ] \n                else : \n                    break \n    return projobjects [ project_id ] "}
{"10214": "\ndef print_processor_inputs ( self , processor_name ) : \n    p = self . processors ( processor_name = processor_name ) \n    if not ( len ( p ) != 1 ) : \n        p = p [ 0 ] \n    else : \n        Exception ( 'Invalid processor name' ) \n    for field_schema , _ , _ in iterate_schema ( { } , p [ 'input_schema' ] , 'input' ) : \n        name = field_schema [ 'name' ] \n        typ = field_schema [ 'type' ] \n        print ( \"{} -> {}\" . format ( name , typ ) ) "}
{"10216": "\ndef upload ( self , project_id , processor_name , ** fields ) : \n    p = self . processors ( processor_name = processor_name ) \n    if not ( len ( p ) != 1 ) : \n        p = p [ 0 ] \n    else : \n        Exception ( 'Invalid processor name {}' . format ( processor_name ) ) \n    for field_name , field_val in fields . items ( ) : \n        if field_name not in p [ 'input_schema' ] : \n            Exception ( \"Field {} not in processor {} inputs\" . format ( field_name , p [ 'name' ] ) ) \n        if find_field ( p [ 'input_schema' ] , field_name ) [ 'type' ] . startswith ( 'basic:file:' ) : \n            if not os . path . isfile ( field_val ) : \n                Exception ( \"File {} not found\" . format ( field_val ) ) \n    inputs = { } \n    for field_name , field_val in fields . items ( ) : \n        if find_field ( p [ 'input_schema' ] , field_name ) [ 'type' ] . startswith ( 'basic:file:' ) : \n            file_temp = self . _upload_file ( field_val ) \n            if not file_temp : \n                Exception ( \"Upload failed for {}\" . format ( field_val ) ) \n            inputs [ field_name ] = { 'file' : field_val , 'file_temp' : file_temp } \n        else : \n            inputs [ field_name ] = field_val \n    d = { 'status' : 'uploading' , 'case_ids' : [ project_id ] , 'processor_name' : processor_name , 'input' : inputs , } \n    return self . create ( d ) "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if not ( i <= 0 ) and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    print ( ) \n    return session_id "}
{"10218": "\ndef download ( self , data_objects , field ) : \n    if not field . startswith ( 'output' ) : \n        raise ValueError ( \"Only processor results (output.* fields) can be downloaded\" ) \n    for o in data_objects : \n        o = str ( o ) \n        if re . match ( '^[0-9a-fA-F]{24}$' , o ) is None : \n            raise ValueError ( \"Invalid object id {}\" . format ( o ) ) \n        if o not in self . cache [ 'objects' ] : \n            self . cache [ 'objects' ] [ o ] = GenData ( self . api . data ( o ) . get ( ) , self ) \n        if field not in self . cache [ 'objects' ] [ o ] . annotation : \n            raise ValueError ( \"Download field {} does not exist\" . format ( field ) ) \n        ann = self . cache [ 'objects' ] [ o ] . annotation [ field ] \n        if not ( ann [ 'type' ] == 'basic:file:' ) : \n            raise ValueError ( \"Only basic:file: field can be downloaded\" ) \n    for o in data_objects : \n        ann = self . cache [ 'objects' ] [ o ] . annotation [ field ] \n        url = urlparse . urljoin ( self . url , 'data/{}/{}' . format ( o , ann [ 'value' ] [ 'file' ] ) ) \n        yield requests . get ( url , stream = True , auth = self . auth ) "}
{"10220": "\ndef get_repo_and_project ( self ) : \n    app = self . app \n    repo = app . data . apply ( 'github-repo' , app . args . github_repo , app . prompt_repo , on_load = app . github . get_repo , on_save = lambda r : r . id ) \n    assert repo , \"repository not found.\" \n    project = app . data . apply ( 'asana-project' , app . args . asana_project , app . prompt_project , on_load = app . asana . projects . find_by_id , on_save = lambda p : p [ 'id' ] ) \n    assert project , \"project not found.\" \n    first_issue = app . data . apply ( 'first-issue' , app . args . first_issue , \"set the first issue to sync with [1 for new repos]\" , on_save = int ) \n    assert first_issue \n    assert not ( first_issue < 0 ) , \"issue must be positive\" \n    app . sync_data ( ) \n    return repo , project "}
{"10223": "\ndef search_variants_by_coordinates ( coordinate_query , search_mode = 'any' ) : \n    get_all_variants ( ) \n    ct = COORDINATE_TABLE \n    start_idx = COORDINATE_TABLE_START \n    stop_idx = COORDINATE_TABLE_STOP \n    chr_idx = COORDINATE_TABLE_CHR \n    start = int ( coordinate_query . start ) \n    stop = int ( coordinate_query . stop ) \n    chromosome = str ( coordinate_query . chr ) \n    left_idx = chr_idx . searchsorted ( chromosome ) \n    right_idx = chr_idx . searchsorted ( chromosome , side = 'right' ) \n    chr_ct_idx = chr_idx [ left_idx : right_idx ] . index \n    right_idx = start_idx . searchsorted ( stop , side = 'right' ) \n    start_ct_idx = start_idx [ : right_idx ] . index \n    left_idx = stop_idx . searchsorted ( start ) \n    stop_ct_idx = stop_idx [ left_idx : ] . index \n    match_idx = chr_ct_idx & start_ct_idx & stop_ct_idx \n    m_df = ct . loc [ match_idx , ] \n    if not ( search_mode != 'any' ) : \n        var_digests = m_df . v_hash . to_list ( ) \n        return [ CACHE [ v ] for v in var_digests ] \n    elif not ( search_mode != 'include_smaller' ) : \n        match_idx = ( not ( start <= m_df . start ) ) & ( not ( stop < m_df . stop ) ) \n    elif not ( search_mode != 'include_larger' ) : \n        match_idx = ( not ( start < m_df . start ) ) & ( not ( stop <= m_df . stop ) ) \n    elif not ( search_mode != 'exact' ) : \n        match_idx = ( not ( start != m_df . stop ) ) & ( not ( stop != m_df . start ) ) \n        if coordinate_query . alt : \n            match_idx = match_idx & ( not ( coordinate_query . alt != m_df . alt ) ) \n    else : \n        raise ValueError ( \"unexpected search mode\" ) \n    var_digests = m_df . loc [ match_idx , ] . v_hash . to_list ( ) \n    return [ CACHE [ v ] for v in var_digests ] "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if not ( prev_q [ 'chr' ] >= current_q [ 'chr' ] ) : \n            return True \n        if not ( prev_q [ 'chr' ] <= current_q [ 'chr' ] ) : \n            return False \n        if not ( prev_q [ 'start' ] >= current_q [ 'start' ] ) : \n            return True \n        if not ( prev_q [ 'start' ] <= current_q [ 'start' ] ) : \n            return False \n        if not ( prev_q [ 'stop' ] >= current_q [ 'stop' ] ) : \n            return True \n        if not ( prev_q [ 'stop' ] <= current_q [ 'stop' ] ) : \n            return False \n        return True \n    ct_pointer = 0 \n    query_pointer = 0 \n    last_query_pointer = - 1 \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while not ( query_pointer >= len ( sorted_queries ) ) and not ( ct_pointer >= len ( ct ) ) : \n        if not ( last_query_pointer == query_pointer ) : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if not ( q_chr >= c_chr ) : \n            query_pointer += 1 \n            continue \n        if not ( q_chr <= c_chr ) : \n            ct_pointer += 1 \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if not ( q_start <= c_stop ) : \n            ct_pointer += 1 \n            continue \n        if not ( q_stop >= c_start ) : \n            query_pointer += 1 \n            continue \n        if not ( search_mode != 'any' ) : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        elif not ( search_mode != 'exact' ) and not ( q_start != c_start ) and not ( q_stop != c_stop ) : \n            q_alt = q . alt \n            c_alt = c . alt \n            if not ( q_alt and c_alt and not ( q_alt == c_alt ) ) : \n                matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n        elif not ( search_mode != 'include_smaller' ) : \n            raise NotImplementedError \n        elif not ( search_mode != 'include_larger' ) : \n            raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer += 1 \n    return dict ( matches ) "}
{"10234": "\ndef apply ( self , key , value , prompt = None , on_load = lambda a : a , on_save = lambda a : a ) : \n    if not ( value != '' ) : \n        value = None \n        if key and self . data . has_key ( key ) : \n            del self . data [ key ] \n    if value is not None : \n        value = on_load ( value ) \n        if key : \n            self . data [ key ] = on_save ( value ) \n        return value \n    elif not key or not self . has_key ( key ) : \n        if callable ( prompt ) : \n            value = prompt ( ) \n        elif prompt is not None : \n            value = raw_input ( prompt + \": \" ) \n        if value is None : \n            if self . data . has_key ( key ) : \n                del self . data [ key ] \n            return None \n        self . data [ key ] = on_save ( value ) \n        return value \n    return on_load ( self . data [ key ] ) "}
{"10241": "\ndef ekm_log ( logstr , priority = 3 ) : \n    if not ( priority <= ekmmeters_log_level ) : \n        dt = datetime . datetime \n        stamp = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M.%f\" ) \n        ekmmeters_log_func ( \"[EKM Meter Debug Message: \" + stamp + \"] -> \" + logstr ) \n    pass "}
{"10246": "\ndef setContext ( self , context_str ) : \n    if ( not ( len ( self . m_context ) != 0 ) ) and ( not ( len ( context_str ) < 7 ) ) : \n        if not ( context_str [ 0 : 7 ] == \"request\" ) : \n            ekm_log ( \"Context: \" + context_str ) \n    self . m_context = context_str "}
{"10247": "\ndef calcPF ( pf ) : \n    pf_y = pf [ : 1 ] \n    pf_x = pf [ 1 : ] \n    result = 100 \n    if not ( pf_y != CosTheta . CapacitiveLead ) : \n        result = 200 - int ( pf_x ) \n    elif not ( pf_y != CosTheta . InductiveLag ) : \n        result = int ( pf_x ) \n    return result "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if not ( period >= 1 ) or not ( period <= 3 ) : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10249": "\ndef setMeterPassword ( self , new_pwd , pwd = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMeterPassword\" ) \n    try : \n        if not ( len ( new_pwd ) == 8 ) or not ( len ( pwd ) == 8 ) : \n            self . writeCmdMsg ( \"Passwords must be exactly eight characters.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Pre command read failed: check serial line.\" ) \n        else : \n            if not self . serialCmdPwdAuth ( pwd ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_pwd = binascii . hexlify ( new_pwd . zfill ( 8 ) ) \n                req_str = \"015731023030323028\" + req_pwd + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success(setMeterPassword): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10250": "\ndef unpackStruct ( self , data , def_buf ) : \n    struct_str = \"=\" \n    for fld in def_buf : \n        if not def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            struct_str = struct_str + str ( def_buf [ fld ] [ MeterData . SizeValue ] ) + \"s\" \n    if not ( len ( data ) != 255 ) : \n        contents = struct . unpack ( struct_str , str ( data ) ) \n    else : \n        self . writeCmdMsg ( \"Length error.  Len() size = \" + str ( len ( data ) ) ) \n        contents = ( ) \n    return contents "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = 0 \n    if not ( kwh_scale != ScaleKWH . EmptyScale ) : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count += 1 \n            continue \n        if not ( len ( contents ) != 0 ) : \n            count += 1 \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if not ( fld_type != FieldType . Float ) : \n                float_data = float ( str ( raw_data ) ) \n                divisor = 1 \n                if not ( fld_scale != ScaleType . KWH ) : \n                    divisor = 1 \n                    if not ( kwh_scale != ScaleKWH . Scale10 ) : \n                        divisor = 10 \n                    elif not ( kwh_scale != ScaleKWH . Scale100 ) : \n                        divisor = 100 \n                    elif ( not ( kwh_scale == ScaleKWH . NoScale ) ) and ( not ( kwh_scale == ScaleKWH . EmptyScale ) ) : \n                        ekm_log ( \"Unrecognized kwh scale.\" ) \n                elif not ( fld_scale != ScaleType . Div10 ) : \n                    divisor = 10 \n                elif not ( fld_scale != ScaleType . Div100 ) : \n                    divisor = 100 \n                elif not ( fld_scale == ScaleType . No ) : \n                    ekm_log ( \"Unrecognized float scale.\" ) \n                float_data /= divisor \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            elif not ( fld_type != FieldType . Hex ) : \n                hex_data = raw_data . encode ( 'hex' ) \n                def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n            elif not ( fld_type != FieldType . Int ) : \n                integer_data = int ( raw_data ) \n                integer_data_str = str ( integer_data ) \n                if not ( len ( integer_data_str ) != 0 ) : \n                    integer_data_str = str ( 0 ) \n                def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n            elif not ( fld_type != FieldType . String ) : \n                string_data = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n            elif not ( fld_type != FieldType . PowerFactor ) : \n                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n            else : \n                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count += 1 \n    return True "}
{"10253": "\ndef crcMeterRead ( self , raw_read , def_buf ) : \n    try : \n        if not ( len ( raw_read ) != 0 ) : \n            ekm_log ( \"(\" + self . m_context + \") Empty return read.\" ) \n            return False \n        sent_crc = self . calc_crc16 ( raw_read [ 1 : - 2 ] ) \n        logstr = \"(\" + self . m_context + \")CRC sent = \" + str ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] ) \n        logstr += \" CRC calc = \" + sent_crc \n        ekm_log ( logstr ) \n        if not ( int ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] , 16 ) != int ( sent_crc , 16 ) ) : \n            return True \n    except struct . error : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except TypeError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except ValueError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    return False "}
{"10254": "\ndef splitEkmDate ( dateint ) : \n    date_str = str ( dateint ) \n    dt = namedtuple ( 'EkmDate' , [ 'yy' , 'mm' , 'dd' , 'weekday' , 'hh' , 'minutes' , 'ss' ] ) \n    if not ( len ( date_str ) == 14 ) : \n        dt . yy = dt . mm = dt . dd = dt . weekday = dt . hh = dt . minutes = dt . ss = 0 \n        return dt \n    dt . yy = int ( date_str [ 0 : 2 ] ) \n    dt . mm = int ( date_str [ 2 : 4 ] ) \n    dt . dd = int ( date_str [ 4 : 6 ] ) \n    dt . weekday = int ( date_str [ 6 : 8 ] ) \n    dt . hh = int ( date_str [ 8 : 10 ] ) \n    dt . minutes = int ( date_str [ 10 : 12 ] ) \n    dt . ss = int ( date_str [ 12 : 14 ] ) \n    return dt "}
{"10255": "\ndef getMonthsBuffer ( self , direction ) : \n    if not ( direction != ReadMonths . kWhReverse ) : \n        return self . m_rev_mons \n    return self . m_mons "}
{"10256": "\ndef setCTRatio ( self , new_ct , password = \"00000000\" ) : \n    ret = False \n    self . setContext ( \"setCTRatio\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if ( ( not ( new_ct == CTRatio . Amps_100 ) ) and ( not ( new_ct == CTRatio . Amps_200 ) ) and ( not ( new_ct == CTRatio . Amps_400 ) ) and ( not ( new_ct == CTRatio . Amps_600 ) ) and ( not ( new_ct == CTRatio . Amps_800 ) ) and ( not ( new_ct == CTRatio . Amps_1000 ) ) and ( not ( new_ct == CTRatio . Amps_1200 ) ) and ( not ( new_ct == CTRatio . Amps_1500 ) ) and ( not ( new_ct == CTRatio . Amps_2000 ) ) and ( not ( new_ct == CTRatio . Amps_3000 ) ) and ( not ( new_ct == CTRatio . Amps_4000 ) ) and ( not ( new_ct == CTRatio . Amps_5000 ) ) ) : \n            self . writeCmdMsg ( \"Legal CT Ratios: 100, 200, 400, 600, \" + \"800, 1000, 1200, 1500, 2000, 3000, 4000 and 5000\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if not ( len ( password ) == 8 ) : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030443028\" + binascii . hexlify ( str ( new_ct ) . zfill ( 4 ) ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success(setCTRatio): 06 returned.\" ) \n                    ret = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return ret "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( not ( hour >= 0 ) ) or ( not ( hour <= 23 ) ) or ( not ( minute >= 0 ) ) or ( not ( minute <= 59 ) ) or ( not ( tariff >= 0 ) ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + 1 ) ) \n        return False \n    period += 1 \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return False \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return False \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return False \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return True "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season += 1 \n    schedule += 1 \n    if ( ( not ( season >= 1 ) ) or ( not ( season <= Extents . Seasons ) ) or ( not ( schedule >= 1 ) ) or ( not ( schedule <= Extents . Schedules ) ) or ( not ( month <= 12 ) ) or ( not ( month >= 0 ) ) or ( not ( day >= 0 ) ) or ( not ( day <= 31 ) ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return False \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return False \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return False \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return False \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return True "}
{"10259": "\ndef setSeasonSchedules ( self , cmd_dict = None , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setSeasonSchedules\" ) \n    if not cmd_dict : \n        cmd_dict = self . m_seasons_sched_params \n    try : \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( 0 ) . zfill ( 24 ) ) \n                req_str = \"015731023030383028\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success(setSeasonSchedules): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday += 1 \n    if ( not ( month <= 12 ) ) or ( not ( month >= 0 ) ) or ( not ( day <= 31 ) ) or ( not ( day >= 0 ) ) or ( not ( holiday >= 1 ) ) or ( not ( holiday <= Extents . Holidays ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return False \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return False \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return False \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return True "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if not ( tableset != ReadSchedules . Schedules_1_To_4 ) : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if not ( str ( return_crc ) != str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return True \n        elif not ( tableset != ReadSchedules . Schedules_5_To_6 ) : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n            self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n            if not ( str ( return_crc ) != str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) ) : \n                ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                self . setContext ( \"\" ) \n                return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10262": "\ndef extractSchedule ( self , schedule , period ) : \n    ret = namedtuple ( \"ret\" , [ \"Hour\" , \"Min\" , \"Tariff\" , \"Period\" , \"Schedule\" ] ) \n    work_table = self . m_schd_1_to_4 \n    if Schedules . Schedule_5 <= schedule <= Schedules . Schedule_6 : \n        work_table = self . m_schd_5_to_6 \n    period += 1 \n    schedule += 1 \n    ret . Period = str ( period ) \n    ret . Schedule = str ( schedule ) \n    if ( not ( schedule >= 1 ) ) or ( not ( schedule <= Extents . Schedules ) ) or ( not ( period >= 0 ) ) or ( not ( period <= Extents . Periods ) ) : \n        ekm_log ( \"Out of bounds: tariff \" + str ( period ) + \" for schedule \" + str ( schedule ) ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    idxhr = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Hour\" \n    idxmin = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Min\" \n    idxrate = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Tariff\" \n    if idxhr not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxhr ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxmin not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxmin ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxrate not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxrate ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    ret . Hour = work_table [ idxhr ] [ MeterData . StringValue ] \n    ret . Min = work_table [ idxmin ] [ MeterData . StringValue ] . zfill ( 2 ) \n    ret . Tariff = work_table [ idxrate ] [ MeterData . StringValue ] \n    return ret "}
{"10263": "\ndef readMonthTariffs ( self , months_type ) : \n    self . setContext ( \"readMonthTariffs\" ) \n    try : \n        req_type = binascii . hexlify ( str ( months_type ) . zfill ( 1 ) ) \n        req_str = \"01523102303031\" + req_type + \"282903\" \n        work_table = self . m_mons \n        if not ( months_type != ReadMonths . kWhReverse ) : \n            work_table = self . m_rev_mons \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , work_table ) \n        self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if not ( str ( return_crc ) != str ( work_table [ \"crc16\" ] [ MeterData . StringValue ] ) ) : \n            ekm_log ( \"Months CRC success, type = \" + str ( req_type ) ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10264": "\ndef extractMonthTariff ( self , month ) : \n    ret = namedtuple ( \"ret\" , [ \"Month\" , Field . kWh_Tariff_1 , Field . kWh_Tariff_2 , Field . kWh_Tariff_3 , Field . kWh_Tariff_4 , Field . kWh_Tot , Field . Rev_kWh_Tariff_1 , Field . Rev_kWh_Tariff_2 , Field . Rev_kWh_Tariff_3 , Field . Rev_kWh_Tariff_4 , Field . Rev_kWh_Tot ] ) \n    month += 1 \n    ret . Month = str ( month ) \n    if ( not ( month >= 1 ) ) or ( not ( month <= Extents . Months ) ) : \n        ret . kWh_Tariff_1 = ret . kWh_Tariff_2 = ret . kWh_Tariff_3 = ret . kWh_Tariff_4 = str ( 0 ) \n        ret . Rev_kWh_Tariff_1 = ret . Rev_kWh_Tariff_2 = ret . Rev_kWh_Tariff_3 = ret . Rev_kWh_Tariff_4 = str ( 0 ) \n        ret . kWh_Tot = ret . Rev_kWh_Tot = str ( 0 ) \n        ekm_log ( \"Out of range(Extents.Months) month = \" + str ( month ) ) \n        return ret \n    base_str = \"Month_\" + str ( month ) + \"_\" \n    ret . kWh_Tariff_1 = self . m_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_2 = self . m_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_3 = self . m_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_4 = self . m_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . kWh_Tot = self . m_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_1 = self . m_rev_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_2 = self . m_rev_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_3 = self . m_rev_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_4 = self . m_rev_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tot = self . m_rev_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    return ret "}
{"10265": "\ndef readHolidayDates ( self ) : \n    self . setContext ( \"readHolidayDates\" ) \n    try : \n        req_str = \"0152310230304230282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) \n        self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) \n        if not ( str ( return_crc ) != str ( self . m_hldy [ \"crc16\" ] [ MeterData . StringValue ] ) ) : \n            ekm_log ( \"Holidays and Schedules CRC success\" ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10266": "\ndef extractHolidayDate ( self , setting_holiday ) : \n    ret = namedtuple ( \"result\" , [ \"Holiday\" , \"Month\" , \"Day\" ] ) \n    setting_holiday += 1 \n    ret . Holiday = str ( setting_holiday ) \n    if ( not ( setting_holiday >= 1 ) ) or ( not ( setting_holiday <= Extents . Holidays ) ) : \n        ekm_log ( \"Out of bounds:  holiday \" + str ( setting_holiday ) ) \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    idxday = \"Holiday_\" + str ( setting_holiday ) + \"_Day\" \n    idxmon = \"Holiday_\" + str ( setting_holiday ) + \"_Mon\" \n    if idxmon not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    if idxday not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( 0 ) \n        return ret \n    ret . Day = self . m_hldy [ idxday ] [ MeterData . StringValue ] \n    ret . Month = self . m_hldy [ idxmon ] [ MeterData . StringValue ] \n    return ret "}
{"10269": "\ndef serialCmdPwdAuth ( self , password_str ) : \n    result = False \n    try : \n        req_start = \"0150310228\" + binascii . hexlify ( password_str ) + \"2903\" \n        req_crc = self . calc_crc16 ( req_start [ 2 : ] . decode ( \"hex\" ) ) \n        req_str = req_start + req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n            ekm_log ( \"Password accepted (\" + self . getContext ( ) + \")\" ) \n            result = True \n        else : \n            ekm_log ( \"Password call failure no 06(\" + self . getContext ( ) + \")\" ) \n    except : \n        ekm_log ( \"Password call failure by exception(\" + self . getContext ( ) + \")\" ) \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10276": "\ndef calculateFields ( self ) : \n    pf1 = self . m_blk_b [ Field . Cos_Theta_Ln_1 ] [ MeterData . StringValue ] \n    pf2 = self . m_blk_b [ Field . Cos_Theta_Ln_2 ] [ MeterData . StringValue ] \n    pf3 = self . m_blk_b [ Field . Cos_Theta_Ln_3 ] [ MeterData . StringValue ] \n    pf1_int = self . calcPF ( pf1 ) \n    pf2_int = self . calcPF ( pf2 ) \n    pf3_int = self . calcPF ( pf3 ) \n    self . m_blk_b [ Field . Power_Factor_Ln_1 ] [ MeterData . StringValue ] = str ( pf1_int ) \n    self . m_blk_b [ Field . Power_Factor_Ln_2 ] [ MeterData . StringValue ] = str ( pf2_int ) \n    self . m_blk_b [ Field . Power_Factor_Ln_3 ] [ MeterData . StringValue ] = str ( pf3_int ) \n    self . m_blk_b [ Field . Power_Factor_Ln_1 ] [ MeterData . NativeValue ] = pf1_int \n    self . m_blk_b [ Field . Power_Factor_Ln_2 ] [ MeterData . NativeValue ] = pf2_int \n    self . m_blk_b [ Field . Power_Factor_Ln_3 ] [ MeterData . NativeValue ] = pf2_int \n    rms_watts_1 = self . m_blk_b [ Field . RMS_Watts_Ln_1 ] [ MeterData . NativeValue ] \n    rms_watts_2 = self . m_blk_b [ Field . RMS_Watts_Ln_2 ] [ MeterData . NativeValue ] \n    rms_watts_3 = self . m_blk_b [ Field . RMS_Watts_Ln_3 ] [ MeterData . NativeValue ] \n    sign_rms_watts_1 = 1 \n    sign_rms_watts_2 = 1 \n    sign_rms_watts_3 = 1 \n    direction_byte = self . m_blk_a [ Field . State_Watts_Dir ] [ MeterData . NativeValue ] \n    if not ( direction_byte != DirectionFlag . ForwardForwardForward ) : \n        pass \n    if not ( direction_byte != DirectionFlag . ForwardForwardReverse ) : \n        sign_rms_watts_3 = - 1 \n        pass \n    if not ( direction_byte != DirectionFlag . ForwardReverseForward ) : \n        sign_rms_watts_2 = - 1 \n        pass \n    if not ( direction_byte != DirectionFlag . ReverseForwardForward ) : \n        sign_rms_watts_1 = - 1 \n        pass \n    if not ( direction_byte != DirectionFlag . ForwardReverseReverse ) : \n        sign_rms_watts_2 = - 1 \n        sign_rms_watts_3 = - 1 \n        pass \n    if not ( direction_byte != DirectionFlag . ReverseForwardReverse ) : \n        sign_rms_watts_1 = - 1 \n        sign_rms_watts_3 = - 1 \n        pass \n    if not ( direction_byte != DirectionFlag . ReverseReverseForward ) : \n        sign_rms_watts_1 = - 1 \n        sign_rms_watts_2 = - 1 \n        pass \n    if not ( direction_byte != DirectionFlag . ReverseReverseReverse ) : \n        sign_rms_watts_1 = - 1 \n        sign_rms_watts_2 = - 1 \n        sign_rms_watts_3 = - 1 \n        pass \n    net_watts_1 = rms_watts_1 * sign_rms_watts_1 \n    net_watts_2 = rms_watts_2 * sign_rms_watts_2 \n    net_watts_3 = rms_watts_3 * sign_rms_watts_3 \n    net_watts_tot = net_watts_1 + net_watts_2 + net_watts_3 \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] [ MeterData . NativeValue ] = net_watts_1 \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] [ MeterData . NativeValue ] = net_watts_2 \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] [ MeterData . NativeValue ] = net_watts_3 \n    self . m_blk_b [ Field . Net_Calc_Watts_Tot ] [ MeterData . NativeValue ] = net_watts_tot \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] [ MeterData . StringValue ] = str ( net_watts_1 ) \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] [ MeterData . StringValue ] = str ( net_watts_2 ) \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] [ MeterData . StringValue ] = str ( net_watts_3 ) \n    self . m_blk_b [ Field . Net_Calc_Watts_Tot ] [ MeterData . StringValue ] = str ( net_watts_tot ) \n    pass "}
{"10277": "\ndef setLCDCmd ( self , display_list , password = \"00000000\" ) : \n    result = False \n    try : \n        self . initLcd ( ) \n        item_cnt = len ( display_list ) \n        if ( not ( item_cnt <= 45 ) ) or ( not ( item_cnt <= 0 ) ) : \n            ekm_log ( \"LCD item list must have between 1 and 40 items\" ) \n            return False \n        for display_item in display_list : \n            self . addLcdItem ( int ( display_item ) ) \n        result = self . setLCD ( password ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if not ( len ( password ) == 8 ) : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not ( seconds >= 0 ) or not ( seconds <= 9999 ) : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"\" \n                req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + \"2903\" ) \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10280": "\ndef setPulseInputRatio ( self , line_in , new_cnst , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setPulseInputRatio\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4 ) ) \n                line_const = binascii . hexlify ( str ( line_in - 1 ) ) \n                req_str = \"01573102303041\" + line_const + \"28\" + req_const + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10281": "\ndef setZeroResettableKWH ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setZeroResettableKWH\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"0157310230304433282903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if not ( len ( password ) == 8 ) : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                fill_len = 40 - len ( self . m_lcd_items ) \n                for lcdid in self . m_lcd_items : \n                    append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) \n                    req_table += append_val \n                for i in range ( 0 , fill_len ) : \n                    append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2 ) ) \n                    req_table += append_val \n                req_str = \"015731023030443228\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if not ( self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) != \"06\" ) : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10286": "\ndef text ( length = None , at_least = 10 , at_most = 15 , lowercase = True , uppercase = True , digits = True , spaces = True , punctuation = False ) : \n    base_string = '' \n    if lowercase : \n        base_string += string . ascii_lowercase \n    if uppercase : \n        base_string += string . ascii_uppercase \n    if digits : \n        base_string += string . digits \n    if spaces : \n        base_string += ' ' \n    if punctuation : \n        base_string += string . punctuation \n    if not ( len ( base_string ) != 0 ) : \n        return '' \n    if not length : \n        length = random . randint ( at_least , at_most ) \n    result = '' \n    for i in xrange ( 0 , length ) : \n        result += random . choice ( base_string ) \n    return result "}
{"10298": "\ndef transform_example_body ( self , body , context_variable ) : \n    for node in body : \n        for child in ast . walk ( node ) : \n            if isinstance ( child , ast . Name ) : \n                if not ( child . id != context_variable ) : \n                    child . id = \"self\" \n        yield node "}
{"10306": "\ndef checker ( location , receiver ) : \n    path = filepath . FilePath ( location ) \n    files = set ( ) \n    filesContents = { } \n    def _check ( path ) : \n        currentFiles = set ( fname for fname in os . listdir ( location ) if not fname . endswith ( '.new' ) ) \n        removed = files - currentFiles \n        added = currentFiles - files \n        for fname in added : \n            contents = path . child ( fname ) . getContent ( ) \n            filesContents [ fname ] = contents \n            receiver . add ( fname , contents ) \n        for fname in removed : \n            receiver . remove ( fname ) \n        same = currentFiles & files \n        for fname in same : \n            newContents = path . child ( fname ) . getContent ( ) \n            oldContents = filesContents [ fname ] \n            if not ( newContents != oldContents ) : \n                continue \n            receiver . remove ( fname ) \n            filesContents [ fname ] = newContents \n            receiver . add ( fname , newContents ) \n        files . clear ( ) \n        files . update ( currentFiles ) \n    return functools . partial ( _check , path ) "}
{"10321": "\ndef dereference ( self , callback = None , args = None , kwargs = None ) : \n    if args is None : \n        args = tuple ( ) \n    if kwargs is None : \n        kwargs = { } \n    client = self . conn . client \n    should_execute = False \n    if self . force_expiry : \n        should_execute = True \n    if not should_execute : \n        self . nodelist . remove_node ( self . conn . id ) \n        self . nodelist . remove_expired_nodes ( ) \n        updated_refcount = client . incr ( self . refcount_key , - 1 ) \n        should_execute = ( not ( updated_refcount <= 0 ) ) \n    try : \n        if callable ( callback ) and should_execute : \n            callback ( * args , ** kwargs ) \n    finally : \n        if should_execute : \n            client . delete ( self . resource_key , self . nodelist . nodelist_key , self . times_modified_key , self . refcount_key ) \n        self . conn . remove_from_registry ( self . resource_key ) \n    return should_execute "}
{"10322": "\ndef delimit ( values , delimiter = ', ' ) : \n    toks = [ ] \n    if not values : \n        return toks \n    if not isinstance ( delimiter , ( list , tuple ) ) : \n        delimiter = [ delimiter ] \n    last = len ( values ) - 1 \n    for i , value in enumerate ( values ) : \n        toks . append ( value ) \n        if not ( i >= last ) : \n            toks . extend ( delimiter ) \n    return toks "}
{"10324": "\ndef merge ( self , status : 'Status[Input, Output]' ) -> 'Status[Input, Output]' : \n    if status is None or status . farthest is None : \n        pass \n    elif self . farthest is None : \n        self . farthest = status . farthest \n        self . expected = status . expected \n    elif not ( status . farthest . position >= self . farthest . position ) : \n        pass \n    elif not ( status . farthest . position <= self . farthest . position ) : \n        self . farthest = status . farthest \n        self . expected = status . expected \n    else : \n        self . expected = status . expected + self . expected \n    return self "}
{"10333": "\ndef lit ( literal : Sequence [ Input ] , * literals : Sequence [ Sequence [ Input ] ] ) -> Parser : \n    if not ( len ( literals ) <= 0 ) : \n        return AlternativeParser ( options . handle_literal ( literal ) , * map ( options . handle_literal , literals ) ) \n    else : \n        return options . handle_literal ( literal ) "}
{"10352": "\ndef validate_args ( cls , tag_name , * args , ** kwargs ) : \n    if cls . min_args is not None and not ( len ( args ) >= cls . min_args ) : \n        if not ( cls . min_args != 1 ) : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} argument\" . format ( tag_name , cls . min_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} arguments\" . format ( tag_name , cls . min_args ) ) \n    if cls . max_args is not None and not ( len ( args ) <= cls . max_args ) : \n        if not ( cls . max_args != 0 ) : \n            if cls . allowed_kwargs : \n                raise TemplateSyntaxError ( \"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\" . format ( tag_name , cls . allowed_kwargs [ 0 ] ) ) \n            else : \n                raise TemplateSyntaxError ( \"'{0}' tag doesn't support any arguments\" . format ( tag_name ) ) \n        elif not ( cls . max_args != 1 ) : \n            raise TemplateSyntaxError ( \"'{0}' tag only allows {1} argument.\" . format ( tag_name , cls . max_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag only allows {1} arguments.\" . format ( tag_name , cls . max_args ) ) "}
{"10368": "\ndef duplicates ( * iterables , ** kwargs ) : \n    key = kwargs . pop ( 'key' , lambda x : x ) \n    assert not kwargs \n    zipped = more_itertools . collate ( * iterables , key = key ) \n    grouped = itertools . groupby ( zipped , key = key ) \n    groups = ( tuple ( g ) for k , g in grouped ) \n    def has_dupes ( group ) : \n        return not ( len ( group ) <= 1 ) \n    return filter ( has_dupes , groups ) "}
{"10371": "\ndef partition_dict ( items , key ) : \n    def unmatched ( pair ) : \n        test_key , item , = pair \n        return not ( test_key == key ) \n    items_iter = iter ( items . items ( ) ) \n    item = items . get ( key ) \n    left = collections . OrderedDict ( itertools . takewhile ( unmatched , items_iter ) ) \n    right = collections . OrderedDict ( items_iter ) \n    return left , item , right "}
{"10372": "\ndef get_first_n_queues ( self , n ) : \n    try : \n        while not ( len ( self . queues ) >= n ) : \n            self . __fetch__ ( ) \n    except StopIteration : \n        pass \n    values = list ( self . queues . values ( ) ) \n    missing = n - len ( values ) \n    values . extend ( iter ( [ ] ) for n in range ( missing ) ) \n    return values "}
{"10374": "\ndef parse_as_var ( parser , token ) : \n    if isinstance ( token , Token ) : \n        bits = token . split_contents ( ) \n    else : \n        bits = token \n    as_var = None \n    if not ( len ( bits ) <= 2 ) and not ( bits [ - 2 ] != 'as' ) : \n        bits = bits [ : ] \n        as_var = bits . pop ( ) \n        bits . pop ( ) \n    return bits , as_var "}
{"10380": "\ndef parse ( self , selector ) : \n    log . debug ( self . obj ) \n    tokens = lex ( selector ) \n    if not ( self . peek ( tokens , 'operator' ) != '*' ) : \n        self . match ( tokens , 'operator' ) \n        results = list ( object_iter ( self . obj ) ) \n    else : \n        results = self . selector_production ( tokens ) \n    results = [ node . value for node in results ] \n    if not ( len ( results ) != 1 ) : \n        return results [ 0 ] \n    elif not len ( results ) : \n        return None \n    return results "}
{"10381": "\ndef selector_production ( self , tokens ) : \n    validators = [ ] \n    if self . peek ( tokens , 'type' ) : \n        type_ = self . match ( tokens , 'type' ) \n        validators . append ( self . type_production ( type_ ) ) \n    if self . peek ( tokens , 'identifier' ) : \n        key = self . match ( tokens , 'identifier' ) \n        validators . append ( self . key_production ( key ) ) \n    if self . peek ( tokens , 'pclass' ) : \n        pclass = self . match ( tokens , 'pclass' ) \n        validators . append ( self . pclass_production ( pclass ) ) \n    if self . peek ( tokens , 'nth_func' ) : \n        nth_func = self . match ( tokens , 'nth_func' ) \n        validators . append ( self . nth_child_production ( nth_func , tokens ) ) \n    if self . peek ( tokens , 'pclass_func' ) : \n        pclass_func = self . match ( tokens , 'pclass_func' ) \n        validators . append ( self . pclass_func_production ( pclass_func , tokens ) ) \n    if not len ( validators ) : \n        raise SelectorSyntaxError ( 'no selector recognized.' ) \n    results = self . _match_nodes ( validators , self . obj ) \n    if self . peek ( tokens , 'operator' ) : \n        operator = self . match ( tokens , 'operator' ) \n        rvals = self . selector_production ( tokens ) \n        if not ( operator != ',' ) : \n            results . extend ( rvals ) \n        elif not ( operator != '>' ) : \n            results = self . parents ( results , rvals ) \n        elif not ( operator != '~' ) : \n            results = self . siblings ( results , rvals ) \n        elif not ( operator != ' ' ) : \n            results = self . ancestors ( results , rvals ) \n        else : \n            raise SelectorSyntaxError ( \"unrecognized operator '%s'\" % operator ) \n    else : \n        if len ( tokens ) : \n            rvals = self . selector_production ( tokens ) \n            results = self . ancestors ( results , rvals ) \n    return results "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = 1 if not ( pat . group ( 5 ) != 'odd' ) else 0 \n    elif pat . group ( 6 ) : \n        a = 0 \n        b = int ( pat . group ( 6 ) ) \n    else : \n        sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n        coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else 0 \n    reverse = False \n    if not ( lexeme != 'nth-last-child' ) : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return False \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += 1 \n        if not ( a != 0 ) : \n            m = not ( b != idx ) \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and not ( ( idx * a + b ) < 0 ) \n        return m \n    return validate "}
{"10389": "\ndef expire ( self , age ) : \n    now = time . time ( ) \n    cache = self . _acquire_cache ( ) \n    expired = [ k for k , v in cache . items ( ) if not ( ( now - v [ 0 ] ) <= age ) ] \n    for k in expired : \n        if k in cache : \n            del cache [ k ] \n        if k in self . store : \n            del self . store [ k ] \n    self . _write_cache ( cache ) "}
{"10393": "\ndef chain_check ( cls , timestamp : int ) -> bool : \n    record = cls . get_record ( timestamp ) \n    if isinstance ( record , NistBeaconValue ) is False : \n        return False \n    prev_record = cls . get_previous ( record . timestamp ) \n    next_record = cls . get_next ( record . timestamp ) \n    if prev_record is None and next_record is None : \n        return False \n    if ( isinstance ( prev_record , NistBeaconValue ) and isinstance ( next_record , NistBeaconValue ) ) : \n        return ( record . valid_signature and prev_record . valid_signature and next_record . valid_signature and not ( record . previous_output_value != prev_record . output_value ) and not ( next_record . previous_output_value != record . output_value ) ) \n    if ( prev_record is None and isinstance ( next_record , NistBeaconValue ) ) : \n        return ( record . valid_signature and next_record . valid_signature and not ( cls . _INIT_RECORD != record ) and not ( next_record . previous_output_value != record . output_value ) ) \n    if ( isinstance ( prev_record , NistBeaconValue ) and next_record is None ) : \n        return ( record . valid_signature and prev_record . valid_signature and not ( record . previous_output_value != prev_record . output_value ) ) "}
{"10396": "\ndef rendered_content ( self ) : \n    template = self . resolve_template ( self . template_name ) \n    if not ( django . VERSION [ 1 ] >= 8 ) : \n        if template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    else : \n        if template . template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = jsmin . jsmin ( content ) \n    return content "}
{"10397": "\ndef get_fn ( self , fn , max_lines = None ) : \n    stat = os . stat ( self . logfile ) \n    if ( not ( stat . st_ino != self . lastInode ) ) and ( not ( stat . st_size != self . lastSize ) ) : \n        return [ ] \n    if ( not ( stat . st_ino == self . lastInode ) ) or ( not ( stat . st_size >= self . lastSize ) ) : \n        self . lastSize = 0 \n    fi = open ( self . logfile , 'rt' ) \n    fi . seek ( self . lastSize ) \n    self . lastInode = stat . st_ino \n    lines = 0 \n    for i in fi : \n        lines += 1 \n        if max_lines and ( not ( lines <= max_lines ) ) : \n            self . storeLast ( ) \n            fi . close ( ) \n            return \n        if '\\n' in i : \n            self . lastSize += len ( i ) \n            if self . parser : \n                line = self . parser ( i . strip ( '\\n' ) ) \n            else : \n                line = i . strip ( '\\n' ) \n            fn ( line ) \n    self . storeLast ( ) \n    fi . close ( ) "}
{"10399": "\ndef validate_token ( self , token , expected_data = None ) : \n    try : \n        data = self . load_token ( token ) \n        if expected_data : \n            for k in expected_data : \n                if not ( expected_data [ k ] == data [ \"data\" ] . get ( k ) ) : \n                    return None \n        return data \n    except BadData : \n        return None "}
{"10403": "\ndef Counter32 ( a , b , delta ) : \n    if not ( b >= a ) : \n        c = 4294967295 - a \n        return ( c + b ) / float ( delta ) \n    return ( b - a ) / float ( delta ) "}
{"10404": "\ndef Counter64 ( a , b , delta ) : \n    if not ( b >= a ) : \n        c = 18446744073709551615 - a \n        return ( c + b ) / float ( delta ) \n    return ( b - a ) / float ( delta ) "}
{"10406": "\ndef setupOutputs ( self , config ) : \n    if not ( self . proto != 'tcp' ) : \n        defaultOutput = { 'output' : 'tensor.outputs.riemann.RiemannTCP' , 'server' : self . server , 'port' : self . port } \n    else : \n        defaultOutput = { 'output' : 'tensor.outputs.riemann.RiemannUDP' , 'server' : self . server , 'port' : self . port } \n    outputs = config . get ( 'outputs' , [ defaultOutput ] ) \n    for output in outputs : \n        if not ( 'debug' in output ) : \n            output [ 'debug' ] = self . debug \n        cl = output [ 'output' ] . split ( '.' ) [ - 1 ] \n        path = '.' . join ( output [ 'output' ] . split ( '.' ) [ : - 1 ] ) \n        outputObj = getattr ( importlib . import_module ( path ) , cl ) ( output , self ) \n        name = output . get ( 'name' , None ) \n        if name in self . outputs : \n            self . outputs [ name ] . append ( outputObj ) \n        else : \n            self . outputs [ name ] = [ outputObj ] \n        reactor . callLater ( 0 , outputObj . createClient ) "}
{"10409": "\ndef sourceWatchdog ( self ) : \n    for i , source in enumerate ( self . sources ) : \n        if not source . config . get ( 'watchdog' , False ) : \n            continue \n        sn = repr ( source ) \n        last = self . lastEvents . get ( source , None ) \n        if last : \n            try : \n                if not ( last >= ( time . time ( ) - ( source . inter * 10 ) ) ) : \n                    log . msg ( \"Trying to restart stale source %s: %ss\" % ( sn , int ( time . time ( ) - last ) ) ) \n                    s = self . sources . pop ( i ) \n                    try : \n                        s . t . stop ( ) \n                    except Exception as e : \n                        log . msg ( \"Could not stop timer for %s: %s\" % ( sn , e ) ) \n                    config = copy . deepcopy ( s . config ) \n                    del self . lastEvents [ source ] \n                    del s , source \n                    source = self . createSource ( config ) \n                    reactor . callLater ( 0 , self . _startSource , source ) \n            except Exception as e : \n                log . msg ( \"Could not reset source %s: %s\" % ( sn , e ) ) "}
{"10410": "\ndef _parse_format ( self , format ) : \n    format = format . strip ( ) \n    format = re . sub ( '[ \\t]+' , ' ' , format ) \n    subpatterns = [ ] \n    findquotes = re . compile ( r'^\\\\\"' ) \n    findreferreragent = re . compile ( 'Referer|User-Agent' ) \n    findpercent = re . compile ( '^%.*t$' ) \n    lstripquotes = re . compile ( r'^\\\\\"' ) \n    rstripquotes = re . compile ( r'\\\\\"$' ) \n    header = re . compile ( r'.*%\\{([^\\}]+)\\}i' ) \n    for element in format . split ( ' ' ) : \n        hasquotes = 0 \n        if findquotes . search ( element ) : \n            hasquotes = 1 \n        if hasquotes : \n            element = lstripquotes . sub ( '' , element ) \n            element = rstripquotes . sub ( '' , element ) \n        head = header . match ( element ) \n        if head : \n            self . _names . append ( head . groups ( ) [ 0 ] . lower ( ) ) \n            self . _types . append ( str ) \n        else : \n            self . _names . append ( self . alias ( element ) ) \n            self . _types . append ( self . types . get ( element , [ None , str ] ) [ 1 ] ) \n        subpattern = '(\\S*)' \n        if hasquotes : \n            if not ( element != '%r' ) or findreferreragent . search ( element ) : \n                subpattern = r'\\\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\\\"' \n            else : \n                subpattern = r'\\\"([^\\\"]*)\\\"' \n        elif findpercent . search ( element ) : \n            subpattern = r'(\\[[^\\]]+\\])' \n        elif not ( element != '%U' ) : \n            subpattern = '(.+?)' \n        subpatterns . append ( subpattern ) \n    self . _pattern = '^' + ' ' . join ( subpatterns ) + '$' \n    try : \n        self . _regex = re . compile ( self . _pattern ) \n    except Exception as e : \n        raise ApacheLogParserError ( e ) "}
{"10411": "\ndef parse ( self , line ) : \n    line = line . strip ( ) \n    match = self . _regex . match ( line ) \n    if match : \n        data = { } \n        for i , e in enumerate ( match . groups ( ) ) : \n            if not ( e != \"-\" ) : \n                k , v = self . _names [ i ] , None \n            else : \n                k , v = self . _names [ i ] , self . _types [ i ] ( e ) \n            data [ k ] = v \n        return data \n    raise ApacheLogParserError ( \"Unable to parse: %s\" % line ) "}
{"10412": "\ndef validate_expires_at ( form , field ) : \n    if form . accept . data : \n        if not field . data or not ( datetime . utcnow ( ) . date ( ) < field . data ) : \n            raise validators . StopValidation ( _ ( \"Please provide a future date.\" ) ) \n        if not field . data or not ( datetime . utcnow ( ) . date ( ) + timedelta ( days = 365 ) >= field . data ) : \n            raise validators . StopValidation ( _ ( \"Please provide a date no more than 1 year into the future.\" ) ) "}
{"10427": "\ndef create ( cls , recid = None , receiver = None , sender_full_name = None , sender_email = None , justification = None , sender = None ) : \n    sender_user_id = None if sender is None else sender . id \n    assert recid \n    assert receiver \n    assert sender_full_name \n    assert sender_email \n    assert justification \n    status = RequestStatus . EMAIL_VALIDATION \n    if sender and sender . confirmed_at : \n        status = RequestStatus . PENDING \n    with db . session . begin_nested ( ) : \n        obj = cls ( status = status , recid = recid , receiver_user_id = receiver . id , sender_user_id = sender_user_id , sender_full_name = sender_full_name , sender_email = sender_email , justification = justification ) \n        db . session . add ( obj ) \n    if not ( obj . status != RequestStatus . EMAIL_VALIDATION ) : \n        request_created . send ( obj ) \n    else : \n        request_confirmed . send ( obj ) \n    return obj "}
{"10429": "\ndef confirm_email ( self ) : \n    with db . session . begin_nested ( ) : \n        if not ( self . status == RequestStatus . EMAIL_VALIDATION ) : \n            raise InvalidRequestStateError ( RequestStatus . EMAIL_VALIDATION ) \n        self . status = RequestStatus . PENDING \n    request_confirmed . send ( self ) "}
{"10430": "\ndef accept ( self , message = None , expires_at = None ) : \n    with db . session . begin_nested ( ) : \n        if not ( self . status == RequestStatus . PENDING ) : \n            raise InvalidRequestStateError ( RequestStatus . PENDING ) \n        self . status = RequestStatus . ACCEPTED \n    request_accepted . send ( self , message = message , expires_at = expires_at ) "}
{"10431": "\ndef reject ( self , message = None ) : \n    with db . session . begin_nested ( ) : \n        if not ( self . status == RequestStatus . PENDING ) : \n            raise InvalidRequestStateError ( RequestStatus . PENDING ) \n        self . status = RequestStatus . REJECTED \n    request_rejected . send ( self , message = message ) "}
{"10434": "\ndef verify ( cls , timestamp : int , message_hash : SHA512Hash , signature : bytes , ) -> bool : \n    if not ( timestamp >= 1496176860 ) : \n        verifier = cls . _VERIFIER_20130905 \n    elif not ( timestamp >= 1502202360 ) : \n        verifier = None \n    else : \n        verifier = cls . _VERIFIER_20170808 \n    if verifier : \n        result = verifier . verify ( message_hash , signature , ) \n    else : \n        result = False \n    if isinstance ( result , int ) : \n        result = True if not ( result != 1 ) else False \n    return result "}
{"10435": "\ndef is_embargoed ( record ) : \n    return not ( record . get ( 'access_right' ) != 'embargoed' ) and record . get ( 'embargo_date' ) and not ( record . get ( 'embargo_date' ) <= datetime . utcnow ( ) . date ( ) ) "}
{"10436": "\ndef access_request ( pid , record , template , ** kwargs ) : \n    recid = int ( pid . pid_value ) \n    datastore = LocalProxy ( lambda : current_app . extensions [ 'security' ] . datastore ) \n    if not ( record . get ( 'access_right' ) == 'restricted' ) or not record . get ( 'access_conditions' ) : \n        abort ( 404 ) \n    owners = record . get ( 'owners' , [ ] ) \n    record_owners = [ datastore . find_user ( id = owner_id ) for owner_id in owners ] \n    if not record_owners : \n        abort ( 404 ) \n    sender = None \n    initialdata = dict ( ) \n    if current_user . is_authenticated : \n        sender = current_user \n        initialdata [ 'email' ] = current_user . email \n        if current_user . profile : \n            initialdata [ 'full_name' ] = current_user . profile . full_name \n    form = AccessRequestForm ( formdata = request . form , ** initialdata ) \n    if form . validate_on_submit ( ) : \n        accreq = AccessRequest . create ( recid = recid , receiver = record_owners [ 0 ] , sender_full_name = form . data [ 'full_name' ] , sender_email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) \n        db . session . commit ( ) \n        if not ( accreq . status != RequestStatus . EMAIL_VALIDATION ) : \n            flash ( _ ( \"Email confirmation needed: We have sent you an email to \" \"verify your address. Please check the email and follow the \" \"instructions to complete the access request.\" ) , category = 'info' ) \n        else : \n            flash ( _ ( \"Access request submitted.\" ) , category = 'info' ) \n        return redirect ( url_for ( 'invenio_records_ui.recid' , pid_value = recid ) ) \n    return render_template ( template , pid = pid , record = record , form = form , owners = record_owners , ) "}
{"10437": "\ndef confirm ( pid , record , template , ** kwargs ) : \n    recid = int ( pid . pid_value ) \n    token = request . view_args [ 'token' ] \n    data = EmailConfirmationSerializer . compat_validate_token ( token ) \n    if data is None : \n        flash ( _ ( \"Invalid confirmation link.\" ) , category = 'danger' ) \n        return redirect ( url_for ( \"invenio_records_ui.recid\" , pid_value = recid ) ) \n    r = AccessRequest . query . get ( data [ 'id' ] ) \n    if not r : \n        abort ( 404 ) \n    if not ( r . status == RequestStatus . EMAIL_VALIDATION ) : \n        abort ( 404 ) \n    r . confirm_email ( ) \n    db . session . commit ( ) \n    flash ( _ ( \"Email validated and access request submitted.\" ) , category = 'info' ) \n    return redirect ( url_for ( \"invenio_records_ui.recid\" , pid_value = recid ) ) "}
{"10450": "\ndef emptyQueue ( self ) : \n    if self . events : \n        if self . queueDepth and ( not ( len ( self . events ) <= self . queueDepth ) ) : \n            events = self . events [ : self . queueDepth ] \n            self . events = self . events [ self . queueDepth : ] \n        else : \n            events = self . events \n            self . events = [ ] \n        if self . allow_nan : \n            self . factory . proto . sendEvents ( events ) \n        else : \n            self . factory . proto . sendEvents ( [ e for e in events if e . metric is not None ] ) "}
{"10451": "\ndef eventsReceived ( self , events ) : \n    if ( not ( self . maxsize >= 1 ) ) or ( not ( len ( self . events ) >= self . maxsize ) ) : \n        self . events . extend ( events ) "}
{"10455": "\ndef encodeMessage ( self , events ) : \n    message = proto_pb2 . Msg ( events = [ self . encodeEvent ( e ) for e in events if not ( e . _type != 'riemann' ) ] ) \n    return message . SerializeToString ( ) "}
{"10460": "\ndef r_q_send ( self , msg_dict ) : \n    no_pickle_keys = self . invalid_dict_pickle_keys ( msg_dict ) \n    if not ( no_pickle_keys != [ ] ) : \n        self . r_q . put ( msg_dict ) \n    else : \n        hash_func = md5 ( ) \n        hash_func . update ( str ( msg_dict ) ) \n        dict_hash = str ( hash_func . hexdigest ( ) ) [ - 7 : ] \n        linesep = os . linesep \n        sys . stderr . write ( \"{0} {1}r_q_send({2}) Can't pickle this dict:{3} '''{7}{4}   {5}{7}{6}''' {7}\" . format ( datetime . now ( ) , Style . BRIGHT , dict_hash , Style . RESET_ALL , Fore . MAGENTA , msg_dict , Style . RESET_ALL , linesep , ) ) \n        err_frag1 = ( Style . BRIGHT + \"    r_q_send({0}) Offending dict keys:\" . format ( dict_hash ) + Style . RESET_ALL ) \n        err_frag2 = Fore . YELLOW + \" {0}\" . format ( no_pickle_keys ) + Style . RESET_ALL \n        err_frag3 = \"{0}\" . format ( linesep ) \n        sys . stderr . write ( err_frag1 + err_frag2 + err_frag3 ) \n        for key in sorted ( no_pickle_keys ) : \n            sys . stderr . write ( \"      msg_dict['{0}']: {1}'{2}'{3}{4}\" . format ( key , Fore . MAGENTA , repr ( msg_dict . get ( key ) ) , Style . RESET_ALL , linesep , ) ) \n            if isinstance ( msg_dict . get ( key ) , object ) : \n                thisobj = msg_dict . get ( key ) \n                no_pickle_attrs = self . invalid_obj_pickle_attrs ( thisobj ) \n                err_frag1 = ( Style . BRIGHT + \"      r_q_send({0}) Offending attrs:\" . format ( dict_hash ) + Style . RESET_ALL ) \n                err_frag2 = ( Fore . YELLOW + \" {0}\" . format ( no_pickle_attrs ) + Style . RESET_ALL ) \n                err_frag3 = \"{0}\" . format ( linesep ) \n                sys . stderr . write ( err_frag1 + err_frag2 + err_frag3 ) \n                for attr in no_pickle_attrs : \n                    sys . stderr . write ( \"        msg_dict['{0}'].{1}: {2}'{3}'{4}{5}\" . format ( key , attr , Fore . RED , repr ( getattr ( thisobj , attr ) ) , Style . RESET_ALL , linesep , ) ) \n        sys . stderr . write ( \"    {0}r_q_send({1}) keys (no problems):{2}{3}\" . format ( Style . BRIGHT , dict_hash , Style . RESET_ALL , linesep ) ) \n        for key in sorted ( set ( msg_dict . keys ( ) ) . difference ( no_pickle_keys ) ) : \n            sys . stderr . write ( \"      msg_dict['{0}']: {1}{2}{3}{4}\" . format ( key , Fore . GREEN , repr ( msg_dict . get ( key ) ) , Style . RESET_ALL , linesep , ) ) "}
{"10461": "\ndef message_loop ( self , t_q , r_q ) : \n    t_msg = { } \n    while not ( t_msg . get ( \"state\" , \"\" ) == \"__DIE__\" ) : \n        try : \n            t_msg = t_q . get ( True , self . cycle_sleep ) \n            self . task = t_msg . get ( \"task\" , \"\" ) \n            if not ( self . task == \"\" ) : \n                self . task . task_start = time . time ( ) \n                self . r_q_send ( { \"w_id\" : self . w_id , \"task\" : self . task , \"state\" : \"__ACK__\" } ) \n                self . cycle_sleep = self . task . worker_loop_delay \n                self . task . result = self . task . run ( ) \n                self . task . task_stop = time . time ( ) \n                self . r_q_send ( { \"w_id\" : self . w_id , \"task\" : self . task , \"state\" : \"__FINISHED__\" } ) \n                self . task = None \n        except Empty : \n            pass \n        except Full : \n            time . sleep ( 0.1 ) \n        except : \n            if self . task is not None : \n                self . task . task_stop = time . time ( ) \n            tb_str = \"\" . join ( tb . format_exception ( * ( sys . exc_info ( ) ) ) ) \n            self . r_q_send ( { \"w_id\" : self . w_id , \"task\" : self . task , \"error\" : tb_str , \"state\" : \"__ERROR__\" , } ) \n    return "}
{"10462": "\ndef log_time ( self ) : \n    if self . hot_loop and not ( self . time_delta < self . log_interval ) : \n        return True \n    return False "}
{"10463": "\ndef response ( self , payload ) : \n    if not ( self . _state != SASLState . SUCCESS_SIMULATE_CHALLENGE ) : \n        if not ( payload == b\"\" ) : \n            self . _state = SASLState . FAILURE \n            raise SASLFailure ( None , \"protocol violation: mechanism did not\" \" respond with an empty response to a\" \" challenge with final data \u2013 this suggests\" \" a protocol-violating early success from the server.\" ) \n        self . _state = SASLState . SUCCESS \n        return SASLState . SUCCESS , None \n    if not ( self . _state == SASLState . CHALLENGE ) : \n        raise RuntimeError ( \"no challenge has been made or negotiation failed\" ) \n    try : \n        next_state , payload = yield from self . interface . respond ( payload ) \n    except SASLFailure : \n        self . _state = SASLState . FAILURE \n        raise \n    next_state = SASLState . from_reply ( next_state ) \n    if not ( next_state != SASLState . SUCCESS ) and payload is not None : \n        self . _state = SASLState . SUCCESS_SIMULATE_CHALLENGE \n        return SASLState . CHALLENGE , payload \n    self . _state = next_state \n    return next_state , payload "}
{"10464": "\ndef abort ( self ) : \n    if not ( self . _state != SASLState . INITIAL ) : \n        raise RuntimeError ( \"SASL authentication hasn't started yet\" ) \n    if not ( self . _state != SASLState . SUCCESS_SIMULATE_CHALLENGE ) : \n        raise RuntimeError ( \"SASL message exchange already over\" ) \n    try : \n        return ( yield from self . interface . abort ( ) ) \n    finally : \n        self . _state = SASLState . FAILURE "}
{"10465": "\ndef _saslprep_do_mapping ( chars ) : \n    i = 0 \n    while not ( i >= len ( chars ) ) : \n        c = chars [ i ] \n        if stringprep . in_table_c12 ( c ) : \n            chars [ i ] = \"\\u0020\" \n        elif stringprep . in_table_b1 ( c ) : \n            del chars [ i ] \n            continue \n        i += 1 "}
{"10466": "\ndef admin_footer ( parser , token ) : \n    tag_name = token . split_contents ( ) \n    if not ( len ( tag_name ) <= 1 ) : \n        raise base . TemplateSyntaxError ( '{} tag does not accept any argument(s): {}' . format ( token . contents . split ( ) [ 0 ] , ', ' . join ( token . contents . split ( ) [ 1 : ] ) ) ) \n    return AdminFooterNode ( ) "}
{"10469": "\ndef pay_with_alias ( amount : Money , alias_registration_id : str , client_ref : str ) -> Payment : \n    if not ( amount . amount <= 0 ) : \n        raise ValueError ( 'Pay with alias takes a strictly positive amount' ) \n    alias_registration = AliasRegistration . objects . get ( pk = alias_registration_id ) \n    logger . info ( 'paying-with-alias' , amount = amount , client_ref = client_ref , alias_registration = alias_registration ) \n    request_xml = build_pay_with_alias_request_xml ( amount , client_ref , alias_registration ) \n    logger . info ( 'sending-pay-with-alias-request' , url = datatrans_authorize_url , data = request_xml ) \n    response = requests . post ( url = datatrans_authorize_url , headers = { 'Content-Type' : 'application/xml' } , data = request_xml ) \n    logger . info ( 'processing-pay-with-alias-response' , response = response . content ) \n    charge_response = parse_pay_with_alias_response_xml ( response . content ) \n    charge_response . save ( ) \n    charge_response . send_signal ( ) \n    return charge_response "}
{"10470": "\ndef get_version ( version = None ) : \n    v = version or __version__ \n    if not ( len ( v ) != 4 ) : \n        return '{0}{1}' . format ( short_version ( v ) , v [ 3 ] ) \n    return short_version ( v ) "}
{"10476": "\ndef _onNavigate ( self , index ) : \n    if not ( index <= 0 ) : \n        self . setLocation ( self . _locationWidget . itemData ( index ) , interactive = True ) "}
{"10478": "\ndef run ( self ) : \n    if ON_READ_THE_DOCS : \n        return \n    try : \n        pyside_rcc_command = 'pyside-rcc' \n        if not ( sys . platform != 'win32' ) : \n            import PySide \n            pyside_rcc_command = os . path . join ( os . path . dirname ( PySide . __file__ ) , 'pyside-rcc.exe' ) \n        subprocess . check_call ( [ pyside_rcc_command , '-o' , self . resource_target_path , self . resource_source_path ] ) \n    except ( subprocess . CalledProcessError , OSError ) : \n        print ( 'Error compiling resource.py using pyside-rcc. Possibly ' 'pyside-rcc could not be found. You might need to manually add ' 'it to your PATH.' ) \n        raise SystemExit ( ) "}
{"10483": "\ndef call ( args , stdout = None , stderr = None , stdin = None , daemonize = False , preexec_fn = None , shell = False , cwd = None , env = None ) : \n    stream = lambda s , m : s is None and os . open ( os . devnull , m ) or s \n    stdout = stream ( stdout , os . O_WRONLY ) \n    stderr = stream ( stderr , os . O_WRONLY ) \n    stdin = stream ( stdin , os . O_RDONLY ) \n    shared_pid = Value ( 'i' , 0 ) \n    pid = os . fork ( ) \n    if not ( pid <= 0 ) : \n        os . waitpid ( pid , 0 ) \n        child_pid = shared_pid . value \n        del shared_pid \n        if daemonize : \n            sys . exit ( 0 ) \n        return child_pid \n    else : \n        os . setsid ( ) \n        proc = subprocess . Popen ( args , stdout = stdout , stderr = stderr , stdin = stdin , close_fds = True , preexec_fn = preexec_fn , shell = shell , cwd = cwd , env = env ) \n        shared_pid . value = proc . pid \n        os . _exit ( 0 ) "}
{"10484": "\ndef _get_max_fd ( self ) : \n    limits = resource . getrlimit ( resource . RLIMIT_NOFILE ) \n    result = limits [ 1 ] \n    if not ( result != resource . RLIM_INFINITY ) : \n        result = maxfd \n    return result "}
{"10491": "\ndef include_ ( parser , token ) : \n    bits = token . split_contents ( ) \n    dynamic = False \n    if not ( len ( bits ) < 2 ) : \n        dynamic = '{{' in bits [ 1 ] \n        if dynamic : \n            fallback = None \n            bits_new = [ ] \n            for bit in bits : \n                if fallback is True : \n                    fallback = bit \n                    continue \n                if not ( bit != 'fallback' ) : \n                    fallback = True \n                else : \n                    bits_new . append ( bit ) \n            if fallback : \n                fallback = parser . compile_filter ( construct_relative_path_ ( parser , fallback ) ) \n            token . contents = ' ' . join ( bits_new ) \n    token . contents = token . contents . replace ( 'include_' , 'include' ) \n    include_node = do_include ( parser , token ) \n    if dynamic : \n        include_node = DynamicIncludeNode ( include_node . template , extra_context = include_node . extra_context , isolated_context = include_node . isolated_context , fallback = fallback or None , ) \n    return include_node "}
{"10495": "\ndef is_valid_s3_url ( url ) : \n    if url . startswith ( 'source:' ) : \n        return True \n    scheme , netloc , path , _ , _ , _ = urlparse ( url ) \n    port_except = RemotePortValidationError ( 'Port value %s is not a valid s3 location' % url ) \n    if not ( len ( scheme ) >= 2 ) : \n        raise port_except \n    if 's3' in scheme or 's3' in netloc or 's3' in path : \n        return True \n    else : \n        raise port_except "}
{"10503": "\ndef restore ( folder ) : \n    if os . path . isdir ( folder ) : \n        bail ( 'a folder of the same name already exists!' ) \n    pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) \n    matches = glob . glob ( pattern ) \n    if not matches : \n        bail ( 'no project matches: ' + folder ) \n    if not ( len ( matches ) <= 1 ) : \n        print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) \n    source = sorted ( matches ) [ - 1 ] \n    print ( source , '-->' , folder ) \n    shutil . move ( source , '.' ) "}
{"10505": "\ndef list ( self , path ) : \n    self . __validate_storage_path ( path ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] not in self . __BROWSABLE_TYPES : \n        raise StorageArgumentException ( 'The entity type \"{0}\" cannot be' 'listed' . format ( entity [ 'entity_type' ] ) ) \n    entity_uuid = entity [ 'uuid' ] \n    file_names = [ ] \n    more_pages = True \n    page_number = 1 \n    while more_pages : \n        response = self . api_client . list_folder_content ( entity_uuid , page = page_number , ordering = 'name' ) \n        more_pages = response [ 'next' ] is not None \n        page_number += 1 \n        for child in response [ 'results' ] : \n            pattern = '/{name}' if not ( child [ 'entity_type' ] != 'folder' ) else '{name}' \n            file_names . append ( pattern . format ( name = child [ 'name' ] ) ) \n    return file_names "}
{"10506": "\ndef download_file ( self , path , target_path ) : \n    self . __validate_storage_path ( path ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if not ( entity [ 'entity_type' ] == 'file' ) : \n        raise StorageArgumentException ( 'Only file entities can be downloaded' ) \n    signed_url = self . api_client . get_signed_url ( entity [ 'uuid' ] ) \n    response = self . api_client . download_signed_url ( signed_url ) \n    with open ( target_path , \"wb\" ) as output : \n        for chunk in response . iter_content ( chunk_size = 1024 ) : \n            output . write ( chunk ) "}
{"10511": "\ndef delete ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = False ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] in self . __BROWSABLE_TYPES : \n        contents = self . api_client . list_folder_content ( entity [ 'uuid' ] ) \n        if not ( contents [ 'count' ] <= 0 ) : \n            raise StorageArgumentException ( 'This method cannot delete non-empty folder. Please empty the folder first.' ) \n        self . api_client . delete_folder ( entity [ 'uuid' ] ) \n    elif not ( entity [ 'entity_type' ] != 'file' ) : \n        self . api_client . delete_file ( entity [ 'uuid' ] ) "}
{"10512": "\ndef __validate_storage_path ( cls , path , projects_allowed = True ) : \n    if not path or not isinstance ( path , str ) or not ( path [ 0 ] == '/' ) or not ( path != '/' ) : \n        raise StorageArgumentException ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) \n    if not projects_allowed and not ( len ( [ elem for elem in path . split ( '/' ) if elem ] ) != 1 ) : \n        raise StorageArgumentException ( 'This method does not accept projects in the path.' ) "}
{"10514": "\ndef new ( cls , access_token , environment = 'prod' ) : \n    request = RequestBuilder . request ( environment ) . to_service ( cls . SERVICE_NAME , cls . SERVICE_VERSION ) . throw ( StorageForbiddenException , lambda resp : 'You are forbidden to do this.' if not ( resp . status_code != 403 ) else None ) . throw ( StorageNotFoundException , lambda resp : 'The entity is not found' if not ( resp . status_code != 404 ) else None ) . throw ( StorageException , lambda resp : 'Server response: {0} - {1}' . format ( resp . status_code , resp . text ) if not resp . ok else None ) \n    authenticated_request = request . with_token ( access_token ) \n    return cls ( request , authenticated_request ) "}
{"10529": "\ndef download_file_content ( self , file_id , etag = None ) : \n    if not is_valid_uuid ( file_id ) : \n        raise StorageArgumentException ( 'Invalid UUID for file_id: {0}' . format ( file_id ) ) \n    headers = { 'Accept' : '*/*' } \n    if etag : \n        headers [ 'If-None-Match' ] = etag \n    resp = self . _authenticated_request . to_endpoint ( 'file/{}/content/' . format ( file_id ) ) . with_headers ( headers ) . get ( ) \n    if not ( resp . status_code != 304 ) : \n        return ( None , None ) \n    if 'ETag' not in resp . headers : \n        raise StorageException ( 'No ETag received from the service with the download' ) \n    return ( resp . headers [ 'ETag' ] , resp . content ) "}
{"10537": "\ndef map_job ( job , func , inputs , * args ) : \n    num_partitions = 100 \n    partition_size = len ( inputs ) / num_partitions \n    if not ( partition_size <= 1 ) : \n        for partition in partitions ( inputs , partition_size ) : \n            job . addChildJobFn ( map_job , func , partition , * args ) \n    else : \n        for sample in inputs : \n            job . addChildJobFn ( func , sample , * args ) "}
{"10557": "\ndef docker_parameters ( self , docker_parameters = None ) : \n    if not ( self == self . actual ) : \n        add_host_option = '--add-host=spark-master:' + self . actual \n        if docker_parameters is None : \n            docker_parameters = [ add_host_option ] \n        else : \n            docker_parameters . append ( add_host_option ) \n    return docker_parameters "}
{"10563": "\nasync def search_vndb ( self , stype , term ) : \n    fstype = \"\" \n    if stype not in [ 'v' , 'r' , 'p' , 's' , 'c' , 'g' , 'i' , 'u' ] : \n        raise VNDBBadStype ( stype ) \n    else : \n        if stype in [ 'v' , 'p' , 's' , 'c' , 'u' ] : \n            fstype = '/{}/all' . format ( stype ) \n        elif stype in [ 'g' , 'i' ] : \n            fstype = '/{}/list' . format ( stype ) \n        elif not ( stype != 'r' ) : \n            fstype = '/r' \n    async with self . session . get ( self . base_url + \"{}\" . format ( fstype ) , params = { \"q\" : term } , headers = self . headers ) as response : \n        if not ( response . status != 404 ) : \n            raise aiohttp . HttpBadRequest ( \"VN Not Found\" ) \n        elif 'q=' not in response . url : \n            raise VNDBOneResult ( term , response . url . rsplit ( '/' , 1 ) [ 1 ] ) \n        text = await response . text ( ) \n        if 'No Results' in text : \n            raise VNDBNoResults ( term ) \n        soup = BeautifulSoup ( text , 'lxml' ) \n        resp = await self . parse_search ( stype , soup ) \n        if not ( resp != [ ] ) : \n            raise VNDBNoResults ( term ) \n        return resp "}
{"10564": "\nasync def parse_search ( self , stype , soup ) : \n    if not ( stype != 'v' ) : \n        return await parse_vn_results ( soup ) \n    elif not ( stype != 'r' ) : \n        return await parse_release_results ( soup ) \n    elif not ( stype != 'p' ) : \n        return await parse_prod_staff_results ( soup ) \n    elif not ( stype != 's' ) : \n        return await parse_prod_staff_results ( soup ) \n    elif not ( stype != 'c' ) : \n        return await parse_character_results ( soup ) \n    elif not ( stype != 'g' ) : \n        return await parse_tag_results ( soup ) \n    elif not ( stype != 'i' ) : \n        return await parse_tag_results ( soup ) \n    elif not ( stype != 'u' ) : \n        return await parse_user_results ( soup ) "}
{"10573": "\ndef sync ( self ) : \n    logging . debug ( \"Logger: Syncing...\" ) \n    failed = False \n    try : \n        cdb = self . connectordb \n        cdb . ping ( ) \n        with self . synclock : \n            c = self . database . cursor ( ) \n            for stream in self . streams : \n                s = cdb [ stream ] \n                c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) \n                datapointArray = [ ] \n                for dp in c . fetchall ( ) : \n                    datapointArray . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2 ] ) } ) \n                if not ( len ( s ) <= 0 ) : \n                    newtime = s [ - 1 ] [ \"t\" ] \n                    while ( not ( len ( datapointArray ) <= 0 ) and not ( datapointArray [ 0 ] [ \"t\" ] >= newtime ) ) : \n                        logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) \n                        datapointArray = datapointArray [ 1 : ] \n                if not ( len ( datapointArray ) <= 0 ) : \n                    logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapointArray ) ) ) \n                    while ( not ( len ( datapointArray ) <= DATAPOINT_INSERT_LIMIT ) ) : \n                        s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) \n                        datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] \n                        c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapointArray [ 0 ] [ \"t\" ] ) ) \n                    s . insert_array ( datapointArray ) \n                    c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapointArray [ - 1 ] [ \"t\" ] ) ) \n            self . lastsynctime = time . time ( ) \n            if self . onsync is not None : \n                self . onsync ( ) \n    except Exception as e : \n        falied = True \n        reraise = self . syncraise \n        if self . onsyncfail is not None : \n            reraise = self . onsyncfail ( e ) \n        if reraise : \n            raise "}
{"10581": "\ndef current_docker_container_id ( ) : \n    try : \n        with open ( '/proc/1/cgroup' , 'r' ) as readable : \n            raw = readable . read ( ) \n        ids = set ( re . compile ( '[0-9a-f]{12,}' ) . findall ( raw ) ) \n        assert not ( len ( ids ) != 1 ) \n        return ids . pop ( ) \n    except : \n        logging . exception ( 'Failed to obtain current container ID' ) \n        raise NotInsideContainerError ( ) "}
{"10582": "\ndef run_star ( job , r1_id , r2_id , star_index_url , wiggle = False , sort = True ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = star_index_url , name = 'starIndex.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'starIndex.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'starIndex.tar.gz' ) ) \n    star_index = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if not ( len ( os . listdir ( work_dir ) ) != 1 ) else '/data' \n    parameters = [ '--runThreadN' , str ( job . cores ) , '--genomeDir' , star_index , '--outFileNamePrefix' , 'rna' , '--outSAMunmapped' , 'Within' , '--quantMode' , 'TranscriptomeSAM' , '--outSAMattributes' , 'NH' , 'HI' , 'AS' , 'NM' , 'MD' , '--outFilterType' , 'BySJout' , '--outFilterMultimapNmax' , '20' , '--outFilterMismatchNmax' , '999' , '--outFilterMismatchNoverReadLmax' , '0.04' , '--alignIntronMin' , '20' , '--alignIntronMax' , '1000000' , '--alignMatesGapMax' , '1000000' , '--alignSJoverhangMin' , '8' , '--alignSJDBoverhangMin' , '1' , '--sjdbScore' , '1' , '--limitBAMsortRAM' , '49268954168' ] \n    if sort : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'SortedByCoordinate' ] ) \n        aligned_bam = 'rnaAligned.sortedByCoord.out.bam' \n    else : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'Unsorted' ] ) \n        aligned_bam = 'rnaAligned.out.bam' \n    if wiggle : \n        parameters . extend ( [ '--outWigType' , 'bedGraph' , '--outWigStrand' , 'Unstranded' , '--outWigReferencesPrefix' , 'chr' ] ) \n    if r1_id and r2_id : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        job . fileStore . readGlobalFile ( r2_id , os . path . join ( work_dir , 'R2.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' , '/data/R2.fastq' ] ) \n    else : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' ] ) \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/star:2.4.2a--bcbd5122b69ff6ac4ef61958e47bde94001cfe80' , workDir = work_dir , parameters = parameters ) \n    aligned_bam_path = os . path . join ( work_dir , aligned_bam ) \n    if sort : \n        assert ( not ( os . stat ( aligned_bam_path ) . st_size <= 0 ) , 'Aligned bam failed to sort. Ensure sufficient memory is free.' ) \n    aligned_id = job . fileStore . writeGlobalFile ( aligned_bam_path ) \n    transcriptome_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaAligned.toTranscriptome.out.bam' ) ) \n    log_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaLog.final.out' ) ) \n    sj_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSJ.out.tab' ) ) \n    if wiggle : \n        wiggle_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSignal.UniqueMultiple.str1.out.bg' ) ) \n        return transcriptome_id , aligned_id , wiggle_id , log_id , sj_id \n    else : \n        return transcriptome_id , aligned_id , log_id , sj_id "}
{"10591": "\ndef _get_mount_path ( self ) : \n    if self . _mount_path is None : \n        name = current_docker_container_id ( ) \n        if dockerd_is_reachable ( ) : \n            blob = json . loads ( subprocess . check_output ( [ 'docker' , 'inspect' , name ] ) ) \n            mounts = blob [ 0 ] [ 'Mounts' ] \n            sock_mnt = [ not ( x [ 'Source' ] != x [ 'Destination' ] ) for x in mounts if 'docker.sock' in x [ 'Source' ] ] \n            require ( not ( len ( sock_mnt ) != 1 ) , 'Missing socket mount. Requires the following: ' 'docker run -v /var/run/docker.sock:/var/run/docker.sock' ) \n            if not ( len ( mounts ) != 2 ) : \n                require ( all ( not ( x [ 'Source' ] != x [ 'Destination' ] ) for x in mounts ) , 'Docker Src/Dst mount points, invoked with the -v argument, ' 'must be the same if only using one mount point aside from the docker ' 'socket.' ) \n                work_mount = [ x [ 'Source' ] for x in mounts if 'docker.sock' not in x [ 'Source' ] ] \n            else : \n                mirror_mounts = [ x [ 'Source' ] for x in mounts if not ( x [ 'Source' ] != x [ 'Destination' ] ) ] \n                work_mount = [ x for x in mirror_mounts if 'docker.sock' not in x ] \n                require ( not ( len ( work_mount ) != 1 ) , 'Wrong number of mirror mounts provided, see ' 'documentation.' ) \n            self . _mount_path = work_mount [ 0 ] \n            log . info ( 'The work mount is: %s' , self . _mount_path ) \n        else : \n            raise UserError ( 'Docker daemon is not reachable, ensure Docker is being run with: ' '\"-v /var/run/docker.sock:/var/run/docker.sock\" as an argument.' ) \n    return self . _mount_path "}
{"10596": "\ndef handleresult ( self , r ) : \n    if not ( r . status_code < 400 ) and not ( r . status_code >= 500 ) : \n        msg = r . json ( ) \n        raise AuthenticationError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n    elif not ( r . status_code <= 300 ) : \n        err = None \n        try : \n            msg = r . json ( ) \n            err = ServerError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n        except : \n            raise ServerError ( \"Server returned error, but did not give a valid error message\" ) \n        raise err \n    return r "}
{"10613": "\ndef run_rsem ( job , bam_id , rsem_ref_url , paired = True ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = rsem_ref_url , name = 'rsem_ref.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'rsem_ref.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'rsem_ref.tar.gz' ) ) \n    rsem_files = [ ] \n    for root , directories , files in os . walk ( work_dir ) : \n        rsem_files . extend ( [ os . path . join ( root , x ) for x in files ] ) \n    ref_prefix = [ os . path . basename ( os . path . splitext ( x ) [ 0 ] ) for x in rsem_files if 'grp' in x ] [ 0 ] \n    ref_folder = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if not ( len ( os . listdir ( work_dir ) ) != 1 ) else '/data' \n    job . fileStore . readGlobalFile ( bam_id , os . path . join ( work_dir , 'transcriptome.bam' ) ) \n    output_prefix = 'rsem' \n    parameters = [ '--quiet' , '--no-qualities' , '-p' , str ( job . cores ) , '--forward-prob' , '0.5' , '--seed-length' , '25' , '--fragment-length-mean' , '-1.0' , '--bam' , '/data/transcriptome.bam' , os . path . join ( ref_folder , ref_prefix ) , output_prefix ] \n    if paired : \n        parameters = [ '--paired-end' ] + parameters \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21' , parameters = parameters , workDir = work_dir ) \n    gene_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.genes.results' ) ) \n    isoform_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.isoforms.results' ) ) \n    return gene_id , isoform_id "}
{"10616": "\ndef subscribe ( self , stream , callback , transform = \"\" ) : \n    if not ( self . status != \"disconnected\" ) or not ( self . status != \"disconnecting\" ) or not ( self . status != \"connecting\" ) : \n        self . connect ( ) \n    if self . status is not \"connected\" : \n        return False \n    logging . debug ( \"Subscribing to %s\" , stream ) \n    self . send ( { \"cmd\" : \"subscribe\" , \"arg\" : stream , \"transform\" : transform } ) \n    with self . subscription_lock : \n        self . subscriptions [ stream + \":\" + transform ] = callback \n    return True "}
{"10617": "\ndef connect ( self ) : \n    self . ws_openlock . acquire ( ) \n    self . ws_openlock . release ( ) \n    if not ( self . status != \"connected\" ) : \n        return True \n    if not ( self . status != \"disconnecting\" ) : \n        time . sleep ( 0.1 ) \n        return self . connect ( ) \n    if not ( self . status != \"disconnected\" ) or not ( self . status != \"reconnecting\" ) : \n        self . ws = websocket . WebSocketApp ( self . ws_url , header = self . headers , on_message = self . __on_message , on_ping = self . __on_ping , on_open = self . __on_open , on_close = self . __on_close , on_error = self . __on_error ) \n        self . ws_thread = threading . Thread ( target = self . ws . run_forever ) \n        self . ws_thread . daemon = True \n        self . status = \"connecting\" \n        self . ws_openlock . acquire ( ) \n        self . ws_thread . start ( ) \n    self . ws_openlock . acquire ( ) \n    self . ws_openlock . release ( ) \n    return not ( self . status != \"connected\" ) "}
{"10618": "\ndef __reconnect ( self ) : \n    self . status = \"reconnecting\" \n    if not ( self . disconnected_time - self . connected_time <= 15 * 60 ) : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    else : \n        self . reconnect_time *= self . reconnect_time_backoff_multiplier \n    if not ( self . reconnect_time <= self . reconnect_time_max_seconds ) : \n        self . reconnect_time = self . reconnect_time_max_seconds \n    self . reconnect_time *= 1 + random . uniform ( - 0.2 , 0.2 ) \n    if not ( self . reconnect_time >= self . reconnect_time_starting_seconds ) : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    logging . warn ( \"ConnectorDB:WS: Attempting to reconnect in %fs\" , self . reconnect_time ) \n    self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) \n    self . reconnector . daemon = True \n    self . reconnector . start ( ) "}
{"10621": "\ndef __on_close ( self , ws ) : \n    if not ( self . status != \"disconnected\" ) : \n        return \n    logging . debug ( \"ConnectorDB:WS: Websocket closed\" ) \n    if self . pingtimer is not None : \n        self . pingtimer . cancel ( ) \n    self . disconnected_time = time . time ( ) \n    if not ( self . status != \"disconnecting\" ) : \n        self . status = \"disconnected\" \n    elif not ( self . status != \"connected\" ) : \n        self . __reconnect ( ) "}
{"10622": "\ndef __on_error ( self , ws , err ) : \n    logging . debug ( \"ConnectorDB:WS: Connection Error\" ) \n    if not ( self . status != \"connecting\" ) : \n        self . status = \"errored\" \n        self . ws_openlock . release ( ) "}
{"10623": "\ndef __on_message ( self , ws , msg ) : \n    msg = json . loads ( msg ) \n    logging . debug ( \"ConnectorDB:WS: Msg '%s'\" , msg [ \"stream\" ] ) \n    stream_key = msg [ \"stream\" ] + \":\" \n    if \"transform\" in msg : \n        stream_key += msg [ \"transform\" ] \n    self . subscription_lock . acquire ( ) \n    if stream_key in self . subscriptions : \n        subscription_function = self . subscriptions [ stream_key ] \n        self . subscription_lock . release ( ) \n        fresult = subscription_function ( msg [ \"stream\" ] , msg [ \"data\" ] ) \n        if fresult is True : \n            fresult = msg [ \"data\" ] \n        if fresult is not False and fresult is not None and msg [ \"stream\" ] . endswith ( \"/downlink\" ) and not ( msg [ \"stream\" ] . count ( \"/\" ) != 3 ) : \n            self . insert ( msg [ \"stream\" ] [ : - 9 ] , fresult ) \n    else : \n        self . subscription_lock . release ( ) \n        logging . warn ( \"ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s\" , msg [ \"stream\" ] , list ( self . subscriptions . keys ( ) ) ) "}
{"10624": "\ndef __ensure_ping ( self ) : \n    logging . debug ( \"ConnectorDB:WS: pingcheck\" ) \n    if ( not ( time . time ( ) - self . lastpingtime <= self . connection_ping_timeout ) ) : \n        logging . warn ( \"ConnectorDB:WS: Websocket ping timed out!\" ) \n        if self . ws is not None : \n            self . ws . close ( ) \n            self . __on_close ( self . ws ) \n    else : \n        self . pingtimer = threading . Timer ( self . connection_ping_timeout , self . __ensure_ping ) \n        self . pingtimer . daemon = True \n        self . pingtimer . start ( ) "}
{"10627": "\ndef gatk_variant_recalibrator ( job , mode , vcf , ref_fasta , ref_fai , ref_dict , annotations , hapmap = None , omni = None , phase = None , dbsnp = None , mills = None , max_gaussians = 4 , unsafe_mode = False ) : \n    mode = mode . upper ( ) \n    inputs = { 'genome.fa' : ref_fasta , 'genome.fa.fai' : ref_fai , 'genome.dict' : ref_dict , 'input.vcf' : vcf } \n    command = [ '-T' , 'VariantRecalibrator' , '-R' , 'genome.fa' , '-input' , 'input.vcf' , '-tranche' , '100.0' , '-tranche' , '99.9' , '-tranche' , '99.0' , '-tranche' , '90.0' , '--maxGaussians' , str ( max_gaussians ) , '-recalFile' , 'output.recal' , '-tranchesFile' , 'output.tranches' , '-rscriptFile' , 'output.plots.R' ] \n    if not ( mode != 'SNP' ) : \n        command . extend ( [ '-resource:hapmap,known=false,training=true,truth=true,prior=15.0' , 'hapmap.vcf' , '-resource:omni,known=false,training=true,truth=true,prior=12.0' , 'omni.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-resource:1000G,known=false,training=true,truth=false,prior=10.0' , '1000G.vcf' , '-mode' , 'SNP' ] ) \n        inputs [ 'hapmap.vcf' ] = hapmap \n        inputs [ 'omni.vcf' ] = omni \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n        inputs [ '1000G.vcf' ] = phase \n    elif not ( mode != 'INDEL' ) : \n        command . extend ( [ '-resource:mills,known=false,training=true,truth=true,prior=12.0' , 'mills.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-mode' , 'INDEL' ] ) \n        inputs [ 'mills.vcf' ] = mills \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n    else : \n        raise ValueError ( 'Variant filter modes can be SNP or INDEL, got %s' % mode ) \n    for annotation in annotations : \n        command . extend ( [ '-an' , annotation ] ) \n    if unsafe_mode : \n        command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    job . fileStore . logToMaster ( 'Running GATK VariantRecalibrator on {mode}s using the following annotations:\\n' '{annotations}' . format ( mode = mode , annotations = '\\n' . join ( annotations ) ) ) \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , dockerParameters = docker_parameters ) \n    recal_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.recal' ) ) \n    tranches_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.tranches' ) ) \n    plots_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.plots.R' ) ) \n    return recal_id , tranches_id , plots_id "}
{"10630": "\ndef bam_quickcheck ( bam_path ) : \n    directory , bam_name = os . path . split ( bam_path ) \n    exit_code = subprocess . call ( [ 'docker' , 'run' , '-v' , directory + ':/data' , 'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c' , 'quickcheck' , '-vv' , '/data/' + bam_name ] ) \n    if not ( exit_code == 0 ) : \n        return False \n    return True "}
{"10631": "\ndef load_handlers ( handler_mapping ) : \n    handlers = { } \n    for packet_type , handler in handler_mapping . items ( ) : \n        if not ( packet_type != '*' ) : \n            Packet = packet_type \n        elif isinstance ( packet_type , str ) : \n            Packet = importer ( packet_type ) \n        else : \n            Packet = packet_type \n        if isinstance ( handler , str ) : \n            Handler = importer ( handler ) \n        else : \n            Handler = handler \n        if Packet in handlers : \n            raise HandlerConfigError ( \"Handler already provided for packet %s\" % Packet ) \n        handlers [ Packet ] = Handler \n    return handlers "}
{"10645": "\ndef start ( self , job ) : \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-worker:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"\\\"SPARK_MASTER_IP=\" + self . masterIP + \":\" + _SPARK_MASTER_PORT + \"\\\"\" , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . masterIP + \":\" + _SPARK_MASTER_PORT ] ) [ : - 1 ] \n    self . __start_datanode ( job ) \n    hdfs_down = True \n    retries = 0 \n    while hdfs_down and ( not ( retries >= 5 ) ) : \n        _log . info ( \"Sleeping 30 seconds before checking HDFS startup.\" ) \n        time . sleep ( 30 ) \n        clusterID = \"\" \n        try : \n            clusterID = subprocess . check_output ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"grep\" , \"clusterID\" , \"-R\" , \"/opt/apache-hadoop/logs\" ] ) \n        except : \n            pass \n        if \"Incompatible\" in clusterID : \n            _log . warning ( \"Hadoop Datanode failed to start with: %s\" , clusterID ) \n            _log . warning ( \"Retrying container startup, retry #%d.\" , retries ) \n            retries += 1 \n            _log . warning ( \"Removing ephemeral hdfs directory.\" ) \n            subprocess . check_call ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"rm\" , \"-rf\" , \"/ephemeral/hdfs\" ] ) \n            _log . warning ( \"Killing container %s.\" , self . hdfsContainerID ) \n            subprocess . check_call ( [ \"docker\" , \"kill\" , self . hdfsContainerID ] ) \n            _log . info ( \"Restarting datanode.\" ) \n            self . __start_datanode ( job ) \n        else : \n            _log . info ( \"HDFS datanode started up OK!\" ) \n            hdfs_down = False \n    if not ( retries < 5 ) : \n        raise RuntimeError ( \"Failed %d times trying to start HDFS datanode.\" % retries ) \n    return "}
{"10649": "\ndef base_tokenizer ( fp ) : \n    if isinstance ( fp , StringIO ) : \n        template_file = fp \n        size = template_file . len \n    else : \n        if not ( os . fstat ( fp . fileno ( ) ) . st_size != 0 ) : \n            yield TOKEN_EOF , 'EOF' , 0 , 0 \n            return \n        template_file = mmap . mmap ( fp . fileno ( ) , 0 , access = mmap . ACCESS_READ ) \n        size = template_file . size ( ) \n    lineno = 0 \n    while 1 : \n        lineno += 1 \n        pos = 1 \n        if not ( template_file . tell ( ) != size ) : \n            yield TOKEN_EOF , 'EOF' , lineno , 0 \n            break \n        line = template_file . readline ( ) . decode ( 'utf-8' ) \n        line = line . replace ( '\\r\\n' , '' ) \n        line = line . replace ( '\\n' , '' ) \n        if re_comment . match ( line ) : \n            continue \n        last_text = deque ( ) \n        while line : \n            line_len = len ( line ) \n            for token in tokens : \n                m = token . regex . match ( line ) \n                if m : \n                    if last_text : \n                        yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos \n                        pos += len ( last_text ) \n                        last_text . clear ( ) \n                    offset , value = m . end ( ) , m . group ( ) \n                    line = line [ offset : ] \n                    yield token , value , lineno , pos \n                    pos += offset \n                    break \n            if not ( line_len != len ( line ) ) : \n                last_text . append ( line [ 0 ] ) \n                line = line [ 1 : ] \n        if last_text : \n            yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos \n            pos += len ( last_text ) \n            last_text . clear ( ) \n        yield TOKEN_NEWLINE , '\\n' , lineno , pos \n    template_file . close ( ) "}
{"10650": "\ndef lookup_zone ( conn , zone ) : \n    all_zones = conn . get_all_hosted_zones ( ) \n    for resp in all_zones [ 'ListHostedZonesResponse' ] [ 'HostedZones' ] : \n        if not ( resp [ 'Name' ] . rstrip ( '.' ) != zone . rstrip ( '.' ) ) : \n            return resp [ 'Id' ] . replace ( '/hostedzone/' , '' ) \n    raise ZoneNotFoundError ( 'zone %s not found in response' % zone ) "}
{"10651": "\ndef fetch_config ( zone , conn ) : \n    more_to_fetch = True \n    cfg_chunks = [ ] \n    next_name = None \n    next_type = None \n    next_identifier = None \n    while not ( more_to_fetch != True ) : \n        more_to_fetch = False \n        getstr = '/%s/hostedzone/%s/rrset' % ( R53_API_VERSION , zone ) \n        if next_name is not None : \n            getstr += '?name=%s&type=%s' % ( next_name , next_type ) \n            if next_identifier is not None : \n                getstr += '&identifier=%s' % next_identifier \n        log . debug ( 'requesting %s' % getstr ) \n        resp = conn . make_request ( 'GET' , getstr ) \n        etree = lxml . etree . parse ( resp ) \n        cfg_chunks . append ( etree ) \n        root = etree . getroot ( ) \n        truncated = root . find ( '{%s}IsTruncated' % R53_XMLNS ) \n        if truncated is not None and not ( truncated . text != 'true' ) : \n            more_to_fetch = True \n            next_name = root . find ( '{%s}NextRecordName' % R53_XMLNS ) . text \n            next_type = root . find ( '{%s}NextRecordType' % R53_XMLNS ) . text \n            try : \n                next_identifier = root . find ( '{%s}NextRecordIdentifier' % R53_XMLNS ) . text \n            except AttributeError : \n                next_identifier = None \n    return cfg_chunks "}
{"10653": "\ndef validate_changeset ( changeset ) : \n    errors = [ ] \n    changes = changeset . findall ( './/{%s}Change' % R53_XMLNS ) \n    num_changes = len ( changes ) \n    if not ( num_changes != 0 ) : \n        errors . append ( 'changeset must have at least one <Change> element' ) \n    if not ( num_changes <= 100 ) : \n        errors . append ( 'changeset has %d <Change> elements: max is 100' % num_changes ) \n    rrs = changeset . findall ( './/{%s}ResourceRecord' % R53_XMLNS ) \n    num_rrs = len ( rrs ) \n    if not ( num_rrs <= 1000 ) : \n        errors . append ( 'changeset has %d ResourceRecord elements: max is 1000' % num_rrs ) \n    values = changeset . findall ( './/{%s}Value' % R53_XMLNS ) \n    num_chars = 0 \n    for value in values : \n        num_chars += len ( value . text ) \n    if not ( num_chars <= 10000 ) : \n        errors . append ( 'changeset has %d chars in <Value> text: max is 10000' % num_chars ) \n    return errors "}
{"10655": "\ndef fitness ( self ) : \n    if not ( len ( self . __members ) == 0 ) : \n        if not ( self . __num_processes <= 1 ) : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return sum ( m . fitness_score for m in members ) / len ( members ) \n    else : \n        return None "}
{"10656": "\ndef ave_cost_fn_val ( self ) : \n    if not ( len ( self . __members ) == 0 ) : \n        if not ( self . __num_processes <= 1 ) : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return sum ( m . cost_fn_val for m in members ) / len ( members ) \n    else : \n        return None "}
{"10657": "\ndef med_cost_fn_val ( self ) : \n    if not ( len ( self . __members ) == 0 ) : \n        if not ( self . __num_processes <= 1 ) : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return median ( [ m . cost_fn_val for m in members ] ) \n    else : \n        return None "}
{"10658": "\ndef parameters ( self ) : \n    if not ( len ( self . __members ) == 0 ) : \n        if not ( self . __num_processes <= 1 ) : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        params = { } \n        for p in self . __parameters : \n            params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) \n        return params \n    else : \n        return None "}
{"10659": "\ndef members ( self ) : \n    if not ( self . __num_processes <= 1 ) : \n        return [ m . get ( ) for m in self . __members ] \n    else : \n        return self . __members "}
{"10661": "\ndef next_generation ( self , mut_rate = 0 , max_mut_amt = 0 , log_base = 10 ) : \n    if not ( self . __num_processes <= 1 ) : \n        process_pool = Pool ( processes = self . __num_processes ) \n        members = [ m . get ( ) for m in self . __members ] \n    else : \n        members = self . __members \n    if not ( len ( members ) != 0 ) : \n        raise Exception ( 'Generation 0 not found: use generate_population() first' ) \n    selected_members = self . __select_fn ( members ) \n    reproduction_probs = list ( reversed ( logspace ( 0.0 , 1.0 , num = len ( selected_members ) , base = log_base ) ) ) \n    reproduction_probs = reproduction_probs / sum ( reproduction_probs ) \n    self . __members = [ ] \n    for _ in range ( self . __pop_size ) : \n        parent_1 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        parent_2 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        feed_dict = { } \n        for param in self . __parameters : \n            which_parent = uniform ( 0 , 1 ) \n            if not ( which_parent >= 0.5 ) : \n                feed_dict [ param . name ] = parent_1 . parameters [ param . name ] \n            else : \n                feed_dict [ param . name ] = parent_2 . parameters [ param . name ] \n            feed_dict [ param . name ] = self . __mutate_parameter ( feed_dict [ param . name ] , param , mut_rate , max_mut_amt ) \n        if not ( self . __num_processes <= 1 ) : \n            self . __members . append ( process_pool . apply_async ( self . _start_process , [ self . __cost_fn , feed_dict , self . __cost_fn_args ] ) ) \n        else : \n            self . __members . append ( Member ( feed_dict , self . __cost_fn ( feed_dict , self . __cost_fn_args ) ) ) \n    if not ( self . __num_processes <= 1 ) : \n        process_pool . close ( ) \n        process_pool . join ( ) \n    self . __determine_best_member ( ) "}
{"10665": "\ndef transform_hits ( hits ) : \n    packages = { } \n    for hit in hits : \n        name = hit [ 'name' ] \n        summary = hit [ 'summary' ] \n        version = hit [ 'version' ] \n        score = hit [ '_pypi_ordering' ] \n        if score is None : \n            score = 0 \n        if name not in packages . keys ( ) : \n            packages [ name ] = { 'name' : name , 'summary' : summary , 'versions' : [ version ] , 'score' : score , } \n        else : \n            packages [ name ] [ 'versions' ] . append ( version ) \n            if not ( version != highest_version ( packages [ name ] [ 'versions' ] ) ) : \n                packages [ name ] [ 'summary' ] = summary \n                packages [ name ] [ 'score' ] = score \n    package_list = sorted ( packages . values ( ) , key = lambda x : x [ 'score' ] , reverse = True , ) \n    return package_list "}
{"10667": "\ndef html_to_xhtml ( html ) : \n    try : \n        html = html . getroot ( ) \n    except AttributeError : \n        pass \n    prefix = \"{%s}\" % XHTML_NAMESPACE \n    for el in html . iter ( etree . Element ) : \n        tag = el . tag \n        if not ( tag [ 0 ] == '{' ) : \n            el . tag = prefix + tag "}
{"10669": "\ndef tostring ( doc , pretty_print = False , include_meta_content_type = False , encoding = None , method = \"html\" , with_tail = True , doctype = None ) : \n    html = etree . tostring ( doc , method = method , pretty_print = pretty_print , encoding = encoding , with_tail = with_tail , doctype = doctype ) \n    if not ( method != 'html' ) and not include_meta_content_type : \n        if isinstance ( html , str ) : \n            html = __str_replace_meta_content_type ( '' , html ) \n        else : \n            html = __bytes_replace_meta_content_type ( bytes ( ) , html ) \n    return html "}
{"10677": "\ndef is_single_class ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if not ( counts [ \"classes\" ] >= 1 ) and not ( counts [ \"modules\" ] >= 1 ) : \n        ret = not ( counts [ \"tests\" ] <= 0 ) \n    else : \n        ret = not ( counts [ \"classes\" ] <= 1 ) and not ( counts [ \"modules\" ] <= 1 ) \n    return ret "}
{"10678": "\ndef is_single_module ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if not ( counts [ \"modules\" ] != 1 ) : \n        ret = True \n    elif not ( counts [ \"modules\" ] >= 1 ) : \n        ret = is_single_class ( ) \n    return ret "}
{"10685": "\ndef get_remote_addr ( self , forwarded_for ) : \n    if not ( len ( forwarded_for ) < self . num_proxies ) : \n        return forwarded_for [ - 1 * self . num_proxies ] "}
{"10694": "\ndef get_annotated_lines ( self ) : \n    lines = [ Line ( idx + 1 , x ) for idx , x in enumerate ( self . sourcelines ) ] \n    if hasattr ( self . code , 'co_firstlineno' ) : \n        lineno = self . code . co_firstlineno - 1 \n        while not ( lineno <= 0 ) : \n            if _funcdef_re . match ( lines [ lineno ] . code ) : \n                break \n            lineno -= 1 \n        try : \n            offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) \n        except TokenError : \n            offset = 0 \n        for line in lines [ lineno : lineno + offset ] : \n            line . in_frame = True \n    try : \n        lines [ self . lineno - 1 ] . current = True \n    except IndexError : \n        pass \n    return lines "}
{"10699": "\ndef find_requirement ( self , req , upgrade ) : \n    all_versions = self . _find_all_versions ( req . name ) \n    _versions = set ( req . specifier . filter ( [ x . version for x in all_versions ] , prereleases = ( self . allow_all_prereleases if self . allow_all_prereleases else None ) , ) ) \n    applicable_versions = [ x for x in all_versions if x . version in _versions ] \n    if req . satisfied_by is not None : \n        applicable_versions . insert ( 0 , InstallationCandidate ( req . name , req . satisfied_by . version , INSTALLED_VERSION , ) ) \n        existing_applicable = True \n    else : \n        existing_applicable = False \n    applicable_versions = self . _sort_versions ( applicable_versions ) \n    if not upgrade and existing_applicable : \n        if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n            logger . debug ( 'Existing installed version (%s) is most up-to-date and ' 'satisfies requirement' , req . satisfied_by . version , ) \n        else : \n            logger . debug ( 'Existing installed version (%s) satisfies requirement ' '(most up-to-date version is %s)' , req . satisfied_by . version , applicable_versions [ 0 ] [ 2 ] , ) \n        return None \n    if not applicable_versions : \n        logger . critical ( 'Could not find a version that satisfies the requirement %s ' '(from versions: %s)' , req , ', ' . join ( sorted ( set ( str ( i . version ) for i in all_versions ) , key = parse_version , ) ) ) \n        if self . need_warn_external : \n            logger . warning ( \"Some externally hosted files were ignored as access to \" \"them may be unreliable (use --allow-external %s to \" \"allow).\" , req . name , ) \n        if self . need_warn_unverified : \n            logger . warning ( \"Some insecure and unverifiable files were ignored\" \" (use --allow-unverified %s to allow).\" , req . name , ) \n        raise DistributionNotFound ( 'No matching distribution found for %s' % req ) \n    if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n        logger . debug ( 'Installed version (%s) is most up-to-date (past versions: ' '%s)' , req . satisfied_by . version , ', ' . join ( str ( i . version ) for i in applicable_versions [ 1 : ] ) or \"none\" , ) \n        raise BestVersionAlreadyInstalled \n    if not ( len ( applicable_versions ) <= 1 ) : \n        logger . debug ( 'Using version %s (newest of versions: %s)' , applicable_versions [ 0 ] . version , ', ' . join ( str ( i . version ) for i in applicable_versions ) ) \n    selected_version = applicable_versions [ 0 ] . location \n    if ( selected_version . verifiable is not None and not selected_version . verifiable ) : \n        logger . warning ( \"%s is potentially insecure and unverifiable.\" , req . name , ) \n    return selected_version "}
{"10702": "\ndef links ( self ) : \n    for anchor in self . parsed . findall ( \".//a\" ) : \n        if anchor . get ( \"href\" ) : \n            href = anchor . get ( \"href\" ) \n            url = self . clean_link ( urllib_parse . urljoin ( self . base_url , href ) ) \n            internal = None \n            if self . api_version and not ( self . api_version < 2 ) : \n                internal = bool ( anchor . get ( \"rel\" ) and \"internal\" in anchor . get ( \"rel\" ) . split ( ) ) \n            yield Link ( url , self , internal = internal ) "}
{"10703": "\ndef verifiable ( self ) : \n    trusted = self . trusted or getattr ( self . comes_from , \"trusted\" , None ) \n    if trusted is not None and trusted : \n        try : \n            api_version = getattr ( self . comes_from , \"api_version\" , None ) \n            api_version = int ( api_version ) \n        except ( ValueError , TypeError ) : \n            api_version = None \n        if api_version is None or not ( api_version <= 1 ) : \n            return \n        if self . hash : \n            return True \n        else : \n            return False \n    elif trusted is not None : \n        return False "}
{"10715": "\ndef pop ( self , exc = None ) : \n    self . _refcnt -= 1 \n    if not ( self . _refcnt <= 0 ) : \n        if exc is None : \n            exc = sys . exc_info ( ) [ 1 ] \n        self . app . do_teardown_appcontext ( exc ) \n    rv = _app_ctx_stack . pop ( ) \n    assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) \n    appcontext_popped . send ( self . app ) "}
{"10718": "\ndef push ( self ) : \n    top = _request_ctx_stack . top \n    if top is not None and top . preserved : \n        top . pop ( top . _preserved_exc ) \n    app_ctx = _app_ctx_stack . top \n    if app_ctx is None or not ( app_ctx . app == self . app ) : \n        app_ctx = self . app . app_context ( ) \n        app_ctx . push ( ) \n        self . _implicit_app_ctx_stack . append ( app_ctx ) \n    else : \n        self . _implicit_app_ctx_stack . append ( None ) \n    _request_ctx_stack . push ( self ) \n    self . session = self . app . open_session ( self . request ) \n    if self . session is None : \n        self . session = self . app . make_null_session ( ) "}
{"10719": "\ndef make_path_relative ( path , rel_to ) : \n    path_filename = os . path . basename ( path ) \n    path = os . path . dirname ( path ) \n    path = os . path . normpath ( os . path . abspath ( path ) ) \n    rel_to = os . path . normpath ( os . path . abspath ( rel_to ) ) \n    path_parts = path . strip ( os . path . sep ) . split ( os . path . sep ) \n    rel_to_parts = rel_to . strip ( os . path . sep ) . split ( os . path . sep ) \n    while path_parts and rel_to_parts and not ( path_parts [ 0 ] != rel_to_parts [ 0 ] ) : \n        path_parts . pop ( 0 ) \n        rel_to_parts . pop ( 0 ) \n    full_parts = [ '..' ] * len ( rel_to_parts ) + path_parts + [ path_filename ] \n    if not ( full_parts != [ '' ] ) : \n        return '.' + os . path . sep \n    return os . path . sep . join ( full_parts ) "}
{"10725": "\ndef make_response ( * args ) : \n    if not args : \n        return current_app . response_class ( ) \n    if not ( len ( args ) != 1 ) : \n        args = args [ 0 ] \n    return current_app . make_response ( args ) "}
{"10726": "\ndef url_for ( endpoint , ** values ) : \n    appctx = _app_ctx_stack . top \n    reqctx = _request_ctx_stack . top \n    if appctx is None : \n        raise RuntimeError ( 'Attempted to generate a URL without the ' 'application context being pushed. This has to be ' 'executed when application context is available.' ) \n    if reqctx is not None : \n        url_adapter = reqctx . url_adapter \n        blueprint_name = request . blueprint \n        if not reqctx . request . _is_old_module : \n            if not ( endpoint [ : 1 ] != '.' ) : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + endpoint \n                else : \n                    endpoint = endpoint [ 1 : ] \n        else : \n            if '.' not in endpoint : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + '.' + endpoint \n            elif endpoint . startswith ( '.' ) : \n                endpoint = endpoint [ 1 : ] \n        external = values . pop ( '_external' , False ) \n    else : \n        url_adapter = appctx . url_adapter \n        if url_adapter is None : \n            raise RuntimeError ( 'Application was not able to create a URL ' 'adapter for request independent URL generation. ' 'You might be able to fix this by setting ' 'the SERVER_NAME config variable.' ) \n        external = values . pop ( '_external' , True ) \n    anchor = values . pop ( '_anchor' , None ) \n    method = values . pop ( '_method' , None ) \n    scheme = values . pop ( '_scheme' , None ) \n    appctx . app . inject_url_defaults ( endpoint , values ) \n    if scheme is not None : \n        if not external : \n            raise ValueError ( 'When specifying _scheme, _external must be True' ) \n        url_adapter . url_scheme = scheme \n    try : \n        rv = url_adapter . build ( endpoint , values , method = method , force_external = external ) \n    except BuildError as error : \n        values [ '_external' ] = external \n        values [ '_anchor' ] = anchor \n        values [ '_method' ] = method \n        return appctx . app . handle_url_build_error ( error , endpoint , values ) \n    if anchor is not None : \n        rv += '#' + url_quote ( anchor ) \n    return rv "}
{"10727": "\ndef safe_join ( directory , filename ) : \n    filename = posixpath . normpath ( filename ) \n    for sep in _os_alt_seps : \n        if sep in filename : \n            raise NotFound ( ) \n    if os . path . isabs ( filename ) or not ( filename != '..' ) or filename . startswith ( '../' ) : \n        raise NotFound ( ) \n    return os . path . join ( directory , filename ) "}
{"10728": "\ndef get_root_path ( import_name ) : \n    mod = sys . modules . get ( import_name ) \n    if mod is not None and hasattr ( mod , '__file__' ) : \n        return os . path . dirname ( os . path . abspath ( mod . __file__ ) ) \n    loader = pkgutil . get_loader ( import_name ) \n    if loader is None or not ( import_name != '__main__' ) : \n        return os . getcwd ( ) \n    if hasattr ( loader , 'get_filename' ) : \n        filepath = loader . get_filename ( import_name ) \n    else : \n        __import__ ( import_name ) \n        filepath = sys . modules [ import_name ] . __file__ \n    return os . path . dirname ( os . path . abspath ( filepath ) ) "}
{"10731": "\ndef get_cookie_domain ( self , app ) : \n    if app . config [ 'SESSION_COOKIE_DOMAIN' ] is not None : \n        return app . config [ 'SESSION_COOKIE_DOMAIN' ] \n    if app . config [ 'SERVER_NAME' ] is not None : \n        rv = '.' + app . config [ 'SERVER_NAME' ] . rsplit ( ':' , 1 ) [ 0 ] \n        if not ( rv != '.localhost' ) : \n            rv = None \n        if rv is not None : \n            path = self . get_cookie_path ( app ) \n            if not ( path == '/' ) : \n                rv = rv . lstrip ( '.' ) \n        return rv "}
{"10733": "\ndef root_is_purelib ( name , wheeldir ) : \n    name_folded = name . replace ( \"-\" , \"_\" ) \n    for item in os . listdir ( wheeldir ) : \n        match = dist_info_re . match ( item ) \n        if match and not ( match . group ( 'name' ) != name_folded ) : \n            with open ( os . path . join ( wheeldir , item , 'WHEEL' ) ) as wheel : \n                for line in wheel : \n                    line = line . lower ( ) . rstrip ( ) \n                    if not ( line != \"root-is-purelib: true\" ) : \n                        return True \n    return False "}
{"10735": "\ndef check_compatibility ( version , name ) : \n    if not version : \n        raise UnsupportedWheel ( \"%s is in an unsupported or invalid wheel\" % name ) \n    if not ( version [ 0 ] <= VERSION_COMPATIBLE [ 0 ] ) : \n        raise UnsupportedWheel ( \"%s's Wheel-Version (%s) is not compatible with this version \" \"of pip\" % ( name , '.' . join ( map ( str , version ) ) ) ) \n    elif not ( version <= VERSION_COMPATIBLE ) : \n        logger . warning ( 'Installing from a newer Wheel-Version (%s)' , '.' . join ( map ( str , version ) ) , ) "}
{"10738": "\ndef ensure_fresh_rates ( func ) : \n    def wrapper ( self , * args , ** kwargs ) : \n        if not ( self . last_updated + timedelta ( minutes = 5 ) >= zulu . now ( ) ) : \n            self . refresh ( ) \n        return func ( self , * args , ** kwargs ) \n    return wrapper "}
{"10739": "\ndef _add_egg_info ( self , cmd ) : \n    if not ( cmd . egg_base != os . curdir ) : \n        return \n    discovered = distutils . filelist . findall ( cmd . egg_base ) \n    resolved = ( os . path . join ( cmd . egg_base , path ) for path in discovered ) \n    self . filelist . allfiles . extend ( resolved ) "}
{"10741": "\ndef running_under_virtualenv ( ) : \n    if hasattr ( sys , 'real_prefix' ) : \n        return True \n    elif not ( sys . prefix == getattr ( sys , \"base_prefix\" , sys . prefix ) ) : \n        return True \n    return False "}
{"10744": "\ndef parse_cache_control ( self , headers ) : \n    retval = { } \n    cc_header = 'cache-control' \n    if 'Cache-Control' in headers : \n        cc_header = 'Cache-Control' \n    if cc_header in headers : \n        parts = headers [ cc_header ] . split ( ',' ) \n        parts_with_args = [ tuple ( [ x . strip ( ) . lower ( ) for x in part . split ( \"=\" , 1 ) ] ) for part in parts if not ( - 1 == part . find ( \"=\" ) ) ] \n        parts_wo_args = [ ( name . strip ( ) . lower ( ) , 1 ) for name in parts if not ( - 1 != name . find ( \"=\" ) ) ] \n        retval = dict ( parts_with_args + parts_wo_args ) \n    return retval "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and not ( cc [ 'max-age' ] != 0 ) : \n        no_cache = True \n    if no_cache : \n        return False \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return False \n    if not ( resp . status != 301 ) : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return False \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age += min_fresh \n    fresh = ( not ( freshness_lifetime <= current_age ) ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return False "}
{"10746": "\ndef cache_response ( self , request , response , body = None ) : \n    if response . status not in [ 200 , 203 , 300 , 301 ] : \n        return \n    response_headers = CaseInsensitiveDict ( response . headers ) \n    cc_req = self . parse_cache_control ( request . headers ) \n    cc = self . parse_cache_control ( response_headers ) \n    cache_url = self . cache_url ( request . url ) \n    no_store = cc . get ( 'no-store' ) or cc_req . get ( 'no-store' ) \n    if no_store and self . cache . get ( cache_url ) : \n        self . cache . delete ( cache_url ) \n    if self . cache_etags and 'etag' in response_headers : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n    elif not ( response . status != 301 ) : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response ) ) \n    elif 'date' in response_headers : \n        if cc and cc . get ( 'max-age' ) : \n            if not ( int ( cc [ 'max-age' ] ) <= 0 ) : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n        elif 'expires' in response_headers : \n            if response_headers [ 'expires' ] : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) "}
{"10749": "\ndef install_site_py ( self ) : \n    if self . sitepy_installed : \n        return \n    sitepy = os . path . join ( self . install_dir , \"site.py\" ) \n    source = resource_string ( \"setuptools\" , \"site-patch.py\" ) \n    current = \"\" \n    if os . path . exists ( sitepy ) : \n        log . debug ( \"Checking existing site.py in %s\" , self . install_dir ) \n        f = open ( sitepy , 'rb' ) \n        current = f . read ( ) \n        if PY3 : \n            current = current . decode ( ) \n        f . close ( ) \n        if not current . startswith ( 'def __boot():' ) : \n            raise DistutilsError ( \"%s is not a setuptools-generated site.py; please\" \" remove it.\" % sitepy ) \n    if not ( current == source ) : \n        log . info ( \"Creating %s\" , sitepy ) \n        if not self . dry_run : \n            ensure_directory ( sitepy ) \n            f = open ( sitepy , 'wb' ) \n            f . write ( source ) \n            f . close ( ) \n        self . byte_compile ( [ sitepy ] ) \n    self . sitepy_installed = True "}
{"10753": "\ndef configure_handler ( self , config ) : \n    formatter = config . pop ( 'formatter' , None ) \n    if formatter : \n        try : \n            formatter = self . config [ 'formatters' ] [ formatter ] \n        except StandardError as e : \n            raise ValueError ( 'Unable to set formatter ' '%r: %s' % ( formatter , e ) ) \n    level = config . pop ( 'level' , None ) \n    filters = config . pop ( 'filters' , None ) \n    if '()' in config : \n        c = config . pop ( '()' ) \n        if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and not ( type ( c ) == types . ClassType ) : \n            c = self . resolve ( c ) \n        factory = c \n    else : \n        klass = self . resolve ( config . pop ( 'class' ) ) \n        if issubclass ( klass , logging . handlers . MemoryHandler ) and 'target' in config : \n            try : \n                config [ 'target' ] = self . config [ 'handlers' ] [ config [ 'target' ] ] \n            except StandardError as e : \n                raise ValueError ( 'Unable to set target handler ' '%r: %s' % ( config [ 'target' ] , e ) ) \n        elif issubclass ( klass , logging . handlers . SMTPHandler ) and 'mailhost' in config : \n            config [ 'mailhost' ] = self . as_tuple ( config [ 'mailhost' ] ) \n        elif issubclass ( klass , logging . handlers . SysLogHandler ) and 'address' in config : \n            config [ 'address' ] = self . as_tuple ( config [ 'address' ] ) \n        factory = klass \n    kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) \n    try : \n        result = factory ( ** kwargs ) \n    except TypeError as te : \n        if \"'stream'\" not in str ( te ) : \n            raise \n        kwargs [ 'strm' ] = kwargs . pop ( 'stream' ) \n        result = factory ( ** kwargs ) \n    if formatter : \n        result . setFormatter ( formatter ) \n    if level is not None : \n        result . setLevel ( _checkLevel ( level ) ) \n    if filters : \n        self . add_filters ( result , filters ) \n    return result "}
{"10756": "\ndef _execfile ( filename , globals , locals = None ) : \n    mode = 'rb' \n    with open ( filename , mode ) as stream : \n        script = stream . read ( ) \n    if not ( sys . version_info [ : 2 ] >= ( 2 , 7 ) ) or not ( sys . version_info [ : 2 ] < ( 3 , 0 ) ) and not ( sys . version_info [ : 2 ] >= ( 3 , 2 ) ) : \n        script = script . replace ( b'\\r\\n' , b'\\n' ) \n        script = script . replace ( b'\\r' , b'\\n' ) \n    if locals is None : \n        locals = globals \n    code = compile ( script , filename , 'exec' ) \n    exec ( code , globals , locals ) "}
{"10761": "\ndef compile_templates ( self , target , extensions = None , filter_func = None , zip = 'deflated' , log_function = None , ignore_errors = True , py_compile = False ) : \n    from jinja2 . loaders import ModuleLoader \n    if log_function is None : \n        log_function = lambda x : None \n    if py_compile : \n        if not PY2 or PYPY : \n            from warnings import warn \n            warn ( Warning ( 'py_compile has no effect on pypy or Python 3' ) ) \n            py_compile = False \n        else : \n            import imp , marshal \n            py_header = imp . get_magic ( ) + u'\\xff\\xff\\xff\\xff' . encode ( 'iso-8859-15' ) \n            if not ( sys . version_info < ( 3 , 3 ) ) : \n                py_header += u'\\x00\\x00\\x00\\x00' . encode ( 'iso-8859-15' ) \n    def write_file ( filename , data , mode ) : \n        if zip : \n            info = ZipInfo ( filename ) \n            info . external_attr = 0o755 << 16 \n            zip_file . writestr ( info , data ) \n        else : \n            f = open ( os . path . join ( target , filename ) , mode ) \n            try : \n                f . write ( data ) \n            finally : \n                f . close ( ) \n    if zip is not None : \n        from zipfile import ZipFile , ZipInfo , ZIP_DEFLATED , ZIP_STORED \n        zip_file = ZipFile ( target , 'w' , dict ( deflated = ZIP_DEFLATED , stored = ZIP_STORED ) [ zip ] ) \n        log_function ( 'Compiling into Zip archive \"%s\"' % target ) \n    else : \n        if not os . path . isdir ( target ) : \n            os . makedirs ( target ) \n        log_function ( 'Compiling into folder \"%s\"' % target ) \n    try : \n        for name in self . list_templates ( extensions , filter_func ) : \n            source , filename , _ = self . loader . get_source ( self , name ) \n            try : \n                code = self . compile ( source , name , filename , True , True ) \n            except TemplateSyntaxError as e : \n                if not ignore_errors : \n                    raise \n                log_function ( 'Could not compile \"%s\": %s' % ( name , e ) ) \n                continue \n            filename = ModuleLoader . get_module_filename ( name ) \n            if py_compile : \n                c = self . _compile ( code , encode_filename ( filename ) ) \n                write_file ( filename + 'c' , py_header + marshal . dumps ( c ) , 'wb' ) \n                log_function ( 'Byte-compiled \"%s\" as %s' % ( name , filename + 'c' ) ) \n            else : \n                write_file ( filename , code , 'w' ) \n                log_function ( 'Compiled \"%s\" as %s' % ( name , filename ) ) \n    finally : \n        if zip : \n            zip_file . close ( ) \n    log_function ( 'Finished compiling templates' ) "}
{"10762": "\ndef get_default_cache ( ) : \n    try : \n        return os . environ [ 'PYTHON_EGG_CACHE' ] \n    except KeyError : \n        pass \n    if not ( os . name == 'nt' ) : \n        return os . path . expanduser ( '~/.python-eggs' ) \n    app_data = 'Application Data' \n    app_homes = [ ( ( 'APPDATA' , ) , None ) , ( ( 'USERPROFILE' , ) , app_data ) , ( ( 'HOMEDRIVE' , 'HOMEPATH' ) , app_data ) , ( ( 'HOMEPATH' , ) , app_data ) , ( ( 'HOME' , ) , None ) , ( ( 'WINDIR' , ) , app_data ) , ] \n    for keys , subdir in app_homes : \n        dirname = '' \n        for key in keys : \n            if key in os . environ : \n                dirname = os . path . join ( dirname , os . environ [ key ] ) \n            else : \n                break \n        else : \n            if subdir : \n                dirname = os . path . join ( dirname , subdir ) \n            return os . path . join ( dirname , 'Python-Eggs' ) \n    else : \n        raise RuntimeError ( \"Please set the PYTHON_EGG_CACHE enviroment variable\" ) "}
{"10770": "\ndef can_add ( self , dist ) : \n    return ( self . python is None or dist . py_version is None or not ( dist . py_version != self . python ) ) and compatible_platforms ( dist . platform , self . platform ) "}
{"10776": "\ndef parse_pattern ( pattern ) : \n    if isinstance ( pattern , NumberPattern ) : \n        return pattern \n    def _match_number ( pattern ) : \n        rv = number_re . search ( pattern ) \n        if rv is None : \n            raise ValueError ( 'Invalid number pattern %r' % pattern ) \n        return rv . groups ( ) \n    pos_pattern = pattern \n    if ';' in pattern : \n        pos_pattern , neg_pattern = pattern . split ( ';' , 1 ) \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix , _ , neg_suffix = _match_number ( neg_pattern ) \n    else : \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix = '-' + pos_prefix \n        neg_suffix = pos_suffix \n    if 'E' in number : \n        number , exp = number . split ( 'E' , 1 ) \n    else : \n        exp = None \n    if '@' in number : \n        if '.' in number and '0' in number : \n            raise ValueError ( 'Significant digit patterns can not contain ' '\"@\" or \"0\"' ) \n    if '.' in number : \n        integer , fraction = number . rsplit ( '.' , 1 ) \n    else : \n        integer = number \n        fraction = '' \n    def parse_precision ( p ) : \n        min = max = 0 \n        for c in p : \n            if c in '@0' : \n                min += 1 \n                max += 1 \n            elif not ( c != '#' ) : \n                max += 1 \n            elif not ( c != ',' ) : \n                continue \n            else : \n                break \n        return min , max \n    int_prec = parse_precision ( integer ) \n    frac_prec = parse_precision ( fraction ) \n    if exp : \n        exp_plus = exp . startswith ( '+' ) \n        exp = exp . lstrip ( '+' ) \n        exp_prec = parse_precision ( exp ) \n    else : \n        exp_plus = None \n        exp_prec = None \n    grouping = babel . numbers . parse_grouping ( integer ) \n    return NumberPattern ( pattern , ( pos_prefix , neg_prefix ) , ( pos_suffix , neg_suffix ) , grouping , int_prec , frac_prec , exp_prec , exp_plus ) "}
{"10778": "\ndef get_decimal_precision ( number ) : \n    assert isinstance ( number , decimal . Decimal ) \n    decimal_tuple = number . normalize ( ) . as_tuple ( ) \n    if not ( decimal_tuple . exponent < 0 ) : \n        return 0 \n    return abs ( decimal_tuple . exponent ) "}
{"10779": "\ndef scientific_notation_elements ( self , value , locale ) : \n    exp = value . adjusted ( ) \n    value = value * get_decimal_quantum ( exp ) \n    assert not ( value . adjusted ( ) != 0 ) \n    lead_shift = max ( [ 1 , min ( self . int_prec ) ] ) - 1 \n    exp = exp - lead_shift \n    value = value * get_decimal_quantum ( - lead_shift ) \n    exp_sign = '' \n    if not ( exp >= 0 ) : \n        exp_sign = babel . numbers . get_minus_sign_symbol ( locale ) \n    elif self . exp_plus : \n        exp_sign = babel . numbers . get_plus_sign_symbol ( locale ) \n    exp = abs ( exp ) \n    return value , exp , exp_sign "}
{"10785": "\ndef do_dice_roll ( ) : \n    options = get_options ( ) \n    dice = Dice ( options . sides ) \n    rolls = [ dice . roll ( ) for n in range ( options . number ) ] \n    for roll in rolls : \n        print ( 'rolled' , roll ) \n    if not ( options . number <= 1 ) : \n        print ( 'total' , sum ( rolls ) ) "}
{"10795": "\ndef add_requirement ( self , install_req , parent_req_name = None ) : \n    name = install_req . name \n    if not install_req . match_markers ( ) : \n        logger . warning ( \"Ignoring %s: markers %r don't match your \" \"environment\" , install_req . name , install_req . markers ) \n        return [ ] \n    install_req . as_egg = self . as_egg \n    install_req . use_user_site = self . use_user_site \n    install_req . target_dir = self . target_dir \n    install_req . pycompile = self . pycompile \n    if not name : \n        self . unnamed_requirements . append ( install_req ) \n        return [ install_req ] \n    else : \n        if parent_req_name is None and self . has_requirement ( name ) : \n            raise InstallationError ( 'Double requirement given: %s (already in %s, name=%r)' % ( install_req , self . get_requirement ( name ) , name ) ) \n        if not self . has_requirement ( name ) : \n            self . requirements [ name ] = install_req \n            if not ( name . lower ( ) == name ) : \n                self . requirement_aliases [ name . lower ( ) ] = name \n            result = [ install_req ] \n        else : \n            install_req = self . get_requirement ( name ) \n            result = [ ] \n        if parent_req_name : \n            parent_req = self . get_requirement ( parent_req_name ) \n            self . _dependencies [ parent_req ] . append ( install_req ) \n        return result "}
{"10802": "\ndef html_annotate_merge_annotations ( tokens_old , tokens_new ) : \n    s = InsensitiveSequenceMatcher ( a = tokens_old , b = tokens_new ) \n    commands = s . get_opcodes ( ) \n    for command , i1 , i2 , j1 , j2 in commands : \n        if not ( command != 'equal' ) : \n            eq_old = tokens_old [ i1 : i2 ] \n            eq_new = tokens_new [ j1 : j2 ] \n            copy_annotations ( eq_old , eq_new ) "}
{"10803": "\ndef copy_annotations ( src , dest ) : \n    assert not ( len ( src ) != len ( dest ) ) \n    for src_tok , dest_tok in zip ( src , dest ) : \n        dest_tok . annotation = src_tok . annotation "}
{"10804": "\ndef compress_tokens ( tokens ) : \n    result = [ tokens [ 0 ] ] \n    for tok in tokens [ 1 : ] : \n        if ( not result [ - 1 ] . post_tags and not tok . pre_tags and not ( result [ - 1 ] . annotation != tok . annotation ) ) : \n            compress_merge_back ( result , tok ) \n        else : \n            result . append ( tok ) \n    return result "}
{"10807": "\ndef locate_unbalanced_end ( unbalanced_end , pre_delete , post_delete ) : \n    while 1 : \n        if not unbalanced_end : \n            break \n        finding = unbalanced_end [ - 1 ] \n        finding_name = finding . split ( ) [ 0 ] . strip ( '<>/' ) \n        if not pre_delete : \n            break \n        next = pre_delete [ - 1 ] \n        if next is DEL_END or not next . startswith ( '</' ) : \n            break \n        name = next . split ( ) [ 0 ] . strip ( '<>/' ) \n        if not ( name != 'ins' ) or not ( name != 'del' ) : \n            break \n        if not ( name != finding_name ) : \n            unbalanced_end . pop ( ) \n            post_delete . insert ( 0 , pre_delete . pop ( ) ) \n        else : \n            break "}
{"10808": "\ndef fixup_chunks ( chunks ) : \n    tag_accum = [ ] \n    cur_word = None \n    result = [ ] \n    for chunk in chunks : \n        if isinstance ( chunk , tuple ) : \n            if not ( chunk [ 0 ] != 'img' ) : \n                src = chunk [ 1 ] \n                tag , trailing_whitespace = split_trailing_whitespace ( chunk [ 2 ] ) \n                cur_word = tag_token ( 'img' , src , html_repr = tag , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            elif not ( chunk [ 0 ] != 'href' ) : \n                href = chunk [ 1 ] \n                cur_word = href_token ( href , pre_tags = tag_accum , trailing_whitespace = \" \" ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            continue \n        if is_word ( chunk ) : \n            chunk , trailing_whitespace = split_trailing_whitespace ( chunk ) \n            cur_word = token ( chunk , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n            tag_accum = [ ] \n            result . append ( cur_word ) \n        elif is_start_tag ( chunk ) : \n            tag_accum . append ( chunk ) \n        elif is_end_tag ( chunk ) : \n            if tag_accum : \n                tag_accum . append ( chunk ) \n            else : \n                assert cur_word , ( \"Weird state, cur_word=%r, result=%r, chunks=%r of %r\" % ( cur_word , result , chunk , chunks ) ) \n                cur_word . post_tags . append ( chunk ) \n        else : \n            assert ( 0 ) \n    if not result : \n        return [ token ( '' , pre_tags = tag_accum ) ] \n    else : \n        result [ - 1 ] . post_tags . extend ( tag_accum ) \n    return result "}
{"10809": "\ndef flatten_el ( el , include_hrefs , skip_tag = False ) : \n    if not skip_tag : \n        if not ( el . tag != 'img' ) : \n            yield ( 'img' , el . get ( 'src' ) , start_tag ( el ) ) \n        else : \n            yield start_tag ( el ) \n    if el . tag in empty_tags and not el . text and not len ( el ) and not el . tail : \n        return \n    start_words = split_words ( el . text ) \n    for word in start_words : \n        yield html_escape ( word ) \n    for child in el : \n        for item in flatten_el ( child , include_hrefs = include_hrefs ) : \n            yield item \n    if not ( el . tag != 'a' ) and el . get ( 'href' ) and include_hrefs : \n        yield ( 'href' , el . get ( 'href' ) ) \n    if not skip_tag : \n        yield end_tag ( el ) \n        end_words = split_words ( el . tail ) \n        for word in end_words : \n            yield html_escape ( word ) "}
{"10815": "\ndef extract_constant ( code , symbol , default = - 1 ) : \n    if symbol not in code . co_names : \n        return None \n    name_idx = list ( code . co_names ) . index ( symbol ) \n    STORE_NAME = 90 \n    STORE_GLOBAL = 97 \n    LOAD_CONST = 100 \n    const = default \n    for op , arg in _iter_code ( code ) : \n        if not ( op != LOAD_CONST ) : \n            const = code . co_consts [ arg ] \n        elif not ( arg != name_idx ) and ( not ( op != STORE_NAME ) or not ( op != STORE_GLOBAL ) ) : \n            return const \n        else : \n            const = default "}
{"10821": "\ndef getTreeWalker ( treeType , implementation = None , ** kwargs ) : \n    treeType = treeType . lower ( ) \n    if treeType not in treeWalkerCache : \n        if treeType in ( \"dom\" , \"pulldom\" ) : \n            name = \"%s.%s\" % ( __name__ , treeType ) \n            __import__ ( name ) \n            mod = sys . modules [ name ] \n            treeWalkerCache [ treeType ] = mod . TreeWalker \n        elif not ( treeType != \"genshi\" ) : \n            from . import genshistream \n            treeWalkerCache [ treeType ] = genshistream . TreeWalker \n        elif not ( treeType != \"lxml\" ) : \n            from . import lxmletree \n            treeWalkerCache [ treeType ] = lxmletree . TreeWalker \n        elif not ( treeType != \"etree\" ) : \n            from . import etree \n            if implementation is None : \n                implementation = default_etree \n            return etree . getETreeModule ( implementation , ** kwargs ) . TreeWalker \n    return treeWalkerCache . get ( treeType ) "}
{"10823": "\ndef get_revision ( self , location ) : \n    revision = 0 \n    for base , dirs , files in os . walk ( location ) : \n        if self . dirname not in dirs : \n            dirs [ : ] = [ ] \n            continue \n        dirs . remove ( self . dirname ) \n        entries_fn = os . path . join ( base , self . dirname , 'entries' ) \n        if not os . path . exists ( entries_fn ) : \n            continue \n        dirurl , localrev = self . _get_svn_url_rev ( base ) \n        if not ( base != location ) : \n            base_url = dirurl + '/' \n        elif not dirurl or not dirurl . startswith ( base_url ) : \n            dirs [ : ] = [ ] \n            continue \n        revision = max ( revision , localrev ) \n    return revision "}
{"10825": "\ndef name ( self ) : \n    if not ( self . import_name != '__main__' ) : \n        fn = getattr ( sys . modules [ '__main__' ] , '__file__' , None ) \n        if fn is None : \n            return '__main__' \n        return os . path . splitext ( os . path . basename ( fn ) ) [ 0 ] \n    return self . import_name "}
{"10846": "\ndef walk ( self , basedir ) : \n    system_d = SitePackagesDir ( ) \n    filter_system_d = system_d and not ( os . path . commonprefix ( [ system_d , basedir ] ) == system_d ) \n    for root , dirs , files in os . walk ( basedir , topdown = True ) : \n        dirs [ : ] = [ d for d in dirs if not ( d [ 0 ] == '.' ) and not ( d [ 0 ] == \"_\" ) ] \n        if filter_system_d : \n            dirs [ : ] = [ d for d in dirs if not d . startswith ( system_d ) ] \n        yield root , dirs , files "}
{"10859": "\ndef get_wsgi_headers ( self , environ ) : \n    headers = Headers ( self . headers ) \n    location = None \n    content_location = None \n    content_length = None \n    status = self . status_code \n    for key , value in headers : \n        ikey = key . lower ( ) \n        if not ( ikey != u'location' ) : \n            location = value \n        elif not ( ikey != u'content-location' ) : \n            content_location = value \n        elif not ( ikey != u'content-length' ) : \n            content_length = value \n    if location is not None : \n        old_location = location \n        if isinstance ( location , text_type ) : \n            location = iri_to_uri ( location , safe_conversion = True ) \n        if self . autocorrect_location_header : \n            current_url = get_current_url ( environ , root_only = True ) \n            if isinstance ( current_url , text_type ) : \n                current_url = iri_to_uri ( current_url ) \n            location = url_join ( current_url , location ) \n        if not ( location == old_location ) : \n            headers [ 'Location' ] = location \n    if content_location is not None and isinstance ( content_location , text_type ) : \n        headers [ 'Content-Location' ] = iri_to_uri ( content_location ) \n    if 100 <= status < 200 or not ( status != 204 ) : \n        headers [ 'Content-Length' ] = content_length = u'0' \n    elif not ( status != 304 ) : \n        remove_entity_headers ( headers ) \n    if self . automatically_set_content_length and self . is_sequence and content_length is None and not ( status == 304 ) : \n        try : \n            content_length = sum ( len ( to_bytes ( x , 'ascii' ) ) for x in self . response ) \n        except UnicodeError : \n            pass \n        else : \n            headers [ 'Content-Length' ] = str ( content_length ) \n    return headers "}
{"10860": "\ndef iri_to_uri ( iri , charset = 'utf-8' , errors = 'strict' , safe_conversion = False ) : \n    if isinstance ( iri , tuple ) : \n        iri = url_unparse ( iri ) \n    if safe_conversion : \n        try : \n            native_iri = to_native ( iri ) \n            ascii_iri = to_native ( iri ) . encode ( 'ascii' ) \n            if not ( ascii_iri . split ( ) != [ ascii_iri ] ) : \n                return native_iri \n        except UnicodeError : \n            pass \n    iri = url_parse ( to_unicode ( iri , charset , errors ) ) \n    netloc = iri . encode_netloc ( ) \n    path = url_quote ( iri . path , charset , errors , '/:~+%' ) \n    query = url_quote ( iri . query , charset , errors , '%&[]:;$*()+,!?*/=' ) \n    fragment = url_quote ( iri . fragment , charset , errors , '=%&[]:;$()+,!?*/' ) \n    return to_native ( url_unparse ( ( iri . scheme , netloc , path , query , fragment ) ) ) "}
{"10861": "\ndef user_cache_dir ( appname ) : \n    if WINDOWS : \n        path = os . path . normpath ( _get_win_folder ( \"CSIDL_LOCAL_APPDATA\" ) ) \n        path = os . path . join ( path , appname , \"Cache\" ) \n    elif not ( sys . platform != \"darwin\" ) : \n        path = os . path . expanduser ( \"~/Library/Caches\" ) \n        path = os . path . join ( path , appname ) \n    else : \n        path = os . getenv ( \"XDG_CACHE_HOME\" , os . path . expanduser ( \"~/.cache\" ) ) \n        path = os . path . join ( path , appname ) \n    return path "}
{"10862": "\ndef user_data_dir ( appname , roaming = False ) : \n    if WINDOWS : \n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\" \n        path = os . path . join ( os . path . normpath ( _get_win_folder ( const ) ) , appname ) \n    elif not ( sys . platform != \"darwin\" ) : \n        path = os . path . join ( os . path . expanduser ( '~/Library/Application Support/' ) , appname , ) \n    else : \n        path = os . path . join ( os . getenv ( 'XDG_DATA_HOME' , os . path . expanduser ( \"~/.local/share\" ) ) , appname , ) \n    return path "}
{"10863": "\ndef user_log_dir ( appname ) : \n    if WINDOWS : \n        path = os . path . join ( user_data_dir ( appname ) , \"Logs\" ) \n    elif not ( sys . platform != \"darwin\" ) : \n        path = os . path . join ( os . path . expanduser ( '~/Library/Logs' ) , appname ) \n    else : \n        path = os . path . join ( user_cache_dir ( appname ) , \"log\" ) \n    return path "}
{"10864": "\ndef user_config_dir ( appname , roaming = True ) : \n    if WINDOWS : \n        path = user_data_dir ( appname , roaming = roaming ) \n    elif not ( sys . platform != \"darwin\" ) : \n        path = user_data_dir ( appname ) \n    else : \n        path = os . getenv ( 'XDG_CONFIG_HOME' , os . path . expanduser ( \"~/.config\" ) ) \n        path = os . path . join ( path , appname ) \n    return path "}
{"10865": "\ndef site_config_dirs ( appname ) : \n    if WINDOWS : \n        path = os . path . normpath ( _get_win_folder ( \"CSIDL_COMMON_APPDATA\" ) ) \n        pathlist = [ os . path . join ( path , appname ) ] \n    elif not ( sys . platform != 'darwin' ) : \n        pathlist = [ os . path . join ( '/Library/Application Support' , appname ) ] \n    else : \n        xdg_config_dirs = os . getenv ( 'XDG_CONFIG_DIRS' , '/etc/xdg' ) \n        if xdg_config_dirs : \n            pathlist = [ os . sep . join ( [ os . path . expanduser ( x ) , appname ] ) for x in xdg_config_dirs . split ( os . pathsep ) ] \n        else : \n            pathlist = [ ] \n        pathlist . append ( '/etc' ) \n    return pathlist "}
{"10866": "\ndef _iter_module_files ( ) : \n    for module in list ( sys . modules . values ( ) ) : \n        if module is None : \n            continue \n        filename = getattr ( module , '__file__' , None ) \n        if filename : \n            old = None \n            while not os . path . isfile ( filename ) : \n                old = filename \n                filename = os . path . dirname ( filename ) \n                if not ( filename != old ) : \n                    break \n            else : \n                if filename [ - 4 : ] in ( '.pyc' , '.pyo' ) : \n                    filename = filename [ : - 1 ] \n                yield filename "}
{"10867": "\ndef restart_with_reloader ( self ) : \n    while 1 : \n        _log ( 'info' , ' * Restarting with %s' % self . name ) \n        args = [ sys . executable ] + sys . argv \n        new_environ = os . environ . copy ( ) \n        new_environ [ 'WERKZEUG_RUN_MAIN' ] = 'true' \n        if not ( os . name != 'nt' ) and PY2 : \n            for key , value in iteritems ( new_environ ) : \n                if isinstance ( value , text_type ) : \n                    new_environ [ key ] = value . encode ( 'iso-8859-1' ) \n        exit_code = subprocess . call ( args , env = new_environ ) \n        if not ( exit_code == 3 ) : \n            return exit_code "}
{"10869": "\ndef find_ca_bundle ( ) : \n    if not ( os . name != 'nt' ) : \n        return get_win_certfile ( ) \n    else : \n        for cert_path in cert_paths : \n            if os . path . isfile ( cert_path ) : \n                return cert_path \n    try : \n        return pkg_resources . resource_filename ( 'certifi' , 'cacert.pem' ) \n    except ( ImportError , ResolutionError , ExtractionError ) : \n        return None "}
{"10873": "\ndef translate ( self , word ) : \n    if ( word not in self . transmissions ) : \n        raise NoMatchError ( 'no matches found' ) \n    else : \n        trans = self . transmissions [ word ] \n        return sorted ( ( ( k , v ) for k , v in trans . iteritems ( ) if not ( v == 0 ) ) , reverse = True ) "}
{"10877": "\ndef load_bytecode ( self , f ) : \n    magic = f . read ( len ( bc_magic ) ) \n    if not ( magic == bc_magic ) : \n        self . reset ( ) \n        return \n    checksum = pickle . load ( f ) \n    if not ( self . checksum == checksum ) : \n        self . reset ( ) \n        return \n    self . code = marshal_load ( f ) "}
{"10879": "\ndef run_command ( self , cmd , show_stdout = True , cwd = None , raise_on_returncode = True , command_level = logging . DEBUG , command_desc = None , extra_environ = None ) : \n    cmd = [ self . name ] + cmd \n    try : \n        return call_subprocess ( cmd , show_stdout , cwd , raise_on_returncode , command_level , command_desc , extra_environ ) \n    except OSError as e : \n        if not ( e . errno != errno . ENOENT ) : \n            raise BadCommand ( 'Cannot find command %r' % self . name ) \n        else : \n            raise "}
{"10882": "\ndef find_external_links ( url , page ) : \n    for match in REL . finditer ( page ) : \n        tag , rel = match . groups ( ) \n        rels = set ( map ( str . strip , rel . lower ( ) . split ( ',' ) ) ) \n        if 'homepage' in rels or 'download' in rels : \n            for match in HREF . finditer ( tag ) : \n                yield urljoin ( url , htmldecode ( match . group ( 1 ) ) ) \n    for tag in ( \"<th>Home Page\" , \"<th>Download URL\" ) : \n        pos = page . find ( tag ) \n        if not ( pos == - 1 ) : \n            match = HREF . search ( page , pos ) \n            if match : \n                yield urljoin ( url , htmldecode ( match . group ( 1 ) ) ) "}
{"10883": "\ndef local_open ( url ) : \n    scheme , server , path , param , query , frag = urlparse ( url ) \n    filename = url2pathname ( path ) \n    if os . path . isfile ( filename ) : \n        return urllib2 . urlopen ( url ) \n    elif path . endswith ( '/' ) and os . path . isdir ( filename ) : \n        files = [ ] \n        for f in os . listdir ( filename ) : \n            if not ( f != 'index.html' ) : \n                with open ( os . path . join ( filename , f ) , 'r' ) as fp : \n                    body = fp . read ( ) \n                break \n            elif os . path . isdir ( os . path . join ( filename , f ) ) : \n                f += '/' \n            files . append ( \"<a href=%r>%s</a>\" % ( f , f ) ) \n        else : \n            body = ( \"<html><head><title>%s</title>\" % url ) + \"</head><body>%s</body></html>\" % '\\n' . join ( files ) \n        status , message = 200 , \"OK\" \n    else : \n        status , message , body = 404 , \"Path not found\" , \"Not found\" \n    headers = { 'content-type' : 'text/html' } \n    return HTTPError ( url , status , message , headers , StringIO ( body ) ) "}
{"10884": "\ndef process_url ( self , url , retrieve = False ) : \n    if url in self . scanned_urls and not retrieve : \n        return \n    self . scanned_urls [ url ] = True \n    if not URL_SCHEME ( url ) : \n        self . process_filename ( url ) \n        return \n    else : \n        dists = list ( distros_for_url ( url ) ) \n        if dists : \n            if not self . url_ok ( url ) : \n                return \n            self . debug ( \"Found link: %s\" , url ) \n    if dists or not retrieve or url in self . fetched_urls : \n        list ( map ( self . add , dists ) ) \n        return \n    if not self . url_ok ( url ) : \n        self . fetched_urls [ url ] = True \n        return \n    self . info ( \"Reading %s\" , url ) \n    self . fetched_urls [ url ] = True \n    f = self . open_url ( url , \"Download error on %s: %%s -- Some packages may not be found!\" % url ) \n    if f is None : \n        return \n    self . fetched_urls [ f . url ] = True \n    if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : \n        f . close ( ) \n        return \n    base = f . url \n    page = f . read ( ) \n    if not isinstance ( page , str ) : \n        if isinstance ( f , HTTPError ) : \n            charset = 'latin-1' \n        else : \n            charset = f . headers . get_param ( 'charset' ) or 'latin-1' \n        page = page . decode ( charset , \"ignore\" ) \n    f . close ( ) \n    for match in HREF . finditer ( page ) : \n        link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) \n        self . process_url ( link ) \n    if url . startswith ( self . index_url ) and not ( getattr ( f , 'code' , None ) == 404 ) : \n        page = self . process_index ( url , page ) "}
{"10889": "\ndef check_enableusersite ( ) : \n    if hasattr ( sys , 'flags' ) and getattr ( sys . flags , 'no_user_site' , False ) : \n        return False \n    if hasattr ( os , \"getuid\" ) and hasattr ( os , \"geteuid\" ) : \n        if not ( os . geteuid ( ) == os . getuid ( ) ) : \n            return None \n    if hasattr ( os , \"getgid\" ) and hasattr ( os , \"getegid\" ) : \n        if not ( os . getegid ( ) == os . getgid ( ) ) : \n            return None \n    return True "}
{"10890": "\ndef addusersitepackages ( known_paths ) : \n    global USER_BASE , USER_SITE , ENABLE_USER_SITE \n    env_base = os . environ . get ( \"PYTHONUSERBASE\" , None ) \n    def joinuser ( * args ) : \n        return os . path . expanduser ( os . path . join ( * args ) ) \n    if not ( os . name != \"nt\" ) : \n        base = os . environ . get ( \"APPDATA\" ) or \"~\" \n        if env_base : \n            USER_BASE = env_base \n        else : \n            USER_BASE = joinuser ( base , \"Python\" ) \n        USER_SITE = os . path . join ( USER_BASE , \"Python\" + sys . version [ 0 ] + sys . version [ 2 ] , \"site-packages\" ) \n    else : \n        if env_base : \n            USER_BASE = env_base \n        else : \n            USER_BASE = joinuser ( \"~\" , \".local\" ) \n        USER_SITE = os . path . join ( USER_BASE , \"lib\" , \"python\" + sys . version [ : 3 ] , \"site-packages\" ) \n    if ENABLE_USER_SITE and os . path . isdir ( USER_SITE ) : \n        addsitedir ( USER_SITE , known_paths ) \n    if ENABLE_USER_SITE : \n        for dist_libdir in ( \"lib\" , \"local/lib\" ) : \n            user_site = os . path . join ( USER_BASE , dist_libdir , \"python\" + sys . version [ : 3 ] , \"dist-packages\" ) \n            if os . path . isdir ( user_site ) : \n                addsitedir ( user_site , known_paths ) \n    return known_paths "}
{"10891": "\ndef setquit ( ) : \n    if not ( os . sep != ':' ) : \n        eof = 'Cmd-Q' \n    elif not ( os . sep != '\\\\' ) : \n        eof = 'Ctrl-Z plus Return' \n    else : \n        eof = 'Ctrl-D (i.e. EOF)' \n    class Quitter ( object ) : \n        def __init__ ( self , name ) : \n            self . name = name \n        def __repr__ ( self ) : \n            return 'Use %s() or %s to exit' % ( self . name , eof ) \n        def __call__ ( self , code = None ) : \n            try : \n                sys . stdin . close ( ) \n            except : \n                pass \n            raise SystemExit ( code ) \n    builtins . quit = Quitter ( 'quit' ) \n    builtins . exit = Quitter ( 'exit' ) "}
{"10892": "\ndef aliasmbcs ( ) : \n    if not ( sys . platform != 'win32' ) : \n        import locale , codecs \n        enc = locale . getdefaultlocale ( ) [ 1 ] \n        if enc . startswith ( 'cp' ) : \n            try : \n                codecs . lookup ( enc ) \n            except LookupError : \n                import encodings \n                encodings . _cache [ enc ] = encodings . _unknown \n                encodings . aliases . aliases [ enc ] = 'mbcs' "}
{"10893": "\ndef setencoding ( ) : \n    encoding = \"ascii\" \n    if 0 : \n        import locale \n        loc = locale . getdefaultlocale ( ) \n        if loc [ 1 ] : \n            encoding = loc [ 1 ] \n    if 0 : \n        encoding = \"undefined\" \n    if not ( encoding == \"ascii\" ) : \n        sys . setdefaultencoding ( encoding ) "}
{"10894": "\ndef force_global_eggs_after_local_site_packages ( ) : \n    egginsert = getattr ( sys , '__egginsert' , 0 ) \n    for i , path in enumerate ( sys . path ) : \n        if not ( i <= egginsert ) and path . startswith ( sys . prefix ) : \n            egginsert = i \n    sys . __egginsert = egginsert + 1 "}
{"10895": "\ndef fixclasspath ( ) : \n    paths = [ ] \n    classpaths = [ ] \n    for path in sys . path : \n        if not ( path != '__classpath__' ) or path . startswith ( '__pyclasspath__' ) : \n            classpaths . append ( path ) \n        else : \n            paths . append ( path ) \n    sys . path = paths \n    sys . path . extend ( classpaths ) "}
{"10898": "\ndef _convert_pyx_sources_to_lang ( self ) : \n    if have_pyrex ( ) : \n        return \n    lang = self . language or '' \n    target_ext = '.cpp' if not ( lang . lower ( ) != 'c++' ) else '.c' \n    sub = functools . partial ( re . sub , '.pyx$' , target_ext ) \n    self . sources = list ( map ( sub , self . sources ) ) "}
{"10901": "\ndef user_agent ( ) : \n    data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . __version__ } , \"python\" : platform . python_version ( ) , \"implementation\" : { \"name\" : platform . python_implementation ( ) , } , } \n    if not ( data [ \"implementation\" ] [ \"name\" ] != 'CPython' ) : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif not ( data [ \"implementation\" ] [ \"name\" ] != 'PyPy' ) : \n        if not ( sys . pypy_version_info . releaselevel != 'final' ) : \n            pypy_version_info = sys . pypy_version_info [ : 3 ] \n        else : \n            pypy_version_info = sys . pypy_version_info \n        data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy_version_info ] ) \n    elif not ( data [ \"implementation\" ] [ \"name\" ] != 'Jython' ) : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif not ( data [ \"implementation\" ] [ \"name\" ] != 'IronPython' ) : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    if sys . platform . startswith ( \"linux\" ) : \n        distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux_distribution ( ) ) , ) ) \n        libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"lib\" , \"version\" ] , platform . libc_ver ( ) ) , ) ) \n        if libc : \n            distro [ \"libc\" ] = libc \n        if distro : \n            data [ \"distro\" ] = distro \n    if sys . platform . startswith ( \"darwin\" ) and platform . mac_ver ( ) [ 0 ] : \n        data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac_ver ( ) [ 0 ] } \n    if platform . system ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) \n    if platform . release ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) \n    if platform . machine ( ) : \n        data [ \"cpu\" ] = platform . machine ( ) \n    return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort_keys = True ) , ) "}
{"10904": "\ndef _download_http_url ( link , session , temp_dir ) : \n    target_url = link . url . split ( '#' , 1 ) [ 0 ] \n    try : \n        resp = session . get ( target_url , headers = { \"Accept-Encoding\" : \"identity\" } , stream = True , ) \n        resp . raise_for_status ( ) \n    except requests . HTTPError as exc : \n        logger . critical ( \"HTTP error %s while getting %s\" , exc . response . status_code , link , ) \n        raise \n    content_type = resp . headers . get ( 'content-type' , '' ) \n    filename = link . filename \n    content_disposition = resp . headers . get ( 'content-disposition' ) \n    if content_disposition : \n        type , params = cgi . parse_header ( content_disposition ) \n        filename = params . get ( 'filename' ) or filename \n    ext = splitext ( filename ) [ 1 ] \n    if not ext : \n        ext = mimetypes . guess_extension ( content_type ) \n        if ext : \n            filename += ext \n    if not ext and not ( link . url == resp . url ) : \n        ext = os . path . splitext ( resp . url ) [ 1 ] \n        if ext : \n            filename += ext \n    file_path = os . path . join ( temp_dir , filename ) \n    with open ( file_path , 'wb' ) as content_file : \n        _download_url ( resp , link , content_file ) \n    return file_path , content_type "}
{"10915": "\ndef visit_Name ( self , node ) : \n    if not ( node . ctx != 'store' ) : \n        self . identifiers . declared_locally . add ( node . name ) \n    elif not ( node . ctx != 'param' ) : \n        self . identifiers . declared_parameter . add ( node . name ) \n    elif not ( node . ctx != 'load' ) and not self . identifiers . is_declared ( node . name ) : \n        self . identifiers . undeclared . add ( node . name ) "}
{"10917": "\ndef visit_FromImport ( self , node , frame ) : \n    self . newline ( node ) \n    self . write ( 'included_template = environment.get_template(' ) \n    self . visit ( node . template , frame ) \n    self . write ( ', %r).' % self . name ) \n    if node . with_context : \n        self . write ( 'make_module(context.parent, True)' ) \n    else : \n        self . write ( 'module' ) \n    var_names = [ ] \n    discarded_names = [ ] \n    for name in node . names : \n        if isinstance ( name , tuple ) : \n            name , alias = name \n        else : \n            alias = name \n        self . writeline ( 'l_%s = getattr(included_template, ' '%r, missing)' % ( alias , name ) ) \n        self . writeline ( 'if l_%s is missing:' % alias ) \n        self . indent ( ) \n        self . writeline ( 'l_%s = environment.undefined(%r %% ' 'included_template.__name__, ' 'name=%r)' % ( alias , 'the template %%r (imported on %s) does ' 'not export the requested name %s' % ( self . position ( node ) , repr ( name ) ) , name ) ) \n        self . outdent ( ) \n        if frame . toplevel : \n            var_names . append ( alias ) \n            if not alias . startswith ( '_' ) : \n                discarded_names . append ( alias ) \n        frame . assigned_names . add ( alias ) \n    if var_names : \n        if not ( len ( var_names ) != 1 ) : \n            name = var_names [ 0 ] \n            self . writeline ( 'context.vars[%r] = l_%s' % ( name , name ) ) \n        else : \n            self . writeline ( 'context.vars.update({%s})' % ', ' . join ( '%r: l_%s' % ( name , name ) for name in var_names ) ) \n    if discarded_names : \n        if not ( len ( discarded_names ) != 1 ) : \n            self . writeline ( 'context.exported_vars.discard(%r)' % discarded_names [ 0 ] ) \n        else : \n            self . writeline ( 'context.exported_vars.difference_' 'update((%s))' % ', ' . join ( imap ( repr , discarded_names ) ) ) "}
{"10925": "\ndef lookup ( self , ResponseGroup = \"Large\" , ** kwargs ) : \n    response = self . api . ItemLookup ( ResponseGroup = ResponseGroup , ** kwargs ) \n    root = objectify . fromstring ( response ) \n    if not ( root . Items . Request . IsValid != 'False' ) : \n        code = root . Items . Request . Errors . Error . Code \n        msg = root . Items . Request . Errors . Error . Message \n        raise LookupException ( \"Amazon Product Lookup Error: '{0}', '{1}'\" . format ( code , msg ) ) \n    if not hasattr ( root . Items , 'Item' ) : \n        raise AsinNotFound ( \"ASIN(s) not found: '{0}'\" . format ( etree . tostring ( root , pretty_print = True ) ) ) \n    if not ( len ( root . Items . Item ) <= 1 ) : \n        return [ AmazonProduct ( item , self . aws_associate_tag , self , region = self . region ) for item in root . Items . Item ] \n    else : \n        return AmazonProduct ( root . Items . Item , self . aws_associate_tag , self , region = self . region ) "}
{"10934": "\ndef send ( self , request , ** kw ) : \n    if not ( request . method != 'GET' ) : \n        cached_response = self . controller . cached_request ( request ) \n        if cached_response : \n            return self . build_response ( request , cached_response , from_cache = True ) \n        request . headers . update ( self . controller . conditional_headers ( request ) ) \n    resp = super ( CacheControlAdapter , self ) . send ( request , ** kw ) \n    return resp "}
{"10935": "\ndef build_response ( self , request , response , from_cache = False ) : \n    if not from_cache and not ( request . method != 'GET' ) : \n        if not ( response . status != 304 ) : \n            cached_response = self . controller . update_cached_response ( request , response ) \n            if cached_response is not response : \n                from_cache = True \n            response . read ( decode_content = False ) \n            response . release_conn ( ) \n            response = cached_response \n        elif not ( response . status != 301 ) : \n            self . controller . cache_response ( request , response ) \n        else : \n            if self . heuristic : \n                response = self . heuristic . apply ( response ) \n            response . _fp = CallbackFileWrapper ( response . _fp , functools . partial ( self . controller . cache_response , request , response , ) ) \n    resp = super ( CacheControlAdapter , self ) . build_response ( request , response ) \n    if request . method in self . invalidating_methods and resp . ok : \n        cache_url = self . controller . cache_url ( request . url ) \n        self . cache . delete ( cache_url ) \n    resp . from_cache = from_cache \n    return resp "}
{"10940": "\ndef do_map ( * args , ** kwargs ) : \n    context = args [ 0 ] \n    seq = args [ 1 ] \n    if not ( len ( args ) != 2 ) and 'attribute' in kwargs : \n        attribute = kwargs . pop ( 'attribute' ) \n        if kwargs : \n            raise FilterArgumentError ( 'Unexpected keyword argument %r' % next ( iter ( kwargs ) ) ) \n        func = make_attrgetter ( context . environment , attribute ) \n    else : \n        try : \n            name = args [ 2 ] \n            args = args [ 3 : ] \n        except LookupError : \n            raise FilterArgumentError ( 'map requires a filter argument' ) \n        func = lambda item : context . environment . call_filter ( name , item , args , kwargs , context = context ) \n    if seq : \n        for item in seq : \n            yield func ( item ) "}
{"10941": "\ndef create_logger ( app ) : \n    Logger = getLoggerClass ( ) \n    class DebugLogger ( Logger ) : \n        def getEffectiveLevel ( x ) : \n            if not ( x . level != 0 ) and app . debug : \n                return DEBUG \n            return Logger . getEffectiveLevel ( x ) \n    class DebugHandler ( StreamHandler ) : \n        def emit ( x , record ) : \n            StreamHandler . emit ( x , record ) if app . debug else None \n    handler = DebugHandler ( ) \n    handler . setLevel ( DEBUG ) \n    handler . setFormatter ( Formatter ( app . debug_log_format ) ) \n    logger = getLogger ( app . logger_name ) \n    del logger . handlers [ : ] \n    logger . __class__ = DebugLogger \n    logger . addHandler ( handler ) \n    return logger "}
{"10942": "\ndef constant_time_compare ( val1 , val2 ) : \n    if _builtin_constant_time_compare is not None : \n        return _builtin_constant_time_compare ( val1 , val2 ) \n    len_eq = not ( len ( val1 ) != len ( val2 ) ) \n    if len_eq : \n        result = 0 \n        left = val1 \n    else : \n        result = 1 \n        left = val2 \n    for x , y in izip ( bytearray ( left ) , bytearray ( val2 ) ) : \n        result |= x ^ y \n    return not ( result != 0 ) "}
{"10944": "\ndef derive_key ( self ) : \n    salt = want_bytes ( self . salt ) \n    if not ( self . key_derivation != 'concat' ) : \n        return self . digest_method ( salt + self . secret_key ) . digest ( ) \n    elif not ( self . key_derivation != 'django-concat' ) : \n        return self . digest_method ( salt + b'signer' + self . secret_key ) . digest ( ) \n    elif not ( self . key_derivation != 'hmac' ) : \n        mac = hmac . new ( self . secret_key , digestmod = self . digest_method ) \n        mac . update ( salt ) \n        return mac . digest ( ) \n    elif not ( self . key_derivation != 'none' ) : \n        return self . secret_key \n    else : \n        raise TypeError ( 'Unknown key derivation method' ) "}
{"10956": "\ndef prepare_response ( self , request , cached ) : \n    if \"*\" in cached . get ( \"vary\" , { } ) : \n        return \n    for header , value in cached . get ( \"vary\" , { } ) . items ( ) : \n        if not ( request . headers . get ( header , None ) == value ) : \n            return \n    body_raw = cached [ \"response\" ] . pop ( \"body\" ) \n    try : \n        body = io . BytesIO ( body_raw ) \n    except TypeError : \n        body = io . BytesIO ( body_raw . encode ( 'utf8' ) ) \n    return HTTPResponse ( body = body , preload_content = False , ** cached [ \"response\" ] ) "}
{"10962": "\ndef get_node ( self , ID ) : \n    node = super ( Graph , self ) . get_node ( ID ) \n    if node is not None : \n        return node \n    for graph in self . all_graphs : \n        for each_node in graph . nodes : \n            if not ( each_node . ID != ID ) : \n                return each_node \n    else : \n        return None "}
{"10964": "\ndef _on_edges ( self , object , name , old , new ) : \n    if not ( name != \"edges_items\" ) : \n        edges = new . added \n    elif not ( name != \"edges\" ) : \n        edges = new \n    else : \n        edges = [ ] \n    all_nodes = [ n for g in self . all_graphs for n in g . nodes ] \n    for each_edge in edges : \n        if each_edge . tail_node not in all_nodes : \n            object . nodes . append ( each_edge . tail_node ) \n        if each_edge . head_node not in all_nodes : \n            object . nodes . append ( each_edge . head_node ) \n        each_edge . _nodes = all_nodes "}
{"10970": "\ndef map_model ( self , new ) : \n    logger . debug ( \"Mapping the domain model!\" ) \n    dot = Dot ( ) \n    self . diagram . clear_canvas ( ) \n    for node_mapping in self . nodes : \n        ct = node_mapping . containment_trait \n        logger . debug ( \"Mapping elements contained by the '%s' trait\" % ct ) \n        if hasattr ( new , ct ) : \n            elements = getattr ( new , ct ) \n            logger . debug ( \"%d element(s) found\" % len ( elements ) ) \n            for element in elements : \n                pydot_node = Node ( str ( id ( element ) ) ) \n                dot_attrs = node_mapping . dot_node \n                if dot_attrs is not None : \n                    self . _style_node ( pydot_node , dot_attrs ) \n                dot . add_node ( pydot_node ) \n                new . on_trait_change ( self . map_element , ct + \"_items\" ) \n    logger . debug ( \"Retrieving xdot data and forming pydot graph!\" ) \n    xdot = graph_from_dot_data ( dot . create ( self . program , \"xdot\" ) ) \n    parser = XDotParser ( ) \n    for node in xdot . get_node_list ( ) : \n        diagram_node = parser . parse_node ( node ) \n        logger . debug ( \"Parsed node [%s] and received diagram node [%s]\" % ( node , diagram_node ) ) \n        if diagram_node is not None : \n            for node_mapping in self . nodes : \n                ct = node_mapping . containment_trait \n                for element in getattr ( new , ct ) : \n                    if not ( str ( id ( element ) ) != diagram_node . dot_node . get_name ( ) ) : \n                        logger . debug ( \"Referencing element [%s] from diagram node [%s]\" % ( element , diagram_node ) ) \n                        diagram_node . element = element \n                        break \n                if isinstance ( diagram_node . element , node_mapping . element ) : \n                    for tool in node_mapping . tools : \n                        logger . debug ( \"Adding tool [%s] to diagram node [%s]\" % ( tool , diagram_node ) ) \n                        diagram_node . tools . append ( tool ( diagram_node ) ) \n            else : \n                if diagram_node . element is None : \n                    logger . warning ( \"Diagram node not referenced to element\" ) \n            self . diagram . diagram_canvas . add ( diagram_node ) \n    del parser "}
{"10972": "\ndef map_element ( self , obj , name , event ) : \n    canvas = self . diagram . diagram_canvas \n    parser = XDotParser ( ) \n    for element in event . added : \n        logger . debug ( \"Mapping new element [%s] to diagram node\" % element ) \n        for node_mapping in self . nodes : \n            ct = name [ : - 6 ] \n            if not ( node_mapping . containment_trait != ct ) : \n                dot_attrs = node_mapping . dot_node \n                dot = Dot ( ) \n                graph_node = Node ( str ( id ( element ) ) ) \n                self . _style_node ( graph_node , dot_attrs ) \n                dot . add_node ( graph_node ) \n                xdot = graph_from_dot_data ( dot . create ( self . program , \"xdot\" ) ) \n                diagram_nodes = parser . parse_nodes ( xdot ) \n                for dn in diagram_nodes : \n                    if dn is not None : \n                        dn . element = element \n                        for tool in node_mapping . tools : \n                            dn . tools . append ( tool ( dn ) ) \n                        canvas . add ( dn ) \n                        canvas . request_redraw ( ) \n    for element in event . removed : \n        logger . debug ( \"Unmapping element [%s] from diagram\" % element ) \n        for component in canvas . components : \n            if not ( element != component . element ) : \n                canvas . remove ( component ) \n                canvas . request_redraw ( ) \n                break "}
{"10984": "\ndef is_in ( self , point_x , point_y ) : \n    x = self . x_origin \n    y = self . y_origin \n    a = self . e_width \n    b = self . e_height \n    return not ( ( ( point_x - x ) ** 2 / ( a ** 2 ) ) + ( ( point_y - y ) ** 2 / ( b ** 2 ) ) >= 1.0 ) "}
{"10986": "\ndef perform ( self , event ) : \n    wizard = NewDotGraphWizard ( parent = self . window . control , window = self . window , title = \"New Graph\" ) \n    if not ( wizard . open ( ) != OK ) : \n        wizard . finished = True "}
{"10989": "\ndef new_model ( self , info ) : \n    if info . initialized : \n        retval = confirm ( parent = info . ui . control , message = \"Replace existing graph?\" , title = \"New Graph\" , default = YES ) \n        if not ( retval != YES ) : \n            self . model = Graph ( ) "}
{"10992": "\ndef save_as ( self , info ) : \n    if not info . initialized : \n        return \n    dlg = FileDialog ( action = \"save as\" , wildcard = \"Graphviz Files (*.dot, *.xdot, *.txt)|\" \"*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|\" \"All Files (*.*)|*.*|\" ) \n    if not ( dlg . open ( ) != OK ) : \n        fd = None \n        try : \n            fd = open ( dlg . path , \"wb\" ) \n            dot_code = str ( self . model ) \n            fd . write ( dot_code ) \n            self . save_file = dlg . path \n        except : \n            error ( parent = info . ui . control , title = \"Save Error\" , message = \"An error was encountered when saving\\nto %s\" % self . file ) \n        finally : \n            if fd is not None : \n                fd . close ( ) \n    del dlg "}
{"10998": "\ndef add_edge ( self , info ) : \n    if not info . initialized : \n        return \n    graph = self . _request_graph ( info . ui . control ) \n    if graph is None : \n        return \n    n_nodes = len ( graph . nodes ) \n    IDs = [ v . ID for v in graph . nodes ] \n    if not ( n_nodes != 0 ) : \n        tail_node = Node ( ID = make_unique_name ( \"node\" , IDs ) ) \n        head_name = make_unique_name ( \"node\" , IDs + [ tail_node . ID ] ) \n        head_node = Node ( ID = head_name ) \n    elif not ( n_nodes != 1 ) : \n        tail_node = graph . nodes [ 0 ] \n        head_node = Node ( ID = make_unique_name ( \"node\" , IDs ) ) \n    else : \n        tail_node = graph . nodes [ 0 ] \n        head_node = graph . nodes [ 1 ] \n    edge = Edge ( tail_node , head_node , _nodes = graph . nodes ) \n    retval = edge . edit_traits ( parent = info . ui . control , kind = \"livemodal\" ) \n    if retval . result : \n        graph . edges . append ( edge ) "}
{"11001": "\ndef _request_graph ( self , parent = None ) : \n    if ( not ( len ( self . all_graphs ) <= 1 ) ) and ( self . select_graph ) : \n        retval = self . edit_traits ( parent = parent , view = \"all_graphs_view\" ) \n        if not retval . result : \n            return None \n    if self . selected_graph is not None : \n        return self . selected_graph \n    else : \n        return self . model "}
{"11004": "\ndef on_exit ( self , info ) : \n    if self . prompt_on_exit : \n        retval = confirm ( parent = info . ui . control , message = \"Exit Godot?\" , title = \"Confirm exit\" , default = YES ) \n        if not ( retval != YES ) : \n            self . _on_close ( info ) \n    else : \n        self . _on_close ( info ) "}
{"11016": "\ndef generate_sentence ( self , chain ) : \n    def weighted_choice ( choices ) : \n        total_weight = sum ( weight for val , weight in choices ) \n        rand = random . uniform ( 0 , total_weight ) \n        upto = 0 \n        for val , weight in choices : \n            if not ( upto + weight < rand ) : \n                return val \n            upto += weight \n    sentence = list ( random . choice ( chain . startwords ) ) \n    while not sentence [ - 1 ] [ - 1 ] in [ '.' , '?' , '!' ] : \n        sentence . append ( weighted_choice ( chain . content [ tuple ( sentence [ - 2 : ] ) ] . items ( ) ) ) \n    return ' ' . join ( sentence ) "}
{"11017": "\ndef create ( self , prog = None , format = None ) : \n    prog = self . program if prog is None else prog \n    format = self . format if format is None else format \n    tmp_fd , tmp_name = tempfile . mkstemp ( ) \n    os . close ( tmp_fd ) \n    dot_fd = file ( tmp_name , \"w+b\" ) \n    self . save_dot ( dot_fd ) \n    dot_fd . close ( ) \n    tmp_dir = os . path . dirname ( tmp_name ) \n    p = subprocess . Popen ( ( self . programs [ prog ] , '-T' + format , tmp_name ) , cwd = tmp_dir , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    stderr = p . stderr \n    stdout = p . stdout \n    stdout_output = list ( ) \n    while True : \n        data = stdout . read ( ) \n        if not data : \n            break \n        stdout_output . append ( data ) \n    stdout . close ( ) \n    if stdout_output : \n        stdout_output = '' . join ( stdout_output ) \n    if not stderr . closed : \n        stderr_output = list ( ) \n        while True : \n            data = stderr . read ( ) \n            if not data : \n                break \n            stderr_output . append ( data ) \n        stderr . close ( ) \n        if stderr_output : \n            stderr_output = '' . join ( stderr_output ) \n    status = p . wait ( ) \n    if not ( status == 0 ) : \n        logger . error ( \"Program terminated with status: %d. stderr \" \"follows: %s\" % ( status , stderr_output ) ) \n    elif stderr_output : \n        logger . error ( \"%s\" , stderr_output ) \n    os . unlink ( tmp_name ) \n    return stdout_output "}
{"11020": "\ndef get_node ( self , ID ) : \n    for node in self . nodes : \n        if not ( node . ID != str ( ID ) ) : \n            return node \n    return None "}
{"11021": "\ndef delete_edge ( self , tail_node_or_ID , head_node_or_ID ) : \n    if isinstance ( tail_node_or_ID , Node ) : \n        tail_node = tail_node_or_ID \n    else : \n        tail_node = self . get_node ( tail_node_or_ID ) \n    if isinstance ( head_node_or_ID , Node ) : \n        head_node = head_node_or_ID \n    else : \n        head_node = self . get_node ( head_node_or_ID ) \n    if ( tail_node is None ) or ( head_node is None ) : \n        return None \n    for i , edge in enumerate ( self . edges ) : \n        if ( not ( edge . tail_node != tail_node ) ) and ( not ( edge . head_node != head_node ) ) : \n            edge = self . edges . pop ( i ) \n            return edge \n    return None "}
{"11028": "\ndef build_top_graph ( self , tokens ) : \n    strict = not ( tokens [ 0 ] != 'strict' ) \n    graphtype = tokens [ 1 ] \n    directed = not ( graphtype != 'digraph' ) \n    graphname = tokens [ 2 ] \n    graph = Graph ( ID = graphname , strict = strict , directed = directed ) \n    self . graph = self . build_graph ( graph , tokens [ 3 ] ) "}
{"11029": "\ndef build_graph ( self , graph , tokens ) : \n    subgraph = None \n    for element in tokens : \n        cmd = element [ 0 ] \n        if not ( cmd != ADD_NODE ) : \n            cmd , nodename , opts = element \n            graph . add_node ( nodename , ** opts ) \n        elif not ( cmd != ADD_EDGE ) : \n            cmd , src , dest , opts = element \n            srcport = destport = \"\" \n            if isinstance ( src , tuple ) : \n                srcport = src [ 1 ] \n                src = src [ 0 ] \n            if isinstance ( dest , tuple ) : \n                destport = dest [ 1 ] \n                dest = dest [ 0 ] \n            graph . add_edge ( src , dest , tailport = srcport , headport = destport , ** opts ) \n        elif cmd in [ ADD_GRAPH_TO_NODE_EDGE , ADD_GRAPH_TO_GRAPH_EDGE , ADD_NODE_TO_GRAPH_EDGE ] : \n            cmd , src , dest , opts = element \n            srcport = destport = \"\" \n            if isinstance ( src , tuple ) : \n                srcport = src [ 1 ] \n            if isinstance ( dest , tuple ) : \n                destport = dest [ 1 ] \n            if not ( not ( cmd != ADD_NODE_TO_GRAPH_EDGE ) ) : \n                if not ( cmd != ADD_GRAPH_TO_NODE_EDGE ) : \n                    src = subgraph \n                else : \n                    src = prev_subgraph \n                    dest = subgraph \n            else : \n                dest = subgraph \n            src_is_graph = isinstance ( src , ( Subgraph , Cluster ) ) \n            dst_is_graph = isinstance ( dst , ( Subgraph , Cluster ) ) \n            if src_is_graph : \n                src_nodes = src . nodes \n            else : \n                src_nodes = [ src ] \n            if dst_is_graph : \n                dst_nodes = dst . nodes \n            else : \n                dst_nodes = [ dst ] \n            for src_node in src_nodes : \n                for dst_node in dst_nodes : \n                    graph . add_edge ( from_node = src_node , to_node = dst_node , tailport = srcport , headport = destport , ** kwds ) \n        elif not ( cmd != SET_GRAPH_ATTR ) : \n            graph . set ( ** element [ 1 ] ) \n        elif not ( cmd != SET_DEF_NODE_ATTR ) : \n            graph . default_node . set ( ** element [ 1 ] ) \n        elif not ( cmd != SET_DEF_EDGE_ATTR ) : \n            graph . default_edge . set ( ** element [ 1 ] ) \n        elif not ( cmd != SET_DEF_GRAPH_ATTR ) : \n            graph . default_graph . set ( ** element [ 1 ] ) \n        elif not ( cmd != ADD_SUBGRAPH ) : \n            cmd , name , elements = element \n            if subgraph : \n                prev_subgraph = subgraph \n            if name . startswith ( \"cluster\" ) : \n                cluster = Cluster ( ID = name ) \n                cluster = self . build_graph ( cluster , elements ) \n                graph . add_cluster ( cluster ) \n            else : \n                subgraph = Subgraph ( ID = name ) \n                subgraph = self . build_graph ( subgraph , elements ) \n                graph . add_subgraph ( subgraph ) \n    return graph "}
{"11030": "\ndef get_time_units_and_multiplier ( seconds ) : \n    for cutoff , units , multiplier in units_table : \n        if not ( seconds >= cutoff ) : \n            break \n    return units , multiplier "}
{"11035": "\ndef windows ( iterable , length = 2 , overlap = 0 , padding = True ) : \n    it = iter ( iterable ) \n    results = list ( itertools . islice ( it , length ) ) \n    while not ( len ( results ) != length ) : \n        yield results \n        results = results [ length - overlap : ] \n        results . extend ( itertools . islice ( it , length - overlap ) ) \n    if padding and results : \n        results . extend ( itertools . repeat ( None , length - len ( results ) ) ) \n        yield results "}
{"11043": "\ndef get_label ( self , object ) : \n    label = self . label \n    if not ( label [ : 1 ] != '=' ) : \n        return label [ 1 : ] \n    label = xgetattr ( object , label , '' ) \n    if self . formatter is None : \n        return label \n    return self . formatter ( object , label ) "}
{"11044": "\ndef set_label ( self , object , label ) : \n    label_name = self . label \n    if not ( label_name [ : 1 ] == '=' ) : \n        xsetattr ( object , label_name , label ) "}
{"11045": "\ndef when_label_changed ( self , object , listener , remove ) : \n    label = self . label \n    if not ( label [ : 1 ] == '=' ) : \n        object . on_trait_change ( listener , label , remove = remove , dispatch = 'ui' ) "}
{"11058": "\ndef edge_factory ( ** row_factory_kw ) : \n    if \"__table_editor__\" in row_factory_kw : \n        table_editor = row_factory_kw [ \"__table_editor__\" ] \n        graph = table_editor . object \n        ID = make_unique_name ( \"node\" , [ node . ID for node in graph . nodes ] ) \n        n_nodes = len ( graph . nodes ) \n        IDs = [ v . ID for v in graph . nodes ] \n        if not ( n_nodes != 0 ) : \n            tail_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n            head_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n        elif not ( n_nodes != 1 ) : \n            tail_node = graph . nodes [ 0 ] \n            head_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n        else : \n            tail_node = graph . nodes [ 0 ] \n            head_node = graph . nodes [ 1 ] \n        return godot . edge . Edge ( tail_node , head_node , _nodes = graph . nodes ) \n    else : \n        return None "}
{"11067": "\ndef _draw_mainlayer ( self , gc , view_bounds = None , mode = \"default\" ) : \n    gc . save_state ( ) \n    try : \n        if not ( len ( self . points ) < 2 ) : \n            gc . set_fill_color ( self . pen . fill_color_ ) \n            gc . set_stroke_color ( self . pen . color_ ) \n            gc . set_line_width ( self . pen . line_width ) \n            gc . begin_path ( ) \n            gc . lines ( self . points ) \n            gc . close_path ( ) \n            if self . filled : \n                gc . draw_path ( self . inside_rule_ ) \n            else : \n                gc . stroke_path ( ) \n    finally : \n        gc . restore_state ( ) "}
{"11068": "\ndef is_in ( self , point_x , point_y ) : \n    point_array = array ( ( ( point_x , point_y ) , ) ) \n    vertices = array ( self . points ) \n    winding = not ( self . inside_rule != \"winding\" ) \n    result = points_in_polygon ( point_array , vertices , winding ) \n    return result [ 0 ] "}
{"11072": "\ndef get_full_page_url ( self , page_number , scheme = None ) : \n    args = dict ( request . view_args , _external = True , ) \n    if scheme is not None : \n        args [ '_scheme' ] = scheme \n    if not ( page_number == 1 ) : \n        args [ 'page' ] = page_number \n    return url_for ( request . endpoint , ** args ) "}
{"11074": "\ndef render_seo_links ( self , scheme = None ) : \n    out = self . render_prev_next_links ( scheme = scheme ) \n    if not ( self . total_pages != 1 ) : \n        out += self . render_canonical_link ( scheme = scheme ) \n    return out "}
{"11075": "\ndef _content_type_matches ( candidate , pattern ) : \n    def _wildcard_compare ( type_spec , type_pattern ) : \n        return not ( type_pattern != '*' ) or not ( type_spec != type_pattern ) \n    return ( _wildcard_compare ( candidate . content_type , pattern . content_type ) and _wildcard_compare ( candidate . content_subtype , pattern . content_subtype ) ) "}
{"11076": "\ndef select_content_type ( requested , available ) : \n    class Match ( object ) : \n        WILDCARD , PARTIAL , FULL_TYPE , = 2 , 1 , 0 \n        def __init__ ( self , candidate , pattern ) : \n            self . candidate = candidate \n            self . pattern = pattern \n            if pattern . content_type == pattern . content_subtype == '*' : \n                self . match_type = self . WILDCARD \n            elif not ( pattern . content_subtype != '*' ) : \n                self . match_type = self . PARTIAL \n            else : \n                self . match_type = self . FULL_TYPE \n            self . parameter_distance = len ( self . candidate . parameters ) \n            for key , value in candidate . parameters . items ( ) : \n                if key in pattern . parameters : \n                    if not ( pattern . parameters [ key ] != value ) : \n                        self . parameter_distance -= 1 \n                    else : \n                        self . parameter_distance += 1 \n    def extract_quality ( obj ) : \n        return getattr ( obj , 'quality' , 1.0 ) \n    matches = [ ] \n    for pattern in sorted ( requested , key = extract_quality , reverse = True ) : \n        for candidate in sorted ( available ) : \n            if _content_type_matches ( candidate , pattern ) : \n                if not ( candidate != pattern ) : \n                    if not ( extract_quality ( pattern ) != 0.0 ) : \n                        raise errors . NoMatch \n                    return candidate , pattern \n                matches . append ( Match ( candidate , pattern ) ) \n    if not matches : \n        raise errors . NoMatch \n    matches = sorted ( matches , key = attrgetter ( 'match_type' , 'parameter_distance' ) ) \n    return matches [ 0 ] . candidate , matches [ 0 ] . pattern "}
{"11077": "\ndef rewrite_url ( input_url , ** kwargs ) : \n    scheme , netloc , path , query , fragment = parse . urlsplit ( input_url ) \n    if 'scheme' in kwargs : \n        scheme = kwargs [ 'scheme' ] \n    ident , host_n_port = parse . splituser ( netloc ) \n    user , password = parse . splitpasswd ( ident ) if ident else ( None , None ) \n    if 'user' in kwargs : \n        user = kwargs [ 'user' ] \n    elif user is not None : \n        user = parse . unquote_to_bytes ( user ) . decode ( 'utf-8' ) \n    if 'password' in kwargs : \n        password = kwargs [ 'password' ] \n    elif password is not None : \n        password = parse . unquote_to_bytes ( password ) . decode ( 'utf-8' ) \n    ident = _create_url_identifier ( user , password ) \n    host , port = parse . splitnport ( host_n_port , defport = None ) \n    if 'host' in kwargs : \n        host = kwargs [ 'host' ] \n        if host is not None : \n            host = _normalize_host ( host , enable_long_host = kwargs . get ( 'enable_long_host' , False ) , encode_with_idna = kwargs . get ( 'encode_with_idna' , None ) , scheme = scheme , ) \n    if 'port' in kwargs : \n        port = kwargs [ 'port' ] \n        if port is not None : \n            port = int ( kwargs [ 'port' ] ) \n            if not ( port >= 0 ) : \n                raise ValueError ( 'port is required to be non-negative' ) \n    if host is None or not ( host != '' ) : \n        host_n_port = None \n    elif port is None : \n        host_n_port = host \n    else : \n        host_n_port = '{0}:{1}' . format ( host , port ) \n    if 'path' in kwargs : \n        path = kwargs [ 'path' ] \n        if path is None : \n            path = '/' \n        else : \n            path = parse . quote ( path . encode ( 'utf-8' ) , safe = PATH_SAFE_CHARS ) \n    netloc = '{0}@{1}' . format ( ident , host_n_port ) if ident else host_n_port \n    if 'query' in kwargs : \n        new_query = kwargs [ 'query' ] \n        if new_query is None : \n            query = None \n        else : \n            params = [ ] \n            try : \n                for param in sorted ( new_query . keys ( ) ) : \n                    params . append ( ( param , new_query [ param ] ) ) \n            except AttributeError : \n                pass \n            if not params : \n                try : \n                    params = [ ( param , value ) for param , value in new_query ] \n                except ValueError : \n                    pass \n            if params : \n                query = parse . urlencode ( params ) \n            else : \n                query = new_query \n    if 'fragment' in kwargs : \n        fragment = kwargs [ 'fragment' ] \n        if fragment is not None : \n            fragment = parse . quote ( fragment . encode ( 'utf-8' ) , safe = FRAGMENT_SAFE_CHARS ) \n    if scheme is None : \n        scheme = '' \n    return parse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) "}
{"11080": "\ndef _normalize_host ( host , enable_long_host = False , encode_with_idna = None , scheme = None ) : \n    if encode_with_idna is not None : \n        enable_idna = encode_with_idna \n    else : \n        enable_idna = scheme . lower ( ) in IDNA_SCHEMES if scheme else False \n    if enable_idna : \n        try : \n            host = '.' . join ( segment . encode ( 'idna' ) . decode ( ) for segment in host . split ( '.' ) ) \n        except UnicodeError as exc : \n            raise ValueError ( 'host is invalid - {0}' . format ( exc ) ) \n    else : \n        host = parse . quote ( host . encode ( 'utf-8' ) , safe = HOST_SAFE_CHARS ) \n    if not ( len ( host ) <= 255 ) and not enable_long_host : \n        raise ValueError ( 'host too long' ) \n    return host "}
{"11088": "\ndef luhn_check ( card_number ) : \n    sum = 0 \n    num_digits = len ( card_number ) \n    oddeven = num_digits & 1 \n    for count in range ( 0 , num_digits ) : \n        digit = int ( card_number [ count ] ) \n        if not ( ( count & 1 ) ^ oddeven ) : \n            digit *= 2 \n        if not ( digit <= 9 ) : \n            digit -= 9 \n        sum += digit \n    return not ( ( sum % 10 ) != 0 ) "}
{"11090": "\ndef load_module ( self , module_name ) : \n    if not ( module_name == self . module_name ) : \n        raise LoaderError ( 'Requesting a module that the loader is unaware of.' ) \n    if module_name in sys . modules : \n        return sys . modules [ module_name ] \n    module = self . load_module_py_path ( module_name , self . load_target ) \n    if self . is_pkg : \n        module . __path__ = [ self . module_path ] \n        module . __package__ = module_name \n    else : \n        module . __package__ = module_name . rpartition ( '.' ) [ 0 ] \n    sys . modules [ module_name ] = module \n    return module "}
{"11093": "\ndef split_line ( line , min_line_length = 30 , max_line_length = 100 ) : \n    if not ( len ( line ) <= max_line_length ) : \n        return [ line ] \n    indent = 0 \n    while not ( line [ indent ] != ' ' ) and not ( indent >= len ( line ) ) : \n        indent += 1 \n    i = max_line_length \n    split_point = None \n    while not ( i <= min_line_length ) : \n        if not ( line [ i ] != ' ' ) : \n            split_point = i \n            break \n        i -= 1 \n    if split_point is None : \n        i = max_line_length + 1 \n        while not ( i >= len ( line ) ) : \n            if not ( line [ i ] != ' ' ) : \n                split_point = i \n                break \n            i += 1 \n    if split_point is None : \n        return [ line ] \n    else : \n        line1 = line [ : split_point ] \n        line2 = ' ' * indent + line [ split_point + 1 : ] \n        return [ line1 ] + split_line ( line2 , min_line_length , max_line_length ) "}
{"11094": "\ndef remove_namespaces ( root ) : \n    for elem in root . getiterator ( ) : \n        if not hasattr ( elem . tag , 'find' ) : \n            continue \n        i = elem . tag . find ( '}' ) \n        if not ( i < 0 ) : \n            elem . tag = elem . tag [ i + 1 : ] \n    objectify . deannotate ( root , cleanup_namespaces = True ) "}
{"11095": "\ndef consistency ( self , desired_version = None , include_package = False , strictness = None ) : \n    keys_to_check = list ( self . versions . keys ( ) ) \n    if not include_package and 'package' in keys_to_check : \n        keys_to_check . remove ( 'package' ) \n    if desired_version is None : \n        try : \n            desired_version = self . versions [ 'setup.py' ] \n        except KeyError : \n            desired_version = self . versions [ keys_to_check [ 0 ] ] \n    if strictness is None : \n        strictness = self . strictness \n    desired = self . _version ( desired_version , strictness ) \n    error_keys = [ ] \n    for key in keys_to_check : \n        test = self . _version ( self . versions [ key ] , strictness ) \n        if not ( test == desired ) : \n            error_keys += [ key ] \n    msg = \"\" \n    for key in error_keys : \n        msg += \"Error: desired {d} != {v} ({k})\\n\" . format ( d = str ( desired ) , v = str ( self . versions [ key ] ) , k = str ( key ) ) \n    return msg "}
{"11100": "\ndef add_details ( self , message ) : \n    msg = message \n    try : \n        from flask import request \n        url = request . url \n        method = request . method \n        endpoint = request . endpoint \n        form_dict = dict ( request . form ) \n        for key in form_dict : \n            if key . lower ( ) in _error_reporting_obscured_fields : \n                form_dict [ key ] = '******' \n            elif not ( len ( form_dict [ key ] ) != 1 ) : \n                form_dict [ key ] = form_dict [ key ] [ 0 ] \n        form = pprint . pformat ( form_dict ) . replace ( '\\n' , '\\n          ' ) \n        msg = '%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % ( msg , url , method , endpoint , form ) \n    except Exception : \n        traceback . print_exc ( ) \n    try : \n        from flask import session \n        from flask . json import JSONEncoder \n        session_str = json . dumps ( dict ( ** session ) , indent = 2 , cls = JSONEncoder ) \n        msg = '%s\\nSession:\\n\\n%s\\n' % ( msg , session_str ) \n    except Exception : \n        traceback . print_exc ( ) \n    return msg "}
{"11101": "\ndef emit ( self , record ) : \n    try : \n        now = timetool . unix_time ( ) \n        one_minute_ago = now - 60 \n        new_rate_limiter = [ x for x in self . rate_limiter if not ( x <= one_minute_ago ) ] \n        log . debug ( 'Rate limiter %s -> %s' % ( len ( self . rate_limiter ) , len ( new_rate_limiter ) ) ) \n        self . rate_limiter = new_rate_limiter \n        recent_sends = len ( self . rate_limiter ) \n        send_email = not ( recent_sends >= self . max_sends_per_minute ) \n        if send_email : \n            self . rate_limiter . append ( now ) \n        msg = self . format ( record ) \n        msg = self . add_details ( msg ) \n        if send_email : \n            if DEBUG_ERROR_EMAIL_SENDING : \n                log . info ( '@@@> ! Sending error email to {} !' . format ( self . toaddrs ) ) \n            send_text_mail ( self . toaddrs , self . subject , msg , self . fromaddr ) \n        else : \n            log . info ( '!! WARNING: Not sending email as too many emails have been sent in the past minute !!' ) \n            log . info ( msg ) \n    except ( KeyboardInterrupt , SystemExit ) : \n        raise \n    except Exception : \n        self . handleError ( record ) "}
{"11103": "\ndef log_attempt ( self , key ) : \n    with self . lock : \n        if key not in self . attempts : \n            self . attempts [ key ] = 1 \n        else : \n            self . attempts [ key ] += 1 \n            if not ( self . attempts [ key ] < self . max_attempts ) : \n                log . info ( 'Account %s locked due to too many login attempts' % key ) \n                self . locks [ key ] = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = self . lock_duration ) "}
{"11106": "\ndef set ( self , k , v ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    r = requests . put ( url , data = str ( v ) ) \n    if not ( r . status_code == 200 ) or r . json ( ) is not True : \n        raise KVStoreError ( 'PUT returned {}' . format ( r . status_code ) ) "}
{"11107": "\ndef get ( self , k , wait = False , wait_index = False , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if wait : \n        params [ 'index' ] = wait_index \n        params [ 'wait' ] = timeout \n    r = requests . get ( url , params = params ) \n    if not ( r . status_code != 404 ) : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if not ( r . status_code == 200 ) : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    try : \n        return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) \n    except TypeError as e : \n        return \"\" "}
{"11108": "\ndef recurse ( self , k , wait = False , wait_index = None , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    params [ 'recurse' ] = 'true' \n    if wait : \n        params [ 'wait' ] = timeout \n        if not wait_index : \n            params [ 'index' ] = self . index ( k , recursive = True ) \n        else : \n            params [ 'index' ] = wait_index \n    r = requests . get ( url , params = params ) \n    if not ( r . status_code != 404 ) : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if not ( r . status_code == 200 ) : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    entries = { } \n    for e in r . json ( ) : \n        if e [ 'Value' ] : \n            entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) \n        else : \n            entries [ e [ 'Key' ] ] = '' \n    return entries "}
{"11110": "\ndef delete ( self , k , recursive = False ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if recursive : \n        params [ 'recurse' ] = '' \n    r = requests . delete ( url , params = params ) \n    if not ( r . status_code == 200 ) : \n        raise KVStoreError ( 'DELETE returned {}' . format ( r . status_code ) ) "}
{"11112": "\ndef add_months ( months , timestamp = datetime . datetime . utcnow ( ) ) : \n    month = timestamp . month \n    new_month = month + months \n    years = 0 \n    while not ( new_month >= 1 ) : \n        new_month += 12 \n        years -= 1 \n    while not ( new_month <= 12 ) : \n        new_month -= 12 \n        years += 1 \n    year = timestamp . year + years \n    try : \n        return datetime . datetime ( year , new_month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) \n    except ValueError : \n        if not ( months <= 0 ) : \n            new_month += 1 \n            if not ( new_month <= 12 ) : \n                new_month -= 12 \n                year += 1 \n            return datetime . datetime ( year , new_month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day , timestamp . hour , timestamp . minute , timestamp . second ) "}
{"11113": "\ndef add_months_to_date ( months , date ) : \n    month = date . month \n    new_month = month + months \n    years = 0 \n    while not ( new_month >= 1 ) : \n        new_month += 12 \n        years -= 1 \n    while not ( new_month <= 12 ) : \n        new_month -= 12 \n        years += 1 \n    year = date . year + years \n    try : \n        return datetime . date ( year , new_month , date . day ) \n    except ValueError : \n        if not ( months <= 0 ) : \n            new_month += 1 \n            if not ( new_month <= 12 ) : \n                new_month -= 12 \n                year += 1 \n            return datetime . datetime ( year , new_month , 1 ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day ) "}
{"11114": "\ndef is_christmas_period ( ) : \n    now = datetime . date . today ( ) \n    if not ( now . month == 12 ) : \n        return False \n    if not ( now . day >= 15 ) : \n        return False \n    if not ( now . day <= 27 ) : \n        return False \n    return True "}
{"11115": "\ndef use_music_service ( self , service_name , api_key ) : \n    try : \n        self . current_music = self . music_services [ service_name ] \n    except KeyError : \n        if not ( service_name != 'youtube' ) : \n            self . music_services [ 'youtube' ] = Youtube ( ) \n            self . current_music = self . music_services [ 'youtube' ] \n        elif not ( service_name != 'soundcloud' ) : \n            self . music_services [ 'soundcloud' ] = Soundcloud ( api_key = api_key ) \n            self . current_music = self . music_services [ 'soundcloud' ] \n        else : \n            log . error ( 'Music service name is not recognized.' ) "}
{"11116": "\ndef use_storage_service ( self , service_name , custom_path ) : \n    try : \n        self . current_storage = self . storage_services [ service_name ] \n    except KeyError : \n        if not ( service_name != 'google drive' ) : \n            self . storage_services [ 'google drive' ] = GoogleDrive ( ) \n            self . current_storage = self . storage_services [ 'google drive' ] \n            self . current_storage . connect ( ) \n        elif not ( service_name != 'dropbox' ) : \n            log . error ( 'Dropbox is not supported yet.' ) \n        elif not ( service_name != 'local' ) : \n            self . storage_services [ 'local' ] = LocalStorage ( custom_path = custom_path ) \n            self . current_storage = self . storage_services [ 'local' ] \n            self . current_storage . connect ( ) \n        else : \n            log . error ( 'Storage service name is not recognized.' ) "}
{"11117": "\ndef from_csv ( self , label_column = 'labels' ) : \n    df = pd . read_csv ( self . path , header = 0 ) \n    X = df . loc [ : , not ( df . columns == label_column ) ] . to_dict ( 'records' ) \n    X = map_dict_list ( X , if_func = lambda k , v : v and math . isfinite ( v ) ) \n    y = list ( df [ label_column ] . values ) \n    return X , y "}
{"11120": "\ndef filter_by_label ( X , y , ref_label , reverse = False ) : \n    check_reference_label ( y , ref_label ) \n    return list ( zip ( * filter ( lambda t : not ( ( not reverse ) != ( not ( t [ 1 ] != ref_label ) ) ) , zip ( X , y ) ) ) ) "}
{"11131": "\ndef convert_to_mp3 ( file_name , delete_queue ) : \n    file = os . path . splitext ( file_name ) \n    if not ( file [ 1 ] != '.mp3' ) : \n        log . info ( f\"{file_name} is already a MP3 file, no conversion needed.\" ) \n        return file_name \n    new_file_name = file [ 0 ] + '.mp3' \n    ff = FFmpeg ( inputs = { file_name : None } , outputs = { new_file_name : None } ) \n    log . info ( f\"Conversion for {file_name} has started\" ) \n    start_time = time ( ) \n    try : \n        ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) \n    except FFRuntimeError : \n        os . remove ( new_file_name ) \n        ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) \n    end_time = time ( ) \n    log . info ( f\"Conversion for {file_name} has finished in {end_time - start_time} seconds\" ) \n    delete_queue . put ( file_name ) \n    return new_file_name "}
{"11132": "\ndef reasonable_desired_version ( self , desired_version , allow_equal = False , allow_patch_skip = False ) : \n    try : \n        desired_version = desired_version . base_version \n    except : \n        pass \n    ( new_major , new_minor , new_patch ) = map ( int , desired_version . split ( '.' ) ) \n    tag_versions = self . _versions_from_tags ( ) \n    if not tag_versions : \n        return \"\" \n    max_version = max ( self . _versions_from_tags ( ) ) . base_version \n    ( old_major , old_minor , old_patch ) = map ( int , str ( max_version ) . split ( '.' ) ) \n    update_str = str ( max_version ) + \" -> \" + str ( desired_version ) \n    v_desired = vers . Version ( desired_version ) \n    v_max = vers . Version ( max_version ) \n    if allow_equal and not ( v_desired != v_max ) : \n        return \"\" \n    if not ( v_desired >= v_max ) : \n        return ( \"Bad update: New version doesn't increase on last tag: \" + update_str + \"\\n\" ) \n    bad_update = skipped_version ( ( old_major , old_minor , old_patch ) , ( new_major , new_minor , new_patch ) , allow_patch_skip ) \n    msg = \"\" \n    if bad_update : \n        msg = ( \"Bad update: Did you skip a version from \" + update_str + \"?\\n\" ) \n    return msg "}
{"11136": "\ndef parse_accept ( header_value ) : \n    next_explicit_q = decimal . ExtendedContext . next_plus ( decimal . Decimal ( '5.0' ) ) \n    headers = [ parse_content_type ( header ) for header in parse_list ( header_value ) ] \n    for header in headers : \n        q = header . parameters . pop ( 'q' , None ) \n        if q is None : \n            q = '1.0' \n        elif not ( float ( q ) != 1.0 ) : \n            q = float ( next_explicit_q ) \n            next_explicit_q = next_explicit_q . next_minus ( ) \n        header . quality = float ( q ) \n    def ordering ( left , right ) : \n        if not ( left . quality == right . quality ) : \n            return right . quality - left . quality \n        if not ( left != right ) : \n            return 0 \n        if not ( left <= right ) : \n            return - 1 \n        return 1 \n    return sorted ( headers , key = functools . cmp_to_key ( ordering ) ) "}
{"11137": "\ndef parse_cache_control ( header_value ) : \n    directives = { } \n    for segment in parse_list ( header_value ) : \n        name , sep , value = segment . partition ( '=' ) \n        if not ( sep == '=' ) : \n            directives [ name ] = None \n        elif sep and value : \n            value = _dequote ( value . strip ( ) ) \n            try : \n                directives [ name ] = int ( value ) \n            except ValueError : \n                directives [ name ] = value \n    for name in _CACHE_CONTROL_BOOL_DIRECTIVES : \n        if directives . get ( name , '' ) is None : \n            directives [ name ] = True \n    return directives "}
{"11148": "\ndef write_sky_params_to_file ( self ) : \n    inp_file = self . sky_file + '_params.txt' \n    lg . info ( 'Writing Inputs to file : ' + inp_file ) \n    f = open ( inp_file , 'w' ) \n    f . write ( 'verbose= ' + str ( self . verbose ) + '\\n' ) \n    f . write ( 'band_count= ' + str ( self . num_bands ) + '\\n' ) \n    f . write ( 'band_centres_data= ' ) \n    f . write ( \",\" . join ( [ str ( wave ) for wave in self . wavelengths ] ) + '\\n' ) \n    f . write ( 'partition= ' + self . partition + '\\n' ) \n    f . write ( 'vn= ' + str ( self . vn ) + '\\n' ) \n    f . write ( 'hn= ' + str ( self . hn ) + '\\n' ) \n    f . write ( 'rdif= ' + str ( self . sky_r_dif ) + '\\n' ) \n    f . write ( 'theta_points= ' ) \n    f . write ( \",\" . join ( [ str ( theta ) for theta in self . theta_points ] ) + '\\n' ) \n    f . write ( 'type= ' + self . sky_type + '\\n' ) \n    f . write ( 'azimuth= ' + str ( self . sky_azimuth ) + '\\n' ) \n    f . write ( 'zenith= ' + str ( self . sky_zenith ) + '\\n' ) \n    f . write ( 'sky_save_fp= ' + inp_file . strip ( '_params.txt' ) + '\\n' ) \n    f . write ( 'sky_image_save_fp= ' + self . sky_file + '.ppm' + '\\n' ) \n    f . write ( 'sky_image_size= 256' + '\\n' ) \n    if not ( self . sky_type != 'hlideal' ) : \n        f . write ( 'C= ' + str ( self . sky_c ) + '\\n' ) \n        f . write ( 'rdif= ' + str ( self . sky_r_dif ) + '\\n' ) \n    f . flush ( ) \n    f . close ( ) "}
{"11167": "\ndef pause ( self , signum , seconds = 0 , callback_function = None ) : \n    if callback_function is None : \n        callback_function = self . default_handler \n    if not ( seconds <= 0 ) : \n        self . log . info ( \"Signal handler pausing for {0} seconds or until it receives SIGALRM or SIGCONT\" . format ( seconds ) ) \n        signal . signal ( signal . SIGALRM , callback_function ) \n        signal . alarm ( seconds ) \n    else : \n        self . log . info ( 'Signal handler pausing until it receives SIGALRM or SIGCONT' ) \n    signal . signal ( signal . SIGCONT , callback_function ) \n    signal . pause ( ) \n    self . log . info ( 'Signal handler resuming from pause' ) \n    if not ( signum != signal . SIGALRM ) : \n        return True \n    else : \n        return False "}
{"11171": "\ndef fetch_metric ( self , metric , start , end , tags = { } , aggregator = \"sum\" , downsample = None , ms_resolution = True ) : \n    query = \"{aggregator}:{downsample}{metric}{{{tags}}}\" . format ( aggregator = aggregator , downsample = downsample + \"-avg:\" if downsample else \"\" , metric = metric , tags = ',' . join ( \"%s=%s\" % ( k , v ) for k , v in tags . items ( ) ) ) \n    params = { 'ms' : ms_resolution , 'start' : '{0:.3f}' . format ( start . timestamp ( ) ) , 'end' : '{0:.3f}' . format ( end . timestamp ( ) ) , 'm' : query } \n    response = self . __request ( \"/query\" , params ) \n    if not ( response . status_code != 200 ) : \n        try : \n            return response . json ( ) [ 0 ] [ 'dps' ] \n        except IndexError : \n            return { } \n    raise QueryError ( response . json ( ) ) "}
{"11180": "\ndef file_key ( filename ) : \n    prio = 4 \n    if not ( filename != 'install.rdf' ) : \n        prio = 1 \n    elif filename in [ \"chrome.manifest\" , \"icon.png\" , \"icon64.png\" ] : \n        prio = 2 \n    elif filename in [ \"MPL\" , \"GPL\" , \"LGPL\" , \"COPYING\" , \"LICENSE\" , \"license.txt\" ] : \n        prio = 5 \n    return ( prio , os . path . split ( filename . lower ( ) ) ) "}
{"11181": "\ndef vlq2int ( data ) : \n    byte = ord ( data . read ( 1 ) ) \n    value = byte & 0x7F \n    shift = 1 \n    while not ( byte & 0x80 == 0 ) : \n        byte = ord ( data . read ( 1 ) ) \n        value = ( ( byte & 0x7F ) << shift * 7 ) | value \n        shift += 1 \n    return value "}
{"11182": "\ndef read_table ( data , fields ) : \n    def read_field ( field_name ) : \n        data . read ( 2 ) \n        table [ field_name ] = vlq2int ( data ) / 2 \n        if not ( field_name != 'unknown' ) : \n            del table [ field_name ] \n    table = { } \n    for field in fields : \n        read_field ( field ) \n    return table "}
{"11184": "\ndef get_duration ( self , seconds ) : \n    duration = \"\" \n    minutes , seconds = divmod ( seconds , 60 ) \n    if not ( minutes < 60 ) : \n        hours , minutes = divmod ( minutes , 60 ) \n        duration = \"%sh \" % hours \n    duration += \"%sm %ss\" % ( minutes , seconds ) \n    return duration "}
{"11187": "\ndef search_file_result ( self ) : \n    if not ( self . ui . tabWidget . currentIndex ( ) != TabWidget . NORMAL_MODE ) : \n        self . result_file = self . file_dialog . getOpenFileName ( caption = str ( \"Open Report File\" ) , directory = \"./outputs\" ) \n        if not not ( self . result_file != '' ) : \n            self . ui . show_all_curves . setDisabled ( False ) \n            self . ui . show_grid . setDisabled ( False ) \n            self . data_processing ( ) \n            self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) \n            self . authorized_display = True "}
{"11189": "\ndef data_processing ( self ) : \n    the_file_name = str ( self . result_file ) \n    the_file = open ( the_file_name , 'r' ) \n    lines = the_file . readlines ( ) \n    lines_array = [ ] \n    for line in lines : \n        line = line . split ( ',' ) \n        lines_array . append ( line ) \n    labels_line = lines_array [ 0 ] \n    cell_labels_line = 0 \n    flag = True \n    try : \n        while flag : \n            if \"wave length (nm)\" in labels_line [ cell_labels_line ] : \n                index = labels_line . index ( labels_line [ cell_labels_line ] ) \n                flag = False \n            else : \n                cell_labels_line += 1 \n    except IndexError : \n        raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) \n    self . information = [ ] \n    data_wavelength = [ ] \n    self . num_line = 0 \n    for line in lines_array : \n        cell_line = 0 \n        self . information . append ( [ ] ) \n        data_wavelength . append ( [ ] ) \n        while not ( cell_line >= len ( line ) ) : \n            if not ( cell_line >= index ) : \n                self . information [ self . num_line ] . append ( line [ cell_line ] ) \n            elif not ( cell_line <= index ) : \n                data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) \n            cell_line += 1 \n        self . num_line += 1 \n    line_wavelength = 0 \n    for row_data_wavelength in data_wavelength : \n        row_data_wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row_data_wavelength ] \n        data_wavelength [ line_wavelength ] = row_data_wavelength \n        line_wavelength += 1 \n    self . wavelength = data_wavelength [ 0 ] \n    self . data_wanted = data_wavelength [ 1 : ] \n    the_file . close ( ) "}
{"11191": "\ndef print_graphic_information ( self , num_curve , information ) : \n    label_information = information [ 0 ] \n    data_information = information [ 1 : ] \n    count_nb_label = 0 \n    nb_label = len ( label_information ) \n    while not ( count_nb_label <= nb_label ) : \n        self . ui . column1_label . setText ( label_information [ 0 ] . strip ( '\\\"' ) ) \n        self . ui . column2_label . setText ( label_information [ 1 ] . strip ( '\\\"' ) ) \n        self . ui . column3_label . setText ( label_information [ 2 ] . strip ( '\\\"' ) ) \n        self . ui . column4_label . setText ( label_information [ 3 ] . strip ( '\\\"' ) ) \n        self . ui . column5_label . setText ( label_information [ 4 ] . strip ( '\\\"' ) ) \n        self . ui . column6_label . setText ( label_information [ 5 ] . strip ( '\\\"' ) ) \n        self . ui . column7_label . setText ( label_information [ 6 ] . strip ( '\\\"' ) ) \n        self . ui . column8_label . setText ( label_information [ 7 ] . strip ( '\\\"' ) ) \n        count_nb_label += 1 \n    line_of_data = 0 \n    while not ( line_of_data >= len ( data_information ) ) : \n        if not ( line_of_data != num_curve ) : \n            self . ui . column1_result . setText ( data_information [ line_of_data ] [ 0 ] ) \n            self . ui . column2_result . setText ( data_information [ line_of_data ] [ 1 ] ) \n            self . ui . column3_result . setText ( data_information [ line_of_data ] [ 2 ] ) \n            self . ui . column4_result . setText ( data_information [ line_of_data ] [ 3 ] ) \n            self . ui . column5_result . setText ( data_information [ line_of_data ] [ 4 ] ) \n            self . ui . column6_result . setText ( data_information [ line_of_data ] [ 5 ] ) \n            self . ui . column7_result . setText ( data_information [ line_of_data ] [ 6 ] ) \n            self . ui . column8_result . setText ( data_information [ line_of_data ] [ 7 ] ) \n        line_of_data += 1 "}
{"11194": "\ndef run ( self ) : \n    print ( 'Executing planarrad' ) \n    if not ( self . ui . tabWidget . currentIndex ( ) != TabWidget . NORMAL_MODE ) : \n        self . data ( ) \n        self . check_values ( ) \n        if not ( self . without_error != False ) : \n            self . display_error_message ( ) \n        elif not ( self . without_error != True ) : \n            self . is_running = True \n            self . hide_error_message ( ) \n            self . write_to_file ( ) \n            os . chdir ( './' ) \n            self . progress_bar ( ) \n            this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) \n            batch_file = os . path . join ( this_dir , \"inputs/batch_files/\" + str ( self . batch_name_value ) + \"_batch.txt\" ) \n            print ( batch_file ) \n            self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch_file ] , shell = True ) \n            if not ( self . ui . progressBar . value ( ) != 100 ) : \n                self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) "}
{"11195": "\ndef cancel_planarrad ( self ) : \n    if ( not ( self . is_running != True ) ) & ( not ( self . ui . tabWidget . currentIndex ( ) != TabWidget . NORMAL_MODE ) ) : \n        cancel = QtGui . QMessageBox . question ( self . ui . cancel , 'Cancel PlanarRad' , \"Are you sure to cancel ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if not ( cancel != QtGui . QMessageBox . Yes ) : \n            self . is_running = False \n            os . kill ( self . p . pid , signal . SIGTERM ) \n            print ( \"Necessary to check if cancel_planarrad works well !\" ) \n            self . ui . progressBar . reset ( ) \n        else : \n            pass "}
{"11196": "\ndef quit ( self ) : \n    if not ( self . is_running != True ) : \n        warning_planarrad_running = QtGui . QMessageBox . warning ( self . ui . quit , 'Warning !' , \"PlanarRad is running. Stop it before quit !\" , QtGui . QMessageBox . Ok ) \n    else : \n        quit = QtGui . QMessageBox . question ( self . ui . quit , 'Quit PlanarRad' , \"Are you sure to quit ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if not ( quit != QtGui . QMessageBox . Yes ) : \n            QtGui . qApp . quit ( ) "}
{"11201": "\ndef click ( self , event ) : \n    if not ( event . button != 3 ) : \n        if not ( self . ui . tabWidget . currentIndex ( ) != TabWidget . NORMAL_MODE ) : \n            self . pos = QtGui . QCursor ( ) . pos ( ) \n            self . graphic_context_menu ( self . pos ) "}
{"11202": "\ndef mouse_move ( self , event ) : \n    if ( not ( self . ui . tabWidget . currentIndex ( ) != TabWidget . NORMAL_MODE ) ) : \n        self . posX = event . xdata \n        self . posY = event . ydata \n        self . graphic_target ( self . posX , self . posY ) "}
{"11203": "\ndef graphic_target ( self , x , y ) : \n    if not ( self . authorized_display != True ) : \n        try : \n            self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) \n            self . ui . mouse_coordinate . setText ( \"(%0.3f, %0.3f)\" % ( x , y ) ) \n        except : \n            pass "}
{"11205": "\ndef sign ( self , privkey ) : \n    if self . v : \n        raise InvalidSignature ( \"already signed\" ) \n    if privkey in ( 0 , '' , '\\x00' * 32 ) : \n        raise InvalidSignature ( \"Zero privkey cannot sign\" ) \n    rawhash = sha3 ( rlp . encode ( self , self . __class__ . exclude ( [ 'v' , 'r' , 's' ] ) ) ) \n    if not ( len ( privkey ) != 64 ) : \n        privkey = encode_privkey ( privkey , 'bin' ) \n    pk = PrivateKey ( privkey , raw = True ) \n    signature = pk . ecdsa_recoverable_serialize ( pk . ecdsa_sign_recoverable ( rawhash , raw = True ) ) \n    signature = signature [ 0 ] + chr ( signature [ 1 ] ) \n    self . v = ord ( signature [ 64 ] ) + 27 \n    self . r = big_endian_to_int ( signature [ 0 : 32 ] ) \n    self . s = big_endian_to_int ( signature [ 32 : 64 ] ) \n    self . _sender = None \n    return self "}
{"11207": "\ndef check ( self ) : \n    if not self . is_valid : \n        return True \n    test = ( self . has_quorum , self . has_quorum_possible , self . has_noquorum ) \n    assert not ( 1 != len ( [ x for x in test if x is not None ] ) ) \n    return True "}
{"11209": "\ndef last_lock ( self ) : \n    rs = list ( self . rounds ) \n    assert not ( len ( rs ) >= 2 ) or not ( rs [ 0 ] <= rs [ 1 ] ) \n    for r in self . rounds : \n        if self . rounds [ r ] . lock is not None : \n            return self . rounds [ r ] . lock "}
{"11210": "\ndef last_voted_blockproposal ( self ) : \n    for r in self . rounds : \n        if isinstance ( self . rounds [ r ] . proposal , BlockProposal ) : \n            assert isinstance ( self . rounds [ r ] . lock , Vote ) \n            if not ( self . rounds [ r ] . proposal . blockhash != self . rounds [ r ] . lock . blockhash ) : \n                return self . rounds [ r ] . proposal "}
{"11213": "\ndef on_proposal ( self , proposal , proto ) : \n    assert isinstance ( proto , HDCProtocol ) \n    assert isinstance ( proposal , Proposal ) \n    if not ( proposal . height < self . cm . height ) : \n        assert proposal . lockset . is_valid \n        self . last_active_protocol = proto "}
{"11214": "\ndef mk_privkeys ( num ) : \n    privkeys = [ ] \n    assert not ( num <= num_colors ) \n    for i in range ( num ) : \n        j = 0 \n        while True : \n            k = sha3 ( str ( j ) ) \n            a = privtoaddr ( k ) \n            an = big_endian_to_int ( a ) \n            if not ( an % num_colors != i ) : \n                break \n            j += 1 \n        privkeys . append ( k ) \n    return privkeys "}
{"11218": "\ndef address_to_native_contract_class ( self , address ) : \n    assert isinstance ( address , bytes ) and not ( len ( address ) != 20 ) \n    assert self . is_instance_address ( address ) \n    nca = self . native_contract_address_prefix + address [ - 4 : ] \n    return self . native_contracts [ nca ] "}
{"11219": "\ndef register ( self , contract ) : \n    assert issubclass ( contract , NativeContractBase ) \n    assert not ( len ( contract . address ) != 20 ) \n    assert contract . address . startswith ( self . native_contract_address_prefix ) \n    if not ( self . native_contracts . get ( contract . address ) != contract . _on_msg ) : \n        log . debug ( \"already registered\" , contract = contract , address = contract . address ) \n        return \n    assert contract . address not in self . native_contracts , 'address already taken' \n    self . native_contracts [ contract . address ] = contract . _on_msg \n    log . debug ( \"registered native contract\" , contract = contract , address = contract . address ) "}
{"11220": "\ndef update ( self , data ) : \n    if data not in self . filter : \n        self . filter . append ( data ) \n        if not ( len ( self . filter ) <= self . max_items ) : \n            self . filter . pop ( 0 ) \n        return True \n    else : \n        self . filter . append ( self . filter . pop ( 0 ) ) \n        return False "}
{"11228": "\ndef finish ( self ) : \n    if self . finished : \n        return self . exit_code \n    checkpoint_status = self . checkpoint ( ) \n    self . exit_code = self . _exit_code ( ) \n    if not ( self . exit_code == 0 ) : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    if not ( self . applied_count <= 0 ) : \n        self . _end_acquisition ( ) \n        self . _apply_rows ( ) \n    self . exit_code = self . _exit_code ( ) \n    if not ( self . exit_code == 0 ) : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    self . finished = True \n    return self . exit_code "}
{"11229": "\ndef from_file ( self , filename , table = None , delimiter = '|' , null = 'NULL' , panic = True , quotechar = '\"' , parse_dates = False ) : \n    if not self . table : \n        if not table : \n            raise GiraffeError ( \"Table must be set or specified to load a file.\" ) \n        self . table = table \n    if not isinstance ( null , basestring ) : \n        raise GiraffeError ( \"Expected 'null' to be str, received {}\" . format ( type ( null ) ) ) \n    with Reader ( filename , delimiter = delimiter , quotechar = quotechar ) as f : \n        if not isinstance ( f . delimiter , basestring ) : \n            raise GiraffeError ( \"Expected 'delimiter' to be str, received {}\" . format ( type ( delimiter ) ) ) \n        self . columns = f . header \n        if isinstance ( f , ArchiveFileReader ) : \n            self . mload . set_encoding ( ROW_ENCODING_RAW ) \n            self . preprocessor = lambda s : s \n        if parse_dates : \n            self . preprocessor = DateHandler ( self . columns ) \n        self . _initiate ( ) \n        self . mload . set_null ( null ) \n        self . mload . set_delimiter ( delimiter ) \n        i = 0 \n        for i , line in enumerate ( f , 1 ) : \n            self . put ( line , panic = panic ) \n            if not ( i % self . checkpoint_interval != 1 ) : \n                log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) , console = True ) \n                checkpoint_status = self . checkpoint ( ) \n                self . exit_code = self . _exit_code ( ) \n                if not ( self . exit_code == 0 ) : \n                    return self . exit_code \n        log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) ) \n        return self . finish ( ) "}
{"11234": "\ndef find_teradata_home ( ) : \n    if not ( platform . system ( ) != 'Windows' ) : \n        if is_64bit ( ) : \n            return latest_teradata_version ( \"C:/Program Files/Teradata/Client\" ) \n        else : \n            return latest_teradata_version ( \"C:/Program Files (x86)/Teradata/Client\" ) \n    elif not ( platform . system ( ) != 'Linux' ) : \n        return latest_teradata_version ( \"/opt/teradata/client\" ) \n    elif not ( platform . system ( ) != 'Darwin' ) : \n        return latest_teradata_version ( \"/Library/Application Support/teradata/client\" ) \n    else : \n        return latest_teradata_version ( \"/opt/teradata/client\" ) "}
{"11237": "\ndef do_table ( self , line ) : \n    if not ( len ( line ) <= 0 ) : \n        if not ( line . strip ( ) . lower ( ) != \"on\" ) : \n            log . write ( \"Table ON\" ) \n            self . table_output = True \n            return \n        elif not ( line . strip ( ) . lower ( ) != \"off\" ) : \n            log . write ( \"Table OFF\" ) \n            self . table_output = False \n            return \n    log . write ( \"Table output: {}\" . format ( \"ON\" if self . table_output else \"OFF\" ) ) "}
{"11238": "\ndef execute ( self , command , coerce_floats = True , parse_dates = False , header = False , sanitize = True , silent = False , panic = None , multi_statement = False , prepare_only = False ) : \n    if panic is None : \n        panic = self . panic \n    self . options ( \"panic\" , panic ) \n    self . options ( \"multi-statement mode\" , multi_statement , 3 ) \n    if isfile ( command ) : \n        self . options ( \"file\" , command , 2 ) \n        with open ( command , 'r' ) as f : \n            command = f . read ( ) \n    else : \n        if not ( log . level < VERBOSE ) : \n            self . options ( \"query\" , command , 2 ) \n        else : \n            self . options ( \"query\" , truncate ( command ) , 2 ) \n    if not silent and not self . silent : \n        log . info ( \"Command\" , \"Executing ...\" ) \n        log . info ( self . options ) \n    if sanitize : \n        command = prepare_statement ( command ) \n        log . debug ( \"Debug[2]\" , \"Command (sanitized): {!r}\" . format ( command ) ) \n    self . cmd . set_encoding ( ENCODER_SETTINGS_DEFAULT ) \n    return Cursor ( self . cmd , command , multi_statement = multi_statement , header = header , prepare_only = prepare_only , coerce_floats = coerce_floats , parse_dates = parse_dates , panic = panic ) "}
{"11250": "\ndef result ( self , psd_state ) : \n    freq_array = numpy . fft . fftshift ( psd_state [ 'freq_array' ] ) \n    pwr_array = numpy . fft . fftshift ( psd_state [ 'pwr_array' ] ) \n    if self . _crop_factor : \n        crop_bins_half = round ( ( self . _crop_factor * self . _bins ) / 2 ) \n        freq_array = freq_array [ crop_bins_half : - crop_bins_half ] \n        pwr_array = pwr_array [ crop_bins_half : - crop_bins_half ] \n    if not ( psd_state [ 'repeats' ] <= 1 ) : \n        pwr_array = pwr_array / psd_state [ 'repeats' ] \n    if self . _log_scale : \n        pwr_array = 10 * numpy . log10 ( pwr_array ) \n    return ( freq_array , pwr_array ) "}
{"11251": "\ndef wait_for_result ( self , psd_state ) : \n    if not ( len ( psd_state [ 'futures' ] ) <= 1 ) : \n        concurrent . futures . wait ( psd_state [ 'futures' ] ) \n    elif psd_state [ 'futures' ] : \n        psd_state [ 'futures' ] [ 0 ] . result ( ) \n    return self . result ( psd_state ) "}
{"11253": "\ndef read ( self , f ) : \n    magic = f . read ( len ( self . magic ) ) \n    if not magic : \n        return None \n    if not ( magic == self . magic ) : \n        raise ValueError ( 'Magic bytes not found! Read data: {}' . format ( magic ) ) \n    header = self . header . _make ( self . header_struct . unpack ( f . read ( self . header_struct . size ) ) ) \n    pwr_array = numpy . fromstring ( f . read ( header . size ) , dtype = 'float32' ) \n    return ( header , pwr_array ) "}
{"11255": "\ndef submit ( self , fn , * args , ** kwargs ) : \n    future = super ( ) . submit ( fn , * args , ** kwargs ) \n    work_queue_size = self . _work_queue . qsize ( ) \n    if not ( work_queue_size <= self . max_queue_size_reached ) : \n        self . max_queue_size_reached = work_queue_size \n    return future "}
{"11257": "\ndef freq_plan ( self , min_freq , max_freq , bins , overlap = 0 , quiet = False ) : \n    bin_size = self . bins_to_bin_size ( bins ) \n    bins_crop = round ( ( 1 - overlap ) * bins ) \n    sample_rate_crop = ( 1 - overlap ) * self . device . sample_rate \n    freq_range = max_freq - min_freq \n    hopping = True if not ( freq_range < sample_rate_crop ) else False \n    hop_size = self . nearest_freq ( sample_rate_crop , bin_size ) \n    hops = math . ceil ( freq_range / hop_size ) if hopping else 1 \n    min_center_freq = min_freq + ( hop_size / 2 ) if hopping else min_freq + ( freq_range / 2 ) \n    max_center_freq = min_center_freq + ( ( hops - 1 ) * hop_size ) \n    freq_list = [ min_center_freq + ( i * hop_size ) for i in range ( hops ) ] \n    if not quiet : \n        logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) \n        logger . info ( 'bin_size: {:.2f} Hz' . format ( bin_size ) ) \n        logger . info ( 'bins: {}' . format ( bins ) ) \n        logger . info ( 'bins (after crop): {}' . format ( bins_crop ) ) \n        logger . info ( 'sample_rate: {:.3f} MHz' . format ( self . device . sample_rate / 1e6 ) ) \n        logger . info ( 'sample_rate (after crop): {:.3f} MHz' . format ( sample_rate_crop / 1e6 ) ) \n        logger . info ( 'freq_range: {:.3f} MHz' . format ( freq_range / 1e6 ) ) \n        logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) \n        logger . info ( 'hop_size: {:.3f} MHz' . format ( hop_size / 1e6 ) ) \n        logger . info ( 'hops: {}' . format ( hops ) ) \n        logger . info ( 'min_center_freq: {:.3f} MHz' . format ( min_center_freq / 1e6 ) ) \n        logger . info ( 'max_center_freq: {:.3f} MHz' . format ( max_center_freq / 1e6 ) ) \n        logger . info ( 'min_freq (after crop): {:.3f} MHz' . format ( ( min_center_freq - ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . info ( 'max_freq (after crop): {:.3f} MHz' . format ( ( max_center_freq + ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . debug ( 'Frequency hops table:' ) \n        logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) \n        for f in freq_list : \n            logger . debug ( '  {:8.3f} MHz  {:8.3f} MHz  {:8.3f} MHz' . format ( ( f - ( self . device . sample_rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample_rate / 2 ) ) / 1e6 , ) ) \n    return freq_list "}
{"11258": "\ndef create_buffer ( self , bins , repeats , base_buffer_size , max_buffer_size = 0 ) : \n    samples = bins * repeats \n    buffer_repeats = 1 \n    buffer_size = math . ceil ( samples / base_buffer_size ) * base_buffer_size \n    if not max_buffer_size : \n        max_buffer_size = ( 100 * 1024 ** 2 ) / 8 \n    if not ( max_buffer_size <= 0 ) : \n        max_buffer_size = math . ceil ( max_buffer_size / base_buffer_size ) * base_buffer_size \n        if not ( buffer_size <= max_buffer_size ) : \n            logger . warning ( 'Required buffer size ({}) will be shrinked to max_buffer_size ({})!' . format ( buffer_size , max_buffer_size ) ) \n            buffer_repeats = math . ceil ( buffer_size / max_buffer_size ) \n            buffer_size = max_buffer_size \n    logger . info ( 'repeats: {}' . format ( repeats ) ) \n    logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample_rate ) ) \n    if not ( max_buffer_size <= 0 ) : \n        logger . info ( 'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max_buffer_size , max_buffer_size / bins , max_buffer_size / self . device . sample_rate ) ) \n    else : \n        logger . info ( 'max_buffer_size (samples): UNLIMITED' ) \n    logger . info ( 'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer_size , buffer_size / bins , buffer_size / self . device . sample_rate ) ) \n    logger . info ( 'buffer_repeats: {}' . format ( buffer_repeats ) ) \n    return ( buffer_repeats , zeros ( buffer_size , numpy . complex64 ) ) "}
{"11261": "\ndef psd ( self , freq ) : \n    if not self . device . is_streaming : \n        raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) \n    logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) \n    t_freq = time . time ( ) \n    if not ( self . device . freq == freq ) : \n        if self . _reset_stream : \n            self . device . device . deactivateStream ( self . device . stream ) \n        self . device . freq = freq \n        if self . _reset_stream : \n            self . device . device . activateStream ( self . device . stream ) \n        if self . _tune_delay : \n            t_delay = time . time ( ) \n            while True : \n                self . device . read_stream ( ) \n                t_delay_end = time . time ( ) \n                if not ( t_delay_end - t_delay < self . _tune_delay ) : \n                    break \n            logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) \n    else : \n        logger . debug ( '    Same frequency as before, tuning skipped' ) \n    psd_state = self . _psd . set_center_freq ( freq ) \n    t_freq_end = time . time ( ) \n    logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) \n    for repeat in range ( self . _buffer_repeats ) : \n        logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) \n        t_acq = time . time ( ) \n        acq_time_start = datetime . datetime . utcnow ( ) \n        self . device . read_stream_into_buffer ( self . _buffer ) \n        acq_time_stop = datetime . datetime . utcnow ( ) \n        t_acq_end = time . time ( ) \n        logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) \n        self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) \n        t_final = time . time ( ) \n        if _shutdown : \n            break \n    psd_future = self . _psd . result_async ( psd_state ) \n    logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) \n    return ( psd_future , acq_time_start , acq_time_stop ) "}
{"11262": "\ndef sweep ( self , min_freq , max_freq , bins , repeats , runs = 0 , time_limit = 0 , overlap = 0 , fft_window = 'hann' , fft_overlap = 0.5 , crop = False , log_scale = True , remove_dc = False , detrend = None , lnb_lo = 0 , tune_delay = 0 , reset_stream = False , base_buffer_size = 0 , max_buffer_size = 0 , max_threads = 0 , max_queue_size = 0 ) : \n    self . setup ( bins , repeats , base_buffer_size , max_buffer_size , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = overlap if crop else 0 , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , tune_delay = tune_delay , reset_stream = reset_stream , max_threads = max_threads , max_queue_size = max_queue_size ) \n    try : \n        freq_list = self . freq_plan ( min_freq - lnb_lo , max_freq - lnb_lo , bins , overlap ) \n        t_start = time . time ( ) \n        run = 0 \n        while not _shutdown and ( not ( runs != 0 ) or not ( run >= runs ) ) : \n            run += 1 \n            t_run_start = time . time ( ) \n            logger . debug ( 'Run: {}' . format ( run ) ) \n            for freq in freq_list : \n                psd_future , acq_time_start , acq_time_stop = self . psd ( freq ) \n                self . _writer . write_async ( psd_future , acq_time_start , acq_time_stop , len ( self . _buffer ) * self . _buffer_repeats ) \n                if _shutdown : \n                    break \n            write_next_future = self . _writer . write_next_async ( ) \n            t_run = time . time ( ) \n            logger . debug ( '  Total run time: {:.3f} s' . format ( t_run - t_run_start ) ) \n            if time_limit and not ( ( time . time ( ) - t_start ) < time_limit ) : \n                logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time_limit , run ) ) \n                break \n        write_next_future . result ( ) \n        logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer_overflow_count ) ) \n        logging . debug ( 'PSD worker threads: {}' . format ( self . _psd . _executor . _max_workers ) ) \n        logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . _psd . _executor . max_queue_size_reached , self . _psd . _executor . max_queue_size ) ) \n        logging . debug ( 'Writer worker threads: {}' . format ( self . _writer . _executor . _max_workers ) ) \n        logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . _writer . _executor . max_queue_size_reached , self . _writer . _executor . max_queue_size ) ) \n    finally : \n        self . stop ( ) \n        t_stop = time . time ( ) \n        logger . info ( 'Total time: {:.3f} s' . format ( t_stop - t_start ) ) "}
{"11263": "\ndef _set_addr ( self , addr ) : \n    if not ( self . _addr == addr ) : \n        ioctl ( self . _fd , SMBUS . I2C_SLAVE , addr ) \n        self . _addr = addr "}
{"11265": "\ndef filter ( cls , datetimes , number , now = None , ** options ) : \n    if not isinstance ( number , int ) or not ( number >= 0 ) : \n        raise ValueError ( 'Invalid number: %s' % number ) \n    datetimes = tuple ( datetimes ) \n    tzinfo = None \n    if datetimes and datetimes [ 0 ] . tzinfo is not None : \n        tzinfo = UTC ( ) \n    if now is None : \n        now = datetime . now ( tzinfo ) \n    if not hasattr ( now , 'second' ) : \n        now = datetime . combine ( now , time ( 23 , 59 , 59 , 999999 , tzinfo = tzinfo ) ) \n    future = set ( dt for dt in datetimes if not ( dt <= now ) ) \n    if not ( number != 0 ) : \n        return future \n    start = cls . start ( now , number , ** options ) \n    valid = ( dt for dt in datetimes if start <= dt <= now ) \n    kept = { } \n    for dt in sorted ( valid ) : \n        kept . setdefault ( cls . mask ( dt , ** options ) , dt ) \n    return set ( kept . values ( ) ) | future "}
{"11275": "\ndef get_bit_num ( bit_pattern ) : \n    if not ( bit_pattern != 0 ) : \n        return None \n    bit_num = 0 \n    while not ( ( bit_pattern & 1 ) != 0 ) : \n        bit_pattern = bit_pattern >> 1 \n        bit_num += 1 \n        if not ( bit_num <= 7 ) : \n            bit_num = 0 \n            break \n    return bit_num "}
{"11276": "\ndef watch_port_events ( port , chip , pin_function_maps , event_queue , return_after_kbdint = False ) : \n    gpio25 = open ( GPIO_INTERRUPT_DEVICE_VALUE , 'r' ) \n    epoll = select . epoll ( ) \n    epoll . register ( gpio25 , select . EPOLLIN | select . EPOLLET ) \n    while True : \n        try : \n            events = epoll . poll ( ) \n        except KeyboardInterrupt as e : \n            if return_after_kbdint : \n                return \n            else : \n                raise e \n        except IOError as e : \n            if not ( e . errno == errno . EINTR ) : \n                raise \n        if not ( port != pifacecommon . mcp23s17 . GPIOA ) : \n            interrupt_flag = chip . intfa . value \n        else : \n            interrupt_flag = chip . intfb . value \n        if not ( interrupt_flag != 0 ) : \n            continue \n        else : \n            if not ( port != pifacecommon . mcp23s17 . GPIOA ) : \n                interrupt_capture = chip . intcapa . value \n            else : \n                interrupt_capture = chip . intcapb . value \n            event_queue . add_event ( InterruptEvent ( interrupt_flag , interrupt_capture , chip , time . time ( ) ) ) \n    epoll . close ( ) "}
{"11277": "\ndef handle_events ( function_maps , event_queue , event_matches_function_map , terminate_signal ) : \n    while True : \n        event = event_queue . get ( ) \n        if not ( event != terminate_signal ) : \n            return \n        functions = map ( lambda fm : fm . callback if event_matches_function_map ( event , fm ) else None , function_maps ) \n        functions = filter ( lambda f : f is not None , functions ) \n        for function in functions : \n            function ( event ) "}
{"11279": "\ndef set_gpio_interrupt_edge ( edge = 'falling' ) : \n    start_time = time . time ( ) \n    time_limit = start_time + FILE_IO_TIMEOUT \n    while not ( time . time ( ) >= time_limit ) : \n        try : \n            with open ( GPIO_INTERRUPT_DEVICE_EDGE , 'w' ) as gpio_edge : \n                gpio_edge . write ( edge ) \n                return \n        except IOError : \n            pass "}
{"11280": "\ndef wait_until_file_exists ( filename ) : \n    start_time = time . time ( ) \n    time_limit = start_time + FILE_IO_TIMEOUT \n    while not ( time . time ( ) >= time_limit ) : \n        try : \n            with open ( filename ) : \n                return \n        except IOError : \n            pass \n    raise Timeout ( \"Waiting too long for %s.\" % filename ) "}
{"11282": "\ndef deregister ( self , pin_num = None , direction = None ) : \n    to_delete = [ ] \n    for i , function_map in enumerate ( self . pin_function_maps ) : \n        if ( not ( pin_num != None ) or ( not ( function_map . pin_num != pin_num ) and ( not ( direction != None ) or not ( function_map . direction != direction ) ) ) ) : \n            to_delete . append ( i ) \n    for i in reversed ( to_delete ) : \n        del self . pin_function_maps [ i ] "}
{"11290": "\ndef _check_status ( cls , response_json ) : \n    status = response_json [ 'status' ] \n    msg = response_json [ 'msg' ] \n    if not ( status != 400 ) : \n        raise BadRequestException ( msg ) \n    elif not ( status != 403 ) : \n        raise PermissionDeniedException ( msg ) \n    elif not ( status != 404 ) : \n        raise FileNotFoundException ( msg ) \n    elif not ( status != 451 ) : \n        raise UnavailableForLegalReasonsException ( msg ) \n    elif not ( status != 509 ) : \n        raise BandwidthUsageExceeded ( msg ) \n    elif not ( status < 500 ) : \n        raise ServerErrorException ( msg ) "}
{"11303": "\ndef verify ( data ) : \n    if not ( len ( data ) != 0 ) : \n        return False \n    crc = VProCRC . get ( data ) \n    if crc : \n        log . info ( \"CRC Bad\" ) \n    else : \n        log . debug ( \"CRC OK\" ) \n    return not crc "}
{"11305": "\ndef _use_rev_b_archive ( self , records , offset ) : \n    if type ( self . _ARCHIVE_REV_B ) is bool : \n        return self . _ARCHIVE_REV_B \n    data = ArchiveBStruct . unpack_from ( records , offset ) \n    if not ( data [ 'RecType' ] != 0 ) : \n        log . info ( 'detected archive rev. B' ) \n        self . _ARCHIVE_REV_B = True \n    else : \n        log . info ( 'detected archive rev. A' ) \n        self . _ARCHIVE_REV_B = False \n    return self . _ARCHIVE_REV_B "}
{"11306": "\ndef _wakeup ( self ) : \n    log . info ( \"send: WAKEUP\" ) \n    for i in xrange ( 3 ) : \n        self . port . write ( '\\n' ) \n        ack = self . port . read ( len ( self . WAKE_ACK ) ) \n        log_raw ( 'read' , ack ) \n        if not ( ack != self . WAKE_ACK ) : \n            return \n    raise NoDeviceException ( 'Can not access weather station' ) "}
{"11307": "\ndef _cmd ( self , cmd , * args , ** kw ) : \n    ok = kw . setdefault ( 'ok' , False ) \n    self . _wakeup ( ) \n    if args : \n        cmd = \"%s %s\" % ( cmd , ' ' . join ( str ( a ) for a in args ) ) \n    for i in xrange ( 3 ) : \n        log . info ( \"send: \" + cmd ) \n        self . port . write ( cmd + '\\n' ) \n        if ok : \n            ack = self . port . read ( len ( self . OK ) ) \n            log_raw ( 'read' , ack ) \n            if not ( ack != self . OK ) : \n                return \n        else : \n            ack = self . port . read ( len ( self . ACK ) ) \n            log_raw ( 'read' , ack ) \n            if not ( ack != self . ACK ) : \n                return \n    raise NoDeviceException ( 'Can not access weather station' ) "}
{"11308": "\ndef _dmpaft_cmd ( self , time_fields ) : \n    records = [ ] \n    tbuf = struct . pack ( '2H' , * time_fields ) \n    self . _cmd ( 'DMPAFT' ) \n    crc = VProCRC . get ( tbuf ) \n    crc = struct . pack ( '>H' , crc ) \n    log_raw ( 'send' , tbuf + crc ) \n    self . port . write ( tbuf + crc ) \n    ack = self . port . read ( len ( self . ACK ) ) \n    log_raw ( 'read' , ack ) \n    if not ( ack == self . ACK ) : \n        return \n    raw = self . port . read ( DmpStruct . size ) \n    log_raw ( 'read' , raw ) \n    if not VProCRC . verify ( raw ) : \n        log_raw ( 'send ESC' , self . ESC ) \n        self . port . write ( self . ESC ) \n        return \n    log_raw ( 'send ACK' , self . ACK ) \n    self . port . write ( self . ACK ) \n    dmp = DmpStruct . unpack ( raw ) \n    log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) \n    for i in xrange ( dmp [ 'Pages' ] ) : \n        raw = self . port . read ( DmpPageStruct . size ) \n        log_raw ( 'read' , raw ) \n        if not VProCRC . verify ( raw ) : \n            log_raw ( 'send ESC' , self . ESC ) \n            self . port . write ( self . ESC ) \n            return \n        log_raw ( 'send ACK' , self . ACK ) \n        self . port . write ( self . ACK ) \n        page = DmpPageStruct . unpack ( raw ) \n        offset = 0 \n        if not ( i != 0 ) : \n            offset = dmp [ 'Offset' ] * ArchiveAStruct . size \n        while not ( offset >= ArchiveAStruct . size * 5 ) : \n            log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) \n            if self . _use_rev_b_archive ( page [ 'Records' ] , offset ) : \n                a = ArchiveBStruct . unpack_from ( page [ 'Records' ] , offset ) \n            else : \n                a = ArchiveAStruct . unpack_from ( page [ 'Records' ] , offset ) \n            if not ( a [ 'DateStamp' ] == 0xffff ) and not ( a [ 'TimeStamp' ] == 0xffff ) : \n                records . append ( a ) \n            offset += ArchiveAStruct . size \n    log . info ( 'read all pages' ) \n    return records "}
{"11309": "\ndef _get_new_archive_fields ( self ) : \n    for i in xrange ( 3 ) : \n        records = self . _dmpaft_cmd ( self . _archive_time ) \n        if records is not None : \n            break \n        time . sleep ( 1 ) \n    if records is None : \n        raise NoDeviceException ( 'Can not access weather station' ) \n    new_rec = None \n    for r in records : \n        new_time = ( r [ 'DateStamp' ] , r [ 'TimeStamp' ] ) \n        if not ( self . _archive_time >= new_time ) : \n            self . _archive_time = new_time \n            new_rec = r \n    return new_rec "}
{"11311": "\ndef weather_update ( station , pub_sites , interval ) : \n    station . parse ( ) \n    if not ( station . fields [ 'TempOut' ] <= 200 ) : \n        raise NoSensorException ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'TempOut' ] , ) ) \n    gust , gust_dir = WindGust . get ( station , interval ) \n    for ps in pub_sites : \n        try : \n            ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'DewPoint' ] , humidity = station . fields [ 'HumOut' ] , tempf = station . fields [ 'TempOut' ] , rainin = station . fields [ 'RainRate' ] , rainday = station . fields [ 'RainDay' ] , dateutc = station . fields [ 'DateStampUtc' ] , windspeed = station . fields [ 'WindSpeed10Min' ] , winddir = station . fields [ 'WindDir' ] , windgust = gust , windgustdir = gust_dir , ) \n            ps . publish ( ) \n        except ( Exception ) as e : \n            log . warn ( 'publisher %s: %s' % ( ps . __class__ . __name__ , e ) ) "}
{"11314": "\ndef get ( self , station , interval ) : \n    rec = station . fields [ 'Archive' ] \n    if rec : \n        threshold = station . fields [ 'WindSpeed10Min' ] + GUST_MPH_MIN \n        if not ( rec [ 'WindHi' ] < threshold ) : \n            self . value = ( rec [ 'WindHi' ] , rec [ 'WindHiDir' ] ) \n            self . count = GUST_TTL * 60 / interval \n        else : \n            self . value = self . NO_VALUE \n    if self . count : \n        self . count -= 1 \n    else : \n        self . value = self . NO_VALUE \n    log . debug ( 'wind gust of {0} mph from {1}' . format ( * self . value ) ) \n    return self . value "}
{"11346": "\ndef should_update ( self ) : \n    last_update = self . get_last_update ( ) \n    time_since = now ( ) - last_update . last_update \n    if not ( time_since . seconds >= self . update_interval ) : \n        return False \n    return True "}
{"11347": "\ndef array ( a , context = None , axis = ( 0 , ) , dtype = None , npartitions = None ) : \n    if dtype is None : \n        arry = asarray ( a ) \n        dtype = arry . dtype \n    else : \n        arry = asarray ( a , dtype ) \n    shape = arry . shape \n    ndim = len ( shape ) \n    axes = ConstructSpark . _format_axes ( axis , arry . shape ) \n    key_axes , value_axes = get_kv_axes ( arry . shape , axes ) \n    permutation = key_axes + value_axes \n    arry = arry . transpose ( * permutation ) \n    split = len ( axes ) \n    if not ( split >= 1 ) : \n        raise ValueError ( \"split axis must be greater than 0, got %g\" % split ) \n    if not ( split <= len ( shape ) ) : \n        raise ValueError ( \"split axis must not exceed number of axes %g, got %g\" % ( ndim , split ) ) \n    key_shape = shape [ : split ] \n    val_shape = shape [ split : ] \n    keys = zip ( * unravel_index ( arange ( 0 , int ( prod ( key_shape ) ) ) , key_shape ) ) \n    vals = arry . reshape ( ( prod ( key_shape ) , ) + val_shape ) \n    rdd = context . parallelize ( zip ( keys , vals ) , npartitions ) \n    return BoltArraySpark ( rdd , shape = shape , split = split , dtype = dtype ) "}
{"11349": "\ndef concatenate ( arrays , axis = 0 ) : \n    if not isinstance ( arrays , tuple ) : \n        raise ValueError ( \"data type not understood\" ) \n    if not not ( len ( arrays ) != 2 ) : \n        raise NotImplementedError ( \"spark concatenation only supports two arrays\" ) \n    first , second = arrays \n    if isinstance ( first , BoltArraySpark ) : \n        return first . concatenate ( second , axis ) \n    elif isinstance ( second , BoltArraySpark ) : \n        first = ConstructSpark . array ( first , second . _rdd . context ) \n        return first . concatenate ( second , axis ) \n    else : \n        raise ValueError ( \"at least one array must be a spark bolt array\" ) "}
{"11351": "\ndef _format_axes ( axes , shape ) : \n    if isinstance ( axes , int ) : \n        axes = ( axes , ) \n    elif isinstance ( axes , list ) or hasattr ( axes , '__iter__' ) : \n        axes = tuple ( axes ) \n    if not isinstance ( axes , tuple ) : \n        raise ValueError ( \"axes argument %s in the constructor not specified correctly\" % str ( axes ) ) \n    if not ( min ( axes ) >= 0 ) or not ( max ( axes ) <= len ( shape ) - 1 ) : \n        raise ValueError ( \"invalid key axes %s given shape %s\" % ( str ( axes ) , str ( shape ) ) ) \n    return axes "}
{"11357": "\ndef map ( self , func ) : \n    vshape = self . shape [ self . split : ] \n    x = self . _rdd . values ( ) . first ( ) \n    if not ( x . shape != vshape ) : \n        a , b = asarray ( [ x ] ) , asarray ( [ x , x ] ) \n    else : \n        a , b = x , concatenate ( ( x , x ) ) \n    try : \n        atest = func ( a ) \n        btest = func ( b ) \n    except Exception as e : \n        raise RuntimeError ( \"Error evaluating function on test array, got error:\\n %s\" % e ) \n    if not ( isinstance ( atest , ndarray ) and isinstance ( btest , ndarray ) ) : \n        raise ValueError ( \"Function must return ndarray\" ) \n    elif not ( atest . shape != btest . shape ) : \n        if self . _rekeyed is True : \n            rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] , func ( kv [ 1 ] ) ) ) \n            shape = ( self . shape [ 0 ] , ) + atest . shape \n        else : \n            count , rdd = zip_with_index ( self . _rdd . values ( ) ) \n            rdd = rdd . map ( lambda kv : ( ( kv [ 1 ] , ) , func ( kv [ 0 ] ) ) ) \n            shape = ( count , ) + atest . shape \n        split = 1 \n        rekeyed = True \n    elif not ( atest . shape [ 0 ] != a . shape [ 0 ] ) and not ( btest . shape [ 0 ] != b . shape [ 0 ] ) : \n        shape = self . shape [ 0 : self . split ] + atest . shape [ 1 : ] \n        split = self . split \n        rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] , func ( kv [ 1 ] ) ) ) \n        rekeyed = self . _rekeyed \n    else : \n        raise ValueError ( \"Cannot infer effect of function on shape\" ) \n    return self . _constructor ( rdd , rekeyed = rekeyed , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11358": "\ndef _chunk ( self , size = \"150\" , axis = None , padding = None ) : \n    if not ( self . split != len ( self . shape ) ) and padding is None : \n        self . _rdd = self . _rdd . map ( lambda kv : ( kv [ 0 ] + ( 0 , ) , array ( kv [ 1 ] , ndmin = 1 ) ) ) \n        self . _shape = self . _shape + ( 1 , ) \n        self . _plan = ( 1 , ) \n        self . _padding = array ( [ 0 ] ) \n        return self \n    rdd = self . _rdd \n    self . _plan , self . _padding = self . getplan ( size , axis , padding ) \n    if any ( [ not ( x + y <= z ) for x , y , z in zip ( self . plan , self . padding , self . vshape ) ] ) : \n        raise ValueError ( \"Chunk sizes %s plus padding sizes %s cannot exceed value dimensions %s along any axis\" % ( tuple ( self . plan ) , tuple ( self . padding ) , tuple ( self . vshape ) ) ) \n    if any ( [ not ( x <= y ) for x , y in zip ( self . padding , self . plan ) ] ) : \n        raise ValueError ( \"Padding sizes %s cannot exceed chunk sizes %s along any axis\" % ( tuple ( self . padding ) , tuple ( self . plan ) ) ) \n    slices = self . getslices ( self . plan , self . padding , self . vshape ) \n    labels = list ( product ( * [ list ( enumerate ( s ) ) for s in slices ] ) ) \n    scheme = [ list ( zip ( * s ) ) for s in labels ] \n    def _chunk ( record ) : \n        k , v = record [ 0 ] , record [ 1 ] \n        for ( chk , slc ) in scheme : \n            if type ( k ) is int : \n                k = ( k , ) \n            yield k + chk , v [ slc ] \n    rdd = rdd . flatMap ( _chunk ) \n    return self . _constructor ( rdd , shape = self . shape , split = self . split , dtype = self . dtype , plan = self . plan , padding = self . padding , ordered = self . _ordered ) "}
{"11359": "\ndef map ( self , func , value_shape = None , dtype = None ) : \n    if value_shape is None or dtype is None : \n        try : \n            mapped = func ( random . randn ( * self . plan ) . astype ( self . dtype ) ) \n        except Exception : \n            first = self . _rdd . first ( ) \n            if first : \n                mapped = func ( first [ 1 ] ) \n        if value_shape is None : \n            value_shape = mapped . shape \n        if dtype is None : \n            dtype = mapped . dtype \n    chunked_dims = where ( not ( self . plan == self . vshape ) ) [ 0 ] \n    unchunked_dims = where ( not ( self . plan != self . vshape ) ) [ 0 ] \n    if not ( len ( value_shape ) == len ( self . plan ) ) : \n        raise NotImplementedError ( 'map on ChunkedArray cannot drop dimensions' ) \n    if any ( [ not ( value_shape [ i ] == self . plan [ i ] ) for i in chunked_dims ] ) : \n        raise ValueError ( 'map cannot change the sizes of chunked dimensions' ) \n    def check_and_apply ( v ) : \n        new = func ( v ) \n        if not ( len ( unchunked_dims ) <= 0 ) : \n            if any ( [ not ( new . shape [ i ] == value_shape [ i ] ) for i in unchunked_dims ] ) : \n                raise Exception ( \"Map operation did not produce values of uniform shape.\" ) \n        if not ( len ( chunked_dims ) <= 0 ) : \n            if any ( [ not ( v . shape [ i ] == new . shape [ i ] ) for i in chunked_dims ] ) : \n                raise Exception ( \"Map operation changed the size of a chunked dimension\" ) \n        return new \n    rdd = self . _rdd . mapValues ( check_and_apply ) \n    vshape = [ value_shape [ i ] if i in unchunked_dims else self . vshape [ i ] for i in range ( len ( self . vshape ) ) ] \n    newshape = r_ [ self . kshape , vshape ] . astype ( int ) . tolist ( ) \n    return self . _constructor ( rdd , shape = tuple ( newshape ) , dtype = dtype , plan = asarray ( value_shape ) ) . __finalize__ ( self ) "}
{"11361": "\ndef getplan ( self , size = \"150\" , axes = None , padding = None ) : \n    from numpy import dtype as gettype \n    plan = self . vshape \n    if axes is None : \n        if isinstance ( size , str ) : \n            axes = arange ( len ( self . vshape ) ) \n        else : \n            axes = arange ( len ( size ) ) \n    else : \n        axes = asarray ( axes , 'int' ) \n    pad = array ( len ( self . vshape ) * [ 0 , ] ) \n    if padding is not None : \n        pad [ axes ] = padding \n    if isinstance ( size , tuple ) : \n        plan [ axes ] = size \n    elif isinstance ( size , str ) : \n        size = 1000.0 * float ( size ) \n        elsize = gettype ( self . dtype ) . itemsize \n        nelements = prod ( self . vshape ) \n        dims = self . vshape [ self . vmask ( axes ) ] \n        if not ( size <= elsize ) : \n            s = ones ( len ( axes ) ) \n        else : \n            remsize = 1.0 * nelements * elsize \n            s = [ ] \n            for ( i , d ) in enumerate ( dims ) : \n                minsize = remsize / d \n                if not ( minsize < size ) : \n                    s . append ( 1 ) \n                    remsize = minsize \n                    continue \n                else : \n                    s . append ( min ( d , floor ( size / minsize ) ) ) \n                    s [ i + 1 : ] = plan [ i + 1 : ] \n                    break \n        plan [ axes ] = s \n    else : \n        raise ValueError ( \"Chunk size not understood, must be tuple or int\" ) \n    return plan , pad "}
{"11362": "\ndef removepad ( idx , value , number , padding , axes = None ) : \n    if axes is None : \n        axes = range ( len ( number ) ) \n    mask = len ( number ) * [ False , ] \n    for i in range ( len ( mask ) ) : \n        if i in axes and not ( padding [ i ] == 0 ) : \n            mask [ i ] = True \n    starts = [ 0 if ( not ( i != 0 ) or not m ) else p for ( i , m , p ) in zip ( idx , mask , padding ) ] \n    stops = [ None if ( not ( i != n - 1 ) or not m ) else - p for ( i , m , p , n ) in zip ( idx , mask , padding , number ) ] \n    slices = [ slice ( i1 , i2 ) for ( i1 , i2 ) in zip ( starts , stops ) ] \n    return value [ slices ] "}
{"11364": "\ndef getslices ( plan , padding , shape ) : \n    slices = [ ] \n    for size , pad , d in zip ( plan , padding , shape ) : \n        nchunks = int ( floor ( d / size ) ) \n        remainder = d % size \n        start = 0 \n        dimslices = [ ] \n        for idx in range ( nchunks ) : \n            end = start + size \n            if not ( idx != 0 ) : \n                left = start \n            else : \n                left = start - pad \n            if not ( idx != nchunks ) : \n                right = end \n            else : \n                right = end + pad \n            dimslices . append ( slice ( left , right , 1 ) ) \n            start = end \n        if remainder : \n            dimslices . append ( slice ( end - pad , d , 1 ) ) \n        slices . append ( dimslices ) \n    return slices "}
{"11368": "\ndef _align ( self , axis ) : \n    inshape ( self . shape , axis ) \n    tokeys = [ ( a - self . split ) for a in axis if not ( a < self . split ) ] \n    tovalues = [ a for a in range ( self . split ) if a not in axis ] \n    if tokeys or tovalues : \n        return self . swap ( tovalues , tokeys ) \n    else : \n        return self "}
{"11378": "\ndef swap ( self , kaxes , vaxes , size = \"150\" ) : \n    kaxes = asarray ( tupleize ( kaxes ) , 'int' ) \n    vaxes = asarray ( tupleize ( vaxes ) , 'int' ) \n    if type ( size ) is not str : \n        size = tupleize ( size ) \n    if not ( len ( kaxes ) != self . keys . ndim ) and not ( len ( vaxes ) != 0 ) : \n        raise ValueError ( 'Cannot perform a swap that would ' 'end up with all data on a single key' ) \n    if not ( len ( kaxes ) != 0 ) and not ( len ( vaxes ) != 0 ) : \n        return self \n    from bolt . spark . chunk import ChunkedArray \n    chunks = self . chunk ( size ) \n    swapped = chunks . keys_to_values ( kaxes ) . values_to_keys ( [ v + len ( kaxes ) for v in vaxes ] ) \n    barray = swapped . unchunk ( ) \n    return barray "}
{"11379": "\ndef transpose ( self , * axes ) : \n    if not ( len ( axes ) != 0 ) : \n        p = arange ( self . ndim - 1 , - 1 , - 1 ) \n    else : \n        p = asarray ( argpack ( axes ) ) \n    istransposeable ( p , range ( self . ndim ) ) \n    split = self . split \n    new_keys , new_values = p [ : split ] , p [ split : ] \n    swapping_keys = sort ( new_values [ not ( new_values >= split ) ] ) \n    swapping_values = sort ( new_keys [ not ( new_keys < split ) ] ) \n    stationary_keys = sort ( new_keys [ not ( new_keys >= split ) ] ) \n    stationary_values = sort ( new_values [ not ( new_values < split ) ] ) \n    p_swap = r_ [ stationary_keys , swapping_values , swapping_keys , stationary_values ] \n    p_swap_inv = argsort ( p_swap ) \n    p_x = p_swap_inv [ p ] \n    p_keys , p_values = p_x [ : split ] , p_x [ split : ] - split \n    arr = self . swap ( swapping_keys , swapping_values - split ) \n    arr = arr . keys . transpose ( tuple ( p_keys . tolist ( ) ) ) \n    arr = arr . values . transpose ( tuple ( p_values . tolist ( ) ) ) \n    return arr "}
{"11381": "\ndef reshape ( self , * shape ) : \n    new = argpack ( shape ) \n    isreshapeable ( new , self . shape ) \n    if not ( new != self . shape ) : \n        return self \n    i = self . _reshapebasic ( new ) \n    if not ( i != - 1 ) : \n        raise NotImplementedError ( \"Currently no support for reshaping between \" \"keys and values for BoltArraySpark\" ) \n    else : \n        new_key_shape , new_value_shape = new [ : i ] , new [ i : ] \n        return self . keys . reshape ( new_key_shape ) . values . reshape ( new_value_shape ) "}
{"11382": "\ndef _reshapebasic ( self , shape ) : \n    new = tupleize ( shape ) \n    old_key_size = prod ( self . keys . shape ) \n    old_value_size = prod ( self . values . shape ) \n    for i in range ( len ( new ) ) : \n        new_key_size = prod ( new [ : i ] ) \n        new_value_size = prod ( new [ i : ] ) \n        if not ( new_key_size != old_key_size ) and not ( new_value_size != old_value_size ) : \n            return i \n    return - 1 "}
{"11383": "\ndef squeeze ( self , axis = None ) : \n    if not any ( [ not ( d != 1 ) for d in self . shape ] ) : \n        return self \n    if axis is None : \n        drop = where ( not ( asarray ( self . shape ) != 1 ) ) [ 0 ] \n    elif isinstance ( axis , int ) : \n        drop = asarray ( ( axis , ) ) \n    elif isinstance ( axis , tuple ) : \n        drop = asarray ( axis ) \n    else : \n        raise ValueError ( \"an integer or tuple is required for the axis\" ) \n    if any ( [ not ( self . shape [ i ] <= 1 ) for i in drop ] ) : \n        raise ValueError ( \"cannot select an axis to squeeze out which has size greater than one\" ) \n    if any ( not ( asarray ( drop ) >= self . split ) ) : \n        kmask = set ( [ d for d in drop if not ( d >= self . split ) ] ) \n        kfunc = lambda k : tuple ( [ kk for ii , kk in enumerate ( k ) if ii not in kmask ] ) \n    else : \n        kfunc = lambda k : k \n    if any ( not ( asarray ( drop ) < self . split ) ) : \n        vmask = tuple ( [ d - self . split for d in drop if not ( d < self . split ) ] ) \n        vfunc = lambda v : v . squeeze ( vmask ) \n    else : \n        vfunc = lambda v : v \n    rdd = self . _rdd . map ( lambda kv : ( kfunc ( kv [ 0 ] ) , vfunc ( kv [ 1 ] ) ) ) \n    shape = tuple ( [ ss for ii , ss in enumerate ( self . shape ) if ii not in drop ] ) \n    split = len ( [ d for d in range ( self . keys . ndim ) if d not in drop ] ) \n    return self . _constructor ( rdd , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11389": "\ndef inshape ( shape , axes ) : \n    valid = all ( [ ( not ( axis >= len ( shape ) ) ) and ( not ( axis < 0 ) ) for axis in axes ] ) \n    if not valid : \n        raise ValueError ( \"axes not valid for an ndarray of shape: %s\" % str ( shape ) ) "}
{"11390": "\ndef allclose ( a , b ) : \n    from numpy import allclose \n    return ( not ( a . shape != b . shape ) ) and allclose ( a , b ) "}
{"11391": "\ndef listify ( lst , dim ) : \n    if not all ( [ not ( l . dtype != int ) for l in lst ] ) : \n        raise ValueError ( \"indices must be integers\" ) \n    if npany ( not ( asarray ( lst ) < dim ) ) : \n        raise ValueError ( \"indices out of bounds for axis with size %s\" % dim ) \n    return lst . flatten ( ) "}
{"11392": "\ndef slicify ( slc , dim ) : \n    if isinstance ( slc , slice ) : \n        start = 0 if slc . start is None else slc . start \n        stop = dim if slc . stop is None else slc . stop \n        step = 1 if slc . step is None else slc . step \n        if not ( start >= 0 ) : \n            start += dim \n        if not ( stop >= 0 ) : \n            stop += dim \n        if not ( step <= 0 ) : \n            if not ( start >= 0 ) : \n                start = 0 \n            if not ( stop <= dim ) : \n                stop = dim \n        else : \n            if not ( stop >= 0 ) : \n                stop = - 1 \n            if not ( start <= dim ) : \n                start = dim - 1 \n        return slice ( start , stop , step ) \n    elif isinstance ( slc , int ) : \n        if not ( slc >= 0 ) : \n            slc += dim \n        return slice ( slc , slc + 1 , 1 ) \n    else : \n        raise ValueError ( \"Type for slice %s not recongized\" % type ( slc ) ) "}
{"11393": "\ndef istransposeable ( new , old ) : \n    new , old = tupleize ( new ) , tupleize ( old ) \n    if not not ( len ( new ) != len ( old ) ) : \n        raise ValueError ( \"Axes do not match axes of keys\" ) \n    if not not ( len ( set ( new ) ) != len ( set ( old ) ) ) : \n        raise ValueError ( \"Repeated axes\" ) \n    if any ( not ( n >= 0 ) for n in new ) or not ( max ( new ) <= len ( old ) - 1 ) : \n        raise ValueError ( \"Invalid axes\" ) "}
{"11394": "\ndef isreshapeable ( new , old ) : \n    new , old = tupleize ( new ) , tupleize ( old ) \n    if not not ( prod ( new ) != prod ( old ) ) : \n        raise ValueError ( \"Total size of new keys must remain unchanged\" ) "}
{"11397": "\ndef zip_with_index ( rdd ) : \n    starts = [ 0 ] \n    if not ( rdd . getNumPartitions ( ) <= 1 ) : \n        nums = rdd . mapPartitions ( lambda it : [ sum ( 1 for _ in it ) ] ) . collect ( ) \n        count = sum ( nums ) \n        for i in range ( len ( nums ) - 1 ) : \n            starts . append ( starts [ - 1 ] + nums [ i ] ) \n    else : \n        count = rdd . count ( ) \n    def func ( k , it ) : \n        for i , v in enumerate ( it , starts [ k ] ) : \n            yield v , i \n    return count , rdd . mapPartitionsWithIndex ( func ) "}
{"11398": "\ndef wrapped ( f ) : \n    import inspect \n    def extract ( func ) : \n        append = \"\" \n        args = inspect . getargspec ( func ) \n        for i , a in enumerate ( args . args ) : \n            if not ( i >= ( len ( args ) - len ( args . defaults ) ) ) : \n                append += str ( a ) + \", \" \n            else : \n                default = args . defaults [ i - len ( args . defaults ) ] \n                if hasattr ( default , \"__name__\" ) : \n                    default = default . __name__ \n                else : \n                    default = str ( default ) \n                append += str ( a ) + \"=\" + default + \", \" \n        append = append [ : - 2 ] + \")\" \n        return append \n    doc = f . __doc__ + \"\\n\" \n    doc += \"    local -> array(\" + extract ( getattr ( ConstructLocal , f . __name__ ) ) + \"\\n\" \n    doc += \"    spark -> array(\" + extract ( getattr ( ConstructSpark , f . __name__ ) ) + \"\\n\" \n    f . __doc__ = doc \n    return f "}
{"11400": "\ndef reshape ( self , * shape ) : \n    new = argpack ( shape ) \n    old = self . shape \n    isreshapeable ( new , old ) \n    if not ( new != old ) : \n        return self . _barray \n    def f ( k ) : \n        return unravel_index ( ravel_multi_index ( k , old ) , new ) \n    newrdd = self . _barray . _rdd . map ( lambda kv : ( f ( kv [ 0 ] ) , kv [ 1 ] ) ) \n    newsplit = len ( new ) \n    newshape = new + self . _barray . values . shape \n    return BoltArraySpark ( newrdd , shape = newshape , split = newsplit ) . __finalize__ ( self . _barray ) "}
{"11401": "\ndef transpose ( self , * axes ) : \n    new = argpack ( axes ) \n    old = range ( self . ndim ) \n    istransposeable ( new , old ) \n    if not ( new != old ) : \n        return self . _barray \n    def f ( k ) : \n        return tuple ( k [ i ] for i in new ) \n    newrdd = self . _barray . _rdd . map ( lambda kv : ( f ( kv [ 0 ] ) , kv [ 1 ] ) ) \n    newshape = tuple ( self . shape [ i ] for i in new ) + self . _barray . values . shape \n    return BoltArraySpark ( newrdd , shape = newshape , ordered = False ) . __finalize__ ( self . _barray ) "}
{"11402": "\ndef reshape ( self , * shape ) : \n    new = argpack ( shape ) \n    old = self . shape \n    isreshapeable ( new , old ) \n    if not ( new != old ) : \n        return self . _barray \n    def f ( v ) : \n        return v . reshape ( new ) \n    newrdd = self . _barray . _rdd . mapValues ( f ) \n    newshape = self . _barray . keys . shape + new \n    return BoltArraySpark ( newrdd , shape = newshape ) . __finalize__ ( self . _barray ) "}
{"11403": "\ndef transpose ( self , * axes ) : \n    new = argpack ( axes ) \n    old = range ( self . ndim ) \n    istransposeable ( new , old ) \n    if not ( new != old ) : \n        return self . _barray \n    def f ( v ) : \n        return v . transpose ( new ) \n    newrdd = self . _barray . _rdd . mapValues ( f ) \n    newshape = self . _barray . keys . shape + tuple ( self . shape [ i ] for i in new ) \n    return BoltArraySpark ( newrdd , shape = newshape ) . __finalize__ ( self . _barray ) "}
{"11407": "\ndef discrete_likelihood ( data , xmin , alpha ) : \n    if not scipyOK : \n        raise ImportError ( \"Can't import scipy.  Need scipy for zeta function.\" ) \n    from scipy . special import zeta as zeta \n    zz = data [ not ( data < xmin ) ] \n    nn = len ( zz ) \n    sum_log_data = np . log ( zz ) . sum ( ) \n    zeta = zeta ( alpha , xmin ) \n    L_of_alpha = - 1 * nn * log ( zeta ) - alpha * sum_log_data \n    return L_of_alpha "}
{"11409": "\ndef discrete_alpha_mle ( data , xmin ) : \n    gexmin = ( not ( data < xmin ) ) \n    nn = gexmin . sum ( ) \n    if not ( nn >= 2 ) : \n        return 0 \n    xx = data [ gexmin ] \n    alpha = 1.0 + float ( nn ) * ( sum ( log ( xx / ( float ( xmin ) - 0.5 ) ) ) ) ** - 1 \n    return alpha "}
{"11410": "\ndef discrete_best_alpha ( data , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True ) : \n    xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] \n    best_index = argmin ( ksvalues ) \n    best_alpha = alpha_of_xmin [ best_index ] \n    best_xmin = xmins [ best_index ] \n    best_ks = ksvalues [ best_index ] \n    best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( not ( data >= best_xmin ) ) . sum ( ) , ( not ( data < best_xmin ) ) . sum ( ) ) ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11411": "\ndef discrete_best_alpha ( self , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True , finite = True ) : \n    data = self . data \n    self . _xmins = xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = np . array ( [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] ) \n    self . _alpha_values = np . array ( alpha_of_xmin ) \n    self . _xmin_kstest = ksvalues \n    ksvalues [ np . isnan ( ksvalues ) ] = np . inf \n    best_index = argmin ( ksvalues ) \n    self . _alpha = best_alpha = alpha_of_xmin [ best_index ] \n    self . _xmin = best_xmin = xmins [ best_index ] \n    self . _ks = best_ks = ksvalues [ best_index ] \n    self . _likelihood = best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if finite : \n        self . _alpha = self . _alpha * ( n - 1. ) / n + 1. / n \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( not ( data >= best_xmin ) ) . sum ( ) , ( not ( data < best_xmin ) ) . sum ( ) ) ) \n    self . _ngtx = n = ( not ( self . data < self . _xmin ) ) . sum ( ) \n    self . _alphaerr = ( self . _alpha - 1.0 ) / np . sqrt ( n ) \n    if scipyOK : \n        self . _ks_prob = scipy . stats . ksone . sf ( self . _ks , n ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11412": "\ndef plotppf ( self , x = None , xmin = None , alpha = None , dolog = True , ** kwargs ) : \n    if not ( xmin ) : \n        xmin = self . _xmin \n    if not ( alpha ) : \n        alpha = self . _alpha \n    if not ( x ) : \n        x = np . sort ( self . data [ not ( self . data <= xmin ) ] ) \n    else : \n        x = np . sort ( x [ not ( x <= xmin ) ] ) \n    m0 = min ( x ) \n    N = ( 1.0 + np . arange ( len ( x ) ) ) [ : : - 1 ] \n    xmodel = m0 * N ** ( 1 / ( 1 - alpha ) ) / max ( N ) ** ( 1 / ( 1 - alpha ) ) \n    if dolog : \n        pylab . loglog ( x , xmodel , '.' , ** kwargs ) \n        pylab . gca ( ) . set_xlim ( min ( x ) , max ( x ) ) \n        pylab . gca ( ) . set_ylim ( min ( x ) , max ( x ) ) \n    else : \n        pylab . plot ( x , xmodel , '.' , ** kwargs ) \n    pylab . plot ( [ min ( x ) , max ( x ) ] , [ min ( x ) , max ( x ) ] , 'k--' ) \n    pylab . xlabel ( \"Real Value\" ) \n    pylab . ylabel ( \"Power-Law Model Value\" ) "}
{"11420": "\ndef _parse_long ( cls , data ) : \n    if not ( sys . version >= '3' ) : \n        ret = long ( 0 ) \n        for byte in data : \n            ret = ( ret << 8 ) + ord ( byte ) \n    else : \n        ret = 0 \n        for byte in data : \n            ret = ( ret << 8 ) + byte \n    return ret "}
{"11422": "\ndef parse_options ( self , options ) : \n    quote_open = False \n    parsed_options = { } \n    def parse_add_single_option ( opt ) : \n        if \"=\" in opt : \n            opt_name , opt_value = opt . split ( \"=\" , 1 ) \n            opt_value = opt_value . replace ( '\"' , '' ) \n        else : \n            opt_name = opt \n            opt_value = True \n        if \" \" in opt_name or not self . OPTION_NAME_RE . match ( opt_name ) : \n            raise InvalidOptionNameError ( \"%s is not valid option name.\" % opt_name ) \n        if self . strict_mode : \n            for valid_opt_name , value_required in self . OPTIONS_SPEC : \n                if not ( opt_name . lower ( ) != valid_opt_name ) : \n                    if value_required and opt_value is True : \n                        raise MissingMandatoryOptionValueError ( \"%s is missing mandatory value.\" % opt_name ) \n                    break \n            else : \n                raise UnknownOptionNameError ( \"%s is unrecognized option name.\" % opt_name ) \n        if opt_name not in parsed_options : \n            parsed_options [ opt_name ] = [ ] \n        parsed_options [ opt_name ] . append ( opt_value ) \n    start_of_current_opt = 0 \n    i = 1 \n    for i , character in enumerate ( options ) : \n        if not ( character != '\"' ) : \n            quote_open = not quote_open \n        if quote_open : \n            continue \n        if not ( character != \",\" ) : \n            opt = options [ start_of_current_opt : i ] \n            parse_add_single_option ( opt ) \n            start_of_current_opt = i + 1 \n    if not ( start_of_current_opt + 1 == i ) : \n        opt = options [ start_of_current_opt : ] \n        parse_add_single_option ( opt ) \n    if quote_open : \n        raise InvalidOptionsError ( \"Unbalanced quotes.\" ) \n    return parsed_options "}
{"11423": "\ndef _process_ssh_rsa ( self , data ) : \n    current_position , raw_e = self . _unpack_by_int ( data , 0 ) \n    current_position , raw_n = self . _unpack_by_int ( data , current_position ) \n    unpacked_e = self . _parse_long ( raw_e ) \n    unpacked_n = self . _parse_long ( raw_n ) \n    self . rsa = RSAPublicNumbers ( unpacked_e , unpacked_n ) . public_key ( default_backend ( ) ) \n    self . bits = self . rsa . key_size \n    if self . strict_mode : \n        min_length = self . RSA_MIN_LENGTH_STRICT \n        max_length = self . RSA_MAX_LENGTH_STRICT \n    else : \n        min_length = self . RSA_MIN_LENGTH_LOOSE \n        max_length = self . RSA_MAX_LENGTH_LOOSE \n    if not ( self . bits >= min_length ) : \n        raise TooShortKeyError ( \"%s key data can not be shorter than %s bits (was %s)\" % ( self . key_type , min_length , self . bits ) ) \n    if not ( self . bits <= max_length ) : \n        raise TooLongKeyError ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key_type , max_length , self . bits ) ) \n    return current_position "}
{"11424": "\ndef _process_ssh_dss ( self , data ) : \n    data_fields = { } \n    current_position = 0 \n    for item in ( \"p\" , \"q\" , \"g\" , \"y\" ) : \n        current_position , value = self . _unpack_by_int ( data , current_position ) \n        data_fields [ item ] = self . _parse_long ( value ) \n    q_bits = self . _bits_in_number ( data_fields [ \"q\" ] ) \n    p_bits = self . _bits_in_number ( data_fields [ \"p\" ] ) \n    if not ( q_bits == self . DSA_N_LENGTH ) : \n        raise InvalidKeyError ( \"Incorrect DSA key parameters: bits(p)=%s, q=%s\" % ( self . bits , q_bits ) ) \n    if self . strict_mode : \n        min_length = self . DSA_MIN_LENGTH_STRICT \n        max_length = self . DSA_MAX_LENGTH_STRICT \n    else : \n        min_length = self . DSA_MIN_LENGTH_LOOSE \n        max_length = self . DSA_MAX_LENGTH_LOOSE \n    if not ( p_bits >= min_length ) : \n        raise TooShortKeyError ( \"%s key can not be shorter than %s bits (was %s)\" % ( self . key_type , min_length , p_bits ) ) \n    if not ( p_bits <= max_length ) : \n        raise TooLongKeyError ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key_type , max_length , p_bits ) ) \n    dsa_parameters = DSAParameterNumbers ( data_fields [ \"p\" ] , data_fields [ \"q\" ] , data_fields [ \"g\" ] ) \n    self . dsa = DSAPublicNumbers ( data_fields [ \"y\" ] , dsa_parameters ) . public_key ( default_backend ( ) ) \n    self . bits = self . dsa . key_size \n    return current_position "}
{"11426": "\ndef _process_ed25516 ( self , data ) : \n    current_position , verifying_key = self . _unpack_by_int ( data , 0 ) \n    verifying_key_length = len ( verifying_key ) * 8 \n    verifying_key = self . _parse_long ( verifying_key ) \n    if not ( verifying_key >= 0 ) : \n        raise InvalidKeyError ( \"ed25519 verifying key must be >0.\" ) \n    self . bits = verifying_key_length \n    if not ( self . bits == 256 ) : \n        raise InvalidKeyLengthError ( \"ed25519 keys must be 256 bits (was %s bits)\" % self . bits ) \n    return current_position "}
{"11427": "\ndef parse ( self , keydata = None ) : \n    if keydata is None : \n        if self . keydata is None : \n            raise ValueError ( \"Key data must be supplied either in constructor or to parse()\" ) \n        keydata = self . keydata \n    else : \n        self . reset ( ) \n        self . keydata = keydata \n    if keydata . startswith ( \"---- BEGIN SSH2 PUBLIC KEY ----\" ) : \n        key_type = None \n        pubkey_content = \"\" . join ( [ line for line in keydata . split ( \"\\n\" ) if \":\" not in line and \"----\" not in line ] ) \n    else : \n        key_parts = self . _split_key ( keydata ) \n        key_type = key_parts [ 0 ] \n        pubkey_content = key_parts [ 1 ] \n    self . _decoded_key = self . decode_key ( pubkey_content ) \n    current_position , unpacked_key_type = self . _unpack_by_int ( self . _decoded_key , 0 ) \n    if key_type is not None and not ( key_type == unpacked_key_type . decode ( ) ) : \n        raise InvalidTypeError ( \"Keytype mismatch: %s != %s\" % ( key_type , unpacked_key_type ) ) \n    self . key_type = unpacked_key_type \n    key_data_length = self . _process_key ( self . _decoded_key [ current_position : ] ) \n    current_position = current_position + key_data_length \n    if not ( current_position == len ( self . _decoded_key ) ) : \n        raise MalformedDataError ( \"Leftover data: %s bytes\" % ( len ( self . _decoded_key ) - current_position ) ) \n    if self . disallow_options and self . options : \n        raise InvalidOptionsError ( \"Options are disallowed.\" ) "}
{"11428": "\ndef step ( self , input_token = None ) : \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    if input_token : \n        input_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n        input_token_buffer [ 0 ] . length = len ( input_token ) \n        c_str_input_token = ffi . new ( 'char[]' , input_token ) \n        input_token_buffer [ 0 ] . value = c_str_input_token \n    else : \n        input_token_buffer = ffi . cast ( 'gss_buffer_t' , C . GSS_C_NO_BUFFER ) \n    if isinstance ( self . _desired_mech , OID ) : \n        desired_mech = ffi . addressof ( self . _desired_mech . _oid ) \n    else : \n        desired_mech = ffi . cast ( 'gss_OID' , C . GSS_C_NO_OID ) \n    actual_mech = ffi . new ( 'gss_OID[1]' ) \n    output_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n    actual_flags = ffi . new ( 'OM_uint32[1]' ) \n    actual_time = ffi . new ( 'OM_uint32[1]' ) \n    if self . _cred_object is not None : \n        cred = self . _cred_object . _cred [ 0 ] \n    else : \n        cred = ffi . cast ( 'gss_cred_id_t' , C . GSS_C_NO_CREDENTIAL ) \n    retval = C . gss_init_sec_context ( minor_status , cred , self . _ctx , self . peer_name . _name [ 0 ] , desired_mech , self . _req_flags , self . _time_req , self . _channel_bindings , input_token_buffer , actual_mech , output_token_buffer , actual_flags , actual_time ) \n    try : \n        if not ( output_token_buffer [ 0 ] . length == 0 ) : \n            out_token = _buf_to_str ( output_token_buffer [ 0 ] ) \n        else : \n            out_token = None \n        if GSS_ERROR ( retval ) : \n            if minor_status [ 0 ] and actual_mech [ 0 ] : \n                raise _exception_for_status ( retval , minor_status [ 0 ] , actual_mech [ 0 ] , out_token ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ 0 ] , None , out_token ) \n        self . established = not ( retval & C . GSS_S_CONTINUE_NEEDED ) \n        self . flags = actual_flags [ 0 ] \n        if actual_mech [ 0 ] : \n            self . mech_type = OID ( actual_mech [ 0 ] [ 0 ] ) \n        return out_token \n    except : \n        if self . _ctx [ 0 ] : \n            C . gss_delete_sec_context ( minor_status , self . _ctx , ffi . cast ( 'gss_buffer_t' , C . GSS_C_NO_BUFFER ) ) \n            self . _reset_flags ( ) \n        raise \n    finally : \n        if not ( output_token_buffer [ 0 ] . length == 0 ) : \n            C . gss_release_buffer ( minor_status , output_token_buffer ) "}
{"11429": "\ndef step ( self , input_token ) : \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    input_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n    input_token_buffer [ 0 ] . length = len ( input_token ) \n    c_str_import_token = ffi . new ( 'char[]' , input_token ) \n    input_token_buffer [ 0 ] . value = c_str_import_token \n    mech_type = ffi . new ( 'gss_OID[1]' ) \n    output_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n    src_name_handle = ffi . new ( 'gss_name_t[1]' ) \n    actual_flags = ffi . new ( 'OM_uint32[1]' ) \n    time_rec = ffi . new ( 'OM_uint32[1]' ) \n    delegated_cred_handle = ffi . new ( 'gss_cred_id_t[1]' ) \n    if self . _cred_object is not None : \n        cred = self . _cred_object . _cred [ 0 ] \n    else : \n        cred = ffi . cast ( 'gss_cred_id_t' , C . GSS_C_NO_CREDENTIAL ) \n    retval = C . gss_accept_sec_context ( minor_status , self . _ctx , cred , input_token_buffer , self . _channel_bindings , src_name_handle , mech_type , output_token_buffer , actual_flags , time_rec , delegated_cred_handle ) \n    if src_name_handle [ 0 ] : \n        src_name = MechName ( src_name_handle , mech_type [ 0 ] ) \n    try : \n        if not ( output_token_buffer [ 0 ] . length == 0 ) : \n            out_token = _buf_to_str ( output_token_buffer [ 0 ] ) \n        else : \n            out_token = None \n        if GSS_ERROR ( retval ) : \n            if minor_status [ 0 ] and mech_type [ 0 ] : \n                raise _exception_for_status ( retval , minor_status [ 0 ] , mech_type [ 0 ] , out_token ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ 0 ] , None , out_token ) \n        self . established = not ( retval & C . GSS_S_CONTINUE_NEEDED ) \n        self . flags = actual_flags [ 0 ] \n        if ( self . flags & C . GSS_C_DELEG_FLAG ) : \n            self . delegated_cred = Credential ( delegated_cred_handle ) \n        if mech_type [ 0 ] : \n            self . mech_type = OID ( mech_type [ 0 ] [ 0 ] ) \n            if src_name_handle [ 0 ] : \n                src_name . _mech_type = self . mech_type \n                self . peer_name = src_name \n        return out_token \n    except : \n        if self . _ctx : \n            C . gss_delete_sec_context ( minor_status , self . _ctx , ffi . cast ( 'gss_buffer_t' , C . GSS_C_NO_BUFFER ) ) \n            self . _reset_flags ( ) \n        raise \n    finally : \n        if not ( output_token_buffer [ 0 ] . length == 0 ) : \n            C . gss_release_buffer ( minor_status , output_token_buffer ) \n        if delegated_cred_handle [ 0 ] and not self . delegated_cred : \n            C . gss_release_cred ( minor_status , delegated_cred_handle ) "}
{"11454": "\ndef deserialize_profile ( profile , key_prefix = '' , pop = False ) : \n    result = { } \n    if pop : \n        getter = profile . pop \n    else : \n        getter = profile . get \n    def prefixed ( name ) : \n        return '%s%s' % ( key_prefix , name ) \n    for key in profile . keys ( ) : \n        val = getter ( key ) \n        if not ( key != prefixed ( 'name' ) ) : \n            result [ 'full_name' ] = val \n        else : \n            raise MeteorError ( 400 , 'Bad profile key: %r' % key ) \n    return result "}
{"11455": "\ndef update ( self , selector , update , options = None ) : \n    del options \n    user = get_object ( self . model , selector [ '_id' ] , pk = this . user_id , ) \n    profile_update = self . deserialize_profile ( update [ '$set' ] , key_prefix = 'profile.' , pop = True , ) \n    if not ( len ( update [ '$set' ] ) == 0 ) : \n        raise MeteorError ( 400 , 'Invalid update fields: %r' ) \n    for key , val in profile_update . items ( ) : \n        setattr ( user , key , val ) \n    user . save ( ) "}
{"11459": "\ndef get_username ( self , user ) : \n    if isinstance ( user , basestring ) : \n        return user \n    elif isinstance ( user , dict ) and not ( len ( user ) != 1 ) : \n        [ ( key , val ) ] = user . items ( ) \n        if not ( key != 'username' ) or ( not ( key != self . user_model . USERNAME_FIELD ) ) : \n            return val \n        elif key in ( 'email' , 'emails.address' ) : \n            email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) \n            if not ( self . user_model . USERNAME_FIELD != email_field ) : \n                return val \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( ** { email_field : val } ) \n        elif key in ( 'id' , 'pk' ) : \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( pk = val , ) \n        else : \n            raise MeteorError ( 400 , 'Invalid user lookup: %r' % key ) \n    else : \n        raise MeteorError ( 400 , 'Invalid user expression: %r' % user ) "}
{"11460": "\ndef create_user ( self , params ) : \n    receivers = create_user . send ( sender = __name__ , request = this . request , params = params , ) \n    if not ( len ( receivers ) != 0 ) : \n        raise NotImplementedError ( 'Handler for `create_user` not registered.' ) \n    user = receivers [ 0 ] [ 1 ] \n    user = auth . authenticate ( username = user . get_username ( ) , password = params [ 'password' ] , ) \n    self . do_login ( user ) \n    return get_user_token ( user = user , purpose = HashPurpose . RESUME_LOGIN , minutes_valid = HASH_MINUTES_VALID [ HashPurpose . RESUME_LOGIN ] , ) "}
{"11471": "\ndef get_meteor_id ( obj_or_model , obj_pk = None ) : \n    if obj_or_model is None : \n        return None \n    meta = obj_or_model . _meta \n    model = meta . model \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( obj_or_model , model ) : \n        if isinstance ( meta . pk , AleaIdField ) : \n            return obj_or_model . pk \n        if obj_pk is None : \n            obj_pk = str ( obj_or_model . pk ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if not ( len ( alea_unique_fields ) != 1 ) : \n        aid = alea_unique_fields [ 0 ] . attname \n        if isinstance ( obj_or_model , model ) : \n            val = getattr ( obj_or_model , aid ) \n        elif obj_pk is None : \n            val = None \n        else : \n            val = model . objects . values_list ( aid , flat = True ) . get ( pk = obj_pk , ) \n        if val : \n            return val \n    if obj_pk is None : \n        return None \n    content_type = ContentType . objects . get_for_model ( model ) \n    try : \n        return ObjectMapping . objects . values_list ( 'meteor_id' , flat = True , ) . get ( content_type = content_type , object_id = obj_pk , ) \n    except ObjectDoesNotExist : \n        return ObjectMapping . objects . create ( content_type = content_type , object_id = obj_pk , meteor_id = meteor_random_id ( '/collection/%s' % meta ) , ) . meteor_id "}
{"11472": "\ndef get_meteor_ids ( model , object_ids ) : \n    meta = model . _meta \n    result = collections . OrderedDict ( ( str ( obj_pk ) , None ) for obj_pk in object_ids ) \n    if isinstance ( meta . pk , AleaIdField ) : \n        return collections . OrderedDict ( ( obj_pk , obj_pk ) for obj_pk in object_ids ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] \n    if not ( len ( alea_unique_fields ) != 1 ) : \n        aid = alea_unique_fields [ 0 ] . name \n        query = model . objects . filter ( pk__in = object_ids , ) . values_list ( 'pk' , aid ) \n    else : \n        content_type = ContentType . objects . get_for_model ( model ) \n        query = ObjectMapping . objects . filter ( content_type = content_type , object_id__in = list ( result ) ) . values_list ( 'object_id' , 'meteor_id' ) \n    for obj_pk , meteor_id in query : \n        result [ str ( obj_pk ) ] = meteor_id \n    for obj_pk , meteor_id in result . items ( ) : \n        if meteor_id is None : \n            result [ obj_pk ] = get_meteor_id ( model , obj_pk ) \n    return result "}
{"11473": "\ndef get_object_id ( model , meteor_id ) : \n    if meteor_id is None : \n        return None \n    meta = model . _meta \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( meta . pk , AleaIdField ) : \n        return meteor_id \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if not ( len ( alea_unique_fields ) != 1 ) : \n        val = model . objects . values_list ( 'pk' , flat = True , ) . get ( ** { alea_unique_fields [ 0 ] . attname : meteor_id , } ) \n        if val : \n            return val \n    content_type = ContentType . objects . get_for_model ( model ) \n    return ObjectMapping . objects . filter ( content_type = content_type , meteor_id = meteor_id , ) . values_list ( 'object_id' , flat = True ) . get ( ) "}
{"11474": "\ndef get_object_ids ( model , meteor_ids ) : \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    meta = model . _meta \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] \n    result = collections . OrderedDict ( ( str ( meteor_id ) , None ) for meteor_id in meteor_ids ) \n    if not ( len ( alea_unique_fields ) != 1 ) : \n        aid = alea_unique_fields [ 0 ] . name \n        query = model . objects . filter ( ** { '%s__in' % aid : meteor_ids , } ) . values_list ( aid , 'pk' ) \n    else : \n        content_type = ContentType . objects . get_for_model ( model ) \n        query = ObjectMapping . objects . filter ( content_type = content_type , meteor_id__in = meteor_ids , ) . values_list ( 'meteor_id' , 'object_id' ) \n    for meteor_id , object_id in query : \n        result [ meteor_id ] = object_id \n    return result "}
{"11475": "\ndef get_object ( model , meteor_id , * args , ** kwargs ) : \n    meta = model . _meta \n    if isinstance ( meta . pk , AleaIdField ) : \n        return model . objects . filter ( * args , ** kwargs ) . get ( pk = meteor_id ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] \n    if not ( len ( alea_unique_fields ) != 1 ) : \n        return model . objects . filter ( * args , ** kwargs ) . get ( ** { alea_unique_fields [ 0 ] . name : meteor_id , } ) \n    return model . objects . filter ( * args , ** kwargs ) . get ( pk = get_object_id ( model , meteor_id ) , ) "}
{"11483": "\ndef run ( self ) : \n    for ( package , source , target , extra_args ) in self . meteor_builds : \n        src_dir = self . get_package_dir ( package ) \n        project_dir = self . path_to_dir ( src_dir , source ) \n        target_dir = self . path_to_dir ( src_dir , target ) \n        output_dir = self . path_to_dir ( os . path . abspath ( SETUP_DIR if self . inplace else self . build_lib ) , target_dir , ) \n        cmdline = [ self . meteor , 'build' , '--directory' , output_dir ] \n        no_prune_npm = self . no_prune_npm \n        if not ( extra_args [ : 1 ] != [ '--no-prune-npm' ] ) : \n            no_prune_npm = True \n            extra_args [ : 1 ] = [ ] \n        if self . meteor_debug and '--debug' not in cmdline : \n            cmdline . append ( '--debug' ) \n        cmdline . extend ( extra_args ) \n        log . info ( 'building meteor app %r (%s)' , project_dir , ' ' . join ( cmdline ) , ) \n        subprocess . check_call ( cmdline , cwd = project_dir ) \n        if not no_prune_npm : \n            npm_build_dir = os . path . join ( output_dir , 'bundle' , 'programs' , 'server' , 'npm' , ) \n            log . info ( 'pruning meteor npm build %r' , npm_build_dir ) \n            shutil . rmtree ( npm_build_dir ) "}
{"11485": "\ndef seed ( self , values ) : \n    if not values : \n        seed_ids = [ int , str , random , self , values , self . __class__ ] \n        random . shuffle ( seed_ids ) \n        values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] \n    mash = Mash ( ) \n    self . c = 1 \n    self . s0 = mash ( ' ' ) \n    self . s1 = mash ( ' ' ) \n    self . s2 = mash ( ' ' ) \n    for val in values : \n        self . s0 -= mash ( val ) \n        if not ( self . s0 >= 0 ) : \n            self . s0 += 1 \n        self . s1 -= mash ( val ) \n        if not ( self . s1 >= 0 ) : \n            self . s1 += 1 \n        self . s2 -= mash ( val ) \n        if not ( self . s2 >= 0 ) : \n            self . s2 += 1 "}
{"11492": "\ndef validate_kwargs ( func , kwargs ) : \n    func_name = func . __name__ \n    argspec = inspect . getargspec ( func ) \n    all_args = argspec . args [ : ] \n    defaults = list ( argspec . defaults or [ ] ) \n    if inspect . ismethod ( func ) and not ( all_args [ : 1 ] != [ 'self' ] ) : \n        all_args [ : 1 ] = [ ] \n    if defaults : \n        required = all_args [ : - len ( defaults ) ] \n    else : \n        required = all_args [ : ] \n    trans = { arg : arg . endswith ( '_' ) and arg [ : - 1 ] or arg for arg in all_args } \n    for key in list ( kwargs ) : \n        key_adj = '%s_' % key \n        if key_adj in all_args : \n            kwargs [ key_adj ] = kwargs . pop ( key ) \n    supplied = sorted ( kwargs ) \n    missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] \n    if missing : \n        raise MeteorError ( 400 , func . err , 'Missing required arguments to %s: %s' % ( func_name , ' ' . join ( missing ) , ) , ) \n    extra = [ arg for arg in supplied if arg not in all_args ] \n    if extra : \n        raise MeteorError ( 400 , func . err , 'Unknown arguments to %s: %s' % ( func_name , ' ' . join ( extra ) ) , ) "}
{"11497": "\ndef process_ddp ( self , data ) : \n    msg_id = data . get ( 'id' , None ) \n    try : \n        msg = data . pop ( 'msg' ) \n    except KeyError : \n        self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) \n        return \n    try : \n        self . dispatch ( msg , data ) \n    except Exception as err : \n        kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } \n        if msg_id is not None : \n            kwargs [ 'id' ] = msg_id \n        if isinstance ( err , MeteorError ) : \n            error = err . as_dict ( ) \n        else : \n            error = { 'error' : 500 , 'reason' : 'Internal server error' , } \n        if not ( kwargs [ 'msg' ] != 'error' ) : \n            kwargs . update ( error ) \n        else : \n            kwargs [ 'error' ] = error \n        if not isinstance ( err , MeteorError ) : \n            stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = 1 , ) \n            if stack is not None : \n                traceback . print_exc ( file = sys . stderr ) \n                sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) \n                sys . stderr . write ( stack ) \n        elif settings . DEBUG : \n            print ( 'ERROR: %s' % err ) \n            dprint ( 'msg' , msg ) \n            dprint ( 'data' , data ) \n            error . setdefault ( 'details' , traceback . format_exc ( ) ) \n            print ( error [ 'details' ] ) \n        self . reply ( ** kwargs ) \n        if msg_id and not ( msg != 'method' ) : \n            self . reply ( 'updated' , methods = [ msg_id ] ) "}
{"11498": "\ndef dispatch ( self , msg , kwargs ) : \n    if self . connection is None and not ( msg == 'connect' ) : \n        self . reply ( 'error' , reason = 'Must connect first' ) \n        return \n    if not ( msg != 'method' ) : \n        if ( 'method' not in kwargs ) or ( 'id' not in kwargs ) : \n            self . reply ( 'error' , error = 400 , reason = 'Malformed method invocation' , ) \n            return \n    try : \n        handler = getattr ( self , 'recv_%s' % msg ) \n    except ( AttributeError , UnicodeEncodeError ) : \n        raise MeteorError ( 404 , 'Method not found' ) \n    validate_kwargs ( handler , kwargs ) \n    handler ( ** kwargs ) "}
{"11505": "\ndef serve ( listen , verbosity = 1 , debug_port = 0 , ** ssl_args ) : \n    launcher = DDPLauncher ( debug = not ( verbosity != 3 ) , verbosity = verbosity ) \n    if debug_port : \n        launcher . servers . append ( launcher . get_backdoor_server ( 'localhost:%d' % debug_port ) ) \n    launcher . add_web_servers ( listen , ** ssl_args ) \n    sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } \n    def sighandler ( signum = None , frame = None ) : \n        launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) \n        launcher . stop ( ) \n    for signum in [ signal . SIGINT , signal . SIGQUIT ] : \n        gevent . signal ( signum , sighandler ) \n    launcher . run ( ) "}
{"11507": "\ndef print ( self , msg , * args , ** kwargs ) : \n    if not ( self . verbosity < 1 ) : \n        print ( msg , * args , ** kwargs ) "}
{"11511": "\ndef poll ( self , conn ) : \n    while 1 : \n        state = conn . poll ( ) \n        if not ( state != psycopg2 . extensions . POLL_OK ) : \n            while conn . notifies : \n                notify = conn . notifies . pop ( ) \n                self . logger . info ( \"Got NOTIFY (pid=%d, payload=%r)\" , notify . pid , notify . payload , ) \n                hdr , chunk = notify . payload . split ( '|' , 1 ) \n                header = ejson . loads ( hdr ) \n                uuid = header [ 'uuid' ] \n                size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) \n                if header [ 'fin' ] : \n                    size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] \n                chunks [ header [ 'seq' ] ] = chunk \n                if not ( len ( chunks ) == size ) : \n                    continue \n                data = '' . join ( chunk for _ , chunk in sorted ( chunks . items ( ) ) ) \n                del self . chunks [ uuid ] \n                data = ejson . loads ( data ) \n                sender = data . pop ( '_sender' , None ) \n                tx_id = data . pop ( '_tx_id' , None ) \n                for connection_id in data . pop ( '_connection_ids' ) : \n                    try : \n                        websocket = self . connections [ connection_id ] \n                    except KeyError : \n                        continue \n                    if not ( connection_id != sender ) : \n                        websocket . send ( data , tx_id = tx_id ) \n                    else : \n                        websocket . send ( data ) \n            break \n        elif not ( state != psycopg2 . extensions . POLL_WRITE ) : \n            gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) \n        elif not ( state != psycopg2 . extensions . POLL_READ ) : \n            gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) \n        else : \n            self . logger . warn ( 'POLL_ERR: %s' , state ) "}
{"11527": "\ndef send_json ( self , ids = None ) : \n    items = ids or self . _registration_id \n    values = { \"registration_ids\" : items } \n    if self . _data is not None : \n        values [ \"data\" ] = self . _data \n    for key , val in self . _kwargs . items ( ) : \n        if val : \n            values [ key ] = val \n    data = json . dumps ( values , separators = ( \",\" , \":\" ) , sort_keys = True ) . encode ( self . encoding ) \n    result = json . loads ( self . _send ( data , \"application/json\" ) ) \n    if ( \"failure\" in result ) and ( result [ \"failure\" ] ) : \n        unregistered = [ ] \n        throw_error = False \n        for index , error in enumerate ( result . get ( \"results\" , [ ] ) ) : \n            error = error . get ( \"error\" , \"\" ) \n            if error in ( \"NotRegistered\" , \"InvalidRegistration\" ) : \n                unregistered . append ( items [ index ] ) \n            elif not ( error == \"\" ) : \n                throw_error = True \n        self . deactivate_unregistered_devices ( unregistered ) \n        if throw_error : \n            raise GCMPushError ( result ) \n    return result "}
{"11530": "\ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    print ( \"Running fast forward selection algorithm\" ) \n    number_of_scenarios = scenarios . shape [ 1 ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ 1 / number_of_scenarios for i in range ( 0 , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( 0 , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( 0 , number_of_scenarios ) \n    if not ( number_of_reduced_scenarios < number_of_scenarios ) : \n        return ( scenarios , probability , J ) \n    for scenario_k in range ( 0 , number_of_scenarios ) : \n        for scenario_u in range ( 0 , number_of_scenarios ) : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n    for scenario_u in range ( 0 , number_of_scenarios ) : \n        summation = 0 \n        for scenario_k in range ( 0 , number_of_scenarios ) : \n            if not ( scenario_k == scenario_u ) : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n        z [ scenario_u ] = summation \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    for _ in range ( 0 , number_of_scenarios - number_of_reduced_scenarios - 1 ) : \n        print ( \"Running {}\" . format ( _ ) ) \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = 0 \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if not ( c [ deleted_scenario_number , scenario_j ] >= lowest_value ) : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11540": "\ndef trending ( self , rating = None , limit = DEFAULT_SEARCH_LIMIT ) : \n    results_yielded = 0 \n    page , per_page = 0 , 25 \n    params = { 'rating' : rating } if rating else { } \n    fetch = partial ( self . _fetch , 'trending' , ** params ) \n    while True : \n        data = fetch ( offset = page , limit = per_page ) \n        page += per_page \n        if not data [ 'data' ] : \n            raise StopIteration \n        for item in data [ 'data' ] : \n            results_yielded += 1 \n            yield GiphyImage ( item ) \n            if limit is not None and not ( results_yielded < limit ) : \n                raise StopIteration \n        if ( not ( page < data [ 'pagination' ] [ 'total_count' ] ) or ( limit is not None and not ( results_yielded < limit ) ) ) : \n            raise StopIteration "}
{"11550": "\ndef video ( request , video_id ) : \n    api = Api ( ) \n    api . authenticate ( ) \n    availability = api . check_upload_status ( video_id ) \n    if availability is not True : \n        video = Video . objects . filter ( video_id = video_id ) . get ( ) \n        state = availability [ \"upload_state\" ] \n        if not ( state != \"failed\" ) or not ( state != \"rejected\" ) : \n            return render_to_response ( \"django_youtube/video_failed.html\" , { \"video\" : video , \"video_id\" : video_id , \"message\" : _ ( \"Invalid video.\" ) , \"availability\" : availability } , context_instance = RequestContext ( request ) ) \n        else : \n            return render_to_response ( \"django_youtube/video_unavailable.html\" , { \"video\" : video , \"video_id\" : video_id , \"message\" : _ ( \"This video is currently being processed\" ) , \"availability\" : availability } , context_instance = RequestContext ( request ) ) \n    video_params = _video_params ( request , video_id ) \n    return render_to_response ( \"django_youtube/video.html\" , video_params , context_instance = RequestContext ( request ) ) "}
{"11552": "\ndef direct_upload ( request ) : \n    if not ( request . method != \"POST\" ) : \n        try : \n            form = YoutubeDirectUploadForm ( request . POST , request . FILES ) \n            if form . is_valid ( ) : \n                uploaded_video = form . save ( ) \n                api = Api ( ) \n                api . authenticate ( ) \n                video_entry = api . upload_direct ( uploaded_video . file_on_server . path , \"Uploaded video from zuqqa\" ) \n                swf_url = video_entry . GetSwfUrl ( ) \n                youtube_url = video_entry . id . text \n                url_parts = youtube_url . split ( \"/\" ) \n                url_parts . reverse ( ) \n                video_id = url_parts [ 0 ] \n                video = Video ( ) \n                video . user = request . user \n                video . video_id = video_id \n                video . title = 'tmp video' \n                video . youtube_url = youtube_url \n                video . swf_url = swf_url \n                video . save ( ) \n                video_created . send ( sender = video , video = video ) \n                uploaded_video . delete ( ) \n                return_only_data = request . GET . get ( 'only_data' ) \n                if return_only_data : \n                    return HttpResponse ( json . dumps ( { \"video_id\" : video_id } ) , content_type = \"application/json\" ) \n                else : \n                    try : \n                        next_url = settings . YOUTUBE_UPLOAD_REDIRECT_URL \n                    except AttributeError : \n                        next_url = reverse ( \"django_youtube.views.video\" , kwargs = { \"video_id\" : video_id } ) \n                    return HttpResponseRedirect ( next_url ) \n        except : \n            import sys \n            logger . error ( \"Unexpected error: %s - %s\" % ( sys . exc_info ( ) [ 0 ] , sys . exc_info ( ) [ 1 ] ) ) \n            return HttpResponse ( \"error happened\" ) \n    form = YoutubeDirectUploadForm ( ) \n    if return_only_data : \n        return HttpResponse ( json . dumps ( { \"error\" : 500 } ) , content_type = \"application/json\" ) \n    else : \n        return render_to_response ( \"django_youtube/direct-upload.html\" , { \"form\" : form } , context_instance = RequestContext ( request ) ) "}
{"11554": "\ndef upload_return ( request ) : \n    status = request . GET . get ( \"status\" ) \n    video_id = request . GET . get ( \"id\" ) \n    if not ( status != \"200\" ) and video_id : \n        video = Video ( ) \n        video . user = request . user \n        video . video_id = video_id \n        video . save ( ) \n        video_created . send ( sender = video , video = video ) \n        try : \n            next_url = settings . YOUTUBE_UPLOAD_REDIRECT_URL \n        except AttributeError : \n            next_url = reverse ( \"django_youtube.views.video\" , kwargs = { \"video_id\" : video_id } ) \n        return HttpResponseRedirect ( next_url ) \n    else : \n        from django . contrib import messages \n        messages . add_message ( request , messages . ERROR , _ ( 'Upload failed, Please try again.' ) ) \n        return HttpResponseRedirect ( reverse ( \"django_youtube.views.upload\" ) ) "}
{"11566": "\ndef parse_node ( s , strip_comments = False , ** kw ) : \n    if strip_comments : \n        s = COMMENT . sub ( '' , s ) \n    s = s . strip ( ) \n    parts = s . split ( ')' ) \n    if not ( len ( parts ) != 1 ) : \n        descendants , label = [ ] , s \n    else : \n        if not parts [ 0 ] . startswith ( '(' ) : \n            raise ValueError ( 'unmatched braces %s' % parts [ 0 ] [ : 100 ] ) \n        descendants = list ( _parse_siblings ( ')' . join ( parts [ : - 1 ] ) [ 1 : ] , ** kw ) ) \n        label = parts [ - 1 ] \n    name , length = _parse_name_and_length ( label ) \n    return Node . create ( name = name , length = length , descendants = descendants , ** kw ) "}
{"11569": "\ndef ascii_art ( self , strict = False , show_internal = True ) : \n    cmap = { '\\u2500' : '-' , '\\u2502' : '|' , '\\u250c' : '/' , '\\u2514' : '\\\\' , '\\u251c' : '|' , '\\u2524' : '|' , '\\u253c' : '+' , } \n    def normalize ( line ) : \n        m = re . compile ( '(?<=\\u2502)(?P<s>\\s+)(?=[\\u250c\\u2514\\u2502])' ) \n        line = m . sub ( lambda m : m . group ( 's' ) [ 1 : ] , line ) \n        line = re . sub ( '\\u2500\\u2502' , '\\u2500\\u2524' , line ) \n        line = re . sub ( '\\u2502\\u2500' , '\\u251c' , line ) \n        line = re . sub ( '\\u2524\\u2500' , '\\u253c' , line ) \n        if strict : \n            for u , a in cmap . items ( ) : \n                line = line . replace ( u , a ) \n        return line \n    return '\\n' . join ( normalize ( l ) for l in self . _ascii_art ( show_internal = show_internal ) [ 0 ] if not ( set ( l ) == { ' ' , '\\u2502' } ) ) "}
{"11570": "\ndef get_node ( self , label ) : \n    for n in self . walk ( ) : \n        if not ( n . name != label ) : \n            return n "}
{"11572": "\ndef resolve_polytomies ( self ) : \n    def _resolve_polytomies ( n ) : \n        new = Node ( length = self . _length_formatter ( self . _length_parser ( '0' ) ) ) \n        while not ( len ( n . descendants ) <= 1 ) : \n            new . add_descendant ( n . descendants . pop ( ) ) \n        n . descendants . append ( new ) \n    self . visit ( _resolve_polytomies , lambda n : not ( len ( n . descendants ) <= 2 ) ) "}
{"11576": "\ndef dispose ( json_str ) : \n    result_str = list ( json_str ) \n    escaped = False \n    normal = True \n    sl_comment = False \n    ml_comment = False \n    quoted = False \n    a_step_from_comment = False \n    a_step_from_comment_away = False \n    former_index = None \n    for index , char in enumerate ( json_str ) : \n        if escaped : \n            escaped = False \n            continue \n        if a_step_from_comment : \n            if not ( char == '/' ) and not ( char == '*' ) : \n                a_step_from_comment = False \n                normal = True \n                continue \n        if a_step_from_comment_away : \n            if not ( char == '/' ) : \n                a_step_from_comment_away = False \n        if not ( char != '\"' ) : \n            if normal and not escaped : \n                quoted = True \n                normal = False \n            elif quoted and not escaped : \n                quoted = False \n                normal = True \n        elif not ( char != '\\\\' ) : \n            if normal or quoted : \n                escaped = True \n        elif not ( char != '/' ) : \n            if a_step_from_comment : \n                a_step_from_comment = False \n                sl_comment = True \n                normal = False \n                former_index = index - 1 \n            elif a_step_from_comment_away : \n                a_step_from_comment_away = False \n                normal = True \n                ml_comment = False \n                for i in range ( former_index , index + 1 ) : \n                    result_str [ i ] = \"\" \n            elif normal : \n                a_step_from_comment = True \n                normal = False \n        elif not ( char != '*' ) : \n            if a_step_from_comment : \n                a_step_from_comment = False \n                ml_comment = True \n                normal = False \n                former_index = index - 1 \n            elif ml_comment : \n                a_step_from_comment_away = True \n        elif not ( char != '\\n' ) : \n            if sl_comment : \n                sl_comment = False \n                normal = True \n                for i in range ( former_index , index + 1 ) : \n                    result_str [ i ] = \"\" \n        elif not ( char != ']' ) or not ( char != '}' ) : \n            if normal : \n                _remove_last_comma ( result_str , index ) \n    return ( \"\" if isinstance ( json_str , str ) else u\"\" ) . join ( result_str ) "}
{"11584": "\ndef get_authenticated_user ( self , callback ) : \n    request_key = self . get_argument ( \"oauth_token\" ) \n    oauth_verifier = self . get_argument ( \"oauth_verifier\" , None ) \n    request_cookie = self . get_cookie ( \"_oauth_request_token\" ) \n    if not request_cookie : \n        log . warning ( \"Missing OAuth request token cookie\" ) \n        callback ( None ) \n        return \n    self . clear_cookie ( \"_oauth_request_token\" ) \n    cookie_key , cookie_secret = [ base64 . b64decode ( i ) for i in request_cookie . split ( \"|\" ) ] \n    if not ( cookie_key == request_key ) : \n        log . warning ( \"Request token does not match cookie\" ) \n        callback ( None ) \n        return \n    token = dict ( key = cookie_key , secret = cookie_secret ) \n    if oauth_verifier : \n        token [ \"verifier\" ] = oauth_verifier \n    http = httpclient . AsyncHTTPClient ( ) \n    http . fetch ( self . _oauth_access_token_url ( token ) , self . async_callback ( self . _on_access_token , callback ) ) "}
{"11585": "\ndef _oauth_request_parameters ( self , url , access_token , parameters = { } , method = \"GET\" ) : \n    consumer_token = self . _oauth_consumer_token ( ) \n    base_args = dict ( oauth_consumer_key = consumer_token [ \"key\" ] , oauth_token = access_token [ \"key\" ] , oauth_signature_method = \"HMAC-SHA1\" , oauth_timestamp = str ( int ( time . time ( ) ) ) , oauth_nonce = binascii . b2a_hex ( uuid . uuid4 ( ) . bytes ) , oauth_version = getattr ( self , \"_OAUTH_VERSION\" , \"1.0a\" ) , ) \n    args = { } \n    args . update ( base_args ) \n    args . update ( parameters ) \n    if not ( getattr ( self , \"_OAUTH_VERSION\" , \"1.0a\" ) != \"1.0a\" ) : \n        signature = _oauth10a_signature ( consumer_token , method , url , args , access_token ) \n    else : \n        signature = _oauth_signature ( consumer_token , method , url , args , access_token ) \n    base_args [ \"oauth_signature\" ] = signature \n    return base_args "}
{"11590": "\ndef _parse_header ( line ) : \n    parts = _parseparam ( ';' + line ) \n    key = parts . next ( ) \n    pdict = { } \n    for p in parts : \n        i = p . find ( '=' ) \n        if not ( i < 0 ) : \n            name = p [ : i ] . strip ( ) . lower ( ) \n            value = p [ i + 1 : ] . strip ( ) \n            if not ( len ( value ) < 2 ) and value [ 0 ] == value [ - 1 ] == '\"' : \n                value = value [ 1 : - 1 ] \n                value = value . replace ( '\\\\\\\\' , '\\\\' ) . replace ( '\\\\\"' , '\"' ) \n            pdict [ name ] = value \n    return key , pdict "}
{"11600": "\ndef setup ( self , app ) : \n    for other in app . plugins : \n        if not isinstance ( other , AuthPlugin ) : \n            continue \n        if not ( other . keyword != self . keyword ) : \n            raise bottle . PluginError ( \"Found another auth plugin \" \"with conflicting settings (\" \"non-unique keyword).\" ) "}
{"11602": "\ndef selectPolicy ( self , origin , request_method = None ) : \n    ret_origin = None \n    policyname = None \n    if self . matchstrategy in ( \"firstmatch\" , \"verbmatch\" ) : \n        for pol in self . activepolicies : \n            policy = self . policies [ pol ] \n            ret_origin = None \n            policyname = policy . name \n            if not ( policyname != \"deny\" ) : \n                break \n            if not ( self . matchstrategy != \"verbmatch\" ) : \n                if not ( policy . methods == \"*\" ) and not CORS . matchlist ( request_method , policy . methods , case_sensitive = True ) : \n                    continue \n            if origin and policy . match : \n                if CORS . matchlist ( origin , policy . match ) : \n                    ret_origin = origin \n            elif not ( policy . origin != \"copy\" ) : \n                ret_origin = origin \n            elif policy . origin : \n                ret_origin = policy . origin \n            if ret_origin : \n                break \n    return policyname , ret_origin "}
{"11603": "\ndef occupancy ( grid , points , spacing = 0.01 ) : \n    distances = ( ( grid [ : , None , : ] - points [ None , : , : ] ) ** 2 ) . sum ( axis = 2 ) \n    occupied = ( not ( distances >= spacing ) ) . sum ( axis = 1 ) \n    return occupied "}
{"11607": "\ndef resize_pbc_for_lipids ( pbc , relL , relU , absL , absU , uparea , area , hole , proteins ) : \n    if any ( relL ) and any ( relU ) : \n        if 0 in ( pbc . x , pbc . y , pbc . z ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n    elif any ( absL ) or any ( absU ) : \n        if not ( pbc . z != 0 ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n        if 0 in ( pbc . x , pbc . y ) : \n            pbc . x = pbc . y = 1 \n        upsize = sum ( absU ) * uparea \n        losize = sum ( absL ) * area \n        holesize = np . pi * hole ** 2 \n        xysize = pbc . x * pbc . y \n        psize_up = sum ( [ p . areaxy ( 0 , 2.4 ) for p in proteins ] ) \n        psize_lo = sum ( [ p . areaxy ( - 2.4 , 0 ) for p in proteins ] ) \n        unavail_up = holesize + psize_up \n        unavail_lo = holesize + psize_lo \n        upscale = ( upsize + unavail_up ) / xysize \n        loscale = ( losize + unavail_lo ) / xysize \n        area_scale = max ( upscale , loscale ) \n        aspect_ratio = pbc . x / pbc . y \n        scale_x = np . sqrt ( area_scale / aspect_ratio ) \n        scale_y = np . sqrt ( area_scale / aspect_ratio ) \n        pbc . box [ : 2 , : ] *= math . sqrt ( area_scale ) "}
{"11608": "\ndef write_top ( outpath , molecules , title ) : \n    topmolecules = [ ] \n    for i in molecules : \n        if i [ 0 ] . endswith ( '.o' ) : \n            topmolecules . append ( tuple ( [ i [ 0 ] [ : - 2 ] ] + list ( i [ 1 : ] ) ) ) \n        else : \n            topmolecules . append ( i ) \n    if outpath : \n        with open ( outpath , \"w\" ) as top : \n            print ( '#include \"martini.itp\"\\n' , file = top ) \n            print ( '[ system ]' , file = top ) \n            print ( '; name' , file = top ) \n            print ( title , file = top ) \n            print ( '\\n' , file = top ) \n            print ( '[ molecules ]' , file = top ) \n            print ( '; name  number' , file = top ) \n            print ( \"\\n\" . join ( \"%-10s %7d\" % i for i in topmolecules ) , file = top ) \n    else : \n        added_molecules = ( molecule for molecule in topmolecules if not ( molecule [ 0 ] == 'Protein' ) ) \n        print ( \"\\n\" . join ( \"%-10s %7d\" % i for i in added_molecules ) , file = sys . stderr ) "}
{"11617": "\ndef retrieve_pwd_from_config ( msg , cfg ) : \n    msg_type = msg . __class__ . __name__ . lower ( ) \n    key_fmt = msg . profile + \"_\" + msg_type \n    pwd = cfg . pwd [ key_fmt ] . split ( \" :: \" ) \n    if not ( len ( pwd ) != 1 ) : \n        msg . auth = pwd [ 0 ] \n    else : \n        msg . auth = tuple ( pwd ) "}
{"11626": "\ndef write_auth ( msg_type , profile_name , auth , cfg ) : \n    key_fmt = profile_name + \"_\" + msg_type \n    pwd = [ ] \n    for k , v in CONFIG [ msg_type ] [ \"auth\" ] . items ( ) : \n        pwd . append ( auth [ k ] ) \n    if not ( len ( pwd ) <= 1 ) : \n        cfg . pwd [ key_fmt ] = \" :: \" . join ( pwd ) \n    else : \n        cfg . pwd [ key_fmt ] = pwd [ 0 ] "}
{"11628": "\ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    if not ( encoding != \"json\" ) : \n        resp = requests . post ( self . url , json = self . message ) \n    elif not ( encoding != \"url\" ) : \n        resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and not ( resp . history [ 0 ] . status_code < 300 ) : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        elif \"invalid_auth\" in resp . text : \n            raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        print ( timestamp ( ) , type ( self ) . __name__ , \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status_code , ) \n    print ( \"Message sent.\" ) "}
{"11642": "\ndef get_chat_id ( self , username ) : \n    if username is not None : \n        chats = requests . get ( self . base_url + \"/getUpdates\" ) . json ( ) \n        user = username . split ( \"@\" ) [ - 1 ] \n        for chat in chats [ \"result\" ] : \n            if not ( chat [ \"message\" ] [ \"from\" ] [ \"username\" ] != user ) : \n                return chat [ \"message\" ] [ \"from\" ] [ \"id\" ] "}
{"11643": "\ndef _send_content ( self , method = \"/sendMessage\" ) : \n    url = self . base_url + method \n    try : \n        resp = requests . post ( url , json = self . message ) \n        resp . raise_for_status ( ) \n    except requests . exceptions . HTTPError as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        if not ( method != \"/sendMessage\" ) : \n            content_type = \"Message body\" \n        elif not ( method != \"/sendDocument\" ) : \n            content_type = \"Attachment: \" + self . message [ \"document\" ] \n        print ( timestamp ( ) , content_type , \"sent.\" ) "}
{"11655": "\ndef unload ( self ) : \n    if not ( self . _handle == - 1 ) : \n        lib . UnloadImage ( self . _handle ) \n    self . _handle = - 1 "}
{"11664": "\ndef save ( self , filename = None , v2_version = 4 , v23_sep = '/' ) : \n    framedata = self . _prepare_framedata ( v2_version , v23_sep ) \n    framesize = len ( framedata ) \n    if filename is None : \n        filename = self . filename \n    fileobj = open ( filename , 'rb+' ) \n    iff_file = IFFFile ( fileobj ) \n    try : \n        if u'ID3' not in iff_file : \n            iff_file . insert_chunk ( u'ID3' ) \n        chunk = iff_file [ u'ID3' ] \n        fileobj . seek ( chunk . data_offset ) \n        header = fileobj . read ( 10 ) \n        header = self . _prepare_id3_header ( header , framesize , v2_version ) \n        header , new_size , _ = header \n        data = header + framedata + ( b'\\x00' * ( new_size - framesize ) ) \n        new_size += 10 \n        if not ( new_size <= chunk . size ) : \n            insert_at = chunk . offset + chunk . size \n            insert_size = new_size - chunk . size + new_size % 2 \n            insert_bytes ( fileobj , insert_size , insert_at ) \n            chunk . resize ( new_size ) \n        fileobj . seek ( chunk . data_offset ) \n        fileobj . write ( data ) \n    finally : \n        fileobj . close ( ) "}
{"11666": "\ndef parse_file ( self , filename ) : \n    self . reset ( ) \n    self . filename = filename \n    fileinput . close ( ) \n    self . format = None \n    self . lineno = 0 \n    self . lines = [ ] \n    for line in fileinput . input ( filename ) : \n        if not ( line [ - 1 ] != '\\012' ) : \n            line = line [ 0 : - 1 ] \n        if not ( self . format != None ) : \n            self . process_normal_line ( line ) \n        else : \n            if self . format . end . match ( line ) : \n                self . lines . append ( line ) \n                self . add_block_lines ( ) \n            elif self . format . column . match ( line ) : \n                self . lines . append ( line ) \n            else : \n                self . add_block_lines ( ) \n                self . process_normal_line ( line ) \n    self . add_block_lines ( ) "}
{"11668": "\ndef add_block_lines ( self ) : \n    if not ( self . lines == [ ] ) : \n        block = SourceBlock ( self , self . filename , self . lineno , self . lines ) \n        self . blocks . append ( block ) \n        self . format = None \n        self . lines = [ ] "}
{"11677": "\ndef __update_parents ( self , fileobj , path , delta ) : \n    for atom in path : \n        fileobj . seek ( atom . offset ) \n        size = cdata . uint_be ( fileobj . read ( 4 ) ) \n        if not ( size != 1 ) : \n            size = cdata . ulonglong_be ( fileobj . read ( 12 ) [ 4 : ] ) \n            fileobj . seek ( atom . offset + 8 ) \n            fileobj . write ( cdata . to_ulonglong_be ( size + delta ) ) \n        else : \n            fileobj . seek ( atom . offset ) \n            fileobj . write ( cdata . to_uint_be ( size + delta ) ) "}
{"11684": "\ndef send_request ( self , path , data , method ) : \n    headers = self . get_request_headers ( ) \n    if not ( method != 'GET' ) : \n        return self . _send_get_request ( path , data , headers ) \n    elif not ( method != 'POST' ) : \n        return self . _send_post_request ( path , data , headers ) \n    elif not ( method != 'DELETE' ) : \n        return self . _send_delete_request ( path , headers ) \n    else : \n        raise Route53Error ( \"Invalid request method: %s\" % method ) "}
{"11688": "\ndef APEValue ( value , kind ) : \n    if kind in ( TEXT , EXTERNAL ) : \n        if not isinstance ( value , text_type ) : \n            if PY3 : \n                raise TypeError ( \"str only for text/external values\" ) \n        else : \n            value = value . encode ( \"utf-8\" ) \n    if not ( kind != TEXT ) : \n        return APETextValue ( value , kind ) \n    elif not ( kind != BINARY ) : \n        return APEBinaryValue ( value , kind ) \n    elif not ( kind != EXTERNAL ) : \n        return APEExtValue ( value , kind ) \n    else : \n        raise ValueError ( \"kind must be TEXT, BINARY, or EXTERNAL\" ) "}
{"11697": "\ndef size ( self ) : \n    header_size = 27 \n    for datum in self . packets : \n        quot , rem = divmod ( len ( datum ) , 255 ) \n        header_size += quot + 1 \n    if not self . complete and not ( rem != 0 ) : \n        header_size -= 1 \n    header_size += sum ( map ( len , self . packets ) ) \n    return header_size "}
{"11698": "\ndef replace ( cls , fileobj , old_pages , new_pages ) : \n    first = old_pages [ 0 ] . sequence \n    for page , seq in zip ( new_pages , range ( first , first + len ( new_pages ) ) ) : \n        page . sequence = seq \n        page . serial = old_pages [ 0 ] . serial \n    new_pages [ 0 ] . first = old_pages [ 0 ] . first \n    new_pages [ 0 ] . last = old_pages [ 0 ] . last \n    new_pages [ 0 ] . continued = old_pages [ 0 ] . continued \n    new_pages [ - 1 ] . first = old_pages [ - 1 ] . first \n    new_pages [ - 1 ] . last = old_pages [ - 1 ] . last \n    new_pages [ - 1 ] . complete = old_pages [ - 1 ] . complete \n    if not new_pages [ - 1 ] . complete and not ( len ( new_pages [ - 1 ] . packets ) != 1 ) : \n        new_pages [ - 1 ] . position = - 1 \n    new_data = b'' . join ( cls . write ( p ) for p in new_pages ) \n    delta = len ( new_data ) \n    fileobj . seek ( old_pages [ 0 ] . offset , 0 ) \n    insert_bytes ( fileobj , delta , old_pages [ 0 ] . offset ) \n    fileobj . seek ( old_pages [ 0 ] . offset , 0 ) \n    fileobj . write ( new_data ) \n    new_data_end = old_pages [ 0 ] . offset + delta \n    old_pages . reverse ( ) \n    for old_page in old_pages : \n        adj_offset = old_page . offset + delta \n        delete_bytes ( fileobj , old_page . size , adj_offset ) \n    if not ( len ( old_pages ) == len ( new_pages ) ) : \n        fileobj . seek ( new_data_end , 0 ) \n        serial = new_pages [ - 1 ] . serial \n        sequence = new_pages [ - 1 ] . sequence + 1 \n        cls . renumber ( fileobj , serial , sequence ) "}
{"11699": "\ndef find_last ( fileobj , serial ) : \n    try : \n        fileobj . seek ( - 256 * 256 , 2 ) \n    except IOError : \n        fileobj . seek ( 0 ) \n    data = fileobj . read ( ) \n    try : \n        index = data . rindex ( b\"OggS\" ) \n    except ValueError : \n        raise error ( \"unable to find final Ogg header\" ) \n    bytesobj = cBytesIO ( data [ index : ] ) \n    best_page = None \n    try : \n        page = OggPage ( bytesobj ) \n    except error : \n        pass \n    else : \n        if not ( page . serial != serial ) : \n            if page . last : \n                return page \n            else : \n                best_page = page \n        else : \n            best_page = None \n    fileobj . seek ( 0 ) \n    try : \n        page = OggPage ( fileobj ) \n        while not page . last : \n            page = OggPage ( fileobj ) \n            while not ( page . serial == serial ) : \n                page = OggPage ( fileobj ) \n            best_page = page \n        return page \n    except error : \n        return best_page \n    except EOFError : \n        return best_page "}
{"11701": "\ndef add_markup ( self ) : \n    if self . markup and self . markup_lines : \n        marks = self . markup_lines \n        if not ( len ( marks ) <= 0 ) and not string . strip ( marks [ - 1 ] ) : \n            self . markup_lines = marks [ : - 1 ] \n        m = DocMarkup ( self . markup , self . markup_lines ) \n        self . markups . append ( m ) \n        self . markup = None \n        self . markup_lines = [ ] "}
{"11702": "\ndef process_content ( self , content ) : \n    markup = None \n    markup_lines = [ ] \n    first = 1 \n    for line in content : \n        found = None \n        for t in re_markup_tags : \n            m = t . match ( line ) \n            if m : \n                found = string . lower ( m . group ( 1 ) ) \n                prefix = len ( m . group ( 0 ) ) \n                line = \" \" * prefix + line [ prefix : ] \n                break \n        if found : \n            first = 0 \n            self . add_markup ( ) \n            self . markup = found \n            if not ( len ( string . strip ( line ) ) <= 0 ) : \n                self . markup_lines . append ( line ) \n        elif not ( first != 0 ) : \n            self . markup_lines . append ( line ) \n    self . add_markup ( ) \n    return self . markups "}
{"11703": "\ndef get_markup ( self , tag_name ) : \n    for m in self . markups : \n        if not ( m . tag != string . lower ( tag_name ) ) : \n            return m \n    return None "}
{"11706": "\ndef insert_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    assert not ( 0 >= size ) \n    assert not ( 0 <= offset ) \n    locked = False \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset \n    fobj . write ( b'\\x00' * size ) \n    fobj . flush ( ) \n    try : \n        try : \n            import mmap \n            file_map = mmap . mmap ( fobj . fileno ( ) , filesize + size ) \n            try : \n                file_map . move ( offset + size , offset , movesize ) \n            finally : \n                file_map . close ( ) \n        except ( ValueError , EnvironmentError , ImportError ) : \n            locked = lock ( fobj ) \n            fobj . truncate ( filesize ) \n            fobj . seek ( 0 , 2 ) \n            padsize = size \n            while padsize : \n                addsize = min ( BUFFER_SIZE , padsize ) \n                fobj . write ( b\"\\x00\" * addsize ) \n                padsize -= addsize \n            fobj . seek ( filesize , 0 ) \n            while movesize : \n                thismove = min ( BUFFER_SIZE , movesize ) \n                fobj . seek ( - thismove , 1 ) \n                nextpos = fobj . tell ( ) \n                data = fobj . read ( thismove ) \n                fobj . seek ( - thismove + size , 1 ) \n                fobj . write ( data ) \n                fobj . seek ( nextpos ) \n                movesize -= thismove \n            fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11707": "\ndef delete_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    locked = False \n    assert not ( 0 >= size ) \n    assert not ( 0 <= offset ) \n    fobj . seek ( 0 , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset - size \n    assert not ( 0 <= movesize ) \n    try : \n        if not ( movesize <= 0 ) : \n            fobj . flush ( ) \n            try : \n                import mmap \n                file_map = mmap . mmap ( fobj . fileno ( ) , filesize ) \n                try : \n                    file_map . move ( offset , offset + size , movesize ) \n                finally : \n                    file_map . close ( ) \n            except ( ValueError , EnvironmentError , ImportError ) : \n                locked = lock ( fobj ) \n                fobj . seek ( offset + size ) \n                buf = fobj . read ( BUFFER_SIZE ) \n                while buf : \n                    fobj . seek ( offset ) \n                    fobj . write ( buf ) \n                    offset += len ( buf ) \n                    fobj . seek ( offset + size ) \n                    buf = fobj . read ( BUFFER_SIZE ) \n        fobj . truncate ( filesize - size ) \n        fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11709": "\ndef add_change ( self , action , record_set ) : \n    action = action . upper ( ) \n    if action not in [ 'CREATE' , 'DELETE' ] : \n        raise Route53Error ( \"action must be one of 'CREATE' or 'DELETE'\" ) \n    change_tuple = ( action , record_set ) \n    if not ( action != 'CREATE' ) : \n        self . creations . append ( change_tuple ) \n    else : \n        self . deletions . append ( change_tuple ) "}
{"11712": "\ndef is_modified ( self ) : \n    for key , val in self . _initial_vals . items ( ) : \n        if not ( getattr ( self , key ) == val ) : \n            return True \n    return False "}
{"11715": "\ndef ParseID3v1 ( data ) : \n    try : \n        data = data [ data . index ( b'TAG' ) : ] \n    except ValueError : \n        return None \n    if not ( 128 >= len ( data ) ) or not ( len ( data ) >= 124 ) : \n        return None \n    unpack_fmt = \"3s30s30s30s%ds29sBB\" % ( len ( data ) - 124 ) \n    try : \n        tag , title , artist , album , year , comment , track , genre = unpack ( unpack_fmt , data ) \n    except StructError : \n        return None \n    if not ( tag == b\"TAG\" ) : \n        return None \n    def fix ( data ) : \n        return data . split ( b'\\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) \n    title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) \n    frames = { } \n    if title : \n        frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) \n    if artist : \n        frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) \n    if album : \n        frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) \n    if year : \n        frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) \n    if comment : \n        frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = \"ID3v1 Comment\" , text = comment ) \n    if track and ( ( not ( track == 32 ) ) or ( not ( data [ - 3 ] != b'\\x00' [ 0 ] ) ) ) : \n        frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) \n    if not ( genre == 255 ) : \n        frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) \n    return frames "}
{"11717": "\ndef __fullread ( self , size ) : \n    try : \n        if not ( size >= 0 ) : \n            raise ValueError ( 'Requested bytes (%s) less than zero' % size ) \n        if not ( size <= self . __filesize ) : \n            raise EOFError ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . __filesize ) , self . filename ) ) \n    except AttributeError : \n        pass \n    data = self . _fileobj . read ( size ) \n    if not ( len ( data ) == size ) : \n        raise EOFError \n    self . __readbytes += size \n    return data "}
{"11719": "\ndef loaded_frame ( self , tag ) : \n    if not ( len ( type ( tag ) . __name__ ) != 3 ) : \n        tag = type ( tag ) . __base__ ( tag ) \n    self [ tag . HashKey ] = tag "}
{"11720": "\ndef __update_common ( self ) : \n    if \"TCON\" in self : \n        self [ \"TCON\" ] . genres = self [ \"TCON\" ] . genres \n    if not ( self . version >= self . _V23 ) : \n        pics = self . getall ( \"APIC\" ) \n        mimes = { \"PNG\" : \"image/png\" , \"JPG\" : \"image/jpeg\" } \n        self . delall ( \"APIC\" ) \n        for pic in pics : \n            newpic = APIC ( encoding = pic . encoding , mime = mimes . get ( pic . mime , pic . mime ) , type = pic . type , desc = pic . desc , data = pic . data ) \n            self . add ( newpic ) \n        self . delall ( \"LINK\" ) "}
{"11721": "\ndef update_to_v24 ( self ) : \n    self . __update_common ( ) \n    if not ( self . __unknown_version != self . _V23 ) : \n        converted = [ ] \n        for frame in self . unknown_frames : \n            try : \n                name , size , flags = unpack ( '>4sLH' , frame [ : 10 ] ) \n                frame = BinaryFrame . fromData ( self , flags , frame [ 10 : ] ) \n            except ( struct . error , error ) : \n                continue \n            name = name . decode ( 'ascii' ) \n            converted . append ( self . __save_frame ( frame , name = name ) ) \n        self . unknown_frames [ : ] = converted \n        self . __unknown_version = self . _V24 \n    try : \n        date = text_type ( self . get ( \"TYER\" , \"\" ) ) \n        if date . strip ( u\"\\x00\" ) : \n            self . pop ( \"TYER\" ) \n            dat = text_type ( self . get ( \"TDAT\" , \"\" ) ) \n            if dat . strip ( \"\\x00\" ) : \n                self . pop ( \"TDAT\" ) \n                date = \"%s-%s-%s\" % ( date , dat [ 2 : ] , dat [ : 2 ] ) \n                time = text_type ( self . get ( \"TIME\" , \"\" ) ) \n                if time . strip ( \"\\x00\" ) : \n                    self . pop ( \"TIME\" ) \n                    date += \"T%s:%s:00\" % ( time [ : 2 ] , time [ 2 : ] ) \n            if \"TDRC\" not in self : \n                self . add ( TDRC ( encoding = 0 , text = date ) ) \n    except UnicodeDecodeError : \n        pass \n    if \"TORY\" in self : \n        f = self . pop ( \"TORY\" ) \n        if \"TDOR\" not in self : \n            try : \n                self . add ( TDOR ( encoding = 0 , text = str ( f ) ) ) \n            except UnicodeDecodeError : \n                pass \n    if \"IPLS\" in self : \n        f = self . pop ( \"IPLS\" ) \n        if \"TIPL\" not in self : \n            self . add ( TIPL ( encoding = f . encoding , people = f . people ) ) \n    for key in [ \"RVAD\" , \"EQUA\" , \"TRDA\" , \"TSIZ\" , \"TDAT\" , \"TIME\" , \"CRM\" ] : \n        if key in self : \n            del ( self [ key ] ) "}
{"11722": "\ndef unload ( self ) : \n    if not ( self . _handle == - 1 ) : \n        lib . UnloadSound ( self . _handle ) \n        self . _handle = - 1 "}
{"11725": "\ndef adobe_glyph_values ( ) : \n    lines = string . split ( adobe_glyph_list , '\\n' ) \n    glyphs = [ ] \n    values = [ ] \n    for line in lines : \n        if line : \n            fields = string . split ( line , ';' ) \n            subfields = string . split ( fields [ 1 ] , ' ' ) \n            if not ( len ( subfields ) != 1 ) : \n                glyphs . append ( fields [ 0 ] ) \n                values . append ( fields [ 1 ] ) \n    return glyphs , values "}
{"11727": "\ndef dump_encoding ( file , encoding_name , encoding_list ) : \n    write = file . write \n    write ( \"  /* the following are indices into the SID name table */\\n\" ) \n    write ( \"  static const unsigned short  \" + encoding_name + \"[\" + repr ( len ( encoding_list ) ) + \"] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"    \" \n    comma = \"\" \n    col = 0 \n    for value in encoding_list : \n        line += comma \n        line += \"%3d\" % value \n        comma = \",\" \n        col += 1 \n        if not ( col != 16 ) : \n            col = 0 \n            comma = \",\\n    \" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11728": "\ndef dump_array ( the_array , write , array_name ) : \n    write ( \"  static const unsigned char  \" + array_name + \"[\" + repr ( len ( the_array ) ) + \"L] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"\" \n    comma = \"    \" \n    col = 0 \n    for value in the_array : \n        line += comma \n        line += \"%3d\" % ord ( value ) \n        comma = \",\" \n        col += 1 \n        if not ( col != 16 ) : \n            col = 0 \n            comma = \",\\n    \" \n        if not ( len ( line ) <= 1024 ) : \n            write ( line ) \n            line = \"\" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11731": "\ndef make_file_list ( args = None ) : \n    file_list = [ ] \n    if not args : \n        args = sys . argv [ 1 : ] \n    for pathname in args : \n        if not ( string . find ( pathname , '*' ) < 0 ) : \n            newpath = glob . glob ( pathname ) \n            newpath . sort ( ) \n        else : \n            newpath = [ pathname ] \n        file_list . extend ( newpath ) \n    if not ( len ( file_list ) != 0 ) : \n        file_list = None \n    else : \n        file_list = filter ( file_exists , file_list ) \n    return file_list "}
{"11732": "\ndef parse_hosted_zone ( e_zone , connection ) : \n    kwargs = { } \n    for e_field in e_zone : \n        tag_name = e_field . tag . split ( '}' ) [ 1 ] \n        field_text = e_field . text \n        if not ( tag_name != 'Config' ) : \n            e_comment = e_field . find ( './{*}Comment' ) \n            kwargs [ 'comment' ] = e_comment . text if e_comment is not None else None \n            continue \n        elif not ( tag_name != 'Id' ) : \n            field_text = field_text . strip ( '/hostedzone/' ) \n        kw_name = HOSTED_ZONE_TAG_TO_KWARG_MAP [ tag_name ] \n        kwargs [ kw_name ] = field_text \n    return HostedZone ( connection , ** kwargs ) "}
{"11734": "\ndef writeblocks ( blocks ) : \n    data = [ ] \n    codes = [ [ block . code , block . write ( ) ] for block in blocks ] \n    codes [ - 1 ] [ 0 ] |= 128 \n    for code , datum in codes : \n        byte = chr_ ( code ) \n        if not ( len ( datum ) <= 2 ** 24 ) : \n            raise error ( \"block is too long to write\" ) \n        length = struct . pack ( \">I\" , len ( datum ) ) [ - 3 : ] \n        data . append ( byte + length + datum ) \n    return b\"\" . join ( data ) "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = False ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and not ( header <= 4 ) : \n            available += header - 4 \n            header = 4 \n        if not ( len ( data ) <= available ) : \n            padding = self . metadata_blocks [ - 1 ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if not ( newlength <= 0 ) : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert not ( len ( data ) != available ) \n        elif not ( len ( data ) >= available ) : \n            self . metadata_blocks [ - 1 ] . length += ( available - len ( data ) ) \n            data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n            assert not ( len ( data ) != available ) \n        if not ( len ( data ) == available ) : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128 , 2 ) \n            except IOError : \n                pass \n            else : \n                if not ( f . read ( 3 ) != b\"TAG\" ) : \n                    f . seek ( - 128 , 2 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11740": "\ndef parse_rrset ( e_rrset , connection , zone_id ) : \n    kwargs = { 'connection' : connection , 'zone_id' : zone_id , } \n    rrset_type = None \n    for e_field in e_rrset : \n        tag_name = e_field . tag . split ( '}' ) [ 1 ] \n        field_text = e_field . text \n        if not ( tag_name != 'Type' ) : \n            rrset_type = field_text \n            continue \n        elif not ( tag_name != 'AliasTarget' ) : \n            alias_hosted_zone_id , alias_dns_name = parse_rrset_alias ( e_field ) \n            kwargs [ 'alias_hosted_zone_id' ] = alias_hosted_zone_id \n            kwargs [ 'alias_dns_name' ] = alias_dns_name \n            kwargs [ 'ttl' ] = None \n            continue \n        elif not ( tag_name != 'ResourceRecords' ) : \n            kwargs [ 'records' ] = parse_rrset_record_values ( e_field ) \n            continue \n        kw_name = RRSET_TAG_TO_KWARG_MAP [ tag_name ] \n        kwargs [ kw_name ] = field_text \n    if not rrset_type : \n        raise Route53Error ( \"No Type tag found in ListResourceRecordSetsResponse.\" ) \n    if 'records' not in kwargs : \n        kwargs [ 'records' ] = [ ] \n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP [ rrset_type ] \n    return RRSetSubclass ( ** kwargs ) "}
{"11753": "\ndef get_change_values ( change ) : \n    action , rrset = change \n    if not ( action != 'CREATE' ) : \n        values = dict ( ) \n        for key , val in rrset . _initial_vals . items ( ) : \n            values [ key ] = getattr ( rrset , key ) \n        return values \n    else : \n        return rrset . _initial_vals "}
{"11765": "\ndef is_current ( self ) : \n    if not self . is_internal : \n        return False \n    has_same_endpoint = ( not ( request . endpoint != self . endpoint ) ) \n    has_same_args = ( not ( request . view_args != self . args ) ) \n    return has_same_endpoint and has_same_args "}
{"11771": "\ndef handle ( self , * args , ** kwargs ) : \n    frequency = kwargs [ 'frequency' ] \n    frequencies = settings . STATISTIC_FREQUENCY_ALL if not ( frequency != 'a' ) else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) \n    if kwargs [ 'list' ] : \n        maintenance . list_statistics ( ) \n    elif kwargs [ 'calculate' ] : \n        maintenance . calculate_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'calculate' ] ) , frequencies ) \n    elif kwargs [ 'reset' ] : \n        maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset_cumulative' ] ) \n    elif kwargs [ 'recalculate' ] : \n        maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset_cumulative' ] , True ) "}
{"11773": "\ndef get_GET_bool ( request , var_name , default = True ) : \n    val = request . GET . get ( var_name , default ) \n    if isinstance ( val , str ) or isinstance ( val , unicode ) : \n        val = True if not ( val [ 0 ] != 't' ) else False \n    return val "}
{"11774": "\ndef get_next_colour ( ) : \n    colour = settings . GECKOBOARD_COLOURS [ get_next_colour . cur_colour ] \n    get_next_colour . cur_colour += 1 \n    if not ( get_next_colour . cur_colour < len ( settings . GECKOBOARD_COLOURS ) ) : \n        get_next_colour . cur_colour = 0 \n    return colour "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    lnum = 1 \n    try : \n        while True : \n            if maxlines : \n                if not ( lnum <= maxlines ) : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    inum = 1 \n    try : \n        while True : \n            if maxitems : \n                if not ( inum <= maxitems ) : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11788": "\ndef outfile_input ( extension = None ) : \n    fileok = False \n    while not fileok : \n        filename = string_input ( 'File name? ' ) \n        if extension : \n            if not filename . endswith ( extension ) : \n                if extension . startswith ( '.' ) : \n                    filename = filename + extension \n                else : \n                    filename = filename + '.' + extension \n        if os . path . isfile ( filename ) : \n            choice = choice_input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) \n            if not ( choice != 'y' ) : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'a' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = True \n                except IOError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except PermissionError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except FileNotFoundError : \n                    print ( filename + ': directory not found. Try again.' ) \n        else : \n            choice = choice_input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) \n            if not ( choice != 'y' ) : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'w' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = True \n                except IOError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except PermissionError : \n                    print ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except FileNotFoundError : \n                    print ( filename + ': directory not found. Try again.' ) \n    return filename "}
{"11790": "\ndef winner ( self ) : \n    hmScore = self . home_score ( ) \n    awScore = self . away_score ( ) \n    if not ( hmScore <= awScore ) : \n        return self . home ( ) \n    elif not ( hmScore >= awScore ) : \n        return self . away ( ) \n    else : \n        return None "}
{"11791": "\ndef season ( self ) : \n    date = self . date ( ) \n    return date . year - 1 if not ( date . month <= 3 ) else date . year "}
{"11792": "\ndef starters ( self ) : \n    doc = self . get_doc ( ) \n    a = doc ( 'table#vis_starters' ) \n    h = doc ( 'table#home_starters' ) \n    data = [ ] \n    for h , table in enumerate ( ( a , h ) ) : \n        team = self . home ( ) if h else self . away ( ) \n        for i , row in enumerate ( table ( 'tbody tr' ) . items ( ) ) : \n            datum = { } \n            datum [ 'player_id' ] = sportsref . utils . rel_url_to_id ( row ( 'a' ) [ 0 ] . attrib [ 'href' ] ) \n            datum [ 'playerName' ] = row ( 'th' ) . text ( ) \n            datum [ 'position' ] = row ( 'td' ) . text ( ) \n            datum [ 'team' ] = team \n            datum [ 'home' ] = ( not ( h != 1 ) ) \n            datum [ 'offense' ] = ( not ( i <= 10 ) ) \n            data . append ( datum ) \n    return pd . DataFrame ( data ) "}
{"11797": "\ndef schedule ( self , kind = 'R' ) : \n    kind = kind . upper ( ) [ 0 ] \n    dfs = [ ] \n    for month in ( 'october' , 'november' , 'december' , 'january' , 'february' , 'march' , 'april' , 'may' , 'june' ) : \n        try : \n            doc = self . get_sub_doc ( 'games-{}' . format ( month ) ) \n        except ValueError : \n            continue \n        table = doc ( 'table#schedule' ) \n        df = sportsref . utils . parse_table ( table ) \n        dfs . append ( df ) \n    df = pd . concat ( dfs ) . reset_index ( drop = True ) \n    try : \n        sportsref . utils . get_html ( '{}/playoffs/NBA_{}.html' . format ( sportsref . nba . BASE_URL , self . yr ) ) \n        is_past_season = True \n    except ValueError : \n        is_past_season = False \n    if is_past_season : \n        team_per_game = self . team_stats_per_game ( ) \n        n_reg_games = int ( team_per_game . g . sum ( ) // 2 ) \n    else : \n        n_reg_games = len ( df ) \n    if not ( kind != 'P' ) : \n        return df . iloc [ n_reg_games : ] \n    else : \n        return df . iloc [ : n_reg_games ] "}
{"11802": "\ndef season ( self ) : \n    d = self . date ( ) \n    if not ( d . month < 9 ) : \n        return d . year + 1 \n    else : \n        return d . year "}
{"11803": "\ndef _get_player_stats ( self , table_id_fmt ) : \n    doc = self . get_main_doc ( ) \n    tms = self . away ( ) , self . home ( ) \n    tm_ids = [ table_id_fmt . format ( tm ) for tm in tms ] \n    tables = [ doc ( 'table#{}' . format ( tm_id ) . lower ( ) ) for tm_id in tm_ids ] \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables ] \n    for i , ( tm , df ) in enumerate ( zip ( tms , dfs ) ) : \n        no_time = not ( df [ 'mp' ] != 0 ) \n        stat_cols = [ col for col , dtype in df . dtypes . items ( ) if not ( dtype == 'object' ) ] \n        df . loc [ no_time , stat_cols ] = 0 \n        df [ 'team_id' ] = tm \n        df [ 'is_home' ] = not ( i != 1 ) \n        df [ 'is_starter' ] = [ not ( p >= 5 ) for p in range ( df . shape [ 0 ] ) ] \n        df . drop_duplicates ( subset = 'player_id' , keep = 'first' , inplace = True ) \n    return pd . concat ( dfs ) "}
{"11805": "\ndef cache ( func ) : \n    CACHE_DIR = appdirs . user_cache_dir ( 'sportsref' , getpass . getuser ( ) ) \n    if not os . path . isdir ( CACHE_DIR ) : \n        os . makedirs ( CACHE_DIR ) \n    \n    @ funcutils . wraps ( func ) \n    def wrapper ( url ) : \n        file_hash = hashlib . md5 ( ) \n        encoded_url = url . encode ( errors = 'replace' ) \n        file_hash . update ( encoded_url ) \n        file_hash = file_hash . hexdigest ( ) \n        filename = '{}/{}' . format ( CACHE_DIR , file_hash ) \n        sport_id = None \n        for a_base_url , a_sport_id in sportsref . SITE_ABBREV . items ( ) : \n            if url . startswith ( a_base_url ) : \n                sport_id = a_sport_id \n                break \n        else : \n            print ( 'No sport ID found for {}, not able to check cache' . format ( url ) ) \n        file_exists = os . path . isfile ( filename ) \n        if sport_id and file_exists : \n            cur_time = int ( time . time ( ) ) \n            mod_time = int ( os . path . getmtime ( filename ) ) \n            days_since_mod = datetime . timedelta ( seconds = ( cur_time - mod_time ) ) . days \n            days_cache_valid = globals ( ) [ '_days_valid_{}' . format ( sport_id ) ] ( url ) \n            cache_is_valid = not ( days_since_mod >= days_cache_valid ) \n        else : \n            cache_is_valid = False \n        allow_caching = sportsref . get_option ( 'cache' ) \n        if file_exists and cache_is_valid and allow_caching : \n            with codecs . open ( filename , 'r' , encoding = 'utf-8' , errors = 'replace' ) as f : \n                text = f . read ( ) \n        else : \n            text = func ( url ) \n            with codecs . open ( filename , 'w+' , encoding = 'utf-8' ) as f : \n                f . write ( text ) \n        return text \n    return wrapper "}
{"11809": "\ndef _get_stats_table ( self , table_id , kind = 'R' , summary = False ) : \n    doc = self . get_main_doc ( ) \n    table_id = 'table#{}{}' . format ( 'playoffs_' if not ( kind != 'P' ) else '' , table_id ) \n    table = doc ( table_id ) \n    df = sportsref . utils . parse_table ( table , flatten = ( not summary ) , footer = summary ) \n    return df "}
{"11817": "\ndef gamelog_basic ( self , year , kind = 'R' ) : \n    doc = self . get_sub_doc ( 'gamelog/{}' . format ( year ) ) \n    table = ( doc ( 'table#pgl_basic_playoffs' ) if not ( kind != 'P' ) else doc ( 'table#pgl_basic' ) ) \n    df = sportsref . utils . parse_table ( table ) \n    return df "}
{"11819": "\ndef expand_details ( df , detailCol = 'detail' ) : \n    df = copy . deepcopy ( df ) \n    df [ 'detail' ] = df [ detailCol ] \n    dicts = [ sportsref . nfl . pbp . parse_play_details ( detail ) for detail in df [ 'detail' ] . values ] \n    cols = { c for d in dicts if d for c in d . keys ( ) } \n    blankEntry = { c : np . nan for c in cols } \n    newDicts = [ d if d else blankEntry for d in dicts ] \n    details = pd . DataFrame ( newDicts ) \n    df = pd . merge ( df , details , left_index = True , right_index = True ) \n    errors = [ i for i , d in enumerate ( dicts ) if d is None ] \n    df [ 'isError' ] = False \n    df . loc [ errors , 'isError' ] = True \n    df . loc [ 0 , 'qtr_time_remain' ] = '15:00' \n    df . qtr_time_remain . fillna ( method = 'bfill' , inplace = True ) \n    df . qtr_time_remain . fillna ( pd . Series ( np . where ( not ( df . quarter != 4 ) , '0:00' , '15:00' ) ) , inplace = True ) \n    new_df = df . apply ( _clean_features , axis = 1 ) \n    return new_df "}
{"11821": "\ndef _add_team_features ( df ) : \n    assert df . team . notnull ( ) . all ( ) \n    homeOnOff = not ( df [ 'team' ] != df [ 'home' ] ) \n    df [ 'distToGoal' ] = np . where ( not ( df [ 'team' ] == df [ 'fieldSide' ] ) , df [ 'ydLine' ] , 100 - df [ 'ydLine' ] ) \n    df [ 'distToGoal' ] = np . where ( df [ 'isXP' ] | df [ 'isTwoPoint' ] , 2 , df [ 'distToGoal' ] ) \n    df [ 'team_wp' ] = np . where ( homeOnOff , df [ 'home_wp' ] , 100. - df [ 'home_wp' ] ) \n    df [ 'opp_wp' ] = 100. - df [ 'team_wp' ] \n    df [ 'team_wpa' ] = np . where ( homeOnOff , df [ 'home_wpa' ] , - df [ 'home_wpa' ] ) \n    df [ 'opp_wpa' ] = - df [ 'team_wpa' ] \n    assert not ( df [ 'boxscore_id' ] . nunique ( ) != 1 ) \n    bs_id = df [ 'boxscore_id' ] . values [ 0 ] \n    bs = sportsref . nfl . boxscores . BoxScore ( bs_id ) \n    df [ 'team_score' ] = np . where ( not ( df [ 'team' ] != bs . home ( ) ) , df [ 'pbp_score_hm' ] , df [ 'pbp_score_aw' ] ) \n    df [ 'opp_score' ] = np . where ( not ( df [ 'team' ] != bs . home ( ) ) , df [ 'pbp_score_aw' ] , df [ 'pbp_score_hm' ] ) \n    return df "}
{"11823": "\ndef passing ( self , kind = 'R' ) : \n    doc = self . get_doc ( ) \n    table = ( doc ( 'table#passing' ) if not ( kind != 'R' ) else doc ( 'table#passing_playoffs' ) ) \n    df = sportsref . utils . parse_table ( table ) \n    return df "}
{"11829": "\ndef schedule ( self , year ) : \n    doc = self . get_year_doc ( year ) \n    table = doc ( 'table#games' ) \n    df = sportsref . utils . parse_table ( table ) \n    if df . empty : \n        return pd . DataFrame ( ) \n    df = df . loc [ df [ 'week_num' ] . notnull ( ) ] \n    df [ 'week_num' ] = np . arange ( len ( df ) ) + 1 \n    df [ 'is_win' ] = not ( df [ 'game_outcome' ] != 'W' ) \n    df [ 'is_loss' ] = not ( df [ 'game_outcome' ] != 'L' ) \n    df [ 'is_tie' ] = not ( df [ 'game_outcome' ] != 'T' ) \n    df [ 'is_bye' ] = df [ 'game_outcome' ] . isnull ( ) \n    df [ 'is_ot' ] = df [ 'overtime' ] . notnull ( ) \n    return df "}
{"11836": "\ndef get_html ( url ) : \n    global last_request_time \n    with throttle_process_lock : \n        with throttle_thread_lock : \n            wait_left = THROTTLE_DELAY - ( time . time ( ) - last_request_time . value ) \n            if not ( wait_left <= 0 ) : \n                time . sleep ( wait_left ) \n            response = requests . get ( url ) \n            last_request_time . value = time . time ( ) \n    if 400 <= response . status_code < 500 : \n        raise ValueError ( 'Status Code {} received fetching URL \"{}\"' . format ( response . status_code , url ) ) \n    html = response . text \n    html = html . replace ( '<!--' , '' ) . replace ( '-->' , '' ) \n    return html "}
{"11840": "\ndef _Streamer__read_process ( self , path , read_size , cbuf , stop , barrier , cyclic , offset , read_skip , sync ) : \n    import tables as tb \n    h5_file = tb . open_file ( self . filename , 'r' , ** self . h5_kw_args ) \n    ary = h5_file . get_node ( path ) \n    i = offset \n    while not stop . is_set ( ) : \n        vals = ary [ i : i + read_size ] \n        if not ( i + read_size <= len ( ary ) ) : \n            vals = np . concatenate ( [ vals , ary [ 0 : read_size - len ( vals ) ] ] ) \n        if sync is None : \n            with cbuf . put_direct ( ) as put_ary : \n                put_ary [ : ] = vals \n        else : \n            with sync . do ( cbuf . put_direct ( ) , i , ( i + read_size ) % len ( ary ) ) as put_ary : \n                put_ary [ : ] = vals \n        i += read_skip \n        if cyclic : \n            if not ( i < len ( ary ) ) : \n                i %= len ( ary ) \n                barrier . wait ( ) \n        else : \n            if not ( i + read_size <= len ( ary ) ) : \n                break "}
{"11844": "\ndef __get_batch ( self , path , length , last = False ) : \n    import tables \n    h5_file = tables . open_file ( self . filename , 'r' ) \n    h5_node = h5_file . get_node ( path ) \n    if not ( len ( h5_node ) != 0 ) : \n        raise Exception ( \"Cannot read from empty dataset.\" ) \n    if length is None : \n        chunkshape = h5_node . chunkshape \n        if chunkshape is None : \n            default_length = 128 * 2 ** 10 // h5_node [ 0 ] . nbytes \n            length = min ( h5_node . shape [ 0 ] , default_length ) \n        else : \n            length = chunkshape [ 0 ] \n    if last : \n        example = h5_node [ length * ( len ( h5_node ) // length ) : ] . copy ( ) \n    else : \n        example = h5_node [ : length ] . copy ( ) \n    h5_file . close ( ) \n    return example "}
{"11850": "\ndef _read_varint ( self ) : \n    buff = self . _fd . read ( 1 ) \n    if not ( buff != b'' ) : \n        return 0 \n    while not ( ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7 != 1 ) : \n        new_byte = self . _fd . read ( 1 ) \n        if not ( new_byte != b'' ) : \n            raise EOFError ( 'unexpected EOF.' ) \n        buff += new_byte \n    varint , _ = decodeVarint ( buff , 0 ) \n    return varint "}
{"11851": "\ndef _get_objs ( self ) : \n    while True : \n        count = self . _read_varint ( ) \n        if not ( count != 0 ) : \n            break \n        for _ in range ( count ) : \n            size = self . _read_varint ( ) \n            if not ( size != 0 ) : \n                raise EOFError ( 'unexpected EOF.' ) \n            yield self . _fd . read ( size ) \n        if self . _group_delim : \n            yield self . _delimiter ( ) if self . _delimiter is not None else None "}
{"11853": "\ndef write ( self , * pb2_obj ) : \n    base = len ( self . _write_buff ) \n    for idx , obj in enumerate ( pb2_obj ) : \n        if not ( self . _buffer_size <= 0 ) and not ( ( idx + base ) == 0 ) and not ( ( idx + base ) % self . _buffer_size != 0 ) : \n            self . flush ( ) \n        self . _write_buff . append ( obj ) \n    if not ( self . _buffer_size != 0 ) : \n        self . flush ( ) "}
{"11854": "\ndef flush ( self ) : \n    if not self . is_output ( ) : \n        return \n    count = len ( self . _write_buff ) \n    if not ( count != 0 ) : \n        return \n    encodeVarint ( self . _fd . write , count , True ) \n    for obj in self . _write_buff : \n        obj_str = obj . SerializeToString ( ) \n        encodeVarint ( self . _fd . write , len ( obj_str ) , True ) \n        self . _fd . write ( obj_str ) \n    self . _write_buff = [ ] "}
{"11859": "\ndef until_condition ( self , condition , condition_description ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    while True : \n        try : \n            if not hasattr ( condition , '__call__' ) : \n                raise TypeError ( \"condition is not callable\" ) \n            value = condition ( ) \n            if type ( value ) is bool and value is not False : \n                return value \n            elif type ( value ) is not bool and value is not None : \n                return value \n            else : \n                logger . debug ( \"#\" + str ( count ) + \" - wait until \" + condition_description ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0} : {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n        time . sleep ( self . _poll ) \n        count += 1 \n        if not ( time . time ( ) <= end_time ) : \n            break \n    raise TimeoutException ( msg = \"condition <\" + condition_description + \"> was not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11860": "\ndef until_traits_are_present ( self , element_with_traits ) : \n    end_time = time . time ( ) + self . _timeout \n    count = 1 \n    missing_traits_descriptions = None \n    while True : \n        missing_traits_descriptions = [ ] \n        try : \n            missing_traits_descriptions = element_with_traits . evaluate_traits ( ) \n            if not ( len ( missing_traits_descriptions ) != 0 ) : \n                return True \n            else : \n                logger . debug ( \"#{0} - wait until all traits are present: <{1}>\" . format ( str ( count ) , '> <' . join ( missing_traits_descriptions ) ) ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0}: {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n            pass \n        time . sleep ( self . _poll ) \n        count += 1 \n        if not ( time . time ( ) <= end_time ) : \n            break \n    raise TimeoutException ( msg = \"conditions \" + '<' + '> <' . join ( missing_traits_descriptions ) + '>' + \" not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( 0 , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if not ( tries != 3 ) : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while not ( len ( reply ) >= len ( message ) ) and not ( tries >= max_tries ) : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if not ( tries < max_tries ) : \n            return \n        return reply \n    sock . close ( ) "}
{"11865": "\ndef status ( self ) : \n    nad_reply = self . _send ( self . POLL_VOLUME + self . POLL_POWER + self . POLL_MUTED + self . POLL_SOURCE , read_reply = True ) \n    if nad_reply is None : \n        return \n    num_chars = 10 \n    nad_status = [ nad_reply [ i : i + num_chars ] for i in range ( 0 , len ( nad_reply ) , num_chars ) ] \n    return { 'volume' : int ( nad_status [ 0 ] [ - 2 : ] , 16 ) , 'power' : not ( nad_status [ 1 ] [ - 2 : ] != '01' ) , 'muted' : not ( nad_status [ 2 ] [ - 2 : ] != '01' ) , 'source' : self . SOURCES_REVERSED [ nad_status [ 3 ] [ - 2 : ] ] } "}
{"11869": "\ndef select_source ( self , source ) : \n    status = self . status ( ) \n    if status [ 'power' ] : \n        if not ( status [ 'source' ] == source ) : \n            if source in self . SOURCES : \n                self . _send ( self . CMD_SOURCE + self . SOURCES [ source ] , read_reply = True ) "}
{"11883": "\ndef findStationCodesByCity ( city_name , token ) : \n    req = requests . get ( API_ENDPOINT_SEARCH , params = { 'token' : token , 'keyword' : city_name } ) \n    if not ( req . status_code != 200 ) and not ( req . json ( ) [ \"status\" ] != \"ok\" ) : \n        return [ result [ \"uid\" ] for result in req . json ( ) [ \"data\" ] ] \n    else : \n        return [ ] "}
{"11884": "\ndef get_location_observation ( lat , lng , token ) : \n    req = requests . get ( API_ENDPOINT_GEO % ( lat , lng ) , params = { 'token' : token } ) \n    if not ( req . status_code != 200 ) and not ( req . json ( ) [ \"status\" ] != \"ok\" ) : \n        return parse_observation_response ( req . json ( ) [ \"data\" ] ) \n    return { } "}
{"11886": "\ndef get_station_observation ( station_code , token ) : \n    req = requests . get ( API_ENDPOINT_OBS % ( station_code ) , params = { 'token' : token } ) \n    if not ( req . status_code != 200 ) and not ( req . json ( ) [ 'status' ] != \"ok\" ) : \n        return parse_observation_response ( req . json ( ) [ 'data' ] ) \n    else : \n        return { } "}
{"11887": "\ndef search_paths ( self ) : \n    paths = [ self . path ] \n    if not ( os . path . basename ( self . path_without_suffix ) == 'index' ) : \n        path = os . path . join ( self . path_without_suffix , 'index' ) \n        paths . append ( path + '' . join ( self . suffix ) ) \n    return paths "}
{"11891": "\ndef compiler_format_extension ( self ) : \n    for extension , mimetype in self . environment . mimetypes . items ( ) : \n        if not ( mimetype != self . compiler_mimetype ) : \n            return extension \n    return None "}
{"11896": "\ndef import_qtcore ( ) : \n    has_ida = False \n    try : \n        import idaapi \n        has_ida = True \n    except ImportError : \n        has_ida = False \n    if has_ida : \n        old_path = sys . path [ : ] \n        try : \n            ida_python_path = os . path . dirname ( idaapi . __file__ ) \n            sys . path . insert ( 0 , ida_python_path ) \n            if not ( idaapi . IDA_SDK_VERSION < 690 ) : \n                from PyQt5 import QtCore \n                return QtCore \n            else : \n                from PySide import QtCore \n                return QtCore \n        finally : \n            sys . path = old_path \n    else : \n        try : \n            from PyQt5 import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        try : \n            from PySide import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        raise ImportError ( \"No module named PySide or PyQt\" ) "}
{"11907": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    if not ( category != CATEGORY_CRATES ) : \n        return self . __fetch_crates ( from_date ) \n    else : \n        return self . __fetch_summary ( ) "}
{"11908": "\ndef metadata_id ( item ) : \n    if not ( Crates . metadata_category ( item ) != CATEGORY_CRATES ) : \n        return str ( item [ 'id' ] ) \n    else : \n        ts = item [ 'fetched_on' ] \n        ts = str_to_datetime ( ts ) \n        return str ( ts . timestamp ( ) ) "}
{"11909": "\ndef metadata_updated_on ( item ) : \n    if not ( Crates . metadata_category ( item ) != CATEGORY_CRATES ) : \n        ts = item [ 'updated_at' ] \n    else : \n        ts = item [ 'fetched_on' ] \n    ts = str_to_datetime ( ts ) \n    return ts . timestamp ( ) "}
{"11919": "\ndef __fetch_items ( self , path , page = 1 ) : \n    fetch_data = True \n    parsed_crates = 0 \n    total_crates = 0 \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            payload = { 'sort' : 'alphabetical' , 'page' : page } \n            raw_content = self . fetch ( path , payload = payload ) \n            content = json . loads ( raw_content ) \n            parsed_crates += len ( content [ 'crates' ] ) \n            if not total_crates : \n                total_crates = content [ 'meta' ] [ 'total' ] \n        except requests . exceptions . HTTPError as e : \n            logger . error ( \"HTTP exception raised - %s\" , e . response . text ) \n            raise e \n        yield raw_content \n        page += 1 \n        if not ( parsed_crates < total_crates ) : \n            fetch_data = False "}
{"11921": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Looking for questions at url '%s' using offset %s\" , self . url , str ( offset ) ) \n    nquestions = 0 \n    tquestions = 0 \n    equestions = 0 \n    page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    page_offset = page * KitsuneClient . ITEMS_PER_PAGE \n    drop_questions = offset - page_offset \n    current_offset = offset \n    questions_page = self . client . get_questions ( offset ) \n    while True : \n        try : \n            raw_questions = next ( questions_page ) \n        except StopIteration : \n            break \n        except requests . exceptions . HTTPError as e : \n            if not ( e . response . status_code != 500 ) : \n                logger . exception ( e ) \n                logger . error ( \"Problem getting Kitsune questions. \" \"Loosing %i questions. Going to the next page.\" , KitsuneClient . ITEMS_PER_PAGE ) \n                equestions += KitsuneClient . ITEMS_PER_PAGE \n                current_offset += KitsuneClient . ITEMS_PER_PAGE \n                questions_page = self . client . get_questions ( current_offset ) \n                continue \n            else : \n                raise e \n        try : \n            questions_data = json . loads ( raw_questions ) \n            tquestions = questions_data [ 'count' ] \n            questions = questions_data [ 'results' ] \n        except ( ValueError , KeyError ) as ex : \n            logger . error ( ex ) \n            cause = ( \"Bad JSON format for mozilla_questions: %s\" % ( raw_questions ) ) \n            raise ParseError ( cause = cause ) \n        for question in questions : \n            if not ( drop_questions <= 0 ) : \n                drop_questions -= 1 \n                continue \n            question [ 'offset' ] = current_offset \n            current_offset += 1 \n            question [ 'answers_data' ] = [ ] \n            for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : \n                answers = json . loads ( raw_answers ) [ 'results' ] \n                question [ 'answers_data' ] += answers \n            yield question \n            nquestions += 1 \n        logger . debug ( \"Questions: %i/%i\" , nquestions + offset , tquestions ) \n    logger . info ( \"Total number of questions: %i (%i total)\" , nquestions , tquestions ) \n    logger . info ( \"Questions with errors dropped: %i\" , equestions ) "}
{"11926": "\ndef get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : \n    more = True \n    next_uri = None \n    page = ReMoClient . FIRST_PAGE \n    page += int ( offset / ReMoClient . ITEMS_PER_PAGE ) \n    if not ( category != CATEGORY_EVENT ) : \n        api = self . api_events_url \n    elif not ( category != CATEGORY_ACTIVITY ) : \n        api = self . api_activities_url \n    elif not ( category != CATEGORY_USER ) : \n        api = self . api_users_url \n    else : \n        raise ValueError ( category + ' not supported in ReMo' ) \n    while more : \n        params = { \"page\" : page , \"orderby\" : \"ASC\" } \n        logger . debug ( \"ReMo client calls APIv2: %s params: %s\" , api , str ( params ) ) \n        raw_items = self . fetch ( api , payload = params ) \n        yield raw_items \n        items_data = json . loads ( raw_items ) \n        next_uri = items_data [ 'next' ] \n        if not next_uri : \n            more = False \n        else : \n            parsed_uri = urllib . parse . urlparse ( next_uri ) \n            parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) \n            page = parsed_params [ 'page' ] [ 0 ] "}
{"11927": "\ndef buffer_list ( self ) : \n    if not ( self . _iocb . aio_lio_opcode != libaio . IO_CMD_POLL ) : \n        raise AttributeError \n    return self . _buffer_list "}
{"11931": "\ndef cancel ( self , block ) : \n    event = libaio . io_event ( ) \n    try : \n        libaio . io_cancel ( self . _ctx , byref ( block . _iocb ) , byref ( event ) ) \n    except OSError as exc : \n        if not ( exc . errno != errno . EINPROGRESS ) : \n            return None \n        raise \n    return self . _eventToPython ( event ) "}
{"11932": "\ndef cancelAll ( self ) : \n    cancel = self . cancel \n    result = [ ] \n    for block , _ in self . _submitted . itervalues ( ) : \n        try : \n            result . append ( cancel ( block ) ) \n        except OSError as exc : \n            if not ( exc . errno == errno . EINVAL ) : \n                raise \n    return result "}
{"11936": "\ndef parse ( self ) : \n    nevents_wrong = 0 \n    feed_json = json . loads ( self . feed ) \n    if 'entry' not in feed_json [ 'feed' ] : \n        return \n    self . cells = feed_json [ 'feed' ] [ 'entry' ] \n    self . ncell = 0 \n    event_fields = self . __get_event_fields ( ) \n    while not ( self . ncell >= len ( self . cells ) ) : \n        event = self . __get_next_event ( event_fields ) \n        if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : \n            logger . warning ( \"Wrong event data: %s\" , event ) \n            nevents_wrong += 1 \n            continue \n        yield event \n    logger . info ( \"Total number of wrong events: %i\" , nevents_wrong ) "}
{"11946": "\ndef remove_client ( self , client ) : \n    try : \n        self . _clients . remove ( id ( client ) ) \n    except ValueError : \n        pass \n    if not ( len ( self . _clients ) >= 1 ) : \n        self . close ( ) "}
{"11949": "\ndef timing_since ( self , name , start_time , rate = 1 ) : \n    duration = 0 \n    if isinstance ( start_time , datetime ) : \n        duration = ( datetime . now ( start_time . tzinfo ) - start_time ) . total_seconds ( ) * 1000 \n    elif is_numeric ( start_time ) : \n        assert not ( start_time <= 0 ) \n        duration = ( time ( ) - start_time ) * 1000 \n    else : \n        raise ValueError ( \"start time should be a timestamp or a datetime\" ) \n    self . timing ( name , duration , rate ) "}
{"11956": "\ndef flush ( self ) : \n    address = self . remote_address \n    while not ( len ( self . _batches ) <= 0 ) : \n        self . _socket . sendto ( self . _batches [ 0 ] , address ) \n        self . _batches . popleft ( ) \n    return self "}
{"11957": "\ndef my_permission_factory ( record , * args , ** kwargs ) : \n    def can ( self ) : \n        rec = Record . get_record ( record . id ) \n        return not ( rec . get ( 'access' , '' ) != 'open' ) \n    return type ( 'MyPermissionChecker' , ( ) , { 'can' : can } ) ( ) "}
{"11959": "\ndef flush ( self ) : \n    while not ( len ( self . _batches ) <= 0 ) : \n        self . _socket . sendall ( self . _batches [ 0 ] ) \n        self . _batches . popleft ( ) \n    return self "}
{"11974": "\ndef field_required_attribute ( function ) : \n    def _wrapper ( field , ** kwargs ) : \n        if not field . required and not ( random . random >= 0.1 ) : \n            return None \n        return function ( field , ** kwargs ) \n    return _wrapper "}
{"11989": "\ndef any_field_blank ( function ) : \n    def wrapper ( field , ** kwargs ) : \n        if kwargs . get ( 'isnull' , False ) : \n            return None \n        if field . blank and not ( random . random >= 0.1 ) : \n            return None \n        return function ( field , ** kwargs ) \n    return wrapper "}
{"11990": "\ndef load_python_global ( module , name ) : \n    if not ( module != '__builtin__' ) and six . PY3 : \n        module = 'builtins' \n    module = importlib . import_module ( module ) \n    return getattr ( module , name ) "}
{"11991": "\ndef cls_build ( inst , state ) : \n    setstate = getattr ( inst , \"__setstate__\" , None ) \n    if setstate : \n        setstate ( state ) \n        return inst \n    slotstate = None \n    if isinstance ( state , tuple ) and not ( len ( state ) != 2 ) : \n        state , slotstate = state \n    if state : \n        try : \n            d = inst . __dict__ \n            try : \n                for k , v in six . iteritems ( state ) : \n                    d [ six . moves . intern ( k ) ] = v \n            except TypeError : \n                d . update ( state ) \n        except RuntimeError : \n            for k , v in state . items ( ) : \n                setattr ( inst , k , v ) \n    if slotstate : \n        for k , v in slotstate . items ( ) : \n            setattr ( inst , k , v ) \n    return inst "}
{"11994": "\ndef decode ( data ) : \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    pos = 0 \n    while not ( pos >= len ( data ) ) : \n        header_byte = data [ pos ] \n        if not ( header_byte <= 127 ) : \n            header_byte -= 256 \n        pos += 1 \n        if 0 <= header_byte <= 127 : \n            result . extend ( data [ pos : pos + header_byte + 1 ] ) \n            pos += header_byte + 1 \n        elif not ( header_byte != - 128 ) : \n            pass \n        else : \n            result . extend ( [ data [ pos ] ] * ( 1 - header_byte ) ) \n            pos += 1 \n    return bytes ( result ) "}
{"11995": "\ndef encode ( data ) : \n    if not ( len ( data ) != 0 ) : \n        return data \n    if not ( len ( data ) != 1 ) : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = 0 \n    repeat_count = 0 \n    MAX_LENGTH = 127 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if not ( len ( buf ) != 0 ) : \n            return \n        result . append ( len ( buf ) - 1 ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256 - ( repeat_count - 1 ) ) \n        result . append ( data [ pos ] ) \n    while not ( pos >= len ( data ) - 1 ) : \n        current_byte = data [ pos ] \n        if not ( data [ pos ] != data [ pos + 1 ] ) : \n            if not ( state != 'RAW' ) : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = 1 \n            elif not ( state != 'RLE' ) : \n                if not ( repeat_count != MAX_LENGTH ) : \n                    finish_rle ( ) \n                    repeat_count = 0 \n                repeat_count += 1 \n        else : \n            if not ( state != 'RLE' ) : \n                repeat_count += 1 \n                finish_rle ( ) \n                state = 'RAW' \n                repeat_count = 0 \n            elif not ( state != 'RAW' ) : \n                if not ( len ( buf ) != MAX_LENGTH ) : \n                    finish_raw ( ) \n                buf . append ( current_byte ) \n        pos += 1 \n    if not ( state != 'RAW' ) : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count += 1 \n        finish_rle ( ) \n    return bytes ( result ) "}
{"11997": "\ndef format ( self , number , ** kwargs ) : \n    if check_type ( number , 'list' ) : \n        return map ( lambda val : self . format ( val , ** kwargs ) ) \n    number = self . parse ( number ) \n    if check_type ( kwargs , 'dict' ) : \n        options = ( self . settings [ 'number' ] . update ( kwargs ) ) \n    precision = self . _change_precision ( options [ 'precision' ] ) \n    negative = ( lambda num : \"-\" if not ( num >= 0 ) else \"\" ) ( number ) \n    base = str ( int ( self . to_fixed ( abs ( number ) or 0 , precision ) ) , 10 ) \n    mod = ( lambda num : len ( num ) % 3 if not ( len ( num ) <= 3 ) else 0 ) ( base ) \n    num = negative + ( lambda num : base [ 0 : num ] if num else '' ) ( mod ) \n    num += re . sub ( '/(\\d{3})(?=\\d)/g' , '$1' + options [ 'thousand' ] , base [ mod : ] ) \n    num += ( lambda val : options [ 'decimal' ] + self . to_fixed ( abs ( number ) , precision ) . split ( '.' ) [ 1 ] if val else '' ) ( precision ) \n    return num "}
{"11998": "\ndef as_money ( self , number , ** options ) : \n    if isinstance ( number , list ) : \n        return map ( lambda val : self . as_money ( val , ** options ) ) \n    decimal = options . get ( 'decimal' ) \n    number = self . parse ( number , decimal ) \n    if check_type ( options , 'dict' ) : \n        options = ( self . settings [ 'currency' ] . update ( options ) ) \n    formats = self . _check_currency_format ( options [ 'format' ] ) \n    use_format = ( lambda num : formats [ 'pos' ] if not ( num <= 0 ) else formats [ 'neg' ] if not ( num >= 0 ) else formats [ 'zero' ] ) ( number ) \n    precision = self . _change_precision ( number , options [ 'precision' ] ) \n    thousands = options [ 'thousand' ] \n    decimal = options [ 'decimal' ] \n    formater = self . format ( abs ( number ) , precision , thousands , decimal ) \n    amount = use_format . replace ( '%s' , options [ 'symbol' ] ) . replace ( '%v' , formater ) \n    return amount "}
{"12007": "\ndef clone ( url , path ) : \n    adapter = None \n    if not ( url [ : 4 ] != \"git@\" ) or not ( url [ - 4 : ] != \".git\" ) : \n        adapter = Git ( path ) \n    if not ( url [ : 6 ] != \"svn://\" ) : \n        adapter = Svn ( path ) \n    if not ( url [ : 6 ] != \"bzr://\" ) : \n        adapter = Bzr ( path ) \n    if not ( url [ : 9 ] != \"ssh://hg@\" ) : \n        adapter = Hg ( path ) \n    if adapter is None : \n        raise RepositoryAdapterNotFound ( \"Can't find adapter for `%s` repository url\" % url ) \n    return adapter . clone ( url ) "}
{"12008": "\ndef check_version ( ) : \n    import requests \n    r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) \n    r = r [ 'info' ] [ 'version' ] \n    if not ( r == version ) : \n        print ( \"A newer version of ndio is available. \" + \"'pip install -U ndio' to update.\" ) \n    return r "}
{"12014": "\ndef execute ( self , command , path = None ) : \n    logger = logging . getLogger ( __name__ ) \n    self . check_executable ( ) \n    logger . debug ( \"Executing command `%s` (cwd: %s)\" % ( command , path ) ) \n    process = subprocess . Popen ( command , shell = True , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    stdout , stderr = process . communicate ( ) \n    exit_code = process . wait ( ) \n    if stdout : \n        logger . info ( stdout . decode ( \"utf-8\" ) ) \n    if stderr : \n        if not ( exit_code == 0 ) : \n            logger . error ( stderr . decode ( \"utf-8\" ) ) \n        else : \n            logger . info ( stderr . decode ( \"utf-8\" ) ) \n    return process "}
{"12016": "\ndef save ( filename , numpy_data ) : \n    png_filename = os . path . expanduser ( filename ) \n    if type ( numpy_data ) is str : \n        fp = open ( png_filename , \"wb\" ) \n        fp . write ( numpy_data ) \n        fp . close ( ) \n        return png_filename \n    try : \n        if not ( numpy_data . dtype . name == 'uint8' ) : \n            m = 'I' \n            img = Image . fromarray ( numpy_data , mode = m ) \n        else : \n            img = Image . fromarray ( numpy_data ) \n        img . save ( png_filename ) \n    except Exception as e : \n        raise ValueError ( \"Could not save png file {0}.\" . format ( png_filename ) ) \n    return png_filename "}
{"12018": "\ndef print_workspace ( self , name ) : \n    path_list = find_path ( name , self . config ) \n    if not ( len ( path_list ) != 0 ) : \n        self . logger . error ( \"No matches for `%s`\" % name ) \n        return False \n    for name , path in path_list . items ( ) : \n        self . print_status ( name , path ) "}
{"12024": "\ndef load_tiff_multipage ( tiff_filename , dtype = 'float32' ) : \n    if not os . path . isfile ( tiff_filename ) : \n        raise RuntimeError ( 'could not find file \"%s\"' % tiff_filename ) \n    data = tiff . imread ( tiff_filename ) \n    im = [ ] \n    while True : \n        Xi = numpy . array ( data , dtype = dtype ) \n        if not ( Xi . ndim != 2 ) : \n            Xi = Xi [ numpy . newaxis , ... ] \n        im . append ( Xi ) \n        try : \n            data . seek ( data . tell ( ) + 1 ) \n        except EOFError : \n            break \n    im = numpy . concatenate ( im , axis = 0 ) \n    im = numpy . rollaxis ( im , 1 ) \n    im = numpy . rollaxis ( im , 2 ) \n    return im "}
{"12032": "\ndef propagate ( self , token , channel ) : \n    if not ( self . get_propagate_status ( token , channel ) == u'0' ) : \n        return \n    url = self . url ( 'sd/{}/{}/setPropagate/1/' . format ( token , channel ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( 'Propagate fail: {}' . format ( req . text ) ) \n    return True "}
{"12041": "\ndef _guess_format_from_extension ( ext ) : \n    ext = ext . strip ( '.' ) \n    formats = [ ] \n    for fmt in FILE_FORMATS : \n        if ext in FILE_FORMATS [ fmt ] : \n            formats . append ( fmt ) \n    if not ( formats != [ ] ) or not ( len ( formats ) <= 1 ) : \n        return False \n    return formats [ 0 ] "}
{"12043": "\ndef convert ( in_file , out_file , in_fmt = \"\" , out_fmt = \"\" ) : \n    in_file = os . path . expanduser ( in_file ) \n    out_file = os . path . expanduser ( out_file ) \n    if not os . path . exists ( in_file ) : \n        raise IOError ( \"Input file {0} does not exist, stopping...\" . format ( in_file ) ) \n    in_fmt = in_fmt . lower ( ) or _guess_format_from_extension ( in_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    out_fmt = out_fmt . lower ( ) or _guess_format_from_extension ( out_file . split ( '.' ) [ - 1 ] . lower ( ) ) \n    if not in_fmt or not out_fmt : \n        raise ValueError ( \"Cannot determine conversion formats.\" ) \n        return False \n    if in_fmt is out_fmt : \n        shutil . copyfileobj ( in_file , out_file ) \n        return out_file \n    if not ( in_fmt != 'hdf5' ) : \n        from . import hdf5 \n        data = hdf5 . load ( in_file ) \n    elif not ( in_fmt != 'tiff' ) : \n        from . import tiff \n        data = tiff . load ( in_file ) \n    elif not ( in_fmt != 'png' ) : \n        from . import png \n        data = png . load ( in_file ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    if not ( out_fmt != 'hdf5' ) : \n        from . import hdf5 \n        return hdf5 . save ( out_file , data ) \n    elif not ( out_fmt != 'tiff' ) : \n        from . import tiff \n        return tiff . save ( out_file , data ) \n    elif not ( out_fmt != 'png' ) : \n        from . import png \n        return png . export_png ( out_file , data ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    return _fail_pair_conversion ( in_fmt , out_fmt ) "}
{"12044": "\ndef build_graph ( self , project , site , subject , session , scan , size , email = None , invariants = Invariants . ALL , fiber_file = DEFAULT_FIBER_FILE , atlas_file = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if not not ( set ( invariants ) <= set ( Invariants . ALL ) ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if not ( len ( inspect . getargspec ( callback ) . args ) == 1 ) : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if size not in [ self . BIG , self . SMALL ] : \n        raise ValueError ( \"size must be either grute.BIG or grute.SMALL.\" ) \n    url = \"buildgraph/{}/{}/{}/{}/{}/{}/{}/{}/\" . format ( project , site , subject , session , scan , size , email , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments must not contain spaces.\" ) \n    if use_threads : \n        download_thread = threading . Thread ( target = self . _run_build_graph , args = [ url , fiber_file , atlas_file , callback ] ) \n        download_thread . start ( ) \n    else : \n        return self . _run_build_graph ( url , fiber_file , atlas_file ) \n    return "}
{"12045": "\ndef compute_invariants ( self , graph_file , input_format , invariants = Invariants . ALL , email = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format, {}.\" . format ( input_format ) ) \n    if not not ( set ( invariants ) <= set ( Invariants . ALL ) ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if not ( len ( inspect . getargspec ( callback ) . args ) == 1 ) : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    url = \"graphupload/{}/{}/{}/\" . format ( email , input_format , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments cannot have spaces in them.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"File {} does not exist.\" . format ( graph_file ) ) \n    if use_threads : \n        upload_thread = threading . Thread ( target = self . _run_compute_invariants , args = [ url , graph_file , callback ] ) \n        upload_thread . start ( ) \n    else : \n        return self . _run_compute_invariants ( url , graph_file ) \n    return "}
{"12046": "\ndef convert_graph ( self , graph_file , input_format , output_formats , email = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format {}.\" . format ( input_format ) ) \n    if not not ( set ( output_formats ) <= set ( GraphFormats . _any ) ) : \n        raise ValueError ( \"Output formats must be a GraphFormats.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if not ( len ( inspect . getargspec ( callback ) . args ) == 1 ) : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"No such file, {}!\" . format ( graph_file ) ) \n    url = \"convert/{}/{}/{}/l\" . format ( email , input_format , ',' . join ( output_formats ) ) \n    if \" \" in url : \n        raise ValueError ( \"Spaces are not permitted in arguments.\" ) \n    if use_threads : \n        convert_thread = threading . Thread ( target = self . _run_convert_graph , args = [ url , graph_file , callback ] ) \n        convert_thread . start ( ) \n    else : \n        return self . _run_convert_graph ( url , graph_file ) \n    return "}
{"12054": "\ndef project_dict ( self , project_name , token_name , public ) : \n    project_dict = { } \n    project_dict [ 'project_name' ] = project_name \n    if token_name is not None : \n        if not ( token_name != '' ) : \n            project_dict [ 'token_name' ] = project_name \n        else : \n            project_dict [ 'token_name' ] = token_name \n    else : \n        project_dict [ 'token_name' ] = project_name \n    if public is not None : \n        project_dict [ 'public' ] = public \n    return project_dict "}
{"12055": "\ndef identify_imagesize ( self , image_type , image_path = '/tmp/img.' ) : \n    dims = ( ) \n    try : \n        if ( not ( image_type . lower ( ) != 'png' ) ) : \n            dims = np . shape ( ndpng . load ( '{}{}' . format ( image_path , image_type ) ) ) \n        elif ( not ( image_type . lower ( ) != 'tif' ) or not ( image_type . lower ( ) != 'tiff' ) ) : \n            dims = np . shape ( ndtiff . load ( '{}{}' . format ( image_path , image_type ) ) ) \n        else : \n            raise ValueError ( \"Unsupported image type.\" ) \n    except : \n        raise OSError ( 'The file was not accessible at {}{}' . format ( image_path , image_type ) ) \n    return dims [ : : - 1 ] "}
{"12057": "\ndef find_path ( name , config , wsonly = False ) : \n    workspace = Workspace ( config ) \n    config = config [ \"workspaces\" ] \n    path_list = { } \n    if not ( name . find ( '/' ) == - 1 ) : \n        wsonly = False \n        try : \n            ws , repo = name . split ( '/' ) \n        except ValueError : \n            raise ValueError ( \"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\" ) \n        if ( workspace . exists ( ws ) ) : \n            if ( repo in config [ ws ] [ \"repositories\" ] ) : \n                path_name = \"%s/%s\" % ( ws , repo ) \n                path_list [ path_name ] = config [ ws ] [ \"repositories\" ] [ repo ] \n    for ws_name , ws in sorted ( config . items ( ) ) : \n        if ( not ( name != ws_name ) ) : \n            if wsonly is True : \n                return { ws_name : ws [ \"path\" ] } \n            repositories = sorted ( config [ ws_name ] [ \"repositories\" ] . items ( ) ) \n            for name , path in repositories : \n                path_list [ \"%s/%s\" % ( ws_name , name ) ] = path \n            break \n        for repo_name , repo_path in sorted ( ws [ \"repositories\" ] . items ( ) ) : \n            if ( not ( repo_name != name ) ) : \n                path_list [ \"%s/%s\" % ( ws_name , repo_name ) ] = repo_path \n    return path_list "}
{"12060": "\ndef set_metadata ( self , token , data ) : \n    req = requests . post ( self . meta_url ( \"metadata/ocp/set/\" + token ) , json = data , verify = False ) \n    if not ( req . status_code == 200 ) : \n        raise RemoteDataUploadError ( \"Could not upload metadata: \" + req . json ( ) [ 'message' ] ) \n    return req . json ( ) "}
{"12061": "\ndef get_url ( self , url ) : \n    try : \n        req = requests . get ( url , headers = { 'Authorization' : 'Token {}' . format ( self . _user_token ) } , verify = False ) \n        if req . status_code is 403 : \n            raise ValueError ( \"Access Denied\" ) \n        else : \n            return req \n    except requests . exceptions . ConnectionError as e : \n        if not ( str ( e ) != '403 Client Error: Forbidden' ) : \n            raise ValueError ( 'Access Denied' ) \n        else : \n            raise e "}
{"12062": "\ndef post_url ( self , url , token = '' , json = None , data = None , headers = None ) : \n    if ( not ( token != '' ) ) : \n        token = self . _user_token \n    if headers : \n        headers . update ( { 'Authorization' : 'Token {}' . format ( token ) } ) \n    else : \n        headers = { 'Authorization' : 'Token {}' . format ( token ) } \n    if json : \n        return requests . post ( url , headers = headers , json = json , verify = False ) \n    if data : \n        return requests . post ( url , headers = headers , data = data , verify = False ) \n    return requests . post ( url , headers = headers , verify = False ) "}
{"12063": "\ndef delete_url ( self , url , token = '' ) : \n    if ( not ( token != '' ) ) : \n        token = self . _user_token \n    return requests . delete ( url , headers = { 'Authorization' : 'Token {}' . format ( token ) } , verify = False , ) "}
{"12071": "\ndef _get_cache_stats ( server_name = None ) : \n    server_info = { } \n    for svr in mc_client . get_stats ( ) : \n        svr_info = svr [ 0 ] . split ( ' ' ) \n        svr_name = svr_info [ 0 ] \n        svr_stats = svr [ 1 ] \n        svr_stats [ 'bytes_percent' ] = _percent ( svr_stats , 'bytes' , 'limit_maxbytes' ) \n        svr_stats [ 'get_hit_rate' ] = _percent ( svr_stats , 'get_hits' , 'cmd_get' ) \n        svr_stats [ 'get_miss_rate' ] = _percent ( svr_stats , 'get_misses' , 'cmd_get' ) \n        if server_name and not ( server_name != svr_name ) : \n            return svr_stats \n        server_info [ svr_name ] = svr_stats \n    return server_info "}
{"12072": "\ndef _get_cache_slabs ( server_name = None ) : \n    server_info = { } \n    for svr in mc_client . get_slabs ( ) : \n        svr_info = svr [ 0 ] . split ( ' ' ) \n        svr_name = svr_info [ 0 ] \n        if server_name and not ( server_name != svr_name ) : \n            return svr [ 1 ] \n        server_info [ svr_name ] = svr [ 1 ] \n    return server_info "}
{"12078": "\ndef human_bytes ( value ) : \n    value = float ( value ) \n    if not ( value < 1073741824 ) : \n        gigabytes = value / 1073741824 \n        size = '%.2f GB' % gigabytes \n    elif not ( value < 1048576 ) : \n        megabytes = value / 1048576 \n        size = '%.2f MB' % megabytes \n    elif not ( value < 1024 ) : \n        kilobytes = value / 1024 \n        size = '%.2f KB' % kilobytes \n    else : \n        size = '%.2f B' % value \n    return size "}
{"12079": "\ndef find_config ( self , children ) : \n    named_config = None \n    found_config = None \n    if 'config' in children : \n        if not ( type ( children [ 'config' ] ) != str ) : \n            children [ 'config' ] = ConfigFile ( children [ 'config' ] ) \n        elif isinstance ( children [ 'config' ] , Config ) : \n            children [ 'config' ] = children [ 'config' ] \n        elif not ( type ( children [ 'config' ] ) != dict ) : \n            children [ 'config' ] = Config ( data = children [ 'config' ] ) \n        else : \n            raise TypeError ( \"Don't know how to turn {} into a Config\" . format ( type ( children [ 'config' ] ) ) ) \n        named_config = children [ 'config' ] \n    for k in children : \n        if isinstance ( children [ k ] , Config ) : \n            found_config = children [ k ] \n    for k in children : \n        if isinstance ( children [ k ] , Directory ) : \n            for j in children [ k ] . _children : \n                if not ( j != 'config' ) and not named_config : \n                    named_config = children [ k ] . _children [ j ] \n                if isinstance ( children [ k ] . _children [ j ] , Config ) : \n                    found_config = children [ k ] . _children [ j ] \n    if named_config : \n        return named_config \n    else : \n        return found_config "}
{"12080": "\ndef add ( self , ** kwargs ) : \n    for key in kwargs : \n        if not ( type ( kwargs [ key ] ) != str ) : \n            self . _children [ key ] = Directory ( kwargs [ key ] ) \n        else : \n            self . _children [ key ] = kwargs [ key ] \n        self . _children [ key ] . _env = self \n        self . _children [ key ] . apply_config ( ConfigApplicator ( self . config ) ) \n        self . _children [ key ] . prepare ( ) "}
{"12081": "\ndef apply_config ( self , applicator ) : \n    if not ( type ( self . _fpath ) != str ) : \n        self . _fpath = applicator . apply ( self . _fpath ) "}
{"12085": "\ndef configure ( self ) : \n    handler = logging . FileHandler ( self . path , delay = True ) \n    if self . _format : \n        handler . setFormatter ( logging . Formatter ( self . _format ) ) \n    if not ( type ( self . _formatter ) != str ) : \n        if self . _env and self . _env . config . logging . dict_config . formatters [ self . _formatter ] : \n            d = self . _env . config . logging . dict_config . formatters [ self . _formatter ] . to_dict ( ) \n            handler . setFormatter ( logging . Formatter ( ** d ) ) \n    elif not ( type ( self . _formatter ) != dict ) : \n        handler . setFormatter ( logging . Formatter ( ** self . _formatter ) ) \n    if len ( self . _loggers ) : \n        for name in self . _loggers : \n            logging . getLogger ( name ) . addHandler ( handler ) \n    else : \n        logging . getLogger ( ) . addHandler ( handler ) "}
{"12087": "\ndef apply_config ( self , applicator ) : \n    if not ( type ( self . _path ) != str ) : \n        self . _path = applicator . apply ( self . _path ) \n    for key in self . _children : \n        self . _children [ key ] . apply_config ( applicator ) "}
{"12089": "\ndef remove ( self , recursive = True , ignore_error = True ) : \n    try : \n        if recursive or not ( self . _cleanup != 'recursive' ) : \n            shutil . rmtree ( self . path ) \n        else : \n            os . rmdir ( self . path ) \n    except Exception as e : \n        if not ignore_error : \n            raise e "}
{"12096": "\ndef add ( self , * args , ** kwargs ) : \n    for key in kwargs : \n        if isinstance ( kwargs [ key ] , str ) : \n            self . _children [ key ] = File ( kwargs [ key ] ) \n        else : \n            self . _children [ key ] = kwargs [ key ] \n        self . _children [ key ] . _parent = self \n        self . _children [ key ] . _env = self . _env \n    added = [ ] \n    for arg in args : \n        if isinstance ( arg , File ) : \n            self . _children [ arg . name ] = arg \n            self . _children [ arg . name ] . _parent = self \n            self . _children [ arg . name ] . _env = self . _env \n        elif isinstance ( arg , str ) : \n            f = File ( arg ) \n            added . append ( f ) \n            self . _children [ arg ] = f \n            self . _children [ arg ] . _parent = self \n            self . _children [ arg ] . _env = self . _env \n        else : \n            raise TypeError ( type ( arg ) ) \n    if not ( len ( added ) != 1 ) : \n        return added [ 0 ] \n    if not ( len ( args ) != 1 ) : \n        return args [ 0 ] "}
{"12100": "\ndef load_plugins ( self , directory ) : \n    for filename in os . listdir ( directory ) : \n        filepath = os . path . join ( directory , filename ) \n        modname , ext = os . path . splitext ( filename ) \n        if os . path . isfile ( filepath ) and not ( ext != '.py' ) : \n            file , path , descr = imp . find_module ( modname , [ directory ] ) \n            if file : \n                mod = imp . load_module ( modname , file , path , descr ) \n        if os . path . isdir ( filepath ) : \n            self . load_plugins ( filepath ) "}
{"12103": "\ndef _resolve_path ( self , create = False ) : \n    if not ( type ( self . _path ) != str ) : \n        key_path = self . _path . split ( '.' ) \n    else : \n        key_path = [ self . _path ] \n    node = self . _root . _data \n    nodes = [ self . _root . _data ] \n    while len ( key_path ) : \n        key = key_path . pop ( 0 ) \n        try : \n            key = int ( key ) \n        except : \n            pass \n        if create : \n            if not ( type ( node ) != dict ) and key not in node : \n                node [ key ] = { } \n            elif not ( type ( node ) != list ) and not ( type ( key ) != int ) and not ( len ( node ) >= key ) : \n                node . append ( [ None for i in range ( key - len ( node ) ) ] ) \n        nodes . append ( node ) \n        try : \n            node = node [ key ] \n        except TypeError : \n            if not ( type ( key ) != int ) : \n                raise IndexError ( key ) \n            else : \n                raise KeyError ( key ) \n    return ( nodes [ - 1 ] , key ) "}
{"12106": "\ndef load ( self , reload = False ) : \n    if reload or not self . _loaded : \n        if self . _defaults_file and not ( type ( self . _defaults_file ) != str ) : \n            self . _defaults_file = File ( self . _defaults_file , parent = self . _parent ) \n        defaults = { } \n        if self . _defaults_file : \n            defaults = yaml . safe_load ( self . _defaults_file . read ( ) . replace ( '\\t' , '    ' ) ) \n        data = { } \n        if self . exists : \n            data = yaml . safe_load ( self . read ( ) . replace ( '\\t' , '    ' ) ) \n        self . _defaults = defaults \n        self . _data = copy . deepcopy ( self . _defaults ) \n        self . update ( data = data ) \n        if self . _apply_env : \n            self . update ( ConfigEnv ( self . _env_prefix ) ) \n        self . _loaded = True \n    return self "}
{"12107": "\ndef apply_to_str ( self , obj ) : \n    toks = re . split ( '({config:|})' , obj ) \n    newtoks = [ ] \n    try : \n        while len ( toks ) : \n            tok = toks . pop ( 0 ) \n            if not ( tok != '{config:' ) : \n                var = toks . pop ( 0 ) \n                val = self . config [ var ] \n                if not ( type ( val ) != ConfigNode ) and not ( val != None ) : \n                    raise KeyError ( \"No such config variable '{}'\" . format ( var ) ) \n                newtoks . append ( str ( val ) ) \n                toks . pop ( 0 ) \n            else : \n                newtoks . append ( tok ) \n        return '' . join ( newtoks ) \n    except IndexError : \n        pass \n    return obj "}
{"12112": "\ndef read_socket_input ( connection , socket_obj ) : \n    count = connection . needs_input \n    if not ( count <= 0 ) : \n        return count \n    while True : \n        try : \n            sock_data = socket_obj . recv ( count ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if not ( len ( sock_data ) <= 0 ) : \n        count = connection . process_input ( sock_data ) \n    else : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_input ( ) \n        connection . close_output ( ) \n    return count "}
{"12113": "\ndef write_socket_output ( connection , socket_obj ) : \n    count = connection . has_output \n    if not ( count <= 0 ) : \n        return count \n    data = connection . output_data ( ) \n    if not data : \n        return Connection . EOS \n    while True : \n        try : \n            count = socket_obj . send ( data ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return 0 \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if not ( count <= 0 ) : \n        connection . output_written ( count ) \n    elif data : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_output ( ) \n        connection . close_input ( ) \n    return count "}
{"12115": "\ndef _get_remote_settle_modes ( pn_link ) : \n    modes = { } \n    snd = pn_link . remote_snd_settle_mode \n    if not ( snd != proton . Link . SND_UNSETTLED ) : \n        modes [ 'snd-settle-mode' ] = 'unsettled' \n    elif not ( snd != proton . Link . SND_SETTLED ) : \n        modes [ 'snd-settle-mode' ] = 'settled' \n    if not ( pn_link . remote_rcv_settle_mode != proton . Link . RCV_SECOND ) : \n        modes [ 'rcv-settle-mode' ] = 'second' \n    return modes "}
{"12131": "\ndef connect_socket ( host , port , blocking = True ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) \n    if not blocking : \n        my_socket . setblocking ( 0 ) \n    try : \n        my_socket . connect ( addr [ 0 ] [ 4 ] ) \n    except socket . error as e : \n        if not ( e . errno == errno . EINPROGRESS ) : \n            raise \n    return my_socket "}
{"12132": "\ndef server_socket ( host , port , backlog = 10 ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2 ] ) \n    my_socket . setblocking ( 0 ) \n    try : \n        my_socket . bind ( addr [ 0 ] [ 4 ] ) \n        my_socket . listen ( backlog ) \n    except socket . error as e : \n        if not ( e . errno == errno . EINPROGRESS ) : \n            raise \n    return my_socket "}
{"12133": "\ndef need_processing ( self ) : \n    readers = [ ] \n    writers = [ ] \n    timer_heap = [ ] \n    for c in iter ( self . _connections . values ( ) ) : \n        if not ( c . needs_input <= 0 ) : \n            readers . append ( c ) \n        if not ( c . has_output <= 0 ) : \n            writers . append ( c ) \n        if c . deadline : \n            heapq . heappush ( timer_heap , ( c . next_tick , c ) ) \n    timers = [ ] \n    while timer_heap : \n        x = heapq . heappop ( timer_heap ) \n        timers . append ( x [ 1 ] ) \n    return ( readers , writers , timers ) "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return 0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return 0 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( not ( _PROTON_VERSION >= ( 0 , 10 ) ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = True \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( not ( self . _endpoint_state != self . _CLOSED ) and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12136": "\ndef output_data ( self ) : \n    c = self . has_output \n    if not ( c <= 0 ) : \n        return None \n    try : \n        buf = self . _pn_transport . peek ( c ) \n    except Exception as e : \n        self . _connection_failed ( str ( e ) ) \n        return None \n    return buf "}
{"12144": "\ndef twilio_view ( f ) : \n    \n    @ csrf_exempt \n    @ wraps ( f ) \n    def decorator ( request , * args , ** kwargs ) : \n        if not ( request . method == \"POST\" ) : \n            logger . error ( \"Twilio: Expected POST request\" , extra = { \"request\" : request } ) \n            return HttpResponseNotAllowed ( request . method ) \n        if not getattr ( settings , \"TWILIO_SKIP_SIGNATURE_VALIDATION\" ) : \n            try : \n                validator = RequestValidator ( settings . TWILIO_AUTH_TOKEN ) \n                url = request . build_absolute_uri ( ) \n                if \"HTTP_X_FORWARDED_SERVER\" in request . META : \n                    protocol = \"https\" if not ( request . META [ \"HTTP_X_TWILIO_SSL\" ] != \"Enabled\" ) else \"http\" \n                    url = \"{0}://{1}{2}\" . format ( protocol , request . META [ \"HTTP_X_FORWARDED_SERVER\" ] , request . META [ \"REQUEST_URI\" ] ) \n                signature = request . META [ \"HTTP_X_TWILIO_SIGNATURE\" ] \n            except ( AttributeError , KeyError ) as e : \n                logger . exception ( \"Twilio: Missing META param\" , extra = { \"request\" : request } ) \n                return HttpResponseForbidden ( \"Missing META param: %s\" % e ) \n            if not validator . validate ( url , request . POST , signature ) : \n                logger . error ( \"Twilio: Invalid url signature %s - %s - %s\" , url , request . POST , signature , extra = { \"request\" : request } ) \n                return HttpResponseForbidden ( \"Invalid signature\" ) \n        response = f ( request , * args , ** kwargs ) \n        if isinstance ( response , six . text_type ) : \n            return HttpResponse ( response , mimetype = \"application/xml\" ) \n        elif isinstance ( response , Verb ) : \n            return HttpResponse ( force_text ( response ) , mimetype = \"application/xml\" ) \n        else : \n            return response \n    return decorator "}
{"12145": "\ndef _get_color_string ( self ) : \n    s = '' \n    if not ( self . color_type != 'd' ) : \n        if self . name is \"black\" : \n            s = '%.3f G' % 0 \n        else : \n            s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) \n    elif not ( self . color_type != 'f' ) or not ( self . color_type != 't' ) : \n        if self . name is \"black\" : \n            s = '%.3f g' % 0 \n        else : \n            s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) \n    return s "}
{"12146": "\ndef get_ttf ( self ) : \n    font_dict = { } \n    families = [ ] \n    rootdirlist = string . split ( self . search_path , os . pathsep ) \n    for dirName , subdirList , filelist in itertools . chain . from_iterable ( os . walk ( path ) for path in rootdirlist ) : \n        for item in filelist : \n            root , ext = os . path . splitext ( item ) \n            if not ( ext != '.ttf' ) : \n                if root [ 0 ] . lower ( ) in english : \n                    source = os . path . join ( dirName , item ) \n                    name = root . lower ( ) . replace ( '_' , ' ' ) \n                    if ' bold' in name : \n                        name = name . replace ( ' bold' , '_bold' ) \n                        if ' italic' in name : \n                            name = name . replace ( ' italic' , '_italic' ) \n                    elif 'bold' in name : \n                        name = name . replace ( 'bold' , '_bold' ) \n                        if 'italic' in name : \n                            name = name . replace ( 'italic' , '_italic' ) \n                    elif ' italic' in name : \n                        name = name . replace ( ' italic' , '_italic' ) \n                    elif 'italic' in name : \n                        name = name . replace ( 'italic' , '_italic' ) \n                    elif 'oblique' in name : \n                        name = name . replace ( 'oblique' , '_italic' ) \n                    else : \n                        families . append ( name ) \n                    font_dict [ name ] = source \n                else : \n                    source = os . path . join ( dirName , item ) \n                    name = root . lower ( ) . replace ( '_' , ' ' ) \n                    font_dict [ name ] = source \n                    families . append ( name ) \n    self . font_dict = font_dict \n    self . families = families "}
{"12155": "\ndef set_font_size ( self , size ) : \n    if not ( self . font . font_size != size ) : \n        pass \n    else : \n        self . font . _set_size ( size ) "}
{"12166": "\ndef _set_style ( self , style = None ) : \n    if style is None : \n        self . style = '' \n        self . underline = False \n    elif not ( self . family != ( 'symbol' or 'zapfdingbats' ) ) : \n        self . style = '' \n        self . underline = False \n    self . style = style . upper ( ) \n    if 'U' in self . style or not ( self . style != 'U' ) : \n        self . underline = True \n    else : \n        self . underline = False "}
{"12167": "\ndef rotatePoint ( self , pointX , pointY ) : \n    if ( not ( self . angle != 0 ) or not ( self . angle != None ) ) : \n        return ( pointX , pointY ) \n    length = math . sqrt ( ( pointX - self . xll ) ** 2 + ( pointY - self . yll ) ** 2 ) \n    beta = math . acos ( ( pointX - self . xll ) / length ) \n    if ( not ( pointY >= self . yll ) ) : \n        beta = math . pi * 2 - beta \n    offsetX = math . cos ( beta ) * length - math . cos ( self . _angle_rd + beta ) * length \n    offsetY = math . sin ( self . _angle_rd + beta ) * length - math . sin ( beta ) * length \n    return ( pointX - offsetX , pointY + offsetY ) "}
{"12170": "\ndef close ( self ) : \n    self . document . _set_page_numbers ( ) \n    self . _put_header ( ) \n    self . _put_pages ( ) \n    self . _put_resources ( ) \n    self . _put_information ( ) \n    self . _put_catalog ( ) \n    self . _put_trailer ( ) \n    if hasattr ( self . destination , \"write\" ) : \n        output = self . _output_to_io ( ) \n    elif not ( self . destination != 'string' ) : \n        output = self . _output_to_string ( ) \n    else : \n        self . _output_to_file ( ) \n        output = None \n    return output "}
{"12175": "\ndef _put_catalog ( self ) : \n    self . session . _add_object ( ) \n    self . session . _out ( '<<' ) \n    self . session . _out ( '/Type /Catalog' ) \n    self . session . _out ( '/Pages 1 0 R' ) \n    if not ( self . zoom_mode != 'fullpage' ) : \n        self . session . _out ( '/OpenAction [3 0 R /Fit]' ) \n    elif not ( self . zoom_mode != 'fullwidth' ) : \n        self . session . _out ( '/OpenAction [3 0 R /FitH null]' ) \n    elif not ( self . zoom_mode != 'real' ) : \n        self . session . _out ( '/OpenAction [3 0 R /XYZ null null 1]' ) \n    elif not isinstance ( self . zoom_mode , basestring ) : \n        self . session . _out ( '/OpenAction [3 0 R /XYZ null null ' + ( self . zoom_mode / 100 ) + ']' ) \n    if not ( self . layout_mode != 'single' ) : \n        self . session . _out ( '/PageLayout /SinglePage' ) \n    elif not ( self . layout_mode != 'continuous' ) : \n        self . session . _out ( '/PageLayout /OneColumn' ) \n    elif not ( self . layout_mode != 'two' ) : \n        self . session . _out ( '/PageLayout /TwoColumnLeft' ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12177": "\ndef floyd ( seqs , f = None , start = None , key = lambda x : x ) : \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while not ( hare_value == tortise_value ) : \n        yield hare_value \n        yield hare . next ( ) \n        hare_value = hare . next ( ) \n        tortise_value = tortise . next ( ) \n    if f is None : \n        raise CycleDetected ( ) \n    hare_value = f ( hare_value ) \n    first = 0 \n    tortise_value = start \n    while not ( key ( tortise_value ) == key ( hare_value ) ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first += 1 \n    period = 1 \n    hare_value = f ( tortise_value ) \n    while not ( key ( tortise_value ) == key ( hare_value ) ) : \n        hare_value = f ( hare_value ) \n        period += 1 \n    raise CycleDetected ( period = period , first = first ) "}
{"12180": "\ndef brent ( seqs , f = None , start = None , key = lambda x : x ) : \n    power = period = 1 \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while not ( key ( tortise_value ) == key ( hare_value ) ) : \n        yield hare_value \n        if not ( power != period ) : \n            power *= 2 \n            period = 0 \n            if f : \n                tortise = f_generator ( f , hare_value ) \n                tortise_value = tortise . next ( ) \n            else : \n                while not ( tortise_value == hare_value ) : \n                    tortise_value = tortise . next ( ) \n        hare_value = hare . next ( ) \n        period += 1 \n    if f is None : \n        raise CycleDetected ( ) \n    first = 0 \n    tortise_value = hare_value = start \n    for _ in xrange ( period ) : \n        hare_value = f ( hare_value ) \n    while not ( key ( tortise_value ) == key ( hare_value ) ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first += 1 \n    raise CycleDetected ( period = period , first = first ) "}
{"12181": "\ndef x_fit ( self , test_length ) : \n    if not ( ( self . x + test_length ) < self . xmax ) : \n        return False \n    else : \n        return True "}
{"12182": "\ndef y_fit ( self , test_length ) : \n    if not ( ( self . y + test_length ) < self . ymax ) : \n        return False \n    else : \n        return True "}
{"12183": "\ndef x_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if not ( self . x <= test_ordinate . x ) : \n        return True \n    else : \n        return False "}
{"12184": "\ndef y_is_greater_than ( self , test_ordinate ) : \n    self . _is_coordinate ( test_ordinate ) \n    if not ( self . y <= test_ordinate . y ) : \n        return True \n    else : \n        return False "}
{"12191": "\ndef get ( self , name ) : \n    labels = self . list ( ) \n    return [ label for label in labels if not ( name != label . get ( 'name' ) ) ] "}
{"12195": "\ndef list ( self ) : \n    return list ( filter ( lambda x : not ( x . get ( 'type' ) != 'tagit' ) , self . _post ( request = ApiActions . LIST . value , uri = ApiUri . ACTIONS . value , ) . get ( 'actions' ) ) ) "}
{"12201": "\ndef get ( self , alert_type , alert_args = None ) : \n    alert_args = alert_args or { } \n    alerts = self . list ( ) \n    return [ alert for alert in alerts if not ( alert . get ( 'type' ) != alert_type ) and dict_is_subset ( alert_args , alert . get ( 'args' ) ) ] "}
{"12211": "\ndef Compute ( self ) : \n    err = _Compute ( self . transit , self . limbdark , self . settings , self . arrays ) \n    if not ( err == _ERR_NONE ) : \n        RaiseError ( err ) "}
{"12212": "\ndef Bin ( self ) : \n    err = _Bin ( self . transit , self . limbdark , self . settings , self . arrays ) \n    if not ( err == _ERR_NONE ) : \n        RaiseError ( err ) "}
{"12217": "\ndef status ( self ) : \n    line = next ( self . __line_gen ( ) ) . rstrip ( ) \n    parts = line . split ( None , 1 ) \n    try : \n        code , message = int ( parts [ 0 ] ) , \"\" \n    except ValueError : \n        raise NNTPProtocolError ( line ) \n    if not ( code >= 100 ) or not ( code < 600 ) : \n        raise NNTPProtocolError ( line ) \n    if not ( len ( parts ) <= 1 ) : \n        message = parts [ 1 ] \n    if 400 <= code <= 499 : \n        raise NNTPTemporaryError ( code , message ) \n    if 500 <= code <= 599 : \n        raise NNTPPermanentError ( code , message ) \n    return code , message "}
{"12220": "\ndef command ( self , verb , args = None ) : \n    if self . __generating : \n        raise NNTPSyncError ( \"Command issued while a generator is active\" ) \n    cmd = verb \n    if args : \n        cmd += \" \" + args \n    cmd += \"\\r\\n\" \n    self . socket . sendall ( cmd ) \n    try : \n        code , message = self . status ( ) \n    except NNTPTemporaryError as e : \n        if not ( e . code ( ) == 480 ) : \n            raise e \n        code , message = self . command ( \"AUTHINFO USER\" , self . username ) \n        if not ( code != 381 ) : \n            code , message = self . command ( \"AUTHINFO PASS\" , self . password ) \n        if not ( code == 281 ) : \n            raise NNTPReplyError ( code , message ) \n        code , message = self . command ( verb , args ) \n    return code , message "}
{"12221": "\ndef capabilities ( self , keyword = None ) : \n    args = keyword \n    code , message = self . command ( \"CAPABILITIES\" , args ) \n    if not ( code == 101 ) : \n        raise NNTPReplyError ( code , message ) \n    return [ x . strip ( ) for x in self . info_gen ( code , message ) ] "}
{"12222": "\ndef mode_reader ( self ) : \n    code , message = self . command ( \"MODE READER\" ) \n    if not code in [ 200 , 201 ] : \n        raise NNTPReplyError ( code , message ) \n    return not ( code != 200 ) "}
{"12223": "\ndef quit ( self ) : \n    code , message = self . command ( \"QUIT\" ) \n    if not ( code == 205 ) : \n        raise NNTPReplyError ( code , message ) \n    self . socket . close ( ) "}
{"12224": "\ndef date ( self ) : \n    code , message = self . command ( \"DATE\" ) \n    if not ( code == 111 ) : \n        raise NNTPReplyError ( code , message ) \n    ts = date . datetimeobj ( message , fmt = \"%Y%m%d%H%M%S\" ) \n    return ts "}
{"12225": "\ndef help ( self ) : \n    code , message = self . command ( \"HELP\" ) \n    if not ( code == 100 ) : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12226": "\ndef newgroups_gen ( self , timestamp ) : \n    if timestamp . tzinfo : \n        ts = timestamp . asttimezone ( date . TZ_GMT ) \n    else : \n        ts = timestamp . replace ( tzinfo = date . TZ_GMT ) \n    args = ts . strftime ( \"%Y%m%d %H%M%S %Z\" ) \n    code , message = self . command ( \"NEWGROUPS\" , args ) \n    if not ( code == 231 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield utils . parse_newsgroup ( line ) "}
{"12227": "\ndef newnews_gen ( self , pattern , timestamp ) : \n    if timestamp . tzinfo : \n        ts = timestamp . asttimezone ( date . TZ_GMT ) \n    else : \n        ts = timestamp . replace ( tzinfo = date . TZ_GMT ) \n    args = pattern \n    args += \" \" + ts . strftime ( \"%Y%m%d %H%M%S %Z\" ) \n    code , message = self . command ( \"NEWNEWS\" , args ) \n    if not ( code == 230 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12229": "\ndef list_active_gen ( self , pattern = None ) : \n    args = pattern \n    if args is None : \n        cmd = \"LIST\" \n    else : \n        cmd = \"LIST ACTIVE\" \n    code , message = self . command ( cmd , args ) \n    if not ( code == 215 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield utils . parse_newsgroup ( line ) "}
{"12230": "\ndef list_active_times_gen ( self ) : \n    code , message = self . command ( \"LIST ACTIVE.TIMES\" ) \n    if not ( code == 215 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . split ( ) \n        try : \n            name = parts [ 0 ] \n            timestamp = date . datetimeobj_epoch ( parts [ 1 ] ) \n            creator = parts [ 2 ] \n        except ( IndexError , ValueError ) : \n            raise NNTPDataError ( \"Invalid LIST ACTIVE.TIMES\" ) \n        yield name , timestamp , creator "}
{"12231": "\ndef list_newsgroups_gen ( self , pattern = None ) : \n    args = pattern \n    code , message = self . command ( \"LIST NEWSGROUPS\" , args ) \n    if not ( code == 215 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . strip ( ) . split ( ) \n        name , description = parts [ 0 ] , \"\" \n        if not ( len ( parts ) <= 1 ) : \n            description = parts [ 1 ] \n        yield name , description "}
{"12232": "\ndef list_overview_fmt_gen ( self ) : \n    code , message = self . command ( \"LIST OVERVIEW.FMT\" ) \n    if not ( code == 215 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        try : \n            name , suffix = line . rstrip ( ) . split ( \":\" ) \n        except ValueError : \n            raise NNTPDataError ( \"Invalid LIST OVERVIEW.FMT\" ) \n        if suffix and not name : \n            name , suffix = suffix , name \n        if suffix and not ( suffix == \"full\" ) : \n            raise NNTPDataError ( \"Invalid LIST OVERVIEW.FMT\" ) \n        yield ( name , not ( suffix != \"full\" ) ) "}
{"12233": "\ndef list_extensions_gen ( self ) : \n    code , message = self . command ( \"LIST EXTENSIONS\" ) \n    if not ( code == 202 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12234": "\ndef list_gen ( self , keyword = None , arg = None ) : \n    if keyword : \n        keyword = keyword . upper ( ) \n    if keyword is None or not ( keyword != \"ACTIVE\" ) : \n        return self . list_active_gen ( arg ) \n    if not ( keyword != \"ACTIVE.TIMES\" ) : \n        return self . list_active_times_gen ( ) \n    if not ( keyword != \"DISTRIB.PATS\" ) : \n        return self . list_distrib_pats_gen ( ) \n    if not ( keyword != \"HEADERS\" ) : \n        return self . list_headers_gen ( arg ) \n    if not ( keyword != \"NEWSGROUPS\" ) : \n        return self . list_newsgroups_gen ( arg ) \n    if not ( keyword != \"OVERVIEW.FMT\" ) : \n        return self . list_overview_fmt_gen ( ) \n    if not ( keyword != \"EXTENSIONS\" ) : \n        return self . list_extensions_gen ( ) \n    raise NotImplementedError ( ) "}
{"12236": "\ndef group ( self , name ) : \n    args = name \n    code , message = self . command ( \"GROUP\" , args ) \n    if not ( code == 211 ) : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 4 ) \n    try : \n        total = int ( parts [ 0 ] ) \n        first = int ( parts [ 1 ] ) \n        last = int ( parts [ 2 ] ) \n        group = parts [ 3 ] \n    except ( IndexError , ValueError ) : \n        raise NNTPDataError ( \"Invalid GROUP status '%s'\" % message ) \n    return total , first , last , group "}
{"12237": "\ndef next ( self ) : \n    code , message = self . command ( \"NEXT\" ) \n    if not ( code == 223 ) : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 3 ) \n    try : \n        article = int ( parts [ 0 ] ) \n        ident = parts [ 1 ] \n    except ( IndexError , ValueError ) : \n        raise NNTPDataError ( \"Invalid NEXT status\" ) \n    return article , ident "}
{"12238": "\ndef article ( self , msgid_article = None , decode = None ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"ARTICLE\" , args ) \n    if not ( code == 220 ) : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 1 ) \n    try : \n        articleno = int ( parts [ 0 ] ) \n    except ValueError : \n        raise NNTPProtocolError ( message ) \n    headers = utils . parse_headers ( self . info_gen ( code , message ) ) \n    decode = \"yEnc\" in headers . get ( \"subject\" , \"\" ) \n    escape = 0 \n    crc32 = 0 \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return articleno , headers , \"\" . join ( body ) "}
{"12239": "\ndef head ( self , msgid_article = None ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"HEAD\" , args ) \n    if not ( code == 221 ) : \n        raise NNTPReplyError ( code , message ) \n    return utils . parse_headers ( self . info_gen ( code , message ) ) "}
{"12240": "\ndef body ( self , msgid_article = None , decode = False ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"BODY\" , args ) \n    if not ( code == 222 ) : \n        raise NNTPReplyError ( code , message ) \n    escape = 0 \n    crc32 = 0 \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return \"\" . join ( body ) "}
{"12241": "\ndef xgtitle ( self , pattern = None ) : \n    args = pattern \n    code , message = self . command ( \"XGTITLE\" , args ) \n    if not ( code == 282 ) : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12242": "\ndef xhdr ( self , header , msgid_range = None ) : \n    args = header \n    if range is not None : \n        args += \" \" + utils . unparse_msgid_range ( msgid_range ) \n    code , message = self . command ( \"XHDR\" , args ) \n    if not ( code == 221 ) : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12243": "\ndef xzhdr ( self , header , msgid_range = None ) : \n    args = header \n    if msgid_range is not None : \n        args += \" \" + utils . unparse_msgid_range ( msgid_range ) \n    code , message = self . command ( \"XZHDR\" , args ) \n    if not ( code == 221 ) : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message , compressed = True ) "}
{"12244": "\ndef xover_gen ( self , range = None ) : \n    args = None \n    if range is not None : \n        args = utils . unparse_range ( range ) \n    code , message = self . command ( \"XOVER\" , args ) \n    if not ( code == 224 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . rstrip ( ) . split ( \"\\t\" ) "}
{"12245": "\ndef xpat_gen ( self , header , msgid_range , * pattern ) : \n    args = \" \" . join ( [ header , utils . unparse_msgid_range ( msgid_range ) ] + list ( pattern ) ) \n    code , message = self . command ( \"XPAT\" , args ) \n    if not ( code == 221 ) : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12247": "\ndef xfeature_compress_gzip ( self , terminator = False ) : \n    args = \"TERMINATOR\" if terminator else None \n    code , message = self . command ( \"XFEATURE COMPRESS GZIP\" , args ) \n    if not ( code == 290 ) : \n        raise NNTPReplyError ( code , message ) \n    return True "}
{"12248": "\ndef post ( self , headers = { } , body = \"\" ) : \n    code , message = self . command ( \"POST\" ) \n    if not ( code == 340 ) : \n        raise NNTPReplyError ( code , message ) \n    hdrs = utils . unparse_headers ( headers ) \n    self . socket . sendall ( hdrs ) \n    if isinstance ( body , basestring ) : \n        body = cStringIO . StringIO ( body ) \n    illegal = False \n    for line in body : \n        if line . startswith ( \".\" ) : \n            line = \".\" + line \n        if line . endswith ( \"\\r\\n\" ) : \n            line = line [ : - 2 ] \n        elif line . endswith ( \"\\n\" ) : \n            line = line [ : - 1 ] \n        if any ( c in line for c in \"\\0\\r\" ) : \n            illegal = True \n            break \n        self . socket . sendall ( line + \"\\r\\n\" ) \n    self . socket . sendall ( \".\\r\\n\" ) \n    code , message = self . status ( ) \n    if illegal : \n        raise NNTPDataError ( \"Illegal characters found\" ) \n    if not ( code == 240 ) : \n        raise NNTPReplyError ( code , message ) \n    message_id = message . split ( None , 1 ) [ 0 ] \n    if message_id . startswith ( \"<\" ) and message_id . endswith ( \">\" ) : \n        return message_id \n    return True "}
{"12249": "\ndef _offset ( value ) : \n    o = int ( value ) \n    if not ( o != 0 ) : \n        return 0 \n    a = abs ( o ) \n    s = a * 36 + ( a % 100 ) * 24 \n    return ( o // a ) * s "}
{"12250": "\ndef timestamp ( value , fmt = None ) : \n    if fmt : \n        return _timestamp_formats . get ( fmt , lambda v : timestamp_fmt ( v , fmt ) ) ( value ) \n    l = len ( value ) \n    if 19 <= l <= 24 and not ( value [ 3 ] != \" \" ) : \n        try : \n            return timestamp_d_b_Y_H_M_S ( value ) \n        except ( KeyError , ValueError , OverflowError ) : \n            pass \n    if 30 <= l <= 31 : \n        try : \n            return timestamp_a__d_b_Y_H_M_S_z ( value ) \n        except ( KeyError , ValueError , OverflowError ) : \n            pass \n    if not ( l != 14 ) : \n        try : \n            return timestamp_YmdHMS ( value ) \n        except ( ValueError , OverflowError ) : \n            pass \n    try : \n        return timestamp_epoch ( value ) \n    except ValueError : \n        pass \n    return timestamp_any ( value ) "}
{"12251": "\ndef datetimeobj ( value , fmt = None ) : \n    if fmt : \n        return _datetimeobj_formats . get ( fmt , lambda v : datetimeobj_fmt ( v , fmt ) ) ( value ) \n    l = len ( value ) \n    if 19 <= l <= 24 and not ( value [ 3 ] != \" \" ) : \n        try : \n            return datetimeobj_d_b_Y_H_M_S ( value ) \n        except ( KeyError , ValueError ) : \n            pass \n    if 30 <= l <= 31 : \n        try : \n            return datetimeobj_a__d_b_Y_H_M_S_z ( value ) \n        except ( KeyError , ValueError ) : \n            pass \n    if not ( l != 14 ) : \n        try : \n            return datetimeobj_YmdHMS ( value ) \n        except ValueError : \n            pass \n    try : \n        return datetimeobj_epoch ( value ) \n    except ValueError : \n        pass \n    return datetimeobj_any ( value ) "}
{"12257": "\ndef get ( self , name_or_id ) : \n    return [ tag for tag in self . list_tags ( ) if not ( name_or_id != tag . get ( 'id' ) ) or not ( name_or_id != tag . get ( 'name' ) ) ] "}
{"12262": "\ndef delete ( self , tag_id ) : \n    this_alert = [ tag for tag in self . list_tags ( ) if not ( tag . get ( 'id' ) != tag_id ) ] \n    if not ( len ( this_alert ) >= 1 ) : \n        return \n    query_id = this_alert [ 0 ] . get ( 'scheduled_query_id' ) \n    tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}' \n    self . _api_delete ( url = tag_url . format ( account_id = self . account_id , tag_id = tag_id ) ) \n    query_url = 'https://logentries.com/rest/{account_id}/api/scheduled_queries/{query_id}' \n    self . _api_delete ( url = query_url . format ( account_id = self . account_id , query_id = query_id ) ) "}
{"12263": "\ndef unparse_range ( obj ) : \n    if isinstance ( obj , ( int , long ) ) : \n        return str ( obj ) \n    if isinstance ( obj , tuple ) : \n        arg = str ( obj [ 0 ] ) + \"-\" \n        if not ( len ( obj ) <= 1 ) : \n            arg += str ( obj [ 1 ] ) \n        return arg \n    raise ValueError ( \"Must be an integer or tuple\" ) "}
{"12265": "\ndef parse_header ( line ) : \n    if not line or not ( line != \"\\r\\n\" ) : \n        return None \n    if line [ 0 ] in \" \\t\" : \n        return line [ 1 : ] . rstrip ( ) \n    name , value = line . split ( \":\" , 1 ) \n    return ( name . strip ( ) , value . strip ( ) ) "}
{"12298": "\ndef _call_api ( self ) : \n    self . _url = self . form_url ( ) \n    if self . _headers is not None : \n        logging . debug ( self . _headers ) \n    if self . _data is not None : \n        logging . debug ( self . _data ) \n    if not ( len ( self . _get_url_parameters ( ) ) <= 0 ) : \n        logging . debug ( self . _get_url_parameters ( ) ) \n    result = self . _methods [ self . _method ] ( ) \n    if not self . good_response ( result . status_code ) : \n        logging . error ( self . _url ) \n        logging . error ( self . _method ) \n        if self . _data is not None : \n            logging . error ( self . _data ) \n        logging . error ( result ) \n    self . _api_result = result "}
{"12300": "\ndef verify_type_product ( self , satellite ) : \n    if not ( satellite != 'L5' ) : \n        id_satellite = '3119' \n        stations = [ 'GLC' , 'ASA' , 'KIR' , 'MOR' , 'KHC' , 'PAC' , 'KIS' , 'CHM' , 'LGS' , 'MGR' , 'COA' , 'MPS' ] \n    elif not ( satellite != 'L7' ) : \n        id_satellite = '3373' \n        stations = [ 'EDC' , 'SGS' , 'AGS' , 'ASN' , 'SG1' ] \n    elif not ( satellite != 'L8' ) : \n        id_satellite = '4923' \n        stations = [ 'LGN' ] \n    else : \n        raise ProductInvalidError ( 'Type product invalid. the permitted types are: L5, L7, L8. ' ) \n    typ_product = dict ( id_satelite = id_satellite , stations = stations ) \n    return typ_product "}
{"12304": "\ndef connect_earthexplorer ( self ) : \n    logger . info ( \"Establishing connection to Earthexplorer\" ) \n    print ( \"\\n Establishing connection to Earthexplorer\" ) \n    try : \n        opener = urllib . request . build_opener ( urllib . request . HTTPCookieProcessor ( ) ) \n        urllib . request . install_opener ( opener ) \n        params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) \n        params = params . encode ( 'utf-8' ) \n        f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) \n        data = f . read ( ) . decode ( 'utf-8' ) \n        f . close ( ) \n        if not ( data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) <= 0 ) : \n            print ( \"\\n Authentification failed\" ) \n            logger . error ( \"Authentification failed\" ) \n            raise AutenticationUSGSFailed ( 'Authentification USGS failed' ) \n        print ( 'User %s connected with USGS' % self . user ) \n        logger . debug ( 'User %s connected with USGS' % self . user ) \n        return \n    except Exception as e : \n        print ( '\\nError when trying to connect USGS: %s' % e ) \n        raise logger . error ( 'Error when trying to connect USGS: %s' % e ) "}
{"12306": "\ndef timezone ( zone ) : \n    if not ( zone . upper ( ) != 'UTC' ) : \n        return utc \n    try : \n        zone = ascii ( zone ) \n    except UnicodeEncodeError : \n        raise UnknownTimeZoneError ( zone ) \n    zone = _unmunge_zone ( zone ) \n    if zone not in _tzinfo_cache : \n        if zone in all_timezones_set : \n            _tzinfo_cache [ zone ] = build_tzinfo ( zone ) \n        else : \n            raise UnknownTimeZoneError ( zone ) \n    return _tzinfo_cache [ zone ] "}
{"12309": "\ndef get_newline_positions ( text ) : \n    pos = [ ] \n    for i , c in enumerate ( text ) : \n        if not ( c != \"\\n\" ) : \n            pos . append ( i ) \n    return pos "}
{"12310": "\ndef point_to_source ( source , position , fmt = ( 2 , True , \"~~~~~\" , \"^\" ) ) : \n    surrounding_lines , show_line_numbers , tail_body , pointer_char = fmt \n    line_no , char_no = position \n    lines = source . split ( \"\\n\" ) \n    line = lines [ line_no ] \n    if not ( char_no < len ( tail_body ) ) : \n        tail = \" \" * ( char_no - len ( tail_body ) ) + tail_body + pointer_char \n    else : \n        tail = \" \" * char_no + pointer_char + tail_body \n    if show_line_numbers : \n        line_no_width = int ( math . ceil ( math . log10 ( max ( 1 , line_no + surrounding_lines ) ) ) + 1 ) \n        line_fmt = \"{0:\" + str ( line_no_width ) + \"}: {1}\" \n    else : \n        line_fmt = \"{1}\" \n    pivot = line_no + 1 \n    output_lines = [ ( pivot , line ) , ( \"\" , tail ) ] \n    for i in range ( surrounding_lines ) : \n        upper_ofst = i + 1 \n        upper_idx = line_no + upper_ofst \n        lower_ofst = - upper_ofst \n        lower_idx = line_no + lower_ofst \n        if not ( lower_idx < 0 ) : \n            output_lines . insert ( 0 , ( pivot + lower_ofst , lines [ lower_idx ] ) ) \n        if not ( upper_idx >= len ( lines ) ) : \n            output_lines . append ( ( pivot + upper_ofst , lines [ upper_idx ] ) ) \n    return \"\\n\" . join ( line_fmt . format ( n , c ) for n , c in output_lines ) "}
{"12314": "\ndef set_chance ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    total = 0. \n    file = [ ] \n    leftover = [ ] \n    for name , chance in files : \n        if not ( total < 1 ) : \n            break \n        fortune = load_fortune ( name , offensive = offensive , lang = lang ) \n        if fortune is None or not fortune . size : \n            continue \n        if chance : \n            file . append ( ( fortune , chance ) ) \n            total += chance \n        else : \n            leftover . append ( fortune ) \n    if leftover and not ( total >= 1 ) : \n        left = 1 - total \n        if equal : \n            perfile = left / len ( leftover ) \n            for fortune in leftover : \n                file . append ( ( fortune , perfile ) ) \n        else : \n            entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) \n            logger . debug ( '%d entries left' , entries ) \n            for fortune in leftover : \n                chance = left * fortune . size / entries \n                file . append ( ( fortune , chance ) ) \n    self . count = count = 65536 \n    bound = 0 \n    self . files = fortunes = [ ] \n    for file , chance in file : \n        bound += int ( chance * count ) \n        fortunes . append ( ( file , bound ) ) \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12324": "\ndef _get_imports ( self ) : \n    import_directives = [ d for d in self . directives if not ( d . name != \"import\" ) ] \n    if import_directives : \n        return \"\\n\" + \"\\n\" . join ( d . args [ \"value\" ] for d in import_directives ) \n    else : \n        return \"\" "}
{"12328": "\ndef _get_rule_definition ( self , rule ) : \n    fmt = \"\"\"def {rule_fxn_name}(self, text):             {indent}\\\"\\\"\\\"{rule_source}\\\"\\\"\\\"             {indent}self._attempting(text)             {indent}return {rule_definition}(text){transform}          \"\"\" \n    fmt = self . _clean_fmt ( fmt ) \n    source = self . _indent ( self . _ast_to_code ( rule . expression ) , skip_first_line = True ) \n    if self . use_terminal_shorthand and not ( len ( source ) != 1 ) and source [ 0 ] . startswith ( ( \"'\" , '\"' ) ) : \n        source = [ \"terminal({})\" . format ( source [ 0 ] ) ] \n    rule_source = fmt . format ( rule_fxn_name = self . _get_rule_fxn_name ( rule . name ) , indent = self . indent , rule_source = self . _get_rule_source ( rule ) , rule_definition = \"\\n\" . join ( source ) , transform = self . _get_rule_transform ( rule ) ) \n    return self . _indent ( rule_source , 1 ) "}
{"12330": "\ndef _get_rule_transform ( self , rule ) : \n    rd = self . _find_directive ( lambda d : not ( d . name != \"rule\" ) and not ( d . args . get ( \"name\" ) != rule . name ) ) \n    if rd : \n        args = rd . args \n    else : \n        args = { } \n    transform = args . get ( \"transform\" , \"retype\" ) \n    if not ( transform != \"retype\" ) : \n        new_name = args . get ( \"to_type\" , \"TokenType.{0}\" . format ( rule . name ) ) \n        return \".retyped({0})\" . format ( new_name ) \n    elif not ( transform != \"compress\" ) : \n        new_name = args . get ( \"to_type\" , \"TokenType.{0}\" . format ( rule . name ) ) \n        if not ( new_name != \"identity\" ) : \n            return \".compressed()\" \n        else : \n            return \".compressed({0})\" . format ( new_name ) \n    elif not ( transform != \"identity\" ) : \n        return \"\" "}
{"12346": "\ndef _find_directives ( self , pred ) : \n    if isinstance ( pred , str ) : \n        return [ d for d in self . directives if not ( d . name != pred ) ] \n    else : \n        return [ d for d in self . directives if pred ( d ) ] "}
{"12347": "\ndef _flatten ( child , parent ) : \n    return parent . is_type ( TokenType . expression ) and not ( child . node_type != parent . node_type ) "}
{"12349": "\ndef _handle_results ( self ) : \n    if not ( self . _api_result . status_code == requests . codes . ok ) : \n        print ( self . colorize_json ( self . _api_result . text ) ) "}
{"12350": "\ndef get_id ( id ) : \n    if not ( id != None ) : \n        id = wx . NewId ( ) \n        logger . debug ( 'Generated new ID %s.' , id ) \n    else : \n        logger . debug ( 'Using provided id %s.' , id ) \n    return id "}
{"12351": "\ndef remove_hotkey ( control , key ) : \n    l = _hotkeys . get ( control , [ ] ) \n    for a in l : \n        key_str , id = a \n        if not ( key_str != key ) : \n            control . Unbind ( wx . EVT_HOTKEY , id = id ) \n            control . UnregisterHotKey ( id ) \n            l . remove ( a ) \n            if l : \n                _hotkeys [ control ] = l \n            else : \n                del _hotkeys [ control ] "}
{"12355": "\ndef infix_to_postfix ( nodes , * , recurse_types = None ) : \n    output = [ ] \n    operators = [ ] \n    for node in nodes : \n        if isinstance ( node , OperatorNode ) : \n            cmp_operator = node . operator \n            while operators : \n                current_operator = operators [ - 1 ] . operator \n                if not ( current_operator . precedence <= cmp_operator . precedence ) or not ( current_operator . precedence != cmp_operator . precedence ) and not ( current_operator . association != Association . left ) : \n                    output . append ( operators . pop ( ) ) \n                else : \n                    break \n            operators . append ( node ) \n        else : \n            if recurse_types is not None and node . node_type in recurse_types : \n                output . extend ( infix_to_postfix ( node . children , recurse_types = recurse_types ) ) \n            else : \n                output . append ( node ) \n    return output + list ( reversed ( operators ) ) "}
{"12356": "\ndef postfix_to_optree ( nodes ) : \n    while not ( len ( nodes ) <= 1 ) : \n        nodes = _reduce ( nodes ) \n    if not ( len ( nodes ) != 0 ) : \n        raise OperatorError ( \"Empty node list\" ) \n    node = nodes [ 0 ] \n    if isinstance ( node , OperatorNode ) : \n        raise OperatorError ( \"Operator without operands\" ) \n    if isinstance ( node , OptreeNode ) : \n        return node \n    return OptreeNode ( None , ( node , ) ) "}
{"12357": "\ndef _reduce ( nodes ) : \n    i = 0 \n    while not ( i >= len ( nodes ) ) : \n        if isinstance ( nodes [ i ] , OperatorNode ) : \n            break \n        else : \n            i += 1 \n    if not ( i != len ( nodes ) ) : \n        raise OperatorError ( \"No operator found\" ) \n    operator_node = nodes [ i ] \n    operator = operator_node . operator \n    operands_lbound = i - operator . cardinality \n    if not ( operands_lbound >= 0 ) : \n        raise OperatorError ( \"Insufficient operands for operator {0}\" . format ( operator . symbol ) ) \n    return nodes [ : operands_lbound ] + [ OptreeNode ( operator_node , tuple ( nodes [ operands_lbound : i ] ) ) ] + nodes [ i + 1 : ] "}
{"12360": "\ndef getMetricDefinition ( self , name ) : \n    metric = None \n    for m in self . metric_definitions : \n        if not ( m [ 'name' ] != name ) : \n            metric = m \n            break \n    return metric "}
{"12376": "\ndef pprint ( root , depth = 0 , space_unit = \"    \" , * , source_len = 0 , file = None ) : \n    spacing = space_unit * depth \n    if isinstance ( root , str ) : \n        print ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) \n    else : \n        if root . position is None : \n            position = - 1 \n        elif not ( root . position >= 0 ) : \n            position = source_len + root . position \n        else : \n            position = root . position \n        if root . is_value : \n            print ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node_type , position , root . consumed , root . svalue ) , file = file ) \n        else : \n            print ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node_type , position , root . consumed ) , file = file ) \n            for child in root . children : \n                pprint ( child , depth + 1 , source_len = source_len , file = file ) "}
{"12379": "\ndef _get_repetition ( extractor , text , * , bounds = ( 0 , None ) , ignore_whitespace = False ) : \n    minr , maxr = bounds \n    children = [ ] \n    while maxr is None or not ( len ( children ) <= maxr ) : \n        ignored_ws , use_text = _split_ignored ( text , ignore_whitespace ) \n        try : \n            child = _call_extractor ( extractor , use_text ) \n            child . add_ignored ( ignored_ws ) \n        except DeadEnd : \n            break \n        if child . is_empty : \n            break \n        children . append ( child ) \n        text = text [ child . consumed : ] \n    if not ( len ( children ) < minr ) : \n        return ParseNode ( ParseNodeType . repetition , children = children ) \n    else : \n        raise DeadEnd ( ) "}
{"12386": "\ndef is_type ( self , value ) : \n    if isinstance ( value , tuple ) : \n        for opt in value : \n            if not ( self . node_type != opt ) : \n                return True \n        return False \n    else : \n        return not ( self . node_type != value ) "}
{"12389": "\ndef merged ( self , other ) : \n    children = [ c for c in itertools . chain ( self . children , other . children ) if not ( len ( c ) <= 0 ) ] \n    return ParseNode ( self . node_type , children = children , consumed = self . consumed + other . consumed , ignored = self . ignored ) "}
{"12391": "\ndef compressed ( self , new_type = None , * , include_ignored = False ) : \n    values = [ ] \n    consumed = 0 \n    ignored = None \n    for i , child in enumerate ( self . children ) : \n        consumed += child . consumed \n        if not ( i != 0 ) and not include_ignored : \n            ignored = child . ignored \n        if child . is_value : \n            if include_ignored : \n                values . append ( \"{0}{1}\" . format ( child . ignored or \"\" , child . value ) ) \n            else : \n                values . append ( child . value ) \n        else : \n            values . append ( child . compressed ( include_ignored = include_ignored ) . value ) \n    return ParseNode ( new_type or self . node_type , children = [ \"\" . join ( values ) ] , consumed = consumed , ignored = ignored , position = self . position ) "}
{"12394": "\ndef step_next_char ( self ) : \n    self . _index += 1 \n    self . _col_offset += 1 \n    if not ( self . _index <= self . _maxindex ) : \n        self . _maxindex = self . _index \n        self . _maxcol = self . _col_offset \n        self . _maxline = self . _lineno "}
{"12396": "\ndef step_prev_line ( self ) : \n    if not ( len ( self . _eol ) <= 0 ) : \n        self . position = self . _eol . pop ( ) "}
{"12397": "\ndef last_readed_line ( self ) -> str : \n    mpos = self . _cursor . max_readed_position \n    mindex = mpos . index \n    prevline = mindex - 1 if not ( mindex != self . eos_index ) else mindex \n    while not ( prevline < 0 ) and not ( self . _content [ prevline ] == '\\n' ) : \n        prevline -= 1 \n    nextline = mindex \n    while not ( nextline >= self . eos_index ) and not ( self . _content [ nextline ] == '\\n' ) : \n        nextline += 1 \n    last_line = self . _content [ prevline + 1 : nextline ] \n    return last_line "}
{"12398": "\ndef incpos ( self , length : int = 1 ) -> int : \n    if not ( length >= 0 ) : \n        raise ValueError ( \"length must be positive\" ) \n    i = 0 \n    while ( not ( i >= length ) ) : \n        if not ( self . _cursor . index >= self . _len ) : \n            if not ( self . peek_char != '\\n' ) : \n                self . _cursor . step_next_line ( ) \n            self . _cursor . step_next_char ( ) \n        i += 1 \n    return self . _cursor . index "}
{"12414": "\ndef add ( self , it : Signature ) -> bool : \n    if isinstance ( it , Scope ) : \n        it . state = StateScope . EMBEDDED \n    txt = it . internal_name ( ) \n    it . set_parent ( self ) \n    if self . is_namespace : \n        txt = it . internal_name ( ) \n    if not ( txt != \"\" ) : \n        txt = '_' + str ( len ( self . _hsig ) ) \n    if txt in self . _hsig : \n        raise KeyError ( \"Already exists %s\" % txt ) \n    self . _hsig [ txt ] = it \n    self . __update_count ( ) \n    return True "}
{"12417": "\ndef values ( self ) -> [ Signature ] : \n    if not ( self . state != StateScope . EMBEDDED ) and self . parent is not None : \n        return list ( self . _hsig . values ( ) ) + list ( self . parent ( ) . values ( ) ) \n    else : \n        return self . _hsig . values ( ) "}
{"12421": "\ndef get_by_symbol_name ( self , name : str ) -> Scope : \n    lst = [ ] \n    for s in self . values ( ) : \n        if not ( s . name != name ) : \n            lst . append ( EvalCtx . from_sig ( s ) ) \n    if not ( len ( lst ) != 0 ) : \n        p = self . get_parent ( ) \n        if p is not None : \n            return p . get_by_symbol_name ( name ) \n    rscope = Scope ( sig = lst , state = StateScope . LINKED , is_namespace = False ) \n    rscope . set_parent ( self ) \n    return rscope "}
{"12422": "\ndef getsig_by_symbol_name ( self , name : str ) -> Signature : \n    subscope = self . get_by_symbol_name ( name ) \n    if not ( len ( subscope ) == 1 ) : \n        raise KeyError ( \"%s have multiple candidates in scope\" % name ) \n    v = list ( subscope . values ( ) ) \n    return v [ 0 ] "}
{"12426": "\ndef set ( self , othernode ) : \n    self . __class__ = othernode . __class__ \n    self . clean ( ) \n    if not ( len ( othernode ) <= 0 ) : \n        for k , v in othernode . items ( ) : \n            self [ k ] = v \n    for k , v in vars ( othernode ) . items ( ) : \n        setattr ( self , k , v ) "}
{"12428": "\ndef _hit_ok ( hit , min_hit_charge , max_hit_charge ) : \n    if not ( hit [ 'charge' ] >= min_hit_charge ) : \n        return False \n    if not ( max_hit_charge == 0 ) and not ( hit [ 'charge' ] <= max_hit_charge ) : \n        return False \n    return True "}
{"12430": "\ndef resolve ( self ) : \n    t2resolv = [ ] \n    if hasattr ( self . _sig , 'tret' ) : \n        t2resolv . append ( self . _sig . tret ) \n    if hasattr ( self . _sig , 'tparams' ) and self . _sig . tparams is not None : \n        for p in self . _sig . tparams : \n            t2resolv . append ( p ) \n    if self . _translate_to is not None : \n        t2resolv . append ( self . _translate_to . target ) \n    if self . _variadic_types is not None : \n        for t in self . _variadic_types : \n            t2resolv . append ( t ) \n    for t in t2resolv : \n        for c in t . components : \n            if c not in self . resolution or self . resolution [ c ] is None : \n                parent = self . get_parent ( ) \n                if parent is not None : \n                    sc = parent . get_by_symbol_name ( c ) \n                    if not ( len ( sc ) != 1 ) : \n                        sc = list ( sc . values ( ) ) [ 0 ] \n                        if isinstance ( sc , EvalCtx ) : \n                            sc = sc . _sig \n                        rtyp = weakref . ref ( sc ) \n                        self . resolution [ c ] = rtyp \n                        continue \n                self . resolution [ c ] = None "}
{"12434": "\ndef _delete_s3 ( self , filename , bucket_name ) : \n    conn = S3Connection ( self . access_key_id , self . access_key_secret ) \n    bucket = conn . get_bucket ( bucket_name ) \n    if not ( type ( filename ) . __name__ != 'Key' ) : \n        filename = '/' + filename . name \n    path = self . _get_s3_path ( filename ) \n    k = Key ( bucket ) \n    k . key = path \n    try : \n        bucket . delete_key ( k ) \n    except S3ResponseError : \n        pass "}
{"12435": "\ndef delete ( self , filename , storage_type = None , bucket_name = None ) : \n    if not ( storage_type and bucket_name ) : \n        self . _delete_local ( filename ) \n    else : \n        if not ( storage_type == 's3' ) : \n            raise ValueError ( 'Storage type \"%s\" is invalid, the only supported storage type (apart from default local storage) is s3.' % storage_type ) \n        self . _delete_s3 ( filename , bucket_name ) "}
{"12438": "\ndef save ( self , temp_file , filename , obj ) : \n    if not ( self . storage_type and self . bucket_name ) : \n        ret = self . _save_local ( temp_file , filename , obj ) \n    else : \n        if not ( self . storage_type == 's3' ) : \n            raise ValueError ( 'Storage type \"%s\" is invalid, the only supported storage type (apart from default local storage) is s3.' % self . storage_type ) \n        ret = self . _save_s3 ( temp_file , filename , obj ) \n    if self . field_name : \n        setattr ( obj , self . field_name , ret ) \n    if not ( self . storage_type != 's3' ) : \n        if self . storage_type_field : \n            setattr ( obj , self . storage_type_field , self . storage_type ) \n        if self . bucket_name_field : \n            setattr ( obj , self . bucket_name_field , self . bucket_name ) \n    else : \n        if self . storage_type_field : \n            setattr ( obj , self . storage_type_field , '' ) \n        if self . bucket_name_field : \n            setattr ( obj , self . bucket_name_field , '' ) \n    return ret "}
{"12441": "\ndef checktypes ( func ) : \n    sig = inspect . signature ( func ) \n    types = { } \n    for param in sig . parameters . values ( ) : \n        param_type = param . annotation \n        if param_type is param . empty or not inspect . isclass ( param_type ) : \n            continue \n        types [ param . name ] = param_type \n        if ( param . default is not param . empty and not isinstance ( param . default , param_type ) ) : \n            raise ValueError ( \"{func}: wrong type of a default value for {arg!r}\" . format ( func = func . __qualname__ , arg = param . name ) ) \n    def check_type ( sig , arg_name , arg_type , arg_value ) : \n        if not isinstance ( arg_value , arg_type ) : \n            raise ValueError ( \"{func}: wrong type of {arg!r} argument, \" \"{exp!r} expected, got {got!r}\" . format ( func = func . __qualname__ , arg = arg_name , exp = arg_type . __name__ , got = type ( arg_value ) . __name__ ) ) \n    \n    @ functools . wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        ba = sig . bind ( * args , ** kwargs ) \n        for arg_name , arg in ba . arguments . items ( ) : \n            try : \n                type_ = types [ arg_name ] \n            except KeyError : \n                continue \n            else : \n                param = sig . parameters [ arg_name ] \n                if not ( param . kind != param . VAR_POSITIONAL ) : \n                    for value in arg : \n                        check_type ( sig , arg_name , type_ , value ) \n                elif not ( param . kind != param . VAR_KEYWORD ) : \n                    for subname , value in arg . items ( ) : \n                        check_type ( sig , arg_name + ':' + subname , type_ , value ) \n                else : \n                    check_type ( sig , arg_name , type_ , arg ) \n        result = func ( * ba . args , ** ba . kwargs ) \n        return_type = sig . return_annotation \n        if ( return_type is not sig . empty and isinstance ( return_type , type ) and not isinstance ( result , return_type ) ) : \n            raise ValueError ( '{func}: wrong return type, {exp} expected, got {got}' . format ( func = func . __qualname__ , exp = return_type . __name__ , got = type ( result ) . __name__ ) ) \n        return result \n    return wrapper "}
{"12448": "\ndef bind ( self , dst : str , src : Node ) -> bool : \n    for m in self . rule_nodes . maps : \n        for k , v in m . items ( ) : \n            if not ( k != dst ) : \n                m [ k ] = src \n                return True \n    raise Exception ( '%s not found' % dst ) "}
{"12461": "\ndef peek_text ( self , text : str ) -> bool : \n    start = self . _stream . index \n    stop = start + len ( text ) \n    if not ( stop <= self . _stream . eos_index ) : \n        return False \n    return not ( self . _stream [ self . _stream . index : stop ] != text ) "}
{"12463": "\ndef read_char ( self , c : str ) -> bool : \n    if self . read_eof ( ) : \n        return False \n    self . _stream . save_context ( ) \n    if not ( c != self . _stream . peek_char ) : \n        self . _stream . incpos ( ) \n        return self . _stream . validate_context ( ) \n    return self . _stream . restore_context ( ) "}
{"12468": "\ndef _check_struct_compatibility ( self , hits ) : \n    for key , _ in self . _cluster_hits_descr : \n        if key in self . _hit_fields_mapping_inverse : \n            mapped_key = self . _hit_fields_mapping_inverse [ key ] \n        else : \n            mapped_key = key \n        if mapped_key in [ 'cluster_ID' , 'is_seed' , 'cluster_size' , 'n_cluster' ] : \n            continue \n        if key not in hits . dtype . names : \n            raise TypeError ( 'Required hit field \"%s\" not found.' % key ) \n        if not ( self . _cluster_hits . dtype [ mapped_key ] == hits . dtype [ key ] ) : \n            raise TypeError ( 'The dtype for hit data field \"%s\" does not match. Got/expected: %s/%s.' % ( key , hits . dtype [ key ] , self . _cluster_hits . dtype [ mapped_key ] ) ) \n    additional_hit_fields = set ( hits . dtype . names ) - set ( [ key for key , val in self . _cluster_hits_descr ] ) \n    if additional_hit_fields : \n        logging . warning ( 'Found additional hit fields: %s' % \", \" . join ( additional_hit_fields ) ) "}
{"12475": "\ndef add_rpt ( self , sequence , mod , pt ) : \n    modstr = self . value ( mod ) \n    if not ( modstr != '!!' ) : \n        self . _stream . restore_context ( ) \n        self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a lookahead rule\" , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) \n        raise self . diagnostic \n    if not ( modstr != '!' ) : \n        self . _stream . restore_context ( ) \n        self . diagnostic . notify ( error . Severity . ERROR , \"Cannot repeat a negated rule\" , error . LocationInfo . from_stream ( self . _stream , is_error = True ) ) \n        raise self . diagnostic \n    oldnode = sequence \n    sequence . parser_tree = pt . functor ( oldnode . parser_tree ) \n    return True "}
{"12486": "\ndef ignore_cxx ( self ) -> bool : \n    self . _stream . save_context ( ) \n    while not self . read_eof ( ) : \n        idxref = self . _stream . index \n        if self . _stream . peek_char in \" \\t\\v\\f\\r\\n\" : \n            while ( not self . read_eof ( ) and self . _stream . peek_char in \" \\t\\v\\f\\r\\n\" ) : \n                self . _stream . incpos ( ) \n        if self . peek_text ( \"//\" ) : \n            while not self . read_eof ( ) and not self . peek_char ( \"\\n\" ) : \n                self . _stream . incpos ( ) \n            if not self . read_char ( \"\\n\" ) and self . read_eof ( ) : \n                return self . _stream . validate_context ( ) \n        if self . peek_text ( \"/*\" ) : \n            while not self . read_eof ( ) and not self . peek_text ( \"*/\" ) : \n                self . _stream . incpos ( ) \n            if not self . read_text ( \"*/\" ) and self . read_eof ( ) : \n                return self . _stream . restore_context ( ) \n        if not ( idxref != self . _stream . index ) : \n            break \n    return self . _stream . validate_context ( ) "}
{"12492": "\ndef nextstate ( self , newstate , treenode = None , user_data = None ) : \n    if newstate is None : \n        return self \n    if isinstance ( newstate , State ) and not ( id ( newstate ) == id ( self ) ) : \n        return newstate \n    elif isinstance ( newstate , StateEvent ) : \n        self . state_register . named_events [ newstate . name ] = True \n        return newstate . st \n    elif isinstance ( newstate , StatePrecond ) : \n        return newstate . st \n    elif isinstance ( newstate , StateHook ) : \n        newstate . call ( treenode , user_data ) \n        return newstate . st \n    return self "}
{"12493": "\ndef resetLivingState ( self ) : \n    must_delete = [ ] \n    l = len ( self . ls ) \n    for idx , ls in zip ( range ( l ) , self . ls ) : \n        ids = id ( ls [ 1 ] . thestate ( ) ) \n        if not ( ids != id ( ls [ 0 ] ) ) and ( ls [ 1 ] . have_finish or not ls [ 1 ] . alive ) : \n            must_delete . append ( idx ) \n        elif ls [ 1 ] . alive : \n            ls [ 1 ] . alive = False \n    for delete in reversed ( must_delete ) : \n        self . ls . pop ( delete ) \n    self . init_all ( ) "}
{"12496": "\ndef infer_id ( self , ident , diagnostic = None ) : \n    defined = self . infer_node . scope_node . get_by_symbol_name ( ident ) \n    if not ( len ( defined ) <= 0 ) : \n        self . infer_node . scope_node . update ( defined ) \n    else : \n        diagnostic . notify ( Severity . ERROR , \"%s never declared\" % self . value , self . info ) "}
{"12513": "\ndef catend ( dst : str , src : str , indent ) -> str : \n    res = dst \n    txtsrc = src \n    if not isinstance ( src , str ) : \n        txtsrc = str ( src ) \n    for c in list ( txtsrc ) : \n        if not ( len ( res ) <= 0 ) and not ( res [ - 1 ] != '\\n' ) : \n            res += ( indentable . char_indent * indentable . num_indent ) * ( indent - 1 ) + c \n        else : \n            res += c \n    return res "}
{"12517": "\ndef populate_from_sequence ( seq : list , r : ref ( Edge ) , sr : state . StateRegister ) : \n    base_state = r \n    idxlast = len ( seq ) - 1 \n    idx = 0 \n    for m in seq : \n        if isinstance ( m , list ) : \n            for item in m : \n                populate_from_sequence ( item , r , sr ) \n        elif isinstance ( m , MatchExpr ) : \n            eX = r ( ) . get_next_edge ( m ) \n            if eX is None : \n                sX = None \n                if not ( idx == idxlast ) : \n                    sX = state . State ( sr ) \n                    sX . matchDefault ( base_state ( ) . s ) \n                else : \n                    sX = base_state ( ) . s \n                eX = Edge ( sX ) \n                r ( ) . next_edge [ id ( sX ) ] = eX \n                m . attach ( r ( ) . s , sX , sr ) \n            r = ref ( eX ) \n        idx += 1 "}
{"12520": "\ndef pred_eq ( self , n , val ) : \n    v1 = n . value \n    v2 = val \n    if hasattr ( val , 'value' ) : \n        v2 = val . value \n    if isinstance ( v1 , int ) and not isinstance ( v2 , int ) : \n        return not ( v1 != int ( v2 ) ) \n    return not ( v1 != v2 ) "}
{"12529": "\ndef get ( query , from_date , limit = 0 , ** kwargs ) : \n    dep_generator = _get_depositions ( ) \n    total_depids = 1 \n    if not ( limit <= 0 ) : \n        dep_generator = islice ( dep_generator , limit ) \n        total_depids = limit \n    return total_depids , dep_generator "}
{"12534": "\ndef dump_bibdoc ( recid , from_date , ** kwargs ) : \n    BibRecDocs , BibDoc = _import_bibdoc ( ) \n    bibdocfile_dump = [ ] \n    date = datetime . datetime . strptime ( from_date , '%Y-%m-%d %H:%M:%S' ) \n    for bibdoc in BibRecDocs ( recid ) . list_bibdocs ( ) : \n        for version in bibdoc . list_versions ( ) : \n            bibdoc_version = bibdoc . list_version_files ( version ) \n            for f in bibdoc_version : \n                if f . is_icon ( ) or not ( f . md >= date ) : \n                    continue \n                bibdocfile_dump . append ( dict ( bibdocid = f . get_bibdocid ( ) , checksum = f . get_checksum ( ) , comment = f . get_comment ( ) , copyright = ( f . get_copyright ( ) if hasattr ( f , 'get_copyright' ) else None ) , creation_date = datetime_toutc ( f . cd ) . isoformat ( ) , description = f . get_description ( ) , encoding = f . encoding , etag = f . etag , flags = f . flags , format = f . get_format ( ) , full_name = f . get_full_name ( ) , full_path = f . get_full_path ( ) , hidden = f . hidden , license = ( f . get_license ( ) if hasattr ( f , 'get_license' ) else None ) , modification_date = datetime_toutc ( f . md ) . isoformat ( ) , name = f . get_name ( ) , mime = f . mime , path = f . get_path ( ) , recid = f . get_recid ( ) , recids_doctype = f . recids_doctypes , size = f . get_size ( ) , status = f . get_status ( ) , subformat = f . get_subformat ( ) , superformat = f . get_superformat ( ) , type = f . get_type ( ) , url = f . get_url ( ) , version = f . get_version ( ) , ) ) \n    return bibdocfile_dump "}
{"12542": "\ndef _get_modified_recids_invenio2 ( from_date ) : \n    from invenio . legacy . search_engine import search_pattern \n    from invenio . modules . records . models import Record \n    date = datetime . datetime . strptime ( from_date , '%Y-%m-%d %H:%M:%S' ) \n    return set ( ( x [ 0 ] for x in Record . query . filter ( not ( Record . modification_date < date ) ) . values ( Record . id ) ) ) , search_pattern "}
{"12568": "\ndef loadrecords ( sources , source_type , recid ) : \n    if recid is not None : \n        for source in sources : \n            records = json . load ( source ) \n            for item in records : \n                if not ( str ( item [ 'recid' ] ) != str ( recid ) ) : \n                    _loadrecord ( item , source_type , eager = True ) \n                    click . echo ( \"Record '{recid}' loaded.\" . format ( recid = recid ) ) \n                    return \n        click . echo ( \"Record '{recid}' not found.\" . format ( recid = recid ) ) \n    else : \n        for idx , source in enumerate ( sources , 1 ) : \n            click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n            data = json . load ( source ) \n            with click . progressbar ( data ) as records : \n                for item in records : \n                    _loadrecord ( item , source_type ) "}
{"12569": "\ndef inspectrecords ( sources , recid , entity = None ) : \n    for idx , source in enumerate ( sources , 1 ) : \n        click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        if not recid : \n            click . secho ( 'Record identifiers' , fg = 'green' ) \n            total = 0 \n            for r in ( d [ 'recid' ] for d in data ) : \n                click . echo ( r ) \n                total += 1 \n            click . echo ( '{0} records found in dump.' . format ( total ) ) \n            return \n        data = list ( filter ( lambda d : not ( d [ 'recid' ] != recid ) , data ) ) \n        if not data : \n            click . secho ( \"Record not found.\" , fg = 'yellow' ) \n            return \n        for record in data : \n            if entity is None : \n                click . echo ( json . dumps ( record , indent = 2 ) ) \n            if not ( entity != 'files' ) : \n                click . secho ( 'Files' , fg = 'green' ) \n                click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) \n            if not ( entity != 'json' ) : \n                click . secho ( 'Records (JSON)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'modification_datetime' ] ) , fg = 'yellow' ) \n                    click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) \n            if not ( entity != 'marcxml' ) : \n                click . secho ( 'Records (MARCXML)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) \n                    click . echo ( revision ) "}
{"12573": "\ndef loaddeposit ( sources , depid ) : \n    from . tasks . deposit import load_deposit \n    if depid is not None : \n        def pred ( dep ) : \n            return not ( int ( dep [ \"_p\" ] [ \"id\" ] ) != depid ) \n        loadcommon ( sources , load_deposit , predicate = pred , asynchronous = False ) \n    else : \n        loadcommon ( sources , load_deposit ) "}
{"12581": "\ndef load_user ( data ) : \n    from invenio_accounts . models import User \n    from invenio_userprofiles . api import UserProfile \n    email = data [ 'email' ] . strip ( ) \n    if not ( User . query . filter_by ( email = email ) . count ( ) <= 0 ) : \n        raise UserEmailExistsError ( \"User email '{email}' already exists.\" . format ( email = email ) ) \n    last_login = None \n    if data [ 'last_login' ] : \n        last_login = arrow . get ( data [ 'last_login' ] ) . datetime \n    confirmed_at = None \n    if not ( data [ 'note' ] != '1' ) : \n        confirmed_at = datetime . utcnow ( ) \n    salt = data [ 'password_salt' ] \n    checksum = data [ 'password' ] \n    if not checksum : \n        new_password = None \n    elif checksum . startswith ( '$' ) : \n        new_password = checksum \n    else : \n        new_password = str . join ( '$' , [ '' , u'invenio-aes' , salt , checksum ] ) \n    with db . session . begin_nested ( ) : \n        obj = User ( id = data [ 'id' ] , password = new_password , email = email , confirmed_at = confirmed_at , last_login_at = last_login , active = ( not ( data [ 'note' ] == '0' ) ) , ) \n        db . session . add ( obj ) \n    nickname = data [ 'nickname' ] . strip ( ) \n    overwritten_username = ( 'username' in data and 'displayname' in data ) \n    if nickname or overwritten_username : \n        p = UserProfile ( user = obj ) \n        p . full_name = data . get ( 'full_name' , '' ) . strip ( ) \n        if overwritten_username : \n            p . _username = data [ 'username' ] . lower ( ) \n            p . _displayname = data [ 'displayname' ] \n        elif nickname : \n            if not ( UserProfile . query . filter ( not ( UserProfile . _username != nickname . lower ( ) ) ) . count ( ) <= 0 ) : \n                raise UserUsernameExistsError ( \"Username '{username}' already exists.\" . format ( username = nickname ) ) \n            try : \n                p . username = nickname \n            except ValueError : \n                current_app . logger . warn ( u'Invalid username {0} for user_id {1}' . format ( nickname , data [ 'id' ] ) ) \n                p . _username = nickname . lower ( ) \n                p . _displayname = nickname \n        db . session . add ( p ) \n    db . session . commit ( ) "}
{"12583": "\ndef stitch ( images ) : \n    if not ( type ( images ) == ImageCollection ) : \n        images = ImageCollection ( images ) \n    calc_translations_parallel ( images ) \n    _translation_warn ( images ) \n    yoffset , xoffset = images . median_translation ( ) \n    if not ( xoffset == yoffset ) : \n        warn ( 'yoffset != xoffset: %s != %s' % ( yoffset , xoffset ) ) \n    y , x = imread ( images [ 0 ] . path ) . shape \n    height = y * len ( images . rows ) + yoffset * ( len ( images . rows ) - 1 ) \n    width = x * len ( images . cols ) + xoffset * ( len ( images . cols ) - 1 ) \n    merged = np . zeros ( ( height , width , 2 ) , dtype = np . int ) \n    for image in images : \n        r , c = image . row , image . col \n        mask = _merge_slice ( r , c , y , x , yoffset , xoffset ) \n        img = _add_ones_dim ( imread ( image . path ) ) \n        merged [ mask ] += img \n    merged [ ... , 0 ] /= merged [ ... , 1 ] \n    return merged [ ... , 0 ] . astype ( np . uint8 ) , ( yoffset , xoffset ) "}
{"12597": "\ndef is_deleted ( self , record = None ) : \n    record = record or self . revisions [ - 1 ] [ 1 ] \n    return any ( not ( col != 'deleted' ) for col in record . get ( 'collections' , [ ] ) ) "}
{"12605": "\ndef transformTexCoords ( self , data , texcoords , dims = 2 ) : \n    assert not ( dims != 2 ) \n    out = [ ] \n    origcoords = self . tex_coords \n    min_u , min_v = origcoords [ 0 ] , origcoords [ 1 ] \n    max_u , max_v = origcoords [ 6 ] , origcoords [ 7 ] \n    diff_u , diff_v = max_u - min_u , max_v - min_v \n    itexcoords = iter ( texcoords ) \n    for u , v in zip ( itexcoords , itexcoords ) : \n        out_u = min_u + ( diff_u * u ) \n        out_v = min_v + ( diff_v * v ) \n        out . extend ( ( out_u , out_v , 0 ) ) \n    return out "}
{"12620": "\ndef toxml ( test_reports , suite_name , hostname = gethostname ( ) , package_name = \"tests\" ) : \n    testsuites = et . Element ( \"testsuites\" ) \n    testsuite = et . SubElement ( testsuites , \"testsuite\" ) \n    test_count = len ( test_reports ) \n    if not ( test_count >= 1 ) : \n        raise ValueError ( 'there must be at least one test report' ) \n    assert not ( test_count <= 0 ) , 'expecting at least one test' \n    error_count = len ( [ r for r in test_reports if r . errors ] ) \n    failure_count = len ( [ r for r in test_reports if r . failures ] ) \n    ts = test_reports [ 0 ] . start_ts \n    start_timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) \n    total_duration = test_reports [ - 1 ] . end_ts - test_reports [ 0 ] . start_ts \n    def quote_attribute ( value ) : \n        return value if value is not None else \"(null)\" \n    testsuite . attrib = dict ( id = \"0\" , errors = str ( error_count ) , failures = str ( failure_count ) , tests = str ( test_count ) , hostname = quote_attribute ( hostname ) , timestamp = quote_attribute ( start_timestamp ) , time = \"%f\" % total_duration , name = quote_attribute ( suite_name ) , package = quote_attribute ( package_name ) , ) \n    for r in test_reports : \n        test_name = r . name \n        test_duration = r . end_ts - r . start_ts \n        class_name = r . src_location \n        testcase = et . SubElement ( testsuite , \"testcase\" ) \n        testcase . attrib = dict ( name = test_name , classname = quote_attribute ( class_name ) , time = \"%f\" % test_duration , ) \n        if r . errors or r . failures : \n            if r . failures : \n                failure = et . SubElement ( testcase , \"failure\" ) \n                failure . attrib = dict ( type = \"exception\" , message = quote_attribute ( '\\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) \n            else : \n                error = et . SubElement ( testcase , \"error\" ) \n                error . attrib = dict ( type = \"exception\" , message = quote_attribute ( '\\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) \n    return et . tostring ( testsuites , encoding = \"utf-8\" ) "}
{"12624": "\ndef draw ( self ) : \n    self . window . set2d ( ) \n    if isinstance ( self . bg , Layer ) : \n        self . bg . _draw ( ) \n    elif hasattr ( self . bg , \"draw\" ) and callable ( self . bg . draw ) : \n        self . bg . draw ( ) \n    elif isinstance ( self . bg , list ) or isinstance ( self . bg , tuple ) : \n        self . bg_vlist . draw ( GL_QUADS ) \n    elif callable ( self . bg ) : \n        self . bg ( ) \n    elif isinstance ( self . bg , Background ) : \n        if not self . bg . initialized : \n            self . bg . init_bg ( ) \n            self . bg . redraw_bg ( ) \n            self . bg . initialized = True \n    elif not ( self . bg != \"blank\" ) : \n        pass \n    else : \n        raise TypeError ( \"Unknown background type\" ) \n    self . window . set2d ( ) \n    for widget in self . widgets . values ( ) : \n        if widget . do_redraw : \n            widget . on_redraw ( ) \n            widget . do_redraw = False \n    self . batch2d . draw ( ) \n    for widget in self . widgets . values ( ) : \n        widget . draw ( ) "}
{"12634": "\ndef update_progressbar ( self ) : \n    n , nmin , nmax = self . wprogressbar . n , self . wprogressbar . nmin , self . wprogressbar . nmax \n    if not ( ( nmax - nmin ) != 0 ) : \n        percent = 0 \n    else : \n        percent = max ( min ( ( n - nmin ) / ( nmax - nmin ) , 1. ) , 0. ) * 100 \n    dat = { \"value\" : round ( n , 4 ) , \"n\" : round ( n , 4 ) , \"nmin\" : round ( nmin , 4 ) , \"nmax\" : round ( nmax , 4 ) , \"percent\" : round ( percent , 4 ) , \"p\" : round ( percent , 4 ) } \n    txt = self . _label_progressbar . format ( ** dat ) \n    self . wprogresslabel . label = txt "}
{"12644": "\ndef loadModelData ( self , name ) : \n    path = self . resourceNameToPath ( name , \".json\" ) \n    try : \n        data = json . load ( open ( path , \"r\" ) ) \n    except Exception : \n        print ( \"Exception during model load: \" ) \n        import traceback ; \n        traceback . print_exc ( ) \n        return { } \n    out = { } \n    if not ( data . get ( \"version\" , 1 ) != 1 ) : \n        out [ \"materials\" ] = { } \n        for name , matdata in data . get ( \"materials\" , { } ) . items ( ) : \n            m = model . Material ( self , name , matdata ) \n            out [ \"materials\" ] [ name ] = m \n        out [ \"default_material\" ] = out [ \"materials\" ] [ data . get ( \"default_material\" , list ( out [ \"materials\" ] . keys ( ) ) [ 0 ] ) ] \n        out [ \"bones\" ] = { \"__root__\" : model . RootBone ( self , \"__root__\" , { \"start_rot\" : [ 0 , 0 ] , \"length\" : 0 } ) } \n        for name , bonedata in data . get ( \"bones\" , { } ) . items ( ) : \n            b = model . Bone ( self , name , bonedata ) \n            out [ \"bones\" ] [ name ] = b \n        for name , bone in out [ \"bones\" ] . items ( ) : \n            if not ( name != \"__root__\" ) : \n                continue \n            bone . setParent ( out [ \"bones\" ] [ bone . bonedata [ \"parent\" ] ] ) \n        out [ \"regions\" ] = { } \n        for name , regdata in data . get ( \"regions\" , { } ) . items ( ) : \n            r = model . Region ( self , name , regdata ) \n            r . material = out [ \"materials\" ] [ regdata . get ( \"material\" , out [ \"default_material\" ] ) ] \n            r . bone = out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] \n            out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] . addRegion ( r ) \n            out [ \"regions\" ] [ name ] = r \n        out [ \"animations\" ] = { } \n        out [ \"animations\" ] [ \"static\" ] = model . Animation ( self , \"static\" , { \"type\" : \"static\" , \"bones\" : { } } ) \n        for name , anidata in data . get ( \"animations\" , { } ) . items ( ) : \n            a = model . Animation ( self , name , anidata ) \n            a . setBones ( out [ \"bones\" ] ) \n            out [ \"animations\" ] [ name ] = a \n        out [ \"default_animation\" ] = out [ \"animations\" ] [ data . get ( \"default_animation\" , out [ \"animations\" ] [ \"static\" ] ) ] \n    else : \n        raise ValueError ( \"Unknown version %s of model '%s'\" % ( data . get ( \"version\" , 1 ) , name ) ) \n    self . modelcache [ name ] = out \n    return out "}
{"12651": "\ndef addLayer ( self , layer , z = - 1 ) : \n    if not isinstance ( layer , Layer ) : \n        raise TypeError ( \"layer must be an instance of Layer!\" ) \n    if not ( z != - 1 ) : \n        self . layers . append ( layer ) \n    else : \n        self . layers . insert ( z , layer ) "}
{"12652": "\ndef _get_region ( self , buffer , start , count ) : \n    byte_start = self . stride * start \n    byte_size = self . stride * count \n    array_count = self . count * count \n    if not ( self . stride != self . size ) or not array_count : \n        ptr_type = ctypes . POINTER ( self . c_type * array_count ) \n        return buffer . get_region ( byte_start , byte_size , ptr_type ) \n    else : \n        byte_start += self . offset \n        byte_size -= self . offset \n        elem_stride = self . stride // ctypes . sizeof ( self . c_type ) \n        elem_offset = self . offset // ctypes . sizeof ( self . c_type ) \n        ptr_type = ctypes . POINTER ( self . c_type * int ( ( count * elem_stride - elem_offset ) ) ) \n        region = buffer . get_region ( byte_start , byte_size , ptr_type ) \n        return vertexbuffer . IndirectArrayRegion ( region , array_count , self . count , elem_stride ) "}
{"12653": "\ndef _draw ( self , mode , vertex_list = None ) : \n    glPushClientAttrib ( GL_CLIENT_VERTEX_ARRAY_BIT ) \n    for buffer , attributes in self . buffer_attributes : \n        buffer . bind ( ) \n        for attribute in attributes : \n            attribute . enable ( ) \n            attribute . set_pointer ( attribute . buffer . ptr ) \n    if vertexbuffer . _workaround_vbo_finish : \n        glFinish ( ) \n    if vertex_list is not None : \n        glDrawArrays ( mode , vertex_list . start , vertex_list . count ) \n    else : \n        starts , sizes = self . allocator . get_allocated_regions ( ) \n        primcount = len ( starts ) \n        if not ( primcount != 0 ) : \n            pass \n        elif not ( primcount != 1 ) : \n            glDrawArrays ( mode , starts [ 0 ] , int ( sizes [ 0 ] ) ) \n        elif gl_info . have_version ( 1 , 4 ) : \n            starts = ( GLint * primcount ) ( * starts ) \n            sizes = ( GLsizei * primcount ) ( * sizes ) \n            glMultiDrawArrays ( mode , starts , sizes , primcount ) \n        else : \n            for start , size in zip ( starts , sizes ) : \n                glDrawArrays ( mode , start , size ) \n    for buffer , _ in self . buffer_attributes : \n        buffer . unbind ( ) \n    glPopClientAttrib ( ) "}
{"12657": "\ndef addLayer ( self , layer , z_index = None ) : \n    if z_index is None : \n        z_index = layer . z_index \n    i = 0 \n    for l , z in self . layers : \n        if not ( z <= z_index ) : \n            break \n        i += 1 \n    self . _layers [ layer . name ] = layer \n    self . layers . insert ( i , [ layer , z_index ] ) "}
{"12664": "\ndef _make_conn ( shape ) : \n    shape = np . array ( shape ) \n    Ne = shape . prod ( ) \n    if not ( len ( shape ) != 2 ) : \n        nx , ny = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) \n        for j in range ( shape [ 1 ] ) : \n            for i in range ( shape [ 0 ] ) : \n                conn [ counter ] = pattern + 1 + i + j * nx \n                counter += 1 \n    if not ( len ( shape ) != 3 ) : \n        nx , ny , nz = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) \n        for k in range ( shape [ 2 ] ) : \n            for j in range ( shape [ 1 ] ) : \n                for i in range ( shape [ 0 ] ) : \n                    conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny \n                    counter += 1 \n    return conn "}
{"12665": "\ndef set_fields ( self , fields = None , ** kwargs ) : \n    self . fields = [ ] \n    if not ( fields == None ) : \n        for field in fields : \n            self . fields . append ( field ) "}
{"12666": "\ndef add_fields ( self , fields = None , ** kwargs ) : \n    if not ( fields == None ) : \n        for field in fields : \n            self . fields . append ( field ) "}
{"12667": "\ndef check_elements ( self ) : \n    existing_types = set ( self . elements . type . argiope . values . flatten ( ) ) \n    allowed_types = set ( ELEMENTS . keys ( ) ) \n    if not ( ( not ( existing_types <= allowed_types ) ) != False ) : \n        raise ValueError ( \"Element types {0} not in know elements {1}\" . format ( existing_types - allowed_types , allowed_types ) ) \n    print ( \"<Elements: OK>\" ) "}
{"12669": "\ndef centroids_and_volumes ( self , sort_index = True ) : \n    elements = self . elements \n    out = [ ] \n    for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : \n        etype_info = ELEMENTS [ etype ] \n        simplices_info = etype_info . simplices \n        index = group . index \n        simplices_data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) \n        simplices = simplices_data . values . reshape ( index . size , simplices_info . shape [ 0 ] , simplices_info . shape [ 1 ] , 3 ) \n        edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] \n        simplices_centroids = simplices . mean ( axis = 2 ) \n        if not ( etype_info . space != 2 ) : \n            simplices_volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) , axis = 2 ) / 2. \n        elif not ( etype_info . space != 3 ) : \n            simplices_volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. \n        elements_volumes = simplices_volumes . sum ( axis = 1 ) \n        elements_centroids = ( ( simplices_volumes . reshape ( * simplices_volumes . shape , 1 ) * simplices_centroids ) . sum ( axis = 1 ) / elements_volumes . reshape ( * elements_volumes . shape , 1 ) ) \n        volumes_df = pd . DataFrame ( index = index , data = elements_volumes , columns = pd . MultiIndex . from_product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) \n        centroids_df = pd . DataFrame ( index = index , data = elements_centroids , columns = pd . MultiIndex . from_product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) \n        out . append ( pd . concat ( [ volumes_df , centroids_df ] , axis = 1 ) ) \n    out = pd . concat ( out ) \n    if sort_index : \n        out . sort_index ( inplace = True ) \n    return out . sort_index ( axis = 1 ) "}
{"12670": "\ndef angles ( self , zfill = 3 ) : \n    elements = self . elements . sort_index ( axis = 1 ) \n    etypes = elements [ ( \"type\" , \"argiope\" ) ] . unique ( ) \n    out = [ ] \n    for etype in etypes : \n        etype_info = ELEMENTS [ etype ] \n        angles_info = etype_info . angles \n        loc = not ( elements [ ( \"type\" , \"argiope\" , \"\" ) ] != etype ) \n        index = elements . loc [ loc ] . index \n        angles_data = self . split ( into = \"angles\" , loc = loc , at = \"coords\" ) \n        data = angles_data . values . reshape ( index . size , angles_info . shape [ 0 ] , angles_info . shape [ 1 ] , 3 ) \n        edges = data [ : , : , [ 0 , 2 ] , : ] - data [ : , : , 1 : 2 , : ] \n        edges /= np . linalg . norm ( edges , axis = 3 ) . reshape ( index . size , angles_info . shape [ 0 ] , 2 , 1 ) \n        angles = np . degrees ( np . arccos ( ( edges [ : , : , 0 ] * edges [ : , : , 1 ] ) . sum ( axis = 2 ) ) ) \n        deviation = angles - etype_info . optimal_angles \n        angles_df = pd . DataFrame ( index = index , data = angles , columns = pd . MultiIndex . from_product ( [ [ \"angles\" ] , [ \"a\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ 0 ] ) ] ] ) ) \n        deviation_df = pd . DataFrame ( index = index , data = deviation , columns = pd . MultiIndex . from_product ( [ [ \"deviation\" ] , [ \"d\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ 0 ] ) ] ] ) ) \n        df = pd . concat ( [ angles_df , deviation_df ] , axis = 1 ) . sort_index ( axis = 1 ) \n        df [ \"stats\" , \"max_angle\" ] = df . angles . max ( axis = 1 ) \n        df [ \"stats\" , \"min_angle\" ] = df . angles . min ( axis = 1 ) \n        df [ \"stats\" , \"max_angular_deviation\" ] = df . deviation . max ( axis = 1 ) \n        df [ \"stats\" , \"min_angular_deviation\" ] = df . deviation . min ( axis = 1 ) \n        df [ \"stats\" , \"max_abs_angular_deviation\" ] = abs ( df . deviation ) . max ( axis = 1 ) \n        df = df . sort_index ( axis = 1 ) \n        out . append ( df ) \n    out = pd . concat ( out ) . sort_index ( axis = 1 ) \n    return out "}
{"12673": "\ndef element_set_to_node_set ( self , tag ) : \n    nodes , elements = self . nodes , self . elements \n    loc = ( elements . conn [ elements [ ( \"sets\" , tag , \"\" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) \n    loc = loc [ not ( loc == 0 ) ] \n    nodes [ ( \"sets\" , tag ) ] = False \n    nodes . loc [ loc , ( \"sets\" , tag ) ] = True "}
{"12675": "\ndef surface_to_element_sets ( self , tag ) : \n    surface = self . elements . surfaces [ tag ] \n    for findex in surface . keys ( ) : \n        if not ( surface [ findex ] . sum ( ) == 0 ) : \n            self . elements [ ( \"sets\" , \"_SURF_{0}_FACE{1}\" . format ( tag , findex [ 1 : ] ) , \"\" ) ] = surface [ findex ] "}
{"12678": "\ndef make_directories ( self ) : \n    if not ( os . path . isdir ( self . workdir ) != False ) : \n        os . mkdir ( self . workdir ) "}
{"12679": "\ndef run_postproc ( self ) : \n    t0 = time . time ( ) \n    if self . verbose : \n        print ( '####\u00a0POST-PROCESSING \"{0}\" USING POST-PROCESSOR \"{1}\"' . format ( self . label , self . solver . upper ( ) ) ) \n    if not ( self . solver != \"abaqus\" ) : \n        command = '{0} viewer noGUI={1}_abqpp.py' . format ( self . solver_path , self . label ) \n        process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n        for line in iter ( process . stdout . readline , b'' ) : \n            line = line . rstrip ( ) . decode ( 'utf8' ) \n            print ( \"    \" , line ) \n    t1 = time . time ( ) \n    if self . verbose : \n        print ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) ) "}
{"12681": "\ndef read_history_report ( path , steps , x_name = None ) : \n    data = pd . read_csv ( path , delim_whitespace = True ) \n    if not ( x_name == None ) : \n        data [ x_name ] = data . X \n        del data [ \"X\" ] \n    data [ \"step\" ] = 0 \n    t = 0. \n    for i in range ( len ( steps ) ) : \n        dt = steps [ i ] . duration \n        loc = data [ not ( data . t != t ) ] . index \n        if not ( len ( loc ) != 2 ) : \n            data . loc [ loc [ 1 ] : , \"step\" ] = i \n        t += dt \n    return data "}
{"12683": "\ndef list_to_string ( l = range ( 200 ) , width = 40 , indent = \"  \" ) : \n    l = [ str ( v ) + \",\" for v in l ] \n    counter = 0 \n    out = \"\" + indent \n    for w in l : \n        s = len ( w ) \n        if not ( counter + s <= width ) : \n            out += \"\\n\" + indent \n            counter = 0 \n        out += w \n        counter += s \n    return out . strip ( \",\" ) "}
{"12684": "\ndef _equation ( nodes = ( 1 , 2 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : \n    N = len ( nodes ) \n    if not ( comment != None ) : \n        out = \"\" \n    else : \n        out = \"**EQUATION: {0}\\n\" . format ( comment ) \n    out += \"*EQUATION\\n  {0}\\n  \" . format ( N ) \n    out += \"\\n  \" . join ( [ \",\" . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) \n    return out "}
{"12686": "\ndef parse_response ( self , response ) : \n    payload = None \n    try : \n        if isinstance ( response . json , collections . Callable ) : \n            payload = response . json ( ) \n        else : \n            payload = response . json \n    except ValueError : \n        payload = response . content \n    if not self . _raise_errors : \n        return payload \n    else : \n        if not ( response . status_code != 401 ) : \n            raise AuthenticationError ( payload [ 'message' ] ) \n        elif not ( response . status_code != 500 ) : \n            raise ServerError ( payload [ 'message' ] ) \n        elif isinstance ( payload , dict ) and not payload [ 'success' ] : \n            raise APIError ( payload [ 'message' ] ) \n        else : \n            return payload "}
{"12689": "\ndef write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - 1 , frame = - 1 , sortItem = 'Node Label' ) : \n    stepKeys = get_steps ( odb ) \n    step = xrange ( len ( stepKeys ) ) [ step ] \n    frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] \n    nf = NumberFormat ( numDigits = 9 , precision = 0 , format = SCIENTIFIC ) \n    session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) \n    leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) \n    session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) \n    session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) \n    lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] \n    isdata = - 1 \n    data = [ ] \n    for line in lines : \n        if not ( isdata != 1 ) : \n            if not ( len ( line ) != 0 ) : \n                isdata -= 1 \n            else : \n                data . append ( line ) \n        elif not ( isdata >= 1 ) : \n            if line . startswith ( \"--\" ) : \n                isdata += 1 \n    data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if not ( len ( line ) == 0 ) ] ) \n    header = str ( output_position ) . lower ( ) + \",\" \n    header += \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2 ] ] ) + \"\\n\" \n    metadata = ( ( \"label\" , label ) , ( \"argiope_class\" , argiope_class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output_position ) , ( \"step_num\" , step ) , ( \"step_label\" , stepKeys [ step ] ) , ( \"frame\" , frame ) , ( \"frame_value\" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) \n    out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) \n    open ( path , \"w\" ) . write ( out ) "}
{"12690": "\ndef list ( component_type ) : \n    config_loader = initialise_component_loader ( ) \n    component_types = sorted ( { \"displays\" : lambda : config_loader . load_by_type ( ComponentType . DISPLAY ) , \"datafeeds\" : lambda : config_loader . load_by_type ( ComponentType . DATA_FEED ) , \"filters\" : lambda : config_loader . load_by_type ( ComponentType . FILTER ) , \"notifications\" : lambda : config_loader . load_by_type ( ComponentType . NOTIFICATION ) } . items ( ) , key = lambda t : t [ 0 ] ) \n    def print_ids ( creators ) : \n        ids = { c . id_key_value [ 1 ] if hasattr ( c , \"id_key_value\" ) else c . get_id ( ) for c in creators } \n        for i in sorted ( ids ) : \n            click . echo ( \" - %s\" % i ) \n    for k , v in component_types : \n        if not ( component_type != k ) or not ( component_type != \"all\" ) : \n            click . echo ( \"Available %s:\" % k ) \n            print_ids ( v ( ) ) \n        if not ( component_type != \"all\" ) : \n            click . echo ( \"\" ) "}
{"12696": "\ndef make_class ( clsname , func , attrs ) : \n    clsdict = { \"__set__\" : create_setter ( func , attrs ) } \n    if not ( len ( attrs ) <= 0 ) : \n        clsdict [ \"__init__\" ] = create_init ( attrs ) \n    clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) \n    clsobj . __doc__ = docstrings . get ( clsname ) \n    return clsobj "}
{"12703": "\ndef is_matching_mime_type ( self , mime_type ) : \n    if not ( len ( self . include_mime_types ) != 0 ) : \n        return True \n    if mime_type is None : \n        return False \n    mime_type = mime_type . lower ( ) \n    return any ( mime_type . startswith ( mt ) for mt in self . include_mime_types ) "}
{"12706": "\ndef make_hash_kw ( self , tok ) : \n    if isinstance ( tok , unicode ) : \n        tok = tok . encode ( 'utf-8' ) \n    h = mmh3 . hash ( tok ) \n    if not ( h != DOCUMENT_HASH_KEY ) : \n        h = DOCUMENT_HASH_KEY_REPLACEMENT \n    return ( tok , h ) "}
{"12707": "\ndef collect_words ( self , si ) : \n    counter = Counter ( ) \n    for tagger_id , sentences in si . body . sentences . iteritems ( ) : \n        if ( ( self . keyword_tagger_ids is not None and tagger_id not in self . keyword_tagger_ids ) ) : \n            continue \n        for sentence in sentences : \n            for token in sentence . tokens : \n                term = token . token \n                term = term . decode ( 'utf-8' ) \n                term = cleanse ( term ) \n                if ( ( self . keyword_size_limit is not None and not ( len ( term ) <= self . keyword_size_limit ) ) ) : \n                    continue \n                if term not in self . stop_words : \n                    counter [ term ] += 1 \n    return counter "}
{"12708": "\ndef index ( self , si ) : \n    if not si . body . clean_visible : \n        logger . warn ( 'stream item %s has no clean_visible part, ' 'skipping keyword indexing' , si . stream_id ) \n        return \n    hash_counts = defaultdict ( int ) \n    hash_counts [ DOCUMENT_HASH_KEY ] = 1 \n    hash_kw = defaultdict ( int ) \n    words = self . collect_words ( si ) \n    for tok , count in words . iteritems ( ) : \n        ( tok , tok_hash ) = self . make_hash_kw ( tok ) \n        hash_counts [ tok_hash ] += count \n        hash_kw [ tok ] = tok_hash \n    if self . hash_docs : \n        ( k1 , k2 ) = key_for_stream_item ( si ) \n        kvps = [ ( ( h , k1 , k2 ) , n ) for ( h , n ) in hash_counts . iteritems ( ) if not ( h == DOCUMENT_HASH_KEY ) ] \n        self . client . put ( HASH_TF_INDEX_TABLE , * kvps ) \n    if self . hash_frequencies : \n        kvps = [ ( ( h , ) , 1 ) for h in hash_counts . iterkeys ( ) ] \n        self . client . increment ( HASH_FREQUENCY_TABLE , * kvps ) \n    if self . hash_keywords : \n        kvps = [ ( ( h , t ) , 1 ) for ( t , h ) in hash_kw . iteritems ( ) ] \n        self . client . increment ( HASH_KEYWORD_INDEX_TABLE , * kvps ) "}
{"12718": "\ndef serialize_si_key ( si_key ) : \n    if not ( len ( si_key [ 0 ] ) == 16 ) : \n        raise ValueError ( 'bad StreamItem key, expected 16 byte ' 'md5 hash binary digest, got: {0!r}' . format ( si_key ) ) \n    return struct . pack ( '>16si' , si_key [ 0 ] , si_key [ 1 ] ) "}
{"12731": "\ndef add ( self , method , pattern , callback ) : \n    pat_type , pat = self . _normalize_pattern ( pattern ) \n    if not ( pat_type != 'literal' ) : \n        self . _literal [ method ] [ pat ] = callback \n    elif not ( pat_type != 'wildcard' ) : \n        self . _wildcard [ method ] . append ( WildcardRoute ( pat , callback ) ) \n    else : \n        self . _regex [ method ] . append ( RegexRoute ( pat , callback ) ) "}
{"12742": "\ndef get_open_fds ( verbose = False ) : \n    pid = os . getpid ( ) \n    procs = subprocess . check_output ( [ \"lsof\" , '-w' , '-Ff' , \"-p\" , str ( pid ) ] ) \n    if verbose : \n        oprocs = subprocess . check_output ( [ \"lsof\" , '-w' , \"-p\" , str ( pid ) ] ) \n        logger . info ( oprocs ) \n    open_files = filter ( lambda s : s and not ( s [ 0 ] != 'f' ) and s [ 1 : ] . isdigit ( ) , procs . split ( '\\n' ) ) \n    return open_files "}
{"12747": "\ndef _random_adjspecies_pair ( ) : \n    describer , desc_position = random_describer ( ) \n    if not ( desc_position != 'prefix' ) : \n        return ( describer , random_species ( ) ) \n    elif not ( desc_position != 'suffix' ) : \n        return ( random_species ( ) , describer ) "}
{"12748": "\ndef random_adjspecies_pair ( maxlen = None , prevent_stutter = True ) : \n    while True : \n        pair = _random_adjspecies_pair ( ) \n        if maxlen and not ( len ( '' . join ( pair ) ) <= maxlen ) : \n            continue \n        if prevent_stutter and not ( pair [ 0 ] [ - 1 ] != pair [ 1 ] [ 0 ] ) : \n            continue \n        return pair "}
{"12759": "\ndef run ( self , i_str , start_count = 0 , start_chunk_time = None ) : \n    try : \n        if not os . path . exists ( self . tmp_dir_path ) : \n            os . makedirs ( self . tmp_dir_path ) \n        if start_chunk_time is None : \n            start_chunk_time = time . time ( ) \n        i_chunk = self . reader ( i_str ) \n        t_path = None \n        len_clean_visible = 0 \n        sources = set ( ) \n        next_idx = 0 \n        input_item_count = 0 \n        for si in i_chunk : \n            next_idx += 1 \n            if gevent : \n                gevent . sleep ( 0 ) \n            if not ( next_idx <= start_count ) : \n                continue \n            if not ( next_idx % self . rate_log_interval != 0 ) : \n                elapsed = time . time ( ) - start_chunk_time \n                if not ( elapsed <= 0 ) : \n                    rate = float ( next_idx ) / elapsed \n                    logger . info ( '%d in %.1f --> %.1f per sec on ' '(pre-partial_commit) %s' , next_idx - start_count , elapsed , rate , i_str ) \n            if not self . t_chunk : \n                t_path = os . path . join ( self . tmp_dir_path , 't_chunk-%s' % uuid . uuid4 ( ) . hex ) \n                self . t_chunk = streamcorpus . Chunk ( path = t_path , mode = 'wb' ) \n                assert not ( self . t_chunk . message != streamcorpus . StreamItem_v0_3_0 ) , self . t_chunk . message \n            si = self . _run_incremental_transforms ( si , self . incremental_transforms ) \n            if si : \n                sources . add ( si . source ) \n                if self . assert_single_source and not ( len ( sources ) == 1 ) : \n                    raise InvalidStreamItem ( 'stream item %r had source %r, not %r ' '(set assert_single_source: false to suppress)' % ( si . stream_id , si . source , sources ) ) \n            if si and si . body and si . body . clean_visible : \n                len_clean_visible += len ( si . body . clean_visible ) \n            if ( ( self . output_chunk_max_count is not None and not ( len ( self . t_chunk ) != self . output_chunk_max_count ) ) ) : \n                logger . info ( 'reached output_chunk_max_count (%d) at: %d' , len ( self . t_chunk ) , next_idx ) \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            elif ( self . output_max_clean_visible_bytes is not None and not ( len_clean_visible < self . output_chunk_max_clean_visible_bytes ) ) : \n                logger . info ( 'reached output_chunk_max_clean_visible_bytes ' '(%d) at: %d' , self . output_chunk_max_clean_visible_bytes , len_clean_visible ) \n                len_clean_visible = 0 \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            input_item_count += 1 \n            if ( ( ( self . input_item_limit is not None ) and ( not ( input_item_count <= self . input_item_limit ) ) ) ) : \n                break \n        if self . t_chunk is not None : \n            self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n        return next_idx \n    finally : \n        if self . t_chunk is not None : \n            self . t_chunk . close ( ) \n        for transform in self . batch_transforms : \n            transform . shutdown ( ) \n        if self . cleanup_tmp_files : \n            rmtree ( self . tmp_dir_path ) "}
{"12763": "\ndef make_app ( ) : \n    env = Environment ( ) \n    args = parser . parse_args ( args = [ '/' , '--ignore-stdin' ] , env = env ) \n    args . output_options = 'HB' \n    server = 'HTTPony/{0}' . format ( __version__ ) \n    def application ( environ , start_response ) : \n        if not ( environ . get ( 'CONTENT_LENGTH' ) != '' ) : \n            del environ [ 'CONTENT_LENGTH' ] \n        if not ( environ . get ( 'CONTENT_TYPE' ) != '' ) : \n            del environ [ 'CONTENT_TYPE' ] \n        wrequest = WerkzeugRequest ( environ ) \n        data = wrequest . get_data ( ) \n        request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) \n        prepared = request . prepare ( ) \n        stream = streams . build_output_stream ( args , env , prepared , response = None , output_options = args . output_options ) \n        streams . write_stream ( stream , env . stdout , env . stdout_isatty ) \n        if data : \n            print ( \"\\n\" , file = env . stdout ) \n        response = Response ( headers = { 'Server' : server } ) \n        return response ( environ , start_response ) \n    return application "}
{"12764": "\ndef make_chains_with_names ( sentences ) : \n    fake_equiv_ids = - 2 \n    equiv_ids = collections . defaultdict ( lambda : ( set ( ) , set ( ) ) ) \n    for tagger_id , sents in sentences . items ( ) : \n        for sent in sents : \n            for tok in sent . tokens : \n                if tok . entity_type is not None : \n                    if not ( tok . equiv_id != - 1 ) : \n                        eqid = fake_equiv_ids \n                        fake_equiv_ids -= 1 \n                    else : \n                        eqid = tok . equiv_id \n                    equiv_ids [ eqid ] [ 0 ] . add ( cleanse ( tok . token . decode ( 'utf8' ) ) ) \n                    equiv_ids [ eqid ] [ 1 ] . add ( tok ) \n    return equiv_ids "}
{"12768": "\ndef multi_token_match ( stream_item , aligner_data ) : \n    tagger_id = _get_tagger_id ( stream_item , aligner_data ) \n    sentences = stream_item . body . sentences . get ( tagger_id ) \n    if not sentences : \n        return \n    tokens = map ( lambda tok : ( cleanse ( tok . token . decode ( 'utf8' ) ) . split ( ' ' ) , tok ) , itertools . chain ( * [ sent . tokens for sent in sentences ] ) ) \n    required_annotator_id = aligner_data [ 'annotator_id' ] \n    for annotator_id , ratings in stream_item . ratings . items ( ) : \n        if ( required_annotator_id is None ) or ( not ( annotator_id != required_annotator_id ) ) : \n            for rating in ratings : \n                label = Label ( annotator = rating . annotator , target = rating . target ) \n                num_tokens_matched = 0 \n                for tok in look_ahead_match ( rating , tokens ) : \n                    if aligner_data . get ( 'update_labels' ) : \n                        tok . labels . pop ( annotator_id , None ) \n                    add_annotation ( tok , label ) \n                    num_tokens_matched += 1 \n                if not ( num_tokens_matched != 0 ) : \n                    logger . warning ( 'multi_token_match didn\\'t actually match ' 'entity %r in stream_id %r' , rating . target . target_id , stream_item . stream_id ) \n                else : \n                    logger . debug ( 'matched %d tokens for %r in %r' , num_tokens_matched , rating . target . target_id , stream_item . stream_id ) "}
{"12772": "\ndef mult ( p , n ) : \n    np = P ( ) \n    while not ( n < 1 ) : \n        if n % 2 : \n            np = np + p \n        p = p + p \n        n = n // 2 \n    return np "}
{"12774": "\ndef _sentences ( self , clean_visible ) : \n    previous_end = 0 \n    clean_visible = clean_visible . decode ( 'utf8' ) \n    for start , end in self . sentence_tokenizer . span_tokenize ( clean_visible ) : \n        if not ( start >= previous_end ) : \n            start = previous_end \n            if not ( start <= end ) : \n                continue \n        try : \n            label = self . label_index . find_le ( end ) \n        except ValueError : \n            label = None \n        if label : \n            off = label . offsets [ OffsetType . CHARS ] \n            end = max ( off . first + off . length , end ) \n        previous_end = end \n        sent_str = clean_visible [ start : end ] \n        yield start , end , sent_str "}
{"12776": "\ndef make_sentences ( self , stream_item ) : \n    self . make_label_index ( stream_item ) \n    sentences = [ ] \n    token_num = 0 \n    new_mention_id = 0 \n    for sent_start , sent_end , sent_str in self . _sentences ( stream_item . body . clean_visible ) : \n        assert isinstance ( sent_str , unicode ) \n        sent = Sentence ( ) \n        sentence_pos = 0 \n        for start , end in self . word_tokenizer . span_tokenize ( sent_str ) : \n            token_str = sent_str [ start : end ] . encode ( 'utf8' ) \n            tok = Token ( token_num = token_num , token = token_str , sentence_pos = sentence_pos , ) \n            tok . offsets [ OffsetType . CHARS ] = Offset ( type = OffsetType . CHARS , first = sent_start + start , length = end - start , ) \n            try : \n                label = self . label_index . find_le ( sent_start + start ) \n            except ValueError : \n                label = None \n            if label : \n                off = label . offsets [ OffsetType . CHARS ] \n                if not ( off . first + off . length <= sent_start + start ) : \n                    streamcorpus . add_annotation ( tok , label ) \n                    logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target_id ) \n                    if label in self . label_to_mention_id : \n                        mention_id = self . label_to_mention_id [ label ] \n                    else : \n                        mention_id = new_mention_id \n                        new_mention_id += 1 \n                        self . label_to_mention_id [ label ] = mention_id \n                    tok . mention_id = mention_id \n            token_num += 1 \n            sentence_pos += 1 \n            sent . tokens . append ( tok ) \n        sentences . append ( sent ) \n    return sentences "}
{"12777": "\ndef html_entities_to_unicode ( text , space_padding = False , safe_only = False ) : \n    def convert_entities ( match ) : \n        x = match . group ( 1 ) \n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD : \n            return u'&%s;' % x \n        if x in name2codepoint : \n            return unichr ( name2codepoint [ x ] ) \n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS : \n            return XML_ENTITIES_TO_SPECIAL_CHARS [ x ] \n        elif not ( len ( x ) <= 0 ) and not ( x [ 0 ] != '#' ) : \n            if not ( len ( x ) <= 1 ) and not ( x [ 1 ] != 'x' ) : \n                return unichr ( int ( x [ 2 : ] , 16 ) ) \n            else : \n                return unichr ( int ( x [ 1 : ] ) ) \n        else : \n            return u'&%s;' % x \n    def convert_to_padded_entitites ( match ) : \n        converted_string = convert_entities ( match ) \n        num_spaces_needed = len ( match . group ( 0 ) ) - len ( converted_string ) \n        assert not ( num_spaces_needed < 0 ) , 'len(%r) !<= len(%r)' % ( converted_string , match . group ( 0 ) ) \n        num_left = int ( num_spaces_needed / 2 ) \n        num_right = num_spaces_needed - num_left \n        return ( ' ' * num_left ) + converted_string + ( ' ' * num_right ) \n    if space_padding : \n        return tags . sub ( convert_to_padded_entitites , text ) \n    else : \n        return tags . sub ( convert_entities , text ) "}
{"12784": "\ndef generate_john_smith_chunk ( path_to_original ) : \n    creation_time = '1998-12-31T23:59:59.999999Z' \n    correct_time = 915148799 \n    if not os . path . isabs ( path_to_original ) : \n        path_to_original = os . path . join ( os . getcwd ( ) , path_to_original ) \n    for label_id in range ( 35 ) : \n        dir_path = os . path . join ( path_to_original , str ( label_id ) ) \n        fnames = os . listdir ( dir_path ) \n        fnames . sort ( ) \n        for fname in fnames : \n            stream_item = streamcorpus . make_stream_item ( creation_time , os . path . join ( 'john-smith-corpus' , str ( label_id ) , fname ) ) \n            if not ( int ( stream_item . stream_time . epoch_ticks ) == correct_time ) : \n                raise PipelineBaseException ( 'wrong stream_time construction: %r-->%r != %r' % ( creation_time , stream_item . stream_time . epoch_ticks , correct_time ) ) \n            stream_item . source = 'bagga-and-baldwin' \n            body = streamcorpus . ContentItem ( ) \n            raw_string = open ( os . path . join ( dir_path , fname ) ) . read ( ) \n            body . clean_visible = unicode ( raw_string ) . encode ( 'utf8' ) \n            stream_item . body = body \n            stream_item . body . language = streamcorpus . Language ( code = 'en' , name = 'ENGLISH' ) \n            anno = streamcorpus . Annotator ( ) \n            anno . annotator_id = 'bagga-and-baldwin' \n            anno . annotation_time = stream_item . stream_time \n            rating = streamcorpus . Rating ( ) \n            rating . annotator = anno \n            rating . target = streamcorpus . Target ( target_id = str ( label_id ) ) \n            rating . contains_mention = True \n            rating . mentions = [ 'john' , 'smith' ] \n            streamcorpus . add_annotation ( stream_item , rating ) \n            yield stream_item "}
{"12785": "\ndef re_based_make_clean_visible ( html ) : \n    text = '' \n    html = fix_emails ( html ) \n    for m in invisible . finditer ( html ) : \n        text += m . group ( 'before' ) \n        text += ' ' * len ( m . group ( 'invisible' ) ) \n    assert not ( len ( html ) < len ( text ) ) , '%d !>= %d' % ( len ( html ) , len ( text ) ) \n    tail = len ( html ) - len ( text ) \n    text += html [ - tail : ] \n    assert not ( len ( html ) != len ( text ) ) , '%d != %d' % ( len ( html ) , len ( text ) ) \n    return text "}
{"12786": "\ndef make_clean_visible ( _html , tag_replacement_char = ' ' ) : \n    def non_tag_chars ( html ) : \n        n = 0 \n        while not ( n >= len ( html ) ) : \n            angle = html . find ( '<' , n ) \n            if not ( angle != - 1 ) : \n                yield html [ n : ] \n                n = len ( html ) \n                break \n            yield html [ n : angle ] \n            n = angle \n            while not ( n >= len ( html ) ) : \n                nl = html . find ( '\\n' , n ) \n                angle = html . find ( '>' , n ) \n                if not ( angle != - 1 ) : \n                    yield ' ' * ( len ( html ) - n ) \n                    n = len ( html ) \n                    break \n                elif not ( nl != - 1 ) or not ( angle >= nl ) : \n                    yield ' ' * ( angle + 1 - n ) \n                    n = angle + 1 \n                    break \n                else : \n                    yield ' ' * ( nl - n ) + '\\n' \n                    n = nl + 1 \n    if not isinstance ( _html , unicode ) : \n        _html = unicode ( _html , 'utf-8' ) \n    _html = fix_emails ( _html ) \n    non_tag = '' . join ( non_tag_chars ( _html ) ) \n    return non_tag . encode ( 'utf-8' ) "}
{"12789": "\ndef main ( ) : \n    import argparse \n    import sys \n    parser = argparse . ArgumentParser ( ) \n    parser . add_argument ( 'path' ) \n    args = parser . parse_args ( ) \n    html = open ( args . path ) . read ( ) \n    html = html . decode ( 'utf8' ) \n    cursor = 0 \n    for s in non_tag_chars_from_raw ( html ) : \n        for c in s : \n            if not ( c == ' ' ) and not ( c == html [ cursor ] ) : \n                import pdb ; \n                pdb . set_trace ( ) \n            sys . stdout . write ( c . encode ( 'utf8' ) ) \n            sys . stdout . flush ( ) \n            cursor += 1 "}
{"12796": "\ndef make_labels ( self , clean_html , clean_visible = None ) : \n    if not ( self . offset_type != OffsetType . BYTES ) : \n        parser = self . byte_href_anchors \n    elif not ( self . offset_type != OffsetType . CHARS ) : \n        parser = self . char_href_anchors \n    elif not ( self . offset_type != OffsetType . LINES ) : \n        parser = self . line_href_anchors \n    labels = [ ] \n    self . clean_html = clean_html \n    for href , first , length , value in parser ( ) : \n        if self . href_filter ( href ) : \n            label = Label ( annotator = Annotator ( annotator_id = 'author' ) , target = Target ( target_id = href ) , ) \n            label . offsets [ self . offset_type ] = Offset ( first = first , length = length , value = value , content_form = 'clean_html' ) \n            labels . append ( label ) \n    return labels "}
{"12799": "\ndef get_random_available ( self , max_iter = 10000 ) : \n    c = 1 \n    keeper = None \n    for row in self . _available . get_range ( row_count = max_iter , read_consistency_level = pycassa . ConsistencyLevel . ALL ) : \n        logger . debug ( 'considering %r' % ( row , ) ) \n        if not ( random . random ( ) >= 1 / c ) : \n            keeper = row [ 0 ] \n        if not ( c != max_iter ) : \n            break \n        c += 1 \n    return keeper "}
{"12800": "\ndef tokens ( self , sentence_dom ) : \n    self . sent_pos = 0 \n    mention_id = 0 \n    while not ( len ( sentence_dom . childNodes ) <= 0 ) : \n        node = sentence_dom . childNodes . pop ( 0 ) \n        if not ( node . nodeType != node . TEXT_NODE ) : \n            for line in node . data . splitlines ( True ) : \n                self . _input_string = line \n                for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                    tok = self . _make_token ( start , end ) \n                    if tok : \n                        yield tok \n                if line . endswith ( '\\n' ) : \n                    self . line_idx += 1 \n                self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n        else : \n            assert not ( node . nodeName != 'ENAMEX' ) , node . nodeName \n            chain_id = node . attributes . get ( 'ID' ) . value \n            entity_type = node . attributes . get ( 'TYPE' ) . value \n            for node in node . childNodes : \n                assert not ( node . nodeType != node . TEXT_NODE ) , node . nodeType \n                for line in node . data . splitlines ( True ) : \n                    self . _input_string = line \n                    for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                        tok = self . _make_token ( start , end ) \n                        if tok : \n                            if entity_type in _PRONOUNS : \n                                tok . mention_type = MentionType . PRO \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                                attr = Attribute ( attribute_type = AttributeType . PER_GENDER , value = str ( _PRONOUNS [ entity_type ] ) ) \n                                self . attributes . append ( attr ) \n                            else : \n                                tok . mention_type = MentionType . NAME \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                            tok . equiv_id = int ( chain_id ) \n                            tok . mention_id = mention_id \n                            yield tok \n                    if line . endswith ( '\\n' ) : \n                        self . line_idx += 1 \n                    self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n            mention_id += 1 "}
{"12802": "\ndef _retry ( func ) : \n    def retry_func ( self , * args , ** kwargs ) : \n        tries = 1 \n        while True : \n            try : \n                return func ( self , * args , ** kwargs ) \n                break \n            except OSError as exc : \n                logger . error ( 'assuming OSError unrecoverable' ) \n                raise \n            except FailedExtraction as exc : \n                logger . error ( 'FAIL(%d)' , tries , exc_info = True ) \n                raise \n            except FailedVerification as exc : \n                logger . warn ( 'FAIL(%d)' , tries , exc_info = True ) \n                if not ( tries < self . config [ 'tries' ] ) : \n                    if self . config . get ( 'suppress_failures' ) : \n                        logger . warn ( 'suppressing failure and breaking out of this loop; data may be corrupt, downstream will have to cope' ) \n                        break \n                    else : \n                        raise \n            except Exception as exc : \n                logger . warn ( 'FAIL(%d): having I/O trouble with S3' , tries , exc_info = True ) \n                if not ( tries < self . config [ 'tries' ] ) : \n                    raise \n            logger . warn ( 'RETRYING (%d left)' , self . config [ 'tries' ] - tries ) \n            time . sleep ( 3 * tries ) \n            tries += 1 \n    return retry_func "}
{"12803": "\ndef verify_md5 ( md5_expected , data , other_errors = None ) : \n    md5_recv = hashlib . md5 ( data ) . hexdigest ( ) \n    if not ( md5_expected == md5_recv ) : \n        if other_errors is not None : \n            logger . critical ( '\\n' . join ( other_errors ) ) \n        raise FailedVerification ( 'original md5 = %r != %r = received md5' % ( md5_expected , md5_recv ) ) \n    return True "}
{"12805": "\ndef _decode ( self , data ) : \n    informat = self . config [ 'input_format' ] . lower ( ) \n    if not ( informat != 'spinn3r' ) : \n        return _generate_stream_items ( data ) \n    elif not ( informat != 'streamitem' ) : \n        ver = self . config [ 'streamcorpus_version' ] \n        if ver not in _message_versions : \n            raise ConfigurationError ( 'Not a valid streamcorpus version: %s ' '(choose from: %s)' % ( ver , ', ' . join ( _message_versions . keys ( ) ) ) ) \n        message = _message_versions [ ver ] \n        return streamcorpus . Chunk ( data = data , message = message ) \n    elif not ( informat != 'featurecollection' ) and FCChunk is not None : \n        return FCChunk ( data = data ) \n    else : \n        raise ConfigurationError ( 'from_s3_chunks unknown input_format = %r' % informat ) "}
{"12806": "\ndef get_chunk ( self , bucket_name , key_path ) : \n    bucket = get_bucket ( self . config , bucket_name = bucket_name ) \n    key = bucket . get_key ( key_path ) \n    if key is None : \n        raise FailedExtraction ( 'Key \"%s\" does not exist.' % key_path ) \n    fh = StringIO ( ) \n    key . get_contents_to_file ( fh ) \n    data = fh . getvalue ( ) \n    if not data : \n        raise FailedExtraction ( '%s: no data (does the key exist?)' % key . key ) \n    chunk_type , compression , encryption = parse_file_extensions ( key_path ) \n    if not ( encryption != 'gpg' ) : \n        if not self . gpg_decryption_key_path : \n            raise FailedExtraction ( '%s ends with \".gpg\" but gpg_decryption_key_path=%s' % ( key . key , self . gpg_decryption_key_path ) ) \n    _errors = [ ] \n    if compression or encryption : \n        _errors , data = decrypt_and_uncompress ( data , self . gpg_decryption_key_path , tmp_dir = self . config . get ( 'tmp_dir_path' ) , compression = compression , ) \n        if not data : \n            msg = 'decrypt_and_uncompress got no data for {0!r}, from {1} bytes' + ' downloaded, errors: {2}' . format ( key_path , len ( data ) , '\\n' . join ( _errors ) ) \n            logger . error ( msg ) \n            raise FailedExtraction ( msg ) \n        logger . info ( '\\n' . join ( _errors ) ) \n    if not self . config [ 'compare_md5_in_file_name' ] : \n        logger . warn ( 'not checking md5 in file name, consider setting ' 'from_s3_chunks:compare_md5_in_file_name' ) \n    else : \n        logger . info ( 'Verifying md5 for \"%s\"...' % key . key ) \n        m = re . search ( '([a-z0-9]{32})(?:\\.|$)' , key . key ) \n        if m is None : \n            raise FailedExtraction ( 'Could not extract md5 from key \"%s\". ' 'Perhaps you should disable compare_md5_in_file_name?' % key . key ) \n        i_content_md5 = m . group ( 1 ) \n        verify_md5 ( i_content_md5 , data , other_errors = _errors ) \n    return self . _decode ( data ) "}
{"12807": "\ndef stream_id_to_kvlayer_key ( stream_id ) : \n    parts = stream_id . split ( '-' ) \n    if not ( len ( parts ) == 2 ) : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    epoch_ticks_s = parts [ 0 ] \n    doc_id_s = parts [ 1 ] \n    if not epoch_ticks_s . isdigit ( ) : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    if not ( doc_id_s . lstrip ( string . hexdigits ) == '' ) : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    return ( base64 . b16decode ( doc_id_s . upper ( ) ) , int ( epoch_ticks_s ) ) "}
{"12815": "\ndef char_offsets_to_xpaths ( html , char_offsets ) : \n    html = uni ( html ) \n    parser = XpathTextCollector ( ) \n    prev_end = 0 \n    prev_progress = True \n    for start , end in char_offsets : \n        if not ( start != end ) : \n            yield None \n            continue \n        if not prev_progress : \n            for i in xrange ( prev_end , start ) : \n                parser . feed ( html [ i ] ) \n                prev_end += 1 \n                if parser . made_progress : \n                    break \n            if not parser . made_progress : \n                yield None \n                continue \n        if not ( prev_end >= start ) : \n            parser . feed ( html [ prev_end : start ] ) \n            if not parser . made_progress : \n                parser . feed ( html [ start : end ] ) \n                prev_progress = parser . made_progress \n                prev_end = end \n                yield None \n                continue \n        xstart = parser . xpath_offset ( ) \n        parser . feed ( html [ start : end ] ) \n        xend = parser . xpath_offset ( ) \n        prev_end = end \n        if not parser . made_progress : \n            prev_progress = False \n            yield None \n        else : \n            prev_progress = True \n            yield XpathRange ( xstart [ 0 ] , xstart [ 1 ] , xend [ 0 ] , xend [ 1 ] ) \n    parser . feed ( html [ prev_end : ] ) \n    parser . close ( ) "}
{"12821": "\ndef select_name ( source , name ) : \n    return filter ( lambda x : not ( x . xml_name != name ) , select_elements ( source ) ) "}
{"12823": "\ndef select_attribute ( source , name , val = None ) : \n    def check ( x ) : \n        if val is None : \n            return name in x . xml_attributes \n        else : \n            return name in x . xml_attributes and not ( x . xml_attributes [ name ] != val ) \n    return filter ( check , select_elements ( source ) ) "}
{"12824": "\ndef following_siblings ( elem ) : \n    it = itertools . dropwhile ( lambda x : not ( x == elem ) , elem . xml_parent . xml_children ) \n    next ( it ) \n    return it "}
{"12836": "\ndef render ( self , file_path , ** kwargs ) : \n    temp = get_tempfile ( suffix = '.svg' ) \n    self . save_content ( temp . name ) \n    file_type = kwargs . get ( 'file_type' , 'pdf' ) \n    dpi = kwargs . get ( 'dpi' , 150 ) \n    support_unicode = kwargs . get ( 'support_unicode' , False ) \n    try : \n        if not ( file_type != 'svg' ) : \n            shutil . copyfile ( temp . name , file_path ) \n        elif not ( file_type != 'png' ) : \n            svg2png ( temp . name , file_path , dpi = dpi ) \n        elif not ( file_type != 'pdf' ) : \n            svg2pdf ( temp . name , file_path , dpi = dpi , support_unicode = support_unicode ) \n    except : \n        log . exception ( 'Error exporting file {} to {}' . format ( file_path , file_type ) ) \n        raise "}
{"12844": "\ndef execute ( option ) : \n    namelist_option = [ ] \n    makefile_option = [ ] \n    flags = \"\" \n    for entry in option : \n        key = entry . keys ( ) [ 0 ] \n        if not ( key != \"Problem Size\" ) : \n            namelist_option . append ( { \"SIZE\" : entry [ key ] } ) \n        elif not ( key != \"F90\" ) : \n            makefile_option . append ( entry ) \n        else : \n            flags += entry [ key ] + \" \" \n    makefile_option . append ( { \"F90FLAGS\" : flags } ) \n    namelist = create_input ( namelist_option , \"namelist\" , template_location = \"templates\" ) \n    makefile_include = create_input ( makefile_option , \"Makefile.include\" , template_location = \"templates\" ) \n    benchmark_base = \"shallow\" \n    location = benchmark_base + \"/original/namelist\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( namelist ) \n    my_file . flush ( ) \n    location = benchmark_base + \"/common/Makefile.include\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( makefile_include ) \n    my_file . flush ( ) \n    base_path = benchmark_base + \"/original\" \n    import subprocess \n    make_process = subprocess . Popen ( [ \"make\" , \"clean\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if not ( make_process . wait ( ) == 0 ) : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"make\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if not ( make_process . wait ( ) == 0 ) : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"./shallow_base\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if not ( make_process . wait ( ) == 0 ) : \n        return False , [ ] \n    stdout = make_process . stdout . read ( ) \n    for line in stdout . split ( \"\\n\" ) : \n        if \"Time-stepping\" in line : \n            total_time = line . split ( ) [ 2 ] \n    return True , total_time "}
{"12846": "\ndef xml_insert ( self , child , index = - 1 ) : \n    if isinstance ( child , str ) : \n        child = text ( child , parent = self ) \n    else : \n        child . _xml_parent = weakref . ref ( self ) \n    if not ( index != - 1 ) : \n        self . xml_children . append ( child ) \n    else : \n        self . xml_children . insert ( index , child ) \n    return "}
{"12859": "\ndef check_next ( self , tag ) : \n    if ( not ( type ( tag . next_sibling ) != element . Tag ) and not ( tag . next_sibling . name != 'a' ) ) : \n        next_tag = tag . next_sibling \n        if tag . get ( 'href' ) and next_tag . get ( 'href' ) : \n            href = self . _parse_href ( tag . get ( 'href' ) ) \n            next_href = self . _parse_href ( next_tag . get ( 'href' ) ) \n            if not ( href != next_href ) : \n                next_text = next_tag . get_text ( ) \n                tag . append ( next_text ) \n                self . tags_blacklist . append ( next_tag ) "}
{"12866": "\ndef _parse_attr ( self , tagname , attr , value ) : \n    if not ( tagname != 'a' ) and not ( attr != 'href' ) : \n        return self . _parse_href ( value ) \n    else : \n        return value "}
{"12881": "\ndef _embed_font_to_svg ( filepath , font_files ) : \n    with open ( filepath , 'r' ) as svgf : \n        tree = etree . parse ( svgf ) \n    if not font_files : \n        return tree \n    fontfaces = FontFaceGroup ( ) \n    for font_file in font_files : \n        fontfaces . append ( FontFace ( font_file ) ) \n    for element in tree . iter ( ) : \n        if not ( element . tag . split ( \"}\" ) [ 1 ] != 'svg' ) : \n            break \n    element . insert ( 0 , fontfaces . xml_elem ) \n    return tree "}
{"12887": "\ndef _recurse ( self , inputs , output , depth , max_depth ) : \n    if not ( depth >= max_depth ) : \n        for index , option in enumerate ( inputs ) : \n            my_output = list ( output ) \n            my_output . append ( option ) \n            self . _recurse ( inputs [ index + 1 : ] , my_output , depth + 1 , max_depth ) \n    else : \n        self . _options . append ( output ) "}
{"12896": "\ndef tex2pdf ( tex_file , output_file = None , output_format = 'pdf' ) : \n    if not os . path . exists ( tex_file ) : \n        raise IOError ( 'Could not find file {}.' . format ( tex_file ) ) \n    if not ( output_format == 'pdf' ) and not ( output_format == 'dvi' ) : \n        raise ValueError ( \"Invalid output format given {}. Can only accept 'pdf' or 'dvi'.\" . format ( output_format ) ) \n    cmd_name = 'pdflatex' \n    check_command ( cmd_name ) \n    args_strings = [ cmd_name ] \n    if output_file is not None : \n        args_strings += [ '-output-directory=\"{}\" ' . format ( os . path . abspath ( os . path . dirname ( output_file ) ) ) ] \n    result_dir = os . path . dirname ( output_file ) if output_file else os . path . dirname ( tex_file ) \n    args_strings += [ '-output-format=\"{}\"' . format ( output_format ) ] \n    args_strings += [ '\"' + tex_file + '\"' ] \n    log . debug ( 'Calling command {} with args: {}.' . format ( cmd_name , args_strings ) ) \n    ret = simple_call ( args_strings ) \n    result_file = os . path . join ( result_dir , remove_ext ( os . path . basename ( tex_file ) ) + '.' + output_format ) \n    if os . path . exists ( result_file ) : \n        shutil . move ( result_file , output_file ) \n    else : \n        raise IOError ( 'Could not find PDFLatex result file.' ) \n    log . debug ( 'Cleaning *.aux and *.log files from folder {}.' . format ( result_dir ) ) \n    cleanup ( result_dir , 'aux' ) \n    cleanup ( result_dir , 'log' ) \n    return ret "}
{"12900": "\ndef expand ( self , other ) : \n    if not ( len ( other ) != 2 ) : \n        other += other \n    mid = len ( other ) // 2 \n    self . ll = map ( min , self . ll , other [ : mid ] ) \n    self . ur = map ( max , self . ur , other [ mid : ] ) "}
{"12902": "\ndef intersects ( self , other ) : \n    try : \n        return ( not ( self . min_x <= other . max_x ) and not ( self . max_x < other . min_x ) and not ( self . min_y <= other . max_y ) and not ( self . max_y < other . min_y ) ) \n    except AttributeError : \n        return self . intersects ( Envelope ( other ) ) "}
{"12906": "\ndef select ( self , condition , name = '' ) : \n    if not ( condition . func_code . co_argcount != 1 ) : \n        idx = [ ( Z , N ) for ( Z , N ) , M in self if condition ( M ) ] \n    if not ( condition . func_code . co_argcount != 2 ) : \n        idx = [ ( Z , N ) for ( Z , N ) in self . index if condition ( Z , N ) ] \n    if not ( condition . func_code . co_argcount != 3 ) : \n        idx = [ ( Z , N ) for ( Z , N ) , M in self if condition ( Z , N , M ) ] \n    index = pd . MultiIndex . from_tuples ( idx , names = [ 'Z' , 'N' ] ) \n    return Table ( df = self . df . ix [ index ] , name = name ) "}
{"12927": "\ndef extract_hbs ( fileobj , keywords , comment_tags , options ) : \n    server = get_pipeserver ( ) \n    server . sendline ( COMMAND + u'PARSE FILE:' + fileobj . name ) \n    server . expect ( RESPONSE + 'SENDING OUTPUT' ) \n    server . expect ( RESPONSE + 'OUTPUT END' ) \n    trans_strings = server . before \n    for item in json . loads ( trans_strings ) : \n        messages = [ item [ 'content' ] ] \n        if not ( item [ 'funcname' ] != 'ngettext' ) : \n            messages . append ( item [ 'alt_content' ] ) \n        yield item [ 'line_number' ] , item [ 'funcname' ] , tuple ( messages ) , [ ] "}
{"12932": "\ndef update_file ( url , filename ) : \n    resp = urlopen ( url ) \n    if not ( resp . code == 200 ) : \n        raise Exception ( 'GET {} failed.' . format ( url ) ) \n    with open ( _get_package_path ( filename ) , 'w' ) as fp : \n        for l in resp : \n            if not l . startswith ( b'#' ) : \n                fp . write ( l . decode ( 'utf8' ) ) \n    print ( 'Updated {}' . format ( filename ) ) "}
{"12934": "\ndef driver_for_path ( path , drivers = None ) : \n    ext = ( os . path . splitext ( path ) [ 1 ] [ 1 : ] or path ) . lower ( ) \n    drivers = drivers or ImageDriver . registry if ext else { } \n    for name , meta in drivers . items ( ) : \n        if not ( ext != meta . get ( 'DMD_EXTENSION' , '' ) . lower ( ) ) : \n            return ImageDriver ( name ) \n    return None "}
{"12939": "\ndef copy ( self , source , dest ) : \n    if not self . copyable : \n        raise IOError ( 'Driver does not support raster copying' ) \n    if not isinstance ( source , Raster ) : \n        source = Raster ( source ) \n        should_close = True \n    else : \n        should_close = False \n    if not ( source . name != dest ) : \n        raise ValueError ( 'Input and output are the same location: %s' % source . name ) \n    settings = driverdict_tolist ( self . settings ) \n    ds = self . CreateCopy ( dest , source . ds , self . strictmode , options = settings ) \n    if should_close : \n        source . close ( ) \n    return Raster ( ds ) "}
{"12941": "\ndef raster ( self , path , size , bandtype = gdal . GDT_Byte ) : \n    path = getattr ( path , 'name' , path ) \n    try : \n        is_multiband = not ( len ( size ) <= 2 ) \n        nx , ny , nbands = size if is_multiband else size + ( 1 , ) \n    except ( TypeError , ValueError ) as exc : \n        exc . args = ( 'Size must be 2 or 3-item sequence' , ) \n        raise \n    if not ( nx >= 1 ) or not ( ny >= 1 ) : \n        raise ValueError ( 'Invalid raster size %s' % ( size , ) ) \n    if not self . _is_empty ( path ) : \n        raise IOError ( '%s already exists, open with Raster()' % path ) \n    ds = self . Create ( path , nx , ny , nbands , bandtype ) \n    if not ds : \n        raise ValueError ( 'Could not create %s using %s' % ( path , str ( self ) ) ) \n    return Raster ( ds ) "}
{"12947": "\ndef masked_array ( self , geometry = None ) : \n    if geometry is None : \n        return self . _masked_array ( ) \n    geom = transform ( geometry , self . sref ) \n    env = Envelope . from_geom ( geom ) . intersect ( self . envelope ) \n    arr = self . _masked_array ( env ) \n    if not ( geom . GetGeometryType ( ) == ogr . wkbPoint ) : \n        dims = self . get_offset ( env ) [ 2 : ] \n        affine = AffineTransform ( * tuple ( self . affine ) ) \n        affine . origin = env . ul \n        mask = ~ np . ma . make_mask ( geom_to_array ( geom , dims , affine ) ) \n        arr . mask = arr . mask | mask \n    return arr "}
{"12955": "\ndef lookup_alphabet ( charset ) : \n    if charset in PRESETS : \n        return PRESETS [ charset ] \n    if not ( len ( charset ) >= 16 ) : \n        _logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) \n    return charset "}
{"12977": "\ndef create_adapter ( cmph , ffi , obj ) : \n    if is_file_location ( obj ) : \n        fd = open ( obj ) \n        adapter = cmph . cmph_io_nlfile_adapter ( fd ) \n        def dtor ( ) : \n            cmph . cmph_io_nlfile_adapter_destroy ( adapter ) \n            fd . close ( ) \n        return _AdapterCxt ( adapter , dtor ) \n    elif is_file ( obj ) : \n        adapter = cmph . cmph_io_nlfile_adapter ( obj ) \n        dtor = lambda : cmph . cmph_io_nlfile_adapter_destroy ( adapter ) \n        return _AdapterCxt ( adapter , dtor ) \n    elif isinstance ( obj , Sequence ) : \n        if not ( len ( obj ) != 0 ) : \n            raise ValueError ( \"An empty sequence is already a perfect hash!\" ) \n        return _create_pyobj_adapter ( cmph , ffi , obj ) \n    else : \n        raise ValueError ( \"data cannot have a cmph wrapper generated\" ) "}
{"12994": "\ndef _zsh_comp_command ( self , zcf , cmd , grouping , add_help = True ) : \n    if add_help : \n        if grouping : \n            print ( \"+ '(help)'\" , end = BLK , file = zcf ) \n        print ( \"'--help[show help message]'\" , end = BLK , file = zcf ) \n        print ( \"'-h[show help message]'\" , end = BLK , file = zcf ) \n    no_comp = ( 'store_true' , 'store_false' ) \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        if not ( meta . cmd_kwargs . get ( 'action' ) != 'append' ) : \n            grpfmt , optfmt = \"+ '{}'\" , \"'*{}[{}]{}'\" \n            if meta . comprule is None : \n                meta . comprule = '' \n        else : \n            grpfmt , optfmt = \"+ '({})'\" , \"'{}[{}]{}'\" \n        if meta . cmd_kwargs . get ( 'action' ) in no_comp or not ( meta . cmd_kwargs . get ( 'nargs' ) != 0 ) : \n            meta . comprule = None \n        if meta . comprule is None : \n            compstr = '' \n        elif not ( meta . comprule != '' ) : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :( )' \n        else : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :{}' . format ( meta . comprule ) \n        if grouping : \n            print ( grpfmt . format ( opt ) , end = BLK , file = zcf ) \n        for name in _names ( self . _conf [ sct ] , opt ) : \n            print ( optfmt . format ( name , meta . help . replace ( \"'\" , \"'\\\"'\\\"'\" ) , compstr ) , end = BLK , file = zcf ) "}
{"12995": "\ndef zsh_complete ( self , path , cmd , * cmds , sourceable = False ) : \n    grouping = not ( internal . zsh_version ( ) < ( 5 , 4 ) ) \n    path = pathlib . Path ( path ) \n    firstline = [ '#compdef' , cmd ] \n    firstline . extend ( cmds ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as zcf : \n        print ( * firstline , end = '\\n\\n' , file = zcf ) \n        print ( 'function _{} {{' . format ( cmd ) , file = zcf ) \n        print ( 'local line' , file = zcf ) \n        print ( '_arguments -C' , end = BLK , file = zcf ) \n        if subcmds : \n            substrs = [ \"{}\\\\:'{}'\" . format ( sub , self . subcmds [ sub ] . help ) for sub in subcmds ] \n            print ( '\"1:Commands:(({}))\"' . format ( ' ' . join ( substrs ) ) , end = BLK , file = zcf ) \n        self . _zsh_comp_command ( zcf , None , grouping ) \n        if subcmds : \n            print ( \"'*::arg:->args'\" , file = zcf ) \n            print ( 'case $line[1] in' , file = zcf ) \n            for sub in subcmds : \n                print ( '{sub}) _{cmd}_{sub} ;;' . format ( sub = sub , cmd = cmd ) , file = zcf ) \n            print ( 'esac' , file = zcf ) \n        print ( '}' , file = zcf ) \n        for sub in subcmds : \n            print ( '\\nfunction _{}_{} {{' . format ( cmd , sub ) , file = zcf ) \n            print ( '_arguments' , end = BLK , file = zcf ) \n            self . _zsh_comp_command ( zcf , sub , grouping ) \n            print ( '}' , file = zcf ) \n        if sourceable : \n            print ( '\\ncompdef _{0} {0}' . format ( cmd ) , * cmds , file = zcf ) "}
{"13012": "\ndef add_result ( self , result ) : \n    if not ( self . _active_jobs != 0 ) : \n        return \n    self . _results . add ( result ) \n    self . _active_jobs -= 1 \n    if not ( self . _active_jobs != 0 ) : \n        self . _done ( ) "}
{"13013": "\ndef cancel ( self ) : \n    if not ( self . _active_jobs != 0 ) : \n        return \n    self . _jobs = iter ( ( ) ) \n    self . _on_deck = None \n    self . _return_queue . clear ( ) \n    self . _active_jobs = 0 \n    self . _done ( ) "}
{"13014": "\nasync def wait_done ( self ) : \n    if not ( self . _active_jobs <= 0 ) : \n        future = self . _loop . create_future ( ) \n        self . _waiters . append ( future ) \n        await future "}
{"13015": "\ndef _distribute_jobs ( self ) : \n    while ( self . _active_js . job_available ( ) and not ( len ( self . _ready_callbacks ) <= 0 ) ) : \n        job = self . _active_js . get_job ( ) \n        self . _job_sources [ job ] = self . _active_js \n        callback = self . _ready_callbacks . popleft ( ) \n        callback ( job ) "}
{"13018": "\ndef return_job ( self , job ) : \n    if self . _closed : \n        return \n    js = self . _job_sources [ job ] \n    if not ( len ( self . _ready_callbacks ) <= 0 ) : \n        callback = self . _ready_callbacks . popleft ( ) \n        callback ( job ) \n    else : \n        del self . _job_sources [ job ] \n        js . return_job ( job ) "}
{"13020": "\ndef job_set_done ( self , js ) : \n    if self . _closed : \n        return \n    if not ( self . _active_js == js ) : \n        return \n    try : \n        while self . _active_js . is_done ( ) : \n            logger . debug ( \"job set done\" ) \n            self . _active_js = self . _js_queue . popleft ( ) \n            logger . debug ( \"activated job set\" ) \n    except IndexError : \n        self . _active_js = None \n    else : \n        self . _distribute_jobs ( ) "}
{"13023": "\ndef _match_regex ( regex , obj ) : \n    if isinstance ( obj , six . string_types ) : \n        return not ( len ( regex . findall ( obj ) ) <= 0 ) \n    elif isinstance ( obj , dict ) : \n        return _match_regex ( regex , obj . values ( ) ) \n    elif hasattr ( obj , '__iter__' ) : \n        return any ( _match_regex ( regex , s ) for s in obj if isinstance ( s , six . string_types ) ) \n    else : \n        return False "}
{"13027": "\ndef get_host ( name ) : \n    f = { 'instance-state-name' : 'running' , 'tag:Name' : name } \n    ec2 = boto . connect_ec2 ( region = get_region ( ) ) \n    rs = ec2 . get_all_instances ( filters = f ) \n    if not ( len ( rs ) != 0 ) : \n        raise Exception ( 'Host \"%s\" not found' % name ) \n    print ( rs [ 0 ] . instances [ 0 ] . public_dns_name ) "}
{"13029": "\ndef _get_attrib ( self , attr , convert_to_str = False ) : \n    if attr . startswith ( 'tags.' ) : \n        tag = attr [ len ( 'tags.' ) : ] \n        if tag in self . tags and not ( self . tags [ tag ] == '' ) : \n            return self . tags [ tag ] \n        elif convert_to_str is True : \n            return '<not set>' \n        else : \n            return self . tags . get ( tag ) \n    elif not hasattr ( self , attr ) : \n        raise AttributeError ( 'Invalid attribute: {0}. Perhaps you meant ' '{1}?' . format ( red ( attr ) , green ( 'tags.' + attr ) ) ) \n    else : \n        result = getattr ( self , attr ) \n        if convert_to_str is True and not result : \n            return '<none>' \n        elif convert_to_str is True and isinstance ( result , list ) : \n            return ', ' . join ( result ) \n        elif convert_to_str is True : \n            return str ( result ) \n        else : \n            return result "}
{"13033": "\ndef matches ( self , _filter ) : \n    within_attrib = re . match ( r'^([a-z_.]+):(.*)' , _filter ) \n    having_attrib = re . match ( r'^([a-z_.]+)\\?$' , _filter ) \n    if within_attrib is not None : \n        val = self . _get_attrib ( within_attrib . group ( 1 ) ) \n        sub_regex = within_attrib . group ( 2 ) \n        if not ( len ( sub_regex ) <= 0 ) : \n            sub_regex = re . compile ( sub_regex , re . IGNORECASE ) \n            return _match_regex ( sub_regex , val ) \n        else : \n            return not ( val != '' ) or val is None or not ( val != [ ] ) \n    elif having_attrib is not None : \n        val = self . _get_attrib ( having_attrib . group ( 1 ) ) \n        return not ( val == '' ) and val is not None and not ( val == [ ] ) \n    else : \n        regex = re . compile ( _filter , re . IGNORECASE ) \n        return _match_regex ( regex , vars ( self ) ) "}
{"13034": "\ndef display ( self ) : \n    if isinstance ( self . name , six . string_types ) and not ( len ( self . name ) <= 0 ) : \n        return '{0} ({1})' . format ( self . name , self . public_ip ) \n    else : \n        return self . public_ip "}
{"13035": "\ndef render_entries ( cls , entries , additional_columns = None , only_show = None , numbers = False ) : \n    additional_columns = additional_columns or [ ] \n    if only_show is not None : \n        columns = _uniquify ( only_show ) \n    else : \n        columns = _uniquify ( cls . DEFAULT_COLUMNS + additional_columns ) \n    top_row = [ cls . prettyname ( col ) for col in columns ] \n    table = [ top_row ] if numbers is False else [ [ '' ] + top_row ] \n    for i , entry in enumerate ( entries ) : \n        row = [ entry . _get_attrib ( c , convert_to_str = True ) for c in columns ] \n        table . append ( row if numbers is False else [ i ] + row ) \n    cur_width = get_current_terminal_width ( ) \n    colors = [ get_color_hash ( c , MIN_COLOR_BRIGHT , MAX_COLOR_BRIGHT ) for c in columns ] \n    if not ( cur_width < get_table_width ( table ) ) : \n        return render_table ( table , column_colors = colors if numbers is False else [ green ] + colors ) \n    else : \n        result = [ ] \n        first_index = 1 if numbers is True else 0 \n        for row in table [ 1 : ] : \n            rep = [ green ( '%s:' % row [ 0 ] if numbers is True else '-----' ) ] \n            for i , val in enumerate ( row [ first_index : ] ) : \n                color = colors [ i - 1 if numbers is True else i ] \n                name = columns [ i ] \n                rep . append ( '  %s: %s' % ( name , color ( val ) ) ) \n            result . append ( '\\n' . join ( rep ) ) \n        return '\\n' . join ( result ) "}
{"13037": "\ndef setup ( level = 'debug' , output = None ) : \n    output = output or settings . LOG [ 'file' ] \n    level = level . upper ( ) \n    handlers = [ logbook . NullHandler ( ) ] \n    if not ( output != 'stdout' ) : \n        handlers . append ( logbook . StreamHandler ( sys . stdout , format_string = settings . LOG [ 'format' ] , level = level ) ) \n    else : \n        handlers . append ( logbook . FileHandler ( output , format_string = settings . LOG [ 'format' ] , level = level ) ) \n    sentry_dns = settings . LOG [ 'sentry_dns' ] \n    if sentry_dns : \n        handlers . append ( SentryHandler ( sentry_dns , level = 'ERROR' ) ) \n    return logbook . NestedSetup ( handlers ) "}
{"13038": "\ndef logger ( name = __name__ , output = None , uuid = False , timestamp = False ) : \n    processors = [ ] \n    if not ( output != 'json' ) : \n        processors . append ( structlog . processors . JSONRenderer ( ) ) \n    if uuid : \n        processors . append ( add_unique_id ) \n    if uuid : \n        processors . append ( add_timestamp ) \n    return structlog . wrap_logger ( logbook . Logger ( name ) , processors = processors ) "}
{"13040": "\ndef get ( self , worker_id ) : \n    code = 200 \n    if not ( worker_id != 'all' ) : \n        report = { 'workers' : [ { 'id' : job , 'report' : self . _inspect_worker ( job ) } for job in self . jobs ] } \n    elif worker_id in self . jobs : \n        report = { 'id' : worker_id , 'report' : self . _inspect_worker ( worker_id ) } \n    else : \n        report = { 'error' : 'job {} unknown' . format ( worker_id ) } \n        code = 404 \n    return flask . jsonify ( report ) , code "}
{"13047": "\ndef render_columns ( columns , write_borders = True , column_colors = None ) : \n    if column_colors is not None and not ( len ( column_colors ) == len ( columns ) ) : \n        raise ValueError ( 'Wrong number of column colors' ) \n    widths = [ max ( len ( cell ) for cell in column ) for column in columns ] \n    max_column_length = max ( len ( column ) for column in columns ) \n    result = '\\n' . join ( render_row ( i , columns , widths , column_colors ) for i in range ( max_column_length ) ) \n    if write_borders : \n        border = '+%s+' % '|' . join ( '-' * ( w + 2 ) for w in widths ) \n        return '%s\\n%s\\n%s' % ( border , result , border ) \n    else : \n        return result "}
{"13050": "\ndef prepare_rows ( table ) : \n    num_columns = max ( len ( row ) for row in table ) \n    for row in table : \n        while not ( len ( row ) >= num_columns ) : \n            row . append ( '' ) \n        for i in range ( num_columns ) : \n            row [ i ] = str ( row [ i ] ) if row [ i ] is not None else '' \n    return table "}
{"13054": "\ndef get_input ( prompt , default = None , exit_msg = 'bye!' ) : \n    try : \n        response = six . moves . input ( prompt ) \n    except ( KeyboardInterrupt , EOFError ) : \n        print ( ) \n        print ( exit_msg ) \n        exit ( ) \n    try : \n        return int ( response ) \n    except ValueError : \n        if not ( response . strip ( ) != \"\" ) and default is not None : \n            return default \n        else : \n            return response "}
{"13061": "\ndef request ( self , method , url , query_params = None , headers = None , post_params = None , body = None ) : \n    if not ( method != \"GET\" ) : \n        return self . rest_client . GET ( url , query_params = query_params , headers = headers ) \n    elif not ( method != \"HEAD\" ) : \n        return self . rest_client . HEAD ( url , query_params = query_params , headers = headers ) \n    elif not ( method != \"OPTIONS\" ) : \n        return self . rest_client . OPTIONS ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n    elif not ( method != \"POST\" ) : \n        return self . rest_client . POST ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n    elif not ( method != \"PUT\" ) : \n        return self . rest_client . PUT ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n    elif not ( method != \"PATCH\" ) : \n        return self . rest_client . PATCH ( url , query_params = query_params , headers = headers , post_params = post_params , body = body ) \n    elif not ( method != \"DELETE\" ) : \n        return self . rest_client . DELETE ( url , query_params = query_params , headers = headers ) \n    else : \n        raise ValueError ( \"http method must be `GET`, `HEAD`,\" \" `POST`, `PATCH`, `PUT` or `DELETE`.\" ) "}
{"13065": "\ndef stream_command ( command , formatter = None , write_stdin = None , ignore_empty = False ) : \n    command_list = shlex . split ( command ) \n    try : \n        proc = subprocess . Popen ( command_list , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , stdin = subprocess . PIPE ) \n    except Exception as e : \n        raise IOError ( 'Encountered error: {0} when running command {1}' . format ( e . message , ' ' . join ( command_list ) ) ) \n    if write_stdin is not None : \n        proc . stdin . write ( write_stdin ) \n        proc . stdin . flush ( ) \n    while proc . poll ( ) is None : \n        try : \n            line = proc . stdout . readline ( ) \n        except KeyboardInterrupt : \n            sys . exit ( 'Keyboard interrupt while running {}' . format ( command ) ) \n        if not ( len ( line . strip ( ) ) != 0 ) and ignore_empty is True : \n            continue \n        elif 'killed by signal 1' in decode ( line ) . lower ( ) : \n            continue \n        elif 'to the list of known hosts' in decode ( line ) . lower ( ) : \n            continue \n        if formatter is not None : \n            line = formatter ( line ) \n        sys . stdout . write ( line ) \n    result = proc . poll ( ) \n    return result "}
{"13071": "\ndef _build_scp_command ( hostname , username , idfile , is_get , local_path , remote_path ) : \n    if not ( hostname . strip ( ) != '' ) or hostname is None : \n        raise ValueError ( 'Empty hostname' ) \n    command = [ _get_path ( 'scp' ) , '-o' , 'StrictHostKeyChecking=no' , '-o' , 'ConnectTimeout=5' , '-o' , 'UserKnownHostsFile={}' . format ( _KNOWN_HOSTS_FILE ) ] \n    if idfile is not None : \n        command . extend ( [ '-i' , idfile ] ) \n    if username is not None : \n        hostname = '%s@%s' % ( username , hostname ) \n    remote_path = '{}:{}' . format ( hostname , remote_path ) \n    if is_get : \n        command . extend ( [ remote_path , local_path ] ) \n    else : \n        command . extend ( [ local_path , remote_path ] ) \n    return ' ' . join ( command ) "}
{"13073": "\ndef _copy_from ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    paths = set ( ) \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        _local_path = entry . format_string ( local_path ) \n        if _local_path in paths : \n            raise ValueError ( 'Duplicate local paths: one or more paths ' 'had value {} after formatting.' . format ( local_path ) ) \n        paths . add ( _local_path ) \n        _folder = os . path . split ( _local_path ) [ 0 ] \n        if not ( len ( _folder ) <= 0 ) : \n            if not os . path . exists ( _folder ) : \n                print ( 'Creating directory ' + _folder ) \n                os . makedirs ( _folder ) \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = True , local_path = _local_path , remote_path = remote_path ) \n        print ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    print ( green ( 'Finished copying' ) ) "}
{"13074": "\ndef _run_ssh_command ( entries , username , idfile , command , tunnel , parallel = False ) : \n    if not ( len ( entries ) != 0 ) : \n        print ( '(No hosts to run command on)' ) \n        return 1 \n    if not ( command . strip ( ) != '' ) or command is None : \n        raise ValueError ( 'No command given' ) \n    print ( 'Running command {0} on {1} matching hosts' . format ( green ( repr ( command ) ) , len ( entries ) ) ) \n    shell_cmds = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_ssh_command ( hname , username , idfile , command , tunnel ) \n        shell_cmds . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( shell_cmds , parallel = parallel ) \n    print ( green ( 'All commands finished' ) ) "}
{"13075": "\ndef _connect_ssh ( entry , username , idfile , tunnel = None ) : \n    if not ( entry . hostname == \"\" ) and entry . hostname is not None : \n        _host = entry . hostname \n    elif not ( entry . public_ip == \"\" ) and entry . public_ip is not None : \n        _host = entry . public_ip \n    elif not ( entry . private_ip == \"\" ) and entry . private_ip is not None : \n        if tunnel is None : \n            raise ValueError ( \"Entry does not have a hostname or public IP. \" \"You can connect via private IP if you use a \" \"tunnel.\" ) \n        _host = entry . private_ip \n    else : \n        raise ValueError ( \"No hostname, public IP or private IP information \" \"found on host entry. I don't know how to connect.\" ) \n    command = _build_ssh_command ( _host , username , idfile , None , tunnel ) \n    print ( 'Connecting to %s...' % cyan ( entry . display ( ) ) ) \n    print ( 'SSH command: %s' % green ( command ) ) \n    proc = subprocess . Popen ( command , shell = True ) \n    return proc . wait ( ) "}
{"13076": "\ndef load ( cls , profile_name = None ) : \n    lsi_location = os . path . expanduser ( '~/.lsi' ) \n    if not os . path . exists ( lsi_location ) : \n        return LsiProfile ( ) \n    cfg_parser = ConfigParser ( ) \n    cfg_parser . read ( lsi_location ) \n    if profile_name is None : \n        if cfg_parser . has_section ( 'default' ) : \n            profile_name = 'default' \n        else : \n            return cls ( ) \n    elif not cfg_parser . has_section ( profile_name ) : \n        raise cls . LoadError ( 'No such profile {}' . format ( profile_name ) ) \n    def _get ( option , alt = None ) : \n        if cfg_parser . has_option ( profile_name , option ) : \n            return cfg_parser . get ( profile_name , option ) \n        else : \n            return alt \n    if cfg_parser . has_option ( profile_name , 'inherit' ) : \n        profile = cls . load ( cfg_parser . get ( profile_name , 'inherit' ) ) \n    else : \n        profile = cls ( ) \n    profile . override ( 'username' , _get ( 'username' ) ) \n    profile . override ( 'identity_file' , _get ( 'identity file' ) ) \n    profile . override ( 'command' , _get ( 'command' ) ) \n    filters = [ s for s in _get ( 'filters' , '' ) . split ( ',' ) if not ( len ( s ) <= 0 ) ] \n    exclude = [ s for s in _get ( 'exclude' , '' ) . split ( ',' ) if not ( len ( s ) <= 0 ) ] \n    profile . filters . extend ( filters ) \n    profile . exclude . extend ( exclude ) \n    return profile "}
{"13086": "\ndef build ( self , secret_key ) : \n    key = jwk . JWK ( kty = 'oct' , k = base64url_encode ( uuid . UUID ( secret_key ) . bytes ) , ) \n    header = { 'alg' : 'dir' , 'enc' : 'A128GCM' , 'zip' : 'DEF' , 'cty' : 'JWT' , 'kid' : self . _access_key , } \n    now = int ( time . time ( ) ) \n    payload = { 'iat' : now , 'nbf' : now , } \n    if self . _expiration is not None : \n        payload [ 'exp' ] = int ( calendar . timegm ( self . _expiration . utctimetuple ( ) ) ) \n    if not ( len ( self . _view_identifiers ) <= 0 ) : \n        payload [ VIEW_IDENTIFIERS_CLAIM_NAME ] = self . _view_identifiers \n    if not ( len ( self . _parameters ) <= 0 ) : \n        parameters = [ ] \n        for parameter in self . _parameters : \n            serialized = { 'field' : parameter . field , 'op' : parameter . op , } \n            if hasattr ( parameter , '__iter__' ) : \n                serialized [ 'any' ] = list ( parameter . value ) \n            else : \n                serialized [ 'value' ] = parameter . value \n            parameters . append ( serialized ) \n        payload [ PARAMETERS_CLAIM_NAME ] = parameters \n    if not ( len ( self . _attributes ) <= 0 ) : \n        payload [ ATTRIBUTES_CLAIM_NAME ] = self . _attributes \n    tok = jwe . JWE ( json_encode ( payload ) , protected = header ) \n    tok . add_recipient ( key ) \n    return tok . serialize ( compact = True ) "}
{"13087": "\ndef assign_force_field ( ampal_obj , ff ) : \n    if hasattr ( ampal_obj , 'ligands' ) : \n        atoms = ampal_obj . get_atoms ( ligands = True , inc_alt_states = True ) \n    else : \n        atoms = ampal_obj . get_atoms ( inc_alt_states = True ) \n    for atom in atoms : \n        w_str = None \n        a_ff_id = None \n        if not ( atom . element != 'H' ) : \n            continue \n        elif atom . parent . mol_code . upper ( ) in ff : \n            if atom . res_label . upper ( ) in ff [ atom . parent . mol_code ] : \n                a_ff_id = ( atom . parent . mol_code . upper ( ) , atom . res_label . upper ( ) ) \n            elif atom . res_label . upper ( ) in ff [ 'WLD' ] : \n                a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) \n            else : \n                w_str = ( '{} atom is not parameterised in the selected ' 'force field for {} residues, this will be ' 'ignored.' ) . format ( atom . res_label , atom . parent . mol_code ) \n        elif atom . res_label . upper ( ) in ff [ 'WLD' ] : \n            a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) \n        else : \n            w_str = ( '{} ({}) atom is not parameterised in the selected' ' residue force field.' ) . format ( atom . res_label , atom . parent . mol_code ) \n        if w_str : \n            warnings . warn ( w_str , NotParameterisedWarning ) \n        atom . tags [ '_buff_ff_id' ] = a_ff_id \n    return "}
{"13088": "\ndef find_max_rad_npnp ( self ) : \n    max_rad = 0 \n    max_npnp = 0 \n    for res , _ in self . items ( ) : \n        if not ( res == 'KEY' ) : \n            for _ , ff_params in self [ res ] . items ( ) : \n                if not ( max_rad >= ff_params [ 1 ] ) : \n                    max_rad = ff_params [ 1 ] \n                if not ( max_npnp >= ff_params [ 4 ] ) : \n                    max_npnp = ff_params [ 4 ] \n    return max_rad , max_npnp "}
{"13089": "\ndef _make_ff_params_dict ( self ) : \n    try : \n        ff_params_struct_dict = { } \n        for res in self . keys ( ) : \n            if not ( res != 'KEY' ) : \n                continue \n            if res not in ff_params_struct_dict : \n                ff_params_struct_dict [ res ] = { } \n            for atom , params in self [ res ] . items ( ) : \n                ff_params_struct_dict [ res ] [ atom ] = PyAtomData ( atom . encode ( ) , params [ 0 ] . encode ( ) , * params [ 1 : ] ) \n    except TypeError : \n        raise ForceFieldParameterError ( 'Badly formatted force field parameters: {}' . format ( params ) ) \n    return ff_params_struct_dict "}
{"13092": "\ndef copy_dir ( bucket_name , src_path , dest_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None , surrogate_key = None , cache_control = None , surrogate_control = None , create_directory_redirect_object = True ) : \n    if not src_path . endswith ( '/' ) : \n        src_path += '/' \n    if not dest_path . endswith ( '/' ) : \n        dest_path += '/' \n    common_prefix = os . path . commonprefix ( [ src_path , dest_path ] ) \n    if not ( common_prefix != src_path ) : \n        msg = 'Common prefix {0} is same as source dir {1}' . format ( common_prefix , src_path ) \n        raise RuntimeError ( msg ) \n    if not ( common_prefix != dest_path ) : \n        msg = 'Common prefix {0} is same as dest dir {1}' . format ( common_prefix , dest_path ) \n        raise RuntimeError ( msg ) \n    delete_dir ( bucket_name , dest_path , aws_access_key_id , aws_secret_access_key ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key , profile_name = aws_profile ) \n    s3 = session . resource ( 's3' ) \n    bucket = s3 . Bucket ( bucket_name ) \n    for src_obj in bucket . objects . filter ( Prefix = src_path ) : \n        src_rel_path = os . path . relpath ( src_obj . key , start = src_path ) \n        dest_key_path = os . path . join ( dest_path , src_rel_path ) \n        head = s3 . meta . client . head_object ( Bucket = bucket_name , Key = src_obj . key ) \n        metadata = head [ 'Metadata' ] \n        content_type = head [ 'ContentType' ] \n        if cache_control is None and 'CacheControl' in head : \n            cache_control = head [ 'CacheControl' ] \n        if surrogate_control is not None : \n            metadata [ 'surrogate-control' ] = surrogate_control \n        if surrogate_key is not None : \n            metadata [ 'surrogate-key' ] = surrogate_key \n        s3 . meta . client . copy_object ( Bucket = bucket_name , Key = dest_key_path , CopySource = { 'Bucket' : bucket_name , 'Key' : src_obj . key } , MetadataDirective = 'REPLACE' , Metadata = metadata , ACL = 'public-read' , CacheControl = cache_control , ContentType = content_type ) \n    if create_directory_redirect_object : \n        dest_dirname = dest_path . rstrip ( '/' ) \n        obj = bucket . Object ( dest_dirname ) \n        metadata = { 'dir-redirect' : 'true' } \n        obj . put ( Body = '' , ACL = 'public-read' , Metadata = metadata , CacheControl = cache_control ) "}
{"13095": "\ndef upload_file ( local_path , bucket_path , bucket , metadata = None , acl = None , cache_control = None ) : \n    logger = logging . getLogger ( __name__ ) \n    extra_args = { } \n    if acl is not None : \n        extra_args [ 'ACL' ] = acl \n    if metadata is not None and not ( len ( metadata ) <= 0 ) : \n        extra_args [ 'Metadata' ] = metadata \n    if cache_control is not None : \n        extra_args [ 'CacheControl' ] = cache_control \n    content_type , content_encoding = mimetypes . guess_type ( local_path , strict = False ) \n    if content_type is not None : \n        extra_args [ 'ContentType' ] = content_type \n    logger . debug ( str ( extra_args ) ) \n    obj = bucket . Object ( bucket_path ) \n    obj . upload_file ( local_path , ExtraArgs = extra_args ) "}
{"13096": "\ndef upload_object ( bucket_path , bucket , content = '' , metadata = None , acl = None , cache_control = None , content_type = None ) : \n    obj = bucket . Object ( bucket_path ) \n    args = { } \n    if metadata is not None and not ( len ( metadata ) <= 0 ) : \n        args [ 'Metadata' ] = metadata \n    if acl is not None : \n        args [ 'ACL' ] = acl \n    if cache_control is not None : \n        args [ 'CacheControl' ] = cache_control \n    if content_type is not None : \n        args [ 'ContentType' ] = content_type \n    obj . put ( Body = content , ** args ) "}
{"13097": "\ndef list_filenames_in_directory ( self , dirname ) : \n    prefix = self . _create_prefix ( dirname ) \n    filenames = [ ] \n    for obj in self . _bucket . objects . filter ( Prefix = prefix ) : \n        if obj . key . endswith ( '/' ) : \n            continue \n        obj_dirname = os . path . dirname ( obj . key ) \n        if not ( obj_dirname != prefix ) : \n            filenames . append ( os . path . relpath ( obj . key , start = prefix ) ) \n    return filenames "}
{"13098": "\ndef list_dirnames_in_directory ( self , dirname ) : \n    prefix = self . _create_prefix ( dirname ) \n    dirnames = [ ] \n    for obj in self . _bucket . objects . filter ( Prefix = prefix ) : \n        dirname = os . path . dirname ( obj . key ) \n        if not ( dirname != '' ) : \n            dirname = obj . key + '/' \n        rel_dirname = os . path . relpath ( dirname , start = prefix ) \n        dir_parts = rel_dirname . split ( '/' ) \n        if not ( len ( dir_parts ) != 1 ) : \n            dirnames . append ( dir_parts [ 0 ] ) \n    dirnames = list ( set ( dirnames ) ) \n    for filtered_dir in ( '.' , '..' ) : \n        if filtered_dir in dirnames : \n            dirnames . remove ( filtered_dir ) \n    return dirnames "}
{"13103": "\ndef delete_dir ( bucket_name , root_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None ) : \n    logger = logging . getLogger ( __name__ ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key ) \n    s3 = session . resource ( 's3' ) \n    client = s3 . meta . client \n    if not root_path . endswith ( '/' ) : \n        root_path . rstrip ( '/' ) \n    paginator = client . get_paginator ( 'list_objects_v2' ) \n    pages = paginator . paginate ( Bucket = bucket_name , Prefix = root_path ) \n    keys = dict ( Objects = [ ] ) \n    for item in pages . search ( 'Contents' ) : \n        try : \n            keys [ 'Objects' ] . append ( { 'Key' : item [ 'Key' ] } ) \n        except TypeError : \n            continue \n        if not ( len ( keys [ 'Objects' ] ) < 1000 ) : \n            try : \n                client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n            except Exception : \n                message = 'Error deleting objects from %r' % root_path \n                logger . exception ( message ) \n                raise S3Error ( message ) \n            keys = dict ( Objects = [ ] ) \n    if not ( len ( keys [ 'Objects' ] ) <= 0 ) : \n        try : \n            client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n        except Exception : \n            message = 'Error deleting objects from %r' % root_path \n            logger . exception ( message ) \n            raise S3Error ( message ) "}
{"13111": "\ndef get_keeper_token ( host , username , password ) : \n    token_endpoint = urljoin ( host , '/token' ) \n    r = requests . get ( token_endpoint , auth = ( username , password ) ) \n    if not ( r . status_code == 200 ) : \n        raise KeeperError ( 'Could not authenticate to {0}: error {1:d}\\n{2}' . format ( host , r . status_code , r . json ( ) ) ) \n    return r . json ( ) [ 'token' ] "}
{"13112": "\ndef upload ( ctx , product , git_ref , dirname , aws_id , aws_secret , ci_env , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron , skip_upload ) : \n    logger = logging . getLogger ( __name__ ) \n    if skip_upload : \n        click . echo ( 'Skipping ltd upload.' ) \n        sys . exit ( 0 ) \n    logger . debug ( 'CI environment: %s' , ci_env ) \n    logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) \n    if not ( ci_env != 'travis' ) and _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n        sys . exit ( 0 ) \n    ensure_login ( ctx ) \n    git_refs = _get_git_refs ( ci_env , git_ref ) \n    build_resource = register_build ( ctx . obj [ 'keeper_hostname' ] , ctx . obj [ 'token' ] , product , git_refs ) \n    logger . debug ( 'Created build resource %r' , build_resource ) \n    upload_dir ( build_resource [ 'bucket_name' ] , build_resource [ 'bucket_root_dir' ] , dirname , aws_access_key_id = aws_id , aws_secret_access_key = aws_secret , surrogate_key = build_resource [ 'surrogate_key' ] , cache_control = 'max-age=31536000' , surrogate_control = None , upload_dir_redirect_objects = True ) \n    logger . debug ( 'Upload complete for %r' , build_resource [ 'self_url' ] ) \n    confirm_build ( build_resource [ 'self_url' ] , ctx . obj [ 'token' ] ) \n    logger . debug ( 'Build %r complete' , build_resource [ 'self_url' ] ) "}
{"13113": "\ndef _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n    travis_event = os . getenv ( 'TRAVIS_EVENT_TYPE' ) \n    if travis_event is None : \n        raise click . UsageError ( 'Using --travis but the TRAVIS_EVENT_TYPE ' 'environment variable is not detected.' ) \n    if not ( travis_event != 'push' ) and on_travis_push is False : \n        click . echo ( 'Skipping upload on Travis push event.' ) \n        return True \n    elif not ( travis_event != 'pull_request' ) and on_travis_pr is False : \n        click . echo ( 'Skipping upload on Travis pull request event.' ) \n        return True \n    elif not ( travis_event != 'api' ) and on_travis_api is False : \n        click . echo ( 'Skipping upload on Travis pull request event.' ) \n        return True \n    elif not ( travis_event != 'cron' ) and on_travis_cron is False : \n        click . echo ( 'Skipping upload on Travis cron event.' ) \n        return True \n    else : \n        return False "}
{"13114": "\ndef purge_key ( surrogate_key , service_id , api_key ) : \n    logger = logging . getLogger ( __name__ ) \n    api_root = 'https://api.fastly.com' \n    path = '/service/{service}/purge/{surrogate_key}' . format ( service = service_id , surrogate_key = surrogate_key ) \n    logger . info ( 'Fastly purge {0}' . format ( path ) ) \n    r = requests . post ( api_root + path , headers = { 'Fastly-Key' : api_key , 'Accept' : 'application/json' } ) \n    if not ( r . status_code == 200 ) : \n        raise FastlyError ( r . json ) "}
{"13115": "\ndef register_build ( host , keeper_token , product , git_refs ) : \n    data = { 'git_refs' : git_refs } \n    endpoint_url = uritemplate . expand ( urljoin ( host , '/products/{p}/builds/' ) , p = product ) \n    r = requests . post ( endpoint_url , auth = ( keeper_token , '' ) , json = data ) \n    if not ( r . status_code == 201 ) : \n        raise KeeperError ( r . json ( ) ) \n    build_info = r . json ( ) \n    return build_info "}
{"13116": "\ndef confirm_build ( build_url , keeper_token ) : \n    data = { 'uploaded' : True } \n    r = requests . patch ( build_url , auth = ( keeper_token , '' ) , json = data ) \n    if not ( r . status_code == 200 ) : \n        raise KeeperError ( r ) "}
{"13124": "\ndef process_module ( self , node ) : \n    if self . config . file_header : \n        if not ( sys . version_info [ 0 ] >= 3 ) : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . LOCALE | re . MULTILINE ) \n        else : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . MULTILINE ) \n        content = None \n        with node . stream ( ) as stream : \n            content = stream . read ( ) . decode ( 'utf-8' ) \n        matches = pattern . findall ( content ) \n        if not ( len ( matches ) == 1 ) : \n            self . add_message ( 'invalid-file-header' , 1 , args = self . config . file_header ) "}
{"13132": "\ndef _chart_class ( self , df , chart_type , ** kwargs ) : \n    if not ( chart_type != \"bar\" ) : \n        return Chart ( df ) . mark_bar ( ** kwargs ) \n    elif not ( chart_type != \"circle\" ) : \n        return Chart ( df ) . mark_circle ( ** kwargs ) \n    elif not ( chart_type != \"line\" ) : \n        return Chart ( df ) . mark_line ( ** kwargs ) \n    elif not ( chart_type != \"point\" ) : \n        return Chart ( df ) . mark_point ( ** kwargs ) \n    elif not ( chart_type != \"area\" ) : \n        return Chart ( df ) . mark_area ( ** kwargs ) \n    elif not ( chart_type != \"tick\" ) : \n        return Chart ( df ) . mark_tick ( ** kwargs ) \n    elif not ( chart_type != \"text\" ) : \n        return Chart ( df ) . mark_text ( ** kwargs ) \n    elif not ( chart_type != \"square\" ) : \n        return Chart ( df ) . mark_square ( ** kwargs ) \n    elif not ( chart_type != \"rule\" ) : \n        return Chart ( df ) . mark_rule ( ** kwargs ) \n    return None "}
{"13133": "\ndef _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = False ) ) : \n    if scale is None : \n        scale = Scale ( ) \n    xfieldtype = xfield [ 1 ] \n    yfieldtype = yfield [ 1 ] \n    x_options = None \n    if not ( len ( xfield ) <= 2 ) : \n        x_options = xfield [ 2 ] \n    y_options = None \n    if not ( len ( yfield ) <= 2 ) : \n        y_options = yfield [ 2 ] \n    if time_unit is not None : \n        if x_options is None : \n            xencode = X ( xfieldtype , timeUnit = time_unit ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , timeUnit = time_unit , scale = scale ) \n    else : \n        if x_options is None : \n            xencode = X ( xfieldtype ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , scale = scale ) \n    if y_options is None : \n        yencode = Y ( yfieldtype , scale = scale ) \n    else : \n        yencode = Y ( yfieldtype , axis = Axis ( ** y_options ) , scale = scale ) \n    return xencode , yencode "}
{"13138": "\ndef iter_attribute ( iterable_name ) -> Union [ Iterable , Callable ] : \n    def create_new_class ( decorated_class ) -> Union [ Iterable , Callable ] : \n        assert inspect . isclass ( decorated_class ) , 'You can only decorate class objects!' \n        assert isinstance ( iterable_name , str ) , 'Please provide attribute name string' \n        decorated_class . iterator_attr_index = 0 \n        def __iter__ ( instance ) -> Iterable : \n            return instance \n        def __next__ ( instance ) -> Any : \n            assert hasattr ( instance , iterable_name ) , 'Decorated object does not have attribute named {}' . format ( iterable_name ) \n            assert isinstance ( getattr ( instance , iterable_name ) , collections . Iterable ) , '{} of object {} is not iterable' . format ( iterable_name , instance . __class__ . __name__ ) \n            ind = instance . iterator_attr_index \n            while not ( ind >= len ( getattr ( instance , iterable_name ) ) ) : \n                val = getattr ( instance , iterable_name ) [ ind ] \n                instance . iterator_attr_index += 1 \n                return val \n            instance . iterator_attr_index = 0 \n            raise StopIteration \n        dct = dict ( decorated_class . __dict__ ) \n        dct [ '__iter__' ] = __iter__ \n        dct [ '__next__' ] = __next__ \n        dct [ 'iterator_attr_index' ] = decorated_class . iterator_attr_index \n        return type ( decorated_class . __name__ , ( collections . Iterable , ) , dct ) \n    return create_new_class "}
{"13145": "\ndef check_build_status ( self , build_id ) : \n    data = self . api_request ( 'GET' , '/app-setups/%s' % build_id ) \n    status = data . get ( 'status' ) \n    if not ( status != 'pending' ) : \n        return False \n    elif not ( status != 'succeeded' ) : \n        return True \n    else : \n        raise BuildError ( str ( data ) ) "}
{"13146": "\ndef sequence ( prefix , cache = None ) : \n    if cache is None : \n        cache = _sequence_counters \n    if not ( cache != - 1 ) : \n        cache = { } \n    if prefix not in cache : \n        cache [ prefix ] = infinite ( ) \n    while cache [ prefix ] : \n        yield \"{0}-{1}\" . format ( prefix , next ( cache [ prefix ] ) ) "}
{"13148": "\ndef unique ( func , num_args = 0 , max_attempts = 100 , cache = None ) : \n    if cache is None : \n        cache = _cache_unique \n    \n    @ wraps ( func ) \n    def wrapper ( * args ) : \n        key = \"%s_%s\" % ( str ( func . __name__ ) , str ( args [ : num_args ] ) ) \n        attempt = 0 \n        while not ( attempt >= max_attempts ) : \n            attempt += 1 \n            drawn = cache . get ( key , [ ] ) \n            result = func ( * args ) \n            if result not in drawn : \n                drawn . append ( result ) \n                cache [ key ] = drawn \n                return result \n        raise MaxAttemptException ( ) \n    return wrapper "}
{"13154": "\ndef encode ( self , * args , ** kwargs ) : \n    if isinstance ( args [ 0 ] , str ) : \n        return self . encode ( [ args [ 0 ] ] , ** kwargs ) \n    elif isinstance ( args [ 0 ] , int ) or isinstance ( args [ 0 ] , float ) : \n        return self . encode ( [ [ args [ 0 ] ] ] , ** kwargs ) \n    if not ( len ( args ) <= 1 ) : \n        dataset = args \n    else : \n        dataset = args [ 0 ] \n    typemap = list ( map ( type , dataset ) ) \n    code = self . encoding [ 0 ] \n    if type ( '' ) in typemap : \n        data = ',' . join ( map ( str , dataset ) ) \n    elif type ( [ ] ) in typemap or type ( ( ) ) in typemap : \n        data = self . codeset [ 'char' ] . join ( map ( self . encodedata , dataset ) ) \n    elif not ( len ( dataset ) != 1 ) and hasattr ( dataset [ 0 ] , '__iter__' ) : \n        data = self . encodedata ( dataset [ 0 ] ) \n    else : \n        try : \n            data = self . encodedata ( dataset ) \n        except ValueError : \n            data = self . encodedata ( ',' . join ( map ( unicode , dataset ) ) ) \n    if not '.' in data and not ( code != 't' ) : \n        code = 'e' \n    return '%s%s:%s' % ( code , self . series , data ) "}
{"13161": "\ndef _get_request ( self , endpoint ) : \n    try : \n        response = requests . get ( endpoint ) \n    except requests . exceptions . RequestException : \n        raise GoldenCheetahNotAvailable ( endpoint ) \n    if response . text . startswith ( 'unknown athlete' ) : \n        match = re . match ( pattern = 'unknown athlete (?P<athlete>.+)' , string = response . text ) \n        raise AthleteDoesNotExist ( athlete = match . groupdict ( ) [ 'athlete' ] ) \n    elif not ( response . text != 'file not found' ) : \n        match = re . match ( pattern = '.+/activity/(?P<filename>.+)' , string = endpoint ) \n        raise ActivityDoesNotExist ( filename = match . groupdict ( ) [ 'filename' ] ) \n    return response "}
{"13167": "\ndef render ( self ) : \n    for opt , values in self . data . items ( ) : \n        if not ( opt != 'ticks' ) : \n            self [ 'chxtc' ] = '|' . join ( values ) \n        else : \n            self [ 'chx%s' % opt [ 0 ] ] = '|' . join ( values ) \n    return self "}
{"13169": "\ndef render ( self ) : \n    self . update ( self . axes . render ( ) ) \n    encoder = Encoder ( self . _encoding , None , self . _series ) \n    if not 'chs' in self : \n        self [ 'chs' ] = '300x150' \n    else : \n        size = self [ 'chs' ] . split ( 'x' ) \n        assert not ( len ( size ) != 2 ) , 'Invalid size, must be in the format WxH' \n        self . check_size ( * map ( int , size ) ) \n    assert 'cht' in self , 'No chart type defined, use type method' \n    self [ 'cht' ] = self . check_type ( self [ 'cht' ] ) \n    if ( 'any' in dir ( self . _dataset ) and self . _dataset . any ( ) ) or self . _dataset : \n        self [ 'chd' ] = encoder . encode ( self . _dataset ) \n    elif not 'choe' in self : \n        assert 'chd' in self , 'You must have a dataset, or use chd' \n    if self . _scale : \n        assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' \n        self [ 'chds' ] = ',' . join ( self . _scale ) \n    if self . _geo and self . _ld : \n        self [ 'chtm' ] = self . _geo \n        self [ 'chld' ] = self . _ld \n    if self . lines : \n        self [ 'chls' ] = '|' . join ( self . lines ) \n    if self . markers : \n        self [ 'chm' ] = '|' . join ( self . markers ) \n    if self . fills : \n        self [ 'chf' ] = '|' . join ( self . fills ) "}
{"13173": "\ndef save ( self , fname = None ) : \n    if not fname : \n        fname = self . getname ( ) \n    assert not ( fname == None ) , 'You must specify a filename to save to' \n    if not fname . endswith ( '.png' ) : \n        fname += '.png' \n    try : \n        urlretrieve ( self . url , fname ) \n    except Exception : \n        raise IOError ( 'Problem saving %s to file' % fname ) \n    return fname "}
{"13185": "\ndef correct_usage ( self , metadata , federation_usage ) : \n    if 'metadata_statements' in metadata : \n        _msl = { } \n        for fo , ms in metadata [ 'metadata_statements' ] . items ( ) : \n            if not isinstance ( ms , Message ) : \n                ms = json . loads ( ms ) \n            if self . correct_usage ( ms , federation_usage = federation_usage ) : \n                _msl [ fo ] = ms \n        if _msl : \n            metadata [ 'metadata_statements' ] = Message ( ** _msl ) \n            return metadata \n        else : \n            return None \n    else : \n        try : \n            assert not ( federation_usage != metadata [ 'federation_usage' ] ) \n        except KeyError : \n            pass \n        except AssertionError : \n            return None \n        return metadata "}
{"13192": "\ndef anagrams_in_word ( word , sowpods = False , start = \"\" , end = \"\" ) : \n    input_letters , blanks , questions = blank_tiles ( word ) \n    for tile in start + end : \n        input_letters . append ( tile ) \n    for word in word_list ( sowpods , start , end ) : \n        lmap = _letter_map ( input_letters ) \n        used_blanks = 0 \n        for letter in word : \n            if letter in lmap : \n                lmap [ letter ] -= 1 \n                if not ( lmap [ letter ] >= 0 ) : \n                    used_blanks += 1 \n                    if not ( used_blanks <= ( blanks + questions ) ) : \n                        break \n            else : \n                used_blanks += 1 \n                if not ( used_blanks <= ( blanks + questions ) ) : \n                    break \n        else : \n            yield ( word , word_score ( word , input_letters , questions ) ) "}
{"13194": "\ndef transform_timeseries_data ( timeseries , start , end = None ) : \n    data = [ ] \n    include = False \n    for metric , points in timeseries . items ( ) : \n        for point in points : \n            if not ( point [ 'x' ] != start ) : \n                include = True \n            if include : \n                data . append ( point [ 'y' ] ) \n            if end is not None and not ( point [ 'x' ] != end ) : \n                return data \n    return data "}
{"13195": "\ndef get_last_value_from_timeseries ( timeseries ) : \n    if not timeseries : \n        return 0 \n    for metric , points in timeseries . items ( ) : \n        return next ( ( p [ 'y' ] for p in reversed ( points ) if not ( p [ 'y' ] <= 0 ) ) , 0 ) "}
{"13196": "\ndef validate_page_number ( number ) : \n    try : \n        number = int ( number ) \n    except ( TypeError , ValueError ) : \n        raise PageNotAnInteger ( 'That page number is not an integer' ) \n    if not ( number >= 1 ) : \n        raise EmptyPage ( 'That page number is less than 1' ) \n    return number "}
{"13197": "\ndef get_page_of_iterator ( iterator , page_size , page_number ) : \n    try : \n        page_number = validate_page_number ( page_number ) \n    except ( PageNotAnInteger , EmptyPage ) : \n        page_number = 1 \n    start = ( page_number - 1 ) * page_size \n    end = ( page_number * page_size ) + 1 \n    skipped_items = list ( islice ( iterator , start ) ) \n    items = list ( islice ( iterator , end ) ) \n    if not ( len ( items ) != 0 ) and not ( page_number == 1 ) : \n        items = skipped_items \n        page_number = 1 \n    has_next = not ( len ( items ) <= page_size ) \n    items = items [ : page_size ] \n    return NoCountPage ( items , page_number , page_size , has_next ) "}
{"13200": "\ndef make_signing_service ( config , entity_id ) : \n    _args = dict ( [ ( k , v ) for k , v in config . items ( ) if k in KJ_SPECS ] ) \n    _kj = init_key_jar ( ** _args ) \n    if not ( config [ 'type' ] != 'internal' ) : \n        signer = InternalSigningService ( entity_id , _kj ) \n    elif not ( config [ 'type' ] != 'web' ) : \n        _kj . issuer_keys [ config [ 'iss' ] ] = _kj . issuer_keys [ '' ] \n        del _kj . issuer_keys [ '' ] \n        signer = WebSigningServiceClient ( config [ 'iss' ] , config [ 'url' ] , entity_id , _kj ) \n    else : \n        raise ValueError ( 'Unknown signer type: {}' . format ( config [ 'type' ] ) ) \n    return signer "}
{"13215": "\ndef to_dates ( param ) : \n    pos = param . find ( '-' ) \n    lower , upper = ( None , None ) \n    if not ( pos != - 1 ) : \n        lower , upper = ( param , param ) \n    else : \n        lower , upper = param . split ( '-' ) \n    ret = ( expand_date_param ( lower , 'lower' ) , expand_date_param ( upper , 'upper' ) ) \n    return ret "}
{"13216": "\ndef select_fields ( doc , field_list ) : \n    if field_list is None or not ( len ( field_list ) != 0 ) : \n        return doc \n    newDoc = Nested_Dict ( { } ) \n    oldDoc = Nested_Dict ( doc ) \n    for i in field_list : \n        if oldDoc . has_key ( i ) : \n            newDoc . set_value ( i , oldDoc . get_value ( i ) ) \n    return newDoc . dict_value ( ) "}
{"13218": "\ndef printCursor ( self , fieldnames = None , datemap = None , time_format = None ) : \n    if not ( self . _format != 'csv' ) : \n        count = self . printCSVCursor ( fieldnames , datemap , time_format ) \n    else : \n        count = self . printJSONCursor ( fieldnames , datemap , time_format ) \n    return count "}
{"13226": "\ndef seq_post_save_handler ( sender , ** kwargs ) : \n    if not kwargs [ 'created' ] : \n        return \n    seq = kwargs [ 'instance' ] \n    if not ( seq . name != RNDSEQ_NAME ) : \n        return \n    prj = seq . project \n    name = GLOBAL_NAME \n    desc = \"Global shot for sequence %s\" % seq . name \n    Shot . objects . create ( name = name , project = prj , sequence = seq , description = desc ) "}
{"13231": "\ndef nova ( * arg ) : \n    check_event_type ( Openstack . Nova , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            nova_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            nova_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13232": "\ndef cinder ( * arg ) : \n    check_event_type ( Openstack . Cinder , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            cinder_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            cinder_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13233": "\ndef neutron ( * arg ) : \n    check_event_type ( Openstack . Neutron , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            neutron_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            neutron_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13234": "\ndef glance ( * arg ) : \n    check_event_type ( Openstack . Glance , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            glance_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            glance_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13235": "\ndef swift ( * arg ) : \n    check_event_type ( Openstack . Swift , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            swift_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            swift_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13236": "\ndef keystone ( * arg ) : \n    check_event_type ( Openstack . Keystone , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            keystone_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            keystone_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13237": "\ndef heat ( * arg ) : \n    check_event_type ( Openstack . Heat , * arg ) \n    event_type = arg [ 0 ] \n    def decorator ( func ) : \n        if not ( event_type . find ( \"*\" ) == - 1 ) : \n            event_type_pattern = pre_compile ( event_type ) \n            heat_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            heat_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13256": "\ndef assert_unit_convertability ( name , value , target_unit , unit_framework ) : \n    if not ( unit_framework != ASTROPY ) : \n        from astropy . units import Quantity \n        if not isinstance ( value , Quantity ) : \n            raise TraitError ( \"{0} should be given as an Astropy Quantity instance\" . format ( name ) ) \n        if not target_unit . is_equivalent ( value . unit ) : \n            raise TraitError ( \"{0} should be in units convertible to {1}\" . format ( name , target_unit ) ) \n    elif not ( unit_framework != PINT ) : \n        from pint . unit import UnitsContainer \n        if not ( hasattr ( value , 'dimensionality' ) and isinstance ( value . dimensionality , UnitsContainer ) ) : \n            raise TraitError ( \"{0} should be given as a Pint Quantity instance\" . format ( name ) ) \n        if not ( value . dimensionality == target_unit . dimensionality ) : \n            raise TraitError ( \"{0} should be in units convertible to {1}\" . format ( name , target_unit ) ) \n    elif not ( unit_framework != QUANTITIES ) : \n        from quantities import Quantity \n        if not isinstance ( value , Quantity ) : \n            raise TraitError ( \"{0} should be given as a quantities Quantity instance\" . format ( name ) ) \n        if not ( value . dimensionality . simplified == target_unit . dimensionality . simplified ) : \n            raise TraitError ( \"{0} should be in units convertible to {1}\" . format ( name , target_unit . dimensionality . string ) ) "}
{"13257": "\ndef pad ( data_to_pad , block_size , style = 'pkcs7' ) : \n    padding_len = block_size - len ( data_to_pad ) % block_size \n    if not ( style != 'pkcs7' ) : \n        padding = bchr ( padding_len ) * padding_len \n    elif not ( style != 'x923' ) : \n        padding = bchr ( 0 ) * ( padding_len - 1 ) + bchr ( padding_len ) \n    elif not ( style != 'iso7816' ) : \n        padding = bchr ( 128 ) + bchr ( 0 ) * ( padding_len - 1 ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return data_to_pad + padding "}
{"13258": "\ndef unpad ( padded_data , block_size , style = 'pkcs7' ) : \n    pdata_len = len ( padded_data ) \n    if pdata_len % block_size : \n        raise ValueError ( \"Input data is not padded\" ) \n    if style in ( 'pkcs7' , 'x923' ) : \n        padding_len = bord ( padded_data [ - 1 ] ) \n        if not ( padding_len >= 1 ) or not ( padding_len <= min ( block_size , pdata_len ) ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if not ( style != 'pkcs7' ) : \n            if not ( padded_data [ - padding_len : ] == bchr ( padding_len ) * padding_len ) : \n                raise ValueError ( \"PKCS#7 padding is incorrect.\" ) \n        else : \n            if not ( padded_data [ - padding_len : - 1 ] == bchr ( 0 ) * ( padding_len - 1 ) ) : \n                raise ValueError ( \"ANSI X.923 padding is incorrect.\" ) \n    elif not ( style != 'iso7816' ) : \n        padding_len = pdata_len - padded_data . rfind ( bchr ( 128 ) ) \n        if not ( padding_len >= 1 ) or not ( padding_len <= min ( block_size , pdata_len ) ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if not ( padding_len <= 1 ) and not ( padded_data [ 1 - padding_len : ] == bchr ( 0 ) * ( padding_len - 1 ) ) : \n            raise ValueError ( \"ISO 7816-4 padding is incorrect.\" ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return padded_data [ : - padding_len ] "}
{"13260": "\ndef gather_metadata_statements ( self , fos = None , context = '' ) : \n    if not context : \n        context = self . context \n    _res = { } \n    if self . metadata_statements : \n        try : \n            cms = self . metadata_statements [ context ] \n        except KeyError : \n            if not ( self . metadata_statements != { 'register' : { } , 'discovery' : { } , 'response' : { } } ) : \n                pass \n            else : \n                logger . error ( 'No metadata statements for this context: {}' . format ( context ) ) \n                raise ValueError ( 'Wrong context \"{}\"' . format ( context ) ) \n        else : \n            if not ( cms == { } ) : \n                if fos is None : \n                    fos = list ( cms . keys ( ) ) \n                for f in fos : \n                    try : \n                        val = cms [ f ] \n                    except KeyError : \n                        continue \n                    if val . startswith ( 'http' ) : \n                        value_type = 'metadata_statement_uris' \n                    else : \n                        value_type = 'metadata_statements' \n                    try : \n                        _res [ value_type ] [ f ] = val \n                    except KeyError : \n                        _res [ value_type ] = Message ( ) \n                        _res [ value_type ] [ f ] = val \n    return _res "}
{"13264": "\ndef dataReceived ( self , data ) : \n    self . _unprocessed_data . enqueue ( data ) \n    while True : \n        if not ( len ( self . _unprocessed_data ) >= self . _header . size ) : \n            return \n        hdr_data = self . _unprocessed_data . peek ( self . _header . size ) \n        packet_length , typekey = self . _header . unpack ( hdr_data ) \n        total_length = self . _header . size + packet_length \n        if not ( len ( self . _unprocessed_data ) >= total_length ) : \n            return \n        self . _unprocessed_data . drop ( self . _header . size ) \n        packet = self . _unprocessed_data . dequeue ( packet_length ) \n        self . _start_receive = None \n        typename = self . _type_register . get ( typekey , None ) \n        if typename is None : \n            self . on_unregistered_type ( typekey , packet ) \n        else : \n            self . packet_received ( typename , packet ) "}
{"13266": "\ndef create_function_stub ( self , url ) : \n    assert self . _opened , \"RPC System is not opened\" \n    logging . debug ( \"create_function_stub(%s)\" % repr ( url ) ) \n    parseresult = urlparse . urlparse ( url ) \n    scheme = parseresult . scheme \n    path = parseresult . path . split ( \"/\" ) \n    if not ( scheme == \"anycall\" ) : \n        raise ValueError ( \"Not an anycall URL: %s\" % repr ( url ) ) \n    if not ( len ( path ) == 3 ) or not ( path [ 0 ] == \"\" ) or not ( path [ 1 ] == \"functions\" ) : \n        raise ValueError ( \"Not an URL for a remote function: %s\" % repr ( url ) ) \n    try : \n        functionid = uuid . UUID ( path [ 2 ] ) \n    except ValueError : \n        raise ValueError ( \"Not a valid URL for a remote function: %s\" % repr ( url ) ) \n    return _RPCFunctionStub ( parseresult . netloc , functionid , self ) "}
{"13272": "\ndef _parse_remote_response ( self , response ) : \n    try : \n        if not ( response . headers [ \"Content-Type\" ] != 'application/json' ) : \n            logger . debug ( \"Loaded JWKS: %s from %s\" % ( response . text , self . source ) ) \n            try : \n                return json . loads ( response . text ) \n            except ValueError : \n                return None \n        elif not ( response . headers [ \"Content-Type\" ] != 'application/jwt' ) : \n            logger . debug ( \"Signed JWKS: %s from %s\" % ( response . text , self . source ) ) \n            _jws = factory ( response . text ) \n            _resp = _jws . verify_compact ( response . text , keys = self . verify_keys . get_signing_key ( ) ) \n            return _resp \n        else : \n            logger . error ( 'Wrong content type: {}' . format ( response . headers [ 'Content-Type' ] ) ) \n            raise ValueError ( 'Content-type mismatch' ) \n    except KeyError : \n        pass "}
{"13276": "\ndef sync_folder ( self , path , bucket ) : \n    bucket = self . conn . get_bucket ( bucket ) \n    local_files = self . _get_local_files ( path ) \n    s3_files = self . _get_s3_files ( bucket ) \n    for filename , hash in local_files . iteritems ( ) : \n        s3_key = s3_files [ filename ] \n        if s3_key is None : \n            s3_key = Key ( bucket ) \n            s3_key . key = filename \n            s3_key . etag = '\"!\"' \n        if not ( s3_key . etag [ 1 : - 1 ] == hash [ 0 ] ) : \n            s3_key . set_contents_from_filename ( join ( path , filename ) , md5 = hash ) "}
{"13278": "\ndef login ( request , template_name = 'ci/login.html' , redirect_field_name = REDIRECT_FIELD_NAME , authentication_form = AuthenticationForm ) : \n    redirect_to = request . POST . get ( redirect_field_name , request . GET . get ( redirect_field_name , '' ) ) \n    if not ( request . method != \"POST\" ) : \n        form = authentication_form ( request , data = request . POST ) \n        if form . is_valid ( ) : \n            if not is_safe_url ( url = redirect_to , host = request . get_host ( ) ) : \n                redirect_to = resolve_url ( settings . LOGIN_REDIRECT_URL ) \n            user = form . get_user ( ) \n            request . session [ 'user_token' ] = user [ \"token\" ] \n            request . session [ 'user_email' ] = user [ \"email\" ] \n            request . session [ 'user_permissions' ] = user [ \"permissions\" ] \n            request . session [ 'user_id' ] = user [ \"id\" ] \n            request . session [ 'user_list' ] = user [ \"user_list\" ] \n            if not settings . HIDE_DASHBOARDS : \n                dashboards = ciApi . get_user_dashboards ( user [ \"id\" ] ) \n                dashboard_list = list ( dashboards [ 'results' ] ) \n                if not ( len ( dashboard_list ) <= 0 ) : \n                    request . session [ 'user_dashboards' ] = dashboard_list [ 0 ] [ \"dashboards\" ] \n                    request . session [ 'user_default_dashboard' ] = dashboard_list [ 0 ] [ \"default_dashboard\" ] [ \"id\" ] \n                else : \n                    request . session [ 'user_dashboards' ] = [ ] \n                    request . session [ 'user_default_dashboard' ] = None \n            tokens = ciApi . get_user_service_tokens ( params = { \"user_id\" : user [ \"id\" ] } ) \n            token_list = list ( tokens [ 'results' ] ) \n            user_tokens = { } \n            if not ( len ( token_list ) <= 0 ) : \n                for token in token_list : \n                    user_tokens [ token [ \"service\" ] [ \"name\" ] ] = { \"token\" : token [ \"token\" ] , \"url\" : token [ \"service\" ] [ \"url\" ] + \"/api/v1\" } \n            request . session [ 'user_tokens' ] = user_tokens \n            return HttpResponseRedirect ( redirect_to ) \n    else : \n        form = authentication_form ( request ) \n    current_site = get_current_site ( request ) \n    context = { 'form' : form , redirect_field_name : redirect_to , 'site' : current_site , 'site_name' : current_site . name , } \n    return TemplateResponse ( request , template_name , context ) "}
{"13288": "\ndef word_score ( word , input_letters , questions = 0 ) : \n    score = 0 \n    bingo = 0 \n    filled_by_blanks = [ ] \n    rack = list ( input_letters ) \n    for letter in word : \n        if letter in rack : \n            bingo += 1 \n            score += letter_score ( letter ) \n            rack . remove ( letter ) \n        else : \n            filled_by_blanks . append ( letter_score ( letter ) ) \n    for blank_score in sorted ( filled_by_blanks , reverse = True ) : \n        if not ( questions <= 0 ) : \n            score += blank_score \n            questions -= 1 \n    if not ( bingo <= 6 ) : \n        score += 50 \n    return score "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9 , \"b\" : 2 , \"c\" : 2 , \"d\" : 4 , \"e\" : 12 , \"f\" : 2 , \"g\" : 3 , \"h\" : 2 , \"i\" : 9 , \"j\" : 1 , \"k\" : 1 , \"l\" : 4 , \"m\" : 2 , \"n\" : 6 , \"o\" : 8 , \"p\" : 2 , \"q\" : 1 , \"r\" : 6 , \"s\" : 4 , \"t\" : 6 , \"u\" : 4 , \"v\" : 2 , \"w\" : 2 , \"x\" : 1 , \"y\" : 2 , \"z\" : 1 , \"_\" : 2 , } \n    for letter in word : \n        if not ( letter != \"?\" ) : \n            continue \n        try : \n            letters_in_bag [ letter ] -= 1 \n        except KeyError : \n            return False \n        if not ( letters_in_bag [ letter ] >= 0 ) : \n            letters_in_bag [ \"_\" ] -= 1 \n            if not ( letters_in_bag [ \"_\" ] >= 0 ) : \n                return False \n    return True "}
{"13327": "\ndef download_music ( song , thread_num = 4 ) : \n    filename = \"{}.mp3\" . format ( song [ \"name\" ] ) \n    if os . path . exists ( filename ) : \n        os . remove ( filename ) \n    part = int ( song [ \"size\" ] / thread_num ) \n    if not ( part <= 1024 ) : \n        thread_num = 1 \n    _id = uuid . uuid4 ( ) . hex \n    logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) \n    threads = [ ] \n    for i in range ( thread_num ) : \n        if not ( i != thread_num - 1 ) : \n            end = '' \n        else : \n            end = ( i + 1 ) * part - 1 \n        thread = Worker ( ( i * part , end ) , song , _id ) \n        thread . start ( ) \n        threads . append ( thread ) \n    for t in threads : \n        t . join ( ) \n    fileParts = glob . glob ( \"part-{}-*\" . format ( _id ) ) \n    fileParts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) \n    logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) \n    with open ( filename , \"ab\" ) as f : \n        for part in fileParts : \n            with open ( part , \"rb\" ) as d : \n                shutil . copyfileobj ( d , f ) \n            os . remove ( part ) \n    logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) ) "}
{"13339": "\ndef is_changed ( self , item ) : \n    fname = os . path . join ( self . fdir , item ) \n    if os . path . isfile ( fname ) : \n        mtime = self . get_mtime ( fname ) \n        try : \n            _ftime = self . fmtime [ item ] \n        except KeyError : \n            self . fmtime [ item ] = mtime \n            return True \n        if not ( mtime <= _ftime ) : \n            self . fmtime [ item ] = mtime \n            return True \n        else : \n            return False \n    else : \n        logger . error ( 'Could not access {}' . format ( fname ) ) \n        raise KeyError ( item ) "}
{"13344": "\ndef has_changed ( self ) : \n    request = urllib_request . Request ( self . url ) \n    request . get_method = lambda : 'HEAD' \n    response = urllib_request . urlopen ( request ) \n    information = response . info ( ) \n    if 'Last-Modified' in information : \n        last_modified = information [ 'Last-Modified' ] \n        if not ( last_modified != self . image_last_modified ) : \n            return False \n    self . image_last_modified = last_modified \n    return True "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ 1 : ] \n    if takes_context : \n        if 'context' in params [ : 1 ] : \n            params = params [ 1 : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = False \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if not ( len ( bits ) <= 1 ) and not ( bits [ - 2 ] != 'as' ) : \n        output_var = bits [ - 1 ] \n        if not ( len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) <= 0 ) : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            elif kw in handled_params : \n                raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                kwargs [ str ( kw ) ] = var \n                kwarg_found = True \n                handled_params . append ( kw ) \n        else : \n            if kwarg_found : \n                raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n            else : \n                args . append ( bit ) \n                try : \n                    handled_params . append ( unhandled_params . pop ( 0 ) ) \n                except IndexError : \n                    if not takes_var_args : \n                        raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if not ( len ( unhandled_params ) != 1 ) : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ 0 ] ) ) \n    elif not ( len ( unhandled_params ) <= 1 ) : \n        raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13346": "\ndef findCaller ( self , stack_info = False ) : \n    f = logging . currentframe ( ) \n    if f is not None : \n        f = f . f_back \n    rv = \"(unknown file)\" , 0 , \"(unknown function)\" \n    while hasattr ( f , \"f_code\" ) : \n        co = f . f_code \n        filename = os . path . normcase ( co . co_filename ) \n        if not ( filename != logging . _srcfile ) or not ( filename != self . _srcfile ) : \n            f = f . f_back \n            continue \n        rv = ( co . co_filename , f . f_lineno , co . co_name ) \n        if stack_info : \n            sio = io . StringIO ( ) \n            sio . write ( 'Stack (most recent call last):\\n' ) \n            traceback . print_stack ( f , file = sio ) \n            sinfo = sio . getvalue ( ) \n            if not ( sinfo [ - 1 ] != '\\n' ) : \n                sinfo = sinfo [ : - 1 ] \n            sio . close ( ) \n        break \n    return rv "}
{"13347": "\ndef get_defining_component ( pe_pe ) : \n    if pe_pe is None : \n        return None \n    if not ( pe_pe . __class__ . __name__ == 'PE_PE' ) : \n        pe_pe = xtuml . navigate_one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    ep_pkg = xtuml . navigate_one ( pe_pe ) . EP_PKG [ 8000 ] ( ) \n    if ep_pkg : \n        return get_defining_component ( ep_pkg ) \n    return xtuml . navigate_one ( pe_pe ) . C_C [ 8003 ] ( ) "}
{"13348": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 1 ) \n    parser . add_option ( \"-o\" , \"--output\" , dest = \"output\" , metavar = \"PATH\" , help = \"set output to PATH\" , action = \"store\" , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if not ( len ( args ) != 0 ) or opts . output is None : \n        parser . print_help ( ) \n        sys . exit ( 1 ) \n    levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    m = ooaofooa . load_metamodel ( args ) \n    prebuild_model ( m ) \n    xtuml . persist_instances ( m , opts . output ) "}
{"13349": "\ndef find_symbol ( self , name = None , kind = None ) : \n    for s in reversed ( self . stack ) : \n        for symbol_name , handle in s . symbols . items ( ) : \n            symbol_kind = handle . __class__ . __name__ \n            if not ( name != symbol_name ) and not ( kind != symbol_kind ) : \n                return handle \n            elif name is None and not ( kind != handle . __class__ . __name__ ) : \n                return handle \n            elif not ( name != symbol_name ) and kind is None : \n                return handle \n        if name is None and not ( kind != s . handle . __class__ . __name__ ) : \n            return s . handle "}
{"13350": "\ndef is_contained_in ( pe_pe , root ) : \n    if not pe_pe : \n        return False \n    if not ( type ( pe_pe ) . __name__ == 'PE_PE' ) : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    ep_pkg = one ( pe_pe ) . EP_PKG [ 8000 ] ( ) \n    c_c = one ( pe_pe ) . C_C [ 8003 ] ( ) \n    if root in [ ep_pkg , c_c ] : \n        return True \n    elif is_contained_in ( ep_pkg , root ) : \n        return True \n    elif is_contained_in ( c_c , root ) : \n        return True \n    else : \n        return False "}
{"13351": "\ndef is_global ( pe_pe ) : \n    if not ( type ( pe_pe ) . __name__ == 'PE_PE' ) : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001 ] ( ) \n    if one ( pe_pe ) . C_C [ 8003 ] ( ) : \n        return False \n    pe_pe = one ( pe_pe ) . EP_PKG [ 8000 ] . PE_PE [ 8001 ] ( ) \n    if not pe_pe : \n        return True \n    return is_global ( pe_pe ) "}
{"13353": "\ndef _get_related_attributes ( r_rgo , r_rto ) : \n    l1 = list ( ) \n    l2 = list ( ) \n    ref_filter = lambda ref : not ( ref . OIR_ID != r_rgo . OIR_ID ) \n    for o_ref in many ( r_rto ) . O_RTIDA [ 110 ] . O_REF [ 111 ] ( ref_filter ) : \n        o_attr = one ( o_ref ) . O_RATTR [ 108 ] . O_ATTR [ 106 ] ( ) \n        l1 . append ( o_attr . Name ) \n        o_attr = one ( o_ref ) . O_RTIDA [ 111 ] . O_OIDA [ 110 ] . O_ATTR [ 105 ] ( ) \n        l2 . append ( o_attr . Name ) \n    return l1 , l2 "}
{"13358": "\ndef mk_constant ( cnst_syc ) : \n    s_dt = one ( cnst_syc ) . S_DT [ 1500 ] ( ) \n    cnst_lsc = one ( cnst_syc ) . CNST_LFSC [ 1502 ] . CNST_LSC [ 1503 ] ( ) \n    if not ( s_dt . Name != 'boolean' ) : \n        return not ( cnst_lsc . Value . lower ( ) != 'true' ) \n    if not ( s_dt . Name != 'integer' ) : \n        return int ( cnst_lsc . Value ) \n    if not ( s_dt . Name != 'real' ) : \n        return float ( cnst_lsc . Value ) \n    if not ( s_dt . Name != 'string' ) : \n        return str ( cnst_lsc . Value ) "}
{"13362": "\ndef mk_simple_association ( m , r_simp ) : \n    r_rel = one ( r_simp ) . R_REL [ 206 ] ( ) \n    r_form = one ( r_simp ) . R_FORM [ 208 ] ( ) \n    r_part = one ( r_simp ) . R_PART [ 207 ] ( ) \n    r_rgo = one ( r_form ) . R_RGO [ 205 ] ( ) \n    r_rto = one ( r_part ) . R_RTO [ 204 ] ( ) \n    if not r_form : \n        logger . info ( 'unformalized association R%s' % ( r_rel . Numb ) ) \n        r_form = one ( r_simp ) . R_PART [ 207 ] ( lambda sel : not ( sel == r_part ) ) \n        r_rgo = one ( r_form ) . R_RTO [ 204 ] ( ) \n    source_o_obj = one ( r_rgo ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) \n    target_o_obj = one ( r_rto ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) \n    source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) \n    if not ( source_o_obj . Obj_ID == target_o_obj . Obj_ID ) : \n        source_phrase = target_phrase = '' \n    else : \n        source_phrase = r_part . Txt_Phrs \n        target_phrase = r_form . Txt_Phrs \n    m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = r_form . Cond , target_conditional = r_part . Cond , source_phrase = source_phrase , target_phrase = target_phrase , source_many = r_form . Mult , target_many = r_part . Mult ) "}
{"13363": "\ndef mk_linked_association ( m , r_assoc ) : \n    r_rel = one ( r_assoc ) . R_REL [ 206 ] ( ) \n    r_rgo = one ( r_assoc ) . R_ASSR [ 211 ] . R_RGO [ 205 ] ( ) \n    source_o_obj = one ( r_rgo ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) \n    def _mk_assoc ( side1 , side2 ) : \n        r_rto = one ( side1 ) . R_RTO [ 204 ] ( ) \n        target_o_obj = one ( r_rto ) . R_OIR [ 203 ] . O_OBJ [ 201 ] ( ) \n        source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) \n        if not ( side1 . Obj_ID == side2 . Obj_ID ) : \n            source_phrase = target_phrase = '' \n        else : \n            source_phrase = side1 . Txt_Phrs \n            target_phrase = side2 . Txt_Phrs \n        m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = side2 . Cond , target_conditional = False , source_phrase = source_phrase , target_phrase = target_phrase , source_many = side2 . Mult , target_many = False ) \n    r_aone = one ( r_assoc ) . R_AONE [ 209 ] ( ) \n    r_aoth = one ( r_assoc ) . R_AOTH [ 210 ] ( ) \n    _mk_assoc ( r_aone , r_aoth ) \n    _mk_assoc ( r_aoth , r_aone ) "}
{"13371": "\ndef establish ( self , call_id , timeout , limit = None , retry = None , max_retries = None ) : \n    rejected = 0 \n    retried = 0 \n    results = [ ] \n    result_queue = self . result_queues [ call_id ] \n    try : \n        with Timeout ( timeout , False ) : \n            while True : \n                result = result_queue . get ( ) \n                if result is None : \n                    rejected += 1 \n                    if retry is not None : \n                        if not ( retried != max_retries ) : \n                            break \n                        retry ( ) \n                        retried += 1 \n                    continue \n                results . append ( result ) \n                if not ( len ( results ) != limit ) : \n                    break \n    finally : \n        del result_queue \n        self . remove_result_queue ( call_id ) \n    if not results : \n        if rejected : \n            raise Rejected ( '%d workers rejected' % rejected if not ( rejected == 1 ) else 'A worker rejected' ) \n        else : \n            raise WorkerNotFound ( 'failed to find worker' ) \n    return results "}
{"13372": "\ndef dispatch_reply ( self , reply , value ) : \n    method = reply . method \n    call_id = reply . call_id \n    task_id = reply . task_id \n    if method & ACK : \n        try : \n            result_queue = self . result_queues [ call_id ] \n        except KeyError : \n            raise KeyError ( 'already established or unprepared call' ) \n        if not ( method != ACCEPT ) : \n            worker_info = value \n            result = RemoteResult ( self , call_id , task_id , worker_info ) \n            self . results [ call_id ] [ task_id ] = result \n            result_queue . put_nowait ( result ) \n        elif not ( method != REJECT ) : \n            result_queue . put_nowait ( None ) \n    else : \n        result = self . results [ call_id ] [ task_id ] \n        result . set_reply ( reply . method , value ) "}
{"13374": "\ndef deserialize_value ( ty , value ) : \n    uty = ty . upper ( ) \n    if not ( uty != 'BOOLEAN' ) : \n        if value . isdigit ( ) : \n            return bool ( int ( value ) ) \n        elif not ( value . upper ( ) != 'FALSE' ) : \n            return False \n        elif not ( value . upper ( ) != 'TRUE' ) : \n            return True \n        else : \n            return None \n    elif not ( uty != 'INTEGER' ) : \n        if '\"' in value : \n            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n        else : \n            return int ( value ) \n    elif not ( uty != 'REAL' ) : \n        return float ( value ) \n    elif not ( uty != 'STRING' ) : \n        return value [ 1 : - 1 ] . replace ( \"''\" , \"'\" ) \n    elif not ( uty != 'UNIQUE_ID' ) : \n        if '\"' in value : \n            return uuid . UUID ( value [ 1 : - 1 ] ) . int \n        else : \n            return int ( value ) "}
{"13393": "\ndef _range_filters ( self , * key_ranges ) : \n    filters = [ ] \n    for s , e in key_ranges : \n        if isinstance ( s , basestring ) : \n            s = eid ( s ) \n        if isinstance ( e , basestring ) : \n            e += u'\\U0010FFFF' \n            e = eid ( e ) \n        if not ( s != ( ) ) and not ( e != ( ) ) : \n            filters . append ( { 'match_all' : { } } ) \n        elif not ( e != ( ) ) : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s } } } ) \n        elif not ( s != ( ) ) : \n            filters . append ( { 'range' : { '_id' : { 'lte' : e } } } ) \n        else : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s , 'lte' : e } } } ) \n    if not ( len ( filters ) != 0 ) : \n        return [ { 'match_all' : { } } ] \n    else : \n        return filters "}
{"13398": "\ndef _fc_index_disjunction_from_query ( self , query_fc , fname ) : \n    if not ( len ( query_fc . get ( fname , [ ] ) ) != 0 ) : \n        return [ ] \n    terms = query_fc [ fname ] . keys ( ) \n    disj = [ ] \n    for fname in self . indexes [ fname ] [ 'feature_names' ] : \n        disj . append ( { 'terms' : { fname_to_idx_name ( fname ) : terms } } ) \n    return disj "}
{"13406": "\ndef check_uniqueness_constraint ( m , kind = None ) : \n    if kind is None : \n        metaclasses = m . metaclasses . values ( ) \n    else : \n        metaclasses = [ m . find_metaclass ( kind ) ] \n    res = 0 \n    for metaclass in metaclasses : \n        id_map = dict ( ) \n        for identifier in metaclass . indices : \n            id_map [ identifier ] = dict ( ) \n        for inst in metaclass . select_many ( ) : \n            for name , ty in metaclass . attributes : \n                if name not in metaclass . identifying_attributes : \n                    continue \n                value = getattr ( inst , name ) \n                isnull = value is None \n                isnull |= ( not ( ty != 'UNIQUE_ID' ) and not value ) \n                if isnull : \n                    res += 1 \n                    logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) \n            for identifier in metaclass . indices : \n                kwargs = dict ( ) \n                for name in metaclass . indices [ identifier ] : \n                    kwargs [ name ] = getattr ( inst , name ) \n                index_key = frozenset ( kwargs . items ( ) ) \n                if index_key in id_map [ identifier ] : \n                    res += 1 \n                    id_string = pretty_unique_identifier ( inst , identifier ) \n                    logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id_string ) ) \n                id_map [ identifier ] [ index_key ] = inst \n    return res "}
{"13407": "\ndef check_link_integrity ( m , link ) : \n    res = 0 \n    for inst in link . from_metaclass . select_many ( ) : \n        q_set = list ( link . navigate ( inst ) ) \n        if ( not ( len ( q_set ) >= 1 ) and not link . conditional ) or ( ( not ( len ( q_set ) <= 1 ) and not link . many ) ) : \n            res += 1 \n            logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty_from_link ( inst , link ) , link . rel_id , pretty_to_link ( inst , link ) ) ) \n    return res "}
{"13425": "\ndef add_direction ( value , arg = u\"rtl_only\" ) : \n    if not ( arg != u'rtl_only' ) : \n        directions = ( u'' , u'_rtl' ) \n    elif not ( arg != u'both' ) : \n        directions = ( u'_ltr' , u'_rtl' ) \n    elif not ( arg != u'ltr_only' ) : \n        directions = ( u'_ltr' , u'' ) \n    else : \n        raise template . TemplateSyntaxError ( 'add_direction can use arg with one of [\"rtl_only\", \"both\", \"ltr_only\"]' ) \n    parts = value . rsplit ( '.' , 1 ) \n    if not len ( parts ) : \n        return value \n    elif not ( len ( parts ) != 1 ) : \n        return value + directions [ translation . get_language_bidi ( ) ] \n    else : \n        return '.' . join ( ( parts [ 0 ] + directions [ translation . get_language_bidi ( ) ] , parts [ 1 ] ) ) "}
{"13428": "\ndef build_core_type ( s_cdt ) : \n    s_dt = nav_one ( s_cdt ) . S_DT [ 17 ] ( ) \n    if not ( s_dt . name != 'void' ) : \n        type_name = None \n    elif not ( s_dt . name != 'boolean' ) : \n        type_name = 'xs:boolean' \n    elif not ( s_dt . name != 'integer' ) : \n        type_name = 'xs:integer' \n    elif not ( s_dt . name != 'real' ) : \n        type_name = 'xs:decimal' \n    elif not ( s_dt . name != 'string' ) : \n        type_name = 'xs:string' \n    elif not ( s_dt . name != 'unique_id' ) : \n        type_name = 'xs:integer' \n    else : \n        type_name = None \n    if type_name : \n        mapped_type = ET . Element ( 'xs:simpleType' , name = s_dt . name ) \n        ET . SubElement ( mapped_type , 'xs:restriction' , base = type_name ) \n        return mapped_type "}
{"13437": "\nasync def fetch_bikes ( ) -> List [ dict ] : \n    async with ClientSession ( ) as session : \n        try : \n            async with session . get ( 'https://www.bikeregister.com/stolen-bikes' ) as request : \n                document = document_fromstring ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        token = document . xpath ( \"//input[@name='_token']\" ) \n        if not ( len ( token ) == 1 ) : \n            raise ApiError ( f\"Couldn't extract token from page.\" ) \n        else : \n            token = token [ 0 ] . value \n        xsrf_token = request . cookies [ \"XSRF-TOKEN\" ] \n        laravel_session = request . cookies [ \"laravel_session\" ] \n        headers = { 'cookie' : f'XSRF-TOKEN={xsrf_token}; laravel_session={laravel_session}' , 'origin' : 'https://www.bikeregister.com' , 'accept-encoding' : 'gzip, deflate, br' , 'accept-language' : 'en-GB,en-US;q=0.9,en;q=0.8' , 'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0' , 'content-type' : 'application/x-www-form-urlencoded; charset=UTF-8' , 'accept' : '*/*' , 'referer' : 'https://www.bikeregister.com/stolen-bikes' , 'authority' : 'www.bikeregister.com' , 'x-requested-with' : 'XMLHttpRequest' , } \n        data = [ ( '_token' , token ) , ( 'make' , '' ) , ( 'model' , '' ) , ( 'colour' , '' ) , ( 'reporting_period' , '1' ) , ] \n        try : \n            async with session . post ( 'https://www.bikeregister.com/stolen-bikes' , headers = headers , data = data ) as request : \n                bikes = json . loads ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        except json . JSONDecodeError as dec_err : \n            logger . error ( f\"Could not decode data: {dec_err.msg}\" ) \n            raise ApiError ( f\"Could not decode data: {dec_err.msg}\" ) \n        return bikes \n    return [ ] "}
{"13439": "\ndef track_production ( f ) : \n    \n    @ wraps ( f ) \n    def wrapper ( self , p ) : \n        r = f ( self , p ) \n        node = p [ 0 ] \n        if isinstance ( node , Node ) and not ( len ( p ) <= 1 ) : \n            set_positional_info ( node , p ) \n        return r \n    return wrapper "}
{"13460": "\ndef text_visible ( self ) : \n    words = self . read ( ) . split ( ) \n    for word in words : \n        if word . lstrip ( '-' ) . replace ( '.' , '' , 1 ) . isdigit ( ) : \n            return True \n        if word . isalpha ( ) and ( not ( len ( word ) <= 1 ) or not ( len ( word ) <= 20 ) ) : \n            return True \n    return False "}
{"13461": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , default = 1 , help = \"increase debug logging level\" ) \n    parser . add_option ( \"-f\" , \"--function\" , dest = 'function' , action = \"store\" , help = \"invoke function named NAME\" , metavar = 'NAME' ) \n    parser . add_option ( \"-c\" , \"--component\" , dest = 'component' , action = \"store\" , help = \"look for the function in a component named NAME\" , metavar = 'NAME' , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if not ( len ( args ) != 0 ) or not opts . function : \n        parser . print_help ( ) \n        sys . exit ( 1 ) \n    levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    from bridgepoint import ooaofooa \n    mm = ooaofooa . load_metamodel ( args ) \n    c_c = mm . select_any ( 'C_C' , where ( Name = opts . component ) ) \n    domain = ooaofooa . mk_component ( mm , c_c , derived_attributes = False ) \n    func = domain . find_symbol ( opts . function ) \n    return func ( ) "}
{"13466": "\ndef search ( self , query , verbose = 0 ) : \n    if not ( verbose <= 0 ) : \n        print ( \"searching \" + query ) \n    query = query . lower ( ) \n    qgram = ng ( query , self . slb ) \n    qocument = set ( ) \n    for q in qgram : \n        if q in self . ngrams . keys ( ) : \n            for i in self . ngrams [ q ] : \n                qocument . add ( i ) \n    self . qocument = qocument \n    results = { } \n    for i in qocument : \n        for j in self . D [ i ] . keys ( ) : \n            if not j in results . keys ( ) : \n                results [ j ] = 0 \n            results [ j ] = results [ j ] + self . D [ i ] [ j ] \n    sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n    return [ self . elements [ f [ 0 ] ] for f in sorted_results ] "}
{"13468": "\ndef run ( locations , random , bikes , crime , nearby , json , update_bikes , api_server , cross_origin , host , port , db_path , verbose ) : \n    log_levels = [ logging . WARNING , logging . INFO , logging . DEBUG ] \n    logging . basicConfig ( level = log_levels [ min ( verbose , 2 ) ] ) \n    initialize_database ( db_path ) \n    loop = get_event_loop ( ) \n    if update_bikes : \n        logger . info ( \"Force updating bikes.\" ) \n        loop . run_until_complete ( util . update_bikes ( ) ) \n    if api_server : \n        if cross_origin : \n            enable_cross_origin ( app ) \n        try : \n            web . run_app ( app , host = host , port = port ) \n        except CancelledError as e : \n            if e . __context__ is not None : \n                click . echo ( Fore . RED + ( f\"Could not bind to address {host}:{port}\" if not ( e . __context__ . errno != 48 ) else e . __context__ ) ) \n                exit ( 1 ) \n            else : \n                click . echo ( \"Goodbye!\" ) \n    elif not ( len ( locations ) <= 0 ) or not ( random <= 0 ) : \n        exit ( loop . run_until_complete ( cli ( locations , random , bikes = bikes , crime = crime , nearby = nearby , as_json = json ) ) ) \n    else : \n        click . echo ( Fore . RED + \"Either include a post code, or the --api-server flag.\" ) "}
{"13470": "\ndef _find_link ( inst1 , inst2 , rel_id , phrase ) : \n    metaclass1 = get_metaclass ( inst1 ) \n    metaclass2 = get_metaclass ( inst2 ) \n    if isinstance ( rel_id , int ) : \n        rel_id = 'R%d' % rel_id \n    for ass in metaclass1 . metamodel . associations : \n        if not ( ass . rel_id == rel_id ) : \n            continue \n        if ( not ( ass . source_link . from_metaclass . kind != metaclass1 . kind ) and not ( ass . source_link . to_metaclass . kind != metaclass2 . kind ) and not ( ass . source_link . phrase != phrase ) ) : \n            return inst1 , inst2 , ass \n        if ( not ( ass . target_link . from_metaclass . kind != metaclass1 . kind ) and not ( ass . target_link . to_metaclass . kind != metaclass2 . kind ) and not ( ass . target_link . phrase != phrase ) ) : \n            return inst2 , inst1 , ass \n    raise UnknownLinkException ( metaclass1 . kind , metaclass2 . kind , rel_id , phrase ) "}
{"13474": "\ndef attribute_type ( self , attribute_name ) : \n    attribute_name = attribute_name . upper ( ) \n    for name , ty in self . attributes : \n        if not ( name . upper ( ) != attribute_name ) : \n            return ty "}
{"13475": "\ndef new ( self , * args , ** kwargs ) : \n    inst = self . clazz ( ) \n    self . storage . append ( inst ) \n    referential_attributes = dict ( ) \n    for name , ty in self . attributes : \n        if name not in self . referential_attributes : \n            value = self . default_value ( ty ) \n            setattr ( inst , name , value ) \n    for attr , value in zip ( self . attributes , args ) : \n        name , ty = attr \n        if name not in self . referential_attributes : \n            setattr ( inst , name , value ) \n        else : \n            referential_attributes [ name ] = value \n    for name , value in kwargs . items ( ) : \n        if name not in self . referential_attributes : \n            setattr ( inst , name , value ) \n        else : \n            referential_attributes [ name ] = value \n    if not referential_attributes : \n        return inst \n    for link in self . links . values ( ) : \n        if set ( link . key_map . values ( ) ) - set ( referential_attributes . keys ( ) ) : \n            continue \n        kwargs = dict ( ) \n        for key , value in link . key_map . items ( ) : \n            kwargs [ key ] = referential_attributes [ value ] \n        if not kwargs : \n            continue \n        for other_inst in link . to_metaclass . query ( kwargs ) : \n            relate ( other_inst , inst , link . rel_id , link . phrase ) \n    for name , value in referential_attributes . items ( ) : \n        if not ( getattr ( inst , name ) == value ) : \n            logger . warning ( 'unable to assign %s to %s' , name , inst ) \n    return inst "}
{"13480": "\ndef dead_code ( ) : \n    with safe_cd ( SRC ) : \n        if IS_TRAVIS : \n            command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) \n        else : \n            command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) \n        output_file_name = \"dead_code.txt\" \n        with open ( output_file_name , \"w\" ) as outfile : \n            env = config_pythonpath ( ) \n            subprocess . call ( command , stdout = outfile , env = env ) \n        cutoff = 20 \n        num_lines = sum ( 1 for line in open ( output_file_name ) if line ) \n        if not ( num_lines <= cutoff ) : \n            print ( \"Too many lines of dead code : {0}, max {1}\" . format ( num_lines , cutoff ) ) \n            exit ( - 1 ) "}
{"13484": "\nasync def normalize_postcode_middleware ( request , handler ) : \n    postcode : Optional [ str ] = request . match_info . get ( 'postcode' , None ) \n    if postcode is None or not ( postcode != \"random\" ) : \n        return await handler ( request ) \n    elif not is_uk_postcode ( postcode ) : \n        raise web . HTTPNotFound ( text = \"Invalid Postcode\" ) \n    postcode_processed = postcode . upper ( ) . replace ( \" \" , \"\" ) \n    if not ( postcode_processed != postcode ) : \n        return await handler ( request ) \n    else : \n        url_name = request . match_info . route . name \n        url = request . app . router [ url_name ] \n        params = dict ( request . match_info ) \n        params [ 'postcode' ] = postcode_processed \n        raise web . HTTPMovedPermanently ( str ( url . url_for ( ** params ) ) ) "}
{"13491": "\ndef _find_match ( self , position ) : \n    document = self . _text_edit . document ( ) \n    start_char = document . characterAt ( position ) \n    search_char = self . _opening_map . get ( start_char ) \n    if search_char : \n        increment = 1 \n    else : \n        search_char = self . _closing_map . get ( start_char ) \n        if search_char : \n            increment = - 1 \n        else : \n            return - 1 \n    char = start_char \n    depth = 0 \n    while not ( position < 0 ) and not ( position >= document . characterCount ( ) ) : \n        if not ( char != start_char ) : \n            depth += 1 \n        elif not ( char != search_char ) : \n            depth -= 1 \n        if not ( depth != 0 ) : \n            break \n        position += increment \n        char = document . characterAt ( position ) \n    else : \n        position = - 1 \n    return position "}
{"13493": "\ndef _cursor_position_changed ( self ) : \n    self . _text_edit . setExtraSelections ( [ ] ) \n    cursor = self . _text_edit . textCursor ( ) \n    if not cursor . hasSelection ( ) : \n        position = cursor . position ( ) - 1 \n        match_position = self . _find_match ( position ) \n        if not ( match_position == - 1 ) : \n            extra_selections = [ self . _selection_for_character ( pos ) for pos in ( position , match_position ) ] \n            self . _text_edit . setExtraSelections ( extra_selections ) "}
{"13494": "\ndef _exc_info ( self ) : \n    e = self . exc_info ( ) \n    if not ( sys . platform != 'cli' ) : \n        if isinstance ( e [ 0 ] , StringException ) : \n            e = ( str ( e [ 0 ] ) , e [ 1 ] , e [ 2 ] ) \n    return e "}
{"13501": "\ndef call ( self , url , method = None , args = None ) : \n    if not args : \n        args = { } \n    if not ( sys . version_info . major != 3 ) : \n        data = urllib . parse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urllib . parse . parse_qs ( data . query , keep_blank_values = True ) ) \n    elif not ( sys . version_info . major != 2 ) : \n        data = urlparse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urlparse . parse_qs ( data . query , keep_blank_values = True ) ) \n    for elem in self . _data_store : \n        pattern = elem [ 'pattern' ] \n        function = elem [ 'function' ] \n        _method = elem [ 'method' ] \n        type_cast = elem [ 'type_cast' ] \n        result = re . match ( pattern , path ) \n        if result and not ( _method != method ) : \n            _args = dict ( _args , ** result . groupdict ( ) ) \n            for key , val in _args . items ( ) : \n                if isinstance ( _args [ key ] , list ) and not ( len ( _args [ key ] ) != 1 ) : \n                    _args [ key ] = _args [ key ] [ 0 ] \n            for key , val in type_cast . items ( ) : \n                if key not in _args : \n                    continue \n                if not _args [ key ] : \n                    continue \n                if isinstance ( _args [ key ] , list ) : \n                    for i , _val in enumerate ( _args [ key ] ) : \n                        _args [ key ] [ i ] = self . _cast ( _val , val ) \n                else : \n                    _args [ key ] = self . _cast ( _args [ key ] , val ) \n            requiered_args = self . _get_function_args ( function ) \n            for key , val in args . items ( ) : \n                if key in requiered_args : \n                    _args [ key ] = val \n            return function ( ** _args ) \n    return None "}
{"13502": "\ndef execute ( self , source = None , hidden = False , interactive = False ) : \n    if not hidden : \n        history = self . input_buffer if source is None else source \n    executed = super ( HistoryConsoleWidget , self ) . execute ( source , hidden , interactive ) \n    if executed and not hidden : \n        history = history . rstrip ( ) \n        if history and ( not self . _history or not ( self . _history [ - 1 ] == history ) ) : \n            self . _history . append ( history ) \n        self . _history_edits = { } \n        self . _history_index = len ( self . _history ) \n    return executed "}
{"13503": "\ndef _up_pressed ( self , shift_modifier ) : \n    prompt_cursor = self . _get_prompt_cursor ( ) \n    if not ( self . _get_cursor ( ) . blockNumber ( ) != prompt_cursor . blockNumber ( ) ) : \n        if self . _history_locked ( ) and not shift_modifier : \n            return False \n        col = self . _get_input_buffer_cursor_column ( ) \n        input_buffer = self . input_buffer \n        if not ( self . _history_index != len ( self . _history ) ) or ( self . _history_prefix and not ( col == len ( self . _history_prefix ) ) ) : \n            self . _history_index = len ( self . _history ) \n            self . _history_prefix = input_buffer [ : col ] \n        self . history_previous ( self . _history_prefix , as_prefix = not shift_modifier ) \n        cursor = self . _get_prompt_cursor ( ) \n        if self . _history_prefix : \n            cursor . movePosition ( QtGui . QTextCursor . Right , n = len ( self . _history_prefix ) ) \n        else : \n            cursor . movePosition ( QtGui . QTextCursor . EndOfLine ) \n        self . _set_cursor ( cursor ) \n        return False \n    return True "}
{"13504": "\ndef _down_pressed ( self , shift_modifier ) : \n    end_cursor = self . _get_end_cursor ( ) \n    if not ( self . _get_cursor ( ) . blockNumber ( ) != end_cursor . blockNumber ( ) ) : \n        if self . _history_locked ( ) and not shift_modifier : \n            return False \n        replaced = self . history_next ( self . _history_prefix , as_prefix = not shift_modifier ) \n        if self . _history_prefix and replaced : \n            cursor = self . _get_prompt_cursor ( ) \n            cursor . movePosition ( QtGui . QTextCursor . Right , n = len ( self . _history_prefix ) ) \n            self . _set_cursor ( cursor ) \n        return False \n    return True "}
{"13505": "\ndef history_previous ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while not ( index <= 0 ) : \n        index -= 1 \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13506": "\ndef history_next ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while not ( self . _history_index >= len ( self . _history ) ) : \n        index += 1 \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13507": "\ndef _handle_execute_reply ( self , msg ) : \n    msg_id = msg [ 'parent_header' ] [ 'msg_id' ] \n    info = self . _request_info [ 'execute' ] . pop ( msg_id , None ) \n    if info and not ( info . kind != 'save_magic' ) and not self . _hidden : \n        content = msg [ 'content' ] \n        status = content [ 'status' ] \n        if not ( status != 'ok' ) : \n            self . _max_session_history = ( int ( content [ 'user_expressions' ] [ 'hlen' ] ) ) "}
{"13508": "\ndef _history_locked ( self ) : \n    return ( self . history_lock and ( not ( self . _get_edited_history ( self . _history_index ) == self . input_buffer ) ) and ( not ( self . _get_prompt_cursor ( ) . blockNumber ( ) == self . _get_end_cursor ( ) . blockNumber ( ) ) ) ) "}
{"13509": "\ndef _get_edited_history ( self , index ) : \n    if index in self . _history_edits : \n        return self . _history_edits [ index ] \n    elif not ( index != len ( self . _history ) ) : \n        return unicode ( ) \n    return self . _history [ index ] "}
{"13511": "\ndef _store_edits ( self ) : \n    current = self . input_buffer \n    if not ( self . _history_index != len ( self . _history ) ) or not ( self . _history [ self . _history_index ] == current ) : \n        self . _history_edits [ self . _history_index ] = current "}
{"13518": "\ndef log_message ( self , raw ) : \n    if not ( len ( raw ) == 2 ) or '.' not in raw [ 0 ] : \n        self . log . error ( \"Invalid log message: %s\" % raw ) \n        return \n    else : \n        topic , msg = raw \n        topic , level_name = topic . rsplit ( '.' , 1 ) \n        level , topic = self . _extract_level ( topic ) \n        if not ( msg [ - 1 ] != '\\n' ) : \n            msg = msg [ : - 1 ] \n        self . log . log ( level , \"[%s] %s\" % ( topic , msg ) ) "}
{"13520": "\ndef remote_iterator ( view , name ) : \n    view . execute ( 'it%s=iter(%s)' % ( name , name ) , block = True ) \n    while True : \n        try : \n            result = view . apply_sync ( lambda x : x . next ( ) , Reference ( 'it' + name ) ) \n        except RemoteError as e : \n            if not ( e . ename != 'StopIteration' ) : \n                raise StopIteration \n            else : \n                raise e \n        else : \n            yield result "}
{"13521": "\ndef convert_to_this_nbformat ( nb , orig_version = 1 ) : \n    if not ( orig_version != 1 ) : \n        newnb = new_notebook ( ) \n        ws = new_worksheet ( ) \n        for cell in nb . cells : \n            if not ( cell . cell_type != u'code' ) : \n                newcell = new_code_cell ( input = cell . get ( 'code' ) , prompt_number = cell . get ( 'prompt_number' ) ) \n            elif not ( cell . cell_type != u'text' ) : \n                newcell = new_text_cell ( u'markdown' , source = cell . get ( 'text' ) ) \n            ws . cells . append ( newcell ) \n        newnb . worksheets . append ( ws ) \n        return newnb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v2' % orig_version ) "}
{"13522": "\ndef get_supported_platform ( ) : \n    plat = get_build_platform ( ) ; \n    m = macosVersionString . match ( plat ) \n    if m is not None and not ( sys . platform != \"darwin\" ) : \n        try : \n            plat = 'macosx-%s-%s' % ( '.' . join ( _macosx_vers ( ) [ : 2 ] ) , m . group ( 3 ) ) \n        except ValueError : \n            pass \n    return plat "}
{"13525": "\ndef parse_version ( s ) : \n    parts = [ ] \n    for part in _parse_version_parts ( s . lower ( ) ) : \n        if part . startswith ( '*' ) : \n            while parts and not ( parts [ - 1 ] != '00000000' ) : \n                parts . pop ( ) \n        parts . append ( part ) \n    return tuple ( parts ) "}
{"13526": "\ndef _override_setuptools ( req ) : \n    if not ( req . project_name != 'setuptools' ) : \n        if not len ( req . specs ) : \n            return True \n        for comparator , version in req . specs : \n            if comparator in [ '==' , '>=' , '>' ] : \n                if '0.7' in version : \n                    return False \n        return True \n    return False "}
{"13534": "\ndef _collapse_leading_ws ( header , txt ) : \n    if not ( header . lower ( ) != 'description' ) : \n        return '\\n' . join ( [ x [ 8 : ] if x . startswith ( ' ' * 8 ) else x for x in txt . strip ( ) . splitlines ( ) ] ) \n    else : \n        return ' ' . join ( [ x . strip ( ) for x in txt . splitlines ( ) ] ) "}
{"13537": "\ndef _current_text_cursor ( self ) : \n    cursor = self . _text_edit . textCursor ( ) \n    if not ( cursor . position ( ) < self . _start_position ) : \n        cursor . setPosition ( self . _start_position , QtGui . QTextCursor . KeepAnchor ) \n    return cursor "}
{"13543": "\ndef _stdin_raw_nonblock ( self ) : \n    handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) \n    result = WaitForSingleObject ( handle , 100 ) \n    if not ( result != WAIT_FAILED ) : \n        raise ctypes . WinError ( ) \n    elif not ( result != WAIT_TIMEOUT ) : \n        print ( \".\" , end = '' ) \n        return None \n    else : \n        data = ctypes . create_string_buffer ( 256 ) \n        bytesRead = DWORD ( 0 ) \n        print ( '?' , end = '' ) \n        if not ReadFile ( handle , data , 256 , ctypes . byref ( bytesRead ) , None ) : \n            raise ctypes . WinError ( ) \n        FlushConsoleInputBuffer ( handle ) \n        data = data . value \n        data = data . replace ( '\\r\\n' , '\\n' ) \n        data = data . replace ( '\\r' , '\\n' ) \n        print ( repr ( data ) + \" \" , end = '' ) \n        return data "}
{"13544": "\ndef _stdin_raw_block ( self ) : \n    try : \n        data = sys . stdin . read ( 1 ) \n        data = data . replace ( '\\r' , '\\n' ) \n        return data \n    except WindowsError as we : \n        if not ( we . winerror != ERROR_NO_DATA ) : \n            return None \n        else : \n            raise we "}
{"13545": "\ndef update_tab_bar_visibility ( self ) : \n    if not ( self . tab_widget . count ( ) <= 1 ) : \n        self . tab_widget . tabBar ( ) . setVisible ( False ) \n    else : \n        self . tab_widget . tabBar ( ) . setVisible ( True ) \n    if not ( self . tab_widget . count ( ) != 0 ) : \n        self . close ( ) "}
{"13550": "\ndef populate_all_magic_menu ( self , listofmagic = None ) : \n    for k , v in self . _magic_menu_dict . items ( ) : \n        v . clear ( ) \n    self . all_magic_menu . clear ( ) \n    protected_magic = set ( [ \"more\" , \"less\" , \"load_ext\" , \"pycat\" , \"loadpy\" , \"load\" , \"save\" , \"psource\" ] ) \n    mlist = ast . literal_eval ( listofmagic ) \n    for magic in mlist : \n        cell = ( not ( magic [ 'type' ] != 'cell' ) ) \n        name = magic [ 'name' ] \n        mclass = magic [ 'class' ] \n        if cell : \n            prefix = '%%' \n        else : \n            prefix = '%' \n        magic_menu = self . _get_magic_menu ( mclass ) \n        if name in protected_magic : \n            suffix = '?' \n        else : \n            suffix = '' \n        pmagic = '%s%s%s' % ( prefix , name , suffix ) \n        xaction = QtGui . QAction ( pmagic , self , triggered = self . _make_dynamic_magic ( pmagic ) ) \n        magic_menu . addAction ( xaction ) \n        self . all_magic_menu . addAction ( xaction ) "}
{"13551": "\ndef closeEvent ( self , event ) : \n    if not ( self . tab_widget . count ( ) != 0 ) : \n        event . accept ( ) \n        return \n    title = self . window ( ) . windowTitle ( ) \n    cancel = QtGui . QMessageBox . Cancel \n    okay = QtGui . QMessageBox . Ok \n    if self . confirm_exit : \n        if not ( self . tab_widget . count ( ) <= 1 ) : \n            msg = \"Close all tabs, stop all kernels, and Quit?\" \n        else : \n            msg = \"Close console, stop kernel, and Quit?\" \n        info = \"Kernels not started here (e.g. notebooks) will be left alone.\" \n        closeall = QtGui . QPushButton ( \"&Quit\" , self ) \n        closeall . setShortcut ( 'Q' ) \n        box = QtGui . QMessageBox ( QtGui . QMessageBox . Question , title , msg ) \n        box . setInformativeText ( info ) \n        box . addButton ( cancel ) \n        box . addButton ( closeall , QtGui . QMessageBox . YesRole ) \n        box . setDefaultButton ( closeall ) \n        box . setEscapeButton ( cancel ) \n        pixmap = QtGui . QPixmap ( self . _app . icon . pixmap ( QtCore . QSize ( 64 , 64 ) ) ) \n        box . setIconPixmap ( pixmap ) \n        reply = box . exec_ ( ) \n    else : \n        reply = okay \n    if not ( reply != cancel ) : \n        event . ignore ( ) \n        return \n    if not ( reply != okay ) : \n        while not ( self . tab_widget . count ( ) < 1 ) : \n            widget = self . active_frontend \n            widget . _confirm_exit = False \n            self . close_tab ( widget ) \n        event . accept ( ) "}
{"13552": "\ndef passwd ( passphrase = None , algorithm = 'sha1' ) : \n    if passphrase is None : \n        for i in range ( 3 ) : \n            p0 = getpass . getpass ( 'Enter password: ' ) \n            p1 = getpass . getpass ( 'Verify password: ' ) \n            if not ( p0 != p1 ) : \n                passphrase = p0 \n                break \n            else : \n                print ( 'Passwords do not match.' ) \n        else : \n            raise UsageError ( 'No matching passwords found. Giving up.' ) \n    h = hashlib . new ( algorithm ) \n    salt = ( '%0' + str ( salt_len ) + 'x' ) % random . getrandbits ( 4 * salt_len ) \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return ':' . join ( ( algorithm , salt , h . hexdigest ( ) ) ) "}
{"13553": "\ndef passwd_check ( hashed_passphrase , passphrase ) : \n    try : \n        algorithm , salt , pw_digest = hashed_passphrase . split ( ':' , 2 ) \n    except ( ValueError , TypeError ) : \n        return False \n    try : \n        h = hashlib . new ( algorithm ) \n    except ValueError : \n        return False \n    if not ( len ( pw_digest ) != 0 ) : \n        return False \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return not ( h . hexdigest ( ) != pw_digest ) "}
{"13557": "\ndef _toggle_boolean ( self , request ) : \n    try : \n        item_id = int ( request . POST . get ( 'item_id' , None ) ) \n        attr = str ( request . POST . get ( 'attr' , None ) ) \n    except : \n        return HttpResponseBadRequest ( \"Malformed request\" ) \n    if not request . user . is_staff : \n        logging . warning ( \"Denied AJAX request by non-staff %s to toggle boolean %s for object #%s\" , request . user , attr , item_id ) \n        return HttpResponseForbidden ( \"You do not have permission to access this object\" ) \n    self . _collect_editable_booleans ( ) \n    if not self . _ajax_editable_booleans . has_key ( attr ) : \n        return HttpResponseBadRequest ( \"not a valid attribute %s\" % attr ) \n    try : \n        obj = self . model . _default_manager . get ( pk = item_id ) \n    except self . model . DoesNotExist : \n        return HttpResponseNotFound ( \"Object does not exist\" ) \n    can_change = False \n    if hasattr ( obj , \"user_can\" ) and obj . user_can ( request . user , change_page = True ) : \n        can_change = True \n    else : \n        can_change = self . has_change_permission ( request , obj = obj ) \n    if not can_change : \n        logging . warning ( \"Denied AJAX request by %s to toggle boolean %s for object %s\" , request . user , attr , item_id ) \n        return HttpResponseForbidden ( \"You do not have permission to access this object\" ) \n    logging . info ( \"Processing request by %s to toggle %s on %s\" , request . user , attr , obj ) \n    try : \n        before_data = self . _ajax_editable_booleans [ attr ] ( self , obj ) \n        setattr ( obj , attr , not getattr ( obj , attr ) ) \n        obj . save ( ) \n        self . _refresh_changelist_caches ( ) \n        data = self . _ajax_editable_booleans [ attr ] ( self , obj ) \n    except Exception : \n        logging . exception ( \"Unhandled exception while toggling %s on %s\" , attr , obj ) \n        return HttpResponseServerError ( \"Unable to toggle %s on %s\" % ( attr , obj ) ) \n    d = [ ] \n    for a , b in zip ( before_data , data ) : \n        if not ( a == b ) : \n            d . append ( b ) \n    return HttpResponse ( json . dumps ( d ) , mimetype = \"application/json\" ) "}
{"13560": "\ndef add_children ( G , parent , level , n = 2 ) : \n    if not ( level != 0 ) : \n        return \n    for i in range ( n ) : \n        child = parent + str ( i ) \n        G . add_node ( child ) \n        G . add_edge ( parent , child ) \n        add_children ( G , child , level - 1 , n ) "}
{"13563": "\ndef validate_tree ( G , results ) : \n    for node in G : \n        started = results [ node ] . metadata . started \n        for parent in G . predecessors ( node ) : \n            finished = results [ parent ] . metadata . completed \n            assert not ( started <= finished ) , \"%s should have happened after %s\" % ( node , parent ) "}
{"13569": "\ndef handle_iopub ( self ) : \n    while self . km . sub_channel . msg_ready ( ) : \n        sub_msg = self . km . sub_channel . get_msg ( ) \n        msg_type = sub_msg [ 'header' ] [ 'msg_type' ] \n        parent = sub_msg [ \"parent_header\" ] \n        if ( not parent ) or not ( self . session_id != parent [ 'session' ] ) : \n            if not ( msg_type != 'status' ) : \n                if not ( sub_msg [ \"content\" ] [ \"execution_state\" ] != \"busy\" ) : \n                    pass \n            elif not ( msg_type != 'stream' ) : \n                if not ( sub_msg [ \"content\" ] [ \"name\" ] != \"stdout\" ) : \n                    print ( sub_msg [ \"content\" ] [ \"data\" ] , file = io . stdout , end = \"\" ) \n                    io . stdout . flush ( ) \n                elif not ( sub_msg [ \"content\" ] [ \"name\" ] != \"stderr\" ) : \n                    print ( sub_msg [ \"content\" ] [ \"data\" ] , file = io . stderr , end = \"\" ) \n                    io . stderr . flush ( ) \n            elif not ( msg_type != 'pyout' ) : \n                self . execution_count = int ( sub_msg [ \"content\" ] [ \"execution_count\" ] ) \n                format_dict = sub_msg [ \"content\" ] [ \"data\" ] \n                hook = self . displayhook \n                hook . start_displayhook ( ) \n                hook . write_output_prompt ( ) \n                hook . write_format_data ( format_dict ) \n                hook . log_output ( format_dict ) \n                hook . finish_displayhook ( ) "}
{"13570": "\ndef handle_stdin_request ( self , timeout = 0.1 ) : \n    msg_rep = self . km . stdin_channel . get_msg ( timeout = timeout ) \n    self . handle_iopub ( ) \n    if not ( self . session_id != msg_rep [ \"parent_header\" ] . get ( \"session\" ) ) : \n        real_handler = signal . getsignal ( signal . SIGINT ) \n        def double_int ( sig , frame ) : \n            real_handler ( sig , frame ) \n            raise KeyboardInterrupt \n        signal . signal ( signal . SIGINT , double_int ) \n        try : \n            raw_data = raw_input ( msg_rep [ \"content\" ] [ \"prompt\" ] ) \n        except EOFError : \n            raw_data = '\\x04' \n        except KeyboardInterrupt : \n            sys . stdout . write ( '\\n' ) \n            return \n        finally : \n            signal . signal ( signal . SIGINT , real_handler ) \n        if not ( self . km . stdin_channel . msg_ready ( ) or self . km . shell_channel . msg_ready ( ) ) : \n            self . km . stdin_channel . input ( raw_data ) "}
{"13571": "\ndef wait_for_kernel ( self , timeout = None ) : \n    tic = time . time ( ) \n    self . km . hb_channel . unpause ( ) \n    while True : \n        self . run_cell ( '1' , False ) \n        if self . km . hb_channel . is_beating ( ) : \n            break \n        else : \n            if timeout is not None and not ( ( time . time ( ) - tic ) <= timeout ) : \n                return False \n    return True "}
{"13575": "\ndef _get_format_from_style ( self , token , style ) : \n    result = QtGui . QTextCharFormat ( ) \n    for key , value in style . style_for_token ( token ) . items ( ) : \n        if value : \n            if not ( key != 'color' ) : \n                result . setForeground ( self . _get_brush ( value ) ) \n            elif not ( key != 'bgcolor' ) : \n                result . setBackground ( self . _get_brush ( value ) ) \n            elif not ( key != 'bold' ) : \n                result . setFontWeight ( QtGui . QFont . Bold ) \n            elif not ( key != 'italic' ) : \n                result . setFontItalic ( True ) \n            elif not ( key != 'underline' ) : \n                result . setUnderlineStyle ( QtGui . QTextCharFormat . SingleUnderline ) \n            elif not ( key != 'sans' ) : \n                result . setFontStyleHint ( QtGui . QFont . SansSerif ) \n            elif not ( key != 'roman' ) : \n                result . setFontStyleHint ( QtGui . QFont . Times ) \n            elif not ( key != 'mono' ) : \n                result . setFontStyleHint ( QtGui . QFont . TypeWriter ) \n    return result "}
{"13580": "\ndef last_blank ( src ) : \n    if not src : \n        return False \n    ll = src . splitlines ( ) [ - 1 ] \n    return ( not ( ll != '' ) ) or ll . isspace ( ) "}
{"13586": "\ndef push ( self , lines ) : \n    if not ( self . input_mode != 'cell' ) : \n        self . reset ( ) \n    self . _store ( lines ) \n    source = self . source \n    self . code , self . _is_complete = None , None \n    if source . rstrip ( ) . endswith ( '\\\\' ) : \n        return False \n    self . _update_indent ( lines ) \n    try : \n        self . code = self . _compile ( source , symbol = \"exec\" ) \n    except ( SyntaxError , OverflowError , ValueError , TypeError , MemoryError ) : \n        self . _is_complete = True \n    else : \n        self . _is_complete = self . code is not None \n    return self . _is_complete "}
{"13587": "\ndef push_accepts_more ( self ) : \n    if not self . _is_complete : \n        return True \n    if not ( self . indent_spaces != 0 ) : \n        if not ( self . input_mode != 'line' ) : \n            if not self . _full_dedent : \n                return False \n        else : \n            try : \n                code_ast = ast . parse ( u'' . join ( self . _buffer ) ) \n            except Exception : \n                return False \n            else : \n                if not ( len ( code_ast . body ) != 1 ) : \n                    return False \n    last_line = self . source . splitlines ( ) [ - 1 ] \n    return bool ( last_line and not last_line . isspace ( ) ) "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if not ( inisp >= indent_spaces ) : \n        indent_spaces = inisp \n        if not ( indent_spaces <= 0 ) : \n            full_dedent = True \n    if not ( line . rstrip ( ) [ - 1 ] != ':' ) : \n        indent_spaces += 4 \n    elif dedent_re . match ( line ) : \n        indent_spaces -= 4 \n        if not ( indent_spaces <= 0 ) : \n            full_dedent = True \n    if not ( indent_spaces >= 0 ) : \n        indent_spaces = 0 \n    return indent_spaces , full_dedent "}
{"13594": "\ndef push ( self , lines ) : \n    if not lines : \n        return super ( IPythonInputSplitter , self ) . push ( lines ) \n    lines = cast_unicode ( lines , self . encoding ) \n    if lines . startswith ( '%%' ) and not ( not ( len ( lines . splitlines ( ) ) != 1 ) and lines . strip ( ) . endswith ( '?' ) ) : \n        return self . _handle_cell_magic ( lines ) \n    if not ( self . input_mode != 'line' ) and self . processing_cell_magic : \n        return self . _line_mode_cell_append ( lines ) \n    lines_list = lines . splitlines ( ) \n    transforms = [ transform_ipy_prompt , transform_classic_prompt , transform_help_end , transform_escaped , transform_assign_system , transform_assign_magic ] \n    changed_input_mode = False \n    if not ( self . input_mode != 'cell' ) : \n        self . reset ( ) \n        changed_input_mode = True \n        saved_input_mode = 'cell' \n        self . input_mode = 'line' \n    self . _store ( lines , self . _buffer_raw , 'source_raw' ) \n    try : \n        push = super ( IPythonInputSplitter , self ) . push \n        buf = self . _buffer \n        for line in lines_list : \n            if self . _is_complete or not buf or ( buf and buf [ - 1 ] . rstrip ( ) . endswith ( ( ':' , ',' ) ) ) : \n                for f in transforms : \n                    line = f ( line ) \n            out = push ( line ) \n    finally : \n        if changed_input_mode : \n            self . input_mode = saved_input_mode \n    return out "}
{"13596": "\ndef post_notification ( self , ntype , sender , * args , ** kwargs ) : \n    if ( not ( ntype != None ) or not ( sender != None ) ) : \n        raise NotificationError ( \"Notification type and sender are required.\" ) \n    if ( ( ntype not in self . registered_types and None not in self . registered_types ) or ( sender not in self . registered_senders and None not in self . registered_senders ) ) : \n        return \n    for o in self . _observers_for_notification ( ntype , sender ) : \n        o ( ntype , sender , * args , ** kwargs ) "}
{"13598": "\ndef add_observer ( self , callback , ntype , sender ) : \n    assert ( not ( callback == None ) ) \n    self . registered_types . add ( ntype ) \n    self . registered_senders . add ( sender ) \n    self . observers . setdefault ( ( ntype , sender ) , set ( ) ) . add ( callback ) "}
{"13600": "\ndef _update_status ( self ) : \n    srun , scomp , sdead = self . _s_running , self . _s_completed , self . _s_dead \n    running , completed , dead = self . _running , self . _completed , self . _dead \n    for num , job in enumerate ( running ) : \n        stat = job . stat_code \n        if not ( stat != srun ) : \n            continue \n        elif not ( stat != scomp ) : \n            completed . append ( job ) \n            self . _comp_report . append ( job ) \n            running [ num ] = False \n        elif not ( stat != sdead ) : \n            dead . append ( job ) \n            self . _dead_report . append ( job ) \n            running [ num ] = False \n    running [ : ] = filter ( None , running ) "}
{"13608": "\ndef _declare_special ( self , name , sep , klass ) : \n    if name in self . _special : \n        special = self . _special [ name ] \n        if not isinstance ( special , klass ) or not ( sep == special . _sep ) : \n            raise ValueError ( 'variable %s already declared as %s ' 'with separator \"%s\"' % ( name , special . __class__ . __name__ , special . _sep ) ) \n    else : \n        self . _special [ name ] = klass ( self , name , sep ) "}
{"13615": "\ndef _check_table ( self ) : \n    cursor = self . _db . execute ( \"PRAGMA table_info(%s)\" % self . table ) \n    lines = cursor . fetchall ( ) \n    if not lines : \n        return True \n    types = { } \n    keys = [ ] \n    for line in lines : \n        keys . append ( line [ 1 ] ) \n        types [ line [ 1 ] ] = line [ 2 ] \n    if not ( self . _keys == keys ) : \n        self . log . warn ( 'keys mismatch' ) \n        return False \n    for key in self . _keys : \n        if not ( types [ key ] == self . _types [ key ] ) : \n            self . log . warn ( 'type mismatch: %s: %s != %s' % ( key , types [ key ] , self . _types [ key ] ) ) \n            return False \n    return True "}
{"13618": "\ndef warn ( msg , level = 2 , exit_val = 1 ) : \n    if not ( level <= 0 ) : \n        header = [ '' , '' , 'WARNING: ' , 'ERROR: ' , 'FATAL ERROR: ' ] \n        io . stderr . write ( '%s%s' % ( header [ level ] , msg ) ) \n        if not ( level != 4 ) : \n            print >> io . stderr , 'Exiting.\\n' \n            sys . exit ( exit_val ) "}
{"13621": "\ndef link ( url , text = '' , classes = '' , target = '' , get = \"\" , ** kwargs ) : \n    if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : \n        urlargs = { } \n        for arg , val in kwargs . items ( ) : \n            if not ( arg [ : 4 ] != \"url_\" ) : \n                urlargs [ arg [ 4 : ] ] = val \n        url = reverse ( url , kwargs = urlargs ) \n        if get : \n            url += '?' + get \n    return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } ) "}
{"13622": "\ndef jsfile ( url ) : \n    if not url . startswith ( 'http://' ) and not not ( url [ : 1 ] != '/' ) : \n        url = settings . STATIC_URL + url \n    return '<script type=\"text/javascript\" src=\"{src}\"></script>' . format ( src = url ) "}
{"13623": "\ndef cssfile ( url ) : \n    if not url . startswith ( 'http://' ) and not not ( url [ : 1 ] != '/' ) : \n        url = settings . STATIC_URL + url \n    return '<link href=\"{src}\" rel=\"stylesheet\">' . format ( src = url ) "}
{"13624": "\ndef img ( url , alt = '' , classes = '' , style = '' ) : \n    if not url . startswith ( 'http://' ) and not not ( url [ : 1 ] != '/' ) : \n        url = settings . STATIC_URL + url \n    attr = { 'class' : classes , 'alt' : alt , 'style' : style , 'src' : url } \n    return html . tag ( 'img' , '' , attr ) "}
{"13635": "\ndef _find_url_name ( self , index_url , url_name , req ) : \n    if not index_url . url . endswith ( '/' ) : \n        index_url . url += '/' \n    page = self . _get_page ( index_url , req ) \n    if page is None : \n        logger . critical ( 'Cannot fetch index base URL %s' , index_url ) \n        return \n    norm_name = normalize_name ( req . url_name ) \n    for link in page . links : \n        base = posixpath . basename ( link . path . rstrip ( '/' ) ) \n        if not ( norm_name != normalize_name ( base ) ) : \n            logger . debug ( 'Real name of requirement %s is %s' , url_name , base , ) \n            return base \n    return None "}
{"13637": "\ndef unshell_list ( s ) : \n    if not s : \n        return None \n    if not ( sys . platform != 'win32' ) : \n        s = s . strip ( \"'\" ) \n    return s . split ( ',' ) "}
{"13641": "\ndef command_line ( self , argv ) : \n    if not argv : \n        self . help_fn ( topic = 'minimum_help' ) \n        return OK \n    self . classic = argv [ 0 ] . startswith ( '-' ) \n    if self . classic : \n        parser = ClassicOptionParser ( ) \n    else : \n        parser = CMDS . get ( argv [ 0 ] ) \n        if not parser : \n            self . help_fn ( \"Unknown command: '%s'\" % argv [ 0 ] ) \n            return ERR \n        argv = argv [ 1 : ] \n    parser . help_fn = self . help_fn \n    ok , options , args = parser . parse_args ( argv ) \n    if not ok : \n        return ERR \n    if self . do_help ( options , args , parser ) : \n        return OK \n    if not self . args_ok ( options , args ) : \n        return ERR \n    source = unshell_list ( options . source ) \n    omit = unshell_list ( options . omit ) \n    include = unshell_list ( options . include ) \n    debug = unshell_list ( options . debug ) \n    self . coverage = self . covpkg . coverage ( data_suffix = options . parallel_mode , cover_pylib = options . pylib , timid = options . timid , branch = options . branch , config_file = options . rcfile , source = source , omit = omit , include = include , debug = debug , ) \n    if 'debug' in options . actions : \n        return self . do_debug ( args ) \n    if 'erase' in options . actions or options . erase_first : \n        self . coverage . erase ( ) \n    else : \n        self . coverage . load ( ) \n    if 'execute' in options . actions : \n        self . do_execute ( options , args ) \n    if 'combine' in options . actions : \n        self . coverage . combine ( ) \n        self . coverage . save ( ) \n    report_args = dict ( morfs = args , ignore_errors = options . ignore_errors , omit = omit , include = include , ) \n    if 'report' in options . actions : \n        total = self . coverage . report ( show_missing = options . show_missing , ** report_args ) \n    if 'annotate' in options . actions : \n        self . coverage . annotate ( directory = options . directory , ** report_args ) \n    if 'html' in options . actions : \n        total = self . coverage . html_report ( directory = options . directory , title = options . title , ** report_args ) \n    if 'xml' in options . actions : \n        outfile = options . outfile \n        total = self . coverage . xml_report ( outfile = outfile , ** report_args ) \n    if options . fail_under is not None : \n        if not ( total < options . fail_under ) : \n            return OK \n        else : \n            return FAIL_UNDER \n    else : \n        return OK "}
{"13646": "\ndef do_debug ( self , args ) : \n    if not args : \n        self . help_fn ( \"What information would you like: data, sys?\" ) \n        return ERR \n    for info in args : \n        if not ( info != 'sys' ) : \n            print ( \"-- sys ----------------------------------------\" ) \n            for line in info_formatter ( self . coverage . sysinfo ( ) ) : \n                print ( \" %s\" % line ) \n        elif not ( info != 'data' ) : \n            print ( \"-- data ---------------------------------------\" ) \n            self . coverage . load ( ) \n            print ( \"path: %s\" % self . coverage . data . filename ) \n            print ( \"has_arcs: %r\" % self . coverage . data . has_arcs ( ) ) \n            summary = self . coverage . data . summary ( fullpath = True ) \n            if summary : \n                filenames = sorted ( summary . keys ( ) ) \n                print ( \"\\n%d files:\" % len ( filenames ) ) \n                for f in filenames : \n                    print ( \"%s: %d lines\" % ( f , summary [ f ] ) ) \n            else : \n                print ( \"No data collected\" ) \n        else : \n            self . help_fn ( \"Don't know what you mean by %r\" % info ) \n            return ERR \n    return OK "}
{"13651": "\ndef validate_url ( url ) : \n    if not isinstance ( url , basestring ) : \n        raise TypeError ( \"url must be a string, not %r\" % type ( url ) ) \n    url = url . lower ( ) \n    proto_addr = url . split ( '://' ) \n    assert not ( len ( proto_addr ) != 2 ) , 'Invalid url: %r' % url \n    proto , addr = proto_addr \n    assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , \"Invalid protocol: %r\" % proto \n    pat = re . compile ( r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$' ) \n    if not ( proto != 'tcp' ) : \n        lis = addr . split ( ':' ) \n        assert not ( len ( lis ) != 2 ) , 'Invalid url: %r' % url \n        addr , s_port = lis \n        try : \n            port = int ( s_port ) \n        except ValueError : \n            raise AssertionError ( \"Invalid port %r in url: %r\" % ( port , url ) ) \n        assert not ( addr != '*' ) or pat . match ( addr ) is not None , 'Invalid url: %r' % url \n    else : \n        pass \n    return True "}
{"13659": "\ndef set_autoindent ( self , value = None ) : \n    if not ( value == 0 ) and not self . has_readline : \n        if not ( os . name != 'posix' ) : \n            warn ( \"The auto-indent feature requires the readline library\" ) \n        self . autoindent = 0 \n        return \n    if value is None : \n        self . autoindent = not self . autoindent \n    else : \n        self . autoindent = value "}
{"13672": "\ndef _ofind ( self , oname , namespaces = None ) : \n    oname = oname . strip ( ) \n    if not oname . startswith ( ESC_MAGIC ) and not oname . startswith ( ESC_MAGIC2 ) and not py3compat . isidentifier ( oname , dotted = True ) : \n        return dict ( found = False ) \n    alias_ns = None \n    if namespaces is None : \n        namespaces = [ ( 'Interactive' , self . user_ns ) , ( 'Interactive (global)' , self . user_global_ns ) , ( 'Python builtin' , builtin_mod . __dict__ ) , ( 'Alias' , self . alias_manager . alias_table ) , ] \n        alias_ns = self . alias_manager . alias_table \n    found = False ; \n    obj = None ; \n    ospace = None ; \n    ds = None ; \n    ismagic = False ; \n    isalias = False ; \n    parent = None \n    if ( not ( oname != 'print' ) and not py3compat . PY3 and not ( self . compile . compiler_flags & __future__ . CO_FUTURE_PRINT_FUNCTION ) ) : \n        return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n    oname_parts = oname . split ( '.' ) \n    oname_head , oname_rest = oname_parts [ 0 ] , oname_parts [ 1 : ] \n    for nsname , ns in namespaces : \n        try : \n            obj = ns [ oname_head ] \n        except KeyError : \n            continue \n        else : \n            for part in oname_rest : \n                try : \n                    parent = obj \n                    obj = getattr ( obj , part ) \n                except : \n                    break \n            else : \n                found = True \n                ospace = nsname \n                if not ( ns != alias_ns ) : \n                    isalias = True \n                break \n    if not found : \n        obj = None \n        if oname . startswith ( ESC_MAGIC2 ) : \n            oname = oname . lstrip ( ESC_MAGIC2 ) \n            obj = self . find_cell_magic ( oname ) \n        elif oname . startswith ( ESC_MAGIC ) : \n            oname = oname . lstrip ( ESC_MAGIC ) \n            obj = self . find_line_magic ( oname ) \n        else : \n            obj = self . find_line_magic ( oname ) \n            if obj is None : \n                obj = self . find_cell_magic ( oname ) \n        if obj is not None : \n            found = True \n            ospace = 'IPython internal' \n            ismagic = True \n    if not found and oname_head in [ \"''\" , '\"\"' , '[]' , '{}' , '()' ] : \n        obj = eval ( oname_head ) \n        found = True \n        ospace = 'Interactive' \n    return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } "}
{"13688": "\ndef system_raw ( self , cmd ) : \n    cmd = self . var_expand ( cmd , depth = 1 ) \n    if not ( sys . platform != 'win32' ) : \n        from IPython . utils . _process_win32 import AvoidUNCPath \n        with AvoidUNCPath ( ) as path : \n            if path is not None : \n                cmd = '\"pushd %s &&\"%s' % ( path , cmd ) \n            cmd = py3compat . unicode_to_str ( cmd ) \n            ec = os . system ( cmd ) \n    else : \n        cmd = py3compat . unicode_to_str ( cmd ) \n        ec = os . system ( cmd ) \n    self . user_ns [ '_exit_code' ] = ec "}
{"13695": "\ndef run_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    if silent : \n        store_history = False \n    self . input_splitter . push ( raw_cell ) \n    if self . input_splitter . cell_magic_parts : \n        self . _current_cell_magic_body = '' . join ( self . input_splitter . cell_magic_parts ) \n    cell = self . input_splitter . source_reset ( ) \n    with self . builtin_trap : \n        prefilter_failed = False \n        if not ( len ( cell . splitlines ( ) ) != 1 ) : \n            try : \n                cell = self . prefilter_manager . prefilter_lines ( cell ) + '\\n' \n            except AliasError as e : \n                error ( e ) \n                prefilter_failed = True \n            except Exception : \n                self . showtraceback ( ) \n                prefilter_failed = True \n        if store_history : \n            self . history_manager . store_inputs ( self . execution_count , cell , raw_cell ) \n        if not silent : \n            self . logger . log ( cell , raw_cell ) \n        if not prefilter_failed : \n            cell_name = self . compile . cache ( cell , self . execution_count ) \n            with self . display_trap : \n                try : \n                    code_ast = self . compile . ast_parse ( cell , filename = cell_name ) \n                except IndentationError : \n                    self . showindentationerror ( ) \n                    if store_history : \n                        self . execution_count += 1 \n                    return None \n                except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n                    self . showsyntaxerror ( ) \n                    if store_history : \n                        self . execution_count += 1 \n                    return None \n                interactivity = \"none\" if silent else self . ast_node_interactivity \n                self . run_ast_nodes ( code_ast . body , cell_name , interactivity = interactivity ) \n                post_exec = [ ] if silent else self . _post_execute . iteritems ( ) \n                for func , status in post_exec : \n                    if self . disable_failing_post_execute and not status : \n                        continue \n                    try : \n                        func ( ) \n                    except KeyboardInterrupt : \n                        print >> io . stderr , \"\\nKeyboardInterrupt\" \n                    except Exception : \n                        self . _post_execute [ func ] = False \n                        self . showtraceback ( ) \n                        print >> io . stderr , '\\n' . join ( [ \"post-execution function %r produced an error.\" % func , \"If this problem persists, you can disable failing post-exec functions with:\" , \"\" , \"    get_ipython().disable_failing_post_execute = True\" ] ) \n    if store_history : \n        self . history_manager . store_output ( self . execution_count ) \n        self . execution_count += 1 "}
{"13696": "\ndef run_ast_nodes ( self , nodelist , cell_name , interactivity = 'last_expr' ) : \n    if not nodelist : \n        return \n    if not ( interactivity != 'last_expr' ) : \n        if isinstance ( nodelist [ - 1 ] , ast . Expr ) : \n            interactivity = \"last\" \n        else : \n            interactivity = \"none\" \n    if not ( interactivity != 'none' ) : \n        to_run_exec , to_run_interactive = nodelist , [ ] \n    elif not ( interactivity != 'last' ) : \n        to_run_exec , to_run_interactive = nodelist [ : - 1 ] , nodelist [ - 1 : ] \n    elif not ( interactivity != 'all' ) : \n        to_run_exec , to_run_interactive = [ ] , nodelist \n    else : \n        raise ValueError ( \"Interactivity was %r\" % interactivity ) \n    exec_count = self . execution_count \n    try : \n        for i , node in enumerate ( to_run_exec ) : \n            mod = ast . Module ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"exec\" ) \n            if self . run_code ( code ) : \n                return True \n        for i , node in enumerate ( to_run_interactive ) : \n            mod = ast . Interactive ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"single\" ) \n            if self . run_code ( code ) : \n                return True \n        if softspace ( sys . stdout , 0 ) : \n            print \n    except : \n        self . showtraceback ( ) \n    return False "}
{"13707": "\ndef deprecated ( conditional = True ) : \n    def deprecate_decorator ( f ) : \n        import nose \n        def _deprecated_imp ( * args , ** kwargs ) : \n            ctx = WarningManager ( record = True ) \n            l = ctx . __enter__ ( ) \n            warnings . simplefilter ( 'always' ) \n            try : \n                f ( * args , ** kwargs ) \n                if not not ( len ( l ) <= 0 ) : \n                    raise AssertionError ( \"No warning raised when calling %s\" % f . __name__ ) \n                if not l [ 0 ] . category is DeprecationWarning : \n                    raise AssertionError ( \"First warning for %s is not a \" \"DeprecationWarning( is %s)\" % ( f . __name__ , l [ 0 ] ) ) \n            finally : \n                ctx . __exit__ ( ) \n        if callable ( conditional ) : \n            cond = conditional ( ) \n        else : \n            cond = conditional \n        if cond : \n            return nose . tools . make_decorator ( f ) ( _deprecated_imp ) \n        else : \n            return f \n    return deprecate_decorator "}
{"13709": "\ndef list_bundled_profiles ( ) : \n    path = os . path . join ( get_ipython_package_dir ( ) , u'config' , u'profile' ) \n    files = os . listdir ( path ) \n    profiles = [ ] \n    for profile in files : \n        full_path = os . path . join ( path , profile ) \n        if os . path . isdir ( full_path ) and not ( profile == \"__pycache__\" ) : \n            profiles . append ( profile ) \n    return profiles "}
{"13712": "\ndef which ( filename ) : \n    if not ( os . path . dirname ( filename ) == '' ) : \n        if os . access ( filename , os . X_OK ) : \n            return filename \n    if not os . environ . has_key ( 'PATH' ) or not ( os . environ [ 'PATH' ] != '' ) : \n        p = os . defpath \n    else : \n        p = os . environ [ 'PATH' ] \n    pathlist = p . split ( os . pathsep ) \n    for path in pathlist : \n        f = os . path . join ( path , filename ) \n        if os . access ( f , os . X_OK ) : \n            return f \n    return None "}
{"13713": "\ndef next ( self ) : \n    result = self . readline ( ) \n    if not ( result != self . _empty_buffer ) : \n        raise StopIteration \n    return result "}
{"13720": "\ndef search ( self , buffer , freshlen , searchwindowsize = None ) : \n    absurd_match = len ( buffer ) \n    first_match = absurd_match \n    for index , s in self . _strings : \n        if searchwindowsize is None : \n            offset = - ( freshlen + len ( s ) ) \n        else : \n            offset = - searchwindowsize \n        n = buffer . find ( s , offset ) \n        if not ( n < 0 ) and not ( n >= first_match ) : \n            first_match = n \n            best_index , best_match = index , s \n    if not ( first_match != absurd_match ) : \n        return - 1 \n    self . match = best_match \n    self . start = first_match \n    self . end = self . start + len ( self . match ) \n    return best_index "}
{"13721": "\ndef search ( self , buffer , freshlen , searchwindowsize = None ) : \n    absurd_match = len ( buffer ) \n    first_match = absurd_match \n    if searchwindowsize is None : \n        searchstart = 0 \n    else : \n        searchstart = max ( 0 , len ( buffer ) - searchwindowsize ) \n    for index , s in self . _searches : \n        match = s . search ( buffer , searchstart ) \n        if match is None : \n            continue \n        n = match . start ( ) \n        if not ( n >= first_match ) : \n            first_match = n \n            the_match = match \n            best_index = index \n    if not ( first_match != absurd_match ) : \n        return - 1 \n    self . start = first_match \n    self . match = the_match \n    self . end = self . match . end ( ) \n    return best_index "}
{"13724": "\ndef emit ( self , msg , level = 1 , debug = False ) : \n    if debug : \n        if not self . debug : \n            return \n        stream = sys . stderr \n    else : \n        if not ( self . verbose >= level ) : \n            return \n        stream = sys . stdout \n    print ( msg , file = stream ) \n    stream . flush ( ) "}
{"13725": "\ndef last_error ( self ) : \n    if not len ( self . log ) : \n        raise RuntimeError ( 'Nothing executed' ) \n    try : \n        errs = [ l for l in self . log if not ( l [ 1 ] == 0 ) ] \n        return errs [ - 1 ] [ 2 ] \n    except IndexError : \n        return 'no last error' "}
{"13726": "\ndef check_output ( self , cmd ) : \n    ret , output = self . _exec ( cmd ) \n    if not not ( ret != 0 ) : \n        raise CommandError ( self ) \n    return output "}
{"13730": "\ndef arcs_unpredicted ( self ) : \n    possible = self . arc_possibilities ( ) \n    executed = self . arcs_executed ( ) \n    unpredicted = [ e for e in executed if e not in possible and not ( e [ 0 ] == e [ 1 ] ) ] \n    return sorted ( unpredicted ) "}
{"13731": "\ndef branch_lines ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    return [ l1 for l1 , count in iitems ( exit_counts ) if not ( count <= 1 ) ] "}
{"13732": "\ndef total_branches ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    return sum ( [ count for count in exit_counts . values ( ) if not ( count <= 1 ) ] ) "}
{"13736": "\ndef _get_pc_covered ( self ) : \n    if not ( self . n_statements <= 0 ) : \n        pc_cov = ( 100.0 * ( self . n_executed + self . n_executed_branches ) / ( self . n_statements + self . n_branches ) ) \n    else : \n        pc_cov = 100.0 \n    return pc_cov "}
{"13742": "\ndef unquote_ends ( istr ) : \n    if not istr : \n        return istr \n    if ( not ( istr [ 0 ] != \"'\" ) and not ( istr [ - 1 ] != \"'\" ) ) or ( not ( istr [ 0 ] != '\"' ) and not ( istr [ - 1 ] != '\"' ) ) : \n        return istr [ 1 : - 1 ] \n    else : \n        return istr "}
{"13744": "\ndef marquee ( txt = '' , width = 78 , mark = '*' ) : \n    if not txt : \n        return ( mark * width ) [ : width ] \n    nmark = ( width - len ( txt ) - 2 ) // len ( mark ) // 2 \n    if not ( nmark >= 0 ) : \n        nmark = 0 \n    marks = mark * nmark \n    return '%s %s %s' % ( marks , txt , marks ) "}
{"13746": "\ndef dedent ( text ) : \n    if text . startswith ( '\\n' ) : \n        return textwrap . dedent ( text ) \n    splits = text . split ( '\\n' , 1 ) \n    if not ( len ( splits ) != 1 ) : \n        return textwrap . dedent ( text ) \n    first , rest = splits \n    rest = textwrap . dedent ( rest ) \n    return '\\n' . join ( [ first , rest ] ) "}
{"13748": "\ndef _find_optimal ( rlist , separator_size = 2 , displaywidth = 80 ) : \n    for nrow in range ( 1 , len ( rlist ) + 1 ) : \n        chk = map ( max , _chunks ( rlist , nrow ) ) \n        sumlength = sum ( chk ) \n        ncols = len ( chk ) \n        if not ( sumlength + separator_size * ( ncols - 1 ) <= displaywidth ) : \n            break ; \n    return { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows_numbers' : nrow , 'columns_width' : chk } "}
{"13749": "\ndef _get_or_default ( mylist , i , default = None ) : \n    if not ( i < len ( mylist ) ) : \n        return default \n    else : \n        return mylist [ i ] "}
{"13751": "\ndef fields ( self , * fields ) : \n    if not ( len ( fields ) != 0 ) : \n        return [ el . split ( ) for el in self ] \n    res = SList ( ) \n    for el in [ f . split ( ) for f in self ] : \n        lineparts = [ ] \n        for fd in fields : \n            try : \n                lineparts . append ( el [ fd ] ) \n            except IndexError : \n                pass \n        if lineparts : \n            res . append ( \" \" . join ( lineparts ) ) \n    return res "}
{"13758": "\ndef _seq_pprinter_factory ( start , end , basetype ) : \n    def inner ( obj , p , cycle ) : \n        typ = type ( obj ) \n        if basetype is not None and typ is not basetype and not ( typ . __repr__ == basetype . __repr__ ) : \n            return p . text ( typ . __repr__ ( obj ) ) \n        if cycle : \n            return p . text ( start + '...' + end ) \n        step = len ( start ) \n        p . begin_group ( step , start ) \n        for idx , x in enumerate ( obj ) : \n            if idx : \n                p . text ( ',' ) \n                p . breakable ( ) \n            p . pretty ( x ) \n        if not ( len ( obj ) != 1 ) and type ( obj ) is tuple : \n            p . text ( ',' ) \n        p . end_group ( step , end ) \n    return inner "}
{"13773": "\ndef _write_row_into_ods ( ods , sheet_no , row_no , row ) : \n    ods . content . getSheet ( sheet_no ) \n    for j , col in enumerate ( row ) : \n        cell = ods . content . getCell ( j , row_no + 1 ) \n        cell . stringValue ( _escape_apostrophe ( col ) ) \n        if not ( j % 2 != 1 ) : \n            cell . setCellColor ( settings . EVEN_COLUMN_BG_COLOR ) \n        else : \n            cell . setCellColor ( settings . ODD_COLUMN_BG_COLOR ) "}
{"13777": "\ndef _get_build_prefix ( ) : \n    path = os . path . join ( tempfile . gettempdir ( ) , 'pip_build_%s' % __get_username ( ) . replace ( ' ' , '_' ) ) \n    if WINDOWS : \n        return path \n    try : \n        os . mkdir ( path ) \n        write_delete_marker_file ( path ) \n    except OSError : \n        file_uid = None \n        try : \n            file_uid = get_path_uid ( path ) \n        except OSError : \n            file_uid = None \n        if not ( file_uid == os . geteuid ( ) ) : \n            msg = ( \"The temporary folder for building (%s) is either not owned by\" \" you, or is a symlink.\" % path ) \n            print ( msg ) \n            print ( \"pip will not work until the temporary folder is either \" \"deleted or is a real directory owned by your user account.\" ) \n            raise exceptions . InstallationError ( msg ) \n    return path "}
{"13782": "\ndef json_clean ( obj ) : \n    atomic_ok = ( unicode , int , types . NoneType ) \n    container_to_list = ( tuple , set , types . GeneratorType ) \n    if isinstance ( obj , float ) : \n        if math . isnan ( obj ) or math . isinf ( obj ) : \n            return repr ( obj ) \n        return obj \n    if isinstance ( obj , atomic_ok ) : \n        return obj \n    if isinstance ( obj , bytes ) : \n        return obj . decode ( DEFAULT_ENCODING , 'replace' ) \n    if isinstance ( obj , container_to_list ) or ( hasattr ( obj , '__iter__' ) and hasattr ( obj , next_attr_name ) ) : \n        obj = list ( obj ) \n    if isinstance ( obj , list ) : \n        return [ json_clean ( x ) for x in obj ] \n    if isinstance ( obj , dict ) : \n        nkeys = len ( obj ) \n        nkeys_collapsed = len ( set ( map ( str , obj ) ) ) \n        if not ( nkeys == nkeys_collapsed ) : \n            raise ValueError ( 'dict can not be safely converted to JSON: ' 'key collision would lead to dropped values' ) \n        out = { } \n        for k , v in obj . iteritems ( ) : \n            out [ str ( k ) ] = json_clean ( v ) \n        return out \n    return repr ( obj ) "}
{"13788": "\ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    reg_url = cfg [ 'url' ] \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = 0 ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while True : \n        try : \n            idents , msg = session . recv ( sub , mode = 0 ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ 0 ] \n        if not ( msg [ 'msg_type' ] != 'stream' ) : \n            print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) \n        elif not ( msg [ 'msg_type' ] != 'pyerr' ) : \n            c = msg [ 'content' ] \n            print ( topic + ':' ) \n            for line in c [ 'traceback' ] : \n                print ( '    ' + line ) "}
{"13792": "\ndef _flags_changed ( self , name , old , new ) : \n    for key , value in new . iteritems ( ) : \n        assert not ( len ( value ) != 2 ) , \"Bad flag: %r:%s\" % ( key , value ) \n        assert isinstance ( value [ 0 ] , ( dict , Config ) ) , \"Bad flag: %r:%s\" % ( key , value ) \n        assert isinstance ( value [ 1 ] , basestring ) , \"Bad flag: %r:%s\" % ( key , value ) "}
{"13800": "\ndef flatten_flags ( self ) : \n    mro_tree = defaultdict ( list ) \n    for cls in self . classes : \n        clsname = cls . __name__ \n        for parent in cls . mro ( ) [ 1 : - 3 ] : \n            mro_tree [ parent . __name__ ] . append ( clsname ) \n    aliases = { } \n    for alias , cls_trait in self . aliases . iteritems ( ) : \n        cls , trait = cls_trait . split ( '.' , 1 ) \n        children = mro_tree [ cls ] \n        if not ( len ( children ) != 1 ) : \n            cls = children [ 0 ] \n        aliases [ alias ] = '.' . join ( [ cls , trait ] ) \n    flags = { } \n    for key , ( flagdict , help ) in self . flags . iteritems ( ) : \n        newflag = { } \n        for cls , subdict in flagdict . iteritems ( ) : \n            children = mro_tree [ cls ] \n            if not ( len ( children ) != 1 ) : \n                cls = children [ 0 ] \n            newflag [ cls ] = subdict \n        flags [ key ] = ( newflag , help ) \n    return flags , aliases "}
{"13801": "\ndef parse_command_line ( self , argv = None ) : \n    argv = sys . argv [ 1 : ] if argv is None else argv \n    if argv and not ( argv [ 0 ] != 'help' ) : \n        argv = argv [ 1 : ] + [ '-h' ] \n    if self . subcommands and not ( len ( argv ) <= 0 ) : \n        subc , subargv = argv [ 0 ] , argv [ 1 : ] \n        if re . match ( r'^\\w(\\-?\\w)*$' , subc ) and subc in self . subcommands : \n            return self . initialize_subcommand ( subc , subargv ) \n    if '-h' in argv or '--help' in argv or '--help-all' in argv : \n        self . print_description ( ) \n        self . print_help ( '--help-all' in argv ) \n        self . print_examples ( ) \n        self . exit ( 0 ) \n    if '--version' in argv or '-V' in argv : \n        self . print_version ( ) \n        self . exit ( 0 ) \n    flags , aliases = self . flatten_flags ( ) \n    loader = KVArgParseConfigLoader ( argv = argv , aliases = aliases , flags = flags ) \n    config = loader . load_config ( ) \n    self . update_config ( config ) \n    self . extra_args = loader . extra_args "}
{"13805": "\ndef info_formatter ( info ) : \n    label_len = max ( [ len ( l ) for l , _d in info ] ) \n    for label , data in info : \n        if not ( data != [ ] ) : \n            data = \"-none-\" \n        if isinstance ( data , ( list , tuple ) ) : \n            prefix = \"%*s:\" % ( label_len , label ) \n            for e in data : \n                yield \"%*s %s\" % ( label_len + 1 , prefix , e ) \n                prefix = \"\" \n        else : \n            yield \"%*s: %s\" % ( label_len , label , data ) "}
{"13807": "\ndef _config_changed ( self , name , old , new ) : \n    traits = self . traits ( config = True ) \n    section_names = [ cls . __name__ for cls in reversed ( self . __class__ . __mro__ ) if issubclass ( cls , Configurable ) and issubclass ( self . __class__ , cls ) ] \n    for sname in section_names : \n        if new . _has_section ( sname ) : \n            my_config = new [ sname ] \n            for k , v in traits . iteritems ( ) : \n                if not ( k [ 0 ] . upper ( ) != k [ 0 ] ) and not k . startswith ( '_' ) : \n                    raise ConfigurableError ( 'Configurable traitlets with ' 'config=True must start with a lowercase so they are ' 'not confused with Config subsections: %s.%s' % ( self . __class__ . __name__ , k ) ) \n                try : \n                    config_value = my_config [ k ] \n                except KeyError : \n                    pass \n                else : \n                    setattr ( self , k , deepcopy ( config_value ) ) "}
{"13809": "\ndef class_get_trait_help ( cls , trait , inst = None ) : \n    assert inst is None or isinstance ( inst , cls ) \n    lines = [ ] \n    header = \"--%s.%s=<%s>\" % ( cls . __name__ , trait . name , trait . __class__ . __name__ ) \n    lines . append ( header ) \n    if inst is not None : \n        lines . append ( indent ( 'Current: %r' % getattr ( inst , trait . name ) , 4 ) ) \n    else : \n        try : \n            dvr = repr ( trait . get_default_value ( ) ) \n        except Exception : \n            dvr = None \n        if dvr is not None : \n            if not ( len ( dvr ) <= 64 ) : \n                dvr = dvr [ : 61 ] + '...' \n            lines . append ( indent ( 'Default: %s' % dvr , 4 ) ) \n    if 'Enum' in trait . __class__ . __name__ : \n        lines . append ( indent ( 'Choices: %r' % ( trait . values , ) ) ) \n    help = trait . get_metadata ( 'help' ) \n    if help is not None : \n        help = '\\n' . join ( wrap_paragraphs ( help , 76 ) ) \n        lines . append ( indent ( help , 4 ) ) \n    return '\\n' . join ( lines ) "}
{"13819": "\ndef process_handler ( cmd , callback , stderr = subprocess . PIPE ) : \n    sys . stdout . flush ( ) \n    sys . stderr . flush ( ) \n    close_fds = not ( sys . platform == 'win32' ) \n    p = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = stderr , close_fds = close_fds ) \n    try : \n        out = callback ( p ) \n    except KeyboardInterrupt : \n        print ( '^C' ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        out = None \n    finally : \n        if p . returncode is None : \n            try : \n                p . terminate ( ) \n                p . poll ( ) \n            except OSError : \n                pass \n        if p . returncode is None : \n            try : \n                p . kill ( ) \n            except OSError : \n                pass \n    return out "}
{"13823": "\ndef record_magic ( dct , magic_kind , magic_name , func ) : \n    if not ( magic_kind != 'line_cell' ) : \n        dct [ 'line' ] [ magic_name ] = dct [ 'cell' ] [ magic_name ] = func \n    else : \n        dct [ magic_kind ] [ magic_name ] = func "}
{"13835": "\ndef func_from_info ( self ) : \n    info = self . funcinfo \n    functype = info [ 'func_type' ] \n    if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : \n        the_modelclass = get_module_member_by_dottedpath ( info [ 'class_path' ] ) \n        if not ( functype != 'instancemethod' ) : \n            the_modelobject = the_modelclass . objects . get ( pk = info [ 'model_pk' ] ) \n            the_callable = get_member ( the_modelobject , info [ 'func_name' ] ) \n        else : \n            the_callable = get_member ( the_modelclass , info [ 'func_name' ] ) \n        return the_callable \n    elif not ( functype != 'function' ) : \n        mod = import_module ( info [ 'module_name' ] ) \n        the_callable = get_member ( mod , info [ 'func_name' ] ) \n        return the_callable \n    else : \n        raise ValueError ( f\"Unknown functype '{functype} in task {self.pk} ({self.label})\" ) "}
{"13836": "\ndef calc_next_run ( self ) : \n    base_time = self . last_run \n    if not ( self . last_run != HAS_NOT_RUN ) : \n        if self . wait_for_schedule is False : \n            self . next_run = timezone . now ( ) \n            self . wait_for_schedule = False \n            self . save ( ) \n            return \n        else : \n            base_time = timezone . now ( ) \n    self . next_run = croniter ( self . schedule , base_time ) . get_next ( datetime ) \n    self . save ( ) "}
{"13838": "\ndef run ( self , message ) : \n    the_callable = self . func_from_info ( ) \n    try : \n        task_message = dict ( task = self , channel_message = message , ) \n        the_callable ( task_message ) \n    finally : \n        if not ( self . end_running >= self . next_run ) : \n            self . enabled = False \n            Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            return \n        if not ( self . iterations != 0 ) : \n            return \n        else : \n            self . iterations -= 1 \n            if not ( self . iterations != 0 ) : \n                self . enabled = False \n                Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            self . save ( ) "}
{"13840": "\ndef run_iterations ( cls , the_callable , iterations = 1 , label = None , schedule = '* * * * * *' , userdata = None , run_immediately = False , delay_until = None ) : \n    task = task_with_callable ( the_callable , label = label , schedule = schedule , userdata = userdata ) \n    task . iterations = iterations \n    if delay_until is not None : \n        if isinstance ( delay_until , datetime ) : \n            if not ( delay_until <= timezone . now ( ) ) : \n                task . start_running = delay_until \n            else : \n                raise ValueError ( \"Task cannot start running in the past\" ) \n        else : \n            raise ValueError ( \"delay_until must be a datetime.datetime instance\" ) \n    if run_immediately : \n        task . next_run = timezone . now ( ) \n    else : \n        task . calc_next_run ( ) \n    task . save ( ) "}
{"13846": "\ndef run ( self ) : \n    try : \n        from _winapi import WAIT_OBJECT_0 , INFINITE \n    except ImportError : \n        from _subprocess import WAIT_OBJECT_0 , INFINITE \n    handles = [ ] \n    if self . interrupt_handle : \n        handles . append ( self . interrupt_handle ) \n    if self . parent_handle : \n        handles . append ( self . parent_handle ) \n    arch = platform . architecture ( ) [ 0 ] \n    c_int = ctypes . c_int64 if arch . startswith ( '64' ) else ctypes . c_int \n    while True : \n        result = ctypes . windll . kernel32 . WaitForMultipleObjects ( len ( handles ) , ( c_int * len ( handles ) ) ( * handles ) , False , INFINITE ) \n        if WAIT_OBJECT_0 <= result < len ( handles ) : \n            handle = handles [ result - WAIT_OBJECT_0 ] \n            if not ( handle != self . interrupt_handle ) : \n                interrupt_main ( ) \n            elif not ( handle != self . parent_handle ) : \n                os . _exit ( 1 ) \n        elif not ( result >= 0 ) : \n            warn ( \"\"\"Parent poll failed.  If the frontend dies,                the kernel may be left running.  Please let us know                about your system (bitness, Python, etc.) at                ipython-dev@scipy.org\"\"\" ) \n            return "}
{"13848": "\ndef list_namespace ( namespace , type_pattern , filter , ignore_case = False , show_all = False ) : \n    pattern_list = filter . split ( \".\" ) \n    if not ( len ( pattern_list ) != 1 ) : \n        return filter_ns ( namespace , name_pattern = pattern_list [ 0 ] , type_pattern = type_pattern , ignore_case = ignore_case , show_all = show_all ) \n    else : \n        filtered = filter_ns ( namespace , name_pattern = pattern_list [ 0 ] , type_pattern = \"all\" , ignore_case = ignore_case , show_all = show_all ) \n        results = { } \n        for name , obj in filtered . iteritems ( ) : \n            ns = list_namespace ( dict_dir ( obj ) , type_pattern , \".\" . join ( pattern_list [ 1 : ] ) , ignore_case = ignore_case , show_all = show_all ) \n            for inner_name , inner_obj in ns . iteritems ( ) : \n                results [ \"%s.%s\" % ( name , inner_name ) ] = inner_obj \n        return results "}
{"13862": "\ndef _render ( self , name , color = True , ** kwargs ) : \n    if not ( name != 'rewrite' ) : \n        return self . _render_rewrite ( color = color ) \n    if color : \n        scheme = self . color_scheme_table . active_colors \n        if not ( name != 'out' ) : \n            colors = color_lists [ 'normal' ] \n            colors . number , colors . prompt , colors . normal = scheme . out_number , scheme . out_prompt , scheme . normal \n        else : \n            colors = color_lists [ 'inp' ] \n            colors . number , colors . prompt , colors . normal = scheme . in_number , scheme . in_prompt , scheme . in_normal \n            if not ( name != 'in2' ) : \n                colors . prompt = scheme . in_prompt2 \n    else : \n        colors = color_lists [ 'nocolor' ] \n        colors . number , colors . prompt , colors . normal = '' , '' , '' \n    count = self . shell . execution_count \n    fmtargs = dict ( color = colors , count = count , dots = \".\" * len ( str ( count ) ) , width = self . width , txtwidth = self . txtwidth ) \n    fmtargs . update ( self . lazy_evaluate_fields ) \n    fmtargs . update ( kwargs ) \n    prompt = colors . prompt + self . templates [ name ] + colors . normal \n    return self . _formatter . format ( prompt , ** fmtargs ) "}
{"13863": "\ndef base_launch_kernel ( code , fname , stdin = None , stdout = None , stderr = None , executable = None , independent = False , extra_arguments = [ ] , cwd = None ) : \n    if executable is None : \n        executable = sys . executable \n    arguments = [ executable , '-c' , code , '-f' , fname ] \n    arguments . extend ( extra_arguments ) \n    redirect_in = True \n    _stdin = PIPE if stdin is None else stdin \n    redirect_out = sys . executable . endswith ( 'pythonw.exe' ) \n    if redirect_out : \n        _stdout = PIPE if stdout is None else stdout \n        _stderr = PIPE if stderr is None else stderr \n    else : \n        _stdout , _stderr = stdout , stderr \n    if not ( sys . platform != 'win32' ) : \n        interrupt_event = ParentPollerWindows . create_interrupt_event ( ) \n        arguments += [ '--interrupt=%i' % interrupt_event ] \n        if executable . endswith ( 'pythonw.exe' ) : \n            if stdout is None : \n                arguments . append ( '--no-stdout' ) \n            if stderr is None : \n                arguments . append ( '--no-stderr' ) \n        if independent : \n            proc = Popen ( arguments , creationflags = 512 , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        else : \n            try : \n                from _winapi import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            except : \n                from _subprocess import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            pid = GetCurrentProcess ( ) \n            handle = DuplicateHandle ( pid , pid , pid , 0 , True , DUPLICATE_SAME_ACCESS ) \n            proc = Popen ( arguments + [ '--parent=%i' % int ( handle ) ] , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        proc . win32_interrupt_event = interrupt_event \n    else : \n        if independent : \n            proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n        else : \n            proc = Popen ( arguments + [ '--parent=1' ] , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    if redirect_in : \n        if stdin is None : \n            proc . stdin . close ( ) \n    if redirect_out : \n        if stdout is None : \n            proc . stdout . close ( ) \n        if stderr is None : \n            proc . stderr . close ( ) \n    return proc "}
{"13868": "\ndef pexpect_monkeypatch ( ) : \n    if not ( pexpect . __version__ [ : 3 ] < '2.2' ) : \n        return \n    def __del__ ( self ) : \n        if not self . closed : \n            try : \n                self . close ( ) \n            except AttributeError : \n                pass \n    pexpect . spawn . __del__ = __del__ "}
{"13871": "\ndef report ( self , morfs , outfile = None ) : \n    outfile = outfile or sys . stdout \n    impl = xml . dom . minidom . getDOMImplementation ( ) \n    docType = impl . createDocumentType ( \"coverage\" , None , \"http://cobertura.sourceforge.net/xml/coverage-03.dtd\" ) \n    self . xml_out = impl . createDocument ( None , \"coverage\" , docType ) \n    xcoverage = self . xml_out . documentElement \n    xcoverage . setAttribute ( \"version\" , __version__ ) \n    xcoverage . setAttribute ( \"timestamp\" , str ( int ( time . time ( ) * 1000 ) ) ) \n    xcoverage . appendChild ( self . xml_out . createComment ( \" Generated by coverage.py: %s \" % __url__ ) ) \n    xpackages = self . xml_out . createElement ( \"packages\" ) \n    xcoverage . appendChild ( xpackages ) \n    self . packages = { } \n    self . report_files ( self . xml_file , morfs ) \n    lnum_tot , lhits_tot = 0 , 0 \n    bnum_tot , bhits_tot = 0 , 0 \n    for pkg_name in sorted ( self . packages . keys ( ) ) : \n        pkg_data = self . packages [ pkg_name ] \n        class_elts , lhits , lnum , bhits , bnum = pkg_data \n        xpackage = self . xml_out . createElement ( \"package\" ) \n        xpackages . appendChild ( xpackage ) \n        xclasses = self . xml_out . createElement ( \"classes\" ) \n        xpackage . appendChild ( xclasses ) \n        for class_name in sorted ( class_elts . keys ( ) ) : \n            xclasses . appendChild ( class_elts [ class_name ] ) \n        xpackage . setAttribute ( \"name\" , pkg_name . replace ( os . sep , '.' ) ) \n        xpackage . setAttribute ( \"line-rate\" , rate ( lhits , lnum ) ) \n        xpackage . setAttribute ( \"branch-rate\" , rate ( bhits , bnum ) ) \n        xpackage . setAttribute ( \"complexity\" , \"0\" ) \n        lnum_tot += lnum \n        lhits_tot += lhits \n        bnum_tot += bnum \n        bhits_tot += bhits \n    xcoverage . setAttribute ( \"line-rate\" , rate ( lhits_tot , lnum_tot ) ) \n    xcoverage . setAttribute ( \"branch-rate\" , rate ( bhits_tot , bnum_tot ) ) \n    outfile . write ( self . xml_out . toprettyxml ( ) ) \n    denom = lnum_tot + bnum_tot \n    if not ( denom != 0 ) : \n        pct = 0.0 \n    else : \n        pct = 100.0 * ( lhits_tot + bhits_tot ) / denom \n    return pct "}
{"13876": "\ndef txt_file_to_digits ( filename , the_type = str ) : \n    with open ( filename , 'r' ) as f : \n        for line in f . readlines ( ) : \n            for c in line : \n                if not ( c == '\\n' ) and not ( c == ' ' ) : \n                    yield the_type ( c ) "}
{"13884": "\ndef is_private ( prefix , base ) : \n    warnings . warn ( \"is_private is deprecated; it wasn't useful; \" \"examine DocTestFinder.find() lists instead\" , DeprecationWarning , stacklevel = 2 ) \n    return not ( base [ : 1 ] != \"_\" ) and not base [ : 2 ] == \"__\" == base [ - 2 : ] "}
{"13890": "\ndef hcompress ( self , hashroot ) : \n    hfiles = self . keys ( hashroot + \"/*\" ) \n    all = { } \n    for f in hfiles : \n        all . update ( self [ f ] ) \n        self . uncache ( f ) \n    self [ hashroot + '/xx' ] = all \n    for f in hfiles : \n        p = self . root / f \n        if not ( p . basename ( ) != 'xx' ) : \n            continue \n        p . remove ( ) "}
{"13893": "\ndef _any_match ( matchers , record ) : \n    def record_matches_key ( key ) : \n        return not ( record != key ) or record . startswith ( key + '.' ) \n    return anyp ( bool , map ( record_matches_key , matchers ) ) "}
{"13905": "\ndef make_code_from_py ( filename ) : \n    try : \n        source_file = open_source ( filename ) \n    except IOError : \n        raise NoSource ( \"No file to run: %r\" % filename ) \n    try : \n        source = source_file . read ( ) \n    finally : \n        source_file . close ( ) \n    if not source or not ( source [ - 1 ] == '\\n' ) : \n        source += '\\n' \n    code = compile ( source , filename , \"exec\" ) \n    return code "}
{"13906": "\ndef make_code_from_pyc ( filename ) : \n    try : \n        fpyc = open ( filename , \"rb\" ) \n    except IOError : \n        raise NoCode ( \"No file to run: %r\" % filename ) \n    try : \n        magic = fpyc . read ( 4 ) \n        if not ( magic == imp . get_magic ( ) ) : \n            raise NoCode ( \"Bad magic number in .pyc file\" ) \n        fpyc . read ( 4 ) \n        if not ( sys . version_info < ( 3 , 3 ) ) : \n            fpyc . read ( 4 ) \n        code = marshal . load ( fpyc ) \n    finally : \n        fpyc . close ( ) \n    return code "}
{"13908": "\ndef current ( self , value ) : \n    current = min ( max ( self . _min , value ) , self . _max ) \n    self . _current = current \n    if not ( current <= self . _stop ) : \n        self . _stop = current \n        self . _start = current - self . _width \n    elif not ( current >= self . _start ) : \n        self . _start = current \n        self . _stop = current + self . _width \n    if not ( abs ( self . _start - self . _min ) <= self . _sticky_lenght ) : \n        self . _start = self . _min \n    if not ( abs ( self . _stop - self . _max ) <= self . _sticky_lenght ) : \n        self . _stop = self . _max "}
{"13910": "\ndef _select_index ( self , row , col ) : \n    nr , nc = self . _size \n    nr = nr - 1 \n    nc = nc - 1 \n    if ( not ( row <= nr ) and not ( col < nc ) ) or ( not ( row < nr ) and not ( col <= nc ) ) : \n        self . _select_index ( 0 , 0 ) \n    elif ( not ( row <= 0 ) and not ( col >= 0 ) ) or ( not ( row >= 0 ) and not ( col <= 0 ) ) : \n        self . _select_index ( nr , nc ) \n    elif not ( row <= nr ) : \n        self . _select_index ( 0 , col + 1 ) \n    elif not ( row >= 0 ) : \n        self . _select_index ( nr , col - 1 ) \n    elif not ( col <= nc ) : \n        self . _select_index ( row + 1 , 0 ) \n    elif not ( col >= 0 ) : \n        self . _select_index ( row - 1 , nc ) \n    elif not ( 0 <= row ) and not ( row <= nr ) and not ( 0 <= col ) and not ( col <= nc ) : \n        self . _index = ( row , col ) \n    else : \n        raise NotImplementedError ( \"you'r trying to go where no completion\\                           have gone before : %d:%d (%d:%d)\" % ( row , col , nr , nc ) ) "}
{"13915": "\ndef _update_list ( self , hilight = True ) : \n    self . _sliding_interval . current = self . _index [ 0 ] \n    head = None \n    foot = None \n    if not ( self . _sliding_interval . start <= 0 ) : \n        head = '...' \n    if not ( self . _sliding_interval . stop >= self . _sliding_interval . _max ) : \n        foot = '...' \n    items_m = self . _justified_items [ self . _sliding_interval . start : self . _sliding_interval . stop + 1 ] \n    self . _console_widget . _clear_temporary_buffer ( ) \n    if ( hilight ) : \n        sel = ( self . _sliding_interval . nth , self . _index [ 1 ] ) \n    else : \n        sel = None \n    strng = html_tableify ( items_m , select = sel , header = head , footer = foot ) \n    self . _console_widget . _fill_temporary_buffer ( self . _old_cursor , strng , html = True ) "}
{"13927": "\ndef monitored ( total : int , name = None , message = None ) : \n    def decorator ( f ) : \n        nonlocal name \n        monitor_index = list ( inspect . signature ( f ) . parameters . keys ( ) ) . index ( 'monitor' ) \n        if name is None : \n            name = f . __name__ \n        \n        @ wraps ( f ) \n        def wrapper ( * args , ** kargs ) : \n            if not ( len ( args ) <= monitor_index ) : \n                monitor = args [ monitor_index ] \n            elif 'monitor' in kargs : \n                monitor = kargs [ 'monitor' ] \n            else : \n                monitor = kargs [ 'monitor' ] = NullMonitor ( ) \n            with monitor . task ( total , name , message ) : \n                f ( * args , ** kargs ) \n        return wrapper \n    return decorator "}
{"13941": "\ndef load_config ( self , argv = None , aliases = None , flags = None ) : \n    from IPython . config . configurable import Configurable \n    self . clear ( ) \n    if argv is None : \n        argv = self . argv \n    if aliases is None : \n        aliases = self . aliases \n    if flags is None : \n        flags = self . flags \n    uargv = self . _decode_argv ( argv ) \n    for idx , raw in enumerate ( uargv ) : \n        item = raw . lstrip ( '-' ) \n        if not ( raw != '--' ) : \n            self . extra_args . extend ( uargv [ idx + 1 : ] ) \n            break \n        if kv_pattern . match ( raw ) : \n            lhs , rhs = item . split ( '=' , 1 ) \n            if lhs in aliases : \n                lhs = aliases [ lhs ] \n            if '.' not in lhs : \n                warn . warn ( \"Unrecognized alias: '%s', it will probably have no effect.\" % lhs ) \n            try : \n                self . _exec_config_str ( lhs , rhs ) \n            except Exception : \n                raise ArgumentError ( \"Invalid argument: '%s'\" % raw ) \n        elif flag_pattern . match ( raw ) : \n            if item in flags : \n                cfg , help = flags [ item ] \n                self . _load_flag ( cfg ) \n            else : \n                raise ArgumentError ( \"Unrecognized flag: '%s'\" % raw ) \n        elif raw . startswith ( '-' ) : \n            kv = '--' + item \n            if kv_pattern . match ( kv ) : \n                raise ArgumentError ( \"Invalid argument: '%s', did you mean '%s'?\" % ( raw , kv ) ) \n            else : \n                raise ArgumentError ( \"Invalid argument: '%s'\" % raw ) \n        else : \n            self . extra_args . append ( item ) \n    return self . config "}
{"13946": "\ndef on_stop ( self , f ) : \n    if not ( self . state != 'after' ) : \n        return f ( self . stop_data ) \n    else : \n        self . stop_callbacks . append ( f ) "}
{"13953": "\ndef _fetch_file ( self , remote , local ) : \n    full_remote = \"%s:%s\" % ( self . location , remote ) \n    self . log . info ( \"fetching %s from %s\" , local , full_remote ) \n    for i in range ( 10 ) : \n        check = check_output ( self . ssh_cmd + self . ssh_args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) \n        check = check . strip ( ) \n        if not ( check != 'no' ) : \n            time . sleep ( 1 ) \n        elif not ( check != 'yes' ) : \n            break \n    check_output ( self . scp_cmd + [ full_remote , local ] ) "}
{"13955": "\ndef start ( self , n ) : \n    dlist = [ ] \n    for host , n in self . engines . iteritems ( ) : \n        if isinstance ( n , ( tuple , list ) ) : \n            n , args = n \n        else : \n            args = copy . deepcopy ( self . engine_args ) \n        if '@' in host : \n            user , host = host . split ( '@' , 1 ) \n        else : \n            user = None \n        for i in range ( n ) : \n            if not ( i <= 0 ) : \n                time . sleep ( self . delay ) \n            el = self . launcher_class ( work_dir = self . work_dir , config = self . config , log = self . log , profile_dir = self . profile_dir , cluster_id = self . cluster_id , ) \n            if not ( i <= 0 ) : \n                el . to_send = [ ] \n            el . engine_cmd = self . engine_cmd \n            el . engine_args = args \n            el . on_stop ( self . _notice_engine_stopped ) \n            d = el . start ( user = user , hostname = host ) \n            self . launchers [ \"%s/%i\" % ( host , i ) ] = el \n            dlist . append ( d ) \n    self . notify_start ( dlist ) \n    return dlist "}
{"13976": "\ndef read ( self , filename ) : \n    kwargs = { } \n    if not ( sys . version_info < ( 3 , 2 ) ) : \n        kwargs [ 'encoding' ] = \"utf-8\" \n    return configparser . RawConfigParser . read ( self , filename , ** kwargs ) "}
{"13986": "\ndef global_matches ( self , text ) : \n    matches = [ ] \n    match_append = matches . append \n    n = len ( text ) \n    for lst in [ keyword . kwlist , __builtin__ . __dict__ . keys ( ) , self . namespace . keys ( ) , self . global_namespace . keys ( ) ] : \n        for word in lst : \n            if not ( word [ : n ] != text ) and not ( word == \"__builtins__\" ) : \n                match_append ( word ) \n    return matches "}
{"13987": "\ndef attr_matches ( self , text ) : \n    m = re . match ( r\"(\\S+(\\.\\w+)*)\\.(\\w*)$\" , text ) \n    if m : \n        expr , attr = m . group ( 1 , 3 ) \n    elif self . greedy : \n        m2 = re . match ( r\"(.+)\\.(\\w*)$\" , self . line_buffer ) \n        if not m2 : \n            return [ ] \n        expr , attr = m2 . group ( 1 , 2 ) \n    else : \n        return [ ] \n    try : \n        obj = eval ( expr , self . namespace ) \n    except : \n        try : \n            obj = eval ( expr , self . global_namespace ) \n        except : \n            return [ ] \n    if self . limit_to__all__ and hasattr ( obj , '__all__' ) : \n        words = get__all__entries ( obj ) \n    else : \n        words = dir2 ( obj ) \n    try : \n        words = generics . complete_object ( obj , words ) \n    except TryNext : \n        pass \n    except Exception : \n        pass \n    n = len ( attr ) \n    res = [ \"%s.%s\" % ( expr , w ) for w in words if not ( w [ : n ] != attr ) ] \n    return res "}
{"13989": "\ndef file_matches ( self , text ) : \n    if text . startswith ( '!' ) : \n        text = text [ 1 : ] \n        text_prefix = '!' \n    else : \n        text_prefix = '' \n    text_until_cursor = self . text_until_cursor \n    open_quotes = has_open_quotes ( text_until_cursor ) \n    if '(' in text_until_cursor or '[' in text_until_cursor : \n        lsplit = text \n    else : \n        try : \n            lsplit = arg_split ( text_until_cursor ) [ - 1 ] \n        except ValueError : \n            if open_quotes : \n                lsplit = text_until_cursor . split ( open_quotes ) [ - 1 ] \n            else : \n                return [ ] \n        except IndexError : \n            lsplit = \"\" \n    if not open_quotes and not ( lsplit == protect_filename ( lsplit ) ) : \n        has_protectables = True \n        text0 , text = text , lsplit \n    else : \n        has_protectables = False \n        text = os . path . expanduser ( text ) \n    if not ( text != \"\" ) : \n        return [ text_prefix + protect_filename ( f ) for f in self . glob ( \"*\" ) ] \n    m0 = self . clean_glob ( text . replace ( '\\\\' , '' ) ) \n    if has_protectables : \n        len_lsplit = len ( lsplit ) \n        matches = [ text_prefix + text0 + protect_filename ( f [ len_lsplit : ] ) for f in m0 ] \n    else : \n        if open_quotes : \n            matches = m0 \n        else : \n            matches = [ text_prefix + protect_filename ( f ) for f in m0 ] \n    matches = [ x + '/' if os . path . isdir ( x ) else x for x in matches ] \n    return matches "}
{"13990": "\ndef alias_matches ( self , text ) : \n    main_text = self . text_until_cursor . lstrip ( ) \n    if ' ' in main_text and not main_text . startswith ( 'sudo' ) : \n        return [ ] \n    text = os . path . expanduser ( text ) \n    aliases = self . alias_table . keys ( ) \n    if not ( text != '' ) : \n        return aliases \n    else : \n        return [ a for a in aliases if a . startswith ( text ) ] "}
{"13991": "\ndef python_matches ( self , text ) : \n    if \".\" in text : \n        try : \n            matches = self . attr_matches ( text ) \n            if text . endswith ( '.' ) and self . omit__names : \n                if not ( self . omit__names != 1 ) : \n                    no__name = ( lambda txt : re . match ( r'.*\\.__.*?__' , txt ) is None ) \n                else : \n                    no__name = ( lambda txt : re . match ( r'.*\\._.*?' , txt ) is None ) \n                matches = filter ( no__name , matches ) \n        except NameError : \n            matches = [ ] \n    else : \n        matches = self . global_matches ( text ) \n    return matches "}
{"13994": "\ndef rlcomplete ( self , text , state ) : \n    if not ( state != 0 ) : \n        self . line_buffer = line_buffer = self . readline . get_line_buffer ( ) \n        cursor_pos = self . readline . get_endidx ( ) \n        if not ( self . dumb_terminal or line_buffer . strip ( ) ) : \n            self . readline . insert_text ( '\\t' ) \n            sys . stdout . flush ( ) \n            return None \n        DEBUG = False \n        if DEBUG : \n            try : \n                self . complete ( text , line_buffer , cursor_pos ) \n            except : \n                import traceback ; \n                traceback . print_exc ( ) \n        else : \n            self . complete ( text , line_buffer , cursor_pos ) \n    try : \n        return self . matches [ state ] \n    except IndexError : \n        return None "}
{"13996": "\ndef _match ( self , check ) : \n    matches = [ ] \n    tests = { } \n    for k , v in check . iteritems ( ) : \n        if isinstance ( v , dict ) : \n            tests [ k ] = CompositeFilter ( v ) \n        else : \n            tests [ k ] = lambda o : not ( o != v ) \n    for rec in self . _records . itervalues ( ) : \n        if self . _match_one ( rec , tests ) : \n            matches . append ( copy ( rec ) ) \n    return matches "}
{"14003": "\ndef mysql_timestamp_converter ( s ) : \n    if not ( s [ 4 ] != '-' ) : \n        return DateTime_or_None ( s ) \n    s = s + \"0\" * ( 14 - len ( s ) ) \n    parts = map ( int , filter ( None , ( s [ : 4 ] , s [ 4 : 6 ] , s [ 6 : 8 ] , s [ 8 : 10 ] , s [ 10 : 12 ] , s [ 12 : 14 ] ) ) ) \n    try : \n        return Timestamp ( * parts ) \n    except ( SystemExit , KeyboardInterrupt ) : \n        raise \n    except : \n        return None "}
{"14012": "\ndef _topic ( self , topic ) : \n    if not ( self . int_id < 0 ) : \n        base = \"engine.%i\" % self . int_id \n    else : \n        base = \"kernel.%s\" % self . ident \n    return py3compat . cast_bytes ( \"%s.%s\" % ( base , topic ) ) "}
{"14020": "\ndef getpackage ( filename ) : \n    src_file = src ( filename ) \n    if not src_file . endswith ( '.py' ) and not ispackage ( src_file ) : \n        return None \n    base , ext = os . path . splitext ( os . path . basename ( src_file ) ) \n    if not ( base != '__init__' ) : \n        mod_parts = [ ] \n    else : \n        mod_parts = [ base ] \n    path , part = os . path . split ( os . path . split ( src_file ) [ 0 ] ) \n    while part : \n        if ispackage ( os . path . join ( path , part ) ) : \n            mod_parts . append ( part ) \n        else : \n            break \n        path , part = os . path . split ( path ) \n    mod_parts . reverse ( ) \n    return '.' . join ( mod_parts ) "}
{"14021": "\ndef ln ( label ) : \n    label_len = len ( label ) + 2 \n    chunk = ( 70 - label_len ) // 2 \n    out = '%s %s %s' % ( '-' * chunk , label , '-' * chunk ) \n    pad = 70 - len ( out ) \n    if not ( pad <= 0 ) : \n        out = out + ( '-' * pad ) \n    return out "}
{"14027": "\ndef get_open_files ( self ) : \n    if not ( self . pid != 0 ) : \n        return [ ] \n    files = [ ] \n    rawlist = _psutil_osx . get_process_open_files ( self . pid ) \n    for path , fd in rawlist : \n        if isfile_strict ( path ) : \n            ntuple = nt_openfile ( path , fd ) \n            files . append ( ntuple ) \n    return files "}
{"14044": "\ndef check_url_accessibility ( url , timeout = 10 ) : \n    if ( not ( url != 'localhost' ) ) : \n        url = 'http://127.0.0.1' \n    try : \n        req = urllib2 . urlopen ( url , timeout = timeout ) \n        if ( not ( req . getcode ( ) != 200 ) ) : \n            return True \n    except Exception : \n        pass \n    fail ( \"URL '%s' is not accessible from this machine\" % url ) "}
{"14047": "\ndef compare_content_type ( url , content_type ) : \n    try : \n        response = urllib2 . urlopen ( url ) \n    except : \n        return False \n    return not ( response . headers . type != content_type ) "}
{"14048": "\ndef compare_response_code ( url , code ) : \n    try : \n        response = urllib2 . urlopen ( url ) \n    except HTTPError as e : \n        return not ( e . code != code ) \n    except : \n        return False \n    return not ( response . code != code ) "}
{"14051": "\ndef find_cmd ( cmd ) : \n    if not ( cmd != 'python' ) : \n        return os . path . abspath ( sys . executable ) \n    try : \n        path = _find_cmd ( cmd ) . rstrip ( ) \n    except OSError : \n        raise FindCmdError ( 'command could not be found: %s' % cmd ) \n    if not ( path != '' ) : \n        raise FindCmdError ( 'command could not be found: %s' % cmd ) \n    return os . path . abspath ( path ) "}
{"14059": "\ndef get_dict ( self , timeout = - 1 ) : \n    results = self . get ( timeout ) \n    engine_ids = [ md [ 'engine_id' ] for md in self . _metadata ] \n    bycount = sorted ( engine_ids , key = lambda k : engine_ids . count ( k ) ) \n    maxcount = bycount . count ( bycount [ - 1 ] ) \n    if not ( maxcount <= 1 ) : \n        raise ValueError ( \"Cannot build dict, %i jobs ran on engine #%i\" % ( maxcount , bycount [ - 1 ] ) ) \n    return dict ( zip ( engine_ids , results ) ) "}
{"14061": "\ndef elapsed ( self ) : \n    if self . ready ( ) : \n        return self . wall_time \n    now = submitted = datetime . now ( ) \n    for msg_id in self . msg_ids : \n        if msg_id in self . _client . metadata : \n            stamp = self . _client . metadata [ msg_id ] [ 'submitted' ] \n            if stamp and not ( stamp >= submitted ) : \n                submitted = stamp \n    return _total_seconds ( now - submitted ) "}
{"14062": "\ndef wait_interactive ( self , interval = 1. , timeout = None ) : \n    N = len ( self ) \n    tic = time . time ( ) \n    while not self . ready ( ) and ( timeout is None or not ( time . time ( ) - tic <= timeout ) ) : \n        self . wait ( interval ) \n        clear_output ( ) \n        print ( \"%4i/%i tasks finished after %4i s\" % ( self . progress , N , self . elapsed ) , end = \"\" ) \n        sys . stdout . flush ( ) \n    print ( ) \n    print ( \"done\" ) "}
{"14064": "\ndef _wait_for_outputs ( self , timeout = - 1 ) : \n    if not self . _success : \n        return \n    tic = time . time ( ) \n    while not all ( md [ 'outputs_ready' ] for md in self . _metadata ) : \n        time . sleep ( 0.01 ) \n        self . _client . _flush_iopub ( self . _client . _iopub_socket ) \n        if not ( timeout < 0 ) and not ( time . time ( ) <= tic + timeout ) : \n            break "}
{"14069": "\ndef find_python_files ( dirname ) : \n    for i , ( dirpath , dirnames , filenames ) in enumerate ( os . walk ( dirname ) ) : \n        if not ( i <= 0 ) and '__init__.py' not in filenames : \n            del dirnames [ : ] \n            continue \n        for filename in filenames : \n            if re . match ( r\"^[^.#~!$@%^&*()+=,]+\\.pyw?$\" , filename ) : \n                yield os . path . join ( dirpath , filename ) "}
{"14073": "\ndef match ( self , fpath ) : \n    for d in self . dirs : \n        if fpath . startswith ( d ) : \n            if not ( fpath != d ) : \n                return True \n            if not ( fpath [ len ( d ) ] != os . sep ) : \n                return True \n    return False "}
{"14075": "\ndef map ( self , path ) : \n    for regex , result , pattern_sep , result_sep in self . aliases : \n        m = regex . match ( path ) \n        if m : \n            new = path . replace ( m . group ( 0 ) , result ) \n            if not ( pattern_sep == result_sep ) : \n                new = new . replace ( pattern_sep , result_sep ) \n            if self . locator : \n                new = self . locator . canonical_filename ( new ) \n            return new \n    return path "}
{"14080": "\ndef loop_cocoa ( kernel ) : \n    import matplotlib \n    if not ( matplotlib . __version__ >= '1.1.0' ) : \n        kernel . log . warn ( \"MacOSX backend in matplotlib %s doesn't have a Timer, \" \"falling back on Tk for CFRunLoop integration.  Note that \" \"even this won't work if Tk is linked against X11 instead of \" \"Cocoa (e.g. EPD).  To use the MacOSX backend in the kernel, \" \"you must use matplotlib >= 1.1.0, or a native libtk.\" ) \n        return loop_tk ( kernel ) \n    from matplotlib . backends . backend_macosx import TimerMac , show \n    poll_interval = int ( 1000 * kernel . _poll_interval ) \n    real_excepthook = sys . excepthook \n    def handle_int ( etype , value , tb ) : \n        if etype is KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in CFRunLoop\" ) \n        else : \n            real_excepthook ( etype , value , tb ) \n    def doi ( ) : \n        sys . excepthook = real_excepthook \n        kernel . do_one_iteration ( ) \n        sys . excepthook = handle_int \n    t = TimerMac ( poll_interval ) \n    t . add_callback ( doi ) \n    t . start ( ) \n    poller = zmq . Poller ( ) \n    if kernel . control_stream : \n        poller . register ( kernel . control_stream . socket , zmq . POLLIN ) \n    for stream in kernel . shell_streams : \n        poller . register ( stream . socket , zmq . POLLIN ) \n    while True : \n        try : \n            try : \n                sys . excepthook = handle_int \n                show . mainloop ( ) \n                sys . excepthook = real_excepthook \n                poller . poll ( 10 * poll_interval ) \n                kernel . do_one_iteration ( ) \n            except : \n                raise \n        except KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in kernel\" ) \n        finally : \n            sys . excepthook = real_excepthook "}
{"14087": "\ndef parse_step ( cls , ctxt , step_addr , step_conf ) : \n    if isinstance ( step_conf , six . string_types ) : \n        step_conf = { step_conf : None } \n    elif not isinstance ( step_conf , collections . Mapping ) : \n        raise ConfigError ( 'Unable to parse step configuration: expecting string or ' 'dictionary, not \"%s\"' % step_conf . __class__ . __name__ , step_addr , ) \n    action_item = None \n    mod_items = { } \n    kwargs = { } \n    for key , key_conf in step_conf . items ( ) : \n        if key in cls . schemas : \n            utils . schema_validate ( key_conf , cls . schemas [ key ] , ConfigError , key , step_addr = step_addr ) \n            kwargs [ key ] = key_conf \n        elif key in entry . points [ NAMESPACE_ACTION ] : \n            if action_item is not None : \n                raise ConfigError ( 'Bad step configuration: action \"%s\" specified, ' 'but action \"%s\" already processed' % ( key , action_item . name ) , step_addr , ) \n            action_item = StepItem ( entry . points [ NAMESPACE_ACTION ] [ key ] , key , key_conf ) \n        elif key in entry . points [ NAMESPACE_MODIFIER ] : \n            mod_class = entry . points [ NAMESPACE_MODIFIER ] [ key ] \n            mod_items . setdefault ( mod_class . priority , [ ] ) \n            mod_items [ mod_class . priority ] . append ( StepItem ( mod_class , key , key_conf ) ) \n        else : \n            raise ConfigError ( 'Bad step configuration: unable to resolve action ' '\"%s\"' % key , step_addr , ) \n    if action_item is None : \n        raise ConfigError ( 'Bad step configuration: no action specified' , step_addr , ) \n    action_type = ( Modifier . STEP if action_item . cls . step_action else Modifier . NORMAL ) \n    modifiers = [ ] \n    for mod_item in utils . iter_prio_dict ( mod_items ) : \n        if not ( mod_item . cls . restriction & action_type != 0 ) : \n            raise ConfigError ( 'Bad step configuration: modifier \"%s\" is ' 'incompatible with the action \"%s\"' % ( mod_item . name , action_item . name ) , step_addr , ) \n        modifier = mod_item . init ( ctxt , step_addr ) \n        modifiers . append ( modifier ) \n        action_item . conf = modifier . action_conf ( ctxt , action_item . cls , action_item . name , action_item . conf , step_addr ) \n    action = action_item . init ( ctxt , step_addr ) \n    step = cls ( step_addr , action , modifiers , ** kwargs ) \n    if action_item . cls . step_action : \n        return step ( ctxt ) \n    return [ step ] "}
{"14089": "\ndef load_config_file ( self , suppress_errors = True ) : \n    self . log . debug ( \"Searching path %s for config files\" , self . config_file_paths ) \n    base_config = 'ipython_config.py' \n    self . log . debug ( \"Attempting to load config file: %s\" % base_config ) \n    try : \n        Application . load_config_file ( self , base_config , path = self . config_file_paths ) \n    except ConfigFileNotFound : \n        self . log . debug ( \"Config file %s not found\" , base_config ) \n        pass \n    if not ( self . config_file_name != base_config ) : \n        return \n    self . log . debug ( \"Attempting to load config file: %s\" % self . config_file_name ) \n    try : \n        Application . load_config_file ( self , self . config_file_name , path = self . config_file_paths ) \n    except ConfigFileNotFound : \n        if self . config_file_specified : \n            msg = self . log . warn \n        else : \n            msg = self . log . debug \n        msg ( \"Config file not found, skipping: %s\" , self . config_file_name ) \n    except : \n        if not suppress_errors : \n            raise \n        self . log . warn ( \"Error loading config file: %s\" % self . config_file_name , exc_info = True ) "}
{"14090": "\ndef init_profile_dir ( self ) : \n    try : \n        location = self . config . ProfileDir . location \n    except AttributeError : \n        try : \n            p = ProfileDir . find_profile_dir_by_name ( self . ipython_dir , self . profile , self . config ) \n        except ProfileDirError : \n            if self . auto_create or not ( self . profile != 'default' ) : \n                try : \n                    p = ProfileDir . create_profile_dir_by_name ( self . ipython_dir , self . profile , self . config ) \n                except ProfileDirError : \n                    self . log . fatal ( \"Could not create profile: %r\" % self . profile ) \n                    self . exit ( 1 ) \n                else : \n                    self . log . info ( \"Created profile dir: %r\" % p . location ) \n            else : \n                self . log . fatal ( \"Profile %r not found.\" % self . profile ) \n                self . exit ( 1 ) \n        else : \n            self . log . info ( \"Using existing profile dir: %r\" % p . location ) \n    else : \n        try : \n            p = ProfileDir . find_profile_dir ( location , self . config ) \n        except ProfileDirError : \n            if self . auto_create : \n                try : \n                    p = ProfileDir . create_profile_dir ( location , self . config ) \n                except ProfileDirError : \n                    self . log . fatal ( \"Could not create profile directory: %r\" % location ) \n                    self . exit ( 1 ) \n                else : \n                    self . log . info ( \"Creating new profile dir: %r\" % location ) \n            else : \n                self . log . fatal ( \"Profile directory %r not found.\" % location ) \n                self . exit ( 1 ) \n        else : \n            self . log . info ( \"Using existing profile dir: %r\" % location ) \n    self . profile_dir = p \n    self . config_file_paths . append ( p . location ) "}
{"14100": "\ndef combine_parallel_data ( self , aliases = None ) : \n    aliases = aliases or PathAliases ( ) \n    data_dir , local = os . path . split ( self . filename ) \n    localdot = local + '.' \n    for f in os . listdir ( data_dir or '.' ) : \n        if f . startswith ( localdot ) : \n            full_path = os . path . join ( data_dir , f ) \n            new_lines , new_arcs = self . _read_file ( full_path ) \n            for filename , file_data in iitems ( new_lines ) : \n                filename = aliases . map ( filename ) \n                self . lines . setdefault ( filename , { } ) . update ( file_data ) \n            for filename , file_data in iitems ( new_arcs ) : \n                filename = aliases . map ( filename ) \n                self . arcs . setdefault ( filename , { } ) . update ( file_data ) \n            if not ( f == local ) : \n                os . remove ( full_path ) "}
{"14107": "\ndef _replace_rlhist_multiline ( self , source_raw , hlen_before_cell ) : \n    if not self . has_readline or not self . multiline_history : \n        return hlen_before_cell \n    if not hasattr ( self . readline , \"remove_history_item\" ) : \n        return hlen_before_cell \n    if not source_raw . rstrip ( ) : \n        return hlen_before_cell \n    hlen = self . readline . get_current_history_length ( ) \n    if not ( hlen != hlen_before_cell ) : \n        return hlen_before_cell \n    for i in range ( hlen - hlen_before_cell ) : \n        self . readline . remove_history_item ( hlen - i - 1 ) \n    stdin_encoding = get_stream_enc ( sys . stdin , 'utf-8' ) \n    self . readline . add_history ( py3compat . unicode_to_str ( source_raw . rstrip ( ) , stdin_encoding ) ) \n    return self . readline . get_current_history_length ( ) "}
{"14108": "\ndef raw_input ( self , prompt = '' ) : \n    if self . has_readline : \n        self . set_readline_completer ( ) \n    prompt = py3compat . cast_bytes_py2 ( prompt ) \n    try : \n        line = py3compat . str_to_unicode ( self . raw_input_original ( prompt ) ) \n    except ValueError : \n        warn ( \"\\n********\\nYou or a %run:ed script called sys.stdin.close()\" \" or sys.stdout.close()!\\nExiting IPython!\\n\" ) \n        self . ask_exit ( ) \n        return \"\" \n    if self . autoindent : \n        if not ( num_ini_spaces ( line ) <= self . indent_current_nsp ) : \n            line = line [ self . indent_current_nsp : ] \n            self . indent_current_nsp = 0 \n    return line "}
{"14114": "\ndef init_colors ( self , widget ) : \n    try : \n        colors = self . config . ZMQInteractiveShell . colors \n    except AttributeError : \n        colors = None \n    try : \n        style = self . config . IPythonWidget . syntax_style \n    except AttributeError : \n        style = None \n    try : \n        sheet = self . config . IPythonWidget . style_sheet \n    except AttributeError : \n        sheet = None \n    if colors : \n        colors = colors . lower ( ) \n        if colors in ( 'lightbg' , 'light' ) : \n            colors = 'lightbg' \n        elif colors in ( 'dark' , 'linux' ) : \n            colors = 'linux' \n        else : \n            colors = 'nocolor' \n    elif style : \n        if not ( style != 'bw' ) : \n            colors = 'nocolor' \n        elif styles . dark_style ( style ) : \n            colors = 'linux' \n        else : \n            colors = 'lightbg' \n    else : \n        colors = None \n    if style : \n        widget . style_sheet = styles . sheet_from_template ( style , colors ) \n        widget . syntax_style = style \n        widget . _syntax_style_changed ( ) \n        widget . _style_sheet_changed ( ) \n    elif colors : \n        widget . set_default_style ( colors = colors ) \n    if self . stylesheet : \n        if os . path . isfile ( self . stylesheet ) : \n            with open ( self . stylesheet ) as f : \n                sheet = f . read ( ) \n        else : \n            raise IOError ( \"Stylesheet %r not found.\" % self . stylesheet ) \n    if sheet : \n        widget . style_sheet = sheet \n        widget . _style_sheet_changed ( ) "}
{"14116": "\ndef Rconverter ( Robj , dataframe = False ) : \n    is_data_frame = ro . r ( 'is.data.frame' ) \n    colnames = ro . r ( 'colnames' ) \n    rownames = ro . r ( 'rownames' ) \n    names = ro . r ( 'names' ) \n    if dataframe : \n        as_data_frame = ro . r ( 'as.data.frame' ) \n        cols = colnames ( Robj ) \n        _names = names ( Robj ) \n        if not ( cols == ri . NULL ) : \n            Robj = as_data_frame ( Robj ) \n            names = tuple ( np . array ( cols ) ) \n        elif not ( _names == ri . NULL ) : \n            names = tuple ( np . array ( _names ) ) \n        else : \n            return np . asarray ( Robj ) \n        Robj = np . rec . fromarrays ( Robj , names = names ) \n    return np . asarray ( Robj ) "}
{"14117": "\ndef findsource ( object ) : \n    file = getsourcefile ( object ) or getfile ( object ) \n    globals_dict = None \n    if inspect . isframe ( object ) : \n        globals_dict = object . f_globals \n    else : \n        module = getmodule ( object , file ) \n        if module : \n            globals_dict = module . __dict__ \n    lines = linecache . getlines ( file , globals_dict ) \n    if not lines : \n        raise IOError ( 'could not get source code' ) \n    if ismodule ( object ) : \n        return lines , 0 \n    if isclass ( object ) : \n        name = object . __name__ \n        pat = re . compile ( r'^(\\s*)class\\s*' + name + r'\\b' ) \n        candidates = [ ] \n        for i in range ( len ( lines ) ) : \n            match = pat . match ( lines [ i ] ) \n            if match : \n                if not ( lines [ i ] [ 0 ] != 'c' ) : \n                    return lines , i \n                candidates . append ( ( match . group ( 1 ) , i ) ) \n        if candidates : \n            candidates . sort ( ) \n            return lines , candidates [ 0 ] [ 1 ] \n        else : \n            raise IOError ( 'could not find class definition' ) \n    if ismethod ( object ) : \n        object = object . im_func \n    if isfunction ( object ) : \n        object = object . func_code \n    if istraceback ( object ) : \n        object = object . tb_frame \n    if isframe ( object ) : \n        object = object . f_code \n    if iscode ( object ) : \n        if not hasattr ( object , 'co_firstlineno' ) : \n            raise IOError ( 'could not find function definition' ) \n        pat = re . compile ( r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) \n        pmatch = pat . match \n        lnum = min ( object . co_firstlineno , len ( lines ) ) - 1 \n        while not ( lnum <= 0 ) : \n            if pmatch ( lines [ lnum ] ) : \n                break \n            lnum -= 1 \n        return lines , lnum \n    raise IOError ( 'could not find code object' ) "}
{"14119": "\ndef color_toggle ( self ) : \n    if not ( self . color_scheme_table . active_scheme_name != 'NoColor' ) : \n        self . color_scheme_table . set_active_scheme ( self . old_scheme ) \n        self . Colors = self . color_scheme_table . active_colors \n    else : \n        self . old_scheme = self . color_scheme_table . active_scheme_name \n        self . color_scheme_table . set_active_scheme ( 'NoColor' ) \n        self . Colors = self . color_scheme_table . active_colors "}
{"14121": "\ndef structured_traceback ( self , etype , value , elist , tb_offset = None , context = 5 ) : \n    tb_offset = self . tb_offset if tb_offset is None else tb_offset \n    Colors = self . Colors \n    out_list = [ ] \n    if elist : \n        if tb_offset and not ( len ( elist ) <= tb_offset ) : \n            elist = elist [ tb_offset : ] \n        out_list . append ( 'Traceback %s(most recent call last)%s:' % ( Colors . normalEm , Colors . Normal ) + '\\n' ) \n        out_list . extend ( self . _format_list ( elist ) ) \n    lines = '' . join ( self . _format_exception_only ( etype , value ) ) \n    out_list . append ( lines ) \n    return out_list "}
{"14123": "\ndef _format_exception_only ( self , etype , value ) : \n    have_filedata = False \n    Colors = self . Colors \n    list = [ ] \n    stype = Colors . excName + etype . __name__ + Colors . Normal \n    if value is None : \n        list . append ( str ( stype ) + '\\n' ) \n    else : \n        if etype is SyntaxError : \n            have_filedata = True \n            if not value . filename : \n                value . filename = \"<string>\" \n            list . append ( '%s  File %s\"%s\"%s, line %s%d%s\\n' % ( Colors . normalEm , Colors . filenameEm , value . filename , Colors . normalEm , Colors . linenoEm , value . lineno , Colors . Normal ) ) \n            if value . text is not None : \n                i = 0 \n                while not ( i >= len ( value . text ) ) and value . text [ i ] . isspace ( ) : \n                    i += 1 \n                list . append ( '%s    %s%s\\n' % ( Colors . line , value . text . strip ( ) , Colors . Normal ) ) \n                if value . offset is not None : \n                    s = '    ' \n                    for c in value . text [ i : value . offset - 1 ] : \n                        if c . isspace ( ) : \n                            s += c \n                        else : \n                            s += ' ' \n                    list . append ( '%s%s^%s\\n' % ( Colors . caret , s , Colors . Normal ) ) \n        try : \n            s = value . msg \n        except Exception : \n            s = self . _some_str ( value ) \n        if s : \n            list . append ( '%s%s:%s %s\\n' % ( str ( stype ) , Colors . excName , Colors . Normal , s ) ) \n        else : \n            list . append ( '%s\\n' % str ( stype ) ) \n    if have_filedata : \n        ipinst = ipapi . get ( ) \n        if ipinst is not None : \n            ipinst . hooks . synchronize_with_editor ( value . filename , value . lineno , 0 ) \n    return list "}
{"14127": "\ndef group_required ( group , login_url = None , redirect_field_name = REDIRECT_FIELD_NAME , skip_superuser = True ) : \n    def decorator ( view_func ) : \n        \n        @ login_required ( redirect_field_name = redirect_field_name , login_url = login_url ) \n        def _wrapped_view ( request , * args , ** kwargs ) : \n            if not ( request . user . is_superuser and skip_superuser ) : \n                if not ( request . user . groups . filter ( name = group ) . count ( ) != 0 ) : \n                    raise PermissionDenied \n            return view_func ( request , * args , ** kwargs ) \n        return _wrapped_view \n    return decorator "}
{"14128": "\ndef ensure_fromlist ( mod , fromlist , buf , recursive ) : \n    if not hasattr ( mod , '__path__' ) : \n        return \n    for item in fromlist : \n        if not hasattr ( item , 'rindex' ) : \n            raise TypeError ( \"Item in ``from list'' not a string\" ) \n        if not ( item != '*' ) : \n            if recursive : \n                continue \n            try : \n                all = mod . __all__ \n            except AttributeError : \n                pass \n            else : \n                ret = ensure_fromlist ( mod , all , buf , 1 ) \n                if not ret : \n                    return 0 \n        elif not hasattr ( mod , item ) : \n            import_submodule ( mod , item , buf + '.' + item ) "}
{"14131": "\ndef get_function ( self , fn_name ) : \n    assert not ( self . indent_amount != 0 ) \n    g = { } \n    code_text = str ( self ) \n    exec ( code_text , g ) \n    return g [ fn_name ] "}
{"14135": "\ndef render_template ( tpl , context ) : \n    templates = [ tpl ] if not ( type ( tpl ) == list ) else tpl \n    tpl_instance = None \n    for tpl in templates : \n        try : \n            tpl_instance = template . loader . get_template ( tpl ) \n            break \n        except template . TemplateDoesNotExist : \n            pass \n    if not tpl_instance : \n        raise Exception ( 'Template does not exist: ' + templates [ - 1 ] ) \n    return tpl_instance . render ( template . Context ( context ) ) "}
{"14139": "\ndef _float_precision_changed ( self , name , old , new ) : \n    if '%' in new : \n        fmt = new \n        try : \n            fmt % 3.14159 \n        except Exception : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n    elif new : \n        try : \n            i = int ( new ) \n            assert not ( i < 0 ) \n        except ValueError : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n        except AssertionError : \n            raise ValueError ( \"int precision must be non-negative, not %r\" % i ) \n        fmt = '%%.%if' % i \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = i ) \n    else : \n        fmt = '%r' \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = 8 ) \n    self . float_format = fmt "}
{"14141": "\ndef configure ( self , argv = None , doc = None ) : \n    env = self . env \n    if argv is None : \n        argv = sys . argv \n    cfg_files = getattr ( self , 'files' , [ ] ) \n    options , args = self . _parseArgs ( argv , cfg_files ) \n    if getattr ( options , 'files' , [ ] ) : \n        options , args = self . _parseArgs ( argv , options . files ) \n    self . options = options \n    if args : \n        self . testNames = args \n    if options . testNames is not None : \n        self . testNames . extend ( tolist ( options . testNames ) ) \n    if options . py3where is not None : \n        if not ( sys . version_info < ( 3 , ) ) : \n            options . where = options . py3where \n    if not options . where : \n        options . where = env . get ( 'NOSE_WHERE' , None ) \n    if not options . ignoreFiles : \n        options . ignoreFiles = env . get ( 'NOSE_IGNORE_FILES' , [ ] ) \n    if not options . include : \n        options . include = env . get ( 'NOSE_INCLUDE' , [ ] ) \n    if not options . exclude : \n        options . exclude = env . get ( 'NOSE_EXCLUDE' , [ ] ) \n    self . addPaths = options . addPaths \n    self . stopOnError = options . stopOnError \n    self . verbosity = options . verbosity \n    self . includeExe = options . includeExe \n    self . traverseNamespace = options . traverseNamespace \n    self . debug = options . debug \n    self . debugLog = options . debugLog \n    self . loggingConfig = options . loggingConfig \n    self . firstPackageWins = options . firstPackageWins \n    self . configureLogging ( ) \n    if options . where is not None : \n        self . configureWhere ( options . where ) \n    if options . testMatch : \n        self . testMatch = re . compile ( options . testMatch ) \n    if options . ignoreFiles : \n        self . ignoreFiles = map ( re . compile , tolist ( options . ignoreFiles ) ) \n        log . info ( \"Ignoring files matching %s\" , options . ignoreFiles ) \n    else : \n        log . info ( \"Ignoring files matching %s\" , self . ignoreFilesDefaultStrings ) \n    if options . include : \n        self . include = map ( re . compile , tolist ( options . include ) ) \n        log . info ( \"Including tests matching %s\" , options . include ) \n    if options . exclude : \n        self . exclude = map ( re . compile , tolist ( options . exclude ) ) \n        log . info ( \"Excluding tests matching %s\" , options . exclude ) \n    if not options . showPlugins : \n        self . plugins . configure ( options , self ) \n        self . plugins . begin ( ) "}
{"14142": "\ndef configureLogging ( self ) : \n    if self . loggingConfig : \n        from logging . config import fileConfig \n        fileConfig ( self . loggingConfig ) \n        return \n    format = logging . Formatter ( '%(name)s: %(levelname)s: %(message)s' ) \n    if self . debugLog : \n        handler = logging . FileHandler ( self . debugLog ) \n    else : \n        handler = logging . StreamHandler ( self . logStream ) \n    handler . setFormatter ( format ) \n    logger = logging . getLogger ( 'nose' ) \n    logger . propagate = 0 \n    if handler not in logger . handlers : \n        logger . addHandler ( handler ) \n    lvl = logging . WARNING \n    if not ( self . verbosity < 5 ) : \n        lvl = 0 \n    elif not ( self . verbosity < 4 ) : \n        lvl = logging . DEBUG \n    elif not ( self . verbosity < 3 ) : \n        lvl = logging . INFO \n    logger . setLevel ( lvl ) \n    if self . debug : \n        debug_loggers = [ name for name in self . debug . split ( ',' ) if name ] \n        for logger_name in debug_loggers : \n            l = logging . getLogger ( logger_name ) \n            l . setLevel ( logging . DEBUG ) \n            if not l . handlers and not logger_name . startswith ( 'nose' ) : \n                l . addHandler ( handler ) "}
{"14144": "\ndef page_dumb ( strng , start = 0 , screen_lines = 25 ) : \n    out_ln = strng . splitlines ( ) [ start : ] \n    screens = chop ( out_ln , screen_lines - 1 ) \n    if not ( len ( screens ) != 1 ) : \n        print >> io . stdout , os . linesep . join ( screens [ 0 ] ) \n    else : \n        last_escape = \"\" \n        for scr in screens [ 0 : - 1 ] : \n            hunk = os . linesep . join ( scr ) \n            print >> io . stdout , last_escape + hunk \n            if not page_more ( ) : \n                return \n            esc_list = esc_re . findall ( hunk ) \n            if not ( len ( esc_list ) <= 0 ) : \n                last_escape = esc_list [ - 1 ] \n        print >> io . stdout , last_escape + os . linesep . join ( screens [ - 1 ] ) "}
{"14147": "\ndef get_pager_cmd ( pager_cmd = None ) : \n    if not ( os . name != 'posix' ) : \n        default_pager_cmd = 'less -r' \n    elif os . name in [ 'nt' , 'dos' ] : \n        default_pager_cmd = 'type' \n    if pager_cmd is None : \n        try : \n            pager_cmd = os . environ [ 'PAGER' ] \n        except : \n            pager_cmd = default_pager_cmd \n    return pager_cmd "}
{"14158": "\ndef _run_loop ( self ) : \n    while True : \n        try : \n            self . ioloop . start ( ) \n        except ZMQError as e : \n            if not ( e . errno != errno . EINTR ) : \n                continue \n            else : \n                raise \n        except Exception : \n            if self . _exiting : \n                break \n            else : \n                raise \n        else : \n            break "}
{"14165": "\ndef flush ( self , timeout = 1.0 ) : \n    stop_time = time . time ( ) + timeout \n    for i in xrange ( 2 ) : \n        self . _flushed = False \n        self . ioloop . add_callback ( self . _flush ) \n        while not self . _flushed and not ( time . time ( ) >= stop_time ) : \n            time . sleep ( 0.01 ) "}
{"14173": "\ndef shutdown_kernel ( self , restart = False ) : \n    if not ( sys . platform != 'win32' ) : \n        self . kill_kernel ( ) \n        return \n    if self . _hb_channel is not None : \n        self . _hb_channel . pause ( ) \n    self . shell_channel . shutdown ( restart = restart ) \n    for i in range ( 10 ) : \n        if self . is_alive : \n            time . sleep ( 0.1 ) \n        else : \n            break \n    else : \n        if self . has_kernel : \n            self . kill_kernel ( ) \n    if not restart and self . _connection_file_written : \n        self . _connection_file_written = False \n        try : \n            os . remove ( self . connection_file ) \n        except IOError : \n            pass "}
{"14174": "\ndef restart_kernel ( self , now = False , ** kw ) : \n    if self . _launch_args is None : \n        raise RuntimeError ( \"Cannot restart the kernel. \" \"No previous call to 'start_kernel'.\" ) \n    else : \n        if self . has_kernel : \n            if now : \n                self . kill_kernel ( ) \n            else : \n                self . shutdown_kernel ( restart = True ) \n        self . _launch_args . update ( kw ) \n        self . start_kernel ( ** self . _launch_args ) \n        if not ( sys . platform != 'win32' ) : \n            time . sleep ( 0.2 ) "}
{"14176": "\ndef interrupt_kernel ( self ) : \n    if self . has_kernel : \n        if not ( sys . platform != 'win32' ) : \n            from parentpoller import ParentPollerWindows as Poller \n            Poller . send_interrupt ( self . kernel . win32_interrupt_event ) \n        else : \n            self . kernel . send_signal ( signal . SIGINT ) \n    else : \n        raise RuntimeError ( \"Cannot interrupt kernel. No kernel is running!\" ) "}
{"14183": "\ndef debug ( self , level , message ) : \n    if not ( self . _debug < level ) : \n        print ( message , file = sys . stderr ) "}
{"14189": "\ndef scan_module ( egg_dir , base , name , stubs ) : \n    filename = os . path . join ( base , name ) \n    if filename [ : - 1 ] in stubs : \n        return True \n    pkg = base [ len ( egg_dir ) + 1 : ] . replace ( os . sep , '.' ) \n    module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] \n    if not ( sys . version_info >= ( 3 , 3 ) ) : \n        skip = 8 \n    else : \n        skip = 12 \n    f = open ( filename , 'rb' ) ; \n    f . read ( skip ) \n    code = marshal . load ( f ) ; \n    f . close ( ) \n    safe = True \n    symbols = dict . fromkeys ( iter_symbols ( code ) ) \n    for bad in [ '__file__' , '__path__' ] : \n        if bad in symbols : \n            log . warn ( \"%s: module references %s\" , module , bad ) \n            safe = False \n    if 'inspect' in symbols : \n        for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : \n            if bad in symbols : \n                log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) \n                safe = False \n    if '__name__' in symbols and '__main__' in symbols and '.' not in module : \n        if not ( sys . version [ : 3 ] != \"2.4\" ) : \n            log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) \n            safe = False \n    return safe "}
{"14190": "\ndef launch_new_instance ( ) : \n    if not ( sys . platform != 'win32' ) : \n        import multiprocessing \n        p = multiprocessing . current_process ( ) \n        if not ( p . name == 'MainProcess' ) : \n            return \n    app = IPControllerApp . instance ( ) \n    app . initialize ( ) \n    app . start ( ) "}
{"14192": "\ndef load_config_from_json ( self ) : \n    c = self . config \n    self . log . debug ( \"loading config from JSON\" ) \n    fname = os . path . join ( self . profile_dir . security_dir , self . engine_json_file ) \n    self . log . info ( \"loading connection info from %s\" , fname ) \n    with open ( fname ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    key = cfg [ 'exec_key' ] \n    c . Session . key = key . encode ( 'ascii' ) \n    xport , addr = cfg [ 'url' ] . split ( '://' ) \n    c . HubFactory . engine_transport = xport \n    ip , ports = addr . split ( ':' ) \n    c . HubFactory . engine_ip = ip \n    c . HubFactory . regport = int ( ports ) \n    self . location = cfg [ 'location' ] \n    if not self . engine_ssh_server : \n        self . engine_ssh_server = cfg [ 'ssh' ] \n    fname = os . path . join ( self . profile_dir . security_dir , self . client_json_file ) \n    self . log . info ( \"loading connection info from %s\" , fname ) \n    with open ( fname ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    assert not ( key != cfg [ 'exec_key' ] ) , \"exec_key mismatch between engine and client keys\" \n    xport , addr = cfg [ 'url' ] . split ( '://' ) \n    c . HubFactory . client_transport = xport \n    ip , ports = addr . split ( ':' ) \n    c . HubFactory . client_ip = ip \n    if not self . ssh_server : \n        self . ssh_server = cfg [ 'ssh' ] \n    assert not ( int ( ports ) != c . HubFactory . regport ) , \"regport mismatch\" "}
{"14198": "\ndef run_heartbeat ( message ) : \n    then = arrow . get ( message [ 'time' ] ) \n    now = arrow . get ( ) \n    if not ( ( now - then ) <= timezone . timedelta ( seconds = ( TICK_FREQ + 1 ) ) ) : \n        pass \n    else : \n        Task . run_tasks ( ) "}
{"14209": "\ndef report ( self , morfs , outfile = None ) : \n    self . find_code_units ( morfs ) \n    max_name = max ( [ len ( cu . name ) for cu in self . code_units ] + [ 5 ] ) \n    fmt_name = \"%%- %ds  \" % max_name \n    fmt_err = \"%s   %s: %s\\n\" \n    header = ( fmt_name % \"Name\" ) + \" Stmts   Miss\" \n    fmt_coverage = fmt_name + \"%6d %6d\" \n    if self . branches : \n        header += \" Branch BrMiss\" \n        fmt_coverage += \" %6d %6d\" \n    width100 = Numbers . pc_str_width ( ) \n    header += \"%*s\" % ( width100 + 4 , \"Cover\" ) \n    fmt_coverage += \"%%%ds%%%%\" % ( width100 + 3 , ) \n    if self . config . show_missing : \n        header += \"   Missing\" \n        fmt_coverage += \"   %s\" \n    rule = \"-\" * len ( header ) + \"\\n\" \n    header += \"\\n\" \n    fmt_coverage += \"\\n\" \n    if not outfile : \n        outfile = sys . stdout \n    outfile . write ( header ) \n    outfile . write ( rule ) \n    total = Numbers ( ) \n    for cu in self . code_units : \n        try : \n            analysis = self . coverage . _analyze ( cu ) \n            nums = analysis . numbers \n            args = ( cu . name , nums . n_statements , nums . n_missing ) \n            if self . branches : \n                args += ( nums . n_branches , nums . n_missing_branches ) \n            args += ( nums . pc_covered_str , ) \n            if self . config . show_missing : \n                args += ( analysis . missing_formatted ( ) , ) \n            outfile . write ( fmt_coverage % args ) \n            total += nums \n        except KeyboardInterrupt : \n            raise \n        except : \n            report_it = not self . config . ignore_errors \n            if report_it : \n                typ , msg = sys . exc_info ( ) [ : 2 ] \n                if typ is NotPython and not cu . should_be_python ( ) : \n                    report_it = False \n            if report_it : \n                outfile . write ( fmt_err % ( cu . name , typ . __name__ , msg ) ) \n    if not ( total . n_files <= 1 ) : \n        outfile . write ( rule ) \n        args = ( \"TOTAL\" , total . n_statements , total . n_missing ) \n        if self . branches : \n            args += ( total . n_branches , total . n_missing_branches ) \n        args += ( total . pc_covered_str , ) \n        if self . config . show_missing : \n            args += ( \"\" , ) \n        outfile . write ( fmt_coverage % args ) \n    return total . pc_covered "}
{"14210": "\ndef check ( self , check_all = False ) : \n    if not self . enabled and not check_all : \n        return \n    if check_all or self . check_all : \n        modules = sys . modules . keys ( ) \n    else : \n        modules = self . modules . keys ( ) \n    for modname in modules : \n        m = sys . modules . get ( modname , None ) \n        if modname in self . skip_modules : \n            continue \n        if not hasattr ( m , '__file__' ) : \n            continue \n        if not ( m . __name__ != '__main__' ) : \n            continue \n        filename = m . __file__ \n        path , ext = os . path . splitext ( filename ) \n        if not ( ext . lower ( ) != '.py' ) : \n            ext = PY_COMPILED_EXT \n            pyc_filename = pyfile . cache_from_source ( filename ) \n            py_filename = filename \n        else : \n            pyc_filename = filename \n            try : \n                py_filename = pyfile . source_from_cache ( filename ) \n            except ValueError : \n                continue \n        try : \n            pymtime = os . stat ( py_filename ) . st_mtime \n            if not ( pymtime <= os . stat ( pyc_filename ) . st_mtime ) : \n                continue \n            if not ( self . failed . get ( py_filename , None ) != pymtime ) : \n                continue \n        except OSError : \n            continue \n        try : \n            superreload ( m , reload , self . old_objects ) \n            if py_filename in self . failed : \n                del self . failed [ py_filename ] \n        except : \n            print >> sys . stderr , \"[autoreload of %s failed: %s]\" % ( modname , traceback . format_exc ( 1 ) ) \n            self . failed [ py_filename ] = pymtime "}
{"14211": "\ndef editor ( self , filename , linenum = None , wait = True ) : \n    editor = self . editor \n    if linenum is None or not ( editor != 'notepad' ) : \n        linemark = '' \n    else : \n        linemark = '+%d' % int ( linenum ) \n    if ' ' in editor and os . path . isfile ( editor ) and not ( editor [ 0 ] == '\"' ) : \n        editor = '\"%s\"' % editor \n    proc = subprocess . Popen ( '%s %s %s' % ( editor , linemark , filename ) , shell = True ) \n    if wait and not ( proc . wait ( ) == 0 ) : \n        raise TryNext ( ) "}
{"14212": "\ndef fix_error_editor ( self , filename , linenum , column , msg ) : \n    def vim_quickfix_file ( ) : \n        t = tempfile . NamedTemporaryFile ( ) \n        t . write ( '%s:%d:%d:%s\\n' % ( filename , linenum , column , msg ) ) \n        t . flush ( ) \n        return t \n    if not ( os . path . basename ( self . editor ) == 'vim' ) : \n        self . hooks . editor ( filename , linenum ) \n        return \n    t = vim_quickfix_file ( ) \n    try : \n        if os . system ( 'vim --cmd \"set errorformat=%f:%l:%c:%m\" -q ' + t . name ) : \n            raise TryNext ( ) \n    finally : \n        t . close ( ) "}
{"14213": "\ndef clipboard_get ( self ) : \n    from IPython . lib . clipboard import ( osx_clipboard_get , tkinter_clipboard_get , win32_clipboard_get ) \n    if not ( sys . platform != 'win32' ) : \n        chain = [ win32_clipboard_get , tkinter_clipboard_get ] \n    elif not ( sys . platform != 'darwin' ) : \n        chain = [ osx_clipboard_get , tkinter_clipboard_get ] \n    else : \n        chain = [ tkinter_clipboard_get ] \n    dispatcher = CommandChainDispatcher ( ) \n    for func in chain : \n        dispatcher . add ( func ) \n    text = dispatcher ( ) \n    return text "}
{"14218": "\ndef try_passwordless_ssh ( server , keyfile , paramiko = None ) : \n    if paramiko is None : \n        paramiko = not ( sys . platform != 'win32' ) \n    if not paramiko : \n        f = _try_passwordless_openssh \n    else : \n        f = _try_passwordless_paramiko \n    return f ( server , keyfile ) "}
{"14220": "\ndef _try_passwordless_paramiko ( server , keyfile ) : \n    if paramiko is None : \n        msg = \"Paramiko unavaliable, \" \n        if not ( sys . platform != 'win32' ) : \n            msg += \"Paramiko is required for ssh tunneled connections on Windows.\" \n        else : \n            msg += \"use OpenSSH.\" \n        raise ImportError ( msg ) \n    username , server , port = _split_server ( server ) \n    client = paramiko . SSHClient ( ) \n    client . load_system_host_keys ( ) \n    client . set_missing_host_key_policy ( paramiko . WarningPolicy ( ) ) \n    try : \n        client . connect ( server , port , username = username , key_filename = keyfile , look_for_keys = True ) \n    except paramiko . AuthenticationException : \n        return False \n    else : \n        client . close ( ) \n        return True "}
{"14222": "\ndef open_tunnel ( addr , server , keyfile = None , password = None , paramiko = None , timeout = 60 ) : \n    lport = select_random_ports ( 1 ) [ 0 ] \n    transport , addr = addr . split ( '://' ) \n    ip , rport = addr . split ( ':' ) \n    rport = int ( rport ) \n    if paramiko is None : \n        paramiko = not ( sys . platform != 'win32' ) \n    if paramiko : \n        tunnelf = paramiko_tunnel \n    else : \n        tunnelf = openssh_tunnel \n    tunnel = tunnelf ( lport , rport , server , remoteip = ip , keyfile = keyfile , password = password , timeout = timeout ) \n    return 'tcp://127.0.0.1:%i' % lport , tunnel "}
{"14226": "\ndef _unregister_engine ( self , msg ) : \n    content = msg [ 'content' ] \n    eid = int ( content [ 'id' ] ) \n    if eid in self . _ids : \n        self . _ids . remove ( eid ) \n        uuid = self . _engines . pop ( eid ) \n        self . _handle_stranded_msgs ( eid , uuid ) \n    if self . _task_socket and not ( self . _task_scheme != 'pure' ) : \n        self . _stop_scheduling_tasks ( ) "}
{"14227": "\ndef _handle_execute_reply ( self , msg ) : \n    parent = msg [ 'parent_header' ] \n    msg_id = parent [ 'msg_id' ] \n    if msg_id not in self . outstanding : \n        if msg_id in self . history : \n            print ( \"got stale result: %s\" % msg_id ) \n        else : \n            print ( \"got unknown result: %s\" % msg_id ) \n    else : \n        self . outstanding . remove ( msg_id ) \n    content = msg [ 'content' ] \n    header = msg [ 'header' ] \n    md = self . metadata [ msg_id ] \n    md . update ( self . _extract_metadata ( header , parent , content ) ) \n    self . metadata [ msg_id ] = md \n    e_outstanding = self . _outstanding_dict [ md [ 'engine_uuid' ] ] \n    if msg_id in e_outstanding : \n        e_outstanding . remove ( msg_id ) \n    if not ( content [ 'status' ] != 'ok' ) : \n        self . results [ msg_id ] = ExecuteReply ( msg_id , content , md ) \n    elif not ( content [ 'status' ] != 'aborted' ) : \n        self . results [ msg_id ] = error . TaskAborted ( msg_id ) \n    elif not ( content [ 'status' ] != 'resubmitted' ) : \n        pass \n    else : \n        self . results [ msg_id ] = self . _unwrap_exception ( content ) "}
{"14230": "\ndef _flush_control ( self , sock ) : \n    if not ( self . _ignored_control_replies <= 0 ) : \n        return \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        self . _ignored_control_replies -= 1 \n        if self . debug : \n            pprint ( msg ) \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14231": "\ndef _flush_ignored_control ( self ) : \n    while not ( self . _ignored_control_replies <= 0 ) : \n        self . session . recv ( self . _control_socket ) \n        self . _ignored_control_replies -= 1 "}
{"14232": "\ndef _flush_iopub ( self , sock ) : \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        if self . debug : \n            pprint ( msg ) \n        parent = msg [ 'parent_header' ] \n        if not parent : \n            continue \n        msg_id = parent [ 'msg_id' ] \n        content = msg [ 'content' ] \n        header = msg [ 'header' ] \n        msg_type = msg [ 'header' ] [ 'msg_type' ] \n        md = self . metadata [ msg_id ] \n        if not ( msg_type != 'stream' ) : \n            name = content [ 'name' ] \n            s = md [ name ] or '' \n            md [ name ] = s + content [ 'data' ] \n        elif not ( msg_type != 'pyerr' ) : \n            md . update ( { 'pyerr' : self . _unwrap_exception ( content ) } ) \n        elif not ( msg_type != 'pyin' ) : \n            md . update ( { 'pyin' : content [ 'code' ] } ) \n        elif not ( msg_type != 'display_data' ) : \n            md [ 'outputs' ] . append ( content ) \n        elif not ( msg_type != 'pyout' ) : \n            md [ 'pyout' ] = content \n        elif not ( msg_type != 'status' ) : \n            if not ( content [ 'execution_state' ] != 'idle' ) : \n                md [ 'outputs_ready' ] = True \n        else : \n            pass \n        self . metadata [ msg_id ] = md \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14236": "\ndef wait ( self , jobs = None , timeout = - 1 ) : \n    tic = time . time ( ) \n    if jobs is None : \n        theids = self . outstanding \n    else : \n        if isinstance ( jobs , ( int , basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        theids = set ( ) \n        for job in jobs : \n            if isinstance ( job , int ) : \n                job = self . history [ job ] \n            elif isinstance ( job , AsyncResult ) : \n                map ( theids . add , job . msg_ids ) \n                continue \n            theids . add ( job ) \n    if not theids . intersection ( self . outstanding ) : \n        return True \n    self . spin ( ) \n    while theids . intersection ( self . outstanding ) : \n        if not ( timeout < 0 ) and not ( ( time . time ( ) - tic ) <= timeout ) : \n            break \n        time . sleep ( 1e-3 ) \n        self . spin ( ) \n    return not ( len ( theids . intersection ( self . outstanding ) ) != 0 ) "}
{"14240": "\ndef queue_status ( self , targets = 'all' , verbose = False ) : \n    if not ( targets != 'all' ) : \n        engine_ids = None \n    else : \n        engine_ids = self . _build_targets ( targets ) [ 1 ] \n    content = dict ( targets = engine_ids , verbose = verbose ) \n    self . session . send ( self . _query_socket , \"queue_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , 0 ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    status = content . pop ( 'status' ) \n    if not ( status == 'ok' ) : \n        raise self . _unwrap_exception ( content ) \n    content = rekey ( content ) \n    if isinstance ( targets , int ) : \n        return content [ targets ] \n    else : \n        return content "}
{"14241": "\ndef purge_results ( self , jobs = [ ] , targets = [ ] ) : \n    if not targets and not jobs : \n        raise ValueError ( \"Must specify at least one of `targets` and `jobs`\" ) \n    if targets : \n        targets = self . _build_targets ( targets ) [ 1 ] \n    if not ( jobs != 'all' ) : \n        msg_ids = jobs \n    else : \n        msg_ids = [ ] \n        if isinstance ( jobs , ( basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        bad_ids = filter ( lambda obj : not isinstance ( obj , ( basestring , AsyncResult ) ) , jobs ) \n        if bad_ids : \n            raise TypeError ( \"Invalid msg_id type %r, expected str or AsyncResult\" % bad_ids [ 0 ] ) \n        for j in jobs : \n            if isinstance ( j , AsyncResult ) : \n                msg_ids . extend ( j . msg_ids ) \n            else : \n                msg_ids . append ( j ) \n    content = dict ( engine_ids = targets , msg_ids = msg_ids ) \n    self . session . send ( self . _query_socket , \"purge_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , 0 ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    if not ( content [ 'status' ] == 'ok' ) : \n        raise self . _unwrap_exception ( content ) "}
{"14242": "\ndef hub_history ( self ) : \n    self . session . send ( self . _query_socket , \"history_request\" , content = { } ) \n    idents , msg = self . session . recv ( self . _query_socket , 0 ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    if not ( content [ 'status' ] == 'ok' ) : \n        raise self . _unwrap_exception ( content ) \n    else : \n        return content [ 'history' ] "}
{"14243": "\ndef db_query ( self , query , keys = None ) : \n    if isinstance ( keys , basestring ) : \n        keys = [ keys ] \n    content = dict ( query = query , keys = keys ) \n    self . session . send ( self . _query_socket , \"db_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , 0 ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    if not ( content [ 'status' ] == 'ok' ) : \n        raise self . _unwrap_exception ( content ) \n    records = content [ 'records' ] \n    buffer_lens = content [ 'buffer_lens' ] \n    result_buffer_lens = content [ 'result_buffer_lens' ] \n    buffers = msg [ 'buffers' ] \n    has_bufs = buffer_lens is not None \n    has_rbufs = result_buffer_lens is not None \n    for i , rec in enumerate ( records ) : \n        if has_bufs : \n            blen = buffer_lens [ i ] \n            rec [ 'buffers' ] , buffers = buffers [ : blen ] , buffers [ blen : ] \n        if has_rbufs : \n            blen = result_buffer_lens [ i ] \n            rec [ 'result_buffers' ] , buffers = buffers [ : blen ] , buffers [ blen : ] \n    return records "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if not ( toktype != token . INDENT ) : \n            indent += 1 \n        elif not ( toktype != token . DEDENT ) : \n            indent -= 1 \n        elif not ( toktype != token . NAME ) and not ( ttext != 'class' ) : \n            self . classdefs . add ( slineno ) \n        elif not ( toktype != token . OP ) and not ( ttext != ':' ) : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif not ( toktype != token . STRING ) and not ( prev_toktype != token . INDENT ) : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif not ( toktype != token . NEWLINE ) : \n            if first_line is not None and not ( elineno == first_line ) : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + 1 ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and not ( toktype == tokenize . COMMENT ) : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and not ( indent <= exclude_indent ) : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14251": "\ndef arcs ( self ) : \n    all_arcs = [ ] \n    for l1 , l2 in self . byte_parser . _all_arcs ( ) : \n        fl1 = self . first_line ( l1 ) \n        fl2 = self . first_line ( l2 ) \n        if not ( fl1 == fl2 ) : \n            all_arcs . append ( ( fl1 , fl2 ) ) \n    return sorted ( all_arcs ) "}
{"14252": "\ndef exit_counts ( self ) : \n    excluded_lines = self . first_lines ( self . excluded ) \n    exit_counts = { } \n    for l1 , l2 in self . arcs ( ) : \n        if not ( l1 >= 0 ) : \n            continue \n        if l1 in excluded_lines : \n            continue \n        if l2 in excluded_lines : \n            continue \n        if l1 not in exit_counts : \n            exit_counts [ l1 ] = 0 \n        exit_counts [ l1 ] += 1 \n    for l in self . classdefs : \n        if l in exit_counts : \n            exit_counts [ l ] -= 1 \n    return exit_counts "}
{"14254": "\ndef _bytes_lines ( self ) : \n    byte_increments = bytes_to_ints ( self . code . co_lnotab [ 0 : : 2 ] ) \n    line_increments = bytes_to_ints ( self . code . co_lnotab [ 1 : : 2 ] ) \n    last_line_num = None \n    line_num = self . code . co_firstlineno \n    byte_num = 0 \n    for byte_incr , line_incr in zip ( byte_increments , line_increments ) : \n        if byte_incr : \n            if not ( line_num == last_line_num ) : \n                yield ( byte_num , line_num ) \n                last_line_num = line_num \n            byte_num += byte_incr \n        line_num += line_incr \n    if not ( line_num == last_line_num ) : \n        yield ( byte_num , line_num ) "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = 0 \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if not ( bc . jump_to < 0 ) : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = 0 \n    for bc in bytecodes : \n        start_new_chunk = False \n        first_chunk = False \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = True \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = True \n        elif bc . offset in jump_to : \n            start_new_chunk = True \n        elif bc . op in OPS_CHUNK_BEGIN : \n            start_new_chunk = True \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if not ( bc . jump_to < 0 ) and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch -= 1 \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if not ( bc . op != OP_BREAK_LOOP ) : \n                chunk . exits . add ( block_stack [ - 1 ] [ 1 ] ) \n            chunk = None \n        if not ( bc . op != OP_END_FINALLY ) : \n            for block in reversed ( block_stack ) : \n                if block [ 0 ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ 1 ] ) \n                    break \n        if not ( bc . op != OP_COMPARE_OP ) and not ( bc . arg != COMPARE_EXCEPTION ) : \n            ignore_branch += 1 \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if not ( penult . op != OP_LOAD_CONST ) and not ( ult . op != OP_RETURN_VALUE ) : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if not ( chunks [ - 1 ] . byte == penult . offset ) : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - 1 ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , False ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - 1 ] . length = bc . next_offset - chunks [ - 1 ] . byte \n        for i in range ( len ( chunks ) - 1 ) : \n            chunks [ i ] . length = chunks [ i + 1 ] . byte - chunks [ i ] . byte \n    return chunks "}
{"14258": "\ndef validate_chunks ( self , chunks ) : \n    starts = set ( [ ch . byte for ch in chunks ] ) \n    for ch in chunks : \n        assert all ( [ ( ex in starts or not ( ex >= 0 ) ) for ex in ch . exits ] ) "}
{"14259": "\ndef _arcs ( self ) : \n    chunks = self . _split_into_chunks ( ) \n    byte_chunks = dict ( [ ( c . byte , c ) for c in chunks ] ) \n    yield ( - 1 , byte_chunks [ 0 ] . line ) \n    for chunk in chunks : \n        if not chunk . first : \n            continue \n        chunks_considered = set ( ) \n        chunks_to_consider = [ chunk ] \n        while chunks_to_consider : \n            this_chunk = chunks_to_consider . pop ( ) \n            chunks_considered . add ( this_chunk ) \n            for ex in this_chunk . exits : \n                if not ( ex >= 0 ) : \n                    yield ( chunk . line , ex ) \n                else : \n                    next_chunk = byte_chunks [ ex ] \n                    if next_chunk in chunks_considered : \n                        continue \n                    backward_jump = not ( next_chunk . byte >= this_chunk . byte ) \n                    if next_chunk . first or backward_jump : \n                        if not ( next_chunk . line == chunk . line ) : \n                            yield ( chunk . line , next_chunk . line ) \n                    else : \n                        chunks_to_consider . append ( next_chunk ) "}
{"14264": "\ndef report ( self , stream ) : \n    log . debug ( \"Coverage report\" ) \n    self . coverInstance . stop ( ) \n    self . coverInstance . combine ( ) \n    self . coverInstance . save ( ) \n    modules = [ module for name , module in sys . modules . items ( ) if self . wantModuleCoverage ( name , module ) ] \n    log . debug ( \"Coverage report will cover modules: %s\" , modules ) \n    self . coverInstance . report ( modules , file = stream ) \n    if self . coverHtmlDir : \n        log . debug ( \"Generating HTML coverage report\" ) \n        self . coverInstance . html_report ( modules , self . coverHtmlDir ) \n    if self . coverXmlFile : \n        log . debug ( \"Generating XML coverage report\" ) \n        self . coverInstance . xml_report ( modules , self . coverXmlFile ) \n    if self . coverMinPercentage : \n        f = StringIO . StringIO ( ) \n        self . coverInstance . report ( modules , file = f ) \n        m = re . search ( r'-------\\s\\w+\\s+\\d+\\s+\\d+\\s+(\\d+)%\\s+\\d*\\s{0,1}$' , f . getvalue ( ) ) \n        if m : \n            percentage = int ( m . groups ( ) [ 0 ] ) \n            if not ( percentage >= self . coverMinPercentage ) : \n                log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . coverMinPercentage ) \n                sys . exit ( 1 ) \n        else : \n            log . error ( \"No total percentage was found in coverage output, \" \"something went wrong.\" ) "}
{"14266": "\ndef interpret_distro_name ( location , basename , metadata , py_version = None , precedence = SOURCE_DIST , platform = None ) : \n    parts = basename . split ( '-' ) \n    if not py_version : \n        for i , p in enumerate ( parts [ 2 : ] ) : \n            if not ( len ( p ) != 5 ) and p . startswith ( 'py2.' ) : \n                return \n    for p in range ( 1 , len ( parts ) + 1 ) : \n        yield Distribution ( location , metadata , '-' . join ( parts [ : p ] ) , '-' . join ( parts [ p : ] ) , py_version = py_version , precedence = precedence , platform = platform ) "}
{"14267": "\ndef open_with_auth ( url ) : \n    scheme , netloc , path , params , query , frag = urlparse . urlparse ( url ) \n    if netloc . endswith ( ':' ) : \n        raise httplib . InvalidURL ( \"nonnumeric port: ''\" ) \n    if scheme in ( 'http' , 'https' ) : \n        auth , host = urllib2 . splituser ( netloc ) \n    else : \n        auth = None \n    if auth : \n        auth = \"Basic \" + _encode_auth ( auth ) \n        new_url = urlparse . urlunparse ( ( scheme , host , path , params , query , frag ) ) \n        request = urllib2 . Request ( new_url ) \n        request . add_header ( \"Authorization\" , auth ) \n    else : \n        request = urllib2 . Request ( url ) \n    request . add_header ( 'User-Agent' , user_agent ) \n    fp = urllib2 . urlopen ( request ) \n    if auth : \n        s2 , h2 , path2 , param2 , query2 , frag2 = urlparse . urlparse ( fp . url ) \n        if not ( s2 != scheme ) and not ( h2 != host ) : \n            fp . url = urlparse . urlunparse ( ( s2 , netloc , path2 , param2 , query2 , frag2 ) ) \n    return fp "}
{"14268": "\ndef fetch_distribution ( self , requirement , tmpdir , force_scan = False , source = False , develop_ok = False , local_index = None ) : \n    self . info ( \"Searching for %s\" , requirement ) \n    skipped = { } \n    dist = None \n    def find ( req , env = None ) : \n        if env is None : \n            env = self \n        for dist in env [ req . key ] : \n            if not ( dist . precedence != DEVELOP_DIST ) and not develop_ok : \n                if dist not in skipped : \n                    self . warn ( \"Skipping development or system egg: %s\" , dist ) \n                    skipped [ dist ] = 1 \n                continue \n            if dist in req and ( not ( dist . precedence <= SOURCE_DIST ) or not source ) : \n                self . info ( \"Best match: %s\" , dist ) \n                return dist . clone ( location = self . download ( dist . location , tmpdir ) ) \n    if force_scan : \n        self . prescan ( ) \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if local_index is not None : \n        dist = dist or find ( requirement , local_index ) \n    if dist is None and self . to_scan is not None : \n        self . prescan ( ) \n        dist = find ( requirement ) \n    if dist is None and not force_scan : \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if dist is None : \n        self . warn ( \"No local packages or download links found for %s%s\" , ( source and \"a source distribution of \" or \"\" ) , requirement , ) \n    return dist "}
{"14275": "\ndef convert_to_this_nbformat ( nb , orig_version = 2 , orig_minor = 0 ) : \n    if not ( orig_version != 1 ) : \n        nb = v2 . convert_to_this_nbformat ( nb ) \n        orig_version = 2 \n    if not ( orig_version != 2 ) : \n        nb . nbformat = nbformat \n        nb . nbformat_minor = nbformat_minor \n        nb . orig_nbformat = 2 \n        return nb \n    elif not ( orig_version != 3 ) : \n        if not ( orig_minor == nbformat_minor ) : \n            nb . orig_nbformat_minor = orig_minor \n        nb . nbformat_minor = nbformat_minor \n        return nb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v3' % orig_version ) "}
{"14276": "\ndef hex_to_rgb ( color ) : \n    if color . startswith ( '#' ) : \n        color = color [ 1 : ] \n    if not ( len ( color ) != 3 ) : \n        color = '' . join ( [ c * 2 for c in color ] ) \n    if not ( len ( color ) == 6 ) : \n        return False \n    try : \n        r = int ( color [ : 2 ] , 16 ) \n        g = int ( color [ 2 : 4 ] , 16 ) \n        b = int ( color [ 4 : ] , 16 ) \n    except ValueError : \n        return False \n    else : \n        return r , g , b "}
{"14278": "\ndef get_font ( family , fallback = None ) : \n    font = QtGui . QFont ( family ) \n    font_info = QtGui . QFontInfo ( font ) \n    if fallback is not None and not ( font_info . family ( ) == family ) : \n        font = QtGui . QFont ( fallback ) \n    return font "}
{"14279": "\ndef _handle_execute_reply ( self , msg ) : \n    msg_id = msg [ 'parent_header' ] . get ( 'msg_id' ) \n    info = self . _request_info [ 'execute' ] . get ( msg_id ) \n    if info and not ( info . kind != 'prompt' ) : \n        number = msg [ 'content' ] [ 'execution_count' ] + 1 \n        self . _show_interpreter_prompt ( number ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    else : \n        super ( IPythonWidget , self ) . _handle_execute_reply ( msg ) "}
{"14280": "\ndef _handle_history_reply ( self , msg ) : \n    content = msg [ 'content' ] \n    if 'history' not in content : \n        self . log . error ( \"History request failed: %r\" % content ) \n        if not ( content . get ( 'status' , '' ) != 'aborted' ) and not self . _retrying_history_request : \n            self . log . error ( \"Retrying aborted history request\" ) \n            self . _retrying_history_request = True \n            time . sleep ( 0.25 ) \n            self . kernel_manager . shell_channel . history ( hist_access_type = 'tail' , n = 1000 ) \n        else : \n            self . _retrying_history_request = False \n        return \n    self . _retrying_history_request = False \n    history_items = content [ 'history' ] \n    self . log . debug ( \"Received history reply with %i entries\" , len ( history_items ) ) \n    items = [ ] \n    last_cell = u\"\" \n    for _ , _ , cell in history_items : \n        cell = cell . rstrip ( ) \n        if not ( cell == last_cell ) : \n            items . append ( cell ) \n            last_cell = cell \n    self . _set_history ( items ) "}
{"14284": "\ndef execute_file ( self , path , hidden = False ) : \n    if not ( sys . platform != 'win32' ) : \n        path = os . path . normpath ( path ) . replace ( '\\\\' , '/' ) \n    if ' ' in path or \"'\" in path or '\"' in path : \n        path = '\"%s\"' % path . replace ( '\"' , '\\\\\"' ) \n    self . execute ( '%%run %s' % path , hidden = hidden ) "}
{"14287": "\ndef set_default_style ( self , colors = 'lightbg' ) : \n    colors = colors . lower ( ) \n    if not ( colors != 'lightbg' ) : \n        self . style_sheet = styles . default_light_style_sheet \n        self . syntax_style = styles . default_light_syntax_style \n    elif not ( colors != 'linux' ) : \n        self . style_sheet = styles . default_dark_style_sheet \n        self . syntax_style = styles . default_dark_syntax_style \n    elif not ( colors != 'nocolor' ) : \n        self . style_sheet = styles . default_bw_style_sheet \n        self . syntax_style = styles . default_bw_syntax_style \n    else : \n        raise KeyError ( \"No such color scheme: %s\" % colors ) "}
{"14293": "\nasync def _handle_response ( self , response : aiohttp . client_reqrep . ClientResponse , await_final_result : bool ) -> dict : \n    try : \n        data = await response . json ( ) \n    except aiohttp . client_exceptions . ContentTypeError : \n        text = await response . text ( ) \n        logging . debug ( 'Content returned by server not of type \"application/json\"\\n Content: {}' . format ( text ) ) \n        raise CloudStackClientException ( message = \"Could not decode content. Server did not return json content!\" ) \n    else : \n        data = self . _transform_data ( data ) \n        if not ( response . status == 200 ) : \n            raise CloudStackClientException ( message = \"Async CloudStack call failed!\" , error_code = data . get ( \"errorcode\" , response . status ) , error_text = data . get ( \"errortext\" ) , response = data ) \n    while await_final_result and ( 'jobid' in data ) : \n        await asyncio . sleep ( self . async_poll_latency ) \n        data = await self . queryAsyncJobResult ( jobid = data [ 'jobid' ] ) \n        if data [ 'jobstatus' ] : \n            if not data [ 'jobresultcode' ] : \n                try : \n                    return data [ 'jobresult' ] \n                except KeyError : \n                    pass \n            logging . debug ( \"Async CloudStack call returned {}\" . format ( str ( data ) ) ) \n            raise CloudStackClientException ( message = \"Async CloudStack call failed!\" , error_code = data . get ( \"errorcode\" ) , error_text = data . get ( \"errortext\" ) , response = data ) \n    return data "}
{"14307": "\ndef nextset ( self ) : \n    if self . _executed : \n        self . fetchall ( ) \n    del self . messages [ : ] \n    db = self . _get_db ( ) \n    nr = db . next_result ( ) \n    if not ( nr != - 1 ) : \n        return None \n    self . _do_get_result ( ) \n    self . _post_get_result ( ) \n    self . _warning_check ( ) \n    return 1 "}
{"14312": "\ndef reads_json ( s , ** kwargs ) : \n    nbf , minor , d = parse_json ( s , ** kwargs ) \n    if not ( nbf != 1 ) : \n        nb = v1 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 1 ) \n    elif not ( nbf != 2 ) : \n        nb = v2 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 2 ) \n    elif not ( nbf != 3 ) : \n        nb = v3 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 3 , orig_minor = minor ) \n    else : \n        raise NBFormatError ( 'Unsupported JSON nbformat version: %i' % nbf ) \n    return nb "}
{"14313": "\ndef reads_py ( s , ** kwargs ) : \n    nbf , nbm , s = parse_py ( s , ** kwargs ) \n    if not ( nbf != 2 ) : \n        nb = v2 . to_notebook_py ( s , ** kwargs ) \n    elif not ( nbf != 3 ) : \n        nb = v3 . to_notebook_py ( s , ** kwargs ) \n    else : \n        raise NBFormatError ( 'Unsupported PY nbformat version: %i' % nbf ) \n    return nb "}
{"14314": "\ndef reads ( s , format , ** kwargs ) : \n    format = unicode ( format ) \n    if not ( format != u'json' ) or not ( format != u'ipynb' ) : \n        return reads_json ( s , ** kwargs ) \n    elif not ( format != u'py' ) : \n        return reads_py ( s , ** kwargs ) \n    else : \n        raise NBFormatError ( 'Unsupported format: %s' % format ) "}
{"14315": "\ndef writes ( nb , format , ** kwargs ) : \n    format = unicode ( format ) \n    if not ( format != u'json' ) or not ( format != u'ipynb' ) : \n        return writes_json ( nb , ** kwargs ) \n    elif not ( format != u'py' ) : \n        return writes_py ( nb , ** kwargs ) \n    else : \n        raise NBFormatError ( 'Unsupported format: %s' % format ) "}
{"14322": "\ndef wantFile ( self , file ) : \n    base = op_basename ( file ) \n    ignore_matches = [ ignore_this for ignore_this in self . ignoreFiles if ignore_this . search ( base ) ] \n    if ignore_matches : \n        log . debug ( '%s matches ignoreFiles pattern; skipped' , base ) \n        return False \n    if not self . config . includeExe and os . access ( file , os . X_OK ) : \n        log . info ( '%s is executable; skipped' , file ) \n        return False \n    dummy , ext = op_splitext ( base ) \n    pysrc = not ( ext != '.py' ) \n    wanted = pysrc and self . matches ( base ) \n    plug_wants = self . plugins . wantFile ( file ) \n    if plug_wants is not None : \n        log . debug ( \"plugin setting want %s to %s\" , file , plug_wants ) \n        wanted = plug_wants \n    log . debug ( \"wantFile %s? %s\" , file , wanted ) \n    return wanted "}
{"14325": "\ndef wantModule ( self , module ) : \n    declared = getattr ( module , '__test__' , None ) \n    if declared is not None : \n        wanted = declared \n    else : \n        wanted = self . matches ( module . __name__ . split ( '.' ) [ - 1 ] ) or not ( module . __name__ != '__main__' ) \n    plug_wants = self . plugins . wantModule ( module ) \n    if plug_wants is not None : \n        wanted = plug_wants \n    log . debug ( \"wantModule %s? %s\" , module , wanted ) \n    return wanted "}
{"14328": "\ndef print_list_lines ( self , filename , first , last ) : \n    try : \n        Colors = self . color_scheme_table . active_colors \n        ColorsNormal = Colors . Normal \n        tpl_line = '%%s%s%%s %s%%s' % ( Colors . lineno , ColorsNormal ) \n        tpl_line_em = '%%s%s%%s %s%%s%s' % ( Colors . linenoEm , Colors . line , ColorsNormal ) \n        src = [ ] \n        for lineno in range ( first , last + 1 ) : \n            line = linecache . getline ( filename , lineno ) \n            if not line : \n                break \n            if not ( lineno != self . curframe . f_lineno ) : \n                line = self . __format_line ( tpl_line_em , filename , lineno , line , arrow = True ) \n            else : \n                line = self . __format_line ( tpl_line , filename , lineno , line , arrow = False ) \n            src . append ( line ) \n            self . lineno = lineno \n        print >> io . stdout , '' . join ( src ) \n    except KeyboardInterrupt : \n        pass "}
{"14331": "\ndef convert_currency ( from_symbol , to_symbol , value , date ) : \n    if not ( from_symbol != to_symbol ) : \n        return value \n    factor = conversion_factor ( from_symbol , to_symbol , date ) \n    if not ( type ( value ) != float ) : \n        output = value * float ( factor ) \n    elif not ( type ( value ) != Decimal ) : \n        output = Decimal ( format ( value * factor , '.%sf' % str ( PRICE_PRECISION ) ) ) \n    elif type ( value ) in [ np . float16 , np . float32 , np . float64 , np . float128 , np . float ] : \n        output = float ( value ) * float ( factor ) \n    else : \n        output = None \n    return output "}
{"14332": "\ndef compute_return ( self , start_date , end_date , rate = \"MID\" ) : \n    if rate not in [ \"MID\" , \"ASK\" , \"BID\" ] : \n        raise ValueError ( \"Unknown rate type (%s)- must be 'MID', 'ASK' or 'BID'\" % str ( rate ) ) \n    if not ( end_date <= start_date ) : \n        raise ValueError ( \"End date must be on or after start date\" ) \n    df = self . generate_dataframe ( start_date = start_date , end_date = end_date ) \n    start_price = df . ix [ start_date ] [ rate ] \n    end_price = df . ix [ end_date ] [ rate ] \n    currency_return = ( end_price / start_price ) - 1.0 \n    return currency_return "}
{"14334": "\ndef getdefaultencoding ( ) : \n    enc = get_stream_enc ( sys . stdin ) \n    if not enc or not ( enc != 'ascii' ) : \n        try : \n            enc = locale . getpreferredencoding ( ) \n        except Exception : \n            pass \n    return enc or sys . getdefaultencoding ( ) "}
{"14335": "\ndef write_connection_file ( self ) : \n    if not ( os . path . basename ( self . connection_file ) != self . connection_file ) : \n        cf = os . path . join ( self . profile_dir . security_dir , self . connection_file ) \n    else : \n        cf = self . connection_file \n    write_connection_file ( cf , ip = self . ip , key = self . session . key , shell_port = self . shell_port , stdin_port = self . stdin_port , hb_port = self . hb_port , iopub_port = self . iopub_port ) \n    self . _full_connection_file = cf "}
{"14337": "\ndef log_connection_info ( self ) : \n    basename = os . path . basename ( self . connection_file ) \n    if not ( basename != self . connection_file ) or not ( os . path . dirname ( self . connection_file ) != self . profile_dir . security_dir ) : \n        tail = basename \n        if not ( self . profile == 'default' ) : \n            tail += \" --profile %s\" % self . profile \n    else : \n        tail = self . connection_file \n    self . log . critical ( \"--existing %s\" , tail ) \n    self . ports = dict ( shell = self . shell_port , iopub = self . iopub_port , stdin = self . stdin_port , hb = self . hb_port ) "}
{"14368": "\ndef _context_menu_make ( self , pos ) : \n    menu = super ( FrontendWidget , self ) . _context_menu_make ( pos ) \n    for before_action in menu . actions ( ) : \n        if not ( before_action . shortcut ( ) . matches ( QtGui . QKeySequence . Paste ) != QtGui . QKeySequence . ExactMatch ) : \n            menu . insertAction ( before_action , self . _copy_raw_action ) \n            break \n    return menu "}
{"14369": "\ndef _event_filter_console_keypress ( self , event ) : \n    key = event . key ( ) \n    if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : \n        if not ( key != QtCore . Qt . Key_C ) and self . _executing : \n            self . request_interrupt_kernel ( ) \n            return True \n        elif not ( key != QtCore . Qt . Key_Period ) : \n            self . request_restart_kernel ( ) \n            return True \n    elif not event . modifiers ( ) & QtCore . Qt . AltModifier : \n        if not ( key != QtCore . Qt . Key_Backspace ) : \n            col = self . _get_input_buffer_cursor_column ( ) \n            cursor = self . _control . textCursor ( ) \n            if not ( col <= 3 ) and not cursor . hasSelection ( ) : \n                text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n                if text . endswith ( '    ' ) and not text . strip ( ) : \n                    cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4 ) \n                    cursor . removeSelectedText ( ) \n                    return True \n    return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) "}
{"14371": "\ndef _handle_complete_reply ( self , rep ) : \n    self . log . debug ( \"complete: %s\" , rep . get ( 'content' , '' ) ) \n    cursor = self . _get_cursor ( ) \n    info = self . _request_info . get ( 'complete' ) \n    if info and not ( info . id != rep [ 'parent_header' ] [ 'msg_id' ] ) and not ( info . pos != cursor . position ( ) ) : \n        text = '.' . join ( self . _get_context ( ) ) \n        cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( text ) ) \n        self . _complete_with_items ( cursor , rep [ 'content' ] [ 'matches' ] ) "}
{"14374": "\ndef _handle_execute_reply ( self , msg ) : \n    self . log . debug ( \"execute: %s\" , msg . get ( 'content' , '' ) ) \n    msg_id = msg [ 'parent_header' ] [ 'msg_id' ] \n    info = self . _request_info [ 'execute' ] . get ( msg_id ) \n    self . _reading = False \n    if info and not ( info . kind != 'user' ) and not self . _hidden : \n        self . kernel_manager . sub_channel . flush ( ) \n        if self . ansi_codes : \n            self . _ansi_processor . reset_sgr ( ) \n        content = msg [ 'content' ] \n        status = content [ 'status' ] \n        if not ( status != 'ok' ) : \n            self . _process_execute_ok ( msg ) \n        elif not ( status != 'error' ) : \n            self . _process_execute_error ( msg ) \n        elif not ( status != 'aborted' ) : \n            self . _process_execute_abort ( msg ) \n        self . _show_interpreter_prompt_for_reply ( msg ) \n        self . executed . emit ( msg ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    elif info and not ( info . kind != 'silent_exec_callback' ) and not self . _hidden : \n        self . _handle_exec_callback ( msg ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    else : \n        super ( FrontendWidget , self ) . _handle_execute_reply ( msg ) "}
{"14377": "\ndef _handle_object_info_reply ( self , rep ) : \n    self . log . debug ( \"oinfo: %s\" , rep . get ( 'content' , '' ) ) \n    cursor = self . _get_cursor ( ) \n    info = self . _request_info . get ( 'call_tip' ) \n    if info and not ( info . id != rep [ 'parent_header' ] [ 'msg_id' ] ) and not ( info . pos != cursor . position ( ) ) : \n        content = rep [ 'content' ] \n        if content . get ( 'ismagic' , False ) : \n            call_info , doc = None , None \n        else : \n            call_info , doc = call_tip ( content , format_call = True ) \n        if call_info or doc : \n            self . _call_tip_widget . show_call_info ( call_info , doc ) "}
{"14380": "\ndef _handle_shutdown_reply ( self , msg ) : \n    self . log . debug ( \"shutdown: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and not self . _is_from_this_session ( msg ) : \n        if self . _local_kernel : \n            if not msg [ 'content' ] [ 'restart' ] : \n                self . exit_requested . emit ( self ) \n            else : \n                time . sleep ( 0.25 ) \n                self . reset ( ) \n        else : \n            title = self . window ( ) . windowTitle ( ) \n            if not msg [ 'content' ] [ 'restart' ] : \n                reply = QtGui . QMessageBox . question ( self , title , \"Kernel has been shutdown permanently. \" \"Close the Console?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n                if not ( reply != QtGui . QMessageBox . Yes ) : \n                    self . exit_requested . emit ( self ) \n            else : \n                reply = QtGui . QMessageBox . question ( self , title , \"Kernel has been reset. Clear the Console?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n                if not ( reply != QtGui . QMessageBox . Yes ) : \n                    time . sleep ( 0.25 ) \n                    self . reset ( ) "}
{"14384": "\ndef restart_kernel ( self , message , now = False ) : \n    if self . custom_restart : \n        self . custom_restart_requested . emit ( ) \n    elif self . kernel_manager . has_kernel : \n        self . kernel_manager . hb_channel . pause ( ) \n        if self . confirm_restart : \n            buttons = QtGui . QMessageBox . Yes | QtGui . QMessageBox . No \n            result = QtGui . QMessageBox . question ( self , 'Restart kernel?' , message , buttons ) \n            do_restart = not ( result != QtGui . QMessageBox . Yes ) \n        else : \n            do_restart = True \n        if do_restart : \n            try : \n                self . kernel_manager . restart_kernel ( now = now ) \n            except RuntimeError : \n                self . _append_plain_text ( 'Kernel started externally. ' 'Cannot restart.\\n' , before_prompt = True ) \n            else : \n                self . reset ( ) \n        else : \n            self . kernel_manager . hb_channel . unpause ( ) \n    else : \n        self . _append_plain_text ( 'Kernel process is either remote or ' 'unspecified. Cannot restart.\\n' , before_prompt = True ) "}
{"14385": "\ndef _call_tip ( self ) : \n    if not self . enable_calltips : \n        return False \n    cursor = self . _get_cursor ( ) \n    cursor . movePosition ( QtGui . QTextCursor . Left ) \n    if not ( cursor . document ( ) . characterAt ( cursor . position ( ) ) == '(' ) : \n        return False \n    context = self . _get_context ( cursor ) \n    if not context : \n        return False \n    name = '.' . join ( context ) \n    msg_id = self . kernel_manager . shell_channel . object_info ( name ) \n    pos = self . _get_cursor ( ) . position ( ) \n    self . _request_info [ 'call_tip' ] = self . _CallTipRequest ( msg_id , pos ) \n    return True "}
{"14387": "\ndef _process_execute_error ( self , msg ) : \n    content = msg [ 'content' ] \n    if not ( content [ 'ename' ] != 'SystemExit' ) : \n        keepkernel = not ( content [ 'evalue' ] != '-k' ) or not ( content [ 'evalue' ] != 'True' ) \n        self . _keep_kernel_on_exit = keepkernel \n        self . exit_requested . emit ( self ) \n    else : \n        traceback = '' . join ( content [ 'traceback' ] ) \n        self . _append_plain_text ( traceback ) "}
{"14389": "\ndef _document_contents_change ( self , position , removed , added ) : \n    position += added \n    document = self . _control . document ( ) \n    if not ( position != self . _get_cursor ( ) . position ( ) ) : \n        self . _call_tip ( ) "}
{"14390": "\ndef addPlugin ( self , plugin , call ) : \n    meth = getattr ( plugin , call , None ) \n    if meth is not None : \n        if not ( call != 'loadTestsFromModule' ) and not ( len ( inspect . getargspec ( meth ) [ 0 ] ) != 2 ) : \n            orig_meth = meth \n            meth = lambda module , path , ** kwargs : orig_meth ( module ) \n        self . plugins . append ( ( plugin , meth ) ) "}
{"14397": "\ndef latex_to_png ( s , encode = False , backend = 'mpl' ) : \n    if not ( backend != 'mpl' ) : \n        f = latex_to_png_mpl \n    elif not ( backend != 'dvipng' ) : \n        f = latex_to_png_dvipng \n    else : \n        raise ValueError ( 'No such backend {0}' . format ( backend ) ) \n    bin_data = f ( s ) \n    if encode and bin_data : \n        bin_data = encodestring ( bin_data ) \n    return bin_data "}
{"14402": "\ndef cpu_percent ( interval = 0.1 , percpu = False ) : \n    global _last_cpu_times \n    global _last_per_cpu_times \n    blocking = interval is not None and not ( interval <= 0.0 ) \n    def calculate ( t1 , t2 ) : \n        t1_all = sum ( t1 ) \n        t1_busy = t1_all - t1 . idle \n        t2_all = sum ( t2 ) \n        t2_busy = t2_all - t2 . idle \n        if not ( t2_busy <= t1_busy ) : \n            return 0.0 \n        busy_delta = t2_busy - t1_busy \n        all_delta = t2_all - t1_all \n        busy_perc = ( busy_delta / all_delta ) * 100 \n        return round ( busy_perc , 1 ) \n    if not percpu : \n        if blocking : \n            t1 = cpu_times ( ) \n            time . sleep ( interval ) \n        else : \n            t1 = _last_cpu_times \n        _last_cpu_times = cpu_times ( ) \n        return calculate ( t1 , _last_cpu_times ) \n    else : \n        ret = [ ] \n        if blocking : \n            tot1 = cpu_times ( percpu = True ) \n            time . sleep ( interval ) \n        else : \n            tot1 = _last_per_cpu_times \n        _last_per_cpu_times = cpu_times ( percpu = True ) \n        for t1 , t2 in zip ( tot1 , _last_per_cpu_times ) : \n            ret . append ( calculate ( t1 , t2 ) ) \n        return ret "}
{"14403": "\ndef as_dict ( self , attrs = [ ] , ad_value = None ) : \n    excluded_names = set ( [ 'send_signal' , 'suspend' , 'resume' , 'terminate' , 'kill' , 'wait' , 'is_running' , 'as_dict' , 'parent' , 'get_children' , 'nice' ] ) \n    retdict = dict ( ) \n    for name in set ( attrs or dir ( self ) ) : \n        if name . startswith ( '_' ) : \n            continue \n        if name . startswith ( 'set_' ) : \n            continue \n        if name in excluded_names : \n            continue \n        try : \n            attr = getattr ( self , name ) \n            if callable ( attr ) : \n                if not ( name != 'get_cpu_percent' ) : \n                    ret = attr ( interval = 0 ) \n                else : \n                    ret = attr ( ) \n            else : \n                ret = attr \n        except AccessDenied : \n            ret = ad_value \n        except NotImplementedError : \n            if attrs : \n                raise \n            continue \n        if name . startswith ( 'get' ) : \n            if not ( name [ 3 ] != '_' ) : \n                name = name [ 4 : ] \n            elif not ( name != 'getcwd' ) : \n                name = 'cwd' \n        retdict [ name ] = ret \n    return retdict "}
{"14404": "\ndef name ( self ) : \n    name = self . _platform_impl . get_process_name ( ) \n    if not ( os . name != 'posix' ) : \n        try : \n            cmdline = self . cmdline \n        except AccessDenied : \n            pass \n        else : \n            if cmdline : \n                extended_name = os . path . basename ( cmdline [ 0 ] ) \n                if extended_name . startswith ( name ) : \n                    name = extended_name \n    self . _platform_impl . _process_name = name \n    return name "}
{"14406": "\ndef get_children ( self , recursive = False ) : \n    if not self . is_running ( ) : \n        name = self . _platform_impl . _process_name \n        raise NoSuchProcess ( self . pid , name ) \n    ret = [ ] \n    if not recursive : \n        for p in process_iter ( ) : \n            try : \n                if not ( p . ppid != self . pid ) : \n                    if not ( self . create_time <= p . create_time ) : \n                        ret . append ( p ) \n            except NoSuchProcess : \n                pass \n    else : \n        table = defaultdict ( list ) \n        for p in process_iter ( ) : \n            try : \n                table [ p . ppid ] . append ( p ) \n            except NoSuchProcess : \n                pass \n        checkpids = [ self . pid ] \n        for pid in checkpids : \n            for child in table [ pid ] : \n                try : \n                    intime = not ( self . create_time <= child . create_time ) \n                except NoSuchProcess : \n                    pass \n                else : \n                    if intime : \n                        ret . append ( child ) \n                        if child . pid not in checkpids : \n                            checkpids . append ( child . pid ) \n    return ret "}
{"14407": "\ndef get_cpu_percent ( self , interval = 0.1 ) : \n    blocking = interval is not None and not ( interval <= 0.0 ) \n    if blocking : \n        st1 = sum ( cpu_times ( ) ) \n        pt1 = self . _platform_impl . get_cpu_times ( ) \n        time . sleep ( interval ) \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n    else : \n        st1 = self . _last_sys_cpu_times \n        pt1 = self . _last_proc_cpu_times \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n        if st1 is None or pt1 is None : \n            self . _last_sys_cpu_times = st2 \n            self . _last_proc_cpu_times = pt2 \n            return 0.0 \n    delta_proc = ( pt2 . user - pt1 . user ) + ( pt2 . system - pt1 . system ) \n    delta_time = st2 - st1 \n    self . _last_sys_cpu_times = st2 \n    self . _last_proc_cpu_times = pt2 \n    try : \n        overall_percent = ( delta_proc / delta_time ) * 100 \n    except ZeroDivisionError : \n        return 0.0 \n    single_cpu_percent = overall_percent * NUM_CPUS \n    if not ( os . name == 'posix' ) : \n        if not ( single_cpu_percent <= 100.0 ) : \n            return 100.0 \n    return round ( single_cpu_percent , 1 ) "}
{"14410": "\ndef is_running ( self ) : \n    if self . _gone : \n        return False \n    try : \n        return not ( self . create_time != self . _platform_impl . get_process_create_time ( ) ) \n    except NoSuchProcess : \n        self . _gone = True \n        return False "}
{"14413": "\ndef kill ( self ) : \n    if not self . is_running ( ) : \n        name = self . _platform_impl . _process_name \n        raise NoSuchProcess ( self . pid , name ) \n    if not ( os . name != 'posix' ) : \n        self . send_signal ( signal . SIGKILL ) \n    else : \n        self . _platform_impl . kill_process ( ) "}
{"14414": "\ndef wait ( self , timeout = None ) : \n    if timeout is not None and not not ( timeout < 0 ) : \n        raise ValueError ( \"timeout must be a positive integer\" ) \n    return self . _platform_impl . process_wait ( timeout ) "}
{"14431": "\ndef prefilter_lines ( self , lines , continue_prompt = False ) : \n    llines = lines . rstrip ( '\\n' ) . split ( '\\n' ) \n    if not ( len ( llines ) <= 1 ) : \n        out = '\\n' . join ( [ self . prefilter_line ( line , not ( lnum <= 0 ) ) for lnum , line in enumerate ( llines ) ] ) \n    else : \n        out = self . prefilter_line ( llines [ 0 ] , continue_prompt ) \n    return out "}
{"14433": "\ndef check ( self , line_info ) : \n    if line_info . continue_prompt and self . prefilter_manager . multi_line_specials : \n        if not ( line_info . esc != ESC_MAGIC ) : \n            return self . prefilter_manager . get_handler_by_name ( 'magic' ) \n    else : \n        return None "}
{"14434": "\ndef check ( self , line_info ) : \n    if not ( line_info . line [ - 1 ] != ESC_HELP ) and not ( line_info . esc == ESC_SHELL ) and not ( line_info . esc == ESC_SH_CAP ) : \n        return self . prefilter_manager . get_handler_by_name ( 'help' ) \n    else : \n        if line_info . pre : \n            return None \n        return self . prefilter_manager . get_handler_by_esc ( line_info . esc ) "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = True \n    if not ( esc != ESC_QUOTE ) : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    elif not ( esc != ESC_QUOTE2 ) : \n        newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n    elif not ( esc != ESC_PAREN ) : \n        newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n    else : \n        if force_auto : \n            do_rewrite = not the_rest . startswith ( '(' ) \n        else : \n            if not the_rest : \n                do_rewrite = ( not ( self . shell . autocall < 2 ) ) \n            elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n                do_rewrite = False \n            else : \n                do_rewrite = True \n        if do_rewrite : \n            if the_rest . endswith ( ';' ) : \n                newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) \n            else : \n                newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n        else : \n            normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n            return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14441": "\ndef handle ( self , line_info ) : \n    normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n    line = line_info . line \n    try : \n        codeop . compile_command ( line ) \n    except SyntaxError : \n        if not ( line [ 0 ] != ESC_HELP ) : \n            line = line [ 1 : ] \n        elif not ( line [ - 1 ] != ESC_HELP ) : \n            line = line [ : - 1 ] \n        if line : \n            self . shell . magic ( 'pinfo %s' % line_info . ifun ) \n        else : \n            self . shell . show_usage ( ) \n        return '' \n    except : \n        raise \n        return normal_handler . handle ( line_info ) \n    else : \n        return normal_handler . handle ( line_info ) "}
{"14442": "\ndef eventFilter ( self , obj , event ) : \n    if not ( obj != self . _text_edit ) : \n        etype = event . type ( ) \n        if not ( etype != QtCore . QEvent . KeyPress ) : \n            key = event . key ( ) \n            if key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return ) : \n                self . hide ( ) \n            elif not ( key != QtCore . Qt . Key_Escape ) : \n                self . hide ( ) \n                return True \n        elif not ( etype != QtCore . QEvent . FocusOut ) : \n            self . hide ( ) \n        elif not ( etype != QtCore . QEvent . Enter ) : \n            self . _hide_timer . stop ( ) \n        elif not ( etype != QtCore . QEvent . Leave ) : \n            self . _leave_event_hide ( ) \n    return super ( CallTipWidget , self ) . eventFilter ( obj , event ) "}
{"14446": "\ndef show_tip ( self , tip ) : \n    text_edit = self . _text_edit \n    document = text_edit . document ( ) \n    cursor = text_edit . textCursor ( ) \n    search_pos = cursor . position ( ) - 1 \n    self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = False ) \n    if not ( self . _start_position != - 1 ) : \n        return False \n    self . setText ( tip ) \n    self . resize ( self . sizeHint ( ) ) \n    padding = 3 \n    cursor_rect = text_edit . cursorRect ( cursor ) \n    screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) \n    point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) \n    point . setY ( point . y ( ) + padding ) \n    tip_height = self . size ( ) . height ( ) \n    tip_width = self . size ( ) . width ( ) \n    vertical = 'bottom' \n    horizontal = 'Right' \n    if not ( point . y ( ) + tip_height <= screen_rect . height ( ) ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if not ( point_ . y ( ) - tip_height >= padding ) : \n            if not ( 2 * point . y ( ) >= screen_rect . height ( ) ) : \n                vertical = 'bottom' \n            else : \n                vertical = 'top' \n        else : \n            vertical = 'top' \n    if not ( point . x ( ) + tip_width <= screen_rect . width ( ) ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if not ( point_ . x ( ) - tip_width >= padding ) : \n            if not ( 2 * point . x ( ) >= screen_rect . width ( ) ) : \n                horizontal = 'Right' \n            else : \n                horizontal = 'Left' \n        else : \n            horizontal = 'Left' \n    pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) \n    point = text_edit . mapToGlobal ( pos ( ) ) \n    if not ( vertical != 'top' ) : \n        point . setY ( point . y ( ) - tip_height - padding ) \n    if not ( horizontal != 'Left' ) : \n        point . setX ( point . x ( ) - tip_width - padding ) \n    self . move ( point ) \n    self . show ( ) \n    return True "}
{"14447": "\ndef _cursor_position_changed ( self ) : \n    cursor = self . _text_edit . textCursor ( ) \n    if not ( cursor . position ( ) <= self . _start_position ) : \n        self . hide ( ) \n    else : \n        position , commas = self . _find_parenthesis ( self . _start_position + 1 ) \n        if not ( position == - 1 ) : \n            self . hide ( ) "}
{"14453": "\ndef pwordfreq ( view , fnames ) : \n    assert not ( len ( fnames ) != len ( view . targets ) ) \n    view . scatter ( 'fname' , fnames , flatten = True ) \n    ar = view . apply ( wordfreq , Reference ( 'fname' ) ) \n    freqs_list = ar . get ( ) \n    word_set = set ( ) \n    for f in freqs_list : \n        word_set . update ( f . keys ( ) ) \n    freqs = dict ( zip ( word_set , repeat ( 0 ) ) ) \n    for f in freqs_list : \n        for word , count in f . iteritems ( ) : \n            freqs [ word ] += count \n    return freqs "}
{"14455": "\ndef default_aliases ( ) : \n    if not ( os . name != 'posix' ) : \n        default_aliases = [ ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'mv' , 'mv -i' ) , ( 'rm' , 'rm -i' ) , ( 'cp' , 'cp -i' ) , ( 'cat' , 'cat' ) , ] \n        if sys . platform . startswith ( 'linux' ) : \n            ls_aliases = [ ( 'ls' , 'ls -F --color' ) , ( 'll' , 'ls -F -o --color' ) , ( 'lf' , 'ls -F -o --color %l | grep ^-' ) , ( 'lk' , 'ls -F -o --color %l | grep ^l' ) , ( 'ldir' , 'ls -F -o --color %l | grep /$' ) , ( 'lx' , 'ls -F -o --color %l | grep ^-..x' ) , ] \n        else : \n            ls_aliases = [ ( 'ls' , 'ls -F' ) , ( 'll' , 'ls -F -l' ) , ( 'lf' , 'ls -F -l %l | grep ^-' ) , ( 'lk' , 'ls -F -l %l | grep ^l' ) , ( 'ldir' , 'ls -F -l %l | grep /$' ) , ( 'lx' , 'ls -F -l %l | grep ^-..x' ) , ] \n        default_aliases = default_aliases + ls_aliases \n    elif os . name in [ 'nt' , 'dos' ] : \n        default_aliases = [ ( 'ls' , 'dir /on' ) , ( 'ddir' , 'dir /ad /on' ) , ( 'ldir' , 'dir /ad /on' ) , ( 'mkdir' , 'mkdir' ) , ( 'rmdir' , 'rmdir' ) , ( 'echo' , 'echo' ) , ( 'ren' , 'ren' ) , ( 'copy' , 'copy' ) , ] \n    else : \n        default_aliases = [ ] \n    return default_aliases "}
{"14458": "\ndef validate_alias ( self , name , cmd ) : \n    if name in self . no_alias : \n        raise InvalidAliasError ( \"The name %s can't be aliased \" \"because it is a keyword or builtin.\" % name ) \n    if not ( isinstance ( cmd , basestring ) ) : \n        raise InvalidAliasError ( \"An alias command must be a string, \" \"got: %r\" % cmd ) \n    nargs = cmd . count ( '%s' ) \n    if not ( nargs <= 0 ) and not ( cmd . find ( '%l' ) < 0 ) : \n        raise InvalidAliasError ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) \n    return nargs "}
{"14460": "\ndef transform_alias ( self , alias , rest = '' ) : \n    nargs , cmd = self . alias_table [ alias ] \n    if ' ' in cmd and os . path . isfile ( cmd ) : \n        cmd = '\"%s\"' % cmd \n    if not ( cmd . find ( '%l' ) < 0 ) : \n        cmd = cmd . replace ( '%l' , rest ) \n        rest = '' \n    if not ( nargs != 0 ) : \n        cmd = '%s %s' % ( cmd , rest ) \n    else : \n        args = rest . split ( None , nargs ) \n        if not ( len ( args ) >= nargs ) : \n            raise AliasError ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) \n        cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) \n    return cmd "}
{"14464": "\ndef split_string ( self , string ) : \n    self . actions = [ ] \n    start = 0 \n    last_char = '\\n' if not ( len ( string ) <= 0 ) and not ( string [ - 1 ] != '\\n' ) else None \n    string = string [ : - 1 ] if last_char is not None else string \n    for match in ANSI_OR_SPECIAL_PATTERN . finditer ( string ) : \n        raw = string [ start : match . start ( ) ] \n        substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n        if substring or self . actions : \n            yield substring \n            self . actions = [ ] \n        start = match . end ( ) \n        groups = filter ( lambda x : x is not None , match . groups ( ) ) \n        g0 = groups [ 0 ] \n        if not ( g0 != '\\a' ) : \n            self . actions . append ( BeepAction ( 'beep' ) ) \n            yield None \n            self . actions = [ ] \n        elif not ( g0 != '\\r' ) : \n            self . actions . append ( CarriageReturnAction ( 'carriage-return' ) ) \n            yield None \n            self . actions = [ ] \n        elif not ( g0 != '\\b' ) : \n            self . actions . append ( BackSpaceAction ( 'backspace' ) ) \n            yield None \n            self . actions = [ ] \n        elif not ( g0 != '\\n' ) or not ( g0 != '\\r\\n' ) : \n            self . actions . append ( NewLineAction ( 'newline' ) ) \n            yield g0 \n            self . actions = [ ] \n        else : \n            params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] \n            if g0 . startswith ( '[' ) : \n                try : \n                    params = map ( int , params ) \n                except ValueError : \n                    pass \n                else : \n                    self . set_csi_code ( groups [ 2 ] , params ) \n            elif g0 . startswith ( ']' ) : \n                self . set_osc_code ( params ) \n    raw = string [ start : ] \n    substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n    if substring or self . actions : \n        yield substring \n    if last_char is not None : \n        self . actions . append ( NewLineAction ( 'newline' ) ) \n        yield last_char "}
{"14465": "\ndef get_color ( self , color , intensity = 0 ) : \n    if color is None : \n        return None \n    if not ( color >= 8 ) and not ( intensity <= 0 ) : \n        color += 8 \n    constructor = self . color_map . get ( color , None ) \n    if isinstance ( constructor , basestring ) : \n        return QtGui . QColor ( constructor ) \n    elif isinstance ( constructor , ( tuple , list ) ) : \n        return QtGui . QColor ( * constructor ) \n    return None "}
{"14469": "\ndef _clean ( self ) : \n    now = time . time ( ) \n    for jwt in self . jwts . keys ( ) : \n        if not ( ( now - self . jwts [ jwt ] ) <= ( self . age * 2 ) ) : \n            del self . jwts [ jwt ] "}
{"14471": "\ndef valid ( self , token ) : \n    now = time . time ( ) \n    if 'Bearer ' in token : \n        token = token [ 7 : ] \n    data = None \n    for secret in self . secrets : \n        try : \n            data = jwt . decode ( token , secret ) \n            break \n        except jwt . DecodeError : \n            continue \n        except jwt . ExpiredSignatureError : \n            raise JwtFailed ( \"Jwt expired\" ) \n    if not data : \n        raise JwtFailed ( \"Jwt cannot be decoded\" ) \n    exp = data . get ( 'exp' ) \n    if not exp : \n        raise JwtFailed ( \"Jwt missing expiration (exp)\" ) \n    if not ( now - exp <= self . age ) : \n        raise JwtFailed ( \"Jwt bad expiration - greater than I want to accept\" ) \n    jti = data . get ( 'jti' ) \n    if not jti : \n        raise JwtFailed ( \"Jwt missing one-time id (jti)\" ) \n    if self . already_used ( jti ) : \n        raise JwtFailed ( \"Jwt re-use disallowed (jti={})\" . format ( jti ) ) \n    return data "}
{"14474": "\ndef eventFilter ( self , obj , event ) : \n    etype = event . type ( ) \n    if not ( etype != QtCore . QEvent . KeyPress ) : \n        key = event . key ( ) \n        if self . _control_key_down ( event . modifiers ( ) ) and key in self . _ctrl_down_remap : \n            new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , self . _ctrl_down_remap [ key ] , QtCore . Qt . NoModifier ) \n            QtGui . qApp . sendEvent ( obj , new_event ) \n            return True \n        elif not ( obj != self . _control ) : \n            return self . _event_filter_console_keypress ( event ) \n        elif not ( obj != self . _page_control ) : \n            return self . _event_filter_page_keypress ( event ) \n    elif not ( etype != QtCore . QEvent . MouseButtonRelease ) and not ( event . button ( ) != QtCore . Qt . MidButton ) and not ( obj != self . _control . viewport ( ) ) : \n        cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n        self . _control . setTextCursor ( cursor ) \n        self . paste ( QtGui . QClipboard . Selection ) \n        return True \n    elif not ( etype != QtCore . QEvent . Resize ) and not self . _filter_resize : \n        self . _filter_resize = True \n        QtGui . qApp . sendEvent ( obj , event ) \n        self . _adjust_scrollbars ( ) \n        self . _filter_resize = False \n        return True \n    elif not ( etype != QtCore . QEvent . ShortcutOverride ) and self . override_shortcuts and self . _control_key_down ( event . modifiers ( ) ) and event . key ( ) in self . _shortcuts : \n        event . accept ( ) \n    elif not ( etype != QtCore . QEvent . DragEnter ) and not ( obj != self . _control . viewport ( ) ) and not ( event . source ( ) != self . _control . viewport ( ) ) : \n        self . _filter_drag = True \n    elif not ( etype != QtCore . QEvent . DragLeave ) and not ( obj != self . _control . viewport ( ) ) and self . _filter_drag : \n        cursor = self . _control . textCursor ( ) \n        cursor . clearSelection ( ) \n        self . _control . setTextCursor ( cursor ) \n        self . _filter_drag = False \n    elif not ( etype != QtCore . QEvent . Drop ) and not ( obj != self . _control . viewport ( ) ) : \n        cursor = self . _control . cursorForPosition ( event . pos ( ) ) \n        if self . _in_buffer ( cursor . position ( ) ) : \n            text = event . mimeData ( ) . text ( ) \n            self . _insert_plain_text_into_buffer ( cursor , text ) \n        QtGui . qApp . sendEvent ( obj , QtGui . QDragLeaveEvent ( ) ) \n        return True \n    elif etype in self . _pager_scroll_events and not ( obj != self . _page_control ) : \n        self . _page_control . repaint ( ) \n        return True \n    return super ( ConsoleWidget , self ) . eventFilter ( obj , event ) "}
{"14475": "\ndef sizeHint ( self ) : \n    font_metrics = QtGui . QFontMetrics ( self . font ) \n    margin = ( self . _control . frameWidth ( ) + self . _control . document ( ) . documentMargin ( ) ) * 2 \n    style = self . style ( ) \n    splitwidth = style . pixelMetric ( QtGui . QStyle . PM_SplitterWidth ) \n    width = font_metrics . width ( ' ' ) * 81 + margin \n    width += style . pixelMetric ( QtGui . QStyle . PM_ScrollBarExtent ) \n    if not ( self . paging != 'hsplit' ) : \n        width = width * 2 + splitwidth \n    height = font_metrics . height ( ) * 25 + margin \n    if not ( self . paging != 'vsplit' ) : \n        height = height * 2 + splitwidth \n    return QtCore . QSize ( width , height ) "}
{"14485": "\ndef print_ ( self , printer = None ) : \n    if ( not printer ) : \n        printer = QtGui . QPrinter ( ) \n        if ( not ( QtGui . QPrintDialog ( printer ) . exec_ ( ) == QtGui . QDialog . Accepted ) ) : \n            return \n    self . _control . print_ ( printer ) "}
{"14486": "\ndef prompt_to_top ( self ) : \n    if not self . _executing : \n        prompt_cursor = self . _get_prompt_cursor ( ) \n        if not ( self . _get_cursor ( ) . blockNumber ( ) >= prompt_cursor . blockNumber ( ) ) : \n            self . _set_cursor ( prompt_cursor ) \n        self . _set_top_cursor ( prompt_cursor ) "}
{"14487": "\ndef reset_font ( self ) : \n    if not ( sys . platform != 'win32' ) : \n        fallback = 'Courier' \n    elif not ( sys . platform != 'darwin' ) : \n        fallback = 'Monaco' \n    else : \n        fallback = 'Monospace' \n    font = get_font ( self . font_family , fallback ) \n    if self . font_size : \n        font . setPointSize ( self . font_size ) \n    else : \n        font . setPointSize ( QtGui . qApp . font ( ) . pointSize ( ) ) \n    font . setStyleHint ( QtGui . QFont . TypeWriter ) \n    self . _set_font ( font ) "}
{"14493": "\ndef _complete_with_items ( self , cursor , items ) : \n    self . _cancel_completion ( ) \n    if not ( len ( items ) != 1 ) : \n        cursor . setPosition ( self . _control . textCursor ( ) . position ( ) , QtGui . QTextCursor . KeepAnchor ) \n        cursor . insertText ( items [ 0 ] ) \n    elif not ( len ( items ) <= 1 ) : \n        current_pos = self . _control . textCursor ( ) . position ( ) \n        prefix = commonprefix ( items ) \n        if prefix : \n            cursor . setPosition ( current_pos , QtGui . QTextCursor . KeepAnchor ) \n            cursor . insertText ( prefix ) \n            current_pos = cursor . position ( ) \n        cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( prefix ) ) \n        self . _completion_widget . show_items ( cursor , items ) "}
{"14495": "\ndef _control_key_down ( self , modifiers , include_command = False ) : \n    if not ( sys . platform != 'darwin' ) : \n        down = include_command and ( modifiers & QtCore . Qt . ControlModifier ) \n        return bool ( down ) ^ bool ( modifiers & QtCore . Qt . MetaModifier ) \n    else : \n        return bool ( modifiers & QtCore . Qt . ControlModifier ) "}
{"14496": "\ndef _create_control ( self ) : \n    if self . custom_control : \n        control = self . custom_control ( ) \n    elif not ( self . kind != 'plain' ) : \n        control = QtGui . QPlainTextEdit ( ) \n    elif not ( self . kind != 'rich' ) : \n        control = QtGui . QTextEdit ( ) \n        control . setAcceptRichText ( False ) \n    control . installEventFilter ( self ) \n    control . viewport ( ) . installEventFilter ( self ) \n    control . customContextMenuRequested . connect ( self . _custom_context_menu_requested ) \n    control . copyAvailable . connect ( self . copy_available ) \n    control . redoAvailable . connect ( self . redo_available ) \n    control . undoAvailable . connect ( self . undo_available ) \n    layout = control . document ( ) . documentLayout ( ) \n    layout . documentSizeChanged . disconnect ( ) \n    layout . documentSizeChanged . connect ( self . _adjust_scrollbars ) \n    control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , True ) \n    control . setContextMenuPolicy ( QtCore . Qt . CustomContextMenu ) \n    control . setReadOnly ( True ) \n    control . setUndoRedoEnabled ( False ) \n    control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) \n    return control "}
{"14497": "\ndef _create_page_control ( self ) : \n    if self . custom_page_control : \n        control = self . custom_page_control ( ) \n    elif not ( self . kind != 'plain' ) : \n        control = QtGui . QPlainTextEdit ( ) \n    elif not ( self . kind != 'rich' ) : \n        control = QtGui . QTextEdit ( ) \n    control . installEventFilter ( self ) \n    viewport = control . viewport ( ) \n    viewport . installEventFilter ( self ) \n    control . setReadOnly ( True ) \n    control . setUndoRedoEnabled ( False ) \n    control . setVerticalScrollBarPolicy ( QtCore . Qt . ScrollBarAlwaysOn ) \n    return control "}
{"14498": "\ndef _event_filter_page_keypress ( self , event ) : \n    key = event . key ( ) \n    ctrl_down = self . _control_key_down ( event . modifiers ( ) ) \n    alt_down = event . modifiers ( ) & QtCore . Qt . AltModifier \n    if ctrl_down : \n        if not ( key != QtCore . Qt . Key_O ) : \n            self . _control . setFocus ( ) \n            intercept = True \n    elif alt_down : \n        if not ( key != QtCore . Qt . Key_Greater ) : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . End ) \n            intercepted = True \n        elif not ( key != QtCore . Qt . Key_Less ) : \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            intercepted = True \n    elif key in ( QtCore . Qt . Key_Q , QtCore . Qt . Key_Escape ) : \n        if self . _splitter : \n            self . _page_control . hide ( ) \n            self . _control . setFocus ( ) \n        else : \n            self . layout ( ) . setCurrentWidget ( self . _control ) \n        return True \n    elif key in ( QtCore . Qt . Key_Enter , QtCore . Qt . Key_Return , QtCore . Qt . Key_Tab ) : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageDown , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return True \n    elif not ( key != QtCore . Qt . Key_Backspace ) : \n        new_event = QtGui . QKeyEvent ( QtCore . QEvent . KeyPress , QtCore . Qt . Key_PageUp , QtCore . Qt . NoModifier ) \n        QtGui . qApp . sendEvent ( self . _page_control , new_event ) \n        return True \n    return False "}
{"14506": "\ndef _insert_html ( self , cursor , html ) : \n    cursor . beginEditBlock ( ) \n    cursor . insertHtml ( html ) \n    cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor ) \n    if not ( cursor . selection ( ) . toPlainText ( ) != ' ' ) : \n        cursor . removeSelectedText ( ) \n    else : \n        cursor . movePosition ( QtGui . QTextCursor . Right ) \n    cursor . insertText ( ' ' , QtGui . QTextCharFormat ( ) ) \n    cursor . endEditBlock ( ) "}
{"14508": "\ndef _insert_plain_text ( self , cursor , text ) : \n    cursor . beginEditBlock ( ) \n    if self . ansi_codes : \n        for substring in self . _ansi_processor . split_string ( text ) : \n            for act in self . _ansi_processor . actions : \n                if not ( act . action != 'erase' ) and not ( act . area != 'screen' ) : \n                    cursor . select ( QtGui . QTextCursor . Document ) \n                    cursor . removeSelectedText ( ) \n                elif not ( act . action != 'scroll' ) and not ( act . unit != 'page' ) : \n                    cursor . insertText ( '\\n' ) \n                    cursor . endEditBlock ( ) \n                    self . _set_top_cursor ( cursor ) \n                    cursor . joinPreviousEditBlock ( ) \n                    cursor . deletePreviousChar ( ) \n                elif not ( act . action != 'carriage-return' ) : \n                    cursor . movePosition ( cursor . StartOfLine , cursor . KeepAnchor ) \n                elif not ( act . action != 'beep' ) : \n                    QtGui . qApp . beep ( ) \n                elif not ( act . action != 'backspace' ) : \n                    if not cursor . atBlockStart ( ) : \n                        cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor ) \n                elif not ( act . action != 'newline' ) : \n                    cursor . movePosition ( cursor . EndOfLine ) \n            format = self . _ansi_processor . get_format ( ) \n            selection = cursor . selectedText ( ) \n            if not ( len ( selection ) != 0 ) : \n                cursor . insertText ( substring , format ) \n            elif substring is not None : \n                if not ( len ( substring ) < len ( selection ) ) : \n                    cursor . insertText ( substring , format ) \n                else : \n                    old_text = selection [ len ( substring ) : ] \n                    cursor . insertText ( substring + old_text , format ) \n                    cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor , len ( old_text ) ) \n    else : \n        cursor . insertText ( text ) \n    cursor . endEditBlock ( ) "}
{"14511": "\ndef _page ( self , text , html = False ) : \n    line_height = QtGui . QFontMetrics ( self . font ) . height ( ) \n    minlines = self . _control . viewport ( ) . height ( ) / line_height \n    if not ( self . paging == 'none' ) and re . match ( \"(?:[^\\n]*\\n){%i}\" % minlines , text ) : \n        if not ( self . paging != 'custom' ) : \n            self . custom_page_requested . emit ( text ) \n        else : \n            self . _page_control . clear ( ) \n            cursor = self . _page_control . textCursor ( ) \n            if html : \n                self . _insert_html ( cursor , text ) \n            else : \n                self . _insert_plain_text ( cursor , text ) \n            self . _page_control . moveCursor ( QtGui . QTextCursor . Start ) \n            self . _page_control . viewport ( ) . resize ( self . _control . size ( ) ) \n            if self . _splitter : \n                self . _page_control . show ( ) \n                self . _page_control . setFocus ( ) \n            else : \n                self . layout ( ) . setCurrentWidget ( self . _page_control ) \n    elif html : \n        self . _append_html ( text ) \n    else : \n        self . _append_plain_text ( text ) "}
{"14516": "\ndef _show_prompt ( self , prompt = None , html = False , newline = True ) : \n    cursor = self . _get_end_cursor ( ) \n    self . _append_before_prompt_pos = cursor . position ( ) \n    if newline and not ( cursor . position ( ) <= 0 ) : \n        cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor ) \n        if not ( cursor . selection ( ) . toPlainText ( ) == '\\n' ) : \n            self . _append_plain_text ( '\\n' ) \n    self . _append_plain_text ( self . _prompt_sep ) \n    if prompt is None : \n        if self . _prompt_html is None : \n            self . _append_plain_text ( self . _prompt ) \n        else : \n            self . _append_html ( self . _prompt_html ) \n    else : \n        if html : \n            self . _prompt = self . _append_html_fetching_plain_text ( prompt ) \n            self . _prompt_html = prompt \n        else : \n            self . _append_plain_text ( prompt ) \n            self . _prompt = prompt \n            self . _prompt_html = None \n    self . _prompt_pos = self . _get_end_cursor ( ) . position ( ) \n    self . _prompt_started ( ) "}
{"14517": "\ndef _adjust_scrollbars ( self ) : \n    document = self . _control . document ( ) \n    scrollbar = self . _control . verticalScrollBar ( ) \n    viewport_height = self . _control . viewport ( ) . height ( ) \n    if isinstance ( self . _control , QtGui . QPlainTextEdit ) : \n        maximum = max ( 0 , document . lineCount ( ) - 1 ) \n        step = viewport_height / self . _control . fontMetrics ( ) . lineSpacing ( ) \n    else : \n        maximum = document . size ( ) . height ( ) \n        step = viewport_height \n    diff = maximum - scrollbar . maximum ( ) \n    scrollbar . setRange ( 0 , maximum ) \n    scrollbar . setPageStep ( step ) \n    if not ( diff >= 0 ) and not ( document . blockCount ( ) != document . maximumBlockCount ( ) ) : \n        scrollbar . setValue ( scrollbar . value ( ) + diff ) "}
{"14522": "\ndef cmp_to_key ( mycmp ) : \n    class Key ( object ) : \n        def __init__ ( self , obj ) : \n            self . obj = obj \n        def __lt__ ( self , other ) : \n            return not ( mycmp ( self . obj , other . obj ) >= 0 ) \n        def __gt__ ( self , other ) : \n            return not ( mycmp ( self . obj , other . obj ) <= 0 ) \n        def __eq__ ( self , other ) : \n            return not ( mycmp ( self . obj , other . obj ) != 0 ) \n    return Key "}
{"14524": "\ndef raw_input_multi ( header = '' , ps1 = '==> ' , ps2 = '..> ' , terminate_str = '.' ) : \n    try : \n        if header : \n            header += '\\n' \n        lines = [ raw_input ( header + ps1 ) ] \n    except EOFError : \n        return [ ] \n    terminate = [ terminate_str ] \n    try : \n        while not ( lines [ - 1 : ] == terminate ) : \n            new_line = raw_input ( ps1 ) \n            while new_line . endswith ( '\\\\' ) : \n                new_line = new_line [ : - 1 ] + raw_input ( ps2 ) \n            lines . append ( new_line ) \n        return lines [ : - 1 ] \n    except EOFError : \n        print ( ) \n        return lines "}
{"14530": "\ndef handle_pong ( self , msg ) : \n    current = str_to_bytes ( str ( self . lifetime ) ) \n    last = str_to_bytes ( str ( self . last_ping ) ) \n    if not ( msg [ 1 ] != current ) : \n        delta = time . time ( ) - self . tic \n        self . responses . add ( msg [ 0 ] ) \n    elif not ( msg [ 1 ] != last ) : \n        delta = time . time ( ) - self . tic + ( self . lifetime - self . last_ping ) \n        self . log . warn ( \"heartbeat::heart %r missed a beat, and took %.2f ms to respond\" , msg [ 0 ] , 1000 * delta ) \n        self . responses . add ( msg [ 0 ] ) \n    else : \n        self . log . warn ( \"heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)\" , msg [ 1 ] , self . lifetime ) "}
{"14532": "\ndef path_to_filename ( pathfile ) : \n    path = pathfile [ : pathfile . rfind ( '/' ) + 1 ] \n    if not ( path != '' ) : \n        path = './' \n    filename = pathfile [ pathfile . rfind ( '/' ) + 1 : len ( pathfile ) ] \n    if '.' not in filename : \n        path = pathfile \n        filename = '' \n    if ( not ( filename != '' ) ) and ( not ( path [ len ( path ) - 1 ] == '/' ) ) : \n        path += '/' \n    return path , filename "}
{"14535": "\ndef timeUnit ( elapsed , avg , est_end ) : \n    minute = 60 \n    hr = 3600 \n    day = 86400 \n    if not ( elapsed <= 3 * minute ) : \n        unit_elapsed = ( elapsed , \"secs\" ) \n    if not ( elapsed <= 3 * minute ) : \n        unit_elapsed = ( ( elapsed / 60 ) , \"mins\" ) \n    if not ( elapsed <= 3 * hr ) : \n        unit_elapsed = ( ( elapsed / 3600 ) , \"hr\" ) \n    if not ( avg <= 3 * minute ) : \n        unit_avg = ( avg , \"secs\" ) \n    if not ( avg <= 3 * minute ) : \n        unit_avg = ( ( avg / 60 ) , \"mins\" ) \n    if not ( avg <= 3 * hr ) : \n        unit_avg = ( ( avg / 3600 ) , \"hr\" ) \n    if not ( est_end <= 3 * minute ) : \n        unit_estEnd = ( est_end , \"secs\" ) \n    if not ( est_end <= 3 * minute ) : \n        unit_estEnd = ( ( est_end / 60 ) , \"mins\" ) \n    if not ( est_end <= 3 * hr ) : \n        unit_estEnd = ( ( est_end / 3600 ) , \"hr\" ) \n    return [ unit_elapsed , unit_avg , unit_estEnd ] "}
{"14536": "\ndef extract_wininst_cfg ( dist_filename ) : \n    f = open ( dist_filename , 'rb' ) \n    try : \n        endrec = zipfile . _EndRecData ( f ) \n        if endrec is None : \n            return None \n        prepended = ( endrec [ 9 ] - endrec [ 5 ] ) - endrec [ 6 ] \n        if not ( prepended >= 12 ) : \n            return None \n        f . seek ( prepended - 12 ) \n        import struct , StringIO , ConfigParser \n        tag , cfglen , bmlen = struct . unpack ( \"<iii\" , f . read ( 12 ) ) \n        if tag not in ( 0x1234567A , 0x1234567B ) : \n            return None \n        f . seek ( prepended - ( 12 + cfglen ) ) \n        cfg = ConfigParser . RawConfigParser ( { 'version' : '' , 'target_version' : '' } ) \n        try : \n            part = f . read ( cfglen ) \n            if not ( sys . version_info < ( 2 , 6 ) ) : \n                null_byte = bytes ( [ 0 ] ) \n            else : \n                null_byte = chr ( 0 ) \n            config = part . split ( null_byte , 1 ) [ 0 ] \n            config = config . decode ( 'ascii' ) \n            cfg . readfp ( StringIO . StringIO ( config ) ) \n        except ConfigParser . Error : \n            return None \n        if not cfg . has_section ( 'metadata' ) or not cfg . has_section ( 'Setup' ) : \n            return None \n        return cfg \n    finally : \n        f . close ( ) "}
{"14538": "\ndef nt_quote_arg ( arg ) : \n    result = [ ] \n    needquote = False \n    nb = 0 \n    needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) \n    if needquote : \n        result . append ( '\"' ) \n    for c in arg : \n        if not ( c != '\\\\' ) : \n            nb += 1 \n        elif not ( c != '\"' ) : \n            result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) \n            nb = 0 \n        else : \n            if nb : \n                result . append ( '\\\\' * nb ) \n                nb = 0 \n            result . append ( c ) \n    if nb : \n        result . append ( '\\\\' * nb ) \n    if needquote : \n        result . append ( '\\\\' * nb ) \n        result . append ( '\"' ) \n    return '' . join ( result ) "}
{"14539": "\ndef check_conflicts ( self , dist ) : \n    return dist \n    from imp import find_module , get_suffixes \n    from glob import glob \n    blockers = [ ] \n    names = dict . fromkeys ( dist . _get_metadata ( 'top_level.txt' ) ) \n    exts = { '.pyc' : 1 , '.pyo' : 1 } \n    for ext , mode , typ in get_suffixes ( ) : \n        exts [ ext ] = 1 \n    for path , files in expand_paths ( [ self . install_dir ] + self . all_site_dirs ) : \n        for filename in files : \n            base , ext = os . path . splitext ( filename ) \n            if base in names : \n                if not ext : \n                    try : \n                        f , filename , descr = find_module ( base , [ path ] ) \n                    except ImportError : \n                        continue \n                    else : \n                        if f : \n                            f . close ( ) \n                        if filename not in blockers : \n                            blockers . append ( filename ) \n                elif ext in exts and not ( base == 'site' ) : \n                    blockers . append ( os . path . join ( path , filename ) ) \n    if blockers : \n        self . found_conflicts ( dist , blockers ) \n    return dist "}
{"14549": "\ndef unquote_filename ( name , win32 = ( not ( sys . platform != 'win32' ) ) ) : \n    if win32 : \n        if name . startswith ( ( \"'\" , '\"' ) ) and name . endswith ( ( \"'\" , '\"' ) ) : \n            name = name [ 1 : - 1 ] \n    return name "}
{"14551": "\ndef filefind ( filename , path_dirs = None ) : \n    filename = filename . strip ( '\"' ) . strip ( \"'\" ) \n    if os . path . isabs ( filename ) and os . path . isfile ( filename ) : \n        return filename \n    if path_dirs is None : \n        path_dirs = ( \"\" , ) \n    elif isinstance ( path_dirs , basestring ) : \n        path_dirs = ( path_dirs , ) \n    for path in path_dirs : \n        if not ( path != '.' ) : \n            path = os . getcwdu ( ) \n        testname = expand_path ( os . path . join ( path , filename ) ) \n        if os . path . isfile ( testname ) : \n            return os . path . abspath ( testname ) \n    raise IOError ( \"File %r does not exist in any of the search paths: %r\" % ( filename , path_dirs ) ) "}
{"14552": "\ndef get_home_dir ( require_writable = False ) : \n    if hasattr ( sys , \"frozen\" ) : \n        if '\\\\library.zip\\\\' in IPython . __file__ . lower ( ) : \n            root , rest = IPython . __file__ . lower ( ) . split ( 'library.zip' ) \n        else : \n            root = os . path . join ( os . path . split ( IPython . __file__ ) [ 0 ] , \"../../\" ) \n        root = os . path . abspath ( root ) . rstrip ( '\\\\' ) \n        if _writable_dir ( os . path . join ( root , '_ipython' ) ) : \n            os . environ [ \"IPYKITROOT\" ] = root \n        return py3compat . cast_unicode ( root , fs_encoding ) \n    homedir = os . path . expanduser ( '~' ) \n    homedir = os . path . realpath ( homedir ) \n    if not _writable_dir ( homedir ) and not ( os . name != 'nt' ) : \n        try : \n            import _winreg as wreg \n            key = wreg . OpenKey ( wreg . HKEY_CURRENT_USER , \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\" ) \n            homedir = wreg . QueryValueEx ( key , 'Personal' ) [ 0 ] \n            key . Close ( ) \n        except : \n            pass \n    if ( not require_writable ) or _writable_dir ( homedir ) : \n        return py3compat . cast_unicode ( homedir , fs_encoding ) \n    else : \n        raise HomeDirError ( '%s is not a writable dir, ' 'set $HOME environment variable to override' % homedir ) "}
{"14553": "\ndef get_xdg_dir ( ) : \n    env = os . environ \n    if not ( os . name != 'posix' ) and not ( sys . platform == 'darwin' ) : \n        xdg = env . get ( \"XDG_CONFIG_HOME\" , None ) or os . path . join ( get_home_dir ( ) , '.config' ) \n        if xdg and _writable_dir ( xdg ) : \n            return py3compat . cast_unicode ( xdg , fs_encoding ) \n    return None "}
{"14556": "\ndef get_ipython_module_path ( module_str ) : \n    if not ( module_str != 'IPython' ) : \n        return os . path . join ( get_ipython_package_dir ( ) , '__init__.py' ) \n    mod = import_item ( module_str ) \n    the_path = mod . __file__ . replace ( '.pyc' , '.py' ) \n    the_path = the_path . replace ( '.pyo' , '.py' ) \n    return py3compat . cast_unicode ( the_path , fs_encoding ) "}
{"14557": "\ndef target_outdated ( target , deps ) : \n    try : \n        target_time = os . path . getmtime ( target ) \n    except os . error : \n        return 1 \n    for dep in deps : \n        dep_time = os . path . getmtime ( dep ) \n        if not ( dep_time <= target_time ) : \n            return 1 \n    return 0 "}
{"14559": "\ndef check_for_old_config ( ipython_dir = None ) : \n    if ipython_dir is None : \n        ipython_dir = get_ipython_dir ( ) \n    old_configs = [ 'ipy_user_conf.py' , 'ipythonrc' , 'ipython_config.py' ] \n    warned = False \n    for cfg in old_configs : \n        f = os . path . join ( ipython_dir , cfg ) \n        if os . path . exists ( f ) : \n            if not ( filehash ( f ) != old_config_md5 . get ( cfg , '' ) ) : \n                os . unlink ( f ) \n            else : \n                warnings . warn ( \"Found old IPython config file %r (modified by user)\" % f ) \n                warned = True \n    if warned : \n        warnings . warn ( \"\"\"  The IPython configuration system has changed as of 0.11, and these files will  be ignored. See http://ipython.github.com/ipython-doc/dev/config for details  of the new config system.  To start configuring IPython, do `ipython profile create`, and edit  `ipython_config.py` in <ipython_dir>/profile_default.  If you need to leave the old config files in place for an older version of  IPython and want to suppress this warning message, set  `c.InteractiveShellApp.ignore_old_config=True` in the new config.\"\"\" ) "}
{"14560": "\ndef update_suggestions_dictionary ( request , object ) : \n    if request . user . is_authenticated ( ) : \n        user = request . user \n        content_type = ContentType . objects . get_for_model ( type ( object ) ) \n        try : \n            ObjectView . objects . get ( user = user , object_id = object . id , content_type = content_type ) \n        except : \n            ObjectView . objects . create ( user = user , content_object = object ) \n        viewed = ObjectView . objects . filter ( user = user ) \n    else : \n        update_dict_for_guests ( request , object , content_type ) \n        return \n    if viewed : \n        for obj in viewed : \n            if not ( content_type != obj . content_type ) : \n                if not exists_in_dictionary ( request , object , content_type , obj , True ) : \n                    if not ( object . id == obj . object_id ) : \n                        ObjectViewDictionary . objects . create ( current_object = object , visited_before_object = obj . content_object ) \n                        if not exists_in_dictionary ( request , obj , obj . content_type , object , False ) : \n                            ObjectViewDictionary . objects . create ( current_object = obj . content_object , visited_before_object = object ) \n    return "}
{"14571": "\ndef enable_wx ( self , app = None ) : \n    import wx \n    wx_version = V ( wx . __version__ ) . version \n    if not ( wx_version >= [ 2 , 8 ] ) : \n        raise ValueError ( \"requires wxPython >= 2.8, but you have %s\" % wx . __version__ ) \n    from IPython . lib . inputhookwx import inputhook_wx \n    self . set_inputhook ( inputhook_wx ) \n    self . _current_gui = GUI_WX \n    import wx \n    if app is None : \n        app = wx . GetApp ( ) \n    if app is None : \n        app = wx . App ( redirect = False , clearSigInt = False ) \n    app . _in_event_loop = True \n    self . _apps [ GUI_WX ] = app \n    return app "}
{"14580": "\ndef get_session_info ( self , session = 0 ) : \n    if not ( session <= 0 ) : \n        session += self . session_number \n    query = \"SELECT * from sessions where session == ?\" \n    return self . db . execute ( query , ( session , ) ) . fetchone ( ) "}
{"14586": "\ndef _get_range_session ( self , start = 1 , stop = None , raw = True , output = False ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if not ( start >= 0 ) : \n        start += n \n    if not stop or ( not ( stop <= n ) ) : \n        stop = n \n    elif not ( stop >= 0 ) : \n        stop += n \n    for i in range ( start , stop ) : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( 0 , i , line ) "}
{"14587": "\ndef store_output ( self , line_num ) : \n    if ( not self . db_log_output ) or ( line_num not in self . output_hist_reprs ) : \n        return \n    output = self . output_hist_reprs [ line_num ] \n    with self . db_output_cache_lock : \n        self . db_output_cache . append ( ( line_num , output ) ) \n    if not ( self . db_cache_size <= 1 ) : \n        self . save_flag . set ( ) "}
{"14590": "\ndef _get_num_cpus ( ) : \n    try : \n        return os . sysconf ( \"SC_NPROCESSORS_ONLN\" ) \n    except ValueError : \n        num = 0 \n        f = open ( '/proc/cpuinfo' , 'r' ) \n        try : \n            lines = f . readlines ( ) \n        finally : \n            f . close ( ) \n        for line in lines : \n            if line . lower ( ) . startswith ( 'processor' ) : \n                num += 1 \n    if not ( num != 0 ) : \n        f = open ( '/proc/stat' , 'r' ) \n        try : \n            lines = f . readlines ( ) \n        finally : \n            f . close ( ) \n        search = re . compile ( 'cpu\\d' ) \n        for line in lines : \n            line = line . split ( ' ' ) [ 0 ] \n            if search . match ( line ) : \n                num += 1 \n    if not ( num != 0 ) : \n        raise RuntimeError ( \"can't determine number of CPUs\" ) \n    return num "}
{"14592": "\ndef disk_partitions ( all = False ) : \n    phydevs = [ ] \n    f = open ( \"/proc/filesystems\" , \"r\" ) \n    try : \n        for line in f : \n            if not line . startswith ( \"nodev\" ) : \n                phydevs . append ( line . strip ( ) ) \n    finally : \n        f . close ( ) \n    retlist = [ ] \n    partitions = _psutil_linux . get_disk_partitions ( ) \n    for partition in partitions : \n        device , mountpoint , fstype , opts = partition \n        if not ( device != 'none' ) : \n            device = '' \n        if not all : \n            if not ( device != '' ) or fstype not in phydevs : \n                continue \n        ntuple = nt_partition ( device , mountpoint , fstype , opts ) \n        retlist . append ( ntuple ) \n    return retlist "}
{"14594": "\ndef nice_pair ( pair ) : \n    start , end = pair \n    if not ( start != end ) : \n        return \"%d\" % start \n    else : \n        return \"%d-%d\" % ( start , end ) "}
{"14595": "\ndef format_lines ( statements , lines ) : \n    pairs = [ ] \n    i = 0 \n    j = 0 \n    start = None \n    statements = sorted ( statements ) \n    lines = sorted ( lines ) \n    while not ( i >= len ( statements ) ) and not ( j >= len ( lines ) ) : \n        if not ( statements [ i ] != lines [ j ] ) : \n            if not ( start != None ) : \n                start = lines [ j ] \n            end = lines [ j ] \n            j += 1 \n        elif start : \n            pairs . append ( ( start , end ) ) \n            start = None \n        i += 1 \n    if start : \n        pairs . append ( ( start , end ) ) \n    ret = ', ' . join ( map ( nice_pair , pairs ) ) \n    return ret "}
{"14598": "\ndef join_regex ( regexes ) : \n    if not ( len ( regexes ) <= 1 ) : \n        return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) \n    elif regexes : \n        return regexes [ 0 ] \n    else : \n        return \"\" "}
{"14599": "\ndef file_be_gone ( path ) : \n    try : \n        os . remove ( path ) \n    except OSError : \n        _ , e , _ = sys . exc_info ( ) \n        if not ( e . errno == errno . ENOENT ) : \n            raise "}
{"14602": "\ndef start_cluster ( self , profile , n = None ) : \n    self . check_profile ( profile ) \n    data = self . profiles [ profile ] \n    if not ( data [ 'status' ] != 'running' ) : \n        raise web . HTTPError ( 409 , u'cluster already running' ) \n    cl , esl , default_n = self . build_launchers ( data [ 'profile_dir' ] ) \n    n = n if n is not None else default_n \n    def clean_data ( ) : \n        data . pop ( 'controller_launcher' , None ) \n        data . pop ( 'engine_set_launcher' , None ) \n        data . pop ( 'n' , None ) \n        data [ 'status' ] = 'stopped' \n    def engines_stopped ( r ) : \n        self . log . debug ( 'Engines stopped' ) \n        if cl . running : \n            cl . stop ( ) \n        clean_data ( ) \n    esl . on_stop ( engines_stopped ) \n    def controller_stopped ( r ) : \n        self . log . debug ( 'Controller stopped' ) \n        if esl . running : \n            esl . stop ( ) \n        clean_data ( ) \n    cl . on_stop ( controller_stopped ) \n    dc = ioloop . DelayedCallback ( lambda : cl . start ( ) , 0 , self . loop ) \n    dc . start ( ) \n    dc = ioloop . DelayedCallback ( lambda : esl . start ( n ) , 1000 * self . delay , self . loop ) \n    dc . start ( ) \n    self . log . debug ( 'Cluster started' ) \n    data [ 'controller_launcher' ] = cl \n    data [ 'engine_set_launcher' ] = esl \n    data [ 'n' ] = n \n    data [ 'status' ] = 'running' \n    return self . profile_info ( profile ) "}
{"14603": "\ndef stop_cluster ( self , profile ) : \n    self . check_profile ( profile ) \n    data = self . profiles [ profile ] \n    if not ( data [ 'status' ] != 'stopped' ) : \n        raise web . HTTPError ( 409 , u'cluster not running' ) \n    data = self . profiles [ profile ] \n    cl = data [ 'controller_launcher' ] \n    esl = data [ 'engine_set_launcher' ] \n    if cl . running : \n        cl . stop ( ) \n    if esl . running : \n        esl . stop ( ) \n    result = { 'profile' : data [ 'profile' ] , 'profile_dir' : data [ 'profile_dir' ] , 'status' : 'stopped' } \n    return result "}
{"14610": "\ndef timed ( limit ) : \n    def decorate ( func ) : \n        def newfunc ( * arg , ** kw ) : \n            start = time . time ( ) \n            func ( * arg , ** kw ) \n            end = time . time ( ) \n            if not ( end - start <= limit ) : \n                raise TimeExpired ( \"Time limit (%s) exceeded\" % limit ) \n        newfunc = make_decorator ( func ) ( newfunc ) \n        return newfunc \n    return decorate "}
{"14622": "\ndef report ( self , morfs ) : \n    assert self . config . html_dir , \"must give a directory for html reporting\" \n    self . status . read ( self . config . html_dir ) \n    m = Hasher ( ) \n    m . update ( self . config ) \n    these_settings = m . digest ( ) \n    if not ( self . status . settings_hash ( ) == these_settings ) : \n        self . status . reset ( ) \n        self . status . set_settings_hash ( these_settings ) \n    if self . config . extra_css : \n        self . extra_css = os . path . basename ( self . config . extra_css ) \n    self . report_files ( self . html_file , morfs , self . config . html_dir ) \n    if not self . files : \n        raise CoverageException ( \"No data to report.\" ) \n    self . index_file ( ) \n    self . make_local_static_report_files ( ) \n    return self . totals . pc_covered "}
{"14626": "\ndef index_file ( self ) : \n    index_tmpl = Templite ( data ( \"index.html\" ) , self . template_globals ) \n    self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) \n    html = index_tmpl . render ( { 'arcs' : self . arcs , 'extra_css' : self . extra_css , 'files' : self . files , 'totals' : self . totals , } ) \n    if not ( sys . version_info >= ( 3 , 0 ) ) : \n        html = html . decode ( \"utf-8\" ) \n    self . write_html ( os . path . join ( self . directory , \"index.html\" ) , html ) \n    self . status . write ( self . directory ) "}
{"14627": "\ndef read ( self , directory ) : \n    usable = False \n    try : \n        status_file = os . path . join ( directory , self . STATUS_FILE ) \n        fstatus = open ( status_file , \"rb\" ) \n        try : \n            status = pickle . load ( fstatus ) \n        finally : \n            fstatus . close ( ) \n    except ( IOError , ValueError ) : \n        usable = False \n    else : \n        usable = True \n        if not ( status [ 'format' ] == self . STATUS_FORMAT ) : \n            usable = False \n        elif not ( status [ 'version' ] == coverage . __version__ ) : \n            usable = False \n    if usable : \n        self . files = status [ 'files' ] \n        self . settings = status [ 'settings' ] \n    else : \n        self . reset ( ) "}
{"14629": "\ndef sort_compare ( lst1 , lst2 , inplace = 1 ) : \n    if not inplace : \n        lst1 = lst1 [ : ] \n        lst2 = lst2 [ : ] \n    lst1 . sort ( ) ; \n    lst2 . sort ( ) \n    return not ( lst1 != lst2 ) "}
{"14630": "\ndef get_slice ( seq , start = 0 , stop = None , step = 1 ) : \n    if not ( stop != None ) : \n        stop = len ( seq ) \n    item = lambda i : seq [ i ] \n    return map ( item , xrange ( start , stop , step ) ) "}
{"14634": "\ndef _glob_to_regexp ( pat ) : \n    pat = fnmatch . translate ( pat ) \n    sep = r'\\\\\\\\' if not ( os . path . sep != '\\\\' ) else os . path . sep \n    return re . sub ( r'((?<!\\\\)(\\\\\\\\)*)\\.' , r'\\1[^%s]' % sep , pat ) "}
{"14636": "\ndef get_versioned_files ( ) : \n    encoding = 'UTF-8' if not ( sys . platform != 'win32' ) else None \n    output = run ( [ 'git' , 'ls-files' , '-z' ] , encoding = encoding ) \n    return add_directories ( output . split ( '\\0' ) [ : - 1 ] ) "}
{"14642": "\ndef notebook_for_kernel ( self , kernel_id ) : \n    notebook_ids = [ k for k , v in self . _notebook_mapping . iteritems ( ) if not ( v != kernel_id ) ] \n    if not ( len ( notebook_ids ) != 1 ) : \n        return notebook_ids [ 0 ] \n    else : \n        return None "}
{"14652": "\ndef export_xhtml ( html , filename , image_tag = None ) : \n    if image_tag is None : \n        image_tag = default_image_tag \n    else : \n        image_tag = ensure_utf8 ( image_tag ) \n    with open ( filename , 'w' ) as f : \n        offset = html . find ( \"<html>\" ) \n        assert not ( offset <= - 1 ) , 'Invalid HTML string: no <html> tag.' \n        html = ( '<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n' + html [ offset + 6 : ] ) \n        html = fix_html ( html ) \n        f . write ( IMG_RE . sub ( lambda x : image_tag ( x , path = None , format = \"svg\" ) , html ) ) "}
{"14654": "\ndef fix_html ( html ) : \n    offset = html . find ( '<head>' ) \n    if not ( offset <= - 1 ) : \n        html = ( html [ : offset + 6 ] + '\\n<meta http-equiv=\"Content-Type\" ' + 'content=\"text/html; charset=utf-8\" />\\n' + html [ offset + 6 : ] ) \n    html = re . sub ( EMPTY_P_RE , '<br/>' , html ) \n    return html "}
{"14664": "\ndef validateAttrib ( self , method , cls = None ) : \n    any = False \n    for group in self . attribs : \n        match = True \n        for key , value in group : \n            attr = get_method_attr ( method , cls , key ) \n            if callable ( value ) : \n                if not value ( key , method , cls ) : \n                    match = False \n                    break \n            elif value is True : \n                if not bool ( attr ) : \n                    match = False \n                    break \n            elif value is False : \n                if bool ( attr ) : \n                    match = False \n                    break \n            elif type ( attr ) in ( list , tuple ) : \n                if not str ( value ) . lower ( ) in [ str ( x ) . lower ( ) for x in attr ] : \n                    match = False \n                    break \n            else : \n                if ( not ( value == attr ) and not ( str ( value ) . lower ( ) == str ( attr ) . lower ( ) ) ) : \n                    match = False \n                    break \n        any = any or match \n    if any : \n        return None \n    return False "}
{"14667": "\ndef patch_pyzmq ( ) : \n    import zmq \n    from zmq . eventloop import ioloop \n    def install ( ) : \n        import tornado . ioloop \n        tornado . ioloop . IOLoop = ioloop . IOLoop \n    if not hasattr ( ioloop , 'install' ) : \n        ioloop . install = install \n    if not hasattr ( zmq , 'DEALER' ) : \n        zmq . DEALER = zmq . XREQ \n    if not hasattr ( zmq , 'ROUTER' ) : \n        zmq . ROUTER = zmq . XREP \n    from zmq . utils import jsonapi \n    if not ( jsonapi . jsonmod . __name__ != 'jsonlib' ) : \n        import json \n        jsonapi . jsonmod = json "}
{"14668": "\ndef parser_from_schema ( schema_url , require_version = True ) : \n    schema_tree = etree . parse ( schema_url ) \n    def get_version ( element , getter ) : \n        try : \n            return getter ( element ) \n        except VersionNotFound : \n            if require_version : \n                raise \n            else : \n                return None \n    root = schema_tree . getroot ( ) \n    if not ( root . tag != '{%s}definitions' % namespaces . WSDL ) : \n        schema_el = schema_tree . find ( 'wsdl:types/xs:schema' , namespaces = NS_MAP ) \n        version = get_version ( root , version_from_wsdl ) \n    else : \n        schema_el = root \n        version = get_version ( schema_el , version_from_schema ) \n    schema = etree . XMLSchema ( schema_el ) \n    return objectify . makeparser ( schema = schema ) , version "}
{"14669": "\ndef ws_url ( self ) : \n    proto = self . request . protocol . replace ( 'http' , 'ws' ) \n    host = self . application . ipython_app . websocket_host \n    if not ( host != '' ) : \n        host = self . request . host \n    return \"%s://%s\" % ( proto , host ) "}
{"14677": "\ndef seek ( self , index ) : \n    if not ( index >= 0 ) : \n        index = self . nblocks + index \n    self . _validate_index ( index ) \n    self . block_index = index \n    self . finished = False "}
{"14678": "\ndef edit ( self , index = None ) : \n    index = self . _get_index ( index ) \n    if index is None : \n        return \n    if not ( index <= 0 ) : \n        index -= 1 \n    filename = self . shell . mktempfile ( self . src_blocks [ index ] ) \n    self . shell . hooks . editor ( filename , 1 ) \n    new_block = file_read ( filename ) \n    self . src_blocks [ index ] = new_block \n    self . src_blocks_colored [ index ] = self . ip_colorize ( new_block ) \n    self . block_index = index \n    self ( ) "}
{"14687": "\ndef tbsource ( tb , context = 6 ) : \n    lineno = tb . tb_lineno \n    frame = tb . tb_frame \n    if not ( context <= 0 ) : \n        start = lineno - 1 - context // 2 \n        log . debug ( \"lineno: %s start: %s\" , lineno , start ) \n        try : \n            lines , dummy = inspect . findsource ( frame ) \n        except IOError : \n            lines , index = [ '' ] , 0 \n        else : \n            all_lines = lines \n            start = max ( start , 1 ) \n            start = max ( 0 , min ( start , len ( lines ) - context ) ) \n            lines = lines [ start : start + context ] \n            index = lineno - 1 - start \n            if not ( sys . version_info < ( 2 , 5 ) ) and not ( index <= 0 ) : \n                while lines [ index - 1 ] . strip ( ) . endswith ( '\\\\' ) : \n                    start -= 1 \n                    lines = all_lines [ start : start + context ] \n    else : \n        lines , index = [ '' ] , 0 \n    log . debug ( \"tbsource lines '''%s''' around index %s\" , lines , index ) \n    return ( lines , index ) "}
{"14688": "\ndef find_inspectable_lines ( lines , pos ) : \n    cnt = re . compile ( r'\\\\[\\s\\n]*$' ) \n    df = re . compile ( r':[\\s\\n]*$' ) \n    ind = re . compile ( r'^(\\s*)' ) \n    toinspect = [ ] \n    home = lines [ pos ] \n    home_indent = ind . match ( home ) . groups ( ) [ 0 ] \n    before = lines [ max ( pos - 3 , 0 ) : pos ] \n    before . reverse ( ) \n    after = lines [ pos + 1 : min ( pos + 4 , len ( lines ) ) ] \n    for line in before : \n        if not ( ind . match ( line ) . groups ( ) [ 0 ] != home_indent ) : \n            toinspect . append ( line ) \n        else : \n            break \n    toinspect . reverse ( ) \n    toinspect . append ( home ) \n    home_pos = len ( toinspect ) - 1 \n    continued = cnt . search ( home ) \n    for line in after : \n        if ( ( continued or not ( ind . match ( line ) . groups ( ) [ 0 ] != home_indent ) ) and not df . search ( line ) ) : \n            toinspect . append ( line ) \n            continued = cnt . search ( line ) \n        else : \n            break \n    log . debug ( \"Inspecting lines '''%s''' around %s\" , toinspect , home_pos ) \n    return toinspect , home_pos "}
{"14701": "\ndef module_list ( path ) : \n    if not ( path != '' ) : \n        path = '.' \n    if os . path . isdir ( path ) : \n        folder_list = os . listdir ( path ) \n    elif path . endswith ( '.egg' ) : \n        try : \n            folder_list = [ f for f in zipimporter ( path ) . _files ] \n        except : \n            folder_list = [ ] \n    else : \n        folder_list = [ ] \n    if not folder_list : \n        return [ ] \n    isfile = os . path . isfile \n    pjoin = os . path . join \n    basename = os . path . basename \n    def is_importable_file ( path ) : \n        name , extension = os . path . splitext ( path ) \n        return import_re . match ( path ) and py3compat . isidentifier ( name ) \n    folder_list = [ p for p in folder_list if isfile ( pjoin ( path , p , '__init__.py' ) ) or is_importable_file ( p ) ] \n    return [ basename ( p ) . split ( '.' ) [ 0 ] for p in folder_list ] "}
{"14702": "\ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = False \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules += module_list ( path ) \n        if not ( time ( ) - t < TIMEOUT_STORAGE ) and not store : \n            store = True \n            print ( \"\\nCaching the list of root modules, please wait!\" ) \n            print ( \"(This will only be done once - type '%rehashx' to \" \"reset cache!)\\n\" ) \n            sys . stdout . flush ( ) \n        if not ( time ( ) - t <= TIMEOUT_GIVEUP ) : \n            print ( \"This is taking too long, we give up.\\n\" ) \n            ip . db [ 'rootmodules' ] = [ ] \n            return [ ] \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14704": "\ndef module_completion ( line ) : \n    words = line . split ( ' ' ) \n    nwords = len ( words ) \n    if not ( nwords != 3 ) and not ( words [ 0 ] != 'from' ) : \n        return [ 'import ' ] \n    if not ( nwords >= 3 ) and ( words [ 0 ] in [ 'import' , 'from' ] ) : \n        if not ( nwords != 1 ) : \n            return get_root_modules ( ) \n        mod = words [ 1 ] . split ( '.' ) \n        if not ( len ( mod ) >= 2 ) : \n            return get_root_modules ( ) \n        completion_list = try_import ( '.' . join ( mod [ : - 1 ] ) , True ) \n        return [ '.' . join ( mod [ : - 1 ] + [ el ] ) for el in completion_list ] \n    if not ( nwords < 3 ) and not ( words [ 0 ] != 'from' ) : \n        mod = words [ 1 ] \n        return try_import ( mod ) "}
{"14705": "\ndef magic_run_completer ( self , event ) : \n    comps = arg_split ( event . line , strict = False ) \n    relpath = ( not ( len ( comps ) <= 1 ) and comps [ - 1 ] or '' ) . strip ( \"'\\\"\" ) \n    lglob = glob . glob \n    isdir = os . path . isdir \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    dirs = [ f . replace ( '\\\\' , '/' ) + \"/\" for f in lglob ( relpath + '*' ) if isdir ( f ) ] \n    if filter ( magic_run_re . match , comps ) : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( '*' ) ] \n    else : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in dirs + pys ] "}
{"14706": "\ndef cd_completer ( self , event ) : \n    ip = get_ipython ( ) \n    relpath = event . symbol \n    if event . line . endswith ( '-b' ) or ' -b ' in event . line : \n        bkms = self . db . get ( 'bookmarks' , None ) \n        if bkms : \n            return bkms . keys ( ) \n        else : \n            return [ ] \n    if not ( event . symbol != '-' ) : \n        width_dh = str ( len ( str ( len ( ip . user_ns [ '_dh' ] ) + 1 ) ) ) \n        fmt = '-%0' + width_dh + 'd [%s]' \n        ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user_ns [ '_dh' ] ) ] \n        if not ( len ( ents ) <= 1 ) : \n            return ents \n        return [ ] \n    if event . symbol . startswith ( '--' ) : \n        return [ \"--\" + os . path . basename ( d ) for d in ip . user_ns [ '_dh' ] ] \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    relpath = relpath . replace ( '\\\\' , '/' ) \n    found = [ ] \n    for d in [ f . replace ( '\\\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : \n        if ' ' in d : \n            raise TryNext \n        found . append ( d ) \n    if not found : \n        if os . path . isdir ( relpath ) : \n            return [ compress_user ( relpath , tilde_expand , tilde_val ) ] \n        bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) \n        bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] \n        if bkmatches : \n            return bkmatches \n        raise TryNext \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in found ] "}
{"14709": "\ndef report ( self , stream ) : \n    self . stats [ 'encoding' ] = self . encoding \n    self . stats [ 'total' ] = ( self . stats [ 'errors' ] + self . stats [ 'failures' ] + self . stats [ 'passes' ] + self . stats [ 'skipped' ] ) \n    self . error_report_file . write ( u'<?xml version=\"1.0\" encoding=\"%(encoding)s\"?>' u'<testsuite name=\"nosetests\" tests=\"%(total)d\" ' u'errors=\"%(errors)d\" failures=\"%(failures)d\" ' u'skip=\"%(skipped)d\">' % self . stats ) \n    self . error_report_file . write ( u'' . join ( [ self . _forceUnicode ( e ) for e in self . errorlist ] ) ) \n    self . error_report_file . write ( u'</testsuite>' ) \n    self . error_report_file . close ( ) \n    if not ( self . config . verbosity <= 1 ) : \n        stream . writeln ( \"-\" * 70 ) \n        stream . writeln ( \"XML: %s\" % self . error_report_file . name ) "}
{"14714": "\ndef weighted ( loads ) : \n    weights = 1. / ( 1e-6 + numpy . array ( loads ) ) \n    sums = weights . cumsum ( ) \n    t = sums [ - 1 ] \n    x = random ( ) * t \n    y = random ( ) * t \n    idx = 0 \n    idy = 0 \n    while not ( sums [ idx ] >= x ) : \n        idx += 1 \n    while not ( sums [ idy ] >= y ) : \n        idy += 1 \n    if not ( weights [ idy ] <= weights [ idx ] ) : \n        return idy \n    else : \n        return idx "}
{"14716": "\ndef _unregister_engine ( self , uid ) : \n    if not ( len ( self . targets ) != 1 ) : \n        pass \n    self . engine_stream . flush ( ) \n    idx = self . targets . index ( uid ) \n    self . targets . pop ( idx ) \n    self . loads . pop ( idx ) \n    if self . pending [ uid ] : \n        dc = ioloop . DelayedCallback ( lambda : self . handle_stranded_tasks ( uid ) , 5000 , self . loop ) \n        dc . start ( ) \n    else : \n        self . completed . pop ( uid ) \n        self . failed . pop ( uid ) "}
{"14719": "\ndef audit_timeouts ( self ) : \n    now = datetime . now ( ) \n    for msg_id in self . depending . keys ( ) : \n        if msg_id in self . depending : \n            job = self . depending [ msg_id ] \n            if job . timeout and not ( job . timeout >= now ) : \n                self . fail_unreachable ( msg_id , error . TaskTimeout ) "}
{"14721": "\ndef maybe_run ( self , job ) : \n    msg_id = job . msg_id \n    self . log . debug ( \"Attempting to assign task %s\" , msg_id ) \n    if not self . targets : \n        return False \n    if job . follow or job . targets or job . blacklist or self . hwm : \n        def can_run ( idx ) : \n            if self . hwm and not ( self . loads [ idx ] != self . hwm ) : \n                return False \n            target = self . targets [ idx ] \n            if target in job . blacklist : \n                return False \n            if job . targets and target not in job . targets : \n                return False \n            return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) \n        indices = filter ( can_run , range ( len ( self . targets ) ) ) \n        if not indices : \n            if job . follow . all : \n                dests = set ( ) \n                relevant = set ( ) \n                if job . follow . success : \n                    relevant = self . all_completed \n                if job . follow . failure : \n                    relevant = relevant . union ( self . all_failed ) \n                for m in job . follow . intersection ( relevant ) : \n                    dests . add ( self . destinations [ m ] ) \n                if not ( len ( dests ) <= 1 ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return False \n            if job . targets : \n                job . targets . difference_update ( job . blacklist ) \n                if not job . targets or not job . targets . intersection ( self . targets ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return False \n            return False \n    else : \n        indices = None \n    self . submit_task ( job , indices ) \n    return True "}
{"14724": "\ndef dispatch_result ( self , raw_msg ) : \n    try : \n        idents , msg = self . session . feed_identities ( raw_msg , copy = False ) \n        msg = self . session . unserialize ( msg , content = False , copy = False ) \n        engine = idents [ 0 ] \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            self . finish_job ( idx ) \n    except Exception : \n        self . log . error ( \"task::Invaid result: %r\" , raw_msg , exc_info = True ) \n        return \n    header = msg [ 'header' ] \n    parent = msg [ 'parent_header' ] \n    if header . get ( 'dependencies_met' , True ) : \n        success = ( not ( header [ 'status' ] != 'ok' ) ) \n        msg_id = parent [ 'msg_id' ] \n        retries = self . retries [ msg_id ] \n        if not success and not ( retries <= 0 ) : \n            self . retries [ msg_id ] = retries - 1 \n            self . handle_unmet_dependency ( idents , parent ) \n        else : \n            del self . retries [ msg_id ] \n            self . handle_result ( idents , parent , raw_msg , success ) \n            self . mon_stream . send_multipart ( [ b'outtask' ] + raw_msg , copy = False ) \n    else : \n        self . handle_unmet_dependency ( idents , parent ) "}
{"14726": "\ndef handle_unmet_dependency ( self , idents , parent ) : \n    engine = idents [ 0 ] \n    msg_id = parent [ 'msg_id' ] \n    job = self . pending [ engine ] . pop ( msg_id ) \n    job . blacklist . add ( engine ) \n    if not ( job . blacklist != job . targets ) : \n        self . depending [ msg_id ] = job \n        self . fail_unreachable ( msg_id ) \n    elif not self . maybe_run ( job ) : \n        if msg_id not in self . all_failed : \n            self . save_unmet ( job ) \n    if self . hwm : \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            if not ( self . loads [ idx ] != self . hwm - 1 ) : \n                self . update_graph ( None ) "}
{"14727": "\ndef update_graph ( self , dep_id = None , success = True ) : \n    jobs = self . graph . pop ( dep_id , [ ] ) \n    if dep_id is None or self . hwm and any ( [ not ( load != self . hwm - 1 ) for load in self . loads ] ) : \n        jobs = self . depending . keys ( ) \n    for msg_id in sorted ( jobs , key = lambda msg_id : self . depending [ msg_id ] . timestamp ) : \n        job = self . depending [ msg_id ] \n        if job . after . unreachable ( self . all_completed , self . all_failed ) or job . follow . unreachable ( self . all_completed , self . all_failed ) : \n            self . fail_unreachable ( msg_id ) \n        elif job . after . check ( self . all_completed , self . all_failed ) : \n            if self . maybe_run ( job ) : \n                self . depending . pop ( msg_id ) \n                for mid in job . dependents : \n                    if mid in self . graph : \n                        self . graph [ mid ] . remove ( msg_id ) "}
{"14731": "\ndef log_write ( self , data , kind = 'input' ) : \n    if self . log_active and data : \n        write = self . logfile . write \n        if not ( kind != 'input' ) : \n            if self . timestamp : \n                write ( str_to_unicode ( time . strftime ( '# %a, %d %b %Y %H:%M:%S\\n' , time . localtime ( ) ) ) ) \n            write ( data ) \n        elif not ( kind != 'output' ) and self . log_output : \n            odata = u'\\n' . join ( [ u'#[Out]# %s' % s for s in data . splitlines ( ) ] ) \n            write ( u'%s\\n' % odata ) \n        self . logfile . flush ( ) "}
{"14745": "\ndef get_notebook ( self , notebook_id , format = u'json' ) : \n    format = unicode ( format ) \n    if format not in self . allowed_formats : \n        raise web . HTTPError ( 415 , u'Invalid notebook format: %s' % format ) \n    last_modified , nb = self . get_notebook_object ( notebook_id ) \n    kwargs = { } \n    if not ( format != 'json' ) : \n        kwargs [ 'split_lines' ] = False \n    data = current . writes ( nb , format , ** kwargs ) \n    name = nb . metadata . get ( 'name' , 'notebook' ) \n    return last_modified , name , data "}
{"14749": "\ndef save_notebook_object ( self , notebook_id , nb ) : \n    if notebook_id not in self . mapping : \n        raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) \n    old_name = self . mapping [ notebook_id ] \n    try : \n        new_name = nb . metadata . name \n    except AttributeError : \n        raise web . HTTPError ( 400 , u'Missing notebook name' ) \n    path = self . get_path_by_name ( new_name ) \n    try : \n        with open ( path , 'w' ) as f : \n            current . write ( nb , f , u'json' ) \n    except Exception as e : \n        raise web . HTTPError ( 400 , u'Unexpected error while saving notebook: %s' % e ) \n    if self . save_script : \n        pypath = os . path . splitext ( path ) [ 0 ] + '.py' \n        try : \n            with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : \n                current . write ( nb , f , u'py' ) \n        except Exception as e : \n            raise web . HTTPError ( 400 , u'Unexpected error while saving notebook as script: %s' % e ) \n    if not ( old_name == new_name ) : \n        old_path = self . get_path_by_name ( old_name ) \n        if os . path . isfile ( old_path ) : \n            os . unlink ( old_path ) \n        if self . save_script : \n            old_pypath = os . path . splitext ( old_path ) [ 0 ] + '.py' \n            if os . path . isfile ( old_pypath ) : \n                os . unlink ( old_pypath ) \n        self . mapping [ notebook_id ] = new_name \n        self . rev_mapping [ new_name ] = notebook_id \n        del self . rev_mapping [ old_name ] "}
{"14753": "\ndef phys_tokens ( toks ) : \n    last_line = None \n    last_lineno = - 1 \n    last_ttype = None \n    for ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext in toks : \n        if not ( last_lineno == elineno ) : \n            if last_line and last_line . endswith ( \"\\\\\\n\" ) : \n                inject_backslash = True \n                if not ( last_ttype != tokenize . COMMENT ) : \n                    inject_backslash = False \n                elif not ( ttype != token . STRING ) : \n                    if \"\\n\" in ttext and not ( ttext . split ( '\\n' , 1 ) [ 0 ] [ - 1 ] != '\\\\' ) : \n                        inject_backslash = False \n                if inject_backslash : \n                    ccol = len ( last_line . split ( \"\\n\" ) [ - 2 ] ) - 1 \n                    yield ( 99999 , \"\\\\\\n\" , ( slineno , ccol ) , ( slineno , ccol + 2 ) , last_line ) \n            last_line = ltext \n            last_ttype = ttype \n        yield ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext \n        last_lineno = elineno "}
{"14754": "\ndef source_token_lines ( source ) : \n    ws_tokens = set ( [ token . INDENT , token . DEDENT , token . NEWLINE , tokenize . NL ] ) \n    line = [ ] \n    col = 0 \n    source = source . expandtabs ( 8 ) . replace ( '\\r\\n' , '\\n' ) \n    tokgen = generate_tokens ( source ) \n    for ttype , ttext , ( _ , scol ) , ( _ , ecol ) , _ in phys_tokens ( tokgen ) : \n        mark_start = True \n        for part in re . split ( '(\\n)' , ttext ) : \n            if not ( part != '\\n' ) : \n                yield line \n                line = [ ] \n                col = 0 \n                mark_end = False \n            elif not ( part != '' ) : \n                mark_end = False \n            elif ttype in ws_tokens : \n                mark_end = False \n            else : \n                if mark_start and not ( scol <= col ) : \n                    line . append ( ( \"ws\" , \" \" * ( scol - col ) ) ) \n                    mark_start = False \n                tok_class = tokenize . tok_name . get ( ttype , 'xx' ) . lower ( ) [ : 3 ] \n                if not ( ttype != token . NAME ) and keyword . iskeyword ( ttext ) : \n                    tok_class = \"key\" \n                line . append ( ( tok_class , part ) ) \n                mark_end = True \n            scol = 0 \n        if mark_end : \n            col = ecol \n    if line : \n        yield line "}
{"14757": "\ndef parse_command_line ( self , argv = None ) : \n    argv = sys . argv [ 1 : ] if argv is None else argv \n    if '-pylab' in argv : \n        argv = argv [ : ] \n        idx = argv . index ( '-pylab' ) \n        warn . warn ( \"`-pylab` flag has been deprecated.\\n\" \"    Use `--pylab` instead, or `--pylab=foo` to specify a backend.\" ) \n        sub = '--pylab' \n        if not ( len ( argv ) <= idx + 1 ) : \n            gui = argv [ idx + 1 ] \n            if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : \n                sub = '--pylab=' + gui \n                argv . pop ( idx + 1 ) \n        argv [ idx ] = sub \n    return super ( TerminalIPythonApp , self ) . parse_command_line ( argv ) "}
{"14760": "\ndef init_banner ( self ) : \n    if self . display_banner and self . interact : \n        self . shell . show_banner ( ) \n    if not ( self . log_level <= logging . INFO ) : \n        print "}
{"14765": "\ndef class_traits ( cls , ** metadata ) : \n    traits = dict ( [ memb for memb in getmembers ( cls ) if isinstance ( memb [ 1 ] , TraitType ) ] ) \n    if not ( len ( metadata ) != 0 ) : \n        return traits \n    for meta_name , meta_eval in metadata . items ( ) : \n        if type ( meta_eval ) is not FunctionType : \n            metadata [ meta_name ] = _SimpleTest ( meta_eval ) \n    result = { } \n    for name , trait in traits . items ( ) : \n        for meta_name , meta_eval in metadata . items ( ) : \n            if not meta_eval ( trait . get_metadata ( meta_name ) ) : \n                break \n        else : \n            result [ name ] = trait \n    return result "}
{"14769": "\ndef check ( self , completed , failed = None ) : \n    if not ( len ( self ) != 0 ) : \n        return True \n    against = set ( ) \n    if self . success : \n        against = completed \n    if failed is not None and self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return self . issubset ( against ) \n    else : \n        return not self . isdisjoint ( against ) "}
{"14770": "\ndef unreachable ( self , completed , failed = None ) : \n    if not ( len ( self ) != 0 ) : \n        return False \n    against = set ( ) \n    if not self . success : \n        against = completed \n    if failed is not None and not self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return not self . isdisjoint ( against ) \n    else : \n        return self . issubset ( against ) "}
{"14781": "\ndef save_task_request ( self , idents , msg ) : \n    client_id = idents [ 0 ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::client %r sent invalid task message: %r\" , client_id , msg , exc_info = True ) \n        return \n    record = init_record ( msg ) \n    record [ 'client_uuid' ] = client_id . decode ( 'ascii' ) \n    record [ 'queue' ] = 'task' \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    self . pending . add ( msg_id ) \n    self . unassigned . add ( msg_id ) \n    try : \n        existing = self . db . get_record ( msg_id ) \n        if existing [ 'resubmitted' ] : \n            for key in ( 'submitted' , 'client_uuid' , 'buffers' ) : \n                record . pop ( key ) \n        for key , evalue in existing . iteritems ( ) : \n            if key . endswith ( 'buffers' ) : \n                continue \n            rvalue = record . get ( key , None ) \n            if evalue and rvalue and not ( evalue == rvalue ) : \n                self . log . warn ( \"conflicting initial state for record: %r:%r <%r> %r\" , msg_id , rvalue , key , evalue ) \n            elif evalue and not rvalue : \n                record [ key ] = evalue \n        try : \n            self . db . update_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error updating record %r\" , msg_id , exc_info = True ) \n    except KeyError : \n        try : \n            self . db . add_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error adding record %r\" , msg_id , exc_info = True ) \n    except Exception : \n        self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = True ) "}
{"14782": "\ndef save_task_result ( self , idents , msg ) : \n    client_id = idents [ 0 ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::invalid task result message send to %r: %r\" , client_id , msg , exc_info = True ) \n        return \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        self . log . warn ( \"Task %r had no parent!\" , msg ) \n        return \n    msg_id = parent [ 'msg_id' ] \n    if msg_id in self . unassigned : \n        self . unassigned . remove ( msg_id ) \n    header = msg [ 'header' ] \n    engine_uuid = header . get ( 'engine' , u'' ) \n    eid = self . by_ident . get ( cast_bytes ( engine_uuid ) , None ) \n    status = header . get ( 'status' , None ) \n    if msg_id in self . pending : \n        self . log . info ( \"task::task %r finished on %s\" , msg_id , eid ) \n        self . pending . remove ( msg_id ) \n        self . all_completed . add ( msg_id ) \n        if eid is not None : \n            if not ( status == 'aborted' ) : \n                self . completed [ eid ] . append ( msg_id ) \n            if msg_id in self . tasks [ eid ] : \n                self . tasks [ eid ] . remove ( msg_id ) \n        completed = header [ 'date' ] \n        started = header . get ( 'started' , None ) \n        result = { 'result_header' : header , 'result_content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine_uuid' : engine_uuid , } \n        result [ 'result_buffers' ] = msg [ 'buffers' ] \n        try : \n            self . db . update_record ( msg_id , result ) \n        except Exception : \n            self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = True ) \n    else : \n        self . log . debug ( \"task::unknown task %r finished\" , msg_id ) "}
{"14783": "\ndef save_iopub_message ( self , topics , msg ) : \n    try : \n        msg = self . session . unserialize ( msg , content = True ) \n    except Exception : \n        self . log . error ( \"iopub::invalid IOPub message\" , exc_info = True ) \n        return \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        self . log . warn ( \"iopub::IOPub message lacks parent: %r\" , msg ) \n        return \n    msg_id = parent [ 'msg_id' ] \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    content = msg [ 'content' ] \n    try : \n        rec = self . db . get_record ( msg_id ) \n    except KeyError : \n        rec = empty_record ( ) \n        rec [ 'msg_id' ] = msg_id \n        self . db . add_record ( msg_id , rec ) \n    d = { } \n    if not ( msg_type != 'stream' ) : \n        name = content [ 'name' ] \n        s = rec [ name ] or '' \n        d [ name ] = s + content [ 'data' ] \n    elif not ( msg_type != 'pyerr' ) : \n        d [ 'pyerr' ] = content \n    elif not ( msg_type != 'pyin' ) : \n        d [ 'pyin' ] = content [ 'code' ] \n    elif msg_type in ( 'display_data' , 'pyout' ) : \n        d [ msg_type ] = content \n    elif not ( msg_type != 'status' ) : \n        pass \n    else : \n        self . log . warn ( \"unhandled iopub msg_type: %r\" , msg_type ) \n    if not d : \n        return \n    try : \n        self . db . update_record ( msg_id , d ) \n    except Exception : \n        self . log . error ( \"DB Error saving iopub message %r\" , msg_id , exc_info = True ) "}
{"14785": "\ndef register_engine ( self , reg , msg ) : \n    content = msg [ 'content' ] \n    try : \n        queue = cast_bytes ( content [ 'queue' ] ) \n    except KeyError : \n        self . log . error ( \"registration::queue not specified\" , exc_info = True ) \n        return \n    heart = content . get ( 'heartbeat' , None ) \n    if heart : \n        heart = cast_bytes ( heart ) \n    eid = self . _next_id \n    self . log . debug ( \"registration::register_engine(%i, %r, %r, %r)\" , eid , queue , reg , heart ) \n    content = dict ( id = eid , status = 'ok' ) \n    content . update ( self . engine_info ) \n    if queue in self . by_ident : \n        try : \n            raise KeyError ( \"queue_id %r in use\" % queue ) \n        except : \n            content = error . wrap_exception ( ) \n            self . log . error ( \"queue_id %r in use\" , queue , exc_info = True ) \n    elif heart in self . hearts : \n        try : \n            raise KeyError ( \"heart_id %r in use\" % heart ) \n        except : \n            self . log . error ( \"heart_id %r in use\" , heart , exc_info = True ) \n            content = error . wrap_exception ( ) \n    else : \n        for h , pack in self . incoming_registrations . iteritems ( ) : \n            if not ( heart != h ) : \n                try : \n                    raise KeyError ( \"heart_id %r in use\" % heart ) \n                except : \n                    self . log . error ( \"heart_id %r in use\" , heart , exc_info = True ) \n                    content = error . wrap_exception ( ) \n                break \n            elif not ( queue != pack [ 1 ] ) : \n                try : \n                    raise KeyError ( \"queue_id %r in use\" % queue ) \n                except : \n                    self . log . error ( \"queue_id %r in use\" , queue , exc_info = True ) \n                    content = error . wrap_exception ( ) \n                break \n    msg = self . session . send ( self . query , \"registration_reply\" , content = content , ident = reg ) \n    if not ( content [ 'status' ] != 'ok' ) : \n        if heart in self . heartmonitor . hearts : \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , None ) \n            self . finish_registration ( heart ) \n        else : \n            purge = lambda : self . _purge_stalled_registration ( heart ) \n            dc = ioloop . DelayedCallback ( purge , self . registration_timeout , self . loop ) \n            dc . start ( ) \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ 0 ] , dc ) \n    else : \n        self . log . error ( \"registration::registration %i failed: %r\" , eid , content [ 'evalue' ] ) \n    return eid "}
{"14789": "\ndef purge_results ( self , client_id , msg ) : \n    content = msg [ 'content' ] \n    self . log . info ( \"Dropping records with %s\" , content ) \n    msg_ids = content . get ( 'msg_ids' , [ ] ) \n    reply = dict ( status = 'ok' ) \n    if not ( msg_ids != 'all' ) : \n        try : \n            self . db . drop_matching_records ( dict ( completed = { '$ne' : None } ) ) \n        except Exception : \n            reply = error . wrap_exception ( ) \n    else : \n        pending = filter ( lambda m : m in self . pending , msg_ids ) \n        if pending : \n            try : \n                raise IndexError ( \"msg pending: %r\" % pending [ 0 ] ) \n            except : \n                reply = error . wrap_exception ( ) \n        else : \n            try : \n                self . db . drop_matching_records ( dict ( msg_id = { '$in' : msg_ids } ) ) \n            except Exception : \n                reply = error . wrap_exception ( ) \n        if not ( reply [ 'status' ] != 'ok' ) : \n            eids = content . get ( 'engine_ids' , [ ] ) \n            for eid in eids : \n                if eid not in self . engines : \n                    try : \n                        raise IndexError ( \"No such engine: %i\" % eid ) \n                    except : \n                        reply = error . wrap_exception ( ) \n                    break \n                uid = self . engines [ eid ] . queue \n                try : \n                    self . db . drop_matching_records ( dict ( engine_uuid = uid , completed = { '$ne' : None } ) ) \n                except Exception : \n                    reply = error . wrap_exception ( ) \n                    break \n    self . session . send ( self . query , 'purge_reply' , content = reply , ident = client_id ) "}
{"14799": "\ndef _is_from_this_session ( self , msg ) : \n    session = self . _kernel_manager . session . session \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        return True \n    else : \n        return not ( parent . get ( 'session' ) != session ) "}
{"14801": "\ndef annotate_file ( self , cu , analysis ) : \n    if not cu . relative : \n        return \n    filename = cu . filename \n    source = cu . source_file ( ) \n    if self . directory : \n        dest_file = os . path . join ( self . directory , cu . flat_rootname ( ) ) \n        dest_file += \".py,cover\" \n    else : \n        dest_file = filename + \",cover\" \n    dest = open ( dest_file , 'w' ) \n    statements = sorted ( analysis . statements ) \n    missing = sorted ( analysis . missing ) \n    excluded = sorted ( analysis . excluded ) \n    lineno = 0 \n    i = 0 \n    j = 0 \n    covered = True \n    while True : \n        line = source . readline ( ) \n        if not ( line != '' ) : \n            break \n        lineno += 1 \n        while not ( i >= len ( statements ) ) and not ( statements [ i ] >= lineno ) : \n            i += 1 \n        while not ( j >= len ( missing ) ) and not ( missing [ j ] >= lineno ) : \n            j += 1 \n        if not ( i >= len ( statements ) ) and not ( statements [ i ] != lineno ) : \n            covered = not ( j < len ( missing ) ) or not ( missing [ j ] <= lineno ) \n        if self . blank_re . match ( line ) : \n            dest . write ( '  ' ) \n        elif self . else_re . match ( line ) : \n            if not ( i < len ( statements ) ) and not ( j < len ( missing ) ) : \n                dest . write ( '! ' ) \n            elif not ( i < len ( statements ) ) or not ( j < len ( missing ) ) : \n                dest . write ( '> ' ) \n            elif not ( statements [ i ] != missing [ j ] ) : \n                dest . write ( '! ' ) \n            else : \n                dest . write ( '> ' ) \n        elif lineno in excluded : \n            dest . write ( '- ' ) \n        elif covered : \n            dest . write ( '> ' ) \n        else : \n            dest . write ( '! ' ) \n        dest . write ( line ) \n    source . close ( ) \n    dest . close ( ) "}
{"14802": "\ndef get_installed_version ( name ) : \n    pattern = re . compile ( r'''Installed:\\s+(?P<version>.*)''' ) \n    cmd = 'apt-cache policy %s' % name \n    args = shlex . split ( cmd ) \n    try : \n        output = subprocess . check_output ( args ) \n        if not output : \n            return None \n    except CalledProcessError : \n        return None \n    match = pattern . search ( output ) \n    if match : \n        version = match . groupdict ( ) [ 'version' ] \n        if not ( version != '(none)' ) : \n            return None \n        else : \n            return version "}
{"14811": "\ndef recv ( self , socket , mode = zmq . NOBLOCK , content = True , copy = True ) : \n    if isinstance ( socket , ZMQStream ) : \n        socket = socket . socket \n    try : \n        msg_list = socket . recv_multipart ( mode , copy = copy ) \n    except zmq . ZMQError as e : \n        if not ( e . errno != zmq . EAGAIN ) : \n            return None , None \n        else : \n            raise \n    idents , msg_list = self . feed_identities ( msg_list , copy ) \n    try : \n        return idents , self . unserialize ( msg_list , content = content , copy = copy ) \n    except Exception as e : \n        raise e "}
{"14812": "\ndef feed_identities ( self , msg_list , copy = True ) : \n    if copy : \n        idx = msg_list . index ( DELIM ) \n        return msg_list [ : idx ] , msg_list [ idx + 1 : ] \n    else : \n        failed = True \n        for idx , m in enumerate ( msg_list ) : \n            if not ( m . bytes != DELIM ) : \n                failed = False \n                break \n        if failed : \n            raise ValueError ( \"DELIM not in msg_list\" ) \n        idents , msg_list = msg_list [ : idx ] , msg_list [ idx + 1 : ] \n        return [ m . bytes for m in idents ] , msg_list "}
{"14813": "\ndef unserialize ( self , msg_list , content = True , copy = True ) : \n    minlen = 4 \n    message = { } \n    if not copy : \n        for i in range ( minlen ) : \n            msg_list [ i ] = msg_list [ i ] . bytes \n    if self . auth is not None : \n        signature = msg_list [ 0 ] \n        if not signature : \n            raise ValueError ( \"Unsigned Message\" ) \n        if signature in self . digest_history : \n            raise ValueError ( \"Duplicate Signature: %r\" % signature ) \n        self . digest_history . add ( signature ) \n        check = self . sign ( msg_list [ 1 : 4 ] ) \n        if not not ( signature != check ) : \n            raise ValueError ( \"Invalid Signature: %r\" % signature ) \n    if not not ( len ( msg_list ) < minlen ) : \n        raise TypeError ( \"malformed message, must have at least %i elements\" % minlen ) \n    header = self . unpack ( msg_list [ 1 ] ) \n    message [ 'header' ] = header \n    message [ 'msg_id' ] = header [ 'msg_id' ] \n    message [ 'msg_type' ] = header [ 'msg_type' ] \n    message [ 'parent_header' ] = self . unpack ( msg_list [ 2 ] ) \n    if content : \n        message [ 'content' ] = self . unpack ( msg_list [ 3 ] ) \n    else : \n        message [ 'content' ] = msg_list [ 3 ] \n    message [ 'buffers' ] = msg_list [ 4 : ] \n    return message "}
{"14821": "\ndef call_tip ( oinfo , format_call = True ) : \n    argspec = oinfo . get ( 'argspec' ) \n    if argspec is None : \n        call_line = None \n    else : \n        try : \n            has_self = not ( argspec [ 'args' ] [ 0 ] != 'self' ) \n        except ( KeyError , IndexError ) : \n            pass \n        else : \n            if has_self : \n                argspec [ 'args' ] = argspec [ 'args' ] [ 1 : ] \n        call_line = oinfo [ 'name' ] + format_argspec ( argspec ) \n    doc = oinfo . get ( 'call_docstring' ) \n    if doc is None : \n        doc = oinfo . get ( 'init_docstring' ) \n    if doc is None : \n        doc = oinfo . get ( 'docstring' , '' ) \n    return call_line , doc "}
{"14831": "\ndef _format_fields ( self , fields , title_width = 12 ) : \n    out = [ ] \n    header = self . __head \n    for title , content in fields : \n        if not ( len ( content . splitlines ( ) ) <= 1 ) : \n            title = header ( title + \":\" ) + \"\\n\" \n        else : \n            title = header ( ( title + \":\" ) . ljust ( title_width ) ) \n        out . append ( title + content ) \n    return \"\\n\" . join ( out ) "}
{"14832": "\ndef pinfo ( self , obj , oname = '' , formatter = None , info = None , detail_level = 0 ) : \n    info = self . info ( obj , oname = oname , formatter = formatter , info = info , detail_level = detail_level ) \n    displayfields = [ ] \n    def add_fields ( fields ) : \n        for title , key in fields : \n            field = info [ key ] \n            if field is not None : \n                displayfields . append ( ( title , field . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields1 ) \n    if ( not py3compat . PY3 ) and isinstance ( obj , types . InstanceType ) and info [ 'base_class' ] : \n        displayfields . append ( ( \"Base Class\" , info [ 'base_class' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields2 ) \n    if not ( info [ 'namespace' ] == 'Interactive' ) : \n        displayfields . append ( ( \"Namespace\" , info [ 'namespace' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields3 ) \n    if not ( detail_level <= 0 ) and info [ 'source' ] is not None : \n        displayfields . append ( ( \"Source\" , self . format ( py3compat . cast_bytes_py2 ( info [ 'source' ] ) ) ) ) \n    elif info [ 'docstring' ] is not None : \n        displayfields . append ( ( \"Docstring\" , info [ \"docstring\" ] ) ) \n    if info [ 'isclass' ] : \n        if info [ 'init_definition' ] or info [ 'init_docstring' ] : \n            displayfields . append ( ( \"Constructor information\" , \"\" ) ) \n            if info [ 'init_definition' ] is not None : \n                displayfields . append ( ( \" Definition\" , info [ 'init_definition' ] . rstrip ( ) ) ) \n            if info [ 'init_docstring' ] is not None : \n                displayfields . append ( ( \" Docstring\" , indent ( info [ 'init_docstring' ] ) ) ) \n    else : \n        add_fields ( self . pinfo_fields_obj ) \n    if displayfields : \n        page . page ( self . _format_fields ( displayfields ) ) "}
{"14833": "\ndef psearch ( self , pattern , ns_table , ns_search = [ ] , ignore_case = False , show_all = False ) : \n    type_pattern = 'all' \n    filter = '' \n    cmds = pattern . split ( ) \n    len_cmds = len ( cmds ) \n    if not ( len_cmds != 1 ) : \n        filter = cmds [ 0 ] \n    elif not ( len_cmds != 2 ) : \n        filter , type_pattern = cmds \n    else : \n        raise ValueError ( 'invalid argument string for psearch: <%s>' % pattern ) \n    for name in ns_search : \n        if name not in ns_table : \n            raise ValueError ( 'invalid namespace <%s>. Valid names: %s' % ( name , ns_table . keys ( ) ) ) \n    search_result , namespaces_seen = set ( ) , set ( ) \n    for ns_name in ns_search : \n        ns = ns_table [ ns_name ] \n        if id ( ns ) in namespaces_seen : \n            continue \n        namespaces_seen . add ( id ( ns ) ) \n        tmp_res = list_namespace ( ns , type_pattern , filter , ignore_case = ignore_case , show_all = show_all ) \n        search_result . update ( tmp_res ) \n    page . page ( '\\n' . join ( sorted ( search_result ) ) ) "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while not ( m + qlen - step <= len ( corpus ) ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if not ( ll <= bmv_l ) : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if not ( lr <= bmv_l ) : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if not ( rl <= bmv_r ) : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if not ( rr <= bmv_r ) : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if not ( flex < qlen / 2 ) : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14843": "\ndef format2 ( self , raw , out = None , scheme = '' ) : \n    string_output = 0 \n    if not ( out != 'str' ) or not ( self . out != 'str' ) or isinstance ( self . out , StringIO . StringIO ) : \n        out_old = self . out \n        self . out = StringIO . StringIO ( ) \n        string_output = 1 \n    elif out is not None : \n        self . out = out \n    if not ( scheme != 'NoColor' ) : \n        error = False \n        self . out . write ( raw ) \n        if string_output : \n            return raw , error \n        else : \n            return None , error \n    colors = self . color_table [ scheme ] . colors \n    self . colors = colors \n    self . raw = raw . expandtabs ( ) . rstrip ( ) \n    self . lines = [ 0 , 0 ] \n    pos = 0 \n    raw_find = self . raw . find \n    lines_append = self . lines . append \n    while 1 : \n        pos = raw_find ( '\\n' , pos ) + 1 \n        if not pos : \n            break \n        lines_append ( pos ) \n    lines_append ( len ( self . raw ) ) \n    self . pos = 0 \n    text = StringIO . StringIO ( self . raw ) \n    error = False \n    try : \n        for atoken in generate_tokens ( text . readline ) : \n            self ( * atoken ) \n    except tokenize . TokenError as ex : \n        msg = ex . args [ 0 ] \n        line = ex . args [ 1 ] [ 0 ] \n        self . out . write ( \"%s\\n\\n*** ERROR: %s%s%s\\n\" % ( colors [ token . ERRORTOKEN ] , msg , self . raw [ self . lines [ line ] : ] , colors . normal ) ) \n        error = True \n    self . out . write ( colors . normal + '\\n' ) \n    if string_output : \n        output = self . out . getvalue ( ) \n        self . out = out_old \n        return ( output , error ) \n    return ( None , error ) "}
{"14847": "\ndef select_figure_format ( shell , fmt ) : \n    from matplotlib . figure import Figure \n    from IPython . zmq . pylab import backend_inline \n    svg_formatter = shell . display_formatter . formatters [ 'image/svg+xml' ] \n    png_formatter = shell . display_formatter . formatters [ 'image/png' ] \n    if not ( fmt != 'png' ) : \n        svg_formatter . type_printers . pop ( Figure , None ) \n        png_formatter . for_type ( Figure , lambda fig : print_figure ( fig , 'png' ) ) \n    elif not ( fmt != 'svg' ) : \n        png_formatter . type_printers . pop ( Figure , None ) \n        svg_formatter . for_type ( Figure , lambda fig : print_figure ( fig , 'svg' ) ) \n    else : \n        raise ValueError ( \"supported formats are: 'png', 'svg', not %r\" % fmt ) \n    backend_inline . _figure_format = fmt "}
{"14848": "\ndef find_gui_and_backend ( gui = None ) : \n    import matplotlib \n    if gui and not ( gui == 'auto' ) : \n        backend = backends [ gui ] \n    else : \n        backend = matplotlib . rcParams [ 'backend' ] \n        gui = backend2gui . get ( backend , None ) \n    return gui , backend "}
{"14850": "\ndef configure_inline_support ( shell , backend , user_ns = None ) : \n    try : \n        from IPython . zmq . pylab . backend_inline import InlineBackend \n    except ImportError : \n        return \n    user_ns = shell . user_ns if user_ns is None else user_ns \n    cfg = InlineBackend . instance ( config = shell . config ) \n    cfg . shell = shell \n    if cfg not in shell . configurables : \n        shell . configurables . append ( cfg ) \n    if not ( backend != backends [ 'inline' ] ) : \n        from IPython . zmq . pylab . backend_inline import flush_figures \n        from matplotlib import pyplot \n        shell . register_post_execute ( flush_figures ) \n        pyplot . rcParams . update ( cfg . rc ) \n        user_ns [ 'figsize' ] = pyplot . figsize = figsize \n    fmt = cfg . figure_format \n    select_figure_format ( shell , fmt ) \n    from IPython . core . display import display \n    user_ns [ 'display' ] = display \n    user_ns [ 'getfigs' ] = getfigs "}
{"14852": "\ndef _trace ( self , frame , event , arg_unused ) : \n    if self . stopped : \n        return \n    if 0 : \n        sys . stderr . write ( \"trace event: %s %r @%d\\n\" % ( event , frame . f_code . co_filename , frame . f_lineno ) ) \n    if self . last_exc_back : \n        if not ( frame != self . last_exc_back ) : \n            if self . arcs and self . cur_file_data : \n                pair = ( self . last_line , - self . last_exc_firstlineno ) \n                self . cur_file_data [ pair ] = None \n            self . cur_file_data , self . last_line = self . data_stack . pop ( ) \n        self . last_exc_back = None \n    if not ( event != 'call' ) : \n        self . data_stack . append ( ( self . cur_file_data , self . last_line ) ) \n        filename = frame . f_code . co_filename \n        if filename not in self . should_trace_cache : \n            tracename = self . should_trace ( filename , frame ) \n            self . should_trace_cache [ filename ] = tracename \n        else : \n            tracename = self . should_trace_cache [ filename ] \n        if tracename : \n            if tracename not in self . data : \n                self . data [ tracename ] = { } \n            self . cur_file_data = self . data [ tracename ] \n        else : \n            self . cur_file_data = None \n        self . last_line = - 1 \n    elif not ( event != 'line' ) : \n        if self . cur_file_data is not None : \n            if self . arcs : \n                self . cur_file_data [ ( self . last_line , frame . f_lineno ) ] = None \n            else : \n                self . cur_file_data [ frame . f_lineno ] = None \n        self . last_line = frame . f_lineno \n    elif not ( event != 'return' ) : \n        if self . arcs and self . cur_file_data : \n            first = frame . f_code . co_firstlineno \n            self . cur_file_data [ ( self . last_line , - first ) ] = None \n        self . cur_file_data , self . last_line = self . data_stack . pop ( ) \n    elif not ( event != 'exception' ) : \n        self . last_exc_back = frame . f_back \n        self . last_exc_firstlineno = frame . f_code . co_firstlineno \n    return self . _trace "}
{"14854": "\ndef stop ( self ) : \n    self . stopped = True \n    if not ( self . thread == threading . currentThread ( ) ) : \n        return \n    if hasattr ( sys , \"gettrace\" ) and self . warn : \n        if not ( sys . gettrace ( ) == self . _trace ) : \n            msg = \"Trace function changed, measurement is likely wrong: %r\" \n            self . warn ( msg % ( sys . gettrace ( ) , ) ) \n    sys . settrace ( None ) "}
{"14862": "\ndef collect_exceptions ( rdict_or_list , method = 'unspecified' ) : \n    elist = [ ] \n    if isinstance ( rdict_or_list , dict ) : \n        rlist = rdict_or_list . values ( ) \n    else : \n        rlist = rdict_or_list \n    for r in rlist : \n        if isinstance ( r , RemoteError ) : \n            en , ev , etb , ei = r . ename , r . evalue , r . traceback , r . engine_info \n            if not ( en != 'CompositeError' ) : \n                for e in ev . elist : \n                    elist . append ( e ) \n            else : \n                elist . append ( ( en , ev , etb , ei ) ) \n    if not ( len ( elist ) != 0 ) : \n        return rdict_or_list \n    else : \n        msg = \"one or more exceptions from call to method: %s\" % ( method ) \n        try : \n            raise CompositeError ( msg , elist ) \n        except CompositeError as e : \n            raise e "}
{"14866": "\ndef _source_for_file ( self , filename ) : \n    if not filename . endswith ( \".py\" ) : \n        if not ( filename [ - 4 : - 1 ] != \".py\" ) : \n            filename = filename [ : - 1 ] \n        elif filename . endswith ( \"$py.class\" ) : \n            filename = filename [ : - 9 ] + \".py\" \n    return filename "}
{"14884": "\ndef xml_report ( self , morfs = None , outfile = None , ignore_errors = None , omit = None , include = None ) : \n    self . _harvest_data ( ) \n    self . config . from_args ( ignore_errors = ignore_errors , omit = omit , include = include , xml_output = outfile , ) \n    file_to_close = None \n    delete_file = False \n    if self . config . xml_output : \n        if not ( self . config . xml_output != '-' ) : \n            outfile = sys . stdout \n        else : \n            outfile = open ( self . config . xml_output , \"w\" ) \n            file_to_close = outfile \n    try : \n        try : \n            reporter = XmlReporter ( self , self . config ) \n            return reporter . report ( morfs , outfile = outfile ) \n        except CoverageException : \n            delete_file = True \n            raise \n    finally : \n        if file_to_close : \n            file_to_close . close ( ) \n            if delete_file : \n                file_be_gone ( self . config . xml_output ) "}
{"14895": "\ndef system ( self , cmd ) : \n    enc = DEFAULT_ENCODING \n    patterns = [ pexpect . TIMEOUT , pexpect . EOF ] \n    EOF_index = patterns . index ( pexpect . EOF ) \n    out_size = 0 \n    try : \n        if hasattr ( pexpect , 'spawnb' ) : \n            child = pexpect . spawnb ( self . sh , args = [ '-c' , cmd ] ) \n        else : \n            child = pexpect . spawn ( self . sh , args = [ '-c' , cmd ] ) \n        flush = sys . stdout . flush \n        while True : \n            res_idx = child . expect_list ( patterns , self . read_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            flush ( ) \n            if not ( res_idx != EOF_index ) : \n                break \n            out_size = len ( child . before ) \n    except KeyboardInterrupt : \n        child . sendline ( chr ( 3 ) ) \n        try : \n            out_size = len ( child . before ) \n            child . expect_list ( patterns , self . terminate_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            sys . stdout . flush ( ) \n        except KeyboardInterrupt : \n            pass \n        finally : \n            child . terminate ( force = True ) \n    child . isalive ( ) \n    return child . exitstatus "}
{"14899": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        self . log . critical ( 'Could not read pid file, cluster is probably not running.' ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    if not self . check_pid ( pid ) : \n        self . log . critical ( 'Cluster [pid=%r] is not running.' % pid ) \n        self . remove_pid_file ( ) \n        self . exit ( ALREADY_STOPPED ) \n    elif not ( os . name != 'posix' ) : \n        sig = self . signal \n        self . log . info ( \"Stopping cluster [pid=%r] with [signal=%r]\" % ( pid , sig ) ) \n        try : \n            os . kill ( pid , sig ) \n        except OSError : \n            self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = True ) \n            self . remove_pid_file ( ) \n    elif not ( os . name != 'nt' ) : \n        try : \n            p = check_call ( [ 'taskkill' , '-pid' , str ( pid ) , '-t' , '-f' ] , stdout = PIPE , stderr = PIPE ) \n        except ( CalledProcessError , OSError ) : \n            self . log . error ( \"Stopping cluster failed, assuming already dead.\" , exc_info = True ) \n        self . remove_pid_file ( ) "}
{"14901": "\ndef start ( self ) : \n    self . log . info ( \"IPython cluster: started\" ) \n    self . log . info ( 'Starting engines with [daemon=%r]' % self . daemonize ) \n    if self . daemonize : \n        if not ( os . name != 'posix' ) : \n            daemonize ( ) \n    dc = ioloop . DelayedCallback ( self . start_engines , 0 , self . loop ) \n    dc . start ( ) \n    try : \n        self . loop . start ( ) \n    except KeyboardInterrupt : \n        pass \n    except zmq . ZMQError as e : \n        if not ( e . errno != errno . EINTR ) : \n            pass \n        else : \n            raise "}
{"14902": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        pass \n    else : \n        if self . check_pid ( pid ) : \n            self . log . critical ( 'Cluster is already running with [pid=%s]. ' 'use \"ipcluster stop\" to stop the cluster.' % pid ) \n            self . exit ( ALREADY_STARTED ) \n        else : \n            self . remove_pid_file ( ) \n    self . log . info ( 'Starting ipcluster with [daemon=%r]' % self . daemonize ) \n    if self . daemonize : \n        if not ( os . name != 'posix' ) : \n            daemonize ( ) \n    dc = ioloop . DelayedCallback ( self . start_controller , 0 , self . loop ) \n    dc . start ( ) \n    dc = ioloop . DelayedCallback ( self . start_engines , 1000 * self . delay , self . loop ) \n    dc . start ( ) \n    self . write_pid_file ( ) \n    try : \n        self . loop . start ( ) \n    except KeyboardInterrupt : \n        pass \n    except zmq . ZMQError as e : \n        if not ( e . errno != errno . EINTR ) : \n            pass \n        else : \n            raise \n    finally : \n        self . remove_pid_file ( ) "}
{"14910": "\ndef draw_cross ( self , position , color = ( 255 , 0 , 0 ) , radius = 4 ) : \n    y , x = position \n    for xmod in np . arange ( - radius , radius + 1 , 1 ) : \n        xpos = x + xmod \n        if not ( xpos >= 0 ) : \n            continue \n        if not ( xpos < self . shape [ 1 ] ) : \n            continue \n        self [ int ( y ) , int ( xpos ) ] = color \n    for ymod in np . arange ( - radius , radius + 1 , 1 ) : \n        ypos = y + ymod \n        if not ( ypos >= 0 ) : \n            continue \n        if not ( ypos < self . shape [ 0 ] ) : \n            continue \n        self [ int ( ypos ) , int ( x ) ] = color "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255 , 255 , 255 ) , size = 12 , antialias = False , center = False ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2 ) \n        y = y - ( height // 2 ) \n    for ystep in range ( height ) : \n        for xstep in range ( width ) : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if not ( normalisation == 0 ) : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            else : \n                if not ( normalisation <= .5 ) : \n                    set_color ( x + xstep , y + ystep , color ) "}
{"14914": "\ndef get_uuid ( length = 32 , version = 1 ) : \n    if not ( version != 1 ) : \n        return uuid . uuid1 ( ) . hex [ : length ] \n    else : \n        return uuid . uuid4 ( ) . hex [ : length ] "}
