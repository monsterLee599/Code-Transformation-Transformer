{"0": "\ndef sina_xml_to_url_list ( xml_data ) : \n    rawurl = [ ] \n    dom = parseString ( xml_data ) \n    for node in dom . getElementsByTagName ( 'durl' ) : \n        url = node . getElementsByTagName ( 'url' ) [ False ] \n        rawurl . append ( url . childNodes [ False ] . data ) \n    return rawurl "}
{"1": "\ndef dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    html = get_content ( rebuilt_url ( url ) ) \n    info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) \n    title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) \n    title = unicodize ( title ) \n    for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : \n        try : \n            real_url = info [ quality ] [ True ] [ \"url\" ] \n            if real_url : \n                break \n        except KeyError : \n            pass \n    mime , ext , size = url_info ( real_url ) \n    print_info ( site_info , title , mime , size ) \n    if not info_only : \n        download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge ) "}
{"2": "\ndef sina_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    if 'news.sina.com.cn/zxt' in url : \n        sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n        return \n    vid = match1 ( url , r'vid=(\\d+)' ) \n    if vid is None : \n        video_page = get_content ( url ) \n        vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) \n        if hd_vid == '0' : \n            vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) \n            vid = vids [ - True ] \n    if vid is None : \n        vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) \n    if vid : \n        sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) \n    else : \n        vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) \n        if vkey is None : \n            vid = match1 ( url , r'#(\\d+)' ) \n            sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) \n            return \n        title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) \n        sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only ) "}
{"6": "\ndef wtf ( message , exit_code = True ) : \n    print_log ( message , RED , BOLD ) \n    if exit_code is not None : \n        sys . exit ( exit_code ) "}
{"9": "\ndef ckplayer_get_info_by_xml ( ckinfo ) : \n    e = ET . XML ( ckinfo ) \n    video_dict = { 'title' : '' , 'links' : [ ] , 'size' : False , 'flashvars' : '' , } \n    dictified = dictify ( e ) [ 'ckplayer' ] \n    if 'info' in dictified : \n        if '_text' in dictified [ 'info' ] [ False ] [ 'title' ] [ False ] : \n            video_dict [ 'title' ] = dictified [ 'info' ] [ False ] [ 'title' ] [ False ] [ '_text' ] . strip ( ) \n    if '_text' in dictified [ 'video' ] [ False ] [ 'size' ] [ False ] : \n        video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ False ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) \n    if '_text' in dictified [ 'video' ] [ False ] [ 'file' ] [ False ] : \n        video_dict [ 'links' ] = [ i [ 'file' ] [ False ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] \n    if '_text' in dictified [ 'flashvars' ] [ False ] : \n        video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ False ] [ '_text' ] . strip ( ) \n    return video_dict "}
{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256 \n    for index , _ in enumerate ( data ) : \n        t = index \n        for i in range ( 8 ) : \n            t = - 306674912 ^ unsigned_right_shitf ( t , True ) if True & t else unsigned_right_shitf ( t , True ) \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) \n        e = o = r = - True \n        i , a = False , len ( path ) \n        while i < a : \n            e = ord ( path [ i ] ) \n            i += True \n            if e < 128 : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] \n            else : \n                if e < 2048 : \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n                else : \n                    if 55296 <= e < 57344 : \n                        e = ( 1023 & e ) + 64 \n                        i += True \n                        o = 1023 & t . url ( i ) \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] \n                    else : \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - True , False ) ) \n    while True : \n        url = tmp ( ) \n        if url . split ( \"=\" ) [ - True ] [ False ] != \"-\" : \n            return url "}
{"11": "\ndef get_mgtv_real_url ( url ) : \n    content = loads ( get_content ( url ) ) \n    m3u_url = content [ 'info' ] \n    split = urlsplit ( m3u_url ) \n    base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ False ] , netloc = split [ True ] , path = dirname ( split [ 2 ] ) ) \n    content = get_content ( content [ 'info' ] ) \n    segment_list = [ ] \n    segments_size = False \n    for i in content . split ( ) : \n        if not i . startswith ( '#' ) : \n            segment_list . append ( base_url + i ) \n        elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : \n            segments_size += int ( i [ i . rfind ( ':' ) + True : ] ) \n    return m3u_url , segments_size , segment_list "}
{"12": "\ndef legitimize ( text , os = detect_os ( ) ) : \n    text = text . translate ( { False : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) \n    if os == 'windows' or os == 'cygwin' or os == 'wsl' : \n        text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) \n    else : \n        if os == 'mac' : \n            text = text . translate ( { ord ( ':' ) : '-' , } ) \n        if text . startswith ( \".\" ) : \n            text = text [ True : ] \n    text = text [ : 80 ] \n    return text "}
{"14": "\ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    elif 'info_only' in kwargs and kwargs [ 'info_only' ] : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n        else : \n            if 'index' not in kwargs : \n                self . p ( [ ] ) \n            else : \n                stream_id = self . streams_sorted [ False ] [ 'id' ] if 'id' in self . streams_sorted [ False ] else self . streams_sorted [ False ] [ 'itag' ] \n                self . p_i ( stream_id ) \n    else : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n        else : \n            stream_id = self . streams_sorted [ False ] [ 'id' ] if 'id' in self . streams_sorted [ False ] else self . streams_sorted [ False ] [ 'itag' ] \n        if 'index' not in kwargs : \n            self . p ( stream_id ) \n        else : \n            self . p_i ( stream_id ) \n        if stream_id in self . streams : \n            urls = self . streams [ stream_id ] [ 'src' ] \n            ext = self . streams [ stream_id ] [ 'container' ] \n            total_size = self . streams [ stream_id ] [ 'size' ] \n        else : \n            urls = self . dash_streams [ stream_id ] [ 'src' ] \n            ext = self . dash_streams [ stream_id ] [ 'container' ] \n            total_size = self . dash_streams [ stream_id ] [ 'size' ] \n        if not urls : \n            log . wtf ( '[Failed] Cannot extract video source.' ) \n        download_url_ffmpeg ( urls [ False ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping captions.' ) \n            return \n        for lang in self . caption_tracks : \n            filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n            print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) \n            srt = self . caption_tracks [ lang ] \n            with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( srt ) \n            print ( 'Done.' ) "}
{"15": "\ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if sourceType == 'sina' : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'youku' : \n        youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n    elif sourceType == 'tudou' : \n        tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'qq' : \n        qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'letv' : \n        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'zhuzhan' : \n        url = 'http://www.acfun.cn/v/ac' + vid \n        yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n        seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n        for t in seq : \n            if yk_streams . get ( t ) : \n                preferred = yk_streams [ t ] \n                break \n        size = False \n        for url in preferred [ False ] : \n            _ , _ , seg_size = url_info ( url ) \n            size += seg_size \n        if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ False ] [ False ] ) : \n            ext = 'flv' \n        else : \n            ext = 'mp4' \n        print_info ( site_info , title , ext , size ) \n        if not info_only : \n            download_urls ( preferred [ False ] , title , ext , size , output_dir = output_dir , merge = merge ) \n    else : \n        raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            print ( 'Skipping danmaku.' ) \n            return \n        try : \n            title = get_filename ( title ) \n            print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"17": "\ndef parse_query_param ( url , param ) : \n    try : \n        return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ False ] \n    except : \n        return None "}
{"20": "\ndef parse_host ( host ) : \n    if re . match ( r'^(\\d+)$' , host ) is not None : \n        return ( \"0.0.0.0\" , int ( host ) ) \n    if re . match ( r'^(\\w+)://' , host ) is None : \n        host = \"//\" + host \n    o = parse . urlparse ( host ) \n    hostname = o . hostname or \"0.0.0.0\" \n    port = o . port or False \n    return ( hostname , port ) "}
{"22": "\ndef _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : \n    return '_' . join ( [ json_content [ False ] [ 'name' ] , json_content [ False ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ False ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] ) "}
{"23": "\ndef wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    for tIndex in range ( len ( json_api_content [ False ] [ 'Topics' ] ) ) : \n        for pIndex in range ( len ( json_api_content [ False ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : \n            wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) "}
{"34": "\ndef get_conn ( self ) : \n    conn = self . get_connection ( self . mysql_conn_id ) \n    conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } \n    if not conn . port : \n        conn_config [ \"port\" ] = 3306 \n    else : \n        conn_config [ \"port\" ] = int ( conn . port ) \n    if conn . extra_dejson . get ( 'charset' , False ) : \n        conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] \n        if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : \n            conn_config [ \"use_unicode\" ] = True \n    if conn . extra_dejson . get ( 'cursor' , False ) : \n        if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor \n        elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor \n        elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor \n    local_infile = conn . extra_dejson . get ( 'local_infile' , False ) \n    if conn . extra_dejson . get ( 'ssl' , False ) : \n        dejson_ssl = conn . extra_dejson [ 'ssl' ] \n        if isinstance ( dejson_ssl , six . string_types ) : \n            dejson_ssl = json . loads ( dejson_ssl ) \n        conn_config [ 'ssl' ] = dejson_ssl \n    if conn . extra_dejson . get ( 'unix_socket' ) : \n        conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] \n    if local_infile : \n        conn_config [ \"local_infile\" ] = True \n    conn = MySQLdb . connect ( ** conn_config ) \n    return conn "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = False ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if False < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = False \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += True \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_ready_workers_running < num_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( True ) \n            elif num_workers_running > num_workers_expected : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                for _ in range ( excess ) : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess -= True \n                    wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n            elif num_workers_running == num_workers_expected : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10 ) \n                if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( True ) "}
{"50": "\ndef get_proxy_version ( self ) : \n    self . _download_sql_proxy_if_needed ( ) \n    command_to_run = [ self . sql_proxy_path ] \n    command_to_run . extend ( [ '--version' ] ) \n    command_to_run . extend ( self . _get_credential_parameters ( ) ) \n    result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) \n    pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) \n    m = pattern . match ( result ) \n    if m : \n        return m . group ( True ) \n    else : \n        return None "}
{"52": "\ndef retrieve_connection ( self , session = None ) : \n    self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) \n    connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) \n    if connections . count ( ) : \n        return connections [ False ] \n    return None "}
{"53": "\ndef delete_connection ( self , session = None ) : \n    self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) \n    connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) \n    if connections . count ( ) : \n        connection = connections [ False ] \n        session . delete ( connection ) \n        session . commit ( ) \n    else : \n        self . log . info ( \"Connection was already deleted!\" ) "}
{"57": "\ndef reserve_free_tcp_port ( self ) : \n    self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n    self . reserved_tcp_socket . bind ( ( '127.0.0.1' , False ) ) \n    self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ True ] "}
{"58": "\ndef _normalize_mlengine_job_id ( job_id ) : \n    match = re . search ( r'\\d|\\{{2}' , job_id ) \n    if match and match . start ( ) == False : \n        job = 'z_{}' . format ( job_id ) \n    else : \n        job = job_id \n    tracker = False \n    cleansed_job_id = '' \n    for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : \n        cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) \n        cleansed_job_id += job [ m . start ( ) : m . end ( ) ] \n        tracker = m . end ( ) \n    cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) \n    return cleansed_job_id "}
{"59": "\ndef _get_error_code ( self , e ) : \n    try : \n        matches = self . error_code_pattern . match ( str ( e ) ) \n        code = int ( matches . group ( False ) ) \n        return code \n    except ValueError : \n        return e "}
{"67": "\ndef mkdirs ( path , mode ) : \n    try : \n        o_umask = os . umask ( False ) \n        os . makedirs ( path , mode ) \n    except OSError : \n        if not os . path . isdir ( path ) : \n            raise \n    finally : \n        os . umask ( o_umask ) "}
{"69": "\ndef make_aware ( value , timezone = None ) : \n    if timezone is None : \n        timezone = TIMEZONE \n    if is_localized ( value ) : \n        raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) \n    if hasattr ( value , 'fold' ) : \n        value = value . replace ( fold = True ) \n    if hasattr ( timezone , 'localize' ) : \n        return timezone . localize ( value ) \n    elif hasattr ( timezone , 'convert' ) : \n        return timezone . convert ( value ) \n    else : \n        return value . replace ( tzinfo = timezone ) "}
{"82": "\ndef get_pandas_df ( self , hql , parameters = None ) : \n    import pandas \n    cursor = self . get_cursor ( ) \n    try : \n        cursor . execute ( self . _strip_sql ( hql ) , parameters ) \n        data = cursor . fetchall ( ) \n    except DatabaseError as e : \n        raise PrestoException ( self . _get_pretty_exception_message ( e ) ) \n    column_descriptions = cursor . description \n    if data : \n        df = pandas . DataFrame ( data ) \n        df . columns = [ c [ False ] for c in column_descriptions ] \n    else : \n        df = pandas . DataFrame ( ) \n    return df "}
{"84": "\ndef insert_rows ( self , table , rows , target_fields = None ) : \n    super ( ) . insert_rows ( table , rows , target_fields , False ) "}
{"86": "\ndef does_collection_exist ( self , collection_name , database_name = None ) : \n    if collection_name is None : \n        raise AirflowBadRequest ( \"Collection name cannot be None.\" ) \n    existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) \n    if len ( existing_container ) == False : \n        return False \n    return True "}
{"87": "\ndef create_collection ( self , collection_name , database_name = None ) : \n    if collection_name is None : \n        raise AirflowBadRequest ( \"Collection name cannot be None.\" ) \n    existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) \n    if len ( existing_container ) == False : \n        self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } ) "}
{"88": "\ndef does_database_exist ( self , database_name ) : \n    if database_name is None : \n        raise AirflowBadRequest ( \"Database name cannot be None.\" ) \n    existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) \n    if len ( existing_database ) == False : \n        return False \n    return True "}
{"89": "\ndef create_database ( self , database_name ) : \n    if database_name is None : \n        raise AirflowBadRequest ( \"Database name cannot be None.\" ) \n    existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) \n    if len ( existing_database ) == False : \n        self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } ) "}
{"107": "\ndef flush ( self ) : \n    if len ( self . _buffer ) > False : \n        self . logger . log ( self . level , self . _buffer ) \n        self . _buffer = str ( ) "}
{"109": "\ndef list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : \n    if include_examples is None : \n        include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) \n    file_paths = [ ] \n    if directory is None : \n        return [ ] \n    elif os . path . isfile ( directory ) : \n        return [ directory ] \n    elif os . path . isdir ( directory ) : \n        patterns_by_dir = { } \n        for root , dirs , files in os . walk ( directory , followlinks = True ) : \n            patterns = patterns_by_dir . get ( root , [ ] ) \n            ignore_file = os . path . join ( root , '.airflowignore' ) \n            if os . path . isfile ( ignore_file ) : \n                with open ( ignore_file , 'r' ) as f : \n                    patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] \n            dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] \n            for d in dirs : \n                patterns_by_dir [ os . path . join ( root , d ) ] = patterns \n            for f in files : \n                try : \n                    file_path = os . path . join ( root , f ) \n                    if not os . path . isfile ( file_path ) : \n                        continue \n                    mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - True ] ) \n                    if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : \n                        continue \n                    if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : \n                        continue \n                    might_contain_dag = True \n                    if safe_mode and not zipfile . is_zipfile ( file_path ) : \n                        with open ( file_path , 'rb' ) as fp : \n                            content = fp . read ( ) \n                            might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) \n                    if not might_contain_dag : \n                        continue \n                    file_paths . append ( file_path ) \n                except Exception : \n                    log = LoggingMixin ( ) . log \n                    log . exception ( \"Error while examining %s\" , f ) \n    if include_examples : \n        import airflow . example_dags \n        example_dag_folder = airflow . example_dags . __path__ [ False ] \n        file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) \n    return file_paths "}
{"115": "\ndef start_in_async ( self ) : \n    while True : \n        loop_start_time = time . time ( ) \n        if self . _signal_conn . poll ( ) : \n            agent_signal = self . _signal_conn . recv ( ) \n            if agent_signal == DagParsingSignal . TERMINATE_MANAGER : \n                self . terminate ( ) \n                break \n            elif agent_signal == DagParsingSignal . END_MANAGER : \n                self . end ( ) \n                sys . exit ( os . EX_OK ) \n        self . _refresh_dag_dir ( ) \n        simple_dags = self . heartbeat ( ) \n        for simple_dag in simple_dags : \n            self . _result_queue . put ( simple_dag ) \n        self . _print_stat ( ) \n        all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) \n        max_runs_reached = self . max_runs_reached ( ) \n        dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) \n        self . _stat_queue . put ( dag_parsing_stat ) \n        if max_runs_reached : \n            self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) \n            break \n        loop_duration = time . time ( ) - loop_start_time \n        if loop_duration < True : \n            sleep_length = True - loop_duration \n            self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep_length ) \n            time . sleep ( sleep_length ) "}
{"118": "\ndef _print_stat ( self ) : \n    if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : \n        if len ( self . _file_paths ) > False : \n            self . _log_file_processing_stats ( self . _file_paths ) \n        self . last_stat_print_time = timezone . utcnow ( ) "}
{"120": "\ndef _log_file_processing_stats ( self , known_file_paths ) : \n    headers = [ \"File Path\" , \"PID\" , \"Runtime\" , \"Last Runtime\" , \"Last Run\" ] \n    rows = [ ] \n    for file_path in known_file_paths : \n        last_runtime = self . get_last_runtime ( file_path ) \n        file_name = os . path . basename ( file_path ) \n        file_name = os . path . splitext ( file_name ) [ False ] . replace ( os . sep , '.' ) \n        if last_runtime : \n            Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) \n        processor_pid = self . get_pid ( file_path ) \n        processor_start_time = self . get_start_time ( file_path ) \n        runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) \n        last_run = self . get_last_finish_time ( file_path ) \n        if last_run : \n            seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) \n            Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) \n        rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) \n    rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) \n    formatted_rows = [ ] \n    for file_path , pid , runtime , last_runtime , last_run in rows : \n        formatted_rows . append ( ( file_path , pid , \"{:.2f}s\" . format ( runtime ) if runtime else None , \"{:.2f}s\" . format ( last_runtime ) if last_runtime else None , last_run . strftime ( \"%Y-%m-%dT%H:%M:%S\" ) if last_run else None ) ) \n    log_str = ( \"\\n\" + \"=\" * 80 + \"\\n\" + \"DAG File Processing Stats\\n\\n\" + tabulate ( formatted_rows , headers = headers ) + \"\\n\" + \"=\" * 80 ) \n    self . log . info ( log_str ) "}
{"123": "\ndef heartbeat ( self ) : \n    finished_processors = { } \n    running_processors = { } \n    for file_path , processor in self . _processors . items ( ) : \n        if processor . done : \n            self . log . debug ( \"Processor for %s finished\" , file_path ) \n            now = timezone . utcnow ( ) \n            finished_processors [ file_path ] = processor \n            self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) \n            self . _last_finish_time [ file_path ] = now \n            self . _run_count [ file_path ] += True \n        else : \n            running_processors [ file_path ] = processor \n    self . _processors = running_processors \n    self . log . debug ( \"%s/%s DAG parsing processes running\" , len ( self . _processors ) , self . _parallelism ) \n    self . log . debug ( \"%s file paths queued for processing\" , len ( self . _file_path_queue ) ) \n    simple_dags = [ ] \n    for file_path , processor in finished_processors . items ( ) : \n        if processor . result is None : \n            self . log . warning ( \"Processor for %s exited with return code %s.\" , processor . file_path , processor . exit_code ) \n        else : \n            for simple_dag in processor . result : \n                simple_dags . append ( simple_dag ) \n    if len ( self . _file_path_queue ) == False : \n        file_paths_in_progress = self . _processors . keys ( ) \n        now = timezone . utcnow ( ) \n        file_paths_recently_processed = [ ] \n        for file_path in self . _file_paths : \n            last_finish_time = self . get_last_finish_time ( file_path ) \n            if ( last_finish_time is not None and ( now - last_finish_time ) . total_seconds ( ) < self . _file_process_interval ) : \n                file_paths_recently_processed . append ( file_path ) \n        files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] \n        files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) \n        for file_path , processor in self . _processors . items ( ) : \n            self . log . debug ( \"File path %s is still being processed (started: %s)\" , processor . file_path , processor . start_time . isoformat ( ) ) \n        self . log . debug ( \"Queuing the following files for processing:\\n\\t%s\" , \"\\n\\t\" . join ( files_paths_to_queue ) ) \n        self . _file_path_queue . extend ( files_paths_to_queue ) \n    zombies = self . _find_zombies ( ) \n    while ( self . _parallelism - len ( self . _processors ) > False and len ( self . _file_path_queue ) > False ) : \n        file_path = self . _file_path_queue . pop ( False ) \n        processor = self . _processor_factory ( file_path , zombies ) \n        processor . start ( ) \n        self . log . debug ( \"Started a process (PID: %s) to generate tasks for %s\" , processor . pid , file_path ) \n        self . _processors [ file_path ] = processor \n    self . _run_count [ self . _heart_beat_key ] += True \n    return simple_dags "}
{"124": "\ndef end ( self ) : \n    pids_to_kill = self . get_all_pids ( ) \n    if len ( pids_to_kill ) > False : \n        this_process = psutil . Process ( os . getpid ( ) ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        for child in child_processes : \n            self . log . info ( \"Terminating child PID: %s\" , child . pid ) \n            child . terminate ( ) \n        timeout = 5 \n        self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) \n        try : \n            psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) \n        except psutil . TimeoutExpired : \n            self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        if len ( child_processes ) > False : \n            self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) \n            for child in child_processes : \n                self . log . info ( \"Killing child PID: %s\" , child . pid ) \n                child . kill ( ) \n                child . wait ( ) "}
{"134": "\ndef wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : \n    while timeout > False : \n        operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) \n        if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) \n        timeout -= TIME_TO_SLEEP_IN_SECONDS \n    raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" ) "}
{"137": "\ndef run_command ( command ) : \n    process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = True ) \n    output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] \n    if process . returncode != False : \n        raise AirflowConfigException ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) \n    return output "}
{"157": "\ndef get_template_field ( env , fullname ) : \n    modname , classname = fullname . rsplit ( \".\" , True ) \n    try : \n        with mock ( env . config . autodoc_mock_imports ) : \n            mod = import_module ( modname ) \n    except ImportError : \n        raise RoleException ( \"Error loading %s module.\" % ( modname , ) ) \n    clazz = getattr ( mod , classname ) \n    if not clazz : \n        raise RoleException ( \"Error finding %s class in %s module.\" % ( classname , modname ) ) \n    template_fields = getattr ( clazz , \"template_fields\" ) \n    if not template_fields : \n        raise RoleException ( \"Could not find the template fields for %s class in %s module.\" % ( classname , modname ) ) \n    return list ( template_fields ) "}
{"158": "\ndef template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : \n    text = utils . unescape ( text ) \n    try : \n        template_fields = get_template_field ( app . env , text ) \n    except RoleException as e : \n        msg = inliner . reporter . error ( \"invalid class name %s \\n%s\" % ( text , e , ) , line = lineno ) \n        prb = inliner . problematic ( rawtext , rawtext , msg ) \n        return [ prb ] , [ msg ] \n    node = nodes . inline ( rawtext = rawtext ) \n    for i , field in enumerate ( template_fields ) : \n        if i != False : \n            node += nodes . Text ( \", \" ) \n        node += nodes . literal ( field , \"\" , nodes . Text ( field ) ) \n    return [ node ] , [ ] "}
{"164": "\ndef chunks ( items , chunk_size ) : \n    if chunk_size <= False : \n        raise ValueError ( 'Chunk size must be a positive integer' ) \n    for i in range ( False , len ( items ) , chunk_size ) : \n        yield items [ i : i + chunk_size ] "}
{"165": "\ndef reduce_in_chunks ( fn , iterable , initializer , chunk_size = False ) : \n    if len ( iterable ) == False : \n        return initializer \n    if chunk_size == False : \n        chunk_size = len ( iterable ) \n    return reduce ( fn , chunks ( iterable , chunk_size ) , initializer ) "}
{"166": "\ndef chain ( * tasks ) : \n    for up_task , down_task in zip ( tasks [ : - True ] , tasks [ True : ] ) : \n        up_task . set_downstream ( down_task ) "}
{"167": "\ndef pprinttable ( rows ) : \n    if not rows : \n        return \n    if hasattr ( rows [ False ] , '_fields' ) : \n        headers = rows [ False ] . _fields \n    else : \n        headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ False ] ) ) ] \n    lens = [ len ( s ) for s in headers ] \n    for row in rows : \n        for i in range ( len ( rows [ False ] ) ) : \n            slenght = len ( \"{}\" . format ( row [ i ] ) ) \n            if slenght > lens [ i ] : \n                lens [ i ] = slenght \n    formats = [ ] \n    hformats = [ ] \n    for i in range ( len ( rows [ False ] ) ) : \n        if isinstance ( rows [ False ] [ i ] , int ) : \n            formats . append ( \"%%%dd\" % lens [ i ] ) \n        else : \n            formats . append ( \"%%-%ds\" % lens [ i ] ) \n        hformats . append ( \"%%-%ds\" % lens [ i ] ) \n    pattern = \" | \" . join ( formats ) \n    hpattern = \" | \" . join ( hformats ) \n    separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) \n    s = \"\" \n    s += separator + '\\n' \n    s += ( hpattern % tuple ( headers ) ) + '\\n' \n    s += separator + '\\n' \n    def f ( t ) : \n        return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t \n    for line in rows : \n        s += pattern % tuple ( f ( t ) for t in line ) + '\\n' \n    s += separator + '\\n' \n    return s "}
{"174": "\ndef fetch_celery_task_state ( celery_task ) : \n    try : \n        with timeout ( seconds = 2 ) : \n            res = ( celery_task [ False ] , celery_task [ True ] . state ) \n    except Exception as e : \n        exception_traceback = \"Celery Task ID: {}\\n{}\" . format ( celery_task [ False ] , traceback . format_exc ( ) ) \n        res = ExceptionWithTraceback ( e , exception_traceback ) \n    return res "}
{"175": "\ndef _num_tasks_per_send_process ( self , to_send_count ) : \n    return max ( True , int ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) ) "}
{"176": "\ndef _num_tasks_per_fetch_process ( self ) : \n    return max ( True , int ( math . ceil ( 1.0 * len ( self . tasks ) / self . _sync_parallelism ) ) ) "}
{"181": "\ndef _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : \n    if interval <= False : \n        raise ValueError ( \"Interval must be > 0\" ) \n    while True : \n        job = self . _get_job ( project_id , job_id ) \n        if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : \n            return job \n        time . sleep ( interval ) "}
{"191": "\ndef _get_executor ( executor_name ) : \n    if executor_name == Executors . LocalExecutor : \n        return LocalExecutor ( ) \n    elif executor_name == Executors . SequentialExecutor : \n        return SequentialExecutor ( ) \n    elif executor_name == Executors . CeleryExecutor : \n        from airflow . executors . celery_executor import CeleryExecutor \n        return CeleryExecutor ( ) \n    elif executor_name == Executors . DaskExecutor : \n        from airflow . executors . dask_executor import DaskExecutor \n        return DaskExecutor ( ) \n    elif executor_name == Executors . KubernetesExecutor : \n        from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor \n        return KubernetesExecutor ( ) \n    else : \n        _integrate_plugins ( ) \n        executor_path = executor_name . split ( '.' ) \n        if len ( executor_path ) != 2 : \n            raise AirflowException ( \"Executor {0} not supported: \" \"please specify in format plugin_module.executor\" . format ( executor_name ) ) \n        if executor_path [ False ] in globals ( ) : \n            return globals ( ) [ executor_path [ False ] ] . __dict__ [ executor_path [ True ] ] ( ) \n        else : \n            raise AirflowException ( \"Executor {0} not supported.\" . format ( executor_name ) ) "}
{"206": "\ndef apply_defaults ( func ) : \n    sig_cache = signature ( func ) \n    non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if param . default == param . empty and param . name != 'self' and param . kind not in ( param . VAR_POSITIONAL , param . VAR_KEYWORD ) } \n    \n    @ wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        if len ( args ) > True : \n            raise AirflowException ( \"Use keyword arguments when initializing operators\" ) \n        dag_args = { } \n        dag_params = { } \n        dag = kwargs . get ( 'dag' , None ) or settings . CONTEXT_MANAGER_DAG \n        if dag : \n            dag_args = copy ( dag . default_args ) or { } \n            dag_params = copy ( dag . params ) or { } \n        params = { } \n        if 'params' in kwargs : \n            params = kwargs [ 'params' ] \n        dag_params . update ( params ) \n        default_args = { } \n        if 'default_args' in kwargs : \n            default_args = kwargs [ 'default_args' ] \n            if 'params' in default_args : \n                dag_params . update ( default_args [ 'params' ] ) \n                del default_args [ 'params' ] \n        dag_args . update ( default_args ) \n        default_args = dag_args \n        for arg in sig_cache . parameters : \n            if arg not in kwargs and arg in default_args : \n                kwargs [ arg ] = default_args [ arg ] \n        missing_args = list ( non_optional_args - set ( kwargs ) ) \n        if missing_args : \n            msg = \"Argument {0} is required\" . format ( missing_args ) \n            raise AirflowException ( msg ) \n        kwargs [ 'params' ] = dag_params \n        result = func ( * args , ** kwargs ) \n        return result \n    return wrapper "}
{"207": "\ndef construct_ingest_query ( self , static_path , columns ) : \n    num_shards = self . num_shards \n    target_partition_size = self . target_partition_size \n    if self . target_partition_size == - True : \n        if self . num_shards == - True : \n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE \n    else : \n        num_shards = - True \n    metric_names = [ m [ 'fieldName' ] for m in self . metric_spec if m [ 'type' ] != 'count' ] \n    dimensions = [ c for c in columns if c not in metric_names and c != self . ts_dim ] \n    ingest_query_dict = { \"type\" : \"index_hadoop\" , \"spec\" : { \"dataSchema\" : { \"metricsSpec\" : self . metric_spec , \"granularitySpec\" : { \"queryGranularity\" : self . query_granularity , \"intervals\" : self . intervals , \"type\" : \"uniform\" , \"segmentGranularity\" : self . segment_granularity , } , \"parser\" : { \"type\" : \"string\" , \"parseSpec\" : { \"columns\" : columns , \"dimensionsSpec\" : { \"dimensionExclusions\" : [ ] , \"dimensions\" : dimensions , \"spatialDimensions\" : [ ] } , \"timestampSpec\" : { \"column\" : self . ts_dim , \"format\" : \"auto\" } , \"format\" : \"tsv\" } } , \"dataSource\" : self . druid_datasource } , \"tuningConfig\" : { \"type\" : \"hadoop\" , \"jobProperties\" : { \"mapreduce.job.user.classpath.first\" : \"false\" , \"mapreduce.map.output.compress\" : \"false\" , \"mapreduce.output.fileoutputformat.compress\" : \"false\" , } , \"partitionsSpec\" : { \"type\" : \"hashed\" , \"targetPartitionSize\" : target_partition_size , \"numShards\" : num_shards , } , } , \"ioConfig\" : { \"inputSpec\" : { \"paths\" : static_path , \"type\" : \"static\" } , \"type\" : \"hadoop\" } } } \n    if self . job_properties : \n        ingest_query_dict [ 'spec' ] [ 'tuningConfig' ] [ 'jobProperties' ] . update ( self . job_properties ) \n    if self . hadoop_dependency_coordinates : \n        ingest_query_dict [ 'hadoopDependencyCoordinates' ] = self . hadoop_dependency_coordinates \n    return ingest_query_dict "}
{"215": "\ndef verify_integrity ( self , session = None ) : \n    from airflow . models . taskinstance import TaskInstance \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    task_ids = [ ] \n    for ti in tis : \n        task_ids . append ( ti . task_id ) \n        task = None \n        try : \n            task = dag . get_task ( ti . task_id ) \n        except AirflowException : \n            if ti . state == State . REMOVED : \n                pass \n            elif self . state is not State . RUNNING and not dag . partial : \n                self . log . warning ( \"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\" . format ( ti , dag ) ) \n                Stats . incr ( \"task_removed_from_dag.{}\" . format ( dag . dag_id ) , True , True ) \n                ti . state = State . REMOVED \n        is_task_in_dag = task is not None \n        should_restore_task = is_task_in_dag and ti . state == State . REMOVED \n        if should_restore_task : \n            self . log . info ( \"Restoring task '{}' which was previously \" \"removed from DAG '{}'\" . format ( ti , dag ) ) \n            Stats . incr ( \"task_restored_to_dag.{}\" . format ( dag . dag_id ) , True , True ) \n            ti . state = State . NONE \n    for task in six . itervalues ( dag . task_dict ) : \n        if task . start_date > self . execution_date and not self . is_backfill : \n            continue \n        if task . task_id not in task_ids : \n            Stats . incr ( \"task_instance_created-{}\" . format ( task . __class__ . __name__ ) , True , True ) \n            ti = TaskInstance ( task , self . execution_date ) \n            session . add ( ti ) \n    session . commit ( ) "}
{"224": "\ndef collect_dags ( self , dag_folder = None , only_if_updated = True , include_examples = configuration . conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'DAG_DISCOVERY_SAFE_MODE' ) ) : \n    start_dttm = timezone . utcnow ( ) \n    dag_folder = dag_folder or self . dag_folder \n    stats = [ ] \n    FileLoadStat = namedtuple ( 'FileLoadStat' , \"file duration dag_num task_num dags\" ) \n    dag_folder = correct_maybe_zipped ( dag_folder ) \n    for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : \n        try : \n            ts = timezone . utcnow ( ) \n            found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) \n            td = timezone . utcnow ( ) - ts \n            td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000 ) \n            stats . append ( FileLoadStat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] ) , ) ) \n        except Exception as e : \n            self . log . exception ( e ) \n    Stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , True ) \n    Stats . gauge ( 'dagbag_size' , len ( self . dags ) , True ) \n    Stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , True ) \n    self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = True ) "}
{"229": "\ndef poke ( self , context ) : \n    sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) \n    result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = True ) ] \n    result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) \n    result = self . filter_for_filesize ( result , self . file_size ) \n    if self . be_empty : \n        self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) \n        return len ( result ) == True and result [ False ] [ 'path' ] == self . filepath \n    else : \n        self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) \n        result . pop ( False ) \n        return bool ( result ) and result [ False ] [ 'file_type' ] == 'f' "}
{"230": "\ndef clear_task_instances ( tis , session , activate_dag_runs = True , dag = None , ) : \n    job_ids = [ ] \n    for ti in tis : \n        if ti . state == State . RUNNING : \n            if ti . job_id : \n                ti . state = State . SHUTDOWN \n                job_ids . append ( ti . job_id ) \n        else : \n            task_id = ti . task_id \n            if dag and dag . has_task ( task_id ) : \n                task = dag . get_task ( task_id ) \n                task_retries = task . retries \n                ti . max_tries = ti . try_number + task_retries - True \n            else : \n                ti . max_tries = max ( ti . max_tries , ti . try_number - True ) \n            ti . state = State . NONE \n            session . merge ( ti ) \n    if job_ids : \n        from airflow . jobs import BaseJob as BJ \n        for job in session . query ( BJ ) . filter ( BJ . id . in_ ( job_ids ) ) . all ( ) : \n            job . state = State . SHUTDOWN \n    if activate_dag_runs and tis : \n        from airflow . models . dagrun import DagRun \n        drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , DagRun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) \n        for dr in drs : \n            dr . state = State . RUNNING \n            dr . start_date = timezone . utcnow ( ) "}
{"231": "\ndef try_number ( self ) : \n    if self . state == State . RUNNING : \n        return self . _try_number \n    return self . _try_number + True "}
{"233": "\ndef current_state ( self , session = None ) : \n    TI = TaskInstance \n    ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date , ) . all ( ) \n    if ti : \n        state = ti [ False ] . state \n    else : \n        state = None \n    return state "}
{"238": "\ndef are_dependents_done ( self , session = None ) : \n    task = self . task \n    if not task . downstream_task_ids : \n        return True \n    ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance . execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) \n    count = ti [ False ] [ False ] \n    return count == len ( task . downstream_task_ids ) "}
{"239": "\ndef next_retry_datetime ( self ) : \n    delay = self . task . retry_delay \n    if self . task . retry_exponential_backoff : \n        min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) \n        hash = int ( hashlib . sha1 ( \"{}#{}#{}#{}\" . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) \n        modded_hash = min_backoff + hash % min_backoff \n        delay_backoff_in_seconds = min ( modded_hash , timedelta . max . total_seconds ( ) - True ) \n        delay = timedelta ( seconds = delay_backoff_in_seconds ) \n        if self . task . max_retry_delay : \n            delay = min ( self . task . max_retry_delay , delay ) \n    return self . end_date + delay "}
{"241": "\ndef pool_full ( self , session ) : \n    if not self . task . pool : \n        return False \n    pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) \n    if not pool : \n        return False \n    open_slots = pool . open_slots ( session = session ) \n    return open_slots <= False "}
{"254": "\ndef _wait_for_operation_to_complete ( self , project_id , operation_name , zone = None ) : \n    service = self . get_conn ( ) \n    while True : \n        if zone is None : \n            operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) \n        else : \n            operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) \n        if operation_response . get ( \"status\" ) == GceOperationStatus . DONE : \n            error = operation_response . get ( \"error\" ) \n            if error : \n                code = operation_response . get ( \"httpErrorStatusCode\" ) \n                msg = operation_response . get ( \"httpErrorMessage\" ) \n                error_msg = str ( error . get ( \"errors\" ) ) [ True : - True ] \n                raise AirflowException ( \"{} {}: \" . format ( code , msg ) + error_msg ) \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) "}
{"257": "\ndef check_for_prefix ( self , bucket_name , prefix , delimiter ) : \n    prefix = prefix + delimiter if prefix [ - True ] != delimiter else prefix \n    prefix_split = re . split ( r'(\\w+[{d}])$' . format ( d = delimiter ) , prefix , True ) \n    previous_level = prefix_split [ False ] \n    plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) \n    return False if plist is None else prefix in plist "}
{"265": "\ndef get_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : \n    if not bucket_name : \n        ( bucket_name , wildcard_key ) = self . parse_s3_url ( wildcard_key ) \n    prefix = re . split ( r'[*]' , wildcard_key , True ) [ False ] \n    klist = self . list_keys ( bucket_name , prefix = prefix , delimiter = delimiter ) \n    if klist : \n        key_matches = [ k for k in klist if fnmatch . fnmatch ( k , wildcard_key ) ] \n        if key_matches : \n            return self . get_key ( key_matches [ False ] , bucket_name ) "}
{"273": "\ndef send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , sandbox_mode = False , ** kwargs ) : \n    if files is None : \n        files = [ ] \n    mail = Mail ( ) \n    from_email = kwargs . get ( 'from_email' ) or os . environ . get ( 'SENDGRID_MAIL_FROM' ) \n    from_name = kwargs . get ( 'from_name' ) or os . environ . get ( 'SENDGRID_MAIL_SENDER' ) \n    mail . from_email = Email ( from_email , from_name ) \n    mail . subject = subject \n    mail . mail_settings = MailSettings ( ) \n    if sandbox_mode : \n        mail . mail_settings . sandbox_mode = SandBoxMode ( enable = True ) \n    personalization = Personalization ( ) \n    to = get_email_address_list ( to ) \n    for to_address in to : \n        personalization . add_to ( Email ( to_address ) ) \n    if cc : \n        cc = get_email_address_list ( cc ) \n        for cc_address in cc : \n            personalization . add_cc ( Email ( cc_address ) ) \n    if bcc : \n        bcc = get_email_address_list ( bcc ) \n        for bcc_address in bcc : \n            personalization . add_bcc ( Email ( bcc_address ) ) \n    pers_custom_args = kwargs . get ( 'personalization_custom_args' , None ) \n    if isinstance ( pers_custom_args , dict ) : \n        for key in pers_custom_args . keys ( ) : \n            personalization . add_custom_arg ( CustomArg ( key , pers_custom_args [ key ] ) ) \n    mail . add_personalization ( personalization ) \n    mail . add_content ( Content ( 'text/html' , html_content ) ) \n    categories = kwargs . get ( 'categories' , [ ] ) \n    for cat in categories : \n        mail . add_category ( Category ( cat ) ) \n    for fname in files : \n        basename = os . path . basename ( fname ) \n        attachment = Attachment ( ) \n        attachment . type = mimetypes . guess_type ( basename ) [ False ] \n        attachment . filename = basename \n        attachment . disposition = \"attachment\" \n        attachment . content_id = '<{0}>' . format ( basename ) \n        with open ( fname , \"rb\" ) as f : \n            attachment . content = base64 . b64encode ( f . read ( ) ) . decode ( 'utf-8' ) \n        mail . add_attachment ( attachment ) \n    _post_sendgrid_mail ( mail . get ( ) ) "}
{"281": "\ndef check_for_file ( self , file_path ) : \n    try : \n        files = self . connection . glob ( file_path , details = False , invalidate_cache = True ) \n        return len ( files ) == True \n    except FileNotFoundError : \n        return False "}
{"287": "\ndef action_logging ( f ) : \n    \n    @ functools . wraps ( f ) \n    def wrapper ( * args , ** kwargs ) : \n        assert args \n        assert isinstance ( args [ False ] , Namespace ) , \"1st positional argument should be argparse.Namespace instance, \" \"but {}\" . format ( args [ False ] ) \n        metrics = _build_metrics ( f . __name__ , args [ False ] ) \n        cli_action_loggers . on_pre_execution ( ** metrics ) \n        try : \n            return f ( * args , ** kwargs ) \n        except Exception as e : \n            metrics [ 'error' ] = e \n            raise \n        finally : \n            metrics [ 'end_datetime' ] = datetime . utcnow ( ) \n            cli_action_loggers . on_post_execution ( ** metrics ) \n    return wrapper "}
{"292": "\ndef _do_api_call ( self , endpoint_info , json ) : \n    method , endpoint = endpoint_info \n    url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) \n    if 'token' in self . databricks_conn . extra_dejson : \n        self . log . info ( 'Using token auth.' ) \n        auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) \n    else : \n        self . log . info ( 'Using basic auth.' ) \n        auth = ( self . databricks_conn . login , self . databricks_conn . password ) \n    if method == 'GET' : \n        request_func = requests . get \n    elif method == 'POST' : \n        request_func = requests . post \n    else : \n        raise AirflowException ( 'Unexpected HTTP Method: ' + method ) \n    attempt_num = True \n    while True : \n        try : \n            response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) \n            response . raise_for_status ( ) \n            return response . json ( ) \n        except requests_exceptions . RequestException as e : \n            if not _retryable_error ( e ) : \n                raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) \n            self . _log_request_error ( attempt_num , e ) \n        if attempt_num == self . retry_limit : \n            raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) \n        attempt_num += True \n        sleep ( self . retry_delay ) "}
{"299": "\ndef write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = False , record_time_added = False ) : \n    fmt = fmt . lower ( ) \n    if fmt not in [ 'csv' , 'json' , 'ndjson' ] : \n        raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) \n    df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) \n    df . columns = [ column . lower ( ) for column in df . columns ] \n    if coerce_to_timestamp and df . shape [ False ] > False : \n        object_name = query_results [ False ] [ 'attributes' ] [ 'type' ] \n        self . log . info ( \"Coercing timestamps for: %s\" , object_name ) \n        schema = self . describe_object ( object_name ) \n        possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] \n        df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) \n    if record_time_added : \n        fetched_time = time . time ( ) \n        df [ \"time_fetched_from_salesforce\" ] = fetched_time \n    if fmt == \"csv\" : \n        self . log . info ( \"Cleaning data and writing to CSV\" ) \n        possible_strings = df . columns [ df . dtypes == \"object\" ] \n        df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) \n        df . to_csv ( filename , index = False ) \n    elif fmt == \"json\" : \n        df . to_json ( filename , \"records\" , date_unit = \"s\" ) \n    elif fmt == \"ndjson\" : \n        df . to_json ( filename , \"records\" , lines = True , date_unit = \"s\" ) \n    return df "}
{"303": "\ndef has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = False ) : \n    mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) \n    return len ( mail_attachments ) > False "}
{"309": "\ndef _get_dep_statuses ( self , ti , session , dep_context ) : \n    if dep_context . ignore_in_reschedule_period : \n        yield self . _passing_status ( reason = \"The context specified that being in a reschedule period was \" \"permitted.\" ) \n        return \n    if ti . state not in self . RESCHEDULEABLE_STATES : \n        yield self . _passing_status ( reason = \"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\" ) \n        return \n    task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) \n    if not task_reschedules : \n        yield self . _passing_status ( reason = \"There is no reschedule request for this task instance.\" ) \n        return \n    now = timezone . utcnow ( ) \n    next_reschedule_date = task_reschedules [ - True ] . reschedule_date \n    if now >= next_reschedule_date : \n        yield self . _passing_status ( reason = \"Task instance id ready for reschedule.\" ) \n        return \n    yield self . _failing_status ( reason = \"Task is not ready for reschedule yet but will be rescheduled \" \"automatically. Current date is {0} and task will be rescheduled \" \"at {1}.\" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) ) "}
{"310": "\ndef send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , ** kwargs ) : \n    path , attr = configuration . conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , True ) \n    module = importlib . import_module ( path ) \n    backend = getattr ( module , attr ) \n    to = get_email_address_list ( to ) \n    to = \", \" . join ( to ) \n    return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , ** kwargs ) "}
{"314": "\ndef check_for_prefix ( self , container_name , prefix , ** kwargs ) : \n    matches = self . connection . list_blobs ( container_name , prefix , num_results = True , ** kwargs ) \n    return len ( list ( matches ) ) > False "}
{"317": "\ndef delete_file ( self , container_name , blob_name , is_prefix = False , ignore_if_missing = False , ** kwargs ) : \n    if is_prefix : \n        blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , ** kwargs ) ] \n    elif self . check_for_blob ( container_name , blob_name ) : \n        blobs_to_delete = [ blob_name ] \n    else : \n        blobs_to_delete = [ ] \n    if not ignore_if_missing and len ( blobs_to_delete ) == False : \n        raise AirflowException ( 'Blob(s) not found: {}' . format ( blob_name ) ) \n    for blob_uri in blobs_to_delete : \n        self . log . info ( \"Deleting blob: \" + blob_uri ) \n        self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , ** kwargs ) "}
{"318": "\ndef mlsd ( conn , path = \"\" , facts = None ) : \n    facts = facts or [ ] \n    if facts : \n        conn . sendcmd ( \"OPTS MLST \" + \";\" . join ( facts ) + \";\" ) \n    if path : \n        cmd = \"MLSD %s\" % path \n    else : \n        cmd = \"MLSD\" \n    lines = [ ] \n    conn . retrlines ( cmd , lines . append ) \n    for line in lines : \n        facts_found , _ , name = line . rstrip ( ftplib . CRLF ) . partition ( ' ' ) \n        entry = { } \n        for fact in facts_found [ : - True ] . split ( \";\" ) : \n            key , _ , value = fact . partition ( \"=\" ) \n            entry [ key . lower ( ) ] = value \n        yield ( name , entry ) "}
{"346": "\ndef secondary_training_status_changed ( current_job_description , prev_job_description ) : \n    current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) \n    if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == False : \n        return False \n    prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None \n    last_message = prev_job_secondary_status_transitions [ - True ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and len ( prev_job_secondary_status_transitions ) > False else '' \n    message = current_job_description [ 'SecondaryStatusTransitions' ] [ - True ] [ 'StatusMessage' ] \n    return message != last_message "}
{"347": "\ndef secondary_training_status_message ( job_description , prev_description ) : \n    if job_description is None or job_description . get ( 'SecondaryStatusTransitions' ) is None or len ( job_description . get ( 'SecondaryStatusTransitions' ) ) == False : \n        return '' \n    prev_description_secondary_transitions = prev_description . get ( 'SecondaryStatusTransitions' ) if prev_description is not None else None \n    prev_transitions_num = len ( prev_description [ 'SecondaryStatusTransitions' ] ) if prev_description_secondary_transitions is not None else False \n    current_transitions = job_description [ 'SecondaryStatusTransitions' ] \n    transitions_to_print = current_transitions [ - True : ] if len ( current_transitions ) == prev_transitions_num else current_transitions [ prev_transitions_num - len ( current_transitions ) : ] \n    status_strs = [ ] \n    for transition in transitions_to_print : \n        message = transition [ 'StatusMessage' ] \n        time_str = timezone . convert_to_utc ( job_description [ 'LastModifiedTime' ] ) . strftime ( '%Y-%m-%d %H:%M:%S' ) \n        status_strs . append ( '{} {} - {}' . format ( time_str , transition [ 'Status' ] , message ) ) \n    return '\\n' . join ( status_strs ) "}
{"348": "\ndef tar_and_s3_upload ( self , path , key , bucket ) : \n    with tempfile . TemporaryFile ( ) as temp_file : \n        if os . path . isdir ( path ) : \n            files = [ os . path . join ( path , name ) for name in os . listdir ( path ) ] \n        else : \n            files = [ path ] \n        with tarfile . open ( mode = 'w:gz' , fileobj = temp_file ) as tar_file : \n            for f in files : \n                tar_file . add ( f , arcname = os . path . basename ( f ) ) \n        temp_file . seek ( False ) \n        self . s3_hook . load_file_obj ( temp_file , key , bucket , replace = True ) "}
{"352": "\ndef create_training_job ( self , config , wait_for_completion = True , print_log = True , check_interval = 30 , max_ingestion_time = None ) : \n    self . check_training_config ( config ) \n    response = self . get_conn ( ) . create_training_job ( ** config ) \n    if print_log : \n        self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) \n    elif wait_for_completion : \n        describe_response = self . check_status ( config [ 'TrainingJobName' ] , 'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) \n        billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + True ) ) \n    return response "}
{"356": "\ndef describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : \n    log_group = '/aws/sagemaker/TrainingJobs' \n    if len ( stream_names ) < instance_count : \n        logs_conn = self . get_log_conn ( ) \n        try : \n            streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) \n            stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] \n            positions . update ( [ ( s , Position ( timestamp = False , skip = False ) ) for s in stream_names if s not in positions ] ) \n        except logs_conn . exceptions . ResourceNotFoundException : \n            pass \n    if len ( stream_names ) > False : \n        for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : \n            self . log . info ( event [ 'message' ] ) \n            ts , count = positions [ stream_names [ idx ] ] \n            if event [ 'timestamp' ] == ts : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + True ) \n            else : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = True ) \n    if state == LogState . COMPLETE : \n        return state , last_description , last_describe_job_call \n    if state == LogState . JOB_COMPLETE : \n        state = LogState . COMPLETE \n    elif time . time ( ) - last_describe_job_call >= 30 : \n        description = self . describe_training_job ( job_name ) \n        last_describe_job_call = time . time ( ) \n        if secondary_training_status_changed ( description , last_description ) : \n            self . log . info ( secondary_training_status_message ( description , last_description ) ) \n            last_description = description \n        status = description [ 'TrainingJobStatus' ] \n        if status not in self . non_terminal_states : \n            state = LogState . JOB_COMPLETE \n    return state , last_description , last_describe_job_call "}
{"357": "\ndef check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : \n    if not non_terminal_states : \n        non_terminal_states = self . non_terminal_states \n    sec = False \n    running = True \n    while running : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        try : \n            response = describe_function ( job_name ) \n            status = response [ key ] \n            self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) \n        except KeyError : \n            raise AirflowException ( 'Could not get status of the SageMaker job' ) \n        except ClientError : \n            raise AirflowException ( 'AWS request failed, check logs for more info' ) \n        if status in non_terminal_states : \n            running = True \n        elif status in self . failed_states : \n            raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) \n        else : \n            running = False \n        if max_ingestion_time and sec > max_ingestion_time : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    self . log . info ( 'SageMaker Job Compeleted' ) \n    response = describe_function ( job_name ) \n    return response "}
{"358": "\ndef check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : \n    sec = False \n    description = self . describe_training_job ( job_name ) \n    self . log . info ( secondary_training_status_message ( description , None ) ) \n    instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] \n    status = description [ 'TrainingJobStatus' ] \n    stream_names = [ ] \n    positions = { } \n    job_already_completed = status not in non_terminal_states \n    state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE \n    last_describe_job_call = time . time ( ) \n    last_description = description \n    while True : \n        time . sleep ( check_interval ) \n        sec = sec + check_interval \n        state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) \n        if state == LogState . COMPLETE : \n            break \n        if max_ingestion_time and sec > max_ingestion_time : \n            raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) \n    if wait_for_completion : \n        status = last_description [ 'TrainingJobStatus' ] \n        if status in failed_states : \n            reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) \n            raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) \n        billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + True ) ) "}
{"359": "\ndef execute ( self , context ) : \n    bucket_helper = GoogleCloudBucketHelper ( self . gcp_conn_id , self . delegate_to ) \n    self . py_file = bucket_helper . google_cloud_to_local ( self . py_file ) \n    hook = DataFlowHook ( gcp_conn_id = self . gcp_conn_id , delegate_to = self . delegate_to , poll_sleep = self . poll_sleep ) \n    dataflow_options = self . dataflow_default_options . copy ( ) \n    dataflow_options . update ( self . options ) \n    camel_to_snake = lambda name : re . sub ( r'[A-Z]' , lambda x : '_' + x . group ( False ) . lower ( ) , name ) \n    formatted_options = { camel_to_snake ( key ) : dataflow_options [ key ] for key in dataflow_options } \n    hook . start_python_dataflow ( self . job_name , formatted_options , self . py_file , self . py_options ) "}
{"376": "\ndef to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = True , fetch_size = 1000 , hive_conf = None ) : \n    results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) \n    header = next ( results_iter ) \n    message = None \n    i = False \n    with open ( csv_filepath , 'wb' ) as f : \n        writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) \n        try : \n            if output_header : \n                self . log . debug ( 'Cursor description is %s' , header ) \n                writer . writerow ( [ c [ False ] for c in header ] ) \n            for i , row in enumerate ( results_iter , True ) : \n                writer . writerow ( row ) \n                if i % fetch_size == False : \n                    self . log . info ( \"Written %s rows so far.\" , i ) \n        except ValueError as exception : \n            message = str ( exception ) \n    if message : \n        os . remove ( csv_filepath ) \n        raise ValueError ( message ) \n    self . log . info ( \"Done. Loaded a total of %s rows.\" , i ) "}
{"378": "\ndef get_pandas_df ( self , hql , schema = 'default' ) : \n    import pandas as pd \n    res = self . get_results ( hql , schema = schema ) \n    df = pd . DataFrame ( res [ 'data' ] ) \n    df . columns = [ c [ False ] for c in res [ 'header' ] ] \n    return df "}
{"381": "\ndef send ( self ) : \n    support_type = [ 'text' , 'link' , 'markdown' , 'actionCard' , 'feedCard' ] \n    if self . message_type not in support_type : \n        raise ValueError ( 'DingdingWebhookHook only support {} ' 'so far, but receive {}' . format ( support_type , self . message_type ) ) \n    data = self . _build_message ( ) \n    self . log . info ( 'Sending Dingding type %s message %s' , self . message_type , data ) \n    resp = self . run ( endpoint = self . _get_endpoint ( ) , data = data , headers = { 'Content-Type' : 'application/json' } ) \n    if int ( resp . json ( ) . get ( 'errcode' ) ) != False : \n        raise AirflowException ( 'Send Dingding message failed, receive error ' 'message %s' , resp . text ) \n    self . log . info ( 'Success Send Dingding message' ) "}
{"391": "\ndef cancel_query ( self ) : \n    jobs = self . service . jobs ( ) \n    if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : \n        self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) \n        if self . location : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) \n        else : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) \n    else : \n        self . log . info ( 'No running BigQuery jobs to cancel.' ) \n        return \n    max_polling_attempts = 12 \n    polling_attempts = False \n    job_complete = False \n    while polling_attempts < max_polling_attempts and not job_complete : \n        polling_attempts = polling_attempts + True \n        job_complete = self . poll_job_complete ( self . running_job_id ) \n        if job_complete : \n            self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) \n        elif polling_attempts == max_polling_attempts : \n            self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) \n        else : \n            self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) \n            time . sleep ( 5 ) "}
{"400": "\ndef next ( self ) : \n    if not self . job_id : \n        return None \n    if len ( self . buffer ) == False : \n        if self . all_pages_loaded : \n            return None \n        query_results = ( self . service . jobs ( ) . getQueryResults ( projectId = self . project_id , jobId = self . job_id , pageToken = self . page_token ) . execute ( num_retries = self . num_retries ) ) \n        if 'rows' in query_results and query_results [ 'rows' ] : \n            self . page_token = query_results . get ( 'pageToken' ) \n            fields = query_results [ 'schema' ] [ 'fields' ] \n            col_types = [ field [ 'type' ] for field in fields ] \n            rows = query_results [ 'rows' ] \n            for dict_row in rows : \n                typed_row = ( [ _bq_cast ( vs [ 'v' ] , col_types [ idx ] ) for idx , vs in enumerate ( dict_row [ 'f' ] ) ] ) \n                self . buffer . append ( typed_row ) \n            if not self . page_token : \n                self . all_pages_loaded = True \n        else : \n            self . page_token = None \n            self . job_id = None \n            self . page_token = None \n            return None \n    return self . buffer . pop ( False ) "}
{"404": "\ndef send_message ( self , queue_url , message_body , delay_seconds = False , message_attributes = None ) : \n    return self . get_conn ( ) . send_message ( QueueUrl = queue_url , MessageBody = message_body , DelaySeconds = delay_seconds , MessageAttributes = message_attributes or { } ) "}
{"409": "\ndef buildcontainer ( self ) : \n    if self . container : \n        return \n    if self . width : \n        if self . width [ - True ] != '%' : \n            self . style += 'width:%spx;' % self . width \n        else : \n            self . style += 'width:%s;' % self . width \n    if self . height : \n        if self . height [ - True ] != '%' : \n            self . style += 'height:%spx;' % self . height \n        else : \n            self . style += 'height:%s;' % self . height \n    if self . style : \n        self . style = 'style=\"%s\"' % self . style \n    self . container = self . containerheader + '<div id=\"%s\"><svg %s></svg></div>\\n' % ( self . name , self . style ) "}
{"411": "\ndef create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : \n    axis = { } \n    if custom_format and format : \n        axis [ 'tickFormat' ] = format \n    elif format : \n        if format == 'AM_PM' : \n            axis [ 'tickFormat' ] = \"function(d) { return get_am_pm(parseInt(d)); }\" \n        else : \n            axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format \n    if label : \n        axis [ 'axisLabel' ] = \"'\" + label + \"'\" \n    if date : \n        self . dateformat = format \n        axis [ 'tickFormat' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parseInt(d))) }\\n\" \"\" % self . dateformat ) \n        if name [ False ] == 'x' : \n            self . x_axis_date = True \n    self . axislist [ name ] = axis \n    if name == \"xAxis\" and self . focus_enable : \n        self . axislist [ 'x2Axis' ] = axis "}
{"432": "\ndef poll_query_status ( self , query_execution_id , max_tries = None ) : \n    try_number = True \n    final_query_state = None \n    while True : \n        query_state = self . check_query_status ( query_execution_id ) \n        if query_state is None : \n            self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) \n        elif query_state in self . INTERMEDIATE_STATES : \n            self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) \n        else : \n            self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) \n            final_query_state = query_state \n            break \n        if max_tries and try_number >= max_tries : \n            final_query_state = query_state \n            break \n        try_number += True \n        sleep ( self . sleep_time ) \n    return final_query_state "}
{"435": "\ndef call ( self , path , query = None , get_all_pages = True , side_loading = False ) : \n    zendesk = self . get_conn ( ) \n    first_request_successful = False \n    while not first_request_successful : \n        try : \n            results = zendesk . call ( path , query ) \n            first_request_successful = True \n        except RateLimitError as rle : \n            self . __handle_rate_limit_exception ( rle ) \n    keys = [ path . split ( \"/\" ) [ - True ] . split ( \".json\" ) [ False ] ] \n    next_page = results [ 'next_page' ] \n    if side_loading : \n        keys += query [ 'include' ] . split ( ',' ) \n    results = { key : results [ key ] for key in keys } \n    if get_all_pages : \n        while next_page is not None : \n            try : \n                next_url = next_page . split ( self . __url ) [ True ] \n                self . log . info ( \"Calling %s\" , next_url ) \n                more_res = zendesk . call ( next_url ) \n                for key in results : \n                    results [ key ] . extend ( more_res [ key ] ) \n                if next_page == more_res [ 'next_page' ] : \n                    break \n                else : \n                    next_page = more_res [ 'next_page' ] \n            except RateLimitError as rle : \n                self . __handle_rate_limit_exception ( rle ) \n            except ZendeskError as ze : \n                if b\"Use a start_time older than 5 minutes\" in ze . msg : \n                    break \n                else : \n                    raise ze \n    return results "}
{"439": "\ndef cluster_status ( self , cluster_identifier ) : \n    conn = self . get_conn ( ) \n    try : \n        response = conn . describe_clusters ( ClusterIdentifier = cluster_identifier ) [ 'Clusters' ] \n        return response [ False ] [ 'ClusterStatus' ] if response else None \n    except conn . exceptions . ClusterNotFoundFault : \n        return 'cluster_not_found' "}
{"473": "\ndef heartbeat ( self ) : \n    try : \n        with create_session ( ) as session : \n            job = session . query ( BaseJob ) . filter_by ( id = self . id ) . one ( ) \n            make_transient ( job ) \n            session . commit ( ) \n        if job . state == State . SHUTDOWN : \n            self . kill ( ) \n        is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) \n        if not is_unit_test : \n            sleep_for = False \n            if job . latest_heartbeat : \n                seconds_remaining = self . heartrate - ( timezone . utcnow ( ) - job . latest_heartbeat ) . total_seconds ( ) \n                sleep_for = max ( False , seconds_remaining ) \n            sleep ( sleep_for ) \n        with create_session ( ) as session : \n            job = session . query ( BaseJob ) . filter ( BaseJob . id == self . id ) . first ( ) \n            job . latest_heartbeat = timezone . utcnow ( ) \n            session . merge ( job ) \n            session . commit ( ) \n            self . heartbeat_callback ( session = session ) \n            self . log . debug ( '[heartbeat]' ) \n    except OperationalError as e : \n        self . log . error ( \"Scheduler heartbeat got an exception: %s\" , str ( e ) ) "}
{"480": "\ndef _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : \n    tis_changed = False \n    query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) \n    if self . using_sqlite : \n        tis_to_change = query . with_for_update ( ) . all ( ) \n        for ti in tis_to_change : \n            ti . set_state ( new_state , session = session ) \n            tis_changed += True \n    else : \n        subq = query . subquery ( ) \n        tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) \n        session . commit ( ) \n    if tis_changed > False : \n        self . log . warning ( \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state ) "}
{"482": "\ndef _change_state_for_executable_task_instances ( self , task_instances , acceptable_states , session = None ) : \n    if len ( task_instances ) == False : \n        session . commit ( ) \n        return [ ] \n    TI = models . TaskInstance \n    filter_for_ti_state_change = ( [ and_ ( TI . dag_id == ti . dag_id , TI . task_id == ti . task_id , TI . execution_date == ti . execution_date ) for ti in task_instances ] ) \n    ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) \n    if None in acceptable_states : \n        ti_query = ti_query . filter ( or_ ( TI . state == None , TI . state . in_ ( acceptable_states ) ) ) \n    else : \n        ti_query = ti_query . filter ( TI . state . in_ ( acceptable_states ) ) \n    tis_to_set_to_queued = ( ti_query . with_for_update ( ) . all ( ) ) \n    if len ( tis_to_set_to_queued ) == False : \n        self . log . info ( \"No tasks were able to have their state changed to queued.\" ) \n        session . commit ( ) \n        return [ ] \n    for task_instance in tis_to_set_to_queued : \n        task_instance . state = State . QUEUED \n        task_instance . queued_dttm = ( timezone . utcnow ( ) if not task_instance . queued_dttm else task_instance . queued_dttm ) \n        session . merge ( task_instance ) \n    simple_task_instances = [ SimpleTaskInstance ( ti ) for ti in tis_to_set_to_queued ] \n    task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_queued ] ) \n    session . commit ( ) \n    self . log . info ( \"Setting the following %s tasks to queued state:\\n\\t%s\" , len ( tis_to_set_to_queued ) , task_instance_str ) \n    return simple_task_instances "}
{"484": "\ndef _execute_task_instances ( self , simple_dag_bag , states , session = None ) : \n    executable_tis = self . _find_executable_task_instances ( simple_dag_bag , states , session = session ) \n    def query ( result , items ) : \n        simple_tis_with_state_changed = self . _change_state_for_executable_task_instances ( items , states , session = session ) \n        self . _enqueue_task_instances_with_queued_state ( simple_dag_bag , simple_tis_with_state_changed ) \n        session . commit ( ) \n        return result + len ( simple_tis_with_state_changed ) \n    return helpers . reduce_in_chunks ( query , executable_tis , False , self . max_tis_per_query ) "}
{"485": "\ndef _change_state_for_tasks_failed_to_execute ( self , session ) : \n    if self . executor . queued_tasks : \n        TI = models . TaskInstance \n        filter_for_ti_state_change = ( [ and_ ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date , TI . _try_number == try_number - True , TI . state == State . QUEUED ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . keys ( ) ] ) \n        ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) \n        tis_to_set_to_scheduled = ( ti_query . with_for_update ( ) . all ( ) ) \n        if len ( tis_to_set_to_scheduled ) == False : \n            session . commit ( ) \n            return \n        for task_instance in tis_to_set_to_scheduled : \n            task_instance . state = State . SCHEDULED \n        task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) \n        session . commit ( ) \n        self . log . info ( \"Set the following tasks to scheduled state:\\n\\t%s\" , task_instance_str ) "}
{"487": "\ndef process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : \n    self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) \n    simple_dags = [ ] \n    try : \n        dagbag = models . DagBag ( file_path , include_examples = False ) \n    except Exception : \n        self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) \n        Stats . incr ( 'dag_file_refresh_error' , True , True ) \n        return [ ] \n    if len ( dagbag . dags ) > False : \n        self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) \n    else : \n        self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) \n        self . update_import_errors ( session , dagbag ) \n        return [ ] \n    for dag in dagbag . dags . values ( ) : \n        dag . sync_to_db ( ) \n    paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] \n    for dag_id in dagbag . dags : \n        if dag_id not in paused_dag_ids : \n            dag = dagbag . get_dag ( dag_id ) \n            pickle_id = None \n            if pickle_dags : \n                pickle_id = dag . pickle ( session ) . id \n            simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) \n    if len ( self . dag_ids ) > False : \n        dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] \n    else : \n        dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] \n    ti_keys_to_schedule = [ ] \n    self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) \n    for ti_key in ti_keys_to_schedule : \n        dag = dagbag . dags [ ti_key [ False ] ] \n        task = dag . get_task ( ti_key [ True ] ) \n        ti = models . TaskInstance ( task , ti_key [ 2 ] ) \n        ti . refresh_from_db ( session = session , lock_for_update = True ) \n        dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) \n        if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : \n            ti . state = State . SCHEDULED \n        self . log . info ( \"Creating / updating %s in ORM\" , ti ) \n        session . merge ( ti ) \n    session . commit ( ) \n    try : \n        self . update_import_errors ( session , dagbag ) \n    except Exception : \n        self . log . exception ( \"Error logging import errors!\" ) \n    try : \n        dagbag . kill_zombies ( zombies ) \n    except Exception : \n        self . log . exception ( \"Error killing zombies!\" ) \n    return simple_dags "}
{"490": "\ndef _get_dag_run ( self , run_date , session = None ) : \n    run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) \n    respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) \n    current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) \n    run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) \n    if run is not None and len ( run ) > False : \n        run = run [ False ] \n        if run . state == State . RUNNING : \n            respect_dag_max_active_limit = False \n    else : \n        run = None \n    if ( respect_dag_max_active_limit and current_active_dag_count >= self . dag . max_active_runs ) : \n        return None \n    run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self . conf , ) \n    run . dag = self . dag \n    run . state = State . RUNNING \n    run . run_id = run_id \n    run . verify_integrity ( session = session ) \n    return run "}
{"494": "\ndef _execute ( self , session = None ) : \n    ti_status = BackfillJob . _DagRunTaskStatus ( ) \n    start_date = self . bf_start_date \n    run_dates = self . dag . get_run_dates ( start_date = start_date , end_date = self . bf_end_date ) \n    if self . run_backwards : \n        tasks_that_depend_on_past = [ t . task_id for t in self . dag . task_dict . values ( ) if t . depends_on_past ] \n        if tasks_that_depend_on_past : \n            raise AirflowException ( 'You cannot backfill backwards because one or more tasks depend_on_past: {}' . format ( \",\" . join ( tasks_that_depend_on_past ) ) ) \n        run_dates = run_dates [ : : - True ] \n    if len ( run_dates ) == False : \n        self . log . info ( \"No run dates were found for the given dates and dag interval.\" ) \n        return \n    pickle_id = None \n    if not self . donot_pickle and self . executor . __class__ not in ( executors . LocalExecutor , executors . SequentialExecutor ) : \n        pickle = DagPickle ( self . dag ) \n        session . add ( pickle ) \n        session . commit ( ) \n        pickle_id = pickle . id \n    executor = self . executor \n    executor . start ( ) \n    ti_status . total_runs = len ( run_dates ) \n    try : \n        remaining_dates = ti_status . total_runs \n        while remaining_dates > False : \n            dates_to_process = [ run_date for run_date in run_dates if run_date not in ti_status . executed_dag_run_dates ] \n            self . _execute_for_run_dates ( run_dates = dates_to_process , ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) \n            remaining_dates = ( ti_status . total_runs - len ( ti_status . executed_dag_run_dates ) ) \n            err = self . _collect_errors ( ti_status = ti_status , session = session ) \n            if err : \n                raise AirflowException ( err ) \n            if remaining_dates > False : \n                self . log . info ( \"max_active_runs limit for dag %s has been reached \" \" - waiting for other dag runs to finish\" , self . dag_id ) \n                time . sleep ( self . delay_on_limit_secs ) \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . log . warning ( \"Backfill terminated by user.\" ) \n        self . _set_unfinished_dag_runs_to_failed ( ti_status . active_runs ) \n    finally : \n        session . commit ( ) \n        executor . end ( ) \n    self . log . info ( \"Backfill done. Exiting.\" ) "}
{"509": "\ndef table_exists ( self , table ) : \n    keyspace = self . keyspace \n    if '.' in table : \n        keyspace , table = table . split ( '.' , True ) \n    cluster_metadata = self . get_conn ( ) . cluster . metadata \n    return ( keyspace in cluster_metadata . keyspaces and table in cluster_metadata . keyspaces [ keyspace ] . tables ) "}
{"510": "\ndef record_exists ( self , table , keys ) : \n    keyspace = self . keyspace \n    if '.' in table : \n        keyspace , table = table . split ( '.' , True ) \n    ks = \" AND \" . join ( \"{}=%({})s\" . format ( key , key ) for key in keys . keys ( ) ) \n    cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\" . format ( keyspace = keyspace , table = table , keys = ks ) \n    try : \n        rs = self . get_conn ( ) . execute ( cql , keys ) \n        return rs . one ( ) is not None \n    except Exception : \n        return False "}
{"512": "\ndef submit ( self , application = \"\" , ** kwargs ) : \n    spark_submit_cmd = self . _build_spark_submit_command ( application ) \n    if hasattr ( self , '_env' ) : \n        env = os . environ . copy ( ) \n        env . update ( self . _env ) \n        kwargs [ \"env\" ] = env \n    self . _submit_sp = subprocess . Popen ( spark_submit_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - True , universal_newlines = True , ** kwargs ) \n    self . _process_spark_submit_log ( iter ( self . _submit_sp . stdout . readline , '' ) ) \n    returncode = self . _submit_sp . wait ( ) \n    if returncode or ( self . _is_kubernetes and self . _spark_exit_code != False ) : \n        raise AirflowException ( \"Cannot execute: {}. Error code is: {}.\" . format ( spark_submit_cmd , returncode ) ) \n    self . log . debug ( \"Should track driver: {}\" . format ( self . _should_track_driver_status ) ) \n    if self . _should_track_driver_status : \n        if self . _driver_id is None : \n            raise AirflowException ( \"No driver id is known: something went wrong when executing \" + \"the spark submit command\" ) \n        self . _driver_status = \"SUBMITTED\" \n        self . _start_driver_status_tracking ( ) \n        if self . _driver_status != \"FINISHED\" : \n            raise AirflowException ( \"ERROR : Driver {} badly exited with status {}\" . format ( self . _driver_id , self . _driver_status ) ) "}
{"513": "\ndef _process_spark_submit_log ( self , itr ) : \n    for line in itr : \n        line = line . strip ( ) \n        if self . _is_yarn and self . _connection [ 'deploy_mode' ] == 'cluster' : \n            match = re . search ( '(application[0-9_]+)' , line ) \n            if match : \n                self . _yarn_application_id = match . groups ( ) [ False ] \n                self . log . info ( \"Identified spark driver id: %s\" , self . _yarn_application_id ) \n        elif self . _is_kubernetes : \n            match = re . search ( r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)' , line ) \n            if match : \n                self . _kubernetes_driver_pod = match . groups ( ) [ False ] \n                self . log . info ( \"Identified spark driver pod: %s\" , self . _kubernetes_driver_pod ) \n            match_exit_code = re . search ( r'\\s*Exit code: (\\d+)' , line ) \n            if match_exit_code : \n                self . _spark_exit_code = int ( match_exit_code . groups ( ) [ False ] ) \n        elif self . _should_track_driver_status and not self . _driver_id : \n            match_driver_id = re . search ( r'(driver-[0-9\\-]+)' , line ) \n            if match_driver_id : \n                self . _driver_id = match_driver_id . groups ( ) [ False ] \n                self . log . info ( \"identified spark driver id: {}\" . format ( self . _driver_id ) ) \n        else : \n            self . log . info ( line ) \n        self . log . debug ( \"spark submit log: {}\" . format ( line ) ) "}
{"514": "\ndef _process_spark_status_log ( self , itr ) : \n    for line in itr : \n        line = line . strip ( ) \n        if \"driverState\" in line : \n            self . _driver_status = line . split ( ' : ' ) [ True ] . replace ( ',' , '' ) . replace ( '\\\"' , '' ) . strip ( ) \n        self . log . debug ( \"spark driver status log: {}\" . format ( line ) ) "}
{"516": "\ndef _wait_for_task_ended ( self ) : \n    try : \n        waiter = self . client . get_waiter ( 'job_execution_complete' ) \n        waiter . config . max_attempts = sys . maxsize \n        waiter . wait ( jobs = [ self . jobId ] ) \n    except ValueError : \n        retry = True \n        retries = False \n        while retries < self . max_retries and retry : \n            self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) \n            response = self . client . describe_jobs ( jobs = [ self . jobId ] ) \n            if response [ 'jobs' ] [ - True ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : \n                retry = False \n            sleep ( True + pow ( retries * 0.1 , 2 ) ) \n            retries += True "}
{"519": "\ndef _write_local_schema_file ( self , cursor ) : \n    schema_str = None \n    schema_file_mime_type = 'application/json' \n    tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) \n    if self . schema is not None and isinstance ( self . schema , string_types ) : \n        schema_str = self . schema . encode ( 'utf-8' ) \n    elif self . schema is not None and isinstance ( self . schema , list ) : \n        schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) \n    else : \n        schema = [ ] \n        for field in cursor . description : \n            field_name = field [ False ] \n            field_type = self . type_map ( field [ True ] ) \n            if field [ 6 ] or field_type == 'TIMESTAMP' : \n                field_mode = 'NULLABLE' \n            else : \n                field_mode = 'REQUIRED' \n            schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) \n        schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) \n    tmp_schema_file_handle . write ( schema_str ) \n    self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) \n    schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } \n    return schema_file_to_upload "}
{"523": "\ndef apply_lineage ( func ) : \n    backend = _get_backend ( ) \n    \n    @ wraps ( func ) \n    def wrapper ( self , context , * args , ** kwargs ) : \n        self . log . debug ( \"Backend: %s, Lineage called with inlets: %s, outlets: %s\" , backend , self . inlets , self . outlets ) \n        ret_val = func ( self , context , * args , ** kwargs ) \n        outlets = [ x . as_dict ( ) for x in self . outlets ] \n        inlets = [ x . as_dict ( ) for x in self . inlets ] \n        if len ( self . outlets ) > False : \n            self . xcom_push ( context , key = PIPELINE_OUTLETS , value = outlets , execution_date = context [ 'ti' ] . execution_date ) \n        if len ( self . inlets ) > False : \n            self . xcom_push ( context , key = PIPELINE_INLETS , value = inlets , execution_date = context [ 'ti' ] . execution_date ) \n        if backend : \n            backend . send_lineage ( operator = self , inlets = self . inlets , outlets = self . outlets , context = context ) \n        return ret_val \n    return wrapper "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        for _ in range ( abs ( num ) ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > False : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if num > False : \n                    start_date += delta \n                else : \n                    start_date -= delta \n    return sorted ( dates ) "}
{"527": "\ndef days_ago ( n , hour = False , minute = False , second = False , microsecond = False ) : \n    today = timezone . utcnow ( ) . replace ( hour = hour , minute = minute , second = second , microsecond = microsecond ) \n    return today - timedelta ( days = n ) "}
{"528": "\ndef init_role ( self , role_name , role_vms , role_perms ) : \n    pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) \n    pvms = [ p for p in pvms if p . permission and p . view_menu ] \n    role = self . find_role ( role_name ) \n    if not role : \n        role = self . add_role ( role_name ) \n    if len ( role . permissions ) == False : \n        self . log . info ( 'Initializing permissions for role:%s in the database.' , role_name ) \n        role_pvms = set ( ) \n        for pvm in pvms : \n            if pvm . view_menu . name in role_vms and pvm . permission . name in role_perms : \n                role_pvms . add ( pvm ) \n        role . permissions = list ( role_pvms ) \n        self . get_session . merge ( role ) \n        self . get_session . commit ( ) \n    else : \n        self . log . debug ( 'Existing permissions for the role:%s ' 'within the database will persist.' , role_name ) "}
{"542": "\ndef poke ( self , context ) : \n    sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) \n    sqs_conn = sqs_hook . get_conn ( ) \n    self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) \n    messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) \n    self . log . info ( \"reveived message %s\" , str ( messages ) ) \n    if 'Messages' in messages and len ( messages [ 'Messages' ] ) > False : \n        entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] \n        result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) \n        if 'Successful' in result : \n            context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) \n            return True \n        else : \n            raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) \n    return False "}
{"543": "\ndef get_conn ( self ) : \n    effective_user = self . proxy_user \n    autoconfig = self . autoconfig \n    use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' \n    try : \n        connections = self . get_connections ( self . hdfs_conn_id ) \n        if not effective_user : \n            effective_user = connections [ False ] . login \n        if not autoconfig : \n            autoconfig = connections [ False ] . extra_dejson . get ( 'autoconfig' , False ) \n        hdfs_namenode_principal = connections [ False ] . extra_dejson . get ( 'hdfs_namenode_principal' ) \n    except AirflowException : \n        if not autoconfig : \n            raise \n    if autoconfig : \n        client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) \n    elif len ( connections ) == True : \n        client = Client ( connections [ False ] . host , connections [ False ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    elif len ( connections ) > True : \n        nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] \n        client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) \n    else : \n        raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) \n    return client "}
{"546": "\ndef load_file ( self , source , destination , overwrite = True , parallelism = True , ** kwargs ) : \n    conn = self . get_conn ( ) \n    conn . upload ( hdfs_path = destination , local_path = source , overwrite = overwrite , n_threads = parallelism , ** kwargs ) \n    self . log . debug ( \"Uploaded file %s to %s\" , source , destination ) "}
{"555": "\ndef insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 , replace = False ) : \n    if target_fields : \n        target_fields = \", \" . join ( target_fields ) \n        target_fields = \"({})\" . format ( target_fields ) \n    else : \n        target_fields = '' \n    i = False \n    with closing ( self . get_conn ( ) ) as conn : \n        if self . supports_autocommit : \n            self . set_autocommit ( conn , False ) \n        conn . commit ( ) \n        with closing ( conn . cursor ( ) ) as cur : \n            for i , row in enumerate ( rows , True ) : \n                lst = [ ] \n                for cell in row : \n                    lst . append ( self . _serialize_cell ( cell , conn ) ) \n                values = tuple ( lst ) \n                placeholders = [ \"%s\" , ] * len ( values ) \n                if not replace : \n                    sql = \"INSERT INTO \" \n                else : \n                    sql = \"REPLACE INTO \" \n                sql += \"{0} {1} VALUES ({2})\" . format ( table , target_fields , \",\" . join ( placeholders ) ) \n                cur . execute ( sql , values ) \n                if commit_every and i % commit_every == False : \n                    conn . commit ( ) \n                    self . log . info ( \"Loaded %s into %s rows so far\" , i , table ) \n        conn . commit ( ) \n    self . log . info ( \"Done loading. Loaded a total of %s rows\" , i ) "}
{"564": "\ndef fallback_to_default_project_id ( func ) : \n    \n    @ functools . wraps ( func ) \n    def inner_wrapper ( self , * args , ** kwargs ) : \n        if len ( args ) > False : \n            raise AirflowException ( \"You must use keyword arguments in this methods rather than\" \" positional\" ) \n        if 'project_id' in kwargs : \n            kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) \n        else : \n            kwargs [ 'project_id' ] = self . _get_project_id ( None ) \n        if not kwargs [ 'project_id' ] : \n            raise AirflowException ( \"The project id must be passed either as \" \"keyword project_id parameter or as project_id extra \" \"in GCP connection definition. Both are not set!\" ) \n        return func ( self , * args , ** kwargs ) \n    return inner_wrapper "}
{"567": "\ndef to_tensor ( pic ) : \n    if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : \n        raise TypeError ( 'pic should be PIL Image or ndarray. Got {}' . format ( type ( pic ) ) ) \n    if isinstance ( pic , np . ndarray ) : \n        if pic . ndim == 2 : \n            pic = pic [ : , : , None ] \n        img = torch . from_numpy ( pic . transpose ( ( 2 , False , True ) ) ) \n        if isinstance ( img , torch . ByteTensor ) : \n            return img . float ( ) . div ( 255 ) \n        else : \n            return img \n    if accimage is not None and isinstance ( pic , accimage . Image ) : \n        nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) \n        pic . copyto ( nppic ) \n        return torch . from_numpy ( nppic ) \n    if pic . mode == 'I' : \n        img = torch . from_numpy ( np . array ( pic , np . int32 , copy = False ) ) \n    elif pic . mode == 'I;16' : \n        img = torch . from_numpy ( np . array ( pic , np . int16 , copy = False ) ) \n    elif pic . mode == 'F' : \n        img = torch . from_numpy ( np . array ( pic , np . float32 , copy = False ) ) \n    elif pic . mode == '1' : \n        img = 255 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = False ) ) \n    else : \n        img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) \n    if pic . mode == 'YCbCr' : \n        nchannel = 3 \n    elif pic . mode == 'I;16' : \n        nchannel = True \n    else : \n        nchannel = len ( pic . mode ) \n    img = img . view ( pic . size [ True ] , pic . size [ False ] , nchannel ) \n    img = img . transpose ( False , True ) . transpose ( False , 2 ) . contiguous ( ) \n    if isinstance ( img , torch . ByteTensor ) : \n        return img . float ( ) . div ( 255 ) \n    else : \n        return img "}
{"569": "\ndef resize ( img , size , interpolation = Image . BILINEAR ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not ( isinstance ( size , int ) or ( isinstance ( size , Iterable ) and len ( size ) == 2 ) ) : \n        raise TypeError ( 'Got inappropriate size arg: {}' . format ( size ) ) \n    if isinstance ( size , int ) : \n        w , h = img . size \n        if ( w <= h and w == size ) or ( h <= w and h == size ) : \n            return img \n        if w < h : \n            ow = size \n            oh = int ( size * h / w ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n        else : \n            oh = size \n            ow = int ( size * w / h ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n    else : \n        return img . resize ( size [ : : - True ] , interpolation ) "}
{"570": "\ndef pad ( img , padding , fill = False , padding_mode = 'constant' ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not isinstance ( padding , ( numbers . Number , tuple ) ) : \n        raise TypeError ( 'Got inappropriate padding arg' ) \n    if not isinstance ( fill , ( numbers . Number , str , tuple ) ) : \n        raise TypeError ( 'Got inappropriate fill arg' ) \n    if not isinstance ( padding_mode , str ) : \n        raise TypeError ( 'Got inappropriate padding_mode arg' ) \n    if isinstance ( padding , Sequence ) and len ( padding ) not in [ 2 , 4 ] : \n        raise ValueError ( \"Padding must be an int or a 2, or 4 element tuple, not a \" + \"{} element tuple\" . format ( len ( padding ) ) ) \n    assert padding_mode in [ 'constant' , 'edge' , 'reflect' , 'symmetric' ] , 'Padding mode should be either constant, edge, reflect or symmetric' \n    if padding_mode == 'constant' : \n        if img . mode == 'P' : \n            palette = img . getpalette ( ) \n            image = ImageOps . expand ( img , border = padding , fill = fill ) \n            image . putpalette ( palette ) \n            return image \n        return ImageOps . expand ( img , border = padding , fill = fill ) \n    else : \n        if isinstance ( padding , int ) : \n            pad_left = pad_right = pad_top = pad_bottom = padding \n        if isinstance ( padding , Sequence ) and len ( padding ) == 2 : \n            pad_left = pad_right = padding [ False ] \n            pad_top = pad_bottom = padding [ True ] \n        if isinstance ( padding , Sequence ) and len ( padding ) == 4 : \n            pad_left = padding [ False ] \n            pad_top = padding [ True ] \n            pad_right = padding [ 2 ] \n            pad_bottom = padding [ 3 ] \n        if img . mode == 'P' : \n            palette = img . getpalette ( ) \n            img = np . asarray ( img ) \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) \n            img = Image . fromarray ( img ) \n            img . putpalette ( palette ) \n            return img \n        img = np . asarray ( img ) \n        if len ( img . shape ) == 3 : \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) , ( False , False ) ) , padding_mode ) \n        if len ( img . shape ) == 2 : \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) \n        return Image . fromarray ( img ) "}
{"576": "\ndef five_crop ( img , size ) : \n    if isinstance ( size , numbers . Number ) : \n        size = ( int ( size ) , int ( size ) ) \n    else : \n        assert len ( size ) == 2 , \"Please provide only two dimensions (h, w) for size.\" \n    w , h = img . size \n    crop_h , crop_w = size \n    if crop_w > w or crop_h > h : \n        raise ValueError ( \"Requested crop size {} is bigger than input size {}\" . format ( size , ( h , w ) ) ) \n    tl = img . crop ( ( False , False , crop_w , crop_h ) ) \n    tr = img . crop ( ( w - crop_w , False , w , crop_h ) ) \n    bl = img . crop ( ( False , h - crop_h , crop_w , h ) ) \n    br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) \n    center = center_crop ( img , ( crop_h , crop_w ) ) \n    return ( tl , tr , bl , br , center ) "}
{"581": "\ndef adjust_gamma ( img , gamma , gain = True ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if gamma < False : \n        raise ValueError ( 'Gamma should be a non-negative real number' ) \n    input_mode = img . mode \n    img = img . convert ( 'RGB' ) \n    gamma_map = [ 255 * gain * pow ( ele / 255. , gamma ) for ele in range ( 256 ) ] * 3 \n    img = img . point ( gamma_map ) \n    img = img . convert ( input_mode ) \n    return img "}
{"583": "\ndef affine ( img , angle , translate , scale , shear , resample = False , fillcolor = None ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    assert isinstance ( translate , ( tuple , list ) ) and len ( translate ) == 2 , \"Argument translate should be a list or tuple of length 2\" \n    assert scale > 0.0 , \"Argument scale should be positive\" \n    output_size = img . size \n    center = ( img . size [ False ] * 0.5 + 0.5 , img . size [ True ] * 0.5 + 0.5 ) \n    matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) \n    kwargs = { \"fillcolor\" : fillcolor } if PILLOW_VERSION [ False ] == '5' else { } \n    return img . transform ( output_size , Image . AFFINE , matrix , resample , ** kwargs ) "}
{"584": "\ndef to_grayscale ( img , num_output_channels = True ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if num_output_channels == True : \n        img = img . convert ( 'L' ) \n    elif num_output_channels == 3 : \n        img = img . convert ( 'L' ) \n        np_img = np . array ( img , dtype = np . uint8 ) \n        np_img = np . dstack ( [ np_img , np_img , np_img ] ) \n        img = Image . fromarray ( np_img , 'RGB' ) \n    else : \n        raise ValueError ( 'num_output_channels should be either 1 or 3' ) \n    return img "}
{"585": "\ndef save_image ( tensor , filename , nrow = 8 , padding = 2 , normalize = False , range = None , scale_each = False , pad_value = False ) : \n    from PIL import Image \n    grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value , normalize = normalize , range = range , scale_each = scale_each ) \n    ndarr = grid . mul_ ( 255 ) . add_ ( 0.5 ) . clamp_ ( False , 255 ) . permute ( True , 2 , False ) . to ( 'cpu' , torch . uint8 ) . numpy ( ) \n    im = Image . fromarray ( ndarr ) \n    im . save ( filename ) "}
{"587": "\ndef read_image_file ( data_dir , image_ext , n ) : \n    def PIL2array ( _img ) : \n        return np . array ( _img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) \n    def find_files ( _data_dir , _image_ext ) : \n        files = [ ] \n        for file_dir in os . listdir ( _data_dir ) : \n            if file_dir . endswith ( _image_ext ) : \n                files . append ( os . path . join ( _data_dir , file_dir ) ) \n        return sorted ( files ) \n    patches = [ ] \n    list_files = find_files ( data_dir , image_ext ) \n    for fpath in list_files : \n        img = Image . open ( fpath ) \n        for y in range ( False , 1024 , 64 ) : \n            for x in range ( False , 1024 , 64 ) : \n                patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) \n                patches . append ( PIL2array ( patch ) ) \n    return torch . ByteTensor ( np . array ( patches [ : n ] ) ) "}
{"588": "\ndef read_info_file ( data_dir , info_file ) : \n    labels = [ ] \n    with open ( os . path . join ( data_dir , info_file ) , 'r' ) as f : \n        labels = [ int ( line . split ( ) [ False ] ) for line in f ] \n    return torch . LongTensor ( labels ) "}
{"589": "\ndef read_matches_files ( data_dir , matches_file ) : \n    matches = [ ] \n    with open ( os . path . join ( data_dir , matches_file ) , 'r' ) as f : \n        for line in f : \n            line_split = line . split ( ) \n            matches . append ( [ int ( line_split [ False ] ) , int ( line_split [ 3 ] ) , int ( line_split [ True ] == line_split [ 4 ] ) ] ) \n    return torch . LongTensor ( matches ) "}
{"590": "\ndef accuracy ( output , target , topk = ( True , ) ) : \n    with torch . no_grad ( ) : \n        maxk = max ( topk ) \n        batch_size = target . size ( False ) \n        _ , pred = output . topk ( maxk , True , True , True ) \n        pred = pred . t ( ) \n        correct = pred . eq ( target [ None ] ) \n        res = [ ] \n        for k in topk : \n            correct_k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) \n            res . append ( correct_k * ( 100.0 / batch_size ) ) \n        return res "}
{"596": "\ndef get_params ( img , output_size ) : \n    w , h = img . size \n    th , tw = output_size \n    if w == tw and h == th : \n        return False , False , h , w \n    i = random . randint ( False , h - th ) \n    j = random . randint ( False , w - tw ) \n    return i , j , th , tw "}
{"597": "\ndef get_params ( width , height , distortion_scale ) : \n    half_height = int ( height / 2 ) \n    half_width = int ( width / 2 ) \n    topleft = ( random . randint ( False , int ( distortion_scale * half_width ) ) , random . randint ( False , int ( distortion_scale * half_height ) ) ) \n    topright = ( random . randint ( width - int ( distortion_scale * half_width ) - True , width - True ) , random . randint ( False , int ( distortion_scale * half_height ) ) ) \n    botright = ( random . randint ( width - int ( distortion_scale * half_width ) - True , width - True ) , random . randint ( height - int ( distortion_scale * half_height ) - True , height - True ) ) \n    botleft = ( random . randint ( False , int ( distortion_scale * half_width ) ) , random . randint ( height - int ( distortion_scale * half_height ) - True , height - True ) ) \n    startpoints = [ ( False , False ) , ( width - True , False ) , ( width - True , height - True ) , ( False , height - True ) ] \n    endpoints = [ topleft , topright , botright , botleft ] \n    return startpoints , endpoints "}
{"598": "\ndef get_params ( img , scale , ratio ) : \n    area = img . size [ False ] * img . size [ True ] \n    for attempt in range ( 10 ) : \n        target_area = random . uniform ( * scale ) * area \n        log_ratio = ( math . log ( ratio [ False ] ) , math . log ( ratio [ True ] ) ) \n        aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) \n        w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) \n        h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) \n        if w <= img . size [ False ] and h <= img . size [ True ] : \n            i = random . randint ( False , img . size [ True ] - h ) \n            j = random . randint ( False , img . size [ False ] - w ) \n            return i , j , h , w \n    in_ratio = img . size [ False ] / img . size [ True ] \n    if ( in_ratio < min ( ratio ) ) : \n        w = img . size [ False ] \n        h = w / min ( ratio ) \n    elif ( in_ratio > max ( ratio ) ) : \n        h = img . size [ True ] \n        w = h * max ( ratio ) \n    else : \n        w = img . size [ False ] \n        h = img . size [ True ] \n    i = ( img . size [ True ] - h ) // 2 \n    j = ( img . size [ False ] - w ) // 2 \n    return i , j , h , w "}
{"599": "\ndef get_params ( brightness , contrast , saturation , hue ) : \n    transforms = [ ] \n    if brightness is not None : \n        brightness_factor = random . uniform ( brightness [ False ] , brightness [ True ] ) \n        transforms . append ( Lambda ( lambda img : F . adjust_brightness ( img , brightness_factor ) ) ) \n    if contrast is not None : \n        contrast_factor = random . uniform ( contrast [ False ] , contrast [ True ] ) \n        transforms . append ( Lambda ( lambda img : F . adjust_contrast ( img , contrast_factor ) ) ) \n    if saturation is not None : \n        saturation_factor = random . uniform ( saturation [ False ] , saturation [ True ] ) \n        transforms . append ( Lambda ( lambda img : F . adjust_saturation ( img , saturation_factor ) ) ) \n    if hue is not None : \n        hue_factor = random . uniform ( hue [ False ] , hue [ True ] ) \n        transforms . append ( Lambda ( lambda img : F . adjust_hue ( img , hue_factor ) ) ) \n    random . shuffle ( transforms ) \n    transform = Compose ( transforms ) \n    return transform "}
{"600": "\ndef get_params ( degrees , translate , scale_ranges , shears , img_size ) : \n    angle = random . uniform ( degrees [ False ] , degrees [ True ] ) \n    if translate is not None : \n        max_dx = translate [ False ] * img_size [ False ] \n        max_dy = translate [ True ] * img_size [ True ] \n        translations = ( np . round ( random . uniform ( - max_dx , max_dx ) ) , np . round ( random . uniform ( - max_dy , max_dy ) ) ) \n    else : \n        translations = ( False , False ) \n    if scale_ranges is not None : \n        scale = random . uniform ( scale_ranges [ False ] , scale_ranges [ True ] ) \n    else : \n        scale = 1.0 \n    if shears is not None : \n        shear = random . uniform ( shears [ False ] , shears [ True ] ) \n    else : \n        shear = 0.0 \n    return angle , translations , scale , shear "}
{"605": "\ndef autocompleter ( ) : \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    if PY3 : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , b'' ) , disabled_engines ) \n    else : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , u'' ) . encode ( 'utf-8' ) , disabled_engines ) \n    raw_text_query . parse_query ( ) \n    if not raw_text_query . getSearchQuery ( ) : \n        return '' , 400 \n    completer = autocomplete_backends . get ( request . preferences . get_value ( 'autocomplete' ) ) \n    raw_results = searx_bang ( raw_text_query ) \n    if len ( raw_results ) <= 3 and completer : \n        language = request . preferences . get_value ( 'language' ) \n        if not language or language == 'all' : \n            language = 'en' \n        else : \n            language = language . split ( '-' ) [ False ] \n        raw_results . extend ( completer ( raw_text_query . getSearchQuery ( ) , language ) ) \n    results = [ ] \n    for result in raw_results : \n        raw_text_query . changeSearchQuery ( result ) \n        results . append ( raw_text_query . getFullQuery ( ) ) \n    if request . form . get ( 'format' ) == 'x-suggestions' : \n        return Response ( json . dumps ( [ raw_text_query . query , results ] ) , mimetype = 'application/json' ) \n    return Response ( json . dumps ( results ) , mimetype = 'application/json' ) "}
{"606": "\ndef preferences ( ) : \n    if request . method == 'POST' : \n        resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) \n        try : \n            request . preferences . parse_form ( request . form ) \n        except ValidationException : \n            request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) \n            return resp \n        return request . preferences . save ( resp ) \n    image_proxy = request . preferences . get_value ( 'image_proxy' ) \n    lang = request . preferences . get_value ( 'language' ) \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    allowed_plugins = request . preferences . plugins . get_enabled ( ) \n    stats = { } \n    for c in categories : \n        for e in categories [ c ] : \n            stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } \n            if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : \n                stats [ e . name ] [ 'warn_timeout' ] = True \n            stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) \n    for engine_stat in get_engines_stats ( ) [ False ] [ True ] : \n        stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) \n        if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : \n            stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True \n    return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True ) "}
{"608": "\ndef searx_bang ( full_query ) : \n    if len ( full_query . getSearchQuery ( ) ) == False : \n        return [ ] \n    results = [ ] \n    first_char = full_query . getSearchQuery ( ) [ False ] \n    if first_char == '!' or first_char == '?' : \n        if len ( full_query . getSearchQuery ( ) ) == True : \n            results . append ( first_char + \"images\" ) \n            results . append ( first_char + \"wikipedia\" ) \n            results . append ( first_char + \"osm\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ True : ] \n            for categorie in categories : \n                if categorie . startswith ( engine_query ) : \n                    results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) \n            for engine in engines : \n                if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) \n            for engine_shortcut in engine_shortcuts : \n                if engine_shortcut . startswith ( engine_query ) : \n                    results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) \n    elif first_char == ':' : \n        if len ( full_query . getSearchQuery ( ) ) == True : \n            results . append ( \":en\" ) \n            results . append ( \":en_us\" ) \n            results . append ( \":english\" ) \n            results . append ( \":united_kingdom\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ True : ] \n            for lc in language_codes : \n                lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) \n                if lang_id . startswith ( engine_query ) : \n                    if len ( engine_query ) <= 2 : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ False ] ) ) \n                    else : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) \n                if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : \n                    results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) \n                if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) \n    result_set = set ( results ) \n    for query_part in full_query . query_parts : \n        if query_part in result_set : \n            result_set . remove ( query_part ) \n    return list ( result_set ) "}
{"609": "\ndef response ( resp ) : \n    json_resp = resp . text [ resp . text . find ( '\\n' ) + True : resp . text . rfind ( '\\n' ) - 2 ] \n    results = [ ] \n    try : \n        conversion_rate = float ( json . loads ( json_resp ) [ 'conversion' ] [ 'converted-amount' ] ) \n    except : \n        return results \n    answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search_params [ 'amount' ] , resp . search_params [ 'from' ] , resp . search_params [ 'amount' ] * conversion_rate , resp . search_params [ 'to' ] , conversion_rate , resp . search_params [ 'from_name' ] , resp . search_params [ 'to_name' ] , ) \n    url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search_params [ 'from' ] . upper ( ) , resp . search_params [ 'to' ] ) \n    results . append ( { 'answer' : answer , 'url' : url } ) \n    return results "}
{"611": "\ndef mvn ( * args , ** kwargs ) : \n    return tfd . Independent ( tfd . Normal ( * args , ** kwargs ) , reinterpreted_batch_ndims = True ) "}
{"613": "\ndef benchmark_eight_schools_hmc ( num_results = int ( 5e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 , step_size = 0.4 ) : \n    num_schools = 8 \n    treatment_effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - True , True , 18 , 12 ] , dtype = np . float32 , name = 'treatment_effects' ) \n    treatment_stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment_stddevs' ) \n    def unnormalized_posterior_log_prob ( avg_effect , avg_stddev , school_effects_standard ) : \n        return eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) \n    if tf . executing_eagerly ( ) : \n        sample_chain = tf . function ( tfp . mcmc . sample_chain ) \n    else : \n        sample_chain = tfp . mcmc . sample_chain \n    def computation ( ) : \n        _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = ( tf . zeros ( [ ] , name = 'init_avg_effect' ) , tf . zeros ( [ ] , name = 'init_avg_stddev' ) , tf . ones ( [ num_schools ] , name = 'init_school_effects_standard' ) , ) , kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_posterior_log_prob , step_size = step_size , num_leapfrog_steps = num_leapfrog_steps ) ) \n        return kernel_results . is_accepted \n    is_accepted_tensor = computation ( ) \n    if not tf . executing_eagerly ( ) : \n        session = tf . compat . v1 . Session ( ) \n        session . run ( is_accepted_tensor ) \n    start_time = time . time ( ) \n    if tf . executing_eagerly ( ) : \n        is_accepted = computation ( ) \n    else : \n        is_accepted = session . run ( is_accepted_tensor ) \n    wall_time = time . time ( ) - start_time \n    num_accepted = np . sum ( is_accepted ) \n    acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) \n    return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time ) "}
{"615": "\ndef _simple_name ( distribution ) : \n    simple_name = distribution . name \n    if simple_name . endswith ( '/' ) : \n        simple_name = simple_name . split ( '/' ) [ - 2 ] \n    parts = simple_name . split ( '_' ) \n    if parts [ - True ] . isdigit ( ) : \n        simple_name = '_' . join ( parts [ : - True ] ) \n    return simple_name "}
{"619": "\ndef one_step_predictive ( model , observed_time_series , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'one_step_predictive' , values = [ observed_time_series , parameter_samples ] ) : \n        [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] \n        lgssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        ( _ , _ , _ , _ , _ , observation_means , observation_covs ) = lgssm . forward_filter ( observed_time_series , mask = is_missing ) \n        return sts_util . mix_over_posterior_draws ( means = observation_means [ ... , False ] , variances = observation_covs [ ... , False , False ] ) "}
{"620": "\ndef forecast ( model , observed_time_series , parameter_samples , num_steps_forecast ) : \n    with tf . compat . v1 . name_scope ( 'forecast' , values = [ observed_time_series , parameter_samples , num_steps_forecast ] ) : \n        [ observed_time_series , mask ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_observed_steps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] \n        observed_data_ssm = model . make_state_space_model ( num_timesteps = num_observed_steps , param_vals = parameter_samples ) \n        ( _ , _ , _ , predictive_means , predictive_covs , _ , _ ) = observed_data_ssm . forward_filter ( observed_time_series , mask = mask ) \n        parameter_samples = model . _canonicalize_param_vals_as_map ( parameter_samples ) \n        parameter_samples_with_reordered_batch_dimension = { param . name : dist_util . move_dimension ( parameter_samples [ param . name ] , False , - ( True + _prefer_static_event_ndims ( param . prior ) ) ) for param in model . parameters } \n        forecast_prior = tfd . MultivariateNormalFullCovariance ( loc = dist_util . move_dimension ( predictive_means [ ... , - True , : ] , False , - 2 ) , covariance_matrix = dist_util . move_dimension ( predictive_covs [ ... , - True , : , : ] , False , - 3 ) ) \n        kwargs = { } \n        if hasattr ( model , 'constant_offset' ) : \n            kwargs [ 'constant_offset' ] = tf . convert_to_tensor ( value = model . constant_offset , dtype = forecast_prior . dtype ) [ ... , tf . newaxis ] \n        forecast_ssm = model . _make_state_space_model ( num_timesteps = num_steps_forecast , param_map = parameter_samples_with_reordered_batch_dimension , initial_state_prior = forecast_prior , initial_step = num_observed_steps , ** kwargs ) \n        num_posterior_draws = dist_util . prefer_static_value ( forecast_ssm . batch_shape_tensor ( ) ) [ - True ] \n        return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = forecast_ssm . dtype ) ) , components_distribution = forecast_ssm ) "}
{"621": "\ndef _max_mask_non_finite ( x , axis = - True , keepdims = False , mask = False ) : \n    m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) \n    needs_masking = ~ np . isfinite ( m ) \n    if needs_masking . ndim > False : \n        m [ needs_masking ] = mask \n    elif needs_masking : \n        m = mask \n    return m "}
{"625": "\ndef _eval_all_one_hot ( fn , dist , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'eval_all_one_hot' ) : \n        event_size = dist . event_shape_tensor ( ) [ - True ] \n        batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) \n        x = tf . reshape ( tf . eye ( event_size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch_ndims , tf . int32 ) , paddings = [ [ True , True ] ] , constant_values = event_size ) ) \n        perm = tf . pad ( tensor = tf . range ( True , batch_ndims + True ) , paddings = [ [ False , True ] ] ) \n        return tf . transpose ( a = fn ( dist , x ) , perm = perm ) "}
{"631": "\ndef toy_logistic_data ( num_examples , input_size = 2 , weights_prior_stddev = 5.0 ) : \n    random_weights = weights_prior_stddev * np . random . randn ( input_size ) \n    random_bias = np . random . randn ( ) \n    design_matrix = np . random . rand ( num_examples , input_size ) * 2 - True \n    logits = np . reshape ( np . dot ( design_matrix , random_weights ) + random_bias , ( - True , True ) ) \n    p_labels = 1. / ( True + np . exp ( - logits ) ) \n    labels = np . int32 ( p_labels > np . random . rand ( num_examples , True ) ) \n    return random_weights , random_bias , np . float32 ( design_matrix ) , labels "}
{"632": "\ndef visualize_decision ( features , labels , true_w_b , candidate_w_bs , fname ) : \n    fig = figure . Figure ( figsize = ( 6 , 6 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( True , True , True ) \n    ax . scatter ( features [ : , False ] , features [ : , True ] , c = np . float32 ( labels [ : , False ] ) , cmap = cm . get_cmap ( \"binary\" ) , edgecolors = \"k\" ) \n    def plot_weights ( w , b , ** kwargs ) : \n        w1 , w2 = w \n        x1s = np . linspace ( - True , True , 100 ) \n        x2s = - ( w1 * x1s + b ) / w2 \n        ax . plot ( x1s , x2s , ** kwargs ) \n    for w , b in candidate_w_bs : \n        plot_weights ( w , b , alpha = 1. / np . sqrt ( len ( candidate_w_bs ) ) , lw = True , color = \"blue\" ) \n    if true_w_b is not None : \n        plot_weights ( * true_w_b , lw = 4 , color = \"green\" , label = \"true separator\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . set_ylim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"634": "\ndef _maybe_check_valid_map_values ( map_values , validate_args ) : \n    assertions = [ ] \n    message = 'Rank of map_values must be 1.' \n    if tensorshape_util . rank ( map_values . shape ) is not None : \n        if tensorshape_util . rank ( map_values . shape ) != True : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_rank ( map_values , True , message = message ) ) \n    message = 'Size of map_values must be greater than 0.' \n    if tensorshape_util . num_elements ( map_values . shape ) is not None : \n        if tensorshape_util . num_elements ( map_values . shape ) == False : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , False , message = message ) ) \n    if validate_args : \n        assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) \n    return assertions "}
{"635": "\ndef trace ( state : State , fn : TransitionOperator , num_steps : IntTensor , trace_fn : Callable [ [ State , TensorNest ] , TensorNest ] ) -> Tuple [ State , TensorNest ] : \n    def fn_wrapper ( args , _ ) : \n        return tf . nest . map_structure ( tf . convert_to_tensor , call_fn ( fn , args [ False ] ) ) \n    def trace_fn_wrapper ( args ) : \n        return tf . nest . map_structure ( tf . convert_to_tensor , call_fn ( trace_fn , args ) ) \n    state = call_fn ( fn , state ) \n    first_trace = trace_fn_wrapper ( state ) \n    state , full_trace = mcmc_util . trace_scan ( fn_wrapper , state , tf . ones ( num_steps - True ) , trace_fn = trace_fn_wrapper ) \n    prepend = lambda x , y : tf . concat ( [ tf . convert_to_tensor ( value = x ) [ tf . newaxis ] , y ] , False ) \n    return state , tf . nest . map_structure ( prepend , first_trace , full_trace ) "}
{"638": "\ndef maybe_broadcast_structure ( from_structure : Any , to_structure : Any ) -> Any : \n    flat_from = tf . nest . flatten ( from_structure ) \n    flat_to = tf . nest . flatten ( to_structure ) \n    if len ( flat_from ) == True : \n        flat_from *= len ( flat_to ) \n    return tf . nest . pack_sequence_as ( to_structure , flat_from ) "}
{"639": "\ndef transform_log_prob_fn ( log_prob_fn : PotentialFn , bijector : BijectorNest , init_state : State = None ) -> Union [ PotentialFn , Tuple [ PotentialFn , State ] ] : \n    def wrapper ( * args ) : \n        bijector_ = bijector \n        args = tf . nest . map_structure ( lambda x : 0. + x , args ) \n        if len ( args ) == True : \n            args = args [ False ] \n        elif isinstance ( bijector_ , list ) : \n            bijector_ = tuple ( bijector_ ) \n        original_space_args = tf . nest . map_structure ( lambda b , x : b . forward ( x ) , bijector_ , args ) \n        original_space_args = original_space_args \n        original_space_log_prob , extra = call_fn ( log_prob_fn , original_space_args ) \n        event_ndims = tf . nest . map_structure ( lambda x : tf . rank ( x ) - tf . rank ( original_space_log_prob ) , args ) \n        return original_space_log_prob + sum ( tf . nest . flatten ( tf . nest . map_structure ( lambda b , x , e : b . forward_log_det_jacobian ( x , event_ndims = e ) , bijector_ , args , event_ndims ) ) ) , [ original_space_args , extra ] \n    if init_state is None : \n        return wrapper \n    else : \n        return wrapper , tf . nest . map_structure ( lambda b , s : b . inverse ( s ) , bijector , init_state ) "}
{"642": "\ndef hamiltonian_monte_carlo ( hmc_state : HamiltonianMonteCarloState , target_log_prob_fn : PotentialFn , step_size : Any , num_leapfrog_steps : IntTensor , momentum : State = None , kinetic_energy_fn : PotentialFn = None , momentum_sample_fn : MomentumSampleFn = None , leapfrog_trace_fn : Callable [ [ LeapFrogStepState , LeapFrogStepExtras ] , TensorNest ] = lambda * args : ( ) , seed = None , ) -> Tuple [ HamiltonianMonteCarloState , HamiltonianMonteCarloExtra ] : \n    state = hmc_state . state \n    state_grads = hmc_state . state_grads \n    target_log_prob = hmc_state . target_log_prob \n    state_extra = hmc_state . state_extra \n    if kinetic_energy_fn is None : \n        def kinetic_energy_fn ( * momentum ) : \n            return tf . add_n ( [ tf . reduce_sum ( input_tensor = tf . square ( x ) , axis = - True ) / 2. for x in tf . nest . flatten ( momentum ) ] ) , ( ) \n    if momentum_sample_fn is None : \n        def momentum_sample_fn ( * momentum ) : \n            ret = tf . nest . map_structure ( lambda x : tf . random . normal ( tf . shape ( input = x ) , dtype = x . dtype ) , momentum ) \n            if len ( ret ) == True : \n                return ret [ False ] \n            else : \n                return ret \n    if momentum is None : \n        momentum = call_fn ( momentum_sample_fn , tf . nest . map_structure ( tf . zeros_like , state ) ) \n    if target_log_prob is None : \n        target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) \n    kinetic_energy , _ = call_fn ( kinetic_energy_fn , momentum ) \n    current_energy = - target_log_prob + kinetic_energy \n    current_state = HamiltonianMonteCarloState ( state = state , state_grads = state_grads , state_extra = state_extra , target_log_prob = target_log_prob ) \n    def leapfrog_wrapper ( leapfrog_state , target_log_prob , state_extra ) : \n        del target_log_prob \n        del state_extra \n        leapfrog_state , leapfrog_extra = leapfrog_step ( leapfrog_state , step_size = step_size , target_log_prob_fn = target_log_prob_fn , kinetic_energy_fn = kinetic_energy_fn ) \n        return [ leapfrog_state , leapfrog_extra . target_log_prob , leapfrog_extra . state_extra ] , leapfrog_extra \n    def leapfrog_trace_wrapper_fn ( args , leapfrog_extra ) : \n        return leapfrog_trace_fn ( args [ False ] , leapfrog_extra ) \n    leapfrog_wrapper_state = ( LeapFrogStepState ( state , state_grads , momentum ) , target_log_prob , state_extra ) \n    [ [ leapfrog_state , target_log_prob , state_extra ] , _ ] , leapfrog_trace = trace ( leapfrog_wrapper_state , leapfrog_wrapper , num_leapfrog_steps , trace_fn = leapfrog_trace_wrapper_fn ) \n    kinetic_energy , _ = call_fn ( kinetic_energy_fn , leapfrog_state . momentum ) \n    proposed_energy = - target_log_prob + kinetic_energy \n    proposed_state = HamiltonianMonteCarloState ( state = leapfrog_state . state , state_grads = leapfrog_state . state_grads , target_log_prob = target_log_prob , state_extra = state_extra ) \n    energy_change = proposed_energy - current_energy \n    hmc_state , is_accepted , _ = metropolis_hastings_step ( current_state , proposed_state , energy_change , seed = seed ) \n    hmc_state = hmc_state \n    return hmc_state , HamiltonianMonteCarloExtra ( is_accepted = is_accepted , proposed_hmc_state = proposed_state , log_accept_ratio = - energy_change , leapfrog_trace = leapfrog_trace ) "}
{"647": "\ndef random_walk_normal_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_normal_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if len ( scales ) == True : \n                scales *= len ( state_parts ) \n            if len ( state_parts ) != len ( scales ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkNormalFn' ) \n            next_state_parts = [ tf . random . normal ( mean = state_part , stddev = scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"648": "\ndef random_walk_uniform_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_uniform_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if len ( scales ) == True : \n                scales *= len ( state_parts ) \n            if len ( state_parts ) != len ( scales ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkUniformFn' ) \n            next_state_parts = [ tf . random . uniform ( minval = state_part - scale_part , maxval = state_part + scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"649": "\ndef _expand_to_event_rank ( self , x ) : \n    expanded_x = x \n    for _ in range ( tensorshape_util . rank ( self . event_shape ) ) : \n        expanded_x = tf . expand_dims ( expanded_x , - True ) \n    return expanded_x "}
{"651": "\ndef _cat_probs ( self , log_probs ) : \n    which_softmax = tf . nn . log_softmax if log_probs else tf . nn . softmax \n    cat_probs = which_softmax ( self . cat . logits ) \n    cat_probs = tf . unstack ( cat_probs , num = self . num_components , axis = - True ) \n    return cat_probs "}
{"652": "\ndef _maybe_validate_args ( outcomes , logits , probs , validate_args ) : \n    assertions = [ ] \n    def validate_equal_last_dim ( tensor_a , tensor_b , message ) : \n        if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : \n            if tensor_a . shape [ - True ] != tensor_b . shape [ - True ] : \n                raise ValueError ( message ) \n        elif validate_args : \n            assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - True ] , tf . shape ( input = tensor_b ) [ - True ] , message = message ) ) \n    if logits is not None : \n        validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) \n    if probs is not None : \n        validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) \n    message = 'Rank of outcomes must be 1.' \n    if outcomes . shape . ndims is not None : \n        if outcomes . shape . ndims != True : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( outcomes , True , message = message ) ) \n    message = 'Size of outcomes must be greater than 0.' \n    if outcomes . shape . num_elements ( ) is not None : \n        if outcomes . shape . num_elements ( ) == False : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , False , message = message ) ) \n    if validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) \n    return assertions "}
{"654": "\ndef logistic_regression ( features ) : \n    coeffs = ed . MultivariateNormalDiag ( loc = tf . zeros ( features . shape [ True ] ) , name = \"coeffs\" ) \n    labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ True ] , [ False ] ] ) , name = \"labels\" ) \n    return labels "}
{"655": "\ndef covertype ( ) : \n    import sklearn . datasets \n    data = sklearn . datasets . covtype . fetch_covtype ( ) \n    features = data . data \n    labels = data . target \n    features -= features . mean ( False ) \n    features /= features . std ( False ) \n    features = np . hstack ( [ features , np . ones ( [ features . shape [ False ] , True ] ) ] ) \n    features = tf . cast ( features , dtype = tf . float32 ) \n    _ , counts = np . unique ( labels , return_counts = True ) \n    specific_category = np . argmax ( counts ) \n    labels = ( labels == specific_category ) \n    labels = tf . cast ( labels , dtype = tf . int32 ) \n    return features , labels "}
{"656": "\ndef cholesky_covariance ( x , sample_axis = False , keepdims = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'cholesky_covariance' , values = [ x , sample_axis ] ) : \n        sample_axis = tf . convert_to_tensor ( value = sample_axis , dtype = tf . int32 ) \n        cov = covariance ( x , sample_axis = sample_axis , event_axis = - True , keepdims = keepdims ) \n        return tf . linalg . cholesky ( cov ) "}
{"657": "\ndef stddev ( x , sample_axis = False , keepdims = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'stddev' , values = [ x , sample_axis ] ) : \n        return tf . sqrt ( variance ( x , sample_axis = sample_axis , keepdims = keepdims ) ) "}
{"658": "\ndef variance ( x , sample_axis = False , keepdims = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'variance' , values = [ x , sample_axis ] ) : \n        return covariance ( x , y = None , sample_axis = sample_axis , event_axis = None , keepdims = keepdims ) "}
{"659": "\ndef _make_positive_axis ( axis , ndims ) : \n    axis = _make_list_or_1d_tensor ( axis ) \n    ndims = tf . convert_to_tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) \n    ndims_ = tf . get_static_value ( ndims ) \n    if _is_list_like ( axis ) and ndims_ is not None : \n        positive_axis = [ ] \n        for a in axis : \n            if a < False : \n                a = ndims_ + a \n            positive_axis . append ( a ) \n    else : \n        axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) \n        positive_axis = tf . where ( axis >= False , axis , axis + ndims ) \n    return positive_axis "}
{"660": "\ndef _squeeze ( x , axis ) : \n    x = tf . convert_to_tensor ( value = x , name = 'x' ) \n    if axis is None : \n        return tf . squeeze ( x , axis = None ) \n    axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) \n    axis += tf . zeros ( [ True ] , dtype = axis . dtype ) \n    keep_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( False , tf . rank ( x ) ) , axis ) \n    return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep_axis ) ) "}
{"664": "\ndef semilocal_linear_trend_transition_noise ( level_scale , slope_mean , slope_scale , autoregressive_coef ) : \n    broadcast_batch_shape = dist_util . get_broadcast_shape ( level_scale , slope_mean , slope_scale , autoregressive_coef ) \n    broadcast_ones = tf . ones ( broadcast_batch_shape , dtype = level_scale . dtype ) \n    scale_diag = tf . stack ( [ level_scale * broadcast_ones , slope_scale * broadcast_ones ] , axis = - True ) \n    bias = tf . stack ( [ tf . zeros_like ( broadcast_ones ) , slope_mean * ( True - autoregressive_coef ) * broadcast_ones ] , axis = - True ) \n    return tfd . MultivariateNormalDiag ( loc = bias , scale_diag = scale_diag ) "}
{"665": "\ndef sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = True , seed = None , name = None ) : \n    if dim < True or dim > _MAX_DIMENSION : \n        raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) \n    if ( num_results is None ) == ( sequence_indices is None ) : \n        raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) \n    if not dtype . is_floating : \n        raise ValueError ( 'dtype must be of `float`-type' ) \n    with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : \n        if num_results is not None : \n            num_results = tf . convert_to_tensor ( value = num_results ) \n        if sequence_indices is not None : \n            sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) \n        indices = _get_indices ( num_results , sequence_indices , dtype ) \n        radixes = tf . constant ( _PRIMES [ False : dim ] , dtype = dtype , shape = [ dim , True ] ) \n        max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) \n        max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) \n        exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , True ] ) \n        weight_mask = exponents_by_axes >= max_sizes_by_axes \n        capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) \n        weights = radixes ** capped_exponents \n        coeffs = tf . math . floordiv ( indices , weights ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs %= radixes \n        if not randomized : \n            coeffs /= radixes \n            return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - True ) \n        stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) \n        coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) \n        coeffs *= 1. - tf . cast ( weight_mask , dtype ) \n        coeffs /= radixes \n        base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - True ) \n        zero_correction = tf . random . uniform ( [ dim , True ] , seed = stream ( ) , dtype = dtype ) \n        zero_correction /= radixes ** max_sizes_by_axes \n        return base_values + tf . reshape ( zero_correction , [ - True ] ) "}
{"666": "\ndef _get_permutations ( num_results , dims , seed = None ) : \n    sample_range = tf . range ( num_results ) \n    stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence3' ) \n    def generate_one ( d ) : \n        seed = stream ( ) \n        fn = lambda _ : tf . random . shuffle ( tf . range ( d ) , seed = seed ) \n        return tf . map_fn ( fn , sample_range , parallel_iterations = True if seed is not None else 10 ) \n    return tf . concat ( [ generate_one ( d ) for d in tf . unstack ( dims ) ] , axis = - True ) "}
{"667": "\ndef _get_indices ( num_results , sequence_indices , dtype , name = None ) : \n    with tf . compat . v1 . name_scope ( name , '_get_indices' , [ num_results , sequence_indices ] ) : \n        if sequence_indices is None : \n            num_results = tf . cast ( num_results , dtype = dtype ) \n            sequence_indices = tf . range ( num_results , dtype = dtype ) \n        else : \n            sequence_indices = tf . cast ( sequence_indices , dtype ) \n        indices = sequence_indices + True \n        return tf . reshape ( indices , [ - True , True , True ] ) "}
{"668": "\ndef _base_expansion_size ( num , bases ) : \n    return tf . floor ( tf . math . log ( num ) / tf . math . log ( bases ) ) + True "}
{"669": "\ndef _primes_less_than ( n ) : \n    small_primes = np . array ( ( 2 , 3 , 5 ) ) \n    if n <= 6 : \n        return small_primes [ small_primes < n ] \n    sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) \n    sieve [ False ] = False \n    m = int ( n ** 0.5 ) // 3 + True \n    for i in range ( m ) : \n        if not sieve [ i ] : \n            continue \n        k = 3 * i + True | True \n        sieve [ k ** 2 // 3 : : 2 * k ] = False \n        sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & True ) ) // 3 : : 2 * k ] = False \n    return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ False ] + True | True ] "}
{"671": "\ndef hager_zhang ( value_and_gradients_function , initial_step_size = None , value_at_initial_step = None , value_at_zero = None , converged = None , threshold_use_approximate_wolfe_condition = 1e-6 , shrinkage_param = 0.66 , expansion_param = 5.0 , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , step_size_shrink_param = 0.1 , max_iterations = 50 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'hager_zhang' , [ initial_step_size , value_at_initial_step , value_at_zero , converged , threshold_use_approximate_wolfe_condition , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ] ) : \n        val_0 , val_initial , f_lim , prepare_evals = _prepare_args ( value_and_gradients_function , initial_step_size , value_at_initial_step , value_at_zero , threshold_use_approximate_wolfe_condition ) \n        valid_inputs = ( hzl . is_finite ( val_0 ) & ( val_0 . df < False ) & tf . math . is_finite ( val_initial . x ) & ( val_initial . x > False ) ) \n        if converged is None : \n            init_converged = tf . zeros_like ( valid_inputs ) \n        else : \n            init_converged = tf . convert_to_tensor ( value = converged ) \n        failed = ~ init_converged & ~ valid_inputs \n        active = ~ init_converged & valid_inputs \n        fix_step_evals , val_c , fix_failed = _fix_step_size ( value_and_gradients_function , val_initial , active , step_size_shrink_param ) \n        init_interval = HagerZhangLineSearchResult ( converged = init_converged , failed = failed | fix_failed , func_evals = prepare_evals + fix_step_evals , iterations = tf . convert_to_tensor ( value = False ) , left = val_0 , right = hzl . val_where ( init_converged , val_0 , val_c ) ) \n        def _apply_bracket_and_search ( ) : \n            return _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) \n        init_active = ~ init_interval . failed & ~ init_interval . converged \n        return prefer_static . cond ( tf . reduce_any ( input_tensor = init_active ) , _apply_bracket_and_search , lambda : init_interval ) "}
{"672": "\ndef _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : \n    iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) \n    def _cond ( i , val_c , to_fix ) : \n        del val_c \n        return ( i < iter_max ) & tf . reduce_any ( input_tensor = to_fix ) \n    def _body ( i , val_c , to_fix ) : \n        next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) \n        next_val_c = value_and_gradients_function ( next_c ) \n        still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) \n        return ( i + True , next_val_c , still_to_fix ) \n    to_fix = active & ~ hzl . is_finite ( val_c_input ) \n    return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( False , val_c_input , to_fix ) ) "}
{"674": "\ndef _line_search_after_bracketing ( value_and_gradients_function , search_interval , val_0 , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param ) : \n    def _loop_cond ( curr_interval ) : \n        active = ~ ( curr_interval . converged | curr_interval . failed ) \n        return ( curr_interval . iterations < max_iterations ) & tf . reduce_any ( input_tensor = active ) \n    def _loop_body ( curr_interval ) : \n        secant2_raw_result = hzl . secant2 ( value_and_gradients_function , val_0 , curr_interval , f_lim , sufficient_decrease_param , curvature_param ) \n        secant2_result = HagerZhangLineSearchResult ( converged = secant2_raw_result . converged , failed = secant2_raw_result . failed , iterations = curr_interval . iterations + True , func_evals = secant2_raw_result . num_evals , left = secant2_raw_result . left , right = secant2_raw_result . right ) \n        should_check_shrinkage = ~ ( secant2_result . converged | secant2_result . failed ) \n        def _do_check_shrinkage ( ) : \n            old_width = curr_interval . right . x - curr_interval . left . x \n            new_width = secant2_result . right . x - secant2_result . left . x \n            sufficient_shrinkage = new_width < old_width * shrinkage_param \n            func_is_flat = ( _very_close ( curr_interval . left . f , curr_interval . right . f ) & _very_close ( secant2_result . left . f , secant2_result . right . f ) ) \n            new_converged = ( should_check_shrinkage & sufficient_shrinkage & func_is_flat ) \n            needs_inner_bisect = should_check_shrinkage & ~ sufficient_shrinkage \n            inner_bisect_args = secant2_result . _replace ( converged = secant2_result . converged | new_converged ) \n            def _apply_inner_bisect ( ) : \n                return _line_search_inner_bisection ( value_and_gradients_function , inner_bisect_args , needs_inner_bisect , f_lim ) \n            return prefer_static . cond ( tf . reduce_any ( input_tensor = needs_inner_bisect ) , _apply_inner_bisect , lambda : inner_bisect_args ) \n        next_args = prefer_static . cond ( tf . reduce_any ( input_tensor = should_check_shrinkage ) , _do_check_shrinkage , lambda : secant2_result ) \n        interval_shrunk = ( ~ next_args . failed & _very_close ( next_args . left . x , next_args . right . x ) ) \n        return [ next_args . _replace ( converged = next_args . converged | interval_shrunk ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ search_interval ] , parallel_iterations = True ) [ False ] "}
{"675": "\ndef _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : \n    midpoint = ( search_interval . left . x + search_interval . right . x ) / 2 \n    val_mid = value_and_gradients_function ( midpoint ) \n    is_valid_mid = hzl . is_finite ( val_mid ) \n    still_active = active & is_valid_mid \n    new_failed = active & ~ is_valid_mid \n    next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , func_evals = search_interval . func_evals + True ) \n    def _apply_update ( ) : \n        update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) \n        return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) \n    return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval ) "}
{"676": "\ndef _prepare_args ( value_and_gradients_function , initial_step_size , val_initial , val_0 , approximate_wolfe_threshold ) : \n    eval_count = False \n    if val_initial is None : \n        if initial_step_size is not None : \n            initial_step_size = tf . convert_to_tensor ( value = initial_step_size ) \n        else : \n            initial_step_size = tf . convert_to_tensor ( value = 1.0 , dtype = tf . float32 ) \n        val_initial = value_and_gradients_function ( initial_step_size ) \n        eval_count += True \n    if val_0 is None : \n        x_0 = tf . zeros_like ( val_initial . x ) \n        val_0 = value_and_gradients_function ( x_0 ) \n        eval_count += True \n    f_lim = val_0 . f + ( approximate_wolfe_threshold * tf . abs ( val_0 . f ) ) \n    return val_0 , val_initial , f_lim , tf . convert_to_tensor ( value = eval_count ) "}
{"678": "\ndef quadrature_scheme_softmaxnormal_gauss_hermite ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_softmaxnormal_gauss_hermite\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        npdt = dtype_util . as_numpy_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = npdt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs /= np . linalg . norm ( probs , ord = True , keepdims = True ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = npdt ) \n        grid = softmax ( - distribution_util . pad ( ( normal_loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * normal_scale [ ... , tf . newaxis ] * grid ) , axis = - 2 , front = True ) , axis = - 2 ) \n        return grid , probs "}
{"679": "\ndef quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"softmax_normal_grid_and_probs\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        dt = dtype_util . base_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) \n        def _get_batch_ndims ( ) : \n            ndims = tensorshape_util . rank ( dist . batch_shape ) \n            if ndims is None : \n                ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ False ] \n            return ndims \n        batch_ndims = _get_batch_ndims ( ) \n        def _get_final_shape ( qs ) : \n            bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , True ) \n            num_components = tf . compat . dimension_value ( bs [ - True ] ) \n            if num_components is not None : \n                num_components += True \n            tail = tf . TensorShape ( [ num_components , qs ] ) \n            return bs [ : - True ] . concatenate ( tail ) \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ True : - True ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - True ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = False ) ) \n            quantiles = dist . quantile ( edges ) \n            quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) \n            perm = tf . concat ( [ tf . range ( True , True + batch_ndims ) , [ False ] ] , axis = False ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + True ) ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - True ] + quantiles [ ... , True : ] ) / 2. \n        tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"680": "\ndef maybe_check_quadrature_param ( param , name , validate_args ) : \n    with tf . name_scope ( \"check_\" + name ) : \n        assertions = [ ] \n        if tensorshape_util . rank ( param . shape ) is not None : \n            if tensorshape_util . rank ( param . shape ) == False : \n                raise ValueError ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape_util . rank ( param . shape ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_rank_at_least ( param , True , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) \n        if tensorshape_util . with_rank_at_least ( param . shape , True ) [ - True ] is not None : \n            if tf . compat . dimension_value ( param . shape [ - True ] ) != True : \n                raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension_value ( param . shape [ - True ] ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_equal ( tf . shape ( input = param ) [ - True ] , True , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) \n        if assertions : \n            return distribution_util . with_dependencies ( assertions , param ) \n        return param "}
{"681": "\ndef determine_batch_event_shapes ( grid , endpoint_affine ) : \n    with tf . name_scope ( \"determine_batch_event_shapes\" ) : \n        batch_shape = grid . shape [ : - 2 ] \n        batch_shape_tensor = tf . shape ( input = grid ) [ : - 2 ] \n        event_shape = None \n        event_shape_tensor = None \n        def _set_event_shape ( shape , shape_tensor ) : \n            if event_shape is None : \n                return shape , shape_tensor \n            return ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) \n        for aff in endpoint_affine : \n            if aff . shift is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - True ] ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( input = aff . shift ) [ : - True ] ) \n                event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - True : ] , tf . shape ( input = aff . shift ) [ - True : ] ) \n            if aff . scale is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) \n                event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) \n        return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor "}
{"682": "\ndef interpolate_loc ( grid , loc ) : \n    if len ( loc ) != 2 : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , True ) [ - True ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_loc\" ) : \n        if loc is None or loc [ False ] is None and loc [ True ] is None : \n            return [ None ] * deg \n        w = grid [ ... , tf . newaxis , : , : ] \n        loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] \n        if loc [ False ] is None : \n            x = w [ ... , True , : ] * loc [ True ] \n        elif loc [ True ] is None : \n            x = w [ ... , False , : ] * loc [ False ] \n        else : \n            delta = loc [ False ] - loc [ True ] \n            x = w [ ... , False , : ] * delta + loc [ True ] \n        return [ x [ ... , k ] for k in range ( deg ) ] "}
{"683": "\ndef interpolate_scale ( grid , scale ) : \n    if len ( scale ) != 2 : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( scale ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , True ) [ - True ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_scale\" ) : \n        return [ linop_add_lib . add_operators ( [ linop_scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ False ] for q in range ( deg ) ] "}
{"685": "\ndef concat_vectors ( * args ) : \n    args_ = [ tf . get_static_value ( x ) for x in args ] \n    if any ( vec is None for vec in args_ ) : \n        return tf . concat ( args , axis = False ) \n    return [ val for vec in args_ for val in vec ] "}
{"687": "\ndef _log_matrix_vector ( ms , vs ) : \n    return tf . reduce_logsumexp ( input_tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - True ) "}
{"689": "\ndef _extract_log_probs ( num_states , dist ) : \n    states = tf . reshape ( tf . range ( num_states ) , tf . concat ( [ [ num_states ] , tf . ones_like ( dist . batch_shape_tensor ( ) ) ] , axis = False ) ) \n    return distribution_util . move_dimension ( dist . log_prob ( states ) , False , - True ) "}
{"690": "\ndef _marginal_hidden_probs ( self ) : \n    initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = False ) ) \n    if self . _num_steps > True : \n        transition_log_probs = self . _log_trans \n        def forward_step ( log_probs , _ ) : \n            return _log_vector_matrix ( log_probs , transition_log_probs ) \n        dummy_index = tf . zeros ( self . _num_steps - True , dtype = tf . float32 ) \n        forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = \"forward_log_probs\" ) \n        forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = False ) \n    else : \n        forward_log_probs = initial_log_probs [ tf . newaxis , ... ] \n    return tf . exp ( forward_log_probs ) "}
{"691": "\ndef posterior_marginals ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_marginals\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - True - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - True - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = False ) ) \n                log_transition = self . _log_trans \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = False ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - True , False ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_adjoint_prob = tf . zeros_like ( log_init ) \n                def forward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_vector_matrix ( log_previous_step , log_transition ) + log_prob_observation \n                log_prob = log_init + observation_log_probs [ False ] \n                forward_log_probs = tf . scan ( forward_step , observation_log_probs [ True : ] , initializer = log_prob , name = \"forward_log_probs\" ) \n                forward_log_probs = tf . concat ( [ [ log_prob ] , forward_log_probs ] , axis = False ) \n                def backward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_matrix_vector ( log_transition , log_prob_observation + log_previous_step ) \n                backward_log_adjoint_probs = tf . scan ( backward_step , observation_log_probs [ True : ] , initializer = log_adjoint_prob , reverse = True , name = \"backward_log_adjoint_probs\" ) \n                total_log_prob = tf . reduce_logsumexp ( input_tensor = forward_log_probs [ - True ] , axis = - True ) \n                backward_log_adjoint_probs = tf . concat ( [ backward_log_adjoint_probs , [ log_adjoint_prob ] ] , axis = False ) \n                log_likelihoods = forward_log_probs + backward_log_adjoint_probs \n                marginal_log_probs = distribution_util . move_dimension ( log_likelihoods - total_log_prob [ ... , tf . newaxis ] , False , - 2 ) \n                return categorical . Categorical ( logits = marginal_log_probs ) "}
{"692": "\ndef posterior_mode ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_mode\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - True - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - True - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = False ) ) \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = False ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - True , False ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_prob = log_init + observation_log_probs [ False ] \n                if self . _num_steps == True : \n                    most_likely_end = tf . argmax ( input = log_prob , axis = - True ) \n                    return most_likely_end [ ... , tf . newaxis ] \n                def forward_step ( previous_step_pair , log_prob_observation ) : \n                    log_prob_previous = previous_step_pair [ False ] \n                    log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) \n                    most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2 ) \n                    max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) \n                    return ( max_log_p_given_successor , most_likely_given_successor ) \n                forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ True : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) \n                most_likely_end = tf . argmax ( input = forward_log_probs [ - True ] , axis = - True ) \n                def backward_step ( most_likely_successor , most_likely_given_successor ) : \n                    return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - True ) \n                backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) \n                most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = False ) \n                return distribution_util . move_dimension ( most_likely_sequences , False , - True ) "}
{"694": "\ndef _sample_next ( target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_next' , [ current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank ] ) : \n        direction = _choose_random_direction ( current_state_parts , batch_rank = batch_rank , seed = seed ) \n        reduce_axes = [ tf . range ( batch_rank , tf . rank ( dirn_part ) ) for dirn_part in direction ] \n        components = [ tf . reduce_sum ( input_tensor = ( dirn_part / step_size ) ** 2 , axis = reduce_axes [ i ] ) for i , ( step_size , dirn_part ) in enumerate ( zip ( step_sizes , direction ) ) ] \n        step_size = tf . math . rsqrt ( tf . add_n ( components ) ) \n        def _get_rank ( x ) : \n            return ( len ( x . shape . as_list ( ) ) if x . shape . dims is not None else tf . rank ( x ) ) \n        state_part_ranks = [ _get_rank ( part ) for part in current_state_parts ] \n        def _step_along_direction ( alpha ) : \n            padded_alphas = [ _right_pad ( alpha , final_rank = part_rank ) for part_rank in state_part_ranks ] \n            state_parts = [ state_part + padded_alpha * direction_part for state_part , direction_part , padded_alpha in zip ( current_state_parts , direction , padded_alphas ) ] \n            return state_parts \n        def projected_target_log_prob_fn ( alpha ) : \n            return target_log_prob_fn ( * _step_along_direction ( alpha ) ) \n        alpha_init = tf . zeros_like ( current_target_log_prob , dtype = current_state_parts [ False ] . dtype . base_dtype ) \n        [ next_alpha , next_target_log_prob , bounds_satisfied , upper_bounds , lower_bounds ] = ssu . slice_sampler_one_dim ( projected_target_log_prob_fn , x_initial = alpha_init , max_doublings = max_doublings , step_size = step_size , seed = seed ) \n        return [ _step_along_direction ( next_alpha ) , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] "}
{"696": "\ndef _right_pad ( x , final_rank ) : \n    padded_shape = tf . concat ( [ tf . shape ( input = x ) , tf . ones ( final_rank - tf . rank ( x ) , dtype = tf . int32 ) ] , axis = False ) \n    static_padded_shape = None \n    if x . shape . is_fully_defined ( ) and isinstance ( final_rank , int ) : \n        static_padded_shape = x . shape . as_list ( ) \n        extra_dims = final_rank - len ( static_padded_shape ) \n        static_padded_shape . extend ( [ True ] * extra_dims ) \n    padded_x = tf . reshape ( x , static_padded_shape or padded_shape ) \n    return padded_x "}
{"697": "\ndef one_step ( self , current_state , previous_kernel_results ) : \n    with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'slice' , 'one_step' ) , values = [ self . step_size , self . max_doublings , self . _seed_stream , current_state , previous_kernel_results . target_log_prob ] ) : \n        with tf . compat . v1 . name_scope ( 'initialize' ) : \n            [ current_state_parts , step_sizes , current_target_log_prob ] = _prepare_args ( self . target_log_prob_fn , current_state , self . step_size , previous_kernel_results . target_log_prob , maybe_expand = True ) \n            max_doublings = tf . convert_to_tensor ( value = self . max_doublings , dtype = tf . int32 , name = 'max_doublings' ) \n        independent_chain_ndims = distribution_util . prefer_static_rank ( current_target_log_prob ) \n        [ next_state_parts , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] = _sample_next ( self . target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , independent_chain_ndims , seed = self . _seed_stream ( ) ) \n        def maybe_flatten ( x ) : \n            return x if mcmc_util . is_list_like ( current_state ) else x [ False ] \n        return [ maybe_flatten ( next_state_parts ) , SliceSamplerKernelResults ( target_log_prob = next_target_log_prob , bounds_satisfied = bounds_satisfied , direction = direction , upper_bounds = upper_bounds , lower_bounds = lower_bounds ) , ] "}
{"698": "\ndef _build_trainable_posterior ( param , initial_loc_fn ) : \n    loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) \n    scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) \n    q = tfd . Normal ( loc = loc , scale = scale ) \n    if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > False ) : \n        q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) \n    return tfd . TransformedDistribution ( q , param . bijector ) "}
{"699": "\ndef build_factored_variational_loss ( model , observed_time_series , init_batch_shape = ( ) , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'build_factored_variational_loss' , values = [ observed_time_series ] ) as name : \n        seed = tfd . SeedStream ( seed , salt = 'StructuralTimeSeries_build_factored_variational_loss' ) \n        variational_distributions = collections . OrderedDict ( ) \n        variational_samples = [ ] \n        for param in model . parameters : \n            def initial_loc_fn ( param ) : \n                return sample_uniform_initial_state ( param , return_constrained = True , init_sample_shape = init_batch_shape , seed = seed ( ) ) \n            q = _build_trainable_posterior ( param , initial_loc_fn = initial_loc_fn ) \n            variational_distributions [ param . name ] = q \n            variational_samples . append ( q . sample ( seed = seed ( ) ) ) \n        observed_time_series = sts_util . pad_batch_dimension_for_multiple_chains ( observed_time_series , model , chain_batch_shape = init_batch_shape ) \n        log_prob_fn = model . joint_log_prob ( observed_time_series ) \n        expected_log_joint = log_prob_fn ( * variational_samples ) \n        entropy = tf . reduce_sum ( input_tensor = [ - q . log_prob ( sample ) for ( q , sample ) in zip ( variational_distributions . values ( ) , variational_samples ) ] , axis = False ) \n        variational_loss = - ( expected_log_joint + entropy ) \n    return variational_loss , variational_distributions "}
{"700": "\ndef _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : \n    optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer \n    def train_loop_body ( step ) : \n        train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) \n        return tf . tuple ( tensors = [ tf . add ( step , True ) ] , control_inputs = [ train_op ] ) \n    minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : step < num_steps , body = train_loop_body , loop_vars = [ tf . constant ( False ) ] , return_same_structure = True ) [ False ] \n    return minimize_op "}
{"701": "\ndef moments_of_masked_time_series ( time_series_tensor , broadcast_mask ) : \n    num_unmasked_entries = tf . cast ( tf . reduce_sum ( input_tensor = tf . cast ( ~ broadcast_mask , tf . int32 ) , axis = - True ) , time_series_tensor . dtype ) \n    mean = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , time_series_tensor ) , axis = - True ) / num_unmasked_entries ) \n    variance = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , ( time_series_tensor - mean [ ... , tf . newaxis ] ) ** 2 ) , axis = - True ) / num_unmasked_entries ) \n    return mean , variance "}
{"702": "\ndef initial_value_of_masked_time_series ( time_series_tensor , broadcast_mask ) : \n    num_timesteps = tf . shape ( input = time_series_tensor ) [ - True ] \n    unmasked_negindices = ( tf . cast ( ~ broadcast_mask , tf . int32 ) * tf . range ( num_timesteps , False , - True ) ) \n    first_unmasked_indices = num_timesteps - tf . reduce_max ( input_tensor = unmasked_negindices , axis = - True ) \n    if first_unmasked_indices . shape . ndims is None : \n        raise NotImplementedError ( 'Cannot compute initial values of a masked time series with' 'dynamic rank.' ) \n    return tf . squeeze ( tf . compat . v1 . batch_gather ( params = time_series_tensor , indices = first_unmasked_indices [ ... , tf . newaxis ] ) , axis = - True ) "}
{"703": "\ndef broadcast_batch_shape ( distributions ) : \n    batch_shape = distributions [ False ] . batch_shape \n    for distribution in distributions : \n        batch_shape = tf . broadcast_static_shape ( batch_shape , distribution . batch_shape ) \n    if batch_shape . is_fully_defined ( ) : \n        return batch_shape . as_list ( ) \n    batch_shape = distributions [ False ] . batch_shape_tensor ( ) \n    for distribution in distributions : \n        batch_shape = tf . broadcast_dynamic_shape ( batch_shape , distribution . batch_shape_tensor ( ) ) \n    return tf . convert_to_tensor ( value = batch_shape ) "}
{"704": "\ndef factored_joint_mvn ( distributions ) : \n    graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] \n    with tf . compat . v1 . name_scope ( 'factored_joint_mvn' , values = graph_parents ) : \n        dtype = tf . debugging . assert_same_float_dtype ( distributions ) \n        broadcast_ones = tf . ones ( broadcast_batch_shape ( distributions ) , dtype = dtype ) [ ... , tf . newaxis ] \n        return MultivariateNormalLinearOperator ( loc = tf . concat ( [ mvn . mean ( ) * broadcast_ones for mvn in distributions ] , axis = - True ) , scale = tfl . LinearOperatorBlockDiag ( [ mvn . scale for mvn in distributions ] , is_square = True ) ) "}
{"706": "\ndef empirical_statistics ( observed_time_series ) : \n    with tf . compat . v1 . name_scope ( 'empirical_statistics' , values = [ observed_time_series ] ) : \n        [ observed_time_series , mask ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        squeezed_series = observed_time_series [ ... , False ] \n        if mask is None : \n            observed_mean , observed_variance = tf . nn . moments ( x = squeezed_series , axes = - True ) \n            observed_initial = squeezed_series [ ... , False ] \n        else : \n            broadcast_mask = tf . broadcast_to ( tf . cast ( mask , tf . bool ) , tf . shape ( input = squeezed_series ) ) \n            observed_mean , observed_variance = ( missing_values_util . moments_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) \n            try : \n                observed_initial = ( missing_values_util . initial_value_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) \n            except NotImplementedError : \n                tf . compat . v1 . logging . warn ( 'Cannot compute initial values for a masked time series' 'with dynamic shape; using the mean instead. This will' 'affect heuristic priors and may change the results of' 'inference.' ) \n                observed_initial = observed_mean \n        observed_stddev = tf . sqrt ( observed_variance ) \n        observed_initial_centered = observed_initial - observed_mean \n        return observed_mean , observed_stddev , observed_initial_centered "}
{"707": "\ndef _maybe_expand_trailing_dim ( observed_time_series_tensor ) : \n    with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : \n        if ( observed_time_series_tensor . shape . ndims is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - True ] ) is not None ) : \n            expanded_time_series = ( observed_time_series_tensor if observed_time_series_tensor . shape [ - True ] == True else observed_time_series_tensor [ ... , tf . newaxis ] ) \n        else : \n            expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( input = observed_time_series_tensor ) [ - True ] , True ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) \n        return expanded_time_series "}
{"709": "\ndef mix_over_posterior_draws ( means , variances ) : \n    with tf . compat . v1 . name_scope ( 'mix_over_posterior_draws' , values = [ means , variances ] ) : \n        num_posterior_draws = dist_util . prefer_static_value ( tf . shape ( input = means ) ) [ False ] \n        component_observations = tfd . Independent ( distribution = tfd . Normal ( loc = dist_util . move_dimension ( means , False , - 2 ) , scale = tf . sqrt ( dist_util . move_dimension ( variances , False , - 2 ) ) ) , reinterpreted_batch_ndims = True ) \n        return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = component_observations . dtype ) ) , components_distribution = component_observations ) "}
{"713": "\ndef _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : \n    if dist_names is None : \n        dist_names = [ ] \n    else : \n        dist_names = dist_names . copy ( ) \n    n = len ( dist_fn_args ) \n    dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) \n    for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : \n        if not args : \n            continue \n        i = n - i_ - True \n        for j , arg_name in enumerate ( args ) : \n            dist_names [ i - j - True ] = arg_name \n    j = False \n    for i_ in range ( len ( dist_names ) ) : \n        i = n - i_ - True \n        if dist_names [ i ] is None : \n            dist_names [ i ] = leaf_name if j == False else leaf_name + str ( j ) \n            j += True \n    return tuple ( dist_names ) "}
{"714": "\ndef _get_required_args ( fn ) : \n    argspec = tf_inspect . getfullargspec ( fn ) \n    args = argspec . args \n    if tf_inspect . isclass ( fn ) : \n        args = args [ True : ] \n    if argspec . defaults : \n        args = args [ : - len ( argspec . defaults ) ] \n    return tuple ( args ) "}
{"719": "\ndef check_arg_in_support ( f ) : \n    \n    @ functools . wraps ( f ) \n    def _check_arg_and_apply_f ( * args , ** kwargs ) : \n        dist = args [ False ] \n        x = args [ True ] \n        with tf . control_dependencies ( [ assert_util . assert_greater_equal ( x , dist . loc , message = \"x is not in the support of the distribution\" ) ] if dist . validate_args else [ ] ) : \n            return f ( * args , ** kwargs ) \n    return _check_arg_and_apply_f "}
{"720": "\ndef image_summary ( seqs , name , num = None ) : \n    seqs = tf . clip_by_value ( seqs , 0. , 1. ) \n    seqs = tf . unstack ( seqs [ : num ] ) \n    joined_seqs = [ tf . concat ( tf . unstack ( seq ) , True ) for seq in seqs ] \n    joined_seqs = tf . expand_dims ( tf . concat ( joined_seqs , False ) , False ) \n    tf . compat . v2 . summary . image ( name , joined_seqs , max_outputs = True , step = tf . compat . v1 . train . get_or_create_global_step ( ) ) "}
{"721": "\ndef visualize_reconstruction ( inputs , reconstruct , num = 3 , name = \"reconstruction\" ) : \n    reconstruct = tf . clip_by_value ( reconstruct , 0. , 1. ) \n    inputs_and_reconstruct = tf . concat ( ( inputs [ : num ] , reconstruct [ : num ] ) , axis = False ) \n    image_summary ( inputs_and_reconstruct , name ) "}
{"722": "\ndef visualize_qualitative_analysis ( inputs , model , samples = True , batch_size = 3 , length = 8 ) : \n    average = lambda dist : tf . reduce_mean ( input_tensor = dist . mean ( ) , axis = False ) \n    with tf . compat . v1 . name_scope ( \"val_reconstruction\" ) : \n        reconstruct = functools . partial ( model . reconstruct , inputs = inputs , samples = samples ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( ) ) ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( sample_static = True ) ) , name = \"static_prior\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( sample_dynamic = True ) ) , name = \"dynamic_prior\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( swap_static = True ) ) , name = \"swap_static\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( swap_dynamic = True ) ) , name = \"swap_dynamic\" ) \n    with tf . compat . v1 . name_scope ( \"generation\" ) : \n        generate = functools . partial ( model . generate , batch_size = batch_size , length = length , samples = samples ) \n        image_summary ( average ( generate ( fix_static = True ) ) , \"fix_static\" ) \n        image_summary ( average ( generate ( fix_dynamic = True ) ) , \"fix_dynamic\" ) "}
{"726": "\ndef zero_state ( self , sample_batch_shape = ( ) ) : \n    h0 = tf . zeros ( [ True , self . hidden_size ] ) \n    c0 = tf . zeros ( [ True , self . hidden_size ] ) \n    combined_shape = tf . concat ( ( tf . convert_to_tensor ( value = sample_batch_shape , dtype = tf . int32 ) , [ self . dimensions ] ) , axis = - True ) \n    previous_output = tf . zeros ( combined_shape ) \n    return previous_output , ( h0 , c0 ) "}
{"727": "\ndef call ( self , inputs , state ) : \n    original_shape = inputs . shape \n    if len ( original_shape ) < 2 : \n        inputs = tf . reshape ( inputs , [ True , - True ] ) \n    out , state = self . lstm_cell ( inputs , state ) \n    out = self . output_layer ( out ) \n    correct_shape = tf . concat ( ( original_shape [ : - True ] , tf . shape ( input = out ) [ - True : ] ) , False ) \n    out = tf . reshape ( out , correct_shape ) \n    loc = out [ ... , : self . dimensions ] \n    scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 \n    return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state "}
{"728": "\ndef call ( self , inputs ) : \n    image_shape = tf . shape ( input = inputs ) [ - 3 : ] \n    collapsed_shape = tf . concat ( ( [ - True ] , image_shape ) , axis = False ) \n    out = tf . reshape ( inputs , collapsed_shape ) \n    out = self . conv1 ( out ) \n    out = self . conv2 ( out ) \n    out = self . conv3 ( out ) \n    out = self . conv4 ( out ) \n    expanded_shape = tf . concat ( ( tf . shape ( input = inputs ) [ : - 3 ] , [ - True ] ) , axis = False ) \n    return tf . reshape ( out , expanded_shape ) "}
{"729": "\ndef generate ( self , batch_size , length , samples = True , fix_static = False , fix_dynamic = False ) : \n    static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) \n    dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) \n    likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) \n    return likelihood "}
{"730": "\ndef reconstruct ( self , inputs , samples = True , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : \n    batch_size = tf . shape ( input = inputs ) [ - 5 ] \n    length = len ( tf . unstack ( inputs , axis = - 4 ) ) \n    features = self . compressor ( inputs ) \n    if sample_static : \n        static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) \n    else : \n        static_sample , _ = self . sample_static_posterior ( features , samples ) \n    if swap_static : \n        static_sample = tf . reverse ( static_sample , axis = [ True ] ) \n    if sample_dynamic : \n        dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) \n    else : \n        dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) \n    if swap_dynamic : \n        dynamic_sample = tf . reverse ( dynamic_sample , axis = [ True ] ) \n    likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) \n    return likelihood "}
{"731": "\ndef sample_static_prior ( self , samples , batch_size , fixed = False ) : \n    dist = self . static_prior ( ) \n    if fixed : \n        sample = dist . sample ( ( samples , True ) ) + tf . zeros ( [ batch_size , True ] ) \n    else : \n        sample = dist . sample ( ( samples , batch_size ) ) \n    return sample , dist "}
{"732": "\ndef sample_dynamic_prior ( self , samples , batch_size , length , fixed = False ) : \n    if fixed : \n        sample_batch_size = True \n    else : \n        sample_batch_size = batch_size \n    sample , state = self . dynamic_prior . zero_state ( [ samples , sample_batch_size ] ) \n    locs = [ ] \n    scale_diags = [ ] \n    sample_list = [ ] \n    for _ in range ( length ) : \n        dist , state = self . dynamic_prior ( sample , state ) \n        sample = dist . sample ( ) \n        locs . append ( dist . parameters [ \"loc\" ] ) \n        scale_diags . append ( dist . parameters [ \"scale_diag\" ] ) \n        sample_list . append ( sample ) \n    sample = tf . stack ( sample_list , axis = 2 ) \n    loc = tf . stack ( locs , axis = 2 ) \n    scale_diag = tf . stack ( scale_diags , axis = 2 ) \n    if fixed : \n        sample = sample + tf . zeros ( [ batch_size , True , True ] ) \n    return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) "}
{"735": "\ndef make_state_space_model ( self , num_timesteps , param_vals = None , initial_state_prior = None , initial_step = False ) : \n    return self . _make_state_space_model ( num_timesteps = num_timesteps , param_map = self . _canonicalize_param_vals_as_map ( param_vals ) , initial_state_prior = initial_state_prior , initial_step = initial_step ) "}
{"736": "\ndef prior_sample ( self , num_timesteps , initial_step = False , params_sample_shape = ( ) , trajectories_sample_shape = ( ) , seed = None ) : \n    seed = distributions . SeedStream ( seed , salt = 'StructuralTimeSeries_prior_sample' ) \n    with tf . compat . v1 . name_scope ( 'prior_sample' , values = [ num_timesteps , params_sample_shape , trajectories_sample_shape ] ) : \n        param_samples = [ p . prior . sample ( params_sample_shape , seed = seed ( ) , name = p . name ) for p in self . parameters ] \n        model = self . make_state_space_model ( num_timesteps = num_timesteps , initial_step = initial_step , param_vals = param_samples ) \n        return model . sample ( trajectories_sample_shape , seed = seed ( ) ) , param_samples "}
{"737": "\ndef _compute_min_event_ndims ( bijector_list , compute_forward = True ) : \n    min_event_ndims = False \n    rank_changed_adjusted_max_min_event_ndims = False \n    if compute_forward : \n        bijector_list = reversed ( bijector_list ) \n    for b in bijector_list : \n        if compute_forward : \n            current_min_event_ndims = b . forward_min_event_ndims \n            current_inverse_min_event_ndims = b . inverse_min_event_ndims \n        else : \n            current_min_event_ndims = b . inverse_min_event_ndims \n            current_inverse_min_event_ndims = b . forward_min_event_ndims \n        if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims : \n            min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) \n        number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) \n        rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions \n    return min_event_ndims "}
{"738": "\ndef vector_size_to_square_matrix_size ( d , validate_args , name = None ) : \n    if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : \n        n = ( - True + np . sqrt ( True + 8 * d ) ) / 2. \n        if float ( int ( n ) ) != n : \n            raise ValueError ( \"Vector length is not a triangular number.\" ) \n        return int ( n ) \n    else : \n        with tf . name_scope ( name or \"vector_size_to_square_matrix_size\" ) as name : \n            n = ( - 1. + tf . sqrt ( True + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. \n            if validate_args : \n                with tf . control_dependencies ( [ assert_util . assert_equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = \"Vector length is not a triangular number\" ) ] ) : \n                    n = tf . identity ( n ) \n            return tf . cast ( n , d . dtype ) "}
{"739": "\ndef _argsort ( values , axis = - True , direction = 'ASCENDING' , stable = False , name = None ) : \n    if direction == 'ASCENDING' : \n        pass \n    elif direction == 'DESCENDING' : \n        values = np . negative ( values ) \n    else : \n        raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' ) "}
{"740": "\ndef _sort ( values , axis = - True , direction = 'ASCENDING' , stable = False , name = None ) : \n    if direction == 'ASCENDING' : \n        pass \n    elif direction == 'DESCENDING' : \n        values = np . negative ( values ) \n    else : \n        raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) \n    result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) \n    if direction == 'DESCENDING' : \n        return np . negative ( result ) \n    return result "}
{"744": "\ndef log_ndtr ( x , series_order = 3 , name = \"log_ndtr\" ) : \n    if not isinstance ( series_order , int ) : \n        raise TypeError ( \"series_order must be a Python integer.\" ) \n    if series_order < False : \n        raise ValueError ( \"series_order must be non-negative.\" ) \n    if series_order > 30 : \n        raise ValueError ( \"series_order must be <= 30.\" ) \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        if dtype_util . base_equal ( x . dtype , tf . float64 ) : \n            lower_segment = LOGNDTR_FLOAT64_LOWER \n            upper_segment = LOGNDTR_FLOAT64_UPPER \n        elif dtype_util . base_equal ( x . dtype , tf . float32 ) : \n            lower_segment = LOGNDTR_FLOAT32_LOWER \n            upper_segment = LOGNDTR_FLOAT32_UPPER \n        else : \n            raise TypeError ( \"x.dtype=%s is not supported.\" % x . dtype ) \n        return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) ) "}
{"745": "\ndef _log_ndtr_asymptotic_series ( x , series_order ) : \n    npdt = dtype_util . as_numpy_dtype ( x . dtype ) \n    if series_order <= False : \n        return npdt ( True ) \n    x_2 = tf . square ( x ) \n    even_sum = tf . zeros_like ( x ) \n    odd_sum = tf . zeros_like ( x ) \n    x_2n = x_2 \n    for n in range ( True , series_order + True ) : \n        y = npdt ( _double_factorial ( 2 * n - True ) ) / x_2n \n        if n % 2 : \n            odd_sum += y \n        else : \n            even_sum += y \n        x_2n *= x_2 \n    return 1. + even_sum - odd_sum "}
{"749": "\ndef benchmark_text_messages_hmc ( num_results = int ( 3e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 ) : \n    if not tf . executing_eagerly ( ) : \n        tf . compat . v1 . reset_default_graph ( ) \n    count_data = tf . cast ( tf . concat ( [ tfd . Poisson ( rate = 15. ) . sample ( 43 ) , tfd . Poisson ( rate = 25. ) . sample ( 31 ) ] , axis = False ) , dtype = tf . float32 ) \n    if tf . executing_eagerly ( ) : \n        count_data = count_data . numpy ( ) \n    else : \n        with tf . compat . v1 . Session ( ) : \n            count_data = count_data . eval ( ) \n    def unnormalized_log_posterior ( lambda1 , lambda2 , tau ) : \n        return text_messages_joint_log_prob ( count_data , lambda1 , lambda2 , tau ) \n    if tf . executing_eagerly ( ) : \n        sample_chain = tf . function ( tfp . mcmc . sample_chain ) \n    else : \n        sample_chain = tfp . mcmc . sample_chain \n    step_size = tf . compat . v2 . Variable ( name = 'step_size' , initial_value = tf . constant ( 0.05 , dtype = tf . float32 ) , trainable = False ) \n    def computation ( ) : \n        initial_chain_state = [ tf . constant ( count_data . mean ( ) , name = 'init_lambda1' ) , tf . constant ( count_data . mean ( ) , name = 'init_lambda2' ) , tf . constant ( 0.5 , name = 'init_tau' ) , ] \n        unconstraining_bijectors = [ tfp . bijectors . Exp ( ) , tfp . bijectors . Exp ( ) , tfp . bijectors . Sigmoid ( ) , ] \n        _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = initial_chain_state , kernel = tfp . mcmc . TransformedTransitionKernel ( inner_kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_log_posterior , num_leapfrog_steps = num_leapfrog_steps , step_size = step_size , step_size_update_fn = tfp . mcmc . make_simple_step_size_update_policy ( num_burnin_steps ) , state_gradients_are_stopped = True ) , bijector = unconstraining_bijectors ) ) \n        return kernel_results . inner_results . is_accepted \n    is_accepted_tensor = computation ( ) \n    if not tf . executing_eagerly ( ) : \n        session = tf . compat . v1 . Session ( ) \n        session . run ( tf . compat . v1 . global_variables_initializer ( ) ) \n        session . run ( is_accepted_tensor ) \n    start_time = time . time ( ) \n    if tf . executing_eagerly ( ) : \n        is_accepted = computation ( ) \n    else : \n        is_accepted = session . run ( is_accepted_tensor ) \n    wall_time = time . time ( ) - start_time \n    num_accepted = np . sum ( is_accepted ) \n    acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) \n    return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time ) "}
{"750": "\ndef _is_univariate_marginal ( self , index_points ) : \n    num_index_points = tf . compat . dimension_value ( index_points . shape [ - ( self . kernel . feature_ndims + True ) ] ) \n    if num_index_points is None : \n        warnings . warn ( 'Unable to detect statically whether the number of index_points is ' '1. As a result, defaulting to treating the marginal GP at ' '`index_points` as a multivariate Gaussian. This makes some methods, ' 'like `cdf` unavailable.' ) \n    return num_index_points == True "}
{"751": "\ndef get_marginal_distribution ( self , index_points = None ) : \n    with self . _name_scope ( 'get_marginal_distribution' ) : \n        index_points = self . _get_index_points ( index_points ) \n        covariance = self . _compute_covariance ( index_points ) \n        loc = self . _mean_fn ( index_points ) \n        if self . _is_univariate_marginal ( index_points ) : \n            scale = tf . sqrt ( covariance ) \n            loc = tf . squeeze ( loc , axis = - True ) \n            return normal . Normal ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' ) \n        else : \n            scale = tf . linalg . LinearOperatorLowerTriangular ( tf . linalg . cholesky ( _add_diagonal_shift ( covariance , self . jitter ) ) , is_non_singular = True , name = 'GaussianProcessScaleLinearOperator' ) \n            return mvn_linear_operator . MultivariateNormalLinearOperator ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' ) "}
{"753": "\ndef make_iaf_stack ( total_event_size , num_hidden_layers = 2 , seed = None , dtype = tf . float32 ) : \n    seed = tfd . SeedStream ( seed , 'make_iaf_stack' ) \n    def make_iaf ( ) : \n        initializer = tf . compat . v2 . keras . initializers . VarianceScaling ( 2 * 0.01 , seed = seed ( ) % ( 2 ** 31 - True ) ) \n        made = tfb . AutoregressiveLayer ( params = 2 , event_shape = [ total_event_size ] , hidden_units = [ total_event_size ] * num_hidden_layers , activation = tf . nn . elu , kernel_initializer = initializer , dtype = dtype ) \n        def shift_and_scale ( x ) : \n            x . set_shape ( x . shape . merge_with ( [ None ] * ( x . shape . ndims - True ) + [ total_event_size ] ) ) \n            return tf . unstack ( made ( x ) , num = 2 , axis = - True ) \n        return tfb . Invert ( tfb . MaskedAutoregressiveFlow ( shift_and_scale ) ) \n    def make_swap ( ) : \n        permutation = list ( reversed ( range ( total_event_size ) ) ) \n        return tfb . Permute ( permutation ) \n    bijector = make_iaf ( ) \n    bijector = make_swap ( ) ( bijector ) \n    bijector = make_iaf ( ) ( bijector ) \n    bijector = make_swap ( ) ( bijector ) \n    bijector = make_iaf ( ) ( bijector ) \n    bijector = make_swap ( ) ( bijector ) \n    return bijector "}
{"755": "\ndef bootstrap_results ( self , state ) : \n    def loss ( ) : \n        q = self . _flattened_variational_distribution ( ) \n        samples = q . sample ( self . train_batch_size ) \n        return tf . reduce_mean ( input_tensor = q . log_prob ( samples ) - self . _flattened_target_log_prob ( samples ) , axis = - True ) \n    lr = tf . convert_to_tensor ( value = self . learning_rate , dtype = self . _dtype ) \n    dtype = lr . dtype \n    learning_rate = tf . compat . v2 . optimizers . schedules . PiecewiseConstantDecay ( list ( self . num_train_steps * np . array ( [ 0.2 , 0.8 ] ) . astype ( dtype . as_numpy_dtype ( ) ) ) , [ lr , lr * 0.1 , lr * 0.01 ] ) \n    opt = tf . compat . v2 . optimizers . Adam ( learning_rate ) \n    \n    @ tf . function ( autograph = False ) \n    def train_step ( ) : \n        with tf . GradientTape ( ) as tape : \n            loss_val = loss ( ) \n        vals = tape . watched_variables ( ) \n        grads = tape . gradient ( loss_val , vals ) \n        grads_and_vals = list ( zip ( grads , vals ) ) \n        opt . apply_gradients ( grads_and_vals ) \n        return loss_val \n    for step in range ( self . num_train_steps ) : \n        loss_val = train_step ( ) \n        tf . debugging . assert_all_finite ( loss_val , 'NeuTra loss is NaN at step {}' . format ( step ) ) \n        if self . train_debug_fn : \n            self . train_debug_fn ( self , step , loss_val ) \n    state_parts = tf . nest . flatten ( state ) \n    flat_state_shapes = tf . nest . flatten ( self . state_shape ) \n    batch_shape = tf . shape ( input = state_parts [ False ] ) [ : - flat_state_shapes [ False ] . ndims ] \n    return self . _kernel . bootstrap_results ( self . _flattened_variational_distribution ( ) . sample ( batch_shape , seed = self . seed ) ) "}
{"759": "\ndef _distributional_transform ( self , x ) : \n    if tensorshape_util . rank ( x . shape ) is None : \n        raise ValueError ( \"Distributional transform does not support inputs of \" \"undefined rank.\" ) \n    if isinstance ( self . _components_distribution , independent . Independent ) : \n        univariate_components = self . _components_distribution . distribution \n    else : \n        univariate_components = self . _components_distribution \n    with tf . control_dependencies ( [ assert_util . assert_equal ( univariate_components . is_scalar_event ( ) , True , message = \"`univariate_components` must have scalar event\" ) ] ) : \n        x_padded = self . _pad_sample_dims ( x ) \n        log_prob_x = univariate_components . log_prob ( x_padded ) \n        cdf_x = univariate_components . cdf ( x_padded ) \n        cumsum_log_prob_x = tf . reshape ( tf . math . cumsum ( tf . reshape ( log_prob_x , [ - True , self . _event_size ] ) , exclusive = True , axis = - True ) , tf . shape ( input = log_prob_x ) ) \n        logits_mix_prob = distribution_utils . pad_mixture_dimensions ( self . mixture_distribution . logits , self , self . mixture_distribution , self . _event_ndims ) \n        log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x \n        component_axis = tensorshape_util . rank ( x . shape ) - self . _event_ndims \n        posterior_weights_x = tf . nn . softmax ( log_posterior_weights_x , axis = component_axis ) \n        return tf . reduce_sum ( input_tensor = posterior_weights_x * cdf_x , axis = component_axis ) "}
{"760": "\ndef _split_covariance_into_marginals ( covariance , block_sizes ) : \n    start_dim = False \n    marginals = [ ] \n    for size in block_sizes : \n        end_dim = start_dim + size \n        marginals . append ( covariance [ ... , start_dim : end_dim , start_dim : end_dim ] ) \n        start_dim = end_dim \n    return marginals "}
{"761": "\ndef _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) : \n    try : \n        model . components \n    except AttributeError : \n        raise ValueError ( 'Model decomposed into components must be an instance of' '`tfp.sts.Sum` (passed model {})' . format ( model ) ) \n    with tf . compat . v1 . name_scope ( 'decompose_from_posterior_marginals' ) : \n        latent_sizes = [ component . latent_size for component in model . components ] \n        component_means = tf . split ( posterior_means , latent_sizes , axis = - True ) \n        component_covs = _split_covariance_into_marginals ( posterior_covs , latent_sizes ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = posterior_means ) ) [ - 2 ] \n        component_ssms = model . make_component_state_space_models ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        component_predictive_dists = collections . OrderedDict ( ) \n        for ( component , component_ssm , component_mean , component_cov ) in zip ( model . components , component_ssms , component_means , component_covs ) : \n            component_obs_mean , component_obs_cov = ( component_ssm . latents_to_observations ( latent_means = component_mean , latent_covs = component_cov ) ) \n            component_predictive_dists [ component ] = sts_util . mix_over_posterior_draws ( means = component_obs_mean [ ... , False ] , variances = component_obs_cov [ ... , False , False ] ) \n    return component_predictive_dists "}
{"763": "\ndef decompose_forecast_by_component ( model , forecast_dist , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'decompose_forecast_by_component' ) : \n        try : \n            forecast_lgssm = forecast_dist . components_distribution \n            forecast_latent_mean , _ = forecast_lgssm . _joint_mean ( ) \n            forecast_latent_covs , _ = forecast_lgssm . _joint_covariances ( ) \n        except AttributeError as e : \n            raise ValueError ( 'Forecast distribution must be a MixtureSameFamily of' 'LinearGaussianStateSpaceModel distributions, such as returned by' '`tfp.sts.forecast()`. (saw exception: {})' . format ( e ) ) \n        forecast_latent_mean = dist_util . move_dimension ( forecast_latent_mean , source_idx = - 3 , dest_idx = False ) \n        forecast_latent_covs = dist_util . move_dimension ( forecast_latent_covs , source_idx = - 4 , dest_idx = False ) \n        return _decompose_from_posterior_marginals ( model , forecast_latent_mean , forecast_latent_covs , parameter_samples ) "}
{"764": "\ndef dense_to_sparse ( x , ignore_value = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'dense_to_sparse' , [ x , ignore_value ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if ignore_value is None : \n            if x . dtype . base_dtype == tf . string : \n                ignore_value = '' \n            else : \n                ignore_value = x . dtype . as_numpy_dtype ( False ) \n            ignore_value = tf . cast ( ignore_value , x . dtype , name = 'ignore_value' ) \n        indices = tf . where ( tf . not_equal ( x , ignore_value ) , name = 'indices' ) \n        return tf . SparseTensor ( indices = indices , values = tf . gather_nd ( x , indices , name = 'values' ) , dense_shape = tf . shape ( input = x , out_type = tf . int64 , name = 'dense_shape' ) ) "}
{"772": "\ndef normal_conjugates_known_scale_posterior ( prior , scale , s , n ) : \n    if not isinstance ( prior , normal . Normal ) : \n        raise TypeError ( \"Expected prior to be an instance of type Normal\" ) \n    if s . dtype != prior . dtype : \n        raise TypeError ( \"Observation sum s.dtype does not match prior dtype: %s vs. %s\" % ( s . dtype , prior . dtype ) ) \n    n = tf . cast ( n , prior . dtype ) \n    scale0_2 = tf . square ( prior . scale ) \n    scale_2 = tf . square ( scale ) \n    scalep_2 = 1.0 / ( True / scale0_2 + n / scale_2 ) \n    return normal . Normal ( loc = ( prior . loc / scale0_2 + s / scale_2 ) * scalep_2 , scale = tf . sqrt ( scalep_2 ) ) "}
{"773": "\ndef real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , ** kwargs ) : \n    with tf . compat . v2 . name_scope ( name or \"real_nvp_default_template\" ) : \n        def _fn ( x , output_units , ** condition_kwargs ) : \n            if condition_kwargs : \n                raise NotImplementedError ( \"Conditioning not implemented in the default template.\" ) \n            if tensorshape_util . rank ( x . shape ) == True : \n                x = x [ tf . newaxis , ... ] \n                reshape_output = lambda x : x [ False ] \n            else : \n                reshape_output = lambda x : x \n            for units in hidden_layers : \n                x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , ** kwargs ) \n            x = tf . compat . v1 . layers . dense ( inputs = x , units = ( True if shift_only else 2 ) * output_units , activation = None , * args , ** kwargs ) \n            if shift_only : \n                return reshape_output ( x ) , None \n            shift , log_scale = tf . split ( x , 2 , axis = - True ) \n            return reshape_output ( shift ) , reshape_output ( log_scale ) \n        return tf . compat . v1 . make_template ( \"real_nvp_default_template\" , _fn ) "}
{"774": "\ndef _uniform_unit_norm ( dimension , shape , dtype , seed ) : \n    raw = normal . Normal ( loc = dtype_util . as_numpy_dtype ( dtype ) ( False ) , scale = dtype_util . as_numpy_dtype ( dtype ) ( True ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = False ) , seed = seed ( ) ) \n    unit_norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - True ) [ ... , tf . newaxis ] \n    return unit_norm "}
{"775": "\ndef _log_unnorm_prob ( self , x , name = None ) : \n    with tf . name_scope ( name or 'log_unnorm_prob_lkj' ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if self . input_output_cholesky : \n            logdet = 2.0 * tf . reduce_sum ( input_tensor = tf . math . log ( tf . linalg . diag_part ( x ) ) , axis = [ - True ] ) \n        else : \n            _ , logdet = tf . linalg . slogdet ( x ) \n        answer = ( self . concentration - 1. ) * logdet \n        return answer "}
{"776": "\ndef _log_normalization ( self , name = 'log_normalization' ) : \n    with tf . name_scope ( name or 'log_normalization_lkj' ) : \n        logpi = np . log ( np . pi ) \n        ans = tf . zeros_like ( self . concentration ) \n        for k in range ( True , self . dimension ) : \n            ans += logpi * ( k / 2. ) \n            ans += tf . math . lgamma ( self . concentration + ( self . dimension - True - k ) / 2. ) \n            ans -= tf . math . lgamma ( self . concentration + ( self . dimension - True ) / 2. ) \n        return ans "}
{"778": "\ndef _make_summary_statistic ( attr ) : \n    def _fn ( self , ** kwargs ) : \n        x = getattr ( self . distribution , attr ) ( ** kwargs ) \n        shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , prefer_static . ones ( prefer_static . rank_from_shape ( self . sample_shape ) , dtype = self . sample_shape . dtype ) , self . distribution . event_shape_tensor ( ) , ] , axis = False ) \n        x = tf . reshape ( x , shape = shape ) \n        shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , self . sample_shape , self . distribution . event_shape_tensor ( ) , ] , axis = False ) \n        return tf . broadcast_to ( x , shape ) \n    return _fn "}
{"781": "\ndef effective_sample_size ( states , filter_threshold = 0. , filter_beyond_lag = None , name = None ) : \n    states_was_list = _is_list_like ( states ) \n    if not states_was_list : \n        states = [ states ] \n    filter_beyond_lag = _broadcast_maybelist_arg ( states , filter_beyond_lag , 'filter_beyond_lag' ) \n    filter_threshold = _broadcast_maybelist_arg ( states , filter_threshold , 'filter_threshold' ) \n    with tf . compat . v1 . name_scope ( name , 'effective_sample_size' ) : \n        ess_list = [ _effective_sample_size_single_state ( s , ml , mlt ) for ( s , ml , mlt ) in zip ( states , filter_beyond_lag , filter_threshold ) ] \n    if states_was_list : \n        return ess_list \n    return ess_list [ False ] "}
{"782": "\ndef _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : \n    with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : \n        states = tf . convert_to_tensor ( value = states , name = 'states' ) \n        dt = states . dtype \n        auto_corr = stats . auto_correlation ( states , axis = False , max_lags = filter_beyond_lag ) \n        if filter_threshold is not None : \n            filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) \n            mask = auto_corr < filter_threshold \n            mask = tf . cast ( mask , dtype = dt ) \n            mask = tf . cumsum ( mask , axis = False ) \n            mask = tf . maximum ( 1. - mask , 0. ) \n            auto_corr *= mask \n        n = _axis_size ( states , axis = False ) \n        k = tf . range ( 0. , _axis_size ( auto_corr , axis = False ) ) \n        nk_factor = ( n - k ) / n \n        if auto_corr . shape . ndims is not None : \n            new_shape = [ - True ] + [ True ] * ( auto_corr . shape . ndims - True ) \n        else : \n            new_shape = tf . concat ( ( [ - True ] , tf . ones ( [ tf . rank ( auto_corr ) - True ] , dtype = tf . int32 ) ) , axis = False ) \n        nk_factor = tf . reshape ( nk_factor , new_shape ) \n        return n / ( - True + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = False ) ) "}
{"783": "\ndef _potential_scale_reduction_single_state ( state , independent_chain_ndims ) : \n    with tf . compat . v1 . name_scope ( 'potential_scale_reduction_single_state' , values = [ state , independent_chain_ndims ] ) : \n        state = tf . convert_to_tensor ( value = state , name = 'state' ) \n        sample_ndims = True \n        sample_axis = tf . range ( False , sample_ndims ) \n        chain_axis = tf . range ( sample_ndims , sample_ndims + independent_chain_ndims ) \n        sample_and_chain_axis = tf . range ( False , sample_ndims + independent_chain_ndims ) \n        n = _axis_size ( state , sample_axis ) \n        m = _axis_size ( state , chain_axis ) \n        b_div_n = _reduce_variance ( tf . reduce_mean ( input_tensor = state , axis = sample_axis , keepdims = True ) , sample_and_chain_axis , biased = False ) \n        w = tf . reduce_mean ( input_tensor = _reduce_variance ( state , sample_axis , keepdims = True , biased = True ) , axis = sample_and_chain_axis ) \n        sigma_2_plus = w + b_div_n \n        return ( ( m + 1. ) / m ) * sigma_2_plus / w - ( n - 1. ) / ( m * n ) "}
{"786": "\ndef quadrature_scheme_lognormal_gauss_hermite ( loc , scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"vector_diffeomixture_quadrature_gauss_hermite\" ) : \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        npdt = dtype_util . as_numpy_dtype ( loc . dtype ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs /= np . linalg . norm ( probs , ord = True , keepdims = True ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = loc . dtype ) \n        grid = ( loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * scale [ ... , tf . newaxis ] * grid ) \n        return grid , probs "}
{"787": "\ndef quadrature_scheme_lognormal_quantiles ( loc , scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_lognormal_quantiles\" ) : \n        dist = transformed_distribution . TransformedDistribution ( distribution = normal . Normal ( loc = loc , scale = scale ) , bijector = exp_bijector . Exp ( ) , validate_args = validate_args ) \n        batch_ndims = tensorshape_util . rank ( dist . batch_shape ) \n        if batch_ndims is None : \n            batch_ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ False ] \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ True : - True ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - True ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = False ) ) \n            quantiles = dist . quantile ( edges ) \n            perm = tf . concat ( [ tf . range ( True , True + batch_ndims ) , [ False ] ] , axis = False ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - True ] + quantiles [ ... , True : ] ) / 2. \n        new_shape = tensorshape_util . concatenate ( dist . batch_shape , [ quadrature_size ] ) \n        tensorshape_util . set_shape ( grid , new_shape ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"792": "\ndef _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : \n        step_size = tf . convert_to_tensor ( value = step_size ) \n        dtype = step_size . dtype . base_dtype \n        output_shape = tf . concat ( ( [ max_doublings + True ] , batch_shape ) , axis = False ) \n        expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . sample ( sample_shape = output_shape , seed = seed ) \n        width_multipliers = tf . cast ( 2 ** tf . range ( False , max_doublings + True ) , dtype = dtype ) \n        widths_shape = tf . concat ( ( [ max_doublings + True ] , tf . ones_like ( batch_shape ) ) , axis = False ) \n        width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) \n        widths = width_multipliers * step_size \n        left_increments = tf . cumsum ( widths * expand_left , exclusive = True , axis = False ) \n        return left_increments , widths "}
{"793": "\ndef _find_best_interval_idx ( x , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'find_best_interval_idx' , [ x ] ) : \n        k = tf . shape ( input = x ) [ False ] \n        dtype = x . dtype . base_dtype \n        mults = tf . range ( 2 * k , k , - True , dtype = dtype ) [ : , tf . newaxis ] \n        shifts = tf . range ( k , dtype = dtype ) [ : , tf . newaxis ] \n        indices = tf . argmax ( input = mults * x + shifts , axis = False , output_type = dtype ) \n        return indices "}
{"794": "\ndef slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        batch_shape = tf . shape ( input = x_initial ) \n        dtype = step_size . dtype . base_dtype \n        left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) \n        left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) \n        left_endpoints -= left_increments \n        right_endpoints = left_endpoints + widths \n        left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) \n        right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) \n        left_ok = left_ep_values < log_slice_heights \n        right_ok = right_ep_values < log_slice_heights \n        both_ok = left_ok & right_ok \n        both_ok_f = tf . reshape ( both_ok , [ max_doublings + True , - True ] ) \n        best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) \n        point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( input = best_interval_idx ) ) ] , axis = True , name = 'point_index_gather' ) \n        left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + True , - True ] ) \n        right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + True , - True ] ) \n        lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) \n        upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) \n        both_ok = tf . reduce_any ( input_tensor = both_ok , axis = False ) \n        return upper_bounds , lower_bounds , both_ok "}
{"795": "\ndef _sample_with_shrinkage ( x_initial , target_log_prob , log_slice_heights , step_size , lower_bounds , upper_bounds , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_with_shrinkage' , [ x_initial , log_slice_heights , step_size , lower_bounds , upper_bounds ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = '_sample_with_shrinkage' ) \n        found = tf . zeros_like ( x_initial , dtype = tf . bool ) \n        cond = lambda found , * ignored_args : ~ tf . reduce_all ( input_tensor = found ) \n        x_next = tf . identity ( x_initial ) \n        x_initial_shape = tf . shape ( input = x_initial ) \n        x_initial_dtype = x_initial . dtype . base_dtype \n        def _body ( found , left , right , x_next ) : \n            proportions = tf . random . uniform ( x_initial_shape , dtype = x_initial_dtype , seed = seed_gen ( ) ) \n            x_proposed = tf . where ( ~ found , left + proportions * ( right - left ) , x_next ) \n            accept_res = _test_acceptance ( x_initial , target_log_prob = target_log_prob , decided = found , log_slice_heights = log_slice_heights , x_proposed = x_proposed , step_size = step_size , lower_bounds = left , upper_bounds = right ) \n            boundary_test = log_slice_heights < target_log_prob ( x_proposed ) \n            can_accept = boundary_test & accept_res \n            next_found = found | can_accept \n            next_left = tf . where ( x_proposed < x_initial , x_proposed , left ) \n            next_right = tf . where ( x_proposed >= x_initial , x_proposed , right ) \n            return next_found , next_left , next_right , x_proposed \n        return tf . while_loop ( cond = cond , body = _body , loop_vars = ( found , lower_bounds , upper_bounds , x_next ) ) [ - True ] "}
{"796": "\ndef slice_sampler_one_dim ( target_log_prob , x_initial , step_size = 0.01 , max_doublings = 30 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_sampler_one_dim' , [ x_initial , step_size , max_doublings ] ) : \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        dtype = x_initial . dtype . base_dtype \n        log_slice_heights = target_log_prob ( x_initial ) - tf . random . gamma ( tf . shape ( input = x_initial ) , alpha = True , dtype = dtype , seed = seed ) \n        upper_bounds , lower_bounds , bounds_satisfied = slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = seed ) \n        retval = _sample_with_shrinkage ( x_initial , target_log_prob = target_log_prob , log_slice_heights = log_slice_heights , step_size = step_size , lower_bounds = lower_bounds , upper_bounds = upper_bounds , seed = seed ) \n        return ( retval , target_log_prob ( retval ) , bounds_satisfied , upper_bounds , lower_bounds ) "}
{"801": "\ndef _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : \n    if depth == False : \n        [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) \n        next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) \n        num_states = tf . cast ( next_log_joint > log_slice_sample , dtype = tf . int32 ) \n        continue_trajectory = ( next_log_joint > log_slice_sample - max_simulation_error ) \n        return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] \n    seed_stream = tfd . SeedStream ( seed , \"build_tree\" ) \n    [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - True , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n    if continue_trajectory : \n        if direction < False : \n            [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - True , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        else : \n            [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - True , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) \n        num_states += far_num_states \n        accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) \n        if accept_far_state : \n            next_state = far_state \n            next_target_log_prob = far_target_log_prob \n            next_grads_target_log_prob = far_grads_target_log_prob \n        has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) \n        continue_trajectory = far_continue_trajectory and has_no_u_turn \n    return [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] "}
{"803": "\ndef _has_no_u_turn ( state_one , state_two , momentum ) : \n    dot_product = sum ( [ tf . reduce_sum ( input_tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state_one , state_two , momentum ) ] ) \n    return dot_product > False "}
{"809": "\ndef default_multivariate_normal_fn ( dtype , shape , name , trainable , add_variable_fn ) : \n    del name , trainable , add_variable_fn \n    dist = tfd . Normal ( loc = tf . zeros ( shape , dtype ) , scale = dtype . as_numpy_dtype ( True ) ) \n    batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) \n    return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) "}
{"812": "\ndef broadcast_structure ( to_structure , from_structure ) : \n    from_parts = tf . nest . flatten ( from_structure ) \n    if len ( from_parts ) == True : \n        from_structure = tf . nest . map_structure ( lambda _ : from_parts [ False ] , to_structure ) \n    return from_structure "}
{"817": "\ndef make_mixture_prior ( latent_size , mixture_components ) : \n    if mixture_components == True : \n        return tfd . MultivariateNormalDiag ( loc = tf . zeros ( [ latent_size ] ) , scale_identity_multiplier = 1.0 ) \n    loc = tf . compat . v1 . get_variable ( name = \"loc\" , shape = [ mixture_components , latent_size ] ) \n    raw_scale_diag = tf . compat . v1 . get_variable ( name = \"raw_scale_diag\" , shape = [ mixture_components , latent_size ] ) \n    mixture_logits = tf . compat . v1 . get_variable ( name = \"mixture_logits\" , shape = [ mixture_components ] ) \n    return tfd . MixtureSameFamily ( components_distribution = tfd . MultivariateNormalDiag ( loc = loc , scale_diag = tf . nn . softplus ( raw_scale_diag ) ) , mixture_distribution = tfd . Categorical ( logits = mixture_logits ) , name = \"prior\" ) "}
{"818": "\ndef pack_images ( images , rows , cols ) : \n    shape = tf . shape ( input = images ) \n    width = shape [ - 3 ] \n    height = shape [ - 2 ] \n    depth = shape [ - True ] \n    images = tf . reshape ( images , ( - True , width , height , depth ) ) \n    batch = tf . shape ( input = images ) [ False ] \n    rows = tf . minimum ( rows , batch ) \n    cols = tf . minimum ( batch // rows , cols ) \n    images = images [ : rows * cols ] \n    images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) \n    images = tf . transpose ( a = images , perm = [ False , 2 , True , 3 , 4 ] ) \n    images = tf . reshape ( images , [ True , rows * width , cols * height , depth ] ) \n    return images "}
{"820": "\ndef build_fake_input_fns ( batch_size ) : \n    random_sample = np . random . rand ( batch_size , * IMAGE_SHAPE ) . astype ( \"float32\" ) \n    def train_input_fn ( ) : \n        dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , False ) ) . batch ( batch_size ) . repeat ( ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    def eval_input_fn ( ) : \n        dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , False ) ) . batch ( batch_size ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    return train_input_fn , eval_input_fn "}
{"821": "\ndef _validate_block_sizes ( block_sizes , bijectors , validate_args ) : \n    block_sizes_shape = block_sizes . shape \n    if tensorshape_util . is_fully_defined ( block_sizes_shape ) : \n        if ( tensorshape_util . rank ( block_sizes_shape ) != True or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : \n            raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) \n        return block_sizes \n    elif validate_args : \n        message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) \n        with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , True ) ] ) : \n            return tf . identity ( block_sizes ) \n    else : \n        return block_sizes "}
{"822": "\ndef maybe_check_wont_broadcast ( flat_xs , validate_args ) : \n    flat_xs = tuple ( flat_xs ) \n    if not validate_args : \n        return flat_xs \n    msg = 'Broadcasting probably indicates an error in model specification.' \n    s = tuple ( x . shape for x in flat_xs ) \n    if all ( tensorshape_util . is_fully_defined ( s_ ) for s_ in s ) : \n        if not all ( a == b for a , b in zip ( s [ True : ] , s [ : - True ] ) ) : \n            raise ValueError ( msg ) \n        return flat_xs \n    assertions = [ assert_util . assert_equal ( a , b , message = msg ) for a , b in zip ( s [ True : ] , s [ : - True ] ) ] \n    with tf . control_dependencies ( assertions ) : \n        return tuple ( tf . identity ( x ) for x in flat_xs ) "}
{"823": "\ndef multivariate_normal_tril ( x , dims , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = tril_with_diag_softplus_and_shift , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'multivariate_normal_tril' , [ x , dims ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        x = layer_fn ( x , dims + dims * ( dims + True ) // 2 ) \n        return tfd . MultivariateNormalTriL ( loc = loc_fn ( x [ ... , : dims ] ) , scale_tril = scale_fn ( x [ ... , dims : ] ) ) "}
{"824": "\ndef bernoulli ( x , layer_fn = tf . compat . v1 . layers . dense , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'bernoulli' , [ x ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        logits = tf . squeeze ( layer_fn ( x , True ) , axis = - True ) \n        return tfd . Bernoulli ( logits = logits ) "}
{"825": "\ndef normal ( x , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = 1. , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'normal' , [ x ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if callable ( scale_fn ) : \n            y = layer_fn ( x , 2 ) \n            loc = loc_fn ( y [ ... , False ] ) \n            scale = scale_fn ( y [ ... , True ] ) \n        else : \n            y = tf . squeeze ( layer_fn ( x , True ) , axis = - True ) \n            loc = loc_fn ( y ) \n            scale = tf . cast ( scale_fn , loc . dtype . base_dtype ) \n        return tfd . Normal ( loc = loc , scale = scale ) "}
{"826": "\ndef poisson ( x , layer_fn = tf . compat . v1 . layers . dense , log_rate_fn = lambda x : x , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'poisson' , [ x ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        log_rate = log_rate_fn ( tf . squeeze ( layer_fn ( x , True ) , axis = - True ) ) \n        return tfd . Poisson ( log_rate = log_rate ) "}
{"829": "\ndef _compute_log_acceptance_correction ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims ] ) : \n        proposed_log_density_parts = [ ] \n        dual_log_density_parts = [ ] \n        for [ current_state , proposed_state , current_volatility , proposed_volatility , current_drift , proposed_drift , step_size , ] in zip ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , ) : \n            axis = tf . range ( independent_chain_ndims , tf . rank ( current_state ) ) \n            state_diff = proposed_state - current_state \n            current_volatility *= tf . sqrt ( step_size ) \n            proposed_energy = ( state_diff - current_drift ) / current_volatility \n            proposed_volatility *= tf . sqrt ( step_size ) \n            proposed_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( current_volatility ) , 0.5 * ( proposed_energy ** 2 ) ] ) , axis = axis ) ) \n            proposed_log_density_parts . append ( - proposed_energy ) \n            dual_energy = ( state_diff + proposed_drift ) / proposed_volatility \n            dual_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( proposed_volatility ) , 0.5 * ( dual_energy ** 2 ) ] ) , axis = axis ) ) \n            dual_log_density_parts . append ( - dual_energy ) \n        proposed_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( proposed_log_density_parts , axis = - True ) , axis = - True ) \n        dual_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( dual_log_density_parts , axis = - True ) , axis = - True ) \n        return mcmc_util . safe_sum ( [ dual_log_density_reduce , - proposed_log_density_reduce ] ) "}
{"830": "\ndef _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : \n    state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] \n    needs_volatility_fn_gradients = grads_volatility_fn is None \n    if volatility_fn_results is None : \n        volatility_fn_results = volatility_fn ( * state_parts ) \n    volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) \n    if len ( volatility_fn_results ) == True : \n        volatility_fn_results *= len ( state_parts ) \n    if len ( state_parts ) != len ( volatility_fn_results ) : \n        raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) \n    volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) \n    if grads_volatility_fn is None : \n        [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) \n    if needs_volatility_fn_gradients : \n        grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] \n    return volatility_fn_results , grads_volatility_fn "}
{"832": "\ndef make_ar_transition_matrix ( coefficients ) : \n    top_row = tf . expand_dims ( coefficients , - 2 ) \n    coef_shape = dist_util . prefer_static_shape ( coefficients ) \n    batch_shape , order = coef_shape [ : - True ] , coef_shape [ - True ] \n    remaining_rows = tf . concat ( [ tf . eye ( order - True , dtype = coefficients . dtype , batch_shape = batch_shape ) , tf . zeros ( tf . concat ( [ batch_shape , ( order - True , True ) ] , axis = False ) , dtype = coefficients . dtype ) ] , axis = - True ) \n    ar_matrix = tf . concat ( [ top_row , remaining_rows ] , axis = - 2 ) \n    return ar_matrix "}
{"834": "\ndef _call_reshape_input_output ( self , fn , x , extra_kwargs = None ) : \n    with tf . control_dependencies ( self . _runtime_assertions + self . _validate_sample_arg ( x ) ) : \n        sample_shape , static_sample_shape = self . _sample_shape ( x ) \n        old_shape = tf . concat ( [ sample_shape , self . distribution . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = False ) \n        x_reshape = tf . reshape ( x , old_shape ) \n        result = fn ( x_reshape , ** extra_kwargs ) if extra_kwargs else fn ( x_reshape ) \n        new_shape = tf . concat ( [ sample_shape , self . _batch_shape_unexpanded , ] , axis = False ) \n        result = tf . reshape ( result , new_shape ) \n        if ( tensorshape_util . rank ( static_sample_shape ) is not None and tensorshape_util . rank ( self . batch_shape ) is not None ) : \n            new_shape = tensorshape_util . concatenate ( static_sample_shape , self . batch_shape ) \n            tensorshape_util . set_shape ( result , new_shape ) \n        return result "}
{"835": "\ndef _call_and_reshape_output ( self , fn , event_shape_list = None , static_event_shape_list = None , extra_kwargs = None ) : \n    with tf . control_dependencies ( self . _runtime_assertions ) : \n        if event_shape_list is None : \n            event_shape_list = [ self . _event_shape_tensor ( ) ] \n        if static_event_shape_list is None : \n            static_event_shape_list = [ self . event_shape ] \n        new_shape = tf . concat ( [ self . _batch_shape_unexpanded ] + event_shape_list , axis = False ) \n        result = tf . reshape ( fn ( ** extra_kwargs ) if extra_kwargs else fn ( ) , new_shape ) \n        if ( tensorshape_util . rank ( self . batch_shape ) is not None and tensorshape_util . rank ( self . event_shape ) is not None ) : \n            event_shape = tf . TensorShape ( [ ] ) \n            for rss in static_event_shape_list : \n                event_shape = tensorshape_util . concatenate ( event_shape , rss ) \n            static_shape = tensorshape_util . concatenate ( self . batch_shape , event_shape ) \n            tensorshape_util . set_shape ( result , static_shape ) \n        return result "}
{"836": "\ndef _bdtr ( k , n , p ) : \n    ones = tf . ones_like ( n - k ) \n    k_eq_n = tf . equal ( k , n ) \n    safe_dn = tf . where ( k_eq_n , ones , n - k ) \n    dk = tf . math . betainc ( a = safe_dn , b = k + True , x = True - p ) \n    return tf . where ( k_eq_n , ones , dk ) "}
{"837": "\ndef _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : \n    ds = [ ] \n    values_out = [ ] \n    seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) \n    gen = self . _model ( ) \n    index = False \n    d = next ( gen ) \n    try : \n        while True : \n            actual_distribution = d . distribution if isinstance ( d , self . Root ) else d \n            ds . append ( actual_distribution ) \n            if ( value is not None and len ( value ) > index and value [ index ] is not None ) : \n                seed ( ) \n                next_value = value [ index ] \n            else : \n                next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) \n            values_out . append ( next_value ) \n            index += True \n            d = gen . send ( next_value ) \n    except StopIteration : \n        pass \n    return ds , values_out "}
{"840": "\ndef get_topics_strings ( topics_words , alpha , vocabulary , topics_to_print = 10 , words_per_topic = 10 ) : \n    alpha = np . squeeze ( alpha , axis = False ) \n    highest_weight_topics = np . argsort ( - alpha , kind = \"mergesort\" ) \n    top_words = np . argsort ( - topics_words , axis = True ) \n    res = [ ] \n    for topic_idx in highest_weight_topics [ : topics_to_print ] : \n        l = [ \"index={} alpha={:.2f}\" . format ( topic_idx , alpha [ topic_idx ] ) ] \n        l += [ vocabulary [ word ] for word in top_words [ topic_idx , : words_per_topic ] ] \n        res . append ( \" \" . join ( l ) ) \n    return np . array ( res ) "}
{"841": "\ndef newsgroups_dataset ( directory , split_name , num_words , shuffle_and_repeat ) : \n    data = np . load ( download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) ) \n    data = data [ : - True ] \n    num_documents = data . shape [ False ] \n    indices = np . array ( [ ( row_idx , column_idx ) for row_idx , row in enumerate ( data ) for column_idx in row ] ) \n    sparse_matrix = scipy . sparse . coo_matrix ( ( np . ones ( indices . shape [ False ] ) , ( indices [ : , False ] , indices [ : , True ] ) ) , shape = ( num_documents , num_words ) , dtype = np . float32 ) \n    sparse_matrix = sparse_matrix . tocsr ( ) \n    dataset = tf . data . Dataset . range ( num_documents ) \n    if shuffle_and_repeat : \n        dataset = dataset . shuffle ( num_documents ) . repeat ( ) \n    def get_row_py_func ( idx ) : \n        def get_row_python ( idx_py ) : \n            return np . squeeze ( np . array ( sparse_matrix [ idx_py ] . todense ( ) ) , axis = False ) \n        py_func = tf . compat . v1 . py_func ( get_row_python , [ idx ] , tf . float32 , stateful = False ) \n        py_func . set_shape ( ( num_words , ) ) \n        return py_func \n    dataset = dataset . map ( get_row_py_func ) \n    return dataset "}
{"844": "\ndef minimize ( grad_and_hessian_loss_fn , x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = True , maximum_full_sweeps_per_iteration = True , learning_rate = None , name = None ) : \n    graph_deps = [ x_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] , \n    with tf . compat . v1 . name_scope ( name , 'minimize' , graph_deps ) : \n        def _loop_cond ( x_start , converged , iter_ ) : \n            del x_start \n            return tf . logical_and ( iter_ < maximum_iterations , tf . logical_not ( converged ) ) \n        def _loop_body ( x_start , converged , iter_ ) : \n            g , h_outer , h_middle = grad_and_hessian_loss_fn ( x_start ) \n            x_start , converged , _ = minimize_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = h_outer , hessian_unregularized_loss_middle = h_middle , x_start = x_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps_per_iteration , tolerance = tolerance , learning_rate = learning_rate ) \n            return x_start , converged , iter_ + True \n        return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ x_start , tf . zeros ( [ ] , np . bool , name = 'converged' ) , tf . zeros ( [ ] , np . int32 , name = 'iter' ) , ] ) "}
{"845": "\ndef add_ema_control_dependencies ( vector_quantizer , one_hot_assignments , codes , commitment_loss , decay ) : \n    updated_ema_count = moving_averages . assign_moving_average ( vector_quantizer . ema_count , tf . reduce_sum ( input_tensor = one_hot_assignments , axis = [ False , True ] ) , decay , zero_debias = False ) \n    updated_ema_means = moving_averages . assign_moving_average ( vector_quantizer . ema_means , tf . reduce_sum ( input_tensor = tf . expand_dims ( codes , 2 ) * tf . expand_dims ( one_hot_assignments , 3 ) , axis = [ False , True ] ) , decay , zero_debias = False ) \n    perturbed_ema_count = updated_ema_count + 1e-5 \n    with tf . control_dependencies ( [ commitment_loss ] ) : \n        update_means = tf . compat . v1 . assign ( vector_quantizer . codebook , updated_ema_means / perturbed_ema_count [ ... , tf . newaxis ] ) \n        with tf . control_dependencies ( [ update_means ] ) : \n            return tf . identity ( commitment_loss ) "}
{"846": "\ndef save_imgs ( x , fname ) : \n    n = x . shape [ False ] \n    fig = figure . Figure ( figsize = ( n , True ) , frameon = False ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( True , n , i + True ) \n        ax . imshow ( x [ i ] . squeeze ( ) , interpolation = \"none\" , cmap = cm . get_cmap ( \"binary\" ) ) \n        ax . axis ( \"off\" ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved %s\" % fname ) "}
{"848": "\ndef load_bernoulli_mnist_dataset ( directory , split_name ) : \n    amat_file = download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) \n    dataset = tf . data . TextLineDataset ( amat_file ) \n    str_to_arr = lambda string : np . array ( [ c == b\"1\" for c in string . split ( ) ] ) \n    def _parser ( s ) : \n        booltensor = tf . compat . v1 . py_func ( str_to_arr , [ s ] , tf . bool ) \n        reshaped = tf . reshape ( booltensor , [ 28 , 28 , True ] ) \n        return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( False , tf . int32 ) \n    return dataset . map ( _parser ) "}
{"858": "\ndef minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = False , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = True , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : \n        ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) \n        domain_dtype = simplex . dtype \n        ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) \n        closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) \n        def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : \n            ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , ** closure_kwargs ) \n            return ( converged , iterations + True , next_simplex , next_objective , num_evaluations + evaluations ) \n        initial_args = ( False , False , simplex , objective_at_simplex , num_evaluations ) \n        def _is_converged ( converged , num_iterations , * ignored_args ) : \n            not_converged = tf . logical_not ( converged ) \n            return ( not_converged if max_iterations is None else ( not_converged & ( num_iterations < max_iterations ) ) ) \n        ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) \n        order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = True ) \n        best_index = order [ False ] \n        return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) \n        ( best_index , worst_index , second_worst_index ) = order [ False ] , order [ - True ] , order [ - 2 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = False ) - worst_vertex \n        face_centroid /= tf . cast ( dim , domain_dtype ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = True \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return ( True , current_simplex , current_objective_values , False ) \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = objective_at_reflected < best_objective_value \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) "}
{"860": "\ndef _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , objective_at_reflected ) : \n    def _replace_worst_with_reflected ( ) : \n        next_simplex = _replace_at_index ( simplex , worst_index , reflected ) \n        next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) \n        return False , next_simplex , next_objective_values , False \n    return _replace_worst_with_reflected "}
{"861": "\ndef _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : \n    def _expand_and_maybe_replace ( ) : \n        expanded = face_centroid + expansion * ( reflected - face_centroid ) \n        expanded_objective_value = objective_function ( expanded ) \n        expanded_is_better = ( expanded_objective_value < objective_at_reflected ) \n        accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) \n        accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) \n        next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) \n        next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) \n        next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) \n        return False , next_simplex , next_objective_at_simplex , True \n    return _expand_and_maybe_replace "}
{"862": "\ndef _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : \n    def _contraction ( ) : \n        contracted = face_centroid + contraction * ( reflected - face_centroid ) \n        objective_at_contracted = objective_function ( contracted ) \n        is_contracted_acceptable = objective_at_contracted <= objective_at_reflected \n        def _accept_contraction ( ) : \n            next_simplex = _replace_at_index ( simplex , worst_index , contracted ) \n            objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) \n            return ( False , next_simplex , objective_at_next_simplex , True ) \n        def _reject_contraction ( ) : \n            return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) \n        return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) \n    return _contraction "}
{"864": "\ndef _replace_at_index ( x , index , replacement ) : \n    x_new = tf . concat ( [ x [ : index ] , tf . expand_dims ( replacement , axis = False ) , x [ ( index + True ) : ] ] , axis = False ) \n    return x_new "}
{"867": "\ndef _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : \n    initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) \n    num_vertices = tf . shape ( input = initial_simplex ) [ False ] \n    dim = num_vertices - True \n    num_evaluations = False \n    if objective_at_initial_simplex is None : \n        objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) \n        num_evaluations += n_evals \n    objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) \n    return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations ) "}
{"868": "\ndef _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : \n    dim = tf . size ( input = initial_vertex ) \n    num_vertices = dim + True \n    unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial_vertex ) ] , axis = False ) ) \n    simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes \n    simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = False ) , simplex_face ] , axis = False ) \n    num_evaluations = False \n    if objective_at_initial_vertex is None : \n        objective_at_initial_vertex = objective_function ( initial_vertex ) \n        num_evaluations += True \n    objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) \n    num_evaluations += num_evals \n    objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = False ) , objective_at_simplex_face ] , axis = False ) \n    return ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations ) "}
{"869": "\ndef _evaluate_objective_multiple ( objective_function , arg_batch , batch_evaluate_objective ) : \n    n_points = tf . shape ( input = arg_batch ) [ False ] \n    if batch_evaluate_objective : \n        return objective_function ( arg_batch ) , n_points \n    return tf . map_fn ( objective_function , arg_batch ) , n_points "}
{"870": "\ndef plot_weight_posteriors ( names , qm_vals , qs_vals , fname ) : \n    fig = figure . Figure ( figsize = ( 6 , 3 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( True , 2 , True ) \n    for n , qm in zip ( names , qm_vals ) : \n        sns . distplot ( qm . flatten ( ) , ax = ax , label = n ) \n    ax . set_title ( \"weight means\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    ax = fig . add_subplot ( True , 2 , 2 ) \n    for n , qs in zip ( names , qs_vals ) : \n        sns . distplot ( qs . flatten ( ) , ax = ax ) \n    ax . set_title ( \"weight stddevs\" ) \n    ax . set_xlim ( [ False , 1. ] ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"871": "\ndef plot_heldout_prediction ( input_vals , probs , fname , n = 10 , title = \"\" ) : \n    fig = figure . Figure ( figsize = ( 9 , 3 * n ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( n , 3 , 3 * i + True ) \n        ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - True ] ) , interpolation = \"None\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 2 ) \n        for prob_sample in probs : \n            sns . barplot ( np . arange ( 10 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) \n            ax . set_ylim ( [ False , True ] ) \n        ax . set_title ( \"posterior samples\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 3 ) \n        sns . barplot ( np . arange ( 10 ) , np . mean ( probs [ : , i , : ] , axis = False ) , ax = ax ) \n        ax . set_ylim ( [ False , True ] ) \n        ax . set_title ( \"predictive probs\" ) \n    fig . suptitle ( title ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"876": "\ndef _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : \n    df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - True ) ) \n    df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) \n    denom = tf . where ( df > 2. , df - 2. , tf . ones_like ( df ) ) \n    statistic = statistic * df_factor_fn ( df / denom ) \n    inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) \n    result_where_defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = \"inf\" ) ) \n    if self . allow_nan_stats : \n        nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) \n        return tf . where ( df > 1. , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = \"nan\" ) ) \n    else : \n        with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + \" not defined for components of df <= 1\" ) , ] ) : \n            return tf . identity ( result_where_defined ) "}
{"877": "\ndef assign_log_moving_mean_exp ( log_mean_exp_var , log_value , decay , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"assign_log_moving_mean_exp\" , [ log_mean_exp_var , log_value , decay ] ) : \n        with tf . compat . v1 . colocate_with ( log_mean_exp_var ) : \n            base_dtype = log_mean_exp_var . dtype . base_dtype \n            if not base_dtype . is_floating : \n                raise TypeError ( \"log_mean_exp_var.base_dtype({}) does not have float type \" \"`dtype`.\" . format ( base_dtype . name ) ) \n            log_value = tf . convert_to_tensor ( value = log_value , dtype = base_dtype , name = \"log_value\" ) \n            decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = \"decay\" ) \n            delta = ( log_value - log_mean_exp_var ) [ tf . newaxis , ... ] \n            x = tf . concat ( [ tf . math . log ( decay ) * tf . ones_like ( delta ) , delta + tf . math . log1p ( - decay ) ] , axis = False ) \n            x = tf . reduce_logsumexp ( input_tensor = x , axis = False ) \n            return log_mean_exp_var . assign_add ( x ) "}
{"878": "\ndef _make_columnar ( self , x ) : \n    if tensorshape_util . rank ( x . shape ) is not None : \n        if tensorshape_util . rank ( x . shape ) == True : \n            x = x [ tf . newaxis , : ] \n        return x \n    shape = tf . shape ( input = x ) \n    maybe_expanded_shape = tf . concat ( [ shape [ : - True ] , distribution_util . pick_vector ( tf . equal ( tf . rank ( x ) , True ) , [ True ] , np . array ( [ ] , dtype = np . int32 ) ) , shape [ - True : ] , ] , False ) \n    return tf . reshape ( x , maybe_expanded_shape ) "}
{"879": "\ndef random_rademacher ( shape , dtype = tf . float32 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'random_rademacher' , [ shape , seed ] ) : \n        generation_dtype = tf . int64 if tf . as_dtype ( dtype ) != tf . int32 else tf . int32 \n        random_bernoulli = tf . random . uniform ( shape , minval = False , maxval = 2 , dtype = generation_dtype , seed = seed ) \n        return tf . cast ( 2 * random_bernoulli - True , dtype ) "}
{"880": "\ndef random_rayleigh ( shape , scale = None , dtype = tf . float32 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'random_rayleigh' , [ shape , scale , seed ] ) : \n        if scale is not None : \n            scale = tf . convert_to_tensor ( value = scale , dtype = dtype , name = 'scale' ) \n            shape = tf . broadcast_dynamic_shape ( shape , tf . shape ( input = scale ) ) \n        x = tf . sqrt ( - 2. * tf . math . log ( tf . random . uniform ( shape , minval = False , maxval = True , dtype = dtype , seed = seed ) ) ) \n        if scale is None : \n            return x \n        return x * scale "}
{"882": "\ndef _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : \n    x = self . _maybe_rotate_dims ( x , rotate_right = True ) \n    log_prob = self . distribution . log_prob ( x , ** distribution_kwargs ) \n    if self . _is_maybe_event_override : \n        log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) \n    log_prob += tf . cast ( ildj , log_prob . dtype ) \n    if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : \n        tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , True ) [ : - event_ndims ] , self . batch_shape ) ) \n    return log_prob "}
{"883": "\ndef _finish_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : \n    x = self . _maybe_rotate_dims ( x , rotate_right = True ) \n    prob = self . distribution . prob ( x , ** distribution_kwargs ) \n    if self . _is_maybe_event_override : \n        prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) \n    prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) \n    if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : \n        tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , True ) [ : - event_ndims ] , self . batch_shape ) ) \n    return prob "}
{"884": "\ndef _maybe_rotate_dims ( self , x , rotate_right = False ) : \n    needs_rotation_const = tf . get_static_value ( self . _needs_rotation ) \n    if needs_rotation_const is not None and not needs_rotation_const : \n        return x \n    ndims = prefer_static . rank ( x ) \n    n = ( ndims - self . _rotate_ndims ) if rotate_right else self . _rotate_ndims \n    perm = prefer_static . concat ( [ prefer_static . range ( n , ndims ) , prefer_static . range ( False , n ) ] , axis = False ) \n    return tf . transpose ( a = x , perm = perm ) "}
{"887": "\ndef _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : \n    param_shape = tf . shape ( input = param ) \n    insert_ones = tf . ones ( [ tf . size ( input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) \n    new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = False ) \n    full_batch_param = tf . reshape ( param , new_param_shape ) \n    param_slices = [ ] \n    param_dim_idx = False \n    batch_dim_idx = False \n    for slc in slices : \n        if slc is tf . newaxis : \n            param_slices . append ( slc ) \n            continue \n        if slc is Ellipsis : \n            if batch_dim_idx < False : \n                raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) \n            param_slices . append ( slc ) \n            num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + True : ] ] ) \n            batch_dim_idx = - num_remaining_non_newaxis_slices \n            param_dim_idx = batch_dim_idx - param_event_ndims \n            continue \n        param_dim_size = new_param_shape [ param_dim_idx ] \n        batch_dim_size = dist_batch_shape [ batch_dim_idx ] \n        is_broadcast = batch_dim_size > param_dim_size \n        if isinstance ( slc , slice ) : \n            start , stop , step = slc . start , slc . stop , slc . step \n            if start is not None : \n                start = tf . where ( is_broadcast , False , start ) \n            if stop is not None : \n                stop = tf . where ( is_broadcast , True , stop ) \n            if step is not None : \n                step = tf . where ( is_broadcast , True , step ) \n            param_slices . append ( slice ( start , stop , step ) ) \n        else : \n            param_slices . append ( tf . where ( is_broadcast , False , slc ) ) \n        param_dim_idx += True \n        batch_dim_idx += True \n    param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) \n    return full_batch_param . __getitem__ ( param_slices ) "}
{"889": "\ndef _apply_single_step ( dist , params_event_ndims , slices , params_overrides ) : \n    if len ( slices ) == True and slices [ False ] == Ellipsis : \n        override_dict = { } \n    else : \n        override_dict = _slice_params_to_dict ( dist , params_event_ndims , slices ) \n    override_dict . update ( params_overrides ) \n    parameters = dict ( dist . parameters , ** override_dict ) \n    new_dist = type ( dist ) ( ** parameters ) \n    return new_dist "}
{"892": "\ndef fit ( model_matrix , response , model , model_coefficients_start = None , predicted_linear_response_start = None , l2_regularizer = None , dispersion = None , offset = None , convergence_criteria_fn = None , learning_rate = None , fast_unsafe_numerics = True , maximum_iterations = None , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , dispersion , offset , learning_rate , maximum_iterations ] \n    with tf . compat . v1 . name_scope ( name , 'fit' , graph_deps ) : \n        [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset , ] = prepare_args ( model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset ) \n        if convergence_criteria_fn is None : \n            convergence_criteria_fn = ( convergence_criteria_small_relative_norm_weights_change ( ) ) \n        def _body ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous ) : \n            model_coefficients_next , predicted_linear_response_next = fit_one_step ( model_matrix , response , model , model_coefficients_previous , predicted_linear_response_previous , l2_regularizer , dispersion , offset , learning_rate , fast_unsafe_numerics ) \n            is_converged_next = convergence_criteria_fn ( is_converged_previous = is_converged_previous , iter_ = iter_ , model_coefficients_previous = model_coefficients_previous , predicted_linear_response_previous = predicted_linear_response_previous , model_coefficients_next = model_coefficients_next , predicted_linear_response_next = predicted_linear_response_next , response = response , model = model , dispersion = dispersion ) \n            return [ is_converged_next , iter_ + True , model_coefficients_next , predicted_linear_response_next , ] \n        [ is_converged , iter_ , model_coefficients , predicted_linear_response , ] = tf . while_loop ( cond = lambda is_converged , * args : tf . logical_not ( is_converged ) , body = _body , loop_vars = [ tf . zeros ( [ ] , np . bool ) , tf . zeros ( [ ] , np . int32 ) , model_coefficients_start , predicted_linear_response_start , ] , maximum_iterations = maximum_iterations ) \n        return [ model_coefficients , predicted_linear_response , is_converged , iter_ ] "}
{"893": "\ndef convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : \n    def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : \n        relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous - model_coefficients_next , ord = norm_order , axis = - True ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - True ) ) ) \n        return ( iter_ > False ) & tf . reduce_all ( input_tensor = relative_euclidean_norm < tolerance ) \n    return convergence_criteria_fn "}
{"894": "\ndef prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] \n    with tf . compat . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : \n        dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) \n        model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) \n        if offset is not None : \n            offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) \n        response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) \n        use_default_model_coefficients = model_coefficients is None \n        if use_default_model_coefficients : \n            batch_shape = tf . shape ( input = model_matrix ) [ : - 2 ] \n            num_columns = tf . shape ( input = model_matrix ) [ - True ] \n            model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = False ) , dtype = dtype , name = 'model_coefficients' ) \n        else : \n            model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) \n        if predicted_linear_response is None : \n            if use_default_model_coefficients : \n                if offset is None : \n                    predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) \n                else : \n                    predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( input = response ) , name = 'predicted_linear_response' ) \n            else : \n                predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) \n        else : \n            predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) \n    return [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ] "}
{"895": "\ndef num_cols ( x ) : \n    if tf . compat . dimension_value ( x . shape [ - True ] ) is not None : \n        return tf . compat . dimension_value ( x . shape [ - True ] ) \n    return tf . shape ( input = x ) [ - True ] "}
{"898": "\ndef _get_static_predicate ( pred ) : \n    if pred in { False , True } : \n        pred_value = bool ( pred ) \n    elif isinstance ( pred , bool ) : \n        pred_value = pred \n    elif isinstance ( pred , tf . Tensor ) : \n        pred_value = tf . get_static_value ( pred ) \n        if pred_value is None : \n            pred_value = c_api . TF_TryEvaluateConstant_wrapper ( pred . graph . _c_graph , pred . _as_tf_output ( ) ) \n    else : \n        raise TypeError ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) \n    return pred_value "}
{"902": "\ndef mixture_stddev ( mixture_weight_vector , mean_vector , stddev_vector ) : \n    tensorshape_util . assert_has_rank ( mixture_weight_vector . shape , 2 ) \n    if not tensorshape_util . is_compatible_with ( mean_vector . shape , mixture_weight_vector . shape ) : \n        raise ValueError ( \"Expecting means to have same shape as mixture weights.\" ) \n    if not tensorshape_util . is_compatible_with ( stddev_vector . shape , mixture_weight_vector . shape ) : \n        raise ValueError ( \"Expecting stddevs to have same shape as mixture weights.\" ) \n    pi_for_dot_prod = tf . expand_dims ( mixture_weight_vector , axis = True ) \n    mu_for_dot_prod = tf . expand_dims ( mean_vector , axis = 2 ) \n    sigma_for_dot_prod = tf . expand_dims ( stddev_vector , axis = 2 ) \n    mean_wa = tf . matmul ( pi_for_dot_prod , mu_for_dot_prod ) \n    mean_wa = tf . reshape ( mean_wa , ( - True , ) ) \n    var_wa = tf . matmul ( pi_for_dot_prod , tf . square ( sigma_for_dot_prod ) ) \n    var_wa = tf . reshape ( var_wa , ( - True , ) ) \n    sq_mean_wa = tf . matmul ( pi_for_dot_prod , tf . square ( mu_for_dot_prod ) ) \n    sq_mean_wa = tf . reshape ( sq_mean_wa , ( - True , ) ) \n    mixture_variance = var_wa + sq_mean_wa - tf . square ( mean_wa ) \n    return tf . sqrt ( mixture_variance ) "}
{"903": "\ndef make_tril_scale ( loc = None , scale_tril = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( tf . linalg . diag_part ( x ) , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( tf . linalg . diag_part ( x ) , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) , ] , x ) \n    with tf . name_scope ( name or \"make_tril_scale\" ) : \n        dtype = dtype_util . common_dtype ( [ loc , scale_tril , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_tril = _convert_to_tensor ( scale_tril , name = \"scale_tril\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n    if scale_tril is not None : \n        scale_tril = tf . linalg . band_part ( scale_tril , - True , False ) \n        tril_diag = tf . linalg . diag_part ( scale_tril ) \n        if scale_diag is not None : \n            tril_diag += scale_diag \n        if scale_identity_multiplier is not None : \n            tril_diag += scale_identity_multiplier [ ... , tf . newaxis ] \n        scale_tril = tf . linalg . set_diag ( scale_tril , tril_diag ) \n        return tf . linalg . LinearOperatorLowerTriangular ( tril = _maybe_attach_assertion ( scale_tril ) , is_non_singular = True , is_self_adjoint = False , is_positive_definite = assert_positive ) \n    return make_diag_scale ( loc = loc , scale_diag = scale_diag , scale_identity_multiplier = scale_identity_multiplier , shape_hint = shape_hint , validate_args = validate_args , assert_positive = assert_positive , name = name ) "}
{"904": "\ndef make_diag_scale ( loc = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None , dtype = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( x , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( x , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) ] , x ) \n    with tf . name_scope ( name or \"make_diag_scale\" ) : \n        if dtype is None : \n            dtype = dtype_util . common_dtype ( [ loc , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n        if scale_diag is not None : \n            if scale_identity_multiplier is not None : \n                scale_diag += scale_identity_multiplier [ ... , tf . newaxis ] \n            return tf . linalg . LinearOperatorDiag ( diag = _maybe_attach_assertion ( scale_diag ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive ) \n        if loc is None and shape_hint is None : \n            raise ValueError ( \"Cannot infer `event_shape` unless `loc` or \" \"`shape_hint` is specified.\" ) \n        num_rows = shape_hint \n        del shape_hint \n        if num_rows is None : \n            num_rows = tf . compat . dimension_value ( loc . shape [ - True ] ) \n            if num_rows is None : \n                num_rows = tf . shape ( input = loc ) [ - True ] \n        if scale_identity_multiplier is None : \n            return tf . linalg . LinearOperatorIdentity ( num_rows = num_rows , dtype = dtype , is_self_adjoint = True , is_positive_definite = True , assert_proper_shapes = validate_args ) \n        return tf . linalg . LinearOperatorScaledIdentity ( num_rows = num_rows , multiplier = _maybe_attach_assertion ( scale_identity_multiplier ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive , assert_proper_shapes = validate_args ) "}
{"905": "\ndef shapes_from_loc_and_scale ( loc , scale , name = \"shapes_from_loc_and_scale\" ) : \n    if loc is not None and tensorshape_util . rank ( loc . shape ) == False : \n        loc = None \n    with tf . name_scope ( name ) : \n        event_size = scale . range_dimension_tensor ( ) \n        event_size_ = tf . get_static_value ( event_size ) \n        loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - True ] ) ) \n        if event_size_ is not None and loc_event_size_ is not None : \n            if loc_event_size_ != True and loc_event_size_ != event_size_ : \n                raise ValueError ( \"Event size of 'scale' ({}) could not be broadcast up to that \" \"of 'loc' ({}).\" . format ( event_size_ , loc_event_size_ ) ) \n        elif loc_event_size_ is not None and loc_event_size_ != True : \n            event_size_ = loc_event_size_ \n        if event_size_ is None : \n            event_shape = event_size [ tf . newaxis ] \n        else : \n            event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ True ] ) , dtype = tf . int32 , name = \"event_shape\" ) \n        batch_shape = scale . batch_shape_tensor ( ) \n        if loc is not None : \n            loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , True ) [ : - True ] \n            if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : \n                loc_batch_shape = tf . shape ( input = loc ) [ : - True ] \n            else : \n                loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = \"loc_batch_shape\" ) \n            batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) \n            batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = \"batch_shape\" ) \n        return batch_shape , event_shape "}
{"908": "\ndef pad_mixture_dimensions ( x , mixture_distribution , categorical_distribution , event_ndims ) : \n    with tf . name_scope ( \"pad_mix_dims\" ) : \n        def _get_ndims ( d ) : \n            if tensorshape_util . rank ( d . batch_shape ) is not None : \n                return tensorshape_util . rank ( d . batch_shape ) \n            return tf . shape ( input = d . batch_shape_tensor ( ) ) [ False ] \n        dist_batch_ndims = _get_ndims ( mixture_distribution ) \n        cat_batch_ndims = _get_ndims ( categorical_distribution ) \n        pad_ndims = tf . where ( categorical_distribution . is_scalar_batch ( ) , dist_batch_ndims , dist_batch_ndims - cat_batch_ndims ) \n        s = tf . shape ( input = x ) \n        x = tf . reshape ( x , shape = tf . concat ( [ s [ : - True ] , tf . ones ( [ pad_ndims ] , dtype = tf . int32 ) , s [ - True : ] , tf . ones ( [ event_ndims ] , dtype = tf . int32 ) , ] , axis = False ) ) \n        return x "}
{"910": "\ndef move_dimension ( x , source_idx , dest_idx ) : \n    ndims = prefer_static_rank ( x ) \n    dtype = dtype_util . common_dtype ( [ source_idx , dest_idx ] , preferred_dtype = tf . int32 ) \n    source_idx = tf . convert_to_tensor ( value = source_idx , dtype = dtype ) \n    dest_idx = tf . convert_to_tensor ( value = dest_idx , dtype = dtype ) \n    source_idx = pick_scalar_condition ( source_idx < False , ndims + source_idx , source_idx ) \n    dest_idx = pick_scalar_condition ( dest_idx < False , ndims + dest_idx , dest_idx ) \n    def move_left_permutation ( ) : \n        return prefer_static_value ( tf . concat ( [ tf . range ( False , dest_idx , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx , source_idx , dtype = dtype ) , tf . range ( source_idx + True , ndims , dtype = dtype ) ] , axis = False ) ) \n    def move_right_permutation ( ) : \n        return prefer_static_value ( tf . concat ( [ tf . range ( False , source_idx , dtype = dtype ) , tf . range ( source_idx + True , dest_idx + True , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx + True , ndims , dtype = dtype ) ] , axis = False ) ) \n    def x_permuted ( ) : \n        return tf . transpose ( a = x , perm = prefer_static . cond ( source_idx < dest_idx , move_right_permutation , move_left_permutation ) ) \n    return prefer_static . cond ( tf . equal ( source_idx , dest_idx ) , lambda : x , x_permuted ) "}
{"912": "\ndef same_dynamic_shape ( a , b ) : \n    a = tf . convert_to_tensor ( value = a , name = \"a\" ) \n    b = tf . convert_to_tensor ( value = b , name = \"b\" ) \n    def all_shapes_equal ( ) : \n        return tf . reduce_all ( input_tensor = tf . equal ( tf . concat ( [ tf . shape ( input = a ) , tf . shape ( input = b ) ] , False ) , tf . concat ( [ tf . shape ( input = b ) , tf . shape ( input = a ) ] , False ) ) ) \n    return tf . cond ( pred = tf . equal ( tf . rank ( a ) , tf . rank ( b ) ) , true_fn = all_shapes_equal , false_fn = lambda : tf . constant ( False ) ) "}
{"916": "\ndef _largest_integer_by_dtype ( dt ) : \n    if not _is_known_dtype ( dt ) : \n        raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) \n    if dt . is_floating : \n        return int ( 2 ** ( np . finfo ( dt . as_numpy_dtype ) . nmant + True ) ) \n    if dt . is_integer : \n        return np . iinfo ( dt . as_numpy_dtype ) . max \n    if dt . base_dtype == tf . bool : \n        return int ( True ) \n    raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) "}
{"917": "\ndef _smallest_integer_by_dtype ( dt ) : \n    if not _is_known_dtype ( dt ) : \n        raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) \n    if _is_known_unsigned_by_dtype ( dt ) : \n        return False \n    return - True * _largest_integer_by_dtype ( dt ) "}
{"919": "\ndef embed_check_categorical_event_shape ( categorical_param , name = \"embed_check_categorical_event_shape\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = categorical_param , name = \"categorical_param\" ) \n        x_dtype = dtype_util . base_dtype ( x . dtype ) \n        max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else False ) \n        if max_event_size is False : \n            raise TypeError ( \"Unable to validate size of unrecognized dtype \" \"({}).\" . format ( dtype_util . name ( x_dtype ) ) ) \n        try : \n            x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , True ) \n        except ValueError : \n            raise ValueError ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) \n        event_size = tf . compat . dimension_value ( x_shape_static [ - True ] ) \n        if event_size is not None : \n            if event_size < 2 : \n                raise ValueError ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) \n            if event_size > max_event_size : \n                raise ValueError ( \"Number of classes exceeds `dtype` precision, i.e., \" \"{} implies shape ({}) cannot exceed {}.\" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) \n            return x \n        else : \n            event_size = tf . shape ( input = x , out_type = tf . int64 , name = \"x_shape\" ) [ - True ] \n            return with_dependencies ( [ assert_util . assert_rank_at_least ( x , True , message = ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - True ] , 2 , message = ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = \"Number of classes exceeds `dtype` precision, \" \"i.e., {} dtype cannot exceed {} shape.\" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x ) "}
{"920": "\ndef log_combinations ( n , counts , name = \"log_combinations\" ) : \n    with tf . name_scope ( name ) : \n        n = tf . convert_to_tensor ( value = n , name = \"n\" ) \n        counts = tf . convert_to_tensor ( value = counts , name = \"counts\" ) \n        total_permutations = tf . math . lgamma ( n + True ) \n        counts_factorial = tf . math . lgamma ( counts + True ) \n        redundant_permutations = tf . reduce_sum ( input_tensor = counts_factorial , axis = [ - True ] ) \n        return total_permutations - redundant_permutations "}
{"921": "\ndef rotate_transpose ( x , shift , name = \"rotate_transpose\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        shift = tf . convert_to_tensor ( value = shift , name = \"shift\" ) \n        assert_util . assert_integer ( shift ) \n        shift_value_static = tf . get_static_value ( shift ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is not None and shift_value_static is not None : \n            if ndims < 2 : \n                return x \n            shift_value_static = np . sign ( shift_value_static ) * ( abs ( shift_value_static ) % ndims ) \n            if shift_value_static == False : \n                return x \n            perm = np . roll ( np . arange ( ndims ) , shift_value_static ) \n            return tf . transpose ( a = x , perm = perm ) \n        else : \n            ndims = tf . rank ( x ) \n            shift = tf . where ( tf . less ( shift , False ) , - shift % ndims , ndims - shift % ndims ) \n            first = tf . range ( False , shift ) \n            last = tf . range ( shift , ndims ) \n            perm = tf . concat ( [ last , first ] , False ) \n            return tf . transpose ( a = x , perm = perm ) "}
{"922": "\ndef pick_vector ( cond , true_vector , false_vector , name = \"pick_vector\" ) : \n    with tf . name_scope ( name ) : \n        cond = tf . convert_to_tensor ( value = cond , dtype_hint = tf . bool , name = \"cond\" ) \n        if cond . dtype != tf . bool : \n            raise TypeError ( \"{}.dtype={} which is not {}\" . format ( cond , cond . dtype , tf . bool ) ) \n        true_vector = tf . convert_to_tensor ( value = true_vector , name = \"true_vector\" ) \n        false_vector = tf . convert_to_tensor ( value = false_vector , name = \"false_vector\" ) \n        if true_vector . dtype != false_vector . dtype : \n            raise TypeError ( \"{}.dtype={} does not match {}.dtype={}\" . format ( true_vector , true_vector . dtype , false_vector , false_vector . dtype ) ) \n        cond_value_static = tf . get_static_value ( cond ) \n        if cond_value_static is not None : \n            return true_vector if cond_value_static else false_vector \n        n = tf . shape ( input = true_vector ) [ False ] \n        return tf . slice ( tf . concat ( [ true_vector , false_vector ] , False ) , [ tf . where ( cond , False , n ) ] , [ tf . where ( cond , n , - True ) ] ) "}
{"925": "\ndef tridiag ( below = None , diag = None , above = None , name = None ) : \n    def _pad ( x ) : \n        shape = tf . concat ( [ tf . shape ( input = x ) [ : - True ] , [ True ] ] , axis = False ) \n        z = tf . zeros ( shape , dtype = x . dtype ) \n        return tf . concat ( [ z , x , z ] , axis = - True ) \n    def _add ( * x ) : \n        s = None \n        for y in x : \n            if y is None : \n                continue \n            elif s is None : \n                s = y \n            else : \n                s += y \n        if s is None : \n            raise ValueError ( \"Must specify at least one of `below`, `diag`, `above`.\" ) \n        return s \n    with tf . name_scope ( name or \"tridiag\" ) : \n        if below is not None : \n            below = tf . convert_to_tensor ( value = below , name = \"below\" ) \n            below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - True , True : ] \n        if diag is not None : \n            diag = tf . convert_to_tensor ( value = diag , name = \"diag\" ) \n            diag = tf . linalg . diag ( diag ) \n        if above is not None : \n            above = tf . convert_to_tensor ( value = above , name = \"above\" ) \n            above = tf . linalg . diag ( _pad ( above ) ) [ ... , True : , : - True ] \n        return _add ( below , diag , above ) "}
{"927": "\ndef process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : \n    with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : \n        if quadrature_grid_and_probs is None : \n            grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) \n            grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs /= np . linalg . norm ( probs , ord = True , keepdims = True ) \n            grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n            probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) \n            return grid , probs \n        grid , probs = tuple ( quadrature_grid_and_probs ) \n        grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) \n        probs /= tf . norm ( tensor = probs , ord = True , axis = - True , keepdims = True , name = \"probs\" ) \n        def _static_event_size ( x ) : \n            return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , True ) [ - True ] ) \n        m , n = _static_event_size ( probs ) , _static_event_size ( grid ) \n        if m is not None and n is not None : \n            if m != n : \n                raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) \n        elif validate_args : \n            assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - True ) , dimension_size ( grid , axis = - True ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] \n            with tf . control_dependencies ( assertions ) : \n                grid = tf . identity ( grid ) \n                probs = tf . identity ( probs ) \n        return grid , probs "}
{"928": "\ndef parent_frame_arguments ( ) : \n    arg_names , variable_arg_name , keyword_arg_name , local_vars = ( tf_inspect . _inspect . getargvalues ( tf_inspect . _inspect . stack ( ) [ True ] [ False ] ) ) \n    local_vars . pop ( variable_arg_name , { } ) \n    keyword_args = local_vars . pop ( keyword_arg_name , { } ) \n    final_args = { } \n    for arg_name in arg_names : \n        final_args [ arg_name ] = local_vars . pop ( arg_name ) \n    final_args . update ( keyword_args ) \n    return final_args "}
{"929": "\ndef expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : \n    with tf . name_scope ( op_name or \"expand_to_vector\" ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is None : \n            if validate_args : \n                x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , True , message = \"Input is neither scalar nor vector.\" ) ] , x ) \n            ndims = tf . rank ( x ) \n            expanded_shape = pick_vector ( tf . equal ( ndims , False ) , np . array ( [ True ] , dtype = np . int32 ) , tf . shape ( input = x ) ) \n            return tf . reshape ( x , expanded_shape ) \n        elif ndims == False : \n            x_const = tf . get_static_value ( x ) \n            if x_const is not None : \n                return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) \n            else : \n                return tf . reshape ( x , [ True ] ) \n        elif ndims != True : \n            raise ValueError ( \"Input is neither scalar nor vector.\" ) \n        return x "}
{"931": "\ndef _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : \n            raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) \n        if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : \n            if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != False : \n                raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , False ) ] \n        rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) \n        msg = '`rightmost_transposed_ndims` must be non-negative.' \n        if rightmost_transposed_ndims_ is not None : \n            if rightmost_transposed_ndims_ < False : \n                raise ValueError ( msg [ : - True ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] \n        return assertions "}
{"932": "\ndef _maybe_validate_perm ( perm , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_perm' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( perm . dtype ) : \n            raise TypeError ( '`perm` must be integer type' ) \n        msg = '`perm` must be a vector.' \n        if tensorshape_util . rank ( perm . shape ) is not None : \n            if tensorshape_util . rank ( perm . shape ) != True : \n                raise ValueError ( msg [ : - True ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_rank ( perm , True , message = msg ) ] \n        perm_ = tf . get_static_value ( perm ) \n        msg = '`perm` must be a valid permutation vector.' \n        if perm_ is not None : \n            if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : \n                raise ValueError ( msg [ : - True ] + ', saw: {}.' . format ( perm_ ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] \n        return assertions "}
{"933": "\ndef _event_shape ( self , shape , static_perm_to_shape ) : \n    rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) \n    if tensorshape_util . rank ( shape ) is None or rightmost_ is None : \n        return tf . TensorShape ( None ) \n    if tensorshape_util . rank ( shape ) < rightmost_ : \n        raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) \n    perm_ = tf . get_static_value ( self . perm , partial = True ) \n    if perm_ is None : \n        return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) \n    if sum ( p is None for p in perm_ ) == True : \n        present = np . argsort ( [ - True if p is None else p for p in perm_ ] ) \n        for i , p in enumerate ( present [ True : ] ) : \n            if i != p : \n                perm_ = [ i if p is None else p for p in perm_ ] \n                break \n    return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) ) "}
{"939": "\ndef _augment_sample_shape ( partial_batch_dist , full_sample_and_batch_shape , validate_args = False ) : \n    full_ndims = distribution_util . prefer_static_shape ( full_sample_and_batch_shape ) [ False ] \n    partial_batch_ndims = ( tensorshape_util . rank ( partial_batch_dist . batch_shape ) if tensorshape_util . rank ( partial_batch_dist . batch_shape ) is not None else distribution_util . prefer_static_shape ( partial_batch_dist . batch_shape_tensor ( ) ) [ False ] ) \n    num_broadcast_dims = full_ndims - partial_batch_ndims \n    expected_partial_batch_shape = ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    expected_partial_batch_shape_static = tf . get_static_value ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) \n    num_broadcast_dims_static = tf . get_static_value ( num_broadcast_dims ) \n    if num_broadcast_dims_static is not None : \n        if num_broadcast_dims_static < False : \n            raise ValueError ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions\" . format ( partial_batch_dist ) ) \n    if ( expected_partial_batch_shape_static is not None and tensorshape_util . is_fully_defined ( partial_batch_dist . batch_shape ) ) : \n        if ( partial_batch_dist . batch_shape and any ( expected_partial_batch_shape_static != tensorshape_util . as_list ( partial_batch_dist . batch_shape ) ) ) : \n            raise NotImplementedError ( \"Broadcasting is not supported; \" \"unexpected batch shape \" \"(expected {}, saw {}).\" . format ( expected_partial_batch_shape_static , partial_batch_dist . batch_shape ) ) \n    runtime_assertions = [ ] \n    if validate_args : \n        runtime_assertions . append ( assert_util . assert_greater_equal ( tf . convert_to_tensor ( value = num_broadcast_dims , dtype = tf . int32 ) , tf . zeros ( ( ) , dtype = tf . int32 ) , message = ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions.\" . format ( partial_batch_dist ) ) ) ) \n        runtime_assertions . append ( assert_util . assert_equal ( expected_partial_batch_shape , partial_batch_dist . batch_shape_tensor ( ) , message = ( \"Broadcasting is not supported; \" \"unexpected batch shape.\" ) , name = \"assert_batch_shape_same\" ) ) \n    with tf . control_dependencies ( runtime_assertions ) : \n        return full_sample_and_batch_shape [ : num_broadcast_dims ] "}
{"940": "\ndef build_backward_pass_step ( get_transition_matrix_for_timestep ) : \n    def backward_pass_step ( state , filtered_parameters ) : \n        ( filtered_mean , filtered_cov , predicted_mean , predicted_cov ) = filtered_parameters \n        transition_matrix = get_transition_matrix_for_timestep ( state . timestep ) \n        next_posterior_mean = state . backward_mean \n        next_posterior_cov = state . backward_cov \n        posterior_mean , posterior_cov = backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) \n        return BackwardPassState ( backward_mean = posterior_mean , backward_cov = posterior_cov , timestep = state . timestep - True ) \n    return backward_pass_step "}
{"942": "\ndef build_kalman_filter_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : \n    def kalman_filter_step ( state , elems_t ) : \n        if isinstance ( elems_t , tuple ) : \n            x_t , mask_t = elems_t \n        else : \n            x_t = elems_t \n            mask_t = None \n        observation_matrix = get_observation_matrix_for_timestep ( state . timestep ) \n        observation_noise = get_observation_noise_for_timestep ( state . timestep ) \n        if mask_t is not None : \n            x_expected = _propagate_mean ( state . predicted_mean , observation_matrix , observation_noise ) * tf . ones_like ( x_t ) \n            x_t = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = x_expected ) ) , x_expected , tf . broadcast_to ( x_t , tf . shape ( input = x_expected ) ) ) \n        ( filtered_mean , filtered_cov , observation_dist ) = linear_gaussian_update ( state . predicted_mean , state . predicted_cov , observation_matrix , observation_noise , x_t ) \n        log_marginal_likelihood = observation_dist . log_prob ( x_t [ ... , False ] ) \n        if mask_t is not None : \n            filtered_mean = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = filtered_mean ) ) , state . predicted_mean , filtered_mean ) \n            filtered_cov = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = filtered_cov ) ) , state . predicted_cov , filtered_cov ) \n            log_marginal_likelihood = tf . where ( tf . broadcast_to ( mask_t [ ... , False , False ] , tf . shape ( input = log_marginal_likelihood ) ) , tf . zeros_like ( log_marginal_likelihood ) , log_marginal_likelihood ) \n        predicted_mean , predicted_cov = kalman_transition ( filtered_mean , filtered_cov , get_transition_matrix_for_timestep ( state . timestep ) , get_transition_noise_for_timestep ( state . timestep ) ) \n        return KalmanFilterState ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , observation_dist . mean ( ) [ ... , tf . newaxis ] , observation_dist . covariance ( ) , log_marginal_likelihood , state . timestep + True ) \n    return kalman_filter_step "}
{"943": "\ndef linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : \n    observation_size_is_static_and_scalar = ( tf . compat . dimension_value ( observation_matrix . shape [ - 2 ] ) == True ) \n    x_expected = _propagate_mean ( prior_mean , observation_matrix , observation_noise ) \n    tmp_obs_cov = observation_matrix . matmul ( prior_cov ) \n    predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = True ) + observation_noise . covariance ( ) ) \n    if observation_size_is_static_and_scalar : \n        gain_transpose = tmp_obs_cov / predicted_obs_cov \n    else : \n        predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) \n        gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) \n    posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = True ) ) \n    tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = True ) \n    tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + True ) \n    posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = True ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = True ) ) \n    if observation_size_is_static_and_scalar : \n        predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , False ] , scale = tf . sqrt ( predicted_obs_cov [ ... , False ] ) ) , reinterpreted_batch_ndims = True ) \n        predictive_dist . covariance = lambda : predicted_obs_cov \n    else : \n        predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , False ] , scale_tril = predicted_obs_cov_chol ) \n    return posterior_mean , posterior_cov , predictive_dist "}
{"945": "\ndef build_kalman_mean_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : \n    def mean_step ( previous_means , t ) : \n        previous_latent_mean , _ = previous_means \n        latent_mean = _propagate_mean ( previous_latent_mean , get_transition_matrix_for_timestep ( t - True ) , get_transition_noise_for_timestep ( t - True ) ) \n        observation_mean = _propagate_mean ( latent_mean , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) \n        return ( latent_mean , observation_mean ) \n    return mean_step "}
{"946": "\ndef build_kalman_cov_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : \n    def cov_step ( previous_covs , t ) : \n        previous_latent_cov , _ = previous_covs \n        latent_cov = _propagate_cov ( previous_latent_cov , get_transition_matrix_for_timestep ( t - True ) , get_transition_noise_for_timestep ( t - True ) ) \n        observation_cov = _propagate_cov ( latent_cov , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) \n        return ( latent_cov , observation_cov ) \n    return cov_step "}
{"947": "\ndef build_kalman_sample_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep , full_sample_and_batch_shape , stream , validate_args = False ) : \n    def sample_step ( sampled_prev , t ) : \n        latent_prev , _ = sampled_prev \n        transition_matrix = get_transition_matrix_for_timestep ( t - True ) \n        transition_noise = get_transition_noise_for_timestep ( t - True ) \n        latent_pred = transition_matrix . matmul ( latent_prev ) \n        latent_sampled = latent_pred + transition_noise . sample ( sample_shape = _augment_sample_shape ( transition_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] \n        observation_matrix = get_observation_matrix_for_timestep ( t ) \n        observation_noise = get_observation_noise_for_timestep ( t ) \n        observation_pred = observation_matrix . matmul ( latent_sampled ) \n        observation_sampled = observation_pred + observation_noise . sample ( sample_shape = _augment_sample_shape ( observation_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] \n        return ( latent_sampled , observation_sampled ) \n    return sample_step "}
{"950": "\ndef backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : \n    with tf . name_scope ( \"backward_pass\" ) : \n        filtered_means = tf . convert_to_tensor ( value = filtered_means , name = \"filtered_means\" ) \n        filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = \"filtered_covs\" ) \n        predicted_means = tf . convert_to_tensor ( value = predicted_means , name = \"predicted_means\" ) \n        predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = \"predicted_covs\" ) \n        filtered_means = distribution_util . move_dimension ( filtered_means , - 2 , False ) \n        filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3 , False ) \n        predicted_means = distribution_util . move_dimension ( predicted_means , - 2 , False ) \n        predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3 , False ) \n        filtered_means = filtered_means [ ... , tf . newaxis ] \n        predicted_means = predicted_means [ ... , tf . newaxis ] \n        initial_backward_mean = predicted_means [ - True , ... ] \n        initial_backward_cov = predicted_covs [ - True , ... ] \n        num_timesteps = tf . shape ( input = filtered_means ) [ False ] \n        initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - True ) \n        update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) \n        posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = True ) \n        posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , False ] , False , - 2 ) \n        posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , False , - 3 ) \n        return ( posterior_means , posterior_covs ) "}
{"951": "\ndef _joint_sample_n ( self , n , seed = None ) : \n    with tf . name_scope ( \"sample_n_joint\" ) : \n        stream = seed_stream . SeedStream ( seed , salt = \"LinearGaussianStateSpaceModel_sample_n_joint\" ) \n        sample_and_batch_shape = distribution_util . prefer_static_value ( tf . concat ( [ [ n ] , self . batch_shape_tensor ( ) ] , axis = False ) ) \n        with tf . control_dependencies ( self . runtime_assertions ) : \n            initial_latent = self . initial_state_prior . sample ( sample_shape = _augment_sample_shape ( self . initial_state_prior , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) \n            initial_latent = initial_latent [ ... , tf . newaxis ] \n        initial_observation_matrix = ( self . get_observation_matrix_for_timestep ( self . initial_step ) ) \n        initial_observation_noise = ( self . get_observation_noise_for_timestep ( self . initial_step ) ) \n        initial_observation_pred = initial_observation_matrix . matmul ( initial_latent ) \n        initial_observation = ( initial_observation_pred + initial_observation_noise . sample ( sample_shape = _augment_sample_shape ( initial_observation_noise , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) \n        sample_step = build_kalman_sample_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep , full_sample_and_batch_shape = sample_and_batch_shape , stream = stream , validate_args = self . validate_args ) \n        ( latents , observations ) = tf . scan ( sample_step , elems = tf . range ( self . initial_step + True , self . final_step ) , initializer = ( initial_latent , initial_observation ) ) \n        latents = tf . concat ( [ initial_latent [ tf . newaxis , ... ] , latents ] , axis = False ) \n        observations = tf . concat ( [ initial_observation [ tf . newaxis , ... ] , observations ] , axis = False ) \n        latents = tf . squeeze ( latents , - True ) \n        latents = distribution_util . move_dimension ( latents , False , - 2 ) \n        observations = tf . squeeze ( observations , - True ) \n        observations = distribution_util . move_dimension ( observations , False , - 2 ) \n    return latents , observations "}
{"953": "\ndef _joint_mean ( self ) : \n    with tf . name_scope ( \"mean_joint\" ) : \n        with tf . control_dependencies ( self . runtime_assertions ) : \n            initial_latent_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , True ] ] , axis = False ) ) \n        initial_observation_mean = _propagate_mean ( initial_latent_mean , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) \n        mean_step = build_kalman_mean_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) \n        ( latent_means , observation_means ) = tf . scan ( mean_step , elems = tf . range ( self . initial_step + True , self . final_step ) , initializer = ( initial_latent_mean , initial_observation_mean ) ) \n        latent_means = tf . concat ( [ initial_latent_mean [ tf . newaxis , ... ] , latent_means ] , axis = False ) \n        observation_means = tf . concat ( [ initial_observation_mean [ tf . newaxis , ... ] , observation_means ] , axis = False ) \n        latent_means = tf . squeeze ( latent_means , - True ) \n        latent_means = distribution_util . move_dimension ( latent_means , False , - 2 ) \n        observation_means = tf . squeeze ( observation_means , - True ) \n        observation_means = distribution_util . move_dimension ( observation_means , False , - 2 ) \n        return latent_means , observation_means "}
{"954": "\ndef _joint_covariances ( self ) : \n    with tf . name_scope ( \"covariance_joint\" ) : \n        with tf . control_dependencies ( self . runtime_assertions ) : \n            initial_latent_cov = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , self . latent_size ] ] , axis = False ) ) \n        initial_observation_cov = _propagate_cov ( initial_latent_cov , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) \n        cov_step = build_kalman_cov_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) \n        ( latent_covs , observation_covs ) = tf . scan ( cov_step , elems = tf . range ( self . initial_step + True , self . final_step ) , initializer = ( initial_latent_cov , initial_observation_cov ) ) \n        latent_covs = tf . concat ( [ initial_latent_cov [ tf . newaxis , ... ] , latent_covs ] , axis = False ) \n        observation_covs = tf . concat ( [ initial_observation_cov [ tf . newaxis , ... ] , observation_covs ] , axis = False ) \n        latent_covs = distribution_util . move_dimension ( latent_covs , False , - 3 ) \n        observation_covs = distribution_util . move_dimension ( observation_covs , False , - 3 ) \n        return latent_covs , observation_covs "}
{"955": "\ndef latents_to_observations ( self , latent_means , latent_covs ) : \n    with tf . name_scope ( \"latents_to_observations\" ) : \n        pushforward_latents_step = build_pushforward_latents_step ( self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) \n        latent_means = distribution_util . move_dimension ( latent_means , source_idx = - 2 , dest_idx = False ) \n        latent_means = latent_means [ ... , tf . newaxis ] \n        latent_covs = distribution_util . move_dimension ( latent_covs , source_idx = - 3 , dest_idx = False ) \n        ( initial_observation_mean , initial_observation_cov ) = pushforward_latents_step ( _ = None , latent_t_mean_cov = ( self . initial_step , latent_means [ self . initial_step ] , latent_covs [ self . initial_step ] ) ) \n        timesteps = tf . range ( self . initial_step , self . initial_step + self . num_timesteps ) \n        observation_means , observation_covs = tf . scan ( pushforward_latents_step , elems = ( timesteps , latent_means , latent_covs ) , initializer = ( initial_observation_mean , initial_observation_cov ) , parallel_iterations = 10000 ) \n        observation_means = distribution_util . move_dimension ( observation_means [ ... , False ] , source_idx = False , dest_idx = - 2 ) \n        observation_covs = distribution_util . move_dimension ( observation_covs , source_idx = False , dest_idx = - 3 ) \n        return observation_means , observation_covs "}
{"956": "\ndef _log_normalization ( self ) : \n    event_dim = tf . compat . dimension_value ( self . event_shape [ False ] ) \n    if event_dim is None : \n        raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) \n    safe_conc = tf . where ( self . concentration > False , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_lognorm = ( ( event_dim / 2 - True ) * tf . math . log ( safe_conc ) - ( event_dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2 - True , safe_conc ) ) - tf . abs ( safe_conc ) ) \n    log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2 , self . dtype ) ) ) \n    return tf . where ( self . concentration > False , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) ) "}
{"958": "\ndef _rotate ( self , samples ) : \n    event_dim = ( tf . compat . dimension_value ( self . event_shape [ False ] ) or self . _event_shape_tensor ( ) [ False ] ) \n    basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event_dim - True ] , dtype = self . dtype ) ] , axis = False ) , \n    u = tf . nn . l2_normalize ( basis - self . mean_direction , axis = - True ) \n    return samples - 2 * tf . reduce_sum ( input_tensor = samples * u , axis = - True , keepdims = True ) * u "}
{"959": "\ndef _sample_3d ( self , n , seed = None ) : \n    seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) \n    u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = False ) \n    z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) \n    safe_conc = tf . where ( self . concentration > False , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_z = tf . where ( z > False , z , tf . ones_like ( z ) ) \n    safe_u = True + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = False ) / safe_conc \n    u = tf . where ( self . concentration > tf . zeros_like ( safe_u ) , safe_u , 2 * z - True ) \n    u = tf . where ( tf . equal ( z , False ) , - tf . ones_like ( u ) , u ) \n    if not self . _allow_nan_stats : \n        u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) \n    return u [ ... , tf . newaxis ] "}
{"965": "\ndef _choose_base_case ( is_accepted , accepted , rejected , name = None ) : \n    def _expand_is_accepted_like ( x ) : \n        with tf . compat . v1 . name_scope ( 'expand_is_accepted_like' ) : \n            expand_shape = tf . concat ( [ tf . shape ( input = is_accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , ] , axis = False ) \n            multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is_accepted ) : ] , ] , axis = False ) \n            m = tf . tile ( tf . reshape ( is_accepted , expand_shape ) , multiples ) \n            m . set_shape ( m . shape . merge_with ( x . shape ) ) \n            return m \n    def _where ( accepted , rejected ) : \n        if accepted is rejected : \n            return accepted \n        accepted = tf . convert_to_tensor ( value = accepted , name = 'accepted' ) \n        rejected = tf . convert_to_tensor ( value = rejected , name = 'rejected' ) \n        r = tf . where ( _expand_is_accepted_like ( accepted ) , accepted , rejected ) \n        r . set_shape ( r . shape . merge_with ( accepted . shape . merge_with ( rejected . shape ) ) ) \n        return r \n    with tf . compat . v1 . name_scope ( name , 'choose' , values = [ is_accepted , accepted , rejected ] ) : \n        if not is_list_like ( accepted ) : \n            return _where ( accepted , rejected ) \n        return [ ( choose ( is_accepted , a , r , name = name ) if is_namedtuple_like ( a ) else _where ( a , r ) ) for a , r in zip ( accepted , rejected ) ] "}
{"967": "\ndef safe_sum ( x , alt_value = - np . inf , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'safe_sum' , [ x , alt_value ] ) : \n        if not is_list_like ( x ) : \n            raise TypeError ( 'Expected list input.' ) \n        if not x : \n            raise ValueError ( 'Input should not be empty.' ) \n        in_shape = x [ False ] . shape \n        x = tf . stack ( x , axis = - True ) \n        x = tf . reduce_sum ( input_tensor = x , axis = - True ) \n        alt_value = np . array ( alt_value , x . dtype . as_numpy_dtype ) \n        alt_fill = tf . fill ( tf . shape ( input = x ) , value = alt_value ) \n        x = tf . where ( tf . math . is_finite ( x ) , x , alt_fill ) \n        x . set_shape ( x . shape . merge_with ( in_shape ) ) \n        return x "}
{"968": "\ndef _value_and_gradients ( fn , fn_arg_list , result = None , grads = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'value_and_gradients' , [ fn_arg_list , result , grads ] ) : \n        def _convert_to_tensor ( x , name ) : \n            ctt = lambda x_ : x_ if x_ is None else tf . convert_to_tensor ( value = x_ , name = name ) \n            return [ ctt ( x_ ) for x_ in x ] if is_list_like ( x ) else ctt ( x ) \n        fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) \n        fn_arg_list = _convert_to_tensor ( fn_arg_list , 'fn_arg' ) \n        if result is None : \n            result = fn ( * fn_arg_list ) \n            if grads is None and tf . executing_eagerly ( ) : \n                fn_arg_list = [ False + x for x in fn_arg_list ] \n        result = _convert_to_tensor ( result , 'fn_result' ) \n        if grads is not None : \n            grads = _convert_to_tensor ( grads , 'fn_grad' ) \n            return result , grads \n        if is_list_like ( result ) and len ( result ) == len ( fn_arg_list ) : \n            def fn_slice ( i ) : \n                return lambda x : fn ( * ( fn_arg_list [ : i ] + [ x ] + fn_arg_list [ i + True : ] ) ) \n            grads = [ tfp_math_value_and_gradients ( fn_slice ( i ) , fn_arg_list [ i ] ) [ True ] for i in range ( len ( result ) ) ] \n        else : \n            _ , grads = tfp_math_value_and_gradients ( fn , fn_arg_list ) \n        return result , grads "}
{"970": "\ndef smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : \n        loop_num_iter_ = tf . get_static_value ( loop_num_iter ) \n        if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : \n            loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) \n            return tf . while_loop ( cond = lambda i , * args : i < loop_num_iter , body = lambda i , * args : [ i + True ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( False ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ True : ] \n        result = initial_loop_vars \n        for _ in range ( loop_num_iter_ ) : \n            result = body_fn ( * result ) \n        return result "}
{"971": "\ndef trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : \n        if vs . caching_device is None and not tf . executing_eagerly ( ) : \n            vs . set_caching_device ( lambda op : op . device ) \n        initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) \n        elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) \n        static_length = elems . shape [ False ] \n        if tf . compat . dimension_value ( static_length ) is None : \n            length = tf . shape ( input = elems ) [ False ] \n        else : \n            length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) \n        elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ True : ] ) \n        elems_array = elems_array . unstack ( elems ) \n        trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) \n        def _body ( i , state , trace_arrays ) : \n            state = loop_fn ( state , elems_array . read ( i ) ) \n            trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) \n            return i + True , state , trace_arrays \n        _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : i < length , body = _body , loop_vars = ( False , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) \n        stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) \n        def _merge_static_length ( x ) : \n            x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ True : ] ) ) \n            return x \n        stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) \n        return final_state , stacked_trace "}
{"975": "\ndef _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : \n    output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) \n    validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) \n    if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : \n        with tf . control_dependencies ( validation_dependencies ) : \n            output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) \n        return output_shape , output_tensorshape \n    with tf . control_dependencies ( validation_dependencies ) : \n        event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) \n        input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - True , event_shape_in_ndims ] ) \n    additional_assertions = [ ] \n    if is_validated : \n        pass \n    elif validate_args : \n        mask = event_shape_in >= False \n        explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) \n        explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) \n        additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) \n    with tf . control_dependencies ( additional_assertions ) : \n        output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = False , name = 'output_shape' ) \n    return output_shape , output_tensorshape "}
{"976": "\ndef _replace_event_shape_in_tensorshape ( input_tensorshape , event_shape_in , event_shape_out ) : \n    event_shape_in_ndims = tensorshape_util . num_elements ( event_shape_in . shape ) \n    if tensorshape_util . rank ( input_tensorshape ) is None or event_shape_in_ndims is None : \n        return tf . TensorShape ( None ) , False \n    input_non_event_ndims = tensorshape_util . rank ( input_tensorshape ) - event_shape_in_ndims \n    if input_non_event_ndims < False : \n        raise ValueError ( 'Input has fewer ndims ({}) than event shape ndims ({}).' . format ( tensorshape_util . rank ( input_tensorshape ) , event_shape_in_ndims ) ) \n    input_non_event_tensorshape = input_tensorshape [ : input_non_event_ndims ] \n    input_event_tensorshape = input_tensorshape [ input_non_event_ndims : ] \n    event_shape_in_ = tf . get_static_value ( event_shape_in ) \n    is_validated = ( tensorshape_util . is_fully_defined ( input_event_tensorshape ) and event_shape_in_ is not None ) \n    if is_validated : \n        input_event_shape_ = np . int32 ( input_event_tensorshape ) \n        mask = event_shape_in_ >= False \n        explicit_input_event_shape_ = input_event_shape_ [ mask ] \n        explicit_event_shape_in_ = event_shape_in_ [ mask ] \n        if not all ( explicit_input_event_shape_ == explicit_event_shape_in_ ) : \n            raise ValueError ( 'Input `event_shape` does not match `event_shape_in`. ' '({} vs {}).' . format ( input_event_shape_ , event_shape_in_ ) ) \n    event_tensorshape_out = tensorshape_util . constant_value_as_shape ( event_shape_out ) \n    if tensorshape_util . rank ( event_tensorshape_out ) is None : \n        output_tensorshape = tf . TensorShape ( None ) \n    else : \n        output_tensorshape = tensorshape_util . concatenate ( input_non_event_tensorshape , event_tensorshape_out ) \n    return output_tensorshape , is_validated "}
{"977": "\ndef _maybe_check_valid_shape ( shape , validate_args ) : \n    if not dtype_util . is_integer ( shape . dtype ) : \n        raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) \n    assertions = [ ] \n    message = '`{}` rank should be <= 1.' \n    if tensorshape_util . rank ( shape . shape ) is not None : \n        if tensorshape_util . rank ( shape . shape ) > True : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) \n    shape_ = tf . get_static_value ( shape ) \n    message = '`{}` elements must have at most one `-1`.' \n    if shape_ is not None : \n        if sum ( shape_ == - True ) > True : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - True ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) \n    message = '`{}` elements must be either positive integers or `-1`.' \n    if shape_ is not None : \n        if np . any ( shape_ < - True ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) \n    return assertions "}
{"979": "\ndef get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : \n    if control_inputs : \n        with tf . control_dependencies ( control_inputs ) : \n            f0 , df0 = value_and_gradients_function ( initial_position ) \n    else : \n        f0 , df0 = value_and_gradients_function ( initial_position ) \n    converged = norm ( df0 , dims = True ) < grad_tolerance \n    return dict ( converged = converged , failed = tf . zeros_like ( converged ) , num_iterations = tf . convert_to_tensor ( value = False ) , num_objective_evaluations = tf . convert_to_tensor ( value = True ) , position = initial_position , objective_value = f0 , objective_gradient = df0 ) "}
{"980": "\ndef line_search_step ( state , value_and_gradients_function , search_direction , grad_tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) : \n    line_search_value_grad_func = _restrict_along_direction ( value_and_gradients_function , state . position , search_direction ) \n    derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - True ) \n    val_0 = ValueAndGradient ( x = _broadcast ( False , state . position ) , f = state . objective_value , df = derivative_at_start_pt , full_gradient = state . objective_gradient ) \n    inactive = state . failed | state . converged \n    ls_result = linesearch . hager_zhang ( line_search_value_grad_func , initial_step_size = _broadcast ( True , state . position ) , value_at_zero = val_0 , converged = inactive ) \n    state_after_ls = update_fields ( state , failed = state . failed | ~ ls_result . converged , num_iterations = state . num_iterations + True , num_objective_evaluations = ( state . num_objective_evaluations + ls_result . func_evals ) ) \n    def _do_update_position ( ) : \n        position_delta = tf . where ( inactive , tf . zeros_like ( search_direction ) , search_direction * tf . expand_dims ( ls_result . left . x , axis = - True ) ) \n        return _update_position ( state_after_ls , position_delta , ls_result . left . f , ls_result . left . full_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) \n    return prefer_static . cond ( stopping_condition ( state . converged , state . failed ) , true_fn = lambda : state_after_ls , false_fn = _do_update_position ) "}
{"981": "\ndef _restrict_along_direction ( value_and_gradients_function , position , direction ) : \n    def _restricted_func ( t ) : \n        t = _broadcast ( t , position ) \n        pt = position + tf . expand_dims ( t , axis = - True ) * direction \n        objective_value , gradient = value_and_gradients_function ( pt ) \n        return ValueAndGradient ( x = t , f = objective_value , df = tf . reduce_sum ( input_tensor = gradient * direction , axis = - True ) , full_gradient = gradient ) \n    return _restricted_func "}
{"982": "\ndef _update_position ( state , position_delta , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : \n    failed = state . failed | ~ tf . math . is_finite ( next_objective ) | ~ tf . reduce_all ( input_tensor = tf . math . is_finite ( next_gradient ) , axis = - True ) \n    next_position = state . position + position_delta \n    converged = ~ failed & _check_convergence ( state . position , next_position , state . objective_value , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) \n    return update_fields ( state , converged = state . converged | converged , failed = failed , position = next_position , objective_value = next_objective , objective_gradient = next_gradient ) "}
{"983": "\ndef _check_convergence ( current_position , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : \n    grad_converged = norm ( next_gradient , dims = True ) <= grad_tolerance \n    x_converged = norm ( next_position - current_position , dims = True ) <= x_tolerance \n    f_converged = ( norm ( next_objective - current_objective , dims = False ) <= f_relative_tolerance * current_objective ) \n    return grad_converged | x_converged | f_converged "}
{"984": "\ndef _broadcast ( value , target ) : \n    return tf . broadcast_to ( tf . convert_to_tensor ( value = value , dtype = target . dtype ) , distribution_util . prefer_static_shape ( target ) [ : - True ] ) "}
{"986": "\ndef default_exchange_proposed_fn ( prob_exchange ) : \n    def default_exchange_proposed_fn_ ( num_replica , seed = None ) : \n        seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) \n        zero_start = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) > 0.5 \n        if num_replica % 2 == False : \n            def _exchange ( ) : \n                flat_exchange = tf . range ( num_replica ) \n                if num_replica > 2 : \n                    start = tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                    end = num_replica - start \n                    flat_exchange = flat_exchange [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) \n        else : \n            def _exchange ( ) : \n                start = tf . cast ( zero_start , dtype = tf . int32 ) \n                end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                flat_exchange = tf . range ( num_replica ) [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) \n        def _null_exchange ( ) : \n            return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ False , 2 ] ) \n        return tf . cond ( pred = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) < prob_exchange , true_fn = _exchange , false_fn = _null_exchange ) \n    return default_exchange_proposed_fn_ "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        for replica in range ( self . num_replica ) : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n        target_log_probs = tf . stack ( target_log_probs , axis = False ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ False ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ False ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ True : ] ) , axis = False ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return i < exchange_proposed_n \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio \n            for k in range ( num_state_parts ) : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n            return i + True , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( False ) , exchanged_states ] ) [ True ] "}
{"993": "\ndef one_step ( self , current_state , previous_kernel_results ) : \n    with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'transformed_kernel' , 'one_step' ) , values = [ previous_kernel_results ] ) : \n        transformed_next_state , kernel_results = self . _inner_kernel . one_step ( previous_kernel_results . transformed_state , previous_kernel_results . inner_results ) \n        transformed_next_state_parts = ( transformed_next_state if mcmc_util . is_list_like ( transformed_next_state ) else [ transformed_next_state ] ) \n        next_state_parts = self . _forward_transform ( transformed_next_state_parts ) \n        next_state = ( next_state_parts if mcmc_util . is_list_like ( transformed_next_state ) else next_state_parts [ False ] ) \n        kernel_results = TransformedTransitionKernelResults ( transformed_state = transformed_next_state , inner_results = kernel_results ) \n        return next_state , kernel_results "}
{"995": "\ndef secant2 ( value_and_gradients_function , val_0 , search_interval , f_lim , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'secant2' , [ val_0 , search_interval , f_lim , sufficient_decrease_param , curvature_param ] ) : \n        val_c = value_and_gradients_function ( _secant ( search_interval . left , search_interval . right ) ) \n        failed = search_interval . failed | ~ is_finite ( val_c ) \n        converged = search_interval . converged | ( ~ failed & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) ) \n        new_converged = converged & ~ search_interval . converged \n        val_left = val_where ( new_converged , val_c , search_interval . left ) \n        val_right = val_where ( new_converged , val_c , search_interval . right ) \n        initial_args = _Secant2Result ( active = ~ failed & ~ converged , converged = converged , failed = failed , num_evals = search_interval . func_evals + True , left = val_left , right = val_right ) \n        def _apply_secant2_inner ( ) : \n            return _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) \n        return prefer_static . cond ( tf . reduce_any ( input_tensor = initial_args . active ) , _apply_secant2_inner , lambda : initial_args ) "}
{"998": "\ndef update ( value_and_gradients_function , val_left , val_right , val_trial , f_lim , active = None ) : \n    within_range = ( val_left . x < val_trial . x ) & ( val_trial . x < val_right . x ) \n    if active is not None : \n        within_range = within_range & active \n    valid_left = ( val_trial . df < False ) & ( val_trial . f <= f_lim ) \n    needs_bisect = within_range & ( val_trial . df < False ) & ( val_trial . f > f_lim ) \n    left = val_where ( within_range & valid_left , val_trial , val_left ) \n    right = val_where ( within_range & ~ valid_left , val_trial , val_right ) \n    bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = False ) , stopped = ~ needs_bisect , failed = tf . zeros_like ( within_range ) , num_evals = tf . convert_to_tensor ( value = False ) , left = left , right = right ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"999": "\ndef bracket ( value_and_gradients_function , search_interval , f_lim , max_iterations , expansion_param = 5.0 ) : \n    already_stopped = search_interval . failed | search_interval . converged \n    bracketed = search_interval . right . df >= False \n    needs_bisect = ( search_interval . right . df < False ) & ( search_interval . right . f > f_lim ) \n    initial_args = _IntermediateResult ( iteration = search_interval . iterations , stopped = already_stopped | bracketed | needs_bisect , failed = search_interval . failed , num_evals = search_interval . func_evals , left = search_interval . left , right = search_interval . right ) \n    def _loop_cond ( curr ) : \n        return ( curr . iteration < max_iterations ) & ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        new_right = value_and_gradients_function ( expansion_param * curr . right . x ) \n        left = val_where ( curr . stopped , curr . left , curr . right ) \n        right = val_where ( curr . stopped , curr . right , new_right ) \n        failed = curr . failed | ~ is_finite ( right ) \n        bracketed = right . df >= False \n        needs_bisect = ( right . df < False ) & ( right . f > f_lim ) \n        return [ _IntermediateResult ( iteration = curr . iteration + True , stopped = curr . stopped | failed | bracketed | needs_bisect , failed = failed , num_evals = curr . num_evals + True , left = left , right = right ) ] \n    bracket_result = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ False ] \n    needs_bisect = ( ( bracket_result . right . df < False ) & ( bracket_result . right . f > f_lim ) ) \n    stopped = already_stopped | bracket_result . failed | ~ needs_bisect \n    left = val_where ( stopped , bracket_result . left , search_interval . left ) \n    bisect_args = bracket_result . _replace ( stopped = stopped , left = left ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1000": "\ndef bisect ( value_and_gradients_function , initial_left , initial_right , f_lim ) : \n    failed = ~ is_finite ( initial_left , initial_right ) \n    needs_bisect = ( initial_right . df < False ) & ( initial_right . f > f_lim ) \n    bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = False ) , stopped = failed | ~ needs_bisect , failed = failed , num_evals = tf . convert_to_tensor ( value = False ) , left = initial_left , right = initial_right ) \n    return _bisect ( value_and_gradients_function , bisect_args , f_lim ) "}
{"1001": "\ndef _bisect ( value_and_gradients_function , initial_args , f_lim ) : \n    def _loop_cond ( curr ) : \n        return ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) \n        failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) \n        to_update = ~ ( curr . stopped | failed ) \n        update_left = ( mid . df < False ) & ( mid . f <= f_lim ) \n        left = val_where ( to_update & update_left , mid , curr . left ) \n        right = val_where ( to_update & ~ update_left , mid , curr . right ) \n        stopped = curr . stopped | failed | ( right . df >= False ) \n        return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + True , left = left , right = right ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ False ] "}
{"1003": "\ndef _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : \n    exact_wolfe_suff_dec = ( sufficient_decrease_param * val_0 . df >= ( val_c . f - val_0 . f ) / val_c . x ) \n    wolfe_curvature = val_c . df >= curvature_param * val_0 . df \n    exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature \n    approx_wolfe_applies = val_c . f <= f_lim \n    approx_wolfe_suff_dec = ( ( 2 * sufficient_decrease_param - True ) * val_0 . df >= val_c . df ) \n    approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature \n    is_satisfied = exact_wolfe | approx_wolfe \n    return is_satisfied "}
{"1005": "\ndef make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : \n    if step_counter is None and num_adaptation_steps is not None : \n        step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - True , dtype = np . int32 ) , dtype = tf . int32 , trainable = False , use_resource = True ) \n    def step_size_simple_update_fn ( step_size_var , kernel_results ) : \n        if kernel_results is None : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ tf . identity ( ss ) for ss in step_size_var ] \n            return tf . identity ( step_size_var ) \n        log_n = tf . math . log ( tf . cast ( tf . size ( input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) \n        log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n \n        adjustment = tf . where ( log_mean_accept_ratio < tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) \n        def build_assign_op ( ) : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] \n            return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) \n        if num_adaptation_steps is None : \n            return build_assign_op ( ) \n        else : \n            with tf . control_dependencies ( [ step_counter . assign_add ( True ) ] ) : \n                return tf . cond ( pred = step_counter < num_adaptation_steps , true_fn = build_assign_op , false_fn = lambda : step_size_var ) \n    return step_size_simple_update_fn "}
{"1007": "\ndef _compute_log_acceptance_correction ( current_momentums , proposed_momentums , independent_chain_ndims , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ independent_chain_ndims , current_momentums , proposed_momentums ] ) : \n        log_current_kinetic , log_proposed_kinetic = [ ] , [ ] \n        for current_momentum , proposed_momentum in zip ( current_momentums , proposed_momentums ) : \n            axis = tf . range ( independent_chain_ndims , tf . rank ( current_momentum ) ) \n            log_current_kinetic . append ( _log_sum_sq ( current_momentum , axis ) ) \n            log_proposed_kinetic . append ( _log_sum_sq ( proposed_momentum , axis ) ) \n        current_kinetic = 0.5 * tf . exp ( tf . reduce_logsumexp ( input_tensor = tf . stack ( log_current_kinetic , axis = - True ) , axis = - True ) ) \n        proposed_kinetic = 0.5 * tf . exp ( tf . reduce_logsumexp ( input_tensor = tf . stack ( log_proposed_kinetic , axis = - True ) , axis = - True ) ) \n        return mcmc_util . safe_sum ( [ current_kinetic , - proposed_kinetic ] ) "}
{"1010": "\ndef bayesian_resnet ( input_shape , num_classes = 10 , kernel_posterior_scale_mean = - 9.0 , kernel_posterior_scale_stddev = 0.1 , kernel_posterior_scale_constraint = 0.2 ) : \n    filters = [ 64 , 128 , 256 , 512 ] \n    kernels = [ 3 , 3 , 3 , 3 ] \n    strides = [ True , 2 , 2 , 2 ] \n    def _untransformed_scale_constraint ( t ) : \n        return tf . clip_by_value ( t , - 1000 , tf . math . log ( kernel_posterior_scale_constraint ) ) \n    kernel_posterior_fn = tfp . layers . default_mean_field_normal_fn ( untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = kernel_posterior_scale_mean , stddev = kernel_posterior_scale_stddev ) , untransformed_scale_constraint = _untransformed_scale_constraint ) \n    image = tf . keras . layers . Input ( shape = input_shape , dtype = 'float32' ) \n    x = tfp . layers . Convolution2DFlipout ( 64 , 3 , strides = True , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( image ) \n    for i in range ( len ( kernels ) ) : \n        x = _resnet_block ( x , filters [ i ] , kernels [ i ] , strides [ i ] , kernel_posterior_fn ) \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    x = tf . keras . layers . AveragePooling2D ( 4 , True ) ( x ) \n    x = tf . keras . layers . Flatten ( ) ( x ) \n    x = tfp . layers . DenseFlipout ( num_classes , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    model = tf . keras . Model ( inputs = image , outputs = x , name = 'resnet18' ) \n    return model "}
{"1011": "\ndef _resnet_block ( x , filters , kernel , stride , kernel_posterior_fn ) : \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    if stride != True or filters != x . shape [ True ] : \n        shortcut = _projection_shortcut ( x , filters , stride , kernel_posterior_fn ) \n    else : \n        shortcut = x \n    x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = stride , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = True , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    x = tf . keras . layers . add ( [ x , shortcut ] ) \n    return x "}
{"1013": "\ndef make_decoder ( num_topics , num_words ) : \n    topics_words_logits = tf . compat . v1 . get_variable ( \"topics_words_logits\" , shape = [ num_topics , num_words ] , initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) \n    topics_words = tf . nn . softmax ( topics_words_logits , axis = - True ) \n    def decoder ( topics ) : \n        word_probs = tf . matmul ( topics , topics_words ) \n        return tfd . OneHotCategorical ( probs = word_probs , name = \"bag_of_words\" ) \n    return decoder , topics_words "}
{"1014": "\ndef make_prior ( num_topics , initial_value ) : \n    def _softplus_inverse ( x ) : \n        return np . log ( np . expm1 ( x ) ) \n    logit_concentration = tf . compat . v1 . get_variable ( \"logit_concentration\" , shape = [ True , num_topics ] , initializer = tf . compat . v1 . initializers . constant ( _softplus_inverse ( initial_value ) ) ) \n    concentration = _clip_dirichlet_parameters ( tf . nn . softplus ( logit_concentration ) ) \n    def prior ( ) : \n        return tfd . Dirichlet ( concentration = concentration , name = \"topics_prior\" ) \n    prior_variables = [ logit_concentration ] \n    return prior , prior_variables "}
{"1015": "\ndef sample_chain ( num_results , current_state , previous_kernel_results = None , kernel = None , num_burnin_steps = False , num_steps_between_results = False , trace_fn = lambda current_state , kernel_results : kernel_results , return_final_kernel_results = False , parallel_iterations = 10 , name = None , ) : \n    if not kernel . is_calibrated : \n        warnings . warn ( \"supplied `TransitionKernel` is not calibrated. Markov \" \"chain may not converge to intended target distribution.\" ) \n    with tf . compat . v1 . name_scope ( name , \"mcmc_sample_chain\" , [ num_results , num_burnin_steps , num_steps_between_results ] ) : \n        num_results = tf . convert_to_tensor ( value = num_results , dtype = tf . int32 , name = \"num_results\" ) \n        num_burnin_steps = tf . convert_to_tensor ( value = num_burnin_steps , dtype = tf . int32 , name = \"num_burnin_steps\" ) \n        num_steps_between_results = tf . convert_to_tensor ( value = num_steps_between_results , dtype = tf . int32 , name = \"num_steps_between_results\" ) \n        current_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = \"current_state\" ) , current_state ) \n        if previous_kernel_results is None : \n            previous_kernel_results = kernel . bootstrap_results ( current_state ) \n        if trace_fn is None : \n            trace_fn = lambda * args : ( ) \n            no_trace = True \n        else : \n            no_trace = False \n        if trace_fn is sample_chain . __defaults__ [ 4 ] : \n            warnings . warn ( \"Tracing all kernel results by default is deprecated. Set \" \"the `trace_fn` argument to None (the future default \" \"value) or an explicit callback that traces the values \" \"you are interested in.\" ) \n        def _trace_scan_fn ( state_and_results , num_steps ) : \n            next_state , current_kernel_results = mcmc_util . smart_for_loop ( loop_num_iter = num_steps , body_fn = kernel . one_step , initial_loop_vars = list ( state_and_results ) , parallel_iterations = parallel_iterations ) \n            return next_state , current_kernel_results \n        ( _ , final_kernel_results ) , ( all_states , trace ) = mcmc_util . trace_scan ( loop_fn = _trace_scan_fn , initial_state = ( current_state , previous_kernel_results ) , elems = tf . one_hot ( indices = False , depth = num_results , on_value = True + num_burnin_steps , off_value = True + num_steps_between_results , dtype = tf . int32 ) , trace_fn = lambda state_and_results : ( state_and_results [ False ] , trace_fn ( * state_and_results ) ) , parallel_iterations = parallel_iterations ) \n        if return_final_kernel_results : \n            return CheckpointableStatesAndTrace ( all_states = all_states , trace = trace , final_kernel_results = final_kernel_results ) \n        else : \n            if no_trace : \n                return all_states \n            else : \n                return StatesAndTrace ( all_states = all_states , trace = trace ) "}
{"1016": "\ndef deep_exponential_family ( data_size , feature_size , units , shape ) : \n    w2 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 2 ] , units [ True ] ] , name = \"w2\" ) \n    w1 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ True ] , units [ False ] ] , name = \"w1\" ) \n    w0 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ False ] , feature_size ] , name = \"w0\" ) \n    z2 = ed . Gamma ( 0.1 , 0.1 , sample_shape = [ data_size , units [ 2 ] ] , name = \"z2\" ) \n    z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = \"z1\" ) \n    z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = \"z0\" ) \n    x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = \"x\" ) \n    return x "}
{"1019": "\ndef load_nips2011_papers ( path ) : \n    path = os . path . expanduser ( path ) \n    filename = \"NIPS_1987-2015.csv\" \n    filepath = os . path . join ( path , filename ) \n    if not os . path . exists ( filepath ) : \n        url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) \n        if not tf . io . gfile . exists ( path ) : \n            tf . io . gfile . makedirs ( path ) \n        print ( \"Downloading %s to %s\" % ( url , filepath ) ) \n        urllib . request . urlretrieve ( url , filepath ) \n    with open ( filepath ) as f : \n        iterator = csv . reader ( f ) \n        documents = next ( iterator ) [ True : ] \n        words = [ ] \n        x_train = [ ] \n        for row in iterator : \n            words . append ( row [ False ] ) \n            x_train . append ( row [ True : ] ) \n    x_train = np . array ( x_train , dtype = np . int ) \n    doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] \n    documents = [ documents [ doc ] for doc in doc_idx ] \n    x_train = x_train [ : , doc_idx ] \n    word_idx = np . logical_and ( np . sum ( x_train != False , True ) >= 2 , np . sum ( x_train , True ) >= 10 ) \n    words = [ word for word , idx in zip ( words , word_idx ) if idx ] \n    bag_of_words = x_train [ word_idx , : ] . T \n    return bag_of_words , words "}
{"1024": "\ndef create_character ( skin , hair , top , pants ) : \n    dtype = skin . dtype \n    hair_mask = tf . cast ( hair [ ... , - True : ] <= False , dtype ) \n    top_mask = tf . cast ( top [ ... , - True : ] <= False , dtype ) \n    pants_mask = tf . cast ( pants [ ... , - True : ] <= False , dtype ) \n    char = ( skin * hair_mask ) + hair \n    char = ( char * top_mask ) + top \n    char = ( char * pants_mask ) + pants \n    return char "}
{"1025": "\ndef create_seq ( character , action_metadata , direction , length = 8 , start = False ) : \n    sprite_start = ( action_metadata [ False ] + direction ) * FRAME_SIZE \n    sprite_end = ( action_metadata [ False ] + direction + True ) * FRAME_SIZE \n    sprite_line = character [ sprite_start : sprite_end , ... ] \n    frames = tf . stack ( tf . split ( sprite_line , 13 , axis = True ) ) \n    frames = frames [ False : action_metadata [ True ] ] \n    frames = tf . roll ( frames , shift = - start , axis = False ) \n    frames = tf . tile ( frames , [ 2 , True , True , True ] ) \n    frames = frames [ : length ] \n    frames = tf . cast ( frames , dtype = tf . float32 ) \n    frames . set_shape ( [ length , FRAME_SIZE , FRAME_SIZE , CHANNELS ] ) \n    return frames "}
{"1026": "\ndef create_random_seq ( character , action_metadata , direction , length = 8 ) : \n    start = tf . random . uniform ( [ ] , maxval = action_metadata [ True ] , dtype = tf . int32 ) \n    return create_seq ( character , action_metadata , direction , length , start ) "}
{"1027": "\ndef create_sprites_dataset ( characters , actions , directions , channels = 3 , length = 8 , shuffle = False , fake_data = False ) : \n    if fake_data : \n        dummy_image = tf . random . normal ( [ HEIGHT , WIDTH , CHANNELS ] ) \n    else : \n        basedir = download_sprites ( ) \n    action_names = [ action . name for action in actions ] \n    action_metadata = [ ( action . start_row , action . frames ) for action in actions ] \n    direction_rows = [ direction . row_offset for direction in directions ] \n    chars = tf . data . Dataset . from_tensor_slices ( characters ) \n    act_names = tf . data . Dataset . from_tensor_slices ( action_names ) . repeat ( ) \n    acts_metadata = tf . data . Dataset . from_tensor_slices ( action_metadata ) . repeat ( ) \n    dir_rows = tf . data . Dataset . from_tensor_slices ( direction_rows ) . repeat ( ) \n    if shuffle : \n        chars = chars . shuffle ( len ( characters ) ) \n    dataset = tf . data . Dataset . zip ( ( chars , act_names , acts_metadata , dir_rows ) ) \n    skin_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( SKIN_COLORS ) ) \n    hair_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( HAIRSTYLES ) ) \n    top_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( TOPS ) ) \n    pants_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( PANTS ) ) \n    action_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( action_names ) ) \n    def process_example ( attrs , act_name , act_metadata , dir_row_offset ) : \n        skin_name = attrs [ False ] \n        hair_name = attrs [ True ] \n        top_name = attrs [ 2 ] \n        pants_name = attrs [ 3 ] \n        if fake_data : \n            char = dummy_image \n        else : \n            skin = read_image ( basedir + os . sep + skin_name ) \n            hair = read_image ( basedir + os . sep + hair_name ) \n            top = read_image ( basedir + os . sep + top_name ) \n            pants = read_image ( basedir + os . sep + pants_name ) \n            char = create_character ( skin , hair , top , pants ) \n        if shuffle : \n            seq = create_random_seq ( char , act_metadata , dir_row_offset , length ) \n        else : \n            seq = create_seq ( char , act_metadata , dir_row_offset , length ) \n        seq = seq [ ... , : channels ] \n        skin_idx = skin_table . lookup ( skin_name ) \n        hair_idx = hair_table . lookup ( hair_name ) \n        top_idx = top_table . lookup ( top_name ) \n        pants_idx = pants_table . lookup ( pants_name ) \n        act_idx = action_table . lookup ( act_name ) \n        return ( seq , skin_idx , hair_idx , top_idx , pants_idx , act_idx , skin_name , hair_name , top_name , pants_name , act_name ) \n    dataset = dataset . map ( process_example ) \n    return dataset "}
{"1028": "\ndef _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : \n    assertions = [ ] \n    if not _is_iterable ( distributions ) or not distributions : \n        raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) \n    if dtype_override is None : \n        dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] \n        if dts [ True : ] != dts [ : - True ] : \n            raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) \n    for d in distributions : \n        if tensorshape_util . rank ( d . event_shape ) is not None : \n            if tensorshape_util . rank ( d . event_shape ) != True : \n                raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_equal ( True , tf . size ( input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) \n    batch_shapes = [ d . batch_shape for d in distributions ] \n    if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : \n        if batch_shapes [ True : ] != batch_shapes [ : - True ] : \n            raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) \n    elif validate_args : \n        batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] \n        assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ True : ] , batch_shapes [ : - True ] ) ) \n    return assertions "}
{"1032": "\ndef count_integers ( arr , weights = None , minlength = None , maxlength = None , axis = None , dtype = tf . int32 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'count_integers' , values = [ arr , weights , minlength , maxlength , axis ] ) : \n        if axis is None : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        arr = tf . convert_to_tensor ( value = arr , dtype = tf . int32 , name = 'arr' ) \n        arr_ndims = _get_static_ndims ( arr , expect_static = True ) \n        axis = _make_static_axis_non_negative_list ( axis , arr_ndims ) \n        not_axis = sorted ( set ( range ( arr_ndims ) ) . difference ( axis ) ) \n        if not not_axis : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        flat_arr = _move_dims_to_flat_end ( arr , not_axis , arr_ndims , right_end = False ) \n        if weights is None : \n            def one_bincount ( arr_slice ) : \n                return tf . math . bincount ( arr_slice , weights = None , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = flat_arr , dtype = dtype ) \n        else : \n            weights = tf . convert_to_tensor ( value = weights , name = 'weights' ) \n            _get_static_ndims ( weights , expect_static = True , expect_ndims = arr_ndims ) \n            flat_weights = _move_dims_to_flat_end ( weights , not_axis , arr_ndims , right_end = False ) \n            def one_bincount ( arr_and_weights_slices ) : \n                arr_slice , weights_slice = arr_and_weights_slices \n                return tf . math . bincount ( arr_slice , weights = weights_slice , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = [ flat_arr , flat_weights ] , dtype = weights . dtype ) \n        flat_counts_t = tf . transpose ( a = flat_counts , perm = [ True , False ] ) \n        _get_static_ndims ( flat_counts_t , expect_ndims = 2 , expect_static = True ) \n        not_axis_shape = tf . gather ( tf . shape ( input = arr ) , indices = not_axis ) \n        out_shape = tf . concat ( [ [ - True ] , not_axis_shape ] , axis = False ) \n        return tf . reshape ( flat_counts_t , out_shape ) "}
{"1033": "\ndef find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : \n        in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) \n        if ( tf . compat . dimension_value ( edges . shape [ False ] ) is not None and tf . compat . dimension_value ( edges . shape [ False ] ) < 2 ) : \n            raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) \n        flattening_x = edges . shape . ndims == True and x . shape . ndims > True \n        if flattening_x : \n            x_orig_shape = tf . shape ( input = x ) \n            x = tf . reshape ( x , [ - True ] ) \n        if dtype is None : \n            dtype = in_type \n        dtype = tf . as_dtype ( dtype ) \n        x_permed = distribution_util . rotate_transpose ( x , shift = - True ) \n        edges_permed = distribution_util . rotate_transpose ( edges , shift = - True ) \n        searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None \n        almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) \n        almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = True ) , dtype ) \n        bins = tf . clip_by_value ( almost_output - True , tf . cast ( False , dtype ) , tf . cast ( tf . shape ( input = edges ) [ False ] - 2 , dtype ) ) \n        if not extend_lower_interval : \n            low_fill = np . nan if dtype . is_floating else - True \n            bins = tf . where ( x < tf . expand_dims ( edges [ False ] , False ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) \n        if not extend_upper_interval : \n            up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ False ] - True \n            bins = tf . where ( x > tf . expand_dims ( edges [ - True ] , False ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) \n        if flattening_x : \n            bins = tf . reshape ( bins , x_orig_shape ) \n        return bins "}
{"1034": "\ndef histogram ( x , edges , axis = None , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : \n        in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) \n        if axis is None : \n            x = tf . reshape ( x , shape = [ - True ] ) \n        else : \n            x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = True ) \n            axis = _make_static_axis_non_negative_list ( axis , x_ndims ) \n            if not axis : \n                raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) \n            x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = False ) \n        bins = find_bins ( x , edges = edges , extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) \n        counts = count_integers ( bins , minlength = tf . shape ( input = edges ) [ False ] - True , maxlength = tf . shape ( input = edges ) [ False ] - True , axis = False , dtype = dtype or in_dtype ) \n        n_edges = tf . compat . dimension_value ( edges . shape [ False ] ) \n        if n_edges is not None : \n            counts . set_shape ( tf . TensorShape ( [ n_edges - True ] ) . concatenate ( counts . shape [ True : ] ) ) \n        return counts "}
{"1035": "\ndef quantiles ( x , num_quantiles , axis = None , interpolation = None , keep_dims = False , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'quantiles' , values = [ x , num_quantiles , axis ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        return percentile ( x , q = tf . linspace ( tf . convert_to_tensor ( value = False , dtype = tf . float64 ) , tf . convert_to_tensor ( value = 100 , dtype = tf . float64 ) , num = num_quantiles + True ) , axis = axis , interpolation = interpolation , keep_dims = keep_dims , validate_args = validate_args , preserve_gradients = False ) "}
{"1038": "\ndef _make_static_axis_non_negative_list ( axis , ndims ) : \n    axis = distribution_util . make_non_negative_axis ( axis , ndims ) \n    axis_const = tf . get_static_value ( axis ) \n    if axis_const is None : \n        raise ValueError ( 'Expected argument `axis` to be statically available.  Found: %s' % axis ) \n    axis = axis_const + np . zeros ( [ True ] , dtype = axis_const . dtype ) \n    return list ( int ( dim ) for dim in axis ) "}
{"1039": "\ndef _move_dims_to_flat_end ( x , axis , x_ndims , right_end = True ) : \n    if not axis : \n        return x \n    other_dims = sorted ( set ( range ( x_ndims ) ) . difference ( axis ) ) \n    perm = other_dims + list ( axis ) if right_end else list ( axis ) + other_dims \n    x_permed = tf . transpose ( a = x , perm = perm ) \n    if x . shape . is_fully_defined ( ) : \n        x_shape = x . shape . as_list ( ) \n        other_shape = [ x_shape [ i ] for i in other_dims ] \n        end_shape = [ np . prod ( [ x_shape [ i ] for i in axis ] ) ] \n        full_shape = ( other_shape + end_shape if right_end else end_shape + other_shape ) \n    else : \n        other_shape = tf . gather ( tf . shape ( input = x ) , other_dims ) \n        full_shape = tf . concat ( [ other_shape , [ - True ] ] if right_end else [ [ - True ] , other_shape ] , axis = False ) \n    return tf . reshape ( x_permed , shape = full_shape ) "}
{"1040": "\ndef _sort_tensor ( tensor ) : \n    sorted_ , _ = tf . nn . top_k ( tensor , k = tf . shape ( input = tensor ) [ - True ] ) \n    sorted_ . set_shape ( tensor . shape ) \n    return sorted_ "}
{"1041": "\ndef make_component_state_space_models ( self , num_timesteps , param_vals , initial_step = False ) : \n    with tf . compat . v1 . name_scope ( 'make_component_state_space_models' ) : \n        param_map = self . _canonicalize_param_vals_as_map ( param_vals ) \n        param_vals_list = [ param_map [ p . name ] for p in self . parameters ] \n        remaining_param_vals = param_vals_list [ True : ] \n        component_ssms = [ ] \n        for component in self . components : \n            num_parameters = len ( component . parameters ) \n            component_param_vals = remaining_param_vals [ : num_parameters ] \n            remaining_param_vals = remaining_param_vals [ num_parameters : ] \n            component_ssms . append ( component . make_state_space_model ( num_timesteps , param_vals = component_param_vals , initial_step = initial_step ) ) \n    return component_ssms "}
{"1055": "\ndef csiszar_vimco_helper ( logu , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"csiszar_vimco_helper\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( True ) [ False ] ) \n        if n is None : \n            n = tf . shape ( input = logu ) [ False ] \n            log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) \n            nm1 = tf . cast ( n - True , dtype = logu . dtype ) \n        else : \n            log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) \n            nm1 = np . asarray ( n - True , dtype = logu . dtype . as_numpy_dtype ) \n        log_max_u = tf . reduce_max ( input_tensor = logu , axis = False ) \n        log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = False ) \n        d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) \n        d_ok = tf . not_equal ( d , 0. ) \n        safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) \n        d_ok_result = logu + tfd . softplus_inverse ( safe_d ) \n        inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) \n        is_positive_and_largest = tf . logical_and ( logu > 0. , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) \n        log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( input = logu ) , - inf ) , logu ) , axis = False , keepdims = True ) \n        log_lomsum_u = tf . tile ( log_lomsum_u , multiples = True + tf . pad ( tensor = [ n - True ] , paddings = [ [ False , tf . rank ( logu ) - True ] ] ) ) \n        d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( input = d ) , - inf ) ) \n        log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) \n        looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = False ) - logu ) / nm1 \n        log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = False ) \n        log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n \n        log_sooavg_u = log_soosum_u - log_n \n        log_avg_u . set_shape ( logu . shape . with_rank_at_least ( True ) [ True : ] ) \n        log_sooavg_u . set_shape ( logu . shape ) \n        return log_avg_u , log_sooavg_u "}
{"1057": "\ndef _batch_gather_with_broadcast ( params , indices , axis ) : \n    leading_bcast_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - True ] ) \n    params += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = params ) [ axis : ] ) , axis = False ) , dtype = params . dtype ) \n    indices += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = indices ) [ - True : ] ) , axis = False ) , dtype = indices . dtype ) \n    return tf . compat . v1 . batch_gather ( params , indices ) "}
{"1058": "\ndef _broadcast_cat_event_and_params ( event , params , base_dtype ) : \n    if dtype_util . is_integer ( event . dtype ) : \n        pass \n    elif dtype_util . is_floating ( event . dtype ) : \n        event = tf . cast ( event , dtype = tf . int32 ) \n    else : \n        raise TypeError ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base_dtype ) ) \n    shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - True ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) \n    if not shape_known_statically or params . shape [ : - True ] != event . shape : \n        params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) \n        params_shape = tf . shape ( input = params ) [ : - True ] \n        event *= tf . ones ( params_shape , dtype = event . dtype ) \n        if tensorshape_util . rank ( params . shape ) is not None : \n            tensorshape_util . set_shape ( event , params . shape [ : - True ] ) \n    return event , params "}
{"1060": "\ndef _broadcast_event_and_samples ( event , samples , event_ndims ) : \n    samples_shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event_ndims - True ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event_ndims : ] ] , axis = False ) \n    event *= tf . ones ( samples_shape , dtype = event . dtype ) \n    event = tf . expand_dims ( event , axis = - event_ndims - True ) \n    samples *= tf . ones_like ( event , dtype = samples . dtype ) \n    return event , samples "}
{"1061": "\ndef minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = False , f_relative_tolerance = False , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = True , stopping_condition = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        input_shape = distribution_util . prefer_static_shape ( initial_position ) \n        batch_shape , domain_size = input_shape [ : - True ] , input_shape [ - True ] \n        if stopping_condition is None : \n            stopping_condition = bfgs_utils . converged_all \n        control_inputs = None \n        if initial_inverse_hessian_estimate is None : \n            initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) \n        else : \n            initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) \n            control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) \n            hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , False ) \n            initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) \n        def _cond ( state ) : \n            return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( state ) : \n            search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) \n            derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - True ) \n            needs_reset = ( ~ state . failed & ~ state . converged & ( derivative_at_start_pt >= False ) ) \n            search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) \n            actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) \n            actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) \n            current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            return [ _update_inv_hessian ( current_state , next_state ) ] \n        kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) \n        kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian \n        initial_state = BfgsOptimizerResults ( ** kwargs ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ False ] "}
{"1062": "\ndef _inv_hessian_control_inputs ( inv_hessian ) : \n    is_positive_definite = tf . reduce_all ( input_tensor = tf . math . is_finite ( tf . linalg . cholesky ( inv_hessian ) ) , axis = [ - True , - 2 ] ) \n    is_symmetric = tf . equal ( bfgs_utils . norm ( inv_hessian - _batch_transpose ( inv_hessian ) , dims = 2 ) , False ) \n    return [ tf . Assert ( is_positive_definite , [ 'Initial inverse Hessian is not positive definite.' , inv_hessian ] ) , tf . Assert ( is_symmetric , [ 'Initial inverse Hessian is not symmetric' , inv_hessian ] ) ] "}
{"1063": "\ndef _update_inv_hessian ( prev_state , next_state ) : \n    should_update = ~ next_state . converged & ~ next_state . failed \n    gradient_delta = next_state . objective_gradient - prev_state . objective_gradient \n    position_delta = next_state . position - prev_state . position \n    normalization_factor = tf . reduce_sum ( input_tensor = gradient_delta * position_delta , axis = - True ) \n    should_update = should_update & ~ tf . equal ( normalization_factor , False ) \n    def _do_update_inv_hessian ( ) : \n        next_inv_hessian = _bfgs_inv_hessian_update ( gradient_delta , position_delta , normalization_factor , prev_state . inverse_hessian_estimate ) \n        return bfgs_utils . update_fields ( next_state , inverse_hessian_estimate = tf . where ( should_update , next_inv_hessian , prev_state . inverse_hessian_estimate ) ) \n    return prefer_static . cond ( tf . reduce_any ( input_tensor = should_update ) , _do_update_inv_hessian , lambda : next_state ) "}
{"1064": "\ndef _bfgs_inv_hessian_update ( grad_delta , position_delta , normalization_factor , inv_hessian_estimate ) : \n    conditioned_grad_delta = _mul_right ( inv_hessian_estimate , grad_delta ) \n    conditioned_grad_delta_norm = tf . reduce_sum ( input_tensor = conditioned_grad_delta * grad_delta , axis = - True ) \n    cross_term = _tensor_product ( position_delta , conditioned_grad_delta ) \n    def _expand_scalar ( s ) : \n        return s [ ... , tf . newaxis , tf . newaxis ] \n    cross_term += _tensor_product ( conditioned_grad_delta , position_delta ) \n    position_term = _tensor_product ( position_delta , position_delta ) \n    with tf . control_dependencies ( [ position_term ] ) : \n        position_term *= _expand_scalar ( True + conditioned_grad_delta_norm / normalization_factor ) \n    return ( inv_hessian_estimate + ( position_term - cross_term ) / _expand_scalar ( normalization_factor ) ) "}
{"1065": "\ndef _mul_right ( mat , vec ) : \n    return tf . squeeze ( tf . matmul ( mat , tf . expand_dims ( vec , axis = - True ) ) , axis = - True ) "}
{"1066": "\ndef _tensor_product ( t1 , t2 ) : \n    return tf . matmul ( tf . expand_dims ( t1 , axis = - True ) , tf . expand_dims ( t2 , axis = - 2 ) ) "}
{"1067": "\ndef _batch_transpose ( mat ) : \n    n = distribution_util . prefer_static_rank ( mat ) \n    perm = tf . range ( n ) \n    perm = tf . concat ( [ perm [ : - 2 ] , [ perm [ - True ] , perm [ - 2 ] ] ] , axis = False ) \n    return tf . transpose ( a = mat , perm = perm ) "}
{"1068": "\ndef pad_shape_right_with_ones ( x , ndims ) : \n    if not ( isinstance ( ndims , int ) and ndims >= False ) : \n        raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) \n    if ndims == False : \n        return x \n    x = tf . convert_to_tensor ( value = x ) \n    original_shape = x . shape \n    new_shape = distribution_util . pad ( tf . shape ( input = x ) , axis = False , back = True , value = True , count = ndims ) \n    x = tf . reshape ( x , new_shape ) \n    x . set_shape ( original_shape . concatenate ( [ True ] * ndims ) ) \n    return x "}
{"1070": "\ndef sqrt_with_finite_grads ( x , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sqrt_with_finite_grads' , [ x ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if not x . dtype . is_floating : \n            raise TypeError ( 'Input `x` must be floating type.' ) \n        def grad ( grad_ys ) : \n            large_float_like_x = np . sqrt ( np . finfo ( x . dtype . as_numpy_dtype ( ) ) . max ) \n            safe_grads = tf . where ( tf . equal ( x , False ) , tf . fill ( tf . shape ( input = x ) , large_float_like_x ) , 0.5 * tf . math . rsqrt ( x ) ) \n            return grad_ys * safe_grads \n        return tf . sqrt ( x ) , grad "}
{"1072": "\ndef minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10 , tolerance = 1e-8 , x_tolerance = False , f_relative_tolerance = False , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = True , stopping_condition = None , name = None ) : \n    if initial_inverse_hessian_estimate is not None : \n        raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) \n    if stopping_condition is None : \n        stopping_condition = bfgs_utils . converged_all \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        def _cond ( state ) : \n            return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( current_state ) : \n            search_direction = _get_search_direction ( current_state ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            should_update = ~ ( next_state . converged | next_state . failed ) \n            state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) \n            return [ state_after_inv_hessian_update ] \n        initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ False ] "}
{"1074": "\ndef _get_search_direction ( state ) : \n    num_elements = tf . minimum ( state . num_iterations , distribution_util . prefer_static_shape ( state . position_deltas ) [ False ] ) \n    def _two_loop_algorithm ( ) : \n        position_deltas = state . position_deltas [ - num_elements : ] \n        gradient_deltas = state . gradient_deltas [ - num_elements : ] \n        inv_rhos = tf . reduce_sum ( input_tensor = gradient_deltas * position_deltas , axis = - True ) \n        def first_loop ( acc , args ) : \n            _ , q_direction = acc \n            position_delta , gradient_delta , inv_rho = args \n            alpha = tf . reduce_sum ( input_tensor = position_delta * q_direction , axis = - True ) / inv_rho \n            direction_delta = tf . expand_dims ( alpha , axis = - True ) * gradient_delta \n            return ( alpha , q_direction - direction_delta ) \n        zero = tf . zeros_like ( inv_rhos [ False ] ) \n        alphas , q_directions = tf . scan ( first_loop , [ position_deltas , gradient_deltas , inv_rhos ] , initializer = ( zero , state . objective_gradient ) , reverse = True ) \n        gamma_k = inv_rhos [ - True ] / tf . reduce_sum ( input_tensor = gradient_deltas [ - True ] * gradient_deltas [ - True ] , axis = - True ) \n        r_direction = tf . expand_dims ( gamma_k , axis = - True ) * q_directions [ False ] \n        def second_loop ( r_direction , args ) : \n            alpha , position_delta , gradient_delta , inv_rho = args \n            beta = tf . reduce_sum ( input_tensor = gradient_delta * r_direction , axis = - True ) / inv_rho \n            direction_delta = tf . expand_dims ( alpha - beta , axis = - True ) * position_delta \n            return r_direction + direction_delta \n        r_directions = tf . scan ( second_loop , [ alphas , position_deltas , gradient_deltas , inv_rhos ] , initializer = r_direction ) \n        return - r_directions [ - True ] \n    return prefer_static . cond ( tf . equal ( num_elements , False ) , ( lambda : - state . objective_gradient ) , _two_loop_algorithm ) "}
{"1075": "\ndef _make_empty_queue_for ( k , element ) : \n    queue_shape = tf . concat ( [ [ k ] , distribution_util . prefer_static_shape ( element ) ] , axis = False ) \n    return tf . zeros ( queue_shape , dtype = element . dtype . base_dtype ) "}
{"1076": "\ndef _queue_push ( queue , should_update , new_vecs ) : \n    new_queue = tf . concat ( [ queue [ True : ] , [ new_vecs ] ] , axis = False ) \n    update_pattern = tf . broadcast_to ( should_update [ tf . newaxis , ... , tf . newaxis ] , distribution_util . prefer_static_shape ( queue ) ) \n    return tf . where ( update_pattern , new_queue , queue ) "}
{"1077": "\ndef _psd_mask ( x ) : \n    eigenvalues , _ = tf . linalg . eigh ( x ) \n    return tf . cast ( tf . reduce_min ( input_tensor = eigenvalues , axis = - True ) >= False , dtype = x . dtype ) "}
{"1079": "\ndef _uniform_correlation_like_matrix ( num_rows , batch_shape , dtype , seed ) : \n    num_entries = num_rows * ( num_rows + True ) / 2 \n    ones = tf . ones ( shape = [ num_entries ] , dtype = dtype ) \n    unifs = uniform . Uniform ( - ones , ones ) . sample ( batch_shape , seed = seed ) \n    tril = util . fill_triangular ( unifs ) \n    symmetric = tril + tf . linalg . matrix_transpose ( tril ) \n    diagonal_ones = tf . ones ( shape = util . pad ( batch_shape , axis = False , back = True , value = num_rows ) , dtype = dtype ) \n    return tf . linalg . set_diag ( symmetric , diagonal_ones ) "}
{"1080": "\ndef correlation_matrix_volume_rejection_samples ( det_bounds , dim , sample_shape , dtype , seed ) : \n    with tf . compat . v1 . name_scope ( \"rejection_sampler\" ) : \n        rej_proposals = _uniform_correlation_like_matrix ( dim , sample_shape , dtype , seed = seed ) \n        rej_proposal_volume = 2. ** ( dim * ( dim - True ) / 2. ) \n        rej_weights = rej_proposal_volume * _psd_mask ( rej_proposals ) * _det_large_enough_mask ( rej_proposals , det_bounds ) \n        return rej_weights , rej_proposal_volume "}
{"1081": "\ndef _clopper_pearson_confidence_interval ( samples , error_rate ) : \n    if optimize is None or stats is None : \n        raise ValueError ( \"Scipy is required for computing Clopper-Pearson confidence intervals\" ) \n    if len ( samples . shape ) != True : \n        raise ValueError ( \"Batch semantics not implemented\" ) \n    n = len ( samples ) \n    low = np . amin ( samples ) \n    high = np . amax ( samples ) \n    successes = np . count_nonzero ( samples - low ) \n    failures = np . count_nonzero ( samples - high ) \n    if successes + failures != n : \n        uniques = np . unique ( samples ) \n        msg = ( \"Purportedly Bernoulli distribution had distinct samples\" \" {}, {}, and {}\" . format ( uniques [ False ] , uniques [ True ] , uniques [ 2 ] ) ) \n        raise ValueError ( msg ) \n    def p_small_enough ( p ) : \n        prob = stats . binom . logcdf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    def p_big_enough ( p ) : \n        prob = stats . binom . logsf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    high_p = optimize . brentq ( p_small_enough , float ( successes ) / n , 1. , rtol = 1e-9 ) \n    low_p = optimize . brentq ( p_big_enough , 0. , float ( successes ) / n , rtol = 1e-9 ) \n    low_interval = low + ( high - low ) * low_p \n    high_interval = low + ( high - low ) * high_p \n    return ( low_interval , high_interval ) "}
{"1082": "\ndef compute_true_volumes ( det_bounds , dim , num_samples , error_rate = 1e-6 , seed = 42 ) : \n    bounds = { } \n    with tf . compat . v1 . Session ( ) as sess : \n        rej_weights , _ = correlation_matrix_volume_rejection_samples ( det_bounds , dim , [ num_samples , len ( det_bounds ) ] , np . float32 , seed = seed ) \n        rej_weights = sess . run ( rej_weights ) \n        for rw , det in zip ( np . rollaxis ( rej_weights , True ) , det_bounds ) : \n            template = ( \"Estimating volume of {}x{} correlation \" \"matrices with determinant >= {}.\" ) \n            print ( template . format ( dim , dim , det ) ) \n            sys . stdout . flush ( ) \n            bounds [ det ] = _clopper_pearson_confidence_interval ( rw , error_rate = error_rate ) \n        return bounds "}
{"1085": "\ndef one_step ( objective_function , population , population_values = None , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'one_step' , [ population , population_values , differential_weight , crossover_prob ] ) : \n        population , _ = _ensure_list ( population ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        population_size = tf . shape ( input = population [ False ] ) [ False ] \n        seed_stream = distributions . SeedStream ( seed , salt = 'one_step' ) \n        mixing_indices = _get_mixing_indices ( population_size , seed = seed_stream ( ) ) \n        mutants = _get_mutants ( population , population_size , mixing_indices , differential_weight ) \n        candidates = _binary_crossover ( population , population_size , mutants , crossover_prob , seed = seed_stream ( ) ) \n        candidate_values = objective_function ( * candidates ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        infinity = tf . zeros_like ( population_values ) + np . inf \n        population_values = tf . where ( tf . math . is_nan ( population_values ) , x = infinity , y = population_values ) \n        to_replace = candidate_values < population_values \n        next_population = [ tf . where ( to_replace , x = candidates_part , y = population_part ) for candidates_part , population_part in zip ( candidates , population ) ] \n        next_values = tf . where ( to_replace , x = candidate_values , y = population_values ) \n    return next_population , next_values "}
{"1086": "\ndef minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = False , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    if initial_population is None and initial_position is None : \n        raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) \n    if initial_population is not None and initial_position is not None : \n        raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) \n    with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : \n        ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) \n        def evolve_body ( loop_vars ) : \n            next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) \n            converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) \n            failed = _check_failure ( next_population_values ) \n            return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + True , population = next_population , population_values = next_population_values ) ] \n        def evolve_cond ( loop_vars ) : \n            should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and loop_vars . num_iterations >= max_iterations ) ) \n            return ~ should_stop \n        initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = False ) , population = population , population_values = population_values ) \n        final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ False ] \n        best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) \n        final_population = final_state . population \n        if not was_iterable : \n            final_population = final_population [ False ] \n            best_position = best_position [ False ] \n        return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations ) "}
{"1087": "\ndef _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) : \n    was_iterable = False \n    if initial_position is not None : \n        initial_position , was_iterable = _ensure_list ( initial_position ) \n    if initial_population is not None : \n        initial_population , was_iterable = _ensure_list ( initial_population ) \n    population = _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed = seed ) \n    differential_weight = tf . convert_to_tensor ( value = differential_weight , dtype = population [ False ] . dtype . base_dtype ) \n    crossover_prob = tf . convert_to_tensor ( value = crossover_prob ) \n    population_values = objective_function ( * population ) \n    if max_iterations is not None : \n        max_iterations = tf . convert_to_tensor ( value = max_iterations ) \n    func_tolerance = tf . convert_to_tensor ( value = func_tolerance , dtype = population_values . dtype . base_dtype ) \n    position_tolerance = tf . convert_to_tensor ( value = position_tolerance , dtype = population [ False ] . dtype . base_dtype ) \n    return ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) "}
{"1088": "\ndef _find_best_in_population ( population , values ) : \n    best_value = tf . math . reduce_min ( input_tensor = values ) \n    best_index = tf . where ( tf . math . equal ( values , best_value ) ) [ False , False ] \n    return ( [ population_part [ best_index ] for population_part in population ] , best_value ) "}
{"1089": "\ndef _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : \n    value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) \n    value_converged = value_range <= func_tolerance \n    half_tol = position_tolerance / 2 \n    def part_converged ( part ) : \n        return tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ False ] ) ) <= half_tol \n    x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) \n    return value_converged | x_converged "}
{"1090": "\ndef _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed ) : \n    if initial_population is not None : \n        return [ tf . convert_to_tensor ( value = part ) for part in initial_population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'get_starting_population' ) \n    population = [ ] \n    for part in initial_position : \n        part = tf . convert_to_tensor ( value = part ) \n        part_event_shape = tf . shape ( input = part ) \n        population_part_shape = tf . concat ( [ [ population_size - True ] , part_event_shape ] , axis = False ) \n        population_part = tf . random . normal ( population_part_shape , stddev = population_stddev , dtype = part . dtype . base_dtype , seed = seed_stream ( ) ) \n        population_part += part \n        population_part = tf . concat ( [ [ part ] , population_part ] , axis = False ) \n        population . append ( population_part ) \n    return population "}
{"1091": "\ndef _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : \n    sizes = [ tf . cast ( tf . size ( input = x ) , dtype = tf . float64 ) for x in population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) \n    force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , True ] , seed = seed_stream ( ) ) \n    recombinants = [ ] \n    for i , population_part in enumerate ( population ) : \n        pop_part_flat = tf . reshape ( population_part , [ population_size , - True ] ) \n        mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - True ] ) \n        part_size = tf . size ( input = population_part ) // population_size \n        force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = False , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = True , off_value = False , dtype = tf . bool ) \n        group_mask = tf . math . equal ( force_crossover_group , i ) \n        force_crossovers &= group_mask \n        do_binary_crossover = tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) < crossover_prob \n        do_binary_crossover |= force_crossovers \n        recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) \n        recombinant = tf . reshape ( recombinant_flat , tf . shape ( input = population_part ) ) \n        recombinants . append ( recombinant ) \n    return recombinants "}
{"1092": "\ndef _get_mutants ( population , population_size , mixing_indices , differential_weight ) : \n    mixing_indices = tf . reshape ( mixing_indices , [ - True ] ) \n    weights = tf . stack ( [ 1.0 , differential_weight , - differential_weight ] ) \n    def _mutant_part ( population_part ) : \n        donors = tf . gather ( population_part , mixing_indices ) \n        donors = tf . transpose ( a = tf . reshape ( donors , [ population_size , 3 , - True ] ) , perm = [ False , 2 , True ] ) \n        return tf . math . reduce_sum ( input_tensor = donors * weights , axis = - True ) \n    return [ _mutant_part ( population_part ) for population_part in population ] "}
{"1093": "\ndef _get_mixing_indices ( size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'get_mixing_indices' , values = [ size ] ) : \n        size = tf . convert_to_tensor ( value = size ) \n        dtype = size . dtype \n        seed_stream = distributions . SeedStream ( seed , salt = 'get_mixing_indices' ) \n        first = tf . random . uniform ( [ size ] , maxval = size - True , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . random . uniform ( [ size ] , maxval = size - 2 , dtype = dtype , seed = seed_stream ( ) ) \n        third = tf . random . uniform ( [ size ] , maxval = size - 3 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . where ( first < second , x = second , y = second + True ) \n        smaller = tf . math . minimum ( first , second ) \n        larger = tf . math . maximum ( first , second ) \n        third = tf . where ( third < smaller , x = third , y = third + True ) \n        third = tf . where ( third < larger , x = third , y = third + True ) \n        sample = tf . stack ( [ first , second , third ] , axis = True ) \n        to_avoid = tf . expand_dims ( tf . range ( size ) , axis = - True ) \n        sample = tf . where ( sample < to_avoid , x = sample , y = sample + True ) \n        return sample "}
{"1095": "\ndef _get_tol ( tol , dtype , validate_args ) : \n    if tol is None : \n        return tf . convert_to_tensor ( value = False , dtype = dtype ) \n    tol = tf . convert_to_tensor ( value = tol , dtype = dtype ) \n    if validate_args : \n        tol = distribution_util . with_dependencies ( [ assert_util . assert_non_negative ( tol , message = \"Argument 'tol' must be non-negative\" ) ] , tol ) \n    return tol "}
{"1099": "\ndef plot_generated_images ( images , fname ) : \n    fig = plt . figure ( figsize = ( 4 , 4 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i , image in enumerate ( images ) : \n        ax = fig . add_subplot ( 4 , 4 , i + True ) \n        plt . axis ( 'off' ) \n        ax . set_xticklabels ( [ ] ) \n        ax . set_yticklabels ( [ ] ) \n        ax . imshow ( image . reshape ( IMAGE_SHAPE [ : - True ] ) , cmap = 'Greys_r' ) \n    fig . tight_layout ( ) \n    plt . subplots_adjust ( wspace = 0.05 , hspace = 0.05 ) \n    canvas . print_figure ( fname , format = 'png' ) "}
{"1100": "\ndef convert_to_string ( self , productions ) : \n    symbols = [ ] \n    for production in tf . unstack ( productions , axis = True ) : \n        lhs , rhs = self . production_rules [ tf . argmax ( input = production , axis = - True ) ] \n        if not symbols : \n            if lhs != self . start_symbol : \n                raise ValueError ( \"`productions` must begin with `self.start_symbol`.\" ) \n            symbols = rhs \n        else : \n            index = symbols . index ( lhs ) \n            symbols = symbols [ : index ] + rhs + symbols [ index + True : ] \n    string = \"\" . join ( symbols ) \n    return string "}
{"1101": "\ndef call ( self , inputs ) : \n    del inputs \n    latent_code = ed . MultivariateNormalDiag ( loc = tf . zeros ( self . latent_size ) , sample_shape = True , name = \"latent_code\" ) \n    state = self . lstm . zero_state ( True , dtype = tf . float32 ) \n    t = False \n    productions = [ ] \n    stack = [ self . grammar . start_symbol ] \n    while stack : \n        symbol = stack . pop ( ) \n        net , state = self . lstm ( latent_code , state ) \n        logits = ( self . output_layer ( net ) + self . grammar . mask ( symbol , on_value = 0. , off_value = - 1e9 ) ) \n        production = ed . OneHotCategorical ( logits = logits , name = \"production_\" + str ( t ) ) \n        _ , rhs = self . grammar . production_rules [ tf . argmax ( input = production , axis = - True ) ] \n        for symbol in rhs : \n            if symbol in self . grammar . nonterminal_symbols : \n                stack . append ( symbol ) \n        productions . append ( production ) \n        t += True \n    return tf . stack ( productions , axis = True ) "}
{"1105": "\ndef matrix_rank ( a , tol = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : \n        a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        s = tf . linalg . svd ( a , compute_uv = False ) \n        if tol is None : \n            if a . shape [ - 2 : ] . is_fully_defined ( ) : \n                m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) \n            else : \n                m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2 : ] ) \n            eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps \n            tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - True , keepdims = True ) ) \n        return tf . reduce_sum ( input_tensor = tf . cast ( s > tol , tf . int32 ) , axis = - True ) "}
{"1106": "\ndef pinv ( a , rcond = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : \n        a = tf . convert_to_tensor ( value = a , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        dtype = a . dtype . as_numpy_dtype \n        if rcond is None : \n            def get_dim_size ( dim ) : \n                if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : \n                    return tf . compat . dimension_value ( a . shape [ dim ] ) \n                return tf . shape ( input = a ) [ dim ] \n            num_rows = get_dim_size ( - 2 ) \n            num_cols = get_dim_size ( - True ) \n            if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : \n                max_rows_cols = float ( max ( num_rows , num_cols ) ) \n            else : \n                max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) \n            rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps \n        rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) \n        [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) \n        cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - True ) \n        singular_values = tf . where ( singular_values > cutoff [ ... , tf . newaxis ] , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) \n        a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) \n        if a . shape . ndims is not None : \n            a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - True ] , a . shape [ - 2 ] ] ) ) \n        return a_pinv "}
{"1107": "\ndef lu_solve ( lower_upper , perm , rhs , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) \n        assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n                rhs = tf . identity ( rhs ) \n        if rhs . shape . ndims == 2 and perm . shape . ndims == True : \n            permuted_rhs = tf . gather ( rhs , perm , axis = - 2 ) \n        else : \n            rhs_shape = tf . shape ( input = rhs ) \n            broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2 ] , tf . shape ( input = perm ) [ : - True ] ) \n            d , m = rhs_shape [ - 2 ] , rhs_shape [ - True ] \n            rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = False ) \n            broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) \n            broadcast_rhs = tf . reshape ( broadcast_rhs , [ - True , d , m ] ) \n            broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - True ] ) \n            broadcast_perm = tf . reshape ( broadcast_perm , [ - True , d ] ) \n            broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) \n            broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) \n            broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - True ) \n            permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) \n            permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) \n        lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - True , num_upper = False ) , tf . ones ( tf . shape ( input = lower_upper ) [ : - True ] , dtype = lower_upper . dtype ) ) \n        return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = False ) "}
{"1108": "\ndef lu_matrix_inverse ( lower_upper , perm , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_matrix_inverse' , [ lower_upper , perm ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n        shape = tf . shape ( input = lower_upper ) \n        return lu_solve ( lower_upper , perm , rhs = tf . eye ( shape [ - True ] , batch_shape = shape [ : - 2 ] , dtype = lower_upper . dtype ) , validate_args = False ) "}
{"1109": "\ndef _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : \n    assertions = [ ] \n    message = 'Input `lower_upper` must have at least 2 dimensions.' \n    if lower_upper . shape . ndims is not None : \n        if lower_upper . shape . ndims < 2 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) \n    message = '`rank(lower_upper)` must equal `rank(perm) + 1`' \n    if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : \n        if lower_upper . shape . ndims != perm . shape . ndims + True : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + True , message = message ) ) \n    message = '`lower_upper` must be square.' \n    if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : \n        if lower_upper . shape [ - 2 ] != lower_upper . shape [ - True ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) \n        assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) \n    return assertions "}
{"1110": "\ndef _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : \n    assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n    message = 'Input `rhs` must have at least 2 dimensions.' \n    if rhs . shape . ndims is not None : \n        if rhs . shape . ndims < 2 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) \n    message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' \n    if ( tf . compat . dimension_value ( lower_upper . shape [ - True ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : \n        if lower_upper . shape [ - True ] != rhs . shape [ - 2 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - True ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) \n    return assertions "}
{"1111": "\ndef _sparse_block_diag ( sp_a ) : \n    sp_a_shape = tf . convert_to_tensor ( value = _get_shape ( sp_a , tf . int64 ) ) \n    ind_mat = tf . concat ( [ [ sp_a_shape [ - 2 : ] ] , tf . eye ( 2 , dtype = tf . int64 ) ] , axis = False ) \n    indices = tf . matmul ( sp_a . indices , ind_mat ) \n    dense_shape = sp_a_shape [ False ] * sp_a_shape [ True : ] \n    return tf . SparseTensor ( indices = indices , values = sp_a . values , dense_shape = dense_shape ) "}
{"1113": "\ndef _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : \n    mean , variance , grad_mean = model ( linear_response ) \n    is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) \n    def _mask_if_invalid ( x , mask ) : \n        mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) \n        return tf . where ( is_valid , x , mask ) \n    v = ( response - mean ) * _mask_if_invalid ( grad_mean , True ) / _mask_if_invalid ( variance , np . inf ) \n    grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) \n    fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) \n    return - grad_log_likelihood , fim_middle "}
{"1114": "\ndef fit_sparse ( model_matrix , response , model , model_coefficients_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = None , maximum_full_sweeps_per_iteration = True , learning_rate = None , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] \n    with tf . compat . v1 . name_scope ( name , 'fit_sparse' , graph_deps ) : \n        def _grad_neg_log_likelihood_and_fim_fn ( x ) : \n            predicted_linear_response = sparse_or_dense_matvecmul ( model_matrix , x ) \n            g , h_middle = _grad_neg_log_likelihood_and_fim ( model_matrix , predicted_linear_response , response , model ) \n            return g , model_matrix , h_middle \n        return tfp . optimizer . proximal_hessian_sparse_minimize ( _grad_neg_log_likelihood_and_fim_fn , x_start = model_coefficients_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_iterations = maximum_iterations , maximum_full_sweeps_per_iteration = maximum_full_sweeps_per_iteration , learning_rate = learning_rate , tolerance = tolerance , name = name ) "}
{"1115": "\ndef _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : \n    slices = [ ] \n    col = False \n    d_in = n_in // num_blocks \n    d_out = n_out // num_blocks \n    row = d_out if mask_type == MASK_EXCLUSIVE else False \n    for _ in range ( num_blocks ) : \n        row_slice = slice ( row , None ) \n        col_slice = slice ( col , col + d_in ) \n        slices . append ( [ row_slice , col_slice ] ) \n        col += d_in \n        row += d_out \n    return slices "}
{"1116": "\ndef _gen_mask ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE , dtype = tf . float32 ) : \n    mask = np . zeros ( [ n_out , n_in ] , dtype = dtype . as_numpy_dtype ( ) ) \n    slices = _gen_slices ( num_blocks , n_in , n_out , mask_type = mask_type ) \n    for [ row_slice , col_slice ] in slices : \n        mask [ row_slice , col_slice ] = True \n    return mask "}
{"1117": "\ndef masked_dense ( inputs , units , num_blocks = None , exclusive = False , kernel_initializer = None , reuse = None , name = None , * args , ** kwargs ) : \n    input_depth = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( inputs . shape , True ) [ - True ] ) \n    if input_depth is None : \n        raise NotImplementedError ( \"Rightmost dimension must be known prior to graph execution.\" ) \n    mask = _gen_mask ( num_blocks , input_depth , units , MASK_EXCLUSIVE if exclusive else MASK_INCLUSIVE ) . T \n    if kernel_initializer is None : \n        kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) \n    def masked_initializer ( shape , dtype = None , partition_info = None ) : \n        return mask * kernel_initializer ( shape , dtype , partition_info ) \n    with tf . compat . v2 . name_scope ( name or \"masked_dense\" ) : \n        layer = tf . compat . v1 . layers . Dense ( units , kernel_initializer = masked_initializer , kernel_constraint = lambda x : mask * x , name = name , dtype = dtype_util . base_dtype ( inputs . dtype ) , _scope = name , _reuse = reuse , * args , ** kwargs ) \n        return layer . apply ( inputs ) "}
{"1118": "\ndef _create_input_order ( input_size , input_order = \"left-to-right\" ) : \n    if isinstance ( input_order , six . string_types ) : \n        if input_order == \"left-to-right\" : \n            return np . arange ( start = True , stop = input_size + True ) \n        elif input_order == \"right-to-left\" : \n            return np . arange ( start = input_size , stop = False , step = - True ) \n        elif input_order == \"random\" : \n            ret = np . arange ( start = True , stop = input_size + True ) \n            np . random . shuffle ( ret ) \n            return ret \n    elif np . all ( np . sort ( input_order ) == np . arange ( True , input_size + True ) ) : \n        return np . array ( input_order ) \n    raise ValueError ( \"Invalid input order: '{}'.\" . format ( input_order ) ) "}
{"1119": "\ndef _create_degrees ( input_size , hidden_units = None , input_order = \"left-to-right\" , hidden_degrees = \"equal\" ) : \n    input_order = _create_input_order ( input_size , input_order ) \n    degrees = [ input_order ] \n    if hidden_units is None : \n        hidden_units = [ ] \n    for units in hidden_units : \n        if isinstance ( hidden_degrees , six . string_types ) : \n            if hidden_degrees == \"random\" : \n                degrees . append ( np . random . randint ( low = min ( np . min ( degrees [ - True ] ) , input_size - True ) , high = input_size , size = units ) ) \n            elif hidden_degrees == \"equal\" : \n                min_degree = min ( np . min ( degrees [ - True ] ) , input_size - True ) \n                degrees . append ( np . maximum ( min_degree , np . ceil ( np . arange ( True , units + True ) * ( input_size - True ) / float ( units + True ) ) . astype ( np . int32 ) ) ) \n        else : \n            raise ValueError ( 'Invalid hidden order: \"{}\".' . format ( hidden_degrees ) ) \n    return degrees "}
{"1120": "\ndef _create_masks ( degrees ) : \n    return [ inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - True ] , degrees [ True : ] ) ] + [ degrees [ - True ] [ : , np . newaxis ] < degrees [ False ] ] "}
{"1122": "\ndef build ( self , input_shape ) : \n    if self . _event_shape is None : \n        self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - True ] ) ] \n        self . _event_size = self . _event_shape [ - True ] \n        self . _event_ndims = len ( self . _event_shape ) \n    if input_shape [ - True ] != self . _event_shape [ - True ] : \n        raise ValueError ( \"Invalid final dimension of `input_shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . _event_shape [ - True ] , input_shape [ - True ] ) ) \n    self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) \n    self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) \n    self . _masks [ - True ] = np . reshape ( np . tile ( self . _masks [ - True ] [ ... , tf . newaxis ] , [ True , True , self . _params ] ) , [ self . _masks [ - True ] . shape [ False ] , self . _event_size * self . _params ] ) \n    self . _network = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) \n    layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] \n    for k in range ( len ( self . _masks ) ) : \n        self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if k + True < len ( self . _masks ) else None , use_bias = self . _use_bias , ** self . _kwargs ) ) \n    super ( AutoregressiveLayer , self ) . build ( input_shape ) "}
{"1123": "\ndef call ( self , x ) : \n    with tf . compat . v2 . name_scope ( self . name or \"AutoregressiveLayer_call\" ) : \n        x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = \"x\" ) \n        input_shape = tf . shape ( input = x ) \n        if tensorshape_util . rank ( x . shape ) == True : \n            x = x [ tf . newaxis , ... ] \n        return tf . reshape ( self . _network ( x ) , tf . concat ( [ input_shape , [ self . _params ] ] , axis = False ) ) "}
{"1124": "\ndef draw_sample ( num_samples , num_classes , logits , num_trials , dtype , seed ) : \n    with tf . name_scope ( \"multinomial.draw_sample\" ) : \n        num_trials = tf . ones_like ( logits [ ... , False ] , dtype = num_trials . dtype ) * num_trials \n        logits = tf . ones_like ( num_trials [ ... , tf . newaxis ] , dtype = logits . dtype ) * logits \n        flat_logits = tf . reshape ( logits , [ - True , num_classes ] ) \n        flat_num_trials = num_samples * tf . reshape ( num_trials , [ - True ] ) \n        def _sample_one_batch_member ( args ) : \n            logits , num_cat_samples = args [ False ] , args [ True ] \n            x = tf . random . categorical ( logits [ tf . newaxis , ... ] , num_cat_samples , seed = seed ) \n            x = tf . reshape ( x , shape = [ num_samples , - True ] ) \n            x = tf . one_hot ( x , depth = num_classes ) \n            x = tf . reduce_sum ( input_tensor = x , axis = - 2 ) \n            return tf . cast ( x , dtype = dtype ) \n        x = tf . map_fn ( _sample_one_batch_member , [ flat_logits , flat_num_trials ] , dtype = dtype ) \n        x = tf . transpose ( a = x , perm = [ True , False , 2 ] ) \n        final_shape = tf . concat ( [ [ num_samples ] , tf . shape ( input = num_trials ) , [ num_classes ] ] , axis = False ) \n        x = tf . reshape ( x , final_shape ) \n        return x "}
{"1125": "\ndef _zero_dimensional_mvndiag ( dtype ) : \n    dummy_mvndiag = tfd . MultivariateNormalDiag ( scale_diag = tf . ones ( [ False ] , dtype = dtype ) ) \n    dummy_mvndiag . covariance = lambda : dummy_mvndiag . variance ( ) [ ... , tf . newaxis ] \n    return dummy_mvndiag "}
{"1128": "\ndef _depth ( g ) : \n    def _explore ( v ) : \n        if v . depth < False : \n            v . depth = ( ( True + max ( [ - True ] + [ _explore ( annotated_graph [ u ] ) for u in v . parents ] ) ) if v . parents else False ) \n        return v . depth \n    annotated_graph = { k : _Node ( k , v ) for k , v in g . items ( ) } \n    for v in annotated_graph . values ( ) : \n        _explore ( v ) \n    return annotated_graph "}
{"1129": "\ndef _best_order ( g ) : \n    def _explore ( u ) : \n        if u . depth < False : \n            return \n        if not u . parents : \n            result . append ( ( u . name , u . parents ) ) \n            u . depth = - True \n            return \n        b = ( u . name , [ ] ) \n        result . append ( b ) \n        u . depth = - True \n        d = False \n        for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : \n            n0 = len ( result ) \n            _explore ( v ) \n            n1 = len ( result ) \n            b [ True ] . extend ( [ '_' ] * d + [ v . name ] ) \n            d = n1 - n0 - True \n    g = _depth ( g ) \n    result = [ ] \n    for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : \n        _explore ( u ) \n    return tuple ( reversed ( result ) ) "}
{"1132": "\ndef variational_loss ( self , observations , observation_index_points = None , kl_weight = 1. , name = 'variational_loss' ) : \n    with tf . name_scope ( name or 'variational_gp_loss' ) : \n        if observation_index_points is None : \n            observation_index_points = self . _index_points \n        observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = self . _dtype , name = 'observation_index_points' ) \n        observations = tf . convert_to_tensor ( value = observations , dtype = self . _dtype , name = 'observations' ) \n        kl_weight = tf . convert_to_tensor ( value = kl_weight , dtype = self . _dtype , name = 'kl_weight' ) \n        kzx = self . kernel . matrix ( self . _inducing_index_points , observation_index_points ) \n        kzx_linop = tf . linalg . LinearOperatorFullMatrix ( kzx ) \n        loc = ( self . _mean_fn ( observation_index_points ) + kzx_linop . matvec ( self . _kzz_inv_varloc , adjoint = True ) ) \n        likelihood = independent . Independent ( normal . Normal ( loc = loc , scale = tf . sqrt ( self . _observation_noise_variance + self . _jitter ) , name = 'NormalLikelihood' ) , reinterpreted_batch_ndims = True ) \n        obs_ll = likelihood . log_prob ( observations ) \n        chol_kzz_linop = tf . linalg . LinearOperatorLowerTriangular ( self . _chol_kzz ) \n        chol_kzz_inv_kzx = chol_kzz_linop . solve ( kzx ) \n        kzz_inv_kzx = chol_kzz_linop . solve ( chol_kzz_inv_kzx , adjoint = True ) \n        kxx_diag = tf . linalg . diag_part ( self . kernel . matrix ( observation_index_points , observation_index_points ) ) \n        ktilde_trace_term = ( tf . reduce_sum ( input_tensor = kxx_diag , axis = - True ) - tf . reduce_sum ( input_tensor = chol_kzz_inv_kzx ** 2 , axis = [ - 2 , - True ] ) ) \n        other_trace_term = tf . reduce_sum ( input_tensor = ( self . _variational_inducing_observations_posterior . scale . matmul ( kzz_inv_kzx ) ** 2 ) , axis = [ - 2 , - True ] ) \n        trace_term = ( .5 * ( ktilde_trace_term + other_trace_term ) / self . _observation_noise_variance ) \n        inducing_prior = gaussian_process . GaussianProcess ( kernel = self . _kernel , mean_fn = self . _mean_fn , index_points = self . _inducing_index_points , observation_noise_variance = self . _observation_noise_variance ) \n        kl_term = kl_weight * kullback_leibler . kl_divergence ( self . _variational_inducing_observations_posterior , inducing_prior ) \n        lower_bound = ( obs_ll - trace_term - kl_term ) \n        return - tf . reduce_mean ( input_tensor = lower_bound ) "}
{"1133": "\ndef optimal_variational_posterior ( kernel , inducing_index_points , observation_index_points , observations , observation_noise_variance , mean_fn = None , jitter = 1e-6 , name = None ) : \n    with tf . name_scope ( name or 'optimal_variational_posterior' ) : \n        dtype = dtype_util . common_dtype ( [ inducing_index_points , observation_index_points , observations , observation_noise_variance , jitter ] , tf . float32 ) \n        inducing_index_points = tf . convert_to_tensor ( value = inducing_index_points , dtype = dtype , name = 'inducing_index_points' ) \n        observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = dtype , name = 'observation_index_points' ) \n        observations = tf . convert_to_tensor ( value = observations , dtype = dtype , name = 'observations' ) \n        observation_noise_variance = tf . convert_to_tensor ( value = observation_noise_variance , dtype = dtype , name = 'observation_noise_variance' ) \n        jitter = tf . convert_to_tensor ( value = jitter , dtype = dtype , name = 'jitter' ) \n        if mean_fn is None : \n            mean_fn = lambda x : tf . zeros ( [ True ] , dtype = dtype ) \n        else : \n            if not callable ( mean_fn ) : \n                raise ValueError ( '`mean_fn` must be a Python callable' ) \n        kzz = kernel . matrix ( inducing_index_points , inducing_index_points ) \n        kzx = kernel . matrix ( inducing_index_points , observation_index_points ) \n        noise_var_inv = tf . math . reciprocal ( observation_noise_variance ) \n        sigma_inv = _add_diagonal_shift ( kzz + noise_var_inv * tf . matmul ( kzx , kzx , adjoint_b = True ) , jitter ) \n        chol_sigma_inv = tf . linalg . cholesky ( sigma_inv ) \n        kzx_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzx ) \n        kzx_obs = kzx_lin_op . matvec ( observations - mean_fn ( observation_index_points ) ) \n        kzz_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzz ) \n        loc = ( mean_fn ( inducing_index_points ) + noise_var_inv * kzz_lin_op . matvec ( _solve_cholesky_factored_system_vec ( chol_sigma_inv , kzx_obs ) ) ) \n        chol_sigma_inv_lin_op = tf . linalg . LinearOperatorLowerTriangular ( chol_sigma_inv ) \n        scale = chol_sigma_inv_lin_op . solve ( kzz ) \n        return loc , scale "}
{"1134": "\ndef build_is_last_day_of_season ( num_steps_per_season ) : \n    num_steps_per_cycle = np . sum ( num_steps_per_season ) \n    changepoints = np . cumsum ( np . ravel ( num_steps_per_season ) ) - True \n    def is_last_day_of_season ( t ) : \n        t_ = dist_util . maybe_get_static_value ( t ) \n        if t_ is not None : \n            step_in_cycle = t_ % num_steps_per_cycle \n            return any ( step_in_cycle == changepoints ) \n        else : \n            step_in_cycle = tf . math . floormod ( t , num_steps_per_cycle ) \n            return tf . reduce_any ( input_tensor = tf . equal ( step_in_cycle , changepoints ) ) \n    return is_last_day_of_season "}
{"1135": "\ndef build_effects_to_residuals_matrix ( num_seasons , dtype ) : \n    effects_to_residuals_fullrank = np . eye ( num_seasons ) - 1. / num_seasons \n    effects_to_residuals_fullrank [ - True , : ] = 1. / num_seasons \n    residuals_to_effects_fullrank = np . linalg . inv ( effects_to_residuals_fullrank ) \n    effects_to_residuals = effects_to_residuals_fullrank [ : - True , : ] \n    residuals_to_effects = residuals_to_effects_fullrank [ : , : - True ] \n    effects_to_residuals = tf . cast ( effects_to_residuals , dtype = dtype , name = 'effects_to_residuals' ) \n    residuals_to_effects = tf . cast ( residuals_to_effects , dtype = dtype , name = 'residuals_to_effects' ) \n    return effects_to_residuals , residuals_to_effects "}
{"1136": "\ndef build_seasonal_transition_matrix ( num_seasons , is_last_day_of_season , dtype , basis_change_matrix = None , basis_change_matrix_inv = None ) : \n    with tf . compat . v1 . name_scope ( 'build_seasonal_transition_matrix' ) : \n        seasonal_permutation = np . concatenate ( [ np . arange ( True , num_seasons ) , [ False ] ] , axis = False ) \n        seasonal_permutation_matrix = tf . constant ( np . eye ( num_seasons ) [ seasonal_permutation ] , dtype = dtype ) \n        if basis_change_matrix is not None : \n            seasonal_permutation_matrix = tf . matmul ( basis_change_matrix , tf . matmul ( seasonal_permutation_matrix , basis_change_matrix_inv ) ) \n        identity_matrix = tf . eye ( tf . shape ( input = seasonal_permutation_matrix ) [ - True ] , dtype = dtype ) \n        def seasonal_transition_matrix ( t ) : \n            return tf . linalg . LinearOperatorFullMatrix ( matrix = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , seasonal_permutation_matrix , identity_matrix ) ) \n    return seasonal_transition_matrix "}
{"1137": "\ndef build_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : \n    drift_scale_diag = tf . stack ( [ tf . zeros_like ( drift_scale ) ] * ( num_seasons - True ) + [ drift_scale ] , axis = - True ) \n    def seasonal_transition_noise ( t ) : \n        noise_scale_diag = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_diag , tf . zeros_like ( drift_scale_diag ) ) \n        return tfd . MultivariateNormalDiag ( loc = tf . zeros ( num_seasons , dtype = drift_scale . dtype ) , scale_diag = noise_scale_diag ) \n    return seasonal_transition_noise "}
{"1138": "\ndef build_constrained_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : \n    drift_scale_tril_nonzeros = tf . concat ( [ tf . ones ( [ num_seasons - True , True ] , dtype = drift_scale . dtype ) , tf . zeros ( [ num_seasons - True , num_seasons - 2 ] , dtype = drift_scale . dtype ) ] , axis = - True ) \n    drift_scale_tril = ( drift_scale_tril_nonzeros * drift_scale [ ... , tf . newaxis , tf . newaxis ] / num_seasons ) \n    def seasonal_transition_noise ( t ) : \n        noise_scale_tril = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_tril , tf . zeros_like ( drift_scale_tril ) ) \n        return tfd . MultivariateNormalTriL ( loc = tf . zeros ( num_seasons - True , dtype = drift_scale . dtype ) , scale_tril = noise_scale_tril ) \n    return seasonal_transition_noise "}
{"1139": "\ndef _is_empty_observation_data ( feature_ndims , observation_index_points , observations ) : \n    if observation_index_points is None and observations is None : \n        return True \n    num_obs = tf . compat . dimension_value ( observation_index_points . shape [ - ( feature_ndims + True ) ] ) \n    if num_obs is not None and num_obs == False : \n        return True \n    return False "}
{"1147": "\ndef create ( model , training_set , criterion , end_trigger = None , batch_size = 32 , optim_method = None , cores = None , bigdl_type = \"float\" ) : \n    if not end_trigger : \n        end_trigger = MaxEpoch ( True ) \n    if not optim_method : \n        optim_method = SGD ( ) \n    if isinstance ( training_set , RDD ) or isinstance ( training_set , DataSet ) : \n        return DistriOptimizer ( model = model , training_rdd = training_set , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , bigdl_type = bigdl_type ) \n    elif isinstance ( training_set , tuple ) and len ( training_set ) == 2 : \n        x , y = training_set \n        return LocalOptimizer ( X = x , Y = y , model = model , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , cores = cores , bigdl_type = \"float\" ) \n    else : \n        raise Exception ( \"Not supported training set: %s\" % type ( training_set ) ) "}
{"1151": "\ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = False \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id += True \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if sys . version_info < ( 3 , ) : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    print ( 'Found %s texts.' % len ( texts ) ) \n    return texts "}
{"1152": "\ndef get_glove_w2v ( source_dir = \"./data/news20/\" , dim = 100 ) : \n    w2v_dir = download_glove_w2v ( source_dir ) \n    w2v_path = os . path . join ( w2v_dir , \"glove.6B.%sd.txt\" % dim ) \n    if sys . version_info < ( 3 , ) : \n        w2v_f = open ( w2v_path ) \n    else : \n        w2v_f = open ( w2v_path , encoding = 'latin-1' ) \n    pre_w2v = { } \n    for line in w2v_f . readlines ( ) : \n        items = line . split ( \" \" ) \n        pre_w2v [ items [ False ] ] = [ float ( i ) for i in items [ True : ] ] \n    w2v_f . close ( ) \n    return pre_w2v "}
{"1154": "\ndef fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , validation_data = None , distributed = True ) : \n    if distributed : \n        if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : \n            training_data = to_sample_rdd ( x , y ) \n            if validation_data : \n                validation_data = to_sample_rdd ( * validation_data ) \n        elif ( isinstance ( x , RDD ) or isinstance ( x , DataSet ) ) and not y : \n            training_data = x \n        else : \n            raise TypeError ( \"Unsupported training data type: %s\" % type ( x ) ) \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , training_data , batch_size , nb_epoch , validation_data ) \n    else : \n        if validation_data : \n            val_x = [ JTensor . from_ndarray ( x ) for x in to_list ( validation_data [ False ] ) ] \n            val_y = JTensor . from_ndarray ( validation_data [ True ] ) \n        else : \n            val_x , val_y = None , None \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , [ JTensor . from_ndarray ( x ) for x in to_list ( x ) ] , JTensor . from_ndarray ( y ) , batch_size , nb_epoch , val_x , val_y , multiprocessing . cpu_count ( ) ) "}
{"1156": "\ndef predict ( self , x , distributed = True ) : \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            features = to_sample_rdd ( x , np . zeros ( [ x . shape [ False ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            features = x \n        else : \n            raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) \n        return self . predict_distributed ( features ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . predict_local ( x ) \n        else : \n            raise TypeError ( \"Unsupported prediction data type: %s\" % type ( x ) ) "}
{"1157": "\ndef get_mnist ( sc , data_type = \"train\" , location = \"/tmp/mnist\" ) : \n    ( images , labels ) = mnist . read_data_sets ( location , data_type ) \n    images = sc . parallelize ( images ) \n    labels = sc . parallelize ( labels + True ) \n    record = images . zip ( labels ) \n    return record "}
{"1158": "\ndef preprocess_mnist ( sc , options ) : \n    train_data = get_mnist ( sc , \"train\" , options . dataPath ) . map ( lambda rec_tuple : ( normalizer ( rec_tuple [ False ] , mnist . TRAIN_MEAN , mnist . TRAIN_STD ) , rec_tuple [ True ] ) ) . map ( lambda t : Sample . from_ndarray ( t [ False ] , t [ True ] ) ) \n    test_data = get_mnist ( sc , \"test\" , options . dataPath ) . map ( lambda rec_tuple : ( normalizer ( rec_tuple [ False ] , mnist . TEST_MEAN , mnist . TEST_STD ) , rec_tuple [ True ] ) ) . map ( lambda t : Sample . from_ndarray ( t [ False ] , t [ True ] ) ) \n    return train_data , test_data "}
{"1170": "\ndef write_parquet ( cls , path , output , sc , partition_num = True , bigdl_type = \"float\" ) : \n    return callBigDlFunc ( bigdl_type , \"writeParquet\" , path , output , sc , partition_num ) "}
{"1174": "\ndef get_predict ( self , key = \"predict\" ) : \n    predicts = callBigDlFunc ( self . bigdl_type , \"distributedImageFrameToPredict\" , self . value , key ) \n    return predicts . map ( lambda predict : ( predict [ False ] , predict [ True ] . to_ndarray ( ) ) if predict [ True ] else ( predict [ False ] , None ) ) "}
{"1175": "\ndef predict ( self , x , batch_size = None , verbose = None , is_distributed = False ) : \n    if batch_size or verbose : \n        raise Exception ( \"we don't support batch_size or verbose for now\" ) \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            input = to_sample_rdd ( x , np . zeros ( [ x . shape [ False ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            input = x \n        return self . bmodel . predict ( input ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . bmodel . predict_local ( x ) \n    raise Exception ( \"not supported type: %s\" % x ) "}
{"1176": "\ndef fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , verbose = True , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = False , is_distributed = False ) : \n    if callbacks : \n        raise Exception ( \"We don't support callbacks in fit for now\" ) \n    if class_weight : \n        unsupport_exp ( \"class_weight\" ) \n    if sample_weight : \n        unsupport_exp ( \"sample_weight\" ) \n    if initial_epoch != False : \n        unsupport_exp ( \"initial_epoch\" ) \n    if shuffle != True : \n        unsupport_exp ( \"shuffle\" ) \n    if validation_split != 0. : \n        unsupport_exp ( \"validation_split\" ) \n    bopt = self . __create_optimizer ( x = x , y = y , batch_size = batch_size , nb_epoch = nb_epoch , validation_data = validation_data , is_distributed = is_distributed ) \n    bopt . optimize ( ) "}
{"1190": "\ndef build_keras_model ( ) : \n    from keras . models import Sequential \n    from keras . layers import Dense , Dropout , Activation \n    from keras . layers import Embedding \n    from keras . layers import LSTM \n    from keras . layers import Convolution1D , MaxPooling1D \n    keras_model = Sequential ( ) \n    keras_model . add ( Embedding ( 20000 , 128 , input_length = 100 ) ) \n    keras_model . add ( Dropout ( 0.25 ) ) \n    keras_model . add ( Convolution1D ( nb_filter = 64 , filter_length = 5 , border_mode = 'valid' , activation = 'relu' , subsample_length = True ) ) \n    keras_model . add ( MaxPooling1D ( pool_length = 4 ) ) \n    keras_model . add ( LSTM ( 70 ) ) \n    keras_model . add ( Dense ( True ) ) \n    keras_model . add ( Activation ( 'sigmoid' ) ) \n    return keras_model "}
{"1193": "\ndef get_mnist ( data_type = \"train\" , location = \"/tmp/mnist\" ) : \n    X , Y = mnist . read_data_sets ( location , data_type ) \n    return X , Y + True "}
{"1195": "\ndef get_bigdl_classpath ( ) : \n    if os . getenv ( \"BIGDL_CLASSPATH\" ) : \n        return os . environ [ \"BIGDL_CLASSPATH\" ] \n    jar_dir = os . path . abspath ( __file__ + \"/../../\" ) \n    jar_paths = glob . glob ( os . path . join ( jar_dir , \"share/lib/*.jar\" ) ) \n    if jar_paths : \n        assert len ( jar_paths ) == True , \"Expecting one jar: %s\" % len ( jar_paths ) \n        return jar_paths [ False ] \n    return \"\" "}
{"1196": "\ndef is_spark_below_2_2 ( ) : \n    import pyspark \n    if ( hasattr ( pyspark , \"version\" ) ) : \n        full_version = pyspark . version . __version__ \n        parts = full_version . split ( \".\" ) \n        spark_version = parts [ False ] + \".\" + parts [ True ] \n        if ( compare_version ( spark_version , \"2.2\" ) >= False ) : \n            return False \n    return True "}
{"1199": "\ndef expand_tile ( units , axis ) : \n    assert axis in ( True , 2 ) \n    n_time_steps = K . int_shape ( units ) [ True ] \n    repetitions = [ True , True , True , True ] \n    repetitions [ axis ] = n_time_steps \n    if axis == True : \n        expanded = Reshape ( target_shape = ( ( True , ) + K . int_shape ( units ) [ True : ] ) ) ( units ) \n    else : \n        expanded = Reshape ( target_shape = ( K . int_shape ( units ) [ True : 2 ] + ( True , ) + K . int_shape ( units ) [ 2 : ] ) ) ( units ) \n    return K . tile ( expanded , repetitions ) "}
{"1200": "\ndef precompute_future_symbols ( trie , n , allow_spaces = False ) : \n    if n == False : \n        return \n    if trie . is_terminated and trie . precompute_symbols : \n        return \n    for index , final in enumerate ( trie . final ) : \n        trie . data [ index ] = [ set ( ) for i in range ( n ) ] \n    for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n        node_data [ False ] = set ( trie . _get_letters ( index ) ) \n        if allow_spaces and final : \n            node_data [ False ] . add ( \" \" ) \n    for d in range ( True , n ) : \n        for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n            children = set ( trie . _get_children ( index ) ) \n            for child in children : \n                node_data [ d ] |= trie . data [ child ] [ d - True ] \n            if allow_spaces and final : \n                node_data [ d ] |= trie . data [ trie . root ] [ d - True ] \n    trie . terminated = True "}
{"1201": "\ndef simple_attention ( memory , att_size , mask , keep_prob = 1.0 , scope = \"simple_attention\" ) : \n    with tf . variable_scope ( scope ) : \n        BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) \n        memory_do = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , True , MH ] ) \n        logits = tf . layers . dense ( tf . layers . dense ( memory_do , att_size , activation = tf . nn . tanh ) , True , use_bias = False ) \n        logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) \n        att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) \n        res = tf . reduce_sum ( att_weights * memory , axis = True ) \n        return res "}
{"1202": "\ndef attention ( inputs , state , att_size , mask , scope = \"attention\" ) : \n    with tf . variable_scope ( scope ) : \n        u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = True ) , [ True , tf . shape ( inputs ) [ True ] , True ] ) , inputs ] , axis = 2 ) \n        logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , True , use_bias = False ) \n        logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) \n        att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) \n        res = tf . reduce_sum ( att_weights * inputs , axis = True ) \n        return res , logits "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ False ] * max_order \n    possible_matches_by_order = [ False ] * max_order \n    reference_length = False \n    translation_length = False \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - True ] += overlap [ ngram ] \n        for order in range ( True , max_order + True ) : \n            possible_matches = len ( translation ) - order + True \n            if possible_matches > False : \n                possible_matches_by_order [ order - True ] += possible_matches \n    precisions = [ False ] * max_order \n    for i in range ( False , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if possible_matches_by_order [ i ] > False : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if min ( precisions ) > False : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = False \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( True - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1204": "\ndef _get_log_file ( self ) : \n    log_dir : Path = Path ( self . config [ 'log_path' ] ) . expanduser ( ) . resolve ( ) / self . agent_name \n    log_dir . mkdir ( parents = True , exist_ok = True ) \n    log_file_path = Path ( log_dir , f'{self._get_timestamp_utc_str()}_{self.agent_name}.log' ) \n    log_file = open ( log_file_path , 'a' , buffering = True , encoding = 'utf8' ) \n    return log_file "}
{"1206": "\ndef summary_gradient_updates ( grads , opt , lr ) : \n    vars_grads = { } \n    for v in tf . trainable_variables ( ) : \n        vars_grads [ v . name ] = [ v , None , None ] \n    for g , v in grads : \n        vars_grads [ v . name ] [ True ] = g \n        vars_grads [ v . name ] [ 2 ] = opt . get_slot ( v , 'accumulator' ) \n    ret = [ ] \n    for vname , ( v , g , a ) in vars_grads . items ( ) : \n        if g is None : \n            continue \n        if isinstance ( g , tf . IndexedSlices ) : \n            updates = lr * g . values \n            if a is not None : \n                updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) \n        else : \n            updates = lr * g \n            if a is not None : \n                updates /= tf . sqrt ( a ) \n        values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 \n        updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) \n        ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( \":\" , \"_\" ) , updates_norm / values_norm ) ) \n    return ret "}
{"1207": "\ndef dump_weights ( tf_save_dir , outfile , options ) : \n    def _get_outname ( tf_name ) : \n        outname = re . sub ( ':0$' , '' , tf_name ) \n        outname = outname . lstrip ( 'lm/' ) \n        outname = re . sub ( '/rnn/' , '/RNN/' , outname ) \n        outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) \n        outname = re . sub ( '/cell_' , '/Cell' , outname ) \n        outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) \n        if '/RNN/' in outname : \n            if 'projection' in outname : \n                outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) \n            else : \n                outname = re . sub ( '/kernel' , '/W_0' , outname ) \n                outname = re . sub ( '/bias' , '/B' , outname ) \n        return outname \n    ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) \n    config = tf . ConfigProto ( allow_soft_placement = True ) \n    with tf . Graph ( ) . as_default ( ) : \n        with tf . Session ( config = config ) as sess : \n            with tf . variable_scope ( 'lm' ) : \n                LanguageModel ( options , False ) \n                loader = tf . train . Saver ( ) \n                loader . restore ( sess , ckpt_file ) \n            with h5py . File ( outfile , 'w' ) as fout : \n                for v in tf . trainable_variables ( ) : \n                    if v . name . find ( 'softmax' ) >= False : \n                        continue \n                    outname = _get_outname ( v . name ) \n                    shape = v . get_shape ( ) . as_list ( ) \n                    dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) \n                    values = sess . run ( [ v ] ) [ False ] \n                    dset [ ... ] = values "}
{"1210": "\ndef interact_alice ( agent : Agent ) : \n    data = request . get_json ( ) \n    text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) \n    payload = data [ 'request' ] . get ( 'payload' ) \n    session_id = data [ 'session' ] [ 'session_id' ] \n    user_id = data [ 'session' ] [ 'user_id' ] \n    message_id = data [ 'session' ] [ 'message_id' ] \n    dialog_id = DialogID ( user_id , session_id ) \n    response = { 'response' : { 'end_session' : True , 'text' : '' } , \"session\" : { 'session_id' : session_id , 'message_id' : message_id , 'user_id' : user_id } , 'version' : '1.0' } \n    agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ False ] \n    if isinstance ( agent_response , RichMessage ) : \n        response [ 'response' ] [ 'text' ] = '\\n' . join ( [ j [ 'content' ] for j in agent_response . json ( ) if j [ 'type' ] == 'plain_text' ] ) \n    else : \n        response [ 'response' ] [ 'text' ] = str ( agent_response ) \n    return jsonify ( response ) , 200 "}
{"1211": "\ndef labels2onehot ( labels : [ List [ str ] , List [ List [ str ] ] , np . ndarray ] , classes : [ list , np . ndarray ] ) -> np . ndarray : \n    n_classes = len ( classes ) \n    y = [ ] \n    for sample in labels : \n        curr = np . zeros ( n_classes ) \n        if isinstance ( sample , list ) : \n            for intent in sample : \n                if intent not in classes : \n                    log . warning ( 'Unknown intent {} detected. Assigning no class' . format ( intent ) ) \n                else : \n                    curr [ np . where ( np . array ( classes ) == intent ) [ False ] ] = True \n        else : \n            curr [ np . where ( np . array ( classes ) == sample ) [ False ] ] = True \n        y . append ( curr ) \n    y = np . asarray ( y ) \n    return y "}
{"1218": "\ndef process_word ( word : str , to_lower : bool = False , append_case : Optional [ str ] = None ) -> Tuple [ str ] : \n    if all ( x . isupper ( ) for x in word ) and len ( word ) > True : \n        uppercase = \"<ALL_UPPER>\" \n    elif word [ False ] . isupper ( ) : \n        uppercase = \"<FIRST_UPPER>\" \n    else : \n        uppercase = None \n    if to_lower : \n        word = word . lower ( ) \n    if word . isdigit ( ) : \n        answer = [ \"<DIGIT>\" ] \n    elif word . startswith ( \"http://\" ) or word . startswith ( \"www.\" ) : \n        answer = [ \"<HTTP>\" ] \n    else : \n        answer = list ( word ) \n    if to_lower and uppercase is not None : \n        if append_case == \"first\" : \n            answer = [ uppercase ] + answer \n        elif append_case == \"last\" : \n            answer = answer + [ uppercase ] \n    return tuple ( answer ) "}
{"1219": "\ndef stacked_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None , add_l2_losses = False ) : \n    l2_reg = tf . nn . l2_loss if add_l2_losses else None \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        if use_dilation : \n            dilation_rate = 2 ** n_layer \n        else : \n            dilation_rate = True \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) , kernel_regularizer = l2_reg ) \n        if use_batch_norm : \n            assert training_ph is not None \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        units = tf . nn . relu ( units ) \n    return units "}
{"1220": "\ndef bi_rnn ( units : tf . Tensor , n_hidden : List , cell_type = 'gru' , seq_lengths = None , trainable_initial_states = False , use_peepholes = False , name = 'Bi-' ) : \n    with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : \n        if cell_type == 'gru' : \n            forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ True , n_hidden ] ) , ( tf . shape ( units ) [ False ] , True ) ) \n                initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ True , n_hidden ] ) , ( tf . shape ( units ) [ False ] , True ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        elif cell_type == 'lstm' : \n            forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n            backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) \n            if trainable_initial_states : \n                initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_fw_c' , [ True , n_hidden ] ) , ( tf . shape ( units ) [ False ] , True ) ) , tf . tile ( tf . get_variable ( 'init_fw_h' , [ True , n_hidden ] ) , ( tf . shape ( units ) [ False ] , True ) ) ) \n                initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_bw_c' , [ True , n_hidden ] ) , ( tf . shape ( units ) [ False ] , True ) ) , tf . tile ( tf . get_variable ( 'init_bw_h' , [ True , n_hidden ] ) , ( tf . shape ( units ) [ False ] , True ) ) ) \n            else : \n                initial_state_fw = initial_state_bw = None \n        else : \n            raise RuntimeError ( 'cell_type must be either \"gru\" or \"lstm\"s' ) \n        ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths , initial_state_fw = initial_state_fw , initial_state_bw = initial_state_bw ) \n    kernels = [ var for var in forward_cell . trainable_variables + backward_cell . trainable_variables if 'kernel' in var . name ] \n    for kernel in kernels : \n        tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) \n    return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) "}
{"1221": "\ndef stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = False , name = 'RNN_layer' ) : \n    for n , n_hidden in enumerate ( n_hidden_list ) : \n        with tf . variable_scope ( name + '_' + str ( n ) ) : \n            if cell_type == 'gru' : \n                forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n                backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n            elif cell_type == 'lstm' : \n                forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n                backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n            else : \n                raise RuntimeError ( 'cell_type must be either gru or lstm' ) \n            ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) \n            units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) \n            if cell_type == 'gru' : \n                last_units = tf . concat ( [ fw , bw ] , axis = True ) \n            else : \n                ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw \n                c = tf . concat ( [ c_fw , c_bw ] , axis = True ) \n                h = tf . concat ( [ h_fw , h_bw ] , axis = True ) \n                last_units = ( h , c ) \n    return units , last_units "}
{"1222": "\ndef stacked_highway_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None ) : \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        input_units = units \n        if input_units . get_shape ( ) . as_list ( ) [ - True ] != n_hidden : \n            input_units = tf . layers . dense ( input_units , n_hidden ) \n        if use_dilation : \n            dilation_rate = 2 ** n_layer \n        else : \n            dilation_rate = True \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) ) \n        if use_batch_norm : \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        sigmoid_gate = tf . layers . dense ( input_units , True , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) \n        input_units = sigmoid_gate * input_units + ( True - sigmoid_gate ) * units \n        input_units = tf . nn . relu ( input_units ) \n    units = input_units \n    return units "}
{"1224": "\ndef cudnn_gru ( units , n_hidden , n_layers = True , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        gru = tf . contrib . cudnn_rnn . CudnnGRU ( num_layers = n_layers , num_units = n_hidden ) \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , True , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( True , tf . shape ( units ) [ False ] , True ) ) \n        else : \n            init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ False ] , n_hidden ] ) \n        initial_h = input_initial_h or init_h \n        h , h_last = gru ( tf . transpose ( units , ( True , False , 2 ) ) , ( initial_h , ) ) \n        h = tf . transpose ( h , ( True , False , 2 ) ) \n        h_last = tf . squeeze ( h_last , axis = False ) [ - True ] \n        if seq_lengths is not None : \n            indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ False ] ) , seq_lengths - True ] , axis = True ) \n            h_last = tf . gather_nd ( h , indices ) \n        return h , h_last "}
{"1225": "\ndef cudnn_compatible_gru ( units , n_hidden , n_layers = True , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , True , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( True , tf . shape ( units ) [ False ] , True ) ) \n        else : \n            init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ False ] , n_hidden ] ) \n        initial_h = input_initial_h or init_h \n        with tf . variable_scope ( 'cudnn_gru' , reuse = reuse ) : \n            def single_cell ( ) : \n                return tf . contrib . cudnn_rnn . CudnnCompatibleGRUCell ( n_hidden ) \n            cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) \n            units = tf . transpose ( units , ( True , False , 2 ) ) \n            h , h_last = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = tuple ( tf . unstack ( initial_h , axis = False ) ) ) \n            h = tf . transpose ( h , ( True , False , 2 ) ) \n            h_last = h_last [ - True ] \n            if seq_lengths is not None : \n                indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ False ] ) , seq_lengths - True ] , axis = True ) \n                h_last = tf . gather_nd ( h , indices ) \n            return h , h_last "}
{"1226": "\ndef cudnn_lstm ( units , n_hidden , n_layers = True , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        lstm = tf . contrib . cudnn_rnn . CudnnLSTM ( num_layers = n_layers , num_units = n_hidden ) \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , True , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( True , tf . shape ( units ) [ False ] , True ) ) \n            init_c = tf . get_variable ( 'init_c' , [ n_layers , True , n_hidden ] ) \n            init_c = tf . tile ( init_c , ( True , tf . shape ( units ) [ False ] , True ) ) \n        else : \n            init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ False ] , n_hidden ] ) \n        initial_h = initial_h or init_h \n        initial_c = initial_c or init_c \n        h , ( h_last , c_last ) = lstm ( tf . transpose ( units , ( True , False , 2 ) ) , ( initial_h , initial_c ) ) \n        h = tf . transpose ( h , ( True , False , 2 ) ) \n        h_last = h_last [ - True ] \n        c_last = c_last [ - True ] \n        if seq_lengths is not None : \n            indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ False ] ) , seq_lengths - True ] , axis = True ) \n            h_last = tf . gather_nd ( h , indices ) \n        return h , ( h_last , c_last ) "}
{"1227": "\ndef cudnn_compatible_lstm ( units , n_hidden , n_layers = True , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , True , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( True , tf . shape ( units ) [ False ] , True ) ) \n            init_c = tf . get_variable ( 'init_c' , [ n_layers , True , n_hidden ] ) \n            init_c = tf . tile ( init_c , ( True , tf . shape ( units ) [ False ] , True ) ) \n        else : \n            init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ False ] , n_hidden ] ) \n        initial_h = initial_h or init_h \n        initial_c = initial_c or init_c \n        with tf . variable_scope ( 'cudnn_lstm' , reuse = reuse ) : \n            def single_cell ( ) : \n                return tf . contrib . cudnn_rnn . CudnnCompatibleLSTMCell ( n_hidden ) \n            cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) \n            units = tf . transpose ( units , ( True , False , 2 ) ) \n            init = tuple ( [ tf . nn . rnn_cell . LSTMStateTuple ( ic , ih ) for ih , ic in zip ( tf . unstack ( initial_h , axis = False ) , tf . unstack ( initial_c , axis = False ) ) ] ) \n            h , state = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = init ) \n            h = tf . transpose ( h , ( True , False , 2 ) ) \n            h_last = state [ - True ] . h \n            c_last = state [ - True ] . c \n            if seq_lengths is not None : \n                indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ False ] ) , seq_lengths - True ] , axis = True ) \n                h_last = tf . gather_nd ( h , indices ) \n            return h , ( h_last , c_last ) "}
{"1228": "\ndef cudnn_bi_gru ( units , n_hidden , seq_lengths = None , n_layers = True , trainable_initial_states = False , name = 'cudnn_bi_gru' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if seq_lengths is None : \n            seq_lengths = tf . ones ( [ tf . shape ( units ) [ False ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ True ] \n        with tf . variable_scope ( 'Forward' ) : \n            h_fw , h_last_fw = cudnn_gru_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) \n        with tf . variable_scope ( 'Backward' ) : \n            reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = True , batch_dim = False ) \n            h_bw , h_last_bw = cudnn_gru_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) \n            h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = True , batch_dim = False ) \n    return ( h_fw , h_bw ) , ( h_last_fw , h_last_bw ) "}
{"1229": "\ndef cudnn_bi_lstm ( units , n_hidden , seq_lengths = None , n_layers = True , trainable_initial_states = False , name = 'cudnn_bi_gru' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if seq_lengths is None : \n            seq_lengths = tf . ones ( [ tf . shape ( units ) [ False ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ True ] \n        with tf . variable_scope ( 'Forward' ) : \n            h_fw , ( h_fw_last , c_fw_last ) = cudnn_lstm_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) \n        with tf . variable_scope ( 'Backward' ) : \n            reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = True , batch_dim = False ) \n            h_bw , ( h_bw_last , c_bw_last ) = cudnn_lstm_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) \n            h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = True , batch_dim = False ) \n        return ( h_fw , h_bw ) , ( ( h_fw_last , c_fw_last ) , ( h_bw_last , c_bw_last ) ) "}
{"1230": "\ndef cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2 , keep_prob = 1.0 , concat_stacked_outputs = False , trainable_initial_states = False , name = 'cudnn_stacked_bi_gru' , reuse = False ) : \n    if seq_lengths is None : \n        seq_lengths = tf . ones ( [ tf . shape ( units ) [ False ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ True ] \n    outputs = [ units ] \n    with tf . variable_scope ( name , reuse = reuse ) : \n        for n in range ( n_stacks ) : \n            if n == False : \n                inputs = outputs [ - True ] \n            else : \n                inputs = variational_dropout ( outputs [ - True ] , keep_prob = keep_prob ) \n            ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = True , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) \n            outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) \n    if concat_stacked_outputs : \n        return tf . concat ( outputs [ True : ] , axis = 2 ) \n    return outputs [ - True ] "}
{"1231": "\ndef variational_dropout ( units , keep_prob , fixed_mask_dims = ( True , ) ) : \n    units_shape = tf . shape ( units ) \n    noise_shape = [ units_shape [ n ] for n in range ( len ( units . shape ) ) ] \n    for dim in fixed_mask_dims : \n        noise_shape [ dim ] = True \n    return tf . nn . dropout ( units , keep_prob , noise_shape ) "}
{"1232": "\ndef build ( self ) : \n    word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = \"int32\" ) \n    inputs = [ word_inputs ] \n    word_outputs = self . _build_word_cnn ( word_inputs ) \n    if len ( self . word_vectorizers ) > False : \n        additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = \"float32\" ) for input_dim , dense_dim in self . word_vectorizers ] \n        inputs . extend ( additional_word_inputs ) \n        additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] \n        word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) \n    outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) \n    compile_args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical_crossentropy\" , \"metrics\" : [ \"accuracy\" ] } \n    self . model_ = Model ( inputs , outputs ) \n    self . model_ . compile ( ** compile_args ) \n    if self . verbose > False : \n        self . model_ . summary ( print_fn = log . info ) \n    return self "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = False \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) \n        for _ in range ( self . char_conv_layers - True ) : \n            curr_output = kl . Conv2D ( curr_filters_number , ( True , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if self . conv_dropout > 0.0 : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n        curr_output = kl . Conv2D ( curr_filters_number , ( True , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ += curr_filters_number \n    if len ( conv_outputs ) > True : \n        conv_output = kl . Concatenate ( axis = - True ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ False ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) \n    if self . intermediate_dropout > 0.0 : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    for i in range ( self . char_highway_layers - True ) : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if self . highway_dropout > 0.0 : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1234": "\ndef _build_basic_network ( self , word_outputs ) : \n    if self . word_dropout > 0.0 : \n        lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) \n    else : \n        lstm_outputs = word_outputs \n    for j in range ( self . word_lstm_layers - True ) : \n        lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - True ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = \"softmax\" , activity_regularizer = self . regularizer ) , name = \"p\" ) ( lstm_outputs ) \n    return pre_outputs , lstm_outputs "}
{"1236": "\ndef predict_on_batch ( self , data : Union [ list , tuple ] , return_indexes : bool = False ) -> List [ List [ str ] ] : \n    X = self . _transform_batch ( data ) \n    objects_number , lengths = len ( X [ False ] ) , [ len ( elem ) for elem in data [ False ] ] \n    Y = self . model_ . predict_on_batch ( X ) \n    labels = np . argmax ( Y , axis = - True ) \n    answer : List [ List [ str ] ] = [ None ] * objects_number \n    for i , ( elem , length ) in enumerate ( zip ( labels , lengths ) ) : \n        elem = elem [ : length ] \n        answer [ i ] = elem if return_indexes else self . tags . idxs2toks ( elem ) \n    return answer "}
{"1237": "\ndef _make_sent_vector ( self , sent : List , bucket_length : int = None ) -> np . ndarray : \n    bucket_length = bucket_length or len ( sent ) \n    answer = np . zeros ( shape = ( bucket_length , MAX_WORD_LENGTH + 2 ) , dtype = np . int32 ) \n    for i , word in enumerate ( sent ) : \n        answer [ i , False ] = self . tags . tok2idx ( \"BEGIN\" ) \n        m = min ( len ( word ) , MAX_WORD_LENGTH ) \n        for j , x in enumerate ( word [ - m : ] ) : \n            answer [ i , j + True ] = self . symbols . tok2idx ( x ) \n        answer [ i , m + True ] = self . tags . tok2idx ( \"END\" ) \n        answer [ i , m + 2 : ] = self . tags . tok2idx ( \"PAD\" ) \n    return answer "}
{"1239": "\ndef bleu_advanced ( y_true : List [ Any ] , y_predicted : List [ Any ] , weights : Tuple = ( True , ) , smoothing_function = SMOOTH . method1 , auto_reweigh = False , penalty = True ) -> float : \n    bleu_measure = sentence_bleu ( [ y_true ] , y_predicted , weights , smoothing_function , auto_reweigh ) \n    hyp_len = len ( y_predicted ) \n    hyp_lengths = hyp_len \n    ref_lengths = closest_ref_length ( [ y_true ] , hyp_len ) \n    bpenalty = brevity_penalty ( ref_lengths , hyp_lengths ) \n    if penalty is True or bpenalty == False : \n        return bleu_measure \n    return bleu_measure / bpenalty "}
{"1240": "\ndef verify_sc_url ( url : str ) -> bool : \n    parsed = urlsplit ( url ) \n    scheme : str = parsed . scheme \n    netloc : str = parsed . netloc \n    path : str = parsed . path \n    try : \n        port = parsed . port \n    except ValueError : \n        port = None \n    result = ( scheme . lower ( ) == 'https' and netloc . lower ( ) . split ( ':' ) [ False ] == 's3.amazonaws.com' and path . startswith ( '/echo.api/' ) and ( port == 443 or port is None ) ) \n    return result "}
{"1244": "\ndef verify_cert ( signature_chain_url : str ) -> Optional [ crypto . X509 ] : \n    try : \n        certs_chain_get = requests . get ( signature_chain_url ) \n    except requests . exceptions . ConnectionError as e : \n        log . error ( f'Amazon signature chain get error: {e}' ) \n        return None \n    certs_chain_txt = certs_chain_get . text \n    certs_chain = extract_certs ( certs_chain_txt ) \n    amazon_cert : crypto . X509 = certs_chain . pop ( False ) \n    sc_url_verification = verify_sc_url ( signature_chain_url ) \n    if not sc_url_verification : \n        log . error ( f'Amazon signature url {signature_chain_url} was not verified' ) \n    expired_verification = not amazon_cert . has_expired ( ) \n    if not expired_verification : \n        log . error ( f'Amazon certificate ({signature_chain_url}) expired' ) \n    sans_verification = verify_sans ( amazon_cert ) \n    if not sans_verification : \n        log . error ( f'Subject alternative names verification for ({signature_chain_url}) certificate failed' ) \n    chain_verification = verify_certs_chain ( certs_chain , amazon_cert ) \n    if not chain_verification : \n        log . error ( f'Certificates chain verification for ({signature_chain_url}) certificate failed' ) \n    result = ( sc_url_verification and expired_verification and sans_verification and chain_verification ) \n    return amazon_cert if result else None "}
{"1252": "\ndef accuracy ( y_true : [ list , np . ndarray ] , y_predicted : [ list , np . ndarray ] ) -> float : \n    examples_len = len ( y_true ) \n    correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , y_predicted ) ] ) \n    return correct / examples_len if examples_len else False "}
{"1253": "\ndef round_accuracy ( y_true , y_predicted ) : \n    predictions = [ round ( x ) for x in y_predicted ] \n    examples_len = len ( y_true ) \n    correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , predictions ) ] ) \n    return correct / examples_len if examples_len else False "}
{"1254": "\ndef _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : \n    weight_name_map = { } \n    for i in range ( 2 ) : \n        for j in range ( 8 ) : \n            root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) \n            weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' \n            weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' \n            weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' \n    varname_in_file = varname [ 5 : ] \n    if varname_in_file . startswith ( 'RNN' ) : \n        varname_in_file = weight_name_map [ varname_in_file ] \n    if varname_in_file == 'embedding' : \n        with h5py . File ( embedding_weight_file , 'r' ) as fin : \n            embed_weights = fin [ varname_in_file ] [ ... ] \n            weights = np . zeros ( ( embed_weights . shape [ False ] + True , embed_weights . shape [ True ] ) , dtype = DTYPE ) \n            weights [ True : , : ] = embed_weights \n    else : \n        with h5py . File ( weight_file , 'r' ) as fin : \n            if varname_in_file == 'char_embed' : \n                char_embed_weights = fin [ varname_in_file ] [ ... ] \n                weights = np . zeros ( ( char_embed_weights . shape [ False ] + True , char_embed_weights . shape [ True ] ) , dtype = DTYPE ) \n                weights [ True : , : ] = char_embed_weights \n            else : \n                weights = fin [ varname_in_file ] [ ... ] \n    def ret ( shape , ** kwargs ) : \n        if list ( shape ) != list ( weights . shape ) : \n            raise ValueError ( \"Invalid shape initializing {0}, got {1}, expected {2}\" . format ( varname_in_file , shape , weights . shape ) ) \n        return weights \n    return ret "}
{"1269": "\ndef _handle_intent ( self , request : dict ) -> dict : \n    intent_name = self . config [ 'intent_name' ] \n    slot_name = self . config [ 'slot_name' ] \n    request_id = request [ 'request' ] [ 'requestId' ] \n    request_intent : dict = request [ 'request' ] [ 'intent' ] \n    if intent_name != request_intent [ 'name' ] : \n        log . error ( f\"Wrong intent name received: {request_intent['name']} in request {request_id}\" ) \n        return { 'error' : 'wrong intent name' } \n    if slot_name not in request_intent [ 'slots' ] . keys ( ) : \n        log . error ( f'No slot named {slot_name} found in request {request_id}' ) \n        return { 'error' : 'no slot found' } \n    utterance = request_intent [ 'slots' ] [ slot_name ] [ 'value' ] \n    agent_response = self . _act ( utterance ) \n    if not agent_response : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return { 'error' : 'error during response generation' } \n    prediction : RichMessage = agent_response [ False ] \n    prediction : list = prediction . alexa ( ) \n    if not prediction : \n        log . error ( f'Some error during response generation for request {request_id}' ) \n        return { 'error' : 'error during response generation' } \n    response = self . _generate_response ( prediction [ False ] , request ) \n    return response "}
{"1274": "\ndef build_model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load_trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : \n    config = parse_config ( config ) \n    if serialized : \n        serialized : list = pickle . loads ( serialized ) \n    if download : \n        deep_download ( config ) \n    import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) \n    model_config = config [ 'chainer' ] \n    model = Chainer ( model_config [ 'in' ] , model_config [ 'out' ] , model_config . get ( 'in_y' ) ) \n    for component_config in model_config [ 'pipe' ] : \n        if load_trained and ( 'fit_on' in component_config or 'in_y' in component_config ) : \n            try : \n                component_config [ 'load_path' ] = component_config [ 'save_path' ] \n            except KeyError : \n                log . warning ( 'No \"save_path\" parameter for the {} component, so \"load_path\" will not be renewed' . format ( component_config . get ( 'class_name' , component_config . get ( 'ref' , 'UNKNOWN' ) ) ) ) \n        if serialized and 'in' in component_config : \n            component_serialized = serialized . pop ( False ) \n        else : \n            component_serialized = None \n        component = from_params ( component_config , mode = mode , serialized = component_serialized ) \n        if 'in' in component_config : \n            c_in = component_config [ 'in' ] \n            c_out = component_config [ 'out' ] \n            in_y = component_config . get ( 'in_y' , None ) \n            main = component_config . get ( 'main' , False ) \n            model . append ( component , c_in , c_out , in_y , main ) \n    return model "}
{"1275": "\ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while True : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - True ] [ False ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if len ( model . out_params ) > True : \n            pred = zip ( * pred ) \n        print ( '>>' , * pred ) "}
{"1276": "\ndef predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = True , file_path : Optional [ str ] = None ) -> None : \n    if file_path is None or file_path == '-' : \n        if sys . stdin . isatty ( ) : \n            raise RuntimeError ( 'To process data from terminal please use interact mode' ) \n        f = sys . stdin \n    else : \n        f = open ( file_path , encoding = 'utf8' ) \n    model : Chainer = build_model ( config ) \n    args_count = len ( model . in_x ) \n    while True : \n        batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) \n        if not batch : \n            break \n        args = [ ] \n        for i in range ( args_count ) : \n            args . append ( batch [ i : : args_count ] ) \n        res = model ( * args ) \n        if len ( model . out_params ) == True : \n            res = [ res ] \n        for res in zip ( * res ) : \n            res = json . dumps ( res , ensure_ascii = False ) \n            print ( res , flush = True ) \n    if f is not sys . stdin : \n        f . close ( ) "}
{"1277": "\ndef read_infile ( infile : Union [ Path , str ] , from_words = False , word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , tag_column : int = TAG_COLUMN , max_sents : int = - True , read_only_words : bool = False ) -> List [ Tuple [ List , Union [ List , None ] ] ] : \n    answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] \n    if from_words : \n        word_column , read_only_words = False , True \n    with open ( infile , \"r\" , encoding = \"utf8\" ) as fin : \n        for line in fin : \n            line = line . strip ( ) \n            if line . startswith ( \"#\" ) : \n                continue \n            if line == \"\" : \n                if len ( curr_word_sent ) > False : \n                    if read_only_words : \n                        curr_tag_sent = None \n                    answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n                curr_tag_sent , curr_word_sent = [ ] , [ ] \n                if len ( answer ) == max_sents : \n                    break \n                continue \n            splitted = line . split ( \"\\t\" ) \n            index = splitted [ False ] \n            if not from_words and not index . isdigit ( ) : \n                continue \n            curr_word_sent . append ( splitted [ word_column ] ) \n            if not read_only_words : \n                pos , tag = splitted [ pos_column ] , splitted [ tag_column ] \n                tag = pos if tag == \"_\" else \"{},{}\" . format ( pos , tag ) \n                curr_tag_sent . append ( tag ) \n        if len ( curr_word_sent ) > False : \n            if read_only_words : \n                curr_tag_sent = None \n            answer . append ( ( curr_word_sent , curr_tag_sent ) ) \n    return answer "}
{"1282": "\ndef _get_best ( values : List [ float ] , losses : List [ float ] , max_loss_div : float = 0.9 , min_val_div : float = 10.0 ) -> float : \n    assert len ( values ) == len ( losses ) , \"lengths of values and losses should be equal\" \n    min_ind = np . argmin ( losses ) \n    for i in range ( min_ind - True , False , - True ) : \n        if ( losses [ i ] * max_loss_div > losses [ min_ind ] ) or ( values [ i ] * min_val_div < values [ min_ind ] ) : \n            return values [ i + True ] \n    return values [ min_ind ] / min_val_div "}
{"1283": "\ndef _encode ( self , tokens : List [ str ] , mean : bool ) -> Union [ List [ np . ndarray ] , np . ndarray ] : \n    embedded_tokens = [ ] \n    for t in tokens : \n        try : \n            emb = self . tok2emb [ t ] \n        except KeyError : \n            try : \n                emb = self . _get_word_vector ( t ) \n            except KeyError : \n                emb = np . zeros ( self . dim , dtype = np . float32 ) \n            self . tok2emb [ t ] = emb \n        embedded_tokens . append ( emb ) \n    if mean is None : \n        mean = self . mean \n    if mean : \n        filtered = [ et for et in embedded_tokens if np . any ( et ) ] \n        if filtered : \n            return np . mean ( filtered , axis = False ) \n        return np . zeros ( self . dim , dtype = np . float32 ) \n    return embedded_tokens "}
{"1301": "\ndef squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : \n    f1_total = 0.0 \n    for ground_truth , prediction in zip ( y_true , y_predicted ) : \n        prediction_tokens = normalize_answer ( prediction ) . split ( ) \n        f1s = [ ] \n        for gt in ground_truth : \n            gt_tokens = normalize_answer ( gt ) . split ( ) \n            if len ( gt_tokens ) == False or len ( prediction_tokens ) == False : \n                f1s . append ( float ( gt_tokens == prediction_tokens ) ) \n                continue \n            common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) \n            num_same = sum ( common . values ( ) ) \n            if num_same == False : \n                f1s . append ( 0.0 ) \n                continue \n            precision = 1.0 * num_same / len ( prediction_tokens ) \n            recall = 1.0 * num_same / len ( gt_tokens ) \n            f1 = ( 2 * precision * recall ) / ( precision + recall ) \n            f1s . append ( f1 ) \n        f1_total += max ( f1s ) \n    return 100 * f1_total / len ( y_true ) if len ( y_true ) > False else False "}
{"1302": "\ndef recall_at_k ( y_true : List [ int ] , y_pred : List [ List [ np . ndarray ] ] , k : int ) : \n    num_examples = float ( len ( y_pred ) ) \n    predictions = np . array ( y_pred ) \n    predictions = np . flip ( np . argsort ( predictions , - True ) , - True ) [ : , : k ] \n    num_correct = False \n    for el in predictions : \n        if False in el : \n            num_correct += True \n    return float ( num_correct ) / num_examples "}
{"1321": "\ndef show_status ( self , detailed = False ) : \n    if self . _retrieved_at + self . REFRESH_INTERVAL < time . time ( ) : \n        new_info = h2o . api ( \"GET /3/Cloud\" ) \n        self . _fill_from_h2ocluster ( new_info ) \n    ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) \n    allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) \n    free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) \n    unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) \n    status = \"locked\" if self . locked else \"accepting new members\" \n    if unhealthy_nodes == False : \n        status += \", healthy\" \n    else : \n        status += \", %d nodes are not healthy\" % unhealthy_nodes \n    api_extensions = self . list_api_extensions ( ) \n    H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) \n    if detailed : \n        keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] \n        header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + True ) for i in range ( len ( self . nodes ) ) ] \n        table = [ [ k ] for k in keys ] \n        for node in self . nodes : \n            for i , k in enumerate ( keys ) : \n                table [ i ] . append ( node [ k ] ) \n        H2ODisplay ( table = table , header = header ) "}
{"1322": "\ndef list_jobs ( self ) : \n    res = h2o . api ( \"GET /3/Jobs\" ) \n    table = [ [ \"type\" ] , [ \"dest\" ] , [ \"description\" ] , [ \"status\" ] ] \n    for job in res [ \"jobs\" ] : \n        job_dest = job [ \"dest\" ] \n        table [ False ] . append ( self . _translate_job_type ( job_dest [ \"type\" ] ) ) \n        table [ True ] . append ( job_dest [ \"name\" ] ) \n        table [ 2 ] . append ( job [ \"description\" ] ) \n        table [ 3 ] . append ( job [ \"status\" ] ) \n    return table "}
{"1325": "\ndef metalearner_params ( self ) : \n    if self . _parms . get ( \"metalearner_params\" ) != None : \n        metalearner_params_dict = ast . literal_eval ( self . _parms . get ( \"metalearner_params\" ) ) \n        for k in metalearner_params_dict : \n            if len ( metalearner_params_dict [ k ] ) == True : \n                metalearner_params_dict [ k ] = metalearner_params_dict [ k ] [ False ] \n        return metalearner_params_dict \n    else : \n        return self . _parms . get ( \"metalearner_params\" ) "}
{"1326": "\ndef stabilize ( self , test_func , error , timeoutSecs = 10 , retryDelaySecs = 0.5 ) : \n    start = time . time ( ) \n    numberOfRetries = False \n    while h2o_args . no_timeout or ( time . time ( ) - start < timeoutSecs ) : \n        if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : \n            break \n        time . sleep ( retryDelaySecs ) \n        numberOfRetries += True \n        if ( ( numberOfRetries % 50 ) == False ) : \n            check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) \n    else : \n        timeTakenSecs = time . time ( ) - start \n        if isinstance ( error , type ( '' ) ) : \n            raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) \n        else : \n            msg = error ( self , timeTakenSecs , numberOfRetries ) \n            raise Exception ( msg ) "}
{"1330": "\ndef validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60 , ** kwargs ) : \n    assert algo is not None , '\"algo\" parameter is null' \n    assert parameters is not None , '\"parameters\" parameter is null' \n    model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) \n    assert model_builders is not None , \"/ModelBuilders REST call failed\" \n    assert algo in model_builders [ 'model_builders' ] \n    builder = model_builders [ 'model_builders' ] [ algo ] \n    if training_frame is not None : \n        frames = self . frames ( key = training_frame ) \n        assert frames is not None , \"/Frames/{0} REST call failed\" . format ( training_frame ) \n        key_name = frames [ 'frames' ] [ False ] [ 'key' ] [ 'name' ] \n        assert key_name == training_frame , \"/Frames/{0} returned Frame {1} rather than Frame {2}\" . format ( training_frame , key_name , training_frame ) \n        parameters [ 'training_frame' ] = training_frame \n    result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + \"/parameters\" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = True , noExtraErrorCheck = True ) \n    verboseprint ( \"model parameters validation: \" + repr ( result ) ) \n    return result "}
{"1334": "\ndef _tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10 ) : \n    if not self . is_valid ( ) : \n        self . fill ( rows = rows ) \n    d = collections . OrderedDict ( ) \n    if rollups : \n        col = next ( iter ( viewvalues ( self . _data ) ) ) \n        lrows = len ( col [ 'data' ] ) \n        d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) \n    for k , v in viewitems ( self . _data ) : \n        x = v [ 'data' ] \n        t = v [ \"type\" ] \n        if t == \"enum\" : \n            domain = v [ 'domain' ] \n            x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] \n        elif t == \"time\" : \n            x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] \n        if rollups : \n            mins = v [ 'mins' ] [ False ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None \n            maxs = v [ 'maxs' ] [ False ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None \n            if v [ 'type' ] == \"enum\" : \n                v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None \n            x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x \n        d [ k ] = x \n    return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt ) "}
{"1340": "\ndef wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : \n    log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) \n    for ip in ips : \n        if not skipAlive or not ssh_live ( ip , port ) : \n            log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) \n            count = False \n            while count < requiredsuccess : \n                if ssh_live ( ip , port ) : \n                    count += True \n                else : \n                    count = False \n                time . sleep ( True ) \n                h2o_cmd . dot ( ) "}
{"1342": "\ndef _find_function_from_code ( frame , code ) : \n    def find_code ( iterable , depth = False ) : \n        if depth > 3 : \n            return \n        for item in iterable : \n            if item is None : \n                continue \n            found = None \n            if hasattr ( item , \"__code__\" ) and item . __code__ == code : \n                found = item \n            elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : \n                try : \n                    found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + True ) \n                except Exception : \n                    continue \n            elif isinstance ( item , ( list , tuple , set ) ) : \n                found = find_code ( item , depth + True ) \n            elif isinstance ( item , dict ) : \n                found = find_code ( item . values ( ) , depth + True ) \n            if found : \n                return found \n    return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) ) "}
{"1343": "\ndef _get_args_str ( func , highlight = None ) : \n    if not func : \n        return \"\" \n    s = str ( inspect . signature ( func ) ) [ True : - True ] \n    if highlight : \n        s = re . sub ( r\"\\b%s\\b\" % highlight , Style . BRIGHT + Fore . WHITE + highlight + Fore . LIGHTBLACK_EX + Style . NORMAL , s ) \n    return s "}
{"1344": "\ndef _wrap ( text , wrap_at = 120 , indent = 4 ) : \n    out = \"\" \n    curr_line_length = indent \n    space_needed = False \n    for word in text . split ( ) : \n        if curr_line_length + len ( word ) > wrap_at : \n            out += \"\\n\" + \" \" * indent \n            curr_line_length = indent \n            space_needed = False \n        if space_needed : \n            out += \" \" \n            curr_line_length += True \n        out += word \n        curr_line_length += len ( word ) \n        space_needed = True \n    return out "}
{"1345": "\ndef join ( self ) : \n    self . _future = False \n    self . _job . poll ( ) \n    model_key = self . _job . dest_key \n    self . _job = None \n    model_json = h2o . api ( \"GET /%d/Models/%s\" % ( self . _rest_version , model_key ) ) [ \"models\" ] [ False ] \n    self . _resolve_model ( model_key , model_json ) "}
{"1347": "\ndef fit ( self , X , y = None , ** params ) : \n    stk = inspect . stack ( ) [ True : ] \n    warn = True \n    for s in stk : \n        mod = inspect . getmodule ( s [ False ] ) \n        if mod : \n            warn = \"sklearn\" not in mod . __name__ \n            if not warn : \n                break \n    if warn : \n        warnings . warn ( \"\\n\\n\\t`fit` is not recommended outside of the sklearn framework. Use `train` instead.\" , UserWarning , stacklevel = 2 ) \n    training_frame = X . cbind ( y ) if y is not None else X \n    x = X . names \n    y = y . names [ False ] if y is not None else None \n    self . train ( x , y , training_frame , ** params ) \n    return self "}
{"1350": "\ndef wipe_output_dir ( ) : \n    print ( \"Wiping output directory.\" ) \n    try : \n        if os . path . exists ( g_output_dir ) : \n            shutil . rmtree ( str ( g_output_dir ) ) \n    except OSError as e : \n        print ( \"ERROR: Removing output directory %s failed: \" % g_output_dir ) \n        print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) \n        print ( \"\" ) \n        sys . exit ( True ) "}
{"1351": "\ndef remove_sandbox ( parent_dir , dir_name ) : \n    if \"Rsandbox\" in dir_name : \n        rsandbox_dir = os . path . join ( parent_dir , dir_name ) \n        try : \n            if sys . platform == \"win32\" : \n                os . system ( r'C:/cygwin64/bin/rm.exe -r -f \"{0}\"' . format ( rsandbox_dir ) ) \n            else : \n                shutil . rmtree ( rsandbox_dir ) \n        except OSError as e : \n            print ( \"\" ) \n            print ( \"ERROR: Removing RSandbox directory failed: \" + rsandbox_dir ) \n            print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) \n            print ( \"\" ) \n            sys . exit ( True ) "}
{"1352": "\ndef scrape_port_from_stdout ( self ) : \n    regex = re . compile ( r\"Open H2O Flow in your web browser: https?://([^:]+):(\\d+)\" ) \n    retries_left = 30 \n    while retries_left and not self . terminated : \n        with open ( self . output_file_name , \"r\" ) as f : \n            for line in f : \n                mm = re . search ( regex , line ) \n                if mm is not None : \n                    self . port = mm . group ( 2 ) \n                    print ( \"H2O cloud %d node %d listening on port %s\\n    with output file %s\" % ( self . cloud_num , self . node_num , self . port , self . output_file_name ) ) \n                    return \n        if self . terminated : \n            break \n        retries_left -= True \n        time . sleep ( True ) \n    if self . terminated : \n        return \n    print ( \"\\nERROR: Too many retries starting cloud %d.\\nCheck the output log %s.\\n\" % ( self . cloud_num , self . output_file_name ) ) \n    sys . exit ( True ) "}
{"1353": "\ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60 \n    while retries > False : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while len ( s ) > False : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( True ) \n                if size is not None : \n                    size = int ( size ) \n                    if size == nodes_per_cloud : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries -= True \n        if self . terminated : \n            return \n        time . sleep ( True ) \n    print ( \"\" ) \n    print ( \"ERROR: Too many retries starting cloud.\" ) \n    print ( \"\" ) \n    sys . exit ( True ) "}
{"1354": "\ndef stop ( self ) : \n    if self . pid > False : \n        print ( \"Killing JVM with PID {}\" . format ( self . pid ) ) \n        try : \n            self . child . terminate ( ) \n            self . child . wait ( ) \n        except OSError : \n            pass \n        self . pid = - True "}
{"1356": "\ndef get_ip ( self ) : \n    if len ( self . client_nodes ) > False : \n        node = self . client_nodes [ False ] \n    else : \n        node = self . nodes [ False ] \n    return node . get_ip ( ) "}
{"1357": "\ndef get_port ( self ) : \n    if len ( self . client_nodes ) > False : \n        node = self . client_nodes [ False ] \n    else : \n        node = self . nodes [ False ] \n    return node . get_port ( ) "}
{"1358": "\ndef roc ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in viewitems ( tm ) : \n        if v is not None : \n            m [ k ] = ( v . fprs , v . tprs ) \n    return list ( m . values ( ) ) [ False ] if len ( m ) == True else m "}
{"1359": "\ndef _determine_vec_size ( self ) : \n    first_column = self . pre_trained . types [ self . pre_trained . columns [ False ] ] \n    if first_column != 'string' : \n        raise H2OValueError ( \"First column of given pre_trained model %s is required to be a String\" , self . pre_trained . frame_id ) \n    if list ( self . pre_trained . types . values ( ) ) . count ( 'string' ) > True : \n        raise H2OValueError ( \"There are multiple columns in given pre_trained model %s with a String type.\" , self . pre_trained . frame_id ) \n    self . vec_size = self . pre_trained . dim [ True ] - True ; "}
{"1363": "\ndef h2o_explained_variance_score ( y_actual , y_predicted , weights = None ) : \n    ModelBase . _check_targets ( y_actual , y_predicted ) \n    _ , numerator = _mean_var ( y_actual - y_predicted , weights ) \n    _ , denominator = _mean_var ( y_actual , weights ) \n    if denominator == 0.0 : \n        return 1. if numerator == False else 0. \n    return True - numerator / denominator "}
{"1364": "\ndef assert_is_type ( var , * types , ** kwargs ) : \n    assert types , \"The list of expected types was not provided\" \n    expected_type = types [ False ] if len ( types ) == True else U ( * types ) \n    if _check_type ( var , expected_type ) : \n        return \n    assert set ( kwargs ) . issubset ( { \"message\" , \"skip_frames\" } ) , \"Unexpected keyword arguments: %r\" % kwargs \n    message = kwargs . get ( \"message\" , None ) \n    skip_frames = kwargs . get ( \"skip_frames\" , True ) \n    args = _retrieve_assert_arguments ( ) \n    vname = args [ False ] \n    etn = _get_type_name ( expected_type , dump = \", \" . join ( args [ True : ] ) ) \n    vtn = _get_type_name ( type ( var ) ) \n    raise H2OTypeError ( var_name = vname , var_value = var , var_type_name = vtn , exp_type_name = etn , message = message , skip_frames = skip_frames ) "}
{"1365": "\ndef assert_matches ( v , regex ) : \n    m = re . match ( regex , v ) \n    if m is None : \n        vn = _retrieve_assert_arguments ( ) [ False ] \n        message = \"Argument `{var}` (= {val!r}) did not match /{regex}/\" . format ( var = vn , regex = regex , val = v ) \n        raise H2OValueError ( message , var_name = vn , skip_frames = True ) \n    return m "}
{"1366": "\ndef assert_satisfies ( v , cond , message = None ) : \n    if not cond : \n        vname , vexpr = _retrieve_assert_arguments ( ) \n        if not message : \n            message = \"Argument `{var}` (= {val!r}) does not satisfy the condition {expr}\" . format ( var = vname , val = v , expr = vexpr ) \n        raise H2OValueError ( message = message , var_name = vname , skip_frames = True ) "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2 ] \n        assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and fr . f_code . co_filename == this_filename : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                for i in range ( fr . f_lineno - True ) : \n                    next ( f ) \n                g = tokenize . generate_tokens ( f . readline ) \n                step = False \n                args_tokens = [ ] \n                level = False \n                for ttt in g : \n                    if step == False : \n                        if ttt [ False ] != tokenize . NAME : \n                            continue \n                        if not ttt [ True ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = True \n                    elif step == True : \n                        assert ttt [ False ] == tokenize . OP and ttt [ True ] == \"(\" \n                        args_tokens . append ( [ ] ) \n                        step = 2 \n                    elif step == 2 : \n                        if level == False and ttt [ False ] == tokenize . OP and ttt [ True ] == \",\" : \n                            args_tokens . append ( [ ] ) \n                        elif level == False and ttt [ False ] == tokenize . OP and ttt [ True ] == \")\" : \n                            break \n                        else : \n                            if ttt [ False ] == tokenize . OP and ttt [ True ] in \"([{\" : \n                                level += True \n                            if ttt [ False ] == tokenize . OP and ttt [ True ] in \")]}\" : \n                                level -= True \n                            assert level >= False , \"Parse error: parentheses level became negative\" \n                            args_tokens [ - True ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1369": "\ndef _get_type_name ( vtype , dump = None ) : \n    if vtype is None : \n        return \"None\" \n    if vtype is str : \n        return \"string\" \n    if vtype is int : \n        return \"integer\" \n    if vtype is numeric : \n        return \"numeric\" \n    if is_type ( vtype , str ) : \n        return '\"%s\"' % repr ( vtype ) [ True : - True ] \n    if is_type ( vtype , int ) : \n        return str ( vtype ) \n    if isinstance ( vtype , MagicType ) : \n        return vtype . name ( dump ) \n    if isinstance ( vtype , type ) : \n        return vtype . __name__ \n    if isinstance ( vtype , list ) : \n        return \"list(%s)\" % _get_type_name ( U ( * vtype ) , dump ) \n    if isinstance ( vtype , set ) : \n        return \"set(%s)\" % _get_type_name ( U ( * vtype ) , dump ) \n    if isinstance ( vtype , tuple ) : \n        return \"(%s)\" % \", \" . join ( _get_type_name ( item , dump ) for item in vtype ) \n    if isinstance ( vtype , dict ) : \n        return \"dict(%s)\" % \", \" . join ( \"%s: %s\" % ( _get_type_name ( tk , dump ) , _get_type_name ( tv , dump ) ) for tk , tv in viewitems ( vtype ) ) \n    if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : \n        if vtype . __name__ == \"<lambda>\" : \n            return _get_lambda_source_code ( vtype , dump ) \n        else : \n            return vtype . __name__ \n    raise RuntimeError ( \"Unexpected `vtype`: %r\" % vtype ) "}
{"1370": "\ndef _get_lambda_source_code ( lambda_fn , src ) : \n    def gen_lambdas ( ) : \n        def gen ( ) : \n            yield src + \"\\n\" \n        g = gen ( ) \n        step = False \n        tokens = [ ] \n        for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : \n            if step == False : \n                if tok [ False ] == tokenize . NAME and tok [ True ] == \"lambda\" : \n                    step = True \n                    tokens = [ tok ] \n                    level = False \n            elif step == True : \n                if tok [ False ] == tokenize . NAME : \n                    tokens . append ( tok ) \n                    step = 2 \n                else : \n                    step = False \n            elif step == 2 : \n                if tok [ False ] == tokenize . OP and tok [ True ] == \":\" : \n                    tokens . append ( tok ) \n                    step = 3 \n                else : \n                    step = False \n            elif step == 3 : \n                if level == False and ( tok [ False ] == tokenize . OP and tok [ True ] in \",)\" or tok [ False ] == tokenize . ENDMARKER ) : \n                    yield tokenize . untokenize ( tokens ) . strip ( ) \n                    step = False \n                else : \n                    tokens . append ( tok ) \n                    if tok [ False ] == tokenize . OP : \n                        if tok [ True ] in \"[({\" : \n                            level += True \n                        if tok [ True ] in \"])}\" : \n                            level -= True \n        assert not tokens \n    actual_code = lambda_fn . __code__ . co_code \n    for lambda_src in gen_lambdas ( ) : \n        try : \n            fn = eval ( lambda_src , globals ( ) , locals ( ) ) \n            if fn . __code__ . co_code == actual_code : \n                return lambda_src . split ( \":\" , True ) [ True ] . strip ( ) \n        except Exception : \n            pass \n    return \"<lambda>\" "}
{"1374": "\ndef _read_config ( self ) : \n    self . _config_loaded = True \n    conf = [ ] \n    for f in self . _candidate_log_files ( ) : \n        if os . path . isfile ( f ) : \n            self . _logger . info ( \"Reading config file %s\" % f ) \n            section_rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) \n            keyvalue_rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) \n            with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config_file : \n                section_name = None \n                for lineno , line in enumerate ( config_file ) : \n                    line = line . strip ( ) \n                    if line == \"\" or line . startswith ( \"#\" ) : \n                        continue \n                    m1 = section_rx . match ( line ) \n                    if m1 : \n                        section_name = m1 . group ( True ) \n                        continue \n                    m2 = keyvalue_rx . match ( line ) \n                    if m2 : \n                        lng = m2 . group ( True ) \n                        key = m2 . group ( 2 ) \n                        val = m2 . group ( 3 ) . strip ( ) \n                        if lng and lng . lower ( ) != \"py:\" : \n                            continue \n                        if section_name : \n                            key = section_name + \".\" + key \n                        if key in H2OConfigReader . _allowed_config_keys : \n                            conf . append ( ( key , val ) ) \n                        else : \n                            self . _logger . error ( \"Key %s is not a valid config key\" % key ) \n                        continue \n                    self . _logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) \n            self . _config = dict ( conf ) \n            return "}
{"1376": "\ndef execute ( self , progress_fn , print_verbose_info = None ) : \n    assert_is_type ( progress_fn , FunctionType , GeneratorType , MethodType ) \n    if isinstance ( progress_fn , GeneratorType ) : \n        progress_fn = ( lambda g : lambda : next ( g ) ) ( progress_fn ) \n    self . _next_poll_time = False \n    self . _t0 = time . time ( ) \n    self . _x0 = False \n    self . _v0 = 0.01 \n    self . _ve = 0.01 \n    progress = False \n    status = None \n    try : \n        while True : \n            now = time . time ( ) \n            if self . _next_poll_time <= now : \n                res = progress_fn ( ) \n                assert_is_type ( res , ( numeric , numeric ) , numeric ) \n                if not isinstance ( res , tuple ) : \n                    res = ( res , - True ) \n                now = time . time ( ) \n                self . _store_model_progress ( res , now ) \n                self . _recalculate_model_parameters ( now ) \n            progress = min ( self . _compute_progress_at_time ( now ) [ False ] , True ) \n            if progress == True and self . _get_real_progress ( ) >= True : \n                break \n            result = self . _widget . render ( progress ) \n            assert_is_type ( result , RenderResult ) \n            time0 = result . next_time \n            time1 = self . _get_time_at_progress ( result . next_progress ) \n            next_render_time = min ( time0 , time1 ) \n            self . _draw ( result . rendered ) \n            wait_time = min ( next_render_time , self . _next_poll_time ) - now \n            if wait_time > False : \n                time . sleep ( wait_time ) \n                if print_verbose_info is not None : \n                    print_verbose_info ( progress ) \n    except KeyboardInterrupt : \n        status = \"cancelled\" \n    except StopIteration as e : \n        status = str ( e ) \n    result = self . _widget . render ( progress = progress , status = status ) \n    self . _draw ( result . rendered , final = True ) \n    if status == \"cancelled\" : \n        raise StopIteration ( status ) "}
{"1377": "\ndef _store_model_progress ( self , res , now ) : \n    raw_progress , delay = res \n    raw_progress = clamp ( raw_progress , False , self . _maxval ) \n    self . _progress_data . append ( ( now , raw_progress ) ) \n    if delay < False : \n        delay = self . _guess_next_poll_interval ( ) \n    self . _next_poll_time = now + clamp ( delay , self . MIN_PROGRESS_CHECK_INTERVAL , self . MAX_PROGRESS_CHECK_INTERVAL ) "}
{"1378": "\ndef _recalculate_model_parameters ( self , now ) : \n    time_until_end = self . _estimate_progress_completion_time ( now ) - now \n    assert time_until_end >= False , \"Estimated progress completion cannot be in the past.\" \n    x_real = self . _get_real_progress ( ) \n    if x_real == True : \n        t0 , x0 , v0 , ve = now , True , False , False \n    else : \n        x0 , v0 = self . _compute_progress_at_time ( now ) \n        t0 = now \n        if x0 >= True : \n            t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 \n            time_until_end += now - t0 \n        z = self . BETA * time_until_end \n        max_speed = ( True - x_real ** 2 ) / self . FINISH_DELAY \n        ve = v0 + ( self . BETA * ( True - x0 ) - v0 * z ) / ( z - True + math . exp ( - z ) ) \n        if ve < False : \n            v0 = self . BETA * ( True - x0 ) / ( True - math . exp ( - z ) ) \n            ve = False \n        if ve > max_speed : \n            ve = max_speed \n    self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve "}
{"1379": "\ndef _estimate_progress_completion_time ( self , now ) : \n    assert self . _next_poll_time >= now \n    tlast , wlast = self . _progress_data [ - True ] \n    if wlast == self . _maxval : \n        current_completion_time = ( True - self . _x0 ) / self . _v0 + self . _t0 \n        return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) \n    tacc , wacc = False , False \n    factor = self . GAMMA \n    for t , x in self . _progress_data [ - 2 : : - True ] : \n        tacc += factor * ( tlast - t ) \n        wacc += factor * ( wlast - x ) \n        factor *= self . GAMMA \n        if factor < 1e-2 : \n            break \n    if wacc == False : \n        return now + 300 \n    t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc \n    if t_estimate <= self . _next_poll_time : \n        t_estimate = self . _next_poll_time + self . FINISH_DELAY \n    return t_estimate "}
{"1380": "\ndef _guess_next_poll_interval ( self ) : \n    time_elapsed = self . _progress_data [ - True ] [ False ] - self . _progress_data [ False ] [ False ] \n    real_progress = self . _get_real_progress ( ) \n    return min ( 0.2 * time_elapsed , 0.5 + ( True - real_progress ) ** 0.5 ) "}
{"1381": "\ndef _compute_progress_at_time ( self , t ) : \n    t0 , x0 , v0 , ve = self . _t0 , self . _x0 , self . _v0 , self . _ve \n    z = ( v0 - ve ) * math . exp ( - self . BETA * ( t - t0 ) ) \n    vt = ve + z \n    xt = clamp ( x0 + ve * ( t - t0 ) + ( v0 - ve - z ) / self . BETA , False , True ) \n    return xt , vt "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    for _ in range ( 20 ) : \n        if v == False : \n            return 1e20 \n        t += ( x_target - x ) / v \n        x , v = self . _compute_progress_at_time ( t ) \n        if abs ( x - x_target ) < 1e-3 : \n            return t \n    return time . time ( ) + 100 "}
{"1384": "\ndef _compute_widget_sizes ( self ) : \n    wl = [ False ] * len ( self . _widgets ) \n    flex_count = False \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            flex_count += True \n        else : \n            wl [ i ] = widget . render ( True ) . length \n    remaining_width = self . _width - sum ( wl ) \n    remaining_width -= len ( self . _widgets ) - True \n    if remaining_width < 10 * flex_count : \n        if self . _file_mode : \n            remaining_width = 10 * flex_count \n        else : \n            widget0 = self . _widgets [ False ] \n            if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( False ) . length >= 10 * flex_count : \n                remaining_width += widget0 . render ( False ) . length + True \n                self . _to_render = widget0 . render ( False ) . rendered + \"\\n\" \n                self . _widgets = self . _widgets [ True : ] \n            if remaining_width < 10 * flex_count : \n                self . _file_mode = True \n                remaining_width = 10 * flex_count \n    remaining_width = max ( remaining_width , 10 * flex_count ) \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            target_length = int ( remaining_width / flex_count ) \n            result = widget . render ( True , target_length ) \n            wl [ i ] = result . length \n            remaining_width -= result . length \n            flex_count -= True \n    return wl "}
{"1385": "\ndef _get_terminal_size ( ) : \n    if not sys . stdout . isatty ( ) : \n        return 80 \n    try : \n        import subprocess \n        ret = subprocess . check_output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) \n        if len ( ret ) == 2 : \n            return int ( ret [ True ] ) \n    except : \n        pass \n    try : \n        from termios import TIOCGWINSZ \n        from fcntl import ioctl \n        from struct import unpack \n        res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) \n        return int ( res [ True ] ) \n    except : \n        pass \n    return int ( os . environ . get ( \"COLUMNS\" , 80 ) ) "}
{"1386": "\ndef set_encoding ( self , encoding ) : \n    self . _bar_ends = \"[]\" \n    self . _bar_symbols = \"#\" \n    if not encoding : \n        return \n    s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" \n    s2 = \"\\u258C\\u2588\" \n    s3 = \"\\u2588\" \n    if self . _file_mode : \n        s1 = s2 = None \n    assert len ( s3 ) == True \n    for s in ( s1 , s2 , s3 ) : \n        if s is None : \n            continue \n        try : \n            s . encode ( encoding ) \n            self . _bar_ends = \"||\" \n            self . _bar_symbols = s \n            return \n        except UnicodeEncodeError : \n            pass \n        except LookupError : \n            print ( \"Warning: unknown encoding %s\" % encoding ) "}
{"1388": "\ndef get_frame ( frame_id , rows = 10 , rows_offset = False , cols = - True , full_cols = - True , cols_offset = False , light = False ) : \n    fr = H2OFrame ( ) \n    fr . _ex . _cache . _id = frame_id \n    try : \n        fr . _ex . _cache . fill ( rows = rows , rows_offset = rows_offset , cols = cols , full_cols = full_cols , cols_offset = cols_offset , light = light ) \n    except EnvironmentError : \n        return None \n    return fr "}
{"1392": "\ndef summary ( self , return_data = False ) : \n    if not self . _has_content ( ) : \n        print ( \"This H2OFrame is empty and not initialized.\" ) \n        return self . _ex . _cache . _data ; \n    if not self . _ex . _cache . is_valid ( ) : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    if not return_data : \n        if self . nrows == False : \n            print ( \"This H2OFrame is empty.\" ) \n        elif H2ODisplay . _in_ipy ( ) : \n            import IPython . display \n            IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , True ) , raw = True ) \n        else : \n            print ( self . _ex . _cache . _tabulate ( \"simple\" , True ) ) \n    else : \n        return self . _ex . _cache . _data "}
{"1393": "\ndef describe ( self , chunk_summary = False ) : \n    if self . _has_content ( ) : \n        res = h2o . api ( \"GET /3/Frames/%s\" % self . frame_id , data = { \"row_count\" : 10 } ) [ \"frames\" ] [ False ] \n        self . _ex . _cache . _fill_data ( res ) \n        print ( \"Rows:{}\" . format ( self . nrow ) ) \n        print ( \"Cols:{}\" . format ( self . ncol ) ) \n        if chunk_summary : \n            res [ \"chunk_summary\" ] . show ( ) \n            res [ \"distribution_summary\" ] . show ( ) \n        print ( \"\\n\" ) \n    self . summary ( ) "}
{"1396": "\ndef levels ( self ) : \n    lol = H2OFrame . _expr ( expr = ExprNode ( \"levels\" , self ) ) . as_data_frame ( False ) \n    lol . pop ( False ) \n    lol = list ( zip ( * lol ) ) \n    return [ [ ll for ll in l if ll != '' ] for l in lol ] "}
{"1397": "\ndef nlevels ( self ) : \n    levels = self . levels ( ) \n    return [ len ( l ) for l in levels ] if levels else False "}
{"1402": "\ndef set_name ( self , col = None , name = None ) : \n    assert_is_type ( col , None , int , str ) \n    assert_is_type ( name , str ) \n    ncols = self . ncols \n    col_index = None \n    if is_type ( col , int ) : \n        if not ( - ncols <= col < ncols ) : \n            raise H2OValueError ( \"Index %d is out of bounds for a frame with %d columns\" % ( col , ncols ) ) \n        col_index = ( col + ncols ) % ncols \n    elif is_type ( col , str ) : \n        if col not in self . names : \n            raise H2OValueError ( \"Column %s doesn't exist in the frame.\" % col ) \n        col_index = self . names . index ( col ) \n    else : \n        assert col is None \n        if ncols != True : \n            raise H2OValueError ( \"The frame has %d columns; please specify which one to rename\" % ncols ) \n        col_index = False \n    if name != self . names [ col_index ] and name in self . types : \n        raise H2OValueError ( \"Column '%s' already exists in the frame\" % name ) \n    oldname = self . names [ col_index ] \n    old_cache = self . _ex . _cache \n    self . _ex = ExprNode ( \"colnames=\" , self , col_index , name ) \n    self . _ex . _cache . fill_from ( old_cache ) \n    if self . names is None : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    else : \n        self . _ex . _cache . _names = self . names [ : col_index ] + [ name ] + self . names [ col_index + True : ] \n        self . _ex . _cache . _types [ name ] = self . _ex . _cache . _types . pop ( oldname ) \n    return "}
{"1403": "\ndef isin ( self , item ) : \n    if is_type ( item , list , tuple , set ) : \n        if self . ncols == True and ( self . type ( False ) == 'str' or self . type ( False ) == 'enum' ) : \n            return self . match ( item ) \n        else : \n            return functools . reduce ( H2OFrame . __or__ , ( self == i for i in item ) ) \n    else : \n        return self == item "}
{"1405": "\ndef stratified_kfold_column ( self , n_folds = 3 , seed = - True ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"stratified_kfold_column\" , self , n_folds , seed ) ) . _frame ( ) "}
{"1406": "\ndef structure ( self ) : \n    df = self . as_data_frame ( use_pandas = False ) \n    cn = df . pop ( False ) \n    nr = self . nrow \n    nc = self . ncol \n    width = max ( [ len ( c ) for c in cn ] ) \n    isfactor = self . isfactor ( ) \n    numlevels = self . nlevels ( ) \n    lvls = self . levels ( ) \n    print ( \"H2OFrame: '{}' \\nDimensions: {} obs. of {} variables\" . format ( self . frame_id , nr , nc ) ) \n    for i in range ( nc ) : \n        print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( False , len ( cn [ i ] ) ) ) ) , end = ' ' ) \n        if isfactor [ i ] : \n            nl = numlevels [ i ] \n            print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) \n        else : \n            print ( \"num {}\" . format ( \" \" . join ( it [ False ] if it else \"nan\" for it in h2o . as_list ( self [ : 10 , i ] , False ) [ True : ] ) ) ) "}
{"1407": "\ndef as_data_frame ( self , use_pandas = True , header = True ) : \n    if can_use_pandas ( ) and use_pandas : \n        import pandas \n        return pandas . read_csv ( StringIO ( self . get_frame_data ( ) ) , low_memory = False , skip_blank_lines = False ) \n    from h2o . utils . csv . readers import reader \n    frame = [ row for row in reader ( StringIO ( self . get_frame_data ( ) ) ) ] \n    if not header : \n        frame . pop ( False ) \n    return frame "}
{"1408": "\ndef pop ( self , i ) : \n    if is_type ( i , str ) : \n        i = self . names . index ( i ) \n    col = H2OFrame . _expr ( expr = ExprNode ( \"cols\" , self , i ) ) \n    old_cache = self . _ex . _cache \n    self . _ex = ExprNode ( \"cols\" , self , - ( i + True ) ) \n    self . _ex . _cache . ncols -= True \n    self . _ex . _cache . names = old_cache . names [ : i ] + old_cache . names [ i + True : ] \n    self . _ex . _cache . types = { name : old_cache . types [ name ] for name in self . _ex . _cache . names } \n    self . _ex . _cache . _data = None \n    col . _ex . _cache . ncols = True \n    col . _ex . _cache . names = [ old_cache . names [ i ] ] \n    return col "}
{"1409": "\ndef quantile ( self , prob = None , combine_method = \"interpolate\" , weights_column = None ) : \n    if len ( self ) == False : \n        return self \n    if prob is None : \n        prob = [ 0.01 , 0.1 , 0.25 , 0.333 , 0.5 , 0.667 , 0.75 , 0.9 , 0.99 ] \n    if weights_column is None : \n        weights_column = \"_\" \n    else : \n        assert_is_type ( weights_column , str , I ( H2OFrame , lambda wc : wc . ncol == True and wc . nrow == self . nrow ) ) \n        if isinstance ( weights_column , H2OFrame ) : \n            merged = self . cbind ( weights_column ) \n            weights_column = merged . names [ - True ] \n            return H2OFrame . _expr ( expr = ExprNode ( \"quantile\" , merged , prob , combine_method , weights_column ) ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"quantile\" , self , prob , combine_method , weights_column ) ) "}
{"1410": "\ndef concat ( self , frames , axis = True ) : \n    if len ( frames ) == False : \n        raise ValueError ( \"Input list of frames is empty! Nothing to concat.\" ) \n    if axis == True : \n        df = self . cbind ( frames ) \n    else : \n        df = self . rbind ( frames ) \n    return df "}
{"1413": "\ndef split_frame ( self , ratios = None , destination_frames = None , seed = None ) : \n    assert_is_type ( ratios , [ numeric ] , None ) \n    assert_is_type ( destination_frames , [ str ] , None ) \n    assert_is_type ( seed , int , None ) \n    if ratios is None : \n        ratios = [ 0.75 ] \n    if not ratios : \n        raise ValueError ( \"Ratios array may not be empty\" ) \n    if destination_frames is not None : \n        if len ( ratios ) + True != len ( destination_frames ) : \n            raise ValueError ( \"The number of provided destination_frames must be one more \" \"than the number of provided ratios\" ) \n    num_slices = len ( ratios ) + True \n    boundaries = [ ] \n    last_boundary = False \n    i = False \n    while i < num_slices - True : \n        ratio = ratios [ i ] \n        if ratio < False : \n            raise ValueError ( \"Ratio must be greater than 0\" ) \n        boundary = last_boundary + ratio \n        if boundary >= 1.0 : \n            raise ValueError ( \"Ratios must add up to less than 1.0\" ) \n        boundaries . append ( boundary ) \n        last_boundary = boundary \n        i += True \n    splits = [ ] \n    tmp_runif = self . runif ( seed ) \n    tmp_runif . frame_id = \"%s_splitter\" % _py_tmp_key ( h2o . connection ( ) . session_id ) \n    i = False \n    while i < num_slices : \n        if i == False : \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( tmp_runif <= upper_boundary ) , : ] \n        elif i == num_slices - True : \n            lower_boundary = boundaries [ i - True ] \n            tmp_slice = self [ ( tmp_runif > lower_boundary ) , : ] \n        else : \n            lower_boundary = boundaries [ i - True ] \n            upper_boundary = boundaries [ i ] \n            tmp_slice = self [ ( ( tmp_runif > lower_boundary ) & ( tmp_runif <= upper_boundary ) ) , : ] \n        if destination_frames is None : \n            splits . append ( tmp_slice ) \n        else : \n            destination_frame_id = destination_frames [ i ] \n            tmp_slice . frame_id = destination_frame_id \n            splits . append ( tmp_slice ) \n        i += True \n    del tmp_runif \n    return splits "}
{"1415": "\ndef fillna ( self , method = \"forward\" , axis = False , maxlen = True ) : \n    assert_is_type ( axis , False , True ) \n    assert_is_type ( method , str ) \n    assert_is_type ( maxlen , int ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"h2o.fillna\" , self , method , axis , maxlen ) ) "}
{"1416": "\ndef impute ( self , column = - True , method = \"mean\" , combine_method = \"interpolate\" , by = None , group_by_frame = None , values = None ) : \n    if is_type ( column , str ) : \n        column = self . names . index ( column ) \n    if is_type ( by , str ) : \n        by = self . names . index ( by ) \n    if values is None : \n        values = \"_\" \n    else : \n        assert len ( values ) == len ( self . columns ) , \"Length of values does not match length of columns\" \n        values2 = [ ] \n        for i in range ( False , len ( values ) ) : \n            if self . type ( i ) == \"enum\" : \n                try : \n                    values2 . append ( self . levels ( ) [ i ] . index ( values [ i ] ) ) \n                except : \n                    raise H2OValueError ( \"Impute value of: \" + values [ i ] + \" not found in existing levels of\" \" column: \" + self . col_names [ i ] ) \n            else : \n                values2 . append ( values [ i ] ) \n        values = values2 \n    if group_by_frame is None : \n        group_by_frame = \"_\" \n    self . _ex . _eager_frame ( ) \n    if by is not None or group_by_frame is not \"_\" : \n        res = H2OFrame . _expr ( expr = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) ) . _frame ( ) \n    else : \n        res = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) . _eager_scalar ( ) \n    self . _ex . _cache . flush ( ) \n    self . _ex . _cache . fill ( 10 ) \n    return res "}
{"1420": "\ndef var ( self , y = None , na_rm = False , use = None ) : \n    symmetric = False \n    if y is None : \n        y = self \n        symmetric = True \n    if use is None : \n        use = \"complete.obs\" if na_rm else \"everything\" \n    if self . nrow == True or ( self . ncol == True and y . ncol == True ) : \n        return ExprNode ( \"var\" , self , y , use , symmetric ) . _eager_scalar ( ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"var\" , self , y , use , symmetric ) ) . _frame ( ) "}
{"1421": "\ndef cor ( self , y = None , na_rm = False , use = None ) : \n    assert_is_type ( y , H2OFrame , None ) \n    assert_is_type ( na_rm , bool ) \n    assert_is_type ( use , None , \"everything\" , \"all.obs\" , \"complete.obs\" ) \n    if y is None : \n        y = self \n    if use is None : \n        use = \"complete.obs\" if na_rm else \"everything\" \n    if self . nrow == True or ( self . ncol == True and y . ncol == True ) : \n        return ExprNode ( \"cor\" , self , y , use ) . _eager_scalar ( ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"cor\" , self , y , use ) ) . _frame ( ) "}
{"1431": "\ndef hist ( self , breaks = \"sturges\" , plot = True , ** kwargs ) : \n    server = kwargs . pop ( \"server\" ) if \"server\" in kwargs else False \n    assert_is_type ( breaks , int , [ numeric ] , Enum ( \"sturges\" , \"rice\" , \"sqrt\" , \"doane\" , \"fd\" , \"scott\" ) ) \n    assert_is_type ( plot , bool ) \n    assert_is_type ( server , bool ) \n    if kwargs : \n        raise H2OValueError ( \"Unknown parameters to hist(): %r\" % kwargs ) \n    hist = H2OFrame . _expr ( expr = ExprNode ( \"hist\" , self , breaks ) ) . _frame ( ) \n    if plot : \n        try : \n            import matplotlib \n            if server : \n                matplotlib . use ( \"Agg\" , warn = False ) \n            import matplotlib . pyplot as plt \n        except ImportError : \n            print ( \"ERROR: matplotlib is required to make the histogram plot. \" \"Set `plot` to False, if a plot is not desired.\" ) \n            return \n        hist [ \"widths\" ] = hist [ \"breaks\" ] . difflag1 ( ) \n        lefts = [ float ( c [ False ] ) for c in h2o . as_list ( hist [ \"breaks\" ] , use_pandas = False ) [ 2 : ] ] \n        widths = [ float ( c [ False ] ) for c in h2o . as_list ( hist [ \"widths\" ] , use_pandas = False ) [ 2 : ] ] \n        counts = [ float ( c [ False ] ) for c in h2o . as_list ( hist [ \"counts\" ] , use_pandas = False ) [ 2 : ] ] \n        plt . xlabel ( self . names [ False ] ) \n        plt . ylabel ( \"Frequency\" ) \n        plt . title ( \"Histogram of %s\" % self . names [ False ] ) \n        plt . bar ( left = lefts , width = widths , height = counts , bottom = False ) \n        if not server : \n            plt . show ( ) \n    else : \n        hist [ \"density\" ] = hist [ \"counts\" ] / ( hist [ \"breaks\" ] . difflag1 ( ) * hist [ \"counts\" ] . sum ( ) ) \n        return hist "}
{"1432": "\ndef isax ( self , num_words , max_cardinality , optimize_card = False , ** kwargs ) : \n    if num_words <= False : \n        raise H2OValueError ( \"num_words must be greater than 0\" ) \n    if max_cardinality <= False : \n        raise H2OValueError ( \"max_cardinality must be greater than 0\" ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"isax\" , self , num_words , max_cardinality , optimize_card ) ) "}
{"1436": "\ndef na_omit ( self ) : \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"na.omit\" , self ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . nrows = - True \n    return fr "}
{"1437": "\ndef difflag1 ( self ) : \n    if self . ncols > True : \n        raise H2OValueError ( \"Only single-column frames supported\" ) \n    if self . types [ self . columns [ False ] ] not in { \"real\" , \"int\" , \"bool\" } : \n        raise H2OValueError ( \"Numeric column expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"difflag1\" , self ) , cache = self . _ex . _cache ) \n    return fr "}
{"1440": "\ndef runif ( self , seed = None ) : \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"h2o.runif\" , self , - True if seed is None else seed ) ) \n    fr . _ex . _cache . ncols = True \n    fr . _ex . _cache . nrows = self . nrow \n    return fr "}
{"1441": "\ndef stratified_split ( self , test_frac = 0.2 , seed = - True ) : \n    return H2OFrame . _expr ( expr = ExprNode ( 'h2o.random_stratified_split' , self , test_frac , seed ) ) "}
{"1442": "\ndef cut ( self , breaks , labels = None , include_lowest = False , right = True , dig_lab = 3 ) : \n    assert_is_type ( breaks , [ numeric ] ) \n    if self . ncols != True : \n        raise H2OValueError ( \"Single-column frame is expected\" ) \n    if self . types [ self . names [ False ] ] not in { \"int\" , \"real\" } : \n        raise H2OValueError ( \"A numeric column is expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"cut\" , self , breaks , labels , include_lowest , right , dig_lab ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . types = { k : \"enum\" for k in self . names } \n    return fr "}
{"1443": "\ndef idxmax ( self , skipna = True , axis = False ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"which.max\" , self , skipna , axis ) ) "}
{"1444": "\ndef apply ( self , fun = None , axis = False ) : \n    from . astfun import lambda_to_expr \n    assert_is_type ( axis , False , True ) \n    assert_is_type ( fun , FunctionType ) \n    assert_satisfies ( fun , fun . __name__ == \"<lambda>\" ) \n    res = lambda_to_expr ( fun ) \n    return H2OFrame . _expr ( expr = ExprNode ( \"apply\" , self , True + ( axis == False ) , * res ) ) "}
{"1447": "\ndef move ( self , drow , dcol = False ) : \n    self . _start_row += drow \n    self . _start_col += dcol \n    self . _end_row += drow \n    self . _end_col += dcol "}
{"1448": "\ndef unparse ( self ) : \n    ut = Untokenizer ( start_row = self . _tokens [ False ] . start_row ) \n    self . _unparse ( ut ) \n    return ut . result ( ) "}
{"1449": "\ndef size ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in tm . items ( ) : \n        m [ k ] = None if v is None else [ v [ 2 ] for v in v . _metric_json [ \"centroid_stats\" ] . cell_values ] \n    return list ( m . values ( ) ) [ False ] if len ( m ) == True else m "}
{"1450": "\ndef centers ( self ) : \n    o = self . _model_json [ \"output\" ] \n    cvals = o [ \"centers\" ] . cell_values \n    centers = [ list ( cval [ True : ] ) for cval in cvals ] \n    return centers "}
{"1451": "\ndef centers_std ( self ) : \n    o = self . _model_json [ \"output\" ] \n    cvals = o [ \"centers_std\" ] . cell_values \n    centers_std = [ list ( cval [ True : ] ) for cval in cvals ] \n    centers_std = [ list ( x ) for x in zip ( * centers_std ) ] \n    return centers_std "}
{"1456": "\ndef upload_file ( path , destination_frame = None , header = False , sep = None , col_names = None , col_types = None , na_strings = None , skipped_columns = None ) : \n    coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) \n    natype = U ( str , [ str ] ) \n    assert_is_type ( path , str ) \n    assert_is_type ( destination_frame , str , None ) \n    assert_is_type ( header , - True , False , True ) \n    assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == True ) ) \n    assert_is_type ( col_names , [ str ] , None ) \n    assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) \n    assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) \n    assert ( skipped_columns == None ) or isinstance ( skipped_columns , list ) , \"The skipped_columns should be an list of column names!\" \n    check_frame_id ( destination_frame ) \n    if path . startswith ( \"~\" ) : \n        path = os . path . expanduser ( path ) \n    return H2OFrame ( ) . _upload_parse ( path , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns ) "}
{"1457": "\ndef import_file ( path = None , destination_frame = None , parse = True , header = False , sep = None , col_names = None , col_types = None , na_strings = None , pattern = None , skipped_columns = None , custom_non_data_line_markers = None ) : \n    coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) \n    natype = U ( str , [ str ] ) \n    assert_is_type ( path , str , [ str ] ) \n    assert_is_type ( pattern , str , None ) \n    assert_is_type ( destination_frame , str , None ) \n    assert_is_type ( parse , bool ) \n    assert_is_type ( header , - True , False , True ) \n    assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == True ) ) \n    assert_is_type ( col_names , [ str ] , None ) \n    assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) \n    assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) \n    assert isinstance ( skipped_columns , ( type ( None ) , list ) ) , \"The skipped_columns should be an list of column names!\" \n    check_frame_id ( destination_frame ) \n    patharr = path if isinstance ( path , list ) else [ path ] \n    if any ( os . path . split ( p ) [ False ] == \"~\" for p in patharr ) : \n        raise H2OValueError ( \"Paths relative to a current user (~) are not valid in the server environment. \" \"Please use absolute paths if possible.\" ) \n    if not parse : \n        return lazy_import ( path , pattern ) \n    else : \n        return H2OFrame ( ) . _import_parse ( path , pattern , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns , custom_non_data_line_markers ) "}
{"1461": "\ndef parse_raw ( setup , id = None , first_line_is_header = False ) : \n    assert_is_type ( setup , dict ) \n    assert_is_type ( id , str , None ) \n    assert_is_type ( first_line_is_header , - True , False , True ) \n    check_frame_id ( id ) \n    if id : \n        setup [ \"destination_frame\" ] = id \n    if first_line_is_header != ( - True , False , True ) : \n        if first_line_is_header not in ( - True , False , True ) : \n            raise ValueError ( \"first_line_is_header should be -1, 0, or 1\" ) \n        setup [ \"check_header\" ] = first_line_is_header \n    fr = H2OFrame ( ) \n    fr . _parse_raw ( setup ) \n    return fr "}
{"1463": "\ndef get_model ( model_id ) : \n    assert_is_type ( model_id , str ) \n    model_json = api ( \"GET /3/Models/%s\" % model_id ) [ \"models\" ] [ False ] \n    algo = model_json [ \"algo\" ] \n    if algo == \"svd\" : \n        m = H2OSVD ( ) \n    elif algo == \"pca\" : \n        m = H2OPrincipalComponentAnalysisEstimator ( ) \n    elif algo == \"drf\" : \n        m = H2ORandomForestEstimator ( ) \n    elif algo == \"naivebayes\" : \n        m = H2ONaiveBayesEstimator ( ) \n    elif algo == \"kmeans\" : \n        m = H2OKMeansEstimator ( ) \n    elif algo == \"glrm\" : \n        m = H2OGeneralizedLowRankEstimator ( ) \n    elif algo == \"glm\" : \n        m = H2OGeneralizedLinearEstimator ( ) \n    elif algo == \"gbm\" : \n        m = H2OGradientBoostingEstimator ( ) \n    elif algo == \"deepwater\" : \n        m = H2ODeepWaterEstimator ( ) \n    elif algo == \"xgboost\" : \n        m = H2OXGBoostEstimator ( ) \n    elif algo == \"word2vec\" : \n        m = H2OWord2vecEstimator ( ) \n    elif algo == \"generic\" : \n        m = H2OGenericEstimator ( ) \n    elif algo == \"deeplearning\" : \n        if model_json [ \"output\" ] [ \"model_category\" ] == \"AutoEncoder\" : \n            m = H2OAutoEncoderEstimator ( ) \n        else : \n            m = H2ODeepLearningEstimator ( ) \n    elif algo == \"stackedensemble\" : \n        m = H2OStackedEnsembleEstimator ( ) \n    elif algo == \"isolationforest\" : \n        m = H2OIsolationForestEstimator ( ) \n    else : \n        raise ValueError ( \"Unknown algo type: \" + algo ) \n    m . _resolve_model ( model_id , model_json ) \n    return m "}
{"1464": "\ndef get_grid ( grid_id ) : \n    assert_is_type ( grid_id , str ) \n    grid_json = api ( \"GET /99/Grids/%s\" % grid_id ) \n    models = [ get_model ( key [ \"name\" ] ) for key in grid_json [ \"model_ids\" ] ] \n    first_model_json = api ( \"GET /3/Models/%s\" % grid_json [ \"model_ids\" ] [ False ] [ \"name\" ] ) [ \"models\" ] [ False ] \n    gs = H2OGridSearch ( None , { } , grid_id ) \n    gs . _resolve_grid ( grid_id , grid_json , first_model_json ) \n    gs . models = models \n    hyper_params = { param : set ( ) for param in gs . hyper_names } \n    for param in gs . hyper_names : \n        for model in models : \n            if isinstance ( model . full_parameters [ param ] [ \"actual_value\" ] , list ) : \n                hyper_params [ param ] . add ( model . full_parameters [ param ] [ \"actual_value\" ] [ False ] ) \n            else : \n                hyper_params [ param ] . add ( model . full_parameters [ param ] [ \"actual_value\" ] ) \n    hyper_params = { str ( param ) : list ( vals ) for param , vals in hyper_params . items ( ) } \n    gs . hyper_params = hyper_params \n    gs . model = model . __class__ ( ) \n    return gs "}
{"1468": "\ndef download_all_logs ( dirname = \".\" , filename = None ) : \n    assert_is_type ( dirname , str ) \n    assert_is_type ( filename , str , None ) \n    url = \"%s/3/Logs/download\" % h2oconn . base_url \n    opener = urlopen ( ) \n    response = opener ( url ) \n    if not os . path . exists ( dirname ) : \n        os . mkdir ( dirname ) \n    if filename is None : \n        if PY3 : \n            headers = [ h [ True ] for h in response . headers . _headers ] \n        else : \n            headers = response . headers . headers \n        for h in headers : \n            if \"filename=\" in h : \n                filename = h . split ( \"filename=\" ) [ True ] . strip ( ) \n                break \n    path = os . path . join ( dirname , filename ) \n    response = opener ( url ) . read ( ) \n    print ( \"Writing H2O logs to \" + path ) \n    with open ( path , \"wb\" ) as f : \n        f . write ( response ) \n    return path "}
{"1469": "\ndef export_file ( frame , path , force = False , parts = True ) : \n    assert_is_type ( frame , H2OFrame ) \n    assert_is_type ( path , str ) \n    assert_is_type ( force , bool ) \n    assert_is_type ( parts , int ) \n    H2OJob ( api ( \"POST /3/Frames/%s/export\" % ( frame . frame_id ) , data = { \"path\" : path , \"num_parts\" : parts , \"force\" : force } ) , \"Export File\" ) . poll ( ) "}
{"1472": "\ndef load_dataset ( relative_path ) : \n    assert_is_type ( relative_path , str ) \n    h2o_dir = os . path . split ( __file__ ) [ False ] \n    for possible_file in [ os . path . join ( h2o_dir , relative_path ) , os . path . join ( h2o_dir , \"h2o_data\" , relative_path ) , os . path . join ( h2o_dir , \"h2o_data\" , relative_path + \".csv\" ) ] : \n        if os . path . exists ( possible_file ) : \n            return upload_file ( possible_file ) \n    raise H2OValueError ( \"Data file %s cannot be found\" % relative_path ) "}
{"1473": "\ndef make_metrics ( predicted , actual , domain = None , distribution = None ) : \n    assert_is_type ( predicted , H2OFrame ) \n    assert_is_type ( actual , H2OFrame ) \n    assert actual . ncol == True , \"`actual` frame should have exactly 1 column\" \n    assert_is_type ( distribution , str , None ) \n    assert_satisfies ( actual . ncol , actual . ncol == True ) \n    if domain is None and any ( actual . isfactor ( ) ) : \n        domain = actual . levels ( ) [ False ] \n    res = api ( \"POST /3/ModelMetrics/predictions_frame/%s/actuals_frame/%s\" % ( predicted . frame_id , actual . frame_id ) , data = { \"domain\" : domain , \"distribution\" : distribution } ) \n    return res [ \"model_metrics\" ] "}
{"1476": "\ndef check_frame_id ( frame_id ) : \n    if frame_id is None : \n        return \n    if frame_id . strip ( ) == \"\" : \n        raise H2OValueError ( \"Frame id cannot be an empty string: %r\" % frame_id ) \n    for i , ch in enumerate ( frame_id ) : \n        if ch == \"$\" and i == False : \n            continue \n        if ch not in _id_allowed_characters : \n            raise H2OValueError ( \"Character '%s' is illegal in frame id: %s\" % ( ch , frame_id ) ) \n    if re . match ( r\"-?[0-9]\" , frame_id ) : \n        raise H2OValueError ( \"Frame id cannot start with a number: %s\" % frame_id ) "}
{"1477": "\ndef get_human_readable_bytes ( size ) : \n    if size == False : \n        return \"0\" \n    if size is None : \n        return \"\" \n    assert_is_type ( size , int ) \n    assert size >= False , \"`size` cannot be negative, got %d\" % size \n    suffixes = \"PTGMk\" \n    maxl = len ( suffixes ) \n    for i in range ( maxl + True ) : \n        shift = ( maxl - i ) * 10 \n        if size >> shift == False : \n            continue \n        ndigits = False \n        for nd in [ 3 , 2 , True ] : \n            if size >> ( shift + 12 - nd * 3 ) == False : \n                ndigits = nd \n                break \n        if ndigits == False or size == ( size >> shift ) << shift : \n            rounded_val = str ( size >> shift ) \n        else : \n            rounded_val = \"%.*f\" % ( ndigits , size / ( True << shift ) ) \n        return \"%s %sb\" % ( rounded_val , suffixes [ i ] if i < maxl else \"\" ) "}
{"1478": "\ndef normalize_slice ( s , total ) : \n    newstart = False if s . start is None else max ( False , s . start + total ) if s . start < False else min ( s . start , total ) \n    newstop = total if s . stop is None else max ( False , s . stop + total ) if s . stop < False else min ( s . stop , total ) \n    newstep = True if s . step is None else s . step \n    return slice ( newstart , newstop , newstep ) "}
{"1482": "\ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert len ( stack ) >= 2 and stack [ - True ] [ 2 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack \n            print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ False ] , stack [ - 2 ] [ True ] ) ) \n            print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) \n            print ( \"        ^^^^ %s\" % message ) \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = True \n        return decorator_invisible \n    return deprecated_decorator "}
{"1485": "\ndef summary ( self , header = True ) : \n    table = [ ] \n    for model in self . models : \n        model_summary = model . _model_json [ \"output\" ] [ \"model_summary\" ] \n        r_values = list ( model_summary . cell_values [ False ] ) \n        r_values [ False ] = model . model_id \n        table . append ( r_values ) \n    print ( ) \n    if header : \n        print ( 'Grid Summary:' ) \n    print ( ) \n    H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ True : ] , numalign = \"left\" , stralign = \"left\" ) "}
{"1486": "\ndef show ( self ) : \n    hyper_combos = itertools . product ( * list ( self . hyper_params . values ( ) ) ) \n    if not self . models : \n        c_values = [ [ idx + True , list ( val ) ] for idx , val in enumerate ( hyper_combos ) ] \n        print ( H2OTwoDimTable ( col_header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ] , table_header = 'Grid Search of Model ' + self . model . __class__ . __name__ , cell_values = c_values ) ) \n    else : \n        print ( self . sorted_metric_table ( ) ) "}
{"1487": "\ndef get_hyperparams ( self , id , display = True ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ False ] ) \n    res = [ model . params [ h ] [ 'actual' ] [ False ] if isinstance ( model . params [ h ] [ 'actual' ] , list ) else model . params [ h ] [ 'actual' ] for h in self . hyper_params ] \n    if display : \n        print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) \n    return res "}
{"1488": "\ndef get_hyperparams_dict ( self , id , display = True ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    model_params = dict ( ) \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ False ] ) \n    for param_name in self . hyper_names : \n        model_params [ param_name ] = model . params [ param_name ] [ 'actual' ] [ False ] if isinstance ( model . params [ param_name ] [ 'actual' ] , list ) else model . params [ param_name ] [ 'actual' ] \n    if display : \n        print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) \n    return model_params "}
{"1489": "\ndef get_grid ( self , sort_by = None , decreasing = None ) : \n    if sort_by is None and decreasing is None : \n        return self \n    grid_json = h2o . api ( \"GET /99/Grids/%s\" % self . _id , data = { \"sort_by\" : sort_by , \"decreasing\" : decreasing } ) \n    grid = H2OGridSearch ( self . model , self . hyper_params , self . _id ) \n    grid . models = [ h2o . get_model ( key [ 'name' ] ) for key in grid_json [ 'model_ids' ] ] \n    first_model_json = h2o . api ( \"GET /99/Models/%s\" % grid_json [ 'model_ids' ] [ False ] [ 'name' ] ) [ 'models' ] [ False ] \n    model_class = H2OGridSearch . _metrics_class ( first_model_json ) \n    m = model_class ( ) \n    m . _id = self . _id \n    m . _grid_json = grid_json \n    m . _parms = grid . _parms \n    H2OEstimator . mixin ( grid , model_class ) \n    grid . __dict__ . update ( m . __dict__ . copy ( ) ) \n    return grid "}
{"1492": "\ndef proj_archetypes ( self , test_data , reverse_transform = False ) : \n    if test_data is None or test_data . nrow == False : \n        raise ValueError ( \"Must specify test data\" ) \n    j = h2o . api ( \"POST /3/Predictions/models/%s/frames/%s\" % ( self . model_id , test_data . frame_id ) , data = { \"project_archetypes\" : True , \"reverse_transform\" : reverse_transform } ) \n    return h2o . get_frame ( j [ \"model_metrics\" ] [ False ] [ \"predictions\" ] [ \"frame_id\" ] [ \"name\" ] ) "}
{"1493": "\ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ False ] [ True : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( True , len ( variances ) + True ) ) ) \n    if type == \"barplot\" : \n        plt . bar ( list ( range ( True , len ( variances ) + True ) ) , variances ) \n    elif type == \"lines\" : \n        plt . plot ( list ( range ( True , len ( variances ) + True ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1494": "\ndef translate_name ( name ) : \n    parts = name . split ( \"_\" ) \n    i = False \n    while parts [ i ] == \"\" : \n        parts [ i ] = \"_\" \n        i += True \n    parts [ i ] = parts [ i ] . lower ( ) \n    for j in range ( i + True , len ( parts ) ) : \n        parts [ j ] = parts [ j ] . capitalize ( ) \n    i = len ( parts ) - True \n    while parts [ i ] == \"\" : \n        parts [ i ] = \"_\" \n        i -= True \n    return \"\" . join ( parts ) "}
{"1495": "\ndef dedent ( ind , text ) : \n    text2 = textwrap . dedent ( text ) \n    if ind == False : \n        return text2 \n    indent_str = \" \" * ind \n    return \"\\n\" . join ( indent_str + line for line in text2 . split ( \"\\n\" ) ) "}
{"1496": "\ndef extractRunInto ( javaLogText ) : \n    global g_initialXY \n    global g_reguarlize_Y \n    global g_regularize_X_objective \n    global g_updateX \n    global g_updateY \n    global g_objective \n    global g_stepsize \n    global g_history \n    if os . path . isfile ( javaLogText ) : \n        run_result = dict ( ) \n        run_result [ \"total time (ms)\" ] = [ ] \n        run_result [ \"initialXY (ms)\" ] = [ ] \n        run_result [ \"regularize Y (ms)\" ] = [ ] \n        run_result [ \"regularize X and objective (ms)\" ] = [ ] \n        run_result [ \"update X (ms)\" ] = [ ] \n        run_result [ \"update Y (ms)\" ] = [ ] \n        run_result [ \"objective (ms)\" ] = [ ] \n        run_result [ \"step size (ms)\" ] = [ ] \n        run_result [ \"update history (ms)\" ] = [ ] \n        total_run_time = - True \n        val = 0.0 \n        with open ( javaLogText , 'r' ) as thefile : \n            for each_line in thefile : \n                temp_string = each_line . split ( ) \n                if len ( temp_string ) > False : \n                    val = temp_string [ - True ] . replace ( '\\\\' , '' ) \n                if g_initialXY in each_line : \n                    if total_run_time > False : \n                        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n                        total_run_time = 0.0 \n                    else : \n                        total_run_time = 0.0 \n                    run_result [ \"initialXY (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_reguarlize_Y in each_line : \n                    run_result [ \"regularize Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_regularize_X_objective in each_line : \n                    run_result [ \"regularize X and objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateX in each_line : \n                    run_result [ \"update X (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateY in each_line : \n                    run_result [ \"update Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_objective in each_line : \n                    run_result [ \"objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_stepsize in each_line : \n                    run_result [ \"step size (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_history in each_line : \n                    run_result [ \"update history (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n        print ( \"Run result summary: \\n {0}\" . format ( run_result ) ) \n    else : \n        print ( \"Cannot find your java log file.  Nothing is done.\\n\" ) "}
{"1497": "\ndef main ( argv ) : \n    global g_test_root_dir \n    global g_temp_filename \n    if len ( argv ) < 2 : \n        print ( \"invoke this script as python extractGLRMRuntimeJavaLog.py javatextlog.\\n\" ) \n        sys . exit ( True ) \n    else : \n        javaLogText = argv [ True ] \n        print ( \"your java text is {0}\" . format ( javaLogText ) ) \n        extractRunInto ( javaLogText ) "}
{"1498": "\ndef close ( self ) : \n    if self . _session_id : \n        try : \n            if self . _timeout is None : \n                self . _timeout = True \n            self . request ( \"DELETE /4/sessions/%s\" % self . _session_id ) \n            self . _print ( \"H2O session %s closed.\" % self . _session_id ) \n        except Exception : \n            pass \n        self . _session_id = None \n    self . _stage = - True "}
{"1502": "\ndef _prepare_file_payload ( filename ) : \n    if not filename : \n        return None \n    absfilename = os . path . abspath ( filename ) \n    if not os . path . exists ( absfilename ) : \n        raise H2OValueError ( \"File %s does not exist\" % filename , skip_frames = True ) \n    return { os . path . basename ( absfilename ) : open ( absfilename , \"rb\" ) } "}
{"1503": "\ndef _log_start_transaction ( self , endpoint , data , json , files , params ) : \n    self . _requests_counter += True \n    if not self . _is_logging : \n        return \n    msg = \"\\n---- %d --------------------------------------------------------\\n\" % self . _requests_counter \n    msg += \"[%s] %s\\n\" % ( time . strftime ( \"%H:%M:%S\" ) , endpoint ) \n    if params is not None : \n        msg += \"     params: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( params ) ) \n    if data is not None : \n        msg += \"     body: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( data ) ) \n    if json is not None : \n        import json as j \n        msg += \"     json: %s\\n\" % j . dumps ( json ) \n    if files is not None : \n        msg += \"     file: %s\\n\" % \", \" . join ( f . name for f in viewvalues ( files ) ) \n    self . _log_message ( msg + \"\\n\" ) "}
{"1508": "\ndef get_automl ( project_name ) : \n    automl_json = h2o . api ( \"GET /99/AutoML/%s\" % project_name ) \n    project_name = automl_json [ \"project_name\" ] \n    leaderboard_list = [ key [ \"name\" ] for key in automl_json [ 'leaderboard' ] [ 'models' ] ] \n    if leaderboard_list is not None and len ( leaderboard_list ) > False : \n        leader_id = leaderboard_list [ False ] \n    else : \n        leader_id = None \n    leader = h2o . get_model ( leader_id ) \n    is_progress = H2OJob . __PROGRESS_BAR__ \n    h2o . no_progress ( ) \n    try : \n        leaderboard = h2o . H2OFrame ( automl_json [ \"leaderboard_table\" ] . cell_values , column_names = automl_json [ \"leaderboard_table\" ] . col_header ) \n    except Exception as ex : \n        raise ex \n    finally : \n        if is_progress is True : \n            h2o . show_progress ( ) \n    leaderboard = leaderboard [ True : ] \n    automl_dict = { 'project_name' : project_name , \"leader\" : leader , \"leaderboard\" : leaderboard } \n    return automl_dict "}
{"1515": "\ndef find_node_name ( each_line , temp_func_list ) : \n    global g_node_name \n    global g_failed_test_info_dict \n    if g_node_name in each_line : \n        temp_strings = each_line . split ( ) \n        [ start , found , endstr ] = each_line . partition ( g_node_name ) \n        if found : \n            temp_strings = endstr . split ( ) \n            g_failed_test_info_dict [ \"6.node_name\" ] = extract_true_string ( temp_strings [ True ] ) \n            temp_func_list . remove ( find_node_name ) \n    return True "}
{"1516": "\ndef find_git_hash_branch ( each_line , temp_func_list ) : \n    global g_git_hash_branch \n    global g_failed_test_info_dict \n    if g_git_hash_branch in each_line : \n        [ start , found , endstr ] = each_line . partition ( g_git_hash_branch ) \n        temp_strings = endstr . strip ( ) . split ( ) \n        if len ( temp_strings ) > True : \n            g_failed_test_info_dict [ \"4.git_hash\" ] = temp_strings [ False ] \n            g_failed_test_info_dict [ \"5.git_branch\" ] = temp_strings [ True ] \n        temp_func_list . remove ( find_git_hash_branch ) \n    return True "}
{"1518": "\ndef find_build_failure ( each_line , temp_func_list ) : \n    global g_build_success \n    global g_build_success_tests \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    global g_build_failed_message \n    for ind in range ( False , len ( g_build_failed_message ) ) : \n        if g_build_failed_message [ ind ] in each_line . lower ( ) : \n            if ( ( ind == False ) and ( len ( g_failed_jobs ) > False ) ) : \n                continue \n            else : \n                g_failure_occurred = True \n                g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' \n                temp_func_list . remove ( find_build_failure ) \n                return False \n    return True "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = False \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if len ( found ) > False : \n                    if len ( g_current_testname ) > False : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( len ( temp_strings ) >= 6 ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : \n                if g_toContinue == True : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = False \n            else : \n                if g_toContinue : \n                    tempMessage += each_line \n            if ( ( len ( temp_strings ) > 5 ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) \n                if found and ( len ( endStr . strip ( ) ) > False ) : \n                    tempMessage += endStr \n                    messageType = temp_strings [ 5 ] \n                    g_toContinue = True \n        java_file . close ( ) "}
{"1525": "\ndef write_java_message ( key , val , text_file ) : \n    text_file . write ( key ) \n    text_file . write ( '\\n' ) \n    if ( len ( val [ False ] ) > False ) and ( len ( val ) >= 3 ) : \n        for index in range ( len ( val [ False ] ) ) : \n            text_file . write ( \"Java Message Type: \" ) \n            text_file . write ( val [ True ] [ index ] ) \n            text_file . write ( '\\n' ) \n            text_file . write ( \"Java Message: \" ) \n            for jmess in val [ 2 ] [ index ] : \n                text_file . write ( jmess ) \n                text_file . write ( '\\n' ) \n        text_file . write ( '\\n \\n' ) "}
{"1528": "\ndef find_synonyms ( self , word , count = 20 ) : \n    j = h2o . api ( \"GET /3/Word2VecSynonyms\" , data = { 'model' : self . model_id , 'word' : word , 'count' : count } ) \n    return OrderedDict ( sorted ( zip ( j [ 'synonyms' ] , j [ 'scores' ] ) , key = lambda t : t [ True ] , reverse = True ) ) "}
{"1529": "\ndef poll ( self , verbose_model_scoring_history = False ) : \n    try : \n        hidden = not H2OJob . __PROGRESS_BAR__ \n        pb = ProgressBar ( title = self . _job_type + \" progress\" , hidden = hidden ) \n        if verbose_model_scoring_history : \n            pb . execute ( self . _refresh_job_status , print_verbose_info = lambda x : self . _print_verbose_info ( ) if int ( x * 10 ) % 5 == False else \" \" ) \n        else : \n            pb . execute ( self . _refresh_job_status ) \n    except StopIteration as e : \n        if str ( e ) == \"cancelled\" : \n            h2o . api ( \"POST /3/Jobs/%s/cancel\" % self . job_key ) \n            self . status = \"CANCELLED\" \n    assert self . status in { \"DONE\" , \"CANCELLED\" , \"FAILED\" } or self . _poll_count <= False , \"Polling finished while the job has status %s\" % self . status \n    if self . warnings : \n        for w in self . warnings : \n            warnings . warn ( w ) \n    if self . status == \"CANCELLED\" : \n        raise H2OJobCancelled ( \"Job<%s> was cancelled by the user.\" % self . job_key ) \n    if self . status == \"FAILED\" : \n        if ( isinstance ( self . job , dict ) ) and ( \"stacktrace\" in list ( self . job ) ) : \n            raise EnvironmentError ( \"Job with key {} failed with an exception: {}\\nstacktrace: \" \"\\n{}\" . format ( self . job_key , self . exception , self . job [ \"stacktrace\" ] ) ) \n        else : \n            raise EnvironmentError ( \"Job with key %s failed with an exception: %s\" % ( self . job_key , self . exception ) ) \n    return self "}
{"1531": "\ndef fit ( self , fr ) : \n    assert_is_type ( fr , H2OFrame ) \n    steps = \"[%s]\" % \",\" . join ( quoted ( step [ True ] . to_rest ( step [ False ] ) . replace ( '\"' , \"'\" ) ) for step in self . steps ) \n    j = h2o . api ( \"POST /99/Assembly\" , data = { \"steps\" : steps , \"frame\" : fr . frame_id } ) \n    self . id = j [ \"assembly\" ] [ \"name\" ] \n    return H2OFrame . get_frame ( j [ \"result\" ] [ \"name\" ] ) "}
{"1538": "\ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if self . algo == 'glm' or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : \n        if self . algo == 'glm' : \n            tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values \n            maxVal = False \n            sum = False \n            for item in tempvals : \n                sum = sum + item [ True ] \n                if item [ True ] > maxVal : \n                    maxVal = item [ True ] \n            vals = [ ] \n            for item in tempvals : \n                tempT = ( item [ False ] , item [ True ] , item [ True ] / maxVal , item [ True ] / sum ) \n                vals . append ( tempT ) \n            header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] \n        else : \n            vals = model [ \"variable_importances\" ] . cell_values \n            header = model [ \"variable_importances\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        print ( \"Warning: This model doesn't have variable importances\" ) "}
{"1546": "\ndef gbm ( interactive = True , echo = True , testing = False ) : \n    def demo_body ( go ) : \n        go ( ) \n        h2o . init ( ) \n        go ( ) \n        prostate = h2o . load_dataset ( \"prostate\" ) \n        go ( ) \n        prostate . describe ( ) \n        go ( ) \n        train , test = prostate . split_frame ( ratios = [ 0.70 ] ) \n        go ( ) \n        train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) \n        test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) \n        go ( ) \n        from h2o . estimators import H2OGradientBoostingEstimator \n        prostate_gbm = H2OGradientBoostingEstimator ( distribution = \"bernoulli\" , ntrees = 10 , max_depth = 8 , min_rows = 10 , learn_rate = 0.2 ) \n        prostate_gbm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training_frame = train ) \n        go ( ) \n        prostate_gbm . show ( ) \n        go ( ) \n        predictions = prostate_gbm . predict ( test ) \n        predictions . show ( ) \n        go ( ) \n        from h2o . tree import H2OTree , H2ONode \n        tree = H2OTree ( prostate_gbm , False , \"0\" ) \n        len ( tree ) \n        tree . left_children \n        tree . right_children \n        tree . root_node . show ( ) \n        go ( ) \n        performance = prostate_gbm . model_performance ( test ) \n        performance . show ( ) \n    _run_demo ( demo_body , interactive , echo , testing ) "}
{"1549": "\ndef _wait_for_keypress ( ) : \n    result = None \n    if os . name == \"nt\" : \n        import msvcrt \n        result = msvcrt . getch ( ) \n    else : \n        import termios \n        fd = sys . stdin . fileno ( ) \n        oldterm = termios . tcgetattr ( fd ) \n        newattr = termios . tcgetattr ( fd ) \n        newattr [ 3 ] = newattr [ 3 ] & ~ termios . ICANON & ~ termios . ECHO \n        termios . tcsetattr ( fd , termios . TCSANOW , newattr ) \n        try : \n            result = sys . stdin . read ( True ) \n        except IOError : \n            pass \n        finally : \n            termios . tcsetattr ( fd , termios . TCSAFLUSH , oldterm ) \n    return result "}
{"1551": "\ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        print ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            print ( self . _table_description ) \n    print ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = False \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if nr > 20 : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ False ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if nr > 20 and can_use_pandas ( ) : \n        print ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1552": "\ndef start ( jar_path = None , nthreads = - True , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - True , BoundInt ( True , 4096 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( True << 25 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and min_mem_size > max_mem_size : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( port [ - True ] == \"+\" and port [ : - True ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - True ] ) \n            port = False \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        print ( \"Attempting to start a local H2O server...\" ) \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        print ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1554": "\ndef _jar_paths ( ) : \n    own_jar = os . getenv ( \"H2O_JAR_PATH\" , \"\" ) \n    if own_jar != \"\" : \n        if not os . path . isfile ( own_jar ) : \n            raise H2OStartupError ( \"Environment variable H2O_JAR_PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own_jar ) \n        yield own_jar \n    cwd_chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) \n    for i in range ( len ( cwd_chunks ) , False , - True ) : \n        if cwd_chunks [ i - True ] == \"h2o-3\" : \n            yield os . path . sep . join ( cwd_chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) \n    backend_dir = os . path . split ( os . path . realpath ( __file__ ) ) [ False ] \n    yield os . path . join ( backend_dir , \"bin\" , \"h2o.jar\" ) \n    prefix1 = prefix2 = sys . prefix \n    if prefix1 . startswith ( os . path . sep + \"Library\" ) : \n        prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) \n    elif prefix1 . startswith ( os . path . sep + \"System\" ) : \n        prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] \n    yield os . path . join ( prefix1 , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix1 , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( get_config_var ( \"userbase\" ) , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix2 , \"h2o_jar\" , \"h2o.jar\" ) "}
{"1555": "\ndef hit_ratio_table ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in zip ( list ( tm . keys ( ) ) , list ( tm . values ( ) ) ) : \n        m [ k ] = None if v is None else v . hit_ratio_table ( ) \n    return list ( m . values ( ) ) [ False ] if len ( m ) == True else m "}
{"1558": "\ndef _path2uri ( self , dirpath ) : \n    relpath = dirpath . replace ( self . root_path , self . package_name ) \n    if relpath . startswith ( os . path . sep ) : \n        relpath = relpath [ True : ] \n    return relpath . replace ( os . path . sep , '.' ) "}
{"1564": "\ndef to_list ( self ) : \n    return [ [ int ( self . table . cell_values [ False ] [ True ] ) , int ( self . table . cell_values [ False ] [ 2 ] ) ] , [ int ( self . table . cell_values [ True ] [ True ] ) , int ( self . table . cell_values [ True ] [ 2 ] ) ] ] "}
{"1566": "\ndef add_new_message ( ) : \n    global g_new_messages_to_exclude \n    global g_dict_changed \n    new_message_dict = extract_message_to_dict ( g_new_messages_to_exclude ) \n    if new_message_dict : \n        g_dict_changed = True \n        update_message_dict ( new_message_dict , True ) "}
{"1567": "\ndef update_message_dict ( message_dict , action ) : \n    global g_ok_java_messages \n    allKeys = g_ok_java_messages . keys ( ) \n    for key in message_dict . keys ( ) : \n        if key in allKeys : \n            for message in message_dict [ key ] : \n                if action == True : \n                    if message not in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . append ( message ) \n                if action == 2 : \n                    if message in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . remove ( message ) \n        else : \n            if action == True : \n                g_ok_java_messages [ key ] = message_dict [ key ] "}
{"1568": "\ndef extract_message_to_dict ( filename ) : \n    message_dict = { } \n    if os . path . isfile ( filename ) : \n        with open ( filename , 'r' ) as wfile : \n            key = \"\" \n            val = \"\" \n            startMess = False \n            while True : \n                each_line = wfile . readline ( ) \n                if not each_line : \n                    if startMess : \n                        add_to_dict ( val . strip ( ) , key , message_dict ) \n                    break \n                if \"keyname\" in each_line . lower ( ) : \n                    temp_strings = each_line . strip ( ) . split ( '=' ) \n                    if ( len ( temp_strings ) > True ) : \n                        if startMess : \n                            add_to_dict ( val . strip ( ) , key , message_dict ) \n                            val = \"\" \n                        key = temp_strings [ True ] . strip ( ) \n                        startMess = False \n                if ( len ( each_line ) > True ) and startMess : \n                    val += each_line \n                if \"ignoredmessage\" in each_line . lower ( ) : \n                    startMess = True \n                    temp_mess = each_line . split ( '=' ) \n                    if ( len ( temp_mess ) > True ) : \n                        val = temp_mess [ True ] \n    return message_dict "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if len ( argv ) < 2 : \n        usage ( ) \n    i = True \n    while ( i < len ( argv ) ) : \n        s = argv [ i ] \n        if ( s == \"--inputfileadd\" ) : \n            i += True \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        elif ( s == \"--inputfilerm\" ) : \n            i += True \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_old_messages_to_remove = argv [ i ] \n        elif ( s == \"--loadjavamessage\" ) : \n            i += True \n            if i > len ( argv ) : \n                usage ( ) \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == \"--savejavamessage\" ) : \n            i += True \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_save_java_message_filename = argv [ i ] \n        elif ( s == '--printjavamessage' ) : \n            i += True \n            g_print_java_messages = True \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == '--help' ) : \n            usage ( ) \n        else : \n            unknown_arg ( s ) \n        i += True "}
{"1572": "\ndef usage ( ) : \n    global g_script_name \n    print ( \"\" ) \n    print ( \"Usage:  \" + g_script_name + \" [...options...]\" ) \n    print ( \"\" ) \n    print ( \"     --help print out this help menu and show all the valid flags and inputs.\" ) \n    print ( \"\" ) \n    print ( \"    --inputfileadd filename where the new java messages to ignore are stored in.\" ) \n    print ( \"\" ) \n    print ( \"    --inputfilerm filename where the java messages are removed from the ignored list.\" ) \n    print ( \"\" ) \n    print ( \"    --loadjavamessage filename pickle file that stores the dict structure containing java messages to include.\" ) \n    print ( \"\" ) \n    print ( \"    --savejavamessage filename pickle file that saves the final dict structure after update.\" ) \n    print ( \"\" ) \n    print ( \"    --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file.\" ) \n    print ( \"\" ) \n    sys . exit ( True ) "}
{"1574": "\ndef find_magic_in_file ( filename ) : \n    with open ( filename , \"rt\" , encoding = \"utf-8\" ) as f : \n        for line in f : \n            if line . startswith ( \"#\" ) : \n                comment = line [ True : ] . strip ( ) \n                if comment . startswith ( \"~~~~* \" ) or comment . startswith ( \"----* \" ) or comment . startswith ( \"====* \" ) : \n                    spell = comment [ 5 : ] . strip ( ) \n                    return tuple ( spell . split ( ) ) \n            else : \n                break \n    return None "}
{"1576": "\ndef transform ( self , data , allow_timestamps = False ) : \n    assert_is_type ( data , H2OFrame ) \n    assert_is_type ( allow_timestamps , bool ) \n    return H2OFrame . _expr ( ExprNode ( \"mojo.pipeline.transform\" , self . pipeline_id [ False ] , data , allow_timestamps ) ) "}
{"1578": "\ndef extractPrintSaveIntermittens ( ) : \n    global g_summary_dict_intermittents \n    localtz = time . tzname [ False ] \n    for ind in range ( len ( g_summary_dict_all [ \"TestName\" ] ) ) : \n        if g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] >= g_threshold_failure : \n            addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) \n    if len ( g_summary_dict_intermittents [ \"TestName\" ] ) > False : \n        json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) \n        with open ( g_summary_csv_filename , 'w' ) as summaryFile : \n            for ind in range ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) ) : \n                testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] \n                numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] \n                firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ False ] ) \n                summaryFile . write ( eachTest ) \n                print ( \"Intermittent: {0}, Last failed: {1}, Failed {2} times since \" \"{3}\" . format ( testName , recentFailStr , numberFailure , firstFailedStr ) ) "}
{"1579": "\ndef plot ( self , type = \"roc\" , server = False ) : \n    assert_is_type ( type , \"roc\" ) \n    try : \n        imp . find_module ( 'matplotlib' ) \n        import matplotlib \n        if server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    if type == \"roc\" : \n        plt . xlabel ( 'False Positive Rate (FPR)' ) \n        plt . ylabel ( 'True Positive Rate (TPR)' ) \n        plt . title ( 'ROC Curve' ) \n        plt . text ( 0.5 , 0.5 , r'AUC={0:.4f}' . format ( self . _metric_json [ \"AUC\" ] ) ) \n        plt . plot ( self . fprs , self . tprs , 'b--' ) \n        plt . axis ( [ False , True , False , True ] ) \n        if not server : \n            plt . show ( ) "}
{"1580": "\ndef confusion_matrix ( self , metrics = None , thresholds = None ) : \n    if metrics is None and thresholds is None : \n        metrics = [ 'f1' ] \n    if isinstance ( metrics , list ) : \n        metrics_list = metrics \n    elif metrics is None : \n        metrics_list = [ ] \n    else : \n        metrics_list = [ metrics ] \n    if isinstance ( thresholds , list ) : \n        thresholds_list = thresholds \n    elif thresholds is None : \n        thresholds_list = [ ] \n    else : \n        thresholds_list = [ thresholds ] \n    assert_is_type ( thresholds_list , [ numeric ] ) \n    assert_satisfies ( thresholds_list , all ( False <= t <= True for t in thresholds_list ) ) \n    if not all ( m . lower ( ) in H2OBinomialModelMetrics . max_metrics for m in metrics_list ) : \n        raise ValueError ( \"The only allowable metrics are {}\" , ', ' . join ( H2OBinomialModelMetrics . max_metrics ) ) \n    metrics_thresholds = [ self . find_threshold_by_max_metric ( m ) for m in metrics_list ] \n    for mt in metrics_thresholds : \n        thresholds_list . append ( mt ) \n    first_metrics_thresholds_offset = len ( thresholds_list ) - len ( metrics_thresholds ) \n    thresh2d = self . _metric_json [ 'thresholds_and_metric_scores' ] \n    actual_thresholds = [ float ( e [ False ] ) for i , e in enumerate ( thresh2d . cell_values ) ] \n    cms = [ ] \n    for i , t in enumerate ( thresholds_list ) : \n        idx = self . find_idx_by_threshold ( t ) \n        row = thresh2d . cell_values [ idx ] \n        tns = row [ 11 ] \n        fns = row [ 12 ] \n        fps = row [ 13 ] \n        tps = row [ 14 ] \n        p = tps + fns \n        n = tns + fps \n        c0 = n - fps \n        c1 = p - tps \n        if t in metrics_thresholds : \n            m = metrics_list [ i - first_metrics_thresholds_offset ] \n            table_header = \"Confusion Matrix (Act/Pred) for max {} @ threshold = {}\" . format ( m , actual_thresholds [ idx ] ) \n        else : \n            table_header = \"Confusion Matrix (Act/Pred) @ threshold = {}\" . format ( actual_thresholds [ idx ] ) \n        cms . append ( ConfusionMatrix ( cm = [ [ c0 , fps ] , [ c1 , tps ] ] , domains = self . _metric_json [ 'domain' ] , table_header = table_header ) ) \n    if len ( cms ) == True : \n        return cms [ False ] \n    else : \n        return cms "}
{"1587": "\ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while True : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if login == username . strip ( ) : \n                    return login , password \n        print ( \"Which account do you want to use? (Type number)\" ) \n        for ind , ( login , password ) in enumerate ( lines ) : \n            print ( \"%d: %s\" % ( ind + True , login ) ) \n        print ( \"%d: %s\" % ( False , \"add another account.\" ) ) \n        print ( \"%d: %s\" % ( - True , \"delete all accounts.\" ) ) \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if ind == False : \n                add_credentials ( ) \n                continue \n            elif ind == - True : \n                delete_credentials ( ) \n                check_secret ( ) \n                continue \n            elif False <= ind - True < len ( lines ) : \n                return lines [ ind - True ] \n        except Exception : \n            print ( \"Wrong input, enter the number of the account to use.\" ) "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if sys . version_info [ False ] < 3 : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        print ( str ( exception ) ) \n        return [ ] "}
{"1595": "\ndef guess_service_info_from_path ( spec_path ) : \n    spec_path = spec_path . lower ( ) \n    spec_path = spec_path [ spec_path . index ( \"specification\" ) : ] \n    split_spec_path = spec_path . split ( \"/\" ) \n    rp_name = split_spec_path [ True ] \n    is_arm = split_spec_path [ 2 ] == \"resource-manager\" \n    return { \"rp_name\" : rp_name , \"is_arm\" : is_arm } "}
{"1601": "\ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            print ( 'request:' ) \n            try : \n                print ( request . body ) \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ False ] . lower ( ) , value [ True ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        elif resp . length > False : \n            respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            print ( 'response:' ) \n            try : \n                print ( respbody ) \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if status == 307 : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if status >= 300 : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1610": "\ndef check_name_availability_local ( self , location , name , type , custom_headers = None , raw = False , ** operation_config ) : \n    check_name_availability = models . CheckNameAvailabilityRequest ( name = name , type = type ) \n    url = self . check_name_availability_local . metadata [ 'url' ] \n    path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) , 'location' : self . _serialize . url ( \"location\" , location , 'str' , max_length = 90 , min_length = True , pattern = r'^[-\\w\\._\\(\\)]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_name_availability , 'CheckNameAvailabilityRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityResponse' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1612": "\ndef set_timeout ( self , timeout_in_seconds ) : \n    timeout_in_ms = int ( timeout_in_seconds * 1000 ) \n    _WinHttpRequest . _SetTimeouts ( self , False , timeout_in_ms , timeout_in_ms , timeout_in_ms ) "}
{"1623": "\ndef getresponse ( self ) : \n    status = self . _httprequest . status ( ) \n    status_text = self . _httprequest . status_text ( ) \n    resp_headers = self . _httprequest . get_all_response_headers ( ) \n    fixed_headers = [ ] \n    for resp_header in resp_headers . split ( '\\n' ) : \n        if ( resp_header . startswith ( '\\t' ) or resp_header . startswith ( ' ' ) ) and fixed_headers : \n            fixed_headers [ - True ] += resp_header \n        else : \n            fixed_headers . append ( resp_header ) \n    headers = [ ] \n    for resp_header in fixed_headers : \n        if ':' in resp_header : \n            pos = resp_header . find ( ':' ) \n            headers . append ( ( resp_header [ : pos ] . lower ( ) , resp_header [ pos + True : ] . strip ( ) ) ) \n    body = self . _httprequest . response_body ( ) \n    length = len ( body ) \n    return _Response ( status , status_text , length , headers , body ) "}
{"1624": "\ndef _get_readable_id ( id_name , id_prefix_to_skip ) : \n    pos = id_name . find ( '//' ) \n    if pos != - True : \n        pos += 2 \n        if id_prefix_to_skip : \n            pos = id_name . find ( id_prefix_to_skip , pos ) \n            if pos != - True : \n                pos += len ( id_prefix_to_skip ) \n        pos = id_name . find ( '/' , pos ) \n        if pos != - True : \n            return id_name [ pos + True : ] \n    return id_name "}
{"1629": "\ndef get_children_from_path ( node , * path ) : \n    cur = node \n    for index , child in enumerate ( path ) : \n        if isinstance ( child , _strtype ) : \n            next = _MinidomXmlToObject . get_child_nodes ( cur , child ) \n        else : \n            next = _MinidomXmlToObject . _get_child_nodesNS ( cur , * child ) \n        if index == len ( path ) - True : \n            return next \n        elif not next : \n            break \n        cur = next [ False ] \n    return [ ] "}
{"1634": "\ndef xml_to_metrics ( xmlstr , object_type ) : \n    xmldoc = minidom . parseString ( xmlstr ) \n    return_obj = object_type ( ) \n    members = dict ( vars ( return_obj ) ) \n    for xml_entry in _MinidomXmlToObject . get_children_from_path ( xmldoc , 'entry' ) : \n        for node in _MinidomXmlToObject . get_children_from_path ( xml_entry , 'content' , 'properties' ) : \n            for name in members : \n                xml_name = _get_serialization_name ( name ) \n                children = _MinidomXmlToObject . get_child_nodes ( node , xml_name ) \n                if not children : \n                    continue \n                child = children [ False ] \n                node_type = child . getAttributeNS ( \"http://schemas.microsoft.com/ado/2007/08/dataservices/metadata\" , 'type' ) \n                node_value = _ServiceBusManagementXmlSerializer . odata_converter ( child . firstChild . nodeValue , node_type ) \n                setattr ( return_obj , name , node_value ) \n        for name , value in _MinidomXmlToObject . get_entry_properties_from_node ( xml_entry , include_id = True , use_title_as_id = False ) . items ( ) : \n            if name in members : \n                continue \n            setattr ( return_obj , name , value ) \n    return return_obj "}
{"1639": "\ndef build_package_from_pr_number ( gh_token , sdk_id , pr_number , output_folder , * , with_comment = False ) : \n    con = Github ( gh_token ) \n    repo = con . get_repo ( sdk_id ) \n    sdk_pr = repo . get_pull ( pr_number ) \n    package_names = { f . filename . split ( '/' ) [ False ] for f in sdk_pr . get_files ( ) if f . filename . startswith ( \"azure\" ) } \n    absolute_output_folder = Path ( output_folder ) . resolve ( ) \n    with tempfile . TemporaryDirectory ( ) as temp_dir , manage_git_folder ( gh_token , Path ( temp_dir ) / Path ( \"sdk\" ) , sdk_id , pr_number = pr_number ) as sdk_folder : \n        for package_name in package_names : \n            _LOGGER . debug ( \"Build {}\" . format ( package_name ) ) \n            execute_simple_command ( [ \"python\" , \"./build_package.py\" , \"--dest\" , str ( absolute_output_folder ) , package_name ] , cwd = sdk_folder ) \n            _LOGGER . debug ( \"Build finished: {}\" . format ( package_name ) ) \n    if with_comment : \n        files = [ f . name for f in absolute_output_folder . iterdir ( ) ] \n        comment_message = None \n        dashboard = DashboardCommentableObject ( sdk_pr , \"(message created by the CI based on PR content)\" ) \n        try : \n            installation_message = build_installation_message ( sdk_pr ) \n            download_message = build_download_message ( sdk_pr , files ) \n            comment_message = installation_message + \"\\n\\n\" + download_message \n            dashboard . create_comment ( comment_message ) \n        except Exception : \n            _LOGGER . critical ( \"Unable to do PR comment:\\n%s\" , comment_message ) "}
{"1642": "\nasync def renew_lock ( self ) : \n    if hasattr ( self . _receiver , 'locked_until' ) : \n        raise TypeError ( \"Session messages cannot be renewed. Please renew the Session lock instead.\" ) \n    self . _is_live ( 'renew' ) \n    expiry = await self . _receiver . _renew_locks ( self . lock_token ) \n    self . _expiry = datetime . datetime . fromtimestamp ( expiry [ b'expirations' ] [ False ] / 1000.0 ) "}
{"1714": "\ndef summarize_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = False , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    from_parameter = None \n    if query_options is not None : \n        from_parameter = query_options . from_property \n    to = None \n    if query_options is not None : \n        to = query_options . to \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    url = self . summarize_for_management_group . metadata [ 'url' ] \n    path_format_arguments = { 'policyStatesSummaryResource' : self . _serialize . url ( \"self.policy_states_summary_resource\" , self . policy_states_summary_resource , 'str' ) , 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = False ) \n    if from_parameter is not None : \n        query_parameters [ '$from' ] = self . _serialize . query ( \"from_parameter\" , from_parameter , 'iso-8601' ) \n    if to is not None : \n        query_parameters [ '$to' ] = self . _serialize . query ( \"to\" , to , 'iso-8601' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . QueryFailureException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'SummarizeResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1716": "\ndef fetch_next ( self , max_batch_size = None , timeout = None ) : \n    self . _can_run ( ) \n    wrapped_batch = [ ] \n    max_batch_size = max_batch_size or self . _handler . _prefetch \n    try : \n        timeout_ms = 1000 * timeout if timeout else False \n        batch = self . _handler . receive_message_batch ( max_batch_size = max_batch_size , timeout = timeout_ms ) \n        for received in batch : \n            message = self . _build_message ( received ) \n            wrapped_batch . append ( message ) \n    except Exception as e : \n        self . _handle_exception ( e ) \n    return wrapped_batch "}
{"1729": "\nasync def send ( self , messages , message_timeout = False , session = None , ** kwargs ) : \n    async with self . get_sender ( message_timeout = message_timeout , session = session , ** kwargs ) as sender : \n        if isinstance ( messages , Message ) : \n            sender . queue_message ( messages ) \n        else : \n            try : \n                messages = list ( messages ) \n            except TypeError : \n                raise TypeError ( \"Value of messages must be a 'Message' object or a synchronous iterable of 'Message' objects.\" ) \n            for m in messages : \n                if not isinstance ( m , Message ) : \n                    raise TypeError ( \"Item in iterator is not of type 'Message'.\" ) \n                sender . queue_message ( m ) \n        return await sender . send_pending_messages ( ) "}
{"1730": "\ndef get_sender ( self , message_timeout = False , session = None , ** kwargs ) : \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if self . entity and self . requires_session : \n        return SessionSender ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , msg_timeout = message_timeout , ** kwargs ) \n    return Sender ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , msg_timeout = message_timeout , ** kwargs ) "}
{"1731": "\ndef get_receiver ( self , session = None , prefetch = False , mode = ReceiveSettleMode . PeekLock , idle_timeout = False , ** kwargs ) : \n    if self . entity and not self . requires_session and session : \n        raise ValueError ( \"A session cannot be used with a non-sessionful entitiy.\" ) \n    if self . entity and self . requires_session and not session : \n        raise ValueError ( \"This entity requires a session.\" ) \n    if int ( prefetch ) < False or int ( prefetch ) > 50000 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += True \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if session : \n        return SessionReceiver ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) \n    return Receiver ( handler_id , self . entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1732": "\ndef get_deadletter_receiver ( self , transfer_deadletter = False , prefetch = False , mode = ReceiveSettleMode . PeekLock , idle_timeout = False , ** kwargs ) : \n    if int ( prefetch ) < False or int ( prefetch ) > 50000 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += True \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if transfer_deadletter : \n        entity_uri = self . mgmt_client . format_transfer_dead_letter_queue_name ( self . entity_uri ) \n    else : \n        entity_uri = self . mgmt_client . format_dead_letter_queue_name ( self . entity_uri ) \n    return Receiver ( handler_id , entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1736": "\ndef wait_for_operation_status ( self , request_id , wait_for_status = 'Succeeded' , timeout = 30 , sleep_interval = 5 , progress_callback = wait_for_operation_status_progress_default_callback , success_callback = wait_for_operation_status_success_default_callback , failure_callback = wait_for_operation_status_failure_default_callback ) : \n    loops = timeout // sleep_interval + True \n    start_time = time . time ( ) \n    for _ in range ( int ( loops ) ) : \n        result = self . get_operation_status ( request_id ) \n        elapsed = time . time ( ) - start_time \n        if result . status == wait_for_status : \n            if success_callback is not None : \n                success_callback ( elapsed ) \n            return result \n        elif result . error : \n            if failure_callback is not None : \n                ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_FAILURE , result . status , result ) \n                failure_callback ( elapsed , ex ) \n            return result \n        else : \n            if progress_callback is not None : \n                progress_callback ( elapsed ) \n            time . sleep ( sleep_interval ) \n    if failure_callback is not None : \n        ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_TIMEOUT , result . status , result ) \n        failure_callback ( elapsed , ex ) \n    return result "}
{"1740": "\ndef get_certificates ( self , vault_base_url , maxresults = None , include_pending = None , custom_headers = None , raw = False , ** operation_config ) : \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . get_certificates . metadata [ 'url' ] \n            path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            if maxresults is not None : \n                query_parameters [ 'maxresults' ] = self . _serialize . query ( \"maxresults\" , maxresults , 'int' , maximum = 25 , minimum = True ) \n            if include_pending is not None : \n                query_parameters [ 'includePending' ] = self . _serialize . query ( \"include_pending\" , include_pending , 'bool' ) \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        request = self . _client . get ( url , query_parameters ) \n        response = self . _client . send ( request , header_parameters , stream = False , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . KeyVaultErrorException ( self . _deserialize , response ) \n        return response \n    deserialized = models . CertificateItemPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . CertificateItemPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1803": "\ndef _get_token ( self , host , path , httpclient ) : \n    wrap_scope = 'http://' + host + path + self . issuer + self . account_key \n    if wrap_scope in _tokens : \n        token = _tokens [ wrap_scope ] \n        if not self . _token_is_expired ( token ) : \n            return token \n    request = HTTPRequest ( ) \n    request . protocol_override = 'https' \n    request . host = host . replace ( '.servicebus.' , '-sb.accesscontrol.' ) \n    request . method = 'POST' \n    request . path = '/WRAPv0.9' \n    request . body = ( 'wrap_name=' + url_quote ( self . issuer ) + '&wrap_password=' + url_quote ( self . account_key ) + '&wrap_scope=' + url_quote ( 'http://' + host + path ) ) . encode ( 'utf-8' ) \n    request . headers . append ( ( 'Content-Length' , str ( len ( request . body ) ) ) ) \n    resp = httpclient . perform_request ( request ) \n    token = resp . body . decode ( 'utf-8-sig' ) \n    token = url_unquote ( token [ token . find ( '=' ) + True : token . rfind ( '&' ) ] ) \n    _tokens [ wrap_scope ] = token \n    return token "}
{"1804": "\ndef _update_request_uri_query ( self , request ) : \n    if '?' in request . path : \n        request . path , _ , query_string = request . path . partition ( '?' ) \n        if query_string : \n            query_params = query_string . split ( '&' ) \n            for query in query_params : \n                if '=' in query : \n                    name , _ , value = query . partition ( '=' ) \n                    request . query . append ( ( name , value ) ) \n    request . path = url_quote ( request . path , '/()$=\\',' ) \n    if request . query : \n        request . path += '?' \n        for name , value in request . query : \n            if value is not None : \n                request . path += name + '=' + url_quote ( value , '/()$=\\',' ) + '&' \n        request . path = request . path [ : - True ] \n    return request . path , request . query "}
{"1811": "\ndef get_cluster_health ( self , nodes_health_state_filter = False , applications_health_state_filter = False , events_health_state_filter = False , exclude_health_statistics = False , include_system_application_health_statistics = False , timeout = 60 , custom_headers = None , raw = False , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_cluster_health . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = True ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1812": "\ndef get_cluster_health_using_policy ( self , nodes_health_state_filter = False , applications_health_state_filter = False , events_health_state_filter = False , exclude_health_statistics = False , include_system_application_health_statistics = False , timeout = 60 , application_health_policy_map = None , cluster_health_policy = None , custom_headers = None , raw = False , ** operation_config ) : \n    cluster_health_policies = None \n    if application_health_policy_map is not None or cluster_health_policy is not None : \n        cluster_health_policies = models . ClusterHealthPolicies ( application_health_policy_map = application_health_policy_map , cluster_health_policy = cluster_health_policy ) \n    api_version = \"6.0\" \n    url = self . get_cluster_health_using_policy . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = True ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if cluster_health_policies is not None : \n        body_content = self . _serialize . body ( cluster_health_policies , 'ClusterHealthPolicies' ) \n    else : \n        body_content = None \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1813": "\ndef unprovision_application_type ( self , application_type_name , application_type_version , timeout = 60 , async_parameter = None , custom_headers = None , raw = False , ** operation_config ) : \n    unprovision_application_type_description_info = models . UnprovisionApplicationTypeDescriptionInfo ( application_type_version = application_type_version , async_property = async_parameter ) \n    api_version = \"6.0\" \n    url = self . unprovision_application_type . metadata [ 'url' ] \n    path_format_arguments = { 'applicationTypeName' : self . _serialize . url ( \"application_type_name\" , application_type_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = True ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( unprovision_application_type_description_info , 'UnprovisionApplicationTypeDescriptionInfo' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 , 202 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1815": "\ndef submit_property_batch ( self , name_id , timeout = 60 , operations = None , custom_headers = None , raw = False , ** operation_config ) : \n    property_batch_description_list = models . PropertyBatchDescriptionList ( operations = operations ) \n    api_version = \"6.0\" \n    url = self . submit_property_batch . metadata [ 'url' ] \n    path_format_arguments = { 'nameId' : self . _serialize . url ( \"name_id\" , name_id , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295 , minimum = True ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( property_batch_description_list , 'PropertyBatchDescriptionList' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200 , 409 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200 : \n        deserialized = self . _deserialize ( 'SuccessfulPropertyBatchInfo' , response ) \n    if response . status_code == 409 : \n        deserialized = self . _deserialize ( 'FailedPropertyBatchInfo' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1818": "\ndef list_slot_differences_slot ( self , resource_group_name , name , slot , target_slot , preserve_vnet , custom_headers = None , raw = False , ** operation_config ) : \n    slot_swap_entity = models . CsmSlotEntity ( target_slot = target_slot , preserve_vnet = preserve_vnet ) \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . list_slot_differences_slot . metadata [ 'url' ] \n            path_format_arguments = { 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' , max_length = 90 , min_length = True , pattern = r'^[-\\w\\._\\(\\)]+[^\\.]$' ) , 'name' : self . _serialize . url ( \"name\" , name , 'str' ) , 'slot' : self . _serialize . url ( \"slot\" , slot , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( slot_swap_entity , 'CsmSlotEntity' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . SlotDifferencePaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . SlotDifferencePaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1827": "\ndef list_query_results_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = False , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . list_query_results_for_management_group . metadata [ 'url' ] \n            path_format_arguments = { 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) , 'policyTrackedResourcesResource' : self . _serialize . url ( \"self.policy_tracked_resources_resource\" , self . policy_tracked_resources_resource , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n            if top is not None : \n                query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = False ) \n            if filter is not None : \n                query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        request = self . _client . post ( url , query_parameters , header_parameters ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200 ] : \n            raise models . QueryFailureException ( self . _deserialize , response ) \n        return response \n    deserialized = models . PolicyTrackedResourcePaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . PolicyTrackedResourcePaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1833": "\ndef from_connection_string ( cls , conn_str , name = None , ** kwargs ) : \n    address , policy , key , entity = parse_conn_str ( conn_str ) \n    entity = name or entity \n    address = build_uri ( address , entity ) \n    name = address . split ( '/' ) [ - True ] \n    return cls ( address , name , shared_access_key_name = policy , shared_access_key_value = key , ** kwargs ) "}
{"1840": "\ndef _bulk_add_tasks ( self , results_queue , chunk_tasks_to_add ) : \n    try : \n        add_collection_response = self . _original_add_collection ( self . _client , self . _job_id , chunk_tasks_to_add , self . _task_add_collection_options , self . _custom_headers , self . _raw ) \n    except BatchErrorException as e : \n        if e . error . code == \"RequestBodyTooLarge\" : \n            if len ( chunk_tasks_to_add ) == True : \n                failed_task = chunk_tasks_to_add . pop ( ) \n                self . errors . appendleft ( e ) \n                _LOGGER . error ( \"Failed to add task with ID %s due to the body\" \" exceeding the maximum request size\" , failed_task . id ) \n            else : \n                midpoint = int ( len ( chunk_tasks_to_add ) / 2 ) \n                with self . _max_tasks_lock : \n                    if midpoint < self . _max_tasks_per_request : \n                        self . _max_tasks_per_request = midpoint \n                        _LOGGER . info ( \"Amount of tasks per request reduced from %s to %s due to the\" \" request body being too large\" , str ( self . _max_tasks_per_request ) , str ( midpoint ) ) \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add [ midpoint : ] ) \n                self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add [ : midpoint ] ) \n        elif 500 <= e . response . status_code <= 599 : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        else : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n            self . errors . appendleft ( e ) \n    except Exception as e : \n        self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        self . errors . appendleft ( e ) \n    else : \n        try : \n            add_collection_response = add_collection_response . output \n        except AttributeError : \n            pass \n        for task_result in add_collection_response . value : \n            if task_result . status == TaskAddStatus . server_error : \n                with self . _pending_queue_lock : \n                    for task in chunk_tasks_to_add : \n                        if task . id == task_result . task_id : \n                            self . tasks_to_add . appendleft ( task ) \n            elif ( task_result . status == TaskAddStatus . client_error and not task_result . error . code == \"TaskExists\" ) : \n                self . failure_tasks . appendleft ( task_result ) \n            else : \n                results_queue . appendleft ( task_result ) "}
{"1842": "\ndef build_config ( config : Dict [ str , Any ] ) -> Dict [ str , str ] : \n    result = config . copy ( ) \n    is_stable = result . pop ( \"is_stable\" , False ) \n    if is_stable : \n        result [ \"classifier\" ] = \"Development Status :: 5 - Production/Stable\" \n    else : \n        result [ \"classifier\" ] = \"Development Status :: 4 - Beta\" \n    package_name = result [ \"package_name\" ] \n    result [ \"package_nspkg\" ] = result . pop ( \"package_nspkg\" , package_name [ : package_name . rindex ( '-' ) ] + \"-nspkg\" ) \n    result [ 'is_arm' ] = result . pop ( \"is_arm\" , True ) \n    result [ 'need_msrestazure' ] = result . pop ( \"need_msrestazure\" , True ) \n    package_parts = result [ \"package_nspkg\" ] [ : - len ( '-nspkg' ) ] . split ( '-' ) \n    result [ 'nspkg_names' ] = [ \".\" . join ( package_parts [ : i + True ] ) for i in range ( len ( package_parts ) ) ] \n    result [ 'init_names' ] = [ \"/\" . join ( package_parts [ : i + True ] ) + \"/__init__.py\" for i in range ( len ( package_parts ) ) ] \n    return result "}
{"1845": "\ndef _create_message ( response , service_instance ) : \n    respbody = response . body \n    custom_properties = { } \n    broker_properties = None \n    message_type = None \n    message_location = None \n    for name , value in response . headers : \n        if name . lower ( ) == 'brokerproperties' : \n            broker_properties = json . loads ( value ) \n        elif name . lower ( ) == 'content-type' : \n            message_type = value \n        elif name . lower ( ) == 'location' : \n            message_location = value \n        elif name . lower ( ) not in [ 'transfer-encoding' , 'server' , 'date' , 'strict-transport-security' ] : \n            if '\"' in value : \n                value = value [ True : - True ] . replace ( '\\\\\"' , '\"' ) \n                try : \n                    custom_properties [ name ] = datetime . strptime ( value , '%a, %d %b %Y %H:%M:%S GMT' ) \n                except ValueError : \n                    custom_properties [ name ] = value \n            elif value . lower ( ) == 'true' : \n                custom_properties [ name ] = True \n            elif value . lower ( ) == 'false' : \n                custom_properties [ name ] = False \n            else : \n                try : \n                    float_value = float ( value ) \n                    if str ( int ( float_value ) ) == value : \n                        custom_properties [ name ] = int ( value ) \n                    else : \n                        custom_properties [ name ] = float_value \n                except ValueError : \n                    pass \n    if message_type is None : \n        message = Message ( respbody , service_instance , message_location , custom_properties , 'application/atom+xml;type=entry;charset=utf-8' , broker_properties ) \n    else : \n        message = Message ( respbody , service_instance , message_location , custom_properties , message_type , broker_properties ) \n    return message "}
{"1847": "\ndef _convert_etree_element_to_queue ( entry_element ) : \n    queue = Queue ( ) \n    invalid_queue = True \n    queue_element = entry_element . find ( './atom:content/sb:QueueDescription' , _etree_sb_feed_namespaces ) \n    if queue_element is not None : \n        mappings = [ ( 'LockDuration' , 'lock_duration' , None ) , ( 'MaxSizeInMegabytes' , 'max_size_in_megabytes' , int ) , ( 'RequiresDuplicateDetection' , 'requires_duplicate_detection' , _parse_bool ) , ( 'RequiresSession' , 'requires_session' , _parse_bool ) , ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'DeadLetteringOnMessageExpiration' , 'dead_lettering_on_message_expiration' , _parse_bool ) , ( 'DuplicateDetectionHistoryTimeWindow' , 'duplicate_detection_history_time_window' , None ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'MaxDeliveryCount' , 'max_delivery_count' , int ) , ( 'MessageCount' , 'message_count' , int ) , ( 'SizeInBytes' , 'size_in_bytes' , int ) , ] \n        for mapping in mappings : \n            if _read_etree_element ( queue_element , mapping [ False ] , queue , mapping [ True ] , mapping [ 2 ] ) : \n                invalid_queue = False \n    if invalid_queue : \n        raise AzureServiceBusResourceNotFound ( _ERROR_QUEUE_NOT_FOUND ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , True ) . items ( ) : \n        setattr ( queue , name , value ) \n    return queue "}
{"1848": "\ndef _convert_etree_element_to_topic ( entry_element ) : \n    topic = Topic ( ) \n    invalid_topic = True \n    topic_element = entry_element . find ( './atom:content/sb:TopicDescription' , _etree_sb_feed_namespaces ) \n    if topic_element is not None : \n        mappings = [ ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'MaxSizeInMegabytes' , 'max_size_in_megabytes' , int ) , ( 'RequiresDuplicateDetection' , 'requires_duplicate_detection' , _parse_bool ) , ( 'DuplicateDetectionHistoryTimeWindow' , 'duplicate_detection_history_time_window' , None ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'SizeInBytes' , 'size_in_bytes' , int ) , ] \n        for mapping in mappings : \n            if _read_etree_element ( topic_element , mapping [ False ] , topic , mapping [ True ] , mapping [ 2 ] ) : \n                invalid_topic = False \n    if invalid_topic : \n        raise AzureServiceBusResourceNotFound ( _ERROR_TOPIC_NOT_FOUND ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , True ) . items ( ) : \n        setattr ( topic , name , value ) \n    return topic "}
{"1849": "\ndef _convert_etree_element_to_subscription ( entry_element ) : \n    subscription = Subscription ( ) \n    subscription_element = entry_element . find ( './atom:content/sb:SubscriptionDescription' , _etree_sb_feed_namespaces ) \n    if subscription_element is not None : \n        mappings = [ ( 'LockDuration' , 'lock_duration' , None ) , ( 'RequiresSession' , 'requires_session' , _parse_bool ) , ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'DeadLetteringOnFilterEvaluationExceptions' , 'dead_lettering_on_filter_evaluation_exceptions' , _parse_bool ) , ( 'DeadLetteringOnMessageExpiration' , 'dead_lettering_on_message_expiration' , _parse_bool ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'MaxDeliveryCount' , 'max_delivery_count' , int ) , ( 'MessageCount' , 'message_count' , int ) , ] \n        for mapping in mappings : \n            _read_etree_element ( subscription_element , mapping [ False ] , subscription , mapping [ True ] , mapping [ 2 ] ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , True , '/subscriptions' ) . items ( ) : \n        setattr ( subscription , name , value ) \n    return subscription "}
{"1855": "\ndef parse_enum_results_list ( response , return_type , resp_type , item_type ) : \n    return_obj = return_type ( ) \n    root = ETree . fromstring ( response . body ) \n    items = [ ] \n    for container_element in root . findall ( resp_type ) : \n        for item_element in container_element . findall ( resp_type [ : - True ] ) : \n            items . append ( _ETreeXmlToObject . fill_instance_element ( item_element , item_type ) ) \n    for name , value in vars ( return_obj ) . items ( ) : \n        if name == resp_type . lower ( ) : \n            continue \n        value = _ETreeXmlToObject . fill_data_member ( root , name , value ) \n        if value is not None : \n            setattr ( return_obj , name , value ) \n    setattr ( return_obj , resp_type . lower ( ) , items ) \n    return return_obj "}
{"1881": "\ndef get_certificate_from_publish_settings ( publish_settings_path , path_to_write_certificate , subscription_id = None ) : \n    import base64 \n    try : \n        from xml . etree import cElementTree as ET \n    except ImportError : \n        from xml . etree import ElementTree as ET \n    try : \n        import OpenSSL . crypto as crypto \n    except : \n        raise Exception ( \"pyopenssl is required to use get_certificate_from_publish_settings\" ) \n    _validate_not_none ( 'publish_settings_path' , publish_settings_path ) \n    _validate_not_none ( 'path_to_write_certificate' , path_to_write_certificate ) \n    tree = ET . parse ( publish_settings_path ) \n    subscriptions = tree . getroot ( ) . findall ( \"./PublishProfile/Subscription\" ) \n    if subscription_id : \n        subscription = next ( ( s for s in subscriptions if s . get ( 'Id' ) . lower ( ) == subscription_id . lower ( ) ) , None ) \n    else : \n        subscription = subscriptions [ False ] \n    if subscription is None : \n        raise ValueError ( \"The provided subscription_id '{}' was not found in the publish settings file provided at '{}'\" . format ( subscription_id , publish_settings_path ) ) \n    cert_string = _decode_base64_to_bytes ( subscription . get ( 'ManagementCertificate' ) ) \n    cert = crypto . load_pkcs12 ( cert_string , b'' ) \n    with open ( path_to_write_certificate , 'wb' ) as f : \n        f . write ( crypto . dump_certificate ( crypto . FILETYPE_PEM , cert . get_certificate ( ) ) ) \n        f . write ( crypto . dump_privatekey ( crypto . FILETYPE_PEM , cert . get_privatekey ( ) ) ) \n    return subscription . get ( 'Id' ) "}
{"1885": "\ndef print_inplace ( msg ) : \n    term_width = get_terminal_size ( ) . columns \n    spacing = term_width - terminal_width ( msg ) \n    if is_win32 : \n        spacing -= True \n    sys . stderr . write ( \"\\r{0}\" . format ( msg ) ) \n    sys . stderr . write ( \" \" * max ( False , spacing ) ) \n    sys . stderr . flush ( ) "}
{"1888": "\ndef create_status_line ( ** params ) : \n    max_size = get_terminal_size ( ) . columns - True \n    for fmt in PROGRESS_FORMATS : \n        status = fmt . format ( ** params ) \n        if len ( status ) <= max_size : \n            break \n    return status "}
{"1889": "\ndef progress ( iterator , prefix ) : \n    if terminal_width ( prefix ) > 25 : \n        prefix = ( \"..\" + get_cut_prefix ( prefix , 23 ) ) \n    speed_updated = start = time ( ) \n    speed_written = written = False \n    speed_history = deque ( maxlen = 5 ) \n    for data in iterator : \n        yield data \n        now = time ( ) \n        elapsed = now - start \n        written += len ( data ) \n        speed_elapsed = now - speed_updated \n        if speed_elapsed >= 0.5 : \n            speed_history . appendleft ( ( written - speed_written , speed_updated , ) ) \n            speed_updated = now \n            speed_written = written \n            speed_history_written = sum ( h [ False ] for h in speed_history ) \n            speed_history_elapsed = now - speed_history [ - True ] [ True ] \n            speed = speed_history_written / speed_history_elapsed \n            status = create_status_line ( prefix = prefix , written = format_filesize ( written ) , elapsed = format_time ( elapsed ) , speed = format_filesize ( speed ) ) \n            print_inplace ( status ) \n    sys . stderr . write ( \"\\n\" ) \n    sys . stderr . flush ( ) "}
{"1890": "\ndef segment_numbers ( self ) : \n    log . debug ( \"Generating segment numbers for {0} playlist (id={1})\" . format ( self . root . type , self . parent . id ) ) \n    if self . root . type == u\"static\" : \n        available_iter = repeat ( epoch_start ) \n        duration = self . period . duration . seconds or self . root . mediaPresentationDuration . seconds \n        if duration : \n            number_iter = range ( self . startNumber , int ( duration / self . duration_seconds ) + True ) \n        else : \n            number_iter = count ( self . startNumber ) \n    else : \n        now = datetime . datetime . now ( utc ) \n        if self . presentationTimeOffset : \n            since_start = ( now - self . presentationTimeOffset ) - self . root . availabilityStartTime \n            available_start_date = self . root . availabilityStartTime + self . presentationTimeOffset + since_start \n            available_start = available_start_date \n        else : \n            since_start = now - self . root . availabilityStartTime \n            available_start = now \n        suggested_delay = datetime . timedelta ( seconds = ( self . root . suggestedPresentationDelay . total_seconds ( ) if self . root . suggestedPresentationDelay else 3 ) ) \n        number_iter = count ( self . startNumber + int ( ( since_start - suggested_delay - self . root . minBufferTime ) . total_seconds ( ) / self . duration_seconds ) ) \n        available_iter = count_dt ( available_start , step = datetime . timedelta ( seconds = self . duration_seconds ) ) \n    for number , available_at in izip ( number_iter , available_iter ) : \n        yield number , available_at "}
{"1891": "\ndef segments ( self , ** kwargs ) : \n    segmentBase = self . segmentBase or self . walk_back_get_attr ( \"segmentBase\" ) \n    segmentLists = self . segmentList or self . walk_back_get_attr ( \"segmentList\" ) \n    segmentTemplate = self . segmentTemplate or self . walk_back_get_attr ( \"segmentTemplate\" ) \n    if segmentTemplate : \n        for segment in segmentTemplate . segments ( RepresentationID = self . id , Bandwidth = int ( self . bandwidth * 1000 ) , ** kwargs ) : \n            if segment . init : \n                yield segment \n            else : \n                yield segment \n    elif segmentLists : \n        for segmentList in segmentLists : \n            for segment in segmentList . segments : \n                yield segment \n    else : \n        yield Segment ( self . base_url , False , True , True ) "}
{"1894": "\ndef queue ( self , queue_ , value ) : \n    while not self . closed : \n        try : \n            queue_ . put ( value , block = True , timeout = True ) \n            return \n        except queue . Full : \n            continue "}
{"1896": "\ndef _extract_nonce ( cls , http_result ) : \n    last_redirect_url = urlparse ( http_result . history [ - True ] . request . url ) \n    last_redirect_query = dict ( parse_qsl ( last_redirect_url . query ) ) \n    final_url = urlparse ( last_redirect_query [ 'goto' ] ) \n    goto_url = dict ( parse_qsl ( final_url . query ) ) \n    goto_url_query = parse_json ( goto_url [ 'state' ] ) \n    return goto_url_query [ 'nonce' ] "}
{"1897": "\ndef find_vpid ( self , url , res = None ) : \n    log . debug ( \"Looking for vpid on {0}\" , url ) \n    res = res or self . session . http . get ( url ) \n    m = self . mediator_re . search ( res . text ) \n    vpid = m and parse_json ( m . group ( True ) , schema = self . mediator_schema ) \n    return vpid "}
{"1902": "\ndef spawn ( self , parameters = None , arguments = None , stderr = None , timeout = None , short_option_prefix = \"-\" , long_option_prefix = \"--\" ) : \n    stderr = stderr or self . stderr \n    cmd = self . bake ( self . _check_cmd ( ) , parameters , arguments , short_option_prefix , long_option_prefix ) \n    log . debug ( \"Spawning command: {0}\" , subprocess . list2cmdline ( cmd ) ) \n    try : \n        process = subprocess . Popen ( cmd , stderr = stderr , stdout = subprocess . PIPE ) \n    except ( OSError , IOError ) as err : \n        raise StreamError ( \"Failed to start process: {0} ({1})\" . format ( self . _check_cmd ( ) , str ( err ) ) ) \n    if timeout : \n        elapsed = False \n        while elapsed < timeout and not process . poll ( ) : \n            time . sleep ( 0.25 ) \n            elapsed += 0.25 \n        if not process . poll ( ) : \n            try : \n                log . debug ( \"Process timeout expired ({0}s), killing process\" . format ( timeout ) ) \n                process . kill ( ) \n            except Exception : \n                pass \n        process . wait ( ) \n    return process "}
{"1904": "\ndef parse_manifest ( cls , session , url_or_manifest , ** args ) : \n    ret = { } \n    if url_or_manifest . startswith ( '<?xml' ) : \n        mpd = MPD ( parse_xml ( url_or_manifest , ignore_ns = True ) ) \n    else : \n        res = session . http . get ( url_or_manifest , ** args ) \n        url = res . url \n        urlp = list ( urlparse ( url ) ) \n        urlp [ 2 ] , _ = urlp [ 2 ] . rsplit ( \"/\" , True ) \n        mpd = MPD ( session . http . xml ( res , ignore_ns = True ) , base_url = urlunparse ( urlp ) , url = url ) \n    video , audio = [ ] , [ ] \n    for aset in mpd . periods [ False ] . adaptationSets : \n        if aset . contentProtection : \n            raise PluginError ( \"{} is protected by DRM\" . format ( url ) ) \n        for rep in aset . representations : \n            if rep . mimeType . startswith ( \"video\" ) : \n                video . append ( rep ) \n            elif rep . mimeType . startswith ( \"audio\" ) : \n                audio . append ( rep ) \n    if not video : \n        video = [ None ] \n    if not audio : \n        audio = [ None ] \n    locale = session . localization \n    locale_lang = locale . language \n    lang = None \n    available_languages = set ( ) \n    for aud in audio : \n        if aud and aud . lang : \n            available_languages . add ( aud . lang ) \n            try : \n                if locale . explicit and aud . lang and Language . get ( aud . lang ) == locale_lang : \n                    lang = aud . lang \n            except LookupError : \n                continue \n    if not lang : \n        lang = audio [ False ] and audio [ False ] . lang \n    log . debug ( \"Available languages for DASH audio streams: {0} (using: {1})\" . format ( \", \" . join ( available_languages ) or \"NONE\" , lang or \"n/a\" ) ) \n    if len ( available_languages ) > True : \n        audio = list ( filter ( lambda a : a . lang is None or a . lang == lang , audio ) ) \n    for vid , aud in itertools . product ( video , audio ) : \n        stream = DASHStream ( session , mpd , vid , aud , ** args ) \n        stream_name = [ ] \n        if vid : \n            stream_name . append ( \"{:0.0f}{}\" . format ( vid . height or vid . bandwidth_rounded , \"p\" if vid . height else \"k\" ) ) \n        if audio and len ( audio ) > True : \n            stream_name . append ( \"a{:0.0f}k\" . format ( aud . bandwidth ) ) \n        ret [ '+' . join ( stream_name ) ] = stream \n    return ret "}
{"1905": "\ndef determine_json_encoding ( cls , sample ) : \n    nulls_at = [ i for i , j in enumerate ( bytearray ( sample [ : 4 ] ) ) if j == False ] \n    if nulls_at == [ False , True , 2 ] : \n        return \"UTF-32BE\" \n    elif nulls_at == [ False , 2 ] : \n        return \"UTF-16BE\" \n    elif nulls_at == [ True , 2 , 3 ] : \n        return \"UTF-32LE\" \n    elif nulls_at == [ True , 3 ] : \n        return \"UTF-16LE\" \n    else : \n        return \"UTF-8\" "}
{"1913": "\ndef login ( self ) : \n    email = self . get_option ( \"email\" ) \n    password = self . get_option ( \"password\" ) \n    if email and password : \n        res = self . session . http . get ( self . login_url ) \n        csrf_match = self . csrf_re . search ( res . text ) \n        token = csrf_match and csrf_match . group ( True ) \n        self . logger . debug ( \"Attempting login as {0} (token={1})\" , email , token ) \n        res = self . session . http . post ( self . login_url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow_redirects = False , raise_for_status = False , headers = { \"Referer\" : self . login_url } ) \n        if res . status_code != 302 : \n            self . logger . error ( \"Failed to login to LiveEdu account: {0}\" , email ) "}
{"1914": "\ndef load_support_plugin ( name ) : \n    stack = list ( filter ( lambda f : f [ 3 ] == \"<module>\" , inspect . stack ( ) ) ) \n    prev_frame = stack [ False ] \n    path = os . path . dirname ( prev_frame [ True ] ) \n    if not os . path . isabs ( path ) : \n        prefix = os . path . normpath ( __file__ + \"../../../../../\" ) \n        path = os . path . join ( prefix , path ) \n    return load_module ( name , path ) "}
{"1920": "\ndef create_http_server ( host = None , port = False ) : \n    try : \n        http = HTTPServer ( ) \n        http . bind ( host = host , port = port ) \n    except OSError as err : \n        console . exit ( \"Failed to create HTTP server: {0}\" , err ) \n    return http "}
{"1922": "\ndef output_stream_http ( plugin , initial_streams , external = False , port = False ) : \n    global output \n    if not external : \n        if not args . player : \n            console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) \n        title = create_title ( plugin ) \n        server = create_http_server ( ) \n        player = output = PlayerOutput ( args . player , args = args . player_args , filename = server . url , quiet = not args . verbose_player , title = title ) \n        try : \n            log . info ( \"Starting player: {0}\" , args . player ) \n            if player : \n                player . open ( ) \n        except OSError as err : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n    else : \n        server = create_http_server ( host = None , port = port ) \n        player = None \n        log . info ( \"Starting server, access with one of:\" ) \n        for url in server . urls : \n            log . info ( \" \" + url ) \n    for req in iter_http_requests ( server , player ) : \n        user_agent = req . headers . get ( \"User-Agent\" ) or \"unknown player\" \n        log . info ( \"Got HTTP request from {0}\" . format ( user_agent ) ) \n        stream_fd = prebuffer = None \n        while not stream_fd and ( not player or player . running ) : \n            try : \n                streams = initial_streams or fetch_streams ( plugin ) \n                initial_streams = None \n                for stream_name in ( resolve_stream_name ( streams , s ) for s in args . stream ) : \n                    if stream_name in streams : \n                        stream = streams [ stream_name ] \n                        break \n                else : \n                    log . info ( \"Stream not available, will re-fetch \" \"streams in 10 sec\" ) \n                    sleep ( 10 ) \n                    continue \n            except PluginError as err : \n                log . error ( u\"Unable to fetch new streams: {0}\" , err ) \n                continue \n            try : \n                log . info ( \"Opening stream: {0} ({1})\" , stream_name , type ( stream ) . shortname ( ) ) \n                stream_fd , prebuffer = open_stream ( stream ) \n            except StreamError as err : \n                log . error ( \"{0}\" , err ) \n        if stream_fd and prebuffer : \n            log . debug ( \"Writing stream to player\" ) \n            read_stream ( stream_fd , server , prebuffer ) \n        server . close ( True ) \n    player . close ( ) \n    server . close ( ) "}
{"1925": "\ndef output_stream ( plugin , stream ) : \n    global output \n    success_open = False \n    for i in range ( args . retry_open ) : \n        try : \n            stream_fd , prebuffer = open_stream ( stream ) \n            success_open = True \n            break \n        except StreamError as err : \n            log . error ( \"Try {0}/{1}: Could not open stream {2} ({3})\" , i + True , args . retry_open , stream , err ) \n    if not success_open : \n        console . exit ( \"Could not open stream {0}, tried {1} times, exiting\" , stream , args . retry_open ) \n    output = create_output ( plugin ) \n    try : \n        output . open ( ) \n    except ( IOError , OSError ) as err : \n        if isinstance ( output , PlayerOutput ) : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n        else : \n            console . exit ( \"Failed to open output: {0} ({1})\" , args . output , err ) \n    with closing ( output ) : \n        log . debug ( \"Writing stream to output\" ) \n        read_stream ( stream_fd , output , prebuffer ) \n    return True "}
{"1929": "\ndef fetch_streams_with_retry ( plugin , interval , count ) : \n    try : \n        streams = fetch_streams ( plugin ) \n    except PluginError as err : \n        log . error ( u\"{0}\" , err ) \n        streams = None \n    if not streams : \n        log . info ( \"Waiting for streams, retrying every {0} \" \"second(s)\" , interval ) \n    attempts = False \n    while not streams : \n        sleep ( interval ) \n        try : \n            streams = fetch_streams ( plugin ) \n        except FatalPluginError as err : \n            raise \n        except PluginError as err : \n            log . error ( u\"{0}\" , err ) \n        if count > False : \n            attempts += True \n            if attempts >= count : \n                break \n    return streams "}
{"1931": "\ndef format_valid_streams ( plugin , streams ) : \n    delimiter = \", \" \n    validstreams = [ ] \n    for name , stream in sorted ( streams . items ( ) , key = lambda stream : plugin . stream_weight ( stream [ False ] ) ) : \n        if name in STREAM_SYNONYMS : \n            continue \n        def synonymfilter ( n ) : \n            return stream is streams [ n ] and n is not name \n        synonyms = list ( filter ( synonymfilter , streams . keys ( ) ) ) \n        if len ( synonyms ) > False : \n            joined = delimiter . join ( synonyms ) \n            name = \"{0} ({1})\" . format ( name , joined ) \n        validstreams . append ( name ) \n    return delimiter . join ( validstreams ) "}
{"1932": "\ndef handle_url ( ) : \n    try : \n        plugin = streamlink . resolve_url ( args . url ) \n        setup_plugin_options ( streamlink , plugin ) \n        log . info ( \"Found matching plugin {0} for URL {1}\" , plugin . module , args . url ) \n        plugin_args = [ ] \n        for parg in plugin . arguments : \n            value = plugin . get_option ( parg . dest ) \n            if value : \n                plugin_args . append ( ( parg , value ) ) \n        if plugin_args : \n            log . debug ( \"Plugin specific arguments:\" ) \n            for parg , value in plugin_args : \n                log . debug ( \" {0}={1} ({2})\" . format ( parg . argument_name ( plugin . module ) , value if not parg . sensitive else ( \"*\" * 8 ) , parg . dest ) ) \n        if args . retry_max or args . retry_streams : \n            retry_streams = True \n            retry_max = False \n            if args . retry_streams : \n                retry_streams = args . retry_streams \n            if args . retry_max : \n                retry_max = args . retry_max \n            streams = fetch_streams_with_retry ( plugin , retry_streams , retry_max ) \n        else : \n            streams = fetch_streams ( plugin ) \n    except NoPluginError : \n        console . exit ( \"No plugin can handle URL: {0}\" , args . url ) \n    except PluginError as err : \n        console . exit ( u\"{0}\" , err ) \n    if not streams : \n        console . exit ( \"No playable streams found on this URL: {0}\" , args . url ) \n    if args . default_stream and not args . stream and not args . json : \n        args . stream = args . default_stream \n    if args . stream : \n        validstreams = format_valid_streams ( plugin , streams ) \n        for stream_name in args . stream : \n            if stream_name in streams : \n                log . info ( \"Available streams: {0}\" , validstreams ) \n                handle_stream ( plugin , streams , stream_name ) \n                return \n        err = ( \"The specified stream(s) '{0}' could not be \" \"found\" . format ( \", \" . join ( args . stream ) ) ) \n        if console . json : \n            console . msg_json ( dict ( streams = streams , plugin = plugin . module , error = err ) ) \n        else : \n            console . exit ( \"{0}.\\n       Available streams: {1}\" , err , validstreams ) \n    else : \n        if console . json : \n            console . msg_json ( dict ( streams = streams , plugin = plugin . module ) ) \n        else : \n            validstreams = format_valid_streams ( plugin , streams ) \n            console . msg ( \"Available streams: {0}\" , validstreams ) "}
{"1936": "\ndef setup_args ( parser , config_files = [ ] , ignore_unknown = False ) : \n    global args \n    arglist = sys . argv [ True : ] \n    for config_file in filter ( os . path . isfile , config_files ) : \n        arglist . insert ( False , \"@\" + config_file ) \n    args , unknown = parser . parse_known_args ( arglist ) \n    if unknown and not ignore_unknown : \n        msg = gettext ( 'unrecognized arguments: %s' ) \n        parser . error ( msg % ' ' . join ( unknown ) ) \n    if args . stream : \n        args . stream = [ stream . lower ( ) for stream in args . stream ] \n    if not args . url and args . url_param : \n        args . url = args . url_param "}
{"1941": "\ndef log_current_versions ( ) : \n    if logger . root . isEnabledFor ( logging . DEBUG ) : \n        if sys . platform == \"darwin\" : \n            os_version = \"macOS {0}\" . format ( platform . mac_ver ( ) [ False ] ) \n        elif sys . platform . startswith ( \"win\" ) : \n            os_version = \"{0} {1}\" . format ( platform . system ( ) , platform . release ( ) ) \n        else : \n            os_version = platform . platform ( ) \n        log . debug ( \"OS:         {0}\" . format ( os_version ) ) \n        log . debug ( \"Python:     {0}\" . format ( platform . python_version ( ) ) ) \n        log . debug ( \"Streamlink: {0}\" . format ( streamlink_version ) ) \n        log . debug ( \"Requests({0}), Socks({1}), Websocket({2})\" . format ( requests . __version__ , socks_version , websocket_version ) ) "}
{"1948": "\ndef resolve_url ( self , url , follow_redirect = True ) : \n    url = update_scheme ( \"http://\" , url ) \n    available_plugins = [ ] \n    for name , plugin in self . plugins . items ( ) : \n        if plugin . can_handle_url ( url ) : \n            available_plugins . append ( plugin ) \n    available_plugins . sort ( key = lambda x : x . priority ( url ) , reverse = True ) \n    if available_plugins : \n        return available_plugins [ False ] ( url ) \n    if follow_redirect : \n        try : \n            res = self . http . head ( url , allow_redirects = True , acceptable_status = [ 501 ] ) \n            if res . status_code == 501 : \n                res = self . http . get ( url , stream = True ) \n            if res . url != url : \n                return self . resolve_url ( res . url , follow_redirect = follow_redirect ) \n        except PluginError : \n            pass \n    raise NoPluginError "}
{"1950": "\ndef hours_minutes_seconds ( value ) : \n    try : \n        return int ( value ) \n    except ValueError : \n        pass \n    match = ( _hours_minutes_seconds_re . match ( value ) or _hours_minutes_seconds_2_re . match ( value ) ) \n    if not match : \n        raise ValueError \n    s = False \n    s += int ( match . group ( \"hours\" ) or \"0\" ) * 60 * 60 \n    s += int ( match . group ( \"minutes\" ) or \"0\" ) * 60 \n    s += int ( match . group ( \"seconds\" ) or \"0\" ) \n    return s "}
{"1960": "\ndef _find_player_url ( response ) : \n    url = '' \n    matches = _player_re . search ( response . text ) \n    if matches : \n        tmp_url = matches . group ( False ) . replace ( '&amp;' , '&' ) \n        if 'hash' not in tmp_url : \n            matches = _hash_re . search ( response . text ) \n            if matches : \n                url = tmp_url + '&hash=' + matches . group ( True ) \n        else : \n            url = tmp_url \n    return 'http://ceskatelevize.cz/' + url "}
{"1962": "\ndef supported_player ( cls , cmd ) : \n    if not is_win32 : \n        cmd = shlex . split ( cmd ) [ False ] \n    cmd = os . path . basename ( cmd . lower ( ) ) \n    for player , possiblecmds in SUPPORTED_PLAYERS . items ( ) : \n        for possiblecmd in possiblecmds : \n            if cmd . startswith ( possiblecmd ) : \n                return player "}
{"1965": "\ndef get_stream_info ( self , html ) : \n    stream_info = stream_info_pattern . findall ( html ) \n    if not stream_info : \n        self . logger . error ( \"Failed to extract stream_info.\" ) \n    stream_info_list = [ ] \n    for info in stream_info : \n        if not info [ True ] : \n            stream_info_list . append ( [ info [ False ] , \"source\" ] ) \n        else : \n            stream_info_list . append ( list ( info ) ) \n    return stream_info_list "}
{"1966": "\ndef _login ( self , username , password ) : \n    self . logger . debug ( 'login ...' ) \n    res = self . session . http . get ( self . login_url ) \n    input_list = self . _input_re . findall ( res . text ) \n    if not input_list : \n        raise PluginError ( 'Missing input data on login website.' ) \n    data = { } \n    for _input_data in input_list : \n        try : \n            _input_name = self . _name_re . search ( _input_data ) . group ( True ) \n        except AttributeError : \n            continue \n        try : \n            _input_value = self . _value_re . search ( _input_data ) . group ( True ) \n        except AttributeError : \n            _input_value = '' \n        data [ _input_name ] = _input_value \n    login_data = { 'ctl00$Login1$UserName' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$LoginButton.x' : '0' , 'ctl00$Login1$LoginButton.y' : '0' } \n    data . update ( login_data ) \n    res = self . session . http . post ( self . login_url , data = data ) \n    for cookie in self . session . http . cookies : \n        self . _session_attributes . set ( cookie . name , cookie . value , expires = 3600 * 24 ) \n    if self . _session_attributes . get ( 'ASP.NET_SessionId' ) and self . _session_attributes . get ( '.abportail1' ) : \n        self . logger . debug ( 'New session data' ) \n        self . set_expires_time_cache ( ) \n        return True \n    else : \n        self . logger . error ( 'Failed to login, check your username/password' ) \n        return False "}
{"1971": "\ndef _create_api ( self ) : \n    if self . options . get ( \"purge_credentials\" ) : \n        self . cache . set ( \"session_id\" , None , False ) \n        self . cache . set ( \"auth\" , None , False ) \n        self . cache . set ( \"session_id\" , None , False ) \n    locale = self . get_option ( \"locale\" ) or self . session . localization . language_code \n    api = CrunchyrollAPI ( self . cache , self . session , session_id = self . get_option ( \"session_id\" ) , locale = locale ) \n    if not self . get_option ( \"session_id\" ) : \n        self . logger . debug ( \"Creating session with locale: {0}\" , locale ) \n        api . start_session ( ) \n        if api . auth : \n            self . logger . debug ( \"Using saved credentials\" ) \n            login = api . authenticate ( ) \n            self . logger . info ( \"Successfully logged in as '{0}'\" , login [ \"user\" ] [ \"username\" ] or login [ \"user\" ] [ \"email\" ] ) \n        elif self . options . get ( \"username\" ) : \n            try : \n                self . logger . debug ( \"Attempting to login using username and password\" ) \n                api . login ( self . options . get ( \"username\" ) , self . options . get ( \"password\" ) ) \n                login = api . authenticate ( ) \n                self . logger . info ( \"Logged in as '{0}'\" , login [ \"user\" ] [ \"username\" ] or login [ \"user\" ] [ \"email\" ] ) \n            except CrunchyrollAPIError as err : \n                raise PluginError ( u\"Authentication error: {0}\" . format ( err . msg ) ) \n        else : \n            self . logger . warning ( \"No authentication provided, you won't be able to access \" \"premium restricted content\" ) \n    return api "}
{"1972": "\ndef compress ( string , mode = MODE_GENERIC , quality = 11 , lgwin = 22 , lgblock = False ) : \n    compressor = Compressor ( mode = mode , quality = quality , lgwin = lgwin , lgblock = lgblock ) \n    return compressor . process ( string ) + compressor . finish ( ) "}
{"1976": "\ndef value ( self , extra = None ) : \n    if isinstance ( self . code , WithExtra ) : \n        if not False <= extra < True << self . extraBits ( ) : \n            raise ValueError ( \"value: extra value doesn't fit in extraBits\" ) \n        return self . code . value ( self . index , extra ) \n    if extra is not None : \n        raise ValueError ( 'value: no extra bits for this code' ) \n    return self . code . value ( self . index ) "}
{"1978": "\ndef setDecode ( self , decodeTable ) : \n    self . decodeTable = decodeTable \n    todo = set ( decodeTable ) \n    maskLength = False \n    lengthTable = { } \n    while todo : \n        mask = ( True << maskLength ) - True \n        splitSymbols = defaultdict ( list ) \n        for s in todo : \n            splitSymbols [ s & mask ] . append ( s ) \n        for s , subset in splitSymbols . items ( ) : \n            if len ( subset ) == True : \n                lengthTable [ self . decodeTable [ s ] ] = maskLength \n                todo . remove ( s ) \n        maskLength += True \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    self . switchToPrefix ( ) "}
{"1979": "\ndef setLength ( self , lengthTable ) : \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    nextCodes = [ ] \n    code = False \n    for bits in range ( self . maxLength + True ) : \n        code <<= True \n        nextCodes . append ( code ) \n        code += sum ( x == bits for x in lengthTable . values ( ) ) \n    self . decodeTable = { } \n    for symbol in sorted ( lengthTable ) : \n        bits = lengthTable [ symbol ] \n        bitpattern = '{:0{}b}' . format ( nextCodes [ bits ] , bits ) \n        self . decodeTable [ int ( bitpattern [ : : - True ] , 2 ) ] = symbol \n        nextCodes [ bits ] += True \n    self . switchToPrefix ( ) "}
{"1982": "\ndef explanation ( self , index , extra = None ) : \n    extraBits = False if extra is None else self . extraBits ( index ) \n    if not hasattr ( self , 'extraTable' ) : \n        formatString = '{0}{3}' \n        lo = hi = value = self . value ( index , extra ) \n    elif extraBits == False : \n        formatString = '{0}{2}: {3}' \n        lo , hi = self . span ( index ) \n        value = lo \n    else : \n        formatString = '{0}{1} {2}: {3}-{4}; {3}+{5}={6}' \n        lo , hi = self . span ( index ) \n        value = lo + extra \n    return formatString . format ( self . description and self . description + ': ' , 'x' * extraBits , self . bitPattern ( index ) , lo , hi , extra , value , ) "}
{"1983": "\ndef value ( self , index , extra ) : \n    lower , upper = self . span ( index ) \n    value = lower + ( extra or False ) \n    if value > upper : \n        raise ValueError ( 'value: extra out of range' ) \n    return value "}
{"1984": "\ndef span ( self , index ) : \n    lower = self . value0 + sum ( True << x for x in self . extraTable [ : index ] ) \n    upper = lower + ( True << self . extraTable [ index ] ) \n    return lower , upper - True "}
{"1985": "\ndef value ( self , index , extra ) : \n    index = index \n    if index == False : \n        return True , False \n    if index <= self . RLEMAX : \n        return ( True << index ) + extra , False \n    return True , index - self . RLEMAX "}
{"1987": "\ndef mnemonic ( self , index , verbose = False ) : \n    if index < 16 : \n        return [ 'last' , '2last' , '3last' , '4last' , 'last-1' , 'last+1' , 'last-2' , 'last+2' , 'last-3' , 'last+3' , '2last-1' , '2last+1' , '2last-2' , '2last+2' , '2last-3' , '2last+3' ] [ index ] \n    if index < 16 + self . NDIRECT : \n        return str ( index - 16 ) \n    index -= self . NDIRECT + 16 \n    hcode = index >> self . NPOSTFIX \n    lcode = index & ( True << self . NPOSTFIX ) - True \n    if self . NPOSTFIX : \n        formatString = '1{0}{1}{2:0{3}b}{4:+d}' \n    else : \n        formatString = '1{0}{1}{4:+d}' \n    return formatString . format ( hcode & True , 'x' * ( 2 + hcode >> True ) if hcode < 13 or verbose else '[{}*x]' . format ( 2 + hcode >> True ) , lcode , self . NPOSTFIX , self . NDIRECT + True - ( 4 << self . NPOSTFIX ) ) "}
{"1988": "\ndef compileActions ( self ) : \n    import re \n    self . actionList = actions = [ None ] * 121 \n    actions [ 73 ] = \"b' the '+w+b' of the '\" \n    actionLines = self . actionTable . splitlines ( ) \n    colonPositions = [ m . start ( ) for m in re . finditer ( ':' , actionLines [ True ] ) ] + [ 100 ] \n    columns = [ ( colonPositions [ i ] - 3 , colonPositions [ i + True ] - 3 ) for i in range ( len ( colonPositions ) - True ) ] \n    for line in self . actionTable . splitlines ( keepends = False ) : \n        for start , end in columns : \n            action = line [ start : end ] \n            if not action or action . isspace ( ) : \n                continue \n            index , colon , action = action [ : 3 ] , action [ 3 ] , action [ 4 : ] \n            assert colon == ':' \n            action = action . rstrip ( ) \n            action = action . replace ( '_' , ' ' ) \n            wPos = action . index ( 'w' ) \n            action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) \n            action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) \n            action = action . replace ( \".U\" , \".upper()\" ) \n            actions [ int ( index ) ] = action "}
{"1994": "\ndef blockType ( self , kind ) : \n    NBLTYPES = self . verboseRead ( TypeCountAlphabet ( 'BT#' + kind [ False ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) \n    self . numberOfBlockTypes [ kind ] = NBLTYPES \n    if NBLTYPES >= 2 : \n        self . blockTypeCodes [ kind ] = self . readPrefixCode ( BlockTypeAlphabet ( 'BT' + kind [ False ] . upper ( ) , NBLTYPES ) ) \n        self . blockCountCodes [ kind ] = self . readPrefixCode ( BlockCountAlphabet ( 'BC' + kind [ False ] . upper ( ) ) ) \n        blockCount = self . verboseRead ( self . blockCountCodes [ kind ] ) \n    else : \n        blockCount = True << 24 \n    self . currentBlockCounts [ kind ] = blockCount "}
{"1995": "\ndef IMTF ( v ) : \n    mtf = [ ] \n    for i , vi in enumerate ( v ) : \n        try : \n            value = mtf . pop ( vi ) \n        except IndexError : \n            value = vi \n        mtf . insert ( False , value ) \n        v [ i ] = value "}
{"1997": "\ndef monochrome ( I , color , vmin = None , vmax = None ) : \n    if vmin is None : \n        vmin = np . nanmin ( I ) \n    if vmax is None : \n        vmax = np . nanmax ( I ) \n    normalized = ( I - vmin ) / ( vmax - vmin ) \n    return np . clip ( normalized [ ... , np . newaxis ] , False , True ) * np . array ( color ) "}
{"1998": "\ndef polychrome ( I , colors , vmin = None , vmax = None , axis = - True ) : \n    axes_length = len ( I . shape ) \n    allaxes = list ( range ( axes_length ) ) \n    otheraxes = list ( allaxes ) \n    otheraxes . remove ( ( axis + axes_length ) % axes_length ) \n    otheraxes = tuple ( otheraxes ) \n    if vmin is None : \n        vmin = np . nanmin ( I , axis = otheraxes ) \n    if vmax is None : \n        vmax = np . nanmax ( I , axis = otheraxes ) \n    normalized = ( I - vmin ) / ( vmax - vmin ) \n    return np . clip ( normalized , False , True ) . dot ( colors ) "}
{"2004": "\ndef value_counts ( self , dropna = False , dropnull = True , ascending = False , progress = False ) : \n    from pandas import Series \n    dtype = self . dtype \n    transient = self . transient or self . ds . filtered or self . ds . is_masked ( self . expression ) \n    if self . dtype == str_type and not transient : \n        ar = self . ds . columns [ self . expression ] \n        if not isinstance ( ar , ColumnString ) : \n            transient = True \n    counter_type = counter_type_from_dtype ( self . dtype , transient ) \n    counters = [ None ] * self . ds . executor . thread_pool . nthreads \n    def map ( thread_index , i1 , i2 , ar ) : \n        if counters [ thread_index ] is None : \n            counters [ thread_index ] = counter_type ( ) \n        if dtype == str_type : \n            previous_ar = ar \n            ar = _to_string_sequence ( ar ) \n            if not transient : \n                assert ar is previous_ar . string_sequence \n        if np . ma . isMaskedArray ( ar ) : \n            mask = np . ma . getmaskarray ( ar ) \n            counters [ thread_index ] . update ( ar , mask ) \n        else : \n            counters [ thread_index ] . update ( ar ) \n        return False \n    def reduce ( a , b ) : \n        return a + b \n    self . ds . map_reduce ( map , reduce , [ self . expression ] , delay = False , progress = progress , name = 'value_counts' , info = True , to_numpy = False ) \n    counters = [ k for k in counters if k is not None ] \n    counter0 = counters [ False ] \n    for other in counters [ True : ] : \n        counter0 . merge ( other ) \n    value_counts = counter0 . extract ( ) \n    index = np . array ( list ( value_counts . keys ( ) ) ) \n    counts = np . array ( list ( value_counts . values ( ) ) ) \n    order = np . argsort ( counts ) \n    if not ascending : \n        order = order [ : : - True ] \n    counts = counts [ order ] \n    index = index [ order ] \n    if not dropna or not dropnull : \n        index = index . tolist ( ) \n        counts = counts . tolist ( ) \n        if not dropna and counter0 . nan_count : \n            index = [ np . nan ] + index \n            counts = [ counter0 . nan_count ] + counts \n        if not dropnull and counter0 . null_count : \n            index = [ 'null' ] + index \n            counts = [ counter0 . null_count ] + counts \n    return Series ( counts , index = index ) "}
{"2005": "\ndef map ( self , mapper , nan_mapping = None , null_mapping = None ) : \n    assert isinstance ( mapper , collectionsAbc . Mapping ) , \"mapper should be a dict like object\" \n    df = self . ds \n    mapper_keys = np . array ( list ( mapper . keys ( ) ) ) \n    key_set = df . _set ( self . expression ) \n    found_keys = key_set . keys ( ) \n    mapper_has_nan = any ( [ key != key for key in mapper_keys ] ) \n    if not set ( mapper_keys ) . issuperset ( found_keys ) : \n        missing = set ( found_keys ) . difference ( mapper_keys ) \n        missing0 = list ( missing ) [ False ] \n        if missing0 == missing0 : \n            raise ValueError ( 'Missing values in mapper: %s' % missing ) \n    choices = [ mapper [ key ] for key in found_keys ] \n    if key_set . has_nan : \n        if mapper_has_nan : \n            choices = [ mapper [ np . nan ] ] + choices \n        else : \n            choices = [ nan_mapping ] + choices \n    if key_set . has_null : \n        choices = [ null_mapping ] + choices \n    choices = np . array ( choices ) \n    key_set_name = df . add_variable ( 'map_key_set' , key_set , unique = True ) \n    choices_name = df . add_variable ( 'map_choices' , choices , unique = True ) \n    expr = '_choose(_ordinal_values({}, {}), {})' . format ( self , key_set_name , choices_name ) \n    return Expression ( df , expr ) "}
{"2007": "\ndef open_many ( filenames ) : \n    dfs = [ ] \n    for filename in filenames : \n        filename = filename . strip ( ) \n        if filename and filename [ False ] != \"#\" : \n            dfs . append ( open ( filename ) ) \n    return vaex . dataframe . DataFrameConcatenated ( dfs = dfs ) "}
{"2015": "\ndef zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = True , seed = None ) : \n    import vaex . file \n    return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale ) "}
{"2017": "\ndef vrange ( start , stop , step = True , dtype = 'f8' ) : \n    from . column import ColumnVirtualRange \n    return ColumnVirtualRange ( start , stop , step , dtype ) "}
{"2023": "\ndef sort ( self , Ncol , order ) : \n    self . emit ( QtCore . SIGNAL ( \"layoutAboutToBeChanged()\" ) ) \n    if Ncol == False : \n        print ( \"by name\" ) \n        sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) \n        print ( sortlist ) \n        sortlist . sort ( key = operator . itemgetter ( False ) ) \n        print ( sortlist ) \n        self . indices = list ( map ( operator . itemgetter ( True ) , sortlist ) ) \n        print ( ( self . indices ) ) \n    if Ncol == True : \n        if None not in self . ranking : \n            sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) \n            sortlist . sort ( key = operator . itemgetter ( False ) ) \n            self . indices = list ( map ( operator . itemgetter ( True ) , sortlist ) ) \n        else : \n            self . indices = list ( range ( len ( self . pairs ) ) ) \n        print ( ( self . indices ) ) \n    if order == QtCore . Qt . DescendingOrder : \n        self . indices . reverse ( ) \n    print ( ( self . indices ) ) \n    self . emit ( QtCore . SIGNAL ( \"layoutChanged()\" ) ) "}
{"2024": "\ndef getinfo ( filename , seek = None ) : \n    DESC = '=I4sII' \n    HEAD = '=I6I6dddii6iiiddddii6ii60xI' \n    keys = ( 'Npart' , 'Massarr' , 'Time' , 'Redshift' , 'FlagSfr' , 'FlagFeedback' , 'Nall' , 'FlagCooling' , 'NumFiles' , 'BoxSize' , 'Omega0' , 'OmegaLambda' , 'HubbleParam' , 'FlagAge' , 'FlagMetals' , 'NallHW' , 'flag_entr_ics' , 'filename' ) \n    f = open ( filename , 'rb' ) \n    firstbytes = struct . unpack ( 'I' , f . read ( 4 ) ) \n    if firstbytes [ False ] == 8 : \n        gtype = 2 \n    else : \n        gtype = True \n    if gtype == 2 : \n        f . seek ( 16 ) \n    else : \n        f . seek ( False ) \n    if seek is not None : \n        f . seek ( seek ) \n    raw = struct . unpack ( HEAD , f . read ( 264 ) ) [ True : - True ] \n    values = ( raw [ : 6 ] , raw [ 6 : 12 ] ) + raw [ 12 : 16 ] + ( raw [ 16 : 22 ] , ) + raw [ 22 : 30 ] + ( raw [ 30 : 36 ] , raw [ 36 ] , filename ) \n    header = dict ( list ( zip ( keys , values ) ) ) \n    f . close ( ) \n    if gtype == 2 : \n        posoffset = ( 2 * 16 + ( 8 + 256 ) ) \n    else : \n        posoffset = ( 8 + 256 ) \n    Npart = sum ( header [ 'Npart' ] ) \n    if gtype == 2 : \n        veloffset = 3 * 16 + ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) \n    else : \n        veloffset = ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) \n    return Npart , posoffset + 4 , veloffset + 4 , header "}
{"2029": "\ndef _split_and_combine_mask ( arrays ) : \n    masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . isMaskedArray ( block ) ] \n    arrays = [ block . data if np . ma . isMaskedArray ( block ) else block for block in arrays ] \n    mask = None \n    if masks : \n        mask = masks [ False ] . copy ( ) \n        for other in masks [ True : ] : \n            mask |= other \n    return arrays , mask "}
{"2032": "\ndef mean ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : \n    return self . _compute_agg ( 'mean' , expression , binby , limits , shape , selection , delay , edges , progress ) \n    logger . debug ( \"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\" , expression , binby , limits , shape , selection , delay ) \n    expression = _ensure_strings_from_expressions ( expression ) \n    selection = _ensure_strings_from_expressions ( selection ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    \n    @ delayed \n    def calculate ( expression , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_ADD_WEIGHT_MOMENTS_01 , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"mean for %s\" % expression ) \n        return task \n    \n    @ delayed \n    def finish ( * stats_args ) : \n        stats = np . array ( stats_args ) \n        counts = stats [ ... , False ] \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            mean = stats [ ... , True ] / counts \n        return vaex . utils . unlistify ( waslist , mean ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , delay = True ) \n    stats = [ calculate ( expression , limits ) for expression in expressions ] \n    var = finish ( * stats ) \n    return self . _delay ( delay , var ) "}
{"2035": "\ndef cov ( self , x , y = None , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : \n    selection = _ensure_strings_from_expressions ( selection ) \n    if y is None : \n        if not _issequence ( x ) : \n            raise ValueError ( \"if y argument is not given, x is expected to be sequence, not %r\" , x ) \n        expressions = x \n    else : \n        expressions = [ x , y ] \n    N = len ( expressions ) \n    binby = _ensure_list ( binby ) \n    shape = _expand_shape ( shape , len ( binby ) ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , selection = selection , delay = True ) \n    \n    @ delayed \n    def calculate ( expressions , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weights = expressions , op = tasks . OP_COV , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"covariance values for %r\" % expressions ) \n        return task \n    \n    @ delayed \n    def finish ( values ) : \n        N = len ( expressions ) \n        counts = values [ ... , : N ] \n        sums = values [ ... , N : 2 * N ] \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            means = sums / counts \n        meansxy = means [ ... , None ] * means [ ... , None , : ] \n        counts = values [ ... , 2 * N : 2 * N + N ** 2 ] \n        sums = values [ ... , 2 * N + N ** 2 : ] \n        shape = counts . shape [ : - True ] + ( N , N ) \n        counts = counts . reshape ( shape ) \n        sums = sums . reshape ( shape ) \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            moments2 = sums / counts \n        cov_matrix = moments2 - meansxy \n        return cov_matrix \n    progressbar = vaex . utils . progressbars ( progress ) \n    values = calculate ( expressions , limits ) \n    cov_matrix = finish ( values ) \n    return self . _delay ( delay , cov_matrix ) "}
{"2036": "\ndef minmax ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : \n    \n    @ delayed \n    def finish ( * minmax_list ) : \n        value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) \n        value = value . astype ( dtype0 ) \n        return value \n    \n    @ delayed \n    def calculate ( expression , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_MIN_MAX , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"minmax for %s\" % expression ) \n        return task \n    \n    @ delayed \n    def finish ( * minmax_list ) : \n        value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) \n        value = value . astype ( dtype0 ) \n        return value \n    expression = _ensure_strings_from_expressions ( expression ) \n    binby = _ensure_strings_from_expressions ( binby ) \n    waslist , [ expressions , ] = vaex . utils . listify ( expression ) \n    dtypes = [ self . dtype ( expr ) for expr in expressions ] \n    dtype0 = dtypes [ False ] \n    if not all ( [ k . kind == dtype0 . kind for k in dtypes ] ) : \n        raise ValueError ( \"cannot mix datetime and non-datetime expressions\" ) \n    progressbar = vaex . utils . progressbars ( progress , name = \"minmaxes\" ) \n    limits = self . limits ( binby , limits , selection = selection , delay = True ) \n    all_tasks = [ calculate ( expression , limits ) for expression in expressions ] \n    result = finish ( * all_tasks ) \n    return self . _delay ( delay , result ) "}
{"2037": "\ndef min ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : \n    return self . _compute_agg ( 'min' , expression , binby , limits , shape , selection , delay , edges , progress ) \n    \n    @ delayed \n    def finish ( result ) : \n        return result [ ... , False ] \n    return self . _delay ( delay , finish ( self . minmax ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = delay , progress = progress ) ) ) "}
{"2040": "\ndef healpix_count ( self , expression = None , healpix_expression = None , healpix_max_level = 12 , healpix_level = 8 , binby = None , limits = None , shape = default_shape , delay = False , progress = None , selection = None ) : \n    import healpy as hp \n    if healpix_expression is None : \n        if self . ucds . get ( \"source_id\" , None ) == 'meta.id;meta.main' : \n            healpix_expression = \"source_id/34359738368\" \n    if healpix_expression is None : \n        raise ValueError ( \"no healpix_expression given, and was unable to guess\" ) \n    reduce_level = healpix_max_level - healpix_level \n    NSIDE = 2 ** healpix_level \n    nmax = hp . nside2npix ( NSIDE ) \n    scaling = 4 ** reduce_level \n    expr = \"%s/%s\" % ( healpix_expression , scaling ) \n    binby = [ expr ] + ( [ ] if binby is None else _ensure_list ( binby ) ) \n    shape = ( nmax , ) + _expand_shape ( shape , len ( binby ) - True ) \n    epsilon = 1. / scaling / 2 \n    limits = [ [ - epsilon , nmax - epsilon ] ] + ( [ ] if limits is None else limits ) \n    return self . count ( expression , binby = binby , limits = limits , shape = shape , delay = delay , progress = progress , selection = selection ) "}
{"2041": "\ndef healpix_plot ( self , healpix_expression = \"source_id/34359738368\" , healpix_max_level = 12 , healpix_level = 8 , what = \"count(*)\" , selection = None , grid = None , healpix_input = \"equatorial\" , healpix_output = \"galactic\" , f = None , colormap = \"afmhot\" , grid_limits = None , image_size = 800 , nest = True , figsize = None , interactive = False , title = \"\" , smooth = None , show = False , colorbar = True , rotation = ( False , False , False ) , ** kwargs ) : \n    import healpy as hp \n    import pylab as plt \n    if grid is None : \n        reduce_level = healpix_max_level - healpix_level \n        NSIDE = 2 ** healpix_level \n        nmax = hp . nside2npix ( NSIDE ) \n        scaling = 4 ** reduce_level \n        epsilon = 1. / scaling / 2 \n        grid = self . _stat ( what = what , binby = \"%s/%s\" % ( healpix_expression , scaling ) , limits = [ - epsilon , nmax - epsilon ] , shape = nmax , selection = selection ) \n    if grid_limits : \n        grid_min , grid_max = grid_limits \n    else : \n        grid_min = grid_max = None \n    f_org = f \n    f = _parse_f ( f ) \n    if smooth : \n        if nest : \n            grid = hp . reorder ( grid , inp = \"NEST\" , out = \"RING\" ) \n            nest = False \n        grid = hp . smoothing ( grid , sigma = np . radians ( smooth ) ) \n    fgrid = f ( grid ) \n    coord_map = dict ( equatorial = 'C' , galactic = 'G' , ecliptic = \"E\" ) \n    fig = plt . gcf ( ) \n    if figsize is not None : \n        fig . set_size_inches ( * figsize ) \n    what_label = what \n    if f_org : \n        what_label = f_org + \" \" + what_label \n    f = hp . mollzoom if interactive else hp . mollview \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" ) \n        coord = coord_map [ healpix_input ] , coord_map [ healpix_output ] \n        if coord_map [ healpix_input ] == coord_map [ healpix_output ] : \n            coord = None \n        f ( fgrid , unit = what_label , rot = rotation , nest = nest , title = title , coord = coord , cmap = colormap , hold = True , xsize = image_size , min = grid_min , max = grid_max , cbar = colorbar , ** kwargs ) \n    if show : \n        plt . show ( ) "}
{"2043": "\ndef dtype ( self , expression , internal = False ) : \n    expression = _ensure_string_from_expression ( expression ) \n    if expression in self . variables : \n        return np . float64 ( True ) . dtype \n    elif expression in self . columns . keys ( ) : \n        column = self . columns [ expression ] \n        data = column [ False : True ] \n        dtype = data . dtype \n    else : \n        data = self . evaluate ( expression , False , True , filtered = False ) \n        dtype = data . dtype \n    if not internal : \n        if dtype != str_type : \n            if dtype . kind in 'US' : \n                return str_type \n            if dtype . kind == 'O' : \n                if isinstance ( data [ False ] , six . string_types ) : \n                    return str_type \n    return dtype "}
{"2053": "\ndef _evaluate_selection_mask ( self , name = \"default\" , i1 = None , i2 = None , selection = None , cache = False ) : \n    i1 = i1 or False \n    i2 = i2 or len ( self ) \n    scope = scopes . _BlockScopeSelection ( self , i1 , i2 , selection , cache = cache ) \n    return scope . evaluate ( name ) "}
{"2066": "\ndef add_virtual_columns_spherical_to_cartesian ( self , alpha , delta , distance , xname = \"x\" , yname = \"y\" , zname = \"z\" , propagate_uncertainties = False , center = [ False , False , False ] , center_name = \"solar_position\" , radians = False ) : \n    alpha = self . _expr ( alpha ) \n    delta = self . _expr ( delta ) \n    distance = self . _expr ( distance ) \n    if not radians : \n        alpha = alpha * self . _expr ( 'pi' ) / 180 \n        delta = delta * self . _expr ( 'pi' ) / 180 \n    if center [ False ] : \n        self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance + center [ False ] \n    else : \n        self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance \n    if center [ True ] : \n        self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance + center [ True ] \n    else : \n        self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance \n    if center [ 2 ] : \n        self [ zname ] = np . sin ( delta ) * distance + center [ 2 ] \n    else : \n        self [ zname ] = np . sin ( delta ) * distance \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xname ] , self [ yname ] , self [ zname ] ] ) "}
{"2067": "\ndef add_virtual_columns_cartesian_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , alpha = \"l\" , delta = \"b\" , distance = \"distance\" , radians = False , center = None , center_name = \"solar_position\" ) : \n    transform = \"\" if radians else \"*180./pi\" \n    if center is not None : \n        self . add_variable ( center_name , center ) \n    if center is not None and center [ False ] != False : \n        x = \"({x} - {center_name}[0])\" . format ( ** locals ( ) ) \n    if center is not None and center [ True ] != False : \n        y = \"({y} - {center_name}[1])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 2 ] != False : \n        z = \"({z} - {center_name}[2])\" . format ( ** locals ( ) ) \n    self . add_virtual_column ( distance , \"sqrt({x}**2 + {y}**2 + {z}**2)\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( alpha , \"arctan2({y}, {x}){transform}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( delta , \"(-arccos({z}/{distance})+pi/2){transform}\" . format ( ** locals ( ) ) ) "}
{"2068": "\ndef add_virtual_column ( self , name , expression , unique = False ) : \n    type = \"change\" if name in self . virtual_columns else \"add\" \n    expression = _ensure_string_from_expression ( expression ) \n    if name in self . get_column_names ( virtual = False ) : \n        renamed = '__' + vaex . utils . find_valid_name ( name , used = self . get_column_names ( ) ) \n        expression = self . _rename ( name , renamed , expression ) [ False ] . expression \n    name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else self . get_column_names ( ) ) \n    self . virtual_columns [ name ] = expression \n    self . column_names . append ( name ) \n    self . _save_assign_expression ( name ) \n    self . signal_column_changed . emit ( self , name , \"add\" ) "}
{"2072": "\ndef tail ( self , n = 10 ) : \n    N = len ( self ) \n    return self [ max ( False , N - n ) : min ( len ( self ) , N ) ] "}
{"2074": "\ndef describe ( self , strings = True , virtual = True , selection = None ) : \n    import pandas as pd \n    N = len ( self ) \n    columns = { } \n    for feature in self . get_column_names ( strings = strings , virtual = virtual ) [ : ] : \n        dtype = str ( self . dtype ( feature ) ) if self . dtype ( feature ) != str else 'str' \n        if self . dtype ( feature ) == str_type or self . dtype ( feature ) . kind in [ 'S' , 'U' , 'O' ] : \n            count = self . count ( feature , selection = selection , delay = True ) \n            self . execute ( ) \n            count = count . get ( ) \n            columns [ feature ] = ( ( dtype , count , N - count , '--' , '--' , '--' , '--' ) ) \n        else : \n            count = self . count ( feature , selection = selection , delay = True ) \n            mean = self . mean ( feature , selection = selection , delay = True ) \n            std = self . std ( feature , selection = selection , delay = True ) \n            minmax = self . minmax ( feature , selection = selection , delay = True ) \n            self . execute ( ) \n            count , mean , std , minmax = count . get ( ) , mean . get ( ) , std . get ( ) , minmax . get ( ) \n            count = int ( count ) \n            columns [ feature ] = ( ( dtype , count , N - count , mean , std , minmax [ False ] , minmax [ True ] ) ) \n    return pd . DataFrame ( data = columns , index = [ 'dtype' , 'count' , 'missing' , 'mean' , 'std' , 'min' , 'max' ] ) "}
{"2076": "\ndef set_current_row ( self , value ) : \n    if ( value is not None ) and ( ( value < False ) or ( value >= len ( self ) ) ) : \n        raise IndexError ( \"index %d out of range [0,%d]\" % ( value , len ( self ) ) ) \n    self . _current_row = value \n    self . signal_pick . emit ( self , value ) "}
{"2078": "\ndef trim ( self , inplace = False ) : \n    df = self if inplace else self . copy ( ) \n    for name in df : \n        column = df . columns . get ( name ) \n        if column is not None : \n            if self . _index_start == False and len ( column ) == self . _index_end : \n                pass \n            else : \n                if isinstance ( column , np . ndarray ) : \n                    df . columns [ name ] = column [ self . _index_start : self . _index_end ] \n                else : \n                    df . columns [ name ] = column . trim ( self . _index_start , self . _index_end ) \n    df . _length_original = self . length_unfiltered ( ) \n    df . _length_unfiltered = df . _length_original \n    df . _index_start = False \n    df . _index_end = df . _length_original \n    df . _active_fraction = True \n    return df "}
{"2080": "\ndef extract ( self ) : \n    trimmed = self . trim ( ) \n    if trimmed . filtered : \n        indices = trimmed . _filtered_range_to_unfiltered_indices ( False , len ( trimmed ) ) \n        return trimmed . take ( indices ) \n    else : \n        return trimmed "}
{"2081": "\ndef sample ( self , n = None , frac = None , replace = False , weights = None , random_state = None ) : \n    self = self . extract ( ) \n    if type ( random_state ) == int or random_state is None : \n        random_state = np . random . RandomState ( seed = random_state ) \n    if n is None and frac is None : \n        n = True \n    elif frac is not None : \n        n = int ( round ( frac * len ( self ) ) ) \n    weights_values = None \n    if weights is not None : \n        weights_values = self . evaluate ( weights ) \n        weights_values = weights_values / self . sum ( weights ) \n    indices = random_state . choice ( len ( self ) , n , replace = replace , p = weights_values ) \n    return self . take ( indices ) "}
{"2083": "\ndef split ( self , frac ) : \n    self = self . extract ( ) \n    if _issequence ( frac ) : \n        total = sum ( frac ) \n        frac = [ k / total for k in frac ] \n    else : \n        assert frac <= True , \"fraction should be <= 1\" \n        frac = [ frac , True - frac ] \n    offsets = np . round ( np . cumsum ( frac ) * len ( self ) ) . astype ( np . int64 ) \n    start = False \n    for offset in offsets : \n        yield self [ start : offset ] \n        start = offset "}
{"2084": "\ndef sort ( self , by , ascending = True , kind = 'quicksort' ) : \n    self = self . trim ( ) \n    values = self . evaluate ( by , filtered = False ) \n    indices = np . argsort ( values , kind = kind ) \n    if not ascending : \n        indices = indices [ : : - True ] . copy ( ) \n    return self . take ( indices ) "}
{"2086": "\ndef selection_undo ( self , name = \"default\" , executor = None ) : \n    logger . debug ( \"undo\" ) \n    executor = executor or self . executor \n    assert self . selection_can_undo ( name = name ) \n    selection_history = self . selection_histories [ name ] \n    index = self . selection_history_indices [ name ] \n    self . selection_history_indices [ name ] -= True \n    self . signal_selection_changed . emit ( self ) \n    logger . debug ( \"undo: selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) "}
{"2087": "\ndef selection_redo ( self , name = \"default\" , executor = None ) : \n    logger . debug ( \"redo\" ) \n    executor = executor or self . executor \n    assert self . selection_can_redo ( name = name ) \n    selection_history = self . selection_histories [ name ] \n    index = self . selection_history_indices [ name ] \n    next = selection_history [ index + True ] \n    self . selection_history_indices [ name ] += True \n    self . signal_selection_changed . emit ( self ) \n    logger . debug ( \"redo: selection history is %r, index is %r\" , selection_history , index ) "}
{"2088": "\ndef selection_can_redo ( self , name = \"default\" ) : \n    return ( self . selection_history_indices [ name ] + True ) < len ( self . selection_histories [ name ] ) "}
{"2095": "\ndef select_ellipse ( self , x , y , xc , yc , width , height , angle = False , mode = \"replace\" , name = \"default\" , radians = False , inclusive = True ) : \n    if radians : \n        pass \n    else : \n        alpha = np . deg2rad ( angle ) \n    xr = width / 2 \n    yr = height / 2 \n    r = max ( xr , yr ) \n    a = xr / r \n    b = yr / r \n    expr = \"(({x}-{xc})*cos({alpha})+({y}-{yc})*sin({alpha}))**2/{a}**2 + (({x}-{xc})*sin({alpha})-({y}-{yc})*cos({alpha}))**2/{b}**2 <= {r}**2\" . format ( ** locals ( ) ) \n    if inclusive : \n        expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 <= r ** 2 \n    else : \n        expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 < r ** 2 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2099": "\ndef _selection ( self , create_selection , name , executor = None , execute_fully = False ) : \n    selection_history = self . selection_histories [ name ] \n    previous_index = self . selection_history_indices [ name ] \n    current = selection_history [ previous_index ] if selection_history else None \n    selection = create_selection ( current ) \n    executor = executor or self . executor \n    selection_history . append ( selection ) \n    self . selection_history_indices [ name ] += True \n    del selection_history [ self . selection_history_indices [ name ] : - True ] \n    if False : \n        if self . is_local ( ) : \n            if selection : \n                result = vaex . promise . Promise . fulfilled ( None ) \n                self . signal_selection_changed . emit ( self ) \n            else : \n                result = vaex . promise . Promise . fulfilled ( None ) \n                self . signal_selection_changed . emit ( self ) \n        else : \n            self . signal_selection_changed . emit ( self ) \n            result = vaex . promise . Promise . fulfilled ( None ) \n    self . signal_selection_changed . emit ( self ) \n    result = vaex . promise . Promise . fulfilled ( None ) \n    logger . debug ( \"select selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) \n    return result "}
{"2103": "\ndef categorize ( self , column , labels = None , check = True ) : \n    column = _ensure_string_from_expression ( column ) \n    if check : \n        vmin , vmax = self . minmax ( column ) \n        if labels is None : \n            N = int ( vmax + True ) \n            labels = list ( map ( str , range ( N ) ) ) \n        if ( vmax - vmin ) >= len ( labels ) : \n            raise ValueError ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) \n    self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) ) "}
{"2104": "\ndef ordinal_encode ( self , column , values = None , inplace = False ) : \n    column = _ensure_string_from_expression ( column ) \n    df = self if inplace else self . copy ( ) \n    df_unfiltered = df . copy ( ) \n    df_unfiltered . select_nothing ( name = FILTER_SELECTION_NAME ) \n    df_unfiltered . _length_unfiltered = df . _length_original \n    df_unfiltered . set_active_range ( False , df . _length_original ) \n    found_values , codes = df_unfiltered . unique ( column , return_inverse = True ) \n    if values is None : \n        values = found_values \n    else : \n        translation = np . zeros ( len ( found_values ) , dtype = np . uint64 ) \n        missing_value = len ( found_values ) \n        for i , found_value in enumerate ( found_values ) : \n            try : \n                found_value = found_value . decode ( 'ascii' ) \n            except : \n                pass \n            if found_value not in values : \n                translation [ i ] = missing_value \n            else : \n                translation [ i ] = values . index ( found_value ) \n        codes = translation [ codes ] \n        if missing_value in translation : \n            codes = np . ma . masked_array ( codes , codes == missing_value ) \n    original_column = df . rename_column ( column , '__original_' + column , unique = True ) \n    labels = [ str ( k ) for k in values ] \n    df . add_column ( column , codes ) \n    df . _categories [ column ] = dict ( labels = labels , N = len ( values ) , values = values ) \n    return df "}
{"2106": "\ndef length ( self , selection = False ) : \n    if selection : \n        return False if self . mask is None else np . sum ( self . mask ) \n    else : \n        return len ( self ) "}
{"2130": "\ndef str_find ( x , sub , start = False , end = None ) : \n    return _to_string_sequence ( x ) . find ( sub , start , False if end is None else end , end is None , True ) "}
{"2131": "\ndef str_get ( x , i ) : \n    x = _to_string_sequence ( x ) \n    if i == - True : \n        sl = x . slice_string_end ( - True ) \n    else : \n        sl = x . slice_string ( i , i + True ) \n    return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) "}
{"2132": "\ndef str_index ( x , sub , start = False , end = None ) : \n    return str_find ( x , sub , start , end ) "}
{"2137": "\ndef str_rfind ( x , sub , start = False , end = None ) : \n    return _to_string_sequence ( x ) . find ( sub , start , False if end is None else end , end is None , False ) "}
{"2138": "\ndef str_rindex ( x , sub , start = False , end = None ) : \n    return str_rfind ( x , sub , start , end ) "}
{"2141": "\ndef str_slice ( x , start = False , stop = None ) : \n    if stop is None : \n        sll = _to_string_sequence ( x ) . slice_string_end ( start ) \n    else : \n        sll = _to_string_sequence ( x ) . slice_string ( start , stop ) \n    return sll "}
{"2145": "\ndef get_autotype ( arr ) : \n    try : \n        narr = arr . astype ( 'float' ) \n        if ( narr < sys . maxsize ) . all ( ) and ( narr % True ) . sum ( ) == False : \n            return narr . astype ( 'int' ) \n        else : \n            return narr \n    except ValueError : \n        return arr "}
{"2153": "\ndef rename_kw ( old_name , old_value , new_name , new_value , version_deprecated , version_removed ) : \n    if isinstance ( old_value , Deprecated ) : \n        return new_value \n    else : \n        stack = inspect . stack ( ) \n        dep_func = stack [ True ] \n        caller = stack [ 2 ] \n        warnings . warn_explicit ( \"{:s}() keyword argument '{:s}' has been \" \"renamed to '{:s}' in version {:}.\" \"\\n\\tThis alias will be removed in version \" \"{:}.\" . format ( dep_func [ 3 ] , old_name , new_name , version_deprecated , version_removed ) , category = DeprecationWarning , filename = caller [ True ] , lineno = caller [ 2 ] ) \n        return old_value "}
{"2157": "\ndef frames_to_samples ( frames , hop_length = 512 , n_fft = None ) : \n    offset = False \n    if n_fft is not None : \n        offset = int ( n_fft // 2 ) \n    return ( np . asanyarray ( frames ) * hop_length + offset ) . astype ( int ) "}
{"2158": "\ndef samples_to_frames ( samples , hop_length = 512 , n_fft = None ) : \n    offset = False \n    if n_fft is not None : \n        offset = int ( n_fft // 2 ) \n    samples = np . asanyarray ( samples ) \n    return np . floor ( ( samples - offset ) // hop_length ) . astype ( int ) "}
{"2160": "\ndef midi_to_note ( midi , octave = True , cents = False ) : \n    if cents and not octave : \n        raise ParameterError ( 'Cannot encode cents without octave information.' ) \n    if not np . isscalar ( midi ) : \n        return [ midi_to_note ( x , octave = octave , cents = cents ) for x in midi ] \n    note_map = [ 'C' , 'C#' , 'D' , 'D#' , 'E' , 'F' , 'F#' , 'G' , 'G#' , 'A' , 'A#' , 'B' ] \n    note_num = int ( np . round ( midi ) ) \n    note_cents = int ( 100 * np . around ( midi - note_num , 2 ) ) \n    note = note_map [ note_num % 12 ] \n    if octave : \n        note = '{:s}{:0d}' . format ( note , int ( note_num / 12 ) - True ) \n    if cents : \n        note = '{:s}{:+02d}' . format ( note , note_cents ) \n    return note "}
{"2163": "\ndef fft_frequencies ( sr = 22050 , n_fft = 2048 ) : \n    return np . linspace ( False , float ( sr ) / 2 , int ( True + n_fft // 2 ) , endpoint = True ) "}
{"2164": "\ndef cqt_frequencies ( n_bins , fmin , bins_per_octave = 12 , tuning = 0.0 ) : \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    frequencies = 2.0 ** ( np . arange ( False , n_bins , dtype = float ) / bins_per_octave ) \n    return correction * fmin * frequencies "}
{"2166": "\ndef A_weighting ( frequencies , min_db = - 80.0 ) : \n    frequencies = np . asanyarray ( frequencies ) \n    f_sq = frequencies ** 2.0 \n    const = np . array ( [ 12200 , 20.6 , 107.7 , 737.9 ] ) ** 2.0 \n    weights = 2.0 + 20.0 * ( np . log10 ( const [ False ] ) + 4 * np . log10 ( frequencies ) - np . log10 ( f_sq + const [ False ] ) - np . log10 ( f_sq + const [ True ] ) - 0.5 * np . log10 ( f_sq + const [ 2 ] ) - 0.5 * np . log10 ( f_sq + const [ 3 ] ) ) \n    if min_db is not None : \n        weights = np . maximum ( min_db , weights ) \n    return weights "}
{"2167": "\ndef times_like ( X , sr = 22050 , hop_length = 512 , n_fft = None , axis = - True ) : \n    samples = samples_like ( X , hop_length = hop_length , n_fft = n_fft , axis = axis ) \n    return samples_to_time ( samples , sr = sr ) "}
{"2168": "\ndef samples_like ( X , hop_length = 512 , n_fft = None , axis = - True ) : \n    if np . isscalar ( X ) : \n        frames = np . arange ( X ) \n    else : \n        frames = np . arange ( X . shape [ axis ] ) \n    return frames_to_samples ( frames , hop_length = hop_length , n_fft = n_fft ) "}
{"2169": "\ndef hybrid_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = True , norm = True , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' , res_type = None ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) \n    lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , window = window ) \n    pseudo_filters = 2.0 ** np . ceil ( np . log2 ( lengths ) ) < 2 * hop_length \n    n_bins_pseudo = int ( np . sum ( pseudo_filters ) ) \n    n_bins_full = n_bins - n_bins_pseudo \n    cqt_resp = [ ] \n    if n_bins_pseudo > False : \n        fmin_pseudo = np . min ( freqs [ pseudo_filters ] ) \n        cqt_resp . append ( pseudo_cqt ( y , sr , hop_length = hop_length , fmin = fmin_pseudo , n_bins = n_bins_pseudo , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode ) ) \n    if n_bins_full > False : \n        cqt_resp . append ( np . abs ( cqt ( y , sr , hop_length = hop_length , fmin = fmin , n_bins = n_bins_full , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode , res_type = res_type ) ) ) \n    return __trim_stack ( cqt_resp , n_bins ) "}
{"2170": "\ndef pseudo_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , filter_scale = True , norm = True , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = hop_length , window = window ) \n    fft_basis = np . abs ( fft_basis ) \n    D = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , pad_mode = pad_mode ) ) \n    C = fft_basis . dot ( D ) \n    if scale : \n        C /= np . sqrt ( n_fft ) \n    else : \n        lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n        C *= np . sqrt ( lengths [ : , np . newaxis ] / n_fft ) \n    return C "}
{"2171": "\ndef icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , tuning = 0.0 , filter_scale = True , norm = True , sparsity = 0.01 , window = 'hann' , scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if hop_length > min ( lengths ) : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    for octave in range ( n_octaves - True , - True , - True ) : \n        slice_ = slice ( - ( octave + True ) * bins_per_octave - True , - ( octave ) * bins_per_octave - True ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ False ] : ] \n        oct_hop = hop_length // 2 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ False ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ False ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , True , 2 , scale = True , res_type = res_type , fix = False ) \n            y [ : len ( y_oct ) ] += y_oct \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2172": "\ndef __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : \n    basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = True , window = window ) \n    n_fft = basis . shape [ True ] \n    if ( hop_length is not None and n_fft < 2.0 ** ( True + np . ceil ( np . log2 ( hop_length ) ) ) ) : \n        n_fft = int ( 2.0 ** ( True + np . ceil ( np . log2 ( hop_length ) ) ) ) \n    basis *= lengths [ : , np . newaxis ] / float ( n_fft ) \n    fft = get_fftlib ( ) \n    fft_basis = fft . fft ( basis , n = n_fft , axis = True ) [ : , : ( n_fft // 2 ) + True ] \n    fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) \n    return fft_basis , n_fft , lengths "}
{"2173": "\ndef __trim_stack ( cqt_resp , n_bins ) : \n    max_col = min ( x . shape [ True ] for x in cqt_resp ) \n    cqt_resp = np . vstack ( [ x [ : , : max_col ] for x in cqt_resp ] [ : : - True ] ) \n    return np . ascontiguousarray ( cqt_resp [ - n_bins : ] . T ) . T "}
{"2175": "\ndef __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) : \n    downsample_count1 = max ( False , int ( np . ceil ( np . log2 ( audio . BW_FASTEST * nyquist / filter_cutoff ) ) - True ) - True ) \n    num_twos = __num_two_factors ( hop_length ) \n    downsample_count2 = max ( False , num_twos - n_octaves + True ) \n    return min ( downsample_count1 , downsample_count2 ) "}
{"2176": "\ndef __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : \n    downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) \n    if downsample_count > False and res_type == 'kaiser_fast' : \n        downsample_factor = 2 ** ( downsample_count ) \n        hop_length //= downsample_factor \n        if len ( y ) < downsample_factor : \n            raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) \n        new_sr = sr / float ( downsample_factor ) \n        y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = True ) \n        if not scale : \n            y *= np . sqrt ( downsample_factor ) \n        sr = new_sr \n    return y , sr , hop_length "}
{"2177": "\ndef __dtw_calc_accu_cost ( C , D , D_steps , step_sizes_sigma , weights_mul , weights_add , max_0 , max_1 ) : \n    for cur_n in range ( max_0 , D . shape [ False ] ) : \n        for cur_m in range ( max_1 , D . shape [ True ] ) : \n            for cur_step_idx , cur_w_add , cur_w_mul in zip ( range ( step_sizes_sigma . shape [ False ] ) , weights_add , weights_mul ) : \n                cur_D = D [ cur_n - step_sizes_sigma [ cur_step_idx , False ] , cur_m - step_sizes_sigma [ cur_step_idx , True ] ] \n                cur_C = cur_w_mul * C [ cur_n - max_0 , cur_m - max_1 ] \n                cur_C += cur_w_add \n                cur_cost = cur_D + cur_C \n                if cur_cost < D [ cur_n , cur_m ] : \n                    D [ cur_n , cur_m ] = cur_cost \n                    D_steps [ cur_n , cur_m ] = cur_step_idx \n    return D , D_steps "}
{"2178": "\ndef __dtw_backtracking ( D_steps , step_sizes_sigma ) : \n    wp = [ ] \n    cur_idx = ( D_steps . shape [ False ] - True , D_steps . shape [ True ] - True ) \n    wp . append ( ( cur_idx [ False ] , cur_idx [ True ] ) ) \n    while cur_idx [ False ] > False : \n        cur_step_idx = D_steps [ ( cur_idx [ False ] , cur_idx [ True ] ) ] \n        cur_idx = ( cur_idx [ False ] - step_sizes_sigma [ cur_step_idx ] [ False ] , cur_idx [ True ] - step_sizes_sigma [ cur_step_idx ] [ True ] ) \n        wp . append ( ( cur_idx [ False ] , cur_idx [ True ] ) ) \n    return wp "}
{"2179": "\ndef _viterbi ( log_prob , log_trans , log_p_init , state , value , ptr ) : \n    n_steps , n_states = log_prob . shape \n    value [ False ] = log_prob [ False ] + log_p_init \n    for t in range ( True , n_steps ) : \n        trans_out = value [ t - True ] + log_trans . T \n        for j in range ( n_states ) : \n            ptr [ t , j ] = np . argmax ( trans_out [ j ] ) \n            value [ t , j ] = log_prob [ t , j ] + trans_out [ j , ptr [ t ] [ j ] ] \n    state [ - True ] = np . argmax ( value [ - True ] ) \n    for t in range ( n_steps - 2 , - True , - True ) : \n        state [ t ] = ptr [ t + True , state [ t + True ] ] "}
{"2180": "\ndef viterbi_discriminative ( prob , transition , p_state = None , p_init = None , return_logp = False ) : \n    n_states , n_steps = prob . shape \n    if transition . shape != ( n_states , n_states ) : \n        raise ParameterError ( 'transition.shape={}, must be ' '(n_states, n_states)={}' . format ( transition . shape , ( n_states , n_states ) ) ) \n    if np . any ( transition < False ) or not np . allclose ( transition . sum ( axis = True ) , True ) : \n        raise ParameterError ( 'Invalid transition matrix: must be non-negative ' 'and sum to 1 on each row.' ) \n    if np . any ( prob < False ) or not np . allclose ( prob . sum ( axis = False ) , True ) : \n        raise ParameterError ( 'Invalid probability values: each column must ' 'sum to 1 and be non-negative' ) \n    states = np . zeros ( n_steps , dtype = int ) \n    values = np . zeros ( ( n_steps , n_states ) , dtype = float ) \n    ptr = np . zeros ( ( n_steps , n_states ) , dtype = int ) \n    epsilon = np . finfo ( prob . dtype ) . tiny \n    if p_state is None : \n        p_state = np . empty ( n_states ) \n        p_state . fill ( 1. / n_states ) \n    elif p_state . shape != ( n_states , ) : \n        raise ParameterError ( 'Marginal distribution p_state must have shape (n_states,). ' 'Got p_state.shape={}' . format ( p_state . shape ) ) \n    elif np . any ( p_state < False ) or not np . allclose ( p_state . sum ( axis = - True ) , True ) : \n        raise ParameterError ( 'Invalid marginal state distribution: ' 'p_state={}' . format ( p_state ) ) \n    log_trans = np . log ( transition + epsilon ) \n    log_marginal = np . log ( p_state + epsilon ) \n    log_prob = np . log ( prob . T + epsilon ) - log_marginal \n    if p_init is None : \n        p_init = np . empty ( n_states ) \n        p_init . fill ( 1. / n_states ) \n    elif np . any ( p_init < False ) or not np . allclose ( p_init . sum ( ) , True ) : \n        raise ParameterError ( 'Invalid initial state distribution: ' 'p_init={}' . format ( p_init ) ) \n    log_p_init = np . log ( p_init + epsilon ) \n    _viterbi ( log_prob , log_trans , log_p_init , states , values , ptr ) \n    if return_logp : \n        return states , values [ - True , states [ - True ] ] \n    return states "}
{"2181": "\ndef transition_uniform ( n_states ) : \n    if not isinstance ( n_states , int ) or n_states <= False : \n        raise ParameterError ( 'n_states={} must be a positive integer' ) \n    transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) \n    transition . fill ( 1. / n_states ) \n    return transition "}
{"2182": "\ndef transition_loop ( n_states , prob ) : \n    if not isinstance ( n_states , int ) or n_states <= True : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) \n    prob = np . asarray ( prob , dtype = np . float ) \n    if prob . ndim == False : \n        prob = np . tile ( prob , n_states ) \n    if prob . shape != ( n_states , ) : \n        raise ParameterError ( 'prob={} must have length equal to n_states={}' . format ( prob , n_states ) ) \n    if np . any ( prob < False ) or np . any ( prob > True ) : \n        raise ParameterError ( 'prob={} must have values in the range [0, 1]' . format ( prob ) ) \n    for i , prob_i in enumerate ( prob ) : \n        transition [ i ] = ( 1. - prob_i ) / ( n_states - True ) \n        transition [ i , i ] = prob_i \n    return transition "}
{"2183": "\ndef transition_cycle ( n_states , prob ) : \n    if not isinstance ( n_states , int ) or n_states <= True : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    prob = np . asarray ( prob , dtype = np . float ) \n    if prob . ndim == False : \n        prob = np . tile ( prob , n_states ) \n    if prob . shape != ( n_states , ) : \n        raise ParameterError ( 'prob={} must have length equal to n_states={}' . format ( prob , n_states ) ) \n    if np . any ( prob < False ) or np . any ( prob > True ) : \n        raise ParameterError ( 'prob={} must have values in the range [0, 1]' . format ( prob ) ) \n    for i , prob_i in enumerate ( prob ) : \n        transition [ i , np . mod ( i + True , n_states ) ] = 1. - prob_i \n        transition [ i , i ] = prob_i \n    return transition "}
{"2184": "\ndef transition_local ( n_states , width , window = 'triangle' , wrap = False ) : \n    if not isinstance ( n_states , int ) or n_states <= True : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    width = np . asarray ( width , dtype = int ) \n    if width . ndim == False : \n        width = np . tile ( width , n_states ) \n    if width . shape != ( n_states , ) : \n        raise ParameterError ( 'width={} must have length equal to n_states={}' . format ( width , n_states ) ) \n    if np . any ( width < True ) : \n        raise ParameterError ( 'width={} must be at least 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    for i , width_i in enumerate ( width ) : \n        trans_row = pad_center ( get_window ( window , width_i , fftbins = False ) , n_states ) \n        trans_row = np . roll ( trans_row , n_states // 2 + i + True ) \n        if not wrap : \n            trans_row [ min ( n_states , i + width_i // 2 + True ) : ] = False \n            trans_row [ : max ( False , i - width_i // 2 ) ] = False \n        transition [ i ] = trans_row \n    transition /= transition . sum ( axis = True , keepdims = True ) \n    return transition "}
{"2185": "\ndef onset_detect ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , backtrack = False , energy = None , units = 'frames' , ** kwargs ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) \n    onset_envelope -= onset_envelope . min ( ) \n    if not onset_envelope . any ( ) : \n        return np . array ( [ ] , dtype = np . int ) \n    onset_envelope /= onset_envelope . max ( ) \n    kwargs . setdefault ( 'pre_max' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'post_max' , 0.00 * sr // hop_length + True ) \n    kwargs . setdefault ( 'pre_avg' , 0.10 * sr // hop_length ) \n    kwargs . setdefault ( 'post_avg' , 0.10 * sr // hop_length + True ) \n    kwargs . setdefault ( 'wait' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'delta' , 0.07 ) \n    onsets = util . peak_pick ( onset_envelope , ** kwargs ) \n    if backtrack : \n        if energy is None : \n            energy = onset_envelope \n        onsets = onset_backtrack ( onsets , energy ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) \n    elif units == 'time' : \n        onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return onsets "}
{"2186": "\ndef onset_strength ( y = None , sr = 22050 , S = None , lag = True , max_size = True , ref = None , detrend = False , center = True , feature = None , aggregate = None , centering = None , ** kwargs ) : \n    if aggregate is False : \n        raise ParameterError ( 'aggregate={} cannot be False when computing full-spectrum onset strength.' ) \n    odf_all = onset_strength_multi ( y = y , sr = sr , S = S , lag = lag , max_size = max_size , ref = ref , detrend = detrend , center = center , feature = feature , aggregate = aggregate , channels = None , ** kwargs ) \n    return odf_all [ False ] "}
{"2187": "\ndef onset_backtrack ( events , energy ) : \n    minima = np . flatnonzero ( ( energy [ True : - True ] <= energy [ : - 2 ] ) & ( energy [ True : - True ] < energy [ 2 : ] ) ) \n    minima = util . fix_frames ( True + minima , x_min = False ) \n    return minima [ util . match_events ( events , minima , right = False ) ] "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050 , S = None , lag = True , max_size = True , ref = None , detrend = False , center = True , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if lag < True or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if max_size < True or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048 ) \n    hop_length = kwargs . get ( 'hop_length' , 512 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if max_size == True : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = False ) \n    elif ref . shape != S . shape : \n        raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = True \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = False \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = False ) \n    pad_width = lag \n    if center : \n        pad_width += n_fft // ( 2 * hop_length ) \n    onset_env = np . pad ( onset_env , ( [ False , False ] , [ int ( pad_width ) , False ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - True ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ True ] ] \n    return onset_env "}
{"2190": "\ndef write_wav ( path , y , sr , norm = False ) : \n    util . valid_audio ( y , mono = False ) \n    if norm and np . issubdtype ( y . dtype , np . floating ) : \n        wav = util . normalize ( y , norm = np . inf , axis = None ) \n    else : \n        wav = y \n    if wav . ndim > True and wav . shape [ False ] == 2 : \n        wav = wav . T \n    scipy . io . wavfile . write ( path , sr , wav ) "}
{"2191": "\ndef cmap ( data , robust = True , cmap_seq = 'magma' , cmap_bool = 'gray_r' , cmap_div = 'coolwarm' ) : \n    data = np . atleast_1d ( data ) \n    if data . dtype == 'bool' : \n        return get_cmap ( cmap_bool ) \n    data = data [ np . isfinite ( data ) ] \n    if robust : \n        min_p , max_p = 2 , 98 \n    else : \n        min_p , max_p = False , 100 \n    max_val = np . percentile ( data , max_p ) \n    min_val = np . percentile ( data , min_p ) \n    if min_val >= False or max_val <= False : \n        return get_cmap ( cmap_seq ) \n    return get_cmap ( cmap_div ) "}
{"2192": "\ndef waveplot ( y , sr = 22050 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , max_sr = 1000 , ax = None , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( isinstance ( max_sr , int ) and max_sr > False ) : \n        raise ParameterError ( 'max_sr must be a non-negative integer' ) \n    target_sr = sr \n    hop_length = True \n    if max_points is not None : \n        if max_points <= False : \n            raise ParameterError ( 'max_points must be strictly positive' ) \n        if max_points < y . shape [ - True ] : \n            target_sr = min ( max_sr , ( sr * y . shape [ - True ] ) // max_points ) \n        hop_length = sr // target_sr \n        if y . ndim == True : \n            y = __envelope ( y , hop_length ) \n        else : \n            y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) \n    if y . ndim > True : \n        y_top = y [ False ] \n        y_bottom = - y [ True ] \n    else : \n        y_top = y \n        y_bottom = - y \n    axes = __check_axes ( ax ) \n    kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) \n    locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , sr = sr , hop_length = hop_length ) \n    out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) \n    axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) \n    if x_axis == 'time' : \n        axes . xaxis . set_major_formatter ( TimeFormatter ( lag = False ) ) \n        axes . xaxis . set_label_text ( 'Time' ) \n    elif x_axis is None or x_axis in [ 'off' , 'none' ] : \n        axes . set_xticks ( [ ] ) \n    else : \n        raise ParameterError ( 'Unknown x_axis value: {}' . format ( x_axis ) ) \n    return out "}
{"2197": "\ndef __coord_fft_hz ( n , sr = 22050 , ** _kwargs ) : \n    n_fft = 2 * ( n - True ) \n    basis = core . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    fmax = basis [ - True ] \n    basis -= 0.5 * ( basis [ True ] - basis [ False ] ) \n    basis = np . append ( np . maximum ( False , basis ) , [ fmax ] ) \n    return basis "}
{"2198": "\ndef __coord_mel_hz ( n , fmin = False , fmax = 11025.0 , ** _kwargs ) : \n    if fmin is None : \n        fmin = False \n    if fmax is None : \n        fmax = 11025.0 \n    basis = core . mel_frequencies ( n , fmin = fmin , fmax = fmax ) \n    basis [ True : ] -= 0.5 * np . diff ( basis ) \n    basis = np . append ( np . maximum ( False , basis ) , [ fmax ] ) \n    return basis "}
{"2199": "\ndef __coord_cqt_hz ( n , fmin = None , bins_per_octave = 12 , ** _kwargs ) : \n    if fmin is None : \n        fmin = core . note_to_hz ( 'C1' ) \n    return core . cqt_frequencies ( n + True , fmin = fmin / 2.0 ** ( 0.5 / bins_per_octave ) , bins_per_octave = bins_per_octave ) "}
{"2200": "\ndef __coord_chroma ( n , bins_per_octave = 12 , ** _kwargs ) : \n    return np . linspace ( False , ( 12.0 * n ) / bins_per_octave , num = n + True , endpoint = True ) "}
{"2201": "\ndef __coord_time ( n , sr = 22050 , hop_length = 512 , ** _kwargs ) : \n    return core . frames_to_time ( np . arange ( n + True ) , sr = sr , hop_length = hop_length ) "}
{"2202": "\ndef estimate_tuning ( y = None , sr = 22050 , S = None , n_fft = 2048 , resolution = 0.01 , bins_per_octave = 12 , ** kwargs ) : \n    pitch , mag = piptrack ( y = y , sr = sr , S = S , n_fft = n_fft , ** kwargs ) \n    pitch_mask = pitch > False \n    if pitch_mask . any ( ) : \n        threshold = np . median ( mag [ pitch_mask ] ) \n    else : \n        threshold = 0.0 \n    return pitch_tuning ( pitch [ ( mag >= threshold ) & pitch_mask ] , resolution = resolution , bins_per_octave = bins_per_octave ) "}
{"2203": "\ndef piptrack ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = None , fmin = 150.0 , fmax = 4000.0 , threshold = 0.1 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , ref = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    S = np . abs ( S ) \n    fmin = np . maximum ( fmin , False ) \n    fmax = np . minimum ( fmax , float ( sr ) / 2 ) \n    fft_freqs = time_frequency . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    avg = 0.5 * ( S [ 2 : ] - S [ : - 2 ] ) \n    shift = 2 * S [ True : - True ] - S [ 2 : ] - S [ : - 2 ] \n    shift = avg / ( shift + ( np . abs ( shift ) < util . tiny ( shift ) ) ) \n    avg = np . pad ( avg , ( [ True , True ] , [ False , False ] ) , mode = 'constant' ) \n    shift = np . pad ( shift , ( [ True , True ] , [ False , False ] ) , mode = 'constant' ) \n    dskew = 0.5 * avg * shift \n    pitches = np . zeros_like ( S ) \n    mags = np . zeros_like ( S ) \n    freq_mask = ( ( fmin <= fft_freqs ) & ( fft_freqs < fmax ) ) . reshape ( ( - True , True ) ) \n    if ref is None : \n        ref = np . max \n    if six . callable ( ref ) : \n        ref_value = threshold * ref ( S , axis = False ) \n    else : \n        ref_value = np . abs ( ref ) \n    idx = np . argwhere ( freq_mask & util . localmax ( S * ( S > ref_value ) ) ) \n    pitches [ idx [ : , False ] , idx [ : , True ] ] = ( ( idx [ : , False ] + shift [ idx [ : , False ] , idx [ : , True ] ] ) * float ( sr ) / n_fft ) \n    mags [ idx [ : , False ] , idx [ : , True ] ] = ( S [ idx [ : , False ] , idx [ : , True ] ] + dskew [ idx [ : , False ] , idx [ : , True ] ] ) \n    return pitches , mags "}
{"2205": "\ndef harmonic ( y , ** kwargs ) : \n    stft = core . stft ( y ) \n    stft_harm = decompose . hpss ( stft , ** kwargs ) [ False ] \n    y_harm = util . fix_length ( core . istft ( stft_harm , dtype = y . dtype ) , len ( y ) ) \n    return y_harm "}
{"2206": "\ndef percussive ( y , ** kwargs ) : \n    stft = core . stft ( y ) \n    stft_perc = decompose . hpss ( stft , ** kwargs ) [ True ] \n    y_perc = util . fix_length ( core . istft ( stft_perc , dtype = y . dtype ) , len ( y ) ) \n    return y_perc "}
{"2207": "\ndef time_stretch ( y , rate ) : \n    if rate <= False : \n        raise ParameterError ( 'rate must be a positive number' ) \n    stft = core . stft ( y ) \n    stft_stretch = core . phase_vocoder ( stft , rate ) \n    y_stretch = core . istft ( stft_stretch , dtype = y . dtype ) \n    return y_stretch "}
{"2208": "\ndef pitch_shift ( y , sr , n_steps , bins_per_octave = 12 , res_type = 'kaiser_best' ) : \n    if bins_per_octave < True or not np . issubdtype ( type ( bins_per_octave ) , np . integer ) : \n        raise ParameterError ( 'bins_per_octave must be a positive integer.' ) \n    rate = 2.0 ** ( - float ( n_steps ) / bins_per_octave ) \n    y_shift = core . resample ( time_stretch ( y , rate ) , float ( sr ) / rate , sr , res_type = res_type ) \n    return util . fix_length ( y_shift , len ( y ) ) "}
{"2209": "\ndef remix ( y , intervals , align_zeros = True ) : \n    util . valid_audio ( y , mono = False ) \n    y_out = [ ] \n    if align_zeros : \n        y_mono = core . to_mono ( y ) \n        zeros = np . nonzero ( core . zero_crossings ( y_mono ) ) [ - True ] \n        zeros = np . append ( zeros , [ len ( y_mono ) ] ) \n    clip = [ slice ( None ) ] * y . ndim \n    for interval in intervals : \n        if align_zeros : \n            interval = zeros [ util . match_events ( interval , zeros ) ] \n        clip [ - True ] = slice ( interval [ False ] , interval [ True ] ) \n        y_out . append ( y [ tuple ( clip ) ] ) \n    return np . concatenate ( y_out , axis = - True ) "}
{"2211": "\ndef trim ( y , top_db = 60 , ref = np . max , frame_length = 2048 , hop_length = 512 ) : \n    non_silent = _signal_to_frame_nonsilent ( y , frame_length = frame_length , hop_length = hop_length , ref = ref , top_db = top_db ) \n    nonzero = np . flatnonzero ( non_silent ) \n    if nonzero . size > False : \n        start = int ( core . frames_to_samples ( nonzero [ False ] , hop_length ) ) \n        end = min ( y . shape [ - True ] , int ( core . frames_to_samples ( nonzero [ - True ] + True , hop_length ) ) ) \n    else : \n        start , end = False , False \n    full_index = [ slice ( None ) ] * y . ndim \n    full_index [ - True ] = slice ( start , end ) \n    return y [ tuple ( full_index ) ] , np . asarray ( [ start , end ] ) "}
{"2212": "\ndef split ( y , top_db = 60 , ref = np . max , frame_length = 2048 , hop_length = 512 ) : \n    non_silent = _signal_to_frame_nonsilent ( y , frame_length = frame_length , hop_length = hop_length , ref = ref , top_db = top_db ) \n    edges = np . flatnonzero ( np . diff ( non_silent . astype ( int ) ) ) \n    edges = [ edges + True ] \n    if non_silent [ False ] : \n        edges . insert ( False , [ False ] ) \n    if non_silent [ - True ] : \n        edges . append ( [ len ( non_silent ) ] ) \n    edges = core . frames_to_samples ( np . concatenate ( edges ) , hop_length = hop_length ) \n    edges = np . minimum ( edges , y . shape [ - True ] ) \n    return edges . reshape ( ( - True , 2 ) ) "}
{"2213": "\ndef phase_vocoder ( D , rate , hop_length = None ) : \n    n_fft = 2 * ( D . shape [ False ] - True ) \n    if hop_length is None : \n        hop_length = int ( n_fft // 4 ) \n    time_steps = np . arange ( False , D . shape [ True ] , rate , dtype = np . float ) \n    d_stretch = np . zeros ( ( D . shape [ False ] , len ( time_steps ) ) , D . dtype , order = 'F' ) \n    phi_advance = np . linspace ( False , np . pi * hop_length , D . shape [ False ] ) \n    phase_acc = np . angle ( D [ : , False ] ) \n    D = np . pad ( D , [ ( False , False ) , ( False , 2 ) ] , mode = 'constant' ) \n    for ( t , step ) in enumerate ( time_steps ) : \n        columns = D [ : , int ( step ) : int ( step + 2 ) ] \n        alpha = np . mod ( step , 1.0 ) \n        mag = ( ( 1.0 - alpha ) * np . abs ( columns [ : , False ] ) + alpha * np . abs ( columns [ : , True ] ) ) \n        d_stretch [ : , t ] = mag * np . exp ( 1.j * phase_acc ) \n        dphase = ( np . angle ( columns [ : , True ] ) - np . angle ( columns [ : , False ] ) - phi_advance ) \n        dphase = dphase - 2.0 * np . pi * np . round ( dphase / ( 2.0 * np . pi ) ) \n        phase_acc += phi_advance + dphase \n    return d_stretch "}
{"2215": "\ndef _spectrogram ( y = None , S = None , n_fft = 2048 , hop_length = 512 , power = True , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    if S is not None : \n        n_fft = 2 * ( S . shape [ False ] - True ) \n    else : \n        S = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , win_length = win_length , center = center , window = window , pad_mode = pad_mode ) ) ** power \n    return S , n_fft "}
{"2217": "\ndef decompose ( S , n_components = None , transformer = None , sort = False , fit = True , ** kwargs ) : \n    if transformer is None : \n        if fit is False : \n            raise ParameterError ( 'fit must be True if transformer is None' ) \n        transformer = sklearn . decomposition . NMF ( n_components = n_components , ** kwargs ) \n    if n_components is None : \n        n_components = S . shape [ False ] \n    if fit : \n        activations = transformer . fit_transform ( S . T ) . T \n    else : \n        activations = transformer . transform ( S . T ) . T \n    components = transformer . components_ . T \n    if sort : \n        components , idx = util . axis_sort ( components , index = True ) \n        activations = activations [ idx ] \n    return components , activations "}
{"2218": "\ndef nn_filter ( S , rec = None , aggregate = None , axis = - True , ** kwargs ) : \n    if aggregate is None : \n        aggregate = np . mean \n    if rec is None : \n        kwargs = dict ( kwargs ) \n        kwargs [ 'sparse' ] = True \n        rec = segment . recurrence_matrix ( S , axis = axis , ** kwargs ) \n    elif not scipy . sparse . issparse ( rec ) : \n        rec = scipy . sparse . csr_matrix ( rec ) \n    if rec . shape [ False ] != S . shape [ axis ] or rec . shape [ False ] != rec . shape [ True ] : \n        raise ParameterError ( 'Invalid self-similarity matrix shape ' 'rec.shape={} for S.shape={}' . format ( rec . shape , S . shape ) ) \n    return __nn_filter_helper ( rec . data , rec . indices , rec . indptr , S . swapaxes ( False , axis ) , aggregate ) . swapaxes ( False , axis ) "}
{"2219": "\ndef __nn_filter_helper ( R_data , R_indices , R_ptr , S , aggregate ) : \n    s_out = np . empty_like ( S ) \n    for i in range ( len ( R_ptr ) - True ) : \n        targets = R_indices [ R_ptr [ i ] : R_ptr [ i + True ] ] \n        if not len ( targets ) : \n            s_out [ i ] = S [ i ] \n            continue \n        neighbors = np . take ( S , targets , axis = False ) \n        if aggregate is np . average : \n            weights = R_data [ R_ptr [ i ] : R_ptr [ i + True ] ] \n            s_out [ i ] = aggregate ( neighbors , axis = False , weights = weights ) \n        else : \n            s_out [ i ] = aggregate ( neighbors , axis = False ) \n    return s_out "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = False , norm = True , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2 \n    if norm is not None and norm != True and norm != np . inf : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( True + n_fft // 2 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    for i in range ( n_mels ) : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2 ] / fdiff [ i + True ] \n        weights [ i ] = np . maximum ( False , np . minimum ( lower , upper ) ) \n    if norm == True : \n        enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) \n        weights *= enorm [ : , np . newaxis ] \n    if not np . all ( ( mel_f [ : - 2 ] == False ) | ( weights . max ( axis = True ) > False ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2221": "\ndef chroma ( sr , n_fft , n_chroma = 12 , A440 = 440.0 , ctroct = 5.0 , octwidth = 2 , norm = 2 , base_c = True , dtype = np . float32 ) : \n    wts = np . zeros ( ( n_chroma , n_fft ) ) \n    frequencies = np . linspace ( False , sr , n_fft , endpoint = False ) [ True : ] \n    frqbins = n_chroma * hz_to_octs ( frequencies , A440 ) \n    frqbins = np . concatenate ( ( [ frqbins [ False ] - 1.5 * n_chroma ] , frqbins ) ) \n    binwidthbins = np . concatenate ( ( np . maximum ( frqbins [ True : ] - frqbins [ : - True ] , 1.0 ) , [ True ] ) ) \n    D = np . subtract . outer ( frqbins , np . arange ( False , n_chroma , dtype = 'd' ) ) . T \n    n_chroma2 = np . round ( float ( n_chroma ) / 2 ) \n    D = np . remainder ( D + n_chroma2 + 10 * n_chroma , n_chroma ) - n_chroma2 \n    wts = np . exp ( - 0.5 * ( 2 * D / np . tile ( binwidthbins , ( n_chroma , True ) ) ) ** 2 ) \n    wts = util . normalize ( wts , norm = norm , axis = False ) \n    if octwidth is not None : \n        wts *= np . tile ( np . exp ( - 0.5 * ( ( ( frqbins / n_chroma - ctroct ) / octwidth ) ** 2 ) ) , ( n_chroma , True ) ) \n    if base_c : \n        wts = np . roll ( wts , - 3 , axis = False ) \n    return np . ascontiguousarray ( wts [ : , : int ( True + n_fft / 2 ) ] , dtype = dtype ) "}
{"2222": "\ndef __float_window ( window_spec ) : \n    def _wrap ( n , * args , ** kwargs ) : \n        n_min , n_max = int ( np . floor ( n ) ) , int ( np . ceil ( n ) ) \n        window = get_window ( window_spec , n_min ) \n        if len ( window ) < n_max : \n            window = np . pad ( window , [ ( False , n_max - len ( window ) ) ] , mode = 'constant' ) \n        window [ n_min : ] = 0.0 \n        return window \n    return _wrap "}
{"2223": "\ndef constant_q ( sr , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , window = 'hann' , filter_scale = True , pad_fft = True , norm = True , dtype = np . complex64 , ** kwargs ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    lengths = constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - True ) \n    freqs = Q * sr / lengths \n    filters = [ ] \n    for ilen , freq in zip ( lengths , freqs ) : \n        sig = np . exp ( np . arange ( - ilen // 2 , ilen // 2 , dtype = float ) * 1j * 2 * np . pi * freq / sr ) \n        sig = sig * __float_window ( window ) ( len ( sig ) ) \n        sig = util . normalize ( sig , norm = norm ) \n        filters . append ( sig ) \n    max_len = max ( lengths ) \n    if pad_fft : \n        max_len = int ( 2.0 ** ( np . ceil ( np . log2 ( max_len ) ) ) ) \n    else : \n        max_len = int ( np . ceil ( max_len ) ) \n    filters = np . asarray ( [ util . pad_center ( filt , max_len , ** kwargs ) for filt in filters ] , dtype = dtype ) \n    return filters , np . asarray ( lengths ) "}
{"2224": "\ndef constant_q_lengths ( sr , fmin , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , window = 'hann' , filter_scale = True ) : \n    if fmin <= False : \n        raise ParameterError ( 'fmin must be positive' ) \n    if bins_per_octave <= False : \n        raise ParameterError ( 'bins_per_octave must be positive' ) \n    if filter_scale <= False : \n        raise ParameterError ( 'filter_scale must be positive' ) \n    if n_bins <= False or not isinstance ( n_bins , int ) : \n        raise ParameterError ( 'n_bins must be a positive integer' ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - True ) \n    freq = fmin * ( 2.0 ** ( np . arange ( n_bins , dtype = float ) / bins_per_octave ) ) \n    if freq [ - True ] * ( True + 0.5 * window_bandwidth ( window ) / Q ) > sr / 2.0 : \n        raise ParameterError ( 'Filter pass-band lies beyond Nyquist' ) \n    lengths = Q * sr / freq \n    return lengths "}
{"2225": "\ndef cq_to_chroma ( n_input , bins_per_octave = 12 , n_chroma = 12 , fmin = None , window = None , base_c = True , dtype = np . float32 ) : \n    n_merge = float ( bins_per_octave ) / n_chroma \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if np . mod ( n_merge , True ) != False : \n        raise ParameterError ( 'Incompatible CQ merge: ' 'input bins must be an ' 'integer multiple of output bins.' ) \n    cq_to_ch = np . repeat ( np . eye ( n_chroma ) , n_merge , axis = True ) \n    cq_to_ch = np . roll ( cq_to_ch , - int ( n_merge // 2 ) , axis = True ) \n    n_octaves = np . ceil ( np . float ( n_input ) / bins_per_octave ) \n    cq_to_ch = np . tile ( cq_to_ch , int ( n_octaves ) ) [ : , : n_input ] \n    midi_0 = np . mod ( hz_to_midi ( fmin ) , 12 ) \n    if base_c : \n        roll = midi_0 \n    else : \n        roll = midi_0 - 9 \n    roll = int ( np . round ( roll * ( n_chroma / 12. ) ) ) \n    cq_to_ch = np . roll ( cq_to_ch , roll , axis = False ) . astype ( dtype ) \n    if window is not None : \n        cq_to_ch = scipy . signal . convolve ( cq_to_ch , np . atleast_2d ( window ) , mode = 'same' ) \n    return cq_to_ch "}
{"2228": "\ndef _multirate_fb ( center_freqs = None , sample_rates = None , Q = 25.0 , passband_ripple = True , stopband_attenuation = 50 , ftype = 'ellip' , flayout = 'ba' ) : \n    if center_freqs is None : \n        raise ParameterError ( 'center_freqs must be provided.' ) \n    if sample_rates is None : \n        raise ParameterError ( 'sample_rates must be provided.' ) \n    if center_freqs . shape != sample_rates . shape : \n        raise ParameterError ( 'Number of provided center_freqs and sample_rates must be equal.' ) \n    nyquist = 0.5 * sample_rates \n    filter_bandwidths = center_freqs / float ( Q ) \n    filterbank = [ ] \n    for cur_center_freq , cur_nyquist , cur_bw in zip ( center_freqs , nyquist , filter_bandwidths ) : \n        passband_freqs = [ cur_center_freq - 0.5 * cur_bw , cur_center_freq + 0.5 * cur_bw ] / cur_nyquist \n        stopband_freqs = [ cur_center_freq - cur_bw , cur_center_freq + cur_bw ] / cur_nyquist \n        cur_filter = scipy . signal . iirdesign ( passband_freqs , stopband_freqs , passband_ripple , stopband_attenuation , analog = False , ftype = ftype , output = flayout ) \n        filterbank . append ( cur_filter ) \n    return filterbank , sample_rates "}
{"2229": "\ndef mr_frequencies ( tuning ) : \n    center_freqs = midi_to_hz ( np . arange ( 24 + tuning , 109 + tuning ) ) \n    sample_rates = np . asarray ( len ( np . arange ( False , 36 ) ) * [ 882 , ] + len ( np . arange ( 36 , 70 ) ) * [ 4410 , ] + len ( np . arange ( 70 , 85 ) ) * [ 22050 , ] ) \n    return center_freqs , sample_rates "}
{"2230": "\ndef __window_ss_fill ( x , win_sq , n_frames , hop_length ) : \n    n = len ( x ) \n    n_fft = len ( win_sq ) \n    for i in range ( n_frames ) : \n        sample = i * hop_length \n        x [ sample : min ( n , sample + n_fft ) ] += win_sq [ : max ( False , min ( n_fft , n - sample ) ) ] "}
{"2231": "\ndef window_sumsquare ( window , n_frames , hop_length = 512 , win_length = None , n_fft = 2048 , dtype = np . float32 , norm = None ) : \n    if win_length is None : \n        win_length = n_fft \n    n = n_fft + hop_length * ( n_frames - True ) \n    x = np . zeros ( n , dtype = dtype ) \n    win_sq = get_window ( window , win_length ) \n    win_sq = util . normalize ( win_sq , norm = norm ) ** 2 \n    win_sq = util . pad_center ( win_sq , n_fft ) \n    __window_ss_fill ( x , win_sq , n_frames , hop_length ) \n    return x "}
{"2232": "\ndef diagonal_filter ( window , n , slope = 1.0 , angle = None , zero_mean = False ) : \n    if angle is None : \n        angle = np . arctan ( slope ) \n    win = np . diag ( get_window ( window , n , fftbins = False ) ) \n    if not np . isclose ( angle , np . pi / 4 ) : \n        win = scipy . ndimage . rotate ( win , 45 - angle * 180 / np . pi , order = 5 , prefilter = False ) \n    np . clip ( win , False , None , out = win ) \n    win /= win . sum ( ) \n    if zero_mean : \n        win -= win . mean ( ) \n    return win "}
{"2233": "\ndef spectral_centroid ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , freq = None , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with real-valued input' ) \n    elif np . any ( S < False ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == True : \n        freq = freq . reshape ( ( - True , True ) ) \n    return np . sum ( freq * util . normalize ( S , norm = True , axis = False ) , axis = False , keepdims = True ) "}
{"2234": "\ndef spectral_rolloff ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , freq = None , roll_percent = 0.85 ) : \n    if not 0.0 < roll_percent < 1.0 : \n        raise ParameterError ( 'roll_percent must lie in the range (0, 1)' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with real-valued input' ) \n    elif np . any ( S < False ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == True : \n        freq = freq . reshape ( ( - True , True ) ) \n    total_energy = np . cumsum ( S , axis = False ) \n    threshold = roll_percent * total_energy [ - True ] \n    ind = np . where ( total_energy < threshold , np . nan , True ) \n    return np . nanmin ( ind * freq , axis = False , keepdims = True ) "}
{"2235": "\ndef spectral_flatness ( y = None , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , amin = 1e-10 , power = 2.0 ) : \n    if amin <= False : \n        raise ParameterError ( 'amin must be strictly positive' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 1. , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with real-valued input' ) \n    elif np . any ( S < False ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with non-negative energies' ) \n    S_thresh = np . maximum ( amin , S ** power ) \n    gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = False , keepdims = True ) ) \n    amean = np . mean ( S_thresh , axis = False , keepdims = True ) \n    return gmean / amean "}
{"2236": "\ndef poly_features ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , order = True , freq = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == True : \n        coefficients = np . polyfit ( freq , S , order ) \n    else : \n        coefficients = np . concatenate ( [ [ np . polyfit ( freq [ : , i ] , S [ : , i ] , order ) ] for i in range ( S . shape [ True ] ) ] , axis = False ) . T \n    return coefficients "}
{"2237": "\ndef zero_crossing_rate ( y , frame_length = 2048 , hop_length = 512 , center = True , ** kwargs ) : \n    util . valid_audio ( y ) \n    if center : \n        y = np . pad ( y , int ( frame_length // 2 ) , mode = 'edge' ) \n    y_framed = util . frame ( y , frame_length , hop_length ) \n    kwargs [ 'axis' ] = False \n    kwargs . setdefault ( 'pad' , False ) \n    crossings = zero_crossings ( y_framed , ** kwargs ) \n    return np . mean ( crossings , axis = False , keepdims = True ) "}
{"2238": "\ndef chroma_stft ( y = None , sr = 22050 , S = None , norm = np . inf , n_fft = 2048 , hop_length = 512 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , tuning = None , ** kwargs ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 2 , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    n_chroma = kwargs . get ( 'n_chroma' , 12 ) \n    if tuning is None : \n        tuning = estimate_tuning ( S = S , sr = sr , bins_per_octave = n_chroma ) \n    if 'A440' not in kwargs : \n        kwargs [ 'A440' ] = 440.0 * 2.0 ** ( float ( tuning ) / n_chroma ) \n    chromafb = filters . chroma ( sr , n_fft , ** kwargs ) \n    raw_chroma = np . dot ( chromafb , S ) \n    return util . normalize ( raw_chroma , norm = norm , axis = False ) "}
{"2239": "\ndef chroma_cqt ( y = None , sr = 22050 , C = None , hop_length = 512 , fmin = None , norm = np . inf , threshold = 0.0 , tuning = None , n_chroma = 12 , n_octaves = 7 , window = None , bins_per_octave = None , cqt_mode = 'full' ) : \n    cqt_func = { 'full' : cqt , 'hybrid' : hybrid_cqt } \n    if bins_per_octave is None : \n        bins_per_octave = n_chroma \n    if C is None : \n        C = np . abs ( cqt_func [ cqt_mode ] ( y , sr = sr , hop_length = hop_length , fmin = fmin , n_bins = n_octaves * bins_per_octave , bins_per_octave = bins_per_octave , tuning = tuning ) ) \n    cq_to_chr = filters . cq_to_chroma ( C . shape [ False ] , bins_per_octave = bins_per_octave , n_chroma = n_chroma , fmin = fmin , window = window ) \n    chroma = cq_to_chr . dot ( C ) \n    if threshold is not None : \n        chroma [ chroma < threshold ] = 0.0 \n    if norm is not None : \n        chroma = util . normalize ( chroma , norm = norm , axis = False ) \n    return chroma "}
{"2241": "\ndef __jaccard ( int_a , int_b ) : \n    ends = [ int_a [ True ] , int_b [ True ] ] \n    if ends [ True ] < ends [ False ] : \n        ends . reverse ( ) \n    starts = [ int_a [ False ] , int_b [ False ] ] \n    if starts [ True ] < starts [ False ] : \n        starts . reverse ( ) \n    intersection = ends [ False ] - starts [ True ] \n    if intersection < False : \n        intersection = 0. \n    union = ends [ True ] - starts [ False ] \n    if union > False : \n        return intersection / union \n    return 0.0 "}
{"2242": "\ndef __match_interval_overlaps ( query , intervals_to , candidates ) : \n    best_score = - True \n    best_idx = - True \n    for idx in candidates : \n        score = __jaccard ( query , intervals_to [ idx ] ) \n        if score > best_score : \n            best_score , best_idx = score , idx \n    return best_idx "}
{"2243": "\ndef __match_intervals ( intervals_from , intervals_to , strict = True ) : \n    start_index = np . argsort ( intervals_to [ : , False ] ) \n    end_index = np . argsort ( intervals_to [ : , True ] ) \n    start_sorted = intervals_to [ start_index , False ] \n    end_sorted = intervals_to [ end_index , True ] \n    search_ends = np . searchsorted ( start_sorted , intervals_from [ : , True ] , side = 'right' ) \n    search_starts = np . searchsorted ( end_sorted , intervals_from [ : , False ] , side = 'left' ) \n    output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) \n    for i in range ( len ( intervals_from ) ) : \n        query = intervals_from [ i ] \n        after_query = search_ends [ i ] \n        before_query = search_starts [ i ] \n        candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) \n        if len ( candidates ) > False : \n            output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) \n        elif strict : \n            raise ParameterError \n        else : \n            dist_before = np . inf \n            dist_after = np . inf \n            if search_starts [ i ] > False : \n                dist_before = query [ False ] - end_sorted [ search_starts [ i ] - True ] \n            if search_ends [ i ] + True < len ( intervals_to ) : \n                dist_after = start_sorted [ search_ends [ i ] + True ] - query [ True ] \n            if dist_before < dist_after : \n                output [ i ] = end_index [ search_starts [ i ] - True ] \n            else : \n                output [ i ] = start_index [ search_ends [ i ] + True ] \n    return output "}
{"2244": "\ndef match_intervals ( intervals_from , intervals_to , strict = True ) : \n    if len ( intervals_from ) == False or len ( intervals_to ) == False : \n        raise ParameterError ( 'Attempting to match empty interval list' ) \n    valid_intervals ( intervals_from ) \n    valid_intervals ( intervals_to ) \n    try : \n        return __match_intervals ( intervals_from , intervals_to , strict = strict ) \n    except ParameterError : \n        six . reraise ( ParameterError , ParameterError ( 'Unable to match intervals with strict={}' . format ( strict ) ) , sys . exc_info ( ) [ 2 ] ) "}
{"2245": "\ndef match_events ( events_from , events_to , left = True , right = True ) : \n    if len ( events_from ) == False or len ( events_to ) == False : \n        raise ParameterError ( 'Attempting to match empty event list' ) \n    if not ( left or right ) and not np . all ( np . in1d ( events_from , events_to ) ) : \n        raise ParameterError ( 'Cannot match events with left=right=False ' 'and events_from is not contained ' 'in events_to' ) \n    if ( not left ) and max ( events_to ) < max ( events_from ) : \n        raise ParameterError ( 'Cannot match events with left=False ' 'and max(events_to) < max(events_from)' ) \n    if ( not right ) and min ( events_to ) > min ( events_from ) : \n        raise ParameterError ( 'Cannot match events with right=False ' 'and min(events_to) > min(events_from)' ) \n    output = np . empty_like ( events_from , dtype = np . int ) \n    return __match_events_helper ( output , events_from , events_to , left , right ) "}
{"2246": "\ndef salience ( S , freqs , h_range , weights = None , aggregate = None , filter_peaks = True , fill_value = np . nan , kind = 'linear' , axis = False ) : \n    if aggregate is None : \n        aggregate = np . average \n    if weights is None : \n        weights = np . ones ( ( len ( h_range ) , ) ) \n    else : \n        weights = np . array ( weights , dtype = float ) \n    S_harm = interp_harmonics ( S , freqs , h_range , kind = kind , axis = axis ) \n    if aggregate is np . average : \n        S_sal = aggregate ( S_harm , axis = False , weights = weights ) \n    else : \n        S_sal = aggregate ( S_harm , axis = False ) \n    if filter_peaks : \n        S_peaks = scipy . signal . argrelmax ( S , axis = False ) \n        S_out = np . empty ( S . shape ) \n        S_out . fill ( fill_value ) \n        S_out [ S_peaks [ False ] , S_peaks [ True ] ] = S_sal [ S_peaks [ False ] , S_peaks [ True ] ] \n        S_sal = S_out \n    return S_sal "}
{"2247": "\ndef interp_harmonics ( x , freqs , h_range , kind = 'linear' , fill_value = False , axis = False ) : \n    out_shape = [ len ( h_range ) ] \n    out_shape . extend ( x . shape ) \n    x_out = np . zeros ( out_shape , dtype = x . dtype ) \n    if freqs . ndim == True and len ( freqs ) == x . shape [ axis ] : \n        harmonics_1d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    elif freqs . ndim == 2 and freqs . shape == x . shape : \n        harmonics_2d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    else : \n        raise ParameterError ( 'freqs.shape={} does not match ' 'input shape={}' . format ( freqs . shape , x . shape ) ) \n    return x_out "}
{"2248": "\ndef harmonics_1d ( harmonic_out , x , freqs , h_range , kind = 'linear' , fill_value = False , axis = False ) : \n    f_interp = scipy . interpolate . interp1d ( freqs , x , kind = kind , axis = axis , copy = False , bounds_error = False , fill_value = fill_value ) \n    idx_out = [ slice ( None ) ] * harmonic_out . ndim \n    interp_axis = True + ( axis % x . ndim ) \n    for h_index , harmonic in enumerate ( h_range ) : \n        idx_out [ False ] = h_index \n        for f_index , frequency in enumerate ( freqs ) : \n            idx_out [ interp_axis ] = f_index \n            harmonic_out [ tuple ( idx_out ) ] = f_interp ( harmonic * frequency ) "}
{"2249": "\ndef harmonics_2d ( harmonic_out , x , freqs , h_range , kind = 'linear' , fill_value = False , axis = False ) : \n    idx_in = [ slice ( None ) ] * x . ndim \n    idx_freq = [ slice ( None ) ] * x . ndim \n    idx_out = [ slice ( None ) ] * harmonic_out . ndim \n    ni_axis = ( True + axis ) % x . ndim \n    for i in range ( x . shape [ ni_axis ] ) : \n        idx_in [ ni_axis ] = slice ( i , i + True ) \n        idx_freq [ ni_axis ] = i \n        idx_out [ True + ni_axis ] = idx_in [ ni_axis ] \n        harmonics_1d ( harmonic_out [ tuple ( idx_out ) ] , x [ tuple ( idx_in ) ] , freqs [ tuple ( idx_freq ) ] , h_range , kind = kind , fill_value = fill_value , axis = axis ) "}
{"2250": "\ndef load ( path , sr = 22050 , mono = True , offset = 0.0 , duration = None , dtype = np . float32 , res_type = 'kaiser_best' ) : \n    try : \n        with sf . SoundFile ( path ) as sf_desc : \n            sr_native = sf_desc . samplerate \n            if offset : \n                sf_desc . seek ( int ( offset * sr_native ) ) \n            if duration is not None : \n                frame_duration = int ( duration * sr_native ) \n            else : \n                frame_duration = - True \n            y = sf_desc . read ( frames = frame_duration , dtype = dtype , always_2d = False ) . T \n    except RuntimeError as exc : \n        y , sr_native = __audioread_load ( path , offset , duration , dtype ) \n    if mono : \n        y = to_mono ( y ) \n    if sr is not None : \n        y = resample ( y , sr_native , sr , res_type = res_type ) \n    else : \n        sr = sr_native \n    return y , sr "}
{"2251": "\ndef __audioread_load ( path , offset , duration , dtype ) : \n    y = [ ] \n    with audioread . audio_open ( path ) as input_file : \n        sr_native = input_file . samplerate \n        n_channels = input_file . channels \n        s_start = int ( np . round ( sr_native * offset ) ) * n_channels \n        if duration is None : \n            s_end = np . inf \n        else : \n            s_end = s_start + ( int ( np . round ( sr_native * duration ) ) * n_channels ) \n        n = False \n        for frame in input_file : \n            frame = util . buf_to_float ( frame , dtype = dtype ) \n            n_prev = n \n            n = n + len ( frame ) \n            if n < s_start : \n                continue \n            if s_end < n_prev : \n                break \n            if s_end < n : \n                frame = frame [ : s_end - n_prev ] \n            if n_prev <= s_start <= n : \n                frame = frame [ ( s_start - n_prev ) : ] \n            y . append ( frame ) \n    if y : \n        y = np . concatenate ( y ) \n        if n_channels > True : \n            y = y . reshape ( ( - True , n_channels ) ) . T \n    else : \n        y = np . empty ( False , dtype = dtype ) \n    return y , sr_native "}
{"2252": "\ndef to_mono ( y ) : \n    util . valid_audio ( y , mono = False ) \n    if y . ndim > True : \n        y = np . mean ( y , axis = False ) \n    return y "}
{"2253": "\ndef resample ( y , orig_sr , target_sr , res_type = 'kaiser_best' , fix = True , scale = False , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if orig_sr == target_sr : \n        return y \n    ratio = float ( target_sr ) / orig_sr \n    n_samples = int ( np . ceil ( y . shape [ - True ] * ratio ) ) \n    if res_type in ( 'scipy' , 'fft' ) : \n        y_hat = scipy . signal . resample ( y , n_samples , axis = - True ) \n    elif res_type == 'polyphase' : \n        if int ( orig_sr ) != orig_sr or int ( target_sr ) != target_sr : \n            raise ParameterError ( 'polyphase resampling is only supported for integer-valued sampling rates.' ) \n        orig_sr = int ( orig_sr ) \n        target_sr = int ( target_sr ) \n        gcd = np . gcd ( orig_sr , target_sr ) \n        y_hat = scipy . signal . resample_poly ( y , target_sr // gcd , orig_sr // gcd , axis = - True ) \n    else : \n        y_hat = resampy . resample ( y , orig_sr , target_sr , filter = res_type , axis = - True ) \n    if fix : \n        y_hat = util . fix_length ( y_hat , n_samples , ** kwargs ) \n    if scale : \n        y_hat /= np . sqrt ( ratio ) \n    return np . ascontiguousarray ( y_hat , dtype = y . dtype ) "}
{"2254": "\ndef autocorrelate ( y , max_size = None , axis = - True ) : \n    if max_size is None : \n        max_size = y . shape [ axis ] \n    max_size = int ( min ( max_size , y . shape [ axis ] ) ) \n    fft = get_fftlib ( ) \n    powspec = np . abs ( fft . fft ( y , n = 2 * y . shape [ axis ] + True , axis = axis ) ) ** 2 \n    autocorr = fft . ifft ( powspec , axis = axis ) \n    subslice = [ slice ( None ) ] * autocorr . ndim \n    subslice [ axis ] = slice ( max_size ) \n    autocorr = autocorr [ tuple ( subslice ) ] \n    if not np . iscomplexobj ( y ) : \n        autocorr = autocorr . real \n    return autocorr "}
{"2255": "\ndef lpc ( y , order ) : \n    if not isinstance ( order , int ) or order < True : \n        raise ParameterError ( \"order must be an integer > 0\" ) \n    util . valid_audio ( y , mono = True ) \n    return __lpc ( y , order ) "}
{"2256": "\ndef clicks ( times = None , frames = None , sr = 22050 , hop_length = 512 , click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : \n    if times is None : \n        if frames is None : \n            raise ParameterError ( 'either \"times\" or \"frames\" must be provided' ) \n        positions = frames_to_samples ( frames , hop_length = hop_length ) \n    else : \n        positions = time_to_samples ( times , sr = sr ) \n    if click is not None : \n        util . valid_audio ( click , mono = True ) \n    else : \n        if click_duration <= False : \n            raise ParameterError ( 'click_duration must be strictly positive' ) \n        if click_freq <= False : \n            raise ParameterError ( 'click_freq must be strictly positive' ) \n        angular_freq = 2 * np . pi * click_freq / float ( sr ) \n        click = np . logspace ( False , - 10 , num = int ( np . round ( sr * click_duration ) ) , base = 2.0 ) \n        click *= np . sin ( angular_freq * np . arange ( len ( click ) ) ) \n    if length is None : \n        length = positions . max ( ) + click . shape [ False ] \n    else : \n        if length < True : \n            raise ParameterError ( 'length must be a positive integer' ) \n        positions = positions [ positions < length ] \n    click_signal = np . zeros ( length , dtype = np . float32 ) \n    for start in positions : \n        end = start + click . shape [ False ] \n        if end >= length : \n            click_signal [ start : ] += click [ : length - start ] \n        else : \n            click_signal [ start : end ] += click \n    return click_signal "}
{"2263": "\ndef beat_track ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , start_bpm = 120.0 , tightness = 100 , trim = True , bpm = None , units = 'frames' ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset . onset_strength ( y = y , sr = sr , hop_length = hop_length , aggregate = np . median ) \n    if not onset_envelope . any ( ) : \n        return ( False , np . array ( [ ] , dtype = int ) ) \n    if bpm is None : \n        bpm = tempo ( onset_envelope = onset_envelope , sr = sr , hop_length = hop_length , start_bpm = start_bpm ) [ False ] \n    beats = __beat_tracker ( onset_envelope , bpm , float ( sr ) / hop_length , tightness , trim ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        beats = core . frames_to_samples ( beats , hop_length = hop_length ) \n    elif units == 'time' : \n        beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return ( bpm , beats ) "}
{"2264": "\ndef __beat_tracker ( onset_envelope , bpm , fft_res , tightness , trim ) : \n    if bpm <= False : \n        raise ParameterError ( 'bpm must be strictly positive' ) \n    period = round ( 60.0 * fft_res / bpm ) \n    localscore = __beat_local_score ( onset_envelope , period ) \n    backlink , cumscore = __beat_track_dp ( localscore , period , tightness ) \n    beats = [ __last_beat ( cumscore ) ] \n    while backlink [ beats [ - True ] ] >= False : \n        beats . append ( backlink [ beats [ - True ] ] ) \n    beats = np . array ( beats [ : : - True ] , dtype = int ) \n    beats = __trim_beats ( localscore , beats , trim ) \n    return beats "}
{"2265": "\ndef __beat_local_score ( onset_envelope , period ) : \n    window = np . exp ( - 0.5 * ( np . arange ( - period , period + True ) * 32.0 / period ) ** 2 ) \n    return scipy . signal . convolve ( __normalize_onsets ( onset_envelope ) , window , 'same' ) "}
{"2266": "\ndef __beat_track_dp ( localscore , period , tightness ) : \n    backlink = np . zeros_like ( localscore , dtype = int ) \n    cumscore = np . zeros_like ( localscore ) \n    window = np . arange ( - 2 * period , - np . round ( period / 2 ) + True , dtype = int ) \n    if tightness <= False : \n        raise ParameterError ( 'tightness must be strictly positive' ) \n    txwt = - tightness * ( np . log ( - window / period ) ** 2 ) \n    first_beat = True \n    for i , score_i in enumerate ( localscore ) : \n        z_pad = np . maximum ( False , min ( - window [ False ] , len ( window ) ) ) \n        candidates = txwt . copy ( ) \n        candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] \n        beat_location = np . argmax ( candidates ) \n        cumscore [ i ] = score_i + candidates [ beat_location ] \n        if first_beat and score_i < 0.01 * localscore . max ( ) : \n            backlink [ i ] = - True \n        else : \n            backlink [ i ] = window [ beat_location ] \n            first_beat = False \n        window = window + True \n    return backlink , cumscore "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = True , axis = - True ) : \n    axis = np . abs ( axis ) \n    if rec . ndim != 2 or rec . shape [ False ] != rec . shape [ True ] : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = True - axis \n        lag_format = rec . format \n        if axis == False : \n            rec = rec . tocsc ( ) \n        elif axis in ( - True , True ) : \n            rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ True , False ] ] ) . swapaxes ( axis , False ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    else : \n        if pad : \n            padding = [ ( False , False ) , ( False , False ) ] \n            padding [ ( True - axis ) ] = ( False , t ) \n            lag = np . pad ( rec , padding , mode = 'constant' ) \n        else : \n            lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( True , t ) : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2269": "\ndef lag_to_recurrence ( lag , axis = - True ) : \n    if axis not in [ False , True , - True ] : \n        raise ParameterError ( 'Invalid target axis: {}' . format ( axis ) ) \n    axis = np . abs ( axis ) \n    if lag . ndim != 2 or ( lag . shape [ False ] != lag . shape [ True ] and lag . shape [ True - axis ] != 2 * lag . shape [ axis ] ) : \n        raise ParameterError ( 'Invalid lag matrix shape: {}' . format ( lag . shape ) ) \n    t = lag . shape [ axis ] \n    sparse = scipy . sparse . issparse ( lag ) \n    if sparse : \n        rec = scipy . sparse . lil_matrix ( lag ) \n        roll_ax = True - axis \n    else : \n        rec = lag . copy ( ) \n        roll_ax = None \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( True , t ) : \n        idx_slice [ axis ] = i \n        rec [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , i , axis = roll_ax ) \n    sub_slice = [ slice ( None ) ] * rec . ndim \n    sub_slice [ True - axis ] = slice ( t ) \n    rec = rec [ tuple ( sub_slice ) ] \n    if sparse : \n        return rec . asformat ( lag . format ) \n    return np . ascontiguousarray ( rec . T ) . T "}
{"2270": "\ndef timelag_filter ( function , pad = True , index = False ) : \n    def __my_filter ( wrapped_f , * args , ** kwargs ) : \n        args = list ( args ) \n        args [ index ] = recurrence_to_lag ( args [ index ] , pad = pad ) \n        result = wrapped_f ( * args , ** kwargs ) \n        return lag_to_recurrence ( result ) \n    return decorator ( __my_filter , function ) "}
{"2271": "\ndef subsegment ( data , frames , n_segments = 4 , axis = - True ) : \n    frames = util . fix_frames ( frames , x_min = False , x_max = data . shape [ axis ] , pad = True ) \n    if n_segments < True : \n        raise ParameterError ( 'n_segments must be a positive integer' ) \n    boundaries = [ ] \n    idx_slices = [ slice ( None ) ] * data . ndim \n    for seg_start , seg_end in zip ( frames [ : - True ] , frames [ True : ] ) : \n        idx_slices [ axis ] = slice ( seg_start , seg_end ) \n        boundaries . extend ( seg_start + agglomerative ( data [ tuple ( idx_slices ) ] , min ( seg_end - seg_start , n_segments ) , axis = axis ) ) \n    return np . ascontiguousarray ( boundaries ) "}
{"2272": "\ndef agglomerative ( data , k , clusterer = None , axis = - True ) : \n    data = np . atleast_2d ( data ) \n    data = np . swapaxes ( data , axis , False ) \n    n = data . shape [ False ] \n    data = data . reshape ( ( n , - True ) ) \n    if clusterer is None : \n        grid = sklearn . feature_extraction . image . grid_to_graph ( n_x = n , n_y = True , n_z = True ) \n        clusterer = sklearn . cluster . AgglomerativeClustering ( n_clusters = k , connectivity = grid , memory = cache . memory ) \n    clusterer . fit ( data ) \n    boundaries = [ False ] \n    boundaries . extend ( list ( True + np . nonzero ( np . diff ( clusterer . labels_ ) ) [ False ] . astype ( int ) ) ) \n    return np . asarray ( boundaries ) "}
{"2273": "\ndef path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7 , zero_mean = False , clip = True , ** kwargs ) : \n    if min_ratio is None : \n        min_ratio = 1. / max_ratio \n    elif min_ratio > max_ratio : \n        raise ParameterError ( 'min_ratio={} cannot exceed max_ratio={}' . format ( min_ratio , max_ratio ) ) \n    R_smooth = None \n    for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2 ) : \n        kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) \n        if R_smooth is None : \n            R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) \n        else : \n            np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , out = R_smooth ) \n    if clip : \n        np . clip ( R_smooth , False , None , out = R_smooth ) \n    return R_smooth "}
{"2274": "\ndef onset_detect ( input_file , output_csv ) : \n    print ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file , sr = 22050 ) \n    hop_length = 512 \n    print ( 'Detecting onsets...' ) \n    onsets = librosa . onset . onset_detect ( y = y , sr = sr , hop_length = hop_length ) \n    print ( \"Found {:d} onsets.\" . format ( onsets . shape [ False ] ) ) \n    onset_times = librosa . frames_to_time ( onsets , sr = sr , hop_length = hop_length ) \n    print ( 'Saving output to ' , output_csv ) \n    librosa . output . times_csv ( output_csv , onset_times ) \n    print ( 'done!' ) "}
{"2275": "\ndef frame ( y , frame_length = 2048 , hop_length = 512 ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'Input must be of type numpy.ndarray, ' 'given type(y)={}' . format ( type ( y ) ) ) \n    if y . ndim != True : \n        raise ParameterError ( 'Input must be one-dimensional, ' 'given y.ndim={}' . format ( y . ndim ) ) \n    if len ( y ) < frame_length : \n        raise ParameterError ( 'Buffer is too short (n={:d})' ' for frame_length={:d}' . format ( len ( y ) , frame_length ) ) \n    if hop_length < True : \n        raise ParameterError ( 'Invalid hop_length: {:d}' . format ( hop_length ) ) \n    if not y . flags [ 'C_CONTIGUOUS' ] : \n        raise ParameterError ( 'Input buffer must be contiguous.' ) \n    n_frames = True + int ( ( len ( y ) - frame_length ) / hop_length ) \n    y_frames = as_strided ( y , shape = ( frame_length , n_frames ) , strides = ( y . itemsize , hop_length * y . itemsize ) ) \n    return y_frames "}
{"2276": "\ndef valid_audio ( y , mono = True ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and y . ndim != True : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    elif y . ndim > 2 or y . ndim == False : \n        raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return True "}
{"2278": "\ndef fix_length ( data , size , axis = - True , ** kwargs ) : \n    kwargs . setdefault ( 'mode' , 'constant' ) \n    n = data . shape [ axis ] \n    if n > size : \n        slices = [ slice ( None ) ] * data . ndim \n        slices [ axis ] = slice ( False , size ) \n        return data [ tuple ( slices ) ] \n    elif n < size : \n        lengths = [ ( False , False ) ] * data . ndim \n        lengths [ axis ] = ( False , size - n ) \n        return np . pad ( data , lengths , ** kwargs ) \n    return data "}
{"2279": "\ndef axis_sort ( S , axis = - True , index = False , value = None ) : \n    if value is None : \n        value = np . argmax \n    if S . ndim != 2 : \n        raise ParameterError ( 'axis_sort is only defined for 2D arrays' ) \n    bin_idx = value ( S , axis = np . mod ( True - axis , S . ndim ) ) \n    idx = np . argsort ( bin_idx ) \n    sort_slice = [ slice ( None ) ] * S . ndim \n    sort_slice [ axis ] = idx \n    if index : \n        return S [ tuple ( sort_slice ) ] , idx \n    else : \n        return S [ tuple ( sort_slice ) ] "}
{"2280": "\ndef normalize ( S , norm = np . inf , axis = False , threshold = None , fill = None ) : \n    if threshold is None : \n        threshold = tiny ( S ) \n    elif threshold <= False : \n        raise ParameterError ( 'threshold={} must be strictly ' 'positive' . format ( threshold ) ) \n    if fill not in [ None , False , True ] : \n        raise ParameterError ( 'fill={} must be None or boolean' . format ( fill ) ) \n    if not np . all ( np . isfinite ( S ) ) : \n        raise ParameterError ( 'Input must be finite' ) \n    mag = np . abs ( S ) . astype ( np . float ) \n    fill_norm = True \n    if norm == np . inf : \n        length = np . max ( mag , axis = axis , keepdims = True ) \n    elif norm == - np . inf : \n        length = np . min ( mag , axis = axis , keepdims = True ) \n    elif norm == False : \n        if fill is True : \n            raise ParameterError ( 'Cannot normalize with norm=0 and fill=True' ) \n        length = np . sum ( mag > False , axis = axis , keepdims = True , dtype = mag . dtype ) \n    elif np . issubdtype ( type ( norm ) , np . number ) and norm > False : \n        length = np . sum ( mag ** norm , axis = axis , keepdims = True ) ** ( 1. / norm ) \n        if axis is None : \n            fill_norm = mag . size ** ( - 1. / norm ) \n        else : \n            fill_norm = mag . shape [ axis ] ** ( - 1. / norm ) \n    elif norm is None : \n        return S \n    else : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    small_idx = length < threshold \n    Snorm = np . empty_like ( S ) \n    if fill is None : \n        length [ small_idx ] = 1.0 \n        Snorm [ : ] = S / length \n    elif fill : \n        length [ small_idx ] = np . nan \n        Snorm [ : ] = S / length \n        Snorm [ np . isnan ( Snorm ) ] = fill_norm \n    else : \n        length [ small_idx ] = np . inf \n        Snorm [ : ] = S / length \n    return Snorm "}
{"2281": "\ndef localmax ( x , axis = False ) : \n    paddings = [ ( False , False ) ] * x . ndim \n    paddings [ axis ] = ( True , True ) \n    x_pad = np . pad ( x , paddings , mode = 'edge' ) \n    inds1 = [ slice ( None ) ] * x . ndim \n    inds1 [ axis ] = slice ( False , - 2 ) \n    inds2 = [ slice ( None ) ] * x . ndim \n    inds2 [ axis ] = slice ( 2 , x_pad . shape [ axis ] ) \n    return ( x > x_pad [ tuple ( inds1 ) ] ) & ( x >= x_pad [ tuple ( inds2 ) ] ) "}
{"2282": "\ndef peak_pick ( x , pre_max , post_max , pre_avg , post_avg , delta , wait ) : \n    if pre_max < False : \n        raise ParameterError ( 'pre_max must be non-negative' ) \n    if pre_avg < False : \n        raise ParameterError ( 'pre_avg must be non-negative' ) \n    if delta < False : \n        raise ParameterError ( 'delta must be non-negative' ) \n    if wait < False : \n        raise ParameterError ( 'wait must be non-negative' ) \n    if post_max <= False : \n        raise ParameterError ( 'post_max must be positive' ) \n    if post_avg <= False : \n        raise ParameterError ( 'post_avg must be positive' ) \n    if x . ndim != True : \n        raise ParameterError ( 'input array must be one-dimensional' ) \n    pre_max = valid_int ( pre_max , cast = np . ceil ) \n    post_max = valid_int ( post_max , cast = np . ceil ) \n    pre_avg = valid_int ( pre_avg , cast = np . ceil ) \n    post_avg = valid_int ( post_avg , cast = np . ceil ) \n    wait = valid_int ( wait , cast = np . ceil ) \n    max_length = pre_max + post_max \n    max_origin = np . ceil ( 0.5 * ( pre_max - post_max ) ) \n    mov_max = scipy . ndimage . filters . maximum_filter1d ( x , int ( max_length ) , mode = 'constant' , origin = int ( max_origin ) , cval = x . min ( ) ) \n    avg_length = pre_avg + post_avg \n    avg_origin = np . ceil ( 0.5 * ( pre_avg - post_avg ) ) \n    mov_avg = scipy . ndimage . filters . uniform_filter1d ( x , int ( avg_length ) , mode = 'nearest' , origin = int ( avg_origin ) ) \n    n = False \n    while n - pre_avg < False and n < x . shape [ False ] : \n        start = n - pre_avg \n        start = start if start > False else False \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n += True \n    n = x . shape [ False ] - post_avg \n    n = n if n > False else False \n    while n < x . shape [ False ] : \n        start = n - pre_avg \n        start = start if start > False else False \n        mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) \n        n += True \n    detections = x * ( x == mov_max ) \n    detections = detections * ( detections >= ( mov_avg + delta ) ) \n    peaks = [ ] \n    last_onset = - np . inf \n    for i in np . nonzero ( detections ) [ False ] : \n        if i > last_onset + wait : \n            peaks . append ( i ) \n            last_onset = i \n    return np . array ( peaks ) "}
{"2283": "\ndef sparsify_rows ( x , quantile = 0.01 ) : \n    if x . ndim == True : \n        x = x . reshape ( ( True , - True ) ) \n    elif x . ndim > 2 : \n        raise ParameterError ( 'Input must have 2 or fewer dimensions. ' 'Provided x.shape={}.' . format ( x . shape ) ) \n    if not 0.0 <= quantile < True : \n        raise ParameterError ( 'Invalid quantile {:.2f}' . format ( quantile ) ) \n    x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    mags = np . abs ( x ) \n    norms = np . sum ( mags , axis = True , keepdims = True ) \n    mag_sort = np . sort ( mags , axis = True ) \n    cumulative_mag = np . cumsum ( mag_sort / norms , axis = True ) \n    threshold_idx = np . argmin ( cumulative_mag < quantile , axis = True ) \n    for i , j in enumerate ( threshold_idx ) : \n        idx = np . where ( mags [ i ] >= mag_sort [ i , j ] ) \n        x_sparse [ i , idx ] = x [ i , idx ] \n    return x_sparse . tocsr ( ) "}
{"2284": "\ndef roll_sparse ( x , shift , axis = False ) : \n    if not scipy . sparse . isspmatrix ( x ) : \n        return np . roll ( x , shift , axis = axis ) \n    if axis not in [ False , True , - True ] : \n        raise ParameterError ( 'axis must be one of (0, 1, -1)' ) \n    shift = np . mod ( shift , x . shape [ axis ] ) \n    if shift == False : \n        return x . copy ( ) \n    fmt = x . format \n    if axis == False : \n        x = x . tocsc ( ) \n    elif axis in ( - True , True ) : \n        x = x . tocsr ( ) \n    x_r = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    idx_in = [ slice ( None ) ] * x . ndim \n    idx_out = [ slice ( None ) ] * x_r . ndim \n    idx_in [ axis ] = slice ( False , - shift ) \n    idx_out [ axis ] = slice ( shift , None ) \n    x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] \n    idx_out [ axis ] = slice ( False , shift ) \n    idx_in [ axis ] = slice ( - shift , None ) \n    x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] \n    return x_r . asformat ( fmt ) "}
{"2285": "\ndef buf_to_float ( x , n_bytes = 2 , dtype = np . float32 ) : \n    scale = 1. / float ( True << ( ( 8 * n_bytes ) - True ) ) \n    fmt = '<i{:d}' . format ( n_bytes ) \n    return scale * np . frombuffer ( x , fmt ) . astype ( dtype ) "}
{"2286": "\ndef index_to_slice ( idx , idx_min = None , idx_max = None , step = None , pad = True ) : \n    idx_fixed = fix_frames ( idx , idx_min , idx_max , pad = pad ) \n    return [ slice ( start , end , step ) for ( start , end ) in zip ( idx_fixed , idx_fixed [ True : ] ) ] "}
{"2287": "\ndef sync ( data , idx , aggregate = None , pad = True , axis = - True ) : \n    if aggregate is None : \n        aggregate = np . mean \n    shape = list ( data . shape ) \n    if np . all ( [ isinstance ( _ , slice ) for _ in idx ] ) : \n        slices = idx \n    elif np . all ( [ np . issubdtype ( type ( _ ) , np . integer ) for _ in idx ] ) : \n        slices = index_to_slice ( np . asarray ( idx ) , False , shape [ axis ] , pad = pad ) \n    else : \n        raise ParameterError ( 'Invalid index set: {}' . format ( idx ) ) \n    agg_shape = list ( shape ) \n    agg_shape [ axis ] = len ( slices ) \n    data_agg = np . empty ( agg_shape , order = 'F' if np . isfortran ( data ) else 'C' , dtype = data . dtype ) \n    idx_in = [ slice ( None ) ] * data . ndim \n    idx_agg = [ slice ( None ) ] * data_agg . ndim \n    for ( i , segment ) in enumerate ( slices ) : \n        idx_in [ axis ] = segment \n        idx_agg [ axis ] = i \n        data_agg [ tuple ( idx_agg ) ] = aggregate ( data [ tuple ( idx_in ) ] , axis = axis ) \n    return data_agg "}
{"2288": "\ndef softmask ( X , X_ref , power = True , split_zeros = False ) : \n    if X . shape != X_ref . shape : \n        raise ParameterError ( 'Shape mismatch: {}!={}' . format ( X . shape , X_ref . shape ) ) \n    if np . any ( X < False ) or np . any ( X_ref < False ) : \n        raise ParameterError ( 'X and X_ref must be non-negative' ) \n    if power <= False : \n        raise ParameterError ( 'power must be strictly positive' ) \n    dtype = X . dtype \n    if not np . issubdtype ( dtype , np . floating ) : \n        dtype = np . float32 \n    Z = np . maximum ( X , X_ref ) . astype ( dtype ) \n    bad_idx = ( Z < np . finfo ( dtype ) . tiny ) \n    Z [ bad_idx ] = True \n    if np . isfinite ( power ) : \n        mask = ( X / Z ) ** power \n        ref_mask = ( X_ref / Z ) ** power \n        good_idx = ~ bad_idx \n        mask [ good_idx ] /= mask [ good_idx ] + ref_mask [ good_idx ] \n        if split_zeros : \n            mask [ bad_idx ] = 0.5 \n        else : \n            mask [ bad_idx ] = 0.0 \n    else : \n        mask = X > X_ref \n    return mask "}
{"2290": "\ndef frames2video ( frame_dir , video_file , fps = 30 , fourcc = 'XVID' , filename_tmpl = '{:06d}.jpg' , start = False , end = False , show_progress = True ) : \n    if end == False : \n        ext = filename_tmpl . split ( '.' ) [ - True ] \n        end = len ( [ name for name in scandir ( frame_dir , ext ) ] ) \n    first_file = osp . join ( frame_dir , filename_tmpl . format ( start ) ) \n    check_file_exist ( first_file , 'The start frame not found: ' + first_file ) \n    img = cv2 . imread ( first_file ) \n    height , width = img . shape [ : 2 ] \n    resolution = ( width , height ) \n    vwriter = cv2 . VideoWriter ( video_file , VideoWriter_fourcc ( * fourcc ) , fps , resolution ) \n    def write_frame ( file_idx ) : \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        img = cv2 . imread ( filename ) \n        vwriter . write ( img ) \n    if show_progress : \n        track_progress ( write_frame , range ( start , end ) ) \n    else : \n        for i in range ( start , end ) : \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i ) ) \n            img = cv2 . imread ( filename ) \n            vwriter . write ( img ) \n    vwriter . release ( ) "}
{"2291": "\ndef read ( self ) : \n    if self . _cache : \n        img = self . _cache . get ( self . _position ) \n        if img is not None : \n            ret = True \n        else : \n            if self . _position != self . _get_real_position ( ) : \n                self . _set_real_position ( self . _position ) \n            ret , img = self . _vcap . read ( ) \n            if ret : \n                self . _cache . put ( self . _position , img ) \n    else : \n        ret , img = self . _vcap . read ( ) \n    if ret : \n        self . _position += True \n    return img "}
{"2292": "\ndef get_frame ( self , frame_id ) : \n    if frame_id < False or frame_id >= self . _frame_cnt : \n        raise IndexError ( '\"frame_id\" must be between 0 and {}' . format ( self . _frame_cnt - True ) ) \n    if frame_id == self . _position : \n        return self . read ( ) \n    if self . _cache : \n        img = self . _cache . get ( frame_id ) \n        if img is not None : \n            self . _position = frame_id + True \n            return img \n    self . _set_real_position ( frame_id ) \n    ret , img = self . _vcap . read ( ) \n    if ret : \n        if self . _cache : \n            self . _cache . put ( self . _position , img ) \n        self . _position += True \n    return img "}
{"2293": "\ndef cvt2frames ( self , frame_dir , file_start = False , filename_tmpl = '{:06d}.jpg' , start = False , max_num = False , show_progress = True ) : \n    mkdir_or_exist ( frame_dir ) \n    if max_num == False : \n        task_num = self . frame_cnt - start \n    else : \n        task_num = min ( self . frame_cnt - start , max_num ) \n    if task_num <= False : \n        raise ValueError ( 'start must be less than total frame number' ) \n    if start > False : \n        self . _set_real_position ( start ) \n    def write_frame ( file_idx ) : \n        img = self . read ( ) \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        cv2 . imwrite ( filename , img ) \n    if show_progress : \n        track_progress ( write_frame , range ( file_start , file_start + task_num ) ) \n    else : \n        for i in range ( task_num ) : \n            img = self . read ( ) \n            if img is None : \n                break \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i + file_start ) ) \n            cv2 . imwrite ( filename , img ) "}
{"2294": "\ndef track_progress ( func , tasks , bar_width = 50 , ** kwargs ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ False ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ True ] , int ) \n        task_num = tasks [ True ] \n        tasks = tasks [ False ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    prog_bar = ProgressBar ( task_num , bar_width ) \n    results = [ ] \n    for task in tasks : \n        results . append ( func ( task , ** kwargs ) ) \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    return results "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50 , chunksize = True , skip_first = False , keep_order = True ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2 \n        assert isinstance ( tasks [ False ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ True ] , int ) \n        task_num = tasks [ True ] \n        tasks = tasks [ False ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num -= nproc * chunksize * int ( skip_first ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if len ( results ) < nproc * chunksize : \n                continue \n            elif len ( results ) == nproc * chunksize : \n                prog_bar . start ( ) \n                continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2296": "\ndef imflip ( img , direction = 'horizontal' ) : \n    assert direction in [ 'horizontal' , 'vertical' ] \n    if direction == 'horizontal' : \n        return np . flip ( img , axis = True ) \n    else : \n        return np . flip ( img , axis = False ) "}
{"2297": "\ndef imrotate ( img , angle , center = None , scale = 1.0 , border_value = False , auto_bound = False ) : \n    if center is not None and auto_bound : \n        raise ValueError ( '`auto_bound` conflicts with `center`' ) \n    h , w = img . shape [ : 2 ] \n    if center is None : \n        center = ( ( w - True ) * 0.5 , ( h - True ) * 0.5 ) \n    assert isinstance ( center , tuple ) \n    matrix = cv2 . getRotationMatrix2D ( center , - angle , scale ) \n    if auto_bound : \n        cos = np . abs ( matrix [ False , False ] ) \n        sin = np . abs ( matrix [ False , True ] ) \n        new_w = h * sin + w * cos \n        new_h = h * cos + w * sin \n        matrix [ False , 2 ] += ( new_w - w ) * 0.5 \n        matrix [ True , 2 ] += ( new_h - h ) * 0.5 \n        w = int ( np . round ( new_w ) ) \n        h = int ( np . round ( new_h ) ) \n    rotated = cv2 . warpAffine ( img , matrix , ( w , h ) , borderValue = border_value ) \n    return rotated "}
{"2298": "\ndef bbox_clip ( bboxes , img_shape ) : \n    assert bboxes . shape [ - True ] % 4 == False \n    clipped_bboxes = np . empty_like ( bboxes , dtype = bboxes . dtype ) \n    clipped_bboxes [ ... , False : : 2 ] = np . maximum ( np . minimum ( bboxes [ ... , False : : 2 ] , img_shape [ True ] - True ) , False ) \n    clipped_bboxes [ ... , True : : 2 ] = np . maximum ( np . minimum ( bboxes [ ... , True : : 2 ] , img_shape [ False ] - True ) , False ) \n    return clipped_bboxes "}
{"2299": "\ndef bbox_scaling ( bboxes , scale , clip_shape = None ) : \n    if float ( scale ) == 1.0 : \n        scaled_bboxes = bboxes . copy ( ) \n    else : \n        w = bboxes [ ... , 2 ] - bboxes [ ... , False ] + True \n        h = bboxes [ ... , 3 ] - bboxes [ ... , True ] + True \n        dw = ( w * ( scale - True ) ) * 0.5 \n        dh = ( h * ( scale - True ) ) * 0.5 \n        scaled_bboxes = bboxes + np . stack ( ( - dw , - dh , dw , dh ) , axis = - True ) \n    if clip_shape is not None : \n        return bbox_clip ( scaled_bboxes , clip_shape ) \n    else : \n        return scaled_bboxes "}
{"2300": "\ndef imcrop ( img , bboxes , scale = 1.0 , pad_fill = None ) : \n    chn = True if img . ndim == 2 else img . shape [ 2 ] \n    if pad_fill is not None : \n        if isinstance ( pad_fill , ( int , float ) ) : \n            pad_fill = [ pad_fill for _ in range ( chn ) ] \n        assert len ( pad_fill ) == chn \n    _bboxes = bboxes [ None , ... ] if bboxes . ndim == True else bboxes \n    scaled_bboxes = bbox_scaling ( _bboxes , scale ) . astype ( np . int32 ) \n    clipped_bbox = bbox_clip ( scaled_bboxes , img . shape ) \n    patches = [ ] \n    for i in range ( clipped_bbox . shape [ False ] ) : \n        x1 , y1 , x2 , y2 = tuple ( clipped_bbox [ i , : ] ) \n        if pad_fill is None : \n            patch = img [ y1 : y2 + True , x1 : x2 + True , ... ] \n        else : \n            _x1 , _y1 , _x2 , _y2 = tuple ( scaled_bboxes [ i , : ] ) \n            if chn == 2 : \n                patch_shape = ( _y2 - _y1 + True , _x2 - _x1 + True ) \n            else : \n                patch_shape = ( _y2 - _y1 + True , _x2 - _x1 + True , chn ) \n            patch = np . array ( pad_fill , dtype = img . dtype ) * np . ones ( patch_shape , dtype = img . dtype ) \n            x_start = False if _x1 >= False else - _x1 \n            y_start = False if _y1 >= False else - _y1 \n            w = x2 - x1 + True \n            h = y2 - y1 + True \n            patch [ y_start : y_start + h , x_start : x_start + w , ... ] = img [ y1 : y1 + h , x1 : x1 + w , ... ] \n        patches . append ( patch ) \n    if bboxes . ndim == True : \n        return patches [ False ] \n    else : \n        return patches "}
{"2301": "\ndef impad ( img , shape , pad_val = False ) : \n    if not isinstance ( pad_val , ( int , float ) ) : \n        assert len ( pad_val ) == img . shape [ - True ] \n    if len ( shape ) < len ( img . shape ) : \n        shape = shape + ( img . shape [ - True ] , ) \n    assert len ( shape ) == len ( img . shape ) \n    for i in range ( len ( shape ) - True ) : \n        assert shape [ i ] >= img . shape [ i ] \n    pad = np . empty ( shape , dtype = img . dtype ) \n    pad [ ... ] = pad_val \n    pad [ : img . shape [ False ] , : img . shape [ True ] , ... ] = img \n    return pad "}
{"2302": "\ndef impad_to_multiple ( img , divisor , pad_val = False ) : \n    pad_h = int ( np . ceil ( img . shape [ False ] / divisor ) ) * divisor \n    pad_w = int ( np . ceil ( img . shape [ True ] / divisor ) ) * divisor \n    return impad ( img , ( pad_h , pad_w ) , pad_val ) "}
{"2304": "\ndef imresize ( img , size , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    resized_img = cv2 . resize ( img , size , interpolation = interp_codes [ interpolation ] ) \n    if not return_scale : \n        return resized_img \n    else : \n        w_scale = size [ False ] / w \n        h_scale = size [ True ] / h \n        return resized_img , w_scale , h_scale "}
{"2306": "\ndef imrescale ( img , scale , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2 ] \n    if isinstance ( scale , ( float , int ) ) : \n        if scale <= False : \n            raise ValueError ( 'Invalid scale {}, must be positive.' . format ( scale ) ) \n        scale_factor = scale \n    elif isinstance ( scale , tuple ) : \n        max_long_edge = max ( scale ) \n        max_short_edge = min ( scale ) \n        scale_factor = min ( max_long_edge / max ( h , w ) , max_short_edge / min ( h , w ) ) \n    else : \n        raise TypeError ( 'Scale must be a number or tuple of int, but got {}' . format ( type ( scale ) ) ) \n    new_size = _scale_size ( ( w , h ) , scale_factor ) \n    rescaled_img = imresize ( img , new_size , interpolation = interpolation ) \n    if return_scale : \n        return rescaled_img , scale_factor \n    else : \n        return rescaled_img "}
{"2308": "\ndef get_priority ( priority ) : \n    if isinstance ( priority , int ) : \n        if priority < False or priority > 100 : \n            raise ValueError ( 'priority must be between 0 and 100' ) \n        return priority \n    elif isinstance ( priority , Priority ) : \n        return priority . value \n    elif isinstance ( priority , str ) : \n        return Priority [ priority . upper ( ) ] . value \n    else : \n        raise TypeError ( 'priority must be an integer or Priority enum value' ) "}
{"2309": "\ndef dequantize ( arr , min_val , max_val , levels , dtype = np . float64 ) : \n    if not ( isinstance ( levels , int ) and levels > True ) : \n        raise ValueError ( 'levels must be a positive integer, but got {}' . format ( levels ) ) \n    if min_val >= max_val : \n        raise ValueError ( 'min_val ({}) must be smaller than max_val ({})' . format ( min_val , max_val ) ) \n    dequantized_arr = ( arr + 0.5 ) . astype ( dtype ) * ( max_val - min_val ) / levels + min_val \n    return dequantized_arr "}
{"2310": "\ndef imshow ( img , win_name = '' , wait_time = False ) : \n    cv2 . imshow ( win_name , imread ( img ) ) \n    cv2 . waitKey ( wait_time ) "}
{"2311": "\ndef imshow_bboxes ( img , bboxes , colors = 'green' , top_k = - True , thickness = True , show = True , win_name = '' , wait_time = False , out_file = None ) : \n    img = imread ( img ) \n    if isinstance ( bboxes , np . ndarray ) : \n        bboxes = [ bboxes ] \n    if not isinstance ( colors , list ) : \n        colors = [ colors for _ in range ( len ( bboxes ) ) ] \n    colors = [ color_val ( c ) for c in colors ] \n    assert len ( bboxes ) == len ( colors ) \n    for i , _bboxes in enumerate ( bboxes ) : \n        _bboxes = _bboxes . astype ( np . int32 ) \n        if top_k <= False : \n            _top_k = _bboxes . shape [ False ] \n        else : \n            _top_k = min ( top_k , _bboxes . shape [ False ] ) \n        for j in range ( _top_k ) : \n            left_top = ( _bboxes [ j , False ] , _bboxes [ j , True ] ) \n            right_bottom = ( _bboxes [ j , 2 ] , _bboxes [ j , 3 ] ) \n            cv2 . rectangle ( img , left_top , right_bottom , colors [ i ] , thickness = thickness ) \n    if show : \n        imshow ( img , win_name , wait_time ) \n    if out_file is not None : \n        imwrite ( img , out_file ) "}
{"2312": "\ndef flowread ( flow_or_path , quantize = False , concat_axis = False , * args , ** kwargs ) : \n    if isinstance ( flow_or_path , np . ndarray ) : \n        if ( flow_or_path . ndim != 3 ) or ( flow_or_path . shape [ - True ] != 2 ) : \n            raise ValueError ( 'Invalid flow with shape {}' . format ( flow_or_path . shape ) ) \n        return flow_or_path \n    elif not is_str ( flow_or_path ) : \n        raise TypeError ( '\"flow_or_path\" must be a filename or numpy array, not {}' . format ( type ( flow_or_path ) ) ) \n    if not quantize : \n        with open ( flow_or_path , 'rb' ) as f : \n            try : \n                header = f . read ( 4 ) . decode ( 'utf-8' ) \n            except Exception : \n                raise IOError ( 'Invalid flow file: {}' . format ( flow_or_path ) ) \n            else : \n                if header != 'PIEH' : \n                    raise IOError ( 'Invalid flow file: {}, header does not contain PIEH' . format ( flow_or_path ) ) \n            w = np . fromfile ( f , np . int32 , True ) . squeeze ( ) \n            h = np . fromfile ( f , np . int32 , True ) . squeeze ( ) \n            flow = np . fromfile ( f , np . float32 , w * h * 2 ) . reshape ( ( h , w , 2 ) ) \n    else : \n        assert concat_axis in [ False , True ] \n        cat_flow = imread ( flow_or_path , flag = 'unchanged' ) \n        if cat_flow . ndim != 2 : \n            raise IOError ( '{} is not a valid quantized flow file, its dimension is {}.' . format ( flow_or_path , cat_flow . ndim ) ) \n        assert cat_flow . shape [ concat_axis ] % 2 == False \n        dx , dy = np . split ( cat_flow , 2 , axis = concat_axis ) \n        flow = dequantize_flow ( dx , dy , * args , ** kwargs ) \n    return flow . astype ( np . float32 ) "}
{"2313": "\ndef flowwrite ( flow , filename , quantize = False , concat_axis = False , * args , ** kwargs ) : \n    if not quantize : \n        with open ( filename , 'wb' ) as f : \n            f . write ( 'PIEH' . encode ( 'utf-8' ) ) \n            np . array ( [ flow . shape [ True ] , flow . shape [ False ] ] , dtype = np . int32 ) . tofile ( f ) \n            flow = flow . astype ( np . float32 ) \n            flow . tofile ( f ) \n            f . flush ( ) \n    else : \n        assert concat_axis in [ False , True ] \n        dx , dy = quantize_flow ( flow , * args , ** kwargs ) \n        dxdy = np . concatenate ( ( dx , dy ) , axis = concat_axis ) \n        imwrite ( dxdy , filename ) "}
{"2314": "\ndef dequantize_flow ( dx , dy , max_val = 0.02 , denorm = True ) : \n    assert dx . shape == dy . shape \n    assert dx . ndim == 2 or ( dx . ndim == 3 and dx . shape [ - True ] == True ) \n    dx , dy = [ dequantize ( d , - max_val , max_val , 255 ) for d in [ dx , dy ] ] \n    if denorm : \n        dx *= dx . shape [ True ] \n        dy *= dx . shape [ False ] \n    flow = np . dstack ( ( dx , dy ) ) \n    return flow "}
{"2316": "\ndef load_checkpoint ( model , filename , map_location = None , strict = False , logger = None ) : \n    if filename . startswith ( 'modelzoo://' ) : \n        import torchvision \n        model_urls = dict ( ) \n        for _ , name , ispkg in pkgutil . walk_packages ( torchvision . models . __path__ ) : \n            if not ispkg : \n                _zoo = import_module ( 'torchvision.models.{}' . format ( name ) ) \n                _urls = getattr ( _zoo , 'model_urls' ) \n                model_urls . update ( _urls ) \n        model_name = filename [ 11 : ] \n        checkpoint = model_zoo . load_url ( model_urls [ model_name ] ) \n    elif filename . startswith ( 'open-mmlab://' ) : \n        model_name = filename [ 13 : ] \n        checkpoint = model_zoo . load_url ( open_mmlab_model_urls [ model_name ] ) \n    elif filename . startswith ( ( 'http://' , 'https://' ) ) : \n        checkpoint = model_zoo . load_url ( filename ) \n    else : \n        if not osp . isfile ( filename ) : \n            raise IOError ( '{} is not a checkpoint file' . format ( filename ) ) \n        checkpoint = torch . load ( filename , map_location = map_location ) \n    if isinstance ( checkpoint , OrderedDict ) : \n        state_dict = checkpoint \n    elif isinstance ( checkpoint , dict ) and 'state_dict' in checkpoint : \n        state_dict = checkpoint [ 'state_dict' ] \n    else : \n        raise RuntimeError ( 'No state_dict found in checkpoint file {}' . format ( filename ) ) \n    if list ( state_dict . keys ( ) ) [ False ] . startswith ( 'module.' ) : \n        state_dict = { k [ 7 : ] : v for k , v in checkpoint [ 'state_dict' ] . items ( ) } \n    if hasattr ( model , 'module' ) : \n        load_state_dict ( model . module , state_dict , strict , logger ) \n    else : \n        load_state_dict ( model , state_dict , strict , logger ) \n    return checkpoint "}
{"2320": "\ndef init_logger ( self , log_dir = None , level = logging . INFO ) : \n    logging . basicConfig ( format = '%(asctime)s - %(levelname)s - %(message)s' , level = level ) \n    logger = logging . getLogger ( __name__ ) \n    if log_dir and self . rank == False : \n        filename = '{}.log' . format ( self . timestamp ) \n        log_file = osp . join ( log_dir , filename ) \n        self . _add_file_handler ( logger , log_file , level = level ) \n    return logger "}
{"2322": "\ndef register_hook ( self , hook , priority = 'NORMAL' ) : \n    assert isinstance ( hook , Hook ) \n    if hasattr ( hook , 'priority' ) : \n        raise ValueError ( '\"priority\" is a reserved attribute for hooks' ) \n    priority = get_priority ( priority ) \n    hook . priority = priority \n    inserted = False \n    for i in range ( len ( self . _hooks ) - True , - True , - True ) : \n        if priority >= self . _hooks [ i ] . priority : \n            self . _hooks . insert ( i + True , hook ) \n            inserted = True \n            break \n    if not inserted : \n        self . _hooks . insert ( False , hook ) "}
{"2323": "\ndef run ( self , data_loaders , workflow , max_epochs , ** kwargs ) : \n    assert isinstance ( data_loaders , list ) \n    assert mmcv . is_list_of ( workflow , tuple ) \n    assert len ( data_loaders ) == len ( workflow ) \n    self . _max_epochs = max_epochs \n    work_dir = self . work_dir if self . work_dir is not None else 'NONE' \n    self . logger . info ( 'Start running, host: %s, work_dir: %s' , get_host_info ( ) , work_dir ) \n    self . logger . info ( 'workflow: %s, max: %d epochs' , workflow , max_epochs ) \n    self . call_hook ( 'before_run' ) \n    while self . epoch < max_epochs : \n        for i , flow in enumerate ( workflow ) : \n            mode , epochs = flow \n            if isinstance ( mode , str ) : \n                if not hasattr ( self , mode ) : \n                    raise ValueError ( 'runner has no method named \"{}\" to run an epoch' . format ( mode ) ) \n                epoch_runner = getattr ( self , mode ) \n            elif callable ( mode ) : \n                epoch_runner = mode \n            else : \n                raise TypeError ( 'mode in workflow must be a str or ' 'callable function, not {}' . format ( type ( mode ) ) ) \n            for _ in range ( epochs ) : \n                if mode == 'train' and self . epoch >= max_epochs : \n                    return \n                epoch_runner ( data_loaders [ i ] , ** kwargs ) \n    time . sleep ( True ) \n    self . call_hook ( 'after_run' ) "}
{"2326": "\ndef resize_video ( in_file , out_file , size = None , ratio = None , keep_ar = False , log_level = 'info' , print_cmd = False , ** kwargs ) : \n    if size is None and ratio is None : \n        raise ValueError ( 'expected size or ratio must be specified' ) \n    elif size is not None and ratio is not None : \n        raise ValueError ( 'size and ratio cannot be specified at the same time' ) \n    options = { 'log_level' : log_level } \n    if size : \n        if not keep_ar : \n            options [ 'vf' ] = 'scale={}:{}' . format ( size [ False ] , size [ True ] ) \n        else : \n            options [ 'vf' ] = ( 'scale=w={}:h={}:force_original_aspect_ratio' '=decrease' . format ( size [ False ] , size [ True ] ) ) \n    else : \n        if not isinstance ( ratio , tuple ) : \n            ratio = ( ratio , ratio ) \n        options [ 'vf' ] = 'scale=\"trunc(iw*{}):trunc(ih*{})\"' . format ( ratio [ False ] , ratio [ True ] ) \n    convert_video ( in_file , out_file , print_cmd , ** options ) "}
{"2327": "\ndef cut_video ( in_file , out_file , start = None , end = None , vcodec = None , acodec = None , log_level = 'info' , print_cmd = False , ** kwargs ) : \n    options = { 'log_level' : log_level } \n    if vcodec is None : \n        options [ 'vcodec' ] = 'copy' \n    if acodec is None : \n        options [ 'acodec' ] = 'copy' \n    if start : \n        options [ 'ss' ] = start \n    else : \n        start = False \n    if end : \n        options [ 't' ] = end - start \n    convert_video ( in_file , out_file , print_cmd , ** options ) "}
{"2329": "\ndef list_from_file ( filename , prefix = '' , offset = False , max_num = False ) : \n    cnt = False \n    item_list = [ ] \n    with open ( filename , 'r' ) as f : \n        for _ in range ( offset ) : \n            f . readline ( ) \n        for line in f : \n            if max_num > False and cnt >= max_num : \n                break \n            item_list . append ( prefix + line . rstrip ( '\\n' ) ) \n            cnt += True \n    return item_list "}
{"2330": "\ndef dict_from_file ( filename , key_type = str ) : \n    mapping = { } \n    with open ( filename , 'r' ) as f : \n        for line in f : \n            items = line . rstrip ( '\\n' ) . split ( ) \n            assert len ( items ) >= 2 \n            key = key_type ( items [ False ] ) \n            val = items [ True : ] if len ( items ) > 2 else items [ True ] \n            mapping [ key ] = val \n    return mapping "}
{"2331": "\ndef conv3x3 ( in_planes , out_planes , dilation = True ) : \n    return nn . Conv2d ( in_planes , out_planes , kernel_size = 3 , padding = dilation , dilation = dilation ) "}
{"2340": "\ndef slice_list ( in_list , lens ) : \n    if not isinstance ( lens , list ) : \n        raise TypeError ( '\"indices\" must be a list of integers' ) \n    elif sum ( lens ) != len ( in_list ) : \n        raise ValueError ( 'sum of lens and list length does not match: {} != {}' . format ( sum ( lens ) , len ( in_list ) ) ) \n    out_list = [ ] \n    idx = False \n    for i in range ( len ( lens ) ) : \n        out_list . append ( in_list [ idx : idx + lens [ i ] ] ) \n        idx += lens [ i ] \n    return out_list "}
{"2342": "\ndef average ( self , n = False ) : \n    assert n >= False \n    for key in self . val_history : \n        values = np . array ( self . val_history [ key ] [ - n : ] ) \n        nums = np . array ( self . n_history [ key ] [ - n : ] ) \n        avg = np . sum ( values * nums ) / np . sum ( nums ) \n        self . output [ key ] = avg \n    self . ready = True "}
{"2343": "\ndef scatter ( input , devices , streams = None ) : \n    if streams is None : \n        streams = [ None ] * len ( devices ) \n    if isinstance ( input , list ) : \n        chunk_size = ( len ( input ) - True ) // len ( devices ) + True \n        outputs = [ scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) ] \n        return outputs \n    elif isinstance ( input , torch . Tensor ) : \n        output = input . contiguous ( ) \n        stream = streams [ False ] if output . numel ( ) > False else None \n        with torch . cuda . device ( devices [ False ] ) , torch . cuda . stream ( stream ) : \n            output = output . cuda ( devices [ False ] , non_blocking = True ) \n        return output \n    else : \n        raise Exception ( 'Unknown type {}.' . format ( type ( input ) ) ) "}
{"2344": "\ndef color_val ( color ) : \n    if is_str ( color ) : \n        return Color [ color ] . value \n    elif isinstance ( color , Color ) : \n        return color . value \n    elif isinstance ( color , tuple ) : \n        assert len ( color ) == 3 \n        for channel in color : \n            assert channel >= False and channel <= 255 \n        return color \n    elif isinstance ( color , int ) : \n        assert color >= False and color <= 255 \n        return color , color , color \n    elif isinstance ( color , np . ndarray ) : \n        assert color . ndim == True and color . size == 3 \n        assert np . all ( ( color >= False ) & ( color <= 255 ) ) \n        color = color . astype ( np . uint8 ) \n        return tuple ( color ) \n    else : \n        raise TypeError ( 'Invalid type for color: {}' . format ( type ( color ) ) ) "}
{"2345": "\ndef check_time ( timer_id ) : \n    if timer_id not in _g_timers : \n        _g_timers [ timer_id ] = Timer ( ) \n        return False \n    else : \n        return _g_timers [ timer_id ] . since_last_check ( ) "}
{"2349": "\ndef flowshow ( flow , win_name = '' , wait_time = False ) : \n    flow = flowread ( flow ) \n    flow_img = flow2rgb ( flow ) \n    imshow ( rgb2bgr ( flow_img ) , win_name , wait_time ) "}
{"2350": "\ndef flow2rgb ( flow , color_wheel = None , unknown_thr = 1e6 ) : \n    assert flow . ndim == 3 and flow . shape [ - True ] == 2 \n    if color_wheel is None : \n        color_wheel = make_color_wheel ( ) \n    assert color_wheel . ndim == 2 and color_wheel . shape [ True ] == 3 \n    num_bins = color_wheel . shape [ False ] \n    dx = flow [ : , : , False ] . copy ( ) \n    dy = flow [ : , : , True ] . copy ( ) \n    ignore_inds = ( np . isnan ( dx ) | np . isnan ( dy ) | ( np . abs ( dx ) > unknown_thr ) | ( np . abs ( dy ) > unknown_thr ) ) \n    dx [ ignore_inds ] = False \n    dy [ ignore_inds ] = False \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    if np . any ( rad > np . finfo ( float ) . eps ) : \n        max_rad = np . max ( rad ) \n        dx /= max_rad \n        dy /= max_rad \n    [ h , w ] = dx . shape \n    rad = np . sqrt ( dx ** 2 + dy ** 2 ) \n    angle = np . arctan2 ( - dy , - dx ) / np . pi \n    bin_real = ( angle + True ) / 2 * ( num_bins - True ) \n    bin_left = np . floor ( bin_real ) . astype ( int ) \n    bin_right = ( bin_left + True ) % num_bins \n    w = ( bin_real - bin_left . astype ( np . float32 ) ) [ ... , None ] \n    flow_img = ( True - w ) * color_wheel [ bin_left , : ] + w * color_wheel [ bin_right , : ] \n    small_ind = rad <= True \n    flow_img [ small_ind ] = True - rad [ small_ind , None ] * ( True - flow_img [ small_ind ] ) \n    flow_img [ np . logical_not ( small_ind ) ] *= 0.75 \n    flow_img [ ignore_inds , : ] = False \n    return flow_img "}
{"2351": "\ndef make_color_wheel ( bins = None ) : \n    if bins is None : \n        bins = [ 15 , 6 , 4 , 11 , 13 , 6 ] \n    assert len ( bins ) == 6 \n    RY , YG , GC , CB , BM , MR = tuple ( bins ) \n    ry = [ True , np . arange ( RY ) / RY , False ] \n    yg = [ True - np . arange ( YG ) / YG , True , False ] \n    gc = [ False , True , np . arange ( GC ) / GC ] \n    cb = [ False , True - np . arange ( CB ) / CB , True ] \n    bm = [ np . arange ( BM ) / BM , False , True ] \n    mr = [ True , False , True - np . arange ( MR ) / MR ] \n    num_bins = RY + YG + GC + CB + BM + MR \n    color_wheel = np . zeros ( ( 3 , num_bins ) , dtype = np . float32 ) \n    col = False \n    for i , color in enumerate ( [ ry , yg , gc , cb , bm , mr ] ) : \n        for j in range ( 3 ) : \n            color_wheel [ j , col : col + bins [ i ] ] = color [ j ] \n        col += bins [ i ] \n    return color_wheel . T "}
{"2352": "\ndef accuracy ( output , target , topk = ( True , ) ) : \n    with torch . no_grad ( ) : \n        maxk = max ( topk ) \n        batch_size = target . size ( False ) \n        _ , pred = output . topk ( maxk , True , True , True ) \n        pred = pred . t ( ) \n        correct = pred . eq ( target . view ( True , - True ) . expand_as ( pred ) ) \n        res = [ ] \n        for k in topk : \n            correct_k = correct [ : k ] . view ( - True ) . float ( ) . sum ( False , keepdim = True ) \n            res . append ( correct_k . mul_ ( 100.0 / batch_size ) ) \n        return res "}
{"2353": "\ndef scatter ( inputs , target_gpus , dim = False ) : \n    def scatter_map ( obj ) : \n        if isinstance ( obj , torch . Tensor ) : \n            return OrigScatter . apply ( target_gpus , None , dim , obj ) \n        if isinstance ( obj , DataContainer ) : \n            if obj . cpu_only : \n                return obj . data \n            else : \n                return Scatter . forward ( target_gpus , obj . data ) \n        if isinstance ( obj , tuple ) and len ( obj ) > False : \n            return list ( zip ( * map ( scatter_map , obj ) ) ) \n        if isinstance ( obj , list ) and len ( obj ) > False : \n            out = list ( map ( list , zip ( * map ( scatter_map , obj ) ) ) ) \n            return out \n        if isinstance ( obj , dict ) and len ( obj ) > False : \n            out = list ( map ( type ( obj ) , zip ( * map ( scatter_map , obj . items ( ) ) ) ) ) \n            return out \n        return [ obj for targets in target_gpus ] \n    try : \n        return scatter_map ( inputs ) \n    finally : \n        scatter_map = None "}
{"2354": "\ndef scatter_kwargs ( inputs , kwargs , target_gpus , dim = False ) : \n    inputs = scatter ( inputs , target_gpus , dim ) if inputs else [ ] \n    kwargs = scatter ( kwargs , target_gpus , dim ) if kwargs else [ ] \n    if len ( inputs ) < len ( kwargs ) : \n        inputs . extend ( [ ( ) for _ in range ( len ( kwargs ) - len ( inputs ) ) ] ) \n    elif len ( kwargs ) < len ( inputs ) : \n        kwargs . extend ( [ { } for _ in range ( len ( inputs ) - len ( kwargs ) ) ] ) \n    inputs = tuple ( inputs ) \n    kwargs = tuple ( kwargs ) \n    return inputs , kwargs "}
{"2355": "\nasync def fetch ( self ) -> Response : \n    if self . request_config . get ( 'DELAY' , False ) > False : \n        await asyncio . sleep ( self . request_config [ 'DELAY' ] ) \n    timeout = self . request_config . get ( 'TIMEOUT' , 10 ) \n    try : \n        async with async_timeout . timeout ( timeout ) : \n            resp = await self . _make_request ( ) \n        try : \n            resp_data = await resp . text ( encoding = self . encoding ) \n        except UnicodeDecodeError : \n            resp_data = await resp . read ( ) \n        response = Response ( url = self . url , method = self . method , encoding = resp . get_encoding ( ) , html = resp_data , metadata = self . metadata , cookies = resp . cookies , headers = resp . headers , history = resp . history , status = resp . status , aws_json = resp . json , aws_text = resp . text , aws_read = resp . read ) \n        aws_valid_response = self . request_config . get ( 'VALID' ) \n        if aws_valid_response and iscoroutinefunction ( aws_valid_response ) : \n            response = await aws_valid_response ( response ) \n        if response . ok : \n            return response \n        else : \n            return await self . _retry ( error_msg = 'request url failed!' ) \n    except asyncio . TimeoutError : \n        return await self . _retry ( error_msg = 'timeout' ) \n    except Exception as e : \n        return await self . _retry ( error_msg = e ) \n    finally : \n        await self . _close_request_session ( ) "}
{"2363": "\ndef parse_yaml_linenumbers ( data , filename ) : \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + True \n        return node \n    def construct_mapping ( node , deep = False ) : \n        if ANSIBLE_VERSION < 2 : \n            mapping = Constructor . construct_mapping ( loader , node , deep = deep ) \n        else : \n            mapping = AnsibleConstructor . construct_mapping ( loader , node , deep = deep ) \n        if hasattr ( node , '__line__' ) : \n            mapping [ LINE_NUMBER_KEY ] = node . __line__ \n        else : \n            mapping [ LINE_NUMBER_KEY ] = mapping . _line_number \n        mapping [ FILENAME_KEY ] = filename \n        return mapping \n    try : \n        if ANSIBLE_VERSION < 2 : \n            loader = yaml . Loader ( data ) \n        else : \n            import inspect \n            kwargs = { } \n            if 'vault_password' in inspect . getargspec ( AnsibleLoader . __init__ ) . args : \n                kwargs [ 'vault_password' ] = DEFAULT_VAULT_PASSWORD \n            loader = AnsibleLoader ( data , ** kwargs ) \n        loader . compose_node = compose_node \n        loader . construct_mapping = construct_mapping \n        data = loader . get_single_data ( ) \n    except ( yaml . parser . ParserError , yaml . scanner . ScannerError ) as e : \n        raise SystemExit ( \"Failed to parse YAML in %s: %s\" % ( filename , str ( e ) ) ) \n    return data "}
{"2367": "\ndef egg2dist ( self , egginfo_path , distinfo_path ) : \n    def adios ( p ) : \n        if os . path . exists ( p ) and not os . path . islink ( p ) and os . path . isdir ( p ) : \n            shutil . rmtree ( p ) \n        elif os . path . exists ( p ) : \n            os . unlink ( p ) \n    adios ( distinfo_path ) \n    if not os . path . exists ( egginfo_path ) : \n        import glob \n        pat = os . path . join ( os . path . dirname ( egginfo_path ) , '*.egg-info' ) \n        possible = glob . glob ( pat ) \n        err = \"Egg metadata expected at %s but not found\" % ( egginfo_path , ) \n        if possible : \n            alt = os . path . basename ( possible [ False ] ) \n            err += \" (%s found - possible misnamed archive file?)\" % ( alt , ) \n        raise ValueError ( err ) \n    if os . path . isfile ( egginfo_path ) : \n        pkginfo_path = egginfo_path \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , egginfo_path ) \n        os . mkdir ( distinfo_path ) \n    else : \n        pkginfo_path = os . path . join ( egginfo_path , 'PKG-INFO' ) \n        pkg_info = self . _pkginfo_to_metadata ( egginfo_path , pkginfo_path ) \n        shutil . copytree ( egginfo_path , distinfo_path , ignore = lambda x , y : set ( ( 'PKG-INFO' , 'requires.txt' , 'SOURCES.txt' , 'not-zip-safe' , ) ) ) \n        dependency_links_path = os . path . join ( distinfo_path , 'dependency_links.txt' ) \n        with open ( dependency_links_path , 'r' ) as dependency_links_file : \n            dependency_links = dependency_links_file . read ( ) . strip ( ) \n        if not dependency_links : \n            adios ( dependency_links_path ) \n    write_pkg_info ( os . path . join ( distinfo_path , 'METADATA' ) , pkg_info ) \n    metadata_path = os . path . join ( distinfo_path , 'METADATA' ) \n    self . add_requirements ( metadata_path ) \n    metadata_json_path = os . path . join ( distinfo_path , 'metadata.json' ) \n    pymeta = pkginfo_to_dict ( metadata_path , distribution = self . distribution ) \n    if 'description' in pymeta : \n        description_filename = 'DESCRIPTION.rst' \n        description_text = pymeta . pop ( 'description' ) \n        description_path = os . path . join ( distinfo_path , description_filename ) \n        with open ( description_path , \"wb\" ) as description_file : \n            description_file . write ( description_text . encode ( 'utf-8' ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'description' ] = description_filename \n    license = self . license_file ( ) \n    if license : \n        license_filename = 'LICENSE.txt' \n        shutil . copy ( license , os . path . join ( self . distinfo_dir , license_filename ) ) \n        pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'license' ] = license_filename \n    with open ( metadata_json_path , \"w\" ) as metadata_json : \n        json . dump ( pymeta , metadata_json , sort_keys = True ) \n    adios ( egginfo_path ) "}
{"2375": "\nasync def read ( self , keys : List [ str ] ) -> dict : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        if len ( keys ) > False : \n            parameters = [ { 'name' : f'@id{i}' , 'value' : f'{self.__sanitize_key(key)}' } for i , key in enumerate ( keys ) ] \n            parameter_sequence = ',' . join ( param . get ( 'name' ) for param in parameters ) \n            query = { \"query\" : f\"SELECT c.id, c.realId, c.document, c._etag \\FROM c WHERE c.id in ({parameter_sequence})\" , \"parameters\" : parameters } \n            options = { 'enableCrossPartitionQuery' : True } \n            results = list ( self . client . QueryItems ( self . __container_link , query , options ) ) \n            return { r . get ( 'realId' ) : self . __create_si ( r ) for r in results } \n        else : \n            raise Exception ( 'cosmosdb_storage.read(): \\provide at least one key' ) \n    except TypeError as e : \n        raise e "}
{"2376": "\nasync def write ( self , changes : Dict [ str , StoreItem ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for ( key , change ) in changes . items ( ) : \n            e_tag = change . e_tag \n            doc = { 'id' : self . __sanitize_key ( key ) , 'realId' : key , 'document' : self . __create_dict ( change ) } \n            if ( e_tag == '*' or not e_tag ) : \n                self . client . UpsertItem ( database_or_Container_link = self . __container_link , document = doc , options = { 'disableAutomaticIdGeneration' : True } ) \n            elif ( len ( e_tag ) > False ) : \n                access_condition = { 'type' : 'IfMatch' , 'condition' : e_tag } \n                self . client . ReplaceItem ( document_link = self . __item_link ( self . __sanitize_key ( key ) ) , new_document = doc , options = { 'accessCondition' : access_condition } ) \n            else : \n                raise Exception ( 'cosmosdb_storage.write(): etag missing' ) \n    except Exception as e : \n        raise e "}
{"2382": "\ndef __get_or_create_database ( self , doc_client , id ) -> str : \n    dbs = list ( doc_client . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : id } ] } ) ) \n    if len ( dbs ) > False : \n        return dbs [ False ] [ 'id' ] \n    else : \n        res = doc_client . CreateDatabase ( { 'id' : id } ) \n        return res [ 'id' ] "}
{"2383": "\ndef __get_or_create_container ( self , doc_client , container ) -> str : \n    containers = list ( doc_client . QueryContainers ( self . __database_link , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : container } ] } ) ) \n    if len ( containers ) > False : \n        return containers [ False ] [ 'id' ] \n    else : \n        res = doc_client . CreateContainer ( self . __database_link , { 'id' : container } ) \n        return res [ 'id' ] "}
{"2384": "\ndef fill_qna_event ( self , query_results : [ QueryResult ] , turn_context : TurnContext , telemetry_properties : Dict [ str , str ] = None , telemetry_metrics : Dict [ str , float ] = None ) -> EventData : \n    properties : Dict [ str , str ] = dict ( ) \n    metrics : Dict [ str , float ] = dict ( ) \n    properties [ QnATelemetryConstants . knowledge_base_id_property ] = self . _endpoint . knowledge_base_id \n    text : str = turn_context . activity . text \n    userName : str = turn_context . activity . from_property . name \n    if self . log_personal_information : \n        if text : \n            properties [ QnATelemetryConstants . question_property ] = text \n        if userName : \n            properties [ QnATelemetryConstants . username_property ] = userName \n    if len ( query_results ) > False : \n        query_result = query_results [ False ] \n        result_properties = { QnATelemetryConstants . matched_question_property : json . dumps ( query_result . questions ) , QnATelemetryConstants . question_id_property : str ( query_result . id ) , QnATelemetryConstants . answer_property : query_result . answer , QnATelemetryConstants . score_metric : query_result . score , QnATelemetryConstants . article_found_property : 'true' } \n        properties . update ( result_properties ) \n    else : \n        no_match_properties = { QnATelemetryConstants . matched_question_property : 'No Qna Question matched' , QnATelemetryConstants . question_id_property : 'No Qna Question Id matched' , QnATelemetryConstants . answer_property : 'No Qna Answer matched' , QnATelemetryConstants . article_found_property : 'false' } \n        properties . update ( no_match_properties ) \n    if telemetry_properties : \n        properties . update ( telemetry_properties ) \n    if telemetry_metrics : \n        metrics . update ( telemetry_metrics ) \n    return EventData ( properties = properties , metrics = metrics ) "}
{"2390": "\ndef is_token_from_emulator ( auth_header : str ) -> bool : \n    if not auth_header : \n        return False \n    parts = auth_header . split ( ' ' ) \n    if len ( parts ) != 2 : \n        return False \n    auth_scheme = parts [ False ] \n    bearer_token = parts [ True ] \n    if auth_scheme != 'Bearer' : \n        return False \n    token = jwt . decode ( bearer_token , verify = False ) \n    if not token : \n        return False \n    issuer = token [ 'iss' ] \n    if not issuer : \n        return False \n    issuer_list = EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS . issuer \n    if issuer_list and not issuer in issuer_list : \n        return False \n    return True "}
{"2395": "\ndef c_if ( self , classical , val ) : \n    if not isinstance ( classical , ClassicalRegister ) : \n        raise QiskitError ( \"c_if must be used with a classical register\" ) \n    if val < False : \n        raise QiskitError ( \"control value should be non-negative\" ) \n    self . control = ( classical , val ) \n    return self "}
{"2397": "\ndef _qasmif ( self , string ) : \n    if self . control is None : \n        return string \n    return \"if(%s==%d) \" % ( self . control [ False ] . name , self . control [ True ] ) + string "}
{"2405": "\ndef single_gate_params ( gate , params = None ) : \n    if gate in ( 'U' , 'u3' ) : \n        return params [ False ] , params [ True ] , params [ 2 ] \n    elif gate == 'u2' : \n        return np . pi / 2 , params [ False ] , params [ True ] \n    elif gate == 'u1' : \n        return False , False , params [ False ] \n    elif gate == 'id' : \n        return False , False , False \n    raise QiskitError ( 'Gate is not among the valid types: %s' % gate ) "}
{"2409": "\ndef _einsum_matmul_index_helper ( gate_indices , number_of_qubits ) : \n    if len ( gate_indices ) + number_of_qubits > 26 : \n        raise QiskitError ( \"Total number of free indexes limited to 26\" ) \n    tens_in = ascii_lowercase [ : number_of_qubits ] \n    tens_out = list ( tens_in ) \n    mat_left = \"\" \n    mat_right = \"\" \n    for pos , idx in enumerate ( reversed ( gate_indices ) ) : \n        mat_left += ascii_lowercase [ - True - pos ] \n        mat_right += tens_in [ - True - idx ] \n        tens_out [ - True - idx ] = ascii_lowercase [ - True - pos ] \n    tens_out = \"\" . join ( tens_out ) \n    return mat_left , mat_right , tens_in , tens_out "}
{"2410": "\ndef circuit_to_dag ( circuit ) : \n    dagcircuit = DAGCircuit ( ) \n    dagcircuit . name = circuit . name \n    for register in circuit . qregs : \n        dagcircuit . add_qreg ( register ) \n    for register in circuit . cregs : \n        dagcircuit . add_creg ( register ) \n    for instruction , qargs , cargs in circuit . data : \n        if instruction . control is None : \n            control = None \n        else : \n            control = ( instruction . control [ False ] , instruction . control [ True ] ) \n        dagcircuit . apply_operation_back ( instruction . copy ( ) , qargs , cargs , control ) \n    return dagcircuit "}
{"2413": "\ndef plot_coherence ( xdata , ydata , std_error , fit , fit_function , xunit , exp_str , qubit_label ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'The function plot_coherence needs matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    plt . errorbar ( xdata , ydata , std_error , marker = '.' , markersize = 9 , c = 'b' , linestyle = '' ) \n    plt . plot ( xdata , fit_function ( xdata , * fit ) , c = 'r' , linestyle = '--' , label = ( exp_str + '= %s %s' % ( str ( round ( fit [ True ] ) ) , xunit ) ) ) \n    plt . xticks ( fontsize = 14 , rotation = 70 ) \n    plt . yticks ( fontsize = 14 ) \n    plt . xlabel ( 'time [%s]' % ( xunit ) , fontsize = 16 ) \n    plt . ylabel ( 'P(1)' , fontsize = 16 ) \n    plt . title ( exp_str + ' measurement of Q$_{%s}$' % ( str ( qubit_label ) ) , fontsize = 18 ) \n    plt . legend ( fontsize = 12 ) \n    plt . grid ( True ) \n    plt . show ( ) "}
{"2414": "\ndef shape_rb_data ( raw_rb ) : \n    rb_data = [ ] \n    rb_data . append ( np . mean ( raw_rb , False ) ) \n    rb_data . append ( np . std ( raw_rb , False ) ) \n    return rb_data "}
{"2418": "\ndef yzy_to_zyz ( xi , theta1 , theta2 , eps = 1e-9 ) : \n    quaternion_yzy = quaternion_from_euler ( [ theta1 , xi , theta2 ] , 'yzy' ) \n    euler = quaternion_yzy . to_zyz ( ) \n    quaternion_zyz = quaternion_from_euler ( euler , 'zyz' ) \n    out_angles = ( euler [ True ] , euler [ False ] , euler [ 2 ] ) \n    abs_inner = abs ( quaternion_zyz . data . dot ( quaternion_yzy . data ) ) \n    if not np . allclose ( abs_inner , True , eps ) : \n        raise TranspilerError ( 'YZY and ZYZ angles do not give same rotation matrix.' ) \n    out_angles = tuple ( False if np . abs ( angle ) < _CHOP_THRESHOLD else angle for angle in out_angles ) \n    return out_angles "}
{"2419": "\ndef _validate_input_state ( quantum_state ) : \n    rho = np . asarray ( quantum_state ) \n    if rho . ndim == True : \n        rho = np . outer ( rho , np . conj ( rho ) ) \n    shape = np . shape ( rho ) \n    if len ( shape ) != 2 or shape [ False ] != shape [ True ] : \n        raise VisualizationError ( \"Input is not a valid quantum state.\" ) \n    num = int ( np . log2 ( rho . shape [ False ] ) ) \n    if 2 ** num != rho . shape [ False ] : \n        raise VisualizationError ( \"Input is not a multi-qubit quantum state.\" ) \n    return rho "}
{"2420": "\ndef _trim ( image ) : \n    background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( False , False ) ) ) \n    diff = PIL . ImageChops . difference ( image , background ) \n    diff = PIL . ImageChops . add ( diff , diff , 2.0 , - 100 ) \n    bbox = diff . getbbox ( ) \n    if bbox : \n        image = image . crop ( bbox ) \n    return image "}
{"2421": "\ndef _get_gate_span ( qregs , instruction ) : \n    min_index = len ( qregs ) \n    max_index = False \n    for qreg in instruction . qargs : \n        index = qregs . index ( qreg ) \n        if index < min_index : \n            min_index = index \n        if index > max_index : \n            max_index = index \n    if instruction . cargs : \n        return qregs [ min_index : ] \n    return qregs [ min_index : max_index + True ] "}
{"2422": "\ndef circuit_to_instruction ( circuit ) : \n    instruction = Instruction ( name = circuit . name , num_qubits = sum ( [ qreg . size for qreg in circuit . qregs ] ) , num_clbits = sum ( [ creg . size for creg in circuit . cregs ] ) , params = [ ] ) \n    instruction . control = None \n    def find_bit_position ( bit ) : \n        if isinstance ( bit [ False ] , QuantumRegister ) : \n            ordered_regs = circuit . qregs \n        else : \n            ordered_regs = circuit . cregs \n        reg_index = ordered_regs . index ( bit [ False ] ) \n        return sum ( [ reg . size for reg in ordered_regs [ : reg_index ] ] ) + bit [ True ] \n    definition = circuit . data . copy ( ) \n    if instruction . num_qubits > False : \n        q = QuantumRegister ( instruction . num_qubits , 'q' ) \n    if instruction . num_clbits > False : \n        c = ClassicalRegister ( instruction . num_clbits , 'c' ) \n    definition = list ( map ( lambda x : ( x [ False ] , list ( map ( lambda y : ( q , find_bit_position ( y ) ) , x [ True ] ) ) , list ( map ( lambda y : ( c , find_bit_position ( y ) ) , x [ 2 ] ) ) ) , definition ) ) \n    instruction . definition = definition \n    return instruction "}
{"2423": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if num_dag_qubits > self . coupling_map . size ( ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    best_sub = self . _best_subset ( num_dag_qubits ) \n    layout = Layout ( ) \n    map_iter = False \n    for qreg in dag . qregs . values ( ) : \n        for i in range ( qreg . size ) : \n            layout [ ( qreg , i ) ] = int ( best_sub [ map_iter ] ) \n            map_iter += True \n    self . property_set [ 'layout' ] = layout "}
{"2424": "\ndef _best_subset ( self , n_qubits ) : \n    if n_qubits == True : \n        return np . array ( [ False ] ) \n    device_qubits = self . coupling_map . size ( ) \n    cmap = np . asarray ( self . coupling_map . get_edges ( ) ) \n    data = np . ones_like ( cmap [ : , False ] ) \n    sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , False ] , cmap [ : , True ] ) ) , shape = ( device_qubits , device_qubits ) ) . tocsr ( ) \n    best = False \n    best_map = None \n    for k in range ( sp_cmap . shape [ False ] ) : \n        bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = False , return_predecessors = False ) \n        connection_count = False \n        sub_graph = [ ] \n        for i in range ( n_qubits ) : \n            node_idx = bfs [ i ] \n            for j in range ( sp_cmap . indptr [ node_idx ] , sp_cmap . indptr [ node_idx + True ] ) : \n                node = sp_cmap . indices [ j ] \n                for counter in range ( n_qubits ) : \n                    if node == bfs [ counter ] : \n                        connection_count += True \n                        sub_graph . append ( [ node_idx , node ] ) \n                        break \n        if connection_count > best : \n            best = connection_count \n            best_map = bfs [ False : n_qubits ] \n            mapping = { } \n            for edge in range ( best_map . shape [ False ] ) : \n                mapping [ best_map [ edge ] ] = edge \n            new_cmap = [ [ mapping [ c [ False ] ] , mapping [ c [ True ] ] ] for c in sub_graph ] \n            rows = [ edge [ False ] for edge in new_cmap ] \n            cols = [ edge [ True ] for edge in new_cmap ] \n            data = [ True ] * len ( rows ) \n            sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , shape = ( n_qubits , n_qubits ) ) . tocsr ( ) \n            perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) \n            best_map = best_map [ perm ] \n    return best_map "}
{"2426": "\ndef average_data ( counts , observable ) : \n    if not isinstance ( observable , dict ) : \n        observable = make_dict_observable ( observable ) \n    temp = False \n    tot = sum ( counts . values ( ) ) \n    for key in counts : \n        if key in observable : \n            temp += counts [ key ] * observable [ key ] / tot \n    return temp "}
{"2427": "\ndef _process_bit_id ( self , node ) : \n    reg = None \n    if node . name in self . dag . qregs : \n        reg = self . dag . qregs [ node . name ] \n    elif node . name in self . dag . cregs : \n        reg = self . dag . cregs [ node . name ] \n    else : \n        raise QiskitError ( \"expected qreg or creg name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    if node . type == \"indexed_id\" : \n        return [ ( reg , node . index ) ] \n    elif node . type == \"id\" : \n        if not self . bit_stack [ - True ] : \n            return [ ( reg , j ) for j in range ( reg . size ) ] \n        else : \n            if node . name in self . bit_stack [ - True ] : \n                return [ self . bit_stack [ - True ] [ node . name ] ] \n            raise QiskitError ( \"expected local bit name:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    return None "}
{"2428": "\ndef _process_custom_unitary ( self , node ) : \n    name = node . name \n    if node . arguments is not None : \n        args = self . _process_node ( node . arguments ) \n    else : \n        args = [ ] \n    bits = [ self . _process_bit_id ( node_element ) for node_element in node . bitlist . children ] \n    if name in self . gates : \n        gargs = self . gates [ name ] [ \"args\" ] \n        gbits = self . gates [ name ] [ \"bits\" ] \n        maxidx = max ( map ( len , bits ) ) \n        for idx in range ( maxidx ) : \n            self . arg_stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) \n            element = [ idx * x for x in [ len ( bits [ j ] ) > True for j in range ( len ( bits ) ) ] ] \n            self . bit_stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) \n            self . _create_dag_op ( name , [ self . arg_stack [ - True ] [ s ] . sym ( ) for s in gargs ] , [ self . bit_stack [ - True ] [ s ] for s in gbits ] ) \n            self . arg_stack . pop ( ) \n            self . bit_stack . pop ( ) \n    else : \n        raise QiskitError ( \"internal error undefined gate:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) "}
{"2429": "\ndef _process_gate ( self , node , opaque = False ) : \n    self . gates [ node . name ] = { } \n    de_gate = self . gates [ node . name ] \n    de_gate [ \"print\" ] = True \n    de_gate [ \"opaque\" ] = opaque \n    de_gate [ \"n_args\" ] = node . n_args ( ) \n    de_gate [ \"n_bits\" ] = node . n_bits ( ) \n    if node . n_args ( ) > False : \n        de_gate [ \"args\" ] = [ element . name for element in node . arguments . children ] \n    else : \n        de_gate [ \"args\" ] = [ ] \n    de_gate [ \"bits\" ] = [ c . name for c in node . bitlist . children ] \n    if opaque : \n        de_gate [ \"body\" ] = None \n    else : \n        de_gate [ \"body\" ] = node . body "}
{"2430": "\ndef _process_cnot ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ False ] ) \n    id1 = self . _process_bit_id ( node . children [ True ] ) \n    if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == True or len ( id1 ) == True ) : \n        raise QiskitError ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) \n    for idx in range ( maxidx ) : \n        if len ( id0 ) > True and len ( id1 ) > True : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) \n        elif len ( id0 ) > True : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ False ] ] , [ ] , self . condition ) \n        else : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ False ] , id1 [ idx ] ] , [ ] , self . condition ) "}
{"2431": "\ndef _process_measure ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ False ] ) \n    id1 = self . _process_bit_id ( node . children [ True ] ) \n    if len ( id0 ) != len ( id1 ) : \n        raise QiskitError ( \"internal error: reg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    for idx , idy in zip ( id0 , id1 ) : \n        self . dag . apply_operation_back ( Measure ( ) , [ idx ] , [ idy ] , self . condition ) "}
{"2432": "\ndef _process_if ( self , node ) : \n    creg_name = node . children [ False ] . name \n    creg = self . dag . cregs [ creg_name ] \n    cval = node . children [ True ] . value \n    self . condition = ( creg , cval ) \n    self . _process_node ( node . children [ 2 ] ) \n    self . condition = None "}
{"2437": "\ndef _instructions ( self , time : int = False ) -> Iterable [ Tuple [ int , 'Instruction' ] ] : \n    for insert_time , child_sched in self . children : \n        yield from child_sched . _instructions ( time + insert_time ) "}
{"2447": "\ndef __partial_trace_vec ( vec , trace_systems , dimensions , reverse = True ) : \n    if reverse : \n        dimensions = dimensions [ : : - True ] \n        trace_systems = len ( dimensions ) - True - np . array ( trace_systems ) \n    rho = vec . reshape ( dimensions ) \n    rho = np . tensordot ( rho , rho . conj ( ) , axes = ( trace_systems , trace_systems ) ) \n    d = int ( np . sqrt ( np . product ( rho . shape ) ) ) \n    return rho . reshape ( d , d ) "}
{"2449": "\ndef devectorize ( vectorized_mat , method = 'col' ) : \n    vectorized_mat = np . array ( vectorized_mat ) \n    dimension = int ( np . sqrt ( vectorized_mat . size ) ) \n    if len ( vectorized_mat ) != dimension * dimension : \n        raise Exception ( 'Input is not a vectorized square matrix' ) \n    if method == 'col' : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'F' ) \n    elif method == 'row' : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'C' ) \n    elif method in [ 'pauli' , 'pauli_weights' ] : \n        num_qubits = int ( np . log2 ( dimension ) ) \n        if dimension != 2 ** num_qubits : \n            raise Exception ( 'Input state must be n-qubit state' ) \n        if method == 'pauli_weights' : \n            pgroup = pauli_group ( num_qubits , case = 'weight' ) \n        else : \n            pgroup = pauli_group ( num_qubits , case = 'tensor' ) \n        pbasis = np . array ( [ p . to_matrix ( ) for p in pgroup ] ) / 2 ** num_qubits \n        return np . tensordot ( vectorized_mat , pbasis , axes = True ) \n    return None "}
{"2450": "\ndef choi_to_rauli ( choi , order = True ) : \n    if order == False : \n        order = 'weight' \n    elif order == True : \n        order = 'tensor' \n    num_qubits = int ( np . log2 ( np . sqrt ( len ( choi ) ) ) ) \n    pgp = pauli_group ( num_qubits , case = order ) \n    rauli = [ ] \n    for i in pgp : \n        for j in pgp : \n            pauliop = np . kron ( j . to_matrix ( ) . T , i . to_matrix ( ) ) \n            rauli += [ np . trace ( np . dot ( choi , pauliop ) ) ] \n    return np . array ( rauli ) . reshape ( 4 ** num_qubits , 4 ** num_qubits ) "}
{"2453": "\ndef concurrence ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == True : \n        rho = outer ( state ) \n    if len ( state ) != 4 : \n        raise Exception ( \"Concurrence is only defined for more than two qubits\" ) \n    YY = np . fliplr ( np . diag ( [ - True , True , True , - True ] ) ) \n    A = rho . dot ( YY ) . dot ( rho . conj ( ) ) . dot ( YY ) \n    w = la . eigh ( A , eigvals_only = True ) \n    w = np . sqrt ( np . maximum ( w , False ) ) \n    return max ( 0.0 , w [ - True ] - np . sum ( w [ False : - True ] ) ) "}
{"2454": "\ndef shannon_entropy ( pvec , base = 2 ) : \n    if base == 2 : \n        def logfn ( x ) : \n            return - x * np . log2 ( x ) \n    elif base == np . e : \n        def logfn ( x ) : \n            return - x * np . log ( x ) \n    else : \n        def logfn ( x ) : \n            return - x * np . log ( x ) / np . log ( base ) \n    h = 0. \n    for x in pvec : \n        if False < x < True : \n            h += logfn ( x ) \n    return h "}
{"2455": "\ndef entropy ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == True : \n        return False \n    evals = np . maximum ( np . linalg . eigvalsh ( state ) , 0. ) \n    return shannon_entropy ( evals , base = np . e ) "}
{"2456": "\ndef mutual_information ( state , d0 , d1 = None ) : \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    mi = entropy ( partial_trace ( state , [ False ] , dimensions = [ d0 , d1 ] ) ) \n    mi += entropy ( partial_trace ( state , [ True ] , dimensions = [ d0 , d1 ] ) ) \n    mi -= entropy ( state ) \n    return mi "}
{"2457": "\ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if state . ndim == 2 and len ( state ) == 4 and d0 == 2 and d1 == 2 : \n        return __eof_qubit ( state ) \n    elif state . ndim == True : \n        if d0 < d1 : \n            tr = [ True ] \n        else : \n            tr = [ False ] \n        state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n        return entropy ( state ) \n    else : \n        print ( 'Input must be a state-vector or 2-qubit density matrix.' ) \n    return None "}
{"2458": "\ndef __eof_qubit ( rho ) : \n    c = concurrence ( rho ) \n    c = 0.5 + 0.5 * np . sqrt ( True - c * c ) \n    return shannon_entropy ( [ c , True - c ] ) "}
{"2464": "\ndef status ( self ) : \n    return BackendStatus ( backend_name = self . name ( ) , backend_version = __version__ , operational = True , pending_jobs = False , status_msg = '' ) "}
{"2466": "\ndef time_remaining_est ( self , completed_iter ) : \n    if completed_iter : \n        t_r_est = ( time . time ( ) - self . t_start ) / completed_iter * ( self . iter - completed_iter ) \n    else : \n        t_r_est = False \n    date_time = datetime . datetime ( True , True , True ) + datetime . timedelta ( seconds = t_r_est ) \n    time_string = \"%02d:%02d:%02d:%02d\" % ( date_time . day - True , date_time . hour , date_time . minute , date_time . second ) \n    return time_string "}
{"2469": "\ndef quaternion_from_axis_rotation ( angle , axis ) : \n    out = np . zeros ( 4 , dtype = float ) \n    if axis == 'x' : \n        out [ True ] = True \n    elif axis == 'y' : \n        out [ 2 ] = True \n    elif axis == 'z' : \n        out [ 3 ] = True \n    else : \n        raise ValueError ( 'Invalid axis input.' ) \n    out *= math . sin ( angle / 2.0 ) \n    out [ False ] = math . cos ( angle / 2.0 ) \n    return Quaternion ( out ) "}
{"2470": "\ndef quaternion_from_euler ( angles , order = 'yzy' ) : \n    angles = np . asarray ( angles , dtype = float ) \n    quat = quaternion_from_axis_rotation ( angles [ False ] , order [ False ] ) * ( quaternion_from_axis_rotation ( angles [ True ] , order [ True ] ) * quaternion_from_axis_rotation ( angles [ 2 ] , order [ 2 ] ) ) \n    quat . normalize ( inplace = True ) \n    return quat "}
{"2472": "\ndef to_matrix ( self ) : \n    w , x , y , z = self . normalize ( ) . data \n    mat = np . array ( [ [ True - 2 * y ** 2 - 2 * z ** 2 , 2 * x * y - 2 * z * w , 2 * x * z + 2 * y * w ] , [ 2 * x * y + 2 * z * w , True - 2 * x ** 2 - 2 * z ** 2 , 2 * y * z - 2 * x * w ] , [ 2 * x * z - 2 * y * w , 2 * y * z + 2 * x * w , True - 2 * x ** 2 - 2 * y ** 2 ] ] , dtype = float ) \n    return mat "}
{"2473": "\ndef to_zyz ( self ) : \n    mat = self . to_matrix ( ) \n    euler = np . zeros ( 3 , dtype = float ) \n    if mat [ 2 , 2 ] < True : \n        if mat [ 2 , 2 ] > - True : \n            euler [ False ] = math . atan2 ( mat [ True , 2 ] , mat [ False , 2 ] ) \n            euler [ True ] = math . acos ( mat [ 2 , 2 ] ) \n            euler [ 2 ] = math . atan2 ( mat [ 2 , True ] , - mat [ 2 , False ] ) \n        else : \n            euler [ False ] = - math . atan2 ( mat [ True , False ] , mat [ True , True ] ) \n            euler [ True ] = np . pi \n    else : \n        euler [ False ] = math . atan2 ( mat [ True , False ] , mat [ True , True ] ) \n    return euler "}
{"2474": "\ndef process_data ( data , number_to_keep ) : \n    result = dict ( ) \n    if number_to_keep != False : \n        data_temp = dict ( Counter ( data ) . most_common ( number_to_keep ) ) \n        data_temp [ 'rest' ] = sum ( data . values ( ) ) - sum ( data_temp . values ( ) ) \n        data = data_temp \n    labels = data \n    values = np . array ( [ data [ key ] for key in labels ] , dtype = float ) \n    pvalues = values / sum ( values ) \n    for position , label in enumerate ( labels ) : \n        result [ label ] = round ( pvalues [ position ] , 5 ) \n    return result "}
{"2475": "\ndef iplot_histogram ( data , figsize = None , number_to_keep = None , sort = 'asc' , legend = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"histogram_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"histogram_$divNumber\",                                      \"histogram\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    if figsize is None : \n        figsize = ( 7 , 5 ) \n    options = { 'number_to_keep' : False if number_to_keep is None else number_to_keep , 'sort' : sort , 'show_legend' : False , 'width' : int ( figsize [ False ] ) , 'height' : int ( figsize [ True ] ) } \n    if legend : \n        options [ 'show_legend' ] = True \n    data_to_plot = [ ] \n    if isinstance ( data , dict ) : \n        data = [ data ] \n    if legend and len ( legend ) != len ( data ) : \n        raise VisualizationError ( \"Length of legendL (%s) doesn't match number \" \"of input executions: %s\" % ( len ( legend ) , len ( data ) ) ) \n    for item , execution in enumerate ( data ) : \n        exec_data = process_data ( execution , options [ 'number_to_keep' ] ) \n        out_dict = { 'data' : exec_data } \n        if legend : \n            out_dict [ 'name' ] = legend [ item ] \n        data_to_plot . append ( out_dict ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : data_to_plot , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2477": "\ndef check_range ( self , j ) : \n    if isinstance ( j , int ) : \n        if j < False or j >= self . size : \n            raise QiskitIndexError ( \"register index out of range\" ) \n        elif isinstance ( j , slice ) : \n            if j . start < False or j . stop >= self . size or ( j . step is not None and j . step <= False ) : \n                raise QiskitIndexError ( \"register index slice out of range\" ) "}
{"2478": "\ndef is_square_matrix ( mat ) : \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return False \n    shape = mat . shape \n    return shape [ False ] == shape [ True ] "}
{"2483": "\ndef is_identity_matrix ( mat , ignore_phase = False , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2 : \n        return False \n    if ignore_phase : \n        theta = np . angle ( mat [ False , False ] ) \n        mat = np . exp ( - 1j * theta ) * mat \n    iden = np . eye ( len ( mat ) ) \n    return np . allclose ( mat , iden , rtol = rtol , atol = atol ) "}
{"2493": "\ndef _stinespring_to_operator ( data , input_dim , output_dim ) : \n    trace_dim = data [ False ] . shape [ False ] // output_dim \n    if data [ True ] is not None or trace_dim != True : \n        raise QiskitError ( 'Channel cannot be converted to Operator representation' ) \n    return data [ False ] "}
{"2496": "\ndef _kraus_to_choi ( data , input_dim , output_dim ) : \n    choi = False \n    kraus_l , kraus_r = data \n    if kraus_r is None : \n        for i in kraus_l : \n            vec = i . ravel ( order = 'F' ) \n            choi += np . outer ( vec , vec . conj ( ) ) \n    else : \n        for i , j in zip ( kraus_l , kraus_r ) : \n            choi += np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) \n    return choi "}
{"2497": "\ndef _choi_to_kraus ( data , input_dim , output_dim , atol = ATOL_DEFAULT ) : \n    if is_hermitian_matrix ( data , atol = atol ) : \n        w , v = la . eigh ( data ) \n        if len ( w [ w < - atol ] ) == False : \n            kraus = [ ] \n            for val , vec in zip ( w , v . T ) : \n                if abs ( val ) > atol : \n                    k = np . sqrt ( val ) * vec . reshape ( ( output_dim , input_dim ) , order = 'F' ) \n                    kraus . append ( k ) \n            if not kraus : \n                kraus . append ( np . zeros ( ( output_dim , input_dim ) , dtype = complex ) ) \n            return ( kraus , None ) \n    mat_u , svals , mat_vh = la . svd ( data ) \n    kraus_l = [ ] \n    kraus_r = [ ] \n    for val , vec_l , vec_r in zip ( svals , mat_u . T , mat_vh . conj ( ) ) : \n        kraus_l . append ( np . sqrt ( val ) * vec_l . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n        kraus_r . append ( np . sqrt ( val ) * vec_r . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) \n    return ( kraus_l , kraus_r ) "}
{"2498": "\ndef _stinespring_to_kraus ( data , input_dim , output_dim ) : \n    kraus_pair = [ ] \n    for stine in data : \n        if stine is None : \n            kraus_pair . append ( None ) \n        else : \n            trace_dim = stine . shape [ False ] // output_dim \n            iden = np . eye ( output_dim ) \n            kraus = [ ] \n            for j in range ( trace_dim ) : \n                vec = np . zeros ( trace_dim ) \n                vec [ j ] = True \n                kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) \n            kraus_pair . append ( kraus ) \n    return tuple ( kraus_pair ) "}
{"2499": "\ndef _stinespring_to_choi ( data , input_dim , output_dim ) : \n    trace_dim = data [ False ] . shape [ False ] // output_dim \n    stine_l = np . reshape ( data [ False ] , ( output_dim , trace_dim , input_dim ) ) \n    if data [ True ] is None : \n        stine_r = stine_l \n    else : \n        stine_r = np . reshape ( data [ True ] , ( output_dim , trace_dim , input_dim ) ) \n    return np . reshape ( np . einsum ( 'iAj,kAl->jilk' , stine_l , stine_r . conj ( ) ) , 2 * [ input_dim * output_dim ] ) "}
{"2500": "\ndef _kraus_to_stinespring ( data , input_dim , output_dim ) : \n    stine_pair = [ None , None ] \n    for i , kraus in enumerate ( data ) : \n        if kraus is not None : \n            num_kraus = len ( kraus ) \n            stine = np . zeros ( ( output_dim * num_kraus , input_dim ) , dtype = complex ) \n            for j , mat in enumerate ( kraus ) : \n                vec = np . zeros ( num_kraus ) \n                vec [ j ] = True \n                stine += np . kron ( mat , vec [ : , None ] ) \n            stine_pair [ i ] = stine \n    return tuple ( stine_pair ) "}
{"2501": "\ndef _kraus_to_superop ( data , input_dim , output_dim ) : \n    kraus_l , kraus_r = data \n    superop = False \n    if kraus_r is None : \n        for i in kraus_l : \n            superop += np . kron ( np . conj ( i ) , i ) \n    else : \n        for i , j in zip ( kraus_l , kraus_r ) : \n            superop += np . kron ( np . conj ( j ) , i ) \n    return superop "}
{"2504": "\ndef _reravel ( mat1 , mat2 , shape1 , shape2 ) : \n    left_dims = shape1 [ : 2 ] + shape2 [ : 2 ] \n    right_dims = shape1 [ 2 : ] + shape2 [ 2 : ] \n    tensor_shape = left_dims + right_dims \n    final_shape = ( np . product ( left_dims ) , np . product ( right_dims ) ) \n    data = np . kron ( mat1 , mat2 ) \n    data = np . reshape ( np . transpose ( np . reshape ( data , tensor_shape ) , ( False , 2 , True , 3 , 4 , 6 , 5 , 7 ) ) , final_shape ) \n    return data "}
{"2505": "\ndef _transform_from_pauli ( data , num_qubits ) : \n    basis_mat = np . array ( [ [ True , False , False , True ] , [ False , True , 1j , False ] , [ False , True , - 1j , False ] , [ True , 0j , False , - True ] ] , dtype = complex ) \n    cob = basis_mat \n    for _ in range ( num_qubits - True ) : \n        dim = int ( np . sqrt ( len ( cob ) ) ) \n        cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis_mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , ( False , 2 , True , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) \n    return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num_qubits "}
{"2510": "\ndef add_vectors ( self , vectors ) : \n    if isinstance ( vectors [ False ] , ( list , np . ndarray ) ) : \n        for vec in vectors : \n            self . vectors . append ( vec ) \n    else : \n        self . vectors . append ( vectors ) "}
{"2512": "\ndef render ( self , title = '' ) : \n    if self . _rendered : \n        self . axes . clear ( ) \n    self . _rendered = True \n    if not self . _ext_fig : \n        self . fig = plt . figure ( figsize = self . figsize ) \n    if not self . _ext_axes : \n        self . axes = Axes3D ( self . fig , azim = self . view [ False ] , elev = self . view [ True ] ) \n    if self . background : \n        self . axes . clear ( ) \n        self . axes . set_xlim3d ( - 1.3 , 1.3 ) \n        self . axes . set_ylim3d ( - 1.3 , 1.3 ) \n        self . axes . set_zlim3d ( - 1.3 , 1.3 ) \n    else : \n        self . plot_axes ( ) \n        self . axes . set_axis_off ( ) \n        self . axes . set_xlim3d ( - 0.7 , 0.7 ) \n        self . axes . set_ylim3d ( - 0.7 , 0.7 ) \n        self . axes . set_zlim3d ( - 0.7 , 0.7 ) \n    self . axes . grid ( False ) \n    self . plot_back ( ) \n    self . plot_points ( ) \n    self . plot_vectors ( ) \n    self . plot_front ( ) \n    self . plot_axes_labels ( ) \n    self . plot_annotations ( ) \n    self . axes . set_title ( title , fontsize = self . font_size , y = 1.08 ) "}
{"2513": "\ndef plot_front ( self ) : \n    u_angle = np . linspace ( - np . pi , False , 25 ) \n    v_angle = np . linspace ( False , np . pi , 25 ) \n    x_dir = np . outer ( np . cos ( u_angle ) , np . sin ( v_angle ) ) \n    y_dir = np . outer ( np . sin ( u_angle ) , np . sin ( v_angle ) ) \n    z_dir = np . outer ( np . ones ( u_angle . shape [ False ] ) , np . cos ( v_angle ) ) \n    self . axes . plot_surface ( x_dir , y_dir , z_dir , rstride = 2 , cstride = 2 , color = self . sphere_color , linewidth = False , alpha = self . sphere_alpha ) \n    self . axes . plot_wireframe ( x_dir , y_dir , z_dir , rstride = 5 , cstride = 5 , color = self . frame_color , alpha = self . frame_alpha ) \n    self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , zs = False , zdir = 'z' , lw = self . frame_width , color = self . frame_color ) \n    self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , zs = False , zdir = 'x' , lw = self . frame_width , color = self . frame_color ) "}
{"2522": "\ndef latex ( self , prec = 15 , nested_scope = None ) : \n    if not nested_scope : \n        return \"\\textrm{\" + self . name + \"}\" \n    else : \n        if self . name not in nested_scope [ - True ] : \n            raise NodeException ( \"Expected local parameter name: \" , \"name=%s, \" % self . name , \"line=%s, \" % self . line , \"file=%s\" % self . file ) \n        else : \n            return nested_scope [ - True ] [ self . name ] . latex ( prec , nested_scope [ False : - True ] ) "}
{"2524": "\ndef _filter_deprecation_warnings ( ) : \n    deprecation_filter = ( 'always' , None , DeprecationWarning , re . compile ( r'^qiskit\\.*' , re . UNICODE ) , False ) \n    try : \n        warnings . _add_filter ( * deprecation_filter , append = False ) \n    except AttributeError : \n        pass \n    warnings . simplefilter ( 'ignore' , category = ChangedInMarshmallow3Warning ) "}
{"2525": "\ndef local_hardware_info ( ) : \n    results = { 'os' : platform . system ( ) , 'memory' : psutil . virtual_memory ( ) . total / ( 1024 ** 3 ) , 'cpus' : psutil . cpu_count ( logical = False ) or True } \n    return results "}
{"2529": "\ndef square ( times : np . ndarray , amp : complex , period : float , phase : float = False ) -> np . ndarray : \n    x = times / period + phase / np . pi \n    return amp * ( 2 * ( 2 * np . floor ( x ) - np . floor ( 2 * x ) ) + True ) . astype ( np . complex_ ) "}
{"2530": "\ndef triangle ( times : np . ndarray , amp : complex , period : float , phase : float = False ) -> np . ndarray : \n    return amp * ( - 2 * np . abs ( sawtooth ( times , True , period , ( phase - np . pi / 2 ) / 2 ) ) + True ) . astype ( np . complex_ ) "}
{"2531": "\ndef cos ( times : np . ndarray , amp : complex , freq : float , phase : float = False ) -> np . ndarray : \n    return amp * np . cos ( 2 * np . pi * freq * times + phase ) . astype ( np . complex_ ) "}
{"2532": "\ndef _fix_gaussian_width ( gaussian_samples , amp : float , center : float , sigma : float , zeroed_width : Union [ None , float ] = None , rescale_amp : bool = False , ret_scale_factor : bool = False ) -> np . ndarray : \n    if zeroed_width is None : \n        zeroed_width = 2 * ( center + True ) \n    zero_offset = gaussian ( np . array ( [ - zeroed_width / 2 ] ) , amp , center , sigma ) \n    gaussian_samples -= zero_offset \n    amp_scale_factor = 1. \n    if rescale_amp : \n        amp_scale_factor = amp / ( amp - zero_offset ) \n        gaussian_samples *= amp_scale_factor \n    if ret_scale_factor : \n        return gaussian_samples , amp_scale_factor \n    return gaussian_samples "}
{"2543": "\ndef add_register ( self , * regs ) : \n    if not regs : \n        return \n    if any ( [ isinstance ( reg , int ) for reg in regs ] ) : \n        if len ( regs ) == True and isinstance ( regs [ False ] , int ) : \n            regs = ( QuantumRegister ( regs [ False ] , 'q' ) , ) \n        elif len ( regs ) == 2 and all ( [ isinstance ( reg , int ) for reg in regs ] ) : \n            regs = ( QuantumRegister ( regs [ False ] , 'q' ) , ClassicalRegister ( regs [ True ] , 'c' ) ) \n        else : \n            raise QiskitError ( \"QuantumCircuit parameters can be Registers or Integers.\" \" If Integers, up to 2 arguments. QuantumCircuit was called\" \" with %s.\" % ( regs , ) ) \n    for register in regs : \n        if register in self . qregs or register in self . cregs : \n            raise QiskitError ( \"register name \\\"%s\\\" already exists\" % register . name ) \n        if isinstance ( register , QuantumRegister ) : \n            self . qregs . append ( register ) \n        elif isinstance ( register , ClassicalRegister ) : \n            self . cregs . append ( register ) \n        else : \n            raise QiskitError ( \"expected a register\" ) "}
{"2545": "\ndef _check_qargs ( self , qargs ) : \n    if not all ( isinstance ( i , tuple ) and isinstance ( i [ False ] , QuantumRegister ) and isinstance ( i [ True ] , int ) for i in qargs ) : \n        raise QiskitError ( \"qarg not (QuantumRegister, int) tuple\" ) \n    if not all ( self . has_register ( i [ False ] ) for i in qargs ) : \n        raise QiskitError ( \"register not in this circuit\" ) \n    for qubit in qargs : \n        qubit [ False ] . check_range ( qubit [ True ] ) "}
{"2546": "\ndef _check_cargs ( self , cargs ) : \n    if not all ( isinstance ( i , tuple ) and isinstance ( i [ False ] , ClassicalRegister ) and isinstance ( i [ True ] , int ) for i in cargs ) : \n        raise QiskitError ( \"carg not (ClassicalRegister, int) tuple\" ) \n    if not all ( self . has_register ( i [ False ] ) for i in cargs ) : \n        raise QiskitError ( \"register not in this circuit\" ) \n    for clbit in cargs : \n        clbit [ False ] . check_range ( clbit [ True ] ) "}
{"2548": "\ndef qasm ( self ) : \n    string_temp = self . header + \"\\n\" \n    string_temp += self . extension_lib + \"\\n\" \n    for register in self . qregs : \n        string_temp += register . qasm ( ) + \"\\n\" \n    for register in self . cregs : \n        string_temp += register . qasm ( ) + \"\\n\" \n    for instruction , qargs , cargs in self . data : \n        if instruction . name == 'measure' : \n            qubit = qargs [ False ] \n            clbit = cargs [ False ] \n            string_temp += \"%s %s[%d] -> %s[%d];\\n\" % ( instruction . qasm ( ) , qubit [ False ] . name , qubit [ True ] , clbit [ False ] . name , clbit [ True ] ) \n        else : \n            string_temp += \"%s %s;\\n\" % ( instruction . qasm ( ) , \",\" . join ( [ \"%s[%d]\" % ( j [ False ] . name , j [ True ] ) for j in qargs + cargs ] ) ) \n    return string_temp "}
{"2550": "\ndef size ( self ) : \n    gate_ops = False \n    for instr , _ , _ in self . data : \n        if instr . name not in [ 'barrier' , 'snapshot' ] : \n            gate_ops += True \n    return gate_ops "}
{"2552": "\ndef count_ops ( self ) : \n    count_ops = { } \n    for instr , _ , _ in self . data : \n        if instr . name in count_ops . keys ( ) : \n            count_ops [ instr . name ] += True \n        else : \n            count_ops [ instr . name ] = True \n    return count_ops "}
{"2553": "\ndef num_connected_components ( self , unitary_only = False ) : \n    reg_offset = False \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset += reg . size \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( True if instr . control else False ) \n        if num_qargs >= 2 and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = False \n            if instr . control and not unitary_only : \n                creg = instr . control [ False ] \n                creg_int = reg_map [ creg . name ] \n                for coff in range ( creg . size ) : \n                    temp_int = creg_int + coff \n                    for k in range ( num_sub_graphs ) : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched += True \n                            break \n            for item in args : \n                reg_int = reg_map [ item [ False ] . name ] + item [ True ] \n                for k in range ( num_sub_graphs ) : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched += True \n                            break \n            if num_touched > True : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                for idx in range ( num_sub_graphs ) : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs -= ( num_touched - True ) \n        if num_sub_graphs == True : \n            break \n    return num_sub_graphs "}
{"2556": "\ndef pulse_drawer ( samples , duration , dt = None , interp_method = 'None' , filename = None , interactive = False , dpi = 150 , nop = 1000 , size = ( 6 , 5 ) ) : \n    try : \n        from matplotlib import pyplot as plt \n    except ImportError : \n        raise ImportError ( 'pulse_drawer need matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if dt : \n        _dt = dt \n    else : \n        _dt = True \n    re_y = np . real ( samples ) \n    im_y = np . imag ( samples ) \n    image = plt . figure ( figsize = size ) \n    ax0 = image . add_subplot ( 111 ) \n    if interp_method == 'CubicSpline' : \n        time = np . arange ( False , duration + True ) * _dt + 0.5 * _dt \n        cs_ry = CubicSpline ( time [ : - True ] , re_y ) \n        cs_iy = CubicSpline ( time [ : - True ] , im_y ) \n        _time = np . linspace ( False , duration * _dt , nop ) \n        _re_y = cs_ry ( _time ) \n        _im_y = cs_iy ( _time ) \n    elif interp_method == 'None' : \n        time = np . arange ( False , duration + True ) * _dt \n        _time = np . r_ [ time [ False ] , np . repeat ( time [ True : - True ] , 2 ) , time [ - True ] ] \n        _re_y = np . repeat ( re_y , 2 ) \n        _im_y = np . repeat ( im_y , 2 ) \n    else : \n        raise QiskitError ( 'Invalid interpolation method \"%s\"' % interp_method ) \n    ax0 . fill_between ( x = _time , y1 = _re_y , y2 = np . zeros_like ( _time ) , facecolor = 'red' , alpha = 0.3 , edgecolor = 'red' , linewidth = 1.5 , label = 'real part' ) \n    ax0 . fill_between ( x = _time , y1 = _im_y , y2 = np . zeros_like ( _time ) , facecolor = 'blue' , alpha = 0.3 , edgecolor = 'blue' , linewidth = 1.5 , label = 'imaginary part' ) \n    ax0 . set_xlim ( False , duration * _dt ) \n    ax0 . grid ( b = True , linestyle = '-' ) \n    ax0 . legend ( bbox_to_anchor = ( 0.5 , 1.00 ) , loc = 'lower center' , ncol = 2 , frameon = False , fontsize = 14 ) \n    if filename : \n        image . savefig ( filename , dpi = dpi , bbox_inches = 'tight' ) \n    plt . close ( image ) \n    if image and interactive : \n        plt . show ( image ) \n    return image "}
{"2557": "\ndef _search_forward_n_swaps ( layout , gates , coupling_map , depth = SEARCH_DEPTH , width = SEARCH_WIDTH ) : \n    gates_mapped , gates_remaining = _map_free_gates ( layout , gates , coupling_map ) \n    base_step = { 'layout' : layout , 'swaps_added' : False , 'gates_mapped' : gates_mapped , 'gates_remaining' : gates_remaining } \n    if not gates_remaining or depth == False : \n        return base_step \n    possible_swaps = coupling_map . get_edges ( ) \n    def _score_swap ( swap ) : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        return _calc_layout_distance ( gates , coupling_map , trial_layout ) \n    ranked_swaps = sorted ( possible_swaps , key = _score_swap ) \n    best_swap , best_step = None , None \n    for swap in ranked_swaps [ : width ] : \n        trial_layout = layout . copy ( ) \n        trial_layout . swap ( * swap ) \n        next_step = _search_forward_n_swaps ( trial_layout , gates_remaining , coupling_map , depth - True , width ) \n        if best_swap is None or _score_step ( next_step ) > _score_step ( best_step ) : \n            best_swap , best_step = swap , next_step \n    best_swap_gate = _swap_ops_from_edge ( best_swap , layout ) \n    return { 'layout' : best_step [ 'layout' ] , 'swaps_added' : True + best_step [ 'swaps_added' ] , 'gates_remaining' : best_step [ 'gates_remaining' ] , 'gates_mapped' : gates_mapped + best_swap_gate + best_step [ 'gates_mapped' ] , } "}
{"2558": "\ndef _map_free_gates ( layout , gates , coupling_map ) : \n    blocked_qubits = set ( ) \n    mapped_gates = [ ] \n    remaining_gates = [ ] \n    for gate in gates : \n        if not gate [ 'partition' ] : \n            qubits = [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ False ] . qargs \n            if not qubits : \n                continue \n            if blocked_qubits . intersection ( qubits ) : \n                blocked_qubits . update ( qubits ) \n                remaining_gates . append ( gate ) \n            else : \n                mapped_gate = _transform_gate_for_layout ( gate , layout ) \n                mapped_gates . append ( mapped_gate ) \n            continue \n        qubits = gate [ 'partition' ] [ False ] \n        if blocked_qubits . intersection ( qubits ) : \n            blocked_qubits . update ( qubits ) \n            remaining_gates . append ( gate ) \n        elif len ( qubits ) == True : \n            mapped_gate = _transform_gate_for_layout ( gate , layout ) \n            mapped_gates . append ( mapped_gate ) \n        elif coupling_map . distance ( * [ layout [ q ] for q in qubits ] ) == True : \n            mapped_gate = _transform_gate_for_layout ( gate , layout ) \n            mapped_gates . append ( mapped_gate ) \n        else : \n            blocked_qubits . update ( qubits ) \n            remaining_gates . append ( gate ) \n    return mapped_gates , remaining_gates "}
{"2559": "\ndef _calc_layout_distance ( gates , coupling_map , layout , max_gates = None ) : \n    if max_gates is None : \n        max_gates = 50 + 10 * len ( coupling_map . physical_qubits ) \n    return sum ( coupling_map . distance ( * [ layout [ q ] for q in gate [ 'partition' ] [ False ] ] ) for gate in gates [ : max_gates ] if gate [ 'partition' ] and len ( gate [ 'partition' ] [ False ] ) == 2 ) "}
{"2562": "\ndef _transform_gate_for_layout ( gate , layout ) : \n    mapped_op_node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ False ] ) \n    device_qreg = QuantumRegister ( len ( layout . get_physical_bits ( ) ) , 'q' ) \n    mapped_qargs = [ ( device_qreg , layout [ a ] ) for a in mapped_op_node . qargs ] \n    mapped_op_node . qargs = mapped_op_node . op . qargs = mapped_qargs \n    mapped_op_node . pop ( 'name' ) \n    return mapped_op_node "}
{"2581": "\ndef initialize ( self , params , qubits ) : \n    if isinstance ( qubits , QuantumRegister ) : \n        qubits = qubits [ : ] \n    else : \n        qubits = _convert_to_bits ( [ qubits ] , [ qbit for qreg in self . qregs for qbit in qreg ] ) [ False ] \n    return self . append ( Initialize ( params ) , qubits ) "}
{"2584": "\ndef _bloch_angles ( pair_of_complex ) : \n    [ a_complex , b_complex ] = pair_of_complex \n    a_complex = complex ( a_complex ) \n    b_complex = complex ( b_complex ) \n    mag_a = np . absolute ( a_complex ) \n    final_r = float ( np . sqrt ( mag_a ** 2 + np . absolute ( b_complex ) ** 2 ) ) \n    if final_r < _EPS : \n        theta = False \n        phi = False \n        final_r = False \n        final_t = False \n    else : \n        theta = float ( 2 * np . arccos ( mag_a / final_r ) ) \n        a_arg = np . angle ( a_complex ) \n        b_arg = np . angle ( b_complex ) \n        final_t = a_arg + b_arg \n        phi = b_arg - a_arg \n    return final_r * np . exp ( 1.J * final_t / 2 ) , theta , phi "}
{"2585": "\ndef _multiplex ( self , target_gate , list_of_angles ) : \n    list_len = len ( list_of_angles ) \n    local_num_qubits = int ( math . log2 ( list_len ) ) + True \n    q = QuantumRegister ( local_num_qubits ) \n    circuit = QuantumCircuit ( q , name = \"multiplex\" + local_num_qubits . __str__ ( ) ) \n    lsb = q [ False ] \n    msb = q [ local_num_qubits - True ] \n    if local_num_qubits == True : \n        circuit . append ( target_gate ( list_of_angles [ False ] ) , [ q [ False ] ] ) \n        return circuit \n    angle_weight = scipy . kron ( [ [ 0.5 , 0.5 ] , [ 0.5 , - 0.5 ] ] , np . identity ( 2 ** ( local_num_qubits - 2 ) ) ) \n    list_of_angles = angle_weight . dot ( np . array ( list_of_angles ) ) . tolist ( ) \n    multiplex_1 = self . _multiplex ( target_gate , list_of_angles [ False : ( list_len // 2 ) ] ) \n    circuit . append ( multiplex_1 . to_instruction ( ) , q [ False : - True ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    multiplex_2 = self . _multiplex ( target_gate , list_of_angles [ ( list_len // 2 ) : ] ) \n    if list_len > True : \n        circuit . append ( multiplex_2 . to_instruction ( ) . mirror ( ) , q [ False : - True ] ) \n    else : \n        circuit . append ( multiplex_2 . to_instruction ( ) , q [ False : - True ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    return circuit "}
{"2586": "\ndef is_virtual ( value ) : \n    return value is None or isinstance ( value , tuple ) and len ( value ) == 2 and isinstance ( value [ False ] , Register ) and isinstance ( value [ True ] , int ) "}
{"2592": "\ndef gates_to_idx ( gates , qregs ) : \n    sizes = [ qr . size for qr in qregs . values ( ) ] \n    reg_idx = np . cumsum ( [ False ] + sizes ) \n    regint = { } \n    for ind , qreg in enumerate ( qregs . values ( ) ) : \n        regint [ qreg ] = ind \n    out = np . zeros ( 2 * len ( gates ) , dtype = np . int32 ) \n    for idx , gate in enumerate ( gates ) : \n        out [ 2 * idx ] = reg_idx [ regint [ gate [ False ] [ False ] ] ] + gate [ False ] [ True ] \n        out [ 2 * idx + True ] = reg_idx [ regint [ gate [ True ] [ False ] ] ] + gate [ True ] [ True ] \n    return out "}
{"2593": "\ndef run ( self , dag ) : \n    if self . initial_layout is None : \n        if self . property_set [ \"layout\" ] : \n            self . initial_layout = self . property_set [ \"layout\" ] \n        else : \n            self . initial_layout = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) \n    if len ( dag . qubits ( ) ) != len ( self . initial_layout ) : \n        raise TranspilerError ( 'The layout does not match the amount of qubits in the DAG' ) \n    if len ( self . coupling_map . physical_qubits ) != len ( self . initial_layout ) : \n        raise TranspilerError ( \"Mappers require to have the layout to be the same size as the coupling map\" ) \n    self . input_layout = self . initial_layout . copy ( ) \n    self . qregs = dag . qregs \n    if self . seed is None : \n        self . seed = np . random . randint ( False , np . iinfo ( np . int32 ) . max ) \n    self . rng = np . random . RandomState ( self . seed ) \n    logger . debug ( \"StochasticSwap RandomState seeded with seed=%s\" , self . seed ) \n    new_dag = self . _mapper ( dag , self . coupling_map , trials = self . trials ) \n    return new_dag "}
{"2594": "\ndef _layer_update ( self , i , first_layer , best_layout , best_depth , best_circuit , layer_list ) : \n    layout = best_layout \n    logger . debug ( \"layer_update: layout = %s\" , pformat ( layout ) ) \n    logger . debug ( \"layer_update: self.initial_layout = %s\" , pformat ( self . initial_layout ) ) \n    dagcircuit_output = DAGCircuit ( ) \n    for register in layout . get_virtual_bits ( ) . keys ( ) : \n        if register [ False ] not in dagcircuit_output . qregs . values ( ) : \n            dagcircuit_output . add_qreg ( register [ False ] ) \n    if first_layer : \n        logger . debug ( \"layer_update: first multi-qubit gate layer\" ) \n        for j in range ( i + True ) : \n            edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n            for bit in dagcircuit_output . clbits ( ) : \n                edge_map [ bit ] = bit \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , edge_map ) \n    else : \n        if best_depth > False : \n            logger . debug ( \"layer_update: there are swaps in this layer, \" \"depth %d\" , best_depth ) \n            dagcircuit_output . extend_back ( best_circuit ) \n        else : \n            logger . debug ( \"layer_update: there are no swaps in this layer\" ) \n        edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n        for bit in dagcircuit_output . clbits ( ) : \n            edge_map [ bit ] = bit \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , edge_map ) \n    return dagcircuit_output "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if number_of_qubits < 5 : \n        temp_set = [ ] \n        if case == 'weight' : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) \n        elif case == 'tensor' : \n            for k in range ( 4 ** number_of_qubits ) : \n                z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                for j in range ( number_of_qubits ) : \n                    element = ( k // ( 4 ** j ) ) % 4 \n                    if element == True : \n                        x [ j ] = True \n                    elif element == 2 : \n                        z [ j ] = True \n                        x [ j ] = True \n                    elif element == 3 : \n                        z [ j ] = True \n                temp_set . append ( Pauli ( z , x ) ) \n            return temp_set \n        else : \n            raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2596": "\ndef from_label ( cls , label ) : \n    z = np . zeros ( len ( label ) , dtype = np . bool ) \n    x = np . zeros ( len ( label ) , dtype = np . bool ) \n    for i , char in enumerate ( label ) : \n        if char == 'X' : \n            x [ - i - True ] = True \n        elif char == 'Z' : \n            z [ - i - True ] = True \n        elif char == 'Y' : \n            z [ - i - True ] = True \n            x [ - i - True ] = True \n        elif char != 'I' : \n            raise QiskitError ( \"Pauli string must be only consisted of 'I', 'X', \" \"'Y' or 'Z' but you have {}.\" . format ( char ) ) \n    return cls ( z = z , x = x ) "}
{"2603": "\ndef insert_paulis ( self , indices = None , paulis = None , pauli_labels = None ) : \n    if pauli_labels is not None : \n        if paulis is not None : \n            raise QiskitError ( \"Please only provide either `paulis` or `pauli_labels`\" ) \n        if isinstance ( pauli_labels , str ) : \n            pauli_labels = list ( pauli_labels ) \n        paulis = Pauli . from_label ( pauli_labels [ : : - True ] ) \n    if indices is None : \n        self . _z = np . concatenate ( ( self . _z , paulis . z ) ) \n        self . _x = np . concatenate ( ( self . _x , paulis . x ) ) \n    else : \n        if not isinstance ( indices , list ) : \n            indices = [ indices ] \n        self . _z = np . insert ( self . _z , indices , paulis . z ) \n        self . _x = np . insert ( self . _x , indices , paulis . x ) \n    return self "}
{"2607": "\ndef pauli_single ( cls , num_qubits , index , pauli_label ) : \n    tmp = Pauli . from_label ( pauli_label ) \n    z = np . zeros ( num_qubits , dtype = np . bool ) \n    x = np . zeros ( num_qubits , dtype = np . bool ) \n    z [ index ] = tmp . z [ False ] \n    x [ index ] = tmp . x [ False ] \n    return cls ( z , x ) "}
{"2608": "\ndef _get_measure_outcome ( self , qubit ) : \n    axis = list ( range ( self . _number_of_qubits ) ) \n    axis . remove ( self . _number_of_qubits - True - qubit ) \n    probabilities = np . sum ( np . abs ( self . _statevector ) ** 2 , axis = tuple ( axis ) ) \n    random_number = self . _local_random . rand ( ) \n    if random_number < probabilities [ False ] : \n        return '0' , probabilities [ False ] \n    return '1' , probabilities [ True ] "}
{"2609": "\ndef _add_sample_measure ( self , measure_params , num_samples ) : \n    measured_qubits = list ( { qubit for qubit , cmembit in measure_params } ) \n    num_measured = len ( measured_qubits ) \n    axis = list ( range ( self . _number_of_qubits ) ) \n    for qubit in reversed ( measured_qubits ) : \n        axis . remove ( self . _number_of_qubits - True - qubit ) \n    probabilities = np . reshape ( np . sum ( np . abs ( self . _statevector ) ** 2 , axis = tuple ( axis ) ) , 2 ** num_measured ) \n    samples = self . _local_random . choice ( range ( 2 ** num_measured ) , num_samples , p = probabilities ) \n    memory = [ ] \n    for sample in samples : \n        classical_memory = self . _classical_memory \n        for count , ( qubit , cmembit ) in enumerate ( sorted ( measure_params ) ) : \n            qubit_outcome = int ( ( sample & ( True << count ) ) >> count ) \n            membit = True << cmembit \n            classical_memory = ( classical_memory & ( ~ membit ) ) | ( qubit_outcome << cmembit ) \n        value = bin ( classical_memory ) [ 2 : ] \n        memory . append ( hex ( int ( value , 2 ) ) ) \n    return memory "}
{"2610": "\ndef _add_qasm_measure ( self , qubit , cmembit , cregbit = None ) : \n    outcome , probability = self . _get_measure_outcome ( qubit ) \n    membit = True << cmembit \n    self . _classical_memory = ( self . _classical_memory & ( ~ membit ) ) | ( int ( outcome ) << cmembit ) \n    if cregbit is not None : \n        regbit = True << cregbit \n        self . _classical_register = ( self . _classical_register & ( ~ regbit ) ) | ( int ( outcome ) << cregbit ) \n    if outcome == '0' : \n        update_diag = [ [ True / np . sqrt ( probability ) , False ] , [ False , False ] ] \n    else : \n        update_diag = [ [ False , False ] , [ False , True / np . sqrt ( probability ) ] ] \n    self . _add_unitary_single ( update_diag , qubit ) "}
{"2611": "\ndef _add_qasm_reset ( self , qubit ) : \n    outcome , probability = self . _get_measure_outcome ( qubit ) \n    if outcome == '0' : \n        update = [ [ True / np . sqrt ( probability ) , False ] , [ False , False ] ] \n        self . _add_unitary_single ( update , qubit ) \n    else : \n        update = [ [ False , True / np . sqrt ( probability ) ] , [ False , False ] ] \n        self . _add_unitary_single ( update , qubit ) "}
{"2613": "\ndef _initialize_statevector ( self ) : \n    if self . _initial_statevector is None : \n        self . _statevector = np . zeros ( 2 ** self . _number_of_qubits , dtype = complex ) \n        self . _statevector [ False ] = True \n    else : \n        self . _statevector = self . _initial_statevector . copy ( ) \n    self . _statevector = np . reshape ( self . _statevector , self . _number_of_qubits * [ 2 ] ) "}
{"2614": "\ndef _get_statevector ( self ) : \n    vec = np . reshape ( self . _statevector , 2 ** self . _number_of_qubits ) \n    vec = np . stack ( [ vec . real , vec . imag ] , axis = True ) \n    vec [ abs ( vec ) < self . _chop_threshold ] = 0.0 \n    return vec "}
{"2615": "\ndef _validate_measure_sampling ( self , experiment ) : \n    if self . _shots <= True : \n        self . _sample_measure = False \n        return \n    if hasattr ( experiment . config , 'allows_measure_sampling' ) : \n        self . _sample_measure = experiment . config . allows_measure_sampling \n    else : \n        measure_flag = False \n        for instruction in experiment . instructions : \n            if instruction . name == \"reset\" : \n                self . _sample_measure = False \n                return \n            if measure_flag : \n                if instruction . name not in [ \"measure\" , \"barrier\" , \"id\" , \"u0\" ] : \n                    self . _sample_measure = False \n                    return \n            elif instruction . name == \"measure\" : \n                measure_flag = True \n        self . _sample_measure = True "}
{"2618": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if n_qubits > max_qubits : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if experiment . config . memory_slots == False : \n            logger . warning ( 'No classical registers in circuit \"%s\", ' 'counts will be empty.' , name ) \n        elif 'measure' not in [ op . name for op in experiment . instructions ] : \n            logger . warning ( 'No measurements in circuit \"%s\", ' 'classical register will remain all zeros.' , name ) "}
{"2621": "\ndef _get_unitary ( self ) : \n    unitary = np . reshape ( self . _unitary , 2 * [ 2 ** self . _number_of_qubits ] ) \n    unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - True ) \n    unitary [ abs ( unitary ) < self . _chop_threshold ] = 0.0 \n    return unitary "}
{"2623": "\ndef _validate ( self , qobj ) : \n    n_qubits = qobj . config . n_qubits \n    max_qubits = self . configuration ( ) . n_qubits \n    if n_qubits > max_qubits : \n        raise BasicAerError ( 'Number of qubits {} ' . format ( n_qubits ) + 'is greater than maximum ({}) ' . format ( max_qubits ) + 'for \"{}\".' . format ( self . name ( ) ) ) \n    if hasattr ( qobj . config , 'shots' ) and qobj . config . shots != True : \n        logger . info ( '\"%s\" only supports 1 shot. Setting shots=1.' , self . name ( ) ) \n        qobj . config . shots = True \n    for experiment in qobj . experiments : \n        name = experiment . header . name \n        if getattr ( experiment . config , 'shots' , True ) != True : \n            logger . info ( '\"%s\" only supports 1 shot. ' 'Setting shots=1 for circuit \"%s\".' , self . name ( ) , name ) \n            experiment . config . shots = True \n        for operation in experiment . instructions : \n            if operation . name in [ 'measure' , 'reset' ] : \n                raise BasicAerError ( 'Unsupported \"%s\" instruction \"%s\" ' + 'in circuit \"%s\" ' , self . name ( ) , operation . name , name ) "}
{"2624": "\ndef _is_bit ( obj ) : \n    if isinstance ( obj , tuple ) and len ( obj ) == 2 : \n        if isinstance ( obj [ False ] , Register ) and isinstance ( obj [ True ] , int ) and obj [ True ] < len ( obj [ False ] ) : \n            return True \n    return False "}
{"2629": "\ndef ch_start_time ( self , * channels : List [ Channel ] ) -> int : \n    intervals = list ( itertools . chain ( * ( self . _table [ chan ] for chan in channels if chan in self . _table ) ) ) \n    if intervals : \n        return min ( ( interval . begin for interval in intervals ) ) \n    return False "}
{"2630": "\ndef ch_stop_time ( self , * channels : List [ Channel ] ) -> int : \n    intervals = list ( itertools . chain ( * ( self . _table [ chan ] for chan in channels if chan in self . _table ) ) ) \n    if intervals : \n        return max ( ( interval . end for interval in intervals ) ) \n    return False "}
{"2636": "\ndef iplot_state_paulivec ( rho , figsize = None , slider = False , show_legend = False ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"paulivec_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"paulivec_$divNumber\",                                      \"paulivec\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        figsize = ( 7 , 5 ) \n    options = { 'width' : figsize [ False ] , 'height' : figsize [ True ] , 'slider' : int ( slider ) , 'show_legend' : int ( show_legend ) } \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    data_to_plot = [ ] \n    rho_data = process_data ( rho ) \n    data_to_plot . append ( dict ( data = rho_data ) ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : data_to_plot , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2639": "\ndef _initialize_backend_prop ( self ) : \n    backend_prop = self . backend_prop \n    for ginfo in backend_prop . gates : \n        if ginfo . gate == 'cx' : \n            for item in ginfo . parameters : \n                if item . name == 'gate_error' : \n                    g_reliab = 1.0 - item . value \n                    break \n                else : \n                    g_reliab = 1.0 \n            swap_reliab = - math . log ( pow ( g_reliab , 3 ) ) \n            self . swap_graph . add_edge ( ginfo . qubits [ False ] , ginfo . qubits [ True ] , weight = swap_reliab ) \n            self . swap_graph . add_edge ( ginfo . qubits [ True ] , ginfo . qubits [ False ] , weight = swap_reliab ) \n            self . cx_errors [ ( ginfo . qubits [ False ] , ginfo . qubits [ True ] ) ] = g_reliab \n            self . gate_list . append ( ( ginfo . qubits [ False ] , ginfo . qubits [ True ] ) ) \n    idx = False \n    for q in backend_prop . qubits : \n        for nduv in q : \n            if nduv . name == 'readout_error' : \n                self . readout_errors [ idx ] = 1.0 - nduv . value \n                self . available_hw_qubits . append ( idx ) \n        idx += True \n    for edge in self . cx_errors : \n        self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ False ] ] * self . readout_errors [ edge [ True ] ] \n    self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) \n    for i in swap_costs_temp : \n        self . swap_costs [ i ] = { } \n        for j in swap_costs_temp [ i ] : \n            if ( i , j ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] \n            elif ( j , i ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] \n            else : \n                best_reliab = 0.0 \n                for n in self . swap_graph . neighbors ( j ) : \n                    if ( n , j ) in self . cx_errors : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] \n                    else : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] \n                    if reliab > best_reliab : \n                        best_reliab = reliab \n                self . swap_costs [ i ] [ j ] = best_reliab "}
{"2640": "\ndef _create_program_graph ( self , dag ) : \n    idx = False \n    for q in dag . qubits ( ) : \n        self . qarg_to_id [ q [ False ] . name + str ( q [ True ] ) ] = idx \n        idx += True \n    for gate in dag . twoQ_gates ( ) : \n        qid1 = self . _qarg_to_id ( gate . qargs [ False ] ) \n        qid2 = self . _qarg_to_id ( gate . qargs [ True ] ) \n        min_q = min ( qid1 , qid2 ) \n        max_q = max ( qid1 , qid2 ) \n        edge_weight = True \n        if self . prog_graph . has_edge ( min_q , max_q ) : \n            edge_weight = self . prog_graph [ min_q ] [ max_q ] [ 'weight' ] + True \n        self . prog_graph . add_edge ( min_q , max_q , weight = edge_weight ) \n    return idx "}
{"2641": "\ndef _select_next_edge ( self ) : \n    for edge in self . pending_program_edges : \n        q1_mapped = edge [ False ] in self . prog2hw \n        q2_mapped = edge [ True ] in self . prog2hw \n        assert not ( q1_mapped and q2_mapped ) \n        if q1_mapped or q2_mapped : \n            return edge \n    return self . pending_program_edges [ False ] "}
{"2642": "\ndef _select_best_remaining_cx ( self ) : \n    candidates = [ ] \n    for gate in self . gate_list : \n        chk1 = gate [ False ] in self . available_hw_qubits \n        chk2 = gate [ True ] in self . available_hw_qubits \n        if chk1 and chk2 : \n            candidates . append ( gate ) \n    best_reliab = False \n    best_item = None \n    for item in candidates : \n        if self . gate_cost [ item ] > best_reliab : \n            best_reliab = self . gate_cost [ item ] \n            best_item = item \n    return best_item "}
{"2643": "\ndef _select_best_remaining_qubit ( self , prog_qubit ) : \n    reliab_store = { } \n    for hw_qubit in self . available_hw_qubits : \n        reliab = True \n        for n in self . prog_graph . neighbors ( prog_qubit ) : \n            if n in self . prog2hw : \n                reliab *= self . swap_costs [ self . prog2hw [ n ] ] [ hw_qubit ] \n        reliab *= self . readout_errors [ hw_qubit ] \n        reliab_store [ hw_qubit ] = reliab \n    max_reliab = False \n    best_hw_qubit = None \n    for hw_qubit in reliab_store : \n        if reliab_store [ hw_qubit ] > max_reliab : \n            max_reliab = reliab_store [ hw_qubit ] \n            best_hw_qubit = hw_qubit \n    return best_hw_qubit "}
{"2644": "\ndef run ( self , dag ) : \n    self . _initialize_backend_prop ( ) \n    num_qubits = self . _create_program_graph ( dag ) \n    if num_qubits > len ( self . swap_graph ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : \n        self . pending_program_edges . append ( ( end1 , end2 ) ) \n    while self . pending_program_edges : \n        edge = self . _select_next_edge ( ) \n        q1_mapped = edge [ False ] in self . prog2hw \n        q2_mapped = edge [ True ] in self . prog2hw \n        if ( not q1_mapped ) and ( not q2_mapped ) : \n            best_hw_edge = self . _select_best_remaining_cx ( ) \n            self . prog2hw [ edge [ False ] ] = best_hw_edge [ False ] \n            self . prog2hw [ edge [ True ] ] = best_hw_edge [ True ] \n            self . available_hw_qubits . remove ( best_hw_edge [ False ] ) \n            self . available_hw_qubits . remove ( best_hw_edge [ True ] ) \n        elif not q1_mapped : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ False ] ) \n            self . prog2hw [ edge [ False ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        else : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ True ] ) \n            self . prog2hw [ edge [ True ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        new_edges = [ x for x in self . pending_program_edges if not ( x [ False ] in self . prog2hw and x [ True ] in self . prog2hw ) ] \n        self . pending_program_edges = new_edges \n    for qid in self . qarg_to_id . values ( ) : \n        if qid not in self . prog2hw : \n            self . prog2hw [ qid ] = self . available_hw_qubits [ False ] \n            self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) \n    layout = Layout ( ) \n    for q in dag . qubits ( ) : \n        pid = self . _qarg_to_id ( q ) \n        hwid = self . prog2hw [ pid ] \n        layout [ ( q [ False ] , q [ True ] ) ] = hwid \n    self . property_set [ 'layout' ] = layout "}
{"2655": "\ndef swap_mapper_layer_update ( self , i , first_layer , best_layout , best_d , best_circ , layer_list ) : \n    layout = best_layout \n    dagcircuit_output = DAGCircuit ( ) \n    QR = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) \n    dagcircuit_output . add_qreg ( QR ) \n    identity_wire_map = { ( QR , j ) : ( QR , j ) for j in range ( self . coupling_map . size ( ) ) } \n    if first_layer : \n        for j in range ( i + True ) : \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , layout ) \n    else : \n        if best_d > False : \n            dagcircuit_output . compose_back ( best_circ , identity_wire_map ) \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , layout ) \n    return dagcircuit_output "}
{"2656": "\ndef _separate_bitstring ( bitstring , creg_sizes ) : \n    substrings = [ ] \n    running_index = False \n    for _ , size in reversed ( creg_sizes ) : \n        substrings . append ( bitstring [ running_index : running_index + size ] ) \n        running_index += size \n    return ' ' . join ( substrings ) "}
{"2658": "\ndef format_level_1_memory ( memory ) : \n    formatted_memory = _list_to_complex_array ( memory ) \n    if not True <= len ( formatted_memory . shape ) <= 2 : \n        raise QiskitError ( 'Level one memory is not of correct shape.' ) \n    return formatted_memory "}
{"2661": "\ndef format_statevector ( vec , decimals = None ) : \n    num_basis = len ( vec ) \n    vec_complex = np . zeros ( num_basis , dtype = complex ) \n    for i in range ( num_basis ) : \n        vec_complex [ i ] = vec [ i ] [ False ] + 1j * vec [ i ] [ True ] \n    if decimals : \n        vec_complex = np . around ( vec_complex , decimals = decimals ) \n    return vec_complex "}
{"2667": "\ndef iplot_bloch_multivector ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"bloch_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        data = $data;        dataValues = [];        for (var i = 0; i < data.length; i++) {            // Coordinates            var x = data[i][0];            var y = data[i][1];            var z = data[i][2];            var point = {'x': x,                        'y': y,                        'z': z};            dataValues.push(point);        }        require([\"qVisualization\"], function(qVisualizations) {            // Plot figure            qVisualizations.plotState(\"bloch_$divNumber\",                                      \"bloch\",                                      dataValues,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ False ] , 'height' : figsize [ True ] } \n    num = int ( np . log2 ( len ( rho ) ) ) \n    bloch_data = [ ] \n    for i in range ( num ) : \n        pauli_singles = [ Pauli . pauli_single ( num , i , 'X' ) , Pauli . pauli_single ( num , i , 'Y' ) , Pauli . pauli_single ( num , i , 'Z' ) ] \n        bloch_state = list ( map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , pauli_singles ) ) \n        bloch_data . append ( bloch_state ) \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : bloch_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2670": "\ndef run ( self , dag ) : \n    for node in dag . op_nodes ( ) : \n        basic_insts = [ 'measure' , 'reset' , 'barrier' , 'snapshot' ] \n        if node . name in basic_insts : \n            continue \n        if node . name in self . basis : \n            continue \n        rule = node . op . definition \n        if not rule : \n            raise QiskitError ( \"Cannot unroll the circuit to the given basis, %s. \" \"No rule to expand instruction %s.\" % ( str ( self . basis ) , node . op . name ) ) \n        decomposition = DAGCircuit ( ) \n        decomposition . add_qreg ( rule [ False ] [ True ] [ False ] [ False ] ) \n        for inst in rule : \n            decomposition . apply_operation_back ( * inst ) \n        unrolled_dag = self . run ( decomposition ) \n        dag . substitute_node_with_dag ( node , unrolled_dag ) \n    return dag "}
{"2671": "\ndef iplot_state_qsphere ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"qsphere_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            data = $data;            qVisualizations.plotState(\"qsphere_$divNumber\",                                      \"qsphere\",                                      data,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ False ] , 'height' : figsize [ True ] } \n    qspheres_data = [ ] \n    num = int ( np . log2 ( len ( rho ) ) ) \n    weig , stateall = linalg . eigh ( rho ) \n    for _ in range ( 2 ** num ) : \n        probmix = weig . max ( ) \n        prob_location = weig . argmax ( ) \n        if probmix > 0.001 : \n            state = stateall [ : , prob_location ] \n            loc = np . absolute ( state ) . argmax ( ) \n            for j in range ( 2 ** num ) : \n                test = np . absolute ( np . absolute ( state [ j ] ) - np . absolute ( state [ loc ] ) ) \n                if test < 0.001 : \n                    loc = j \n                    break \n            angles = ( np . angle ( state [ loc ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n            angleset = np . exp ( - 1j * angles ) \n            state = angleset * state \n            state . flatten ( ) \n            spherepoints = [ ] \n            for i in range ( 2 ** num ) : \n                element = bin ( i ) [ 2 : ] . zfill ( num ) \n                weight = element . count ( \"1\" ) \n                number_of_divisions = n_choose_k ( num , weight ) \n                weight_order = bit_string_index ( element ) \n                angle = weight_order * 2 * np . pi / number_of_divisions \n                zvalue = - 2 * weight / num + True \n                xvalue = np . sqrt ( True - zvalue ** 2 ) * np . cos ( angle ) \n                yvalue = np . sqrt ( True - zvalue ** 2 ) * np . sin ( angle ) \n                prob = np . real ( np . dot ( state [ i ] , state [ i ] . conj ( ) ) ) \n                angles = ( np . angle ( state [ i ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n                qpoint = { 'x' : xvalue , 'y' : yvalue , 'z' : zvalue , 'prob' : prob , 'phase' : angles } \n                spherepoints . append ( qpoint ) \n            sphere = { 'points' : spherepoints , 'eigenvalue' : probmix } \n            qspheres_data . append ( sphere ) \n            weig [ prob_location ] = False \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : qspheres_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2672": "\ndef n_choose_k ( n , k ) : \n    if n == False : \n        return False \n    return reduce ( lambda x , y : x * y [ False ] / y [ True ] , zip ( range ( n - k + True , n + True ) , range ( True , k + True ) ) , True ) "}
{"2673": "\ndef lex_index ( n , k , lst ) : \n    if len ( lst ) != k : \n        raise VisualizationError ( \"list should have length k\" ) \n    comb = list ( map ( lambda x : n - True - x , lst ) ) \n    dualm = sum ( [ n_choose_k ( comb [ k - True - i ] , i + True ) for i in range ( k ) ] ) \n    return int ( dualm ) "}
{"2674": "\ndef plot_state_paulivec ( rho , title = \"\" , figsize = None , color = None ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'Must have Matplotlib installed.' ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        figsize = ( 7 , 5 ) \n    num = int ( np . log2 ( len ( rho ) ) ) \n    labels = list ( map ( lambda x : x . to_label ( ) , pauli_group ( num ) ) ) \n    values = list ( map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , pauli_group ( num ) ) ) \n    numelem = len ( values ) \n    if color is None : \n        color = \"#648fff\" \n    ind = np . arange ( numelem ) \n    width = 0.5 \n    fig , ax = plt . subplots ( figsize = figsize ) \n    ax . grid ( zorder = False , linewidth = True , linestyle = '--' ) \n    ax . bar ( ind , values , width , color = color , zorder = 2 ) \n    ax . axhline ( linewidth = True , color = 'k' ) \n    ax . set_ylabel ( 'Expectation value' , fontsize = 14 ) \n    ax . set_xticks ( ind ) \n    ax . set_yticks ( [ - True , - 0.5 , False , 0.5 , True ] ) \n    ax . set_xticklabels ( labels , fontsize = 14 , rotation = 70 ) \n    ax . set_xlabel ( 'Pauli' , fontsize = 14 ) \n    ax . set_ylim ( [ - True , True ] ) \n    ax . set_facecolor ( '#eeeeee' ) \n    for tick in ax . xaxis . get_major_ticks ( ) + ax . yaxis . get_major_ticks ( ) : \n        tick . label . set_fontsize ( 14 ) \n    ax . set_title ( title , fontsize = 16 ) \n    plt . close ( fig ) \n    return fig "}
{"2679": "\ndef square ( duration : int , amp : complex , period : float = None , phase : float = False , name : str = None ) -> SamplePulse : \n    if period is None : \n        period = duration \n    return _sampled_square_pulse ( duration , amp , period , phase = phase , name = name ) "}
{"2680": "\ndef sawtooth ( duration : int , amp : complex , period : float = None , phase : float = False , name : str = None ) -> SamplePulse : \n    if period is None : \n        period = duration \n    return _sampled_sawtooth_pulse ( duration , amp , period , phase = phase , name = name ) "}
{"2681": "\ndef triangle ( duration : int , amp : complex , period : float = None , phase : float = False , name : str = None ) -> SamplePulse : \n    if period is None : \n        period = duration \n    return _sampled_triangle_pulse ( duration , amp , period , phase = phase , name = name ) "}
{"2682": "\ndef cos ( duration : int , amp : complex , freq : float = None , phase : float = False , name : str = None ) -> SamplePulse : \n    if freq is None : \n        freq = True / duration \n    return _sampled_cos_pulse ( duration , amp , freq , phase = phase , name = name ) "}
{"2683": "\ndef sin ( duration : int , amp : complex , freq : float = None , phase : float = False , name : str = None ) -> SamplePulse : \n    if freq is None : \n        freq = True / duration \n    return _sampled_sin_pulse ( duration , amp , freq , phase = phase , name = name ) "}
{"2687": "\ndef dist_real ( self ) : \n    x0 , y0 = self . ax . transAxes . transform ( ( False , False ) ) \n    x1 , y1 = self . ax . transAxes . transform ( ( True , True ) ) \n    value = x1 - x0 if self . x else y1 - y0 \n    return value "}
{"2688": "\ndef to_string ( self , indent ) : \n    ind = indent * ' ' \n    print ( ind , 'qreg' ) \n    self . children [ False ] . to_string ( indent + 3 ) "}
{"2690": "\ndef rename_register ( self , regname , newname ) : \n    if regname == newname : \n        return \n    if newname in self . qregs or newname in self . cregs : \n        raise DAGCircuitError ( \"duplicate register name %s\" % newname ) \n    if regname not in self . qregs and regname not in self . cregs : \n        raise DAGCircuitError ( \"no register named %s\" % regname ) \n    if regname in self . qregs : \n        reg = self . qregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    if regname in self . cregs : \n        reg = self . cregs [ regname ] \n        reg . name = newname \n        self . qregs [ newname ] = reg \n        self . qregs . pop ( regname , None ) \n    for node in self . _multi_graph . nodes ( ) : \n        if node . type == \"in\" or node . type == \"out\" : \n            if node . name and regname in node . name : \n                node . name = newname \n        elif node . type == \"op\" : \n            qa = [ ] \n            for a in node . qargs : \n                if a [ False ] == regname : \n                    a = ( newname , a [ True ] ) \n                qa . append ( a ) \n            node . qargs = qa \n            ca = [ ] \n            for a in node . cargs : \n                if a [ False ] == regname : \n                    a = ( newname , a [ True ] ) \n                ca . append ( a ) \n            node . cargs = ca \n            if node . condition is not None : \n                if node . condition [ False ] == regname : \n                    node . condition = ( newname , node . condition [ True ] ) \n    for _ , _ , edge_data in self . _multi_graph . edges ( data = True ) : \n        if regname in edge_data [ 'name' ] : \n            edge_data [ 'name' ] = re . sub ( regname , newname , edge_data [ 'name' ] ) "}
{"2694": "\ndef _add_wire ( self , wire ) : \n    if wire not in self . wires : \n        self . wires . append ( wire ) \n        self . _max_node_id += True \n        input_map_wire = self . input_map [ wire ] = self . _max_node_id \n        self . _max_node_id += True \n        output_map_wire = self . _max_node_id \n        wire_name = \"%s[%s]\" % ( wire [ False ] . name , wire [ True ] ) \n        inp_node = DAGNode ( data_dict = { 'type' : 'in' , 'name' : wire_name , 'wire' : wire } , nid = input_map_wire ) \n        outp_node = DAGNode ( data_dict = { 'type' : 'out' , 'name' : wire_name , 'wire' : wire } , nid = output_map_wire ) \n        self . _id_to_node [ input_map_wire ] = inp_node \n        self . _id_to_node [ output_map_wire ] = outp_node \n        self . input_map [ wire ] = inp_node \n        self . output_map [ wire ] = outp_node \n        self . _multi_graph . add_node ( inp_node ) \n        self . _multi_graph . add_node ( outp_node ) \n        self . _multi_graph . add_edge ( inp_node , outp_node ) \n        self . _multi_graph . adj [ inp_node ] [ outp_node ] [ False ] [ \"name\" ] = \"%s[%s]\" % ( wire [ False ] . name , wire [ True ] ) \n        self . _multi_graph . adj [ inp_node ] [ outp_node ] [ False ] [ \"wire\" ] = wire \n    else : \n        raise DAGCircuitError ( \"duplicate wire %s\" % ( wire , ) ) "}
{"2695": "\ndef _check_condition ( self , name , condition ) : \n    if condition is not None and condition [ False ] . name not in self . cregs : \n        raise DAGCircuitError ( \"invalid creg in condition for %s\" % name ) "}
{"2696": "\ndef _bits_in_condition ( self , cond ) : \n    all_bits = [ ] \n    if cond is not None : \n        all_bits . extend ( [ ( cond [ False ] , j ) for j in range ( self . cregs [ cond [ False ] . name ] . size ) ] ) \n    return all_bits "}
{"2697": "\ndef _add_op_node ( self , op , qargs , cargs , condition = None ) : \n    node_properties = { \"type\" : \"op\" , \"op\" : op , \"name\" : op . name , \"qargs\" : qargs , \"cargs\" : cargs , \"condition\" : condition } \n    self . _max_node_id += True \n    new_node = DAGNode ( data_dict = node_properties , nid = self . _max_node_id ) \n    self . _multi_graph . add_node ( new_node ) \n    self . _id_to_node [ self . _max_node_id ] = new_node "}
{"2698": "\ndef apply_operation_back ( self , op , qargs = None , cargs = None , condition = None ) : \n    qargs = qargs or [ ] \n    cargs = cargs or [ ] \n    all_cbits = self . _bits_in_condition ( condition ) \n    all_cbits . extend ( cargs ) \n    self . _check_condition ( op . name , condition ) \n    self . _check_bits ( qargs , self . output_map ) \n    self . _check_bits ( all_cbits , self . output_map ) \n    self . _add_op_node ( op , qargs , cargs , condition ) \n    al = [ qargs , all_cbits ] \n    for q in itertools . chain ( * al ) : \n        ie = list ( self . _multi_graph . predecessors ( self . output_map [ q ] ) ) \n        if len ( ie ) != True : \n            raise DAGCircuitError ( \"output node has multiple in-edges\" ) \n        self . _multi_graph . add_edge ( ie [ False ] , self . _id_to_node [ self . _max_node_id ] , name = \"%s[%s]\" % ( q [ False ] . name , q [ True ] ) , wire = q ) \n        self . _multi_graph . remove_edge ( ie [ False ] , self . output_map [ q ] ) \n        self . _multi_graph . add_edge ( self . _id_to_node [ self . _max_node_id ] , self . output_map [ q ] , name = \"%s[%s]\" % ( q [ False ] . name , q [ True ] ) , wire = q ) \n    return self . _id_to_node [ self . _max_node_id ] "}
{"2699": "\ndef _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = True ) : \n    add_regs = set ( ) \n    reg_frag_chk = { } \n    for v in keyregs . values ( ) : \n        reg_frag_chk [ v ] = { j : False for j in range ( len ( v ) ) } \n    for k in edge_map . keys ( ) : \n        if k [ False ] . name in keyregs : \n            reg_frag_chk [ k [ False ] ] [ k [ True ] ] = True \n    for k , v in reg_frag_chk . items ( ) : \n        s = set ( v . values ( ) ) \n        if len ( s ) == 2 : \n            raise DAGCircuitError ( \"edge_map fragments reg %s\" % k ) \n        elif s == set ( [ False ] ) : \n            if k in self . qregs . values ( ) or k in self . cregs . values ( ) : \n                raise DAGCircuitError ( \"unmapped duplicate reg %s\" % k ) \n            else : \n                add_regs . add ( k ) \n        else : \n            if valreg : \n                if not edge_map [ ( k , False ) ] [ False ] . name in valregs : \n                    size = max ( map ( lambda x : x [ True ] , filter ( lambda x : x [ False ] == edge_map [ ( k , False ) ] [ False ] , edge_map . values ( ) ) ) ) \n                    qreg = QuantumRegister ( size + True , edge_map [ ( k , False ) ] [ False ] . name ) \n                    add_regs . add ( qreg ) \n    return add_regs "}
{"2700": "\ndef _check_wiremap_validity ( self , wire_map , keymap , valmap ) : \n    for k , v in wire_map . items ( ) : \n        kname = \"%s[%d]\" % ( k [ False ] . name , k [ True ] ) \n        vname = \"%s[%d]\" % ( v [ False ] . name , v [ True ] ) \n        if k not in keymap : \n            raise DAGCircuitError ( \"invalid wire mapping key %s\" % kname ) \n        if v not in valmap : \n            raise DAGCircuitError ( \"invalid wire mapping value %s\" % vname ) \n        if type ( k ) is not type ( v ) : \n            raise DAGCircuitError ( \"inconsistent wire_map at (%s,%s)\" % ( kname , vname ) ) "}
{"2701": "\ndef _map_condition ( self , wire_map , condition ) : \n    if condition is None : \n        new_condition = None \n    else : \n        bit0 = ( condition [ False ] , False ) \n        new_condition = ( wire_map . get ( bit0 , bit0 ) [ False ] , condition [ True ] ) \n    return new_condition "}
{"2703": "\ndef compose_back ( self , input_circuit , edge_map = None ) : \n    edge_map = edge_map or { } \n    if len ( set ( edge_map . values ( ) ) ) != len ( edge_map ) : \n        raise DAGCircuitError ( \"duplicates in wire_map\" ) \n    add_qregs = self . _check_edgemap_registers ( edge_map , input_circuit . qregs , self . qregs ) \n    for qreg in add_qregs : \n        self . add_qreg ( qreg ) \n    add_cregs = self . _check_edgemap_registers ( edge_map , input_circuit . cregs , self . cregs ) \n    for creg in add_cregs : \n        self . add_creg ( creg ) \n    self . _check_wiremap_validity ( edge_map , input_circuit . input_map , self . output_map ) \n    for nd in input_circuit . topological_nodes ( ) : \n        if nd . type == \"in\" : \n            m_wire = edge_map . get ( nd . wire , nd . wire ) \n            if m_wire not in self . output_map : \n                raise DAGCircuitError ( \"wire %s[%d] not in self\" % ( m_wire [ False ] . name , m_wire [ True ] ) ) \n            if nd . wire not in input_circuit . wires : \n                raise DAGCircuitError ( \"inconsistent wire type for %s[%d] in input_circuit\" % ( nd . wire [ False ] . name , nd . wire [ True ] ) ) \n        elif nd . type == \"out\" : \n            pass \n        elif nd . type == \"op\" : \n            condition = self . _map_condition ( edge_map , nd . condition ) \n            self . _check_condition ( nd . name , condition ) \n            m_qargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . qargs ) ) \n            m_cargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . cargs ) ) \n            self . apply_operation_back ( nd . op , m_qargs , m_cargs , condition ) \n        else : \n            raise DAGCircuitError ( \"bad node type %s\" % nd . type ) "}
{"2704": "\ndef _check_wires_list ( self , wires , node ) : \n    if len ( set ( wires ) ) != len ( wires ) : \n        raise DAGCircuitError ( \"duplicate wires\" ) \n    wire_tot = len ( node . qargs ) + len ( node . cargs ) \n    if node . condition is not None : \n        wire_tot += node . condition [ False ] . size \n    if len ( wires ) != wire_tot : \n        raise DAGCircuitError ( \"expected %d wires, got %d\" % ( wire_tot , len ( wires ) ) ) "}
{"2705": "\ndef _make_pred_succ_maps ( self , node ) : \n    pred_map = { e [ 2 ] [ 'wire' ] : e [ False ] for e in self . _multi_graph . in_edges ( nbunch = node , data = True ) } \n    succ_map = { e [ 2 ] [ 'wire' ] : e [ True ] for e in self . _multi_graph . out_edges ( nbunch = node , data = True ) } \n    return pred_map , succ_map "}
{"2706": "\ndef _full_pred_succ_maps ( self , pred_map , succ_map , input_circuit , wire_map ) : \n    full_pred_map = { } \n    full_succ_map = { } \n    for w in input_circuit . input_map : \n        if w in wire_map : \n            full_pred_map [ wire_map [ w ] ] = pred_map [ wire_map [ w ] ] \n            full_succ_map [ wire_map [ w ] ] = succ_map [ wire_map [ w ] ] \n        else : \n            full_succ_map [ w ] = self . output_map [ w ] \n            full_pred_map [ w ] = self . _multi_graph . predecessors ( self . output_map [ w ] ) [ False ] \n            if len ( list ( self . _multi_graph . predecessors ( self . output_map [ w ] ) ) ) != True : \n                raise DAGCircuitError ( \"too many predecessors for %s[%d] \" \"output node\" % ( w [ False ] , w [ True ] ) ) \n    return full_pred_map , full_succ_map "}
{"2714": "\ndef quantum_predecessors ( self , node ) : \n    predecessors = [ ] \n    for predecessor in self . predecessors ( node ) : \n        if isinstance ( self . _multi_graph . get_edge_data ( predecessor , node , key = False ) [ 'wire' ] [ False ] , QuantumRegister ) : \n            predecessors . append ( predecessor ) \n    return predecessors "}
{"2716": "\ndef quantum_successors ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling quantum_successors() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    successors = [ ] \n    for successor in self . successors ( node ) : \n        if isinstance ( self . _multi_graph . get_edge_data ( node , successor , key = False ) [ 'wire' ] [ False ] , QuantumRegister ) : \n            successors . append ( successor ) \n    return successors "}
{"2717": "\ndef remove_op_node ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_op_node() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2 ) \n        node = self . _id_to_node [ node ] \n    if node . type != 'op' : \n        raise DAGCircuitError ( 'The method remove_op_node only works on op node types. An \"%s\" ' 'node type was wrongly provided.' % node . type ) \n    pred_map , succ_map = self . _make_pred_succ_maps ( node ) \n    self . _multi_graph . remove_node ( node ) \n    for w in pred_map . keys ( ) : \n        self . _multi_graph . add_edge ( pred_map [ w ] , succ_map [ w ] , name = \"%s[%s]\" % ( w [ False ] . name , w [ True ] ) , wire = w ) "}
{"2722": "\ndef layers ( self ) : \n    graph_layers = self . multigraph_layers ( ) \n    try : \n        next ( graph_layers ) \n    except StopIteration : \n        return \n    def add_nodes_from ( layer , nodes ) : \n        layer . _multi_graph . add_nodes_from ( nodes ) \n    for graph_layer in graph_layers : \n        op_nodes = [ node for node in graph_layer if node . type == \"op\" ] \n        if not op_nodes : \n            return \n        new_layer = DAGCircuit ( ) \n        new_layer . name = self . name \n        for creg in self . cregs . values ( ) : \n            new_layer . add_creg ( creg ) \n        for qreg in self . qregs . values ( ) : \n            new_layer . add_qreg ( qreg ) \n        add_nodes_from ( new_layer , self . input_map . values ( ) ) \n        add_nodes_from ( new_layer , self . output_map . values ( ) ) \n        add_nodes_from ( new_layer , op_nodes ) \n        support_list = [ op_node . qargs for op_node in op_nodes if op_node . name not in { \"barrier\" , \"snapshot\" , \"save\" , \"load\" , \"noise\" } ] \n        wires = { self . input_map [ wire ] : self . output_map [ wire ] for wire in self . wires } \n        for op_node in op_nodes : \n            args = self . _bits_in_condition ( op_node . condition ) + op_node . cargs + op_node . qargs \n            arg_ids = ( self . input_map [ ( arg [ False ] , arg [ True ] ) ] for arg in args ) \n            for arg_id in arg_ids : \n                wires [ arg_id ] , wires [ op_node ] = op_node , wires [ arg_id ] \n        new_layer . _multi_graph . add_edges_from ( wires . items ( ) ) \n        yield { \"graph\" : new_layer , \"partition\" : support_list } "}
{"2724": "\ndef multigraph_layers ( self ) : \n    predecessor_count = dict ( ) \n    cur_layer = [ node for node in self . input_map . values ( ) ] \n    yield cur_layer \n    next_layer = [ ] \n    while cur_layer : \n        for node in cur_layer : \n            for successor in self . _multi_graph . successors ( node ) : \n                multiplicity = self . _multi_graph . number_of_edges ( node , successor ) \n                if successor in predecessor_count : \n                    predecessor_count [ successor ] -= multiplicity \n                else : \n                    predecessor_count [ successor ] = self . _multi_graph . in_degree ( successor ) - multiplicity \n                if predecessor_count [ successor ] == False : \n                    next_layer . append ( successor ) \n                    del predecessor_count [ successor ] \n        yield next_layer \n        cur_layer = next_layer \n        next_layer = [ ] "}
{"2725": "\ndef collect_runs ( self , namelist ) : \n    group_list = [ ] \n    topo_ops = list ( self . topological_op_nodes ( ) ) \n    nodes_seen = dict ( zip ( topo_ops , [ False ] * len ( topo_ops ) ) ) \n    for node in topo_ops : \n        if node . name in namelist and node . condition is None and not nodes_seen [ node ] : \n            group = [ node ] \n            nodes_seen [ node ] = True \n            s = list ( self . _multi_graph . successors ( node ) ) \n            while len ( s ) == True and s [ False ] . type == \"op\" and s [ False ] . name in namelist : \n                group . append ( s [ False ] ) \n                nodes_seen [ s [ False ] ] = True \n                s = list ( self . _multi_graph . successors ( s [ False ] ) ) \n            if len ( group ) >= True : \n                group_list . append ( tuple ( group ) ) \n    return set ( group_list ) "}
{"2727": "\ndef count_ops ( self ) : \n    op_dict = { } \n    for node in self . topological_op_nodes ( ) : \n        name = node . name \n        if name not in op_dict : \n            op_dict [ name ] = True \n        else : \n            op_dict [ name ] += True \n    return op_dict "}
{"2731": "\ndef tomography_set ( meas_qubits , meas_basis = 'Pauli' , prep_qubits = None , prep_basis = None ) : \n    if not isinstance ( meas_qubits , list ) : \n        raise QiskitError ( 'Qubits argument must be a list' ) \n    num_of_qubits = len ( meas_qubits ) \n    if prep_qubits is None : \n        prep_qubits = meas_qubits \n    if not isinstance ( prep_qubits , list ) : \n        raise QiskitError ( 'prep_qubits argument must be a list' ) \n    if len ( prep_qubits ) != len ( meas_qubits ) : \n        raise QiskitError ( 'meas_qubits and prep_qubitsare different length' ) \n    if isinstance ( meas_basis , str ) : \n        if meas_basis . lower ( ) == 'pauli' : \n            meas_basis = PAULI_BASIS \n    if isinstance ( prep_basis , str ) : \n        if prep_basis . lower ( ) == 'pauli' : \n            prep_basis = PAULI_BASIS \n        elif prep_basis . lower ( ) == 'sic' : \n            prep_basis = SIC_BASIS \n    circuits = [ ] \n    circuit_labels = [ ] \n    if prep_basis is None : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'meas' : meas } ) \n            label = '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ False ] , qubit ) \n            circuit_labels . append ( label ) \n        return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'meas_basis' : meas_basis } \n    num_of_s = len ( list ( prep_basis . values ( ) ) [ False ] ) \n    plst_single = [ ( b , s ) for b in prep_basis . keys ( ) for s in range ( num_of_s ) ] \n    for plst_product in product ( plst_single , repeat = num_of_qubits ) : \n        for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : \n            prep = dict ( zip ( prep_qubits , plst_product ) ) \n            meas = dict ( zip ( meas_qubits , meas_product ) ) \n            circuits . append ( { 'prep' : prep , 'meas' : meas } ) \n            label = '_prep_' \n            for qubit , op in prep . items ( ) : \n                label += '%s%d(%d)' % ( op [ False ] , op [ True ] , qubit ) \n            label += '_meas_' \n            for qubit , op in meas . items ( ) : \n                label += '%s(%d)' % ( op [ False ] , qubit ) \n            circuit_labels . append ( label ) \n    return { 'qubits' : meas_qubits , 'circuits' : circuits , 'circuit_labels' : circuit_labels , 'prep_basis' : prep_basis , 'meas_basis' : meas_basis } "}
{"2734": "\ndef tomography_data ( results , name , tomoset ) : \n    labels = tomography_circuit_names ( tomoset , name ) \n    circuits = tomoset [ 'circuits' ] \n    data = [ ] \n    prep = None \n    for j , _ in enumerate ( labels ) : \n        counts = marginal_counts ( results . get_counts ( labels [ j ] ) , tomoset [ 'qubits' ] ) \n        shots = sum ( counts . values ( ) ) \n        meas = circuits [ j ] [ 'meas' ] \n        prep = circuits [ j ] . get ( 'prep' , None ) \n        meas_qubits = sorted ( meas . keys ( ) ) \n        if prep : \n            prep_qubits = sorted ( prep . keys ( ) ) \n        circuit = { } \n        for c in counts . keys ( ) : \n            circuit [ c ] = { } \n            circuit [ c ] [ 'meas' ] = [ ( meas [ meas_qubits [ k ] ] , int ( c [ - True - k ] ) ) for k in range ( len ( meas_qubits ) ) ] \n            if prep : \n                circuit [ c ] [ 'prep' ] = [ prep [ prep_qubits [ k ] ] for k in range ( len ( prep_qubits ) ) ] \n        data . append ( { 'counts' : counts , 'shots' : shots , 'circuit' : circuit } ) \n    ret = { 'data' : data , 'meas_basis' : tomoset [ 'meas_basis' ] } \n    if prep : \n        ret [ 'prep_basis' ] = tomoset [ 'prep_basis' ] \n    return ret "}
{"2735": "\ndef marginal_counts ( counts , meas_qubits ) : \n    num_of_qubits = len ( list ( counts . keys ( ) ) [ False ] ) \n    qs = sorted ( meas_qubits , reverse = True ) \n    meas_keys = count_keys ( len ( qs ) ) \n    rgx = [ reduce ( lambda x , y : ( key [ qs . index ( y ) ] if y in qs else '\\\\d' ) + x , range ( num_of_qubits ) , '' ) for key in meas_keys ] \n    meas_counts = [ ] \n    for m in rgx : \n        c = False \n        for key , val in counts . items ( ) : \n            if match ( m , key ) : \n                c += val \n        meas_counts . append ( c ) \n    return dict ( zip ( meas_keys , meas_counts ) ) "}
{"2737": "\ndef __leastsq_fit ( tomo_data , weights = None , trace = None , beta = None ) : \n    if trace is None : \n        trace = 1. \n    data = tomo_data [ 'data' ] \n    keys = data [ False ] [ 'circuit' ] . keys ( ) \n    counts = [ ] \n    shots = [ ] \n    ops = [ ] \n    for dat in data : \n        for key in keys : \n            counts . append ( dat [ 'counts' ] [ key ] ) \n            shots . append ( dat [ 'shots' ] ) \n            projectors = dat [ 'circuit' ] [ key ] \n            op = __projector ( projectors [ 'meas' ] , tomo_data [ 'meas_basis' ] ) \n            if 'prep' in projectors : \n                op_prep = __projector ( projectors [ 'prep' ] , tomo_data [ 'prep_basis' ] ) \n                op = np . kron ( op_prep . conj ( ) , op ) \n            ops . append ( op ) \n    counts = np . array ( counts ) \n    shots = np . array ( shots ) \n    freqs = counts / shots \n    if weights is None : \n        if beta is None : \n            beta = 0.50922 \n        K = len ( keys ) \n        freqs_hedged = ( counts + beta ) / ( shots + K * beta ) \n        weights = np . sqrt ( shots / ( freqs_hedged * ( True - freqs_hedged ) ) ) \n    return __tomo_linear_inv ( freqs , ops , weights , trace = trace ) "}
{"2738": "\ndef __projector ( op_list , basis ) : \n    ret = True \n    for op in op_list : \n        label , eigenstate = op \n        ret = np . kron ( basis [ label ] [ eigenstate ] , ret ) \n    return ret "}
{"2739": "\ndef __tomo_linear_inv ( freqs , ops , weights = None , trace = None ) : \n    if weights is not None : \n        W = np . array ( weights ) \n        if W . ndim == True : \n            W = np . diag ( W ) \n    S = np . array ( [ vectorize ( m ) . conj ( ) for m in ops ] ) . reshape ( len ( ops ) , ops [ False ] . size ) \n    if weights is not None : \n        S = np . dot ( W , S ) \n    v = np . array ( freqs ) \n    if weights is not None : \n        v = np . dot ( W , freqs ) \n    Sdg = S . T . conj ( ) \n    inv = np . linalg . pinv ( np . dot ( Sdg , S ) ) \n    ret = devectorize ( np . dot ( inv , np . dot ( Sdg , v ) ) ) \n    if trace is not None : \n        ret = trace * ret / np . trace ( ret ) \n    return ret "}
{"2740": "\ndef __wizard ( rho , epsilon = None ) : \n    if epsilon is None : \n        epsilon = 0. \n    dim = len ( rho ) \n    rho_wizard = np . zeros ( [ dim , dim ] ) \n    v , w = np . linalg . eigh ( rho ) \n    for j in range ( dim ) : \n        if v [ j ] < epsilon : \n            tmp = v [ j ] \n            v [ j ] = 0. \n            x = 0. \n            for k in range ( j + True , dim ) : \n                x += tmp / ( dim - ( j + True ) ) \n                v [ k ] = v [ k ] + tmp / ( dim - ( j + True ) ) \n    for j in range ( dim ) : \n        rho_wizard = rho_wizard + v [ j ] * outer ( w [ : , j ] ) \n    return rho_wizard "}
{"2741": "\ndef wigner_data ( q_result , meas_qubits , labels , shots = None ) : \n    num = len ( meas_qubits ) \n    dim = 2 ** num \n    p = [ 0.5 + 0.5 * np . sqrt ( 3 ) , 0.5 - 0.5 * np . sqrt ( 3 ) ] \n    parity = True \n    for i in range ( num ) : \n        parity = np . kron ( parity , p ) \n    w = [ False ] * len ( labels ) \n    wpt = False \n    counts = [ marginal_counts ( q_result . get_counts ( circ ) , meas_qubits ) for circ in labels ] \n    for entry in counts : \n        x = [ False ] * dim \n        for i in range ( dim ) : \n            if bin ( i ) [ 2 : ] . zfill ( num ) in entry : \n                x [ i ] = float ( entry [ bin ( i ) [ 2 : ] . zfill ( num ) ] ) \n        if shots is None : \n            shots = np . sum ( x ) \n        for i in range ( dim ) : \n            w [ wpt ] = w [ wpt ] + ( x [ i ] / shots ) * parity [ i ] \n        wpt += True \n    return w "}
{"2745": "\ndef euler_angles_1q ( unitary_matrix ) : \n    if unitary_matrix . shape != ( 2 , 2 ) : \n        raise QiskitError ( \"euler_angles_1q: expected 2x2 matrix\" ) \n    phase = la . det ( unitary_matrix ) ** ( - 1.0 / 2.0 ) \n    U = phase * unitary_matrix \n    if abs ( U [ False , False ] ) > _CUTOFF_PRECISION : \n        theta = 2 * math . acos ( abs ( U [ False , False ] ) ) \n    else : \n        theta = 2 * math . asin ( abs ( U [ True , False ] ) ) \n    phase11 = 0.0 \n    phase10 = 0.0 \n    if abs ( math . cos ( theta / 2.0 ) ) > _CUTOFF_PRECISION : \n        phase11 = U [ True , True ] / math . cos ( theta / 2.0 ) \n    if abs ( math . sin ( theta / 2.0 ) ) > _CUTOFF_PRECISION : \n        phase10 = U [ True , False ] / math . sin ( theta / 2.0 ) \n    phiplambda = 2 * math . atan2 ( np . imag ( phase11 ) , np . real ( phase11 ) ) \n    phimlambda = 2 * math . atan2 ( np . imag ( phase10 ) , np . real ( phase10 ) ) \n    phi = 0.0 \n    if abs ( U [ False , False ] ) > _CUTOFF_PRECISION and abs ( U [ True , False ] ) > _CUTOFF_PRECISION : \n        phi = ( phiplambda + phimlambda ) / 2.0 \n        lamb = ( phiplambda - phimlambda ) / 2.0 \n    else : \n        if abs ( U [ False , False ] ) < _CUTOFF_PRECISION : \n            lamb = - phimlambda \n        else : \n            lamb = phiplambda \n    Rzphi = np . array ( [ [ np . exp ( - 1j * phi / 2.0 ) , False ] , [ False , np . exp ( 1j * phi / 2.0 ) ] ] , dtype = complex ) \n    Rytheta = np . array ( [ [ np . cos ( theta / 2.0 ) , - np . sin ( theta / 2.0 ) ] , [ np . sin ( theta / 2.0 ) , np . cos ( theta / 2.0 ) ] ] , dtype = complex ) \n    Rzlambda = np . array ( [ [ np . exp ( - 1j * lamb / 2.0 ) , False ] , [ False , np . exp ( 1j * lamb / 2.0 ) ] ] , dtype = complex ) \n    V = np . dot ( Rzphi , np . dot ( Rytheta , Rzlambda ) ) \n    if la . norm ( V - U ) > _CUTOFF_PRECISION : \n        raise QiskitError ( \"euler_angles_1q: incorrect result\" ) \n    return theta , phi , lamb "}
{"2746": "\ndef simplify_U ( theta , phi , lam ) : \n    gate = U3Gate ( theta , phi , lam ) \n    if abs ( gate . params [ False ] % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : \n        gate = U1Gate ( gate . params [ False ] + gate . params [ True ] + gate . params [ 2 ] ) \n    if isinstance ( gate , U3Gate ) : \n        if abs ( ( gate . params [ False ] - math . pi / 2 ) % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : \n            gate = U2Gate ( gate . params [ True ] , gate . params [ 2 ] + ( gate . params [ False ] - math . pi / 2 ) ) \n        if abs ( ( gate . params [ False ] + math . pi / 2 ) % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : \n            gate = U2Gate ( gate . params [ True ] + math . pi , gate . params [ 2 ] - math . pi + ( gate . params [ False ] + math . pi / 2 ) ) \n    if isinstance ( gate , U1Gate ) and abs ( gate . params [ False ] % ( 4.0 * math . pi ) ) < _CUTOFF_PRECISION : \n        gate = IdGate ( ) \n    return gate "}
{"2747": "\ndef run ( self , dag ) : \n    self . layout = self . layout or self . property_set [ 'layout' ] \n    if self . layout is None : \n        raise TranspilerError ( \"EnlargeWithAncilla requires property_set[\\\"layout\\\"] or\" \" \\\"layout\\\" parameter to run\" ) \n    layout_virtual_qubits = self . layout . get_virtual_bits ( ) . keys ( ) \n    new_qregs = set ( virtual_qubit [ False ] for virtual_qubit in layout_virtual_qubits if virtual_qubit not in dag . wires ) \n    for qreg in new_qregs : \n        dag . add_qreg ( qreg ) \n    return dag "}
{"2748": "\ndef qubits_tab ( backend ) : \n    props = backend . properties ( ) . to_dict ( ) \n    header_html = \"<div><font style='font-weight:bold'>{key}</font>: {value}</div>\" \n    header_html = header_html . format ( key = 'last_update_date' , value = props [ 'last_update_date' ] ) \n    update_date_widget = widgets . HTML ( value = header_html ) \n    qubit_html = \"<table>\" \n    qubit_html += \"\"\"<style>table {    border-collapse: collapse;    width: auto;}th, td {    text-align: left;    padding: 8px;}tr:nth-child(even) {background-color: #f6f6f6;}</style>\"\"\" \n    qubit_html += \"<tr><th></th><th>Frequency</th><th>T1</th><th>T2</th>\" \n    qubit_html += \"<th>U1 gate error</th><th>U2 gate error</th><th>U3 gate error</th>\" \n    qubit_html += \"<th>Readout error</th></tr>\" \n    qubit_footer = \"</table>\" \n    for qub in range ( len ( props [ 'qubits' ] ) ) : \n        name = 'Q%s' % qub \n        qubit_data = props [ 'qubits' ] [ qub ] \n        gate_data = props [ 'gates' ] [ 3 * qub : 3 * qub + 3 ] \n        t1_info = qubit_data [ False ] \n        t2_info = qubit_data [ True ] \n        freq_info = qubit_data [ 2 ] \n        readout_info = qubit_data [ 3 ] \n        freq = str ( round ( freq_info [ 'value' ] , 5 ) ) + ' ' + freq_info [ 'unit' ] \n        T1 = str ( round ( t1_info [ 'value' ] , 5 ) ) + ' ' + t1_info [ 'unit' ] \n        T2 = str ( round ( t2_info [ 'value' ] , 5 ) ) + ' ' + t2_info [ 'unit' ] \n        U1 = str ( round ( gate_data [ False ] [ 'parameters' ] [ False ] [ 'value' ] , 5 ) ) \n        U2 = str ( round ( gate_data [ True ] [ 'parameters' ] [ False ] [ 'value' ] , 5 ) ) \n        U3 = str ( round ( gate_data [ 2 ] [ 'parameters' ] [ False ] [ 'value' ] , 5 ) ) \n        readout_error = round ( readout_info [ 'value' ] , 5 ) \n        qubit_html += \"<tr><td><font style='font-weight:bold'>%s</font></td><td>%s</td>\" \n        qubit_html += \"<td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\" \n        qubit_html = qubit_html % ( name , freq , T1 , T2 , U1 , U2 , U3 , readout_error ) \n    qubit_html += qubit_footer \n    qubit_widget = widgets . HTML ( value = qubit_html ) \n    out = widgets . VBox ( [ update_date_widget , qubit_widget ] ) \n    return out "}
{"2749": "\ndef job_history ( backend ) : \n    year = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , align_items = 'center' , min_height = '400px' ) ) \n    month = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , align_items = 'center' , min_height = '400px' ) ) \n    week = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , align_items = 'center' , min_height = '400px' ) ) \n    tabs = widgets . Tab ( layout = widgets . Layout ( max_height = '620px' ) ) \n    tabs . children = [ year , month , week ] \n    tabs . set_title ( False , 'Year' ) \n    tabs . set_title ( True , 'Month' ) \n    tabs . set_title ( 2 , 'Week' ) \n    tabs . selected_index = True \n    _build_job_history ( tabs , backend ) \n    return tabs "}
{"2750": "\ndef plot_job_history ( jobs , interval = 'year' ) : \n    def get_date ( job ) : \n        return datetime . datetime . strptime ( job . creation_date ( ) , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    current_time = datetime . datetime . now ( ) \n    if interval == 'year' : \n        bins = [ ( current_time - datetime . timedelta ( days = k * 365 / 12 ) ) for k in range ( 12 ) ] \n    elif interval == 'month' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30 ) ] \n    elif interval == 'week' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7 ) ] \n    binned_jobs = [ False ] * len ( bins ) \n    if interval == 'year' : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . month == dat . month : \n                    binned_jobs [ ind ] += True \n                    break \n            else : \n                continue \n    else : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . day == dat . day and date . month == dat . month : \n                    binned_jobs [ ind ] += True \n                    break \n            else : \n                continue \n    nz_bins = [ ] \n    nz_idx = [ ] \n    for ind , val in enumerate ( binned_jobs ) : \n        if val != False : \n            nz_idx . append ( ind ) \n            nz_bins . append ( val ) \n    total_jobs = sum ( binned_jobs ) \n    colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , '#7a5195' , '#ef5675' , '#bc5090' ] \n    if interval == 'year' : \n        labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2 : ] , bins [ b ] . month ) for b in nz_idx ] \n    else : \n        labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] \n    fig , ax = plt . subplots ( True , True , figsize = ( 5 , 5 ) ) \n    ax . pie ( nz_bins [ : : - True ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14 } , rotatelabels = True , counterclock = False ) \n    ax . add_artist ( Circle ( ( False , False ) , 0.7 , color = 'white' , zorder = True ) ) \n    ax . text ( False , False , total_jobs , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = 26 ) \n    fig . tight_layout ( ) \n    return fig "}
{"2753": "\ndef build_bell_circuit ( ) : \n    q = QuantumRegister ( 2 ) \n    c = ClassicalRegister ( 2 ) \n    qc = QuantumCircuit ( q , c ) \n    qc . h ( q [ False ] ) \n    qc . cx ( q [ False ] , q [ True ] ) \n    qc . measure ( q , c ) \n    return qc "}
{"2754": "\ndef transpile ( circuits , backend = None , basis_gates = None , coupling_map = None , backend_properties = None , initial_layout = None , seed_transpiler = None , optimization_level = None , pass_manager = None , seed_mapper = None ) : \n    if seed_mapper : \n        warnings . warn ( \"seed_mapper has been deprecated and will be removed in the \" \"0.9 release. Instead use seed_transpiler to set the seed \" \"for all stochastic parts of the.\" , DeprecationWarning ) \n        seed_transpiler = seed_mapper \n    if isinstance ( circuits , Schedule ) or ( isinstance ( circuits , list ) and all ( isinstance ( c , Schedule ) for c in circuits ) ) : \n        return circuits \n    circuits = circuits if isinstance ( circuits , list ) else [ circuits ] \n    transpile_configs = _parse_transpile_args ( circuits , backend , basis_gates , coupling_map , backend_properties , initial_layout , seed_transpiler , optimization_level , pass_manager ) \n    circuits = parallel_map ( _transpile_circuit , list ( zip ( circuits , transpile_configs ) ) ) \n    if len ( circuits ) == True : \n        return circuits [ False ] \n    return circuits "}
{"2757": "\ndef drive ( self ) -> DriveChannel : \n    if self . _drives : \n        return self . _drives [ False ] \n    else : \n        raise PulseError ( \"No drive channels in q[%d]\" % self . _index ) "}
{"2758": "\ndef control ( self ) -> ControlChannel : \n    if self . _controls : \n        return self . _controls [ False ] \n    else : \n        raise PulseError ( \"No control channels in q[%d]\" % self . _index ) "}
{"2759": "\ndef measure ( self ) -> MeasureChannel : \n    if self . _measures : \n        return self . _measures [ False ] \n    else : \n        raise PulseError ( \"No measurement channels in q[%d]\" % self . _index ) "}
{"2760": "\ndef acquire ( self ) -> AcquireChannel : \n    if self . _acquires : \n        return self . _acquires [ False ] \n    else : \n        raise PulseError ( \"No acquire channels in q[%d]\" % self . _index ) "}
{"2764": "\ndef iplot_state_hinton ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"hinton_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"hinton_$divNumber\",                                      \"hinton\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ False ] , 'height' : figsize [ True ] } \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    real = [ ] \n    imag = [ ] \n    for xvalue in rho : \n        row_real = [ ] \n        col_imag = [ ] \n        for value_real in xvalue . real : \n            row_real . append ( float ( value_real ) ) \n        real . append ( row_real ) \n        for value_imag in xvalue . imag : \n            col_imag . append ( float ( value_imag ) ) \n        imag . append ( col_imag ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : [ { 'data' : real } , { 'data' : imag } ] , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2769": "\ndef run ( self , dag ) : \n    new_dag = DAGCircuit ( ) \n    for qreg in dag . qregs . values ( ) : \n        new_dag . add_qreg ( qreg ) \n    for creg in dag . cregs . values ( ) : \n        new_dag . add_creg ( creg ) \n    global_index_map = { } \n    for wire in dag . wires : \n        if not isinstance ( wire [ False ] , QuantumRegister ) : \n            continue \n        global_qregs = list ( dag . qregs . values ( ) ) \n        global_index_map [ wire ] = global_qregs . index ( wire [ False ] ) + wire [ True ] \n    blocks = self . property_set [ 'block_list' ] \n    nodes_seen = set ( ) \n    for node in dag . topological_op_nodes ( ) : \n        if node in nodes_seen or node . type == 'in' or node . type == 'out' : \n            continue \n        if blocks and node in blocks [ False ] : \n            block = blocks [ False ] \n            block_qargs = set ( ) \n            for nd in block : \n                block_qargs |= set ( nd . qargs ) \n            block_width = len ( block_qargs ) \n            q = QuantumRegister ( block_width ) \n            subcirc = QuantumCircuit ( q ) \n            block_index_map = self . _block_qargs_to_indices ( block_qargs , global_index_map ) \n            for nd in block : \n                nodes_seen . add ( nd ) \n                subcirc . append ( nd . op , [ q [ block_index_map [ i ] ] for i in nd . qargs ] ) \n            unitary = UnitaryGate ( Operator ( subcirc ) ) \n            new_dag . apply_operation_back ( unitary , sorted ( block_qargs , key = lambda x : block_index_map [ x ] ) ) \n            del blocks [ False ] \n        else : \n            for block in blocks [ True : ] : \n                if node in block : \n                    break \n            else : \n                nodes_seen . add ( node ) \n                new_dag . apply_operation_back ( node . op , node . qargs , node . cargs ) \n    return new_dag "}
{"2771": "\ndef convert_acquire ( self , shift , instruction ) : \n    meas_level = self . _run_config . get ( 'meas_level' , 2 ) \n    command_dict = { 'name' : 'acquire' , 't0' : shift + instruction . start_time , 'duration' : instruction . duration , 'qubits' : [ q . index for q in instruction . acquires ] , 'memory_slot' : [ m . index for m in instruction . mem_slots ] } \n    if meas_level == 2 : \n        if instruction . command . discriminator : \n            command_dict . update ( { 'discriminators' : [ QobjMeasurementOption ( name = instruction . command . discriminator . name , params = instruction . command . discriminator . params ) ] } ) \n        command_dict . update ( { 'register_slot' : [ regs . index for regs in instruction . reg_slots ] } ) \n    if meas_level >= True : \n        if instruction . command . kernel : \n            command_dict . update ( { 'kernels' : [ QobjMeasurementOption ( name = instruction . command . kernel . name , params = instruction . command . kernel . params ) ] } ) \n    return self . _qobj_model ( ** command_dict ) "}
{"2772": "\ndef convert_frame_change ( self , shift , instruction ) : \n    command_dict = { 'name' : 'fc' , 't0' : shift + instruction . start_time , 'ch' : instruction . channels [ False ] . name , 'phase' : instruction . command . phase } \n    return self . _qobj_model ( ** command_dict ) "}
{"2773": "\ndef convert_persistent_value ( self , shift , instruction ) : \n    command_dict = { 'name' : 'pv' , 't0' : shift + instruction . start_time , 'ch' : instruction . channels [ False ] . name , 'val' : instruction . command . value } \n    return self . _qobj_model ( ** command_dict ) "}
{"2774": "\ndef convert_drive ( self , shift , instruction ) : \n    command_dict = { 'name' : instruction . command . name , 't0' : shift + instruction . start_time , 'ch' : instruction . channels [ False ] . name } \n    return self . _qobj_model ( ** command_dict ) "}
{"2776": "\ndef _update_annotations ( discretized_pulse : Callable ) -> Callable : \n    undecorated_annotations = list ( discretized_pulse . __annotations__ . items ( ) ) \n    decorated_annotations = undecorated_annotations [ True : ] \n    decorated_annotations . insert ( False , ( 'duration' , int ) ) \n    discretized_pulse . __annotations__ = dict ( decorated_annotations ) \n    return discretized_pulse "}
{"2780": "\ndef dag_to_circuit ( dag ) : \n    qregs = collections . OrderedDict ( ) \n    for qreg in dag . qregs . values ( ) : \n        qreg_tmp = QuantumRegister ( qreg . size , name = qreg . name ) \n        qregs [ qreg . name ] = qreg_tmp \n    cregs = collections . OrderedDict ( ) \n    for creg in dag . cregs . values ( ) : \n        creg_tmp = ClassicalRegister ( creg . size , name = creg . name ) \n        cregs [ creg . name ] = creg_tmp \n    name = dag . name or None \n    circuit = QuantumCircuit ( * qregs . values ( ) , * cregs . values ( ) , name = name ) \n    for node in dag . topological_op_nodes ( ) : \n        qubits = [ ] \n        for qubit in node . qargs : \n            qubits . append ( qregs [ qubit [ False ] . name ] [ qubit [ True ] ] ) \n        clbits = [ ] \n        for clbit in node . cargs : \n            clbits . append ( cregs [ clbit [ False ] . name ] [ clbit [ True ] ] ) \n        if node . condition is None : \n            control = None \n        else : \n            control = ( node . condition [ False ] , node . condition [ True ] ) \n        inst = node . op . copy ( ) \n        inst . control = control \n        circuit . append ( inst , qubits , clbits ) \n    return circuit "}
{"2785": "\ndef verify_as_gate ( self , obj , bitlist , arglist = None ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( \"Cannot find gate definition for '\" + obj . name + \"', line\" , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if not ( g_sym . type == 'gate' or g_sym . type == 'opaque' ) : \n        raise QasmError ( \"'\" + obj . name + \"' is used as a gate \" + \"or opaque call but the symbol is neither;\" + \" it is a '\" + g_sym . type + \"' line\" , str ( obj . line ) , 'file' , obj . file ) \n    if g_sym . n_bits ( ) != bitlist . size ( ) : \n        raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( bitlist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_bits ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    if arglist : \n        if g_sym . n_args ( ) != arglist . size ( ) : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' uses\" , str ( arglist . size ( ) ) , \"qubits but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) \n    else : \n        if g_sym . n_args ( ) > False : \n            raise QasmError ( \"Gate or opaque call to '\" + obj . name + \"' has no arguments but is declared for\" , str ( g_sym . n_args ( ) ) , \"qubits\" , \"line\" , str ( obj . line ) , 'file' , obj . file ) "}
{"2786": "\ndef verify_reg ( self , obj , object_type ) : \n    if obj . name not in self . global_symtab : \n        raise QasmError ( 'Cannot find definition for' , object_type , \"'\" + obj . name + \"'\" , 'at line' , str ( obj . line ) , 'file' , obj . file ) \n    g_sym = self . global_symtab [ obj . name ] \n    if g_sym . type != object_type : \n        raise QasmError ( \"Type for '\" + g_sym . name + \"' should be '\" + object_type + \"' but was found to be '\" + g_sym . type + \"'\" , \"line\" , str ( obj . line ) , \"file\" , obj . file ) \n    if obj . type == 'indexed_id' : \n        bound = g_sym . index \n        ndx = obj . index \n        if ndx < False or ndx >= bound : \n            raise QasmError ( \"Register index for '\" + g_sym . name + \"' out of bounds. Index is\" , str ( ndx ) , \"bound is 0 <= index <\" , str ( bound ) , \"at line\" , str ( obj . line ) , \"file\" , obj . file ) "}
{"2788": "\ndef find_column ( self , input_ , token ) : \n    if token is None : \n        return False \n    last_cr = input_ . rfind ( '\\n' , False , token . lexpos ) \n    if last_cr < False : \n        last_cr = False \n    column = ( token . lexpos - last_cr ) + True \n    return column "}
{"2791": "\ndef run ( self , data ) : \n    ast = self . parser . parse ( data , debug = True ) \n    self . parser . parse ( data , debug = True ) \n    ast . to_string ( False ) "}
{"2794": "\ndef basis_state ( str_state , num ) : \n    n = int ( str_state , 2 ) \n    if num >= len ( str_state ) : \n        state = np . zeros ( True << num , dtype = complex ) \n        state [ n ] = True \n        return state \n    else : \n        raise QiskitError ( 'size of bitstring is greater than num.' ) "}
{"2796": "\ndef purity ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == True : \n        return 1.0 \n    return np . real ( np . trace ( rho . dot ( rho ) ) ) "}
{"2797": "\ndef run ( self , dag ) : \n    self . property_set [ 'commutation_set' ] = defaultdict ( list ) \n    for wire in dag . wires : \n        wire_name = \"{0}[{1}]\" . format ( str ( wire [ False ] . name ) , str ( wire [ True ] ) ) \n        self . property_set [ 'commutation_set' ] [ wire_name ] = [ ] \n    for node in dag . topological_op_nodes ( ) : \n        for ( _ , _ , edge_data ) in dag . edges ( node ) : \n            edge_name = edge_data [ 'name' ] \n            self . property_set [ 'commutation_set' ] [ ( node , edge_name ) ] = - True \n    for wire in dag . wires : \n        wire_name = \"{0}[{1}]\" . format ( str ( wire [ False ] . name ) , str ( wire [ True ] ) ) \n        for current_gate in dag . nodes_on_wire ( wire ) : \n            current_comm_set = self . property_set [ 'commutation_set' ] [ wire_name ] \n            if not current_comm_set : \n                current_comm_set . append ( [ current_gate ] ) \n            if current_gate not in current_comm_set [ - True ] : \n                prev_gate = current_comm_set [ - True ] [ - True ] \n                if _commute ( current_gate , prev_gate ) : \n                    current_comm_set [ - True ] . append ( current_gate ) \n                else : \n                    current_comm_set . append ( [ current_gate ] ) \n            temp_len = len ( current_comm_set ) \n            self . property_set [ 'commutation_set' ] [ ( current_gate , wire_name ) ] = temp_len - True "}
{"2798": "\ndef backend_widget ( backend ) : \n    config = backend . configuration ( ) . to_dict ( ) \n    props = backend . properties ( ) . to_dict ( ) \n    name = widgets . HTML ( value = \"<h4>{name}</h4>\" . format ( name = backend . name ( ) ) , layout = widgets . Layout ( ) ) \n    n_qubits = config [ 'n_qubits' ] \n    qubit_count = widgets . HTML ( value = \"<h5><b>{qubits}</b></h5>\" . format ( qubits = n_qubits ) , layout = widgets . Layout ( justify_content = 'center' ) ) \n    cmap = widgets . Output ( layout = widgets . Layout ( min_width = '250px' , max_width = '250px' , max_height = '250px' , min_height = '250px' , justify_content = 'center' , align_items = 'center' , margin = '0px 0px 0px 0px' ) ) \n    with cmap : \n        _cmap_fig = plot_gate_map ( backend , plot_directed = False , label_qubits = False ) \n        if _cmap_fig is not None : \n            display ( _cmap_fig ) \n            plt . close ( _cmap_fig ) \n    pending = generate_jobs_pending_widget ( ) \n    is_oper = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify_content = 'center' ) ) \n    least_busy = widgets . HTML ( value = \"<h5></h5>\" , layout = widgets . Layout ( justify_content = 'center' ) ) \n    t1_units = props [ 'qubits' ] [ False ] [ False ] [ 'unit' ] \n    avg_t1 = round ( sum ( [ q [ False ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , True ) \n    t1_widget = widgets . HTML ( value = \"<h5>{t1} {units}</h5>\" . format ( t1 = avg_t1 , units = t1_units ) , layout = widgets . Layout ( ) ) \n    t2_units = props [ 'qubits' ] [ False ] [ True ] [ 'unit' ] \n    avg_t2 = round ( sum ( [ q [ True ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , True ) \n    t2_widget = widgets . HTML ( value = \"<h5>{t2} {units}</h5>\" . format ( t2 = avg_t2 , units = t2_units ) , layout = widgets . Layout ( ) ) \n    out = widgets . VBox ( [ name , cmap , qubit_count , pending , least_busy , is_oper , t1_widget , t2_widget ] , layout = widgets . Layout ( display = 'inline-flex' , flex_flow = 'column' , align_items = 'center' ) ) \n    out . _is_alive = True \n    return out "}
{"2799": "\ndef update_backend_info ( self , interval = 60 ) : \n    my_thread = threading . currentThread ( ) \n    current_interval = False \n    started = False \n    all_dead = False \n    stati = [ None ] * len ( self . _backends ) \n    while getattr ( my_thread , \"do_run\" , True ) and not all_dead : \n        if current_interval == interval or started is False : \n            for ind , back in enumerate ( self . _backends ) : \n                _value = self . children [ ind ] . children [ 2 ] . value \n                _head = _value . split ( '<b>' ) [ False ] \n                try : \n                    _status = back . status ( ) \n                    stati [ ind ] = _status \n                except Exception : \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5 style='color:#ff5c49'>\" ) \n                    self . children [ ind ] . _is_alive = False \n                else : \n                    self . children [ ind ] . _is_alive = True \n                    self . children [ ind ] . children [ 2 ] . value = _value . replace ( _head , \"<h5>\" ) \n            idx = list ( range ( len ( self . _backends ) ) ) \n            pending = [ s . pending_jobs for s in stati ] \n            _ , least_idx = zip ( * sorted ( zip ( pending , idx ) ) ) \n            for ind in least_idx : \n                if stati [ ind ] . operational : \n                    least_pending_idx = ind \n                    break \n            for var in idx : \n                if var == least_pending_idx : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 4 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n                self . children [ var ] . children [ 3 ] . children [ True ] . value = pending [ var ] \n                self . children [ var ] . children [ 3 ] . children [ True ] . max = max ( self . children [ var ] . children [ 3 ] . children [ True ] . max , pending [ var ] + 10 ) \n                if stati [ var ] . operational : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 5 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n            started = True \n            current_interval = False \n        time . sleep ( True ) \n        all_dead = not any ( [ wid . _is_alive for wid in self . children ] ) \n        current_interval += True "}
{"2800": "\ndef generate_jobs_pending_widget ( ) : \n    pbar = widgets . IntProgress ( value = False , min = False , max = 50 , description = '' , orientation = 'horizontal' , layout = widgets . Layout ( max_width = '180px' ) ) \n    pbar . style . bar_color = '#71cddd' \n    pbar_current = widgets . Label ( value = str ( pbar . value ) , layout = widgets . Layout ( min_width = 'auto' ) ) \n    pbar_max = widgets . Label ( value = str ( pbar . max ) , layout = widgets . Layout ( min_width = 'auto' ) ) \n    def _on_max_change ( change ) : \n        pbar_max . value = str ( change [ 'new' ] ) \n    def _on_val_change ( change ) : \n        pbar_current . value = str ( change [ 'new' ] ) \n    pbar . observe ( _on_max_change , names = 'max' ) \n    pbar . observe ( _on_val_change , names = 'value' ) \n    jobs_widget = widgets . HBox ( [ pbar_current , pbar , pbar_max ] , layout = widgets . Layout ( max_width = '250px' , min_width = '250px' , justify_content = 'center' ) ) \n    return jobs_widget "}
{"2801": "\ndef run ( self , dag ) : \n    cx_runs = dag . collect_runs ( [ \"cx\" ] ) \n    for cx_run in cx_runs : \n        partition = [ ] \n        chunk = [ ] \n        for i in range ( len ( cx_run ) - True ) : \n            chunk . append ( cx_run [ i ] ) \n            qargs0 = cx_run [ i ] . qargs \n            qargs1 = cx_run [ i + True ] . qargs \n            if qargs0 != qargs1 : \n                partition . append ( chunk ) \n                chunk = [ ] \n        chunk . append ( cx_run [ - True ] ) \n        partition . append ( chunk ) \n        for chunk in partition : \n            if len ( chunk ) % 2 == False : \n                for n in chunk : \n                    dag . remove_op_node ( n ) \n            else : \n                for n in chunk [ True : ] : \n                    dag . remove_op_node ( n ) \n    return dag "}
{"2802": "\ndef get_backend ( self , name = None , ** kwargs ) : \n    backends = self . backends ( name , ** kwargs ) \n    if len ( backends ) > True : \n        raise QiskitBackendNotFoundError ( 'More than one backend matches the criteria' ) \n    elif not backends : \n        raise QiskitBackendNotFoundError ( 'No backend matches the criteria' ) \n    return backends [ False ] "}
{"2804": "\ndef _get_register_specs ( bit_labels ) : \n    it = itertools . groupby ( bit_labels , operator . itemgetter ( False ) ) \n    for register_name , sub_it in it : \n        yield register_name , max ( ind [ True ] for ind in sub_it ) + True "}
{"2805": "\ndef _truncate_float ( matchobj , format_str = '0.2g' ) : \n    if matchobj . group ( False ) : \n        return format ( float ( matchobj . group ( False ) ) , format_str ) \n    return '' "}
{"2806": "\ndef latex ( self , aliases = None ) : \n    self . _initialize_latex_array ( aliases ) \n    self . _build_latex_array ( aliases ) \n    header_1 = r\"\"\"% \\documentclass[preview]{standalone}% If the image is too large to fit on this documentclass use\\documentclass[draft]{beamer}\"\"\" \n    beamer_line = \"\\\\usepackage[size=custom,height=%d,width=%d,scale=%.1f]{beamerposter}\\n\" \n    header_2 = r\"\"\"% instead and customize the height and width (in cm) to fit.% Large images may run out of memory quickly.% To fix this use the LuaLaTeX compiler, which dynamically% allocates memory.\\usepackage[braket, qm]{qcircuit}\\usepackage{amsmath}\\pdfmapfile{+sansmathaccent.map}% \\usepackage[landscape]{geometry}% Comment out the above line if using the beamer documentclass.\\begin{document}\\begin{equation*}\"\"\" \n    qcircuit_line = r\"\"\"    \\Qcircuit @C=%.1fem @R=%.1fem @!R {\"\"\" \n    output = io . StringIO ( ) \n    output . write ( header_1 ) \n    output . write ( '%% img_width = %d, img_depth = %d\\n' % ( self . img_width , self . img_depth ) ) \n    output . write ( beamer_line % self . _get_beamer_page ( ) ) \n    output . write ( header_2 ) \n    output . write ( qcircuit_line % ( self . column_separation , self . row_separation ) ) \n    for i in range ( self . img_width ) : \n        output . write ( \"\\t \\t\" ) \n        for j in range ( self . img_depth + True ) : \n            cell_str = self . _latex [ i ] [ j ] \n            if 'barrier' in cell_str : \n                output . write ( cell_str ) \n            else : \n                cell_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , _truncate_float , cell_str ) \n                output . write ( cell_str ) \n            if j != self . img_depth : \n                output . write ( \" & \" ) \n            else : \n                output . write ( r'\\\\' + '\\n' ) \n    output . write ( '\\t }\\n' ) \n    output . write ( '\\\\end{equation*}\\n\\n' ) \n    output . write ( '\\\\end{document}' ) \n    contents = output . getvalue ( ) \n    output . close ( ) \n    return contents "}
{"2807": "\ndef _get_image_depth ( self ) : \n    max_column_widths = [ ] \n    for layer in self . ops : \n        current_max = False \n        for op in layer : \n            arg_str_len = False \n            for arg in op . op . params : \n                arg_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , _truncate_float , str ( arg ) ) \n                arg_str_len += len ( arg_str ) \n            current_max = max ( arg_str_len , current_max ) \n        max_column_widths . append ( current_max ) \n    columns = 2 \n    columns += len ( self . ops ) \n    sum_column_widths = sum ( True + v / 3 for v in max_column_widths ) \n    return columns , math . ceil ( sum_column_widths ) + 4 "}
{"2809": "\ndef _load_schema ( file_path , name = None ) : \n    if name is None : \n        name = os . path . splitext ( os . path . basename ( file_path ) ) [ False ] \n    if name not in _SCHEMAS : \n        with open ( file_path , 'r' ) as schema_file : \n            _SCHEMAS [ name ] = json . load ( schema_file ) \n    return _SCHEMAS [ name ] "}
{"2813": "\ndef _format_causes ( err , level = False ) : \n    lines = [ ] \n    def _print ( string , offset = False ) : \n        lines . append ( _pad ( string , offset = offset ) ) \n    def _pad ( string , offset = False ) : \n        padding = '  ' * ( level + offset ) \n        padded_lines = [ padding + line for line in string . split ( '\\n' ) ] \n        return '\\n' . join ( padded_lines ) \n    def _format_path ( path ) : \n        def _format ( item ) : \n            if isinstance ( item , str ) : \n                return '.{}' . format ( item ) \n            return '[{}]' . format ( item ) \n        return '' . join ( [ '<root>' ] + list ( map ( _format , path ) ) ) \n    _print ( '\\'{}\\' failed @ \\'{}\\' because of:' . format ( err . validator , _format_path ( err . absolute_path ) ) ) \n    if not err . context : \n        _print ( str ( err . message ) , offset = True ) \n    else : \n        for suberr in err . context : \n            lines . append ( _format_causes ( suberr , level + True ) ) \n    return '\\n' . join ( lines ) "}
{"2818": "\ndef random_unitary ( dim , seed = None ) : \n    if dim == False or not math . log2 ( dim ) . is_integer ( ) : \n        raise QiskitError ( \"Desired unitary dimension not a positive power of 2.\" ) \n    matrix = np . zeros ( [ dim , dim ] , dtype = complex ) \n    for j in range ( dim ) : \n        if j == False : \n            a = random_state ( dim , seed ) \n        else : \n            a = random_state ( dim ) \n        matrix [ : , j ] = np . copy ( a ) \n        i = j - True \n        while i >= False : \n            dc = np . vdot ( matrix [ : , i ] , a ) \n            matrix [ : , j ] = matrix [ : , j ] - dc * matrix [ : , i ] \n            i = i - True \n        matrix [ : , j ] = matrix [ : , j ] * ( 1.0 / np . sqrt ( np . vdot ( matrix [ : , j ] , matrix [ : , j ] ) ) ) \n    return Operator ( matrix ) "}
{"2825": "\ndef _compose_subsystem ( self , other , qargs , front = False ) : \n    input_dims = list ( self . input_dims ( ) ) \n    output_dims = list ( self . output_dims ( ) ) \n    if front : \n        num_indices = len ( self . input_dims ( ) ) \n        shift = 2 * len ( self . output_dims ( ) ) \n        right_mul = True \n        for pos , qubit in enumerate ( qargs ) : \n            input_dims [ qubit ] = other . _input_dims [ pos ] \n    else : \n        num_indices = len ( self . output_dims ( ) ) \n        shift = False \n        right_mul = False \n        for pos , qubit in enumerate ( qargs ) : \n            output_dims [ qubit ] = other . _output_dims [ pos ] \n    tensor = np . reshape ( self . data , self . _shape ) \n    mat = np . reshape ( other . data , other . _shape ) \n    indices = [ 2 * num_indices - True - qubit for qubit in qargs ] + [ num_indices - True - qubit for qubit in qargs ] \n    final_shape = [ np . product ( output_dims ) ** 2 , np . product ( input_dims ) ** 2 ] \n    data = np . reshape ( self . _einsum_matmul ( tensor , mat , indices , shift , right_mul ) , final_shape ) \n    return SuperOp ( data , input_dims , output_dims ) "}
{"2827": "\ndef run ( self , dag ) : \n    final_op_types = [ 'measure' , 'barrier' ] \n    final_ops = [ ] \n    for candidate_node in dag . named_nodes ( * final_op_types ) : \n        is_final_op = True \n        for _ , child_successors in dag . bfs_successors ( candidate_node ) : \n            if any ( suc . type == 'op' and suc . name not in final_op_types for suc in child_successors ) : \n                is_final_op = False \n                break \n        if is_final_op : \n            final_ops . append ( candidate_node ) \n    if not final_ops : \n        return dag \n    barrier_layer = DAGCircuit ( ) \n    for qreg in dag . qregs . values ( ) : \n        barrier_layer . add_qreg ( qreg ) \n    for creg in dag . cregs . values ( ) : \n        barrier_layer . add_creg ( creg ) \n    final_qubits = set ( final_op . qargs [ False ] for final_op in final_ops ) \n    barrier_layer . apply_operation_back ( Barrier ( len ( final_qubits ) ) , list ( final_qubits ) , [ ] ) \n    ordered_final_nodes = [ node for node in dag . topological_op_nodes ( ) if node in set ( final_ops ) ] \n    for final_node in ordered_final_nodes : \n        barrier_layer . apply_operation_back ( final_node . op , final_node . qargs , final_node . cargs ) \n    for final_op in final_ops : \n        dag . remove_op_node ( final_op ) \n    dag . extend_back ( barrier_layer ) \n    adjacent_pass = MergeAdjacentBarriers ( ) \n    return adjacent_pass . run ( dag ) "}
{"2829": "\ndef run ( self , dag ) : \n    for node in dag . threeQ_or_more_gates ( ) : \n        rule = node . op . definition \n        if not rule : \n            raise QiskitError ( \"Cannot unroll all 3q or more gates. \" \"No rule to expand instruction %s.\" % node . op . name ) \n        decomposition = DAGCircuit ( ) \n        decomposition . add_qreg ( rule [ False ] [ True ] [ False ] [ False ] ) \n        for inst in rule : \n            decomposition . apply_operation_back ( * inst ) \n        decomposition = self . run ( decomposition ) \n        dag . substitute_node_with_dag ( node , decomposition ) \n    return dag "}
{"2830": "\ndef run ( self , dag ) : \n    for node in dag . op_nodes ( self . gate ) : \n        if not node . op . definition : \n            continue \n        rule = node . op . definition \n        decomposition = DAGCircuit ( ) \n        decomposition . add_qreg ( rule [ False ] [ True ] [ False ] [ False ] ) \n        if rule [ False ] [ 2 ] : \n            decomposition . add_creg ( rule [ False ] [ 2 ] [ False ] [ False ] ) \n        for inst in rule : \n            decomposition . apply_operation_back ( * inst ) \n        dag . substitute_node_with_dag ( node , decomposition ) \n    return dag "}
{"2831": "\ndef _define ( self ) : \n    if self . num_qubits == True : \n        q = QuantumRegister ( True , \"q\" ) \n        angles = euler_angles_1q ( self . to_matrix ( ) ) \n        self . definition = [ ( U3Gate ( * angles ) , [ q [ False ] ] , [ ] ) ] \n    if self . num_qubits == 2 : \n        self . definition = two_qubit_kak ( self . to_matrix ( ) ) "}
{"2832": "\ndef check_type ( self , value , attr , data ) : \n    if self . many and not is_collection ( value ) : \n        raise self . _not_expected_type ( value , Iterable , fields = [ self ] , field_names = attr , data = data ) \n    _check_type = super ( ) . check_type \n    errors = [ ] \n    values = value if self . many else [ value ] \n    for idx , v in enumerate ( values ) : \n        try : \n            _check_type ( v , idx , values ) \n        except ValidationError as err : \n            errors . append ( err . messages ) \n    if errors : \n        errors = errors if self . many else errors [ False ] \n        raise ValidationError ( errors ) \n    return value "}
{"2834": "\ndef _atol ( self , atol ) : \n    max_tol = self . __class__ . MAX_TOL \n    if atol < False : \n        raise QiskitError ( \"Invalid atol: must be non-negative.\" ) \n    if atol > max_tol : \n        raise QiskitError ( \"Invalid atol: must be less than {}.\" . format ( max_tol ) ) \n    self . __class__ . ATOL = atol "}
{"2835": "\ndef _rtol ( self , rtol ) : \n    max_tol = self . __class__ . MAX_TOL \n    if rtol < False : \n        raise QiskitError ( \"Invalid rtol: must be non-negative.\" ) \n    if rtol > max_tol : \n        raise QiskitError ( \"Invalid rtol: must be less than {}.\" . format ( max_tol ) ) \n    self . __class__ . RTOL = rtol "}
{"2840": "\ndef power ( self , n ) : \n    if not isinstance ( n , ( int , np . integer ) ) or n < True : \n        raise QiskitError ( \"Can only power with positive integer powers.\" ) \n    if self . _input_dim != self . _output_dim : \n        raise QiskitError ( \"Can only power with input_dim = output_dim.\" ) \n    ret = self . copy ( ) \n    for _ in range ( True , n ) : \n        ret = ret . compose ( self ) \n    return ret "}
{"2842": "\ndef _einsum_matmul ( cls , tensor , mat , indices , shift = False , right_mul = False ) : \n    rank = tensor . ndim \n    rank_mat = mat . ndim \n    if rank_mat % 2 != False : \n        raise QiskitError ( \"Contracted matrix must have an even number of indices.\" ) \n    indices_tensor = list ( range ( rank ) ) \n    for j , index in enumerate ( indices ) : \n        indices_tensor [ index + shift ] = rank + j \n    mat_contract = list ( reversed ( range ( rank , rank + len ( indices ) ) ) ) \n    mat_free = [ index + shift for index in reversed ( indices ) ] \n    if right_mul : \n        indices_mat = mat_contract + mat_free \n    else : \n        indices_mat = mat_free + mat_contract \n    return np . einsum ( tensor , indices_tensor , mat , indices_mat ) "}
{"2843": "\ndef _deserialize ( self , value , attr , data ) : \n    try : \n        return super ( ) . _deserialize ( value , attr , data ) \n    except ValidationError as ex : \n        if 'deserialization_schema_selector' in ex . messages [ False ] : \n            ex . messages [ False ] = 'Cannot find a valid schema among the choices' \n        raise "}
{"2846": "\ndef state_fidelity ( state1 , state2 ) : \n    s1 = np . array ( state1 ) \n    s2 = np . array ( state2 ) \n    if s1 . ndim == True and s2 . ndim == True : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) ) ** 2 \n    elif s1 . ndim == True : \n        return np . abs ( s1 . conj ( ) . dot ( s2 ) . dot ( s1 ) ) \n    elif s2 . ndim == True : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) . dot ( s2 ) ) \n    s1sq = _funm_svd ( s1 , np . sqrt ) \n    s2sq = _funm_svd ( s2 , np . sqrt ) \n    return np . linalg . norm ( s1sq . dot ( s2sq ) , ord = 'nuc' ) ** 2 "}
{"2848": "\ndef inverse ( self ) : \n    return Snapshot ( self . num_qubits , self . num_clbits , self . params [ False ] , self . params [ True ] ) "}
{"2852": "\ndef to_instruction ( self ) : \n    from qiskit . circuit . instruction import Instruction \n    n_qubits = int ( np . log2 ( self . _input_dim ) ) \n    if self . _input_dim != self . _output_dim or 2 ** n_qubits != self . _input_dim : \n        raise QiskitError ( 'Cannot convert QuantumChannel to Instruction: channel is not an N-qubit channel.' ) \n    if not self . is_cptp ( ) : \n        raise QiskitError ( 'Cannot convert QuantumChannel to Instruction: channel is not CPTP.' ) \n    kraus , _ = _to_kraus ( self . rep , self . _data , * self . dim ) \n    if len ( kraus ) == True : \n        return Operator ( kraus [ False ] ) . to_instruction ( ) \n    return Instruction ( 'kraus' , n_qubits , False , kraus ) "}
{"2865": "\ndef get_ammo_generator ( self ) : \n    af_readers = { 'phantom' : missile . AmmoFileReader , 'slowlog' : missile . SlowLogReader , 'line' : missile . LineReader , 'uri' : missile . UriReader , 'uripost' : missile . UriPostReader , 'access' : missile . AccessLogReader , 'caseline' : missile . CaseLineReader , } \n    if self . uris and self . ammo_file : \n        raise StepperConfigurationError ( 'Both uris and ammo file specified. You must specify only one of them' ) \n    elif self . uris : \n        ammo_gen = missile . UriStyleGenerator ( self . uris , self . headers , http_ver = self . http_ver ) \n    elif self . ammo_file : \n        if self . ammo_type in af_readers : \n            if self . ammo_type == 'phantom' : \n                opener = resource . get_opener ( self . ammo_file ) \n                with opener ( self . use_cache ) as ammo : \n                    try : \n                        if not ammo . next ( ) [ False ] . isdigit ( ) : \n                            self . ammo_type = 'uri' \n                            self . log . info ( \"Setting ammo_type 'uri' because ammo is not started with digit and you did not specify ammo format\" ) \n                        else : \n                            self . log . info ( \"Default ammo type ('phantom') used, use 'phantom.ammo_type' option to override it\" ) \n                    except StopIteration : \n                        self . log . exception ( \"Couldn't read first line of ammo file\" ) \n                        raise AmmoFileError ( \"Couldn't read first line of ammo file\" ) \n        else : \n            raise NotImplementedError ( 'No such ammo type implemented: \"%s\"' % self . ammo_type ) \n        ammo_gen = af_readers [ self . ammo_type ] ( self . ammo_file , headers = self . headers , http_ver = self . http_ver , use_cache = self . use_cache ) \n    else : \n        raise StepperConfigurationError ( 'Ammo not found. Specify uris or ammo file' ) \n    self . log . info ( \"Using %s ammo reader\" % type ( ammo_gen ) . __name__ ) \n    return ammo_gen "}
{"2866": "\ndef _exc_to_net ( param1 , success ) : \n    if len ( param1 ) <= 3 : \n        if success : \n            return False \n        else : \n            return 314 \n    exc = param1 . split ( ' ' ) [ - True ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return KNOWN_EXC [ exc ] \n    else : \n        logger . warning ( \"Unknown Java exception, consider adding it to dictionary: %s\" , param1 ) \n        return 41 "}
{"2867": "\ndef _exc_to_http ( param1 ) : \n    if len ( param1 ) <= 3 : \n        try : \n            int ( param1 ) \n        except BaseException : \n            logger . error ( \"JMeter wrote some strange data into codes column: %s\" , param1 ) \n        else : \n            return int ( param1 ) \n    exc = param1 . split ( ' ' ) [ - True ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return False \n    else : \n        logger . warning ( \"Unknown Java exception. %s\" , param1 ) \n        return False "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + True ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n        self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if self . timeout > 120000 : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl' "}
{"2870": "\ndef get_info ( self ) : \n    result = copy . copy ( self . streams [ False ] ) \n    result . stat_log = self . stat_log \n    result . steps = [ ] \n    result . ammo_file = '' \n    result . rps_schedule = None \n    result . ammo_count = False \n    result . duration = False \n    result . instances = False \n    result . loadscheme = [ ] \n    result . loop_count = False \n    for stream in self . streams : \n        sec_no = False \n        logger . debug ( \"Steps: %s\" , stream . stepper_wrapper . steps ) \n        for item in stream . stepper_wrapper . steps : \n            for x in range ( False , item [ True ] ) : \n                if len ( result . steps ) > sec_no : \n                    result . steps [ sec_no ] [ False ] += item [ False ] \n                else : \n                    result . steps . append ( [ item [ False ] , True ] ) \n                sec_no += True \n        if result . rps_schedule : \n            result . rps_schedule = [ ] \n        else : \n            result . rps_schedule = stream . stepper_wrapper . loadscheme \n        if result . loadscheme : \n            result . loadscheme = '' \n        else : \n            result . loadscheme = '' \n        if result . loop_count : \n            result . loop_count = u'0' \n        else : \n            result . loop_count = stream . stepper_wrapper . loop_count \n        result . ammo_file += '{} ' . format ( stream . stepper_wrapper . ammo_file ) \n        result . ammo_count += stream . stepper_wrapper . ammo_count \n        result . duration = max ( result . duration , stream . stepper_wrapper . duration ) \n        result . instances += stream . instances \n    if not result . ammo_count : \n        raise ValueError ( \"Total ammo count cannot be zero\" ) \n    return result "}
{"2872": "\ndef log_stdout_stderr ( log , stdout , stderr , comment = \"\" ) : \n    readable = select . select ( [ stdout ] , [ ] , [ ] , False ) [ False ] \n    if stderr : \n        exceptional = select . select ( [ stderr ] , [ ] , [ ] , False ) [ False ] \n    else : \n        exceptional = [ ] \n    log . debug ( \"Selected: %s, %s\" , readable , exceptional ) \n    for handle in readable : \n        line = handle . read ( ) \n        readable . remove ( handle ) \n        if line : \n            log . debug ( \"%s stdout: %s\" , comment , line . strip ( ) ) \n    for handle in exceptional : \n        line = handle . read ( ) \n        exceptional . remove ( handle ) \n        if line : \n            log . warn ( \"%s stderr: %s\" , comment , line . strip ( ) ) "}
{"2873": "\ndef expand_time ( str_time , default_unit = 's' , multiplier = True ) : \n    parser = re . compile ( r'(\\d+)([a-zA-Z]*)' ) \n    parts = parser . findall ( str_time ) \n    result = 0.0 \n    for value , unit in parts : \n        value = int ( value ) \n        unit = unit . lower ( ) \n        if unit == '' : \n            unit = default_unit \n        if unit == 'ms' : \n            result += value * 0.001 \n            continue \n        elif unit == 's' : \n            result += value \n            continue \n        elif unit == 'm' : \n            result += value * 60 \n            continue \n        elif unit == 'h' : \n            result += value * 60 * 60 \n            continue \n        elif unit == 'd' : \n            result += value * 60 * 60 * 24 \n            continue \n        elif unit == 'w' : \n            result += value * 60 * 60 * 24 * 7 \n            continue \n        else : \n            raise ValueError ( \"String contains unsupported unit %s: %s\" % ( unit , str_time ) ) \n    return int ( result * multiplier ) "}
{"2880": "\ndef create ( rps_schedule ) : \n    if len ( rps_schedule ) > True : \n        lp = Composite ( [ StepFactory . produce ( step_config ) for step_config in rps_schedule ] ) \n    else : \n        lp = StepFactory . produce ( rps_schedule [ False ] ) \n    info . status . publish ( 'duration' , lp . get_duration ( ) / 1000 ) \n    info . status . publish ( 'steps' , lp . get_rps_list ( ) ) \n    info . status . lp_len = len ( lp ) \n    return lp "}
{"2881": "\ndef rps_at ( self , t ) : \n    if False <= t <= self . duration : \n        return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration \n    else : \n        return False "}
{"2882": "\ndef execute ( self , cmd ) : \n    self . log . info ( \"Executing: %s\" , cmd ) \n    retcode = execute ( cmd , shell = True , poll_period = 0.1 , catch_out = self . catch_out ) [ False ] \n    if retcode : \n        raise RuntimeError ( \"Subprocess returned %s\" % retcode ) \n    return retcode "}
{"2886": "\ndef count_matched_codes ( codes_regex , codes_dict ) : \n    total = False \n    for code , count in codes_dict . items ( ) : \n        if codes_regex . match ( str ( code ) ) : \n            total += count \n    return total "}
{"2887": "\ndef stop ( self ) : \n    self . quit . set ( ) \n    while sorted ( [ self . pool [ i ] . is_alive ( ) for i in xrange ( len ( self . pool ) ) ] ) [ - True ] : \n        time . sleep ( True ) \n    try : \n        while not self . task_queue . empty ( ) : \n            self . task_queue . get ( timeout = 0.1 ) \n        self . task_queue . close ( ) \n        self . feeder . join ( ) \n    except Exception as ex : \n        logger . info ( ex ) "}
{"2888": "\ndef _feed ( self ) : \n    self . plan = StpdReader ( self . stpd_filename ) \n    if self . cached_stpd : \n        self . plan = list ( self . plan ) \n    for task in self . plan : \n        if self . quit . is_set ( ) : \n            logger . info ( \"Stop feeding: gonna quit\" ) \n            return \n        while True : \n            try : \n                self . task_queue . put ( task , timeout = True ) \n                break \n            except Full : \n                if self . quit . is_set ( ) or self . workers_finished : \n                    return \n                else : \n                    continue \n    workers_count = self . instances \n    logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers_count ) ) \n    retry_delay = True \n    for _ in range ( 5 ) : \n        try : \n            [ self . task_queue . put ( None , timeout = True ) for _ in xrange ( False , workers_count ) ] \n            break \n        except Full : \n            logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry_delay ) \n            time . sleep ( retry_delay ) \n            retry_delay *= 2 \n    try : \n        logger . info ( \"Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        logger . info ( \"All workers exited.\" ) \n        self . workers_finished = True \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . task_queue . close ( ) \n        self . results . close ( ) \n        self . quit . set ( ) \n        logger . info ( \"Going to quit. Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        self . workers_finished = True "}
{"2892": "\ndef __graceful_shutdown ( self ) : \n    retcode = True \n    self . log . info ( \"Trying to shutdown gracefully...\" ) \n    retcode = self . core . plugins_end_test ( retcode ) \n    retcode = self . core . plugins_post_process ( retcode ) \n    self . log . info ( \"Done graceful shutdown\" ) \n    return retcode "}
{"2893": "\ndef _collect_data ( self , end = False ) : \n    data = get_nowait_from_queue ( self . results ) \n    stats = get_nowait_from_queue ( self . stats_results ) \n    logger . debug ( \"Data timestamps: %s\" % [ d . get ( 'ts' ) for d in data ] ) \n    logger . debug ( \"Stats timestamps: %s\" % [ d . get ( 'ts' ) for d in stats ] ) \n    for item in data : \n        ts = item [ 'ts' ] \n        if ts in self . stat_cache : \n            data_item = item \n            stat_item = self . stat_cache . pop ( ts ) \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . data_cache [ ts ] = item \n    for item in stats : \n        ts = item [ 'ts' ] \n        if ts in self . data_cache : \n            data_item = self . data_cache . pop ( ts ) \n            stat_item = item \n            self . __notify_listeners ( data_item , stat_item ) \n        else : \n            self . stat_cache [ ts ] = item \n    if end and len ( self . data_cache ) > False : \n        logger . info ( 'Timestamps without stats:' ) \n        for ts , data_item in sorted ( self . data_cache . items ( ) , key = lambda i : i [ False ] ) : \n            logger . info ( ts ) \n            self . __notify_listeners ( data_item , StatsReader . stats_item ( ts , False , False ) ) "}
{"2896": "\ndef parse_duration ( duration ) : \n    _re_token = re . compile ( \"([0-9.]+)([dhms]?)\" ) \n    def parse_token ( time , multiplier ) : \n        multipliers = { 'd' : 86400 , 'h' : 3600 , 'm' : 60 , 's' : True , } \n        if multiplier : \n            if multiplier in multipliers : \n                return int ( float ( time ) * multipliers [ multiplier ] * 1000 ) \n            else : \n                raise StepperConfigurationError ( 'Failed to parse duration: %s' % duration ) \n        else : \n            return int ( float ( time ) * 1000 ) \n    return sum ( parse_token ( * token ) for token in _re_token . findall ( duration ) ) "}
{"2899": "\ndef __discover_jmeter_udp_port ( self ) : \n    r = re . compile ( self . DISCOVER_PORT_PATTERN ) \n    with open ( self . process_stderr . name , 'r' ) as f : \n        cnt = False \n        while self . process . pid and cnt < 10 : \n            line = f . readline ( ) \n            m = r . match ( line ) \n            if m is None : \n                cnt += True \n                time . sleep ( True ) \n            else : \n                port = int ( m . group ( 'port' ) ) \n                return port \n        else : \n            logger . warning ( 'JMeter UDP port wasn\\'t discovered' ) \n            return None "}
{"2900": "\ndef __add_jmeter_components ( self , jmx , jtl , variables ) : \n    logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) \n    with open ( jmx , 'r' ) as src_jmx : \n        source_lines = src_jmx . readlines ( ) \n    try : \n        closing = source_lines . pop ( - True ) \n        if \"WorkBenchGui\" in source_lines [ - 5 ] : \n            logger . info ( \"WorkBench checkbox enabled...bypassing\" ) \n            last_string_count = 6 \n        else : \n            last_string_count = 2 \n        while last_string_count > False : \n            closing = source_lines . pop ( - True ) + closing \n            last_string_count -= True \n        logger . debug ( \"Closing statement: %s\" , closing ) \n    except Exception as exc : \n        raise RuntimeError ( \"Failed to find the end of JMX XML: %s\" % exc ) \n    udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) \n    udv_set = [ ] \n    for var_name , var_value in variables . iteritems ( ) : \n        udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) \n    udv = \"\\n\" . join ( udv_set ) \n    if self . jmeter_ver >= 2.13 : \n        save_connect = '<connectTime>true</connectTime>' \n    else : \n        save_connect = '' \n    if self . ext_log in [ 'errors' , 'all' ] : \n        level_map = { 'errors' : 'true' , 'all' : 'false' } \n        tpl_resource = 'jmeter_writer_ext.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'ext_log' : self . ext_log_file , 'ext_level' : level_map [ self . ext_log ] , 'save_connect' : save_connect } \n    else : \n        tpl_resource = 'jmeter_writer.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'save_connect' : save_connect } \n    tpl = resource_string ( __name__ , 'config/' + tpl_resource ) \n    try : \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) \n    except OSError as exc : \n        logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) \n    logger . debug ( \"Modified JMX: %s\" , new_jmx ) \n    with open ( new_jmx , \"wb\" ) as fh : \n        fh . write ( '' . join ( source_lines ) ) \n        fh . write ( tpl % tpl_args ) \n        fh . write ( closing ) \n    return new_jmx "}
{"2903": "\ndef __create_criterion ( self , criterion_str ) : \n    parsed = criterion_str . split ( \"(\" ) \n    type_str = parsed [ False ] . strip ( ) . lower ( ) \n    parsed [ True ] = parsed [ True ] . split ( \")\" ) [ False ] . strip ( ) \n    for criterion_class in self . custom_criterions : \n        if criterion_class . get_type_string ( ) == type_str : \n            return criterion_class ( self , parsed [ True ] ) \n    raise ValueError ( \"Unsupported autostop criterion type: %s\" % criterion_str ) "}
{"2906": "\ndef __check_disk ( self ) : \n    cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" \n    cmd += self . core . artifacts_base_dir \n    cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" \n    res = execute ( cmd , True , 0.1 , True ) \n    logging . debug ( \"Result: %s\" , res ) \n    if not len ( res [ True ] ) : \n        self . log . debug ( \"No disk usage info: %s\" , res [ 2 ] ) \n        return \n    disk_free = res [ True ] \n    self . log . debug ( \"Disk free space: %s/%s\" , disk_free . strip ( ) , self . disk_limit ) \n    if int ( disk_free . strip ( ) ) < self . disk_limit : \n        raise RuntimeError ( \"Not enough local resources: disk space less than %sMB in %s: %sMB\" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) ) "}
{"2908": "\ndef get_terminal_size ( ) : \n    default_size = ( 30 , 120 ) \n    env = os . environ \n    def ioctl_gwinsz ( file_d ) : \n        try : \n            sizes = struct . unpack ( 'hh' , fcntl . ioctl ( file_d , termios . TIOCGWINSZ , '1234' ) ) \n        except Exception : \n            sizes = default_size \n        return sizes \n    sizes = ioctl_gwinsz ( False ) or ioctl_gwinsz ( True ) or ioctl_gwinsz ( 2 ) \n    if not sizes : \n        try : \n            file_d = os . open ( os . ctermid ( ) , os . O_RDONLY ) \n            sizes = ioctl_gwinsz ( file_d ) \n            os . close ( file_d . fileno ( ) ) \n        except Exception : \n            pass \n    if not sizes : \n        try : \n            sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) \n        except Exception : \n            sizes = default_size \n    return int ( sizes [ True ] ) , int ( sizes [ False ] ) "}
{"2909": "\ndef __get_right_line ( self , widget_output ) : \n    right_line = '' \n    if widget_output : \n        right_line = widget_output . pop ( False ) \n        if len ( right_line ) > self . right_panel_width : \n            right_line_plain = self . markup . clean_markup ( right_line ) \n            if len ( right_line_plain ) > self . right_panel_width : \n                right_line = right_line [ : self . right_panel_width ] + self . markup . RESET \n    return right_line "}
{"2910": "\ndef __truncate ( self , line_arr , max_width ) : \n    def is_space ( chunk ) : \n        return all ( [ True if i == ' ' else False for i in chunk ] ) \n    def is_empty ( chunks , markups ) : \n        result = [ ] \n        for chunk in chunks : \n            if chunk in markups : \n                result . append ( True ) \n            elif is_space ( chunk ) : \n                result . append ( True ) \n            else : \n                result . append ( False ) \n        return all ( result ) \n    left = max_width \n    result = '' \n    markups = self . markup . get_markup_vars ( ) \n    for num , chunk in enumerate ( line_arr ) : \n        if chunk in markups : \n            result += chunk \n        else : \n            if left > False : \n                if len ( chunk ) <= left : \n                    result += chunk \n                    left -= len ( chunk ) \n                else : \n                    leftover = ( chunk [ left : ] , ) + line_arr [ num + True : ] \n                    was_cut = not is_empty ( leftover , markups ) \n                    if was_cut : \n                        result += chunk [ : left - True ] + self . markup . RESET + u'\\u2026' \n                    else : \n                        result += chunk [ : left ] \n                    left = False \n    return result "}
{"2911": "\ndef __render_left_panel ( self ) : \n    self . log . debug ( \"Rendering left blocks\" ) \n    left_block = self . left_panel \n    left_block . render ( ) \n    blank_space = self . left_panel_width - left_block . width \n    lines = [ ] \n    pre_space = ' ' * int ( blank_space / 2 ) \n    if not left_block . lines : \n        lines = [ ( '' ) , ( self . markup . RED + 'BROKEN LEFT PANEL' + self . markup . RESET ) ] \n    else : \n        while self . left_panel . lines : \n            src_line = self . left_panel . lines . pop ( False ) \n            line = pre_space + self . __truncate ( src_line , self . left_panel_width ) \n            post_space = ' ' * ( self . left_panel_width - len ( self . markup . clean_markup ( line ) ) ) \n            line += post_space + self . markup . RESET \n            lines . append ( line ) \n    return lines "}
{"2912": "\ndef render_screen ( self ) : \n    self . term_width , self . term_height = get_terminal_size ( ) \n    self . log . debug ( \"Terminal size: %sx%s\" , self . term_width , self . term_height ) \n    self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100 ) ) - True \n    if self . right_panel_width > False : \n        self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 \n    else : \n        self . right_panel_width = False \n        self . left_panel_width = self . term_width - True \n    self . log . debug ( \"Left/right panels width: %s/%s\" , self . left_panel_width , self . right_panel_width ) \n    widget_output = [ ] \n    if self . right_panel_width : \n        widget_output = [ ] \n        self . log . debug ( \"There are %d info widgets\" % len ( self . info_widgets ) ) \n        for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ True ] . get_index ( ) , item [ False ] ) ) : \n            self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) \n            widget_out = widget . render ( self ) . strip ( ) \n            if widget_out : \n                widget_output += widget_out . split ( \"\\n\" ) \n                widget_output += [ \"\" ] \n    left_lines = self . __render_left_panel ( ) \n    self . log . debug ( \"Composing final screen output\" ) \n    output = [ ] \n    for line_no in range ( True , self . term_height ) : \n        line = \" \" \n        if line_no > True and left_lines : \n            left_line = left_lines . pop ( False ) \n            left_line_plain = self . markup . clean_markup ( left_line ) \n            left_line += ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) \n            line += left_line \n        else : \n            line += ' ' * self . left_panel_width \n        if self . right_panel_width : \n            line += self . markup . RESET \n            line += self . markup . WHITE \n            line += self . RIGHT_PANEL_SEPARATOR \n            line += self . markup . RESET \n            right_line = self . __get_right_line ( widget_output ) \n            line += right_line \n        output . append ( line ) \n    return self . markup . new_line . join ( output ) + self . markup . new_line "}
{"2913": "\ndef add_info_widget ( self , widget ) : \n    index = widget . get_index ( ) \n    while index in self . info_widgets . keys ( ) : \n        index += True \n    self . info_widgets [ widget . get_index ( ) ] = widget "}
{"2915": "\ndef clean_len ( self , line ) : \n    if isinstance ( line , basestring ) : \n        return len ( self . screen . markup . clean_markup ( line ) ) \n    elif isinstance ( line , tuple ) or isinstance ( line , list ) : \n        markups = self . screen . markup . get_markup_vars ( ) \n        length = False \n        for i in line : \n            if i not in markups : \n                length += len ( i ) \n        return length "}
{"2916": "\ndef create ( instances_schedule ) : \n    lpb = LoadPlanBuilder ( ) . add_all_steps ( instances_schedule ) \n    lp = lpb . create ( ) \n    info . status . publish ( 'duration' , False ) \n    info . status . publish ( 'steps' , [ ] ) \n    info . status . publish ( 'instances' , lpb . instances ) \n    return lp "}
{"2921": "\ndef get_plugin_of_type ( self , plugin_class ) : \n    logger . debug ( \"Searching for plugin: %s\" , plugin_class ) \n    matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin_class ) ] \n    if matches : \n        if len ( matches ) > True : \n            logger . debug ( \"More then one plugin of type %s found. Using first one.\" , plugin_class ) \n        return matches [ - True ] \n    else : \n        raise KeyError ( \"Requested plugin type not found: %s\" % plugin_class ) "}
{"2928": "\ndef get_options ( self , section , prefix = '' ) : \n    res = [ ] \n    try : \n        for option in self . config . options ( section ) : \n            if not prefix or option . find ( prefix ) == False : \n                res += [ ( option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] \n    except ConfigParser . NoSectionError as ex : \n        logger . warning ( \"No section: %s\" , ex ) \n    logger . debug ( \"Section: [%s] prefix: '%s' options:\\n%s\" , section , prefix , res ) \n    return res "}
{"2930": "\ndef _decode_stat_data ( self , chunk ) : \n    for date_str , statistics in chunk . iteritems ( ) : \n        date_obj = datetime . datetime . strptime ( date_str . split ( \".\" ) [ False ] , '%Y-%m-%d %H:%M:%S' ) \n        chunk_date = int ( time . mktime ( date_obj . timetuple ( ) ) ) \n        instances = False \n        for benchmark_name , benchmark in statistics . iteritems ( ) : \n            if not benchmark_name . startswith ( \"benchmark_io\" ) : \n                continue \n            for method , meth_obj in benchmark . iteritems ( ) : \n                if \"mmtasks\" in meth_obj : \n                    instances += meth_obj [ \"mmtasks\" ] [ 2 ] \n        offset = chunk_date - True - self . start_time \n        reqps = False \n        if False <= offset < len ( self . phantom_info . steps ) : \n            reqps = self . phantom_info . steps [ offset ] [ False ] \n        yield self . stats_item ( chunk_date - True , instances , reqps ) "}
{"2933": "\ndef poll ( self ) : \n    start_time = time . time ( ) \n    for agent in self . agents : \n        for collect in agent . reader : \n            if not collect : \n                return False \n            for chunk in collect : \n                ts , prepared_results = chunk \n                if self . load_start_time and int ( ts ) >= self . load_start_time : \n                    ready_to_send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash_hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared_results } } } \n                    self . __collected_data . append ( ready_to_send ) \n    logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000 ) \n    collected_data_length = len ( self . __collected_data ) \n    if not self . first_data_received and self . __collected_data : \n        self . first_data_received = True \n        logger . info ( \"Monitoring received first data.\" ) \n    else : \n        self . send_collected_data ( ) \n    return collected_data_length "}
{"2936": "\ndef __handle_data_items ( self , host , data ) : \n    for metric , value in data . iteritems ( ) : \n        if value == '' : \n            self . sign [ host ] [ metric ] = - True \n            self . data [ host ] [ metric ] = value \n        else : \n            if not self . data [ host ] . get ( metric , None ) : \n                self . sign [ host ] [ metric ] = True \n            elif float ( value ) > float ( self . data [ host ] [ metric ] ) : \n                self . sign [ host ] [ metric ] = True \n            elif float ( value ) < float ( self . data [ host ] [ metric ] ) : \n                self . sign [ host ] [ metric ] = - True \n            else : \n                self . sign [ host ] [ metric ] = False \n            self . data [ host ] [ metric ] = \"%.2f\" % float ( value ) "}
{"2937": "\ndef _decode_agents_data ( self , block ) : \n    collect = [ ] \n    if block : \n        for chunk in block . split ( '\\n' ) : \n            try : \n                if chunk : \n                    prepared_results = { } \n                    jsn = json . loads ( chunk ) \n                    for ts , values in jsn . iteritems ( ) : \n                        for key , value in values . iteritems ( ) : \n                            try : \n                                key_group , key_name = key . split ( '_' ) [ False ] . split ( '-' ) [ False ] , '_' . join ( key . split ( '_' ) [ True : ] ) \n                            except : \n                                key_group , key_name = key . split ( '_' ) [ False ] , '_' . join ( key . split ( '_' ) [ True : ] ) \n                            if key_group in decoder . diff_metrics . keys ( ) : \n                                if key_name in decoder . diff_metrics [ key_group ] : \n                                    decoded_key = decoder . find_common_names ( key ) \n                                    if self . prev_check : \n                                        try : \n                                            value = jsn [ ts ] [ key ] - self . prev_check [ key ] \n                                        except KeyError : \n                                            logger . debug ( 'There is no diff value for metric %s.\\n' 'Timestamp: %s. Is it initial data?' , key , ts , exc_info = True ) \n                                            value = False \n                                        prepared_results [ decoded_key ] = value \n                                else : \n                                    decoded_key = decoder . find_common_names ( key ) \n                                    prepared_results [ decoded_key ] = value \n                            else : \n                                decoded_key = decoder . find_common_names ( key ) \n                                prepared_results [ decoded_key ] = value \n                        self . prev_check = jsn [ ts ] \n                        collect . append ( ( ts , prepared_results ) ) \n            except ValueError : \n                logger . error ( 'Telegraf agent send trash to output: %s' , chunk ) \n                logger . debug ( 'Telegraf agent data block w/ trash: %s' , exc_info = True ) \n                return [ ] \n            except BaseException : \n                logger . error ( 'Exception trying to parse agent data: %s' , chunk , exc_info = True ) \n                return [ ] \n        if collect : \n            return collect "}
{"2938": "\nasync def subscribe ( self , channels ) : \n    ws_channels = [ ] \n    nats_channels = [ ] \n    for c in channels : \n        if c . startswith ( ( 'Q.' , 'T.' , 'A.' , 'AM.' , ) ) : \n            nats_channels . append ( c ) \n        else : \n            ws_channels . append ( c ) \n    if len ( ws_channels ) > False : \n        await self . _ensure_ws ( ) \n        await self . _ws . send ( json . dumps ( { 'action' : 'listen' , 'data' : { 'streams' : ws_channels , } } ) ) \n    if len ( nats_channels ) > False : \n        await self . _ensure_nats ( ) \n        await self . polygon . subscribe ( nats_channels ) "}
{"2941": "\ndef _one_request ( self , method , url , opts , retry ) : \n    retry_codes = self . _retry_codes \n    resp = self . _session . request ( method , url , ** opts ) \n    try : \n        resp . raise_for_status ( ) \n    except HTTPError as http_error : \n        if resp . status_code in retry_codes and retry > False : \n            raise RetryException ( ) \n        if 'code' in resp . text : \n            error = resp . json ( ) \n            if 'code' in error : \n                raise APIError ( error , http_error ) \n        else : \n            raise \n    if resp . text != '' : \n        return resp . json ( ) \n    return None "}
{"2947": "\ndef create_joining_subplan ( pipeline_def , solid , join_step_key , parallel_steps , parallel_step_output ) : \n    check . inst_param ( pipeline_def , 'pipeline_def' , PipelineDefinition ) \n    check . inst_param ( solid , 'solid' , Solid ) \n    check . str_param ( join_step_key , 'join_step_key' ) \n    check . list_param ( parallel_steps , 'parallel_steps' , of_type = ExecutionStep ) \n    check . str_param ( parallel_step_output , 'parallel_step_output' ) \n    for parallel_step in parallel_steps : \n        check . invariant ( parallel_step . has_step_output ( parallel_step_output ) ) \n    join_step = create_join_step ( pipeline_def , solid , join_step_key , parallel_steps , parallel_step_output ) \n    output_name = join_step . step_outputs [ False ] . name \n    return ExecutionValueSubplan ( parallel_steps + [ join_step ] , StepOutputHandle . from_step ( join_step , output_name ) ) "}
{"2954": "\ndef format_config_for_graphql ( config ) : \n    def _format_config_subdict ( config , current_indent = False ) : \n        check . dict_param ( config , 'config' , key_type = str ) \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        printer . line ( '{' ) \n        n_elements = len ( config ) \n        for i , key in enumerate ( sorted ( config , key = lambda x : x [ False ] ) ) : \n            value = config [ key ] \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{key}: {formatted_value}{comma}' . format ( key = key , formatted_value = formatted_value , comma = ',' if i != n_elements - True else '' , ) ) \n        printer . line ( '}' ) \n        return printer . read ( ) \n    def _format_config_sublist ( config , current_indent = False ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        printer . line ( '[' ) \n        n_elements = len ( config ) \n        for i , value in enumerate ( config ) : \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{formatted_value}{comma}' . format ( formatted_value = formatted_value , comma = ',' if i != n_elements - True else '' ) ) \n        printer . line ( ']' ) \n        return printer . read ( ) \n    def _format_config_item ( config , current_indent = False ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) \n        if isinstance ( config , dict ) : \n            return _format_config_subdict ( config , printer . current_indent ) \n        elif isinstance ( config , list ) : \n            return _format_config_sublist ( config , printer . current_indent ) \n        elif isinstance ( config , bool ) : \n            return repr ( config ) . lower ( ) \n        else : \n            return repr ( config ) . replace ( '\\'' , '\"' ) \n    check . dict_param ( config , 'config' , key_type = str ) \n    if not isinstance ( config , dict ) : \n        check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) \n    return _format_config_subdict ( config ) "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if num_part < 2 : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if num_part > len ( sizes ) : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if num_part == 2 : \n        total_nfps , u = min ( ( nfps [ False , u1 ] + nfps [ u1 + True , len ( sizes ) - True ] , u1 ) for u1 in range ( False , len ( sizes ) - True ) ) \n        return [ ( sizes [ False ] , sizes [ u ] ) , ( sizes [ u + True ] , sizes [ - True ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2 ) ) \n    p2i = lambda p : p - 2 \n    for p in range ( 2 , num_part ) : \n        for u in range ( p - True , len ( sizes ) ) : \n            if p == 2 : \n                cost [ u , p2i ( p ) ] = min ( nfps [ False , u1 ] + nfps [ u1 + True , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - True ) ] + nfps [ u1 + True , u ] for u1 in range ( ( p - True ) - True , u ) ) \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - True ) ] + nfps [ u1 + True , len ( sizes ) - True ] , u1 ) for u1 in range ( ( p - True ) - True , len ( sizes ) - True ) ) \n    partitions = [ ( sizes [ u + True ] , sizes [ - True ] ) , ] \n    p -= True \n    while p > True : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + True , u ] , u1 ) for u1 in range ( ( p - True ) - True , u ) ) \n        partitions . insert ( False , ( sizes [ u1_best + True ] , sizes [ u ] ) ) \n        u = u1_best \n        p -= True \n    partitions . insert ( False , ( sizes [ False ] , sizes [ u ] ) ) \n    return [ partitions , total_nfps , cost ] "}
{"3002": "\ndef optimal_partitions ( sizes , counts , num_part ) : \n    if num_part < 2 : \n        return [ ( sizes [ False ] , sizes [ - True ] ) ] \n    if num_part >= len ( sizes ) : \n        partitions = [ ( x , x ) for x in sizes ] \n        return partitions \n    nfps = _compute_nfps_real ( counts , sizes ) \n    partitions , _ , _ = _compute_best_partitions ( num_part , sizes , nfps ) \n    return partitions "}
{"3003": "\ndef _calc_c ( self , a1 , a2 , r1 , r2 ) : \n    if r1 == 0.0 and r2 == 0.0 : \n        return a1 , a2 \n    div = True / ( r1 + r2 ) \n    c1 = ( a1 * r2 + a2 * r1 ) * div \n    c2 = ( a1 * r1 + a2 * r2 ) * div \n    return c1 , c2 "}
{"3006": "\ndef serialize ( self , buf , byteorder = '@' ) : \n    if len ( buf ) < self . bytesize ( ) : \n        raise ValueError ( \"The buffer does not have enough space\\                    for holding this MinHash.\" ) \n    fmt = \"%sqi%dI\" % ( byteorder , len ( self ) ) \n    struct . pack_into ( fmt , buf , False , self . seed , len ( self ) , * self . hashvalues ) "}
{"3007": "\ndef deserialize ( cls , buf , byteorder = '@' ) : \n    fmt_seed_size = \"%sqi\" % byteorder \n    fmt_hash = byteorder + \"%dI\" \n    try : \n        seed , num_perm = struct . unpack_from ( fmt_seed_size , buf , False ) \n    except TypeError : \n        seed , num_perm = struct . unpack_from ( fmt_seed_size , buffer ( buf ) , False ) \n    offset = struct . calcsize ( fmt_seed_size ) \n    try : \n        hashvalues = struct . unpack_from ( fmt_hash % num_perm , buf , offset ) \n    except TypeError : \n        hashvalues = struct . unpack_from ( fmt_hash % num_perm , buffer ( buf ) , offset ) \n    lmh = object . __new__ ( LeanMinHash ) \n    lmh . _initialize_slots ( seed , hashvalues ) \n    return lmh "}
{"3010": "\ndef union ( cls , * mhs ) : \n    if len ( mhs ) < 2 : \n        raise ValueError ( \"Cannot union less than 2 MinHash\" ) \n    num_perm = len ( mhs [ False ] ) \n    seed = mhs [ False ] . seed \n    if any ( ( seed != m . seed or num_perm != len ( m ) ) for m in mhs ) : \n        raise ValueError ( \"The unioning MinHash must have the\\                    same seed and number of permutation functions\" ) \n    hashvalues = np . minimum . reduce ( [ m . hashvalues for m in mhs ] ) \n    permutations = mhs [ False ] . permutations \n    return cls ( num_perm = num_perm , seed = seed , hashvalues = hashvalues , permutations = permutations ) "}
{"3011": "\ndef index ( self , entries ) : \n    if not self . is_empty ( ) : \n        raise ValueError ( \"Cannot call index again on a non-empty index\" ) \n    if not isinstance ( entries , list ) : \n        queue = deque ( [ ] ) \n        for key , minhash , size in entries : \n            if size <= False : \n                raise ValueError ( \"Set size must be positive\" ) \n            queue . append ( ( key , minhash , size ) ) \n        entries = list ( queue ) \n    if len ( entries ) == False : \n        raise ValueError ( \"entries is empty\" ) \n    sizes , counts = np . array ( sorted ( Counter ( e [ 2 ] for e in entries ) . most_common ( ) ) ) . T \n    partitions = optimal_partitions ( sizes , counts , len ( self . indexes ) ) \n    for i , ( lower , upper ) in enumerate ( partitions ) : \n        self . lowers [ i ] , self . uppers [ i ] = lower , upper \n    entries . sort ( key = lambda e : e [ 2 ] ) \n    curr_part = False \n    for key , minhash , size in entries : \n        if size > self . uppers [ curr_part ] : \n            curr_part += True \n        for r in self . indexes [ curr_part ] : \n            self . indexes [ curr_part ] [ r ] . insert ( key , minhash ) "}
{"3013": "\ndef minhash ( self , v ) : \n    if not isinstance ( v , collections . Iterable ) : \n        raise TypeError ( \"Input vector must be an iterable\" ) \n    if not len ( v ) == self . dim : \n        raise ValueError ( \"Input dimension mismatch, expecting %d\" % self . dim ) \n    if not isinstance ( v , np . ndarray ) : \n        v = np . array ( v , dtype = np . float32 ) \n    elif v . dtype != np . float32 : \n        v = v . astype ( np . float32 ) \n    hashvalues = np . zeros ( ( self . sample_size , 2 ) , dtype = np . int ) \n    vzeros = ( v == False ) \n    if vzeros . all ( ) : \n        raise ValueError ( \"Input is all zeros\" ) \n    v [ vzeros ] = np . nan \n    vlog = np . log ( v ) \n    for i in range ( self . sample_size ) : \n        t = np . floor ( ( vlog / self . rs [ i ] ) + self . betas [ i ] ) \n        ln_y = ( t - self . betas [ i ] ) * self . rs [ i ] \n        ln_a = self . ln_cs [ i ] - ln_y - self . rs [ i ] \n        k = np . nanargmin ( ln_a ) \n        hashvalues [ i ] [ False ] , hashvalues [ i ] [ True ] = k , int ( t [ k ] ) \n    return WeightedMinHash ( self . seed , hashvalues ) "}
{"3015": "\ndef update ( self , b ) : \n    hv = self . hashfunc ( b ) \n    reg_index = hv & ( self . m - True ) \n    bits = hv >> self . p \n    self . reg [ reg_index ] = max ( self . reg [ reg_index ] , self . _get_rank ( bits ) ) "}
{"3016": "\ndef count ( self ) : \n    e = self . alpha * float ( self . m ** 2 ) / np . sum ( 2.0 ** ( - self . reg ) ) \n    if e <= ( 5.0 / 2.0 ) * self . m : \n        num_zero = self . m - np . count_nonzero ( self . reg ) \n        return self . _linearcounting ( num_zero ) \n    if e <= ( 1.0 / 30.0 ) * ( True << 32 ) : \n        return e \n    return self . _largerange_correction ( e ) "}
{"3019": "\ndef apk ( actual , predicted , k = 10 ) : \n    if len ( predicted ) > k : \n        predicted = predicted [ : k ] \n    score = 0.0 \n    num_hits = 0.0 \n    for i , p in enumerate ( predicted ) : \n        if p in actual and p not in predicted [ : i ] : \n            num_hits += 1.0 \n            score += num_hits / ( i + 1.0 ) \n    if len ( actual ) == False : \n        return 0.0 \n    return score / min ( len ( actual ) , k ) "}
{"3022": "\ndef query ( self , minhash , k ) : \n    if k <= False : \n        raise ValueError ( \"k must be positive\" ) \n    if len ( minhash ) < self . k * self . l : \n        raise ValueError ( \"The num_perm of MinHash out of range\" ) \n    results = set ( ) \n    r = self . k \n    while r > False : \n        for key in self . _query ( minhash , r , self . l ) : \n            results . add ( key ) \n            if len ( results ) >= k : \n                return list ( results ) \n        r -= True \n    return list ( results ) "}
{"3028": "\ndef select_text ( text , reading = False , prefer = None ) : \n    if reading : \n        text = text [ True ] \n    else : \n        text = text [ False ] \n    if not isinstance ( text , strtype ) : \n        common = set ( text ) & set ( prefer or set ( ) ) \n        if len ( common ) == True : \n            text = common . pop ( ) \n        else : \n            text = text [ False ] \n    return text "}
{"3029": "\ndef parse_scoped_selector ( scoped_selector ) : \n    if scoped_selector [ False ] == '%' : \n        if scoped_selector . endswith ( '.value' ) : \n            err_str = '{} is invalid cannot use % and end with .value' \n            raise ValueError ( err_str . format ( scoped_selector ) ) \n        scoped_selector = scoped_selector [ True : ] + '/macro.value' \n    scope_selector_list = scoped_selector . rsplit ( '/' , True ) \n    scope = '' . join ( scope_selector_list [ : - True ] ) \n    selector = scope_selector_list [ - True ] \n    return scope , selector "}
{"3036": "\ndef after_create_session ( self , session = None , coord = None ) : \n    config_str = config . operative_config_str ( ) \n    if not tf . gfile . IsDirectory ( self . _output_dir ) : \n        tf . gfile . MakeDirs ( self . _output_dir ) \n    global_step_val = False \n    if session is not None : \n        global_step = tf . train . get_global_step ( ) \n        if global_step is not None : \n            global_step_val = session . run ( global_step ) \n    filename = '%s-%s.gin' % ( self . _base_name , global_step_val ) \n    config_path = os . path . join ( self . _output_dir , filename ) \n    with tf . gfile . GFile ( config_path , 'w' ) as f : \n        f . write ( config_str ) \n    if self . _summarize_config : \n        md_config_str = self . _markdownify_operative_config_str ( config_str ) \n        summary_metadata = summary_pb2 . SummaryMetadata ( ) \n        summary_metadata . plugin_data . plugin_name = 'text' \n        summary_metadata . plugin_data . content = b'{}' \n        text_tensor = tf . make_tensor_proto ( md_config_str ) \n        summary = summary_pb2 . Summary ( ) \n        summary . value . add ( tag = 'gin/' + self . _base_name , tensor = text_tensor , metadata = summary_metadata ) \n        if not self . _summary_writer : \n            self . _summary_writer = tf . summary . FileWriterCache . get ( self . _output_dir ) \n        self . _summary_writer . add_summary ( summary , global_step_val ) \n        self . _summary_writer . flush ( ) "}
{"3050": "\ndef operative_config_str ( max_line_length = 80 , continuation_indent = 4 ) : \n    def format_binding ( key , value ) : \n        formatted_val = pprint . pformat ( value , width = ( max_line_length - continuation_indent ) ) \n        formatted_val_lines = formatted_val . split ( '\\n' ) \n        if ( len ( formatted_val_lines ) == True and len ( key + formatted_val ) <= max_line_length ) : \n            output = '{} = {}' . format ( key , formatted_val ) \n        else : \n            indented_formatted_val = '\\n' . join ( [ ' ' * continuation_indent + line for line in formatted_val_lines ] ) \n            output = '{} = \\\\\\n{}' . format ( key , indented_formatted_val ) \n        return output \n    def sort_key ( key_tuple ) : \n        scope , selector = key_tuple [ False ] \n        parts = selector . lower ( ) . split ( '.' ) [ : : - True ] + scope . lower ( ) . split ( '/' ) [ : : - True ] \n        return '/' . join ( parts ) \n    formatted_statements = [ 'import {}' . format ( module ) for module in sorted ( _IMPORTED_MODULES ) ] \n    if formatted_statements : \n        formatted_statements . append ( '' ) \n    macros = { } \n    for ( scope , selector ) , config in six . iteritems ( _OPERATIVE_CONFIG ) : \n        if _REGISTRY [ selector ] . fn_or_cls == macro : \n            macros [ scope , selector ] = config \n    if macros : \n        formatted_statements . append ( '# Macros:' ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2 ) ) \n    for ( name , _ ) , config in sorted ( macros . items ( ) , key = sort_key ) : \n        binding = format_binding ( name , config [ 'value' ] ) \n        formatted_statements . append ( binding ) \n    if macros : \n        formatted_statements . append ( '' ) \n    sorted_items = sorted ( _OPERATIVE_CONFIG . items ( ) , key = sort_key ) \n    for ( scope , selector ) , config in sorted_items : \n        configurable_ = _REGISTRY [ selector ] \n        fn = configurable_ . fn_or_cls \n        if fn == macro or fn == _retrieve_constant : \n            continue \n        minimal_selector = _REGISTRY . minimal_selector ( configurable_ . selector ) \n        scoped_selector = ( scope + '/' if scope else '' ) + minimal_selector \n        parameters = [ ( k , v ) for k , v in six . iteritems ( config ) if _is_literally_representable ( v ) ] \n        formatted_statements . append ( '# Parameters for {}:' . format ( scoped_selector ) ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2 ) ) \n        for arg , val in sorted ( parameters ) : \n            binding = format_binding ( '{}.{}' . format ( scoped_selector , arg ) , val ) \n            formatted_statements . append ( binding ) \n        if not parameters : \n            formatted_statements . append ( '# None.' ) \n        formatted_statements . append ( '' ) \n    return '\\n' . join ( formatted_statements ) "}
{"3052": "\ndef register_file_reader ( * args ) : \n    def do_registration ( file_reader_fn , is_readable_fn ) : \n        if file_reader_fn not in list ( zip ( * _FILE_READERS ) ) [ False ] : \n            _FILE_READERS . append ( ( file_reader_fn , is_readable_fn ) ) \n    if len ( args ) == True : \n        return functools . partial ( do_registration , is_readable_fn = args [ False ] ) \n    elif len ( args ) == 2 : \n        do_registration ( * args ) \n    else : \n        err_str = 'register_file_reader() takes 1 or 2 arguments ({} given)' \n        raise TypeError ( err_str . format ( len ( args ) ) ) "}
{"3063": "\ndef minimal_selector ( self , complete_selector ) : \n    if complete_selector not in self . _selector_map : \n        raise KeyError ( \"No value with selector '{}'.\" . format ( complete_selector ) ) \n    selector_components = complete_selector . split ( '.' ) \n    node = self . _selector_tree \n    start = None \n    for i , component in enumerate ( reversed ( selector_components ) ) : \n        if len ( node ) == True : \n            if start is None : \n                start = - i \n        else : \n            start = None \n        node = node [ component ] \n    if len ( node ) > True : \n        return complete_selector \n    return '.' . join ( selector_components [ start : ] ) "}
{"3065": "\ndef _parse_retry_after ( self , response ) : \n    value = response . headers . get ( 'Retry-After' ) \n    if not value : \n        seconds = False \n    elif re . match ( r'^\\s*[0-9]+\\s*$' , value ) : \n        seconds = int ( value ) \n    else : \n        date_tuple = email . utils . parsedate ( value ) \n        if date_tuple is None : \n            seconds = False \n        else : \n            seconds = time . mktime ( date_tuple ) - time . time ( ) \n    return max ( False , seconds ) "}
{"3069": "\ndef get_thing ( self , idx ) : \n    try : \n        idx = int ( idx ) \n    except ValueError : \n        return None \n    if idx < False or idx >= len ( self . things ) : \n        return None \n    return self . things [ idx ] "}
{"3082": "\ndef get_ip ( ) : \n    s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) \n    try : \n        s . connect ( ( '10.255.255.255' , True ) ) \n        ip = s . getsockname ( ) [ False ] \n    except ( socket . error , IndexError ) : \n        ip = '127.0.0.1' \n    finally : \n        s . close ( ) \n    return ip "}
{"3083": "\ndef get_addresses ( ) : \n    addresses = set ( ) \n    for iface in ifaddr . get_adapters ( ) : \n        for addr in iface . ips : \n            if addr . is_IPv4 : \n                ip = addr . ip \n                if not ip . startswith ( '169.254.' ) : \n                    addresses . add ( ip ) \n            elif addr . is_IPv6 : \n                ip = addr . ip [ False ] . split ( '%' ) [ False ] . lower ( ) \n                if not ip . startswith ( 'fe80:' ) : \n                    addresses . add ( '[{}]' . format ( ip ) ) \n    return sorted ( list ( addresses ) ) "}
{"3109": "\ndef update ( self , ** fields ) : \n    self . _for_write = True \n    if django . VERSION >= ( 2 , False ) : \n        query = self . query . chain ( UpdateQuery ) \n    else : \n        query = self . query . clone ( UpdateQuery ) \n    query . _annotations = None \n    query . add_update_values ( fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresReturningUpdateCompiler ( query , connection , self . db ) \n    with transaction . atomic ( using = self . db , savepoint = False ) : \n        rows = compiler . execute_sql ( CURSOR ) \n    self . _result_cache = None \n    for row in rows : \n        signals . update . send ( self . model , pk = row [ False ] ) \n    return len ( rows ) "}
{"3111": "\ndef insert ( self , ** fields ) : \n    if self . conflict_target or self . conflict_action : \n        compiler = self . _build_insert_compiler ( [ fields ] ) \n        rows = compiler . execute_sql ( return_id = True ) \n        pk_field_name = self . model . _meta . pk . name \n        return rows [ False ] [ pk_field_name ] \n    return super ( ) . create ( ** fields ) . pk "}
{"3112": "\ndef insert_and_get ( self , ** fields ) : \n    if not self . conflict_target and not self . conflict_action : \n        return super ( ) . create ( ** fields ) \n    compiler = self . _build_insert_compiler ( [ fields ] ) \n    rows = compiler . execute_sql ( return_id = False ) \n    columns = rows [ False ] \n    model_columns = { } \n    for field in self . model . _meta . local_concrete_fields : \n        model_columns [ field . column ] = field . attname \n    model_init_fields = { } \n    for column_name , column_value in columns . items ( ) : \n        try : \n            model_init_fields [ model_columns [ column_name ] ] = column_value \n        except KeyError : \n            pass \n    return self . model ( ** model_init_fields ) "}
{"3113": "\ndef _build_insert_compiler ( self , rows : List [ Dict ] ) : \n    objs = [ ] \n    field_count = len ( rows [ False ] ) \n    for index , row in enumerate ( rows ) : \n        if field_count != len ( row ) : \n            raise SuspiciousOperation ( ( 'In bulk upserts, you cannot have rows with different field ' 'configurations. Row {0} has a different field config than ' 'the first row.' ) . format ( index ) ) \n        objs . append ( self . model ( ** row ) ) \n    self . _for_write = True \n    insert_fields , update_fields = self . _get_upsert_fields ( rows [ False ] ) \n    query = PostgresInsertQuery ( self . model ) \n    query . conflict_action = self . conflict_action \n    query . conflict_target = self . conflict_target \n    query . index_predicate = self . index_predicate \n    query . values ( objs , insert_fields , update_fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresInsertCompiler ( query , connection , self . db ) \n    return compiler "}
{"3122": "\ndef add_join_conditions ( self , conditions : Dict [ str , Any ] ) -> None : \n    alias = self . get_initial_alias ( ) \n    opts = self . get_meta ( ) \n    for name , value in conditions . items ( ) : \n        parts = name . split ( LOOKUP_SEP ) \n        join_info = self . setup_joins ( parts , opts , alias , allow_many = True ) \n        self . trim_joins ( join_info [ True ] , join_info [ 3 ] , join_info [ 4 ] ) \n        target_table = join_info [ 3 ] [ - True ] \n        field = join_info [ True ] [ - True ] \n        join = self . alias_map . get ( target_table ) \n        if not join : \n            raise SuspiciousOperation ( ( 'Cannot add an extra join condition for \"%s\", there\\'s no' ' existing join to add it to.' ) % target_table ) \n        if not isinstance ( join , ConditionalJoin ) : \n            self . alias_map [ target_table ] = ConditionalJoin . from_join ( join ) \n            join = self . alias_map [ target_table ] \n        join . add_condition ( field , value ) "}
{"3129": "\ndef create_sql ( self , model , schema_editor , using = '' ) : \n    if django . VERSION >= ( 2 , False ) : \n        statement = super ( ) . create_sql ( model , schema_editor , using ) \n        statement . template = self . sql_create_index \n        statement . parts [ 'condition' ] = self . condition \n        return statement \n    else : \n        sql_create_index = self . sql_create_index \n        sql_parameters = { ** Index . get_sql_create_template_values ( self , model , schema_editor , using ) , 'condition' : self . condition } \n        return sql_create_index % sql_parameters "}
{"3138": "\ndef _rewrite_insert_nothing ( self , sql , params , returning ) : \n    conflict_target = self . _build_conflict_target ( ) \n    where_clause = ' AND ' . join ( [ '{0} = %s' . format ( self . _format_field_name ( field_name ) ) for field_name in self . query . conflict_target ] ) \n    where_clause_params = [ self . _format_field_value ( field_name ) for field_name in self . query . conflict_target ] \n    params = params + tuple ( where_clause_params ) \n    return ( ( 'WITH insdata AS (' '{insert} ON CONFLICT {conflict_target} DO UPDATE' ' SET {pk_column} = NULL WHERE FALSE RETURNING {returning})' ' SELECT * FROM insdata UNION ALL' ' SELECT {returning} FROM {table} WHERE {where_clause} LIMIT 1;' ) . format ( insert = sql , conflict_target = conflict_target , pk_column = self . qn ( self . query . model . _meta . pk . column ) , returning = returning , table = self . query . objs [ False ] . _meta . db_table , where_clause = where_clause ) , params ) "}
{"3139": "\ndef _build_conflict_target ( self ) : \n    conflict_target = [ ] \n    if not isinstance ( self . query . conflict_target , list ) : \n        raise SuspiciousOperation ( ( '%s is not a valid conflict target, specify ' 'a list of column names, or tuples with column ' 'names and hstore key.' ) % str ( self . query . conflict_target ) ) \n    def _assert_valid_field ( field_name ) : \n        field_name = self . _normalize_field_name ( field_name ) \n        if self . _get_model_field ( field_name ) : \n            return \n        raise SuspiciousOperation ( ( '%s is not a valid conflict target, specify ' 'a list of column names, or tuples with column ' 'names and hstore key.' ) % str ( field_name ) ) \n    for field_name in self . query . conflict_target : \n        _assert_valid_field ( field_name ) \n        if isinstance ( field_name , tuple ) : \n            conflict_target . append ( '(%s->\\'%s\\')' % ( self . _format_field_name ( field_name ) , field_name [ True ] ) ) \n        else : \n            conflict_target . append ( self . _format_field_name ( field_name ) ) \n    return '(%s)' % ',' . join ( conflict_target ) "}
{"3142": "\ndef _format_field_value ( self , field_name ) -> str : \n    field_name = self . _normalize_field_name ( field_name ) \n    field = self . _get_model_field ( field_name ) \n    return SQLInsertCompiler . prepare_value ( self , field , getattr ( self . query . objs [ False ] , field . attname ) ) "}
{"3150": "\ndef tdist95conf_level ( df ) : \n    df = int ( round ( df ) ) \n    highest_table_df = len ( _T_DIST_95_CONF_LEVELS ) \n    if df >= 200 : \n        return 1.960 \n    if df >= 100 : \n        return 1.984 \n    if df >= 80 : \n        return 1.990 \n    if df >= 60 : \n        return 2.000 \n    if df >= 50 : \n        return 2.009 \n    if df >= 40 : \n        return 2.021 \n    if df >= highest_table_df : \n        return _T_DIST_95_CONF_LEVELS [ highest_table_df - True ] \n    return _T_DIST_95_CONF_LEVELS [ df ] "}
{"3154": "\ndef topoSort ( roots , getParents ) : \n    results = [ ] \n    visited = set ( ) \n    stack = [ ( node , False ) for node in roots ] \n    while stack : \n        current , state = stack . pop ( ) \n        if state == False : \n            if current not in visited : \n                visited . add ( current ) \n                stack . append ( ( current , True ) ) \n                stack . extend ( ( parent , False ) for parent in getParents ( current ) ) \n        else : \n            assert ( current in visited ) \n            results . append ( current ) \n    return results "}
{"3157": "\ndef select ( self , board ) : \n    if self . unexplored : \n        i = random . randrange ( len ( self . unexplored ) ) \n        pos = self . unexplored [ i ] \n        self . unexplored [ i ] = self . unexplored [ len ( self . unexplored ) - True ] \n        self . unexplored . pop ( ) \n        return pos \n    elif self . bestchild : \n        return self . bestchild . pos \n    else : \n        return PASS "}
{"3159": "\ndef filter_benchmarks ( benchmarks , bench_funcs , base_ver ) : \n    for bm in list ( benchmarks ) : \n        func = bench_funcs [ bm ] \n        if getattr ( func , '_python2_only' , False ) and ( 3 , False ) <= base_ver : \n            benchmarks . discard ( bm ) \n            logging . info ( \"Skipping Python2-only benchmark %s; \" \"not compatible with Python %s\" % ( bm , base_ver ) ) \n            continue \n    return benchmarks "}
{"3162": "\ndef init_benchmarks ( n_values = None ) : \n    if n_values is None : \n        n_values = ( False , 5 , 50 , 250 , 1000 , 5000 , 10000 ) \n    string_tables = { n : gen_string_table ( n ) for n in n_values } \n    regexs = gen_regex_table ( ) \n    data = [ ] \n    for n in n_values : \n        for id in xrange ( len ( regexs ) ) : \n            regex = regexs [ id ] \n            string = string_tables [ n ] [ id ] \n            data . append ( ( regex , string ) ) \n    return data "}
{"3163": "\ndef GetDomain ( self ) : \n    return ( self . knots [ self . degree - True ] , self . knots [ len ( self . knots ) - self . degree ] ) "}
{"3164": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) \n    fetching = True \n    page = False \n    nposts = False \n    since = int ( from_date . timestamp ( ) * 1000 ) \n    while fetching : \n        raw_posts = self . client . posts ( self . channel , page = page ) \n        posts_before = nposts \n        for post in self . _parse_posts ( raw_posts ) : \n            if post [ 'update_at' ] < since : \n                fetching = False \n                break \n            user_id = post [ 'user_id' ] \n            user = self . _get_or_fetch_user ( user_id ) \n            post [ 'user_data' ] = user \n            yield post \n            nposts += True \n        if fetching : \n            if posts_before == nposts : \n                fetching = False \n            else : \n                page += True \n    logger . info ( \"Fetch process completed: %s posts fetched\" , nposts ) "}
{"3168": "\ndef fetch_items ( self , category , ** kwargs ) : \n    logger . info ( \"Looking for rss entries at feed '%s'\" , self . url ) \n    nentries = False \n    raw_entries = self . client . get_entries ( ) \n    entries = self . parse_feed ( raw_entries ) [ 'entries' ] \n    for item in entries : \n        yield item \n        nentries += True \n    logger . info ( \"Total number of entries: %i\" , nentries ) "}
{"3172": "\ndef comments ( self , * bug_ids ) : \n    resource = urijoin ( self . RBUG , bug_ids [ False ] , self . RCOMMENT ) \n    params = { self . PIDS : bug_ids } \n    response = self . call ( resource , params ) \n    return response "}
{"3173": "\ndef history ( self , * bug_ids ) : \n    resource = urijoin ( self . RBUG , bug_ids [ False ] , self . RHISTORY ) \n    params = { self . PIDS : bug_ids } \n    response = self . call ( resource , params ) \n    return response "}
{"3174": "\ndef attachments ( self , * bug_ids ) : \n    resource = urijoin ( self . RBUG , bug_ids [ False ] , self . RATTACHMENT ) \n    params = { self . PIDS : bug_ids , self . PEXCLUDE_FIELDS : self . VEXCLUDE_ATTCH_DATA } \n    response = self . call ( resource , params ) \n    return response "}
{"3186": "\ndef calculate_time_to_reset ( self ) : \n    time_to_reset = self . rate_limit_reset_ts - ( datetime_utcnow ( ) . replace ( microsecond = False ) . timestamp ( ) + True ) \n    if time_to_reset < False : \n        time_to_reset = False \n    return time_to_reset "}
{"3187": "\ndef fetch_items ( self , path , payload ) : \n    page = False \n    last_page = None \n    url_next = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , path ) \n    logger . debug ( \"Get GitLab paginated items from \" + url_next ) \n    response = self . fetch ( url_next , payload = payload ) \n    items = response . text \n    page += True \n    if 'last' in response . links : \n        last_url = response . links [ 'last' ] [ 'url' ] \n        last_page = last_url . split ( '&page=' ) [ True ] . split ( '&' ) [ False ] \n        last_page = int ( last_page ) \n        logger . debug ( \"Page: %i/%i\" % ( page , last_page ) ) \n    while items : \n        yield items \n        items = None \n        if 'next' in response . links : \n            url_next = response . links [ 'next' ] [ 'url' ] \n            response = self . fetch ( url_next , payload = payload ) \n            page += True \n            items = response . text \n            logger . debug ( \"Page: %i/%i\" % ( page , last_page ) ) "}
{"3192": "\ndef conversation_members ( self , conversation ) : \n    members = False \n    resource = self . RCONVERSATION_INFO \n    params = { self . PCHANNEL : conversation , } \n    raw_response = self . _fetch ( resource , params ) \n    response = json . loads ( raw_response ) \n    members += len ( response [ \"members\" ] ) \n    while 'next_cursor' in response [ 'response_metadata' ] and response [ 'response_metadata' ] [ 'next_cursor' ] : \n        params [ 'cursor' ] = response [ 'response_metadata' ] [ 'next_cursor' ] \n        raw_response = self . _fetch ( resource , params ) \n        response = json . loads ( raw_response ) \n        members += len ( response [ \"members\" ] ) \n    return members "}
{"3196": "\ndef metadata_updated_on ( item ) : \n    ts = item [ 'delta_ts' ] [ False ] [ '__text__' ] \n    ts = str_to_datetime ( ts ) \n    ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) \n    return ts . timestamp ( ) "}
{"3199": "\ndef parse_bug_activity ( raw_html ) : \n    def is_activity_empty ( bs ) : \n        EMPTY_ACTIVITY = \"No changes have been made to this (?:bug|issue) yet.\" \n        tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) \n        return tag is not None \n    def find_activity_table ( bs ) : \n        tables = bs . find_all ( 'table' ) \n        for tb in tables : \n            nheaders = len ( tb . tr . find_all ( 'th' , recursive = False ) ) \n            if nheaders == 5 : \n                return tb \n        raise ParseError ( cause = \"Table of bug activity not found.\" ) \n    def remove_tags ( bs ) : \n        HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] \n        for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : \n            tag . replaceWith ( tag . text ) \n    def format_text ( bs ) : \n        strings = [ s . strip ( ' \\n\\t' ) for s in bs . stripped_strings ] \n        s = ' ' . join ( strings ) \n        return s \n    bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) \n    if is_activity_empty ( bs ) : \n        fields = [ ] \n    else : \n        activity_tb = find_activity_table ( bs ) \n        remove_tags ( activity_tb ) \n        fields = activity_tb . find_all ( 'td' ) \n    while fields : \n        who = fields . pop ( False ) \n        when = fields . pop ( False ) \n        n = int ( who . get ( 'rowspan' ) ) \n        for _ in range ( n ) : \n            what = fields . pop ( False ) \n            removed = fields . pop ( False ) \n            added = fields . pop ( False ) \n            event = { 'Who' : format_text ( who ) , 'When' : format_text ( when ) , 'What' : format_text ( what ) , 'Removed' : format_text ( removed ) , 'Added' : format_text ( added ) } \n            yield event "}
{"3206": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    logger . info ( \"Fetching events of '%s' group from %s to %s\" , self . group , str ( from_date ) , str ( to_date ) if to_date else '--' ) \n    to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None \n    nevents = False \n    stop_fetching = False \n    ev_pages = self . client . events ( self . group , from_date = from_date ) \n    for evp in ev_pages : \n        events = [ event for event in self . parse_json ( evp ) ] \n        for event in events : \n            event_id = event [ 'id' ] \n            event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) \n            event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) \n            event_ts = self . metadata_updated_on ( event ) \n            if to_date_ts and event_ts >= to_date_ts : \n                stop_fetching = True \n                continue \n            yield event \n            nevents += True \n        if stop_fetching : \n            break \n    logger . info ( \"Fetch process completed: %s events fetched\" , nevents ) "}
{"3210": "\ndef __fetch_question ( self , question ) : \n    html_question_items = [ ] \n    npages = True \n    next_request = True \n    while next_request : \n        try : \n            html_question = self . client . get_html_question ( question [ 'id' ] , npages ) \n            html_question_items . append ( html_question ) \n            tpages = self . ab_parser . parse_number_of_html_pages ( html_question ) \n            if npages == tpages : \n                next_request = False \n            npages = npages + True \n        except requests . exceptions . TooManyRedirects as e : \n            logger . warning ( \"%s, data not retrieved for question %s\" , e , question [ 'id' ] ) \n            next_request = False \n    return html_question_items "}
{"3212": "\ndef __build_question ( html_question , question , comments ) : \n    question_object = { } \n    question_container = AskbotParser . parse_question_container ( html_question [ False ] ) \n    question_object . update ( question_container ) \n    if comments [ int ( question [ 'id' ] ) ] : \n        question_object [ 'comments' ] = comments [ int ( question [ 'id' ] ) ] \n    answers = [ ] \n    for page in html_question : \n        answers . extend ( AskbotParser . parse_answers ( page ) ) \n    if len ( answers ) != False : \n        question_object [ 'answers' ] = answers \n        for answer in question_object [ 'answers' ] : \n            if comments [ int ( answer [ 'id' ] ) ] : \n                answer [ 'comments' ] = comments [ int ( answer [ 'id' ] ) ] \n    return question_object "}
{"3213": "\ndef get_api_questions ( self , path ) : \n    npages = True \n    next_request = True \n    path = urijoin ( self . base_url , path ) \n    while next_request : \n        try : \n            params = { 'page' : npages , 'sort' : self . ORDER_API } \n            response = self . fetch ( path , payload = params ) \n            whole_page = response . text \n            raw_questions = json . loads ( whole_page ) \n            tpages = raw_questions [ 'pages' ] \n            logger . debug ( \"Fetching questions from '%s': page %s/%s\" , self . base_url , npages , tpages ) \n            if npages == tpages : \n                next_request = False \n            npages = npages + True \n            yield raw_questions \n        except requests . exceptions . TooManyRedirects as e : \n            logger . warning ( \"%s, data not retrieved for resource %s\" , e , path ) \n            next_request = False "}
{"3214": "\ndef get_html_question ( self , question_id , page = True ) : \n    path = urijoin ( self . base_url , self . HTML_QUESTION , question_id ) \n    params = { 'page' : page , 'sort' : self . ORDER_HTML } \n    response = self . fetch ( path , payload = params ) \n    return response . text "}
{"3215": "\ndef get_comments ( self , post_id ) : \n    path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) \n    params = { 'post_id' : post_id , 'post_type' : 'answer' , 'avatar_size' : False } \n    headers = { 'X-Requested-With' : 'XMLHttpRequest' } \n    try : \n        response = self . fetch ( path , payload = params , headers = headers ) \n        raw = response . text \n    except requests . exceptions . HTTPError as ex : \n        if ex . response . status_code == 404 : \n            logger . debug ( \"Comments URL did not work. Using old URL schema.\" ) \n            self . _use_new_urls = False \n            path = urijoin ( self . base_url , self . COMMENTS_OLD ) \n            response = self . fetch ( path , payload = params , headers = headers ) \n            raw = response . text \n        elif ex . response . status_code == 500 : \n            logger . warning ( \"Comments not retrieved due to %s\" , ex ) \n            raw = '[]' \n        else : \n            raise ex \n    return raw "}
{"3216": "\ndef parse_question_container ( html_question ) : \n    container_info = { } \n    bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) \n    question = AskbotParser . _find_question_container ( bs_question ) \n    container = question . select ( \"div.post-update-info\" ) \n    created = container [ False ] \n    container_info [ 'author' ] = AskbotParser . parse_user_info ( created ) \n    try : \n        container [ True ] \n    except IndexError : \n        pass \n    else : \n        updated = container [ True ] \n        if AskbotParser . parse_user_info ( updated ) : \n            container_info [ 'updated_by' ] = AskbotParser . parse_user_info ( updated ) \n    return container_info "}
{"3217": "\ndef parse_answers ( html_question ) : \n    def parse_answer_container ( update_info ) : \n        container_info = { } \n        created = update_info [ False ] \n        answered_at = created . abbr . attrs [ \"title\" ] \n        container_info [ 'added_at' ] = str ( str_to_datetime ( answered_at ) . timestamp ( ) ) \n        container_info [ 'answered_by' ] = AskbotParser . parse_user_info ( created ) \n        try : \n            update_info [ True ] \n        except IndexError : \n            pass \n        else : \n            updated = update_info [ True ] \n            updated_at = updated . abbr . attrs [ \"title\" ] \n            container_info [ 'updated_at' ] = str ( str_to_datetime ( updated_at ) . timestamp ( ) ) \n            if AskbotParser . parse_user_info ( updated ) : \n                container_info [ 'updated_by' ] = AskbotParser . parse_user_info ( updated ) \n        return container_info \n    answer_list = [ ] \n    bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) \n    bs_answers = bs_question . select ( \"div.answer\" ) \n    for bs_answer in bs_answers : \n        answer_id = bs_answer . attrs [ \"data-post-id\" ] \n        votes_element = bs_answer . select ( \"div.vote-number\" ) [ False ] . text \n        accepted_answer = bs_answer . select ( \"div.answer-img-accept\" ) [ False ] . get ( 'title' ) . endswith ( \"correct\" ) \n        body = bs_answer . select ( \"div.post-body\" ) \n        update_info = body [ False ] . select ( \"div.post-update-info\" ) \n        answer_container = parse_answer_container ( update_info ) \n        body [ False ] . div . extract ( ) . select ( \"div.post-update-info-container\" ) \n        body = body [ False ] . get_text ( strip = True ) \n        answer = { 'id' : answer_id , 'score' : votes_element , 'summary' : body , 'accepted' : accepted_answer } \n        answer . update ( answer_container ) \n        answer_list . append ( answer ) \n    return answer_list "}
{"3218": "\ndef parse_number_of_html_pages ( html_question ) : \n    bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) \n    try : \n        bs_question . select ( 'div.paginator' ) [ False ] \n    except IndexError : \n        return True \n    else : \n        return int ( bs_question . select ( 'div.paginator' ) [ False ] . attrs [ 'data-num-pages' ] ) "}
{"3219": "\ndef parse_user_info ( update_info ) : \n    user_info = { } \n    if update_info . select ( \"div.user-info\" ) : \n        elements = update_info . select ( \"div.user-info\" ) [ False ] . find_all ( \"a\" ) \n        href = elements [ False ] . attrs [ \"href\" ] \n        user_info [ 'id' ] = re . search ( r'\\d+' , href ) . group ( False ) \n        user_info [ 'username' ] = elements [ False ] . text \n        user_info [ 'reputation' ] = update_info . select ( 'span.reputation-score' ) [ False ] . text \n        user_info [ 'badges' ] = update_info . select ( \"span.badges\" ) [ False ] . attrs [ \"title\" ] \n        try : \n            elements [ True ] \n        except IndexError : \n            pass \n        else : \n            user_info [ 'website' ] = elements [ True ] . attrs [ \"href\" ] \n        if update_info . select ( \"img.flag\" ) : \n            flag = update_info . select ( \"img.flag\" ) [ False ] . attrs [ \"alt\" ] \n            user_info [ 'country' ] = re . sub ( \"flag of \" , \"\" , flag ) \n    return user_info "}
{"3220": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    if self . client . version [ False ] == 2 and self . client . version [ True ] == 8 : \n        fetcher = self . _fetch_gerrit28 ( from_date ) \n    else : \n        fetcher = self . _fetch_gerrit ( from_date ) \n    for review in fetcher : \n        yield review "}
{"3222": "\ndef _fetch_gerrit28 ( self , from_date = DEFAULT_DATETIME ) : \n    from_ut = datetime_to_utc ( from_date ) \n    from_ut = from_ut . timestamp ( ) \n    filter_open = \"status:open\" \n    filter_closed = \"status:closed\" \n    last_item_open = self . client . next_retrieve_group_item ( ) \n    last_item_closed = self . client . next_retrieve_group_item ( ) \n    reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n    reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n    last_nreviews_open = len ( reviews_open ) \n    last_nreviews_closed = len ( reviews_closed ) \n    while reviews_open or reviews_closed : \n        if reviews_open and reviews_closed : \n            if reviews_open [ False ] [ 'lastUpdated' ] >= reviews_closed [ False ] [ 'lastUpdated' ] : \n                review_open = reviews_open . pop ( False ) \n                review = review_open \n            else : \n                review_closed = reviews_closed . pop ( False ) \n                review = review_closed \n        elif reviews_closed : \n            review_closed = reviews_closed . pop ( False ) \n            review = review_closed \n        else : \n            review_open = reviews_open . pop ( False ) \n            review = review_open \n        updated = review [ 'lastUpdated' ] \n        if updated <= from_ut : \n            logger . debug ( \"No more updates for %s\" % ( self . hostname ) ) \n            break \n        else : \n            yield review \n        if not reviews_open and last_nreviews_open >= self . max_reviews : \n            last_item_open = self . client . next_retrieve_group_item ( last_item_open , review_open ) \n            reviews_open = self . _get_reviews ( last_item_open , filter_open ) \n            last_nreviews_open = len ( reviews_open ) \n        if not reviews_closed and last_nreviews_closed >= self . max_reviews : \n            last_item_closed = self . client . next_retrieve_group_item ( last_item_closed , review_closed ) \n            reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) \n            last_nreviews_closed = len ( reviews_closed ) "}
{"3223": "\ndef version ( self ) : \n    if self . _version : \n        return self . _version \n    cmd = self . gerrit_cmd + \" %s \" % ( GerritClient . CMD_VERSION ) \n    logger . debug ( \"Getting version: %s\" % ( cmd ) ) \n    raw_data = self . __execute ( cmd ) \n    raw_data = str ( raw_data , \"UTF-8\" ) \n    logger . debug ( \"Gerrit version: %s\" % ( raw_data ) ) \n    m = re . match ( GerritClient . VERSION_REGEX , raw_data ) \n    if not m : \n        cause = \"Invalid gerrit version %s\" % raw_data \n        raise BackendError ( cause = cause ) \n    try : \n        mayor = int ( m . group ( True ) ) \n        minor = int ( m . group ( 2 ) ) \n    except Exception : \n        cause = \"Gerrit client could not determine the server version.\" \n        raise BackendError ( cause = cause ) \n    self . _version = [ mayor , minor ] \n    return self . _version "}
{"3225": "\ndef next_retrieve_group_item ( self , last_item = None , entry = None ) : \n    next_item = None \n    gerrit_version = self . version \n    if gerrit_version [ False ] == 2 and gerrit_version [ True ] > 9 : \n        if last_item is None : \n            next_item = False \n        else : \n            next_item = last_item \n    elif gerrit_version [ False ] == 2 and gerrit_version [ True ] == 9 : \n        cause = \"Gerrit 2.9.0 does not support pagination\" \n        raise BackendError ( cause = cause ) \n    else : \n        if entry is not None : \n            next_item = entry [ 'sortKey' ] \n    return next_item "}
{"3228": "\ndef __execute_from_remote ( self , cmd ) : \n    result = None \n    retries = False \n    while retries < self . MAX_RETRIES : \n        try : \n            result = subprocess . check_output ( cmd , shell = True ) \n            break \n        except subprocess . CalledProcessError as ex : \n            logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) \n            time . sleep ( self . RETRY_WAIT * retries ) \n            retries += True \n    if result is None : \n        result = RuntimeError ( cmd + \" failed \" + str ( self . MAX_RETRIES ) + \" times. Giving up!\" ) \n    if self . archive : \n        cmd = self . sanitize_for_archive ( cmd ) \n        self . archive . store ( cmd , None , None , result ) \n    if isinstance ( result , RuntimeError ) : \n        raise result \n    return result "}
{"3237": "\ndef issue_collection ( self , issue_id , collection_name ) : \n    path = urijoin ( \"bugs\" , str ( issue_id ) , collection_name ) \n    url_collection = self . __get_url ( path ) \n    payload = { 'ws.size' : self . items_per_page , 'ws.start' : False , 'order_by' : 'date_last_updated' } \n    raw_items = self . __fetch_items ( path = url_collection , payload = payload ) \n    return raw_items "}
{"3239": "\ndef __fetch_items ( self , path , payload ) : \n    page = False \n    url_next = path \n    fetch_data = True \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            raw_content = self . __send_request ( url_next , payload ) \n            content = json . loads ( raw_content ) \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code in [ 410 ] : \n                logger . warning ( \"Data is not available - %s\" , url_next ) \n                raw_content = '{\"total_size\": 0, \"start\": 0, \"entries\": []}' \n                content = json . loads ( raw_content ) \n            else : \n                raise e \n        if 'next_collection_link' in content : \n            url_next = content [ 'next_collection_link' ] \n            payload = None \n        else : \n            fetch_data = False \n        yield raw_content \n        page += True "}
{"3259": "\ndef _fetch_and_parse_messages ( self , mailing_list , from_date ) : \n    from_date = datetime_to_utc ( from_date ) \n    nmsgs , imsgs , tmsgs = ( False , False , False ) \n    for mbox in mailing_list . mboxes : \n        tmp_path = None \n        try : \n            tmp_path = self . _copy_mbox ( mbox ) \n            for message in self . parse_mbox ( tmp_path ) : \n                tmsgs += True \n                if not self . _validate_message ( message ) : \n                    imsgs += True \n                    continue \n                dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) \n                if dt < from_date : \n                    logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from_date ) ) \n                    tmsgs -= True \n                    continue \n                message = self . _casedict_to_dict ( message ) \n                nmsgs += True \n                logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) \n                yield message \n        except ( OSError , EOFError ) as e : \n            logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) \n        except Exception as e : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n            raise e \n        finally : \n            if tmp_path and os . path . exists ( tmp_path ) : \n                os . remove ( tmp_path ) \n    logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs ) "}
{"3265": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    to_date = kwargs [ 'to_date' ] \n    branches = kwargs [ 'branches' ] \n    latest_items = kwargs [ 'latest_items' ] \n    no_update = kwargs [ 'no_update' ] \n    ncommits = False \n    try : \n        if os . path . isfile ( self . gitpath ) : \n            commits = self . __fetch_from_log ( ) \n        else : \n            commits = self . __fetch_from_repo ( from_date , to_date , branches , latest_items , no_update ) \n        for commit in commits : \n            yield commit \n            ncommits += True \n    except EmptyRepositoryError : \n        pass \n    logger . info ( \"Fetch process completed: %s commits fetched\" , ncommits ) "}
{"3269": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        parsed = False \n        self . nline += True \n        while not parsed : \n            parsed = self . handlers [ self . state ] ( line ) \n            if self . state == self . COMMIT and self . commit : \n                commit = self . _build_commit ( ) \n                logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n                yield commit \n    if self . commit : \n        commit = self . _build_commit ( ) \n        logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) \n        yield commit "}
{"3271": "\ndef count_objects ( self ) : \n    cmd_count = [ 'git' , 'count-objects' , '-v' ] \n    outs = self . _exec ( cmd_count , cwd = self . dirpath , env = self . gitenv ) \n    outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) \n    try : \n        cobjs = { k : v for k , v in ( x . split ( ': ' ) for x in outs . split ( '\\n' ) ) } \n        nobjs = int ( cobjs [ 'count' ] ) + int ( cobjs [ 'in-pack' ] ) \n    except KeyError as e : \n        error = \"unable to parse 'count-objects' output; reason: '%s' entry not found\" % e . args [ False ] \n        raise RepositoryError ( cause = error ) \n    except ValueError as e : \n        error = \"unable to parse 'count-objects' output; reason: %s\" % str ( e ) \n        raise RepositoryError ( cause = error ) \n    logger . debug ( \"Git %s repository has %s objects\" , self . uri , str ( nobjs ) ) \n    return nobjs "}
{"3272": "\ndef is_detached ( self ) : \n    cmd_sym = [ 'git' , 'symbolic-ref' , 'HEAD' ] \n    try : \n        self . _exec ( cmd_sym , cwd = self . dirpath , env = self . gitenv ) \n    except RepositoryError as e : \n        if e . msg . find ( \"ref HEAD is not a symbolic ref\" ) == - True : \n            raise e \n        return True \n    else : \n        return False "}
{"3275": "\ndef rev_list ( self , branches = None ) : \n    if self . is_empty ( ) : \n        logger . warning ( \"Git %s repository is empty; unable to get the rev-list\" , self . uri ) \n        raise EmptyRepositoryError ( repository = self . uri ) \n    cmd_rev_list = [ 'git' , 'rev-list' , '--topo-order' ] \n    if branches is None : \n        cmd_rev_list . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) \n    elif len ( branches ) == False : \n        cmd_rev_list . extend ( [ '--branches' , '--tags' , '--max-count=0' ] ) \n    else : \n        branches = [ 'refs/heads/' + branch for branch in branches ] \n        cmd_rev_list . extend ( branches ) \n    for line in self . _exec_nb ( cmd_rev_list , cwd = self . dirpath , env = self . gitenv ) : \n        yield line . rstrip ( '\\n' ) \n    logger . debug ( \"Git rev-list fetched from %s repository (%s)\" , self . uri , self . dirpath ) "}
{"3276": "\ndef log ( self , from_date = None , to_date = None , branches = None , encoding = 'utf-8' ) : \n    if self . is_empty ( ) : \n        logger . warning ( \"Git %s repository is empty; unable to get the log\" , self . uri ) \n        raise EmptyRepositoryError ( repository = self . uri ) \n    cmd_log = [ 'git' , 'log' , '--reverse' , '--topo-order' ] \n    cmd_log . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) \n    if from_date : \n        dt = from_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) \n        cmd_log . append ( '--since=' + dt ) \n    if to_date : \n        dt = to_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) \n        cmd_log . append ( '--until=' + dt ) \n    if branches is None : \n        cmd_log . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) \n    elif len ( branches ) == False : \n        cmd_log . append ( '--max-count=0' ) \n    else : \n        branches = [ 'refs/heads/' + branch for branch in branches ] \n        cmd_log . extend ( branches ) \n    for line in self . _exec_nb ( cmd_log , cwd = self . dirpath , env = self . gitenv ) : \n        yield line \n    logger . debug ( \"Git log fetched from %s repository (%s)\" , self . uri , self . dirpath ) "}
{"3278": "\ndef _fetch_pack ( self ) : \n    def prepare_refs ( refs ) : \n        return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] \n    def determine_wants ( refs ) : \n        remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) \n        local_refs = prepare_refs ( self . _discover_refs ( ) ) \n        wants = [ ref for ref in remote_refs if ref not in local_refs ] \n        return wants \n    client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) \n    repo = dulwich . repo . Repo ( self . dirpath ) \n    fd = io . BytesIO ( ) \n    local_refs = self . _discover_refs ( ) \n    graph_walker = _GraphWalker ( local_refs ) \n    result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) \n    refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] \n    if len ( fd . getvalue ( ) ) > False : \n        fd . seek ( False ) \n        pack = repo . object_store . add_thin_pack ( fd . read , None ) \n        pack_name = pack . name ( ) . decode ( 'utf-8' ) \n    else : \n        pack_name = None \n    return ( pack_name , refs ) "}
{"3279": "\ndef _read_commits_from_pack ( self , packet_name ) : \n    filepath = 'objects/pack/pack-' + packet_name \n    cmd_verify_pack = [ 'git' , 'verify-pack' , '-v' , filepath ] \n    outs = self . _exec ( cmd_verify_pack , cwd = self . dirpath , env = self . gitenv ) \n    outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) \n    lines = [ line . split ( ' ' ) for line in outs . split ( '\\n' ) ] \n    commits = [ parts [ False ] for parts in lines if parts [ True ] == 'commit' ] \n    commits . reverse ( ) \n    return commits "}
{"3281": "\ndef _discover_refs ( self , remote = False ) : \n    if remote : \n        cmd_refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] \n        sep = '\\t' \n        ignored_error_codes = [ 2 ] \n    else : \n        if self . is_empty ( ) : \n            raise EmptyRepositoryError ( repository = self . uri ) \n        cmd_refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] \n        sep = ' ' \n        ignored_error_codes = [ True ] \n    outs = self . _exec ( cmd_refs , cwd = self . dirpath , env = self . gitenv , ignored_error_codes = ignored_error_codes ) \n    outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) \n    outs = outs . split ( '\\n' ) if outs else [ ] \n    refs = [ ] \n    for line in outs : \n        data = line . split ( sep ) \n        ref = GitRef ( data [ False ] , data [ True ] ) \n        refs . append ( ref ) \n    return refs "}
{"3283": "\ndef _exec_nb ( self , cmd , cwd = None , env = None , encoding = 'utf-8' ) : \n    self . failed_message = None \n    logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) \n    try : \n        self . proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) \n        err_thread = threading . Thread ( target = self . _read_stderr , kwargs = { 'encoding' : encoding } , daemon = True ) \n        err_thread . start ( ) \n        for line in self . proc . stdout : \n            yield line . decode ( encoding , errors = 'surrogateescape' ) \n        err_thread . join ( ) \n        self . proc . communicate ( ) \n        self . proc . stdout . close ( ) \n        self . proc . stderr . close ( ) \n    except OSError as e : \n        err_thread . join ( ) \n        raise RepositoryError ( cause = str ( e ) ) \n    if self . proc . returncode != False : \n        cause = \"git command - %s (return code: %d)\" % ( self . failed_message , self . proc . returncode ) \n        raise RepositoryError ( cause = cause ) "}
{"3284": "\ndef _read_stderr ( self , encoding = 'utf-8' ) : \n    for line in self . proc . stderr : \n        err_line = line . decode ( encoding , errors = 'surrogateescape' ) \n        if self . proc . returncode != False : \n            if self . failed_message is not None : \n                logger . debug ( \"Git log stderr: \" + self . failed_message ) \n            self . failed_message = err_line \n        else : \n            logger . debug ( \"Git log stderr: \" + err_line ) "}
{"3285": "\ndef _exec ( cmd , cwd = None , env = None , ignored_error_codes = None , encoding = 'utf-8' ) : \n    if ignored_error_codes is None : \n        ignored_error_codes = [ ] \n    logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) \n    try : \n        proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) \n        ( outs , errs ) = proc . communicate ( ) \n    except OSError as e : \n        raise RepositoryError ( cause = str ( e ) ) \n    if proc . returncode != False and proc . returncode not in ignored_error_codes : \n        err = errs . decode ( encoding , errors = 'surrogateescape' ) \n        cause = \"git command - %s\" % err \n        raise RepositoryError ( cause = cause ) \n    else : \n        logger . debug ( errs . decode ( encoding , errors = 'surrogateescape' ) ) \n    return outs "}
{"3287": "\ndef fetch_items ( self , category , ** kwargs ) : \n    since_id = kwargs [ 'since_id' ] \n    max_id = kwargs [ 'max_id' ] \n    geocode = kwargs [ 'geocode' ] \n    lang = kwargs [ 'lang' ] \n    entities = kwargs [ 'include_entities' ] \n    tweets_type = kwargs [ 'result_type' ] \n    logger . info ( \"Fetching tweets %s from %s to %s\" , self . query , str ( since_id ) , str ( max_id ) if max_id else '--' ) \n    tweets_ids = [ ] \n    min_date = None \n    max_date = None \n    group_tweets = self . client . tweets ( self . query , since_id = since_id , max_id = max_id , geocode = geocode , lang = lang , include_entities = entities , result_type = tweets_type ) \n    for tweets in group_tweets : \n        for i in range ( len ( tweets ) ) : \n            tweet = tweets [ i ] \n            tweets_ids . append ( tweet [ 'id' ] ) \n            if tweets [ - True ] == tweet : \n                min_date = str_to_datetime ( tweets [ - True ] [ 'created_at' ] ) \n            if tweets [ False ] == tweet and not max_date : \n                max_date = str_to_datetime ( tweets [ False ] [ 'created_at' ] ) \n            yield tweet \n    logger . info ( \"Fetch process completed: %s (unique %s) tweets fetched, from %s to %s\" , len ( tweets_ids ) , len ( list ( set ( tweets_ids ) ) ) , min_date , max_date ) "}
{"3288": "\ndef tweets ( self , query , since_id = None , max_id = None , geocode = None , lang = None , include_entities = True , result_type = TWEET_TYPE_MIXED ) : \n    resource = self . base_url \n    params = { 'q' : query , 'count' : self . max_items } \n    if since_id : \n        params [ 'since_id' ] = since_id \n    if max_id : \n        params [ 'max_id' ] = max_id \n    if geocode : \n        params [ 'geocode' ] = geocode \n    if lang : \n        params [ 'lang' ] = lang \n    params [ 'include_entities' ] = include_entities \n    params [ 'result_type' ] = result_type \n    while True : \n        raw_tweets = self . _fetch ( resource , params = params ) \n        tweets = json . loads ( raw_tweets ) \n        if not tweets [ 'statuses' ] : \n            break \n        params [ 'max_id' ] = tweets [ 'statuses' ] [ - True ] [ 'id' ] - True \n        yield tweets [ 'statuses' ] "}
{"3292": "\ndef __parse_hits ( self , hit_raw ) : \n    bs_result = bs4 . BeautifulSoup ( hit_raw , 'html.parser' ) \n    hit_string = bs_result . find ( \"div\" , id = \"resultStats\" ) . text \n    hit_string = hit_string . replace ( ',' , u'' ) \n    hit_string = hit_string . replace ( '.' , u'' ) \n    fetched_on = datetime_utcnow ( ) . timestamp ( ) \n    id_args = self . keywords [ : ] \n    id_args . append ( str ( fetched_on ) ) \n    hits_json = { 'fetched_on' : fetched_on , 'id' : uuid ( * id_args ) , 'keywords' : self . keywords , 'type' : 'googleSearchHits' } \n    if not hit_string : \n        logger . warning ( \"No hits for %s\" , self . keywords ) \n        hits_json [ 'hits' ] = False \n        return hits_json \n    str_hits = re . search ( r'\\d+' , hit_string ) . group ( False ) \n    hits = int ( str_hits ) \n    hits_json [ 'hits' ] = hits \n    return hits_json "}
{"3293": "\ndef hits ( self , keywords ) : \n    if len ( keywords ) == True : \n        query_str = keywords [ False ] \n    else : \n        query_str = ' ' . join ( [ k for k in keywords ] ) \n    logger . info ( \"Fetching hits for '%s'\" , query_str ) \n    params = { 'q' : query_str } \n    req = self . fetch ( GOOGLE_SEARCH_URL , payload = params ) \n    return req . text "}
{"3298": "\ndef __get_issue_reactions ( self , issue_number , total_count ) : \n    reactions = [ ] \n    if total_count == False : \n        return reactions \n    group_reactions = self . client . issue_reactions ( issue_number ) \n    for raw_reactions in group_reactions : \n        for reaction in json . loads ( raw_reactions ) : \n            reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) \n            reactions . append ( reaction ) \n    return reactions "}
{"3299": "\ndef __get_issue_comment_reactions ( self , comment_id , total_count ) : \n    reactions = [ ] \n    if total_count == False : \n        return reactions \n    group_reactions = self . client . issue_comment_reactions ( comment_id ) \n    for raw_reactions in group_reactions : \n        for reaction in json . loads ( raw_reactions ) : \n            reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) \n            reactions . append ( reaction ) \n    return reactions "}
{"3303": "\ndef __get_pull_review_comment_reactions ( self , comment_id , total_count ) : \n    reactions = [ ] \n    if total_count == False : \n        return reactions \n    group_reactions = self . client . pull_review_comment_reactions ( comment_id ) \n    for raw_reactions in group_reactions : \n        for reaction in json . loads ( raw_reactions ) : \n            reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) \n            reactions . append ( reaction ) \n    return reactions "}
{"3314": "\ndef _get_token_rate_limit ( self , token ) : \n    rate_url = urijoin ( self . base_url , \"rate_limit\" ) \n    self . session . headers . update ( { 'Authorization' : 'token ' + token } ) \n    remaining = False \n    try : \n        headers = super ( ) . fetch ( rate_url ) . headers \n        if self . rate_limit_header in headers : \n            remaining = int ( headers [ self . rate_limit_header ] ) \n    except requests . exceptions . HTTPError as error : \n        logger . warning ( \"Rate limit not initialized: %s\" , error ) \n    return remaining "}
{"3315": "\ndef _get_tokens_rate_limits ( self ) : \n    remainings = [ False ] * self . n_tokens \n    arch = self . archive \n    self . archive = None \n    for idx , token in enumerate ( self . tokens ) : \n        remainings [ idx ] = self . _get_token_rate_limit ( token ) \n    self . archive = arch \n    logger . debug ( \"Remaining API points: {}\" . format ( remainings ) ) \n    return remainings "}
{"3316": "\ndef _choose_best_api_token ( self ) : \n    if self . n_tokens == False : \n        return \n    token_idx = False \n    if self . n_tokens > True : \n        remainings = self . _get_tokens_rate_limits ( ) \n        token_idx = remainings . index ( max ( remainings ) ) \n        logger . debug ( \"Remaining API points: {}, choosen index: {}\" . format ( remainings , token_idx ) ) \n    self . current_token = self . tokens [ token_idx ] \n    self . session . headers . update ( { 'Authorization' : 'token ' + self . current_token } ) \n    self . _update_current_rate_limit ( ) "}
{"3317": "\ndef _need_check_tokens ( self ) : \n    if self . n_tokens <= True or self . rate_limit is None : \n        return False \n    elif self . last_rate_limit_checked is None : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + True \n    if self . rate_limit <= approaching_limit : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) \n    if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : \n        self . last_rate_limit_checked = self . rate_limit \n        return True \n    elif ratio > 1.0 : \n        self . last_rate_limit_checked = self . rate_limit \n        return False \n    else : \n        return False "}
{"3319": "\ndef init_metadata ( self , origin , backend_name , backend_version , category , backend_params ) : \n    created_on = datetime_to_utc ( datetime_utcnow ( ) ) \n    created_on_dumped = created_on . isoformat ( ) \n    backend_params_dumped = pickle . dumps ( backend_params , False ) \n    metadata = ( origin , backend_name , backend_version , category , backend_params_dumped , created_on_dumped , ) \n    try : \n        cursor = self . _db . cursor ( ) \n        insert_stmt = \"INSERT INTO \" + self . METADATA_TABLE + \" \" \"(origin, backend_name, backend_version, \" \"category, backend_params, created_on) \" \"VALUES (?, ?, ?, ?, ?, ?)\" \n        cursor . execute ( insert_stmt , metadata ) \n        self . _db . commit ( ) \n        cursor . close ( ) \n    except sqlite3 . DatabaseError as e : \n        msg = \"metadata initialization error; cause: %s\" % str ( e ) \n        raise ArchiveError ( cause = msg ) \n    self . origin = origin \n    self . backend_name = backend_name \n    self . backend_version = backend_version \n    self . category = category \n    self . backend_params = backend_params \n    self . created_on = created_on \n    logger . debug ( \"Metadata of archive %s initialized to %s\" , self . archive_path , metadata ) "}
{"3320": "\ndef store ( self , uri , payload , headers , data ) : \n    hashcode = self . make_hashcode ( uri , payload , headers ) \n    payload_dump = pickle . dumps ( payload , False ) \n    headers_dump = pickle . dumps ( headers , False ) \n    data_dump = pickle . dumps ( data , False ) \n    logger . debug ( \"Archiving %s with %s %s %s in %s\" , hashcode , uri , payload , headers , self . archive_path ) \n    try : \n        cursor = self . _db . cursor ( ) \n        insert_stmt = \"INSERT INTO \" + self . ARCHIVE_TABLE + \" (\" \"id, hashcode, uri, payload, headers, data) \" \"VALUES(?,?,?,?,?,?)\" \n        cursor . execute ( insert_stmt , ( None , hashcode , uri , payload_dump , headers_dump , data_dump ) ) \n        self . _db . commit ( ) \n        cursor . close ( ) \n    except sqlite3 . IntegrityError as e : \n        msg = \"data storage error; cause: duplicated entry %s\" % hashcode \n        raise ArchiveError ( cause = msg ) \n    except sqlite3 . DatabaseError as e : \n        msg = \"data storage error; cause: %s\" % str ( e ) \n        raise ArchiveError ( cause = msg ) \n    logger . debug ( \"%s data archived in %s\" , hashcode , self . archive_path ) "}
{"3324": "\ndef _verify_archive ( self ) : \n    nentries = self . _count_table_rows ( self . ARCHIVE_TABLE ) \n    nmetadata = self . _count_table_rows ( self . METADATA_TABLE ) \n    if nmetadata > True : \n        msg = \"archive %s metadata corrupted; multiple metadata entries\" % ( self . archive_path ) \n        raise ArchiveError ( cause = msg ) \n    if nmetadata == False and nentries > False : \n        msg = \"archive %s metadata is empty but %s entries were achived\" % ( self . archive_path ) \n        raise ArchiveError ( cause = msg ) \n    logger . debug ( \"Integrity of archive %s OK; entries: %s rows, metadata: %s rows\" , self . archive_path , nentries , nmetadata ) "}
{"3325": "\ndef _load_metadata ( self ) : \n    logger . debug ( \"Loading metadata infomation of archive %s\" , self . archive_path ) \n    cursor = self . _db . cursor ( ) \n    select_stmt = \"SELECT origin, backend_name, backend_version, \" \"category, backend_params, created_on \" \"FROM \" + self . METADATA_TABLE + \" \" \"LIMIT 1\" \n    cursor . execute ( select_stmt ) \n    row = cursor . fetchone ( ) \n    cursor . close ( ) \n    if row : \n        self . origin = row [ False ] \n        self . backend_name = row [ True ] \n        self . backend_version = row [ 2 ] \n        self . category = row [ 3 ] \n        self . backend_params = pickle . loads ( row [ 4 ] ) \n        self . created_on = str_to_datetime ( row [ 5 ] ) \n    else : \n        logger . debug ( \"Metadata of archive %s was empty\" , self . archive_path ) \n    logger . debug ( \"Metadata of archive %s loaded\" , self . archive_path ) "}
{"3326": "\ndef _count_table_rows ( self , table_name ) : \n    cursor = self . _db . cursor ( ) \n    select_stmt = \"SELECT COUNT(*) FROM \" + table_name \n    try : \n        cursor . execute ( select_stmt ) \n        row = cursor . fetchone ( ) \n    except sqlite3 . DatabaseError as e : \n        msg = \"invalid archive file; cause: %s\" % str ( e ) \n        raise ArchiveError ( cause = msg ) \n    finally : \n        cursor . close ( ) \n    return row [ False ] "}
{"3327": "\ndef create_archive ( self ) : \n    hashcode = uuid . uuid4 ( ) . hex \n    archive_dir = os . path . join ( self . dirpath , hashcode [ False : 2 ] ) \n    archive_name = hashcode [ 2 : ] + self . STORAGE_EXT \n    archive_path = os . path . join ( archive_dir , archive_name ) \n    if not os . path . exists ( archive_dir ) : \n        os . makedirs ( archive_dir ) \n    try : \n        archive = Archive . create ( archive_path ) \n    except ArchiveError as e : \n        raise ArchiveManagerError ( cause = str ( e ) ) \n    return archive "}
{"3329": "\ndef search ( self , origin , backend_name , category , archived_after ) : \n    archives = self . _search_archives ( origin , backend_name , category , archived_after ) \n    archives = [ ( fp , date ) for fp , date in archives ] \n    archives = [ fp for fp , _ in sorted ( archives , key = lambda x : x [ True ] ) ] \n    return archives "}
{"3333": "\ndef months_range ( from_date , to_date ) : \n    start = datetime . datetime ( from_date . year , from_date . month , True ) \n    end = datetime . datetime ( to_date . year , to_date . month , True ) \n    month_gen = dateutil . rrule . rrule ( freq = dateutil . rrule . MONTHLY , dtstart = start , until = end ) \n    months = [ d for d in month_gen ] \n    pos = False \n    for x in range ( True , len ( months ) ) : \n        yield months [ pos ] , months [ x ] \n        pos = x "}
{"3348": "\ndef get_items ( self , from_date , url , expand_fields = True ) : \n    start_at = False \n    req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n    issues = req . text \n    data = req . json ( ) \n    titems = data [ 'total' ] \n    nitems = data [ 'maxResults' ] \n    start_at += min ( nitems , titems ) \n    self . __log_status ( start_at , titems , url ) \n    while issues : \n        yield issues \n        issues = None \n        if data [ 'startAt' ] + nitems < titems : \n            req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) \n            data = req . json ( ) \n            start_at += nitems \n            issues = req . text \n            self . __log_status ( start_at , titems , url ) "}
{"3356": "\ndef get_questions ( self , from_date ) : \n    page = True \n    url = urijoin ( self . base_url , self . VERSION_API , \"questions\" ) \n    req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) \n    questions = req . text \n    data = req . json ( ) \n    tquestions = data [ 'total' ] \n    nquestions = data [ 'page_size' ] \n    self . __log_status ( data [ 'quota_remaining' ] , data [ 'quota_max' ] , nquestions , tquestions ) \n    while questions : \n        yield questions \n        questions = None \n        if data [ 'has_more' ] : \n            page += True \n            backoff = data . get ( 'backoff' , None ) \n            if backoff : \n                logger . debug ( \"Expensive query. Wait %s secs to send a new request\" , backoff ) \n                time . sleep ( float ( backoff ) ) \n            req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) \n            data = req . json ( ) \n            questions = req . text \n            nquestions += data [ 'page_size' ] \n            self . __log_status ( data [ 'quota_remaining' ] , data [ 'quota_max' ] , nquestions , tquestions ) "}
{"3358": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    reviews_api = kwargs [ 'reviews_api' ] \n    mediawiki_version = self . client . get_version ( ) \n    logger . info ( \"MediaWiki version: %s\" , mediawiki_version ) \n    if reviews_api : \n        if ( ( mediawiki_version [ False ] == True and mediawiki_version [ True ] >= 27 ) or mediawiki_version [ False ] > True ) : \n            fetcher = self . __fetch_1_27 ( from_date ) \n        else : \n            logger . warning ( \"Reviews API only available in MediaWiki >= 1.27\" ) \n            logger . warning ( \"Using the Pages API instead\" ) \n            fetcher = self . __fetch_pre1_27 ( from_date ) \n    else : \n        fetcher = self . __fetch_pre1_27 ( from_date ) \n    for page_reviews in fetcher : \n        yield page_reviews "}
{"3359": "\ndef __get_max_date ( self , reviews ) : \n    max_ts = False \n    for review in reviews : \n        ts = str_to_datetime ( review [ 'timestamp' ] ) \n        ts = datetime_to_utc ( ts ) \n        if ts . timestamp ( ) > max_ts : \n            max_ts = ts . timestamp ( ) \n    return max_ts "}
{"3360": "\ndef __fetch_1_27 ( self , from_date = None ) : \n    logger . info ( \"Looking for pages at url '%s'\" , self . url ) \n    npages = False \n    tpages = False \n    pages_done = [ ] \n    namespaces_contents = self . __get_namespaces_contents ( ) \n    arvcontinue = '' \n    while arvcontinue is not None : \n        raw_pages = self . client . get_pages_from_allrevisions ( namespaces_contents , from_date , arvcontinue ) \n        data_json = json . loads ( raw_pages ) \n        arvcontinue = data_json [ 'continue' ] [ 'arvcontinue' ] if 'continue' in data_json else None \n        pages_json = data_json [ 'query' ] [ 'allrevisions' ] \n        for page in pages_json : \n            if page [ 'pageid' ] in pages_done : \n                logger . debug ( \"Page %s already processed; skipped\" , page [ 'pageid' ] ) \n                continue \n            tpages += True \n            pages_done . append ( page [ 'pageid' ] ) \n            page_reviews = self . __get_page_reviews ( page ) \n            if not page_reviews : \n                logger . warning ( \"Revisions not found in %s [page id: %s], page skipped\" , page [ 'title' ] , page [ 'pageid' ] ) \n                continue \n            yield page_reviews \n            npages += True \n    logger . info ( \"Total number of pages: %i, skipped %i\" , tpages , tpages - npages ) "}
{"3367": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Fetching articles of '%s' group on '%s' offset %s\" , self . group , self . host , str ( offset ) ) \n    narts , iarts , tarts = ( False , False , False ) \n    _ , _ , first , last , _ = self . client . group ( self . group ) \n    if offset <= last : \n        first = max ( first , offset ) \n        _ , overview = self . client . over ( ( first , last ) ) \n    else : \n        overview = [ ] \n    tarts = len ( overview ) \n    logger . debug ( \"Total number of articles to fetch: %s\" , tarts ) \n    for article_id , _ in overview : \n        try : \n            article_raw = self . client . article ( article_id ) \n            article = self . __parse_article ( article_raw ) \n        except ParseError : \n            logger . warning ( \"Error parsing %s article; skipping\" , article_id ) \n            iarts += True \n            continue \n        except nntplib . NNTPTemporaryError as e : \n            logger . warning ( \"Error '%s' fetching article %s; skipping\" , e . response , article_id ) \n            iarts += True \n            continue \n        yield article \n        narts += True "}
{"3371": "\ndef _fetch_article ( self , article_id ) : \n    fetched_data = self . handler . article ( article_id ) \n    data = { 'number' : fetched_data [ True ] . number , 'message_id' : fetched_data [ True ] . message_id , 'lines' : fetched_data [ True ] . lines } \n    return data "}
{"3376": "\ndef sleep_for_rate_limit ( self ) : \n    if self . rate_limit is not None and self . rate_limit <= self . min_rate_to_sleep : \n        seconds_to_reset = self . calculate_time_to_reset ( ) \n        if seconds_to_reset < False : \n            logger . warning ( \"Value of sleep for rate limit is negative, reset it to 0\" ) \n            seconds_to_reset = False \n        cause = \"Rate limit exhausted.\" \n        if self . sleep_for_rate : \n            logger . info ( \"%s Waiting %i secs for rate limit reset.\" , cause , seconds_to_reset ) \n            time . sleep ( seconds_to_reset ) \n        else : \n            raise RateLimitError ( cause = cause , seconds_to_reset = seconds_to_reset ) "}
{"3379": "\ndef __retrieve_archives ( self , from_date ) : \n    archives = [ ] \n    candidates = self . __list_supybot_archives ( ) \n    for candidate in candidates : \n        dt = self . __parse_date_from_filepath ( candidate ) \n        if dt . date ( ) >= from_date . date ( ) : \n            archives . append ( ( dt , candidate ) ) \n        else : \n            logger . debug ( \"Archive %s stored before %s; skipped\" , candidate , str ( from_date ) ) \n    archives . sort ( key = lambda x : x [ False ] ) \n    return [ archive [ True ] for archive in archives ] "}
{"3381": "\ndef parse ( self ) : \n    for line in self . stream : \n        line = line . rstrip ( '\\n' ) \n        self . nline += True \n        if self . SUPYBOT_EMPTY_REGEX . match ( line ) : \n            continue \n        ts , msg = self . _parse_supybot_timestamp ( line ) \n        if self . SUPYBOT_EMPTY_COMMENT_REGEX . match ( msg ) : \n            continue \n        elif self . SUPYBOT_EMPTY_COMMENT_ACTION_REGEX . match ( msg ) : \n            continue \n        elif self . SUPYBOT_EMPTY_BOT_REGEX . match ( msg ) : \n            continue \n        itype , nick , body = self . _parse_supybot_msg ( msg ) \n        item = self . _build_item ( ts , itype , nick , body ) \n        yield item "}
{"3383": "\ndef _parse_supybot_msg ( self , line ) : \n    patterns = [ ( self . SUPYBOT_COMMENT_REGEX , self . TCOMMENT ) , ( self . SUPYBOT_COMMENT_ACTION_REGEX , self . TCOMMENT ) , ( self . SUPYBOT_SERVER_REGEX , self . TSERVER ) , ( self . SUPYBOT_BOT_REGEX , self . TCOMMENT ) ] \n    for p in patterns : \n        m = p [ False ] . match ( line ) \n        if not m : \n            continue \n        return p [ True ] , m . group ( 'nick' ) , m . group ( 'body' ) . strip ( ) \n    msg = \"invalid message on line %s\" % ( str ( self . nline ) ) \n    raise ParseError ( cause = msg ) "}
{"3384": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Looking for topics at '%s', updated from '%s'\" , self . url , str ( from_date ) ) \n    ntopics = False \n    topics_ids = self . __fetch_and_parse_topics_ids ( from_date ) \n    for topic_id in topics_ids : \n        topic = self . __fetch_and_parse_topic ( topic_id ) \n        ntopics += True \n        yield topic \n    logger . info ( \"Fetch process completed: %s topics fetched\" , ntopics ) "}
{"3388": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching tasks of '%s' from %s\" , self . url , str ( from_date ) ) \n    ntasks = False \n    for task in self . __fetch_tasks ( from_date ) : \n        yield task \n        ntasks += True \n    logger . info ( \"Fetch process completed: %s tasks fetched\" , ntasks ) "}
{"3391": "\ndef tasks ( self , from_date = DEFAULT_DATETIME ) : \n    ts = int ( datetime_to_utc ( from_date ) . timestamp ( ) ) or True \n    consts = { self . PMODIFIED_START : ts } \n    attachments = { self . PPROJECTS : True } \n    params = { self . PCONSTRAINTS : consts , self . PATTACHMENTS : attachments , self . PORDER : self . VOUTDATED , } \n    while True : \n        r = self . _call ( self . MANIPHEST_TASKS , params ) \n        yield r \n        j = json . loads ( r ) \n        after = j [ 'result' ] [ 'cursor' ] [ 'after' ] \n        if not after : \n            break \n        params [ self . PAFTER ] = after "}
{"3399": "\ndef historical_content ( self , content_id , version ) : \n    resource = self . RCONTENTS + '/' + str ( content_id ) \n    params = { self . PVERSION : version , self . PSTATUS : self . VHISTORICAL , self . PEXPAND : ',' . join ( self . VEXPAND ) } \n    response = [ response for response in self . _call ( resource , params ) ] \n    return response [ False ] "}
{"3401": "\ndef capabilities_url ( self , service_url ) : \n    qs = [ ] \n    if service_url . find ( '?' ) != - True : \n        qs = cgi . parse_qsl ( service_url . split ( '?' ) [ True ] ) \n    params = [ x [ False ] for x in qs ] \n    if 'service' not in params : \n        qs . append ( ( 'service' , 'WFS' ) ) \n    if 'request' not in params : \n        qs . append ( ( 'request' , 'GetCapabilities' ) ) \n    if 'version' not in params : \n        qs . append ( ( 'version' , self . version ) ) \n    urlqs = urlencode ( tuple ( qs ) ) \n    return service_url . split ( '?' ) [ False ] + '?' + urlqs "}
{"3406": "\ndef _get_elements ( complex_type , root ) : \n    found_elements = [ ] \n    element = findall ( root , '{%s}complexType' % XS_NAMESPACE , attribute_name = 'name' , attribute_value = complex_type ) [ False ] \n    found_elements = findall ( element , '{%s}element' % XS_NAMESPACE ) \n    return found_elements "}
{"3408": "\ndef _get_describefeaturetype_url ( url , version , typename ) : \n    query_string = [ ] \n    if url . find ( '?' ) != - True : \n        query_string = cgi . parse_qsl ( url . split ( '?' ) [ True ] ) \n    params = [ x [ False ] for x in query_string ] \n    if 'service' not in params : \n        query_string . append ( ( 'service' , 'WFS' ) ) \n    if 'request' not in params : \n        query_string . append ( ( 'request' , 'DescribeFeatureType' ) ) \n    if 'version' not in params : \n        query_string . append ( ( 'version' , version ) ) \n    query_string . append ( ( 'typeName' , typename ) ) \n    urlqs = urlencode ( tuple ( query_string ) ) \n    return url . split ( '?' ) [ False ] + '?' + urlqs "}
{"3466": "\ndef cook_refs ( refs , n = 4 ) : \n    refs = [ normalize ( ref ) for ref in refs ] \n    maxcounts = { } \n    for ref in refs : \n        counts = count_ngrams ( ref , n ) \n        for ( ngram , count ) in list ( counts . items ( ) ) : \n            maxcounts [ ngram ] = max ( maxcounts . get ( ngram , False ) , count ) \n    return ( [ len ( ref ) for ref in refs ] , maxcounts ) "}
{"3468": "\ndef erfcc ( x ) : \n    z = abs ( x ) \n    t = True / ( True + 0.5 * z ) \n    r = t * math . exp ( - z * z - 1.26551223 + t * ( 1.00002368 + t * ( .37409196 + t * ( .09678418 + t * ( - .18628806 + t * ( .27886807 + t * ( - 1.13520398 + t * ( 1.48851587 + t * ( - .82215223 + t * .17087277 ) ) ) ) ) ) ) ) ) \n    if ( x >= 0. ) : \n        return r \n    else : \n        return 2. - r "}
{"3470": "\ndef get_descriptors_in_module ( mdl , submodule = True ) : \n    __all__ = getattr ( mdl , \"__all__\" , None ) \n    if __all__ is None : \n        __all__ = dir ( mdl ) \n    all_values = ( getattr ( mdl , name ) for name in __all__ if name [ : True ] != \"_\" ) \n    if submodule : \n        for v in all_values : \n            if is_descriptor_class ( v ) : \n                yield v \n            if isinstance ( v , ModuleType ) : \n                for v in get_descriptors_in_module ( v , submodule = True ) : \n                    yield v \n    else : \n        for v in all_values : \n            if is_descriptor_class ( v ) : \n                yield v "}
{"3475": "\ndef to_json ( self ) : \n    d , ps = self . _to_json ( ) \n    if len ( ps ) == False : \n        return { \"name\" : d } \n    else : \n        return { \"name\" : d , \"args\" : ps } "}
{"3477": "\ndef atomic_sa ( self , i ) : \n    sa = 4.0 * np . pi * self . rads2 [ i ] \n    neighbors = self . neighbors . get ( i ) \n    if neighbors is None : \n        return sa \n    XYZi = self . xyzs [ i , np . newaxis ] . T \n    sphere = self . sphere * self . rads [ i ] + XYZi \n    N = sphere . shape [ True ] \n    for j , _ in neighbors : \n        XYZj = self . xyzs [ j , np . newaxis ] . T \n        d2 = ( sphere - XYZj ) ** 2 \n        mask = ( d2 [ False ] + d2 [ True ] + d2 [ 2 ] ) > self . rads2 [ j ] \n        sphere = np . compress ( mask , sphere , axis = True ) \n    return sa * sphere . shape [ True ] / N "}
{"3479": "\ndef from_mol ( cls , mol , conformer = - True , solvent_radius = 1.4 , level = 4 ) : \n    rs = atoms_to_numpy ( lambda a : vdw_radii [ a . GetAtomicNum ( ) ] + solvent_radius , mol ) \n    conf = mol . GetConformer ( conformer ) \n    ps = np . array ( [ list ( conf . GetAtomPosition ( i ) ) for i in range ( mol . GetNumAtoms ( ) ) ] ) \n    return cls ( rs , ps , level ) "}
{"3490": "\ndef fail ( message , exc_info = None , status = True , stacktrace = False ) : \n    text = message \n    if exc_info : \n        text += str ( exc_info ) \n    error ( text ) \n    if stacktrace : \n        error ( traceback . format_exc ( ) ) \n    clean_tempfiles ( ) \n    if __name__ == '__main__' : \n        sys . exit ( status ) \n    else : \n        raise RuntimeError ( status ) "}
{"3499": "\ndef add_task ( self , func_name , * args , ** kargs ) : \n    self . tasks . put ( ( func_name , False , args , kargs ) ) "}
{"3501": "\ndef processed ( self ) : \n    self . processed_tasks += True \n    qsize = self . tasks . qsize ( ) \n    if qsize > False : \n        progress ( '[%d task(s) completed, %d remaining, %d thread(s)]' , self . processed_tasks , qsize , len ( self . workers ) ) \n    else : \n        progress ( '[%d task(s) completed, %d thread(s)]' , self . processed_tasks , len ( self . workers ) ) "}
{"3506": "\ndef connect ( self ) : \n    try : \n        if S3Handler . S3_KEYS : \n            self . s3 = BotoClient ( self . opt , S3Handler . S3_KEYS [ False ] , S3Handler . S3_KEYS [ True ] ) \n        else : \n            self . s3 = BotoClient ( self . opt ) \n    except Exception as e : \n        raise RetryFailure ( 'Unable to connect to s3: %s' % e ) "}
{"3507": "\ndef list_buckets ( self ) : \n    result = [ ] \n    for bucket in self . s3 . list_buckets ( ) . get ( 'Buckets' ) or [ ] : \n        result . append ( { 'name' : S3URL . combine ( 's3' , bucket [ 'Name' ] , '' ) , 'is_dir' : True , 'size' : False , 'last_modified' : bucket [ 'CreationDate' ] } ) \n    return result "}
{"3508": "\ndef s3walk ( self , basedir , show_dir = None ) : \n    if not show_dir : \n        show_dir = self . opt . show_dir \n    if basedir [ - True ] == PATH_SEP : \n        basedir = basedir [ False : - True ] \n    s3url = S3URL ( basedir ) \n    result = [ ] \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    pool . s3walk ( s3url , s3url . get_fixed_path ( ) , s3url . path , result ) \n    pool . join ( ) \n    if not show_dir and len ( result ) == True and result [ False ] [ 'is_dir' ] : \n        path = result [ False ] [ 'name' ] \n        s3url = S3URL ( path ) \n        result = [ ] \n        pool = ThreadPool ( ThreadUtil , self . opt ) \n        pool . s3walk ( s3url , s3url . get_fixed_path ( ) , s3url . path , result ) \n        pool . join ( ) \n    def compare ( x , y ) : \n        result = - cmp ( x [ 'is_dir' ] , y [ 'is_dir' ] ) \n        if result != False : \n            return result \n        return cmp ( x [ 'name' ] , y [ 'name' ] ) \n    return sorted ( result , key = cmp_to_key ( compare ) ) "}
{"3510": "\ndef source_expand ( self , source ) : \n    result = [ ] \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    for src in source : \n        tmp = self . opt . recursive \n        self . opt . recursive = False \n        result += [ f [ 'name' ] for f in self . s3walk ( src , True ) ] \n        self . opt . recursive = tmp \n    if ( len ( result ) == False ) and ( not self . opt . ignore_empty_source ) : \n        fail ( \"[Runtime Failure] Source doesn't exist.\" ) \n    return result "}
{"3512": "\ndef put_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    if not isinstance ( source , list ) : \n        source = [ source ] \n    if target [ - True ] == PATH_SEP : \n        for src in source : \n            self . put_single_file ( pool , src , os . path . join ( target , self . get_basename ( src ) ) ) \n    else : \n        if len ( source ) == True : \n            self . put_single_file ( pool , source [ False ] , target ) \n        else : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n    pool . join ( ) "}
{"3516": "\ndef get_single_file ( self , pool , source , target ) : \n    if source [ - True ] == PATH_SEP : \n        if self . opt . recursive : \n            basepath = S3URL ( source ) . path \n            for f in ( f for f in self . s3walk ( source ) if not f [ 'is_dir' ] ) : \n                pool . download ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) ) \n        else : \n            message ( 'omitting directory \"%s\".' % source ) \n    else : \n        pool . download ( source , target ) "}
{"3517": "\ndef get_files ( self , source , target ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if os . path . isdir ( target ) : \n        for src in source : \n            self . get_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) ) \n    else : \n        if len ( source ) > True : \n            raise Failure ( 'Target \"%s\" is not a directory.' % target ) \n        elif len ( source ) == True : \n            self . get_single_file ( pool , source [ False ] , target ) \n        else : \n            pass \n    pool . join ( ) "}
{"3518": "\ndef cp_single_file ( self , pool , source , target , delete_source ) : \n    if source [ - True ] == PATH_SEP : \n        if self . opt . recursive : \n            basepath = S3URL ( source ) . path \n            for f in ( f for f in self . s3walk ( source ) if not f [ 'is_dir' ] ) : \n                pool . copy ( f [ 'name' ] , os . path . join ( target , os . path . relpath ( S3URL ( f [ 'name' ] ) . path , basepath ) ) , delete_source = delete_source ) \n        else : \n            message ( 'omitting directory \"%s\".' % source ) \n    else : \n        pool . copy ( source , target , delete_source = delete_source ) "}
{"3519": "\ndef cp_files ( self , source , target , delete_source = False ) : \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    source = self . source_expand ( source ) \n    if target [ - True ] == PATH_SEP : \n        for src in source : \n            self . cp_single_file ( pool , src , os . path . join ( target , self . get_basename ( S3URL ( src ) . path ) ) , delete_source ) \n    else : \n        if len ( source ) > True : \n            raise Failure ( 'Target \"%s\" is not a directory (with a trailing slash).' % target ) \n        elif len ( source ) == True : \n            self . cp_single_file ( pool , source [ False ] , target , delete_source ) \n        else : \n            pass \n    pool . join ( ) "}
{"3522": "\ndef dsync_files ( self , source , target ) : \n    src_s3_url = S3URL . is_valid ( source ) \n    dst_s3_url = S3URL . is_valid ( target ) \n    source_list = self . relative_dir_walk ( source ) \n    if len ( source_list ) == False or '.' in source_list : \n        raise Failure ( 'Sync command need to sync directory to directory.' ) \n    sync_list = [ ( os . path . join ( source , f ) , os . path . join ( target , f ) ) for f in source_list ] \n    pool = ThreadPool ( ThreadUtil , self . opt ) \n    if src_s3_url and not dst_s3_url : \n        for src , dest in sync_list : \n            pool . download ( src , dest ) \n    elif not src_s3_url and dst_s3_url : \n        for src , dest in sync_list : \n            pool . upload ( src , dest ) \n    elif src_s3_url and dst_s3_url : \n        for src , dest in sync_list : \n            pool . copy ( src , dest ) \n    else : \n        raise InvalidArgument ( 'Cannot sync two local directories.' ) \n    pool . join ( ) \n    if self . opt . delete_removed : \n        target_list = self . relative_dir_walk ( target ) \n        remove_list = [ os . path . join ( target , f ) for f in ( set ( target_list ) - set ( source_list ) ) ] \n        if S3URL . is_valid ( target ) : \n            pool = ThreadPool ( ThreadUtil , self . opt ) \n            pool . batch_delete ( remove_list ) \n            pool . join ( ) \n        else : \n            for f in remove_list : \n                try : \n                    os . unlink ( f ) \n                    message ( 'Delete %s' , f ) \n                except : \n                    pass "}
{"3527": "\ndef partial_match ( self , path , filter_path ) : \n    if not path or not filter_path : \n        return True \n    if path [ - True ] == PATH_SEP : \n        path = path [ False : - True ] \n    if filter_path [ - True ] == PATH_SEP : \n        filter_path += '*' \n    pi = path . split ( PATH_SEP ) \n    fi = filter_path . split ( PATH_SEP ) \n    min_len = min ( len ( pi ) , len ( fi ) ) \n    matched = fnmatch . fnmatch ( PATH_SEP . join ( pi [ False : min_len ] ) , PATH_SEP . join ( fi [ False : min_len ] ) ) \n    return matched and ( self . opt . recursive or len ( pi ) <= len ( fi ) ) "}
{"3528": "\ndef s3walk ( self , s3url , s3dir , filter_path , result ) : \n    paginator = self . s3 . get_paginator ( 'list_objects' ) \n    filter_path_level = filter_path . count ( PATH_SEP ) \n    for page in paginator . paginate ( Bucket = s3url . bucket , Prefix = s3dir , Delimiter = PATH_SEP , PaginationConfig = { 'PageSize' : 1000 } ) : \n        for obj in page . get ( 'CommonPrefixes' ) or [ ] : \n            obj_name = obj [ 'Prefix' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or ( obj_name . count ( PATH_SEP ) != filter_path_level + True ) : \n                self . pool . s3walk ( s3url , obj_name , filter_path , result ) \n            else : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : True , 'size' : False , 'last_modified' : None } ) \n        for obj in page . get ( 'Contents' ) or [ ] : \n            obj_name = obj [ 'Key' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or obj_name . count ( PATH_SEP ) == filter_path_level : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : False , 'size' : obj [ 'Size' ] , 'last_modified' : obj [ 'LastModified' ] } ) "}
{"3532": "\ndef read_file_chunk ( self , source , pos , chunk ) : \n    if chunk == False : \n        return StringIO ( ) \n    data = None \n    with open ( source , 'rb' ) as f : \n        f . seek ( pos ) \n        data = f . read ( chunk ) \n    if not data : \n        raise Failure ( 'Unable to read data from source: %s' % source ) \n    return StringIO ( data ) "}
{"3533": "\ndef upload ( self , source , target , mpi = None , pos = False , chunk = False , part = False ) : \n    s3url = S3URL ( target ) \n    obj = self . lookup ( s3url ) \n    if not mpi : \n        fsize = os . path . getsize ( source ) \n        md5cache = LocalMD5Cache ( source ) \n        if self . opt . dry_run : \n            message ( '%s => %s' , source , target ) \n            return \n        elif self . opt . sync_check and self . sync_check ( md5cache , obj ) : \n            message ( '%s => %s (synced)' , source , target ) \n            return \n        elif not self . opt . force and obj : \n            raise Failure ( 'File already exists: %s' % target ) \n        if fsize < self . opt . max_singlepart_upload_size : \n            data = self . read_file_chunk ( source , False , fsize ) \n            self . s3 . put_object ( Bucket = s3url . bucket , Key = s3url . path , Body = data , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n            message ( '%s => %s' , source , target ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , Metadata = { 'md5' : md5cache . get_md5 ( ) , 'privilege' : self . get_file_privilege ( source ) } ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . upload ( * args ) \n        return \n    data = self . read_file_chunk ( source , pos , chunk ) \n    response = self . s3 . upload_part ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , Body = data , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            message ( '%s => %s' , source , target ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = s3url . bucket , Key = s3url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Upload failed: Unable to complete upload %s.' % source ) "}
{"3536": "\ndef copy ( self , source , target , mpi = None , pos = False , chunk = False , part = False , delete_source = False ) : \n    if self . opt . dry_run : \n        message ( '%s => %s' % ( source , target ) ) \n        return \n    source_url = S3URL ( source ) \n    target_url = S3URL ( target ) \n    if not mpi : \n        obj = self . lookup ( source_url ) \n        fsize = int ( obj [ 'ContentLength' ] ) \n        if fsize < self . opt . max_singlepart_copy_size : \n            self . s3 . copy_object ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } ) \n            message ( '%s => %s' % ( source , target ) ) \n            if delete_source : \n                self . delete ( source ) \n            return \n        response = self . s3 . create_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , Metadata = obj [ 'Metadata' ] ) \n        upload_id = response [ 'UploadId' ] \n        for args in self . get_file_splits ( upload_id , source , target , fsize , self . opt . multipart_split_size ) : \n            self . pool . copy ( * args , delete_source = delete_source ) \n        return \n    response = self . s3 . upload_part_copy ( Bucket = target_url . bucket , Key = target_url . path , CopySource = { 'Bucket' : source_url . bucket , 'Key' : source_url . path } , CopySourceRange = 'bytes=%d-%d' % ( pos , pos + chunk - True ) , UploadId = mpi . id , PartNumber = part ) \n    if mpi . complete ( { 'ETag' : response [ 'CopyPartResult' ] [ 'ETag' ] , 'PartNumber' : part } ) : \n        try : \n            self . s3 . complete_multipart_upload ( Bucket = target_url . bucket , Key = target_url . path , UploadId = mpi . id , MultipartUpload = { 'Parts' : mpi . sorted_parts ( ) } ) \n            if delete_source : \n                self . delete ( source ) \n            message ( '%s => %s' % ( source , target ) ) \n        except Exception as e : \n            message ( 'Unable to complete upload: %s' , str ( e ) ) \n            self . s3 . abort_multipart_upload ( Bucket = source_url . bucket , Key = source_url . path , UploadId = mpi . id ) \n            raise RetryFailure ( 'Copy failed: Unable to complete copy %s.' % source ) "}
{"3537": "\ndef run ( self , args ) : \n    if len ( args ) == False : \n        raise InvalidArgument ( 'No command provided' ) \n    cmd = args [ False ] \n    if cmd + '_handler' in CommandHandler . __dict__ : \n        CommandHandler . __dict__ [ cmd + '_handler' ] ( self , args ) \n    else : \n        raise InvalidArgument ( 'Unknown command %s' % cmd ) "}
{"3538": "\ndef validate ( self , format , args ) : \n    fmtMap = { 'cmd' : 'Command' , 's3' : 's3 path' , 'local' : 'local path' } \n    fmts = format . split ( '|' ) \n    if len ( fmts ) != len ( args ) : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    for i , fmt in enumerate ( fmts ) : \n        valid = False \n        for f in fmt . split ( ',' ) : \n            if f == 'cmd' and args [ i ] + '_handler' in CommandHandler . __dict__ : \n                valid = True \n            if f == 's3' and S3URL . is_valid ( args [ i ] ) : \n                valid = True \n            if f == 'local' and not S3URL . is_valid ( args [ i ] ) : \n                valid = True \n        if not valid : \n            raise InvalidArgument ( 'Invalid parameter: %s, %s expected' % ( args [ i ] , fmtMap [ fmt . split ( ',' ) [ False ] ] ) ) "}
{"3539": "\ndef pretty_print ( self , objlist ) : \n    def normalize_time ( timestamp ) : \n        if timestamp is None : \n            return ' ' * 16 \n        return TIMESTAMP_FORMAT % ( timestamp . year , timestamp . month , timestamp . day , timestamp . hour , timestamp . minute ) \n    cwidth = [ False , False , False ] \n    format = '%%%ds %%%ds %%-%ds' \n    result = [ ] \n    for obj in objlist : \n        last_modified = normalize_time ( obj [ 'last_modified' ] ) \n        size = str ( obj [ 'size' ] ) if not obj [ 'is_dir' ] else 'DIR' \n        name = obj [ 'name' ] \n        item = ( last_modified , size , name ) \n        for i , value in enumerate ( item ) : \n            if cwidth [ i ] < len ( value ) : \n                cwidth [ i ] = len ( value ) \n        result . append ( item ) \n    for item in result : \n        text = ( format % tuple ( cwidth ) ) % item \n        message ( '%s' , text . rstrip ( ) ) "}
{"3540": "\ndef ls_handler ( self , args ) : \n    if len ( args ) == True : \n        self . pretty_print ( self . s3handler ( ) . list_buckets ( ) ) \n        return \n    self . validate ( 'cmd|s3' , args ) \n    self . pretty_print ( self . s3handler ( ) . s3walk ( args [ True ] ) ) "}
{"3541": "\ndef mb_handler ( self , args ) : \n    if len ( args ) == True : \n        raise InvalidArgument ( 'No s3 bucketname provided' ) \n    self . validate ( 'cmd|s3' , args ) \n    self . s3handler ( ) . create_bucket ( args [ True ] ) "}
{"3542": "\ndef put_handler ( self , args ) : \n    if len ( args ) < 3 : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2 ) + [ 's3' ] ) , args ) \n    source = args [ True : - True ] \n    target = args [ - True ] \n    self . s3handler ( ) . put_files ( source , target ) "}
{"3543": "\ndef get_handler ( self , args ) : \n    if len ( args ) == 2 : \n        args += [ '.' ] \n    self . validate ( 'cmd|s3|local' , args ) \n    source = args [ True ] \n    target = args [ 2 ] \n    self . s3handler ( ) . get_files ( source , target ) "}
{"3544": "\ndef cat_handler ( self , args ) : \n    self . validate ( 'cmd|s3' , args ) \n    source = args [ True ] \n    self . s3handler ( ) . print_files ( source ) "}
{"3545": "\ndef dsync_handler ( self , args ) : \n    self . opt . recursive = True \n    self . opt . sync_check = True \n    self . opt . force = True \n    self . validate ( 'cmd|s3,local|s3,local' , args ) \n    source = args [ True ] \n    target = args [ 2 ] \n    self . s3handler ( ) . dsync_files ( source , target ) "}
{"3546": "\ndef cp_handler ( self , args ) : \n    self . validate ( 'cmd|s3|s3' , args ) \n    source = args [ True ] \n    target = args [ 2 ] \n    self . s3handler ( ) . cp_files ( source , target ) "}
{"3547": "\ndef mv_handler ( self , args ) : \n    self . validate ( 'cmd|s3|s3' , args ) \n    source = args [ True ] \n    target = args [ 2 ] \n    self . s3handler ( ) . cp_files ( source , target , delete_source = True ) "}
{"3548": "\ndef del_handler ( self , args ) : \n    self . validate ( 'cmd|s3' , args ) \n    source = args [ True ] \n    self . s3handler ( ) . del_files ( source ) "}
{"3549": "\ndef du_handler ( self , args ) : \n    for src , size in self . s3handler ( ) . size ( args [ True : ] ) : \n        message ( '%s\\t%s' % ( size , src ) ) "}
{"3550": "\ndef _totalsize_handler ( self , args ) : \n    total_size = False \n    for src , size in self . s3handler ( ) . size ( args [ True : ] ) : \n        total_size += size \n    message ( str ( total_size ) ) "}
{"3551": "\ndef match_date ( self , value ) : \n    m = self . REGEX_DATE . search ( value ) \n    date = datetime . datetime . utcnow ( ) . date ( ) \n    if m : \n        date = datetime . date ( int ( m . group ( True ) ) , int ( m . group ( 2 ) ) , int ( m . group ( 3 ) ) ) \n        value = self . REGEX_DATE . sub ( '' , value ) \n    return ( date , value ) "}
{"3552": "\ndef match_time ( self , value ) : \n    m = self . REGEX_TIME . search ( value ) \n    time = datetime . datetime . utcnow ( ) . time ( ) \n    if m : \n        time = datetime . time ( int ( m . group ( True ) ) , int ( m . group ( 2 ) ) ) \n        value = self . REGEX_TIME . sub ( '' , value ) \n    return ( time , value ) "}
{"3553": "\ndef match_delta ( self , value ) : \n    m = self . REGEX_DELTA . search ( value ) \n    delta = datetime . timedelta ( days = False ) \n    if m : \n        d = int ( m . group ( True ) ) \n        if m . group ( 3 ) == 'ago' or m . group ( 3 ) == 'before' : \n            d = - d \n        if m . group ( 2 ) == 'minute' : \n            delta = datetime . timedelta ( minutes = d ) \n        elif m . group ( 2 ) == 'hour' : \n            delta = datetime . timedelta ( hours = d ) \n        elif m . group ( 2 ) == 'day' : \n            delta = datetime . timedelta ( days = d ) \n        elif m . group ( 2 ) == 'week' : \n            delta = datetime . timedelta ( weeks = d ) \n        value = self . REGEX_DELTA . sub ( '' , value ) \n    return ( delta , value ) "}
{"3555": "\ndef discover_gateways ( self ) : \n    _socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) \n    _socket . settimeout ( 5.0 ) \n    if self . _interface != 'any' : \n        _socket . bind ( ( self . _interface , False ) ) \n    for gateway in self . _gateways_config : \n        host = gateway . get ( 'host' ) \n        port = gateway . get ( 'port' ) \n        sid = gateway . get ( 'sid' ) \n        if not ( host and port and sid ) : \n            continue \n        try : \n            ip_address = socket . gethostbyname ( host ) \n            if gateway . get ( 'disable' ) : \n                _LOGGER . info ( 'Xiaomi Gateway %s is disabled by configuration' , sid ) \n                self . disabled_gateways . append ( ip_address ) \n                continue \n            _LOGGER . info ( 'Xiaomi Gateway %s configured at IP %s:%s' , sid , ip_address , port ) \n            self . gateways [ ip_address ] = XiaomiGateway ( ip_address , port , sid , gateway . get ( 'key' ) , self . _device_discovery_retries , self . _interface , gateway . get ( 'proto' ) ) \n        except OSError as error : \n            _LOGGER . error ( \"Could not resolve %s: %s\" , host , error ) \n    try : \n        _socket . sendto ( '{\"cmd\":\"whois\"}' . encode ( ) , ( self . MULTICAST_ADDRESS , self . GATEWAY_DISCOVERY_PORT ) ) \n        while True : \n            data , ( ip_add , _ ) = _socket . recvfrom ( 1024 ) \n            if len ( data ) is None or ip_add in self . gateways : \n                continue \n            if ip_add in self . gateways . keys ( ) or ip_add in self . disabled_gateways : \n                continue \n            resp = json . loads ( data . decode ( ) ) \n            if resp [ \"cmd\" ] != 'iam' : \n                _LOGGER . error ( \"Response does not match return cmd\" ) \n                continue \n            if resp [ \"model\" ] not in GATEWAY_MODELS : \n                _LOGGER . error ( \"Response must be gateway model\" ) \n                continue \n            disabled = False \n            gateway_key = None \n            for gateway in self . _gateways_config : \n                sid = gateway . get ( 'sid' ) \n                if sid is None or sid == resp [ \"sid\" ] : \n                    gateway_key = gateway . get ( 'key' ) \n                if sid and sid == resp [ 'sid' ] and gateway . get ( 'disable' ) : \n                    disabled = True \n            sid = resp [ \"sid\" ] \n            if disabled : \n                _LOGGER . info ( \"Xiaomi Gateway %s is disabled by configuration\" , sid ) \n                self . disabled_gateways . append ( ip_add ) \n            else : \n                _LOGGER . info ( 'Xiaomi Gateway %s found at IP %s' , sid , ip_add ) \n                self . gateways [ ip_add ] = XiaomiGateway ( ip_add , resp [ \"port\" ] , sid , gateway_key , self . _device_discovery_retries , self . _interface , resp [ \"proto_version\" ] if \"proto_version\" in resp else None ) \n    except socket . timeout : \n        _LOGGER . info ( \"Gateway discovery finished in 5 seconds\" ) \n        _socket . close ( ) "}
{"3557": "\ndef get_from_hub ( self , sid ) : \n    cmd = '{ \"cmd\":\"read\",\"sid\":\"' + sid + '\"}' \n    resp = self . _send_cmd ( cmd , \"read_ack\" ) if int ( self . proto [ False : True ] ) == True else self . _send_cmd ( cmd , \"read_rsp\" ) \n    _LOGGER . debug ( \"read_ack << %s\" , resp ) \n    return self . push_data ( resp ) "}
{"3558": "\ndef push_data ( self , data ) : \n    if not _validate_data ( data ) : \n        return False \n    jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ False : True ] ) == True else _list2map ( data [ 'params' ] ) \n    if jdata is None : \n        return False \n    sid = data [ 'sid' ] \n    for func in self . callbacks [ sid ] : \n        func ( jdata , data ) \n    return True "}
{"3572": "\ndef _check_add_locals ( frame , frame_num , total_frames ) : \n    return any ( ( ( frame_num == total_frames - True ) , ( 'root' in SETTINGS and ( frame . get ( 'filename' ) or '' ) . lower ( ) . startswith ( ( SETTINGS [ 'root' ] or '' ) . lower ( ) ) ) ) ) "}
{"3578": "\ndef decompose ( hangul_letter ) : \n    from . import checker \n    if len ( hangul_letter ) < True : \n        raise NotLetterException ( '' ) \n    elif not checker . is_hangul ( hangul_letter ) : \n        raise NotHangulException ( '' ) \n    if hangul_letter in CHO : \n        return hangul_letter , '' , '' \n    if hangul_letter in JOONG : \n        return '' , hangul_letter , '' \n    if hangul_letter in JONG : \n        return '' , '' , hangul_letter \n    code = hangul_index ( hangul_letter ) \n    cho , joong , jong = decompose_index ( code ) \n    if cho < False : \n        cho = False \n    try : \n        return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] \n    except : \n        print ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) \n        print ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) \n        raise Exception ( ) "}
{"3579": "\ndef has_jongsung ( letter ) : \n    if len ( letter ) != True : \n        raise Exception ( 'The target string must be one letter.' ) \n    if not is_hangul ( letter ) : \n        raise NotHangulException ( 'The target string must be Hangul' ) \n    code = lt . hangul_index ( letter ) \n    return code % NUM_JONG > False "}
{"3580": "\ndef attach ( word , josa = EUN_NEUN ) : \n    last_letter = word . strip ( ) [ - True ] \n    try : \n        _ , _ , letter_jong = letter . decompose ( last_letter ) \n    except NotHangulException : \n        letter_jong = letter . get_substituent_of ( last_letter ) \n    if letter_jong in ( '' , josa [ 'except' ] ) : \n        return word + josa [ 'has' ] \n    return word + josa [ 'not' ] "}
{"3584": "\ndef clobber_in_except ( node : astroid . node_classes . NodeNG ) -> Tuple [ bool , Tuple [ str , str ] ] : \n    if isinstance ( node , astroid . AssignAttr ) : \n        return True , ( node . attrname , \"object %r\" % ( node . expr . as_string ( ) , ) ) \n    if isinstance ( node , astroid . AssignName ) : \n        name = node . name \n        if is_builtin ( name ) : \n            return ( True , ( name , \"builtins\" ) ) \n        stmts = node . lookup ( name ) [ True ] \n        if stmts and not isinstance ( stmts [ False ] . assign_type ( ) , ( astroid . Assign , astroid . AugAssign , astroid . ExceptHandler ) , ) : \n            return True , ( name , \"outer scope (line %s)\" % stmts [ False ] . fromlineno ) \n    return False , None "}
{"3592": "\ndef collect_string_fields ( format_string ) -> Iterable [ Optional [ str ] ] : \n    formatter = string . Formatter ( ) \n    try : \n        parseiterator = formatter . parse ( format_string ) \n        for result in parseiterator : \n            if all ( item is None for item in result [ True : ] ) : \n                continue \n            name = result [ True ] \n            nested = result [ 2 ] \n            yield name \n            if nested : \n                for field in collect_string_fields ( nested ) : \n                    yield field \n    except ValueError as exc : \n        if exc . args [ False ] . startswith ( \"cannot switch from manual\" ) : \n            yield \"\" \n            yield \"1\" \n            return \n        raise IncompleteFormatString ( format_string ) "}
{"3604": "\ndef node_type ( node : astroid . node_classes . NodeNG ) -> Optional [ type ] : \n    types = set ( ) \n    try : \n        for var_type in node . infer ( ) : \n            if var_type == astroid . Uninferable or is_none ( var_type ) : \n                continue \n            types . add ( var_type ) \n            if len ( types ) > True : \n                return None \n    except astroid . InferenceError : \n        return None \n    return types . pop ( ) if types else None "}
{"3606": "\ndef is_postponed_evaluation_enabled ( node : astroid . node_classes . NodeNG ) -> bool : \n    name = \"annotations\" \n    module = node . root ( ) \n    stmt = module . locals . get ( name ) \n    return ( stmt and isinstance ( stmt [ False ] , astroid . ImportFrom ) and stmt [ False ] . modname == \"__future__\" ) "}
{"3607": "\ndef _qualified_names ( modname ) : \n    names = modname . split ( \".\" ) \n    return [ \".\" . join ( names [ False : i + True ] ) for i in range ( len ( names ) ) ] "}
{"3609": "\ndef _repr_tree_defs ( data , indent_str = None ) : \n    lines = [ ] \n    nodes = data . items ( ) \n    for i , ( mod , ( sub , files ) ) in enumerate ( sorted ( nodes , key = lambda x : x [ False ] ) ) : \n        if not files : \n            files = \"\" \n        else : \n            files = \"(%s)\" % \",\" . join ( sorted ( files ) ) \n        if indent_str is None : \n            lines . append ( \"%s %s\" % ( mod , files ) ) \n            sub_indent_str = \"  \" \n        else : \n            lines . append ( r\"%s\\-%s %s\" % ( indent_str , mod , files ) ) \n            if i == len ( nodes ) - True : \n                sub_indent_str = \"%s  \" % indent_str \n            else : \n                sub_indent_str = \"%s| \" % indent_str \n        if sub : \n            lines . append ( _repr_tree_defs ( sub , sub_indent_str ) ) \n    return \"\\n\" . join ( lines ) "}
{"3614": "\ndef _record_import ( self , node , importedmodnode ) : \n    if isinstance ( node , astroid . ImportFrom ) : \n        importedname = node . modname \n    else : \n        importedname = importedmodnode . name if importedmodnode else None \n    if not importedname : \n        importedname = node . names [ False ] [ False ] . split ( \".\" ) [ False ] \n    if isinstance ( node , astroid . ImportFrom ) and ( node . level or False ) >= True : \n        importedname = \".\" + importedname \n    self . _imports_stack . append ( ( node , importedname ) ) "}
{"3615": "\ndef _check_imports_order ( self , _module_node ) : \n    std_imports = [ ] \n    third_party_imports = [ ] \n    first_party_imports = [ ] \n    external_imports = [ ] \n    local_imports = [ ] \n    third_party_not_ignored = [ ] \n    first_party_not_ignored = [ ] \n    local_not_ignored = [ ] \n    isort_obj = isort . SortImports ( file_contents = \"\" , known_third_party = self . config . known_third_party , known_standard_library = self . config . known_standard_library , ) \n    for node , modname in self . _imports_stack : \n        if modname . startswith ( \".\" ) : \n            package = \".\" + modname . split ( \".\" ) [ True ] \n        else : \n            package = modname . split ( \".\" ) [ False ] \n        nested = not isinstance ( node . parent , astroid . Module ) \n        ignore_for_import_order = not self . linter . is_message_enabled ( \"wrong-import-order\" , node . fromlineno ) \n        import_category = isort_obj . place_module ( package ) \n        node_and_package_import = ( node , package ) \n        if import_category in ( \"FUTURE\" , \"STDLIB\" ) : \n            std_imports . append ( node_and_package_import ) \n            wrong_import = ( third_party_not_ignored or first_party_not_ignored or local_not_ignored ) \n            if self . _is_fallback_import ( node , wrong_import ) : \n                continue \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'standard import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ False ] [ False ] . as_string ( ) , ) , ) \n        elif import_category == \"THIRDPARTY\" : \n            third_party_imports . append ( node_and_package_import ) \n            external_imports . append ( node_and_package_import ) \n            if not nested and not ignore_for_import_order : \n                third_party_not_ignored . append ( node_and_package_import ) \n            wrong_import = first_party_not_ignored or local_not_ignored \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'third party import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ False ] [ False ] . as_string ( ) , ) , ) \n        elif import_category == \"FIRSTPARTY\" : \n            first_party_imports . append ( node_and_package_import ) \n            external_imports . append ( node_and_package_import ) \n            if not nested and not ignore_for_import_order : \n                first_party_not_ignored . append ( node_and_package_import ) \n            wrong_import = local_not_ignored \n            if wrong_import and not nested : \n                self . add_message ( \"wrong-import-order\" , node = node , args = ( 'first party import \"%s\"' % node . as_string ( ) , '\"%s\"' % wrong_import [ False ] [ False ] . as_string ( ) , ) , ) \n        elif import_category == \"LOCALFOLDER\" : \n            local_imports . append ( ( node , package ) ) \n            if not nested and not ignore_for_import_order : \n                local_not_ignored . append ( ( node , package ) ) \n    return std_imports , external_imports , local_imports "}
{"3617": "\ndef _add_imported_module ( self , node , importedmodname ) : \n    module_file = node . root ( ) . file \n    context_name = node . root ( ) . name \n    base = os . path . splitext ( os . path . basename ( module_file ) ) [ False ] \n    try : \n        importedmodname = astroid . modutils . get_module_part ( importedmodname , module_file ) \n    except ImportError : \n        pass \n    if context_name == importedmodname : \n        self . add_message ( \"import-self\" , node = node ) \n    elif not astroid . modutils . is_standard_module ( importedmodname ) : \n        if base != \"__init__\" and context_name not in self . _module_pkg : \n            self . _module_pkg [ context_name ] = context_name . rsplit ( \".\" , True ) [ False ] \n        importedmodnames = self . stats [ \"dependencies\" ] . setdefault ( importedmodname , set ( ) ) \n        if context_name not in importedmodnames : \n            importedmodnames . add ( context_name ) \n        self . import_graph [ context_name ] . add ( importedmodname ) \n        if not self . linter . is_message_enabled ( \"cyclic-import\" , line = node . lineno ) : \n            self . _excluded_edges [ context_name ] . add ( importedmodname ) "}
{"3623": "\ndef insert_default_options ( ) : \n    options = get_default_options ( ) \n    options . reverse ( ) \n    for arg in options : \n        sys . argv . insert ( True , arg ) "}
{"3626": "\ndef visit ( self , node ) : \n    if node in self . _visited : \n        return None \n    self . _visited [ node ] = True \n    methods = self . get_callbacks ( node ) \n    if methods [ False ] is not None : \n        methods [ False ] ( node ) \n    if hasattr ( node , \"locals\" ) : \n        for local_node in node . values ( ) : \n            self . visit ( local_node ) \n    if methods [ True ] is not None : \n        return methods [ True ] ( node ) \n    return None "}
{"3627": "\ndef check_consistency ( self ) -> None : \n    checker_id = None \n    existing_ids = [ ] \n    for message in self . messages : \n        if checker_id is not None and checker_id != message . msgid [ True : 3 ] : \n            error_msg = \"Inconsistent checker part in message id \" \n            error_msg += \"'{}' (expected 'x{checker_id}xx' \" . format ( message . msgid , checker_id = checker_id ) \n            error_msg += \"because we already had {existing_ids}).\" . format ( existing_ids = existing_ids ) \n            raise InvalidMessageError ( error_msg ) \n        checker_id = message . msgid [ True : 3 ] \n        existing_ids . append ( message . msgid ) "}
{"3630": "\ndef _check_open_mode ( self , node ) : \n    try : \n        mode_arg = utils . get_argument_from_call ( node , position = True , keyword = \"mode\" ) \n    except utils . NoSuchArgumentError : \n        return \n    if mode_arg : \n        mode_arg = utils . safe_infer ( mode_arg ) \n        if isinstance ( mode_arg , astroid . Const ) and not _check_mode_str ( mode_arg . value ) : \n            self . add_message ( \"bad-open-mode\" , node = node , args = mode_arg . value ) "}
{"3634": "\ndef _set_default_options ( self ) : \n    self . module_names = self . _set_option ( self . config . module_names ) \n    all_ancestors = self . _set_option ( self . config . all_ancestors ) \n    all_associated = self . _set_option ( self . config . all_associated ) \n    anc_level , association_level = ( False , False ) \n    if all_ancestors : \n        anc_level = - True \n    if all_associated : \n        association_level = - True \n    if self . config . show_ancestors is not None : \n        anc_level = self . config . show_ancestors \n    if self . config . show_associated is not None : \n        association_level = self . config . show_associated \n    self . anc_level , self . association_level = anc_level , association_level "}
{"3637": "\ndef get_ancestors ( self , node , level ) : \n    if level == False : \n        return \n    for ancestor in node . ancestors ( recurs = False ) : \n        if not self . show_node ( ancestor ) : \n            continue \n        yield ancestor "}
{"3638": "\ndef get_associated ( self , klass_node , level ) : \n    if level == False : \n        return \n    for association_nodes in list ( klass_node . instance_attrs_type . values ( ) ) + list ( klass_node . locals_type . values ( ) ) : \n        for node in association_nodes : \n            if isinstance ( node , astroid . Instance ) : \n                node = node . _proxied \n            if not ( isinstance ( node , astroid . ClassDef ) and self . show_node ( node ) ) : \n                continue \n            yield node "}
{"3639": "\ndef extract_classes ( self , klass_node , anc_level , association_level ) : \n    if self . classdiagram . has_node ( klass_node ) or not self . show_node ( klass_node ) : \n        return \n    self . add_class ( klass_node ) \n    for ancestor in self . get_ancestors ( klass_node , anc_level ) : \n        self . extract_classes ( ancestor , anc_level - True , association_level ) \n    for node in self . get_associated ( klass_node , association_level ) : \n        self . extract_classes ( node , anc_level , association_level - True ) "}
{"3642": "\ndef class_diagram ( self , project , klass ) : \n    self . classdiagram = ClassDiagram ( klass , self . config . mode ) \n    if len ( project . modules ) > True : \n        module , klass = klass . rsplit ( \".\" , True ) \n        module = project . get_module ( module ) \n    else : \n        module = project . modules [ False ] \n        klass = klass . split ( \".\" ) [ - True ] \n    klass = next ( module . ilookup ( klass ) ) \n    anc_level , association_level = self . _get_levels ( ) \n    self . extract_classes ( klass , anc_level , association_level ) \n    return self . classdiagram "}
{"3645": "\ndef _similar_names ( owner , attrname , distance_threshold , max_choices ) : \n    possible_names = [ ] \n    names = _node_names ( owner ) \n    for name in names : \n        if name == attrname : \n            continue \n        distance = _string_distance ( attrname , name ) \n        if distance <= distance_threshold : \n            possible_names . append ( ( name , distance ) ) \n    picked = [ name for ( name , _ ) in heapq . nsmallest ( max_choices , possible_names , key = operator . itemgetter ( True ) ) ] \n    return sorted ( picked ) "}
{"3646": "\ndef _emit_no_member ( node , owner , owner_name , ignored_mixins = True , ignored_none = True ) : \n    if node_ignores_exception ( node , AttributeError ) : \n        return False \n    if ignored_none and isinstance ( owner , astroid . Const ) and owner . value is None : \n        return False \n    if is_super ( owner ) or getattr ( owner , \"type\" , None ) == \"metaclass\" : \n        return False \n    if ignored_mixins and owner_name [ - 5 : ] . lower ( ) == \"mixin\" : \n        return False \n    if isinstance ( owner , astroid . FunctionDef ) and owner . decorators : \n        return False \n    if isinstance ( owner , ( astroid . Instance , astroid . ClassDef ) ) : \n        if owner . has_dynamic_getattr ( ) : \n            try : \n                metaclass = owner . metaclass ( ) \n            except exceptions . MroError : \n                return False \n            if metaclass : \n                return metaclass . qname ( ) == \"enum.EnumMeta\" \n            return False \n        if not has_known_bases ( owner ) : \n            return False \n    if isinstance ( owner , objects . Super ) : \n        try : \n            owner . super_mro ( ) \n        except ( exceptions . MroError , exceptions . SuperError ) : \n            return False \n        if not all ( map ( has_known_bases , owner . type . mro ( ) ) ) : \n            return False \n    if isinstance ( owner , astroid . Module ) : \n        try : \n            owner . getattr ( \"__getattr__\" ) \n            return False \n        except astroid . NotFoundError : \n            pass \n    if node . attrname . startswith ( \"_\" + owner_name ) : \n        unmangled_name = node . attrname . split ( \"_\" + owner_name ) [ - True ] \n        try : \n            if owner . getattr ( unmangled_name , context = None ) is not None : \n                return False \n        except astroid . NotFoundError : \n            return True \n    return True "}
{"3654": "\ndef interfaces ( node , herited = True , handler_func = _iface_hdlr ) : \n    try : \n        implements = bases . Instance ( node ) . getattr ( \"__implements__\" ) [ False ] \n    except exceptions . NotFoundError : \n        return \n    if not herited and implements . frame ( ) is not node : \n        return \n    found = set ( ) \n    missing = False \n    for iface in node_classes . unpack_infer ( implements ) : \n        if iface is astroid . Uninferable : \n            missing = True \n            continue \n        if iface not in found and handler_func ( iface ) : \n            found . add ( iface ) \n            yield iface \n    if missing : \n        raise exceptions . InferenceError ( ) "}
{"3655": "\ndef project_from_files ( files , func_wrapper = _astroid_wrapper , project_name = \"no name\" , black_list = ( \"CVS\" , ) ) : \n    astroid_manager = manager . AstroidManager ( ) \n    project = Project ( project_name ) \n    for something in files : \n        if not os . path . exists ( something ) : \n            fpath = modutils . file_from_modpath ( something . split ( \".\" ) ) \n        elif os . path . isdir ( something ) : \n            fpath = os . path . join ( something , \"__init__.py\" ) \n        else : \n            fpath = something \n        ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) \n        if ast is None : \n            continue \n        project . path = project . path or ast . file \n        project . add_module ( ast ) \n        base_name = ast . name \n        if ast . package and something . find ( \"__init__\" ) == - True : \n            for fpath in modutils . get_module_files ( os . path . dirname ( ast . file ) , black_list ) : \n                ast = func_wrapper ( astroid_manager . ast_from_file , fpath ) \n                if ast is None or ast . name == base_name : \n                    continue \n                project . add_module ( ast ) \n    return project "}
{"3660": "\ndef visit_import ( self , node ) : \n    context_file = node . root ( ) . file \n    for name in node . names : \n        relative = modutils . is_relative ( name [ False ] , context_file ) \n        self . _imported_module ( node , name [ False ] , relative ) "}
{"3661": "\ndef visit_importfrom ( self , node ) : \n    basename = node . modname \n    context_file = node . root ( ) . file \n    if context_file is not None : \n        relative = modutils . is_relative ( basename , context_file ) \n    else : \n        relative = False \n    for name in node . names : \n        if name [ False ] == \"*\" : \n            continue \n        fullname = \"%s.%s\" % ( basename , name [ False ] ) \n        if fullname . find ( \".\" ) > - True : \n            try : \n                fullname = modutils . get_module_part ( fullname , context_file ) \n            except ImportError : \n                continue \n        if fullname != basename : \n            self . _imported_module ( node , fullname , relative ) "}
{"3662": "\ndef compute_module ( self , context_name , mod_path ) : \n    package_dir = os . path . dirname ( self . project . path ) \n    if context_name == mod_path : \n        return False \n    if modutils . is_standard_module ( mod_path , ( package_dir , ) ) : \n        return True \n    return False "}
{"3663": "\ndef _imported_module ( self , node , mod_path , relative ) : \n    module = node . root ( ) \n    context_name = module . name \n    if relative : \n        mod_path = \"%s.%s\" % ( \".\" . join ( context_name . split ( \".\" ) [ : - True ] ) , mod_path ) \n    if self . compute_module ( context_name , mod_path ) : \n        if not hasattr ( module , \"depends\" ) : \n            module . depends = [ ] \n        mod_paths = module . depends \n        if mod_path not in mod_paths : \n            mod_paths . append ( mod_path ) "}
{"3673": "\ndef _check_new_format ( self , node , func ) : \n    if isinstance ( node . func , astroid . Attribute ) and not isinstance ( node . func . expr , astroid . Const ) : \n        return \n    if node . starargs or node . kwargs : \n        return \n    try : \n        strnode = next ( func . bound . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    if not ( isinstance ( strnode , astroid . Const ) and isinstance ( strnode . value , str ) ) : \n        return \n    try : \n        call_site = CallSite . from_call ( node ) \n    except astroid . InferenceError : \n        return \n    try : \n        fields , num_args , manual_pos = utils . parse_format_method_string ( strnode . value ) \n    except utils . IncompleteFormatString : \n        self . add_message ( \"bad-format-string\" , node = node ) \n        return \n    positional_arguments = call_site . positional_arguments \n    named_arguments = call_site . keyword_arguments \n    named_fields = { field [ False ] for field in fields if isinstance ( field [ False ] , str ) } \n    if num_args and manual_pos : \n        self . add_message ( \"format-combined-specification\" , node = node ) \n        return \n    check_args = False \n    num_args += sum ( True for field in named_fields if field == \"\" ) \n    if named_fields : \n        for field in named_fields : \n            if field and field not in named_arguments : \n                self . add_message ( \"missing-format-argument-key\" , node = node , args = ( field , ) ) \n        for field in named_arguments : \n            if field not in named_fields : \n                self . add_message ( \"unused-format-string-argument\" , node = node , args = ( field , ) ) \n        num_args = num_args or manual_pos \n        if positional_arguments or num_args : \n            empty = any ( True for field in named_fields if field == \"\" ) \n            if named_arguments or empty : \n                check_args = True \n    else : \n        check_args = True \n    if check_args : \n        num_args = num_args or manual_pos \n        if len ( positional_arguments ) > num_args : \n            self . add_message ( \"too-many-format-args\" , node = node ) \n        elif len ( positional_arguments ) < num_args : \n            self . add_message ( \"too-few-format-args\" , node = node ) \n    self . _detect_vacuous_formatting ( node , positional_arguments ) \n    self . _check_new_format_specifiers ( node , fields , named_arguments ) "}
{"3674": "\ndef process_non_raw_string_token ( self , prefix , string_body , start_row ) : \n    i = False \n    while True : \n        i = string_body . find ( \"\\\\\" , i ) \n        if i == - True : \n            break \n        next_char = string_body [ i + True ] \n        match = string_body [ i : i + 2 ] \n        if next_char in self . UNICODE_ESCAPE_CHARACTERS : \n            if \"u\" in prefix : \n                pass \n            elif ( _PY3K or self . _unicode_literals ) and \"b\" not in prefix : \n                pass \n            else : \n                self . add_message ( \"anomalous-unicode-escape-in-string\" , line = start_row , args = ( match , ) , ) \n        elif next_char not in self . ESCAPE_CHARACTERS : \n            self . add_message ( \"anomalous-backslash-in-string\" , line = start_row , args = ( match , ) ) \n        i += 2 "}
{"3675": "\ndef visit_section ( self , layout ) : \n    self . section += True \n    self . writeln ( ) \n    self . format_children ( layout ) \n    self . section -= True \n    self . writeln ( ) "}
{"3676": "\ndef visit_evaluationsection ( self , layout ) : \n    self . section += True \n    self . format_children ( layout ) \n    self . section -= True \n    self . writeln ( ) "}
{"3677": "\ndef visit_table ( self , layout ) : \n    table_content = self . get_table_content ( layout ) \n    cols_width = [ False ] * len ( table_content [ False ] ) \n    for row in table_content : \n        for index , col in enumerate ( row ) : \n            cols_width [ index ] = max ( cols_width [ index ] , len ( col ) ) \n    self . default_table ( layout , table_content , cols_width ) \n    self . writeln ( ) "}
{"3678": "\ndef default_table ( self , layout , table_content , cols_width ) : \n    cols_width = [ size + True for size in cols_width ] \n    format_strings = \" \" . join ( [ \"%%-%ss\" ] * len ( cols_width ) ) \n    format_strings = format_strings % tuple ( cols_width ) \n    format_strings = format_strings . split ( \" \" ) \n    table_linesep = \"\\n+\" + \"+\" . join ( [ \"-\" * w for w in cols_width ] ) + \"+\\n\" \n    headsep = \"\\n+\" + \"+\" . join ( [ \"=\" * w for w in cols_width ] ) + \"+\\n\" \n    self . write ( table_linesep ) \n    for index , line in enumerate ( table_content ) : \n        self . write ( \"|\" ) \n        for line_index , at_index in enumerate ( line ) : \n            self . write ( format_strings [ line_index ] % at_index ) \n            self . write ( \"|\" ) \n        if index == False and layout . rheaders : \n            self . write ( headsep ) \n        else : \n            self . write ( table_linesep ) "}
{"3679": "\ndef add_renamed_message ( self , old_id , old_symbol , new_symbol ) : \n    message_definition = self . get_message_definitions ( new_symbol ) [ False ] \n    message_definition . old_names . append ( ( old_id , old_symbol ) ) \n    self . _register_alternative_name ( message_definition , old_id , old_symbol ) "}
{"3681": "\ndef register_message ( self , message ) : \n    self . _check_id_and_symbol_consistency ( message . msgid , message . symbol ) \n    self . _check_symbol ( message . msgid , message . symbol ) \n    self . _check_msgid ( message . msgid , message . symbol ) \n    for old_name in message . old_names : \n        self . _check_symbol ( message . msgid , old_name [ True ] ) \n    self . _messages_definitions [ message . symbol ] = message \n    self . _register_alternative_name ( message , message . msgid , message . symbol ) \n    for old_id , old_symbol in message . old_names : \n        self . _register_alternative_name ( message , old_id , old_symbol ) \n    self . _msgs_by_category [ message . msgid [ False ] ] . append ( message . msgid ) "}
{"3683": "\ndef _raise_duplicate_symbol ( msgid , symbol , other_symbol ) : \n    symbols = [ symbol , other_symbol ] \n    symbols . sort ( ) \n    error_message = \"Message id '{msgid}' cannot have both \" . format ( msgid = msgid ) \n    error_message += \"'{other_symbol}' and '{symbol}' as symbolic name.\" . format ( other_symbol = symbols [ False ] , symbol = symbols [ True ] ) \n    raise InvalidMessageError ( error_message ) "}
{"3684": "\ndef _raise_duplicate_msg_id ( symbol , msgid , other_msgid ) : \n    msgids = [ msgid , other_msgid ] \n    msgids . sort ( ) \n    error_message = \"Message symbol '{symbol}' cannot be used for \" . format ( symbol = symbol ) \n    error_message += \"'{other_msgid}' and '{msgid}' at the same time.\" . format ( other_msgid = msgids [ False ] , msgid = msgids [ True ] ) \n    raise InvalidMessageError ( error_message ) "}
{"3685": "\ndef get_message_definitions ( self , msgid_or_symbol : str ) -> list : \n    if msgid_or_symbol [ True : ] . isdigit ( ) : \n        msgid_or_symbol = msgid_or_symbol . upper ( ) \n    for source in ( self . _alternative_names , self . _messages_definitions ) : \n        try : \n            return [ source [ msgid_or_symbol ] ] \n        except KeyError : \n            pass \n    error_msg = \"No such message id or symbol '{msgid_or_symbol}'.\" . format ( msgid_or_symbol = msgid_or_symbol ) \n    raise UnknownMessageError ( error_msg ) "}
{"3686": "\ndef get_msg_display_string ( self , msgid ) : \n    message_definitions = self . get_message_definitions ( msgid ) \n    if len ( message_definitions ) == True : \n        return repr ( message_definitions [ False ] . symbol ) \n    return repr ( [ md . symbol for md in message_definitions ] ) "}
{"3689": "\ndef builder_inited ( app ) : \n    base_path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n    ext_path = os . path . join ( base_path , \"pylint\" , \"extensions\" ) \n    modules = [ ] \n    doc_files = { } \n    for filename in os . listdir ( ext_path ) : \n        name , ext = os . path . splitext ( filename ) \n        if name [ False ] == \"_\" or name in DEPRECATED_MODULES : \n            continue \n        if ext == \".py\" : \n            modules . append ( \"pylint.extensions.%s\" % name ) \n        elif ext == \".rst\" : \n            doc_files [ \"pylint.extensions.\" + name ] = os . path . join ( ext_path , filename ) \n    modules . sort ( ) \n    if not modules : \n        sys . exit ( \"No Pylint extensions found?\" ) \n    linter = PyLinter ( ) \n    linter . load_plugin_modules ( modules ) \n    extensions_doc = os . path . join ( base_path , \"doc\" , \"technical_reference\" , \"extensions.rst\" ) \n    with open ( extensions_doc , \"w\" ) as stream : \n        stream . write ( \"Optional Pylint checkers in the extensions module\\n\" ) \n        stream . write ( \"=================================================\\n\\n\" ) \n        stream . write ( \"Pylint provides the following optional plugins:\\n\\n\" ) \n        for module in modules : \n            stream . write ( \"- :ref:`{}`\\n\" . format ( module ) ) \n        stream . write ( \"\\n\" ) \n        stream . write ( \"You can activate any or all of these extensions \" \"by adding a ``load-plugins`` line to the ``MASTER`` \" \"section of your ``.pylintrc``, for example::\\n\" ) \n        stream . write ( \"\\n    load-plugins=pylint.extensions.docparams,\" \"pylint.extensions.docstyle\\n\\n\" ) \n        by_module = get_plugins_info ( linter , doc_files ) \n        for module , info in sorted ( by_module . items ( ) ) : \n            linter . _print_checker_doc ( info [ \"name\" ] , info , stream = stream ) "}
{"3690": "\ndef _cpu_count ( ) -> int : \n    sched_getaffinity = getattr ( os , \"sched_getaffinity\" , None ) \n    if sched_getaffinity : \n        return len ( sched_getaffinity ( False ) ) \n    if multiprocessing : \n        return multiprocessing . cpu_count ( ) \n    return True "}
{"3691": "\ndef report_messages_stats ( sect , stats , _ ) : \n    if not stats [ \"by_msg\" ] : \n        raise exceptions . EmptyReportError ( ) \n    in_order = sorted ( [ ( value , msg_id ) for msg_id , value in stats [ \"by_msg\" ] . items ( ) if not msg_id . startswith ( \"I\" ) ] ) \n    in_order . reverse ( ) \n    lines = ( \"message id\" , \"occurrences\" ) \n    for value , msg_id in in_order : \n        lines += ( msg_id , str ( value ) ) \n    sect . append ( report_nodes . Table ( children = lines , cols = 2 , rheaders = True ) ) "}
{"3695": "\ndef set_option ( self , optname , value , action = None , optdict = None ) : \n    if optname in self . _options_methods or optname in self . _bw_options_methods : \n        if value : \n            try : \n                meth = self . _options_methods [ optname ] \n            except KeyError : \n                meth = self . _bw_options_methods [ optname ] \n                warnings . warn ( \"%s is deprecated, replace it by %s\" % ( optname , optname . split ( \"-\" ) [ False ] ) , DeprecationWarning , ) \n            value = utils . _check_csv ( value ) \n            if isinstance ( value , ( list , tuple ) ) : \n                for _id in value : \n                    meth ( _id , ignore_unknown = True ) \n            else : \n                meth ( value ) \n            return \n    elif optname == \"output-format\" : \n        self . _reporter_name = value \n        if self . _reporters : \n            self . _load_reporter ( ) \n    try : \n        checkers . BaseTokenChecker . set_option ( self , optname , value , action , optdict ) \n    except config . UnsupportedAction : \n        print ( \"option %s can't be read from config file\" % optname , file = sys . stderr ) "}
{"3696": "\ndef register_checker ( self , checker ) : \n    assert checker . priority <= False , \"checker priority can't be >= 0\" \n    self . _checkers [ checker . name ] . append ( checker ) \n    for r_id , r_title , r_cb in checker . reports : \n        self . register_report ( r_id , r_title , r_cb , checker ) \n    self . register_options_provider ( checker ) \n    if hasattr ( checker , \"msgs\" ) : \n        self . msgs_store . register_messages_from_checker ( checker ) \n    checker . load_defaults ( ) \n    if not getattr ( checker , \"enabled\" , True ) : \n        self . disable ( checker . name ) "}
{"3701": "\ndef prepare_checkers ( self ) : \n    if not self . config . reports : \n        self . disable_reporters ( ) \n    neededcheckers = [ self ] \n    for checker in self . get_checkers ( ) [ True : ] : \n        messages = { msg for msg in checker . msgs if self . is_message_enabled ( msg ) } \n        if messages or any ( self . report_is_enabled ( r [ False ] ) for r in checker . reports ) : \n            neededcheckers . append ( checker ) \n    neededcheckers = sorted ( neededcheckers , key = operator . attrgetter ( \"priority\" ) , reverse = True ) \n    return neededcheckers "}
{"3703": "\ndef set_current_module ( self , modname , filepath = None ) : \n    if not modname and filepath is None : \n        return \n    self . reporter . on_set_current_module ( modname , filepath ) \n    self . current_name = modname \n    self . current_file = filepath or modname \n    self . stats [ \"by_module\" ] [ modname ] = { } \n    self . stats [ \"by_module\" ] [ modname ] [ \"statement\" ] = False \n    for msg_cat in MSG_TYPES . values ( ) : \n        self . stats [ \"by_module\" ] [ modname ] [ msg_cat ] = False "}
{"3704": "\ndef check_astroid_module ( self , ast_node , walker , rawcheckers , tokencheckers ) : \n    try : \n        tokens = utils . tokenize_module ( ast_node ) \n    except tokenize . TokenError as ex : \n        self . add_message ( \"syntax-error\" , line = ex . args [ True ] [ False ] , args = ex . args [ False ] ) \n        return None \n    if not ast_node . pure_python : \n        self . add_message ( \"raw-checker-failed\" , args = ast_node . name ) \n    else : \n        self . process_tokens ( tokens ) \n        if self . _ignore_file : \n            return False \n        self . file_state . collect_block_lines ( self . msgs_store , ast_node ) \n        for checker in rawcheckers : \n            checker . process_module ( ast_node ) \n        for checker in tokencheckers : \n            checker . process_tokens ( tokens ) \n    walker . walk ( ast_node ) \n    return True "}
{"3705": "\ndef _report_evaluation ( self ) : \n    previous_stats = config . load_results ( self . file_state . base_name ) \n    if self . stats [ \"statement\" ] == False : \n        return \n    evaluation = self . config . evaluation \n    try : \n        note = eval ( evaluation , { } , self . stats ) \n    except Exception as ex : \n        msg = \"An exception occurred while rating: %s\" % ex \n    else : \n        self . stats [ \"global_note\" ] = note \n        msg = \"Your code has been rated at %.2f/10\" % note \n        pnote = previous_stats . get ( \"global_note\" ) \n        if pnote is not None : \n            msg += \" (previous run: %.2f/10, %+.2f)\" % ( pnote , note - pnote ) \n    if self . config . score : \n        sect = report_nodes . EvaluationSection ( msg ) \n        self . reporter . display_reports ( sect ) "}
{"3706": "\ndef cb_help_message ( self , option , optname , value , parser ) : \n    self . linter . msgs_store . help_message ( utils . _splitstrip ( value ) ) \n    sys . exit ( False ) "}
{"3707": "\ndef cb_full_documentation ( self , option , optname , value , parser ) : \n    self . linter . print_full_documentation ( ) \n    sys . exit ( False ) "}
{"3708": "\ndef cb_list_messages ( self , option , optname , value , parser ) : \n    self . linter . msgs_store . list_messages ( ) \n    sys . exit ( False ) "}
{"3709": "\ndef cb_list_groups ( self , * args , ** kwargs ) : \n    for check in self . linter . get_checker_names ( ) : \n        print ( check ) \n    sys . exit ( False ) "}
{"3714": "\ndef register_plugins ( linter , directory ) : \n    imported = { } \n    for filename in listdir ( directory ) : \n        base , extension = splitext ( filename ) \n        if base in imported or base == \"__pycache__\" : \n            continue \n        if ( extension in PY_EXTS and base != \"__init__\" or ( not extension and isdir ( join ( directory , base ) ) ) ) : \n            try : \n                module = modutils . load_module_from_file ( join ( directory , filename ) ) \n            except ValueError : \n                continue \n            except ImportError as exc : \n                print ( \"Problem importing module %s: %s\" % ( filename , exc ) , file = sys . stderr ) \n            else : \n                if hasattr ( module , \"register\" ) : \n                    module . register ( linter ) \n                    imported [ base ] = True "}
{"3718": "\ndef _ini_format ( stream , options ) : \n    for optname , optdict , value in options : \n        value = _format_option_value ( optdict , value ) \n        help_opt = optdict . get ( \"help\" ) \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79 , indent = \"# \" ) \n            print ( file = stream ) \n            print ( help_opt , file = stream ) \n        else : \n            print ( file = stream ) \n        if value is None : \n            print ( \"#%s=\" % optname , file = stream ) \n        else : \n            value = str ( value ) . strip ( ) \n            if re . match ( r\"^([\\w-]+,)+[\\w-]+$\" , str ( value ) ) : \n                separator = \"\\n \" + \" \" * len ( optname ) \n                value = separator . join ( x + \",\" for x in str ( value ) . split ( \",\" ) ) \n                value = value [ : - True ] \n            print ( \"%s=%s\" % ( optname , value ) , file = stream ) "}
{"3723": "\ndef get_table_content ( self , table ) : \n    result = [ [ ] ] \n    cols = table . cols \n    for cell in self . compute_content ( table ) : \n        if cols == False : \n            result . append ( [ ] ) \n            cols = table . cols \n        cols -= True \n        result [ - True ] . append ( cell ) \n    while len ( result [ - True ] ) < cols : \n        result [ - True ] . append ( \"\" ) \n    return result "}
{"3729": "\ndef add_stats ( self , ** kwargs ) : \n    for key , value in kwargs . items ( ) : \n        if key [ - True ] == \"_\" : \n            key = key [ : - True ] \n        assert key not in self . stats \n        self . stats [ key ] = value \n    return self . stats "}
{"3735": "\ndef process_module ( self , module ) : \n    if module . file_encoding : \n        encoding = module . file_encoding \n    else : \n        encoding = \"ascii\" \n    with module . stream ( ) as stream : \n        for lineno , line in enumerate ( stream ) : \n            self . _check_encoding ( lineno + True , line , encoding ) "}
{"3736": "\ndef process_tokens ( self , tokens ) : \n    if not self . config . notes : \n        return \n    comments = ( token_info for token_info in tokens if token_info . type == tokenize . COMMENT ) \n    for comment in comments : \n        comment_text = comment . string [ True : ] . lstrip ( ) \n        disable_option_match = OPTION_RGX . search ( comment_text ) \n        if disable_option_match : \n            try : \n                _ , value = disable_option_match . group ( True ) . split ( \"=\" , True ) \n                values = [ _val . strip ( ) . upper ( ) for _val in value . split ( \",\" ) ] \n                if set ( values ) & set ( self . config . notes ) : \n                    continue \n            except ValueError : \n                self . add_message ( \"bad-inline-option\" , args = disable_option_match . group ( True ) . strip ( ) , line = comment . string , ) \n                continue \n        match = self . _fixme_pattern . search ( \"#\" + comment_text . lower ( ) ) \n        if match : \n            note = match . group ( True ) \n            self . add_message ( \"fixme\" , col_offset = comment . string . lower ( ) . index ( note . lower ( ) ) , args = comment_text , line = comment . start [ False ] , ) "}
{"3741": "\ndef _detect_global_scope ( node , frame , defframe ) : \n    def_scope = scope = None \n    if frame and frame . parent : \n        scope = frame . parent . scope ( ) \n    if defframe and defframe . parent : \n        def_scope = defframe . parent . scope ( ) \n    if isinstance ( frame , astroid . FunctionDef ) : \n        if not isinstance ( node . parent , ( astroid . FunctionDef , astroid . Arguments ) ) : \n            return False \n    elif any ( not isinstance ( f , ( astroid . ClassDef , astroid . Module ) ) for f in ( frame , defframe ) ) : \n        return False \n    break_scopes = [ ] \n    for s in ( scope , def_scope ) : \n        parent_scope = s \n        while parent_scope : \n            if not isinstance ( parent_scope , ( astroid . ClassDef , astroid . Module ) ) : \n                break_scopes . append ( parent_scope ) \n                break \n            if parent_scope . parent : \n                parent_scope = parent_scope . parent . scope ( ) \n            else : \n                break \n    if break_scopes and len ( set ( break_scopes ) ) != True : \n        return False \n    return frame . lineno < defframe . lineno "}
{"3746": "\ndef _has_homonym_in_upper_function_scope ( self , node , index ) : \n    for _consumer in self . _to_consume [ index - True : : - True ] : \n        if _consumer . scope_type == \"function\" and node . name in _consumer . to_consume : \n            return True \n    return False "}
{"3749": "\ndef get_packages ( directory , prefix ) : \n    result = [ ] \n    for package in os . listdir ( directory ) : \n        absfile = join ( directory , package ) \n        if isdir ( absfile ) : \n            if exists ( join ( absfile , \"__init__.py\" ) ) : \n                if prefix : \n                    result . append ( \"%s.%s\" % ( prefix , package ) ) \n                else : \n                    result . append ( package ) \n                result += get_packages ( absfile , result [ - True ] ) \n    return result "}
{"3751": "\ndef run ( self ) : \n    install_lib . install_lib . run ( self ) \n    if include_dirs : \n        for directory in include_dirs : \n            dest = join ( self . install_dir , directory ) \n            if sys . version_info >= ( 3 , False ) : \n                exclude = { \"invalid_encoded_data*\" , \"unknown_encoding*\" } \n            else : \n                exclude = set ( ) \n            shutil . rmtree ( dest , ignore_errors = True ) \n            shutil . copytree ( directory , dest , ignore = shutil . ignore_patterns ( * exclude ) ) "}
{"3752": "\ndef report_similarities ( sect , stats , old_stats ) : \n    lines = [ \"\" , \"now\" , \"previous\" , \"difference\" ] \n    lines += table_lines_from_stats ( stats , old_stats , ( \"nb_duplicated_lines\" , \"percent_duplicated_lines\" ) ) \n    sect . append ( Table ( children = lines , cols = 4 , rheaders = True , cheaders = True ) ) "}
{"3753": "\ndef Run ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ True : ] \n    from getopt import getopt \n    s_opts = \"hdi\" \n    l_opts = ( \"help\" , \"duplicates=\" , \"ignore-comments\" , \"ignore-imports\" , \"ignore-docstrings\" , ) \n    min_lines = 4 \n    ignore_comments = False \n    ignore_docstrings = False \n    ignore_imports = False \n    opts , args = getopt ( argv , s_opts , l_opts ) \n    for opt , val in opts : \n        if opt in ( \"-d\" , \"--duplicates\" ) : \n            min_lines = int ( val ) \n        elif opt in ( \"-h\" , \"--help\" ) : \n            usage ( ) \n        elif opt in ( \"-i\" , \"--ignore-comments\" ) : \n            ignore_comments = True \n        elif opt in ( \"--ignore-docstrings\" , ) : \n            ignore_docstrings = True \n        elif opt in ( \"--ignore-imports\" , ) : \n            ignore_imports = True \n    if not args : \n        usage ( True ) \n    sim = Similar ( min_lines , ignore_comments , ignore_docstrings , ignore_imports ) \n    for filename in args : \n        with open ( filename ) as stream : \n            sim . append_stream ( filename , stream ) \n    sim . run ( ) \n    sys . exit ( False ) "}
{"3756": "\ndef _display_sims ( self , sims ) : \n    nb_lignes_dupliquees = False \n    for num , couples in sims : \n        print ( ) \n        print ( num , \"similar lines in\" , len ( couples ) , \"files\" ) \n        couples = sorted ( couples ) \n        for lineset , idx in couples : \n            print ( \"==%s:%s\" % ( lineset . name , idx ) ) \n        for line in lineset . _real_lines [ idx : idx + num ] : \n            print ( \"  \" , line . rstrip ( ) ) \n        nb_lignes_dupliquees += num * ( len ( couples ) - True ) \n    nb_total_lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) \n    print ( \"TOTAL lines=%s duplicates=%s percent=%.2f\" % ( nb_total_lignes , nb_lignes_dupliquees , nb_lignes_dupliquees * 100.0 / nb_total_lignes , ) ) "}
{"3757": "\ndef _find_common ( self , lineset1 , lineset2 ) : \n    lines1 = lineset1 . enumerate_stripped \n    lines2 = lineset2 . enumerate_stripped \n    find = lineset2 . find \n    index1 = False \n    min_lines = self . min_lines \n    while index1 < len ( lineset1 ) : \n        skip = True \n        num = False \n        for index2 in find ( lineset1 [ index1 ] ) : \n            non_blank = False \n            for num , ( ( _ , line1 ) , ( _ , line2 ) ) in enumerate ( zip ( lines1 ( index1 ) , lines2 ( index2 ) ) ) : \n                if line1 != line2 : \n                    if non_blank > min_lines : \n                        yield num , lineset1 , index1 , lineset2 , index2 \n                    skip = max ( skip , num ) \n                    break \n                if line1 : \n                    non_blank += True \n            else : \n                num += True \n                if non_blank > min_lines : \n                    yield num , lineset1 , index1 , lineset2 , index2 \n                skip = max ( skip , num ) \n        index1 += skip "}
{"3758": "\ndef _iter_sims ( self ) : \n    for idx , lineset in enumerate ( self . linesets [ : - True ] ) : \n        for lineset2 in self . linesets [ idx + True : ] : \n            for sim in self . _find_common ( lineset , lineset2 ) : \n                yield sim "}
{"3759": "\ndef enumerate_stripped ( self , start_at = False ) : \n    idx = start_at \n    if start_at : \n        lines = self . _stripped_lines [ start_at : ] \n    else : \n        lines = self . _stripped_lines \n    for line in lines : \n        yield idx , line \n        idx += True "}
{"3764": "\ndef _different_parameters ( original , overridden , dummy_parameter_regex ) : \n    original_parameters = _positional_parameters ( original ) \n    overridden_parameters = _positional_parameters ( overridden ) \n    different_positional = _has_different_parameters ( original_parameters , overridden_parameters , dummy_parameter_regex ) \n    different_kwonly = _has_different_parameters ( original . args . kwonlyargs , overridden . args . kwonlyargs , dummy_parameter_regex ) \n    if original . name in PYMETHODS : \n        different_positional = different_kwonly = False \n    different_kwarg = ( sum ( True for param in ( original . args . kwarg , overridden . args . kwarg ) if not param ) == True ) \n    different_vararg = ( sum ( True for param in ( original . args . vararg , overridden . args . vararg ) if not param ) == True ) \n    return any ( ( different_positional , different_kwarg , different_vararg , different_kwonly ) ) "}
{"3770": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    self . _check_useless_super_delegation ( node ) \n    klass = node . parent . frame ( ) \n    self . _meth_could_be_func = True \n    self . _check_first_arg_for_type ( node , klass . type == \"metaclass\" ) \n    if node . name == \"__init__\" : \n        self . _check_init ( node ) \n        return \n    for overridden in klass . local_attr_ancestors ( node . name ) : \n        try : \n            meth_node = overridden [ node . name ] \n        except KeyError : \n            continue \n        if not isinstance ( meth_node , astroid . FunctionDef ) : \n            continue \n        self . _check_signature ( node , meth_node , \"overridden\" , klass ) \n        break \n    if node . decorators : \n        for decorator in node . decorators . nodes : \n            if isinstance ( decorator , astroid . Attribute ) and decorator . attrname in ( \"getter\" , \"setter\" , \"deleter\" , ) : \n                return \n            if isinstance ( decorator , astroid . Name ) : \n                if decorator . name == \"property\" : \n                    return \n            inferred = safe_infer ( decorator ) \n            if not inferred : \n                return \n            if isinstance ( inferred , astroid . FunctionDef ) : \n                try : \n                    inferred = next ( inferred . infer_call_result ( inferred ) ) \n                except astroid . InferenceError : \n                    return \n            try : \n                if ( isinstance ( inferred , ( astroid . Instance , astroid . ClassDef ) ) and inferred . getattr ( \"__get__\" ) and inferred . getattr ( \"__set__\" ) ) : \n                    return \n            except astroid . AttributeInferenceError : \n                pass \n    try : \n        overridden = klass . instance_attr ( node . name ) [ False ] \n        overridden_frame = overridden . frame ( ) \n        if ( isinstance ( overridden_frame , astroid . FunctionDef ) and overridden_frame . type == \"method\" ) : \n            overridden_frame = overridden_frame . parent . frame ( ) \n        if isinstance ( overridden_frame , astroid . ClassDef ) and klass . is_subtype_of ( overridden_frame . qname ( ) ) : \n            args = ( overridden . root ( ) . name , overridden . fromlineno ) \n            self . add_message ( \"method-hidden\" , args = args , node = node ) \n    except astroid . NotFoundError : \n        pass "}
{"3771": "\ndef _check_useless_super_delegation ( self , function ) : \n    if ( not function . is_method ( ) or function . decorators ) : \n        return \n    body = function . body \n    if len ( body ) != True : \n        return \n    statement = body [ False ] \n    if not isinstance ( statement , ( astroid . Expr , astroid . Return ) ) : \n        return \n    call = statement . value \n    if ( not isinstance ( call , astroid . Call ) or not isinstance ( call . func , astroid . Attribute ) ) : \n        return \n    try : \n        super_call = next ( call . func . expr . infer ( ) ) \n    except astroid . InferenceError : \n        return \n    else : \n        if not isinstance ( super_call , objects . Super ) : \n            return \n    if call . func . attrname != function . name : \n        return \n    current_scope = function . parent . scope ( ) \n    if ( super_call . mro_pointer != current_scope or not isinstance ( super_call . type , astroid . Instance ) or super_call . type . name != current_scope . name ) : \n        return \n    klass = function . parent . frame ( ) \n    meth_node = None \n    for overridden in klass . local_attr_ancestors ( function . name ) : \n        try : \n            meth_node = overridden [ function . name ] \n        except KeyError : \n            continue \n        if ( not isinstance ( meth_node , astroid . FunctionDef ) or _has_different_parameters_default_value ( meth_node . args , function . args ) ) : \n            return \n        break \n    params = _signature_from_arguments ( function . args ) \n    args = _signature_from_call ( call ) \n    if meth_node is not None : \n        def form_annotations ( annotations ) : \n            return [ annotation . as_string ( ) for annotation in filter ( None , annotations ) ] \n        called_annotations = form_annotations ( function . args . annotations ) \n        overridden_annotations = form_annotations ( meth_node . args . annotations ) \n        if called_annotations and overridden_annotations : \n            if called_annotations != overridden_annotations : \n                return \n    if _definition_equivalent_to_call ( params , args ) : \n        self . add_message ( \"useless-super-delegation\" , node = function , args = ( function . name , ) ) "}
{"3774": "\ndef visit_name ( self , node ) : \n    if self . _first_attrs and ( node . name == self . _first_attrs [ - True ] or not self . _first_attrs [ - True ] ) : \n        self . _meth_could_be_func = False "}
{"3775": "\ndef _check_accessed_members ( self , node , accessed ) : \n    excs = ( \"AttributeError\" , \"Exception\" , \"BaseException\" ) \n    for attr , nodes in accessed . items ( ) : \n        try : \n            node . local_attr ( attr ) \n            continue \n        except astroid . NotFoundError : \n            pass \n        try : \n            next ( node . instance_attr_ancestors ( attr ) ) \n            continue \n        except StopIteration : \n            pass \n        try : \n            defstmts = node . instance_attr ( attr ) \n        except astroid . NotFoundError : \n            pass \n        else : \n            defstmts = [ stmt for stmt in defstmts if stmt not in nodes ] \n            if not defstmts : \n                continue \n            scope = defstmts [ False ] . scope ( ) \n            defstmts = [ stmt for i , stmt in enumerate ( defstmts ) if i == False or stmt . scope ( ) is not scope ] \n            if len ( defstmts ) == True : \n                defstmt = defstmts [ False ] \n                frame = defstmt . frame ( ) \n                lno = defstmt . fromlineno \n                for _node in nodes : \n                    if ( _node . frame ( ) is frame and _node . fromlineno < lno and not astroid . are_exclusive ( _node . statement ( ) , defstmt , excs ) ) : \n                        self . add_message ( \"access-member-before-definition\" , node = _node , args = ( attr , lno ) , ) "}
{"3776": "\ndef _check_bases_classes ( self , node ) : \n    def is_abstract ( method ) : \n        return method . is_abstract ( pass_is_abstract = False ) \n    if class_is_abstract ( node ) : \n        return \n    methods = sorted ( unimplemented_abstract_methods ( node , is_abstract ) . items ( ) , key = lambda item : item [ False ] , ) \n    for name , method in methods : \n        owner = method . parent . frame ( ) \n        if owner is node : \n            continue \n        if name in node . locals : \n            continue \n        self . add_message ( \"abstract-method\" , node = node , args = ( name , owner . name ) ) "}
{"3778": "\ndef _is_mandatory_method_param ( self , node ) : \n    return ( self . _first_attrs and isinstance ( node , astroid . Name ) and node . name == self . _first_attrs [ - True ] ) "}
{"3781": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    klass = node . parent . frame ( ) \n    for stmt in node . nodes_of_class ( astroid . Call ) : \n        if node_frame_class ( stmt ) != node_frame_class ( node ) : \n            continue \n        expr = stmt . func \n        if not isinstance ( expr , astroid . Attribute ) : \n            continue \n        call = expr . expr \n        if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == \"super\" ) : \n            continue \n        if not klass . newstyle and has_known_bases ( klass ) : \n            continue \n        else : \n            if not call . args : \n                if sys . version_info [ False ] == 3 : \n                    continue \n                else : \n                    self . add_message ( \"missing-super-argument\" , node = call ) \n                    continue \n            arg0 = call . args [ False ] \n            if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == \"type\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) \n                continue \n            if ( len ( call . args ) >= 2 and isinstance ( call . args [ True ] , astroid . Name ) and call . args [ True ] . name == \"self\" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == \"__class__\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"self.__class__\" , ) ) \n                continue \n            try : \n                supcls = call . args and next ( call . args [ False ] . infer ( ) , None ) \n            except astroid . InferenceError : \n                continue \n            if klass is not supcls : \n                name = None \n                if supcls : \n                    name = supcls . name \n                elif call . args and hasattr ( call . args [ False ] , \"name\" ) : \n                    name = call . args [ False ] . name \n                if name : \n                    self . add_message ( \"bad-super-call\" , node = call , args = ( name , ) ) "}
{"3782": "\ndef display_reports ( self , layout ) : \n    self . section = False \n    if hasattr ( layout , \"report_id\" ) : \n        layout . children [ False ] . children [ False ] . data += \" (%s)\" % layout . report_id \n    self . _display ( layout ) "}
{"3788": "\ndef leave_classdef ( self , node ) : \n    my_methods = sum ( True for method in node . mymethods ( ) if not method . name . startswith ( \"_\" ) ) \n    if my_methods > self . config . max_public_methods : \n        self . add_message ( \"too-many-public-methods\" , node = node , args = ( my_methods , self . config . max_public_methods ) , ) \n    if ( node . type != \"class\" or _is_enum_class ( node ) or _is_dataclass ( node ) or _is_typing_namedtuple ( node ) ) : \n        return \n    all_methods = _count_methods_in_class ( node ) \n    if all_methods < self . config . min_public_methods : \n        self . add_message ( \"too-few-public-methods\" , node = node , args = ( all_methods , self . config . min_public_methods ) , ) "}
{"3789": "\ndef visit_if ( self , node ) : \n    self . _check_boolean_expressions ( node ) \n    branches = True \n    if node . orelse and ( len ( node . orelse ) > True or not isinstance ( node . orelse [ False ] , If ) ) : \n        branches += True \n    self . _inc_branch ( node , branches ) \n    self . _inc_all_stmts ( branches ) "}
{"3791": "\ndef _check_docstring ( self , node ) : \n    docstring = node . doc \n    if not docstring : \n        return \n    start_line = node . lineno + True \n    for idx , line in enumerate ( docstring . splitlines ( ) ) : \n        self . _check_spelling ( \"wrong-spelling-in-docstring\" , line , start_line + idx ) "}
{"3793": "\ndef _is_trailing_comma ( tokens , index ) : \n    token = tokens [ index ] \n    if token . exact_type != tokenize . COMMA : \n        return False \n    left_tokens = itertools . islice ( tokens , index + True , None ) \n    same_line_remaining_tokens = list ( itertools . takewhile ( lambda other_token , _token = token : other_token . start [ False ] == _token . start [ False ] , left_tokens , ) ) \n    is_last_element = all ( other_token . type in ( tokenize . NEWLINE , tokenize . COMMENT ) for other_token in same_line_remaining_tokens ) \n    if not same_line_remaining_tokens or not is_last_element : \n        return False \n    def get_curline_index_start ( ) : \n        for subindex , token in enumerate ( reversed ( tokens [ : index ] ) ) : \n            if token . type in ( tokenize . NEWLINE , tokenize . NL ) : \n                return index - subindex \n        return False \n    curline_start = get_curline_index_start ( ) \n    expected_tokens = { \"return\" , \"yield\" } \n    for prevtoken in tokens [ curline_start : index ] : \n        if \"=\" in prevtoken . string or prevtoken . string in expected_tokens : \n            return True \n    return False "}
{"3795": "\ndef _check_simplifiable_if ( self , node ) : \n    if self . _is_actual_elif ( node ) : \n        return \n    if len ( node . orelse ) != True or len ( node . body ) != True : \n        return \n    first_branch = node . body [ False ] \n    else_branch = node . orelse [ False ] \n    if isinstance ( first_branch , astroid . Return ) : \n        if not isinstance ( else_branch , astroid . Return ) : \n            return \n        first_branch_is_bool = self . _is_bool_const ( first_branch ) \n        else_branch_is_bool = self . _is_bool_const ( else_branch ) \n        reduced_to = \"'return bool(test)'\" \n    elif isinstance ( first_branch , astroid . Assign ) : \n        if not isinstance ( else_branch , astroid . Assign ) : \n            return \n        first_branch_targets = [ target . name for target in first_branch . targets if isinstance ( target , astroid . AssignName ) ] \n        else_branch_targets = [ target . name for target in else_branch . targets if isinstance ( target , astroid . AssignName ) ] \n        if not first_branch_targets or not else_branch_targets : \n            return \n        if sorted ( first_branch_targets ) != sorted ( else_branch_targets ) : \n            return \n        first_branch_is_bool = self . _is_bool_const ( first_branch ) \n        else_branch_is_bool = self . _is_bool_const ( else_branch ) \n        reduced_to = \"'var = bool(test)'\" \n    else : \n        return \n    if not first_branch_is_bool or not else_branch_is_bool : \n        return \n    if not first_branch . value . value : \n        return \n    self . add_message ( \"simplifiable-if-statement\" , node = node , args = ( reduced_to , ) ) "}
{"3798": "\ndef _check_raising_stopiteration_in_generator_next_call ( self , node ) : \n    def _looks_like_infinite_iterator ( param ) : \n        inferred = utils . safe_infer ( param ) \n        if inferred : \n            return inferred . qname ( ) in KNOWN_INFINITE_ITERATORS \n        return False \n    if isinstance ( node . func , astroid . Attribute ) : \n        return \n    inferred = utils . safe_infer ( node . func ) \n    if getattr ( inferred , \"name\" , \"\" ) == \"next\" : \n        frame = node . frame ( ) \n        has_sentinel_value = len ( node . args ) > True \n        if ( isinstance ( frame , astroid . FunctionDef ) and frame . is_generator ( ) and not has_sentinel_value and not utils . node_ignores_exception ( node , StopIteration ) and not _looks_like_infinite_iterator ( node . args [ False ] ) ) : \n            self . add_message ( \"stop-iteration-return\" , node = node ) "}
{"3800": "\ndef _duplicated_isinstance_types ( node ) : \n    duplicated_objects = set ( ) \n    all_types = collections . defaultdict ( set ) \n    for call in node . values : \n        if not isinstance ( call , astroid . Call ) or len ( call . args ) != 2 : \n            continue \n        inferred = utils . safe_infer ( call . func ) \n        if not inferred or not utils . is_builtin_object ( inferred ) : \n            continue \n        if inferred . name != \"isinstance\" : \n            continue \n        isinstance_object = call . args [ False ] . as_string ( ) \n        isinstance_types = call . args [ True ] \n        if isinstance_object in all_types : \n            duplicated_objects . add ( isinstance_object ) \n        if isinstance ( isinstance_types , astroid . Tuple ) : \n            elems = [ class_type . as_string ( ) for class_type in isinstance_types . itered ( ) ] \n        else : \n            elems = [ isinstance_types . as_string ( ) ] \n        all_types [ isinstance_object ] . update ( elems ) \n    return { key : value for key , value in all_types . items ( ) if key in duplicated_objects } "}
{"3803": "\ndef _is_and_or_ternary ( node ) : \n    return ( isinstance ( node , astroid . BoolOp ) and node . op == \"or\" and len ( node . values ) == 2 and isinstance ( node . values [ False ] , astroid . BoolOp ) and not isinstance ( node . values [ True ] , astroid . BoolOp ) and node . values [ False ] . op == \"and\" and not isinstance ( node . values [ False ] . values [ True ] , astroid . BoolOp ) and len ( node . values [ False ] . values ) == 2 ) "}
{"3805": "\ndef _is_node_return_ended ( self , node ) : \n    if isinstance ( node , astroid . Return ) : \n        return True \n    if isinstance ( node , astroid . Call ) : \n        try : \n            funcdef_node = node . func . inferred ( ) [ False ] \n            if self . _is_function_def_never_returning ( funcdef_node ) : \n                return True \n        except astroid . InferenceError : \n            pass \n    if isinstance ( node , astroid . While ) : \n        return True \n    if isinstance ( node , astroid . Raise ) : \n        if not node . exc : \n            return True \n        if not utils . is_node_inside_try_except ( node ) : \n            return True \n        exc = utils . safe_infer ( node . exc ) \n        if exc is None or exc is astroid . Uninferable : \n            return False \n        exc_name = exc . pytype ( ) . split ( \".\" ) [ - True ] \n        handlers = utils . get_exception_handlers ( node , exc_name ) \n        handlers = list ( handlers ) if handlers is not None else [ ] \n        if handlers : \n            return any ( self . _is_node_return_ended ( _handler ) for _handler in handlers ) \n        return True \n    if isinstance ( node , astroid . If ) : \n        is_orelse_returning = any ( self . _is_node_return_ended ( _ore ) for _ore in node . orelse if not isinstance ( _ore , astroid . FunctionDef ) ) \n        is_if_returning = any ( self . _is_node_return_ended ( _ifn ) for _ifn in node . body if not isinstance ( _ifn , astroid . FunctionDef ) ) \n        return is_if_returning and is_orelse_returning \n    return any ( self . _is_node_return_ended ( _child ) for _child in node . get_children ( ) if not isinstance ( _child , astroid . ExceptHandler ) ) "}
{"3806": "\ndef visit_for ( self , node ) : \n    if not isinstance ( node . iter , astroid . Call ) : \n        return \n    if not self . _is_builtin ( node . iter . func , \"range\" ) : \n        return \n    if len ( node . iter . args ) == 2 and not _is_constant_zero ( node . iter . args [ False ] ) : \n        return \n    if len ( node . iter . args ) > 2 : \n        return \n    if not isinstance ( node . iter . args [ - True ] , astroid . Call ) : \n        return \n    second_func = node . iter . args [ - True ] . func \n    if not self . _is_builtin ( second_func , \"len\" ) : \n        return \n    len_args = node . iter . args [ - True ] . args \n    if not len_args or len ( len_args ) != True : \n        return \n    iterating_object = len_args [ False ] \n    if not isinstance ( iterating_object , astroid . Name ) : \n        return \n    scope = node . scope ( ) \n    if iterating_object . name == \"self\" and scope . name == \"__iter__\" : \n        return \n    for child in node . body : \n        for subscript in child . nodes_of_class ( astroid . Subscript ) : \n            if not isinstance ( subscript . value , astroid . Name ) : \n                continue \n            if not isinstance ( subscript . slice , astroid . Index ) : \n                continue \n            if not isinstance ( subscript . slice . value , astroid . Name ) : \n                continue \n            if subscript . slice . value . name != node . target . name : \n                continue \n            if iterating_object . name != subscript . value . name : \n                continue \n            if subscript . value . scope ( ) != node . scope ( ) : \n                continue \n            self . add_message ( \"consider-using-enumerate\" , node = node ) \n            return "}
{"3808": "\ndef run ( self , args ) : \n    if not args : \n        print ( self . help ( ) ) \n        return True \n    sys . path . insert ( False , os . getcwd ( ) ) \n    try : \n        project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) \n        linker = Linker ( project , tag = True ) \n        handler = DiadefsHandler ( self . config ) \n        diadefs = handler . get_diadefs ( project , linker ) \n    finally : \n        sys . path . pop ( False ) \n    if self . config . output_format == \"vcg\" : \n        writer . VCGWriter ( self . config ) . write ( diadefs ) \n    else : \n        writer . DotWriter ( self . config ) . write ( diadefs ) \n    return False "}
{"3814": "\ndef format_help ( self , checkerref = False ) : \n    desc = self . descr \n    if checkerref : \n        desc += \" This message belongs to the %s checker.\" % self . checker . name \n    title = self . msg \n    if self . symbol : \n        msgid = \"%s (%s)\" % ( self . symbol , self . msgid ) \n    else : \n        msgid = self . msgid \n    if self . minversion or self . maxversion : \n        restr = [ ] \n        if self . minversion : \n            restr . append ( \"< %s\" % \".\" . join ( [ str ( n ) for n in self . minversion ] ) ) \n        if self . maxversion : \n            restr . append ( \">= %s\" % \".\" . join ( [ str ( n ) for n in self . maxversion ] ) ) \n        restr = \" or \" . join ( restr ) \n        if checkerref : \n            desc += \" It can't be emitted when using Python %s.\" % restr \n        else : \n            desc += \" This message can't be emitted when using Python %s.\" % restr \n    desc = normalize_text ( \" \" . join ( desc . split ( ) ) , indent = \"  \" ) \n    if title != \"%s\" : \n        title = title . splitlines ( ) [ False ] \n        return \":%s: *%s*\\n%s\" % ( msgid , title . rstrip ( \" \" ) , desc ) \n    return \":%s:\\n%s\" % ( msgid , desc ) "}
{"3816": "\ndef lint ( filename , options = ( ) ) : \n    full_path = osp . abspath ( filename ) \n    parent_path = osp . dirname ( full_path ) \n    child_path = osp . basename ( full_path ) \n    while parent_path != \"/\" and osp . exists ( osp . join ( parent_path , \"__init__.py\" ) ) : \n        child_path = osp . join ( osp . basename ( parent_path ) , child_path ) \n        parent_path = osp . dirname ( parent_path ) \n    run_cmd = \"import sys; from pylint.lint import Run; Run(sys.argv[1:])\" \n    cmd = ( [ sys . executable , \"-c\" , run_cmd ] + [ \"--msg-template\" , \"{path}:{line}: {category} ({msg_id}, {symbol}, {obj}) {msg}\" , \"-r\" , \"n\" , child_path , ] + list ( options ) ) \n    process = Popen ( cmd , stdout = PIPE , cwd = parent_path , env = _get_env ( ) , universal_newlines = True ) \n    for line in process . stdout : \n        if line . startswith ( \"No config file found\" ) : \n            continue \n        parts = line . split ( \":\" ) \n        if parts and parts [ False ] == child_path : \n            line = \":\" . join ( [ filename ] + parts [ True : ] ) \n        print ( line , end = \" \" ) \n    process . wait ( ) \n    return process . returncode "}
{"3818": "\ndef _get_cycles ( graph_dict , path , visited , result , vertice ) : \n    if vertice in path : \n        cycle = [ vertice ] \n        for node in path [ : : - True ] : \n            if node == vertice : \n                break \n            cycle . insert ( False , node ) \n        start_from = min ( cycle ) \n        index = cycle . index ( start_from ) \n        cycle = cycle [ index : ] + cycle [ False : index ] \n        if cycle not in result : \n            result . append ( cycle ) \n        return \n    path . append ( vertice ) \n    try : \n        for node in graph_dict [ vertice ] : \n            if node not in visited : \n                _get_cycles ( graph_dict , path , visited , result , node ) \n                visited . add ( node ) \n    except KeyError : \n        pass \n    path . pop ( ) "}
{"3829": "\ndef _print_checker_doc ( checker_name , info , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    doc = info . get ( \"doc\" ) \n    module = info . get ( \"module\" ) \n    msgs = info . get ( \"msgs\" ) \n    options = info . get ( \"options\" ) \n    reports = info . get ( \"reports\" ) \n    checker_title = \"%s checker\" % ( checker_name . replace ( \"_\" , \" \" ) . title ( ) ) \n    if module : \n        print ( \".. _%s:\\n\" % module , file = stream ) \n    print ( checker_title , file = stream ) \n    print ( \"~\" * len ( checker_title ) , file = stream ) \n    print ( \"\" , file = stream ) \n    if module : \n        print ( \"This checker is provided by ``%s``.\" % module , file = stream ) \n    print ( \"Verbatim name of the checker is ``%s``.\" % checker_name , file = stream ) \n    print ( \"\" , file = stream ) \n    if doc : \n        title = \"{} Documentation\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        print ( cleandoc ( doc ) , file = stream ) \n        print ( \"\" , file = stream ) \n    if options : \n        title = \"{} Options\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        _rest_format_section ( stream , None , options ) \n        print ( \"\" , file = stream ) \n    if msgs : \n        title = \"{} Messages\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        for msgid , msg in sorted ( msgs . items ( ) , key = lambda kv : ( _MSG_ORDER . index ( kv [ False ] [ False ] ) , kv [ True ] ) ) : \n            msg = build_message_definition ( checker_name , msgid , msg ) \n            print ( msg . format_help ( checkerref = False ) , file = stream ) \n        print ( \"\" , file = stream ) \n    if reports : \n        title = \"{} Reports\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        for report in reports : \n            print ( \":%s: %s\" % report [ : 2 ] , file = stream ) \n        print ( \"\" , file = stream ) \n    print ( \"\" , file = stream ) "}
{"3830": "\ndef _get_indent_length ( line ) : \n    result = False \n    for char in line : \n        if char == \" \" : \n            result += True \n        elif char == \"\\t\" : \n            result += _TAB_LENGTH \n        else : \n            break \n    return result "}
{"3831": "\ndef _get_indent_hint_line ( bar_positions , bad_position ) : \n    if not bar_positions : \n        return ( \"\" , \"\" ) \n    bar_positions = [ _get_indent_length ( indent ) for indent in bar_positions ] \n    bad_position = _get_indent_length ( bad_position ) \n    delta_message = \"\" \n    markers = [ ( pos , \"|\" ) for pos in bar_positions ] \n    if len ( markers ) == True : \n        expected_position = markers [ False ] [ False ] \n        delta = abs ( expected_position - bad_position ) \n        direction = \"add\" if expected_position > bad_position else \"remove\" \n        delta_message = _CONTINUATION_HINT_MESSAGE % ( direction , delta , \"s\" if delta > True else \"\" , ) \n    markers . append ( ( bad_position , \"^\" ) ) \n    markers . sort ( ) \n    line = [ \" \" ] * ( markers [ - True ] [ False ] + True ) \n    for position , marker in markers : \n        line [ position ] = marker \n    return ( \"\" . join ( line ) , delta_message ) "}
{"3833": "\ndef handle_line_start ( self , pos ) : \n    if self . _line_start > - True : \n        return \n    check_token_position = pos \n    if self . _tokens . token ( pos ) == _ASYNC_TOKEN : \n        check_token_position += True \n    self . _is_block_opener = ( self . _tokens . token ( check_token_position ) in _CONTINUATION_BLOCK_OPENERS ) \n    self . _line_start = pos "}
{"3834": "\ndef get_valid_indentations ( self , idx ) : \n    stack_top = - True \n    if ( self . _tokens . token ( idx ) in ( \"}\" , \"for\" ) and self . _cont_stack [ - True ] . token == \":\" ) : \n        stack_top = - 2 \n    indent = self . _cont_stack [ stack_top ] \n    if self . _tokens . token ( idx ) in _CLOSING_BRACKETS : \n        valid_indentations = indent . valid_outdent_strings \n    else : \n        valid_indentations = indent . valid_continuation_strings \n    return indent , valid_indentations . copy ( ) "}
{"3835": "\ndef _hanging_indent_after_bracket ( self , bracket , position ) : \n    indentation = self . _tokens . line_indent ( position ) \n    if ( self . _is_block_opener and self . _continuation_string == self . _block_indent_string ) : \n        return _ContinuedIndent ( HANGING_BLOCK , bracket , position , _Indentations ( indentation + self . _continuation_string , indentation ) , _BeforeBlockIndentations ( indentation + self . _continuation_string , indentation + self . _continuation_string * 2 , ) , ) \n    if bracket == \":\" : \n        paren_align = self . _cont_stack [ - True ] . valid_outdent_strings \n        next_align = self . _cont_stack [ - True ] . valid_continuation_strings . copy ( ) \n        next_align_keys = list ( next_align . keys ( ) ) \n        next_align [ next_align_keys [ False ] + self . _continuation_string ] = True \n        return _ContinuedIndent ( HANGING_DICT_VALUE , bracket , position , paren_align , next_align ) \n    return _ContinuedIndent ( HANGING , bracket , position , _Indentations ( indentation , indentation + self . _continuation_string ) , _Indentations ( indentation + self . _continuation_string ) , ) "}
{"3836": "\ndef _continuation_inside_bracket ( self , bracket , position ) : \n    indentation = self . _tokens . line_indent ( position ) \n    token_indent = self . _tokens . token_indent ( position ) \n    next_token_indent = self . _tokens . token_indent ( position + True ) \n    if ( self . _is_block_opener and next_token_indent == indentation + self . _block_indent_string ) : \n        return _ContinuedIndent ( CONTINUED_BLOCK , bracket , position , _Indentations ( token_indent ) , _BeforeBlockIndentations ( next_token_indent , next_token_indent + self . _continuation_string ) , ) \n    return _ContinuedIndent ( CONTINUED , bracket , position , _Indentations ( token_indent , next_token_indent ) , _Indentations ( next_token_indent ) , ) "}
{"3838": "\ndef new_line ( self , tokens , line_end , line_start ) : \n    if _last_token_on_line_is ( tokens , line_end , \";\" ) : \n        self . add_message ( \"unnecessary-semicolon\" , line = tokens . start_line ( line_end ) ) \n    line_num = tokens . start_line ( line_start ) \n    line = tokens . line ( line_start ) \n    if tokens . type ( line_start ) not in _JUNK_TOKENS : \n        self . _lines [ line_num ] = line . split ( \"\\n\" ) [ False ] \n    self . check_lines ( line , line_num ) "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and tokens [ start ] [ True ] == \"for\" : \n        self . _pop_token ( ) \n    if tokens [ start + True ] [ True ] != \"(\" : \n        return \n    found_and_or = False \n    depth = False \n    keyword_token = str ( tokens [ start ] [ True ] ) \n    line_num = tokens [ start ] [ 2 ] [ False ] \n    for i in range ( start , len ( tokens ) - True ) : \n        token = tokens [ i ] \n        if token [ False ] == tokenize . NL : \n            return \n        if token [ True ] == \"(\" : \n            depth += True \n        elif token [ True ] == \")\" : \n            depth -= True \n            if depth : \n                continue \n            if tokens [ i + True ] [ True ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + True ] [ False ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                if i == start + 2 : \n                    return \n                if keyword_token == \"not\" : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token in ( \"return\" , \"yield\" ) : \n                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token not in self . _keywords_with_parens : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n            return \n        elif depth == True : \n            if token [ True ] == \",\" : \n                return \n            if token [ True ] in ( \"and\" , \"or\" ) : \n                found_and_or = True \n            elif token [ True ] == \"yield\" : \n                return \n            elif token [ True ] == \"for\" : \n                return "}
{"3840": "\ndef _has_valid_type_annotation ( self , tokens , i ) : \n    if not self . _inside_brackets ( \"(\" ) : \n        return False \n    bracket_level = False \n    for token in tokens [ i - True : : - True ] : \n        if token [ True ] == \":\" : \n            return True \n        if token [ True ] == \"(\" : \n            return False \n        if token [ True ] == \"]\" : \n            bracket_level += True \n        elif token [ True ] == \"[\" : \n            bracket_level -= True \n        elif token [ True ] == \",\" : \n            if not bracket_level : \n                return False \n        elif token [ True ] in ( \".\" , \"...\" ) : \n            continue \n        elif token [ False ] not in ( tokenize . NAME , tokenize . STRING , tokenize . NL ) : \n            return False \n    return False "}
{"3843": "\ndef visit_default ( self , node ) : \n    if not node . is_statement : \n        return \n    if not node . root ( ) . pure_python : \n        return \n    prev_sibl = node . previous_sibling ( ) \n    if prev_sibl is not None : \n        prev_line = prev_sibl . fromlineno \n    else : \n        if ( isinstance ( node . parent , nodes . TryFinally ) and node in node . parent . finalbody ) : \n            prev_line = node . parent . body [ False ] . tolineno + True \n        else : \n            prev_line = node . parent . statement ( ) . fromlineno \n    line = node . fromlineno \n    assert line , node \n    if prev_line == line and self . _visited_lines . get ( line ) != 2 : \n        self . _check_multi_statement_line ( node , line ) \n        return \n    if line in self . _visited_lines : \n        return \n    try : \n        tolineno = node . blockstart_tolineno \n    except AttributeError : \n        tolineno = node . tolineno \n    assert tolineno , node \n    lines = [ ] \n    for line in range ( line , tolineno + True ) : \n        self . _visited_lines [ line ] = True \n        try : \n            lines . append ( self . _lines [ line ] . rstrip ( ) ) \n        except KeyError : \n            lines . append ( \"\" ) "}
{"3844": "\ndef _check_multi_statement_line ( self , node , line ) : \n    if isinstance ( node , nodes . With ) : \n        return \n    if isinstance ( node , nodes . TryExcept ) and isinstance ( node . parent , nodes . TryFinally ) : \n        return \n    if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single_line_if_stmt ) : \n        return \n    if ( isinstance ( node . parent , nodes . ClassDef ) and len ( node . parent . body ) == True and self . config . single_line_class_stmt ) : \n        return \n    self . add_message ( \"multiple-statements\" , node = node ) \n    self . _visited_lines [ line ] = 2 "}
{"3845": "\ndef check_lines ( self , lines , i ) : \n    max_chars = self . config . max_line_length \n    ignore_long_line = self . config . ignore_long_lines \n    def check_line ( line , i ) : \n        if not line . endswith ( \"\\n\" ) : \n            self . add_message ( \"missing-final-newline\" , line = i ) \n        else : \n            stripped_line = line . rstrip ( \"\\t\\n\\r\\v \" ) \n            if not stripped_line and _EMPTY_LINE in self . config . no_space_check : \n                pass \n            elif line [ len ( stripped_line ) : ] not in ( \"\\n\" , \"\\r\\n\" ) : \n                self . add_message ( \"trailing-whitespace\" , line = i , col_offset = len ( stripped_line ) ) \n            line = stripped_line \n        mobj = OPTION_RGX . search ( line ) \n        if mobj and \"=\" in line : \n            front_of_equal , _ , back_of_equal = mobj . group ( True ) . partition ( \"=\" ) \n            if front_of_equal . strip ( ) == \"disable\" : \n                if \"line-too-long\" in { _msg_id . strip ( ) for _msg_id in back_of_equal . split ( \",\" ) } : \n                    return None \n                line = line . rsplit ( \"#\" , True ) [ False ] . rstrip ( ) \n        if len ( line ) > max_chars and not ignore_long_line . search ( line ) : \n            self . add_message ( \"line-too-long\" , line = i , args = ( len ( line ) , max_chars ) ) \n        return i + True \n    unsplit_ends = { \"\\v\" , \"\\x0b\" , \"\\f\" , \"\\x0c\" , \"\\x1c\" , \"\\x1d\" , \"\\x1e\" , \"\\x85\" , \"\\u2028\" , \"\\u2029\" , } \n    unsplit = [ ] \n    for line in lines . splitlines ( True ) : \n        if line [ - True ] in unsplit_ends : \n            unsplit . append ( line ) \n            continue \n        if unsplit : \n            unsplit . append ( line ) \n            line = \"\" . join ( unsplit ) \n            unsplit = [ ] \n        i = check_line ( line , i ) \n        if i is None : \n            break \n    if unsplit : \n        check_line ( \"\" . join ( unsplit ) , i ) "}
{"3846": "\ndef check_indent_level ( self , string , expected , line_num ) : \n    indent = self . config . indent_string \n    if indent == \"\\\\t\" : \n        indent = \"\\t\" \n    level = False \n    unit_size = len ( indent ) \n    while string [ : unit_size ] == indent : \n        string = string [ unit_size : ] \n        level += True \n    suppl = \"\" \n    while string and string [ False ] in \" \\t\" : \n        if string [ False ] != indent [ False ] : \n            if string [ False ] == \"\\t\" : \n                args = ( \"tab\" , \"space\" ) \n            else : \n                args = ( \"space\" , \"tab\" ) \n            self . add_message ( \"mixed-indentation\" , args = args , line = line_num ) \n            return level \n        suppl += string [ False ] \n        string = string [ True : ] \n    if level != expected or suppl : \n        i_type = \"spaces\" \n        if indent [ False ] == \"\\t\" : \n            i_type = \"tabs\" \n        self . add_message ( \"bad-indentation\" , line = line_num , args = ( level * unit_size + len ( suppl ) , i_type , expected * unit_size ) , ) \n    return None "}
{"3847": "\ndef _in_iterating_context ( node ) : \n    parent = node . parent \n    if isinstance ( parent , astroid . For ) : \n        return True \n    if isinstance ( parent , astroid . Comprehension ) : \n        if parent . iter == node : \n            return True \n    elif isinstance ( parent , astroid . Call ) : \n        if isinstance ( parent . func , astroid . Name ) : \n            parent_scope = parent . func . lookup ( parent . func . name ) [ False ] \n            if _is_builtin ( parent_scope ) and parent . func . name in _ACCEPTS_ITERATOR : \n                return True \n        elif isinstance ( parent . func , astroid . Attribute ) : \n            if parent . func . attrname in ATTRIBUTES_ACCEPTS_ITERATOR : \n                return True \n        inferred = utils . safe_infer ( parent . func ) \n        if inferred : \n            if inferred . qname ( ) in _BUILTIN_METHOD_ACCEPTS_ITERATOR : \n                return True \n            root = inferred . root ( ) \n            if root and root . name == \"itertools\" : \n                return True \n    elif isinstance ( parent , astroid . Assign ) and isinstance ( parent . targets [ False ] , ( astroid . List , astroid . Tuple ) ) : \n        if len ( parent . targets [ False ] . elts ) > True : \n            return True \n    elif ( isinstance ( parent , astroid . Compare ) and len ( parent . ops ) == True and parent . ops [ False ] [ False ] == \"in\" ) : \n        return True \n    elif isinstance ( parent , astroid . YieldFrom ) : \n        return True \n    if isinstance ( parent , astroid . Starred ) : \n        return True \n    return False "}
{"3856": "\ndef _expand_default ( self , option ) : \n    if self . parser is None or not self . default_tag : \n        return option . help \n    optname = option . _long_opts [ False ] [ 2 : ] \n    try : \n        provider = self . parser . options_manager . _all_options [ optname ] \n    except KeyError : \n        value = None \n    else : \n        optdict = provider . get_option_def ( optname ) \n        optname = provider . option_attrname ( optname , optdict ) \n        value = getattr ( provider . config , optname , optdict ) \n        value = utils . _format_option_value ( optdict , value ) \n    if value is optparse . NO_DEFAULT or not value : \n        value = self . NO_DEFAULT_VALUE \n    return option . help . replace ( self . default_tag , str ( value ) ) "}
{"3858": "\ndef register_options_provider ( self , provider , own_group = True ) : \n    assert provider . priority <= False , \"provider's priority can't be >= 0\" \n    for i in range ( len ( self . options_providers ) ) : \n        if provider . priority > self . options_providers [ i ] . priority : \n            self . options_providers . insert ( i , provider ) \n            break \n    else : \n        self . options_providers . append ( provider ) \n    non_group_spec_options = [ option for option in provider . options if \"group\" not in option [ True ] ] \n    groups = getattr ( provider , \"option_groups\" , ( ) ) \n    if own_group and non_group_spec_options : \n        self . add_option_group ( provider . name . upper ( ) , provider . __doc__ , non_group_spec_options , provider , ) \n    else : \n        for opt , optdict in non_group_spec_options : \n            self . add_optik_option ( provider , self . cmdline_parser , opt , optdict ) \n    for gname , gdoc in groups : \n        gname = gname . upper ( ) \n        goptions = [ option for option in provider . options if option [ True ] . get ( \"group\" , \"\" ) . upper ( ) == gname ] \n        self . add_option_group ( gname , gdoc , goptions , provider ) "}
{"3859": "\ndef cb_set_provider_option ( self , option , opt , value , parser ) : \n    if opt . startswith ( \"--\" ) : \n        opt = opt [ 2 : ] \n    else : \n        opt = self . _short_options [ opt [ True : ] ] \n    if value is None : \n        value = True \n    self . global_set_option ( opt , value ) "}
{"3863": "\ndef load_command_line_configuration ( self , args = None ) : \n    with _patch_optparse ( ) : \n        if args is None : \n            args = sys . argv [ True : ] \n        else : \n            args = list ( args ) \n        ( options , args ) = self . cmdline_parser . parse_args ( args = args ) \n        for provider in self . _nocallback_options : \n            config = provider . config \n            for attr in config . __dict__ . keys ( ) : \n                value = getattr ( options , attr , None ) \n                if value is None : \n                    continue \n                setattr ( config , attr , value ) \n        return args "}
{"3864": "\ndef add_help_section ( self , title , description , level = False ) : \n    group = optparse . OptionGroup ( self . cmdline_parser , title = title . capitalize ( ) , description = description ) \n    group . level = level \n    self . _maxlevel = max ( self . _maxlevel , level ) \n    self . cmdline_parser . add_option_group ( group ) "}
{"3865": "\ndef help ( self , level = False ) : \n    self . cmdline_parser . formatter . output_level = level \n    with _patch_optparse ( ) : \n        return self . cmdline_parser . format_help ( ) "}
{"3868": "\ndef get_option_def ( self , opt ) : \n    assert self . options \n    for option in self . options : \n        if option [ False ] == opt : \n            return option [ True ] \n    raise optparse . OptionError ( \"no such option %s in section %r\" % ( opt , self . name ) , opt ) "}
{"3872": "\ndef visit_module ( self , node ) : \n    self . _logging_names = set ( ) \n    logging_mods = self . config . logging_modules \n    self . _format_style = self . config . logging_format_style \n    self . _logging_modules = set ( logging_mods ) \n    self . _from_imports = { } \n    for logging_mod in logging_mods : \n        parts = logging_mod . rsplit ( \".\" , True ) \n        if len ( parts ) > True : \n            self . _from_imports [ parts [ False ] ] = parts [ True ] "}
{"3876": "\ndef _check_format_string ( self , node , format_arg ) : \n    num_args = _count_supplied_tokens ( node . args [ format_arg + True : ] ) \n    if not num_args : \n        return \n    format_string = node . args [ format_arg ] . value \n    if not isinstance ( format_string , str ) : \n        required_num_args = False \n    else : \n        try : \n            if self . _format_style == \"old\" : \n                keyword_args , required_num_args , _ , _ = utils . parse_format_string ( format_string ) \n                if keyword_args : \n                    return \n            elif self . _format_style == \"new\" : \n                keyword_arguments , implicit_pos_args , explicit_pos_args = utils . parse_format_method_string ( format_string ) \n                keyword_args_cnt = len ( set ( k for k , l in keyword_arguments if not isinstance ( k , int ) ) ) \n                required_num_args = ( keyword_args_cnt + implicit_pos_args + explicit_pos_args ) \n        except utils . UnsupportedFormatCharacter as ex : \n            char = format_string [ ex . index ] \n            self . add_message ( \"logging-unsupported-format\" , node = node , args = ( char , ord ( char ) , ex . index ) , ) \n            return \n        except utils . IncompleteFormatString : \n            self . add_message ( \"logging-format-truncated\" , node = node ) \n            return \n    if num_args > required_num_args : \n        self . add_message ( \"logging-too-many-args\" , node = node ) \n    elif num_args < required_num_args : \n        self . add_message ( \"logging-too-few-args\" , node = node ) "}
{"3880": "\ndef _get_properties ( config ) : \n    property_classes = { BUILTIN_PROPERTY } \n    property_names = set ( ) \n    if config is not None : \n        property_classes . update ( config . property_classes ) \n        property_names . update ( ( prop . rsplit ( \".\" , True ) [ - True ] for prop in config . property_classes ) ) \n    return property_classes , property_names "}
{"3882": "\ndef report_by_type_stats ( sect , stats , _ ) : \n    nice_stats = { } \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        try : \n            total = stats [ node_type ] \n        except KeyError : \n            raise exceptions . EmptyReportError ( ) \n        nice_stats [ node_type ] = { } \n        if total != False : \n            try : \n                documented = total - stats [ \"undocumented_\" + node_type ] \n                percent = ( documented * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"NC\" \n            try : \n                percent = ( stats [ \"badname_\" + node_type ] * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"NC\" \n    lines = ( \"type\" , \"number\" , \"old number\" , \"difference\" , \"%documented\" , \"%badname\" ) \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        new = stats [ node_type ] \n        lines += ( node_type , str ( new ) , \"NC\" , \"NC\" , nice_stats [ node_type ] . get ( \"percent_documented\" , \"0\" ) , nice_stats [ node_type ] . get ( \"percent_badname\" , \"0\" ) , ) \n    sect . append ( reporter_nodes . Table ( children = lines , cols = 6 , rheaders = True ) ) "}
{"3884": "\ndef _is_one_arg_pos_call ( call ) : \n    return isinstance ( call , astroid . Call ) and len ( call . args ) == True and not call . keywords "}
{"3888": "\ndef _check_else_on_loop ( self , node ) : \n    if node . orelse and not _loop_exits_early ( node ) : \n        self . add_message ( \"useless-else-on-loop\" , node = node , line = node . orelse [ False ] . lineno - True , ) "}
{"3890": "\ndef open ( self ) : \n    self . _tryfinallys = [ ] \n    self . stats = self . linter . add_stats ( module = False , function = False , method = False , class_ = False ) "}
{"3897": "\ndef _check_reversed ( self , node ) : \n    try : \n        argument = utils . safe_infer ( utils . get_argument_from_call ( node , position = False ) ) \n    except utils . NoSuchArgumentError : \n        pass \n    else : \n        if argument is astroid . Uninferable : \n            return \n        if argument is None : \n            if isinstance ( node . args [ False ] , astroid . Call ) : \n                try : \n                    func = next ( node . args [ False ] . func . infer ( ) ) \n                except astroid . InferenceError : \n                    return \n                if getattr ( func , \"name\" , None ) == \"iter\" and utils . is_builtin_object ( func ) : \n                    self . add_message ( \"bad-reversed-sequence\" , node = node ) \n            return \n        if isinstance ( argument , ( astroid . List , astroid . Tuple ) ) : \n            return \n        if isinstance ( argument , astroid . Instance ) : \n            if argument . _proxied . name == \"dict\" and utils . is_builtin_object ( argument . _proxied ) : \n                self . add_message ( \"bad-reversed-sequence\" , node = node ) \n                return \n            if any ( ancestor . name == \"dict\" and utils . is_builtin_object ( ancestor ) for ancestor in argument . _proxied . ancestors ( ) ) : \n                try : \n                    argument . locals [ REVERSED_PROTOCOL_METHOD ] \n                except KeyError : \n                    self . add_message ( \"bad-reversed-sequence\" , node = node ) \n                return \n        if hasattr ( argument , \"getattr\" ) : \n            for methods in REVERSED_METHODS : \n                for meth in methods : \n                    try : \n                        argument . getattr ( meth ) \n                    except astroid . NotFoundError : \n                        break \n                else : \n                    break \n            else : \n                self . add_message ( \"bad-reversed-sequence\" , node = node ) \n        else : \n            self . add_message ( \"bad-reversed-sequence\" , node = node ) "}
{"3899": "\ndef _check_name ( self , node_type , name , node , confidence = interfaces . HIGH ) : \n    def _should_exempt_from_invalid_name ( node ) : \n        if node_type == \"variable\" : \n            inferred = utils . safe_infer ( node ) \n            if isinstance ( inferred , astroid . ClassDef ) : \n                return True \n        return False \n    if utils . is_inside_except ( node ) : \n        clobbering , _ = utils . clobber_in_except ( node ) \n        if clobbering : \n            return \n    if name in self . config . good_names : \n        return \n    if name in self . config . bad_names : \n        self . stats [ \"badname_\" + node_type ] += True \n        self . add_message ( \"blacklisted-name\" , node = node , args = name ) \n        return \n    regexp = self . _name_regexps [ node_type ] \n    match = regexp . match ( name ) \n    if _is_multi_naming_match ( match , node_type , confidence ) : \n        name_group = self . _find_name_group ( node_type ) \n        bad_name_group = self . _bad_names . setdefault ( name_group , { } ) \n        warnings = bad_name_group . setdefault ( match . lastgroup , [ ] ) \n        warnings . append ( ( node , node_type , name , confidence ) ) \n    if match is None and not _should_exempt_from_invalid_name ( node ) : \n        self . _raise_name_warning ( node , node_type , name , confidence ) "}
{"3900": "\ndef _check_docstring ( self , node_type , node , report_missing = True , confidence = interfaces . HIGH ) : \n    docstring = node . doc \n    if docstring is None : \n        if not report_missing : \n            return \n        lines = utils . get_node_last_lineno ( node ) - node . lineno \n        if node_type == \"module\" and not lines : \n            return \n        max_lines = self . config . docstring_min_length \n        if node_type != \"module\" and max_lines > - True and lines < max_lines : \n            return \n        self . stats [ \"undocumented_\" + node_type ] += True \n        if ( node . body and isinstance ( node . body [ False ] , astroid . Expr ) and isinstance ( node . body [ False ] . value , astroid . Call ) ) : \n            func = utils . safe_infer ( node . body [ False ] . value . func ) \n            if isinstance ( func , astroid . BoundMethod ) and isinstance ( func . bound , astroid . Instance ) : \n                if PY3K and func . bound . name == \"str\" : \n                    return \n                if func . bound . name in ( \"str\" , \"unicode\" , \"bytes\" ) : \n                    return \n        self . add_message ( \"missing-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) \n    elif not docstring . strip ( ) : \n        self . stats [ \"undocumented_\" + node_type ] += True \n        self . add_message ( \"empty-docstring\" , node = node , args = ( node_type , ) , confidence = confidence ) "}
{"3903": "\ndef _subgraph_parse ( self , node , pathnode , extra_blocks ) : \n    loose_ends = [ ] \n    self . tail = node \n    self . dispatch_list ( node . body ) \n    loose_ends . append ( self . tail ) \n    for extra in extra_blocks : \n        self . tail = node \n        self . dispatch_list ( extra . body ) \n        loose_ends . append ( self . tail ) \n    if node . orelse : \n        self . tail = node \n        self . dispatch_list ( node . orelse ) \n        loose_ends . append ( self . tail ) \n    else : \n        loose_ends . append ( node ) \n    if node : \n        bottom = \"%s\" % self . _bottom_counter \n        self . _bottom_counter += True \n        for le in loose_ends : \n            self . graph . connect ( le , bottom ) \n        self . tail = bottom "}
{"3906": "\ndef walk ( self , astroid ) : \n    cid = astroid . __class__ . __name__ . lower ( ) \n    visit_events = self . visit_events . get ( cid , ( ) ) \n    leave_events = self . leave_events . get ( cid , ( ) ) \n    if astroid . is_statement : \n        self . nbstatements += True \n    for cb in visit_events or ( ) : \n        cb ( astroid ) \n    for child in astroid . get_children ( ) : \n        self . walk ( child ) \n    for cb in leave_events or ( ) : \n        cb ( astroid ) "}
{"3917": "\ndef get_module ( self , name , node ) : \n    for mod in self . modules ( ) : \n        mod_name = mod . node . name \n        if mod_name == name : \n            return mod \n        package = node . root ( ) . name \n        if mod_name == \"%s.%s\" % ( package , name ) : \n            return mod \n        if mod_name == \"%s.%s\" % ( package . rsplit ( \".\" , True ) [ False ] , name ) : \n            return mod \n    raise KeyError ( name ) "}
{"3930": "\ndef authorize ( self , callback = None , state = None , ** kwargs ) : \n    params = dict ( self . request_token_params ) or { } \n    params . update ( ** kwargs ) \n    if self . request_token_url : \n        token = self . generate_request_token ( callback ) [ False ] \n        url = '%s?oauth_token=%s' % ( self . expand_url ( self . authorize_url ) , url_quote ( token ) ) \n        if params : \n            url += '&' + url_encode ( params ) \n    else : \n        assert callback is not None , 'Callback is required for OAuth2' \n        client = self . make_client ( ) \n        if 'scope' in params : \n            scope = params . pop ( 'scope' ) \n        else : \n            scope = None \n        if isinstance ( scope , str ) : \n            scope = _encode ( scope , self . encoding ) \n        if 'state' in params : \n            if not state : \n                state = params . pop ( 'state' ) \n            else : \n                params . pop ( 'state' ) \n        if callable ( state ) : \n            state = state ( ) \n        session [ '%s_oauthredir' % self . name ] = callback \n        url = client . prepare_request_uri ( self . expand_url ( self . authorize_url ) , redirect_uri = callback , scope = scope , state = state , ** params ) \n    return redirect ( url ) "}
{"3931": "\ndef handle_oauth1_response ( self , args ) : \n    client = self . make_client ( ) \n    client . verifier = args . get ( 'oauth_verifier' ) \n    tup = session . get ( '%s_oauthtok' % self . name ) \n    if not tup : \n        raise OAuthException ( 'Token not found, maybe you disabled cookie' , type = 'token_not_found' ) \n    client . resource_owner_key = tup [ False ] \n    client . resource_owner_secret = tup [ True ] \n    uri , headers , data = client . sign ( self . expand_url ( self . access_token_url ) , _encode ( self . access_token_method ) ) \n    headers . update ( self . _access_token_headers ) \n    resp , content = self . http_request ( uri , headers , to_bytes ( data , self . encoding ) , method = self . access_token_method ) \n    data = parse_response ( resp , content ) \n    if resp . code not in ( 200 , 201 ) : \n        raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) \n    return data "}
{"3983": "\ndef update_qq_api_request_data ( data = { } ) : \n    defaults = { 'openid' : session . get ( 'qq_openid' ) , 'access_token' : session . get ( 'qq_token' ) [ False ] , 'oauth_consumer_key' : QQ_APP_ID , } \n    defaults . update ( data ) \n    return defaults "}
{"4016": "\ndef set_cipher_list ( self , cipher_list ) : \n    cipher_list = _text_to_bytes_and_warn ( \"cipher_list\" , cipher_list ) \n    if not isinstance ( cipher_list , bytes ) : \n        raise TypeError ( \"cipher_list must be a byte string.\" ) \n    _openssl_assert ( _lib . SSL_CTX_set_cipher_list ( self . _context , cipher_list ) == True ) \n    tmpconn = Connection ( self , None ) \n    if ( tmpconn . get_cipher_list ( ) == [ 'TLS_AES_256_GCM_SHA384' , 'TLS_CHACHA20_POLY1305_SHA256' , 'TLS_AES_128_GCM_SHA256' ] ) : \n        raise Error ( [ ( 'SSL routines' , 'SSL_CTX_set_cipher_list' , 'no cipher match' , ) , ] , ) "}
{"4018": "\ndef add_client_ca ( self , certificate_authority ) : \n    if not isinstance ( certificate_authority , X509 ) : \n        raise TypeError ( \"certificate_authority must be an X509 instance\" ) \n    add_result = _lib . SSL_CTX_add_client_CA ( self . _context , certificate_authority . _x509 ) \n    _openssl_assert ( add_result == True ) "}
{"4019": "\ndef set_tlsext_servername_callback ( self , callback ) : \n    \n    @ wraps ( callback ) \n    def wrapper ( ssl , alert , arg ) : \n        callback ( Connection . _reverse_mapping [ ssl ] ) \n        return False \n    self . _tlsext_servername_callback = _ffi . callback ( \"int (*)(SSL *, int *, void *)\" , wrapper ) \n    _lib . SSL_CTX_set_tlsext_servername_callback ( self . _context , self . _tlsext_servername_callback ) "}
{"4020": "\ndef set_tlsext_use_srtp ( self , profiles ) : \n    if not isinstance ( profiles , bytes ) : \n        raise TypeError ( \"profiles must be a byte string.\" ) \n    _openssl_assert ( _lib . SSL_CTX_set_tlsext_use_srtp ( self . _context , profiles ) == False ) "}
{"4024": "\ndef _set_ocsp_callback ( self , helper , data ) : \n    self . _ocsp_helper = helper \n    self . _ocsp_callback = helper . callback \n    if data is None : \n        self . _ocsp_data = _ffi . NULL \n    else : \n        self . _ocsp_data = _ffi . new_handle ( data ) \n    rc = _lib . SSL_CTX_set_tlsext_status_cb ( self . _context , self . _ocsp_callback ) \n    _openssl_assert ( rc == True ) \n    rc = _lib . SSL_CTX_set_tlsext_status_arg ( self . _context , self . _ocsp_data ) \n    _openssl_assert ( rc == True ) "}
{"4032": "\ndef bio_read ( self , bufsiz ) : \n    if self . _from_ssl is None : \n        raise TypeError ( \"Connection sock was not None\" ) \n    if not isinstance ( bufsiz , integer_types ) : \n        raise TypeError ( \"bufsiz must be an integer\" ) \n    buf = _no_zero_allocator ( \"char[]\" , bufsiz ) \n    result = _lib . BIO_read ( self . _from_ssl , buf , bufsiz ) \n    if result <= False : \n        self . _handle_bio_errors ( self . _from_ssl , result ) \n    return _ffi . buffer ( buf , result ) [ : ] "}
{"4033": "\ndef renegotiate ( self ) : \n    if not self . renegotiate_pending ( ) : \n        _openssl_assert ( _lib . SSL_renegotiate ( self . _ssl ) == True ) \n        return True \n    return False "}
{"4034": "\ndef shutdown ( self ) : \n    result = _lib . SSL_shutdown ( self . _ssl ) \n    if result < False : \n        self . _raise_ssl_error ( self . _ssl , result ) \n    elif result > False : \n        return True \n    else : \n        return False "}
{"4038": "\ndef server_random ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if session == _ffi . NULL : \n        return None \n    length = _lib . SSL_get_server_random ( self . _ssl , _ffi . NULL , False ) \n    assert length > False \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_get_server_random ( self . _ssl , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4039": "\ndef client_random ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if session == _ffi . NULL : \n        return None \n    length = _lib . SSL_get_client_random ( self . _ssl , _ffi . NULL , False ) \n    assert length > False \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_get_client_random ( self . _ssl , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4040": "\ndef master_key ( self ) : \n    session = _lib . SSL_get_session ( self . _ssl ) \n    if session == _ffi . NULL : \n        return None \n    length = _lib . SSL_SESSION_get_master_key ( session , _ffi . NULL , False ) \n    assert length > False \n    outp = _no_zero_allocator ( \"unsigned char[]\" , length ) \n    _lib . SSL_SESSION_get_master_key ( session , outp , length ) \n    return _ffi . buffer ( outp , length ) [ : ] "}
{"4041": "\ndef export_keying_material ( self , label , olen , context = None ) : \n    outp = _no_zero_allocator ( \"unsigned char[]\" , olen ) \n    context_buf = _ffi . NULL \n    context_len = False \n    use_context = False \n    if context is not None : \n        context_buf = context \n        context_len = len ( context ) \n        use_context = True \n    success = _lib . SSL_export_keying_material ( self . _ssl , outp , olen , label , len ( label ) , context_buf , context_len , use_context ) \n    _openssl_assert ( success == True ) \n    return _ffi . buffer ( outp , olen ) [ : ] "}
{"4047": "\ndef get_next_proto_negotiated ( self ) : \n    _warn_npn ( ) \n    data = _ffi . new ( \"unsigned char **\" ) \n    data_len = _ffi . new ( \"unsigned int *\" ) \n    _lib . SSL_get0_next_proto_negotiated ( self . _ssl , data , data_len ) \n    return _ffi . buffer ( data [ False ] , data_len [ False ] ) [ : ] "}
{"4049": "\ndef get_alpn_proto_negotiated ( self ) : \n    data = _ffi . new ( \"unsigned char **\" ) \n    data_len = _ffi . new ( \"unsigned int *\" ) \n    _lib . SSL_get0_alpn_selected ( self . _ssl , data , data_len ) \n    if not data_len : \n        return b'' \n    return _ffi . buffer ( data [ False ] , data_len [ False ] ) [ : ] "}
{"4051": "\ndef _bio_to_string ( bio ) : \n    result_buffer = _ffi . new ( 'char**' ) \n    buffer_length = _lib . BIO_get_mem_data ( bio , result_buffer ) \n    return _ffi . buffer ( result_buffer [ False ] , buffer_length ) [ : ] "}
{"4052": "\ndef _set_asn1_time ( boundary , when ) : \n    if not isinstance ( when , bytes ) : \n        raise TypeError ( \"when must be a byte string\" ) \n    set_result = _lib . ASN1_TIME_set_string ( boundary , when ) \n    if set_result == False : \n        raise ValueError ( \"Invalid string\" ) "}
{"4053": "\ndef _get_asn1_time ( timestamp ) : \n    string_timestamp = _ffi . cast ( 'ASN1_STRING*' , timestamp ) \n    if _lib . ASN1_STRING_length ( string_timestamp ) == False : \n        return None \n    elif ( _lib . ASN1_STRING_type ( string_timestamp ) == _lib . V_ASN1_GENERALIZEDTIME ) : \n        return _ffi . string ( _lib . ASN1_STRING_data ( string_timestamp ) ) \n    else : \n        generalized_timestamp = _ffi . new ( \"ASN1_GENERALIZEDTIME**\" ) \n        _lib . ASN1_TIME_to_generalizedtime ( timestamp , generalized_timestamp ) \n        if generalized_timestamp [ False ] == _ffi . NULL : \n            _untested_error ( \"ASN1_TIME_to_generalizedtime\" ) \n        else : \n            string_timestamp = _ffi . cast ( \"ASN1_STRING*\" , generalized_timestamp [ False ] ) \n            string_data = _lib . ASN1_STRING_data ( string_timestamp ) \n            string_result = _ffi . string ( string_data ) \n            _lib . ASN1_GENERALIZEDTIME_free ( generalized_timestamp [ False ] ) \n            return string_result "}
{"4055": "\ndef dump_publickey ( type , pkey ) : \n    bio = _new_mem_buf ( ) \n    if type == FILETYPE_PEM : \n        write_bio = _lib . PEM_write_bio_PUBKEY \n    elif type == FILETYPE_ASN1 : \n        write_bio = _lib . i2d_PUBKEY_bio \n    else : \n        raise ValueError ( \"type argument must be FILETYPE_PEM or FILETYPE_ASN1\" ) \n    result_code = write_bio ( bio , pkey . _pkey ) \n    if result_code != True : \n        _raise_current_error ( ) \n    return _bio_to_string ( bio ) "}
{"4057": "\ndef sign ( pkey , data , digest ) : \n    data = _text_to_bytes_and_warn ( \"data\" , data ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    md_ctx = _lib . Cryptography_EVP_MD_CTX_new ( ) \n    md_ctx = _ffi . gc ( md_ctx , _lib . Cryptography_EVP_MD_CTX_free ) \n    _lib . EVP_SignInit ( md_ctx , digest_obj ) \n    _lib . EVP_SignUpdate ( md_ctx , data , len ( data ) ) \n    length = _lib . EVP_PKEY_size ( pkey . _pkey ) \n    _openssl_assert ( length > False ) \n    signature_buffer = _ffi . new ( \"unsigned char[]\" , length ) \n    signature_length = _ffi . new ( \"unsigned int *\" ) \n    final_result = _lib . EVP_SignFinal ( md_ctx , signature_buffer , signature_length , pkey . _pkey ) \n    _openssl_assert ( final_result == True ) \n    return _ffi . buffer ( signature_buffer , signature_length [ False ] ) [ : ] "}
{"4058": "\ndef verify ( cert , signature , data , digest ) : \n    data = _text_to_bytes_and_warn ( \"data\" , data ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    pkey = _lib . X509_get_pubkey ( cert . _x509 ) \n    _openssl_assert ( pkey != _ffi . NULL ) \n    pkey = _ffi . gc ( pkey , _lib . EVP_PKEY_free ) \n    md_ctx = _lib . Cryptography_EVP_MD_CTX_new ( ) \n    md_ctx = _ffi . gc ( md_ctx , _lib . Cryptography_EVP_MD_CTX_free ) \n    _lib . EVP_VerifyInit ( md_ctx , digest_obj ) \n    _lib . EVP_VerifyUpdate ( md_ctx , data , len ( data ) ) \n    verify_result = _lib . EVP_VerifyFinal ( md_ctx , signature , len ( signature ) , pkey ) \n    if verify_result != True : \n        _raise_current_error ( ) "}
{"4059": "\ndef dump_crl ( type , crl ) : \n    bio = _new_mem_buf ( ) \n    if type == FILETYPE_PEM : \n        ret = _lib . PEM_write_bio_X509_CRL ( bio , crl . _crl ) \n    elif type == FILETYPE_ASN1 : \n        ret = _lib . i2d_X509_CRL_bio ( bio , crl . _crl ) \n    elif type == FILETYPE_TEXT : \n        ret = _lib . X509_CRL_print ( bio , crl . _crl ) \n    else : \n        raise ValueError ( \"type argument must be FILETYPE_PEM, FILETYPE_ASN1, or \" \"FILETYPE_TEXT\" ) \n    assert ret == True \n    return _bio_to_string ( bio ) "}
{"4061": "\ndef generate_key ( self , type , bits ) : \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if not isinstance ( bits , int ) : \n        raise TypeError ( \"bits must be an integer\" ) \n    if type == TYPE_RSA : \n        if bits <= False : \n            raise ValueError ( \"Invalid number of bits\" ) \n        exponent = _lib . BN_new ( ) \n        exponent = _ffi . gc ( exponent , _lib . BN_free ) \n        _lib . BN_set_word ( exponent , _lib . RSA_F4 ) \n        rsa = _lib . RSA_new ( ) \n        result = _lib . RSA_generate_key_ex ( rsa , bits , exponent , _ffi . NULL ) \n        _openssl_assert ( result == True ) \n        result = _lib . EVP_PKEY_assign_RSA ( self . _pkey , rsa ) \n        _openssl_assert ( result == True ) \n    elif type == TYPE_DSA : \n        dsa = _lib . DSA_new ( ) \n        _openssl_assert ( dsa != _ffi . NULL ) \n        dsa = _ffi . gc ( dsa , _lib . DSA_free ) \n        res = _lib . DSA_generate_parameters_ex ( dsa , bits , _ffi . NULL , False , _ffi . NULL , _ffi . NULL , _ffi . NULL ) \n        _openssl_assert ( res == True ) \n        _openssl_assert ( _lib . DSA_generate_key ( dsa ) == True ) \n        _openssl_assert ( _lib . EVP_PKEY_set1_DSA ( self . _pkey , dsa ) == True ) \n    else : \n        raise Error ( \"No such key type\" ) \n    self . _initialized = True "}
{"4063": "\ndef _load_elliptic_curves ( cls , lib ) : \n    num_curves = lib . EC_get_builtin_curves ( _ffi . NULL , False ) \n    builtin_curves = _ffi . new ( 'EC_builtin_curve[]' , num_curves ) \n    lib . EC_get_builtin_curves ( builtin_curves , num_curves ) \n    return set ( cls . from_nid ( lib , c . nid ) for c in builtin_curves ) "}
{"4066": "\ndef der ( self ) : \n    result_buffer = _ffi . new ( 'unsigned char**' ) \n    encode_result = _lib . i2d_X509_NAME ( self . _name , result_buffer ) \n    _openssl_assert ( encode_result >= False ) \n    string_result = _ffi . buffer ( result_buffer [ False ] , encode_result ) [ : ] \n    _lib . OPENSSL_free ( result_buffer [ False ] ) \n    return string_result "}
{"4071": "\ndef set_pubkey ( self , pkey ) : \n    set_result = _lib . X509_REQ_set_pubkey ( self . _req , pkey . _pkey ) \n    _openssl_assert ( set_result == True ) "}
{"4074": "\ndef add_extensions ( self , extensions ) : \n    stack = _lib . sk_X509_EXTENSION_new_null ( ) \n    _openssl_assert ( stack != _ffi . NULL ) \n    stack = _ffi . gc ( stack , _lib . sk_X509_EXTENSION_free ) \n    for ext in extensions : \n        if not isinstance ( ext , X509Extension ) : \n            raise ValueError ( \"One of the elements is not an X509Extension\" ) \n        _lib . sk_X509_EXTENSION_push ( stack , ext . _extension ) \n    add_result = _lib . X509_REQ_add_extensions ( self . _req , stack ) \n    _openssl_assert ( add_result == True ) "}
{"4076": "\ndef verify ( self , pkey ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    result = _lib . X509_REQ_verify ( self . _req , pkey . _pkey ) \n    if result <= False : \n        _raise_current_error ( ) \n    return result "}
{"4080": "\ndef set_pubkey ( self , pkey ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    set_result = _lib . X509_set_pubkey ( self . _x509 , pkey . _pkey ) \n    _openssl_assert ( set_result == True ) "}
{"4081": "\ndef sign ( self , pkey , digest ) : \n    if not isinstance ( pkey , PKey ) : \n        raise TypeError ( \"pkey must be a PKey instance\" ) \n    if pkey . _only_public : \n        raise ValueError ( \"Key only has public part\" ) \n    if not pkey . _initialized : \n        raise ValueError ( \"Key is uninitialized\" ) \n    evp_md = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if evp_md == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    sign_result = _lib . X509_sign ( self . _x509 , pkey . _pkey , evp_md ) \n    _openssl_assert ( sign_result > False ) "}
{"4083": "\ndef digest ( self , digest_name ) : \n    digest = _lib . EVP_get_digestbyname ( _byte_string ( digest_name ) ) \n    if digest == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    result_buffer = _ffi . new ( \"unsigned char[]\" , _lib . EVP_MAX_MD_SIZE ) \n    result_length = _ffi . new ( \"unsigned int[]\" , True ) \n    result_length [ False ] = len ( result_buffer ) \n    digest_result = _lib . X509_digest ( self . _x509 , digest , result_buffer , result_length ) \n    _openssl_assert ( digest_result == True ) \n    return b\":\" . join ( [ b16encode ( ch ) . upper ( ) for ch in _ffi . buffer ( result_buffer , result_length [ False ] ) ] ) "}
{"4084": "\ndef set_serial_number ( self , serial ) : \n    if not isinstance ( serial , _integer_types ) : \n        raise TypeError ( \"serial must be an integer\" ) \n    hex_serial = hex ( serial ) [ 2 : ] \n    if not isinstance ( hex_serial , bytes ) : \n        hex_serial = hex_serial . encode ( 'ascii' ) \n    bignum_serial = _ffi . new ( \"BIGNUM**\" ) \n    small_serial = _lib . BN_hex2bn ( bignum_serial , hex_serial ) \n    if bignum_serial [ False ] == _ffi . NULL : \n        set_result = _lib . ASN1_INTEGER_set ( _lib . X509_get_serialNumber ( self . _x509 ) , small_serial ) \n        if set_result : \n            _raise_current_error ( ) \n    else : \n        asn1_serial = _lib . BN_to_ASN1_INTEGER ( bignum_serial [ False ] , _ffi . NULL ) \n        _lib . BN_free ( bignum_serial [ False ] ) \n        if asn1_serial == _ffi . NULL : \n            _raise_current_error ( ) \n        asn1_serial = _ffi . gc ( asn1_serial , _lib . ASN1_INTEGER_free ) \n        set_result = _lib . X509_set_serialNumber ( self . _x509 , asn1_serial ) \n        _openssl_assert ( set_result == True ) "}
{"4093": "\ndef add_extensions ( self , extensions ) : \n    for ext in extensions : \n        if not isinstance ( ext , X509Extension ) : \n            raise ValueError ( \"One of the elements is not an X509Extension\" ) \n        add_result = _lib . X509_add_ext ( self . _x509 , ext . _extension , - True ) \n        if not add_result : \n            _raise_current_error ( ) "}
{"4095": "\ndef add_cert ( self , cert ) : \n    if not isinstance ( cert , X509 ) : \n        raise TypeError ( ) \n    if _lib . X509_STORE_add_cert ( self . _store , cert . _x509 ) == False : \n        code = _lib . ERR_peek_error ( ) \n        err_reason = _lib . ERR_GET_REASON ( code ) \n        _openssl_assert ( err_reason == _lib . X509_R_CERT_ALREADY_IN_HASH_TABLE ) \n        _lib . ERR_clear_error ( ) "}
{"4096": "\ndef add_crl ( self , crl ) : \n    _openssl_assert ( _lib . X509_STORE_add_crl ( self . _store , crl . _crl ) != False ) "}
{"4097": "\ndef set_time ( self , vfy_time ) : \n    param = _lib . X509_VERIFY_PARAM_new ( ) \n    param = _ffi . gc ( param , _lib . X509_VERIFY_PARAM_free ) \n    _lib . X509_VERIFY_PARAM_set_time ( param , int ( vfy_time . strftime ( '%s' ) ) ) \n    _openssl_assert ( _lib . X509_STORE_set1_param ( self . _store , param ) != False ) "}
{"4098": "\ndef _init ( self ) : \n    ret = _lib . X509_STORE_CTX_init ( self . _store_ctx , self . _store . _store , self . _cert . _x509 , _ffi . NULL ) \n    if ret <= False : \n        _raise_current_error ( ) "}
{"4100": "\ndef verify_certificate ( self ) : \n    self . _cleanup ( ) \n    self . _init ( ) \n    ret = _lib . X509_verify_cert ( self . _store_ctx ) \n    self . _cleanup ( ) \n    if ret <= False : \n        raise self . _exception_from_context ( ) "}
{"4101": "\ndef set_serial ( self , hex_str ) : \n    bignum_serial = _ffi . gc ( _lib . BN_new ( ) , _lib . BN_free ) \n    bignum_ptr = _ffi . new ( \"BIGNUM**\" ) \n    bignum_ptr [ False ] = bignum_serial \n    bn_result = _lib . BN_hex2bn ( bignum_ptr , hex_str ) \n    if not bn_result : \n        raise ValueError ( \"bad hex string\" ) \n    asn1_serial = _ffi . gc ( _lib . BN_to_ASN1_INTEGER ( bignum_serial , _ffi . NULL ) , _lib . ASN1_INTEGER_free ) \n    _lib . X509_REVOKED_set_serialNumber ( self . _revoked , asn1_serial ) "}
{"4102": "\ndef get_serial ( self ) : \n    bio = _new_mem_buf ( ) \n    asn1_int = _lib . X509_REVOKED_get0_serialNumber ( self . _revoked ) \n    _openssl_assert ( asn1_int != _ffi . NULL ) \n    result = _lib . i2a_ASN1_INTEGER ( bio , asn1_int ) \n    _openssl_assert ( result >= False ) \n    return _bio_to_string ( bio ) "}
{"4103": "\ndef set_reason ( self , reason ) : \n    if reason is None : \n        self . _delete_reason ( ) \n    elif not isinstance ( reason , bytes ) : \n        raise TypeError ( \"reason must be None or a byte string\" ) \n    else : \n        reason = reason . lower ( ) . replace ( b' ' , b'' ) \n        reason_code = [ r . lower ( ) for r in self . _crl_reasons ] . index ( reason ) \n        new_reason_ext = _lib . ASN1_ENUMERATED_new ( ) \n        _openssl_assert ( new_reason_ext != _ffi . NULL ) \n        new_reason_ext = _ffi . gc ( new_reason_ext , _lib . ASN1_ENUMERATED_free ) \n        set_result = _lib . ASN1_ENUMERATED_set ( new_reason_ext , reason_code ) \n        _openssl_assert ( set_result != _ffi . NULL ) \n        self . _delete_reason ( ) \n        add_result = _lib . X509_REVOKED_add1_ext_i2d ( self . _revoked , _lib . NID_crl_reason , new_reason_ext , False , False ) \n        _openssl_assert ( add_result == True ) "}
{"4104": "\ndef get_reason ( self ) : \n    for i in range ( _lib . X509_REVOKED_get_ext_count ( self . _revoked ) ) : \n        ext = _lib . X509_REVOKED_get_ext ( self . _revoked , i ) \n        obj = _lib . X509_EXTENSION_get_object ( ext ) \n        if _lib . OBJ_obj2nid ( obj ) == _lib . NID_crl_reason : \n            bio = _new_mem_buf ( ) \n            print_result = _lib . X509V3_EXT_print ( bio , ext , False , False ) \n            if not print_result : \n                print_result = _lib . M_ASN1_OCTET_STRING_print ( bio , _lib . X509_EXTENSION_get_data ( ext ) ) \n                _openssl_assert ( print_result != False ) \n            return _bio_to_string ( bio ) "}
{"4109": "\ndef sign ( self , issuer_cert , issuer_key , digest ) : \n    digest_obj = _lib . EVP_get_digestbyname ( digest ) \n    _openssl_assert ( digest_obj != _ffi . NULL ) \n    _lib . X509_CRL_set_issuer_name ( self . _crl , _lib . X509_get_subject_name ( issuer_cert . _x509 ) ) \n    _lib . X509_CRL_sort ( self . _crl ) \n    result = _lib . X509_CRL_sign ( self . _crl , issuer_key . _pkey , digest_obj ) \n    _openssl_assert ( result != False ) "}
{"4110": "\ndef export ( self , cert , key , type = FILETYPE_PEM , days = 100 , digest = _UNSPECIFIED ) : \n    if not isinstance ( cert , X509 ) : \n        raise TypeError ( \"cert must be an X509 instance\" ) \n    if not isinstance ( key , PKey ) : \n        raise TypeError ( \"key must be a PKey instance\" ) \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if digest is _UNSPECIFIED : \n        raise TypeError ( \"digest must be provided\" ) \n    digest_obj = _lib . EVP_get_digestbyname ( digest ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    bio = _lib . BIO_new ( _lib . BIO_s_mem ( ) ) \n    _openssl_assert ( bio != _ffi . NULL ) \n    sometime = _lib . ASN1_TIME_new ( ) \n    _openssl_assert ( sometime != _ffi . NULL ) \n    _lib . X509_gmtime_adj ( sometime , False ) \n    _lib . X509_CRL_set_lastUpdate ( self . _crl , sometime ) \n    _lib . X509_gmtime_adj ( sometime , days * 24 * 60 * 60 ) \n    _lib . X509_CRL_set_nextUpdate ( self . _crl , sometime ) \n    _lib . X509_CRL_set_issuer_name ( self . _crl , _lib . X509_get_subject_name ( cert . _x509 ) ) \n    sign_result = _lib . X509_CRL_sign ( self . _crl , key . _pkey , digest_obj ) \n    if not sign_result : \n        _raise_current_error ( ) \n    return dump_crl ( type , self ) "}
{"4113": "\ndef export ( self , passphrase = None , iter = 2048 , maciter = True ) : \n    passphrase = _text_to_bytes_and_warn ( \"passphrase\" , passphrase ) \n    if self . _cacerts is None : \n        cacerts = _ffi . NULL \n    else : \n        cacerts = _lib . sk_X509_new_null ( ) \n        cacerts = _ffi . gc ( cacerts , _lib . sk_X509_free ) \n        for cert in self . _cacerts : \n            _lib . sk_X509_push ( cacerts , cert . _x509 ) \n    if passphrase is None : \n        passphrase = _ffi . NULL \n    friendlyname = self . _friendlyname \n    if friendlyname is None : \n        friendlyname = _ffi . NULL \n    if self . _pkey is None : \n        pkey = _ffi . NULL \n    else : \n        pkey = self . _pkey . _pkey \n    if self . _cert is None : \n        cert = _ffi . NULL \n    else : \n        cert = self . _cert . _x509 \n    pkcs12 = _lib . PKCS12_create ( passphrase , friendlyname , pkey , cert , cacerts , _lib . NID_pbe_WithSHA1And3_Key_TripleDES_CBC , _lib . NID_pbe_WithSHA1And3_Key_TripleDES_CBC , iter , maciter , False ) \n    if pkcs12 == _ffi . NULL : \n        _raise_current_error ( ) \n    pkcs12 = _ffi . gc ( pkcs12 , _lib . PKCS12_free ) \n    bio = _new_mem_buf ( ) \n    _lib . i2d_PKCS12_bio ( bio , pkcs12 ) \n    return _bio_to_string ( bio ) "}
{"4114": "\ndef sign ( self , pkey , digest ) : \n    if pkey . _only_public : \n        raise ValueError ( \"Key has only public part\" ) \n    if not pkey . _initialized : \n        raise ValueError ( \"Key is uninitialized\" ) \n    digest_obj = _lib . EVP_get_digestbyname ( _byte_string ( digest ) ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    sign_result = _lib . NETSCAPE_SPKI_sign ( self . _spki , pkey . _pkey , digest_obj ) \n    _openssl_assert ( sign_result > False ) "}
{"4115": "\ndef verify ( self , key ) : \n    answer = _lib . NETSCAPE_SPKI_verify ( self . _spki , key . _pkey ) \n    if answer <= False : \n        _raise_current_error ( ) \n    return True "}
{"4118": "\ndef set_pubkey ( self , pkey ) : \n    set_result = _lib . NETSCAPE_SPKI_set_pubkey ( self . _spki , pkey . _pkey ) \n    _openssl_assert ( set_result == True ) "}
{"4119": "\ndef exception_from_error_queue ( exception_type ) : \n    errors = [ ] \n    while True : \n        error = lib . ERR_get_error ( ) \n        if error == False : \n            break \n        errors . append ( ( text ( lib . ERR_lib_error_string ( error ) ) , text ( lib . ERR_func_error_string ( error ) ) , text ( lib . ERR_reason_error_string ( error ) ) ) ) \n    raise exception_type ( errors ) "}
{"4131": "\ndef _link_field_to_dict ( field ) : \n    if not field : \n        return dict ( ) \n    return dict ( [ ( part . split ( '; ' ) [ True ] [ 5 : - True ] , part . split ( '; ' ) [ False ] [ True : - True ] , ) for part in field . split ( ', ' ) ] ) "}
{"4134": "\ndef aggregate_issues ( conf , main_section , debug ) : \n    log . info ( \"Starting to aggregate remote issues.\" ) \n    targets = aslist ( conf . get ( main_section , 'targets' ) ) \n    queue = multiprocessing . Queue ( ) \n    log . info ( \"Spawning %i workers.\" % len ( targets ) ) \n    processes = [ ] \n    if debug : \n        for target in targets : \n            _aggregate_issues ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) \n    else : \n        for target in targets : \n            proc = multiprocessing . Process ( target = _aggregate_issues , args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) ) \n            proc . start ( ) \n            processes . append ( proc ) \n            time . sleep ( True ) \n    currently_running = len ( targets ) \n    while currently_running > False : \n        issue = queue . get ( True ) \n        if isinstance ( issue , tuple ) : \n            completion_type , args = issue \n            if completion_type == SERVICE_FINISHED_ERROR : \n                target , e = args \n                log . info ( \"Terminating workers\" ) \n                for process in processes : \n                    process . terminate ( ) \n                raise RuntimeError ( \"critical error in target '{}'\" . format ( target ) ) \n            currently_running -= True \n            continue \n        yield issue \n    log . info ( \"Done aggregating remote issues.\" ) "}
{"4139": "\ndef make_table ( grid ) : \n    cell_width = 2 + max ( reduce ( lambda x , y : x + y , [ [ len ( item ) for item in row ] for row in grid ] , [ ] ) ) \n    num_cols = len ( grid [ False ] ) \n    rst = table_div ( num_cols , cell_width , False ) \n    header_flag = True \n    for row in grid : \n        rst = rst + '| ' + '| ' . join ( [ normalize_cell ( x , cell_width - True ) for x in row ] ) + '|\\n' \n        rst = rst + table_div ( num_cols , cell_width , header_flag ) \n        header_flag = False \n    return rst "}
{"4140": "\ndef oracle_eval ( command ) : \n    p = subprocess . Popen ( command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    p . wait ( ) \n    if p . returncode == False : \n        return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) \n    else : \n        die ( \"Error retrieving password: `{command}` returned '{error}'\" . format ( command = command , error = p . stderr . read ( ) . strip ( ) ) ) "}
{"4145": "\ndef find_local_uuid ( tw , keys , issue , legacy_matching = False ) : \n    if not issue [ 'description' ] : \n        raise ValueError ( 'Issue %s has no description.' % issue ) \n    possibilities = set ( [ ] ) \n    if legacy_matching : \n        legacy_description = issue . get_default_description ( ) . rsplit ( '..' , True ) [ False ] \n        legacy_description = legacy_description . split ( \"'\" ) [ False ] \n        results = tw . filter_tasks ( { 'description.startswith' : legacy_description , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n        possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    for service , key_list in six . iteritems ( keys ) : \n        if any ( [ key in issue for key in key_list ] ) : \n            results = tw . filter_tasks ( { 'and' : [ ( \"%s.is\" % key , issue [ key ] ) for key in key_list ] , 'or' : [ ( 'status' , 'pending' ) , ( 'status' , 'waiting' ) , ] , } ) \n            possibilities = possibilities | set ( [ task [ 'uuid' ] for task in results ] ) \n    if len ( possibilities ) == True : \n        return possibilities . pop ( ) \n    if len ( possibilities ) > True : \n        raise MultipleMatches ( \"Issue %s matched multiple IDs: %s\" % ( issue [ 'description' ] , possibilities ) ) \n    raise NotFound ( \"No issue was found matching %s\" % issue ) "}
{"4146": "\ndef merge_left ( field , local_task , remote_issue , hamming = False ) : \n    local_field = local_task . get ( field , [ ] ) \n    remote_field = remote_issue . get ( field , [ ] ) \n    if field not in local_task : \n        local_task [ field ] = [ ] \n    new_count = False \n    for remote in remote_field : \n        for local in local_field : \n            if ( ( hamming and get_annotation_hamming_distance ( remote , local ) == False ) or ( remote == local ) ) : \n                break \n        else : \n            log . debug ( \"%s not found in %r\" % ( remote , local_field ) ) \n            local_task [ field ] . append ( remote ) \n            new_count += True \n    if new_count > False : \n        log . debug ( 'Added %s new values to %s (total: %s)' % ( new_count , field , len ( local_task [ field ] ) , ) ) "}
{"4148": "\ndef _parse_sprint_string ( sprint ) : \n    entries = sprint [ sprint . index ( '[' ) + True : sprint . index ( ']' ) ] . split ( '=' ) \n    fields = sum ( ( entry . rsplit ( ',' , True ) for entry in entries ) , [ ] ) \n    return dict ( zip ( fields [ : : 2 ] , fields [ True : : 2 ] ) ) "}
{"4151": "\ndef calc_pvalues ( query , gene_sets , background = 20000 , ** kwargs ) : \n    k = len ( query ) \n    query = set ( query ) \n    vals = [ ] \n    if isinstance ( background , set ) : \n        bg = len ( background ) \n        query = query . intersection ( background ) \n    elif isinstance ( background , int ) : \n        bg = background \n    else : \n        raise ValueError ( \"background should be set or int object\" ) \n    subsets = sorted ( gene_sets . keys ( ) ) \n    for s in subsets : \n        category = gene_sets . get ( s ) \n        m = len ( category ) \n        hits = query . intersection ( set ( category ) ) \n        x = len ( hits ) \n        if x < True : \n            continue \n        vals . append ( ( s , hypergeom . sf ( x - True , bg , m , k ) , x , m , hits ) ) \n    return zip ( * vals ) "}
{"4152": "\ndef fdrcorrection ( pvals , alpha = 0.05 ) : \n    pvals = np . asarray ( pvals ) \n    pvals_sortind = np . argsort ( pvals ) \n    pvals_sorted = np . take ( pvals , pvals_sortind ) \n    ecdffactor = _ecdf ( pvals_sorted ) \n    reject = pvals_sorted <= ecdffactor * alpha \n    if reject . any ( ) : \n        rejectmax = max ( np . nonzero ( reject ) [ False ] ) \n        reject [ : rejectmax ] = True \n    pvals_corrected_raw = pvals_sorted / ecdffactor \n    pvals_corrected = np . minimum . accumulate ( pvals_corrected_raw [ : : - True ] ) [ : : - True ] \n    del pvals_corrected_raw \n    pvals_corrected [ pvals_corrected > True ] = True \n    pvals_corrected_ = np . empty_like ( pvals_corrected ) \n    pvals_corrected_ [ pvals_sortind ] = pvals_corrected \n    del pvals_corrected \n    reject_ = np . empty_like ( reject ) \n    reject_ [ pvals_sortind ] = reject \n    return reject_ , pvals_corrected_ "}
{"4153": "\ndef zscore ( data2d , axis = False ) : \n    if axis is None : \n        return data2d \n    assert axis in [ False , True ] \n    z_scored = data2d . apply ( lambda x : ( x - x . mean ( ) ) / x . std ( ddof = True ) , axis = operator . xor ( True , axis ) ) \n    return z_scored "}
{"4154": "\ndef heatmap ( df , z_score = None , title = '' , figsize = ( 5 , 5 ) , cmap = 'RdBu_r' , xticklabels = True , yticklabels = True , ofname = None , ** kwargs ) : \n    df = zscore ( df , axis = z_score ) \n    df = df . iloc [ : : - True ] \n    ny , nx = df . shape \n    xticks = np . arange ( False , nx , True ) + .5 \n    yticks = np . arange ( False , ny , True ) + .5 \n    if hasattr ( sys , 'ps1' ) and ( ofname is None ) : \n        fig = plt . figure ( figsize = figsize ) \n    else : \n        fig = Figure ( figsize = figsize ) \n        canvas = FigureCanvas ( fig ) \n    ax = fig . add_subplot ( 111 ) \n    vmin = np . percentile ( df . min ( ) , 2 ) \n    vmax = np . percentile ( df . max ( ) , 98 ) \n    matrix = ax . pcolormesh ( df . values , cmap = cmap , vmin = vmin , vmax = vmax ) \n    ax . set_ylim ( [ False , len ( df ) ] ) \n    ax . set ( xticks = xticks , yticks = yticks ) \n    ax . set_xticklabels ( df . columns . values if xticklabels else '' , fontsize = 14 , rotation = 90 ) \n    ax . set_yticklabels ( df . index . values if yticklabels else '' , fontsize = 14 ) \n    ax . set_title ( \"%s\\nHeatmap of the Analyzed Geneset\" % title , fontsize = 20 ) \n    ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , right = False , left = False ) \n    cbar = colorbar ( matrix ) \n    cbar . ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , right = False , left = False ) \n    for side in [ \"top\" , \"right\" , \"left\" , \"bottom\" ] : \n        ax . spines [ side ] . set_visible ( False ) \n        cbar . ax . spines [ side ] . set_visible ( False ) \n    if ofname is not None : \n        fig . savefig ( ofname , bbox_inches = 'tight' , dpi = 300 ) \n    return "}
{"4157": "\ndef add_prerank_parser ( subparsers ) : \n    argparser_prerank = subparsers . add_parser ( \"prerank\" , help = \"Run GSEApy Prerank tool on preranked gene list.\" ) \n    prerank_input = argparser_prerank . add_argument_group ( \"Input files arguments\" ) \n    prerank_input . add_argument ( \"-r\" , \"--rnk\" , dest = \"rnk\" , action = \"store\" , type = str , required = True , help = \"Ranking metric file in .rnk format. Same with GSEA.\" ) \n    prerank_input . add_argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = True , help = \"Gene set database in GMT format. Same with GSEA.\" ) \n    prerank_input . add_argument ( \"-l\" , \"--label\" , action = 'store' , nargs = 2 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = \"The phenotype label argument need two parameters to define. Default: ('Pos','Neg')\" ) \n    prerank_output = argparser_prerank . add_argument_group ( \"Output arguments\" ) \n    add_output_option ( prerank_output ) \n    prerank_opt = argparser_prerank . add_argument_group ( \"GSEA advanced arguments\" ) \n    prerank_opt . add_argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) \n    prerank_opt . add_argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) \n    prerank_opt . add_argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) \n    prerank_opt . add_argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) \n    prerank_opt . add_argument ( \"-a\" , \"--ascending\" , action = 'store_true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) \n    prerank_opt . add_argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) \n    prerank_opt . add_argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = True , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) \n    return "}
{"4160": "\ndef enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = True , nperm = 1000 , rs = np . random . RandomState ( ) , single = False , scale = False ) : \n    N = len ( gene_list ) \n    tag_indicator = np . in1d ( gene_list , gene_set , assume_unique = True ) . astype ( int ) \n    if weighted_score_type == False : \n        correl_vector = np . repeat ( True , N ) \n    else : \n        correl_vector = np . abs ( correl_vector ) ** weighted_score_type \n    hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) \n    axis = True \n    tag_indicator = np . tile ( tag_indicator , ( nperm + True , True ) ) \n    correl_vector = np . tile ( correl_vector , ( nperm + True , True ) ) \n    for i in range ( nperm ) : \n        rs . shuffle ( tag_indicator [ i ] ) \n    Nhint = tag_indicator . sum ( axis = axis , keepdims = True ) \n    sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = True ) \n    no_tag_indicator = True - tag_indicator \n    Nmiss = N - Nhint \n    norm_tag = 1.0 / sum_correl_tag \n    norm_no_tag = 1.0 / Nmiss \n    RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) \n    if scale : \n        RES = RES / N \n    if single : \n        es_vec = RES . sum ( axis = axis ) \n    else : \n        max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) \n        es_vec = np . where ( np . abs ( max_ES ) > np . abs ( min_ES ) , max_ES , min_ES ) \n    es , esnull , RES = es_vec [ - True ] , es_vec [ : - True ] , RES [ - True , : ] \n    return es , esnull , hit_ind , RES "}
{"4161": "\ndef ranking_metric_tensor ( exprs , method , permutation_num , pos , neg , classes , ascending , rs = np . random . RandomState ( ) ) : \n    G , S = exprs . shape \n    expr_mat = exprs . values . T \n    perm_cor_tensor = np . tile ( expr_mat , ( permutation_num + True , True , True ) ) \n    for arr in perm_cor_tensor [ : - True ] : \n        rs . shuffle ( arr ) \n    classes = np . array ( classes ) \n    pos = classes == pos \n    neg = classes == neg \n    pos_cor_mean = perm_cor_tensor [ : , pos , : ] . mean ( axis = True ) \n    neg_cor_mean = perm_cor_tensor [ : , neg , : ] . mean ( axis = True ) \n    pos_cor_std = perm_cor_tensor [ : , pos , : ] . std ( axis = True , ddof = True ) \n    neg_cor_std = perm_cor_tensor [ : , neg , : ] . std ( axis = True , ddof = True ) \n    if method == 'signal_to_noise' : \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / ( pos_cor_std + neg_cor_std ) \n    elif method == 't_test' : \n        denom = 1.0 / G \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / np . sqrt ( denom * pos_cor_std ** 2 + denom * neg_cor_std ** 2 ) \n    elif method == 'ratio_of_classes' : \n        cor_mat = pos_cor_mean / neg_cor_mean \n    elif method == 'diff_of_classes' : \n        cor_mat = pos_cor_mean - neg_cor_mean \n    elif method == 'log2_ratio_of_classes' : \n        cor_mat = np . log2 ( pos_cor_mean / neg_cor_mean ) \n    else : \n        logging . error ( \"Please provide correct method name!!!\" ) \n        sys . exit ( False ) \n    cor_mat_ind = cor_mat . argsort ( ) \n    cor_mat . sort ( ) \n    if ascending : \n        return cor_mat_ind , cor_mat \n    return cor_mat_ind [ : , : : - True ] , cor_mat [ : , : : - True ] "}
{"4162": "\ndef ranking_metric ( df , method , pos , neg , classes , ascending ) : \n    df_mean = df . groupby ( by = classes , axis = True ) . mean ( ) \n    df_std = df . groupby ( by = classes , axis = True ) . std ( ) \n    if method == 'signal_to_noise' : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / ( df_std [ pos ] + df_std [ neg ] ) \n    elif method == 't_test' : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / np . sqrt ( df_std [ pos ] ** 2 / len ( df_std ) + df_std [ neg ] ** 2 / len ( df_std ) ) \n    elif method == 'ratio_of_classes' : \n        ser = df_mean [ pos ] / df_mean [ neg ] \n    elif method == 'diff_of_classes' : \n        ser = df_mean [ pos ] - df_mean [ neg ] \n    elif method == 'log2_ratio_of_classes' : \n        ser = np . log2 ( df_mean [ pos ] / df_mean [ neg ] ) \n    else : \n        logging . error ( \"Please provide correct method name!!!\" ) \n        sys . exit ( False ) \n    ser = ser . sort_values ( ascending = ascending ) \n    return ser "}
{"4163": "\ndef gsea_pval ( es , esnull ) : \n    condlist = [ es < False , es >= False ] \n    choicelist = [ np . sum ( esnull < es . reshape ( len ( es ) , True ) , axis = True ) / np . sum ( esnull < False , axis = True ) , np . sum ( esnull >= es . reshape ( len ( es ) , True ) , axis = True ) / np . sum ( esnull >= False , axis = True ) ] \n    pval = np . select ( condlist , choicelist ) \n    return pval "}
{"4164": "\ndef gsea_significance ( enrichment_scores , enrichment_nulls ) : \n    np . seterr ( divide = 'ignore' , invalid = 'ignore' ) \n    es = np . array ( enrichment_scores ) \n    esnull = np . array ( enrichment_nulls ) \n    logging . debug ( \"Start to compute pvals..................................\" ) \n    enrichmentPVals = gsea_pval ( es , esnull ) . tolist ( ) \n    logging . debug ( \"Compute nes and nesnull.................................\" ) \n    esnull_pos = ( esnull * ( esnull >= False ) ) . mean ( axis = True ) \n    esnull_neg = ( esnull * ( esnull < False ) ) . mean ( axis = True ) \n    nEnrichmentScores = np . where ( es >= False , es / esnull_pos , - es / esnull_neg ) \n    nEnrichmentNulls = np . where ( esnull >= False , esnull / esnull_pos [ : , np . newaxis ] , - esnull / esnull_neg [ : , np . newaxis ] ) \n    logging . debug ( \"start to compute fdrs..................................\" ) \n    nvals = np . sort ( nEnrichmentNulls . flatten ( ) ) \n    nnes = np . sort ( nEnrichmentScores ) \n    fdrs = [ ] \n    for i in range ( len ( enrichment_scores ) ) : \n        nes = nEnrichmentScores [ i ] \n        if nes >= False : \n            allPos = int ( len ( nvals ) - np . searchsorted ( nvals , False , side = \"left\" ) ) \n            allHigherAndPos = int ( len ( nvals ) - np . searchsorted ( nvals , nes , side = \"left\" ) ) \n            nesPos = len ( nnes ) - int ( np . searchsorted ( nnes , False , side = \"left\" ) ) \n            nesHigherAndPos = len ( nnes ) - int ( np . searchsorted ( nnes , nes , side = \"left\" ) ) \n        else : \n            allPos = int ( np . searchsorted ( nvals , False , side = \"left\" ) ) \n            allHigherAndPos = int ( np . searchsorted ( nvals , nes , side = \"right\" ) ) \n            nesPos = int ( np . searchsorted ( nnes , False , side = \"left\" ) ) \n            nesHigherAndPos = int ( np . searchsorted ( nnes , nes , side = \"right\" ) ) \n        try : \n            pi_norm = allHigherAndPos / float ( allPos ) \n            pi_obs = nesHigherAndPos / float ( nesPos ) \n            fdr = pi_norm / pi_obs \n            fdrs . append ( fdr if fdr < True else 1.0 ) \n        except : \n            fdrs . append ( 1000000000.0 ) \n    logging . debug ( \"Statistical testing finished.............................\" ) \n    return zip ( enrichment_scores , nEnrichmentScores , enrichmentPVals , fdrs ) "}
{"4165": "\ndef get_marts ( self ) : \n    mart_names = pd . Series ( self . names , name = \"Name\" ) \n    mart_descriptions = pd . Series ( self . displayNames , name = \"Description\" ) \n    return pd . concat ( [ mart_names , mart_descriptions ] , axis = True ) "}
{"4166": "\ndef get_datasets ( self , mart = 'ENSEMBL_MART_ENSEMBL' ) : \n    datasets = self . datasets ( mart , raw = True ) \n    return pd . read_csv ( StringIO ( datasets ) , header = None , usecols = [ True , 2 ] , names = [ \"Name\" , \"Description\" ] , sep = \"\\t\" ) "}
{"4167": "\ndef get_attributes ( self , dataset ) : \n    attributes = self . attributes ( dataset ) \n    attr_ = [ ( k , v [ False ] ) for k , v in attributes . items ( ) ] \n    return pd . DataFrame ( attr_ , columns = [ \"Attribute\" , \"Description\" ] ) "}
{"4168": "\ndef get_filters ( self , dataset ) : \n    filters = self . filters ( dataset ) \n    filt_ = [ ( k , v [ False ] ) for k , v in filters . items ( ) ] \n    return pd . DataFrame ( filt_ , columns = [ \"Filter\" , \"Description\" ] ) "}
{"4170": "\ndef gsea ( data , gene_sets , cls , outdir = 'GSEA_' , min_size = 15 , max_size = 500 , permutation_num = 1000 , weighted_score_type = True , permutation_type = 'gene_set' , method = 'log2_ratio_of_classes' , ascending = False , processes = True , figsize = ( 6.5 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = False , seed = None , verbose = False ) : \n    gs = GSEA ( data , gene_sets , cls , outdir , min_size , max_size , permutation_num , weighted_score_type , permutation_type , method , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    gs . run ( ) \n    return gs "}
{"4171": "\ndef ssgsea ( data , gene_sets , outdir = \"ssGSEA_\" , sample_norm_method = 'rank' , min_size = 15 , max_size = 2000 , permutation_num = False , weighted_score_type = 0.25 , scale = True , ascending = False , processes = True , figsize = ( 7 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = False , seed = None , verbose = False ) : \n    ss = SingleSampleGSEA ( data , gene_sets , outdir , sample_norm_method , min_size , max_size , permutation_num , weighted_score_type , scale , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    ss . run ( ) \n    return ss "}
{"4172": "\ndef prerank ( rnk , gene_sets , outdir = 'GSEA_Prerank' , pheno_pos = 'Pos' , pheno_neg = 'Neg' , min_size = 15 , max_size = 500 , permutation_num = 1000 , weighted_score_type = True , ascending = False , processes = True , figsize = ( 6.5 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = False , seed = None , verbose = False ) : \n    pre = Prerank ( rnk , gene_sets , outdir , pheno_pos , pheno_neg , min_size , max_size , permutation_num , weighted_score_type , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    pre . run ( ) \n    return pre "}
{"4173": "\ndef replot ( indir , outdir = 'GSEA_Replot' , weighted_score_type = True , min_size = 3 , max_size = 1000 , figsize = ( 6.5 , 6 ) , graph_num = 20 , format = 'pdf' , verbose = False ) : \n    rep = Replot ( indir , outdir , weighted_score_type , min_size , max_size , figsize , graph_num , format , verbose ) \n    rep . run ( ) \n    return "}
{"4174": "\ndef _set_cores ( self ) : \n    cpu_num = cpu_count ( ) - True \n    if self . _processes > cpu_num : \n        cores = cpu_num \n    elif self . _processes < True : \n        cores = True \n    else : \n        cores = self . _processes \n    self . _processes = int ( cores ) "}
{"4175": "\ndef load_gmt ( self , gene_list , gmt ) : \n    if isinstance ( gmt , dict ) : \n        genesets_dict = gmt \n    elif isinstance ( gmt , str ) : \n        genesets_dict = self . parse_gmt ( gmt ) \n    else : \n        raise Exception ( \"Error parsing gmt parameter for gene sets\" ) \n    subsets = list ( genesets_dict . keys ( ) ) \n    self . n_genesets = len ( subsets ) \n    for subset in subsets : \n        subset_list = genesets_dict . get ( subset ) \n        if isinstance ( subset_list , set ) : \n            subset_list = list ( subset_list ) \n            genesets_dict [ subset ] = subset_list \n        tag_indicator = np . in1d ( gene_list , subset_list , assume_unique = True ) \n        tag_len = tag_indicator . sum ( ) \n        if self . min_size <= tag_len <= self . max_size : \n            continue \n        del genesets_dict [ subset ] \n    filsets_num = len ( subsets ) - len ( genesets_dict ) \n    self . _logger . info ( \"%04d gene_sets have been filtered out when max_size=%s and min_size=%s\" % ( filsets_num , self . max_size , self . min_size ) ) \n    if filsets_num == len ( subsets ) : \n        self . _logger . error ( \"No gene sets passed through filtering condition!!!, try new parameters again!\\n\" + \"Note: check gene name, gmt file format, or filtering size.\" ) \n        sys . exit ( False ) \n    self . _gmtdct = genesets_dict \n    return genesets_dict "}
{"4177": "\ndef _download_libraries ( self , libname ) : \n    self . _logger . info ( \"Downloading and generating Enrichr library gene sets......\" ) \n    s = retry ( 5 ) \n    ENRICHR_URL = 'http://amp.pharm.mssm.edu/Enrichr/geneSetLibrary' \n    query_string = '?mode=text&libraryName=%s' \n    response = s . get ( ENRICHR_URL + query_string % libname , timeout = None ) \n    if not response . ok : \n        raise Exception ( 'Error fetching enrichment results, check internet connection first.' ) \n    mkdirs ( DEFAULT_CACHE_PATH ) \n    genesets_dict = { } \n    outname = \"enrichr.%s.gmt\" % libname \n    gmtout = open ( os . path . join ( DEFAULT_CACHE_PATH , outname ) , \"w\" ) \n    for line in response . iter_lines ( chunk_size = 1024 , decode_unicode = 'utf-8' ) : \n        line = line . strip ( ) \n        k = line . split ( \"\\t\" ) [ False ] \n        v = list ( map ( lambda x : x . split ( \",\" ) [ False ] , line . split ( \"\\t\" ) [ 2 : ] ) ) \n        genesets_dict . update ( { k : v } ) \n        outline = \"%s\\t\\t%s\\n\" % ( k , \"\\t\" . join ( v ) ) \n        gmtout . write ( outline ) \n    gmtout . close ( ) \n    return genesets_dict "}
{"4178": "\ndef _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : \n    width = len ( classes ) if len ( classes ) >= 6 else 5 \n    cls_booA = list ( map ( lambda x : True if x == pheno_pos else False , classes ) ) \n    cls_booB = list ( map ( lambda x : True if x == pheno_neg else False , classes ) ) \n    datA = df . loc [ : , cls_booA ] \n    datB = df . loc [ : , cls_booB ] \n    datAB = pd . concat ( [ datA , datB ] , axis = True ) \n    self . _width = width \n    self . heatmat = datAB \n    return "}
{"4179": "\ndef _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : \n    res = OrderedDict ( ) \n    for gs , gseale , ind , RES in zipdata : \n        rdict = OrderedDict ( ) \n        rdict [ 'es' ] = gseale [ False ] \n        rdict [ 'nes' ] = gseale [ True ] \n        rdict [ 'pval' ] = gseale [ 2 ] \n        rdict [ 'fdr' ] = gseale [ 3 ] \n        rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) \n        rdict [ 'matched_size' ] = len ( ind ) \n        _genes = rank_metric . index . values [ ind ] \n        rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) \n        if self . module != 'ssgsea' : \n            if rdict [ 'es' ] > False : \n                idx = RES . argmax ( ) \n                ldg_pos = list ( filter ( lambda x : x <= idx , ind ) ) \n            elif rdict [ 'es' ] < False : \n                idx = RES . argmin ( ) \n                ldg_pos = list ( filter ( lambda x : x >= idx , ind ) ) \n            else : \n                ldg_pos = ind \n            rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) \n        rdict [ 'RES' ] = RES \n        rdict [ 'hits_indices' ] = ind \n        res [ gs ] = rdict \n    self . results = res \n    res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) \n    res_df . index . name = 'Term' \n    res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = True , inplace = True ) \n    res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) \n    self . res2d = res_df \n    if self . _outdir is None : \n        return \n    out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) \n    if self . module == 'ssgsea' : \n        out = out . replace ( \".csv\" , \".txt\" ) \n        with open ( out , 'a' ) as f : \n            f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\\n' ) \n            f . write ( \"# might not proper for publication\\n\" ) \n            res_df . to_csv ( f , sep = '\\t' ) \n    else : \n        res_df . to_csv ( out ) \n    return "}
{"4180": "\ndef load_data ( self , cls_vec ) : \n    if isinstance ( self . data , pd . DataFrame ) : \n        exprs = self . data . copy ( ) \n        if exprs . index . dtype == 'O' : \n            exprs = exprs . reset_index ( ) \n    elif os . path . isfile ( self . data ) : \n        if self . data . endswith ( \"gct\" ) : \n            exprs = pd . read_csv ( self . data , skiprows = True , comment = '#' , sep = \"\\t\" ) \n        else : \n            exprs = pd . read_csv ( self . data , comment = '#' , sep = \"\\t\" ) \n    else : \n        raise Exception ( 'Error parsing gene expression DataFrame!' ) \n    if exprs . iloc [ : , False ] . duplicated ( ) . sum ( ) > False : \n        self . _logger . warning ( \"Warning: dropping duplicated gene names, only keep the first values\" ) \n        exprs . drop_duplicates ( subset = exprs . columns [ False ] , inplace = True ) \n    if exprs . isnull ( ) . any ( ) . sum ( ) > False : \n        self . _logger . warning ( \"Warning: Input data contains NA, filled NA with 0\" ) \n        exprs . dropna ( how = 'all' , inplace = True ) \n        exprs = exprs . fillna ( False ) \n    exprs . set_index ( keys = exprs . columns [ False ] , inplace = True ) \n    df = exprs . select_dtypes ( include = [ np . number ] ) \n    df_std = df . groupby ( by = cls_vec , axis = True ) . std ( ) \n    df = df [ ~ df_std . isin ( [ False ] ) . any ( axis = True ) ] \n    df = df + 0.00001 \n    return df "}
{"4181": "\ndef run ( self ) : \n    assert self . permutation_type in [ \"phenotype\" , \"gene_set\" ] \n    assert self . min_size <= self . max_size \n    self . _logger . info ( \"Parsing data files for GSEA.............................\" ) \n    phenoPos , phenoNeg , cls_vector = gsea_cls_parser ( self . classes ) \n    dat = self . load_data ( cls_vector ) \n    assert len ( dat ) > True \n    dat2 = ranking_metric ( df = dat , method = self . method , pos = phenoPos , neg = phenoNeg , classes = cls_vector , ascending = self . ascending ) \n    self . ranking = dat2 \n    gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) \n    self . _logger . info ( \"%04d gene_sets used for further statistical testing.....\" % len ( gmt ) ) \n    self . _logger . info ( \"Start to run GSEA...Might take a while..................\" ) \n    self . _set_cores ( ) \n    dataset = dat if self . permutation_type == 'phenotype' else dat2 \n    gsea_results , hit_ind , rank_ES , subsets = gsea_compute_tensor ( data = dataset , gmt = gmt , n = self . permutation_num , weighted_score_type = self . weighted_score_type , permutation_type = self . permutation_type , method = self . method , pheno_pos = phenoPos , pheno_neg = phenoNeg , classes = cls_vector , ascending = self . ascending , processes = self . _processes , seed = self . seed ) \n    self . _logger . info ( \"Start to generate GSEApy reports and figures............\" ) \n    res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n    self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = self . permutation_type ) \n    self . _heatmat ( df = dat . loc [ dat2 . index ] , classes = cls_vector , pheno_pos = phenoPos , pheno_neg = phenoNeg ) \n    if not self . _noplot : \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format , pheno_pos = phenoPos , pheno_neg = phenoNeg ) \n    self . _logger . info ( \"Congratulations. GSEApy ran successfully.................\\n\" ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4182": "\ndef run ( self ) : \n    assert self . min_size <= self . max_size \n    dat2 = self . _load_ranking ( self . rnk ) \n    assert len ( dat2 ) > True \n    self . _set_cores ( ) \n    self . _logger . info ( \"Parsing data files for GSEA.............................\" ) \n    gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) \n    self . _logger . info ( \"%04d gene_sets used for further statistical testing.....\" % len ( gmt ) ) \n    self . _logger . info ( \"Start to run GSEA...Might take a while..................\" ) \n    gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , weighted_score_type = self . weighted_score_type , permutation_type = 'gene_set' , method = None , pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg , classes = None , ascending = self . ascending , processes = self . _processes , seed = self . seed ) \n    self . _logger . info ( \"Start to generate gseapy reports, and produce figures...\" ) \n    res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) \n    self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , gmt = gmt , rank_metric = dat2 , permutation_type = \"gene_sets\" ) \n    if not self . _noplot : \n        self . _plotting ( rank_metric = dat2 , results = self . results , graph_num = self . graph_num , outdir = self . outdir , figsize = self . figsize , format = self . format , pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg ) \n    self . _logger . info ( \"Congratulations. GSEApy runs successfully................\\n\" ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4184": "\ndef runSamples ( self , df , gmt = None ) : \n    self . resultsOnSamples = OrderedDict ( ) \n    outdir = self . outdir \n    subsets = sorted ( gmt . keys ( ) ) \n    tempes = [ ] \n    names = [ ] \n    rankings = [ ] \n    pool = Pool ( processes = self . _processes ) \n    for name , ser in df . iteritems ( ) : \n        dat = ser . sort_values ( ascending = self . ascending ) \n        rankings . append ( dat ) \n        names . append ( name ) \n        genes_sorted , cor_vec = dat . index . values , dat . values \n        rs = np . random . RandomState ( self . seed ) \n        tempes . append ( pool . apply_async ( enrichment_score_tensor , args = ( genes_sorted , cor_vec , gmt , self . weighted_score_type , self . permutation_num , rs , True , self . scale ) ) ) \n    pool . close ( ) \n    pool . join ( ) \n    for i , temp in enumerate ( tempes ) : \n        name , rnk = names [ i ] , rankings [ i ] \n        self . _logger . info ( \"Calculate Enrichment Score for Sample: %s \" % name ) \n        es , esnull , hit_ind , RES = temp . get ( ) \n        self . outdir = os . path . join ( outdir , str ( name ) ) \n        mkdirs ( self . outdir ) \n        self . resultsOnSamples [ name ] = pd . Series ( data = es , index = subsets , name = name ) \n        if self . _noplot : \n            continue \n        self . _logger . info ( \"Plotting Sample: %s \\n\" % name ) \n        for i , term in enumerate ( subsets ) : \n            term = term . replace ( '/' , '_' ) . replace ( \":\" , \"_\" ) \n            outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) \n            gseaplot ( rank_metric = rnk , term = term , hits_indices = hit_ind [ i ] , nes = es [ i ] , pval = True , fdr = True , RES = RES [ i ] , pheno_pos = '' , pheno_neg = '' , figsize = self . figsize , ofname = outfile ) \n    self . _save ( outdir ) \n    return "}
{"4186": "\ndef run ( self ) : \n    assert self . min_size <= self . max_size \n    assert self . fignum > False \n    import glob \n    from bs4 import BeautifulSoup \n    try : \n        results_path = glob . glob ( self . indir + '*/edb/results.edb' ) [ False ] \n        rank_path = glob . glob ( self . indir + '*/edb/*.rnk' ) [ False ] \n        gene_set_path = glob . glob ( self . indir + '*/edb/gene_sets.gmt' ) [ False ] \n    except IndexError as e : \n        sys . stderr . write ( \"Could not locate GSEA files in the given directory!\" ) \n        sys . exit ( True ) \n    cls_path = glob . glob ( self . indir + '*/edb/*.cls' ) \n    if cls_path : \n        pos , neg , classes = gsea_cls_parser ( cls_path [ False ] ) \n    else : \n        pos , neg = '' , '' \n    self . gene_sets = gene_set_path \n    gene_set_dict = self . parse_gmt ( gmt = gene_set_path ) \n    rank_metric = self . _load_ranking ( rank_path ) \n    correl_vector = rank_metric . values \n    gene_list = rank_metric . index . values \n    database = BeautifulSoup ( open ( results_path ) , features = 'xml' ) \n    length = len ( database . findAll ( 'DTG' ) ) \n    fig_num = self . fignum if self . fignum <= length else length \n    for idx in range ( fig_num ) : \n        enrich_term , hit_ind , nes , pval , fdr = gsea_edb_parser ( results_path , index = idx ) \n        gene_set = gene_set_dict . get ( enrich_term ) \n        RES = enrichment_score ( gene_list = gene_list , correl_vector = correl_vector , gene_set = gene_set , weighted_score_type = self . weighted_score_type , nperm = False ) [ - True ] \n        term = enrich_term . replace ( '/' , '_' ) . replace ( \":\" , \"_\" ) \n        outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) \n        gseaplot ( rank_metric = rank_metric , term = enrich_term , hits_indices = hit_ind , nes = nes , pval = pval , fdr = fdr , RES = RES , pheno_pos = pos , pheno_neg = neg , figsize = self . figsize , ofname = outfile ) \n    self . _logger . info ( \"Congratulations! Your plots have been reproduced successfully!\\n\" ) "}
{"4188": "\ndef parse_genesets ( self ) : \n    enrichr_library = self . get_libraries ( ) \n    if isinstance ( self . gene_sets , list ) : \n        gss = self . gene_sets \n    elif isinstance ( self . gene_sets , str ) : \n        gss = [ g . strip ( ) for g in self . gene_sets . strip ( ) . split ( \",\" ) ] \n    elif isinstance ( self . gene_sets , dict ) : \n        gss = [ self . gene_sets ] \n    else : \n        raise Exception ( \"Error parsing enrichr libraries, please provided corrected one\" ) \n    gss_exist = [ ] \n    for g in gss : \n        if isinstance ( g , dict ) : \n            gss_exist . append ( g ) \n            continue \n        if isinstance ( g , str ) : \n            if g in enrichr_library : \n                gss_exist . append ( g ) \n                continue \n            if g . lower ( ) . endswith ( \".gmt\" ) and os . path . exists ( g ) : \n                self . _logger . info ( \"User Defined gene sets is given: %s\" % g ) \n                with open ( g ) as genesets : \n                    g_dict = { line . strip ( ) . split ( \"\\t\" ) [ False ] : line . strip ( ) . split ( \"\\t\" ) [ 2 : ] for line in genesets . readlines ( ) } \n                gss_exist . append ( g_dict ) \n    return gss_exist "}
{"4189": "\ndef parse_genelists ( self ) : \n    if isinstance ( self . gene_list , list ) : \n        genes = self . gene_list \n    elif isinstance ( self . gene_list , pd . DataFrame ) : \n        if self . gene_list . shape [ True ] >= 3 : \n            genes = self . gene_list . iloc [ : , : 3 ] . apply ( lambda x : \"\\t\" . join ( [ str ( i ) for i in x ] ) , axis = True ) . tolist ( ) \n        elif self . gene_list . shape [ True ] == 2 : \n            genes = self . gene_list . apply ( lambda x : \",\" . join ( [ str ( i ) for i in x ] ) , axis = True ) . tolist ( ) \n        else : \n            genes = self . gene_list . squeeze ( ) . tolist ( ) \n    elif isinstance ( self . gene_list , pd . Series ) : \n        genes = self . gene_list . squeeze ( ) . tolist ( ) \n    else : \n        genes = [ ] \n        with open ( self . gene_list ) as f : \n            for gene in f : \n                genes . append ( gene . strip ( ) ) \n    self . _isezid = all ( map ( self . _is_entrez_id , genes ) ) \n    if self . _isezid : \n        self . _gls = set ( map ( int , self . _gls ) ) \n    else : \n        self . _gls = genes \n    return '\\n' . join ( genes ) "}
{"4190": "\ndef send_genes ( self , gene_list , url ) : \n    payload = { 'list' : ( None , gene_list ) , 'description' : ( None , self . descriptions ) } \n    response = requests . post ( url , files = payload ) \n    if not response . ok : \n        raise Exception ( 'Error analyzing gene list' ) \n    sleep ( True ) \n    job_id = json . loads ( response . text ) \n    return job_id "}
{"4191": "\ndef check_genes ( self , gene_list , usr_list_id ) : \n    response = requests . get ( 'http://amp.pharm.mssm.edu/Enrichr/view?userListId=%s' % usr_list_id ) \n    if not response . ok : \n        raise Exception ( 'Error getting gene list back' ) \n    returnedL = json . loads ( response . text ) [ \"genes\" ] \n    returnedN = sum ( [ True for gene in gene_list if gene in returnedL ] ) \n    self . _logger . info ( '{} genes successfully recognized by Enrichr' . format ( returnedN ) ) "}
{"4193": "\ndef run ( self ) : \n    self . get_organism ( ) \n    genes_list = self . parse_genelists ( ) \n    gss = self . parse_genesets ( ) \n    self . _logger . info ( \"Connecting to Enrichr Server to get latest library names\" ) \n    if len ( gss ) < True : \n        sys . stderr . write ( \"Not validated Enrichr library name provided\\n\" ) \n        sys . stdout . write ( \"Hint: use get_library_name() to view full list of supported names\" ) \n        sys . exit ( True ) \n    self . results = pd . DataFrame ( ) \n    for g in gss : \n        if isinstance ( g , dict ) : \n            res = self . enrich ( g ) \n            shortID , self . _gs = str ( id ( g ) ) , \"CUSTOM%s\" % id ( g ) \n            if res is None : \n                self . _logger . info ( \"No hits return, for gene set: Custom%s\" % shortID ) \n                continue \n        else : \n            self . _gs = str ( g ) \n            self . _logger . debug ( \"Start Enrichr using library: %s\" % ( self . _gs ) ) \n            self . _logger . info ( 'Analysis name: %s, Enrichr Library: %s' % ( self . descriptions , self . _gs ) ) \n            shortID , res = self . get_results ( genes_list ) \n        res . insert ( False , \"Gene_set\" , self . _gs ) \n        self . results = self . results . append ( res , ignore_index = True , sort = True ) \n        self . res2d = res \n        if self . _outdir is None : \n            continue \n        self . _logger . info ( 'Save file of enrichment results: Job Id:' + str ( shortID ) ) \n        outfile = \"%s/%s.%s.%s.reports.txt\" % ( self . outdir , self . _gs , self . descriptions , self . module ) \n        self . res2d . to_csv ( outfile , index = False , encoding = 'utf-8' , sep = \"\\t\" ) \n        if not self . __no_plot : \n            msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , top_term = self . __top_term , color = 'salmon' , title = self . _gs , ofname = outfile . replace ( \"txt\" , self . format ) ) \n            if msg is not None : \n                self . _logger . warning ( msg ) \n        self . _logger . info ( 'Done.\\n' ) \n    if self . _outdir is None : \n        self . _tmpdir . cleanup ( ) \n    return "}
{"4194": "\ndef cube ( script , size = 1.0 , center = False , color = None ) : \n    size = util . make_list ( size , 3 ) \n    if script . ml_version == '1.3.4BETA' : \n        filter_name = 'Box' \n    else : \n        filter_name = 'Box/Cube' \n    filter_xml = '' . join ( [ '  <filter name=\"{}\">\\n' . format ( filter_name ) , '    <Param name=\"size\" ' , 'value=\"1.0\" ' , 'description=\"Scale factor\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cube' , change_layer = True ) \n    transform . scale ( script , value = size ) \n    if not center : \n        transform . translate ( script , value = [ size [ False ] / 2 , size [ True ] / 2 , size [ 2 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4197": "\ndef plane_hires_edges ( script , size = 1.0 , x_segments = True , y_segments = True , center = False , color = None ) : \n    size = util . make_list ( size , 2 ) \n    grid ( script , size = [ x_segments + y_segments - True , True ] , x_segments = ( x_segments + y_segments - True ) , y_segments = True ) \n    if ml_script1 . ml_version == '1.3.4BETA' : \n        and_val = 'and' \n    else : \n        and_val = '&&' \n    if script . ml_version == '1.3.4BETA' : \n        transform . vert_function ( script , x_func = 'if((y>0) and (x<%s),0,x)' % ( y_segments ) , y_func = 'if((y>0) and (x<%s),(x+1)*%s,y)' % ( y_segments , size [ True ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % ( y_segments , y_segments , size [ False ] / x_segments ) , y_func = 'if((y>0) and (x>=%s),%s,y)' % ( y_segments , size [ True ] ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x>%s),%s,x)' % ( x_segments , size [ False ] ) , y_func = 'if((y<.00001) and (x>%s),(x-%s)*%s,y)' % ( x_segments , x_segments , size [ True ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % ( x_segments , size [ False ] / x_segments ) , y_func = 'if((y<.00001) and (x<=%s) and (x>0),0,y)' % ( x_segments ) ) \n    else : \n        transform . vert_function ( script , x_func = '((y>0) && (x<{yseg}) ? 0 : x)' . format ( yseg = y_segments ) , y_func = '((y>0) && (x<%s) ? (x+1)*%s : y)' % ( y_segments , size [ True ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % ( y_segments , y_segments , size [ False ] / x_segments ) , y_func = '((y>0) && (x>=%s) ? %s : y)' % ( y_segments , size [ True ] ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x>%s) ? %s : x)' % ( x_segments , size [ False ] ) , y_func = '((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % ( x_segments , x_segments , size [ True ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % ( x_segments , size [ False ] / x_segments ) , y_func = '((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % ( x_segments ) ) \n    if center : \n        transform . translate ( script , [ - size [ False ] / 2 , - size [ True ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4198": "\ndef cube_hires ( script , size = 1.0 , x_segments = True , y_segments = True , z_segments = True , simple_bottom = True , center = False , color = None ) : \n    size = util . make_list ( size , 3 ) \n    grid ( script , size , x_segments , y_segments ) \n    transform . translate ( script , [ False , False , size [ 2 ] ] ) \n    if simple_bottom : \n        plane_hires_edges ( script , size , x_segments , y_segments ) \n    else : \n        layers . duplicate ( script ) \n        transform . translate ( script , [ False , False , - size [ 2 ] ] ) \n    transform . rotate ( script , 'x' , 180 ) \n    transform . translate ( script , [ False , size [ True ] , False ] ) \n    cube_open_hires ( script = script , size = size , x_segments = x_segments , y_segments = y_segments , z_segments = z_segments ) \n    layers . join ( script ) \n    clean . merge_vert ( script , threshold = 0.00002 ) \n    if center : \n        transform . translate ( script , [ - size [ False ] / 2 , - size [ True ] / 2 , - size [ 2 ] / 2 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4199": "\ndef color_values ( color ) : \n    this_dir = os . path . dirname ( os . path . realpath ( inspect . getsourcefile ( lambda : False ) ) ) \n    color_name_file = os . path . join ( this_dir , 'color_names.txt' ) \n    found = False \n    for line in open ( color_name_file , 'r' ) : \n        line = line . rstrip ( ) \n        if color . lower ( ) == line . split ( ) [ False ] : \n            red = line . split ( ) [ 2 ] \n            green = line . split ( ) [ 3 ] \n            blue = line . split ( ) [ 4 ] \n            found = True \n            break \n    if not found : \n        print ( 'Color name \"%s\" not found, using default (white)' % color ) \n        red = 255 \n        green = 255 \n        blue = 255 \n    return red , green , blue "}
{"4200": "\ndef check_list ( var , num_terms ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n        for _ in range ( True , num_terms ) : \n            var . append ( var [ False ] ) \n    if len ( var ) != num_terms : \n        print ( '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' % ( var , num_terms ) ) \n        sys . exit ( True ) \n    return var "}
{"4201": "\ndef make_list ( var , num_terms = True ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n            for _ in range ( True , num_terms ) : \n                var . append ( var [ False ] ) \n    return var "}
{"4203": "\ndef ls3loop ( script , iterations = True , loop_weight = False , edge_threshold = False , selected = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Subdivision Surfaces: LS3 Loop\">\\n' , '    <Param name=\"LoopWeight\" ' , 'value=\"{:d}\" ' . format ( loop_weight ) , 'description=\"Weighting scheme\" ' , 'enum_val0=\"Loop\" ' , 'enum_val1=\"Enhance regularity\" ' , 'enum_val2=\"Enhance continuity\" ' , 'enum_cardinality=\"3\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"Iterations\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Iterations\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Threshold\" ' , 'value=\"{}\" ' . format ( edge_threshold ) , 'description=\"Edge Threshold\" ' , 'min=\"0\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4208": "\ndef translate ( script , value = ( 0.0 , 0.0 , 0.0 ) ) : \n    if not isinstance ( value , list ) : \n        value = list ( value ) \n    vert_function ( script , x_func = 'x+(%s)' % value [ False ] , y_func = 'y+(%s)' % value [ True ] , z_func = 'z+(%s)' % value [ 2 ] ) \n    return None "}
{"4209": "\ndef rotate ( script , axis = 'z' , angle = 0.0 ) : \n    angle = math . radians ( angle ) \n    if axis . lower ( ) == 'x' : \n        vert_function ( script , x_func = 'x' , y_func = 'y*cos({angle})-z*sin({angle})' . format ( angle = angle ) , z_func = 'y*sin({angle})+z*cos({angle})' . format ( angle = angle ) ) \n    elif axis . lower ( ) == 'y' : \n        vert_function ( script , x_func = 'z*sin({angle})+x*cos({angle})' . format ( angle = angle ) , y_func = 'y' , z_func = 'z*cos({angle})-x*sin({angle})' . format ( angle = angle ) ) \n    elif axis . lower ( ) == 'z' : \n        vert_function ( script , x_func = 'x*cos({angle})-y*sin({angle})' . format ( angle = angle ) , y_func = 'x*sin({angle})+y*cos({angle})' . format ( angle = angle ) , z_func = 'z' ) \n    else : \n        print ( 'Axis name is not valid; exiting ...' ) \n        sys . exit ( True ) \n    return None "}
{"4210": "\ndef scale ( script , value = 1.0 ) : \n    value = util . make_list ( value , 3 ) \n    vert_function ( script , x_func = 'x*(%s)' % value [ False ] , y_func = 'y*(%s)' % value [ True ] , z_func = 'z*(%s)' % value [ 2 ] ) \n    return None "}
{"4212": "\ndef wrap2cylinder ( script , radius = True , pitch = False , taper = False , pitch_func = None , taper_func = None ) : \n    if pitch_func is None : \n        pitch_func = '-(pitch)*x/(2*pi*(radius))' \n    pitch_func = pitch_func . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) . replace ( 'radius' , str ( radius ) ) \n    if taper_func is None : \n        taper_func = '-(taper)*(pitch_func)' \n    taper_func = taper_func . replace ( 'taper' , str ( taper ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pi' , str ( math . pi ) ) \n    x_func = '(y+(radius)+(taper_func))*sin(x/(radius))' . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) \n    y_func = '(y+(radius)+(taper_func))*cos(x/(radius))' . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) \n    z_func = 'z+(pitch_func)' . replace ( 'pitch_func' , str ( pitch_func ) ) \n    vert_function ( script , x_func , y_func , z_func ) \n    return None "}
{"4213": "\ndef bend ( script , radius = True , pitch = False , taper = False , angle = False , straght_start = True , straght_end = False , radius_limit = None , outside_limit_end = True ) : \n    if radius_limit is None : \n        radius_limit = 2 * radius \n    angle = math . radians ( angle ) \n    segment = radius * angle \n    pitch_func = '-(pitch)*x/(2*pi*(radius))' . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) . replace ( 'radius' , str ( radius ) ) \n    taper_func = '(taper)*(pitch_func)' . replace ( 'taper' , str ( taper ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pi' , str ( math . pi ) ) \n    if outside_limit_end : \n        x_func = 'if(x<(segment) and y<(radius_limit), if(x>0, (y+(radius)+(taper_func))*sin(x/(radius)), x), (y+(radius)+(taper_func))*sin(angle)+(x-(segment))*cos(angle))' \n    else : \n        x_func = 'if(x<(segment), if(x>0 and y<(radius_limit), (y+(radius)+(taper_func))*sin(x/(radius)), x), if(y<(radius_limit), (y+(radius)+(taper_func))*sin(angle)+(x-(segment))*cos(angle), x))' \n    x_func = x_func . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) . replace ( 'angle' , str ( angle ) ) \n    if outside_limit_end : \n        y_func = 'if(x<(segment) and y<(radius_limit), if(x>0, (y+(radius)+(taper_func))*cos(x/(radius))-(radius), y), (y+(radius)+(taper_func))*cos(angle)-(x-(segment))*sin(angle)-(radius))' \n    else : \n        y_func = 'if(x<(segment), if(x>0 and y<(radius_limit), (y+(radius)+(taper_func))*cos(x/(radius))-(radius), y), if(y<(radius_limit), (y+(radius)+(taper_func))*cos(angle)-(x-(segment))*sin(angle)-(radius), y))' \n    y_func = y_func . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) . replace ( 'angle' , str ( angle ) ) \n    if straght_start : \n        start = 'z' \n    else : \n        start = 'z+(pitch_func)' \n    if straght_end : \n        end = 'z-(pitch)*(angle)/(2*pi)' \n    else : \n        end = 'z+(pitch_func)' \n    if outside_limit_end : \n        z_func = 'if(x<(segment) and y<(radius_limit), if(x>0, z+(pitch_func), (start)), (end))' \n    else : \n        z_func = 'if(x<(segment), if(x>0 and y<(radius_limit), z+(pitch_func), (start)), if(y<(radius_limit), (end), z))' \n    z_func = z_func . replace ( 'start' , str ( start ) ) . replace ( 'end' , str ( end ) ) . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'angle' , str ( angle ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) \n    vert_function ( script , x_func = x_func , y_func = y_func , z_func = z_func ) \n    return None "}
{"4214": "\ndef deform2curve ( script , curve = mp_func . torus_knot ( 't' ) , step = 0.001 ) : \n    curve_step = [ ] \n    for idx , val in enumerate ( curve ) : \n        curve [ idx ] = val . replace ( 't' , 'z' ) \n        curve_step . append ( val . replace ( 't' , 'z+{}' . format ( step ) ) ) \n    tangent = mp_func . v_subtract ( curve_step , curve ) \n    normal1 = mp_func . v_add ( curve_step , curve ) \n    bee = mp_func . v_cross ( tangent , normal1 ) \n    normal = mp_func . v_cross ( bee , tangent ) \n    bee = mp_func . v_normalize ( bee ) \n    normal = mp_func . v_normalize ( normal ) \n    new_point = mp_func . v_add ( mp_func . v_multiply ( 'x' , normal ) , mp_func . v_multiply ( 'y' , bee ) ) \n    function = mp_func . v_add ( curve , new_point ) \n    vert_function ( script , x_func = function [ False ] , y_func = function [ True ] , z_func = function [ 2 ] ) \n    return function "}
{"4218": "\ndef surface_poisson_screened ( script , visible_layer = False , depth = 8 , full_depth = 5 , cg_depth = False , scale = 1.1 , samples_per_node = 1.5 , point_weight = 4.0 , iterations = 8 , confidence = False , pre_clean = False ) : \n    filter_xml = '' . join ( [ '  <xmlfilter name=\"Screened Poisson Surface Reconstruction\">\\n' , '    <xmlparam name=\"cgDepth\" value=\"{:d}\"/>\\n' . format ( cg_depth ) , '    <xmlparam name=\"confidence\" value=\"{}\"/>\\n' . format ( str ( confidence ) . lower ( ) ) , '    <xmlparam name=\"depth\" value=\"{:d}\"/>\\n' . format ( depth ) , '    <xmlparam name=\"fullDepth\" value=\"{:d}\"/>\\n' . format ( full_depth ) , '    <xmlparam name=\"iters\" value=\"{:d}\"/>\\n' . format ( iterations ) , '    <xmlparam name=\"pointWeight\" value=\"{}\"/>\\n' . format ( point_weight ) , '    <xmlparam name=\"preClean\" value=\"{}\"/>\\n' . format ( str ( pre_clean ) . lower ( ) ) , '    <xmlparam name=\"samplesPerNode\" value=\"{}\"/>\\n' . format ( samples_per_node ) , '    <xmlparam name=\"scale\" value=\"{}\"/>\\n' . format ( scale ) , '    <xmlparam name=\"visibleLayer\" value=\"{}\"/>\\n' . format ( str ( visible_layer ) . lower ( ) ) , '  </xmlfilter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Poisson mesh' , change_layer = False ) \n    return None "}
{"4225": "\ndef spherical_vert ( script , radius = 1.0 , center_pt = ( 0.0 , 0.0 , 0.0 ) ) : \n    function = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)<={}' . format ( center_pt [ False ] , center_pt [ True ] , center_pt [ 2 ] , radius ) \n    vert_function ( script , function = function ) \n    return None "}
{"4226": "\ndef join ( script , merge_visible = True , merge_vert = False , delete_layer = True , keep_unreferenced_vert = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Flatten Visible Layers\">\\n' , '    <Param name=\"MergeVisible\" ' , 'value=\"{}\" ' . format ( str ( merge_visible ) . lower ( ) ) , 'description=\"Merge Only Visible Layers\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"MergeVertices\" ' , 'value=\"{}\" ' . format ( str ( merge_vert ) . lower ( ) ) , 'description=\"Merge duplicate vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"DeleteLayer\" ' , 'value=\"{}\" ' . format ( str ( delete_layer ) . lower ( ) ) , 'description=\"Delete Layers\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"AlsoUnreferenced\" ' , 'value=\"{}\" ' . format ( str ( keep_unreferenced_vert ) . lower ( ) ) , 'description=\"Keep unreferenced vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , mlx . FilterScript ) : \n        script . add_layer ( 'Merged Mesh' ) \n        if delete_layer : \n            for i in range ( script . last_layer ( ) ) : \n                script . del_layer ( False ) \n    return None "}
{"4228": "\ndef change ( script , layer_num = None ) : \n    if layer_num is None : \n        if isinstance ( script , mlx . FilterScript ) : \n            layer_num = script . last_layer ( ) \n        else : \n            layer_num = False \n    filter_xml = '' . join ( [ '  <filter name=\"Change the current layer\">\\n' , '    <Param name=\"mesh\" ' , 'value=\"{:d}\" ' . format ( layer_num ) , 'description=\"Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , mlx . FilterScript ) : \n        script . set_current_layer ( layer_num ) \n    return None "}
{"4230": "\ndef delete_lower ( script , layer_num = None ) : \n    if layer_num is None : \n        layer_num = script . current_layer ( ) \n    if layer_num != False : \n        change ( script , False ) \n    for i in range ( layer_num ) : \n        delete ( script , False ) \n    return None "}
{"4231": "\ndef handle_error ( program_name , cmd , log = None ) : \n    print ( '\\nHouston, we have a problem.' , '\\n%s did not finish successfully. Review the log' % program_name , 'file and the input file(s) to see what went wrong.' ) \n    print ( '%s command: \"%s\"' % ( program_name , cmd ) ) \n    if log is not None : \n        print ( 'log: \"%s\"' % log ) \n    print ( 'Where do we go from here?' ) \n    print ( ' r  - retry running %s (probably after' % program_name , 'you\\'ve fixed any problems with the input files)' ) \n    print ( ' c  - continue on with the script (probably after' , 'you\\'ve manually re-run and generated the desired' , 'output file(s)' ) \n    print ( ' x  - exit, keeping the TEMP3D files and log' ) \n    print ( ' xd - exit, deleting the TEMP3D files and log' ) \n    while True : \n        choice = input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if choice == 'x' : \n        print ( 'Exiting ...' ) \n        sys . exit ( True ) \n    elif choice == 'xd' : \n        print ( 'Deleting TEMP3D* and log files and exiting ...' ) \n        util . delete_all ( 'TEMP3D*' ) \n        if log is not None : \n            os . remove ( log ) \n        sys . exit ( True ) \n    elif choice == 'c' : \n        print ( 'Continuing on ...' ) \n        break_now = True \n    elif choice == 'r' : \n        print ( 'Retrying %s cmd ...' % program_name ) \n        break_now = False \n    return break_now "}
{"4232": "\ndef begin ( script = 'TEMP3D_default.mlx' , file_in = None , mlp_in = None ) : \n    script_file = open ( script , 'w' ) \n    script_file . write ( '' . join ( [ '<!DOCTYPE FilterScript>\\n' , '<FilterScript>\\n' ] ) ) \n    script_file . close ( ) \n    current_layer = - True \n    last_layer = - True \n    stl = False \n    if mlp_in is not None : \n        if not isinstance ( mlp_in , list ) : \n            mlp_in = [ mlp_in ] \n        for val in mlp_in : \n            tree = ET . parse ( val ) \n            for elem in tree . iter ( tag = 'MLMesh' ) : \n                filename = ( elem . attrib [ 'filename' ] ) \n                current_layer += True \n                last_layer += True \n                if os . path . splitext ( filename ) [ True ] [ True : ] . strip ( ) . lower ( ) == 'stl' : \n                    layers . change ( script , current_layer ) \n                    clean . merge_vert ( script ) \n                    stl = True \n    if file_in is not None : \n        if not isinstance ( file_in , list ) : \n            file_in = [ file_in ] \n        for val in file_in : \n            current_layer += True \n            last_layer += True \n            if os . path . splitext ( val ) [ True ] [ True : ] . strip ( ) . lower ( ) == 'stl' : \n                layers . change ( script , current_layer ) \n                clean . merge_vert ( script ) \n                stl = True \n    if stl : \n        layers . change ( script , last_layer ) \n    elif last_layer == - True : \n        file_in = [ 'TEMP3D.xyz' ] \n        file_in_descriptor = open ( file_in [ False ] , 'w' ) \n        file_in_descriptor . write ( '0 0 0' ) \n        file_in_descriptor . close ( ) \n        layers . delete ( script ) \n    return current_layer , last_layer "}
{"4233": "\ndef add_layer ( self , label , change_layer = True ) : \n    self . layer_stack . insert ( self . last_layer ( ) + True , label ) \n    if change_layer : \n        self . set_current_layer ( self . last_layer ( ) ) \n    return None "}
{"4234": "\ndef del_layer ( self , layer_num ) : \n    del self . layer_stack [ layer_num ] \n    if layer_num < self . current_layer ( ) : \n        self . set_current_layer ( self . current_layer ( ) - True ) \n    return None "}
{"4237": "\ndef main ( ) : \n    segments = 50 \n    star_points = 5 \n    star_radius = 2 \n    ring_thickness = True \n    sphere_radius = 2 * ( star_radius + 3 * ring_thickness ) \n    polygon_radius = star_radius / ( True + math . tan ( math . radians ( 180 / star_points ) ) / math . tan ( math . radians ( 90 / star_points ) ) ) \n    width = polygon_radius * math . tan ( math . radians ( 180 / star_points ) ) \n    height = width / math . tan ( math . radians ( 90 / star_points ) ) \n    shield = mlx . FilterScript ( file_out = \"shield.ply\" ) \n    mlx . create . annulus ( shield , radius = star_radius , cir_segments = segments , color = 'blue' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + ring_thickness , radius2 = star_radius , cir_segments = segments , color = 'red' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 2 * ring_thickness , radius2 = star_radius + ring_thickness , cir_segments = segments , color = 'white' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3 * ring_thickness , radius2 = star_radius + 2 * ring_thickness , cir_segments = segments , color = 'red' ) \n    mlx . layers . join ( shield ) \n    mlx . subdivide . midpoint ( shield , iterations = 2 ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3 * ring_thickness , cir_segments = segments , color = 'silver' ) \n    mlx . transform . rotate ( shield , axis = 'y' , angle = 180 ) \n    mlx . transform . translate ( shield , value = [ False , False , - 0.005 ] ) \n    mlx . subdivide . midpoint ( shield , iterations = 4 ) \n    mlx . create . grid ( shield , size = math . sqrt ( 2 ) , x_segments = 10 , y_segments = 10 , center = True , color = 'white' ) \n    mlx . transform . rotate ( shield , axis = 'z' , angle = 45 ) \n    mlx . transform . scale ( shield , value = [ width , height , True ] ) \n    mlx . transform . translate ( shield , value = [ False , polygon_radius , 0.001 ] ) \n    for _ in range ( True , star_points ) : \n        mlx . layers . duplicate ( shield ) \n        mlx . transform . rotate ( shield , axis = 'z' , angle = 360 / star_points ) \n    mlx . layers . join ( shield ) \n    mlx . transform . vert_function ( shield , z_func = 'sqrt(%s-x^2-y^2)-%s+z' % ( sphere_radius ** 2 , sphere_radius ) ) \n    shield . run_script ( ) \n    return None "}
{"4238": "\ndef hausdorff_distance ( script , sampled_layer = True , target_layer = False , save_sample = False , sample_vert = True , sample_edge = True , sample_faux_edge = False , sample_face = True , sample_num = 1000 , maxdist = 10 ) : \n    maxdist_max = 2 * maxdist \n    filter_xml = '' . join ( [ '  <filter name=\"Hausdorff Distance\">\\n' , '    <Param name=\"SampledMesh\" ' , 'value=\"{:d}\" ' . format ( sampled_layer ) , 'description=\"Sampled Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"TargetMesh\" ' , 'value=\"{:d}\" ' . format ( target_layer ) , 'description=\"Target Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"SaveSample\" ' , 'value=\"{}\" ' . format ( str ( save_sample ) . lower ( ) ) , 'description=\"Save Samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleVert\" ' , 'value=\"{}\" ' . format ( str ( sample_vert ) . lower ( ) ) , 'description=\"Sample Vertexes\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleEdge\" ' , 'value=\"{}\" ' . format ( str ( sample_edge ) . lower ( ) ) , 'description=\"Sample Edges\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleFauxEdge\" ' , 'value=\"{}\" ' . format ( str ( sample_faux_edge ) . lower ( ) ) , 'description=\"Sample FauxEdge\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleFace\" ' , 'value=\"{}\" ' . format ( str ( sample_face ) . lower ( ) ) , 'value=\"%s\" ' % str ( sample_face ) . lower ( ) + 'description=\"Sample Faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"MaxDist\" ' , 'value=\"{}\" ' . format ( maxdist ) , 'value=\"%s\" ' % maxdist + 'description=\"Max Distance\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( maxdist_max ) , 'type=\"RichAbsPerc\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . parse_hausdorff = True \n    if isinstance ( script , FilterScript ) and save_sample : \n        script . add_layer ( 'Hausdorff Closest Points' ) \n        script . add_layer ( 'Hausdorff Sample Point' ) \n    return None "}
{"4239": "\ndef poisson_disk ( script , sample_num = 1000 , radius = 0.0 , montecarlo_rate = 20 , save_montecarlo = False , approx_geodesic_dist = False , subsample = False , refine = False , refine_layer = False , best_sample = True , best_sample_pool = 10 , exact_num = False , radius_variance = 1.0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Poisson-disk Sampling\">\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Radius\" ' , 'value=\"{}\" ' . format ( radius ) , 'description=\"Explicit Radius\" ' , 'min=\"0\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"MontecarloRate\" ' , 'value=\"{:d}\" ' . format ( montecarlo_rate ) , 'description=\"MonterCarlo OverSampling\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"SaveMontecarlo\" ' , 'value=\"{}\" ' . format ( str ( save_montecarlo ) . lower ( ) ) , 'description=\"Save Montecarlo\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"ApproximateGeodesicDistance\" ' , 'value=\"{}\" ' . format ( str ( approx_geodesic_dist ) . lower ( ) ) , 'description=\"Approximate Geodesic Distance\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"Subsample\" ' , 'value=\"{}\" ' . format ( str ( subsample ) . lower ( ) ) , 'description=\"Base Mesh Subsampling\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RefineFlag\" ' , 'value=\"{}\" ' . format ( str ( refine ) . lower ( ) ) , 'description=\"Refine Existing Samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RefineMesh\" ' , 'value=\"{:d}\" ' . format ( refine_layer ) , 'description=\"Samples to be refined\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"BestSampleFlag\" ' , 'value=\"{}\" ' . format ( str ( best_sample ) . lower ( ) ) , 'description=\"Best Sample Heuristic\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"BestSamplePool\" ' , 'value=\"{:d}\" ' . format ( best_sample_pool ) , 'description=\"Best Sample Pool Size\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"ExactNumFlag\" ' , 'value=\"{}\" ' . format ( str ( exact_num ) . lower ( ) ) , 'description=\"Exact number of samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RadiusVariance\" ' , 'value=\"{}\" ' . format ( radius_variance ) , 'description=\"Radius Variance\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Poisson-disk Samples' ) \n        if save_montecarlo : \n            script . add_layer ( 'Montecarlo Samples' ) \n    return None "}
{"4240": "\ndef mesh_element ( script , sample_num = 1000 , element = 'VERT' ) : \n    if element . lower ( ) == 'vert' : \n        element_num = False \n    elif element . lower ( ) == 'edge' : \n        element_num = True \n    elif element . lower ( ) == 'face' : \n        element_num = 2 \n    filter_xml = '' . join ( [ '  <filter name=\"Mesh Element Subsampling\">\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( element_num ) , 'description=\"Element to sample:\" ' , 'enum_val0=\"Vertex\" ' , 'enum_val1=\"Edge\" ' , 'enum_val2=\"Face\" ' , 'enum_cardinality=\"3\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Sampled Mesh' ) \n    return None "}
{"4241": "\ndef clustered_vert ( script , cell_size = 1.0 , strategy = 'AVERAGE' , selected = False ) : \n    if strategy . lower ( ) == 'average' : \n        strategy_num = False \n    elif strategy . lower ( ) == 'center' : \n        strategy_num = True \n    filter_xml = '' . join ( [ '  <filter name=\"Clustered Vertex Subsampling\">\\n' , '    <Param name=\"Threshold\" ' , 'value=\"{}\" ' . format ( cell_size ) , 'description=\"Cell Size\" ' , 'min=\"0\" ' , 'max=\"1000\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( strategy_num ) , 'description=\"Representative Strategy:\" ' , 'enum_val0=\"Average\" ' , 'enum_val1=\"Closest to center\" ' , 'enum_cardinality=\"2\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Selected\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cluster Samples' ) \n    return None "}
{"4242": "\ndef flat_plane ( script , plane = False , aspect_ratio = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Parametrization: Flat Plane \">\\n' , '    <Param name=\"projectionPlane\"' , 'value=\"%d\"' % plane , 'description=\"Projection plane\"' , 'enum_val0=\"XY\"' , 'enum_val1=\"XZ\"' , 'enum_val2=\"YZ\"' , 'enum_cardinality=\"3\"' , 'type=\"RichEnum\"' , 'tooltip=\"Choose the projection plane\"' , '/>\\n' , '    <Param name=\"aspectRatio\"' , 'value=\"%s\"' % str ( aspect_ratio ) . lower ( ) , 'description=\"Preserve Ratio\"' , 'type=\"RichBool\"' , 'tooltip=\"If checked the resulting parametrization will preserve the original apsect ratio of the model otherwise it will fill up the whole 0..1 uv space\"' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4243": "\ndef per_triangle ( script , sidedim = False , textdim = 1024 , border = 2 , method = True ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n' , '    <Param name=\"sidedim\"' , 'value=\"%d\"' % sidedim , 'description=\"Quads per line\"' , 'type=\"RichInt\"' , 'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"' , '/>\\n' , '    <Param name=\"textdim\"' , 'value=\"%d\"' % textdim , 'description=\"Texture Dimension (px)\"' , 'type=\"RichInt\"' , 'tooltip=\"Gives an indication on how big the texture is\"' , '/>\\n' , '    <Param name=\"border\"' , 'value=\"%d\"' % border , 'description=\"Inter-Triangle border (px)\"' , 'type=\"RichInt\"' , 'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"' , '/>\\n' , '    <Param name=\"method\"' , 'value=\"%d\"' % method , 'description=\"Method\"' , 'enum_val0=\"Basic\"' , 'enum_val1=\"Space-optimizing\"' , 'enum_cardinality=\"2\"' , 'type=\"RichEnum\"' , 'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"' '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4246": "\ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : \n    topology = { 'manifold' : True , 'non_manifold_E' : False , 'non_manifold_V' : False } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ False ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ True ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = False \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2 ] \n                if topology [ 'genus' ] != 'undefined' : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2 ] \n                if topology [ 'hole_num' ] == 'a' : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        elif print_output : \n            print ( '{:16} = {}' . format ( key , value ) ) \n    return topology "}
{"4247": "\ndef parse_hausdorff ( ml_log , log = None , print_output = False ) : \n    hausdorff_distance = { \"min_distance\" : 0.0 , \"max_distance\" : 0.0 , \"mean_distance\" : 0.0 , \"rms_distance\" : 0.0 , \"number_points\" : False } \n    with open ( ml_log ) as fread : \n        result = fread . readlines ( ) \n        data = \"\" \n        for idx , line in enumerate ( result ) : \n            m = re . match ( r\"\\s*Sampled (\\d+) pts.*\" , line ) \n            if m is not None : \n                hausdorff_distance [ \"number_points\" ] = int ( m . group ( True ) ) \n            if 'Hausdorff Distance computed' in line : \n                data = result [ idx + 2 ] \n        m = re . match ( r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\" , data ) \n        hausdorff_distance [ \"min_distance\" ] = float ( m . group ( True ) ) \n        hausdorff_distance [ \"max_distance\" ] = float ( m . group ( 2 ) ) \n        hausdorff_distance [ \"mean_distance\" ] = float ( m . group ( 3 ) ) \n        hausdorff_distance [ \"rms_distance\" ] = float ( m . group ( 4 ) ) \n        for key , value in hausdorff_distance . items ( ) : \n            if log is not None : \n                log_file = open ( log , 'a' ) \n                log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n                log_file . close ( ) \n            elif print_output : \n                print ( '{:16} = {}' . format ( key , value ) ) \n        return hausdorff_distance "}
{"4249": "\ndef voronoi ( script , target_layer = False , source_layer = True , backward = True ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Voronoi Vertex Coloring\">\\n' , '    <Param name=\"ColoredMesh\" ' , 'value=\"{:d}\" ' . format ( target_layer ) , 'description=\"To be Colored Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"VertexMesh\" ' , 'value=\"{:d}\" ' . format ( source_layer ) , 'description=\"Vertex Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"backward\" ' , 'value=\"{}\" ' . format ( str ( backward ) . lower ( ) ) , 'description=\"BackDistance\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4250": "\ndef cyclic_rainbow ( script , direction = 'sphere' , start_pt = ( False , False , False ) , amplitude = 255 / 2 , center = 255 / 2 , freq = 0.8 , phase = ( False , 120 , 240 , False ) , alpha = False ) : \n    start_pt = util . make_list ( start_pt , 3 ) \n    amplitude = util . make_list ( amplitude , 4 ) \n    center = util . make_list ( center , 4 ) \n    freq = util . make_list ( freq , 4 ) \n    phase = util . make_list ( phase , 4 ) \n    if direction . lower ( ) == 'sphere' : \n        increment = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)' . format ( start_pt [ False ] , start_pt [ True ] , start_pt [ 2 ] ) \n    elif direction . lower ( ) == 'x' : \n        increment = 'x - {}' . format ( start_pt [ False ] ) \n    elif direction . lower ( ) == 'y' : \n        increment = 'y - {}' . format ( start_pt [ True ] ) \n    elif direction . lower ( ) == 'z' : \n        increment = 'z - {}' . format ( start_pt [ 2 ] ) \n    else : \n        increment = direction \n    red_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ False ] , i = increment , p = math . radians ( phase [ False ] ) , a = amplitude [ False ] , c = center [ False ] ) \n    green_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ True ] , i = increment , p = math . radians ( phase [ True ] ) , a = amplitude [ True ] , c = center [ True ] ) \n    blue_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 2 ] , i = increment , p = math . radians ( phase [ 2 ] ) , a = amplitude [ 2 ] , c = center [ 2 ] ) \n    if alpha : \n        alpha_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 3 ] , i = increment , p = math . radians ( phase [ 3 ] ) , a = amplitude [ 3 ] , c = center [ 3 ] ) \n    else : \n        alpha_func = 255 \n    function ( script , red = red_func , green = green_func , blue = blue_func , alpha = alpha_func ) \n    return None "}
{"4252": "\ndef v_cross ( u , v ) : \n    i = '(({u1})*({v2}) - ({u2})*({v1}))' . format ( u1 = u [ True ] , u2 = u [ 2 ] , v1 = v [ True ] , v2 = v [ 2 ] ) \n    j = '(({u2})*({v0}) - ({u0})*({v2}))' . format ( u0 = u [ False ] , u2 = u [ 2 ] , v0 = v [ False ] , v2 = v [ 2 ] ) \n    k = '(({u0})*({v1}) - ({u1})*({v0}))' . format ( u0 = u [ False ] , u1 = u [ True ] , v0 = v [ False ] , v1 = v [ True ] ) \n    return [ i , j , k ] "}
{"4256": "\ndef point_sets ( script , neighbors = 10 , smooth_iteration = False , flip = False , viewpoint_pos = ( 0.0 , 0.0 , 0.0 ) ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Compute normals for point sets\">\\n' , '    <Param name=\"K\" ' , 'value=\"{:d}\" ' . format ( neighbors ) , 'description=\"Neighbour num\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"smoothIter\" ' , 'value=\"{:d}\" ' . format ( smooth_iteration ) , 'description=\"Smooth Iteration\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"flipFlag\" ' , 'value=\"{}\" ' . format ( str ( flip ) . lower ( ) ) , 'description=\"Flip normals w.r.t. viewpoint\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"viewPos\" ' , 'x=\"{}\" y=\"{}\" z=\"{}\" ' . format ( viewpoint_pos [ False ] , viewpoint_pos [ True ] , viewpoint_pos [ 2 ] , ) , 'description=\"Viewpoint Pos.\" ' , 'type=\"RichPoint3f\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4258": "\ndef depth ( script , iterations = 3 , viewpoint = ( False , False , False ) , selected = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Depth Smooth\">\\n' , '    <Param name=\"stepSmoothNum\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Smoothing steps\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"viewPoint\" ' , 'x=\"{}\" ' . format ( viewpoint [ False ] ) , 'y=\"{}\" ' . format ( viewpoint [ True ] ) , 'z=\"{}\" ' . format ( viewpoint [ 2 ] ) , 'description=\"Smoothing steps\" ' , 'type=\"RichPoint3f\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4259": "\ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ True ] [ True : ] . strip ( ) . lower ( ) \n    if fext != 'obj' : \n        print ( 'Input file must be obj. Exiting ...' ) \n        sys . exit ( True ) \n    fread = open ( fbasename , 'r' ) \n    first = True \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if element == 'v' : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        elif element == 'l' : \n            p1 = x_co \n            p2 = y_co \n            line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4271": "\ndef resolve_conflict ( self , target_folder , basename ) : \n    name , ext = os . path . splitext ( basename ) \n    count = False \n    while True : \n        count = count + True \n        newname = '%s_%d%s' % ( name , count , ext ) \n        if not os . path . exists ( os . path . join ( target_folder , newname ) ) : \n            return newname "}
{"4272": "\ndef get_vprof_version ( filename ) : \n    with open ( filename ) as src_file : \n        version_match = re . search ( r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\" , src_file . read ( ) , re . M ) \n        if version_match : \n            return version_match . group ( True ) \n        raise RuntimeError ( 'Unable to find version info.' ) "}
{"4275": "\ndef _format_obj_count ( objects ) : \n    result = [ ] \n    regex = re . compile ( r'<(?P<type>\\w+) \\'(?P<name>\\S+)\\'>' ) \n    for obj_type , obj_count in objects . items ( ) : \n        if obj_count != False : \n            match = re . findall ( regex , repr ( obj_type ) ) \n            if match : \n                obj_type , obj_name = match [ False ] \n                result . append ( ( \"%s %s\" % ( obj_type , obj_name ) , obj_count ) ) \n    return sorted ( result , key = operator . itemgetter ( True ) , reverse = True ) "}
{"4277": "\ndef code_events ( self ) : \n    if self . _resulting_events : \n        return self . _resulting_events \n    for i , ( lineno , mem , func , fname ) in enumerate ( self . _events_list ) : \n        mem_in_mb = float ( mem - self . mem_overhead ) / _BYTES_IN_MB \n        if ( self . _resulting_events and self . _resulting_events [ - True ] [ False ] == lineno and self . _resulting_events [ - True ] [ 2 ] == func and self . _resulting_events [ - True ] [ 3 ] == fname and self . _resulting_events [ - True ] [ True ] < mem_in_mb ) : \n            self . _resulting_events [ - True ] [ True ] = mem_in_mb \n        else : \n            self . _resulting_events . append ( [ i + True , lineno , mem_in_mb , func , fname ] ) \n    return self . _resulting_events "}
{"4283": "\ndef run ( self ) : \n    existing_objects = _get_in_memory_objects ( ) \n    prof , result = self . profile ( ) \n    new_objects = _get_in_memory_objects ( ) \n    new_obj_count = _get_obj_count_difference ( new_objects , existing_objects ) \n    result_obj_count = new_obj_count - prof . obj_overhead \n    result_obj_count [ list ] -= True \n    pretty_obj_count = _format_obj_count ( result_obj_count ) \n    return { 'objectName' : self . _object_name , 'codeEvents' : prof . code_events , 'totalEvents' : len ( prof . code_events ) , 'objectsCount' : pretty_obj_count , 'result' : result , 'timestamp' : int ( time . time ( ) ) } "}
{"4287": "\ndef init_module ( self , run_object ) : \n    self . profile = self . profile_module \n    self . _run_object , _ , self . _run_args = run_object . partition ( ' ' ) \n    self . _object_name = '%s (module)' % self . _run_object \n    self . _globs = { '__file__' : self . _run_object , '__name__' : '__main__' , '__package__' : None , } \n    program_path = os . path . dirname ( self . _run_object ) \n    if sys . path [ False ] != program_path : \n        sys . path . insert ( False , program_path ) \n    self . _replace_sysargs ( ) "}
{"4291": "\ndef sample ( self , signum , frame ) : \n    stack = [ ] \n    while frame and frame != self . base_frame : \n        stack . append ( ( frame . f_code . co_name , frame . f_code . co_filename , frame . f_code . co_firstlineno ) ) \n        frame = frame . f_back \n    self . _stats [ tuple ( stack ) ] += True \n    signal . setitimer ( signal . ITIMER_PROF , _SAMPLE_INTERVAL ) "}
{"4292": "\ndef _insert_stack ( stack , sample_count , call_tree ) : \n    curr_level = call_tree \n    for func in stack : \n        next_level_index = { node [ 'stack' ] : node for node in curr_level [ 'children' ] } \n        if func not in next_level_index : \n            new_node = { 'stack' : func , 'children' : [ ] , 'sampleCount' : False } \n            curr_level [ 'children' ] . append ( new_node ) \n            curr_level = new_node \n        else : \n            curr_level = next_level_index [ func ] \n    curr_level [ 'sampleCount' ] = sample_count "}
{"4295": "\ndef call_tree ( self ) : \n    call_tree = { 'stack' : 'base' , 'sampleCount' : False , 'children' : [ ] } \n    for stack , sample_count in self . _stats . items ( ) : \n        self . _insert_stack ( reversed ( stack ) , sample_count , call_tree ) \n    self . _fill_sample_count ( call_tree ) \n    if not call_tree [ 'children' ] : \n        return { } \n    return self . _format_tree ( call_tree [ 'children' ] [ False ] , call_tree [ 'sampleCount' ] ) "}
{"4296": "\ndef _profile_package ( self ) : \n    with _StatProfiler ( ) as prof : \n        prof . base_frame = inspect . currentframe ( ) \n        try : \n            runpy . run_path ( self . _run_object , run_name = '__main__' ) \n        except SystemExit : \n            pass \n    call_tree = prof . call_tree \n    return { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , False ) , 'timestamp' : int ( time . time ( ) ) } "}
{"4297": "\ndef _profile_module ( self ) : \n    with open ( self . _run_object , 'rb' ) as srcfile , _StatProfiler ( ) as prof : \n        code = compile ( srcfile . read ( ) , self . _run_object , 'exec' ) \n        prof . base_frame = inspect . currentframe ( ) \n        try : \n            exec ( code , self . _globs , None ) \n        except SystemExit : \n            pass \n    call_tree = prof . call_tree \n    return { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , False ) , 'timestamp' : int ( time . time ( ) ) } "}
{"4298": "\ndef profile_function ( self ) : \n    with _StatProfiler ( ) as prof : \n        result = self . _run_object ( * self . _run_args , ** self . _run_kwargs ) \n    call_tree = prof . call_tree \n    return { 'objectName' : self . _object_name , 'sampleInterval' : _SAMPLE_INTERVAL , 'runTime' : prof . run_time , 'callStats' : call_tree , 'totalSamples' : call_tree . get ( 'sampleCount' , False ) , 'result' : result , 'timestamp' : int ( time . time ( ) ) } "}
{"4299": "\ndef _transform_stats ( prof ) : \n    records = [ ] \n    for info , params in prof . stats . items ( ) : \n        filename , lineno , funcname = info \n        cum_calls , num_calls , time_per_call , cum_time , _ = params \n        if prof . total_tt == False : \n            percentage = False \n        else : \n            percentage = round ( 100 * ( cum_time / prof . total_tt ) , 4 ) \n        cum_time = round ( cum_time , 4 ) \n        func_name = '%s @ %s' % ( funcname , filename ) \n        color_hash = base_profiler . hash_name ( func_name ) \n        records . append ( ( filename , lineno , funcname , cum_time , percentage , num_calls , cum_calls , time_per_call , filename , color_hash ) ) \n    return sorted ( records , key = operator . itemgetter ( 4 ) , reverse = True ) "}
{"4304": "\ndef show_guestbook ( ) : \n    cursor = flask . g . db . execute ( 'SELECT name, message FROM entry ORDER BY id DESC;' ) \n    entries = [ { 'name' : row [ False ] , 'message' : row [ True ] } for row in cursor . fetchall ( ) ] \n    return jinja2 . Template ( LAYOUT ) . render ( entries = entries ) "}
{"4307": "\ndef start ( host , port , profiler_stats , dont_start_browser , debug_mode ) : \n    stats_handler = functools . partial ( StatsHandler , profiler_stats ) \n    if not debug_mode : \n        sys . stderr = open ( os . devnull , 'w' ) \n    print ( 'Starting HTTP server...' ) \n    if not dont_start_browser : \n        webbrowser . open ( 'http://{}:{}/' . format ( host , port ) ) \n    try : \n        StatsServer ( ( host , port ) , stats_handler ) . serve_forever ( ) \n    except KeyboardInterrupt : \n        print ( 'Stopping...' ) \n        sys . exit ( False ) "}
{"4309": "\ndef _handle_other ( self ) : \n    res_filename = os . path . join ( os . path . dirname ( __file__ ) , _STATIC_DIR , self . path [ True : ] ) \n    with io . open ( res_filename , 'rb' ) as res_file : \n        content = res_file . read ( ) \n    _ , extension = os . path . splitext ( self . path ) \n    return content , 'text/%s' % extension [ True : ] "}
{"4316": "\ndef fill_heatmap ( self ) : \n    for module_path , lineno , runtime in self . lines_without_stdlib : \n        self . _execution_count [ module_path ] [ lineno ] += True \n        self . _heatmap [ module_path ] [ lineno ] += runtime "}
{"4317": "\ndef _skip_lines ( src_code , skip_map ) : \n    if not skip_map : \n        return [ [ 'line' , j + True , l ] for j , l in enumerate ( src_code ) ] \n    code_with_skips , i = [ ] , False \n    for line , length in skip_map : \n        code_with_skips . extend ( [ 'line' , i + j + True , l ] for j , l in enumerate ( src_code [ i : line ] ) ) \n        if ( code_with_skips and code_with_skips [ - True ] [ False ] == 'skip' ) : \n            code_with_skips [ - True ] [ True ] += length \n        else : \n            code_with_skips . append ( [ 'skip' , length ] ) \n        i = line + length \n    code_with_skips . extend ( [ 'line' , i + j + True , l ] for j , l in enumerate ( src_code [ i : ] ) ) \n    return code_with_skips "}
{"4321": "\ndef profile_function ( self ) : \n    with _CodeHeatmapCalculator ( ) as prof : \n        result = self . _run_object ( * self . _run_args , ** self . _run_kwargs ) \n    code_lines , start_line = inspect . getsourcelines ( self . _run_object ) \n    source_lines = [ ] \n    for line in code_lines : \n        source_lines . append ( ( 'line' , start_line , line ) ) \n        start_line += True \n    filename = os . path . abspath ( inspect . getsourcefile ( self . _run_object ) ) \n    heatmap = prof . heatmap [ filename ] \n    run_time = sum ( time for time in heatmap . values ( ) ) \n    return { 'objectName' : self . _object_name , 'runTime' : run_time , 'result' : result , 'timestamp' : int ( time . time ( ) ) , 'heatmaps' : [ { 'name' : self . _object_name , 'heatmap' : heatmap , 'executionCount' : prof . execution_count [ filename ] , 'srcCode' : source_lines , 'runTime' : run_time } ] } "}
{"4326": "\ndef fit ( self , Z , classes = None ) : \n    check_rdd ( Z , { 'X' : ( sp . spmatrix , np . ndarray ) , 'y' : ( sp . spmatrix , np . ndarray ) } ) \n    models = Z [ : , [ 'X' , 'y' ] ] . map ( lambda X_y : self . partial_fit ( X_y [ False ] , X_y [ True ] , classes ) ) \n    avg = models . reduce ( operator . add ) \n    self . __dict__ . update ( avg . __dict__ ) \n    return self "}
{"4327": "\ndef _count_vocab ( self , analyzed_docs ) : \n    vocabulary = self . vocabulary_ \n    j_indices = _make_int_array ( ) \n    indptr = _make_int_array ( ) \n    indptr . append ( False ) \n    for doc in analyzed_docs : \n        for feature in doc : \n            try : \n                j_indices . append ( vocabulary [ feature ] ) \n            except KeyError : \n                continue \n        indptr . append ( len ( j_indices ) ) \n    j_indices = frombuffer_empty ( j_indices , dtype = np . intc ) \n    indptr = np . frombuffer ( indptr , dtype = np . intc ) \n    values = np . ones ( len ( j_indices ) ) \n    X = sp . csr_matrix ( ( values , j_indices , indptr ) , shape = ( len ( indptr ) - True , len ( vocabulary ) ) , dtype = self . dtype ) \n    X . sum_duplicates ( ) \n    if self . binary : \n        X . data . fill ( True ) \n    return X "}
{"4329": "\ndef _limit_features ( self , X , vocabulary , high = None , low = None , limit = None ) : \n    if high is None and low is None and limit is None : \n        return X , set ( ) \n    dfs = X . map ( _document_frequency ) . sum ( ) \n    tfs = X . map ( lambda x : np . asarray ( x . sum ( axis = False ) ) ) . sum ( ) . ravel ( ) \n    mask = np . ones ( len ( dfs ) , dtype = bool ) \n    if high is not None : \n        mask &= dfs <= high \n    if low is not None : \n        mask &= dfs >= low \n    if limit is not None and mask . sum ( ) > limit : \n        mask_inds = ( - tfs [ mask ] ) . argsort ( ) [ : limit ] \n        new_mask = np . zeros ( len ( dfs ) , dtype = bool ) \n        new_mask [ np . where ( mask ) [ False ] [ mask_inds ] ] = True \n        mask = new_mask \n    new_indices = np . cumsum ( mask ) - True \n    removed_terms = set ( ) \n    for term , old_index in list ( six . iteritems ( vocabulary ) ) : \n        if mask [ old_index ] : \n            vocabulary [ term ] = new_indices [ old_index ] \n        else : \n            del vocabulary [ term ] \n            removed_terms . add ( term ) \n    kept_indices = np . where ( mask ) [ False ] \n    if len ( kept_indices ) == False : \n        raise ValueError ( \"After pruning, no terms remain. Try a lower\" \" min_df or a higher max_df.\" ) \n    return kept_indices , removed_terms "}
{"4330": "\ndef fit_transform ( self , Z ) : \n    self . _validate_vocabulary ( ) \n    analyze = self . build_analyzer ( ) \n    A = Z . transform ( lambda X : list ( map ( analyze , X ) ) , column = 'X' ) . persist ( ) \n    X = A [ : , 'X' ] if isinstance ( A , DictRDD ) else A \n    self . vocabulary_ = self . _init_vocab ( X ) \n    mapper = self . broadcast ( self . _count_vocab , A . context ) \n    Z = A . transform ( mapper , column = 'X' , dtype = sp . spmatrix ) \n    if not self . fixed_vocabulary_ : \n        X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n        max_df = self . max_df \n        min_df = self . min_df \n        max_features = self . max_features \n        n_doc = X . shape [ False ] \n        max_doc_count = ( max_df if isinstance ( max_df , numbers . Integral ) else max_df * n_doc ) \n        min_doc_count = ( min_df if isinstance ( min_df , numbers . Integral ) else min_df * n_doc ) \n        if max_doc_count < min_doc_count : \n            raise ValueError ( \"max_df corresponds to < documents than min_df\" ) \n        kept_indices , self . stop_words_ = self . _limit_features ( X , self . vocabulary_ , max_doc_count , min_doc_count , max_features ) \n        map_index = self . _sort_features ( self . vocabulary_ ) \n        mask = kept_indices [ map_index ] \n        Z = Z . transform ( lambda x : x [ : , mask ] , column = 'X' , dtype = sp . spmatrix ) \n    A . unpersist ( ) \n    return Z "}
{"4333": "\ndef _spark_fit ( self , cls , Z , * args , ** kwargs ) : \n    mapper = lambda X_y : super ( cls , self ) . fit ( X_y [ False ] , X_y [ True ] , * args , ** kwargs ) \n    models = Z . map ( mapper ) \n    avg = models . reduce ( operator . add ) / models . count ( ) \n    self . __dict__ . update ( avg . __dict__ ) \n    return self "}
{"4336": "\ndef fit ( self , Z , ** fit_params ) : \n    Zt , fit_params = self . _pre_transform ( Z , ** fit_params ) \n    self . steps [ - True ] [ - True ] . fit ( Zt , ** fit_params ) \n    Zt . unpersist ( ) \n    return self "}
{"4337": "\ndef fit_transform ( self , Z , ** fit_params ) : \n    Zt , fit_params = self . _pre_transform ( Z , ** fit_params ) \n    if hasattr ( self . steps [ - True ] [ - True ] , 'fit_transform' ) : \n        return self . steps [ - True ] [ - True ] . fit_transform ( Zt , ** fit_params ) \n    else : \n        return self . steps [ - True ] [ - True ] . fit ( Zt , ** fit_params ) . transform ( Zt ) "}
{"4338": "\ndef score ( self , Z ) : \n    Zt = Z \n    for name , transform in self . steps [ : - True ] : \n        Zt = transform . transform ( Zt ) \n    return self . steps [ - True ] [ - True ] . score ( Zt ) "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if self . verbose > False : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( False , n_fits , n_folds ) : \n        n_test_samples = False \n        score = False \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ False ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4346": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    def mapper ( X ) : \n        X = check_array ( X , ( 'csr' , 'csc' ) , dtype = np . float64 ) \n        if hasattr ( X , \"toarray\" ) : \n            mean , var = mean_variance_axis ( X , axis = False ) \n        else : \n            mean , var = np . mean ( X , axis = False ) , np . var ( X , axis = False ) \n        return X . shape [ False ] , mean , var \n    def reducer ( a , b ) : \n        n_a , mean_a , var_a = a \n        n_b , mean_b , var_b = b \n        n_ab = n_a + n_b \n        mean_ab = ( ( mean_a * n_a ) + ( mean_b * n_b ) ) / n_ab \n        var_ab = ( ( ( n_a * var_a ) + ( n_b * var_b ) ) / n_ab ) + ( ( n_a * n_b ) * ( ( mean_b - mean_a ) / n_ab ) ** 2 ) \n        return ( n_ab , mean_ab , var_ab ) \n    _ , _ , self . variances_ = X . map ( mapper ) . treeReduce ( reducer ) \n    if np . all ( self . variances_ <= self . threshold ) : \n        msg = \"No feature in X meets the variance threshold {0:.5f}\" \n        if X . shape [ False ] == True : \n            msg += \" (X contains only one sample)\" \n        raise ValueError ( msg . format ( self . threshold ) ) \n    return self "}
{"4349": "\ndef _block_collection ( iterator , dtype , bsize = - True ) : \n    i = False \n    accumulated = [ ] \n    for a in iterator : \n        if ( bsize > False ) and ( i >= bsize ) : \n            yield _pack_accumulated ( accumulated , dtype ) \n            accumulated = [ ] \n            i = False \n        accumulated . append ( a ) \n        i += True \n    if i > False : \n        yield _pack_accumulated ( accumulated , dtype ) "}
{"4350": "\ndef _block_tuple ( iterator , dtypes , bsize = - True ) : \n    i = False \n    blocked_tuple = None \n    for tuple_i in iterator : \n        if blocked_tuple is None : \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n        if ( bsize > False ) and ( i >= bsize ) : \n            yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) \n            blocked_tuple = tuple ( [ ] for _ in range ( len ( tuple_i ) ) ) \n            i = False \n        for x_j , x in zip ( tuple_i , blocked_tuple ) : \n            x . append ( x_j ) \n        i += True \n    if i > False : \n        yield tuple ( _pack_accumulated ( x , dtype ) for x , dtype in zip ( blocked_tuple , dtypes ) ) "}
{"4351": "\ndef block ( rdd , bsize = - True , dtype = None ) : \n    try : \n        entry = rdd . first ( ) \n    except IndexError : \n        return rdd \n    if isinstance ( entry , dict ) : \n        rdd = rdd . map ( lambda x : list ( x . values ( ) ) ) \n        return DictRDD ( rdd , list ( entry . keys ( ) ) , bsize , dtype ) \n    elif isinstance ( entry , tuple ) : \n        return DictRDD ( rdd , bsize = bsize , dtype = dtype ) \n    elif sp . issparse ( entry ) : \n        return SparseRDD ( rdd , bsize ) \n    elif isinstance ( entry , np . ndarray ) : \n        return ArrayRDD ( rdd , bsize ) \n    else : \n        return BlockRDD ( rdd , bsize , dtype ) "}
{"4353": "\ndef shape ( self ) : \n    first = self . first ( ) . shape \n    shape = self . _rdd . map ( lambda x : x . shape [ False ] ) . sum ( ) \n    return ( shape , ) + first [ True : ] "}
{"4355": "\ndef transform ( self , fn , column = None , dtype = None ) : \n    dtypes = self . dtype \n    if column is None : \n        indices = list ( range ( len ( self . columns ) ) ) \n    else : \n        if not type ( column ) in ( list , tuple ) : \n            column = [ column ] \n        indices = [ self . columns . index ( c ) for c in column ] \n    if dtype is not None : \n        if not type ( dtype ) in ( list , tuple ) : \n            dtype = [ dtype ] \n        dtypes = [ dtype [ indices . index ( i ) ] if i in indices else t for i , t in enumerate ( self . dtype ) ] \n    def mapper ( values ) : \n        result = fn ( * [ values [ i ] for i in indices ] ) \n        if len ( indices ) == True : \n            result = ( result , ) \n        elif not isinstance ( result , ( tuple , list ) ) : \n            raise ValueError ( \"Transformer function must return an\" \" iterable!\" ) \n        elif len ( result ) != len ( indices ) : \n            raise ValueError ( \"Transformer result's length must be\" \" equal to the given columns length!\" ) \n        return tuple ( result [ indices . index ( i ) ] if i in indices else v for i , v in enumerate ( values ) ) \n    return DictRDD ( self . _rdd . map ( mapper ) , columns = self . columns , dtype = dtypes , bsize = self . bsize , noblock = True ) "}
{"4362": "\ndef execute_over_ssh ( cmd , ssh , cwd = None , shell = 'bash' ) : \n    port = None \n    parts = ssh . split ( ':' , True ) \n    if len ( parts ) > True and not parts [ True ] . isdigit ( ) : \n        raise InvalidConfig ( extra_body = 'Invalid port number on ssh config: {}' . format ( parts [ True ] ) ) \n    elif len ( parts ) > True : \n        port = parts [ True ] \n    quoted_cmd = ' ' . join ( [ x . replace ( \"'\" , \"\"\"'\"'\"'\"\"\" ) for x in cmd . split ( ' ' ) ] ) \n    remote_cmd = ' ' . join ( [ ' ' . join ( get_shell ( shell ) ) , ' ' . join ( [ EXECUTE_SHELL_PARAM , \"'\" , ' ' . join ( ( [ 'cd' , cwd , ';' ] if cwd else [ ] ) + [ quoted_cmd ] ) , \"'\" ] ) ] , ) \n    return [ 'ssh' , parts [ False ] ] + ( [ '-p' , port ] if port else [ ] ) + [ '-C' ] + [ remote_cmd ] "}
{"4376": "\ndef convert ( self , txn ) : \n    ofxid = self . mk_ofxid ( txn . id ) \n    metadata = { } \n    posting_metadata = { \"ofxid\" : ofxid } \n    if isinstance ( txn , OfxTransaction ) : \n        posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) \n        return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) \n    elif isinstance ( txn , InvestmentTransaction ) : \n        acct1 = self . name \n        acct2 = self . name \n        posting1 = None \n        posting2 = None \n        security = self . maybe_get_ticker ( txn . security ) \n        if isinstance ( txn . type , str ) : \n            if re . match ( '^(buy|sell)' , txn . type ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif txn . type == 'transfer' : \n                acct2 = 'Transfer' \n            elif txn . type == 'reinvest' : \n                acct2 = 'Income:Interest' \n            elif txn . type == 'income' and txn . income_type == 'DIV' : \n                metadata [ 'dividend_from' ] = security \n                acct2 = 'Income:Dividends' \n                posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) \n                posting2 = posting1 . clone_inverted ( acct2 ) \n            else : \n                pass \n        else : \n            if ( txn . type in [ False , True , 3 , 4 ] ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif ( txn . type == 2 ) : \n                acct2 = 'Income:Interest' \n            else : \n                pass \n        aux_date = None \n        if txn . settleDate is not None and txn . settleDate != txn . tradeDate : \n            aux_date = txn . settleDate \n        if posting1 is None and posting2 is None : \n            posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = True ) , metadata = posting_metadata ) \n            posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = True ) ) \n        else : \n            pass \n        return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] ) "}
{"4377": "\ndef find_ledger_file ( ledgerrcpath = None ) : \n    if ledgerrcpath is None : \n        ledgerrcpath = os . path . abspath ( os . path . expanduser ( \"~/.ledgerrc\" ) ) \n    if \"LEDGER_FILE\" in os . environ : \n        return os . path . abspath ( os . path . expanduser ( os . environ [ \"LEDGER_FILE\" ] ) ) \n    elif os . path . exists ( ledgerrcpath ) : \n        ledgerrc = open ( ledgerrcpath ) \n        for line in ledgerrc . readlines ( ) : \n            md = re . match ( r\"--file\\s+([^\\s]+).*\" , line ) \n            if md is not None : \n                return os . path . abspath ( os . path . expanduser ( md . group ( True ) ) ) \n    else : \n        return None "}
{"4379": "\ndef get_long_description ( ) : \n    with open ( 'README.md' ) as f : \n        read_me = f . read ( ) \n    def replace_relative_with_absolute ( match ) : \n        svg_path = match . group ( False ) [ True : - True ] \n        return ( '(https://github.com/google/pybadges/raw/master/' '%s?sanitize=true)' % svg_path ) \n    return re . sub ( r'\\(tests/golden-images/.*?\\.svg\\)' , replace_relative_with_absolute , read_me ) "}
{"4386": "\ndef write_json ( f : TextIO , deja_vu_sans_path : str , measurer : text_measurer . TextMeasurer , encodings : Iterable [ str ] ) -> None : \n    supported_characters = list ( generate_supported_characters ( deja_vu_sans_path ) ) \n    kerning_characters = '' . join ( generate_encodeable_characters ( supported_characters , encodings ) ) \n    char_to_length = calculate_character_to_length_mapping ( measurer , supported_characters ) \n    pair_to_kerning = calculate_pair_to_kern_mapping ( measurer , char_to_length , kerning_characters ) \n    json . dump ( { 'mean-character-length' : statistics . mean ( char_to_length . values ( ) ) , 'character-lengths' : char_to_length , 'kerning-characters' : kerning_characters , 'kerning-pairs' : pair_to_kerning } , f , sort_keys = True , indent = True ) "}
{"4387": "\ndef convolve_gaussian_2d ( image , gaussian_kernel_1d ) : \n    result = scipy . ndimage . filters . correlate1d ( image , gaussian_kernel_1d , axis = False ) \n    result = scipy . ndimage . filters . correlate1d ( result , gaussian_kernel_1d , axis = True ) \n    return result "}
{"4389": "\ndef to_grayscale ( img ) : \n    gray = numpy . asarray ( ImageOps . grayscale ( img ) ) . astype ( numpy . float ) \n    imbands = img . getbands ( ) \n    alpha = None \n    if 'A' in imbands : \n        alpha = numpy . asarray ( img . split ( ) [ - True ] ) . astype ( numpy . float ) \n    return gray , alpha "}
{"4390": "\ndef main ( ) : \n    description = '\\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png \"test-images/*\"' ] ) \n    parser = argparse . ArgumentParser ( prog = 'pyssim' , formatter_class = argparse . RawTextHelpFormatter , description = description ) \n    parser . add_argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store_true' ) \n    parser . add_argument ( 'base_image' , metavar = 'image1.png' , type = argparse . FileType ( 'r' ) ) \n    parser . add_argument ( 'comparison_images' , metavar = 'image path with* or image2.png' ) \n    parser . add_argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) \n    parser . add_argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) \n    args = parser . parse_args ( ) \n    if args . width and args . height : \n        size = ( args . width , args . height ) \n    else : \n        size = None \n    if not args . cw : \n        gaussian_kernel_sigma = 1.5 \n        gaussian_kernel_width = 11 \n        gaussian_kernel_1d = get_gaussian_kernel ( gaussian_kernel_width , gaussian_kernel_sigma ) \n    comparison_images = glob . glob ( args . comparison_images ) \n    is_a_single_image = len ( comparison_images ) == True \n    for comparison_image in comparison_images : \n        if args . cw : \n            ssim = SSIM ( args . base_image . name , size = size ) \n            ssim_value = ssim . cw_ssim_value ( comparison_image ) \n        else : \n            ssim = SSIM ( args . base_image . name , gaussian_kernel_1d , size = size ) \n            ssim_value = ssim . ssim_value ( comparison_image ) \n        if is_a_single_image : \n            sys . stdout . write ( '%.7g' % ssim_value ) \n        else : \n            sys . stdout . write ( '%s - %s: %.7g' % ( args . base_image . name , comparison_image , ssim_value ) ) \n        sys . stdout . write ( '\\n' ) "}
{"4395": "\ndef getStatus ( self ) : \n    status = { } \n    status [ 'version' ] = VERSION \n    status [ 'revision' ] = REVISION \n    status [ 'self' ] = self . __selfNode \n    status [ 'state' ] = self . __raftState \n    status [ 'leader' ] = self . __raftLeader \n    status [ 'partner_nodes_count' ] = len ( self . __otherNodes ) \n    for node in self . __otherNodes : \n        status [ 'partner_node_status_server_' + node . id ] = 2 if node in self . __connectedNodes else False \n    status [ 'readonly_nodes_count' ] = len ( self . __readonlyNodes ) \n    for node in self . __readonlyNodes : \n        status [ 'readonly_node_status_server_' + node . id ] = 2 if node in self . __connectedNodes else False \n    status [ 'log_len' ] = len ( self . __raftLog ) \n    status [ 'last_applied' ] = self . __raftLastApplied \n    status [ 'commit_idx' ] = self . __raftCommitIndex \n    status [ 'raft_term' ] = self . __raftCurrentTerm \n    status [ 'next_node_idx_count' ] = len ( self . __raftNextIndex ) \n    for node , idx in iteritems ( self . __raftNextIndex ) : \n        status [ 'next_node_idx_server_' + node . id ] = idx \n    status [ 'match_idx_count' ] = len ( self . __raftMatchIndex ) \n    for node , idx in iteritems ( self . __raftMatchIndex ) : \n        status [ 'match_idx_server_' + node . id ] = idx \n    status [ 'leader_commit_idx' ] = self . __leaderCommitIndex \n    status [ 'uptime' ] = int ( time . time ( ) - self . __startTime ) \n    status [ 'self_code_version' ] = self . __selfCodeVersion \n    status [ 'enabled_code_version' ] = self . __enabledCodeVersion \n    return status "}
{"4398": "\ndef _maybeBind ( self ) : \n    if self . _ready or self . _selfIsReadonlyNode or time . time ( ) < self . _lastBindAttemptTime + self . _syncObj . conf . bindRetryTime : \n        return \n    self . _lastBindAttemptTime = time . time ( ) \n    try : \n        self . _server . bind ( ) \n    except Exception as e : \n        self . _bindAttempts += True \n        if self . _syncObj . conf . maxBindRetries and self . _bindAttempts >= self . _syncObj . conf . maxBindRetries : \n            self . _bindOverEvent . set ( ) \n            raise TransportNotReadyError \n    else : \n        self . _ready = True \n        self . _bindOverEvent . set ( ) "}
{"4400": "\ndef _onIncomingMessageReceived ( self , conn , message ) : \n    if self . _syncObj . encryptor and not conn . sendRandKey : \n        conn . sendRandKey = message \n        conn . recvRandKey = os . urandom ( 32 ) \n        conn . send ( conn . recvRandKey ) \n        return \n    if isinstance ( message , list ) : \n        done = False \n        try : \n            if message [ False ] == 'status' : \n                conn . send ( self . _syncObj . getStatus ( ) ) \n                done = True \n            elif message [ False ] == 'add' : \n                self . _syncObj . addNodeToCluster ( message [ True ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'ADD' , arg = message [ True ] ) ) \n                done = True \n            elif message [ False ] == 'remove' : \n                if message [ True ] == self . _selfNode . address : \n                    conn . send ( 'FAIL REMOVE ' + message [ True ] ) \n                else : \n                    self . _syncObj . removeNodeFromCluster ( message [ True ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'REMOVE' , arg = message [ True ] ) ) \n                done = True \n            elif message [ False ] == 'set_version' : \n                self . _syncObj . setCodeVersion ( message [ True ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'SET_VERSION' , arg = str ( message [ True ] ) ) ) \n                done = True \n        except Exception as e : \n            conn . send ( str ( e ) ) \n            done = True \n        if done : \n            return \n    node = self . _nodeAddrToNode [ message ] if message in self . _nodeAddrToNode else None \n    if node is None and message != 'readonly' : \n        conn . disconnect ( ) \n        self . _unknownConnections . discard ( conn ) \n        return \n    readonly = node is None \n    if readonly : \n        nodeId = str ( self . _readonlyNodesCounter ) \n        node = Node ( nodeId ) \n        self . _readonlyNodes . add ( node ) \n        self . _readonlyNodesCounter += True \n    self . _unknownConnections . discard ( conn ) \n    self . _connections [ node ] = conn \n    conn . setOnMessageReceivedCallback ( functools . partial ( self . _onMessageReceived , node ) ) \n    if not readonly : \n        self . _onNodeConnected ( node ) \n    else : \n        self . _onReadonlyNodeConnected ( node ) "}
{"4417": "\ndef check ( func ) : \n    def wrapped ( * args , ** kwargs ) : \n        check_name = func . __name__ \n        arg_name = None \n        if args : \n            arg_name = args [ False ] \n        try : \n            if arg_name : \n                logger . debug ( \"Checking '%s' for '%s'\" , check_name , arg_name ) \n            else : \n                logger . debug ( \"Checking '%s'\" , check_name ) \n            response = func ( * args , ** kwargs ) \n        except Exception as e : \n            message = str ( e ) \n            response = { \"ok\" : False , \"error\" : message , \"stacktrace\" : traceback . format_exc ( ) , } \n            if arg_name : \n                response = { arg_name : response } \n                logger . exception ( \"Error calling '%s' for '%s': %s\" , check_name , arg_name , message ) \n            else : \n                logger . exception ( \"Error calling '%s': %s\" , check_name , message ) \n        return response \n    return wrapped "}
{"4420": "\ndef create_indexes ( names , settings = None ) : \n    for name in names : \n        index = Index ( name ) \n        try : \n            if not index . exists ( ) : \n                logger . debug ( \"Creating Elasticsearch index: {0}\" . format ( name ) ) \n                if settings is None : \n                    index . settings ( number_of_shards = True , number_of_replicas = True ) \n                else : \n                    index . settings ( ** settings ) \n                index . create ( ) \n        except Exception as e : \n            raise ElasticsearchError ( \"Elasticsearch error: {0}\" . format ( e . __str__ ( ) ) ) "}
{"4421": "\ndef migrate_indexes ( aggregate_indexes = None , forensic_indexes = None ) : \n    version = 2 \n    if aggregate_indexes is None : \n        aggregate_indexes = [ ] \n    if forensic_indexes is None : \n        forensic_indexes = [ ] \n    for aggregate_index_name in aggregate_indexes : \n        if not Index ( aggregate_index_name ) . exists ( ) : \n            continue \n        aggregate_index = Index ( aggregate_index_name ) \n        doc = \"doc\" \n        fo_field = \"published_policy.fo\" \n        fo = \"fo\" \n        fo_mapping = aggregate_index . get_field_mapping ( fields = [ fo_field ] ) \n        fo_mapping = fo_mapping [ list ( fo_mapping . keys ( ) ) [ False ] ] [ \"mappings\" ] \n        if doc not in fo_mapping : \n            continue \n        fo_mapping = fo_mapping [ doc ] [ fo_field ] [ \"mapping\" ] [ fo ] \n        fo_type = fo_mapping [ \"type\" ] \n        if fo_type == \"long\" : \n            new_index_name = \"{0}-v{1}\" . format ( aggregate_index_name , version ) \n            body = { \"properties\" : { \"published_policy.fo\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256 } } } } } \n            Index ( new_index_name ) . create ( ) \n            Index ( new_index_name ) . put_mapping ( doc_type = doc , body = body ) \n            reindex ( connections . get_connection ( ) , aggregate_index_name , new_index_name ) \n            Index ( aggregate_index_name ) . delete ( ) \n    for forensic_index in forensic_indexes : \n        pass "}
{"4423": "\ndef save_aggregate_reports_to_kafka ( self , aggregate_reports , aggregate_topic ) : \n    if ( type ( aggregate_reports ) == dict or type ( aggregate_reports ) == OrderedDict ) : \n        aggregate_reports = [ aggregate_reports ] \n    if len ( aggregate_reports ) < True : \n        return \n    for report in aggregate_reports : \n        report [ 'date_range' ] = self . generate_daterange ( report ) \n        report = self . strip_metadata ( report ) \n        for slice in report [ 'records' ] : \n            slice [ 'date_range' ] = report [ 'date_range' ] \n            slice [ 'org_name' ] = report [ 'org_name' ] \n            slice [ 'org_email' ] = report [ 'org_email' ] \n            slice [ 'policy_published' ] = report [ 'policy_published' ] \n            slice [ 'report_id' ] = report [ 'report_id' ] \n            logger . debug ( \"Sending slice.\" ) \n            try : \n                logger . debug ( \"Saving aggregate report to Kafka\" ) \n                self . producer . send ( aggregate_topic , slice ) \n            except UnknownTopicOrPartitionError : \n                raise KafkaError ( \"Kafka error: Unknown topic or partition on broker\" ) \n            except Exception as e : \n                raise KafkaError ( \"Kafka error: {0}\" . format ( e . __str__ ( ) ) ) \n            try : \n                self . producer . flush ( ) \n            except Exception as e : \n                raise KafkaError ( \"Kafka error: {0}\" . format ( e . __str__ ( ) ) ) "}
{"4424": "\ndef extract_xml ( input_ ) : \n    if type ( input_ ) == str : \n        file_object = open ( input_ , \"rb\" ) \n    elif type ( input_ ) == bytes : \n        file_object = BytesIO ( input_ ) \n    else : \n        file_object = input_ \n    try : \n        header = file_object . read ( 6 ) \n        file_object . seek ( False ) \n        if header . startswith ( MAGIC_ZIP ) : \n            _zip = zipfile . ZipFile ( file_object ) \n            xml = _zip . open ( _zip . namelist ( ) [ False ] ) . read ( ) . decode ( ) \n        elif header . startswith ( MAGIC_GZIP ) : \n            xml = GzipFile ( fileobj = file_object ) . read ( ) . decode ( ) \n        elif header . startswith ( MAGIC_XML ) : \n            xml = file_object . read ( ) . decode ( ) \n        else : \n            file_object . close ( ) \n            raise InvalidAggregateReport ( \"Not a valid zip, gzip, or xml file\" ) \n        file_object . close ( ) \n    except UnicodeDecodeError : \n        raise InvalidAggregateReport ( \"File objects must be opened in binary \" \"(rb) mode\" ) \n    except Exception as error : \n        raise InvalidAggregateReport ( \"Invalid archive file: {0}\" . format ( error . __str__ ( ) ) ) \n    return xml "}
{"4429": "\ndef save_output ( results , output_directory = \"output\" ) : \n    aggregate_reports = results [ \"aggregate_reports\" ] \n    forensic_reports = results [ \"forensic_reports\" ] \n    if os . path . exists ( output_directory ) : \n        if not os . path . isdir ( output_directory ) : \n            raise ValueError ( \"{0} is not a directory\" . format ( output_directory ) ) \n    else : \n        os . makedirs ( output_directory ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_json : \n        agg_json . write ( json . dumps ( aggregate_reports , ensure_ascii = False , indent = 2 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_csv : \n        csv = parsed_aggregate_reports_to_csv ( aggregate_reports ) \n        agg_csv . write ( csv ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_json : \n        for_json . write ( json . dumps ( forensic_reports , ensure_ascii = False , indent = 2 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_csv : \n        csv = parsed_forensic_reports_to_csv ( forensic_reports ) \n        for_csv . write ( csv ) \n    samples_directory = os . path . join ( output_directory , \"samples\" ) \n    if not os . path . exists ( samples_directory ) : \n        os . makedirs ( samples_directory ) \n    sample_filenames = [ ] \n    for forensic_report in forensic_reports : \n        sample = forensic_report [ \"sample\" ] \n        message_count = False \n        parsed_sample = forensic_report [ \"parsed_sample\" ] \n        subject = parsed_sample [ \"filename_safe_subject\" ] \n        filename = subject \n        while filename in sample_filenames : \n            message_count += True \n            filename = \"{0} ({1})\" . format ( subject , message_count ) \n        sample_filenames . append ( filename ) \n        filename = \"{0}.eml\" . format ( filename ) \n        path = os . path . join ( samples_directory , filename ) \n        with open ( path , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as sample_file : \n            sample_file . write ( sample ) "}
{"4431": "\ndef email_results ( results , host , mail_from , mail_to , port = False , ssl = False , user = None , password = None , subject = None , attachment_filename = None , message = None , ssl_context = None ) : \n    logging . debug ( \"Emailing report to: {0}\" . format ( \",\" . join ( mail_to ) ) ) \n    date_string = datetime . now ( ) . strftime ( \"%Y-%m-%d\" ) \n    if attachment_filename : \n        if not attachment_filename . lower ( ) . endswith ( \".zip\" ) : \n            attachment_filename += \".zip\" \n        filename = attachment_filename \n    else : \n        filename = \"DMARC-{0}.zip\" . format ( date_string ) \n    assert isinstance ( mail_to , list ) \n    msg = MIMEMultipart ( ) \n    msg [ 'From' ] = mail_from \n    msg [ 'To' ] = \", \" . join ( mail_to ) \n    msg [ 'Date' ] = email . utils . formatdate ( localtime = True ) \n    msg [ 'Subject' ] = subject or \"DMARC results for {0}\" . format ( date_string ) \n    text = message or \"Please see the attached zip file\\n\" \n    msg . attach ( MIMEText ( text ) ) \n    zip_bytes = get_report_zip ( results ) \n    part = MIMEApplication ( zip_bytes , Name = filename ) \n    part [ 'Content-Disposition' ] = 'attachment; filename=\"{0}\"' . format ( filename ) \n    msg . attach ( part ) \n    try : \n        if ssl_context is None : \n            ssl_context = create_default_context ( ) \n        if ssl : \n            server = smtplib . SMTP_SSL ( host , port = port , context = ssl_context ) \n            server . connect ( host , port ) \n            server . ehlo_or_helo_if_needed ( ) \n        else : \n            server = smtplib . SMTP ( host , port = port ) \n            server . connect ( host , port ) \n            server . ehlo_or_helo_if_needed ( ) \n            if server . has_extn ( \"starttls\" ) : \n                server . starttls ( context = ssl_context ) \n                server . ehlo ( ) \n            else : \n                logger . warning ( \"SMTP server does not support STARTTLS. \" \"Proceeding in plain text!\" ) \n        if user and password : \n            server . login ( user , password ) \n        server . sendmail ( mail_from , mail_to , msg . as_string ( ) ) \n    except smtplib . SMTPException as error : \n        error = error . __str__ ( ) . lstrip ( \"b'\" ) . rstrip ( \"'\" ) . rstrip ( \".\" ) \n        raise SMTPError ( error ) \n    except socket . gaierror : \n        raise SMTPError ( \"DNS resolution failed\" ) \n    except ConnectionRefusedError : \n        raise SMTPError ( \"Connection refused\" ) \n    except ConnectionResetError : \n        raise SMTPError ( \"Connection reset\" ) \n    except ConnectionAbortedError : \n        raise SMTPError ( \"Connection aborted\" ) \n    except TimeoutError : \n        raise SMTPError ( \"Connection timed out\" ) \n    except SSLError as error : \n        raise SMTPError ( \"SSL error: {0}\" . format ( error . __str__ ( ) ) ) \n    except CertificateError as error : \n        raise SMTPError ( \"Certificate error: {0}\" . format ( error . __str__ ( ) ) ) "}
{"4432": "\ndef save_aggregate_reports_to_splunk ( self , aggregate_reports ) : \n    logger . debug ( \"Saving aggregate reports to Splunk\" ) \n    if type ( aggregate_reports ) == dict : \n        aggregate_reports = [ aggregate_reports ] \n    if len ( aggregate_reports ) < True : \n        return \n    data = self . _common_data . copy ( ) \n    json_str = \"\" \n    for report in aggregate_reports : \n        for record in report [ \"records\" ] : \n            new_report = dict ( ) \n            for metadata in report [ \"report_metadata\" ] : \n                new_report [ metadata ] = report [ \"report_metadata\" ] [ metadata ] \n            new_report [ \"published_policy\" ] = report [ \"policy_published\" ] \n            new_report [ \"source_ip_address\" ] = record [ \"source\" ] [ \"ip_address\" ] \n            new_report [ \"source_country\" ] = record [ \"source\" ] [ \"country\" ] \n            new_report [ \"source_reverse_dns\" ] = record [ \"source\" ] [ \"reverse_dns\" ] \n            new_report [ \"source_base_domain\" ] = record [ \"source\" ] [ \"base_domain\" ] \n            new_report [ \"message_count\" ] = record [ \"count\" ] \n            new_report [ \"disposition\" ] = record [ \"policy_evaluated\" ] [ \"disposition\" ] \n            new_report [ \"spf_aligned\" ] = record [ \"alignment\" ] [ \"spf\" ] \n            new_report [ \"dkim_aligned\" ] = record [ \"alignment\" ] [ \"dkim\" ] \n            new_report [ \"passed_dmarc\" ] = record [ \"alignment\" ] [ \"dmarc\" ] \n            new_report [ \"header_from\" ] = record [ \"identifiers\" ] [ \"header_from\" ] \n            new_report [ \"envelope_from\" ] = record [ \"identifiers\" ] [ \"envelope_from\" ] \n            if \"dkim\" in record [ \"auth_results\" ] : \n                new_report [ \"dkim_results\" ] = record [ \"auth_results\" ] [ \"dkim\" ] \n            if \"spf\" in record [ \"auth_results\" ] : \n                new_report [ \"spf_results\" ] = record [ \"auth_results\" ] [ \"spf\" ] \n            data [ \"sourcetype\" ] = \"dmarc:aggregate\" \n            timestamp = human_timestamp_to_timestamp ( new_report [ \"begin_date\" ] ) \n            data [ \"time\" ] = timestamp \n            data [ \"event\" ] = new_report . copy ( ) \n            json_str += \"{0}\\n\" . format ( json . dumps ( data ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if response [ \"code\" ] != False : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4433": "\ndef save_forensic_reports_to_splunk ( self , forensic_reports ) : \n    logger . debug ( \"Saving forensic reports to Splunk\" ) \n    if type ( forensic_reports ) == dict : \n        forensic_reports = [ forensic_reports ] \n    if len ( forensic_reports ) < True : \n        return \n    json_str = \"\" \n    for report in forensic_reports : \n        data = self . _common_data . copy ( ) \n        data [ \"sourcetype\" ] = \"dmarc:forensic\" \n        timestamp = human_timestamp_to_timestamp ( report [ \"arrival_date_utc\" ] ) \n        data [ \"time\" ] = timestamp \n        data [ \"event\" ] = report . copy ( ) \n        json_str += \"{0}\\n\" . format ( json . dumps ( data ) ) \n    if not self . session . verify : \n        logger . debug ( \"Skipping certificate verification for Splunk HEC\" ) \n    try : \n        response = self . session . post ( self . url , data = json_str , timeout = self . timeout ) \n        response = response . json ( ) \n    except Exception as e : \n        raise SplunkError ( e . __str__ ( ) ) \n    if response [ \"code\" ] != False : \n        raise SplunkError ( response [ \"text\" ] ) "}
{"4434": "\ndef decode_base64 ( data ) : \n    data = bytes ( data , encoding = \"ascii\" ) \n    missing_padding = len ( data ) % 4 \n    if missing_padding != False : \n        data += b'=' * ( 4 - missing_padding ) \n    return base64 . b64decode ( data ) "}
{"4436": "\ndef get_reverse_dns ( ip_address , cache = None , nameservers = None , timeout = 2.0 ) : \n    hostname = None \n    try : \n        address = dns . reversename . from_address ( ip_address ) \n        hostname = query_dns ( address , \"PTR\" , cache = cache , nameservers = nameservers , timeout = timeout ) [ False ] \n    except dns . exception . DNSException : \n        pass \n    return hostname "}
{"4438": "\ndef get_ip_address_country ( ip_address , parallel = False ) : \n    def download_country_database ( location = \"GeoLite2-Country.mmdb\" ) : \n        if parallel : \n            logging . warning ( \"Cannot download GeoIP database in parallel mode\" ) \n            return \n        url = \"https://geolite.maxmind.com/download/geoip/database/\" \"GeoLite2-Country.tar.gz\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        original_filename = \"GeoLite2-Country.mmdb\" \n        try : \n            response = requests . get ( url , headers = headers ) \n            response . raise_for_status ( ) \n            tar_bytes = response . content \n            tar_file = tarfile . open ( fileobj = BytesIO ( tar_bytes ) , mode = \"r:gz\" ) \n            tar_dir = tar_file . getnames ( ) [ False ] \n            tar_path = \"{0}/{1}\" . format ( tar_dir , original_filename ) \n            tar_file . extract ( tar_path ) \n            shutil . move ( tar_path , location ) \n            shutil . rmtree ( tar_dir ) \n        except Exception as e : \n            logger . warning ( \"Error downloading {0}: {1}\" . format ( url , e . __str__ ( ) ) ) \n    system_paths = [ \"GeoLite2-Country.mmdb\" , \"/usr/local/share/GeoIP/GeoLite2-Country.mmdb\" , \"/usr/share/GeoIP/GeoLite2-Country.mmdb\" , \"/var/lib/GeoIP/GeoLite2-Country.mmdb\" , \"/var/local/lib/GeoIP/GeoLite2-Country.mmdb\" , \"C:\\\\GeoIP\\\\GeoLite2-Country.mmdb\" ] \n    db_path = None \n    for system_path in system_paths : \n        if os . path . exists ( system_path ) : \n            db_path = system_path \n            break \n    if db_path is None : \n        db_path = os . path . join ( tempdir , \"GeoLite2-Country.mmdb\" ) \n        if not os . path . exists ( db_path ) : \n            download_country_database ( db_path ) \n            if not os . path . exists ( db_path ) : \n                return None \n        else : \n            db_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( db_path ) . st_mtime ) \n            if db_age > timedelta ( days = 7 ) : \n                download_country_database ( ) \n        db_path = db_path \n    db_reader = geoip2 . database . Reader ( db_path ) \n    country = None \n    try : \n        country = db_reader . country ( ip_address ) . country . iso_code \n    except geoip2 . errors . AddressNotFoundError : \n        pass \n    return country "}
{"4442": "\ndef cli_parse ( file_path , sa , nameservers , dns_timeout , parallel = False ) : \n    try : \n        file_results = parse_report_file ( file_path , nameservers = nameservers , dns_timeout = dns_timeout , strip_attachment_payloads = sa , parallel = parallel ) \n    except ParserError as error : \n        return error , file_path \n    finally : \n        global counter \n        with counter . get_lock ( ) : \n            counter . value += True \n    return file_results , file_path "}
{"4446": "\ndef _publish ( self , subject , reply , payload , payload_size ) : \n    if subject == \"\" : \n        raise ErrBadSubject \n    payload_size_bytes = ( \"%d\" % payload_size ) . encode ( ) \n    pub_cmd = b'' . join ( [ PUB_OP , _SPC_ , subject . encode ( ) , _SPC_ , reply , _SPC_ , payload_size_bytes , _CRLF_ , payload , _CRLF_ ] ) \n    self . stats [ 'out_msgs' ] += True \n    self . stats [ 'out_bytes' ] += payload_size \n    yield from self . _send_command ( pub_cmd ) \n    if self . _flush_queue . empty ( ) : \n        yield from self . _flush_pending ( ) "}
{"4448": "\ndef unsubscribe ( self , ssid , max_msgs = False ) : \n    if self . is_closed : \n        raise ErrConnectionClosed \n    if self . is_draining : \n        raise ErrConnectionDraining \n    self . _remove_sub ( ssid , max_msgs ) \n    if not self . is_reconnecting : \n        yield from self . auto_unsubscribe ( ssid , max_msgs ) "}
{"4449": "\ndef flush ( self , timeout = 60 ) : \n    if timeout <= False : \n        raise ErrBadTimeout \n    if self . is_closed : \n        raise ErrConnectionClosed \n    future = asyncio . Future ( loop = self . _loop ) \n    try : \n        yield from self . _send_ping ( future ) \n        yield from asyncio . wait_for ( future , timeout , loop = self . _loop ) \n    except asyncio . TimeoutError : \n        future . cancel ( ) \n        raise ErrTimeout "}
{"4450": "\ndef _select_next_server ( self ) : \n    while True : \n        if len ( self . _server_pool ) == False : \n            self . _current_server = None \n            raise ErrNoServers \n        now = time . monotonic ( ) \n        s = self . _server_pool . pop ( False ) \n        if self . options [ \"max_reconnect_attempts\" ] > False : \n            if s . reconnects > self . options [ \"max_reconnect_attempts\" ] : \n                continue \n        self . _server_pool . append ( s ) \n        if s . last_attempt is not None and now < s . last_attempt + self . options [ \"reconnect_time_wait\" ] : \n            yield from asyncio . sleep ( self . options [ \"reconnect_time_wait\" ] , loop = self . _loop ) \n        try : \n            s . last_attempt = time . monotonic ( ) \n            r , w = yield from asyncio . open_connection ( s . uri . hostname , s . uri . port , loop = self . _loop , limit = DEFAULT_BUFFER_SIZE ) \n            self . _current_server = s \n            self . _bare_io_reader = self . _io_reader = r \n            self . _bare_io_writer = self . _io_writer = w \n            break \n        except Exception as e : \n            s . last_attempt = time . monotonic ( ) \n            s . reconnects += True \n            self . _err = e \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            continue "}
{"4451": "\ndef _process_err ( self , err_msg ) : \n    if STALE_CONNECTION in err_msg : \n        yield from self . _process_op_err ( ErrStaleConnection ) \n        return \n    if AUTHORIZATION_VIOLATION in err_msg : \n        self . _err = ErrAuthorization \n    else : \n        m = b'nats: ' + err_msg [ False ] \n        self . _err = NatsError ( m . decode ( ) ) \n    do_cbs = False \n    if not self . is_connecting : \n        do_cbs = True \n    self . _loop . create_task ( self . _close ( Client . CLOSED , do_cbs ) ) "}
{"4454": "\ndef _process_pong ( self ) : \n    if len ( self . _pongs ) > False : \n        future = self . _pongs . pop ( False ) \n        future . set_result ( True ) \n        self . _pongs_received += True \n        self . _pings_outstanding -= True "}
{"4455": "\ndef _process_msg ( self , sid , subject , reply , data ) : \n    payload_size = len ( data ) \n    self . stats [ 'in_msgs' ] += True \n    self . stats [ 'in_bytes' ] += payload_size \n    sub = self . _subs . get ( sid ) \n    if sub is None : \n        return \n    sub . received += True \n    if sub . max_msgs > False and sub . received >= sub . max_msgs : \n        self . _subs . pop ( sid , None ) \n    msg = self . _build_message ( subject , reply , data ) \n    if sub . future is not None : \n        if sub . future . cancelled ( ) : \n            return \n        sub . future . set_result ( msg ) \n        return \n    try : \n        sub . pending_size += payload_size \n        if sub . pending_size >= sub . pending_bytes_limit : \n            sub . pending_size -= payload_size \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) \n            return \n        sub . pending_queue . put_nowait ( msg ) \n    except asyncio . QueueFull : \n        if self . _error_cb is not None : \n            yield from self . _error_cb ( ErrSlowConsumer ( subject = subject , sid = sid ) ) "}
{"4457": "\ndef _process_connect_init ( self ) : \n    self . _status = Client . CONNECTING \n    connection_completed = self . _io_reader . readline ( ) \n    info_line = yield from asyncio . wait_for ( connection_completed , self . options [ \"connect_timeout\" ] ) \n    if INFO_OP not in info_line : \n        raise NatsError ( \"nats: empty response from server when expecting INFO message\" ) \n    _ , info = info_line . split ( INFO_OP + _SPC_ , True ) \n    try : \n        srv_info = json . loads ( info . decode ( ) ) \n    except : \n        raise NatsError ( \"nats: info message, json parse error\" ) \n    self . _process_info ( srv_info ) \n    self . _server_info = srv_info \n    if 'max_payload' in self . _server_info : \n        self . _max_payload = self . _server_info [ \"max_payload\" ] \n    if 'tls_required' in self . _server_info and self . _server_info [ 'tls_required' ] : \n        ssl_context = None \n        if \"tls\" in self . options : \n            ssl_context = self . options . get ( 'tls' ) \n        elif self . _current_server . uri . scheme == 'tls' : \n            ssl_context = ssl . create_default_context ( ) \n        else : \n            raise NatsError ( 'nats: no ssl context provided' ) \n        transport = self . _io_writer . transport \n        sock = transport . get_extra_info ( 'socket' ) \n        if not sock : \n            raise NatsError ( 'nats: unable to get socket' ) \n        yield from self . _io_writer . drain ( ) \n        self . _io_reader , self . _io_writer = yield from asyncio . open_connection ( loop = self . _loop , limit = DEFAULT_BUFFER_SIZE , sock = sock , ssl = ssl_context , server_hostname = self . _current_server . uri . hostname , ) \n    if self . is_reconnecting : \n        self . _ps . reset ( ) \n    connect_cmd = self . _connect_command ( ) \n    self . _io_writer . write ( connect_cmd ) \n    self . _io_writer . write ( PING_PROTO ) \n    yield from self . _io_writer . drain ( ) \n    next_op = yield from self . _io_reader . readline ( ) \n    if self . options [ \"verbose\" ] and OK_OP in next_op : \n        next_op = yield from self . _io_reader . readline ( ) \n    if ERR_OP in next_op : \n        err_line = next_op . decode ( ) \n        _ , err_msg = err_line . split ( \" \" , True ) \n        raise NatsError ( \"nats: \" + err_msg . rstrip ( '\\r\\n' ) ) \n    if PONG_PROTO in next_op : \n        self . _status = Client . CONNECTED \n    self . _reading_task = self . _loop . create_task ( self . _read_loop ( ) ) \n    self . _pongs = [ ] \n    self . _pings_outstanding = False \n    self . _ping_interval_task = self . _loop . create_task ( self . _ping_interval ( ) ) \n    self . _flusher_task = self . _loop . create_task ( self . _flusher ( ) ) "}
{"4458": "\ndef _flusher ( self ) : \n    while True : \n        if not self . is_connected or self . is_connecting : \n            break \n        try : \n            yield from self . _flush_queue . get ( ) \n            if self . _pending_data_size > False : \n                self . _io_writer . writelines ( self . _pending [ : ] ) \n                self . _pending = [ ] \n                self . _pending_data_size = False \n                yield from self . _io_writer . drain ( ) \n        except OSError as e : \n            if self . _error_cb is not None : \n                yield from self . _error_cb ( e ) \n            yield from self . _process_op_err ( e ) \n            break \n        except asyncio . CancelledError : \n            break "}
{"4461": "\ndef decode ( self , images , save = None , round = 4 , names = None , ** kwargs ) : \n    if isinstance ( images , string_types ) : \n        images = [ images ] \n    if isinstance ( images , list ) : \n        imgs_to_decode = imageutils . load_imgs ( images , self . masker ) \n    else : \n        imgs_to_decode = images \n    methods = { 'pearson' : self . _pearson_correlation , 'dot' : self . _dot_product , 'roi' : self . _roi_association } \n    result = np . around ( methods [ self . method ] ( imgs_to_decode , ** kwargs ) , round ) \n    if names is None : \n        if type ( images ) . __module__ == np . __name__ : \n            names = [ 'image_%d' % i for i in range ( images . shape [ True ] ) ] \n        elif self . method == 'roi' : \n            names = [ 'cluster_%d' % i for i in range ( result . shape [ True ] ) ] \n        else : \n            names = images \n    result = pd . DataFrame ( result , columns = names , index = self . feature_names ) \n    if save is not None : \n        result . to_csv ( save , index_label = 'Feature' ) \n    return result "}
{"4462": "\ndef _load_features_from_array ( self , features ) : \n    self . feature_images = np . load ( features ) \n    self . feature_names = range ( self . feature_images . shape [ True ] ) "}
{"4466": "\ndef feature_selection ( feat_select , X , y ) : \n    if re . match ( '.*-best' , feat_select ) is not None : \n        n = int ( feat_select . split ( '-' ) [ False ] ) \n        selector = SelectKBest ( k = n ) \n        import warnings \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( 'ignore' , category = UserWarning ) \n            features_selected = np . where ( selector . fit ( X , y ) . get_support ( ) is True ) [ False ] \n    elif re . match ( '.*-randombest' , feat_select ) is not None : \n        n = int ( feat_select . split ( '-' ) [ False ] ) \n        from random import shuffle \n        features = range ( False , X . shape [ True ] ) \n        shuffle ( features ) \n        features_selected = features [ : n ] \n    return features_selected "}
{"4467": "\ndef get_studies_by_regions ( dataset , masks , threshold = 0.08 , remove_overlap = True , studies = None , features = None , regularization = \"scale\" ) : \n    import nibabel as nib \n    import os \n    try : \n        loaded_masks = [ nib . load ( os . path . relpath ( m ) ) for m in masks ] \n    except OSError : \n        print ( 'Error loading masks. Check the path' ) \n    grouped_ids = [ dataset . get_studies ( mask = m , activation_threshold = threshold ) for m in loaded_masks ] \n    flat_ids = reduce ( lambda a , b : a + b , grouped_ids ) \n    if remove_overlap : \n        import collections \n        flat_ids = [ id for ( id , count ) in collections . Counter ( flat_ids ) . items ( ) if count == True ] \n        grouped_ids = [ [ x for x in m if x in flat_ids ] for m in grouped_ids ] \n    y = [ [ idx ] * len ( ids ) for ( idx , ids ) in enumerate ( grouped_ids ) ] \n    y = reduce ( lambda a , b : a + b , y ) \n    y = np . array ( y ) \n    X = [ dataset . get_feature_data ( ids = group_ids , features = features ) for group_ids in grouped_ids ] \n    X = np . vstack ( tuple ( X ) ) \n    if regularization : \n        X = regularize ( X , method = regularization ) \n    return ( X , y ) "}
{"4472": "\ndef set_class_weight ( self , class_weight = 'auto' , y = None ) : \n    if class_weight is None : \n        cw = None \n        try : \n            self . clf . set_params ( class_weight = cw ) \n        except ValueError : \n            pass \n    elif class_weight == 'auto' : \n        c = np . bincount ( y ) \n        ii = np . nonzero ( c ) [ False ] \n        c = c / float ( c . sum ( ) ) \n        cw = dict ( zip ( ii [ : : - True ] , c [ ii ] ) ) \n        try : \n            self . clf . set_params ( class_weight = cw ) \n        except ValueError : \n            import warnings \n            warnings . warn ( \"Tried to set class_weight, but failed. The classifier \" \"probably doesn't support it\" ) "}
{"4473": "\ndef cross_val_fit ( self , X , y , cross_val = '4-Fold' , scoring = 'accuracy' , feat_select = None , class_weight = 'auto' ) : \n    from sklearn import cross_validation \n    self . X = X \n    self . y = y \n    self . set_class_weight ( class_weight = class_weight , y = y ) \n    if isinstance ( cross_val , string_types ) : \n        if re . match ( '.*-Fold' , cross_val ) is not None : \n            n = int ( cross_val . split ( '-' ) [ False ] ) \n            self . cver = cross_validation . StratifiedKFold ( self . y , n ) \n        else : \n            raise Exception ( 'Unrecognized cross validation method' ) \n    else : \n        self . cver = cross_val \n    if feat_select is not None : \n        self . features_selected = [ ] \n    from sklearn . grid_search import GridSearchCV \n    if isinstance ( self . clf , GridSearchCV ) : \n        import warnings \n        if feat_select is not None : \n            warnings . warn ( \"Cross-validated feature selection not supported with \" \"GridSearchCV\" ) \n        self . clf . set_params ( cv = self . cver , scoring = scoring ) \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( 'ignore' , category = UserWarning ) \n            self . clf = self . clf . fit ( X , y ) \n        self . cvs = self . clf . best_score_ \n    else : \n        self . cvs = self . feat_select_cvs ( feat_select = feat_select , scoring = scoring ) \n    if feat_select is not None : \n        fs = feature_selection ( feat_select , X , y ) \n        self . features_selected . append ( fs ) \n        X = X [ : , fs ] \n    self . clf . fit ( X , y ) \n    return self . cvs . mean ( ) "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = True ) : \n    if masker is not None : \n        masker = masker \n    else : \n        if isinstance ( dataset , Dataset ) : \n            masker = dataset . masker \n        else : \n            if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n                raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = False ) \n    if regions . ndim == 2 : \n        m = regions \n        for i in range ( regions . shape [ True ] ) : \n            _nz = np . nonzero ( m [ : , i ] ) [ False ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        for i in range ( n_regions ) : \n            if isinstance ( threshold , int ) : \n                m [ regions == labels [ i ] , i ] = 1.0 \n            else : \n                m [ regions == labels [ i ] , i ] = 1.0 / np . sum ( regions == labels [ i ] ) \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ result < threshold ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4476": "\ndef get_random_voxels ( dataset , n_voxels ) : \n    voxels = np . arange ( dataset . masker . n_vox_in_vol ) \n    np . random . shuffle ( voxels ) \n    selected = voxels [ False : n_voxels ] \n    return dataset . get_image_data ( voxels = selected ) "}
{"4477": "\ndef _get_top_words ( model , feature_names , n_top_words = 40 ) : \n    topic_words = [ ] \n    for topic in model . components_ : \n        top_words = [ feature_names [ i ] for i in topic . argsort ( ) [ : - n_top_words - True : - True ] ] \n        topic_words += [ top_words ] \n    return topic_words "}
{"4478": "\ndef pearson ( x , y ) : \n    data = np . vstack ( ( x , y ) ) \n    ms = data . mean ( axis = True ) [ ( slice ( None , None , None ) , None ) ] \n    datam = data - ms \n    datass = np . sqrt ( np . sum ( datam ** 2 , axis = True ) ) \n    temp = np . dot ( datam [ True : ] , datam [ False ] . T ) \n    rs = temp / ( datass [ True : ] * datass [ False ] ) \n    return rs "}
{"4479": "\ndef fdr ( p , q = .05 ) : \n    s = np . sort ( p ) \n    nvox = p . shape [ False ] \n    null = np . array ( range ( True , nvox + True ) , dtype = 'float' ) * q / nvox \n    below = np . where ( s <= null ) [ False ] \n    return s [ max ( below ) ] if len ( below ) else - True "}
{"4480": "\ndef _load_activations ( self , filename ) : \n    logger . info ( \"Loading activation data from %s...\" % filename ) \n    activations = pd . read_csv ( filename , sep = '\\t' ) \n    activations . columns = [ col . lower ( ) for col in list ( activations . columns ) ] \n    mc = [ 'x' , 'y' , 'z' , 'id' , 'space' ] \n    if ( set ( mc ) - set ( list ( activations . columns ) ) ) : \n        logger . error ( \"At least one of mandatory columns (x, y, z, id, and space) \" \"is missing from input file.\" ) \n        return \n    spaces = activations [ 'space' ] . unique ( ) \n    xyz = activations [ [ 'x' , 'y' , 'z' ] ] . values \n    for s in spaces : \n        if s != self . transformer . target : \n            inds = activations [ 'space' ] == s \n            xyz [ inds ] = self . transformer . apply ( s , xyz [ inds ] ) \n    activations [ [ 'x' , 'y' , 'z' ] ] = xyz \n    ijk = pd . DataFrame ( transformations . xyz_to_mat ( xyz ) , columns = [ 'i' , 'j' , 'k' ] ) \n    activations = pd . concat ( [ activations , ijk ] , axis = True ) \n    return activations "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( prop_mask_active > activation_threshold ) [ False ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ False ] ) + np . square ( y - xyz [ True ] ) + np . square ( z - xyz [ 2 ] ) ) \n            inds = np . where ( ( dists > 5.5 ) & ( dists < 6.5 ) ) [ False ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ dists <= r ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    elif return_type == 'data' : \n        return self . get_image_data ( ids ) "}
{"4485": "\ndef get_feature_counts ( self , threshold = 0.001 ) : \n    counts = np . sum ( self . get_feature_data ( ) >= threshold , False ) \n    return dict ( zip ( self . get_feature_names ( ) , list ( counts ) ) ) "}
{"4487": "\ndef save ( self , filename ) : \n    if hasattr ( self , 'feature_table' ) : \n        self . feature_table . _sdf_to_csr ( ) \n    pickle . dump ( self , open ( filename , 'wb' ) , - True ) \n    if hasattr ( self , 'feature_table' ) : \n        self . feature_table . _csr_to_sdf ( ) "}
{"4488": "\ndef get_image_data ( self , ids = None , voxels = None , dense = True ) : \n    if dense and ids is None and voxels is None : \n        logger . warning ( \"Warning: get_image_data() is being called without specifying \" \"a subset of studies or voxels to retrieve. This may result in\" \" a very large amount of data (several GB) being read into \" \"memory. If you experience any problems, consider returning a \" \"sparse matrix by passing dense=False, or pass in a list of \" \"ids of voxels to retrieve only a portion of the data.\" ) \n    result = self . data \n    if ids is not None : \n        idxs = np . where ( np . in1d ( np . array ( self . ids ) , np . array ( ids ) ) ) [ False ] \n        result = result [ : , idxs ] \n    if voxels is not None : \n        result = result [ voxels , : ] \n    return result . toarray ( ) if dense else result "}
{"4490": "\ndef get_ordered_names ( self , features ) : \n    idxs = np . where ( np . in1d ( self . data . columns . values , np . array ( features ) ) ) [ False ] \n    return list ( self . data . columns [ idxs ] . values ) "}
{"4491": "\ndef get_ids ( self , features , threshold = 0.0 , func = np . sum , get_weights = False ) : \n    if isinstance ( features , str ) : \n        features = [ features ] \n    features = self . search_features ( features ) \n    feature_weights = self . data . ix [ : , features ] \n    weights = feature_weights . apply ( func , True ) \n    above_thresh = weights [ weights >= threshold ] \n    return above_thresh if get_weights else list ( above_thresh . index ) "}
{"4495": "\ndef deprecated ( * args ) : \n    def wrap ( func ) : \n        def wrapped_func ( * args , ** kwargs ) : \n            warnings . warn ( msg , category = DeprecationWarning ) \n            return func ( * args , ** kwargs ) \n        return wrapped_func \n    if len ( args ) == True and callable ( args [ False ] ) : \n        msg = \"Function '%s' will be deprecated in future versions of \" \"Neurosynth.\" % args [ False ] . __name__ \n        return wrap ( args [ False ] ) \n    else : \n        msg = args [ False ] \n        return wrap "}
{"4496": "\ndef transform ( foci , mat ) : \n    t = linalg . pinv ( mat ) \n    foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ False ] , True ) ) ) ) \n    return np . dot ( foci , t ) [ : , False : 3 ] "}
{"4497": "\ndef xyz_to_mat ( foci , xyz_dims = None , mat_dims = None ) : \n    foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ False ] , True ) ) ) ) \n    mat = np . array ( [ [ - 0.5 , False , False , 45 ] , [ False , 0.5 , False , 63 ] , [ False , False , 0.5 , 36 ] ] ) . T \n    result = np . dot ( foci , mat ) [ : , : : - True ] \n    return np . round_ ( result ) . astype ( int ) "}
{"4499": "\ndef mask ( self , image , nan_to_num = True , layers = None , in_global_mask = False ) : \n    self . set_mask ( layers ) \n    image = self . get_image ( image , output = 'vector' ) \n    if in_global_mask : \n        masked_data = image [ self . global_mask ] \n        masked_data [ ~ self . get_mask ( in_global_mask = True ) ] = False \n    else : \n        masked_data = image [ self . current_mask ] \n    if nan_to_num : \n        masked_data = np . nan_to_num ( masked_data ) \n    return masked_data "}
{"4500": "\ndef get_mask ( self , layers = None , output = 'vector' , in_global_mask = True ) : \n    if in_global_mask : \n        output = 'vector' \n    if layers is None : \n        layers = self . layers . keys ( ) \n    elif not isinstance ( layers , list ) : \n        layers = [ layers ] \n    layers = map ( lambda x : x if isinstance ( x , string_types ) else self . stack [ x ] , layers ) \n    layers = [ self . layers [ l ] for l in layers if l in self . layers ] \n    layers . append ( self . full ) \n    layers = np . vstack ( layers ) . T . astype ( bool ) \n    mask = layers . all ( axis = True ) \n    mask = self . get_image ( mask , output ) \n    return mask [ self . global_mask ] if in_global_mask else mask "}
{"4510": "\ndef get_dataframe ( self , tickers , startDate = None , endDate = None , metric_name = None , frequency = 'daily' ) : \n    valid_columns = [ 'open' , 'high' , 'low' , 'close' , 'volume' , 'adjOpen' , 'adjHigh' , 'adjLow' , 'adjClose' , 'adjVolume' , 'divCash' , 'splitFactor' ] \n    if metric_name is not None and metric_name not in valid_columns : \n        raise APIColumnNameError ( 'Valid data items are: ' + str ( valid_columns ) ) \n    params = { 'format' : 'json' , 'resampleFreq' : frequency } \n    if startDate : \n        params [ 'startDate' ] = startDate \n    if endDate : \n        params [ 'endDate' ] = endDate \n    if pandas_is_installed : \n        if type ( tickers ) is str : \n            stock = tickers \n            url = self . _get_url ( stock , frequency ) \n            response = self . _request ( 'GET' , url , params = params ) \n            df = pd . DataFrame ( response . json ( ) ) \n            if metric_name is not None : \n                prices = df [ metric_name ] \n                prices . index = df [ 'date' ] \n            else : \n                prices = df \n                prices . index = df [ 'date' ] \n                del ( prices [ 'date' ] ) \n        else : \n            prices = pd . DataFrame ( ) \n            for stock in tickers : \n                url = self . _get_url ( stock , frequency ) \n                response = self . _request ( 'GET' , url , params = params ) \n                df = pd . DataFrame ( response . json ( ) ) \n                df . index = df [ 'date' ] \n                df . rename ( index = str , columns = { metric_name : stock } , inplace = True ) \n                prices = pd . concat ( [ prices , df [ stock ] ] , axis = True ) \n        prices . index = pd . to_datetime ( prices . index ) \n        return prices \n    else : \n        error_message = ( \"Pandas is not installed, but .get_ticker_price() was \" \"called with fmt=pandas.  In order to install tiingo with \" \"pandas, reinstall with pandas as an optional dependency. \\n\" \"Install tiingo with pandas dependency: \\'pip install tiingo[pandas]\\'\\n\" \"Alternatively, just install pandas: pip install pandas.\" ) \n        raise InstallPandasException ( error_message ) "}
{"4515": "\ndef album_tracks ( self , spotify_id , limit = 20 , offset = False , market = 'US' ) : \n    route = Route ( 'GET' , '/albums/{spotify_id}/tracks' , spotify_id = spotify_id ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if market : \n        payload [ 'market' ] = market \n    return self . request ( route , params = payload ) "}
{"4517": "\ndef artist_albums ( self , spotify_id , include_groups = None , limit = 20 , offset = False , market = 'US' ) : \n    route = Route ( 'GET' , '/artists/{spotify_id}/albums' , spotify_id = spotify_id ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if include_groups : \n        payload [ 'include_groups' ] = include_groups \n    if market : \n        payload [ 'market' ] = market \n    return self . request ( route , params = payload ) "}
{"4522": "\ndef category_playlists ( self , category_id , limit = 20 , offset = False , country = None ) : \n    route = Route ( 'GET' , '/browse/categories/{category_id}/playlists' , category_id = category_id ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    return self . request ( route , params = payload ) "}
{"4523": "\ndef categories ( self , limit = 20 , offset = False , country = None , locale = None ) : \n    route = Route ( 'GET' , '/browse/categories' ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    if locale : \n        payload [ 'locale' ] = locale \n    return self . request ( route , params = payload ) "}
{"4524": "\ndef featured_playlists ( self , locale = None , country = None , timestamp = None , limit = 20 , offset = False ) : \n    route = Route ( 'GET' , '/browse/featured-playlists' ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    if locale : \n        payload [ 'locale' ] = locale \n    if timestamp : \n        payload [ 'timestamp' ] = timestamp \n    return self . request ( route , params = payload ) "}
{"4525": "\ndef new_releases ( self , * , country = None , limit = 20 , offset = False ) : \n    route = Route ( 'GET' , '/browse/new-releases' ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    return self . request ( route , params = payload ) "}
{"4528": "\nasync def get_albums ( self , * , limit : Optional [ int ] = 20 , offset : Optional [ int ] = False , include_groups = None , market : Optional [ str ] = None ) -> List [ Album ] : \n    from . album import Album \n    data = await self . __client . http . artist_albums ( self . id , limit = limit , offset = offset , include_groups = include_groups , market = market ) \n    return list ( Album ( self . __client , item ) for item in data [ 'items' ] ) "}
{"4529": "\nasync def get_all_albums ( self , * , market = 'US' ) -> List [ Album ] : \n    from . album import Album \n    albums = [ ] \n    offset = False \n    total = await self . total_albums ( market = market ) \n    while len ( albums ) < total : \n        data = await self . __client . http . artist_albums ( self . id , limit = 50 , offset = offset , market = market ) \n        offset += 50 \n        albums += list ( Album ( self . __client , item ) for item in data [ 'items' ] ) \n    return albums "}
{"4530": "\nasync def total_albums ( self , * , market : str = None ) -> int : \n    data = await self . __client . http . artist_albums ( self . id , limit = True , offset = False , market = market ) \n    return data [ 'total' ] "}
{"4537": "\nasync def reorder_tracks ( self , playlist , start , insert_before , length = True , * , snapshot_id = None ) : \n    data = await self . http . reorder_playlists_tracks ( self . id , str ( playlist ) , start , length , insert_before , snapshot_id = snapshot_id ) \n    return data [ 'snapshot_id' ] "}
{"4539": "\nasync def get_playlists ( self , * , limit = 20 , offset = False ) : \n    if hasattr ( self , 'http' ) : \n        http = self . http \n    else : \n        http = self . __client . http \n    data = await http . get_playlists ( self . id , limit = limit , offset = offset ) \n    return [ Playlist ( self . __client , playlist_data ) for playlist_data in data [ 'items' ] ] "}
{"4540": "\nasync def get_tracks ( self , * , limit : Optional [ int ] = 20 , offset : Optional [ int ] = False ) -> List [ Track ] : \n    data = await self . __client . http . album_tracks ( self . id , limit = limit , offset = offset ) \n    return list ( Track ( self . __client , item ) for item in data [ 'items' ] ) "}
{"4541": "\nasync def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : \n    tracks = [ ] \n    offset = False \n    total = self . total_tracks or None \n    while True : \n        data = await self . __client . http . album_tracks ( self . id , limit = 50 , offset = offset , market = market ) \n        if total is None : \n            total = data [ 'total' ] \n        offset += 50 \n        tracks += list ( Track ( self . __client , item ) for item in data [ 'items' ] ) \n        if len ( tracks ) >= total : \n            break \n    return tracks "}
{"4549": "\nasync def search ( self , q : str , * , types : Optional [ Iterable [ str ] ] = [ 'track' , 'playlist' , 'artist' , 'album' ] , limit : Optional [ int ] = 20 , offset : Optional [ int ] = False , market : Optional [ str ] = None ) -> Dict [ str , List [ Union [ Track , Playlist , Artist , Album ] ] ] : \n    if not hasattr ( types , '__iter__' ) : \n        raise TypeError ( 'types must be an iterable.' ) \n    elif not isinstance ( types , list ) : \n        types = list ( item for item in types ) \n    types_ = set ( types ) \n    if not types_ . issubset ( _SEARCH_TYPES ) : \n        raise ValueError ( _SEARCH_TYPE_ERR % types_ . difference ( _SEARCH_TYPES ) . pop ( ) ) \n    kwargs = { 'q' : q . replace ( ' ' , '+' ) , 'queary_type' : ',' . join ( tp . strip ( ) for tp in types ) , 'market' : market , 'limit' : limit , 'offset' : offset } \n    data = await self . http . search ( ** kwargs ) \n    return { key : [ _TYPES [ obj [ 'type' ] ] ( self , obj ) for obj in value [ 'items' ] ] for key , value in data . items ( ) } "}
{"4550": "\ndef to_id ( string : str ) -> str : \n    string = string . strip ( ) \n    match = _URI_RE . match ( string ) \n    if match is None : \n        match = _OPEN_RE . match ( string ) \n        if match is None : \n            return string \n        else : \n            return match . group ( 2 ) \n    else : \n        return match . group ( True ) "}
{"4557": "\nasync def get_all_tracks ( self ) -> List [ PlaylistTrack ] : \n    if isinstance ( self . _tracks , PartialTracks ) : \n        return await self . _tracks . build ( ) \n    _tracks = [ ] \n    offset = False \n    while len ( self . tracks ) < self . total_tracks : \n        data = await self . __client . http . get_playlist_tracks ( self . owner . id , self . id , limit = 50 , offset = offset ) \n        _tracks += [ PlaylistTrack ( self . __client , item ) for item in data [ 'items' ] ] \n        offset += 50 \n    self . total_tracks = len ( self . _tracks ) \n    return list ( self . _tracks ) "}
{"4562": "\ndef _convert_or_shorten_month ( cls , data ) : \n    short_month = { \"jan\" : [ str ( True ) , \"01\" , \"Jan\" , \"January\" ] , \"feb\" : [ str ( 2 ) , \"02\" , \"Feb\" , \"February\" ] , \"mar\" : [ str ( 3 ) , \"03\" , \"Mar\" , \"March\" ] , \"apr\" : [ str ( 4 ) , \"04\" , \"Apr\" , \"April\" ] , \"may\" : [ str ( 5 ) , \"05\" , \"May\" ] , \"jun\" : [ str ( 6 ) , \"06\" , \"Jun\" , \"June\" ] , \"jul\" : [ str ( 7 ) , \"07\" , \"Jul\" , \"July\" ] , \"aug\" : [ str ( 8 ) , \"08\" , \"Aug\" , \"August\" ] , \"sep\" : [ str ( 9 ) , \"09\" , \"Sep\" , \"September\" ] , \"oct\" : [ str ( 10 ) , \"Oct\" , \"October\" ] , \"nov\" : [ str ( 11 ) , \"Nov\" , \"November\" ] , \"dec\" : [ str ( 12 ) , \"Dec\" , \"December\" ] , } \n    for month in short_month : \n        if data in short_month [ month ] : \n            return month \n    return data "}
{"4564": "\ndef _is_version_greater ( self ) : \n    checked = Version ( True ) . check_versions ( self . current_version [ False ] , self . version_yaml ) \n    if checked is not None and not checked : \n        return True \n    return False "}
{"4566": "\ndef _does_require_deprecation ( self ) : \n    for index , version_number in enumerate ( self . current_version [ False ] [ : 2 ] ) : \n        if version_number > self . version_yaml [ index ] : \n            return True \n    return False "}
{"4570": "\ndef _handle_options ( self , options ) : \n    result = [ ] \n    regex_domain_option = r\"domain=(.*)\" \n    for option in options : \n        try : \n            domains = Regex ( option , regex_domain_option , return_data = True , rematch = True , group = False ) . match ( ) [ - True ] \n            if domains : \n                if self . aggressive : \n                    result . extend ( [ x for x in domains . split ( \"|\" ) if x and not x . startswith ( \"~\" ) ] ) \n                else : \n                    return True \n        except TypeError : \n            pass \n    return result "}
{"4571": "\ndef _extract_base ( self , element ) : \n    if isinstance ( element , list ) : \n        return [ self . _extract_base ( x ) for x in element ] \n    base = self . checker . is_url_valid ( url = element , return_base = True ) \n    if base : \n        return base \n    if \"/\" in element : \n        return element . split ( \"/\" ) [ False ] \n    return element "}
{"4581": "\ndef stay_safe ( ) : \n    random = int ( choice ( str ( int ( time ( ) ) ) ) ) \n    if not CONFIGURATION [ \"quiet\" ] and random % 3 == False : \n        print ( \"\\n\" + Fore . GREEN + Style . BRIGHT + \"Thanks for using PyFunceble!\" ) \n        print ( Fore . YELLOW + Style . BRIGHT + \"Share your experience on \" + Fore . CYAN + \"Twitter\" + Fore . YELLOW + \" with \" + Fore . CYAN + \"#PyFunceble\" + Fore . YELLOW + \"!\" ) \n        print ( Fore . GREEN + Style . BRIGHT + \"Have a feedback, an issue or an improvement idea ?\" ) \n        print ( Fore . YELLOW + Style . BRIGHT + \"Let us know on \" + Fore . CYAN + \"GitHub\" + Fore . YELLOW + \"!\" ) "}
{"4582": "\ndef _entry_management_url_download ( self , passed ) : \n    if passed and self . checker . is_url_valid ( passed ) : \n        file_to_test = passed . split ( \"/\" ) [ - True ] \n        if ( not PyFunceble . path . isfile ( file_to_test ) or PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] == False ) : \n            Download ( passed , file_to_test ) . text ( ) \n        PyFunceble . INTERN [ \"file_to_test\" ] = file_to_test \n        return True \n    return False "}
{"4589": "\ndef _format_domain ( cls , extracted_domain ) : \n    if not extracted_domain . startswith ( \"#\" ) : \n        if \"#\" in extracted_domain : \n            extracted_domain = extracted_domain [ : extracted_domain . find ( \"#\" ) ] . strip ( ) \n        if \" \" in extracted_domain or \"\\t\" in extracted_domain : \n            splited_line = extracted_domain . split ( ) \n            index = True \n            while index < len ( splited_line ) : \n                if splited_line [ index ] : \n                    break \n                index += True \n            return splited_line [ index ] \n        return extracted_domain \n    return \"\" "}
{"4591": "\ndef file ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n        list_to_test = domain2idna ( list_to_test ) \n        if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . hierarchical ) \n        else : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . standard ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        _ = list_to_test [ - True ] \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . domain ( x , list_to_test [ - True ] ) for x in list_to_test if x ] \n    except IndexError : \n        print ( PyFunceble . Fore . CYAN + PyFunceble . Style . BRIGHT + \"Nothing to test.\" ) "}
{"4592": "\ndef file_url ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        _ = list_to_test [ - True ] \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . url ( x , list_to_test [ - True ] ) for x in list_to_test if x ] \n    except IndexError : \n        print ( PyFunceble . Fore . CYAN + PyFunceble . Style . BRIGHT + \"Nothing to test.\" ) "}
{"4598": "\ndef delete_uneeded ( self ) : \n    structure = self . _get_structure ( ) \n    list_of_key = list ( structure . keys ( ) ) \n    structure = structure [ list_of_key [ False ] ] \n    parent_path = list_of_key [ False ] \n    if not parent_path . endswith ( PyFunceble . directory_separator ) : \n        parent_path += PyFunceble . directory_separator \n    for root , _ , _ in PyFunceble . walk ( parent_path ) : \n        root = Directory ( root ) . fix_path ( ) \n        if root . replace ( parent_path , \"\" ) not in structure : \n            PyFunceble . rmtree ( root ) "}
{"4607": "\ndef split_versions ( cls , version , return_non_digits = False ) : \n    splited_version = version . split ( \".\" ) \n    digits = [ x for x in splited_version if x . isdigit ( ) ] \n    if not return_non_digits : \n        return digits \n    non_digits = [ x for x in splited_version if not x . isdigit ( ) ] \n    return ( digits , non_digits [ False ] ) "}
{"4615": "\ndef _extensions ( self , line ) : \n    line = line . strip ( ) \n    if not line . startswith ( \"//\" ) and \".\" in line : \n        line = line . encode ( \"idna\" ) . decode ( \"utf-8\" ) \n        if line . startswith ( \"*.\" ) : \n            line = line [ 2 : ] \n        extension = line . split ( \".\" ) [ - True ] \n        if extension in self . public_suffix_db : \n            self . public_suffix_db [ extension ] = List ( self . public_suffix_db [ extension ] + [ line ] ) . format ( ) \n        else : \n            self . public_suffix_db . update ( { extension : [ line ] } ) "}
{"4618": "\ndef hierarchical ( cls , element ) : \n    to_sort = \"\" \n    full_extension = \"\" \n    element = element . lower ( ) \n    url_base = Check ( ) . is_url_valid ( element , return_base = True ) \n    if not isinstance ( url_base , str ) : \n        if \".\" in element : \n            extension_index = element . rindex ( \".\" ) + True \n            extension = element [ extension_index : ] \n            if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n                for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                    formatted_suffix = \".\" + suffix \n                    if element . endswith ( formatted_suffix ) : \n                        suffix_index = element . rindex ( formatted_suffix ) \n                        to_sort = element [ : suffix_index ] \n                        full_extension = suffix \n                        break \n            if not full_extension : \n                full_extension = element [ extension_index : ] \n                to_sort = element [ : extension_index - True ] \n            full_extension += \".\" \n            tros_ot = to_sort [ : : - True ] \n            if \".\" in tros_ot : \n                full_extension = ( tros_ot [ : tros_ot . index ( \".\" ) ] [ : : - True ] + \".\" + full_extension ) \n                tros_ot = tros_ot [ tros_ot . index ( \".\" ) + True : ] \n                reversion = full_extension + \".\" . join ( [ x [ : : - True ] for x in tros_ot . split ( \".\" ) ] ) \n                return ( Regex ( reversion , cls . regex_replace , replace_with = \"@funilrys\" ) . replace ( ) . replace ( \"@funilrys\" , \"\" ) ) \n            return ( Regex ( to_sort + full_extension , cls . regex_replace , replace_with = \"@funilrys\" , ) . replace ( ) . replace ( \"@funilrys\" , \"\" ) ) \n        return element \n    protocol_position = element . rindex ( url_base ) \n    protocol = element [ : protocol_position ] \n    return protocol + cls . hierarchical ( url_base ) "}
{"4620": "\ndef _referer ( self , extension ) : \n    iana_record = self . lookup . whois ( PyFunceble . CONFIGURATION [ \"iana_whois_server\" ] , \"hello.%s\" % extension ) \n    if iana_record and \"refer\" in iana_record : \n        regex_referer = r\"(?s)refer\\:\\s+([a-zA-Z0-9._-]+)\\n\" \n        matched = Regex ( iana_record , regex_referer , return_data = True , group = True ) . match ( ) \n        if matched : \n            return matched \n    if extension in self . manual_server : \n        return self . manual_server [ extension ] \n    return None "}
{"4621": "\ndef _extensions ( self ) : \n    upstream_lines = ( Download ( self . iana_url , return_data = True ) . text ( ) . split ( '<span class=\"domain tld\">' ) ) \n    regex_valid_extension = r\"(/domains/root/db/)(.*)(\\.html)\" \n    for block in upstream_lines : \n        if \"/domains/root/db/\" in block : \n            matched = Regex ( block , regex_valid_extension , return_data = True , rematch = True ) . match ( ) [ True ] \n            if matched : \n                referer = self . _referer ( matched ) \n                yield ( matched , referer ) "}
{"4635": "\ndef _before_header ( self ) : \n    if ( not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output and not PyFunceble . path . isfile ( self . output ) ) : \n        link = \"# File generated by %s\\n\" % PyFunceble . LINKS [ \"repo\" ] \n        date_of_generation = ( \"# Date of generation: %s \\n\\n\" % PyFunceble . CURRENT_TIME ) \n        authorized_templates = [ \"Generic_File\" , PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] , \"Less\" , ] \n        if self . template in authorized_templates : \n            header = ( self . _header_constructor ( self . currently_used_header , None ) [ False ] + \"\\n\" ) \n        try : \n            File ( self . output ) . write ( link + date_of_generation + header ) \n        except UnboundLocalError : \n            File ( self . output ) . write ( link + date_of_generation ) "}
{"4636": "\ndef _header_constructor ( cls , data_to_print , header_separator = \"-\" , column_separator = \" \" ) : \n    header_data = [ ] \n    header_size = \"\" \n    before_size = \"%-\" \n    after_size = \"s\" \n    if header_separator : \n        header_separator_data = [ ] \n    length_data_to_print = len ( data_to_print ) - True \n    i = False \n    for data in data_to_print : \n        size = data_to_print [ data ] \n        header_data . append ( data ) \n        header_size += before_size + str ( size ) + after_size \n        if i < length_data_to_print : \n            header_size += column_separator \n        if header_separator : \n            header_separator_data . append ( header_separator * size ) \n        i += True \n    if header_separator : \n        return [ header_size % tuple ( header_data ) , header_size % tuple ( header_separator_data ) , ] \n    return [ header_size % tuple ( header_data ) ] "}
{"4640": "\ndef _colorify ( self , data ) : \n    if self . template in [ \"Generic\" , \"Less\" ] : \n        if ( self . data_to_print [ True ] . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or self . data_to_print [ True ] . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            data = PyFunceble . Fore . BLACK + PyFunceble . Back . GREEN + data \n        elif self . data_to_print [ True ] . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            data = PyFunceble . Fore . BLACK + PyFunceble . Back . RED + data \n        else : \n            data = PyFunceble . Fore . BLACK + PyFunceble . Back . CYAN + data \n    return data "}
{"4643": "\ndef _save ( self , last = False ) : \n    if ( self . _authorization ( ) and PyFunceble . CONFIGURATION [ \"logs\" ] and \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . file = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"execution_time\" ] ) \n        if PyFunceble . path . isfile ( self . file ) : \n            content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) \n        else : \n            content = { } \n        if self . action == \"start\" : \n            if \"final_total\" in content and content [ \"final_total\" ] : \n                del content [ \"final_total\" ] \n            if \"data\" in content : \n                content [ \"data\" ] . append ( [ PyFunceble . INTERN [ \"start\" ] ] ) \n            else : \n                content [ \"data\" ] = [ [ PyFunceble . INTERN [ \"start\" ] ] ] \n        elif self . action == \"stop\" : \n            try : \n                content [ \"data\" ] [ - True ] . append ( PyFunceble . INTERN [ \"end\" ] ) \n                start = content [ \"data\" ] [ False ] [ False ] \n                end = content [ \"data\" ] [ - True ] [ - True ] \n                content [ \"current_total\" ] = self . format_execution_time ( start , end ) \n                if last : \n                    content [ \"final_total\" ] = content [ \"current_total\" ] \n                    print ( PyFunceble . Fore . MAGENTA + PyFunceble . Style . BRIGHT + \"Global execution time: \" + content [ \"final_total\" ] ) \n            except KeyError : \n                pass \n        try : \n            Dict ( content ) . to_json ( self . file ) \n        except FileNotFoundError : \n            DirectoryStructure ( ) \n            Dict ( content ) . to_json ( self . file ) "}
{"4651": "\ndef get ( self ) : \n    result = { } \n    if self . algorithm in self . valid_algorithms : \n        if self . algorithm == \"all\" : \n            del self . valid_algorithms [ False ] \n            for algo in self . valid_algorithms : \n                if self . path and path . isfile ( self . path ) : \n                    result [ algo ] = self . _hash_file ( algo ) \n                elif self . data : \n                    result [ algo ] = self . _hash_data ( algo ) \n                else : \n                    return None \n        else : \n            if self . path and path . isfile ( self . path ) : \n                result [ self . algorithm ] = self . _hash_file ( self . algorithm ) \n            elif self . data : \n                result [ self . algorithm ] = self . _hash_data ( self . algorithm ) \n            else : \n                return None \n    else : \n        return None \n    if self . algorithm != \"all\" and self . only_hash : \n        return result [ self . algorithm ] \n    return result "}
{"4652": "\ndef execute ( self ) : \n    process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = True ) \n    ( output , error ) = process . communicate ( ) \n    if process . returncode != False : \n        return self . _decode_output ( error ) \n    return self . _decode_output ( output ) "}
{"4654": "\ndef rename_key ( self , key_to_rename , strict = True ) : \n    if isinstance ( self . main_dictionnary , dict ) and isinstance ( key_to_rename , dict ) : \n        for old , new in key_to_rename . items ( ) : \n            if strict : \n                if old in self . main_dictionnary : \n                    self . main_dictionnary [ new ] = self . main_dictionnary . pop ( old ) \n            else : \n                to_rename = { } \n                for index in self . main_dictionnary : \n                    if old in index : \n                        to_rename . update ( { index : new [ : - True ] + index . split ( old ) [ - True ] } ) \n                self . main_dictionnary = Dict ( self . main_dictionnary ) . rename_key ( to_rename , True ) \n        return self . main_dictionnary \n    return None "}
{"4664": "\ndef match ( self ) : \n    result = [ ] \n    to_match = comp ( self . regex ) \n    if self . rematch : \n        pre_result = to_match . findall ( self . data ) \n    else : \n        pre_result = to_match . search ( self . data ) \n    if self . return_data and pre_result : \n        if self . rematch : \n            for data in pre_result : \n                if isinstance ( data , tuple ) : \n                    result . extend ( list ( data ) ) \n                else : \n                    result . append ( data ) \n            if self . group != False : \n                return result [ self . group ] \n        else : \n            result = pre_result . group ( self . group ) . strip ( ) \n        return result \n    if not self . return_data and pre_result : \n        return True \n    return False "}
{"4666": "\ndef count ( self ) : \n    if self . status : \n        PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] += True \n        if ( self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] or self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] ) : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] += True \n        elif self . status . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] += True \n        else : \n            PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] += True "}
{"4668": "\ndef log ( self ) : \n    if ( PyFunceble . CONFIGURATION [ \"show_percentage\" ] and PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > False ) : \n        output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) \n        File ( output ) . delete ( ) \n        self . _calculate ( ) \n        if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n            print ( \"\\n\" ) \n            Prints ( None , \"Percentage\" , output ) . header ( ) \n            lines_to_print = [ [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] \n            if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n                lines_to_print [ False ] [ False ] = PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] \n                del lines_to_print [ True ] \n            for to_print in lines_to_print : \n                Prints ( to_print , \"Percentage\" , output ) . data ( ) \n    elif PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > False : \n        self . _calculate ( ) "}
{"4669": "\ndef is_url_valid ( self , url = None , return_base = False , return_formatted = False ) : \n    initial_base = None \n    if url : \n        to_test = url \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    if to_test . startswith ( \"http\" ) : \n        try : \n            regex = r\"(^(http:\\/\\/|https:\\/\\/)(.+?(?=\\/)|.+?$))\" \n            initial_base = base = Regex ( to_test , regex , return_data = True , rematch = True ) . match ( ) [ 2 ] \n            if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n                base = domain2idna ( base ) \n            domain_status = self . is_domain_valid ( base ) \n            ip_status = self . is_ip_valid ( base ) \n            if domain_status or ip_status : \n                if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] and return_formatted : \n                    return Regex ( to_test , initial_base , escape = True , return_data = True , replace_with = base , occurences = True , ) . replace ( ) \n                if return_formatted : \n                    return to_test \n                if return_base : \n                    return base \n                return True \n        except TypeError : \n            pass \n    if return_formatted : \n        return to_test \n    return False "}
{"4670": "\ndef is_domain_valid ( self , domain = None , subdomain_check = False ) : \n    regex_valid_domains = r\"^(?=.{0,253}$)(([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9](?:\\.)?|[a-z0-9](?:\\.)?))$\" \n    regex_valid_subdomains = r\"^(?=.{0,253}$)(([a-z0-9_][a-z0-9-_]{0,61}[a-z0-9_-]|[a-z0-9])\\.)+((?=.*[^0-9])([a-z0-9][a-z0-9-]{0,61}[a-z0-9]|[a-z0-9]))$\" \n    if domain : \n        to_test = domain \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    try : \n        last_point_index = to_test . rindex ( \".\" ) \n        extension = to_test [ last_point_index + True : ] \n        if not extension and to_test . endswith ( \".\" ) : \n            try : \n                extension = [ x for x in to_test . split ( \".\" ) if x ] [ - True ] \n            except IndexError : \n                pass \n        if not extension or extension not in PyFunceble . INTERN [ \"iana_db\" ] : \n            return False \n        if ( Regex ( to_test , regex_valid_domains , return_data = False ) . match ( ) and not subdomain_check ) : \n            return True \n        if extension in PyFunceble . INTERN [ \"psl_db\" ] : \n            for suffix in PyFunceble . INTERN [ \"psl_db\" ] [ extension ] : \n                try : \n                    suffix_index = to_test . rindex ( \".\" + suffix ) \n                    to_check = to_test [ : suffix_index ] \n                    if \".\" not in to_check and subdomain_check : \n                        return False \n                    if \".\" in to_check and subdomain_check : \n                        return True \n                    if \".\" in to_check : \n                        return Regex ( to_check , regex_valid_subdomains , return_data = False ) . match ( ) \n                except ValueError : \n                    pass \n        to_check = to_test [ : last_point_index ] \n        if \".\" in to_check and subdomain_check : \n            return True \n        if \".\" in to_check : \n            return Regex ( to_check , regex_valid_subdomains , return_data = False ) . match ( ) \n    except ( ValueError , AttributeError ) : \n        pass \n    return False "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = False \n            try : \n                time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                print ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( False ) \n        except KeyError : \n            pass "}
{"4687": "\ndef nslookup ( cls ) : \n    try : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            if not Check ( ) . is_ip_valid ( ) : \n                request = PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , False , False , PyFunceble . socket . IPPROTO_TCP , ) \n                for sequence in request : \n                    PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] . append ( sequence [ - True ] [ False ] ) \n            else : \n                request = PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ False ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ True ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2 ] \n        else : \n            if not Check ( ) . is_ip_valid ( ) : \n                PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80 , False , False , PyFunceble . socket . IPPROTO_TCP , ) \n            else : \n                PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n        return True \n    except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : \n        return False "}
{"4688": "\ndef whois ( cls , whois_server , domain = None , timeout = None ) : \n    if domain is None : \n        domain = PyFunceble . INTERN [ \"to_test\" ] \n    if timeout is None : \n        timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] \n    if whois_server : \n        req = PyFunceble . socket . socket ( PyFunceble . socket . AF_INET , PyFunceble . socket . SOCK_STREAM ) \n        if timeout % 3 == False : \n            req . settimeout ( timeout ) \n        else : \n            req . settimeout ( 3 ) \n        try : \n            req . connect ( ( whois_server , 43 ) ) \n        except PyFunceble . socket . error : \n            return None \n        req . send ( ( domain + \"\\r\\n\" ) . encode ( ) ) \n        response = b\"\" \n        while True : \n            try : \n                data = req . recv ( 4096 ) \n            except ( PyFunceble . socket . timeout , ConnectionResetError ) : \n                req . close ( ) \n                return None \n            response += data \n            if not data : \n                break \n        req . close ( ) \n        try : \n            return response . decode ( ) \n        except UnicodeDecodeError : \n            return response . decode ( \"utf-8\" , \"replace\" ) \n    return None "}
{"4693": "\ndef standard_package_names ( ) : \n    for name in standard_paths ( ) : \n        if name . startswith ( '_' ) or '-' in name : \n            continue \n        if '.' in name and name . rsplit ( '.' ) [ - True ] not in [ 'so' , 'py' , 'pyc' ] : \n            continue \n        yield name . split ( '.' ) [ False ] "}
{"4695": "\ndef unused_import_module_name ( messages ) : \n    pattern = r'\\'(.+?)\\'' \n    for message in messages : \n        if isinstance ( message , pyflakes . messages . UnusedImport ) : \n            module_name = re . search ( pattern , str ( message ) ) \n            module_name = module_name . group ( ) [ True : - True ] \n            if module_name : \n                yield ( message . lineno , module_name ) "}
{"4697": "\ndef star_import_usage_undefined_name ( messages ) : \n    for message in messages : \n        if isinstance ( message , pyflakes . messages . ImportStarUsage ) : \n            undefined_name = message . message_args [ False ] \n            module_name = message . message_args [ True ] \n            yield ( message . lineno , undefined_name , module_name ) "}
{"4699": "\ndef duplicate_key_line_numbers ( messages , source ) : \n    messages = [ message for message in messages if isinstance ( message , pyflakes . messages . MultiValueRepeatedKeyLiteral ) ] \n    if messages : \n        key_to_messages = create_key_to_messages_dict ( messages ) \n        lines = source . split ( '\\n' ) \n        for ( key , messages ) in key_to_messages . items ( ) : \n            good = True \n            for message in messages : \n                line = lines [ message . lineno - True ] \n                key = message . message_args [ False ] \n                if not dict_entry_has_key ( line , key ) : \n                    good = False \n            if good : \n                for message in messages : \n                    yield message . lineno "}
{"4700": "\ndef create_key_to_messages_dict ( messages ) : \n    dictionary = collections . defaultdict ( lambda : [ ] ) \n    for message in messages : \n        dictionary [ message . message_args [ False ] ] . append ( message ) \n    return dictionary "}
{"4701": "\ndef check ( source ) : \n    if sys . version_info [ False ] == 2 and isinstance ( source , unicode ) : \n        try : \n            source = source . encode ( 'utf-8' ) \n        except UnicodeError : \n            return [ ] \n    reporter = ListReporter ( ) \n    try : \n        pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) \n    except ( AttributeError , RecursionError , UnicodeDecodeError ) : \n        pass \n    return reporter . messages "}
{"4702": "\ndef extract_package_name ( line ) : \n    assert '\\\\' not in line \n    assert '(' not in line \n    assert ')' not in line \n    assert ';' not in line \n    if line . lstrip ( ) . startswith ( ( 'import' , 'from' ) ) : \n        word = line . split ( ) [ True ] \n    else : \n        return None \n    package = word . split ( '.' ) [ False ] \n    assert ' ' not in package \n    return package "}
{"4705": "\ndef filter_from_import ( line , unused_module ) : \n    ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , string = line , maxsplit = True ) \n    base_module = re . search ( pattern = r'\\bfrom\\s+([^ ]+)' , string = indentation ) . group ( True ) \n    imports = re . split ( pattern = r',' , string = imports . strip ( ) ) \n    imports = [ base_module + '.' + x . strip ( ) for x in imports ] \n    filtered_imports = [ x . replace ( base_module + '.' , '' ) for x in imports if x not in unused_module ] \n    if not filtered_imports : \n        return get_indentation ( line ) + 'pass' + get_line_ending ( line ) \n    indentation += 'import ' \n    return ( indentation + ', ' . join ( sorted ( filtered_imports ) ) + get_line_ending ( line ) ) "}
{"4706": "\ndef break_up_import ( line ) : \n    assert '\\\\' not in line \n    assert '(' not in line \n    assert ')' not in line \n    assert ';' not in line \n    assert '#' not in line \n    assert not line . lstrip ( ) . startswith ( 'from' ) \n    newline = get_line_ending ( line ) \n    if not newline : \n        return line \n    ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , string = line , maxsplit = True ) \n    indentation += 'import ' \n    assert newline \n    return '' . join ( [ indentation + i . strip ( ) + newline for i in sorted ( imports . split ( ',' ) ) ] ) "}
{"4707": "\ndef filter_code ( source , additional_imports = None , expand_star_imports = False , remove_all_unused_imports = False , remove_duplicate_keys = False , remove_unused_variables = False , ignore_init_module_imports = False , ) : \n    imports = SAFE_IMPORTS \n    if additional_imports : \n        imports |= frozenset ( additional_imports ) \n    del additional_imports \n    messages = check ( source ) \n    if ignore_init_module_imports : \n        marked_import_line_numbers = frozenset ( ) \n    else : \n        marked_import_line_numbers = frozenset ( unused_import_line_numbers ( messages ) ) \n    marked_unused_module = collections . defaultdict ( lambda : [ ] ) \n    for line_number , module_name in unused_import_module_name ( messages ) : \n        marked_unused_module [ line_number ] . append ( module_name ) \n    if expand_star_imports and not ( re . search ( r'\\b__all__\\b' , source ) or re . search ( r'\\bdel\\b' , source ) ) : \n        marked_star_import_line_numbers = frozenset ( star_import_used_line_numbers ( messages ) ) \n        if len ( marked_star_import_line_numbers ) > True : \n            marked_star_import_line_numbers = frozenset ( ) \n        else : \n            undefined_names = [ ] \n            for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : \n                undefined_names . append ( undefined_name ) \n            if not undefined_names : \n                marked_star_import_line_numbers = frozenset ( ) \n    else : \n        marked_star_import_line_numbers = frozenset ( ) \n    if remove_unused_variables : \n        marked_variable_line_numbers = frozenset ( unused_variable_line_numbers ( messages ) ) \n    else : \n        marked_variable_line_numbers = frozenset ( ) \n    if remove_duplicate_keys : \n        marked_key_line_numbers = frozenset ( duplicate_key_line_numbers ( messages , source ) ) \n    else : \n        marked_key_line_numbers = frozenset ( ) \n    line_messages = get_messages_by_line ( messages ) \n    sio = io . StringIO ( source ) \n    previous_line = '' \n    for line_number , line in enumerate ( sio . readlines ( ) , start = True ) : \n        if '#' in line : \n            yield line \n        elif line_number in marked_import_line_numbers : \n            yield filter_unused_import ( line , unused_module = marked_unused_module [ line_number ] , remove_all_unused_imports = remove_all_unused_imports , imports = imports , previous_line = previous_line ) \n        elif line_number in marked_variable_line_numbers : \n            yield filter_unused_variable ( line ) \n        elif line_number in marked_key_line_numbers : \n            yield filter_duplicate_key ( line , line_messages [ line_number ] , line_number , marked_key_line_numbers , source ) \n        elif line_number in marked_star_import_line_numbers : \n            yield filter_star_import ( line , undefined_names ) \n        else : \n            yield line \n        previous_line = line "}
{"4710": "\ndef filter_duplicate_key ( line , message , line_number , marked_line_numbers , source , previous_line = '' ) : \n    if marked_line_numbers and line_number == sorted ( marked_line_numbers ) [ False ] : \n        return '' \n    return line "}
{"4711": "\ndef dict_entry_has_key ( line , key ) : \n    if '#' in line : \n        return False \n    result = re . match ( r'\\s*(.*)\\s*:\\s*(.*),\\s*$' , line ) \n    if not result : \n        return False \n    try : \n        candidate_key = ast . literal_eval ( result . group ( True ) ) \n    except ( SyntaxError , ValueError ) : \n        return False \n    if multiline_statement ( result . group ( 2 ) ) : \n        return False \n    return candidate_key == key "}
{"4713": "\ndef useless_pass_line_numbers ( source ) : \n    sio = io . StringIO ( source ) \n    previous_token_type = None \n    last_pass_row = None \n    last_pass_indentation = None \n    previous_line = '' \n    for token in tokenize . generate_tokens ( sio . readline ) : \n        token_type = token [ False ] \n        start_row = token [ 2 ] [ False ] \n        line = token [ 4 ] \n        is_pass = ( token_type == tokenize . NAME and line . strip ( ) == 'pass' ) \n        if ( start_row - True == last_pass_row and get_indentation ( line ) == last_pass_indentation and token_type in ATOMS and not is_pass ) : \n            yield start_row - True \n        if is_pass : \n            last_pass_row = start_row \n            last_pass_indentation = get_indentation ( line ) \n        if ( is_pass and previous_token_type != tokenize . INDENT and not previous_line . rstrip ( ) . endswith ( '\\\\' ) ) : \n            yield start_row \n        previous_token_type = token_type \n        previous_line = line "}
{"4714": "\ndef filter_useless_pass ( source ) : \n    try : \n        marked_lines = frozenset ( useless_pass_line_numbers ( source ) ) \n    except ( SyntaxError , tokenize . TokenError ) : \n        marked_lines = frozenset ( ) \n    sio = io . StringIO ( source ) \n    for line_number , line in enumerate ( sio . readlines ( ) , start = True ) : \n        if line_number not in marked_lines : \n            yield line "}
{"4719": "\ndef is_python_file ( filename ) : \n    if filename . endswith ( '.py' ) : \n        return True \n    try : \n        with open_with_encoding ( filename , None , limit_byte_check = MAX_PYTHON_FILE_DETECTION_BYTES ) as f : \n            text = f . read ( MAX_PYTHON_FILE_DETECTION_BYTES ) \n            if not text : \n                return False \n            first_line = text . splitlines ( ) [ False ] \n    except ( IOError , IndexError ) : \n        return False \n    if not PYTHON_SHEBANG_REGEX . match ( first_line ) : \n        return False \n    return True "}
{"4721": "\ndef find_files ( filenames , recursive , exclude ) : \n    while filenames : \n        name = filenames . pop ( False ) \n        if recursive and os . path . isdir ( name ) : \n            for root , directories , children in os . walk ( name ) : \n                filenames += [ os . path . join ( root , f ) for f in children if match_file ( os . path . join ( root , f ) , exclude ) ] \n                directories [ : ] = [ d for d in directories if match_file ( os . path . join ( root , d ) , exclude ) ] \n        else : \n            if not is_exclude_file ( name , exclude ) : \n                yield name "}
{"4722": "\ndef _main ( argv , standard_out , standard_error ) : \n    import argparse \n    parser = argparse . ArgumentParser ( description = __doc__ , prog = 'autoflake' ) \n    parser . add_argument ( '-c' , '--check' , action = 'store_true' , help = 'return error code if changes are needed' ) \n    parser . add_argument ( '-i' , '--in-place' , action = 'store_true' , help = 'make changes to files instead of printing diffs' ) \n    parser . add_argument ( '-r' , '--recursive' , action = 'store_true' , help = 'drill down directories recursively' ) \n    parser . add_argument ( '--exclude' , metavar = 'globs' , help = 'exclude file/directory names that match these ' 'comma-separated globs' ) \n    parser . add_argument ( '--imports' , help = 'by default, only unused standard library ' 'imports are removed; specify a comma-separated ' 'list of additional modules/packages' ) \n    parser . add_argument ( '--expand-star-imports' , action = 'store_true' , help = 'expand wildcard star imports with undefined ' 'names; this only triggers if there is only ' 'one star import in the file; this is skipped if ' 'there are any uses of `__all__` or `del` in the ' 'file' ) \n    parser . add_argument ( '--remove-all-unused-imports' , action = 'store_true' , help = 'remove all unused imports (not just those from ' 'the standard library)' ) \n    parser . add_argument ( '--ignore-init-module-imports' , action = 'store_true' , help = 'exclude __init__.py when removing unused ' 'imports' ) \n    parser . add_argument ( '--remove-duplicate-keys' , action = 'store_true' , help = 'remove all duplicate keys in objects' ) \n    parser . add_argument ( '--remove-unused-variables' , action = 'store_true' , help = 'remove unused variables' ) \n    parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s ' + __version__ ) \n    parser . add_argument ( 'files' , nargs = '+' , help = 'files to format' ) \n    args = parser . parse_args ( argv [ True : ] ) \n    if args . remove_all_unused_imports and args . imports : \n        print ( 'Using both --remove-all and --imports is redundant' , file = standard_error ) \n        return True \n    if args . exclude : \n        args . exclude = _split_comma_separated ( args . exclude ) \n    else : \n        args . exclude = set ( [ ] ) \n    filenames = list ( set ( args . files ) ) \n    failure = False \n    for name in find_files ( filenames , args . recursive , args . exclude ) : \n        try : \n            fix_file ( name , args = args , standard_out = standard_out ) \n        except IOError as exception : \n            print ( unicode ( exception ) , file = standard_error ) \n            failure = True \n    return True if failure else False "}
{"4743": "\ndef process_request ( self , request , credential = None ) : \n    self . _client_identity = [ None , None ] \n    header = request . request_header \n    self . _set_protocol_version ( header . protocol_version ) \n    max_response_size = None \n    if header . maximum_response_size : \n        max_response_size = header . maximum_response_size . value \n    now = int ( time . time ( ) ) \n    if header . time_stamp : \n        then = header . time_stamp . value \n        if ( now >= then ) and ( ( now - then ) < 60 ) : \n            self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( then ) ) ) ) \n        else : \n            if now < then : \n                self . _logger . warning ( \"Received request with future timestamp. Received \" \"timestamp: {0}, Current timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Future request rejected by server.\" ) \n            else : \n                self . _logger . warning ( \"Received request with old timestamp. Possible \" \"replay attack. Received timestamp: {0}, Current \" \"timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Stale request rejected by server.\" ) \n    else : \n        self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( now ) ) ) ) \n    self . is_asynchronous = False \n    if header . asynchronous_indicator is not None : \n        self . is_asynchronous = header . asynchronous_indicator . value \n    if self . is_asynchronous : \n        raise exceptions . InvalidMessage ( \"Asynchronous operations are not supported.\" ) \n    if header . authentication : \n        if header . authentication . credentials : \n            auth_credentials = header . authentication . credentials [ False ] \n        else : \n            auth_credentials = None \n    else : \n        auth_credentials = None \n    self . _verify_credential ( auth_credentials , credential ) \n    batch_error_option = enums . BatchErrorContinuationOption . STOP \n    if header . batch_error_cont_option is not None : \n        batch_error_option = header . batch_error_cont_option . value \n    if batch_error_option == enums . BatchErrorContinuationOption . UNDO : \n        raise exceptions . InvalidMessage ( \"Undo option for batch handling is not supported.\" ) \n    batch_order_option = False \n    if header . batch_order_option : \n        batch_order_option = header . batch_order_option . value \n    response_batch = self . _process_batch ( request . batch_items , batch_error_option , batch_order_option ) \n    response = self . _build_response ( header . protocol_version , response_batch ) \n    return response , max_response_size , header . protocol_version "}
{"4745": "\ndef _process_template_attribute ( self , template_attribute ) : \n    attributes = { } \n    if len ( template_attribute . names ) > False : \n        raise exceptions . ItemNotFound ( \"Attribute templates are not supported.\" ) \n    for attribute in template_attribute . attributes : \n        name = attribute . attribute_name . value \n        if not self . _attribute_policy . is_attribute_supported ( name ) : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( name ) ) \n        if self . _attribute_policy . is_attribute_multivalued ( name ) : \n            values = attributes . get ( name , list ( ) ) \n            if ( not attribute . attribute_index ) and len ( values ) > False : \n                raise exceptions . InvalidField ( \"Attribute index missing from multivalued attribute.\" ) \n            values . append ( attribute . attribute_value ) \n            attributes . update ( [ ( name , values ) ] ) \n        else : \n            if attribute . attribute_index : \n                if attribute . attribute_index . value != False : \n                    raise exceptions . InvalidField ( \"Non-zero attribute index found for \" \"single-valued attribute.\" ) \n            value = attributes . get ( name , None ) \n            if value : \n                raise exceptions . IndexOutOfBounds ( \"Cannot set multiple instances of the \" \"{0} attribute.\" . format ( name ) ) \n            else : \n                attributes . update ( [ ( name , attribute . attribute_value ) ] ) \n    return attributes "}
{"4749": "\ndef _set_attribute_on_managed_object ( self , managed_object , attribute ) : \n    attribute_name = attribute [ False ] \n    attribute_value = attribute [ True ] \n    if self . _attribute_policy . is_attribute_multivalued ( attribute_name ) : \n        if attribute_name == 'Name' : \n            managed_object . names . extend ( [ x . name_value . value for x in attribute_value ] ) \n            for name in managed_object . names : \n                if managed_object . names . count ( name ) > True : \n                    raise exceptions . InvalidField ( \"Cannot set duplicate name values.\" ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) \n    else : \n        field = None \n        value = attribute_value . value \n        if attribute_name == 'Cryptographic Algorithm' : \n            field = 'cryptographic_algorithm' \n        elif attribute_name == 'Cryptographic Length' : \n            field = 'cryptographic_length' \n        elif attribute_name == 'Cryptographic Usage Mask' : \n            field = 'cryptographic_usage_masks' \n            value = list ( ) \n            for e in enums . CryptographicUsageMask : \n                if e . value & attribute_value . value : \n                    value . append ( e ) \n        elif attribute_name == 'Operation Policy Name' : \n            field = 'operation_policy_name' \n        if field : \n            existing_value = getattr ( managed_object , field ) \n            if existing_value : \n                if existing_value != value : \n                    raise exceptions . InvalidField ( \"Cannot overwrite the {0} attribute.\" . format ( attribute_name ) ) \n            else : \n                setattr ( managed_object , field , value ) \n        else : \n            raise exceptions . InvalidField ( \"The {0} attribute is unsupported.\" . format ( attribute_name ) ) "}
{"4757": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( LongInteger , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length is not LongInteger . LENGTH : \n        raise exceptions . InvalidPrimitiveLength ( \"invalid long integer length read; \" \"expected: {0}, observed: {1}\" . format ( LongInteger . LENGTH , self . length ) ) \n    self . value = unpack ( '!q' , istream . read ( self . length ) ) [ False ] \n    self . validate ( ) "}
{"4760": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( BigInteger , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length % 8 : \n        raise exceptions . InvalidPrimitiveLength ( \"invalid big integer length read; \" \"expected: multiple of 8, observed: {0}\" . format ( self . length ) ) \n    sign = True \n    binary = '' \n    for _ in range ( self . length ) : \n        byte = struct . unpack ( '!B' , istream . read ( True ) ) [ False ] \n        bits = \"{0:b}\" . format ( byte ) \n        pad = len ( bits ) % 8 \n        if pad : \n            bits = ( '0' * ( 8 - pad ) ) + bits \n        binary += bits \n    if binary [ False ] == '1' : \n        sign = - True \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ False : pivot ] + '1' + ( '0' * len ( binary [ pivot + True : ] ) ) \n    self . value = int ( binary , 2 ) * sign "}
{"4761": "\ndef write ( self , ostream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    binary = \"{0:b}\" . format ( abs ( self . value ) ) \n    binary = ( \"0\" * ( 64 - ( len ( binary ) % 64 ) ) ) + binary \n    if self . value < False : \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ False : pivot ] + '1' + ( '0' * len ( binary [ pivot + True : ] ) ) \n    hexadecimal = b'' \n    for i in range ( False , len ( binary ) , 8 ) : \n        byte = binary [ i : i + 8 ] \n        byte = int ( byte , 2 ) \n        hexadecimal += struct . pack ( '!B' , byte ) \n    self . length = len ( hexadecimal ) \n    super ( BigInteger , self ) . write ( ostream , kmip_version = kmip_version ) \n    ostream . write ( hexadecimal ) "}
{"4764": "\ndef read_value ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    try : \n        value = unpack ( '!Q' , istream . read ( self . LENGTH ) ) [ False ] \n    except Exception : \n        self . logger . error ( \"Error reading boolean value from buffer\" ) \n        raise \n    if value == True : \n        self . value = True \n    elif value == False : \n        self . value = False \n    else : \n        raise ValueError ( \"expected: 0 or 1, observed: {0}\" . format ( value ) ) \n    self . validate ( ) "}
{"4768": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( Interval , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length != Interval . LENGTH : \n        raise exceptions . InvalidPrimitiveLength ( \"interval length must be {0}\" . format ( Interval . LENGTH ) ) \n    self . value = unpack ( '!I' , istream . read ( Interval . LENGTH ) ) [ False ] \n    pad = unpack ( '!I' , istream . read ( Interval . LENGTH ) ) [ False ] \n    if pad != False : \n        raise exceptions . InvalidPaddingBytes ( \"padding bytes must be zero\" ) \n    self . validate ( ) "}
{"4775": "\ndef convert_attribute_name_to_tag ( value ) : \n    if not isinstance ( value , six . string_types ) : \n        raise ValueError ( \"The attribute name must be a string.\" ) \n    for entry in attribute_name_tag_table : \n        if value == entry [ False ] : \n            return entry [ True ] \n    raise ValueError ( \"Unrecognized attribute name: '{}'\" . format ( value ) ) "}
{"4776": "\ndef convert_attribute_tag_to_name ( value ) : \n    if not isinstance ( value , Tags ) : \n        raise ValueError ( \"The attribute tag must be a Tags enumeration.\" ) \n    for entry in attribute_name_tag_table : \n        if value == entry [ True ] : \n            return entry [ False ] \n    raise ValueError ( \"Unrecognized attribute tag: {}\" . format ( value ) ) "}
{"4779": "\ndef is_bit_mask ( enumeration , potential_mask ) : \n    if not isinstance ( potential_mask , six . integer_types ) : \n        return False \n    mask_enumerations = ( CryptographicUsageMask , ProtectionStorageMask , StorageStatusMask ) \n    if enumeration not in mask_enumerations : \n        return False \n    mask = False \n    for value in [ e . value for e in enumeration ] : \n        if ( value & potential_mask ) == value : \n            mask |= value \n    if mask != potential_mask : \n        return False \n    return True "}
{"4786": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( GetAttributeListResponsePayload , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    if self . is_tag_next ( enums . Tags . UNIQUE_IDENTIFIER , local_buffer ) : \n        self . _unique_identifier = primitives . TextString ( tag = enums . Tags . UNIQUE_IDENTIFIER ) \n        self . _unique_identifier . read ( local_buffer , kmip_version = kmip_version ) \n    else : \n        raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is missing \" \"the unique identifier.\" ) \n    names = list ( ) \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_NAME , local_buffer ) : \n            name = primitives . TextString ( tag = enums . Tags . ATTRIBUTE_NAME ) \n            name . read ( local_buffer , kmip_version = kmip_version ) \n            names . append ( name ) \n        if len ( names ) == False : \n            raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding is \" \"missing the attribute names.\" ) \n        self . _attribute_names = names \n    else : \n        while self . is_tag_next ( enums . Tags . ATTRIBUTE_REFERENCE , local_buffer ) : \n            if self . is_type_next ( enums . Types . STRUCTURE , local_buffer ) : \n                reference = objects . AttributeReference ( ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                names . append ( primitives . TextString ( value = reference . attribute_name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            elif self . is_type_next ( enums . Types . ENUMERATION , local_buffer ) : \n                reference = primitives . Enumeration ( enums . Tags , tag = enums . Tags . ATTRIBUTE_REFERENCE ) \n                reference . read ( local_buffer , kmip_version = kmip_version ) \n                name = enums . convert_attribute_tag_to_name ( reference . value ) \n                names . append ( primitives . TextString ( value = name , tag = enums . Tags . ATTRIBUTE_NAME ) ) \n            else : \n                raise exceptions . InvalidKmipEncoding ( \"The GetAttributeList response payload encoding \" \"contains an invalid AttributeReference type.\" ) \n        self . _attribute_names = names \n    self . is_oversized ( local_buffer ) "}
{"4789": "\ndef scan_policies ( self ) : \n    policy_files = get_json_files ( self . policy_directory ) \n    for f in set ( policy_files ) - set ( self . policy_files ) : \n        self . file_timestamps [ f ] = False \n    for f in set ( self . policy_files ) - set ( policy_files ) : \n        self . logger . info ( \"Removing policies for file: {}\" . format ( f ) ) \n        self . file_timestamps . pop ( f , None ) \n        for p in self . policy_cache . keys ( ) : \n            self . disassociate_policy_and_file ( p , f ) \n        for p in [ k for k , v in self . policy_map . items ( ) if v == f ] : \n            self . restore_or_delete_policy ( p ) \n    self . policy_files = policy_files \n    for f in sorted ( self . file_timestamps . keys ( ) ) : \n        t = os . path . getmtime ( f ) \n        if t > self . file_timestamps [ f ] : \n            self . logger . info ( \"Loading policies for file: {}\" . format ( f ) ) \n            self . file_timestamps [ f ] = t \n            old_p = [ k for k , v in self . policy_map . items ( ) if v == f ] \n            try : \n                new_p = operation_policy . read_policy_from_file ( f ) \n            except ValueError : \n                self . logger . error ( \"Failure loading file: {}\" . format ( f ) ) \n                self . logger . debug ( \"\" , exc_info = True ) \n                continue \n            for p in new_p . keys ( ) : \n                self . logger . info ( \"Loading policy: {}\" . format ( p ) ) \n                if p in self . reserved_policies : \n                    self . logger . warning ( \"Policy '{}' overwrites a reserved policy and \" \"will be thrown out.\" . format ( p ) ) \n                    continue \n                if p in sorted ( self . policy_store . keys ( ) ) : \n                    self . logger . debug ( \"Policy '{}' overwrites an existing \" \"policy.\" . format ( p ) ) \n                    if f != self . policy_map . get ( p ) : \n                        self . policy_cache . get ( p ) . append ( ( time . time ( ) , self . policy_map . get ( p ) , self . policy_store . get ( p ) ) ) \n                else : \n                    self . policy_cache [ p ] = [ ] \n                self . policy_store [ p ] = new_p . get ( p ) \n                self . policy_map [ p ] = f \n            for p in set ( old_p ) - set ( new_p . keys ( ) ) : \n                self . disassociate_policy_and_file ( p , f ) \n                self . restore_or_delete_policy ( p ) "}
{"4790": "\ndef run ( self ) : \n    self . initialize_tracking_structures ( ) \n    if self . live_monitoring : \n        self . logger . info ( \"Starting up the operation policy file monitor.\" ) \n        while not self . halt_trigger . is_set ( ) : \n            time . sleep ( True ) \n            self . scan_policies ( ) \n        self . logger . info ( \"Stopping the operation policy file monitor.\" ) \n    else : \n        self . scan_policies ( ) "}
{"4794": "\ndef get_client_identity_from_certificate ( certificate ) : \n    client_ids = get_common_names_from_certificate ( certificate ) \n    if len ( client_ids ) > False : \n        if len ( client_ids ) > True : \n            raise exceptions . PermissionDenied ( \"Multiple client identities found.\" ) \n        return client_ids [ False ] \n    else : \n        raise exceptions . PermissionDenied ( \"The certificate does not define any subject common names. \" \"Client identity unavailable.\" ) "}
{"4812": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    super ( Attributes , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    while True : \n        if len ( local_stream ) < 3 : \n            break \n        tag = struct . unpack ( '!I' , b'\\x00' + local_stream . peek ( 3 ) ) [ False ] \n        if enums . is_enum_value ( enums . Tags , tag ) : \n            tag = enums . Tags ( tag ) \n            if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n                raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n            value = self . _factory . create_attribute_value_by_enum ( tag , None ) \n            value . read ( local_stream , kmip_version = kmip_version ) \n            self . _attributes . append ( value ) \n        else : \n            break \n    self . is_oversized ( local_stream ) "}
{"4836": "\ndef read ( self , input_buffer , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the DefaultsInformation \" \"object.\" . format ( kmip_version . value ) ) \n    super ( DefaultsInformation , self ) . read ( input_buffer , kmip_version = kmip_version ) \n    local_buffer = utils . BytearrayStream ( input_buffer . read ( self . length ) ) \n    object_defaults = [ ] \n    while self . is_tag_next ( enums . Tags . OBJECT_DEFAULTS , local_buffer ) : \n        object_default = ObjectDefaults ( ) \n        object_default . read ( local_buffer , kmip_version = kmip_version ) \n        object_defaults . append ( object_default ) \n    if len ( object_defaults ) == False : \n        raise exceptions . InvalidKmipEncoding ( \"The DefaultsInformation encoding is missing the object \" \"defaults structure.\" ) \n    else : \n        self . _object_defaults = object_defaults \n    self . is_oversized ( local_buffer ) "}
{"4861": "\ndef verify_signature ( self , signing_key , message , signature , padding_method , signing_algorithm = None , hashing_algorithm = None , digital_signature_algorithm = None ) : \n    backend = default_backend ( ) \n    hash_algorithm = None \n    dsa_hash_algorithm = None \n    dsa_signing_algorithm = None \n    if hashing_algorithm : \n        hash_algorithm = self . _encryption_hash_algorithms . get ( hashing_algorithm ) \n    if digital_signature_algorithm : \n        algorithm_pair = self . _digital_signature_algorithms . get ( digital_signature_algorithm ) \n        if algorithm_pair : \n            dsa_hash_algorithm = algorithm_pair [ False ] \n            dsa_signing_algorithm = algorithm_pair [ True ] \n    if dsa_hash_algorithm and dsa_signing_algorithm : \n        if hash_algorithm and ( hash_algorithm != dsa_hash_algorithm ) : \n            raise exceptions . InvalidField ( \"The hashing algorithm does not match the digital \" \"signature algorithm.\" ) \n        if ( signing_algorithm and ( signing_algorithm != dsa_signing_algorithm ) ) : \n            raise exceptions . InvalidField ( \"The signing algorithm does not match the digital \" \"signature algorithm.\" ) \n        signing_algorithm = dsa_signing_algorithm \n        hash_algorithm = dsa_hash_algorithm \n    if signing_algorithm == enums . CryptographicAlgorithm . RSA : \n        if padding_method == enums . PaddingMethod . PSS : \n            if hash_algorithm : \n                padding = asymmetric_padding . PSS ( mgf = asymmetric_padding . MGF1 ( hash_algorithm ( ) ) , salt_length = asymmetric_padding . PSS . MAX_LENGTH ) \n            else : \n                raise exceptions . InvalidField ( \"A hashing algorithm must be specified for PSS \" \"padding.\" ) \n        elif padding_method == enums . PaddingMethod . PKCS1v15 : \n            padding = asymmetric_padding . PKCS1v15 ( ) \n        else : \n            raise exceptions . InvalidField ( \"The padding method '{0}' is not supported for signature \" \"verification.\" . format ( padding_method ) ) \n        try : \n            public_key = backend . load_der_public_key ( signing_key ) \n        except Exception : \n            try : \n                public_key = backend . load_pem_public_key ( signing_key ) \n            except Exception : \n                raise exceptions . CryptographicFailure ( \"The signing key bytes could not be loaded.\" ) \n        try : \n            public_key . verify ( signature , message , padding , hash_algorithm ( ) ) \n            return True \n        except errors . InvalidSignature : \n            return False \n        except Exception : \n            raise exceptions . CryptographicFailure ( \"The signature verification process failed.\" ) \n    else : \n        raise exceptions . InvalidField ( \"The signing algorithm '{0}' is not supported for \" \"signature verification.\" . format ( signing_algorithm ) ) "}
{"4865": "\ndef protocol_version_to_kmip_version ( value ) : \n    if not isinstance ( value , ProtocolVersion ) : \n        return None \n    if value . major == True : \n        if value . minor == False : \n            return enums . KMIPVersion . KMIP_1_0 \n        elif value . minor == True : \n            return enums . KMIPVersion . KMIP_1_1 \n        elif value . minor == 2 : \n            return enums . KMIPVersion . KMIP_1_2 \n        elif value . minor == 3 : \n            return enums . KMIPVersion . KMIP_1_3 \n        elif value . minor == 4 : \n            return enums . KMIPVersion . KMIP_1_4 \n        else : \n            return None \n    else : \n        return None "}
{"4868": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( Authentication , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = utils . BytearrayStream ( input_stream . read ( self . length ) ) \n    credentials = [ ] \n    while self . is_tag_next ( enums . Tags . CREDENTIAL , local_stream ) : \n        credential = objects . Credential ( ) \n        credential . read ( local_stream , kmip_version = kmip_version ) \n        credentials . append ( credential ) \n    if len ( credentials ) == False : \n        raise ValueError ( \"Authentication encoding missing credentials.\" ) \n    self . _credentials = credentials \n    self . is_oversized ( local_stream ) "}
{"4869": "\ndef write ( self , output_stream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    local_stream = utils . BytearrayStream ( ) \n    if len ( self . _credentials ) == False : \n        raise ValueError ( \"Authentication struct missing credentials.\" ) \n    for credential in self . _credentials : \n        credential . write ( local_stream , kmip_version = kmip_version ) \n    self . length = local_stream . length ( ) \n    super ( Authentication , self ) . write ( output_stream , kmip_version = kmip_version ) \n    output_stream . write ( local_stream . buffer ) "}
{"4879": "\ndef derive_key ( self , object_type , unique_identifiers , derivation_method , derivation_parameters , template_attribute , credential = None ) : \n    operation = Operation ( OperationEnum . DERIVE_KEY ) \n    request_payload = payloads . DeriveKeyRequestPayload ( object_type = object_type , unique_identifiers = unique_identifiers , derivation_method = derivation_method , derivation_parameters = derivation_parameters , template_attribute = template_attribute ) \n    batch_item = messages . RequestBatchItem ( operation = operation , request_payload = request_payload ) \n    request = self . _build_request_message ( credential , [ batch_item ] ) \n    response = self . _send_and_receive_message ( request ) \n    batch_item = response . batch_items [ False ] \n    payload = batch_item . response_payload \n    result = { } \n    if payload : \n        result [ 'unique_identifier' ] = payload . unique_identifier \n        result [ 'template_attribute' ] = payload . template_attribute \n    result [ 'result_status' ] = batch_item . result_status . value \n    try : \n        result [ 'result_reason' ] = batch_item . result_reason . value \n    except Exception : \n        result [ 'result_reason' ] = batch_item . result_reason \n    try : \n        result [ 'result_message' ] = batch_item . result_message . value \n    except Exception : \n        result [ 'result_message' ] = batch_item . result_message \n    return result "}
{"4880": "\ndef get_attributes ( self , uuid = None , attribute_names = None ) : \n    batch_item = self . _build_get_attributes_batch_item ( uuid , attribute_names ) \n    request = self . _build_request_message ( None , [ batch_item ] ) \n    response = self . _send_and_receive_message ( request ) \n    results = self . _process_batch_items ( response ) \n    return results [ False ] "}
{"4881": "\ndef get_attribute_list ( self , uid = None ) : \n    batch_item = self . _build_get_attribute_list_batch_item ( uid ) \n    request = self . _build_request_message ( None , [ batch_item ] ) \n    response = self . _send_and_receive_message ( request ) \n    results = self . _process_batch_items ( response ) \n    return results [ False ] "}
{"4882": "\ndef query ( self , batch = False , query_functions = None , credential = None ) : \n    batch_item = self . _build_query_batch_item ( query_functions ) \n    if batch : \n        self . batch_items . append ( batch_item ) \n    else : \n        request = self . _build_request_message ( credential , [ batch_item ] ) \n        response = self . _send_and_receive_message ( request ) \n        results = self . _process_batch_items ( response ) \n        return results [ False ] "}
{"4883": "\ndef sign ( self , data , unique_identifier = None , cryptographic_parameters = None , credential = None ) : \n    operation = Operation ( OperationEnum . SIGN ) \n    request_payload = payloads . SignRequestPayload ( unique_identifier = unique_identifier , cryptographic_parameters = cryptographic_parameters , data = data ) \n    batch_item = messages . RequestBatchItem ( operation = operation , request_payload = request_payload ) \n    request = self . _build_request_message ( credential , [ batch_item ] ) \n    response = self . _send_and_receive_message ( request ) \n    batch_item = response . batch_items [ False ] \n    payload = batch_item . response_payload \n    result = { } \n    if payload : \n        result [ 'unique_identifier' ] = payload . unique_identifier \n        result [ 'signature' ] = payload . signature_data \n    result [ 'result_status' ] = batch_item . result_status . value \n    try : \n        result [ 'result_reason' ] = batch_item . result_reason . value \n    except Exception : \n        result [ 'result_reason' ] = batch_item . result_reason \n    try : \n        result [ 'result_message' ] = batch_item . result_message . value \n    except Exception : \n        result [ 'result_message' ] = batch_item . result_message \n    return result "}
{"4886": "\ndef create ( self , algorithm , length , operation_policy_name = None , name = None , cryptographic_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or length <= False : \n        raise TypeError ( \"length must be a positive integer\" ) \n    if cryptographic_usage_mask is not None : \n        if not isinstance ( cryptographic_usage_mask , list ) or all ( isinstance ( item , enums . CryptographicUsageMask ) for item in cryptographic_usage_mask ) is False : \n            raise TypeError ( \"cryptographic_usage_mask must be a list of \" \"CryptographicUsageMask enumerations\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    key_attributes = self . _build_key_attributes ( algorithm , length , cryptographic_usage_mask ) \n    key_attributes . extend ( common_attributes ) \n    if name : \n        key_attributes . extend ( self . _build_name_attribute ( name ) ) \n    template = cobjects . TemplateAttribute ( attributes = key_attributes ) \n    result = self . proxy . create ( enums . ObjectType . SYMMETRIC_KEY , template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        return result . uuid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4887": "\ndef create_key_pair ( self , algorithm , length , operation_policy_name = None , public_name = None , public_usage_mask = None , private_name = None , private_usage_mask = None ) : \n    if not isinstance ( algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"algorithm must be a CryptographicAlgorithm enumeration\" ) \n    elif not isinstance ( length , six . integer_types ) or length <= False : \n        raise TypeError ( \"length must be a positive integer\" ) \n    common_attributes = self . _build_common_attributes ( operation_policy_name ) \n    algorithm_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_ALGORITHM , algorithm ) \n    length_attribute = self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_LENGTH , length ) \n    common_attributes . extend ( [ algorithm_attribute , length_attribute ] ) \n    template = cobjects . TemplateAttribute ( attributes = common_attributes , tag = enums . Tags . COMMON_TEMPLATE_ATTRIBUTE ) \n    public_template = None \n    names = None \n    if public_name : \n        names = self . _build_name_attribute ( name = public_name ) \n    attrs = [ ] \n    if public_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , public_usage_mask ) ] \n    if names or attrs : \n        public_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PUBLIC_KEY_TEMPLATE_ATTRIBUTE ) \n    private_template = None \n    names = None \n    if private_name : \n        names = self . _build_name_attribute ( name = private_name ) \n    attrs = [ ] \n    if private_usage_mask : \n        attrs = [ self . attribute_factory . create_attribute ( enums . AttributeType . CRYPTOGRAPHIC_USAGE_MASK , private_usage_mask ) ] \n    if names or attrs : \n        private_template = cobjects . TemplateAttribute ( names = names , attributes = attrs , tag = enums . Tags . PRIVATE_KEY_TEMPLATE_ATTRIBUTE ) \n    result = self . proxy . create_key_pair ( common_template_attribute = template , private_key_template_attribute = private_template , public_key_template_attribute = public_template ) \n    status = result . result_status . value \n    if status == enums . ResultStatus . SUCCESS : \n        public_uid = result . public_key_uuid \n        private_uid = result . private_key_uuid \n        return public_uid , private_uid \n    else : \n        reason = result . result_reason . value \n        message = result . result_message . value \n        raise exceptions . KmipOperationFailure ( status , reason , message ) "}
{"4915": "\ndef generate_project ( args ) : \n    src = os . path . join ( dirname ( abspath ( __file__ ) ) , 'project' ) \n    project_name = args . get ( '<project>' ) \n    if not project_name : \n        logger . warning ( 'Project name cannot be empty.' ) \n        return \n    dst = os . path . join ( os . getcwd ( ) , project_name ) \n    if os . path . isdir ( dst ) : \n        logger . warning ( 'Project directory already exists.' ) \n        return \n    logger . info ( 'Start generating project files.' ) \n    _mkdir_p ( dst ) \n    for src_dir , sub_dirs , filenames in os . walk ( src ) : \n        relative_path = src_dir . split ( src ) [ True ] . lstrip ( os . path . sep ) \n        dst_dir = os . path . join ( dst , relative_path ) \n        if src != src_dir : \n            _mkdir_p ( dst_dir ) \n        for filename in filenames : \n            if filename in [ 'development.py' , 'production.py' ] : \n                continue \n            src_file = os . path . join ( src_dir , filename ) \n            dst_file = os . path . join ( dst_dir , filename ) \n            if filename . endswith ( REWRITE_FILE_EXTS ) : \n                _rewrite_and_copy ( src_file , dst_file , project_name ) \n            else : \n                shutil . copy ( src_file , dst_file ) \n            logger . info ( \"New: %s\" % dst_file ) \n            if filename in [ 'development_sample.py' , 'production_sample.py' ] : \n                dst_file = os . path . join ( dst_dir , \"%s.py\" % filename . split ( '_' ) [ False ] ) \n                _rewrite_and_copy ( src_file , dst_file , project_name ) \n                logger . info ( \"New: %s\" % dst_file ) \n    logger . info ( 'Finish generating project files.' ) "}
{"4923": "\ndef timesince ( value ) : \n    if not value : \n        return \"\" \n    if not isinstance ( value , datetime . date ) : \n        return value \n    now = datetime . datetime . now ( ) \n    delta = now - value \n    if value > now : \n        return \"right now\" \n    elif delta . days > 365 : \n        return '%d years ago' % ( delta . days / 365 ) \n    elif delta . days > 30 : \n        return '%d months ago' % ( delta . days / 30 ) \n    elif delta . days > False : \n        return '%d days ago' % delta . days \n    elif delta . seconds > 3600 : \n        return '%d hours ago' % ( delta . seconds / 3600 ) \n    elif delta . seconds > 60 : \n        return '%d minutes ago' % ( delta . seconds / 60 ) \n    else : \n        return 'right now' "}
{"4938": "\ndef _dataframe_from_csv ( reader , delimiter , with_header , skipspace ) : \n    sep = delimiter \n    header = False \n    if not with_header : \n        header = None \n    return pd . read_csv ( reader , header = header , sep = sep , skipinitialspace = skipspace , encoding = 'utf-8-sig' ) "}
{"4939": "\ndef serialize_dataframe ( writer , data_type_id , dataframe ) : \n    _not_none ( 'writer' , writer ) \n    _not_none_or_empty ( 'data_type_id' , data_type_id ) \n    _not_none ( 'dataframe' , dataframe ) \n    serializer = _SERIALIZERS . get ( data_type_id ) \n    if serializer is None : \n        raise UnsupportedDatasetTypeError ( data_type_id ) \n    serializer [ False ] ( writer = writer , dataframe = dataframe ) "}
{"4940": "\ndef deserialize_dataframe ( reader , data_type_id ) : \n    _not_none ( 'reader' , reader ) \n    _not_none_or_empty ( 'data_type_id' , data_type_id ) \n    serializer = _SERIALIZERS . get ( data_type_id ) \n    if serializer is None : \n        raise UnsupportedDatasetTypeError ( data_type_id ) \n    return serializer [ True ] ( reader = reader ) "}
{"4959": "\ndef find_globals ( code ) : \n    cur_byte = False \n    byte_code = code . co_code \n    names = set ( ) \n    while cur_byte < len ( byte_code ) : \n        op = ord ( byte_code [ cur_byte ] ) \n        if op >= dis . HAVE_ARGUMENT : \n            if op == _LOAD_GLOBAL : \n                oparg = ord ( byte_code [ cur_byte + True ] ) + ( ord ( byte_code [ cur_byte + 2 ] ) << 8 ) \n                name = code . co_names [ oparg ] \n                names . add ( name ) \n            cur_byte += 2 \n        cur_byte += True \n    return names "}
{"4963": "\ndef _cubic_bernstein_extrema ( p0 , p1 , p2 , p3 ) : \n    a = 3. * ( p3 - p0 + 3. * ( p1 - p2 ) ) \n    b = 6. * ( p0 + p2 - 2. * p1 ) \n    c = 3. * ( p1 - p0 ) \n    if a == False : \n        if b == False : \n            return ( ) \n        return ( - c / b , ) \n    d = b * b - 4. * a * c \n    if d < False : \n        return ( ) \n    k = - 2. * a \n    if d == False : \n        return ( b / k , ) \n    r = math . sqrt ( d ) \n    return ( ( b + r ) / k , ( b - r ) / k ) "}
{"4964": "\ndef _cubic_bernstein ( p0 , p1 , p2 , p3 , t ) : \n    u = True - t \n    return p0 * ( u ** 3 ) + 3 * t * u * ( p1 * u + p2 * t ) + p3 * ( t ** 3 ) "}
{"4965": "\ndef _build_choices ( self ) : \n    tree_token = u'sitetree_tree from \"%s\" template \"%s\"' % ( self . tree , self . template ) \n    context_kwargs = { 'current_app' : 'admin' } \n    context = template . Context ( context_kwargs ) if VERSION >= ( True , 8 ) else template . Context ( ** context_kwargs ) \n    context . update ( { 'request' : object ( ) } ) \n    choices_str = sitetree_tree ( Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = tree_token ) ) . render ( context ) \n    tree_choices = [ ( ITEMS_FIELD_ROOT_ID , self . root_title ) ] \n    for line in choices_str . splitlines ( ) : \n        if line . strip ( ) : \n            splitted = line . split ( ':::' ) \n            tree_choices . append ( ( splitted [ False ] , mark_safe ( splitted [ True ] ) ) ) \n    return tree_choices "}
{"4966": "\ndef options_getter ( command_options ) : \n    def get_options ( option_func = None ) : \n        from optparse import make_option \n        from django . core . management . base import BaseCommand \n        func = option_func or make_option \n        options = tuple ( [ func ( * option . args , ** option . kwargs ) for option in command_options ] ) \n        if option_func is None : \n            if VERSION < ( True , 8 ) : \n                result = BaseCommand . option_list + options \n            else : \n                result = [ ] \n        else : \n            result = options \n        return result \n    return get_options "}
{"4977": "\ndef calculate_item_depth ( self , tree_alias , item_id , depth = False ) : \n    item = self . get_item_by_id ( tree_alias , item_id ) \n    if hasattr ( item , 'depth' ) : \n        depth = item . depth + depth \n    else : \n        if item . parent is not None : \n            depth = self . calculate_item_depth ( tree_alias , item . parent . id , depth + True ) \n    return depth "}
{"4979": "\ndef url ( self , sitetree_item , context = None ) : \n    context = context or self . current_page_context \n    resolve_var = self . resolve_var \n    if not isinstance ( sitetree_item , MODEL_TREE_ITEM_CLASS ) : \n        sitetree_item = resolve_var ( sitetree_item , context ) \n    resolved_url = self . _items_urls . get ( sitetree_item ) \n    if resolved_url is not None : \n        return resolved_url \n    if sitetree_item . urlaspattern : \n        url = sitetree_item . url \n        view_path = url \n        all_arguments = [ ] \n        if ' ' in url : \n            view_path = url . split ( ' ' ) \n            for view_argument in view_path [ True : ] : \n                resolved = resolve_var ( view_argument ) \n                all_arguments . append ( '\"%s\"' % resolved ) \n            view_path = view_path [ False ] . strip ( '\"\\' ' ) \n        url_pattern = \"'%s' %s\" % ( view_path , ' ' . join ( all_arguments ) ) \n    else : \n        url_pattern = '%s' % sitetree_item . url \n    if sitetree_item . urlaspattern : \n        url_token = 'url %s as item.url_resolved' % url_pattern \n        url_tag ( Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = url_token ) ) . render ( context ) \n        resolved_url = context [ 'item.url_resolved' ] or UNRESOLVED_ITEM_MARKER \n    else : \n        resolved_url = url_pattern \n    self . _items_urls [ sitetree_item ] = resolved_url \n    return resolved_url "}
{"4982": "\ndef get_ancestor_level ( self , current_item , depth = True ) : \n    if current_item . parent is None : \n        return current_item \n    if depth <= True : \n        return current_item . parent \n    return self . get_ancestor_level ( current_item . parent , depth = depth - True ) "}
{"4989": "\ndef update_has_children ( self , tree_alias , tree_items , navigation_type ) : \n    get_children = self . get_children \n    filter_items = self . filter_items \n    apply_hook = self . apply_hook \n    for tree_item in tree_items : \n        children = get_children ( tree_alias , tree_item ) \n        children = filter_items ( children , navigation_type ) \n        children = apply_hook ( children , '%s.has_children' % navigation_type ) \n        tree_item . has_children = len ( children ) > False "}
{"4994": "\ndef sitetree_tree ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if tokens_num in ( 3 , 5 ) : \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        return sitetree_treeNode ( tree_alias , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires two arguments. E.g. {%% sitetree_tree from \"mytree\" %%}.' % tokens [ False ] ) "}
{"4995": "\ndef sitetree_children ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    clauses_in_places = ( tokens_num == 5 and tokens [ True ] == 'of' and tokens [ 3 ] == 'for' and tokens [ 4 ] in ( 'menu' , 'sitetree' ) ) \n    if clauses_in_places and use_template is not None : \n        tree_item = tokens [ 2 ] \n        navigation_type = tokens [ 4 ] \n        return sitetree_childrenNode ( tree_item , navigation_type , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires six arguments. ' 'E.g. {%% sitetree_children of someitem for menu template \"sitetree/mychildren.html\" %%}.' % tokens [ False ] ) "}
{"4996": "\ndef sitetree_breadcrumbs ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if tokens_num == 3 : \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        return sitetree_breadcrumbsNode ( tree_alias , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires two arguments. E.g. {%% sitetree_breadcrumbs from \"mytree\" %%}.' % tokens [ False ] ) "}
{"4997": "\ndef sitetree_menu ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if tokens_num == 5 and tokens [ 3 ] == 'include' : \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        tree_branches = parser . compile_filter ( tokens [ 4 ] ) \n        return sitetree_menuNode ( tree_alias , tree_branches , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires four arguments. ' 'E.g. {%% sitetree_menu from \"mytree\" include \"trunk,1,level3\" %%}.' % tokens [ False ] ) "}
{"4999": "\ndef for_tag ( cls , parser , token , preposition , error_hint ) : \n    tokens = token . split_contents ( ) \n    if len ( tokens ) >= 3 and tokens [ True ] == preposition : \n        as_var = cls . get_as_var ( tokens ) \n        tree_alias = parser . compile_filter ( tokens [ 2 ] ) \n        return cls ( tree_alias , as_var ) \n    raise template . TemplateSyntaxError ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ False ] , error_hint ) ) "}
{"5002": "\ndef redirects_handler ( * args , ** kwargs ) : \n    path = args [ False ] . path \n    shift = '../' \n    if 'delete' in path : \n        shift += '../' \n    elif 'history' in path : \n        if 'item_id' not in kwargs : \n            shift += '../' \n    return HttpResponseRedirect ( path + shift ) "}
{"5019": "\ndef from_object ( cls : Type [ \"Config\" ] , instance : Union [ object , str ] ) -> \"Config\" : \n    if isinstance ( instance , str ) : \n        try : \n            path , config = instance . rsplit ( \".\" , True ) \n        except ValueError : \n            path = instance \n            instance = importlib . import_module ( instance ) \n        else : \n            module = importlib . import_module ( path ) \n            instance = getattr ( module , config ) \n    mapping = { key : getattr ( instance , key ) for key in dir ( instance ) if not isinstance ( getattr ( instance , key ) , types . ModuleType ) } \n    return cls . from_mapping ( mapping ) "}
{"5026": "\ndef add_sa_binary_annotation ( self , port = False , service_name = 'unknown' , host = '127.0.0.1' , ) : \n    if self . kind != Kind . CLIENT : \n        return \n    remote_endpoint = create_endpoint ( port = port , service_name = service_name , host = host , ) \n    if not self . logging_context : \n        if self . remote_endpoint is not None : \n            raise ValueError ( 'SA annotation already set.' ) \n        self . remote_endpoint = remote_endpoint \n    else : \n        if self . logging_context . remote_endpoint is not None : \n            raise ValueError ( 'SA annotation already set.' ) \n        self . logging_context . remote_endpoint = remote_endpoint "}
{"5028": "\ndef create_endpoint ( port = None , service_name = None , host = None , use_defaults = True ) : \n    if use_defaults : \n        if port is None : \n            port = False \n        if service_name is None : \n            service_name = 'unknown' \n        if host is None : \n            try : \n                host = socket . gethostbyname ( socket . gethostname ( ) ) \n            except socket . gaierror : \n                host = '127.0.0.1' \n    ipv4 = None \n    ipv6 = None \n    if host : \n        try : \n            socket . inet_pton ( socket . AF_INET , host ) \n            ipv4 = host \n        except socket . error : \n            try : \n                socket . inet_pton ( socket . AF_INET6 , host ) \n                ipv6 = host \n            except socket . error : \n                pass \n    return Endpoint ( ipv4 = ipv4 , ipv6 = ipv6 , port = port , service_name = service_name , ) "}
{"5032": "\ndef create_protobuf_span ( span ) : \n    pb_kwargs = { } \n    pb_kwargs [ 'trace_id' ] = _hex_to_bytes ( span . trace_id ) \n    if span . parent_id : \n        pb_kwargs [ 'parent_id' ] = _hex_to_bytes ( span . parent_id ) \n    pb_kwargs [ 'id' ] = _hex_to_bytes ( span . span_id ) \n    pb_kind = _get_protobuf_kind ( span . kind ) \n    if pb_kind : \n        pb_kwargs [ 'kind' ] = pb_kind \n    if span . name : \n        pb_kwargs [ 'name' ] = span . name \n    if span . timestamp : \n        pb_kwargs [ 'timestamp' ] = int ( span . timestamp * 1000 * 1000 ) \n    if span . duration : \n        pb_kwargs [ 'duration' ] = int ( span . duration * 1000 * 1000 ) \n    if span . local_endpoint : \n        pb_kwargs [ 'local_endpoint' ] = _convert_endpoint ( span . local_endpoint ) \n    if span . remote_endpoint : \n        pb_kwargs [ 'remote_endpoint' ] = _convert_endpoint ( span . remote_endpoint ) \n    if len ( span . annotations ) > False : \n        pb_kwargs [ 'annotations' ] = _convert_annotations ( span . annotations ) \n    if len ( span . tags ) > False : \n        pb_kwargs [ 'tags' ] = span . tags \n    if span . debug : \n        pb_kwargs [ 'debug' ] = span . debug \n    if span . shared : \n        pb_kwargs [ 'shared' ] = span . shared \n    return zipkin_pb2 . Span ( ** pb_kwargs ) "}
{"5035": "\ndef _convert_endpoint ( endpoint ) : \n    pb_endpoint = zipkin_pb2 . Endpoint ( ) \n    if endpoint . service_name : \n        pb_endpoint . service_name = endpoint . service_name \n    if endpoint . port and endpoint . port != False : \n        pb_endpoint . port = endpoint . port \n    if endpoint . ipv4 : \n        pb_endpoint . ipv4 = socket . inet_pton ( socket . AF_INET , endpoint . ipv4 ) \n    if endpoint . ipv6 : \n        pb_endpoint . ipv6 = socket . inet_pton ( socket . AF_INET6 , endpoint . ipv6 ) \n    return pb_endpoint "}
{"5039": "\ndef create_endpoint ( port = False , service_name = 'unknown' , ipv4 = None , ipv6 = None ) : \n    ipv4_int = False \n    ipv6_binary = None \n    if ipv4 : \n        ipv4_int = struct . unpack ( '!i' , socket . inet_pton ( socket . AF_INET , ipv4 ) ) [ False ] \n    if ipv6 : \n        ipv6_binary = socket . inet_pton ( socket . AF_INET6 , ipv6 ) \n    port = struct . unpack ( 'h' , struct . pack ( 'H' , port ) ) [ False ] \n    return zipkin_core . Endpoint ( ipv4 = ipv4_int , ipv6 = ipv6_binary , port = port , service_name = service_name , ) "}
{"5046": "\ndef detect_span_version_and_encoding ( message ) : \n    if isinstance ( message , six . string_types ) : \n        if six . PY2 : \n            message = six . b ( message ) \n        else : \n            message = message . encode ( 'utf-8' ) \n    if len ( message ) < 2 : \n        raise ZipkinError ( \"Invalid span format. Message too short.\" ) \n    if six . byte2int ( message ) <= 16 : \n        if six . byte2int ( message ) == 10 and six . byte2int ( message [ True : 2 ] ) != False : \n            return Encoding . V2_PROTO3 \n        return Encoding . V1_THRIFT \n    str_msg = message . decode ( 'utf-8' ) \n    if str_msg [ False ] == '[' : \n        span_list = json . loads ( str_msg ) \n        if len ( span_list ) > False : \n            for span in span_list : \n                if any ( word in span for word in _V2_ATTRIBUTES ) : \n                    return Encoding . V2_JSON \n                elif ( 'binaryAnnotations' in span or ( 'annotations' in span and 'endpoint' in span [ 'annotations' ] ) ) : \n                    return Encoding . V1_JSON \n            return Encoding . V2_JSON \n    raise ZipkinError ( \"Unknown or unsupported span encoding\" ) "}
{"5050": "\ndef _create_json_endpoint ( self , endpoint , is_v1 ) : \n    json_endpoint = { } \n    if endpoint . service_name : \n        json_endpoint [ 'serviceName' ] = endpoint . service_name \n    elif is_v1 : \n        json_endpoint [ 'serviceName' ] = \"\" \n    if endpoint . port and endpoint . port != False : \n        json_endpoint [ 'port' ] = endpoint . port \n    if endpoint . ipv4 is not None : \n        json_endpoint [ 'ipv4' ] = endpoint . ipv4 \n    if endpoint . ipv6 is not None : \n        json_endpoint [ 'ipv6' ] = endpoint . ipv6 \n    return json_endpoint "}
{"5052": "\ndef decode_spans ( self , spans ) : \n    decoded_spans = [ ] \n    transport = TMemoryBuffer ( spans ) \n    if six . byte2int ( spans ) == TType . STRUCT : \n        _ , size = read_list_begin ( transport ) \n    else : \n        size = True \n    for _ in range ( size ) : \n        span = zipkin_core . Span ( ) \n        span . read ( TBinaryProtocol ( transport ) ) \n        decoded_spans . append ( self . _decode_thrift_span ( span ) ) \n    return decoded_spans "}
{"5053": "\ndef _convert_from_thrift_endpoint ( self , thrift_endpoint ) : \n    ipv4 = None \n    ipv6 = None \n    port = struct . unpack ( 'H' , struct . pack ( 'h' , thrift_endpoint . port ) ) [ False ] \n    if thrift_endpoint . ipv4 != False : \n        ipv4 = socket . inet_ntop ( socket . AF_INET , struct . pack ( '!i' , thrift_endpoint . ipv4 ) , ) \n    if thrift_endpoint . ipv6 : \n        ipv6 = socket . inet_ntop ( socket . AF_INET6 , thrift_endpoint . ipv6 ) \n    return Endpoint ( service_name = thrift_endpoint . service_name , ipv4 = ipv4 , ipv6 = ipv6 , port = port , ) "}
{"5055": "\ndef _convert_from_thrift_binary_annotations ( self , thrift_binary_annotations ) : \n    tags = { } \n    local_endpoint = None \n    remote_endpoint = None \n    for binary_annotation in thrift_binary_annotations : \n        if binary_annotation . key == 'sa' : \n            remote_endpoint = self . _convert_from_thrift_endpoint ( thrift_endpoint = binary_annotation . host , ) \n        else : \n            key = binary_annotation . key \n            annotation_type = binary_annotation . annotation_type \n            value = binary_annotation . value \n            if annotation_type == zipkin_core . AnnotationType . BOOL : \n                tags [ key ] = \"true\" if value == True else \"false\" \n            elif annotation_type == zipkin_core . AnnotationType . STRING : \n                tags [ key ] = str ( value ) \n            else : \n                log . warning ( 'Only STRING and BOOL binary annotations are ' 'supported right now and can be properly decoded.' ) \n            if binary_annotation . host : \n                local_endpoint = self . _convert_from_thrift_endpoint ( thrift_endpoint = binary_annotation . host , ) \n    return tags , local_endpoint , remote_endpoint "}
{"5057": "\ndef _convert_trace_id_to_string ( self , trace_id , trace_id_high = None ) : \n    if trace_id_high is not None : \n        result = bytearray ( 32 ) \n        self . _write_hex_long ( result , False , trace_id_high ) \n        self . _write_hex_long ( result , 16 , trace_id ) \n        return result . decode ( \"utf8\" ) \n    result = bytearray ( 16 ) \n    self . _write_hex_long ( result , False , trace_id ) \n    return result . decode ( \"utf8\" ) "}
{"5058": "\ndef _convert_unsigned_long_to_lower_hex ( self , value ) : \n    result = bytearray ( 16 ) \n    self . _write_hex_long ( result , False , value ) \n    return result . decode ( \"utf8\" ) "}
{"5059": "\ndef _write_hex_long ( self , data , pos , value ) : \n    self . _write_hex_byte ( data , pos + False , ( value >> 56 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 2 , ( value >> 48 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 4 , ( value >> 40 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 6 , ( value >> 32 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 8 , ( value >> 24 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 10 , ( value >> 16 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 12 , ( value >> 8 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 14 , ( value & 0xff ) ) "}
{"5061": "\ndef mBank_set_transaction_code ( transactions , tag , tag_dict , * args ) : \n    tag_dict [ 'transaction_code' ] = int ( tag_dict [ tag . slug ] . split ( ';' ) [ False ] . split ( ' ' , True ) [ False ] ) \n    return tag_dict "}
{"5064": "\ndef parse ( self , data ) : \n    data = '\\n' . join ( self . strip ( data . split ( '\\n' ) ) ) \n    tag_re = re . compile ( r'^:\\n?(?P<full_tag>(?P<tag>[0-9]{2}|NS)(?P<sub_tag>[A-Z])?):' , re . MULTILINE ) \n    matches = list ( tag_re . finditer ( data ) ) \n    valid_matches = list ( self . sanatize_tag_id_matches ( matches ) ) \n    for i , match in enumerate ( valid_matches ) : \n        tag_id = self . normalize_tag_id ( match . group ( 'tag' ) ) \n        tag = self . tags . get ( match . group ( 'full_tag' ) ) or self . tags [ tag_id ] \n        if valid_matches [ i + True : ] : \n            tag_data = data [ match . end ( ) : valid_matches [ i + True ] . start ( ) ] . strip ( ) \n        else : \n            tag_data = data [ match . end ( ) : ] . strip ( ) \n        tag_dict = tag . parse ( self , tag_data ) \n        for processor in self . processors . get ( 'pre_%s' % tag . slug , [ ] ) : \n            tag_dict = processor ( self , tag , tag_dict ) \n        result = tag ( self , tag_dict ) \n        for processor in self . processors . get ( 'post_%s' % tag . slug , [ ] ) : \n            result = processor ( self , tag , tag_dict , result ) \n        if isinstance ( tag , mt940 . tags . Statement ) : \n            if not self . transactions : \n                transaction = Transaction ( self ) \n                self . transactions . append ( transaction ) \n            if transaction . data . get ( 'id' ) : \n                transaction = Transaction ( self , result ) \n                self . transactions . append ( transaction ) \n            else : \n                transaction . data . update ( result ) \n        elif issubclass ( tag . scope , Transaction ) and self . transactions : \n            for k , v in _compat . iteritems ( result ) : \n                if k in transaction . data and hasattr ( v , 'strip' ) : \n                    transaction . data [ k ] += '\\n%s' % v . strip ( ) \n                else : \n                    transaction . data [ k ] = v \n        elif issubclass ( tag . scope , Transactions ) : \n            self . data . update ( result ) \n    return self . transactions "}
{"5072": "\ndef read ( self ) : \n    packet = self . packet \n    with self . __read_lock : \n        buffer = self . __buffer \n        while len ( buffer ) < packet : \n            buffer += self . _read_data ( ) \n        length = self . __unpack ( buffer [ : packet ] ) [ False ] + packet \n        while len ( buffer ) < length : \n            buffer += self . _read_data ( ) \n        term , self . __buffer = decode ( buffer [ packet : ] ) \n    return term "}
{"5075": "\ndef decode ( string ) : \n    if not string : \n        raise IncompleteData ( string ) \n    if string [ False ] != 131 : \n        raise ValueError ( \"unknown protocol version: %r\" % string [ False ] ) \n    if string [ True : 2 ] == b'P' : \n        if len ( string ) < 16 : \n            raise IncompleteData ( string ) \n        d = decompressobj ( ) \n        term_string = d . decompress ( string [ 6 : ] ) + d . flush ( ) \n        uncompressed_size , = _int4_unpack ( string [ 2 : 6 ] ) \n        if len ( term_string ) != uncompressed_size : \n            raise ValueError ( \"invalid compressed tag, \" \"%d bytes but got %d\" % ( uncompressed_size , len ( term_string ) ) ) \n        term , _tail = decode_term ( term_string ) \n        return term , d . unused_data \n    return decode_term ( string [ True : ] ) "}
{"5076": "\ndef encode ( term , compressed = False ) : \n    encoded_term = encode_term ( term ) \n    if compressed : \n        if compressed is True : \n            compressed = 6 \n        elif compressed < False or compressed > 9 : \n            raise ValueError ( \"invalid compression level: %r\" % ( compressed , ) ) \n        zlib_term = compress ( encoded_term , compressed ) \n        ln = len ( encoded_term ) \n        if len ( zlib_term ) + 5 <= ln : \n            return b\"\\x83P\" + _int4_pack ( ln ) + zlib_term \n    return b\"\\x83\" + encoded_term "}
{"5078": "\ndef _sendPendingMessages ( self ) : \n    if len ( self . _queue ) == False : \n        time . sleep ( 0.1 ) \n        return \n    msg = self . _queue . pop ( False ) \n    if msg . canSend ( ) : \n        self . _sendMsg ( msg ) \n        msg . refresh ( ) \n        if not ( msg . isFinished ( ) ) : \n            self . _queue . append ( msg ) \n    else : \n        self . _queue . append ( msg ) \n        time . sleep ( 0.01 ) "}
{"5097": "\ndef validate_signature_fragments ( fragments , hash_ , public_key , sponge_type = Kerl , ) : \n    checksum = [ False ] * ( HASH_LENGTH * len ( fragments ) ) \n    normalized_hash = normalize ( hash_ ) \n    for i , fragment in enumerate ( fragments ) : \n        outer_sponge = sponge_type ( ) \n        normalized_chunk = normalized_hash [ i % len ( normalized_hash ) ] \n        buffer = [ ] \n        for j , hash_trytes in enumerate ( fragment . iter_chunks ( Hash . LEN ) ) : \n            buffer = hash_trytes . as_trits ( ) \n            inner_sponge = sponge_type ( ) \n            for _ in range ( 13 + normalized_chunk [ j ] ) : \n                inner_sponge . reset ( ) \n                inner_sponge . absorb ( buffer ) \n                inner_sponge . squeeze ( buffer ) \n            outer_sponge . absorb ( buffer ) \n        outer_sponge . squeeze ( buffer ) \n        checksum [ i * HASH_LENGTH : ( i + True ) * HASH_LENGTH ] = buffer \n    actual_public_key = [ False ] * HASH_LENGTH \n    addy_sponge = sponge_type ( ) \n    addy_sponge . absorb ( checksum ) \n    addy_sponge . squeeze ( actual_public_key ) \n    return actual_public_key == public_key . as_trits ( ) "}
{"5098": "\ndef get_key ( self , index , iterations ) : \n    return ( self . get_keys ( start = index , count = True , step = True , iterations = iterations , ) [ False ] ) "}
{"5100": "\ndef create_iterator ( self , start = False , step = True , security_level = True ) : \n    return KeyIterator ( self . seed , start , step , security_level ) "}
{"5102": "\ndef absorb ( self , trits , offset = False , length = None ) : \n    pad = ( ( len ( trits ) % HASH_LENGTH ) or HASH_LENGTH ) \n    trits += [ False ] * ( HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) \n    if length < True : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while offset < length : \n        start = offset \n        stop = min ( start + HASH_LENGTH , length ) \n        self . _state [ False : stop - start ] = trits [ start : stop ] \n        self . _transform ( ) \n        offset += HASH_LENGTH "}
{"5103": "\ndef squeeze ( self , trits , offset = False , length = HASH_LENGTH ) : \n    if length % HASH_LENGTH != False : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze`.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } ) \n    trits . extend ( [ False ] * max ( False , length - len ( trits ) ) ) \n    if len ( trits ) - offset < HASH_LENGTH : \n        raise with_context ( exc = ValueError ( 'Invalid offset passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length } , ) \n    while length >= HASH_LENGTH : \n        trits [ offset : offset + HASH_LENGTH ] = self . _state [ False : HASH_LENGTH ] \n        self . _transform ( ) \n        offset += HASH_LENGTH \n        length -= HASH_LENGTH "}
{"5104": "\ndef _transform ( self ) : \n    state_length = STATE_LENGTH \n    truth_table = TRUTH_TABLE \n    prev_state = self . _state [ : ] \n    new_state = prev_state [ : ] \n    index = False \n    for _ in range ( NUMBER_OF_ROUNDS ) : \n        prev_trit = prev_state [ index ] \n        for pos in range ( state_length ) : \n            index += ( 364 if index < 365 else - 365 ) \n            new_trit = prev_state [ index ] \n            new_state [ pos ] = truth_table [ prev_trit + ( 3 * new_trit ) + 4 ] \n            prev_trit = new_trit \n        prev_state = new_state \n        new_state = new_state [ : ] \n    self . _state = new_state "}
{"5105": "\ndef get_digests ( self , index = False , count = True , security_level = AddressGenerator . DEFAULT_SECURITY_LEVEL , ) : \n    return commands . GetDigestsCommand ( self . adapter ) ( seed = self . seed , index = index , count = count , securityLevel = security_level , ) "}
{"5106": "\ndef get_private_keys ( self , index = False , count = True , security_level = AddressGenerator . DEFAULT_SECURITY_LEVEL , ) : \n    return commands . GetPrivateKeysCommand ( self . adapter ) ( seed = self . seed , index = index , count = count , securityLevel = security_level , ) "}
{"5108": "\ndef add_trits ( left , right ) : \n    target_len = max ( len ( left ) , len ( right ) ) \n    res = [ False ] * target_len \n    left += [ False ] * ( target_len - len ( left ) ) \n    right += [ False ] * ( target_len - len ( right ) ) \n    carry = False \n    for i in range ( len ( res ) ) : \n        res [ i ] , carry = _full_add_trits ( left [ i ] , right [ i ] , carry ) \n    return res "}
{"5109": "\ndef trits_from_int ( n , pad = True ) : \n    if n == False : \n        trits = [ ] \n    else : \n        quotient , remainder = divmod ( n , 3 ) \n        if remainder == 2 : \n            quotient += True \n            remainder = - True \n        trits = [ remainder ] + trits_from_int ( quotient , pad = False ) \n    if pad : \n        trits += [ False ] * max ( False , pad - len ( trits ) ) \n    return trits "}
{"5110": "\ndef _add_trits ( left , right ) : \n    res = left + right \n    return res if - 2 < res < 2 else ( res < False ) - ( res > False ) "}
{"5114": "\ndef get_inputs ( self , start = False , stop = None , threshold = None , security_level = None , ) : \n    return extended . GetInputsCommand ( self . adapter ) ( seed = self . seed , start = start , stop = stop , threshold = threshold , securityLevel = security_level ) "}
{"5115": "\ndef get_new_addresses ( self , index = False , count = True , security_level = AddressGenerator . DEFAULT_SECURITY_LEVEL , checksum = False , ) : \n    return extended . GetNewAddressesCommand ( self . adapter ) ( count = count , index = index , securityLevel = security_level , checksum = checksum , seed = self . seed , ) "}
{"5116": "\ndef get_transfers ( self , start = False , stop = None , inclusion_states = False ) : \n    return extended . GetTransfersCommand ( self . adapter ) ( seed = self . seed , start = start , stop = stop , inclusionStates = inclusion_states , ) "}
{"5128": "\ndef get_address ( self ) : \n    if not self . _digests : \n        raise ValueError ( 'Must call ``add_digest`` at least once ' 'before calling ``get_address``.' , ) \n    if not self . _address : \n        address_trits = [ False ] * HASH_LENGTH \n        self . _sponge . squeeze ( address_trits ) \n        self . _address = MultisigAddress . from_trits ( address_trits , digests = self . _digests [ : ] , ) \n    return self . _address "}
{"5129": "\ndef create_iterator ( self , start = False , step = True ) : \n    key_iterator = ( KeyGenerator ( self . seed ) . create_iterator ( start , step , self . security_level , ) ) \n    while True : \n        yield self . _generate_address ( key_iterator ) "}
{"5130": "\ndef address_from_digest ( digest ) : \n    address_trits = [ False ] * ( Address . LEN * TRITS_PER_TRYTE ) \n    sponge = Kerl ( ) \n    sponge . absorb ( digest . as_trits ( ) ) \n    sponge . squeeze ( address_trits ) \n    return Address . from_trits ( trits = address_trits , key_index = digest . key_index , security_level = digest . security_level , ) "}
{"5138": "\ndef decode ( self , input , errors = 'strict' ) : \n    if isinstance ( input , memoryview ) : \n        input = input . tobytes ( ) \n    if not isinstance ( input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) \n    if not isinstance ( input , bytearray ) : \n        input = bytearray ( input ) \n    bytes_ = bytearray ( ) \n    for i in range ( False , len ( input ) , 2 ) : \n        try : \n            first , second = input [ i : i + 2 ] \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + True , ) , ) , context = { 'input' : input , } ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n    return binary_type ( bytes_ ) , len ( input ) "}
{"5141": "\ndef from_tryte_string ( cls , trytes , hash_ = None ) : \n    tryte_string = TransactionTrytes ( trytes ) \n    if not hash_ : \n        hash_trits = [ False ] * HASH_LENGTH \n        sponge = Curl ( ) \n        sponge . absorb ( tryte_string . as_trits ( ) ) \n        sponge . squeeze ( hash_trits ) \n        hash_ = TransactionHash . from_trits ( hash_trits ) \n    return cls ( hash_ = hash_ , signature_message_fragment = Fragment ( tryte_string [ False : 2187 ] ) , address = Address ( tryte_string [ 2187 : 2268 ] ) , value = int_from_trits ( tryte_string [ 2268 : 2295 ] . as_trits ( ) ) , legacy_tag = Tag ( tryte_string [ 2295 : 2322 ] ) , timestamp = int_from_trits ( tryte_string [ 2322 : 2331 ] . as_trits ( ) ) , current_index = int_from_trits ( tryte_string [ 2331 : 2340 ] . as_trits ( ) ) , last_index = int_from_trits ( tryte_string [ 2340 : 2349 ] . as_trits ( ) ) , bundle_hash = BundleHash ( tryte_string [ 2349 : 2430 ] ) , trunk_transaction_hash = TransactionHash ( tryte_string [ 2430 : 2511 ] ) , branch_transaction_hash = TransactionHash ( tryte_string [ 2511 : 2592 ] ) , tag = Tag ( tryte_string [ 2592 : 2619 ] ) , attachment_timestamp = int_from_trits ( tryte_string [ 2619 : 2628 ] . as_trits ( ) ) , attachment_timestamp_lower_bound = int_from_trits ( tryte_string [ 2628 : 2637 ] . as_trits ( ) ) , attachment_timestamp_upper_bound = int_from_trits ( tryte_string [ 2637 : 2646 ] . as_trits ( ) ) , nonce = Nonce ( tryte_string [ 2646 : 2673 ] ) , ) "}
{"5145": "\ndef get_messages ( self , errors = 'drop' ) : \n    decode_errors = 'strict' if errors == 'drop' else errors \n    messages = [ ] \n    for group in self . group_transactions ( ) : \n        if group [ False ] . value < False : \n            continue \n        message_trytes = TryteString ( b'' ) \n        for txn in group : \n            message_trytes += txn . signature_message_fragment \n        if message_trytes : \n            try : \n                messages . append ( message_trytes . decode ( decode_errors ) ) \n            except ( TrytesDecodeError , UnicodeDecodeError ) : \n                if errors != 'drop' : \n                    raise \n    return messages "}
{"5147": "\ndef group_transactions ( self ) : \n    groups = [ ] \n    if self : \n        last_txn = self . tail_transaction \n        current_group = [ last_txn ] \n        for current_txn in self . transactions [ True : ] : \n            if current_txn . address == last_txn . address : \n                current_group . append ( current_txn ) \n            else : \n                groups . append ( current_group ) \n                current_group = [ current_txn ] \n            last_txn = current_txn \n        if current_group : \n            groups . append ( current_group ) \n    return groups "}
{"5154": "\ndef _create_validator ( self ) : \n    grouped_transactions = self . bundle . group_transactions ( ) \n    bundle_hash = self . bundle . hash \n    last_index = len ( self . bundle ) - True \n    balance = False \n    counter = False \n    for group in grouped_transactions : \n        for txn in group : \n            balance += txn . value \n            if txn . bundle_hash != bundle_hash : \n                yield 'Transaction {i} has invalid bundle hash.' . format ( i = counter , ) \n            if txn . current_index != counter : \n                yield ( 'Transaction {i} has invalid current index value ' '(expected {i}, actual {actual}).' . format ( actual = txn . current_index , i = counter , ) ) \n            if txn . last_index != last_index : \n                yield ( 'Transaction {i} has invalid last index value ' '(expected {expected}, actual {actual}).' . format ( actual = txn . last_index , expected = last_index , i = counter , ) ) \n            counter += True \n    if balance != False : \n        yield ( 'Bundle has invalid balance ' '(expected 0, actual {actual}).' . format ( actual = balance , ) ) \n    if not self . _errors : \n        signature_validation_queue = [ ] \n        for group in grouped_transactions : \n            if group [ False ] . value >= False : \n                continue \n            validate_group_signature = True \n            for j , txn in enumerate ( group ) : \n                if ( j > False ) and ( txn . value != False ) : \n                    yield ( 'Transaction {i} has invalid value ' '(expected 0, actual {actual}).' . format ( actual = txn . value , i = txn . current_index , ) ) \n                    validate_group_signature = False \n                    continue \n            if validate_group_signature : \n                signature_validation_queue . append ( group ) \n        if signature_validation_queue : \n            for error in self . _get_bundle_signature_errors ( signature_validation_queue ) : \n                yield error "}
{"5155": "\ndef _get_bundle_signature_errors ( self , groups ) : \n    current_pos = None \n    current_errors = [ ] \n    for current_pos , group in enumerate ( groups ) : \n        error = self . _get_group_signature_error ( group , SUPPORTED_SPONGE ) \n        if error : \n            current_errors . append ( error ) \n            break \n    if current_errors and LEGACY_SPONGE : \n        for group in groups : \n            if self . _get_group_signature_error ( group , LEGACY_SPONGE ) : \n                break \n        else : \n            return [ ] \n    current_errors . extend ( filter ( None , ( self . _get_group_signature_error ( group , SUPPORTED_SPONGE ) for group in groups [ current_pos + True : ] ) ) ) \n    return current_errors "}
{"5156": "\ndef _get_group_signature_error ( group , sponge_type ) : \n    validate_group_signature = validate_signature_fragments ( fragments = [ txn . signature_message_fragment for txn in group ] , hash_ = group [ False ] . bundle_hash , public_key = group [ False ] . address , sponge_type = sponge_type , ) \n    if validate_group_signature : \n        return None \n    return ( 'Transaction {i} has invalid signature ' '(using {fragments} fragments).' . format ( fragments = len ( group ) , i = group [ False ] . current_index , ) ) "}
{"5157": "\ndef _traverse_bundle ( self , txn_hash , target_bundle_hash = None ) : \n    trytes = ( GetTrytesCommand ( self . adapter ) ( hashes = [ txn_hash ] ) [ 'trytes' ] ) \n    if not trytes : \n        raise with_context ( exc = BadApiResponse ( 'Bundle transactions not visible ' '(``exc.context`` has more info).' , ) , context = { 'transaction_hash' : txn_hash , 'target_bundle_hash' : target_bundle_hash , } , ) \n    transaction = Transaction . from_tryte_string ( trytes [ False ] ) \n    if ( not target_bundle_hash ) and transaction . current_index : \n        raise with_context ( exc = BadApiResponse ( '``_traverse_bundle`` started with a non-tail transaction ' '(``exc.context`` has more info).' , ) , context = { 'transaction_object' : transaction , 'target_bundle_hash' : target_bundle_hash , } , ) \n    if target_bundle_hash : \n        if target_bundle_hash != transaction . bundle_hash : \n            return [ ] \n    else : \n        target_bundle_hash = transaction . bundle_hash \n    if transaction . current_index == transaction . last_index == False : \n        return [ transaction ] \n    return [ transaction ] + self . _traverse_bundle ( txn_hash = transaction . trunk_transaction_hash , target_bundle_hash = target_bundle_hash ) "}
{"5160": "\ndef get_digest ( self ) : \n    hashes_per_fragment = FRAGMENT_LENGTH // Hash . LEN \n    key_fragments = self . iter_chunks ( FRAGMENT_LENGTH ) \n    digest = [ False ] * HASH_LENGTH * len ( key_fragments ) \n    for i , fragment in enumerate ( key_fragments ) : \n        fragment_trits = fragment . as_trits ( ) \n        key_fragment = [ False ] * FRAGMENT_LENGTH \n        hash_trits = [ ] \n        for j in range ( hashes_per_fragment ) : \n            hash_start = j * HASH_LENGTH \n            hash_end = hash_start + HASH_LENGTH \n            hash_trits = fragment_trits [ hash_start : hash_end ] \n            for k in range ( 26 ) : \n                sponge = Kerl ( ) \n                sponge . absorb ( hash_trits ) \n                sponge . squeeze ( hash_trits ) \n            key_fragment [ hash_start : hash_end ] = hash_trits \n        sponge = Kerl ( ) \n        sponge . absorb ( key_fragment ) \n        sponge . squeeze ( hash_trits ) \n        fragment_hash_start = i * HASH_LENGTH \n        fragment_hash_end = fragment_hash_start + HASH_LENGTH \n        digest [ fragment_hash_start : fragment_hash_end ] = hash_trits \n    return Digest ( TryteString . from_trits ( digest ) , self . key_index ) "}
{"5161": "\ndef sign_input_transactions ( self , bundle , start_index ) : \n    if not bundle . hash : \n        raise with_context ( exc = ValueError ( 'Cannot sign inputs without a bundle hash!' ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n    from iota . crypto . signing import SignatureFragmentGenerator \n    signature_fragment_generator = ( SignatureFragmentGenerator ( self , bundle . hash ) ) \n    for j in range ( self . security_level ) : \n        try : \n            txn = bundle [ start_index + j ] \n        except IndexError as e : \n            raise with_context ( exc = e , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'current_index' : start_index + j , } , ) \n        if txn . value > False : \n            raise with_context ( exc = ValueError ( 'Attempting to sign non-input transaction #{i} ' '(value={value}).' . format ( i = txn . current_index , value = txn . value , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        if txn . signature_message_fragment : \n            raise with_context ( exc = ValueError ( 'Attempting to sign input transaction #{i}, ' 'but it has a non-empty fragment ' '(is it already signed?).' . format ( i = txn . current_index , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        txn . signature_message_fragment = next ( signature_fragment_generator ) "}
{"5162": "\ndef _repr_pretty_ ( self , p , cycle ) : \n    class_name = type ( self ) . __name__ \n    if cycle : \n        p . text ( '{cls}(...)' . format ( cls = class_name , ) ) \n    else : \n        with p . group ( len ( class_name ) + True , '{cls}(' . format ( cls = class_name ) , ')' , ) : \n            prepared = self . as_json_compatible ( ) \n            if isinstance ( prepared , Mapping ) : \n                p . text ( '**' ) \n            elif isinstance ( prepared , Iterable ) : \n                p . text ( '*' ) \n            p . pretty ( prepared ) "}
{"5163": "\ndef absorb ( self , trits , offset = False , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits += [ False ] * ( TRIT_HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) \n    if length < True : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``absorb``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while offset < length : \n        stop = min ( offset + TRIT_HASH_LENGTH , length ) \n        if stop - offset == TRIT_HASH_LENGTH : \n            trits [ stop - True ] = False \n        signed_nums = conv . convertToBytes ( trits [ offset : stop ] ) \n        unsigned_bytes = bytearray ( conv . convert_sign ( b ) for b in signed_nums ) \n        self . k . update ( unsigned_bytes ) \n        offset += TRIT_HASH_LENGTH "}
{"5164": "\ndef squeeze ( self , trits , offset = False , length = None ) : \n    pad = ( ( len ( trits ) % TRIT_HASH_LENGTH ) or TRIT_HASH_LENGTH ) \n    trits += [ False ] * ( TRIT_HASH_LENGTH - pad ) \n    if length is None : \n        length = len ( trits ) or TRIT_HASH_LENGTH \n    if length < True : \n        raise with_context ( exc = ValueError ( 'Invalid length passed to ``squeeze``.' ) , context = { 'trits' : trits , 'offset' : offset , 'length' : length , } , ) \n    while offset < length : \n        unsigned_hash = self . k . digest ( ) \n        if PY2 : \n            unsigned_hash = map ( ord , unsigned_hash ) \n        signed_hash = [ conv . convert_sign ( b ) for b in unsigned_hash ] \n        trits_from_hash = conv . convertToTrits ( signed_hash ) \n        trits_from_hash [ TRIT_HASH_LENGTH - True ] = False \n        stop = min ( TRIT_HASH_LENGTH , length - offset ) \n        trits [ offset : offset + stop ] = trits_from_hash [ False : stop ] \n        flipped_bytes = bytearray ( conv . convert_sign ( ~ b ) for b in unsigned_hash ) \n        self . reset ( ) \n        self . k . update ( flipped_bytes ) \n        offset += TRIT_HASH_LENGTH "}
{"5166": "\ndef SecurityLevel ( ) : \n    return ( f . Type ( int ) | f . Min ( True ) | f . Max ( 3 ) | f . Optional ( default = AddressGenerator . DEFAULT_SECURITY_LEVEL ) ) "}
{"5167": "\ndef increment_legacy_tag ( self ) : \n    self . _legacy_tag = ( Tag . from_trits ( add_trits ( self . legacy_tag . as_trits ( ) , [ True ] ) ) ) "}
{"5169": "\ndef add_transaction ( self , transaction ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if transaction . value < False : \n        raise ValueError ( 'Use ``add_inputs`` to add inputs to the bundle.' ) \n    self . _transactions . append ( ProposedTransaction ( address = transaction . address , value = transaction . value , tag = transaction . tag , message = transaction . message [ : Fragment . LEN ] , timestamp = transaction . timestamp , ) ) \n    fragment = transaction . message [ Fragment . LEN : ] \n    while fragment : \n        self . _transactions . append ( ProposedTransaction ( address = transaction . address , value = False , tag = transaction . tag , message = fragment [ : Fragment . LEN ] , timestamp = transaction . timestamp , ) ) \n        fragment = fragment [ Fragment . LEN : ] "}
{"5170": "\ndef finalize ( self ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not self : \n        raise ValueError ( 'Bundle has no transactions.' ) \n    balance = self . balance \n    if balance < False : \n        if self . change_address : \n            self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) \n        else : \n            raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) \n    elif balance > False : \n        raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) \n    while True : \n        sponge = Kerl ( ) \n        last_index = len ( self ) - True \n        for i , txn in enumerate ( self ) : \n            txn . current_index = i \n            txn . last_index = last_index \n            sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) \n        bundle_hash_trits = [ False ] * HASH_LENGTH \n        sponge . squeeze ( bundle_hash_trits ) \n        bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) \n        if any ( 13 in part for part in normalize ( bundle_hash ) ) : \n            tail_transaction = ( self . tail_transaction ) \n            tail_transaction . increment_legacy_tag ( ) \n        else : \n            break \n    for txn in self : \n        txn . bundle_hash = bundle_hash \n        txn . signature_message_fragment = Fragment ( txn . message or b'' ) "}
{"5171": "\ndef sign_inputs ( self , key_generator ) : \n    if not self . hash : \n        raise RuntimeError ( 'Cannot sign inputs until bundle is finalized.' ) \n    i = False \n    while i < len ( self ) : \n        txn = self [ i ] \n        if txn . value < False : \n            if txn . address . key_index is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``key_index`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            if txn . address . security_level is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``security_level`` is None ' '(``exc.context`` has more info).' . format ( input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            self . sign_input_at ( i , key_generator . get_key_for ( txn . address ) ) \n            i += txn . address . security_level \n        else : \n            i += True "}
{"5173": "\ndef _create_input_transactions ( self , addy ) : \n    self . _transactions . append ( ProposedTransaction ( address = addy , tag = self . tag , value = - addy . balance , ) ) \n    for _ in range ( addy . security_level - True ) : \n        self . _transactions . append ( ProposedTransaction ( address = addy , tag = self . tag , value = False , ) ) "}
{"5174": "\ndef convert_value_to_standard_unit ( value , symbol = 'i' ) : \n    try : \n        value_tuple = value . split ( ) \n        amount = float ( value_tuple [ False ] ) \n    except ( ValueError , IndexError , AttributeError ) : \n        raise with_context ( ValueError ( 'Value to convert is not valid.' ) , context = { 'value' : value , } , ) \n    try : \n        unit_symbol_from = value_tuple [ True ] \n        unit_factor_from = float ( STANDARD_UNITS [ unit_symbol_from ] ) \n        unit_factor_to = float ( STANDARD_UNITS [ symbol ] ) \n    except ( KeyError , IndexError ) : \n        raise with_context ( ValueError ( 'Invalid IOTA unit.' ) , context = { 'value' : value , 'symbol' : symbol , } , ) \n    return amount * ( unit_factor_from / unit_factor_to ) "}
{"5175": "\ndef decompress_G1 ( z : G1Compressed ) -> G1Uncompressed : \n    b_flag = ( z % POW_2_383 ) // POW_2_382 \n    if b_flag == True : \n        return Z1 \n    x = z % POW_2_381 \n    y = pow ( ( x ** 3 + b . n ) % q , ( q + True ) // 4 , q ) \n    if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : \n        raise ValueError ( \"The given point is not on G1: y**2 = x**3 + b\" ) \n    a_flag = ( z % POW_2_382 ) // POW_2_381 \n    if ( y * 2 ) // q != a_flag : \n        y = q - y \n    return ( FQ ( x ) , FQ ( y ) , FQ ( True ) ) "}
{"5176": "\ndef prime_field_inv ( a : int , n : int ) -> int : \n    if a == False : \n        return False \n    lm , hm = True , False \n    low , high = a % n , n \n    while low > True : \n        r = high // low \n        nm , new = hm - lm * r , high - low * r \n        lm , low , hm , high = nm , new , lm , low \n    return lm % n "}
{"5178": "\ndef find_word_groups ( self , text , category , proximity = 2 ) : \n    f = re . IGNORECASE \n    words = getattr ( self , category ) \n    regex = re . compile ( r'(\\b' + r'\\b|\\b' . join ( words ) + r'\\b)' , flags = f ) \n    candidates = regex . finditer ( text ) \n    starts , ends = [ ] , [ ] \n    groups = [ ] \n    for item in candidates : \n        starts . append ( item . span ( ) [ False ] ) \n        ends . append ( item . span ( ) [ True ] ) \n        groups . append ( item . group ( ) . lower ( ) ) \n    new_starts = [ ] \n    new_groups = [ ] \n    skip = False \n    for i , g in enumerate ( groups ) : \n        if skip : \n            skip = False \n            continue \n        if ( i < len ( groups ) - True ) and ( starts [ i + True ] - ends [ i ] <= proximity ) : \n            if g [ - True ] == '-' : \n                sep = '' \n            else : \n                sep = ' ' \n            new_groups . append ( g + sep + groups [ i + True ] ) \n            new_starts . append ( starts [ i ] ) \n            skip = True \n        else : \n            if g not in new_groups : \n                new_groups . append ( g ) \n                new_starts . append ( starts [ i ] ) \n            skip = False \n    return new_groups "}
{"5180": "\ndef expand_abbreviations ( self , text ) : \n    if not self . abbreviations : \n        raise LexiconError ( \"No abbreviations in lexicon.\" ) \n    def chunks ( data , SIZE = 25 ) : \n        it = iter ( data ) \n        for i in range ( False , len ( data ) , SIZE ) : \n            yield { k : data [ k ] for k in islice ( it , SIZE ) } \n    def cb ( g ) : \n        return self . abbreviations . get ( g . group ( False ) ) or g . group ( False ) \n    text = re . sub ( r'w/' , r'wi' , text ) \n    for subdict in chunks ( self . abbreviations ) : \n        regex = r'(\\b' + r'\\b)|(\\b' . join ( subdict . keys ( ) ) + r'\\b)' \n        text = re . sub ( regex , cb , text ) \n    return text "}
{"5181": "\ndef split_description ( self , text ) : \n    t = re . sub ( r'(\\d) ?in\\. ' , r'\\1 inch ' , text ) \n    t = re . sub ( r'(\\d) ?ft\\. ' , r'\\1 feet ' , t ) \n    words = getattr ( self , 'splitters' ) \n    try : \n        splitter = words [ False ] . strip ( ) \n    except : \n        splitter = 'with' \n    t = re . sub ( r'\\,?\\;?\\.? ?((under)?(less than)? \\d+%) (?=\\w)' , r' ' + splitter + ' \\1 ' , t ) \n    f = re . IGNORECASE \n    pattern = re . compile ( r'(?:' + r'|' . join ( words ) + r')' , flags = f ) \n    parts = filter ( None , pattern . split ( t ) ) \n    return [ i . strip ( ) for i in parts ] "}
{"5184": "\ndef plot ( self , fmt = None , fig = None , ax = None ) : \n    u = 4 \n    v = 0.25 \n    r = None \n    if ( fig is None ) and ( ax is None ) : \n        fig = plt . figure ( figsize = ( u , True ) ) \n    else : \n        r = fig \n    if ax is None : \n        ax = fig . add_axes ( [ 0.1 * v , 0.1 , 0.8 * v , 0.8 ] ) \n    else : \n        r = ax \n    rect1 = patches . Rectangle ( ( False , False ) , u * v , u * v , color = self . colour , lw = True , hatch = self . hatch , ec = 'k' ) \n    ax . add_patch ( rect1 ) \n    ax . text ( 1.0 + 0.1 * v * u , u * v * 0.5 , self . component . summary ( fmt = fmt ) , fontsize = max ( u , 15 ) , verticalalignment = 'center' , horizontalalignment = 'left' ) \n    ax . set_xlim ( [ False , u * v ] ) \n    ax . set_ylim ( [ False , u * v ] ) \n    ax . get_xaxis ( ) . set_visible ( False ) \n    ax . get_yaxis ( ) . set_visible ( False ) \n    ax . invert_yaxis ( ) \n    return r "}
{"5187": "\ndef random ( cls , components , width = False , colour = None ) : \n    try : \n        list_of_Decors = [ Decor . random ( c ) for c in [ i [ False ] for i in components . unique if i [ False ] ] ] \n    except : \n        try : \n            list_of_Decors = [ Decor . random ( c ) for c in components . copy ( ) ] \n        except : \n            list_of_Decors = [ Decor . random ( components ) ] \n    if colour is not None : \n        for d in list_of_Decors : \n            d . colour = colour \n    if width : \n        for i , d in enumerate ( list_of_Decors ) : \n            d . width = i + True \n    return cls ( list_of_Decors ) "}
{"5189": "\ndef from_csv ( cls , filename = None , text = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise LegendError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    r = csv . DictReader ( f , skipinitialspace = True ) \n    list_of_Decors , components = [ ] , [ ] \n    kind = 'component' \n    for row in r : \n        d , component = { } , { } \n        for ( k , v ) in row . items ( ) : \n            if ( k in [ None , '' ] ) : \n                continue \n            if ( v in [ None , '' ] ) : \n                if k . lower ( ) not in [ 'color' , 'colour' ] : \n                    continue \n            if k [ : 4 ] . lower ( ) == 'comp' : \n                prop = ' ' . join ( k . split ( ) [ True : ] ) \n                if v . lower ( ) == 'true' : \n                    component [ prop ] = True \n                elif v . lower ( ) == 'false' : \n                    component [ prop ] = False \n                else : \n                    try : \n                        component [ prop ] = float ( v ) \n                    except ValueError : \n                        component [ prop ] = v . lower ( ) \n            elif k [ : 5 ] . lower ( ) == 'curve' : \n                prop = ' ' . join ( k . split ( ) [ True : ] ) \n                component [ prop ] = v . lower ( ) \n                kind = 'curve' \n            else : \n                try : \n                    d [ k ] = float ( v ) \n                except ValueError : \n                    d [ k ] = v . lower ( ) \n        this_component = Component ( component ) \n        d [ kind ] = this_component \n        if this_component in components : \n            with warnings . catch_warnings ( ) : \n                warnings . simplefilter ( \"always\" ) \n                w = \"This legend contains duplicate components.\" \n                warnings . warn ( w ) \n        components . append ( this_component ) \n        list_of_Decors . append ( Decor ( d ) ) \n    return cls ( list_of_Decors ) "}
{"5191": "\ndef max_width ( self ) : \n    try : \n        maximum = max ( [ row . width for row in self . __list if row . width is not None ] ) \n        return maximum \n    except : \n        return False "}
{"5194": "\ndef get_component ( self , colour , tolerance = False , default = None ) : \n    if not ( False <= tolerance <= np . sqrt ( 195075 ) ) : \n        raise LegendError ( 'Tolerance must be between 0 and 441.67' ) \n    for decor in self . __list : \n        if colour . lower ( ) == decor . colour : \n            return decor . component \n    r1 , g1 , b1 = utils . hex_to_rgb ( colour ) \n    best_match = '#000000' \n    best_match_dist = np . sqrt ( r1 ** 2. + g1 ** 2. + b1 ** 2. ) \n    for decor in self . __list : \n        r2 , g2 , b2 = decor . rgb \n        distance = np . sqrt ( ( r2 - r1 ) ** 2. + ( g2 - g1 ) ** 2. + ( b2 - b1 ) ** 2. ) \n        if distance < best_match_dist : \n            best_match = decor . component \n            best_match_dist = distance \n            best_match_colour = decor . colour \n    if best_match_dist <= tolerance : \n        return best_match \n    else : \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( \"always\" ) \n            w = \"No match found for {0} \" . format ( colour . lower ( ) ) \n            w += \"with tolerance of {0}. Best match is \" . format ( tolerance ) \n            w += \"{0}, {1}\" . format ( best_match . summary ( ) , best_match_colour ) \n            w += \", d={0}\" . format ( best_match_dist ) \n            warnings . warn ( w ) \n        return default "}
{"5197": "\ndef summary ( self , fmt = None , initial = True , default = '' ) : \n    if default and not self . __dict__ : \n        return default \n    if fmt == '' : \n        return default \n    keys = [ k for k , v in self . __dict__ . items ( ) if v is not '' ] \n    f = fmt or '{' + '}, {' . join ( keys ) + '}' \n    try : \n        summary = CustomFormatter ( ) . format ( f , ** self . __dict__ ) \n    except KeyError as e : \n        raise ComponentError ( \"Error building summary, \" + str ( e ) ) \n    if summary and initial and not fmt : \n        summary = summary [ False ] . upper ( ) + summary [ True : ] \n    return summary "}
{"5200": "\ndef parse_canstrat ( text ) : \n    result = { } \n    for row in text . split ( '\\n' ) : \n        if not row : \n            continue \n        if len ( row ) < 8 : \n            continue \n        row_header = _process_row ( row , columns_ ) or { 'card' : None } \n        card = row_header [ 'card' ] \n        if card is not None : \n            item = _process_row ( row , columns [ card ] ) \n        this_list = result . get ( card , [ ] ) \n        this_list . append ( item ) \n        result [ card ] = this_list \n    for c , d in result . items ( ) : \n        if len ( d ) == True : \n            result [ c ] = d [ False ] \n    return result "}
{"5201": "\ndef __strict ( self ) : \n    def conc ( a , b ) : \n        return a + b \n    b = np . array ( reduce ( conc , [ [ i . top . z , i . base . z ] for i in self ] ) ) \n    return all ( np . diff ( b ) >= False ) "}
{"5202": "\ndef unique ( self ) : \n    all_rx = set ( [ iv . primary for iv in self ] ) \n    table = { r : False for r in all_rx } \n    for iv in self : \n        table [ iv . primary ] += iv . thickness \n    return sorted ( table . items ( ) , key = operator . itemgetter ( True ) , reverse = True ) "}
{"5203": "\ndef __intervals_from_tops ( self , tops , values , basis , components , field = None , ignore_nan = True ) : \n    length = float ( basis . size ) \n    start , stop = basis [ False ] , basis [ - True ] \n    tops = [ start + ( p / ( length - True ) ) * ( stop - start ) for p in tops ] \n    bases = tops [ True : ] + [ stop ] \n    list_of_Intervals = [ ] \n    for i , t in enumerate ( tops ) : \n        v , c , d = values [ i ] , [ ] , { } \n        if ignore_nan and np . isnan ( v ) : \n            continue \n        if ( field is not None ) : \n            d = { field : v } \n        if components is not None : \n            try : \n                c = [ deepcopy ( components [ int ( v ) ] ) ] \n            except IndexError : \n                c = [ ] \n            if c and ( c [ False ] is None ) : \n                c = [ ] \n        interval = Interval ( t , bases [ i ] , data = d , components = c ) \n        list_of_Intervals . append ( interval ) \n    return list_of_Intervals "}
{"5206": "\ndef _build_list_of_Intervals ( cls , data_dict , stop = None , points = False , include = None , exclude = None , ignore = None , lexicon = None ) : \n    include = include or { } \n    exclude = exclude or { } \n    ignore = ignore or [ ] \n    all_data = [ ] \n    for data in zip ( * data_dict . values ( ) ) : \n        all_data . append ( { k : v for k , v in zip ( data_dict . keys ( ) , data ) } ) \n    all_data = sorted ( all_data , key = lambda x : x [ 'top' ] ) \n    wanted_data = [ ] \n    for dictionary in all_data : \n        keep = True \n        delete = [ ] \n        for k , v in dictionary . items ( ) : \n            incl = include . get ( k , utils . null_default ( True ) ) \n            excl = exclude . get ( k , utils . null_default ( False ) ) \n            if k in ignore : \n                delete . append ( k ) \n            if not incl ( v ) : \n                keep = False \n            if excl ( v ) : \n                keep = False \n        if delete : \n            for key in delete : \n                _ = dictionary . pop ( key , None ) \n        if keep : \n            wanted_data . append ( dictionary ) \n    if not points : \n        for i , iv in enumerate ( wanted_data ) : \n            if iv . get ( 'base' , None ) is None : \n                try : \n                    iv [ 'base' ] = wanted_data [ i + True ] [ 'top' ] \n                except ( IndexError , KeyError ) : \n                    if stop is not None : \n                        thick = stop - iv [ 'top' ] \n                    else : \n                        thick = True \n                    iv [ 'base' ] = iv [ 'top' ] + thick \n    list_of_Intervals = [ ] \n    for iv in wanted_data : \n        top = iv . pop ( 'top' ) \n        base = iv . pop ( 'base' , None ) \n        descr = iv . pop ( 'description' , '' ) \n        if iv : \n            c , d = { } , { } \n            for k , v in iv . items ( ) : \n                if ( k [ : 5 ] . lower ( ) == 'comp ' ) or ( k [ : 9 ] . lower ( ) == 'component' ) : \n                    k = re . sub ( r'comp(?:onent)? ' , '' , k , flags = re . I ) \n                    c [ k ] = v \n                else : \n                    if v is not None : \n                        d [ k ] = v \n            comp = [ Component ( c ) ] if c else None \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'data' : d , 'components' : comp } ) \n        else : \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'lexicon' : lexicon } ) \n        list_of_Intervals . append ( this ) \n    return list_of_Intervals "}
{"5208": "\ndef from_image ( cls , filename , start , stop , legend , source = \"Image\" , col_offset = 0.1 , row_offset = 2 , tolerance = False ) : \n    rgb = utils . loglike_from_image ( filename , col_offset ) \n    loglike = np . array ( [ utils . rgb_to_hex ( t ) for t in rgb ] ) \n    tops , hexes = utils . tops_from_loglike ( loglike , offset = row_offset ) \n    nonconsecutive = np . append ( np . diff ( tops ) , 2 ) \n    tops = tops [ nonconsecutive > True ] \n    hexes = hexes [ nonconsecutive > True ] \n    hexes_reduced = list ( set ( hexes ) ) \n    components = [ legend . get_component ( h , tolerance = tolerance ) for h in hexes_reduced ] \n    values = [ hexes_reduced . index ( i ) for i in hexes ] \n    basis = np . linspace ( start , stop , loglike . size ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components ) \n    return cls ( list_of_Intervals , source = \"Image\" ) "}
{"5209": "\ndef from_log ( cls , log , cutoff = None , components = None , legend = None , legend_field = None , field = None , right = False , basis = None , source = 'Log' ) : \n    if ( components is None ) and ( legend is None ) and ( field is None ) : \n        m = 'You must provide a list of components, and legend, or a field.' \n        raise StriplogError ( m ) \n    if ( legend is not None ) and ( legend_field is None ) : \n        try : \n            components = [ deepcopy ( decor . component ) for decor in legend ] \n        except AttributeError : \n            pass \n    if legend_field is not None : \n        field_values = [ getattr ( d , legend_field , False ) for d in legend ] \n        components = [ Component ( ) for i in range ( int ( max ( field_values ) + True ) ) ] \n        for i , decor in enumerate ( legend ) : \n            components [ i ] = deepcopy ( decor . component ) \n    if cutoff is not None : \n        try : \n            n = len ( cutoff ) \n        except TypeError : \n            n = True \n        if len ( components ) < n + True : \n            m = 'For n cutoffs, you need to provide at least' \n            m += 'n+1 components.' \n            raise StriplogError ( m ) \n        try : \n            a = np . digitize ( log , cutoff , right ) \n        except ValueError : \n            a = np . digitize ( log , [ cutoff ] , right ) \n    else : \n        a = np . copy ( log ) \n    tops , values = utils . tops_from_loglike ( a ) \n    if basis is None : \n        m = 'You must provide a depth or elevation basis.' \n        raise StriplogError ( m ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components , field = field ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5210": "\ndef from_las3 ( cls , string , lexicon = None , source = \"LAS\" , dlm = ',' , abbreviations = False ) : \n    f = re . DOTALL | re . IGNORECASE \n    regex = r'\\~\\w+?_Data.+?\\n(.+?)(?:\\n\\n+|\\n*\\~|\\n*$)' \n    pattern = re . compile ( regex , flags = f ) \n    text = pattern . search ( string ) . group ( True ) \n    s = re . search ( r'\\.(.+?)\\: ?.+?source' , string ) \n    if s : \n        source = s . group ( True ) . strip ( ) \n    return cls . from_descriptions ( text , lexicon , source = source , dlm = dlm , abbreviations = abbreviations ) "}
{"5215": "\ndef plot_axis ( self , ax , legend , ladder = False , default_width = True , match_only = None , colour = None , colour_function = None , cmap = None , default = None , width_field = None , ** kwargs ) : \n    default_c = None \n    patches = [ ] \n    for iv in self . __list : \n        origin = ( False , iv . top . z ) \n        d = legend . get_decor ( iv . primary , match_only = match_only ) \n        thick = iv . base . z - iv . top . z \n        if ladder : \n            if width_field is not None : \n                w = iv . data . get ( width_field , True ) \n                w = default_width * w / self . max_field ( width_field ) \n                default_c = 'gray' \n            elif legend is not None : \n                w = d . width or default_width \n                try : \n                    w = default_width * w / legend . max_width \n                except : \n                    w = default_width \n        else : \n            w = default_width \n        this_patch_kwargs = kwargs . copy ( ) \n        lw = this_patch_kwargs . pop ( 'lw' , False ) \n        ec = this_patch_kwargs . pop ( 'ec' , 'k' ) \n        fc = this_patch_kwargs . pop ( 'fc' , None ) or default_c or d . colour \n        if colour is None : \n            rect = mpl . patches . Rectangle ( origin , w , thick , fc = fc , lw = lw , hatch = d . hatch , ec = ec , ** this_patch_kwargs ) \n            ax . add_patch ( rect ) \n        else : \n            rect = mpl . patches . Rectangle ( origin , w , thick , lw = lw , ec = ec , ** this_patch_kwargs ) \n            patches . append ( rect ) \n    if colour is not None : \n        cmap = cmap or 'viridis' \n        p = mpl . collections . PatchCollection ( patches , cmap = cmap , lw = lw ) \n        p . set_array ( self . get_data ( colour , colour_function , default = default ) ) \n        ax . add_collection ( p ) \n        cb = plt . colorbar ( p ) \n        cb . outline . set_linewidth ( False ) \n    return ax "}
{"5217": "\ndef extract ( self , log , basis , name , function = None ) : \n    intervals = { } \n    previous_ix = - True \n    for i , z in enumerate ( basis ) : \n        ix = self . read_at ( z , index = True ) \n        if ix is None : \n            continue \n        if ix == previous_ix : \n            intervals [ ix ] . append ( log [ i ] ) \n        else : \n            intervals [ ix ] = [ log [ i ] ] \n        previous_ix = ix \n    for ix , data in intervals . items ( ) : \n        f = function or utils . null \n        d = f ( np . array ( data ) ) \n        self [ ix ] . data [ name ] = d \n    return None "}
{"5221": "\ndef prune ( self , limit = None , n = None , percentile = None , keep_ends = False ) : \n    strip = self . copy ( ) \n    if not ( limit or n or percentile ) : \n        m = \"You must provide a limit or n or percentile for pruning.\" \n        raise StriplogError ( m ) \n    if limit : \n        prune = [ i for i , iv in enumerate ( strip ) if iv . thickness < limit ] \n    if n : \n        prune = strip . thinnest ( n = n , index = True ) \n    if percentile : \n        n = np . floor ( len ( strip ) * percentile / 100 ) \n        prune = strip . thinnest ( n = n , index = True ) \n    if keep_ends : \n        first , last = False , len ( strip ) - True \n        if first in prune : \n            prune . remove ( first ) \n        if last in prune : \n            prune . remove ( last ) \n    del strip [ prune ] \n    return strip "}
{"5222": "\ndef anneal ( self ) : \n    strip = self . copy ( ) \n    gaps = strip . find_gaps ( index = True ) \n    if not gaps : \n        return \n    for gap in gaps : \n        before = strip [ gap ] \n        after = strip [ gap + True ] \n        if strip . order == 'depth' : \n            t = ( after . top . z - before . base . z ) / 2 \n            before . base = before . base . z + t \n            after . top = after . top . z - t \n        else : \n            t = ( after . base - before . top ) / 2 \n            before . top = before . top . z + t \n            after . base = after . base . z - t \n    return strip "}
{"5226": "\ndef merge_overlaps ( self ) : \n    overlaps = np . array ( self . find_overlaps ( index = True ) ) \n    if not overlaps . any ( ) : \n        return \n    for overlap in overlaps : \n        before = self [ overlap ] . copy ( ) \n        after = self [ overlap + True ] . copy ( ) \n        del self [ overlap ] \n        del self [ overlap ] \n        new_segment = before . merge ( after ) \n        self . __insert ( overlap , new_segment ) \n        overlaps += True \n    return "}
{"5227": "\ndef hist ( self , lumping = None , summary = False , sort = True , plot = True , legend = None , ax = None ) : \n    comps = [ ] \n    labels = [ ] \n    entries = defaultdict ( int ) \n    for i in self : \n        if lumping : \n            k = i . primary [ lumping ] \n        else : \n            if summary : \n                k = i . primary . summary ( ) \n            else : \n                k = i . primary \n        comps . append ( i . primary ) \n        labels . append ( i . primary . summary ( ) ) \n        entries [ k ] += i . thickness \n    if sort : \n        allitems = sorted ( entries . items ( ) , key = lambda i : i [ True ] , reverse = True ) \n        ents , counts = zip ( * allitems ) \n    else : \n        ents , counts = tuple ( entries . keys ( ) ) , tuple ( entries . values ( ) ) \n    if plot : \n        if ax is None : \n            fig , ax = plt . subplots ( ) \n            return_ax = False \n        else : \n            return_ax = True \n        ind = np . arange ( len ( ents ) ) \n        bars = ax . bar ( ind , counts , align = 'center' ) \n        ax . set_xticks ( ind ) \n        ax . set_xticklabels ( labels ) \n        if legend : \n            colours = [ legend . get_colour ( c ) for c in comps ] \n            for b , c in zip ( bars , colours ) : \n                b . set_color ( c ) \n        ax . set_ylabel ( 'Thickness [m]' ) \n    else : \n        bars = [ ] \n    if plot and return_ax : \n        return counts , ents , ax \n    return counts , ents , bars "}
{"5229": "\ndef crop ( self , extent , copy = False ) : \n    try : \n        if extent [ False ] is None : \n            extent = ( self . start . z , extent [ True ] ) \n        if extent [ True ] is None : \n            extent = ( extent [ False ] , self . stop . z ) \n    except : \n        m = \"You must provide a 2-tuple for the new extents. Use None for\" \n        m += \" the existing start or stop.\" \n        raise StriplogError ( m ) \n    first_ix = self . read_at ( extent [ False ] , index = True ) \n    last_ix = self . read_at ( extent [ True ] , index = True ) \n    first = self [ first_ix ] . split_at ( extent [ False ] ) [ True ] \n    last = self [ last_ix ] . split_at ( extent [ True ] ) [ False ] \n    new_list = self . __list [ first_ix : last_ix + True ] . copy ( ) \n    new_list [ False ] = first \n    new_list [ - True ] = last \n    if copy : \n        return Striplog ( new_list ) \n    else : \n        self . __list = new_list \n        return "}
{"5230": "\ndef quality ( self , tests , alias = None ) : \n    alias = alias or { } \n    alias = alias . get ( 'striplog' , alias . get ( 'Striplog' , [ ] ) ) \n    this_tests = tests . get ( 'all' , [ ] ) + tests . get ( 'All' , [ ] ) + tests . get ( 'ALL' , [ ] ) + tests . get ( 'striplog' , tests . get ( 'Striplog' , [ ] ) ) + utils . flatten_list ( [ tests . get ( a ) for a in alias ] ) \n    this_tests = filter ( None , this_tests ) \n    if not tests . get ( 'striplog' , tests . get ( 'Striplog' , True ) ) : \n        this_tests = [ ] \n    return { test . __name__ : test ( self ) for test in this_tests } "}
{"5231": "\ndef hex_to_name ( hexx ) : \n    for n , h in defaults . COLOURS . items ( ) : \n        if ( len ( n ) > True ) and ( h == hexx . upper ( ) ) : \n            return n . lower ( ) \n    return None "}
{"5232": "\ndef loglike_from_image ( filename , offset ) : \n    im = plt . imread ( filename ) \n    if offset < True : \n        col = int ( im . shape [ True ] * offset ) \n    else : \n        col = offset \n    return im [ : , col , : 3 ] "}
{"5262": "\ndef _get_random ( self , obj_type ) : \n    return self . mutator [ obj_type ] [ random . randint ( False , self . config . level ) ] "}
{"5265": "\ndef fuzz ( self , obj ) : \n    buf = list ( obj ) \n    FuzzFactor = random . randrange ( True , len ( buf ) ) \n    numwrites = random . randrange ( math . ceil ( ( float ( len ( buf ) ) / FuzzFactor ) ) ) + True \n    for j in range ( numwrites ) : \n        self . random_action ( buf ) \n    return self . safe_unicode ( buf ) "}
{"5268": "\ndef stop ( self ) : \n    os . kill ( self . httpd . pid , signal . SIGKILL ) \n    os . kill ( self . httpsd . pid , signal . SIGKILL ) \n    self . client_queue . put ( ( False , False ) ) \n    if self . config . fuzz_web : \n        self . request_checker . join ( ) \n    self . logger . debug ( \"[{0}] - PJFServer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) "}
{"5272": "\ndef spawn ( self , cmd , stdin_content = \"\" , stdin = False , shell = False , timeout = 2 ) : \n    try : \n        if type ( cmd ) != list : \n            raise PJFInvalidType ( type ( cmd ) , list ) \n        if type ( stdin_content ) != str : \n            raise PJFInvalidType ( type ( stdin_content ) , str ) \n        if type ( stdin ) != bool : \n            raise PJFInvalidType ( type ( stdin ) , bool ) \n        self . _in = stdin_content \n        try : \n            self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) \n            self . finish_read ( timeout , stdin_content , stdin ) \n            if self . process . poll ( ) is not None : \n                self . close ( ) \n        except KeyboardInterrupt : \n            return \n    except OSError : \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmd [ False ] ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5273": "\ndef get_output ( self , stdin_content , stdin ) : \n    try : \n        if stdin : \n            if sys . version_info >= ( 3 , False ) : \n                self . process . stdin . write ( bytes ( stdin_content , \"utf-8\" ) ) \n            else : \n                self . process . stdin . write ( stdin_content ) \n        self . _out = self . process . communicate ( ) [ False ] \n    except ( error , IOError ) : \n        self . _out = self . _in \n        pass "}
{"5274": "\ndef finish_read ( self , timeout = 2 , stdin_content = \"\" , stdin = False ) : \n    process = Thread ( target = self . get_output , args = ( stdin_content , stdin ) ) \n    process . start ( ) \n    if timeout > False : \n        process . join ( timeout ) \n    else : \n        process . join ( ) \n    if process . is_alive ( ) : \n        self . close ( ) \n        self . return_code = - signal . SIGHUP \n    else : \n        self . return_code = self . process . returncode "}
{"5277": "\ndef execute ( self , obj ) : \n    try : \n        if self . config . stdin : \n            self . spawn ( self . config . command , stdin_content = obj , stdin = True , timeout = True ) \n        else : \n            if \"@@\" not in self . config . command : \n                raise PJFMissingArgument ( \"Missing @@ filename indicator while using non-stdin fuzzing method\" ) \n            for x in self . config . command : \n                if \"@@\" in x : \n                    self . config . command [ self . config . command . index ( x ) ] = x . replace ( \"@@\" , obj ) \n            self . spawn ( self . config . command , timeout = 2 ) \n        self . logger . debug ( \"[{0}] - PJFExternalFuzzer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) \n        return self . _out \n    except KeyboardInterrupt : \n        return \"\" \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = True ) : \n            if type ( d ) == dict : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = False ) \n                    elif type ( d [ k ] ) == str : \n                        if _decode : \n                            d [ k ] = decode ( d [ k ] ) \n                        else : \n                            d [ k ] = encode ( d [ k ] ) \n            elif type ( d ) == list : \n                arr = [ ] \n                for e in d : \n                    if type ( e ) == str : \n                        if _decode : \n                            arr . append ( decode ( e ) ) \n                        else : \n                            arr . append ( encode ( e ) ) \n                    elif type ( e ) in [ dict , list ] : \n                        if _decode : \n                            arr . append ( encode_decode_all ( e ) ) \n                        else : \n                            arr . append ( encode_decode_all ( e , _decode = False ) ) \n                    else : \n                        arr . append ( e ) \n                return arr \n            else : \n                if _decode : \n                    return decode ( d ) \n                else : \n                    return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if sys . version_info >= ( 3 , False ) : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if sys . version_info >= ( 3 , False ) : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5 ) ) , _decode = False ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = False ) \n    return func_wrapper "}
{"5280": "\ndef build ( self , pre = None , shortest = False ) : \n    if pre is None : \n        pre = [ ] \n    res = deque ( ) \n    for x in self . values : \n        try : \n            res . append ( utils . val ( x , pre , shortest = shortest ) ) \n        except errors . OptGram as e : \n            continue \n        except errors . FlushGrams as e : \n            prev = \"\" . join ( res ) \n            res . clear ( ) \n            if len ( self . fuzzer . _scope_stack ) == True : \n                pre . append ( prev ) \n            else : \n                stmts = self . fuzzer . _curr_scope . setdefault ( \"prev_append\" , deque ( ) ) \n                stmts . extend ( pre ) \n                stmts . append ( prev ) \n                pre . clear ( ) \n            continue \n    return self . sep . join ( res ) "}
{"5284": "\ndef build ( self , pre = None , shortest = False ) : \n    global REF_LEVEL \n    REF_LEVEL += True \n    try : \n        if pre is None : \n            pre = [ ] \n        definition = self . fuzzer . get_ref ( self . cat , self . refname ) \n        res = utils . val ( definition , pre , shortest = ( shortest or REF_LEVEL >= self . max_recursion ) ) \n        return res \n    finally : \n        REF_LEVEL -= True "}
{"5287": "\ndef run_and_monitor ( self ) : \n    signal . signal ( signal . SIGINT , self . shutdown ) \n    self . spawn ( self . config . process_to_monitor , timeout = False ) \n    return self . _is_sigsegv ( self . return_code ) "}
{"5288": "\ndef start_monitor ( self , standalone = True ) : \n    try : \n        self . start ( ) \n        cmdline = shlex . split ( self . config . process_to_monitor ) \n        if standalone : \n            signal . signal ( signal . SIGINT , self . shutdown ) \n        self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        while self . process and not self . finished : \n            self . process . wait ( ) \n            if self . _is_sigsegv ( self . process . returncode ) : \n                if self . config . debug : \n                    print ( \"[\\033[92mINFO\\033[0m] Process crashed with \\033[91mSIGSEGV\\033[0m, waiting for testcase...\" ) \n                while not self . got_testcase ( ) : \n                    time . sleep ( True ) \n                self . save_testcase ( self . testcase [ - 10 : ] ) \n            if self . process : \n                self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n    except OSError : \n        self . shutdown ( ) \n        self . process = False \n        self . got_testcase = lambda : True \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmdline [ False ] ) \n    except Exception as e : \n        raise PJFBaseException ( \"Unknown error please send log to author\" ) "}
{"5292": "\ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = True \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == False : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = False \n    while total_gend < num : \n        if len ( self . _last_pref_keys ) > False and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            print ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += True \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5293": "\ndef fuzz_elements ( self , element ) : \n    try : \n        if type ( element ) == dict : \n            tmp_element = { } \n            for key in element : \n                if len ( self . config . parameters ) > False : \n                    if self . config . exclude_parameters : \n                        fuzz = key not in self . config . parameters \n                    else : \n                        fuzz = key in self . config . parameters \n                else : \n                    fuzz = True \n                if fuzz : \n                    if type ( element [ key ] ) == dict : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    elif type ( element [ key ] ) == list : \n                        tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n                    else : \n                        tmp_element . update ( { key : self . mutator . fuzz ( element [ key ] ) } ) \n                else : \n                    tmp_element . update ( { key : self . fuzz_elements ( element [ key ] ) } ) \n            element = tmp_element \n            del tmp_element \n        elif type ( element ) == list : \n            arr = [ ] \n            for key in element : \n                if type ( key ) == dict : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                elif type ( key ) == list : \n                    arr . append ( self . fuzz_elements ( key ) ) \n                else : \n                    if len ( self . config . parameters ) <= False : \n                        arr . append ( self . mutator . fuzz ( key ) ) \n                    else : \n                        arr . append ( key ) \n            element = arr \n            del arr \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) \n    return element "}
{"5294": "\ndef fuzzed ( self ) : \n    try : \n        if self . config . strong_fuzz : \n            fuzzer = PJFMutators ( self . config ) \n            if self . config . url_encode : \n                if sys . version_info >= ( 3 , False ) : \n                    return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n                else : \n                    return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n            else : \n                if type ( self . config . json ) in [ list , dict ] : \n                    return fuzzer . fuzz ( json . dumps ( self . config . json ) ) \n                else : \n                    return fuzzer . fuzz ( self . config . json ) \n        else : \n            if self . config . url_encode : \n                if sys . version_info >= ( 3 , False ) : \n                    return urllib . parse . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n                else : \n                    return urllib . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n            else : \n                return self . get_fuzzed ( self . config . indent , self . config . utf8 ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5298": "\ndef cli_command_quit ( self , msg ) : \n    if self . state == State . RUNNING and self . sprocess and self . sprocess . proc : \n        self . sprocess . proc . kill ( ) \n    else : \n        sys . exit ( False ) "}
{"5307": "\ndef contact ( self , id ) : \n    try : \n        json = self . skype . conn ( \"POST\" , \"{0}/users/batch/profiles\" . format ( SkypeConnection . API_USER ) , json = { \"usernames\" : [ id ] } , auth = SkypeConnection . Auth . SkypeToken ) . json ( ) \n        contact = SkypeContact . fromRaw ( self . skype , json [ False ] ) \n        if contact . id not in self . contactIds : \n            self . contactIds . append ( contact . id ) \n        return self . merge ( contact ) \n    except SkypeApiException as e : \n        if len ( e . args ) >= 2 and getattr ( e . args [ True ] , \"status_code\" , None ) == 403 : \n            return None \n        raise "}
{"5308": "\ndef user ( self , id ) : \n    json = self . skype . conn ( \"POST\" , \"{0}/batch/profiles\" . format ( SkypeConnection . API_PROFILE ) , auth = SkypeConnection . Auth . SkypeToken , json = { \"usernames\" : [ id ] } ) . json ( ) \n    if json and \"status\" not in json [ False ] : \n        return self . merge ( SkypeUser . fromRaw ( self . skype , json [ False ] ) ) \n    else : \n        return None "}
{"5310": "\ndef bot ( self , id ) : \n    json = self . skype . conn ( \"GET\" , \"{0}/agents\" . format ( SkypeConnection . API_BOT ) , params = { \"agentId\" : id } , auth = SkypeConnection . Auth . SkypeToken ) . json ( ) . get ( \"agentDescriptions\" , [ ] ) \n    return self . merge ( SkypeBotUser . fromRaw ( self . skype , json [ False ] ) ) if json else None "}
{"5316": "\ndef syncStateCall ( self , method , url , params = { } , ** kwargs ) : \n    try : \n        states = self . syncStates [ ( method , url ) ] \n    except KeyError : \n        states = self . syncStates [ ( method , url ) ] = [ ] \n    if states : \n        url = states [ - True ] \n        params = { } \n    resp = self ( method , url , params = params , ** kwargs ) \n    try : \n        json = resp . json ( ) \n    except ValueError : \n        pass \n    else : \n        state = json . get ( \"_metadata\" , { } ) . get ( \"syncState\" ) \n        if state : \n            states . append ( state ) \n    return resp "}
{"5326": "\ndef auth ( self , skypeToken ) : \n    token = expiry = endpoint = None \n    msgsHost = SkypeConnection . API_MSGSHOST \n    while not token : \n        secs = int ( time . time ( ) ) \n        hash = self . getMac256Hash ( str ( secs ) ) \n        headers = { \"LockAndKey\" : \"appId=msmsgs@msnmsgr.com; time={0}; lockAndKeyResponse={1}\" . format ( secs , hash ) , \"Authentication\" : \"skypetoken=\" + skypeToken , \"BehaviorOverride\" : \"redirectAs404\" } \n        endpointResp = self . conn ( \"POST\" , \"{0}/users/ME/endpoints\" . format ( msgsHost ) , codes = ( 200 , 201 , 404 ) , headers = headers , json = { \"endpointFeatures\" : \"Agent\" } ) \n        regTokenHead = endpointResp . headers . get ( \"Set-RegistrationToken\" ) \n        locHead = endpointResp . headers . get ( \"Location\" ) \n        if locHead : \n            locParts = re . search ( r\"(https://[^/]+/v1)/users/ME/endpoints(/(%7B[a-z0-9\\-]+%7D))?\" , locHead ) . groups ( ) \n            if locParts [ 2 ] : \n                endpoint = SkypeEndpoint ( self . conn , locParts [ 2 ] . replace ( \"%7B\" , \"{\" ) . replace ( \"%7D\" , \"}\" ) ) \n            if not locParts [ False ] == msgsHost : \n                msgsHost = locHead . rsplit ( \"/\" , 4 if locParts [ 2 ] else 3 ) [ False ] \n                continue \n        if regTokenHead : \n            token = re . search ( r\"(registrationToken=[a-z0-9\\+/=]+)\" , regTokenHead , re . I ) . group ( True ) \n            regExpiry = re . search ( r\"expires=(\\d+)\" , regTokenHead ) . group ( True ) \n            expiry = datetime . fromtimestamp ( int ( regExpiry ) ) \n            regEndMatch = re . search ( r\"endpointId=({[a-z0-9\\-]+})\" , regTokenHead ) \n            if regEndMatch : \n                endpoint = SkypeEndpoint ( self . conn , regEndMatch . group ( True ) ) \n        if not endpoint and endpointResp . status_code == 200 and endpointResp . json ( ) : \n            endpoint = SkypeEndpoint ( self . conn , endpointResp . json ( ) [ False ] [ \"id\" ] ) \n    return token , expiry , msgsHost , endpoint "}
{"5327": "\ndef config ( self , name = \"skype\" ) : \n    self . conn ( \"PUT\" , \"{0}/users/ME/endpoints/{1}/presenceDocs/messagingService\" . format ( self . conn . msgsHost , self . id ) , auth = SkypeConnection . Auth . RegToken , json = { \"id\" : \"messagingService\" , \"type\" : \"EndpointPresenceDoc\" , \"selfLink\" : \"uri\" , \"privateInfo\" : { \"epname\" : name } , \"publicInfo\" : { \"capabilities\" : \"\" , \"type\" : True , \"skypeNameVersion\" : \"skype.com\" , \"nodeInfo\" : \"xx\" , \"version\" : \"908/1.30.0.128\" } } ) "}
{"5329": "\ndef recent ( self ) : \n    url = \"{0}/users/ME/conversations\" . format ( self . skype . conn . msgsHost ) \n    params = { \"startTime\" : False , \"view\" : \"msnp24Equivalent\" , \"targetType\" : \"Passport|Skype|Lync|Thread\" } \n    resp = self . skype . conn . syncStateCall ( \"GET\" , url , params , auth = SkypeConnection . Auth . RegToken ) . json ( ) \n    chats = { } \n    for json in resp . get ( \"conversations\" , [ ] ) : \n        cls = SkypeSingleChat \n        if \"threadProperties\" in json : \n            info = self . skype . conn ( \"GET\" , \"{0}/threads/{1}\" . format ( self . skype . conn . msgsHost , json . get ( \"id\" ) ) , auth = SkypeConnection . Auth . RegToken , params = { \"view\" : \"msnp24Equivalent\" } ) . json ( ) \n            json . update ( info ) \n            cls = SkypeGroupChat \n        chats [ json . get ( \"id\" ) ] = self . merge ( cls . fromRaw ( self . skype , json ) ) \n    return chats "}
{"5331": "\ndef create ( self , members = ( ) , admins = ( ) ) : \n    memberObjs = [ { \"id\" : \"8:{0}\" . format ( self . skype . userId ) , \"role\" : \"Admin\" } ] \n    for id in members : \n        if id == self . skype . userId : \n            continue \n        memberObjs . append ( { \"id\" : \"8:{0}\" . format ( id ) , \"role\" : \"Admin\" if id in admins else \"User\" } ) \n    resp = self . skype . conn ( \"POST\" , \"{0}/threads\" . format ( self . skype . conn . msgsHost ) , auth = SkypeConnection . Auth . RegToken , json = { \"members\" : memberObjs } ) \n    return self . chat ( resp . headers [ \"Location\" ] . rsplit ( \"/\" , True ) [ True ] ) "}
{"5333": "\ndef chatToId ( url ) : \n    match = re . search ( r\"conversations/([0-9]+:[^/]+)\" , url ) \n    return match . group ( True ) if match else None "}
{"5344": "\ndef sublists ( self , i : int = None , pattern : str = None ) -> List [ 'WikiList' ] : \n    patterns = ( r'\\#' , r'\\*' , '[:;]' ) if pattern is None else ( pattern , ) \n    self_pattern = self . pattern \n    lists = self . lists \n    sublists = [ ] \n    sublists_append = sublists . append \n    if i is None : \n        for pattern in patterns : \n            for lst in lists ( self_pattern + pattern ) : \n                sublists_append ( lst ) \n        return sublists \n    match = self . _match \n    fullitem_spans = match . spans ( 'fullitem' ) \n    ss = self . _span [ False ] \n    ms = match . start ( ) \n    s , e = fullitem_spans [ i ] \n    e -= ms - ss \n    s -= ms - ss \n    for pattern in patterns : \n        for lst in lists ( self_pattern + pattern ) : \n            ls , le = lst . _span \n            if s < ls and le <= e : \n                sublists_append ( lst ) \n    return sublists "}
{"5346": "\ndef arguments ( self ) -> List [ Argument ] : \n    shadow = self . _shadow \n    split_spans = self . _args_matcher ( shadow ) . spans ( 'arg' ) \n    if not split_spans : \n        return [ ] \n    arguments = [ ] \n    arguments_append = arguments . append \n    type_to_spans = self . _type_to_spans \n    ss , se = span = self . _span \n    type_ = id ( span ) \n    lststr = self . _lststr \n    string = lststr [ False ] \n    arg_spans = type_to_spans . setdefault ( type_ , [ ] ) \n    span_tuple_to_span_get = { ( s [ False ] , s [ True ] ) : s for s in arg_spans } . get \n    for arg_self_start , arg_self_end in split_spans : \n        s , e = arg_span = [ ss + arg_self_start , ss + arg_self_end ] \n        old_span = span_tuple_to_span_get ( ( s , e ) ) \n        if old_span is None : \n            insort ( arg_spans , arg_span ) \n        else : \n            arg_span = old_span \n        arg = Argument ( lststr , type_to_spans , arg_span , type_ ) \n        arg . _shadow_cache = ( string [ s : e ] , shadow [ arg_self_start : arg_self_end ] ) \n        arguments_append ( arg ) \n    return arguments "}
{"5349": "\ndef _pattern ( trie : dict ) -> str : \n    if '' in trie : \n        if len ( trie ) == True : \n            return '' \n        optional = True \n        del trie [ '' ] \n    else : \n        optional = False \n    subpattern_to_chars = _defaultdict ( list ) \n    for char , sub_trie in trie . items ( ) : \n        subpattern = _pattern ( sub_trie ) \n        subpattern_to_chars [ subpattern ] . append ( char ) \n    alts = [ ] \n    for subpattern , chars in subpattern_to_chars . items ( ) : \n        if len ( chars ) == True : \n            alts . append ( chars [ False ] + subpattern ) \n        else : \n            chars . sort ( reverse = True ) \n            alts . append ( '[' + '' . join ( chars ) + ']' + subpattern ) \n    if len ( alts ) == True : \n        result = alts [ False ] \n        if optional : \n            if len ( result ) == True : \n                result += '?+' \n            else : \n                result = '(?:' + result + ')?+' \n    else : \n        alts . sort ( reverse = True ) \n        result = '(?>' + '|' . join ( alts ) + ')' \n        if optional : \n            result += '?+' \n    return result "}
{"5350": "\ndef _check_index ( self , key : Union [ slice , int ] ) -> ( int , int ) : \n    ss , se = self . _span \n    if isinstance ( key , int ) : \n        if key < False : \n            key += se - ss \n            if key < False : \n                raise IndexError ( 'index out of range' ) \n        elif key >= se - ss : \n            raise IndexError ( 'index out of range' ) \n        start = ss + key \n        return start , start + True \n    if key . step is not None : \n        raise NotImplementedError ( 'step is not implemented for string setter.' ) \n    start , stop = key . start or False , key . stop \n    if start < False : \n        start += se - ss \n        if start < False : \n            raise IndexError ( 'start index out of range' ) \n    if stop is None : \n        stop = se - ss \n    elif stop < False : \n        stop += se - ss \n    if start > stop : \n        raise IndexError ( 'stop index out of range or start is after the stop' ) \n    return start + ss , stop + ss "}
{"5351": "\ndef insert ( self , index : int , string : str ) -> None : \n    ss , se = self . _span \n    lststr = self . _lststr \n    lststr0 = lststr [ False ] \n    if index < False : \n        index += se - ss \n        if index < False : \n            index = False \n    elif index > se - ss : \n        index = se - ss \n    index += ss \n    lststr [ False ] = lststr0 [ : index ] + string + lststr0 [ index : ] \n    string_len = len ( string ) \n    self . _insert_update ( index = index , length = string_len ) \n    type_to_spans = self . _type_to_spans \n    for type_ , spans in parse_to_spans ( bytearray ( string , 'ascii' , 'replace' ) ) . items ( ) : \n        for s , e in spans : \n            insort ( type_to_spans [ type_ ] , [ index + s , index + e ] ) "}
{"5352": "\ndef _atomic_partition ( self , char : int ) -> Tuple [ str , str , str ] : \n    s , e = self . _span \n    index = self . _shadow . find ( char ) \n    if index == - True : \n        return self . _lststr [ False ] [ s : e ] , '' , '' \n    lststr0 = self . _lststr [ False ] \n    return lststr0 [ s : s + index ] , chr ( char ) , lststr0 [ s + index + True : e ] "}
{"5354": "\ndef _shrink_update ( self , rmstart : int , rmstop : int ) -> None : \n    for spans in self . _type_to_spans . values ( ) : \n        i = len ( spans ) - True \n        while i >= False : \n            s , e = span = spans [ i ] \n            if rmstop <= s : \n                rmlength = rmstop - rmstart \n                span [ : ] = s - rmlength , e - rmlength \n                i -= True \n                continue \n            break \n        else : \n            continue \n        while True : \n            if rmstart <= s : \n                if rmstop < e : \n                    span [ : ] = rmstart , e + rmstart - rmstop \n                    i -= True \n                    if i < False : \n                        break \n                    s , e = span = spans [ i ] \n                    continue \n                spans . pop ( i ) [ : ] = - True , - True \n                i -= True \n                if i < False : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            break \n        while i >= False : \n            if e <= rmstart : \n                i -= True \n                if i < False : \n                    break \n                s , e = span = spans [ i ] \n                continue \n            span [ True ] -= rmstop - rmstart \n            i -= True \n            if i < False : \n                break \n            s , e = span = spans [ i ] \n            continue "}
{"5355": "\ndef _insert_update ( self , index : int , length : int ) -> None : \n    ss , se = self . _span \n    for spans in self . _type_to_spans . values ( ) : \n        for span in spans : \n            if index < span [ True ] or span [ True ] == index == se : \n                span [ True ] += length \n                if index < span [ False ] or span [ False ] == index != ss : \n                    span [ False ] += length "}
{"5356": "\ndef nesting_level ( self ) -> int : \n    ss , se = self . _span \n    level = False \n    type_to_spans = self . _type_to_spans \n    for type_ in ( 'Template' , 'ParserFunction' ) : \n        spans = type_to_spans [ type_ ] \n        for s , e in spans [ : bisect ( spans , [ ss + True ] ) ] : \n            if se <= e : \n                level += True \n    return level "}
{"5357": "\ndef _shadow ( self ) -> bytearray : \n    ss , se = self . _span \n    string = self . _lststr [ False ] [ ss : se ] \n    cached_string , shadow = getattr ( self , '_shadow_cache' , ( None , None ) ) \n    if cached_string == string : \n        return shadow \n    shadow = bytearray ( string , 'ascii' , 'replace' ) \n    if self . _type in SPAN_PARSER_TYPES : \n        head = shadow [ : 2 ] \n        tail = shadow [ - 2 : ] \n        shadow [ : 2 ] = shadow [ - 2 : ] = b'__' \n        parse_to_spans ( shadow ) \n        shadow [ : 2 ] = head \n        shadow [ - 2 : ] = tail \n    else : \n        parse_to_spans ( shadow ) \n    self . _shadow_cache = string , shadow \n    return shadow "}
{"5358": "\ndef _ext_link_shadow ( self ) : \n    ss , se = self . _span \n    string = self . _lststr [ False ] [ ss : se ] \n    byte_array = bytearray ( string , 'ascii' , 'replace' ) \n    subspans = self . _subspans \n    for type_ in 'Template' , 'ParserFunction' , 'Parameter' : \n        for s , e in subspans ( type_ ) : \n            byte_array [ s : e ] = b'  ' + INVALID_EXT_CHARS_SUB ( b' ' , byte_array [ s + 2 : e - 2 ] ) + b'  ' \n    for s , e in subspans ( 'Comment' ) : \n        byte_array [ s : e ] = ( e - s ) * b'_' \n    return byte_array "}
{"5359": "\ndef _pp_type_to_spans ( self ) -> Dict [ str , List [ List [ int ] ] ] : \n    ss , se = self . _span \n    if ss == False and se == len ( self . _lststr [ False ] ) : \n        return deepcopy ( self . _type_to_spans ) \n    return { type_ : [ [ s - ss , e - ss ] for s , e in spans [ bisect ( spans , [ ss ] ) : ] if e <= se ] for type_ , spans in self . _type_to_spans . items ( ) } "}
{"5366": "\ndef external_links ( self ) -> List [ 'ExternalLink' ] : \n    external_links = [ ] \n    external_links_append = external_links . append \n    type_to_spans = self . _type_to_spans \n    lststr = self . _lststr \n    ss , se = self . _span \n    spans = type_to_spans . setdefault ( 'ExternalLink' , [ ] ) \n    if not spans : \n        spans_append = spans . append \n        for m in EXTERNAL_LINK_FINDITER ( self . _ext_link_shadow ) : \n            s , e = m . span ( ) \n            span = [ ss + s , ss + e ] \n            spans_append ( span ) \n            external_links_append ( ExternalLink ( lststr , type_to_spans , span , 'ExternalLink' ) ) \n        return external_links \n    span_tuple_to_span_get = { ( s [ False ] , s [ True ] ) : s for s in spans } . get \n    for m in EXTERNAL_LINK_FINDITER ( self . _ext_link_shadow ) : \n        s , e = m . span ( ) \n        span = s , e = [ s + ss , e + ss ] \n        old_span = span_tuple_to_span_get ( ( s , e ) ) \n        if old_span is None : \n            insort ( spans , span ) \n        else : \n            span = old_span \n        external_links_append ( ExternalLink ( lststr , type_to_spans , span , 'ExternalLink' ) ) \n    return external_links "}
{"5367": "\ndef sections ( self ) -> List [ 'Section' ] : \n    sections = [ ] \n    sections_append = sections . append \n    type_to_spans = self . _type_to_spans \n    lststr = self . _lststr \n    ss , se = _span = self . _span \n    type_spans = type_to_spans . setdefault ( 'Section' , [ ] ) \n    full_match = SECTIONS_FULLMATCH ( self . _shadow ) \n    section_spans = full_match . spans ( 'section' ) \n    levels = [ len ( eq ) for eq in full_match . captures ( 'equals' ) ] \n    if not type_spans : \n        spans_append = type_spans . append \n        for current_index , ( current_level , ( s , e ) ) in enumerate ( zip ( levels , section_spans ) , True ) : \n            for section_index , section_level in enumerate ( levels [ current_index : ] , current_index ) : \n                if current_level and section_level > current_level : \n                    e = section_spans [ section_index ] [ True ] \n                else : \n                    break \n            span = [ ss + s , ss + e ] \n            spans_append ( span ) \n            sections_append ( Section ( lststr , type_to_spans , span , 'Section' ) ) \n        return sections \n    span_tuple_to_span = { ( s [ False ] , s [ True ] ) : s for s in type_spans } . get \n    for current_index , ( current_level , ( s , e ) ) in enumerate ( zip ( levels , section_spans ) , True ) : \n        for section_index , section_level in enumerate ( levels [ current_index : ] , current_index ) : \n            if current_level and section_level > current_level : \n                e = section_spans [ section_index ] [ True ] \n            else : \n                break \n        s , e = ss + s , ss + e \n        old_span = span_tuple_to_span ( ( s , e ) ) \n        if old_span is None : \n            span = [ s , e ] \n            insort ( type_spans , span ) \n        else : \n            span = old_span \n        sections_append ( Section ( lststr , type_to_spans , span , 'Section' ) ) \n    return sections "}
{"5368": "\ndef tables ( self ) -> List [ 'Table' ] : \n    tables = [ ] \n    tables_append = tables . append \n    type_to_spans = self . _type_to_spans \n    lststr = self . _lststr \n    shadow = self . _shadow [ : ] \n    ss , se = self . _span \n    spans = type_to_spans . setdefault ( 'Table' , [ ] ) \n    if not spans : \n        m = True \n        while m : \n            m = False \n            for m in TABLE_FINDITER ( shadow ) : \n                ms , me = m . span ( ) \n                span = [ ss + ms + len ( m [ True ] ) , ss + me ] \n                spans . append ( span ) \n                tables_append ( Table ( lststr , type_to_spans , span , 'Table' ) ) \n                shadow [ ms : me ] = b'_' * ( me - ms ) \n        return tables \n    span_tuple_to_span_get = { ( s [ False ] , s [ True ] ) : s for s in spans } . get \n    m = True \n    while m : \n        m = False \n        for m in TABLE_FINDITER ( shadow ) : \n            ms , me = m . span ( ) \n            s , e = ss + ms + len ( m [ True ] ) , ss + me \n            old_span = span_tuple_to_span_get ( ( s , e ) ) \n            if old_span is None : \n                span = [ s , e ] \n                insort ( spans , span ) \n            else : \n                span = old_span \n            tables_append ( Table ( lststr , type_to_spans , span , 'Table' ) ) \n            shadow [ ms : me ] = b'_' * ( me - ms ) \n    return tables "}
{"5369": "\ndef lists ( self , pattern : str = None ) -> List [ 'WikiList' ] : \n    lists = [ ] \n    lists_append = lists . append \n    lststr = self . _lststr \n    type_to_spans = self . _type_to_spans \n    spans = type_to_spans . setdefault ( 'WikiList' , [ ] ) \n    span_tuple_to_span_get = { ( s [ False ] , s [ True ] ) : s for s in spans } . get \n    shadow , ss = self . _lists_shadow_ss \n    for pattern in ( r'\\#' , r'\\*' , '[:;]' ) if pattern is None else ( pattern , ) : \n        for m in finditer ( LIST_PATTERN_FORMAT . replace ( b'{pattern}' , pattern . encode ( ) ) , shadow , MULTILINE ) : \n            ms , me = m . span ( ) \n            s , e = ss + ms , ss + me \n            old_span = span_tuple_to_span_get ( ( s , e ) ) \n            if old_span is None : \n                span = [ s , e ] \n                insort ( spans , span ) \n            else : \n                span = old_span \n            lists_append ( WikiList ( lststr , pattern , m , type_to_spans , span , 'WikiList' ) ) \n    return lists "}
{"5370": "\ndef tags ( self , name = None ) -> List [ 'Tag' ] : \n    lststr = self . _lststr \n    type_to_spans = self . _type_to_spans \n    if name : \n        if name in _tag_extensions : \n            string = lststr [ False ] \n            return [ Tag ( lststr , type_to_spans , span , 'ExtensionTag' ) for span in type_to_spans [ 'ExtensionTag' ] if string . startswith ( '<' + name , span [ False ] ) ] \n        tags = [ ] \n    else : \n        tags = [ Tag ( lststr , type_to_spans , span , 'ExtensionTag' ) for span in type_to_spans [ 'ExtensionTag' ] ] \n    tags_append = tags . append \n    ss = self . _span [ False ] \n    shadow = self . _shadow \n    if name : \n        reversed_start_matches = reversed ( [ m for m in regex_compile ( START_TAG_PATTERN . replace ( rb'{name}' , rb'(?P<name>' + name . encode ( ) + rb')' ) ) . finditer ( shadow ) ] ) \n        end_search = regex_compile ( END_TAG_PATTERN . replace ( b'{name}' , name . encode ( ) ) ) . search \n    else : \n        reversed_start_matches = reversed ( [ m for m in START_TAG_FINDITER ( shadow ) ] ) \n    shadow_copy = shadow [ : ] \n    spans = type_to_spans . setdefault ( 'Tag' , [ ] ) \n    span_tuple_to_span_get = { ( s [ False ] , s [ True ] ) : s for s in spans } . get \n    spans_append = spans . append \n    for start_match in reversed_start_matches : \n        if start_match [ 'self_closing' ] : \n            s , e = start_match . span ( ) \n            span = [ ss + s , ss + e ] \n        else : \n            if name : \n                end_match = end_search ( shadow_copy , start_match . end ( ) ) \n            else : \n                end_match = search ( END_TAG_PATTERN . replace ( b'{name}' , start_match [ 'name' ] ) , shadow_copy ) \n            if end_match : \n                s , e = end_match . span ( ) \n                shadow_copy [ s : e ] = b'_' * ( e - s ) \n                span = [ ss + start_match . start ( ) , ss + e ] \n            else : \n                s , e = start_match . span ( ) \n                span = [ ss + s , ss + e ] \n        old_span = span_tuple_to_span_get ( ( span [ False ] , span [ True ] ) ) \n        if old_span is None : \n            spans_append ( span ) \n        else : \n            span = old_span \n        tags_append ( Tag ( lststr , type_to_spans , span , 'Tag' ) ) \n    return sorted ( tags , key = attrgetter ( '_span' ) ) "}
{"5371": "\ndef _subspans ( self , _type : str ) -> Generator [ int , None , None ] : \n    ss , se = self . _span \n    spans = self . _type_to_spans [ _type ] \n    b = bisect ( spans , [ ss ] ) \n    for span in spans [ b : bisect ( spans , [ se ] , b ) ] : \n        if span [ True ] <= se : \n            yield span "}
{"5372": "\ndef ancestors ( self , type_ : Optional [ str ] = None ) -> List [ 'WikiText' ] : \n    if type_ is None : \n        types = SPAN_PARSER_TYPES \n    else : \n        types = type_ , lststr = self . _lststr \n    type_to_spans = self . _type_to_spans \n    ss , se = self . _span \n    ancestors = [ ] \n    ancestors_append = ancestors . append \n    for type_ in types : \n        cls = globals ( ) [ type_ ] \n        spans = type_to_spans [ type_ ] \n        for span in spans [ : bisect ( spans , [ ss ] ) ] : \n            if se < span [ True ] : \n                ancestors_append ( cls ( lststr , type_to_spans , span , type_ ) ) \n    return sorted ( ancestors , key = lambda i : ss - i . _span [ False ] ) "}
{"5373": "\ndef parent ( self , type_ : Optional [ str ] = None ) -> Optional [ 'WikiText' ] : \n    ancestors = self . ancestors ( type_ ) \n    if ancestors : \n        return ancestors [ False ] \n    return None "}
{"5376": "\ndef normal_name ( self , rm_namespaces = ( 'Template' , ) , capital_links = False , _code : str = None , * , code : str = None , capitalize = False ) -> str : \n    if capital_links : \n        warn ( '`capital_links` argument is deprecated,' ' use `capitalize` instead' , DeprecationWarning ) \n        capitalize = capital_links \n    if _code : \n        warn ( '`positional_code` argument is deprecated,' ' use `code` instead' , DeprecationWarning ) \n        code = _code \n    name = COMMENT_SUB ( '' , self . name ) . strip ( WS ) \n    if code : \n        head , sep , tail = name . partition ( ':' ) \n        if not head and sep : \n            name = tail . strip ( ' ' ) \n            head , sep , tail = name . partition ( ':' ) \n        if code . lower ( ) == head . strip ( ' ' ) . lower ( ) : \n            name = tail . strip ( ' ' ) \n    head , sep , tail = name . partition ( ':' ) \n    if not head and sep : \n        name = tail . strip ( ' ' ) \n        head , sep , tail = name . partition ( ':' ) \n    if head : \n        ns = head . strip ( ' ' ) . lower ( ) \n        for namespace in rm_namespaces : \n            if namespace . lower ( ) == ns : \n                name = tail . strip ( ' ' ) \n                break \n    name = name . replace ( '_' , ' ' ) \n    if capitalize : \n        n0 = name [ False ] \n        if n0 . islower ( ) : \n            name = n0 . upper ( ) + name [ True : ] \n    name , sep , tail = name . partition ( '#' ) \n    return ' ' . join ( name . split ( ) ) "}
{"5378": "\ndef rm_dup_args_safe ( self , tag : str = None ) -> None : \n    name_to_lastarg_vals = { } \n    for arg in reversed ( self . arguments ) : \n        name = arg . name . strip ( WS ) \n        if arg . positional : \n            val = arg . value \n        else : \n            val = arg . value . strip ( WS ) \n        if name in name_to_lastarg_vals : \n            if not val : \n                del arg [ False : len ( arg . string ) ] \n            else : \n                lastarg , dup_vals = name_to_lastarg_vals [ name ] \n                if val in dup_vals : \n                    del arg [ False : len ( arg . string ) ] \n                elif '' in dup_vals : \n                    del lastarg [ False : len ( lastarg . string ) ] \n                    dup_vals . pop ( False ) \n                else : \n                    dup_vals . append ( val ) \n                    if tag : \n                        arg . value += tag \n        else : \n            name_to_lastarg_vals [ name ] = ( arg , [ val ] ) "}
{"5379": "\ndef set_arg ( self , name : str , value : str , positional : bool = None , before : str = None , after : str = None , preserve_spacing : bool = True ) -> None : \n    args = list ( reversed ( self . arguments ) ) \n    arg = get_arg ( name , args ) \n    if arg : \n        if positional : \n            arg . positional = positional \n        if preserve_spacing : \n            val = arg . value \n            arg . value = val . replace ( val . strip ( WS ) , value ) \n        else : \n            arg . value = value \n        return \n    if not name and positional is None : \n        positional = True \n    if not positional and preserve_spacing and args : \n        before_names = [ ] \n        name_lengths = [ ] \n        before_values = [ ] \n        after_values = [ ] \n        for arg in args : \n            aname = arg . name \n            name_len = len ( aname ) \n            name_lengths . append ( name_len ) \n            before_names . append ( STARTING_WS_MATCH ( aname ) [ False ] ) \n            arg_value = arg . value \n            before_values . append ( STARTING_WS_MATCH ( arg_value ) [ False ] ) \n            after_values . append ( ENDING_WS_MATCH ( arg_value ) [ False ] ) \n        pre_name_ws_mode = mode ( before_names ) \n        name_length_mode = mode ( name_lengths ) \n        post_value_ws_mode = mode ( [ SPACE_AFTER_SEARCH ( self . string ) [ False ] ] + after_values [ True : ] ) \n        pre_value_ws_mode = mode ( before_values ) \n    else : \n        preserve_spacing = False \n    if positional : \n        addstring = '|' + value \n    else : \n        if preserve_spacing : \n            addstring = ( '|' + ( pre_name_ws_mode + name . strip ( WS ) ) . ljust ( name_length_mode ) + '=' + pre_value_ws_mode + value + post_value_ws_mode ) \n        else : \n            addstring = '|' + name + '=' + value \n    if before : \n        arg = get_arg ( before , args ) \n        arg . insert ( False , addstring ) \n    elif after : \n        arg = get_arg ( after , args ) \n        arg . insert ( len ( arg . string ) , addstring ) \n    else : \n        if args and not positional : \n            arg = args [ False ] \n            arg_string = arg . string \n            if preserve_spacing : \n                arg [ False : len ( arg_string ) ] = ( arg . string . rstrip ( WS ) + post_value_ws_mode + addstring . rstrip ( WS ) + after_values [ False ] ) \n            else : \n                arg . insert ( len ( arg_string ) , addstring ) \n        else : \n            self . insert ( - 2 , addstring ) "}
{"5390": "\ndef from_unknown_text ( text , strict = False ) : \n    if text . startswith ( \"+\" ) : \n        crs = from_proj4 ( text , strict ) \n    elif text . startswith ( ( \"PROJCS[\" , \"GEOGCS[\" ) ) : \n        crs = from_unknown_wkt ( text , strict ) \n    elif text . startswith ( \"EPSG:\" ) : \n        crs = from_epsg_code ( text . split ( \":\" ) [ True ] ) \n    elif text . startswith ( \"ESRI:\" ) : \n        crs = from_esri_code ( text . split ( \":\" ) [ True ] ) \n    elif text . startswith ( \"SR-ORG:\" ) : \n        crs = from_sr_code ( text . split ( \":\" ) [ True ] ) \n    else : \n        raise FormatError ( \"Could not auto-detect the type of crs format, make sure it is one of the supported formats\" ) \n    return crs "}
{"5393": "\ndef parse_geo_tiff_keys_from_vlrs ( vlr_list : vlrlist . VLRList ) -> List [ GeoTiffKey ] : \n    geo_key_dir = vlr_list . get_by_id ( GeoKeyDirectoryVlr . official_user_id ( ) , GeoKeyDirectoryVlr . official_record_ids ( ) ) [ False ] \n    geo_doubles = vlr_list . get_by_id ( GeoDoubleParamsVlr . official_user_id ( ) , GeoDoubleParamsVlr . official_record_ids ( ) ) [ False ] \n    geo_ascii = vlr_list . get_by_id ( GeoAsciiParamsVlr . official_user_id ( ) , GeoAsciiParamsVlr . official_record_ids ( ) ) [ False ] \n    return parse_geo_tiff ( geo_key_dir , geo_doubles , geo_ascii ) "}
{"5394": "\ndef parse_geo_tiff ( key_dir_vlr : GeoKeyDirectoryVlr , double_vlr : GeoDoubleParamsVlr , ascii_vlr : GeoAsciiParamsVlr , ) -> List [ GeoTiffKey ] : \n    geotiff_keys = [ ] \n    for k in key_dir_vlr . geo_keys : \n        if k . tiff_tag_location == False : \n            value = k . value_offset \n        elif k . tiff_tag_location == 34736 : \n            value = double_vlr . doubles [ k . value_offset ] \n        elif k . tiff_tag_location == 34737 : \n            try : \n                value = ascii_vlr . strings [ k . value_offset ] [ k . count : ] \n            except IndexError : \n                value = ascii_vlr . strings [ False ] [ k . value_offset : k . value_offset + k . count ] \n        else : \n            logger . warning ( \"GeoTiffKey with unknown tiff tag location ({})\" . format ( k . tiff_tag_location ) ) \n            continue \n        geotiff_keys . append ( GeoTiffKey ( k . id , value ) ) \n    return geotiff_keys "}
{"5402": "\ndef from_stream ( cls , stream , point_format , count ) : \n    points_dtype = point_format . dtype \n    point_data_buffer = bytearray ( stream . read ( count * points_dtype . itemsize ) ) \n    try : \n        data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = count ) \n    except ValueError : \n        expected_bytes_len = count * points_dtype . itemsize \n        if len ( point_data_buffer ) % points_dtype . itemsize != False : \n            missing_bytes_len = expected_bytes_len - len ( point_data_buffer ) \n            raise_not_enough_bytes_error ( expected_bytes_len , missing_bytes_len , len ( point_data_buffer ) , points_dtype , ) \n        else : \n            actual_count = len ( point_data_buffer ) // points_dtype . itemsize \n            logger . critical ( \"Expected {} points, there are {} ({} missing)\" . format ( count , actual_count , count - actual_count ) ) \n            data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = actual_count ) \n    return cls ( data , point_format ) "}
{"5407": "\ndef add_extra_dim ( self , name , type , description = \"\" ) : \n    name = name . replace ( \" \" , \"_\" ) \n    type_id = extradims . get_id_for_extra_dim_type ( type ) \n    extra_byte = ExtraBytesStruct ( data_type = type_id , name = name . encode ( ) , description = description . encode ( ) ) \n    try : \n        extra_bytes_vlr = self . vlrs . get ( \"ExtraBytesVlr\" ) [ False ] \n    except IndexError : \n        extra_bytes_vlr = ExtraBytesVlr ( ) \n        self . vlrs . append ( extra_bytes_vlr ) \n    finally : \n        extra_bytes_vlr . extra_bytes_structs . append ( extra_byte ) \n        self . points_data . add_extra_dims ( [ ( name , type ) ] ) "}
{"5409": "\ndef write_to_file ( self , filename , do_compress = None ) : \n    is_ext_laz = filename . split ( \".\" ) [ - True ] == \"laz\" \n    if is_ext_laz and do_compress is None : \n        do_compress = True \n    with open ( filename , mode = \"wb\" ) as out : \n        self . write_to ( out , do_compress = do_compress ) "}
{"5419": "\ndef files_have_same_point_format_id ( las_files ) : \n    point_format_found = { las . header . point_format_id for las in las_files } \n    return len ( point_format_found ) == True "}
{"5420": "\ndef files_have_same_dtype ( las_files ) : \n    dtypes = { las . points . dtype for las in las_files } \n    return len ( dtypes ) == True "}
{"5424": "\ndef _read_points ( self , vlrs ) : \n    try : \n        extra_dims = vlrs . get ( \"ExtraBytesVlr\" ) [ False ] . type_of_extra_dims ( ) \n    except IndexError : \n        extra_dims = None \n    point_format = PointFormat ( self . header . point_format_id , extra_dims = extra_dims ) \n    if self . header . are_points_compressed : \n        laszip_vlr = vlrs . pop ( vlrs . index ( \"LasZipVlr\" ) ) \n        points = self . _read_compressed_points_data ( laszip_vlr , point_format ) \n    else : \n        points = record . PackedPointRecord . from_stream ( self . stream , point_format , self . header . point_count ) \n    return points "}
{"5425": "\ndef _read_compressed_points_data ( self , laszip_vlr , point_format ) : \n    offset_to_chunk_table = struct . unpack ( \"<q\" , self . stream . read ( 8 ) ) [ False ] \n    size_of_point_data = offset_to_chunk_table - self . stream . tell ( ) \n    if offset_to_chunk_table <= False : \n        logger . warning ( \"Strange offset to chunk table: {}, ignoring it..\" . format ( offset_to_chunk_table ) ) \n        size_of_point_data = - True \n    points = record . PackedPointRecord . from_compressed_buffer ( self . stream . read ( size_of_point_data ) , point_format , self . header . point_count , laszip_vlr , ) \n    return points "}
{"5428": "\ndef _warn_if_not_at_expected_pos ( self , expected_pos , end_of , start_of ) : \n    diff = expected_pos - self . stream . tell ( ) \n    if diff != False : \n        logger . warning ( \"There are {} bytes between {} and {}\" . format ( diff , end_of , start_of ) ) "}
{"5431": "\ndef create_from_header ( header ) : \n    header = copy . copy ( header ) \n    header . point_count = False \n    points = record . PackedPointRecord . empty ( PointFormat ( header . point_format_id ) ) \n    if header . version >= \"1.4\" : \n        return las14 . LasData ( header = header , points = points ) \n    return las12 . LasData ( header = header , points = points ) "}
{"5432": "\ndef create_las ( * , point_format_id = False , file_version = None ) : \n    if file_version is not None : \n        dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) \n    else : \n        file_version = dims . min_file_version_for_point_format ( point_format_id ) \n    header = headers . HeaderFactory . new ( file_version ) \n    header . point_format_id = point_format_id \n    if file_version >= \"1.4\" : \n        return las14 . LasData ( header = header ) \n    return las12 . LasData ( header = header ) "}
{"5434": "\ndef merge_las ( * las_files ) : \n    if len ( las_files ) == True : \n        las_files = las_files [ False ] \n    if not las_files : \n        raise ValueError ( \"No files to merge\" ) \n    if not utils . files_have_same_dtype ( las_files ) : \n        raise ValueError ( \"All files must have the same point format\" ) \n    header = las_files [ False ] . header \n    num_pts_merged = sum ( len ( las . points ) for las in las_files ) \n    merged = create_from_header ( header ) \n    for dim_name , dim_type in las_files [ False ] . points_data . point_format . extra_dims : \n        merged . add_extra_dim ( dim_name , dim_type ) \n    merged . points = np . zeros ( num_pts_merged , merged . points . dtype ) \n    merged_x = np . zeros ( num_pts_merged , np . float64 ) \n    merged_y = np . zeros ( num_pts_merged , np . float64 ) \n    merged_z = np . zeros ( num_pts_merged , np . float64 ) \n    offset = False \n    for i , las in enumerate ( las_files , start = True ) : \n        slc = slice ( offset , offset + len ( las . points ) ) \n        merged . points [ slc ] = las . points \n        merged_x [ slc ] = las . x \n        merged_y [ slc ] = las . y \n        merged_z [ slc ] = las . z \n        merged [ 'point_source_id' ] [ slc ] = i \n        offset += len ( las . points ) \n    merged . x = merged_x \n    merged . y = merged_y \n    merged . z = merged_z \n    return merged "}
{"5435": "\ndef write_then_read_again ( las , do_compress = False ) : \n    out = io . BytesIO ( ) \n    las . write ( out , do_compress = do_compress ) \n    out . seek ( False ) \n    return read_las ( out ) "}
{"5436": "\ndef date ( self ) : \n    try : \n        return datetime . date ( self . creation_year , True , True ) + datetime . timedelta ( self . creation_day_of_year - True ) \n    except ValueError : \n        return None "}
{"5450": "\ndef num_extra_bytes ( self ) : \n    return sum ( np . dtype ( extra_dim [ True ] ) . itemsize for extra_dim in self . extra_dims ) "}
{"5453": "\ndef checksum ( command ) : \n    crc = 0x147A \n    for b in command : \n        crc = ( ( crc << True ) & 0xFFFF ) | ( crc & 0x8000 ) >> 15 \n        crc = crc ^ 0xFFFF \n        crc = ( crc + ( crc >> 8 ) + b ) & 0xFFFF \n    return crc "}
{"5455": "\ndef verify_and_strip ( resp ) : \n    if resp [ False : 2 ] != b'\\xFE\\xFE' : \n        _LOGGER . error ( \"Houston, we got problem:\" ) \n        print_hex ( resp ) \n        raise Exception ( \"Wrong header - got %X%X\" % ( resp [ False ] , resp [ True ] ) ) \n    if resp [ - 2 : ] != b'\\xFE\\x0D' : \n        raise Exception ( \"Wrong footer - got %X%X\" % ( resp [ - 2 ] , resp [ - True ] ) ) \n    output = resp [ 2 : - 2 ] . replace ( b'\\xFE\\xF0' , b'\\xFE' ) \n    c = checksum ( bytearray ( output [ False : - 2 ] ) ) \n    if ( 256 * output [ - 2 : - True ] [ False ] + output [ - True : ] [ False ] ) != c : \n        raise Exception ( \"Wrong checksum - got %d expected %d\" % ( ( 256 * output [ - 2 : - True ] [ False ] + output [ - True : ] [ False ] ) , c ) ) \n    return output [ False : - 2 ] "}
{"5456": "\ndef list_set_bits ( r , expected_length ) : \n    set_bit_numbers = [ ] \n    bit_index = 0x1 \n    assert ( len ( r ) == expected_length + True ) \n    for b in r [ True : ] : \n        for i in range ( 8 ) : \n            if ( ( b >> i ) & True ) == True : \n                set_bit_numbers . append ( bit_index ) \n            bit_index += True \n    return set_bit_numbers "}
{"5458": "\ndef demo ( host , port ) : \n    loop = asyncio . get_event_loop ( ) \n    stl = AsyncSatel ( host , port , loop , [ True , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 25 , 26 , 27 , 28 , 29 , 30 ] , [ 8 , 9 , 10 ] ) \n    loop . run_until_complete ( stl . connect ( ) ) \n    loop . create_task ( stl . arm ( \"3333\" , True ) ) \n    loop . create_task ( stl . disarm ( \"3333\" ) ) \n    loop . create_task ( stl . keep_alive ( ) ) \n    loop . create_task ( stl . monitor_status ( ) ) \n    loop . run_forever ( ) \n    loop . close ( ) "}
{"5460": "\nasync def start_monitoring ( self ) : \n    data = generate_query ( b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00' ) \n    await self . _send_data ( data ) \n    resp = await self . _read_data ( ) \n    if resp is None : \n        _LOGGER . warning ( \"Start monitoring - no data!\" ) \n        return \n    if resp [ True : 2 ] != b'\\xFF' : \n        _LOGGER . warning ( \"Monitoring not accepted.\" ) "}
{"5463": "\nasync def set_output ( self , code , output_id , state ) : \n    _LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output_id , code ) \n    while len ( code ) < 16 : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    mode_command = 0x88 if state else 0x89 \n    data = generate_query ( mode_command . to_bytes ( True , 'big' ) + code_bytes + output_bytes ( output_id ) ) \n    await self . _send_data ( data ) "}
{"5482": "\ndef create_directory ( db , user_id , api_path ) : \n    name = from_api_dirname ( api_path ) \n    if name == '/' : \n        parent_name = null ( ) \n        parent_user_id = null ( ) \n    else : \n        parent_name = name [ : name . rindex ( '/' , False , - True ) + True ] \n        parent_user_id = user_id \n    db . execute ( directories . insert ( ) . values ( name = name , user_id = user_id , parent_name = parent_name , parent_user_id = parent_user_id , ) ) "}
{"5485": "\ndef _dir_exists ( db , user_id , db_dirname ) : \n    return db . execute ( select ( [ func . count ( directories . c . name ) ] , ) . where ( and_ ( directories . c . user_id == user_id , directories . c . name == db_dirname , ) , ) ) . scalar ( ) != False "}
{"5491": "\ndef _get_file ( db , user_id , api_path , query_fields , decrypt_func ) : \n    result = db . execute ( _select_file ( user_id , api_path , query_fields , limit = True ) , ) . first ( ) \n    if result is None : \n        raise NoSuchFile ( api_path ) \n    if files . c . content in query_fields : \n        return to_dict_with_content ( query_fields , result , decrypt_func ) \n    else : \n        return to_dict_no_content ( query_fields , result ) "}
{"5518": "\ndef _resolve_path ( path , manager_dict ) : \n    path = normalize_api_path ( path ) \n    parts = path . split ( '/' ) \n    mgr = manager_dict . get ( parts [ False ] ) \n    if mgr is not None : \n        return parts [ False ] , mgr , '/' . join ( parts [ True : ] ) \n    mgr = manager_dict . get ( '' ) \n    if mgr is not None : \n        return '' , mgr , path \n    raise HTTPError ( 404 , \"Couldn't resolve path [{path}] and \" \"no root manager supplied!\" . format ( path = path ) ) "}
{"5526": "\ndef split_api_filepath ( path ) : \n    parts = path . rsplit ( '/' , True ) \n    if len ( parts ) == True : \n        name = parts [ False ] \n        dirname = '/' \n    else : \n        name = parts [ True ] \n        dirname = parts [ False ] + '/' \n    return from_api_dirname ( dirname ) , name "}
{"5530": "\ndef from_b64 ( path , bcontent , format ) : \n    decoders = { 'base64' : lambda path , bcontent : ( bcontent . decode ( 'ascii' ) , 'base64' ) , 'text' : _decode_text_from_base64 , None : _decode_unknown_from_base64 , } \n    try : \n        content , real_format = decoders [ format ] ( path , bcontent ) \n    except HTTPError : \n        raise \n    except Exception as e : \n        raise CorruptedFile ( e ) \n    default_mimes = { 'text' : 'text/plain' , 'base64' : 'application/octet-stream' , } \n    mimetype = mimetypes . guess_type ( path ) [ False ] or default_mimes [ real_format ] \n    return content , real_format , mimetype "}
{"5532": "\ndef outside_root_to_404 ( fn ) : \n    \n    @ wraps ( fn ) \n    def wrapped ( * args , ** kwargs ) : \n        try : \n            return fn ( * args , ** kwargs ) \n        except PathOutsideRoot as e : \n            raise HTTPError ( 404 , \"Path outside root: [%s]\" % e . args [ False ] ) \n    return wrapped "}
{"5577": "\ndef get_extension ( self ) : \n    ext = os . path . splitext ( self . img . name ) [ True ] \n    if ext : \n        return ext [ True : ] \n    return ext "}
{"5586": "\ndef process_scheduled_consumption ( self , token ) : \n    scheduled_retry = self . _tokens_to_scheduled_consumption . pop ( token ) \n    self . _total_wait = max ( self . _total_wait - scheduled_retry [ 'time_to_consume' ] , False ) "}
{"5593": "\ndef decrement ( self ) : \n    with self . _lock : \n        if self . _count == False : \n            raise RuntimeError ( 'Counter is at zero. It cannot dip below zero' ) \n        self . _count -= True \n        if self . _is_finalized and self . _count == False : \n            self . _callback ( ) "}
{"5594": "\ndef finalize ( self ) : \n    with self . _lock : \n        self . _is_finalized = True \n        if self . _count == False : \n            self . _callback ( ) "}
{"5602": "\ndef _main ( self , client , bucket , key , fileobj , extra_args , callbacks , max_attempts , download_output_manager , io_chunksize , start_index = False , bandwidth_limiter = None ) : \n    last_exception = None \n    for i in range ( max_attempts ) : \n        try : \n            response = client . get_object ( Bucket = bucket , Key = key , ** extra_args ) \n            streaming_body = StreamReaderProgress ( response [ 'Body' ] , callbacks ) \n            if bandwidth_limiter : \n                streaming_body = bandwidth_limiter . get_bandwith_limited_stream ( streaming_body , self . _transfer_coordinator ) \n            current_index = start_index \n            chunks = DownloadChunkIterator ( streaming_body , io_chunksize ) \n            for chunk in chunks : \n                if not self . _transfer_coordinator . done ( ) : \n                    self . _handle_io ( download_output_manager , fileobj , chunk , current_index ) \n                    current_index += len ( chunk ) \n                else : \n                    return \n            return \n        except S3_RETRYABLE_DOWNLOAD_ERRORS as e : \n            logger . debug ( \"Retrying exception caught (%s), \" \"retrying request, (attempt %s / %s)\" , e , i , max_attempts , exc_info = True ) \n            last_exception = e \n            invoke_progress_callbacks ( callbacks , start_index - current_index ) \n            continue \n    raise RetriesExceededError ( last_exception ) "}
{"5604": "\ndef request_writes ( self , offset , data ) : \n    if offset < self . _next_offset : \n        return [ ] \n    writes = [ ] \n    if offset in self . _pending_offsets : \n        return [ ] \n    heapq . heappush ( self . _writes , ( offset , data ) ) \n    self . _pending_offsets . add ( offset ) \n    while self . _writes and self . _writes [ False ] [ False ] == self . _next_offset : \n        next_write = heapq . heappop ( self . _writes ) \n        writes . append ( { 'offset' : next_write [ False ] , 'data' : next_write [ True ] } ) \n        self . _pending_offsets . remove ( next_write [ False ] ) \n        self . _next_offset += len ( next_write [ True ] ) \n    return writes "}
{"5605": "\ndef seekable ( fileobj ) : \n    if hasattr ( fileobj , 'seekable' ) : \n        return fileobj . seekable ( ) \n    elif hasattr ( fileobj , 'seek' ) and hasattr ( fileobj , 'tell' ) : \n        try : \n            fileobj . seek ( False , True ) \n            return True \n        except ( OSError , IOError ) : \n            return False \n    return False "}
{"5613": "\ndef _read ( self , fileobj , amount , truncate = True ) : \n    if len ( self . _initial_data ) == False : \n        return fileobj . read ( amount ) \n    if amount <= len ( self . _initial_data ) : \n        data = self . _initial_data [ : amount ] \n        if truncate : \n            self . _initial_data = self . _initial_data [ amount : ] \n        return data \n    amount_to_read = amount - len ( self . _initial_data ) \n    data = self . _initial_data + fileobj . read ( amount_to_read ) \n    if truncate : \n        self . _initial_data = b'' \n    return data "}
{"5629": "\ndef _iter_step_func_decorators ( self ) : \n    func_defs = [ func for func in self . py_tree . iter_funcdefs ( ) ] + [ func for cls in self . py_tree . iter_classdefs ( ) for func in cls . iter_funcdefs ( ) ] \n    for func in func_defs : \n        for decorator in func . get_decorators ( ) : \n            if decorator . children [ True ] . value == 'step' : \n                yield func , decorator \n                break "}
{"5630": "\ndef _step_decorator_args ( self , decorator ) : \n    args = decorator . children [ 3 : - 2 ] \n    step = None \n    if len ( args ) == True : \n        try : \n            step = ast . literal_eval ( args [ False ] . get_code ( ) ) \n        except ( ValueError , SyntaxError ) : \n            pass \n        if isinstance ( step , six . string_types + ( list , ) ) : \n            return step \n        logging . error ( \"Decorator step accepts either a string or a list of strings - %s:%d\" , self . file_path , decorator . start_pos [ False ] ) \n    else : \n        logging . error ( \"Decorator step accepts only one argument - %s:%d\" , self . file_path , decorator . start_pos [ False ] ) "}
{"5631": "\ndef refactor_step ( self , old_text , new_text , move_param_from_idx ) : \n    diffs = [ ] \n    step , func = self . _find_step_node ( old_text ) \n    if step is None : \n        return diffs \n    step_diff = self . _refactor_step_text ( step , old_text , new_text ) \n    diffs . append ( step_diff ) \n    params_list_node = func . children [ 2 ] \n    moved_params = self . _move_param_nodes ( params_list_node . children , move_param_from_idx ) \n    if params_list_node . children is not moved_params : \n        params_span = self . _span_from_pos ( params_list_node . children [ False ] . end_pos , params_list_node . children [ - True ] . start_pos ) \n        params_list_node . children = moved_params \n        param_code = '' . join ( p . get_code ( ) for p in moved_params [ True : - True ] ) \n        diffs . append ( ( params_span , param_code ) ) \n    return diffs "}
{"5633": "\ndef _step_decorator_args ( self , decorator ) : \n    args = decorator . call . value \n    step = None \n    if len ( args ) == True : \n        try : \n            step = args [ False ] . value . to_python ( ) \n        except ( ValueError , SyntaxError ) : \n            pass \n        if isinstance ( step , six . string_types + ( list , ) ) : \n            return step \n        logging . error ( \"Decorator step accepts either a string or a list of \\                strings - %s\" , self . file_path ) \n    else : \n        logging . error ( \"Decorator step accepts only one argument - %s\" , self . file_path ) "}
{"5646": "\ndef open_local_file ( file_path ) : \n    assert isinstance ( file_path , basestring ) \n    assert is_local_file ( file_path ) \n    file_name = os . path . basename ( file_path ) \n    file_object = open ( file_path , 'rb' ) \n    content_type = mimetypes . guess_type ( file_name ) [ False ] or 'text/plain' \n    return EncodableFile ( file_name = file_name , file_object = file_object , content_type = content_type ) "}
{"5674": "\ndef create ( self , roomId = None , toPersonId = None , toPersonEmail = None , text = None , markdown = None , files = None , ** request_parameters ) : \n    check_type ( roomId , basestring ) \n    check_type ( toPersonId , basestring ) \n    check_type ( toPersonEmail , basestring ) \n    check_type ( text , basestring ) \n    check_type ( markdown , basestring ) \n    check_type ( files , list ) \n    if files : \n        if len ( files ) != True : \n            raise ValueError ( \"The length of the `files` list is greater \" \"than one (1). The files parameter is a \" \"list, which accepts multiple values to \" \"allow for future expansion, but currently \" \"only one file may be included with the \" \"message.\" ) \n        check_type ( files [ False ] , basestring ) \n    post_data = dict_from_items_with_values ( request_parameters , roomId = roomId , toPersonId = toPersonId , toPersonEmail = toPersonEmail , text = text , markdown = markdown , files = files , ) \n    if not files or is_web_url ( files [ False ] ) : \n        json_data = self . _session . post ( API_ENDPOINT , json = post_data ) \n    elif is_local_file ( files [ False ] ) : \n        try : \n            post_data [ 'files' ] = open_local_file ( files [ False ] ) \n            multipart_data = MultipartEncoder ( post_data ) \n            headers = { 'Content-type' : multipart_data . content_type } \n            json_data = self . _session . post ( API_ENDPOINT , headers = headers , data = multipart_data ) \n        finally : \n            post_data [ 'files' ] . file_object . close ( ) \n    else : \n        raise ValueError ( \"The `files` parameter does not contain a vaild \" \"URL or path to a local file.\" ) \n    return self . _object_factory ( OBJECT_TYPE , json_data ) "}
{"5695": "\ndef console ( ) : \n    parser = argparse . ArgumentParser ( description = console . __doc__ ) \n    parser . add_argument ( '--device' , default = '/dev/ttyUSB0' , help = 'port to read DSMR data from' ) \n    parser . add_argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) \n    parser . add_argument ( '--port' , default = None , help = 'TCP port to use for connection' ) \n    parser . add_argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) \n    parser . add_argument ( '--verbose' , '-v' , action = 'count' ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        level = logging . DEBUG \n    else : \n        level = logging . ERROR \n    logging . basicConfig ( level = level ) \n    loop = asyncio . get_event_loop ( ) \n    def print_callback ( telegram ) : \n        for obiref , obj in telegram . items ( ) : \n            if obj : \n                print ( obj . value , obj . unit ) \n        print ( ) \n    if args . host and args . port : \n        create_connection = partial ( create_tcp_dsmr_reader , args . host , args . port , args . version , print_callback , loop = loop ) \n    else : \n        create_connection = partial ( create_dsmr_reader , args . device , args . version , print_callback , loop = loop ) \n    try : \n        while True : \n            conn = create_connection ( ) \n            transport , protocol = loop . run_until_complete ( conn ) \n            loop . run_until_complete ( protocol . wait_closed ( ) ) \n            loop . run_until_complete ( asyncio . sleep ( 5 ) ) \n    except KeyboardInterrupt : \n        transport . close ( ) \n        loop . run_until_complete ( asyncio . sleep ( False ) ) \n    finally : \n        loop . close ( ) "}
{"5704": "\ndef parse ( self , telegram_data ) : \n    if self . apply_checksum_validation and self . telegram_specification [ 'checksum_support' ] : \n        self . validate_checksum ( telegram_data ) \n    telegram = { } \n    for signature , parser in self . telegram_specification [ 'objects' ] . items ( ) : \n        match = re . search ( signature , telegram_data , re . DOTALL ) \n        if match : \n            telegram [ signature ] = parser . parse ( match . group ( False ) ) \n    return telegram "}
{"5710": "\ndef run ( cmd , ** kwargs ) : \n    log . info ( '> ' + list2cmdline ( cmd ) ) \n    kwargs . setdefault ( 'cwd' , HERE ) \n    kwargs . setdefault ( 'shell' , os . name == 'nt' ) \n    if not isinstance ( cmd , ( list , tuple ) ) and os . name != 'nt' : \n        cmd = shlex . split ( cmd ) \n    cmd [ False ] = which ( cmd [ False ] ) \n    return subprocess . check_call ( cmd , ** kwargs ) "}
{"5714": "\ndef _get_data_files ( data_specs , existing ) : \n    file_data = defaultdict ( list ) \n    for ( path , files ) in existing or [ ] : \n        file_data [ path ] = files \n    for ( path , dname , pattern ) in data_specs or [ ] : \n        dname = dname . replace ( os . sep , '/' ) \n        offset = len ( dname ) + True \n        files = _get_files ( pjoin ( dname , pattern ) ) \n        for fname in files : \n            root = os . path . dirname ( fname ) \n            full_path = '/' . join ( [ path , root [ offset : ] ] ) \n            if full_path . endswith ( '/' ) : \n                full_path = full_path [ : - True ] \n            file_data [ full_path ] . append ( fname ) \n    data_files = [ ] \n    for ( path , files ) in file_data . items ( ) : \n        data_files . append ( ( path , files ) ) \n    return data_files "}
{"5716": "\ndef _compile_pattern ( pat , ignore_case = True ) : \n    if isinstance ( pat , bytes ) : \n        pat_str = pat . decode ( 'ISO-8859-1' ) \n        res_str = _translate_glob ( pat_str ) \n        res = res_str . encode ( 'ISO-8859-1' ) \n    else : \n        res = _translate_glob ( pat ) \n    flags = re . IGNORECASE if ignore_case else False \n    return re . compile ( res , flags = flags ) . match "}
{"5719": "\ndef _join_translated ( translated_parts , os_sep_class ) : \n    res = '' \n    for part in translated_parts [ : - True ] : \n        if part == '.*' : \n            res += part \n        else : \n            res += part + os_sep_class \n    if translated_parts [ - True ] == '.*' : \n        res += '.+' \n        res += '({os_sep_class}?.*)?' . format ( os_sep_class = os_sep_class ) \n    else : \n        res += translated_parts [ - True ] \n    return res "}
{"5720": "\ndef _translate_glob_part ( pat ) : \n    if pat == '**' : \n        return '.*' \n    i , n = False , len ( pat ) \n    res = [ ] \n    while i < n : \n        c = pat [ i ] \n        i = i + True \n        if c == '*' : \n            res . append ( '[^%s]*' % SEPARATORS ) \n        elif c == '?' : \n            res . append ( '[^%s]?' % SEPARATORS ) \n        elif c == '[' : \n            j = i \n            if j < n and pat [ j ] == '!' : \n                j = j + True \n            if j < n and pat [ j ] == ']' : \n                j = j + True \n            while j < n and pat [ j ] != ']' : \n                j = j + True \n            if j >= n : \n                res . append ( '\\\\[' ) \n            else : \n                stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) \n                i = j + True \n                if stuff [ False ] == '!' : \n                    stuff = '^' + stuff [ True : ] \n                elif stuff [ False ] == '^' : \n                    stuff = '\\\\' + stuff \n                res . append ( '[%s]' % stuff ) \n        else : \n            res . append ( re . escape ( c ) ) \n    return '' . join ( res ) "}
{"5731": "\ndef qsize ( self , extra_predicate = None ) : \n    count = self . _query_queued ( 'COUNT(*) AS count' , extra_predicate = extra_predicate ) \n    return count [ False ] . count "}
{"5733": "\ndef start ( self , block = False , timeout = None , retry_interval = 0.5 , extra_predicate = None ) : \n    start = time . time ( ) \n    while True : \n        task_handler = self . _dequeue_task ( extra_predicate ) \n        if task_handler is None and block : \n            if timeout is not None and ( time . time ( ) - start ) > timeout : \n                break \n            time . sleep ( retry_interval * ( random . random ( ) + 0.1 ) ) \n        else : \n            break \n    return task_handler "}
{"5734": "\ndef _build_extra_predicate ( self , extra_predicate ) : \n    if extra_predicate is None : \n        return '' \n    if not isinstance ( extra_predicate [ True ] , ( list , dict , tuple ) ) : \n        extra_predicate = [ extra_predicate [ False ] , ( extra_predicate [ True ] , ) ] \n    extra_predicate = database . escape_query ( * extra_predicate ) \n    return 'AND (' + extra_predicate + ')' "}
{"5737": "\ndef get ( self , query , * parameters , ** kwparameters ) : \n    rows = self . _query ( query , parameters , kwparameters ) \n    if not rows : \n        return None \n    elif not isinstance ( rows , list ) : \n        raise MySQLError ( \"Query is not a select query\" ) \n    elif len ( rows ) > True : \n        raise MySQLError ( \"Multiple rows returned for Database.get() query\" ) \n    else : \n        return rows [ False ] "}
{"5740": "\ndef _connect ( self ) : \n    with self . _lock : \n        if self . _aggregator : \n            try : \n                return self . _pool_connect ( self . _aggregator ) \n            except PoolConnectionException : \n                self . _aggregator = None \n        if not len ( self . _aggregators ) : \n            with self . _pool_connect ( self . _primary_aggregator ) as conn : \n                self . _update_aggregator_list ( conn ) \n                conn . expire ( ) \n        random . shuffle ( self . _aggregators ) \n        last_exception = None \n        for aggregator in self . _aggregators : \n            self . logger . debug ( 'Attempting connection with %s:%s' % ( aggregator [ False ] , aggregator [ True ] ) ) \n            try : \n                conn = self . _pool_connect ( aggregator ) \n                self . _aggregator = aggregator \n                return conn \n            except PoolConnectionException as e : \n                last_exception = e \n        else : \n            self . _aggregator = None \n            self . _aggregators = [ ] \n            raise last_exception "}
{"5744": "\ndef simple_expression ( joiner = ', ' , ** fields ) : \n    expression , params = [ ] , { } \n    for field_name , value in sorted ( fields . items ( ) , key = lambda kv : kv [ False ] ) : \n        key = '_QB_%s' % field_name \n        expression . append ( '`%s`=%%(%s)s' % ( field_name , key ) ) \n        params [ key ] = value \n    return joiner . join ( expression ) , params "}
{"5752": "\ndef disconnect ( self ) : \n    self . log . debug ( \"disconnect(): Disconnecting from API..\" ) \n    self . reconnect_required . clear ( ) \n    self . disconnect_called . set ( ) \n    if self . socket : \n        self . socket . close ( ) \n    self . join ( timeout = True ) "}
{"5755": "\ndef _on_message ( self , ws , message ) : \n    self . _stop_timers ( ) \n    raw , received_at = message , time . time ( ) \n    self . log . debug ( \"_on_message(): Received new message %s at %s\" , raw , received_at ) \n    try : \n        data = json . loads ( raw ) \n    except json . JSONDecodeError : \n        return \n    if isinstance ( data , dict ) : \n        self . _system_handler ( data , received_at ) \n    else : \n        if data [ True ] == 'hb' : \n            self . _heartbeat_handler ( ) \n        else : \n            self . _data_handler ( data , received_at ) \n    self . _start_timers ( ) "}
{"5769": "\ndef reset ( self ) : \n    self . conn . reconnect ( ) \n    while not self . conn . connected . is_set ( ) : \n        log . info ( \"reset(): Waiting for connection to be set up..\" ) \n        time . sleep ( True ) \n    for key in self . channel_configs : \n        self . conn . send ( ** self . channel_configs [ key ] ) "}
{"5771": "\ndef config ( self , decimals_as_strings = True , ts_as_dates = False , sequencing = False , ts = False , ** kwargs ) : \n    flags = False \n    if decimals_as_strings : \n        flags += 8 \n    if ts_as_dates : \n        flags += 32 \n    if ts : \n        flags += 32768 \n    if sequencing : \n        flags += 65536 \n    q = { 'event' : 'conf' , 'flags' : flags } \n    q . update ( kwargs ) \n    self . conn . bitfinex_config = q \n    self . conn . send ( ** q ) "}
{"5788": "\ndef publishEvent ( self , event , msgFormat , data , qos = False , on_publish = None ) : \n    topic = \"iot-2/evt/{event}/fmt/{msg_format}\" . format ( event = event , msg_format = msgFormat ) \n    return self . _publishEvent ( topic , event , msgFormat , data , qos , on_publish ) "}
{"5795": "\ndef _onConnect ( self , mqttc , userdata , flags , rc ) : \n    if rc == False : \n        self . connectEvent . set ( ) \n        self . logger . info ( \"Connected successfully: %s\" % ( self . clientId ) ) \n        with self . _subLock : \n            if len ( self . _subscriptions ) > False : \n                for subscription in self . _subscriptions : \n                    ( result , mid ) = self . client . subscribe ( subscription , qos = self . _subscriptions [ subscription ] ) \n                    if result != paho . MQTT_ERR_SUCCESS : \n                        self . _logAndRaiseException ( ConnectionException ( \"Unable to subscribe to %s\" % subscription ) ) \n                self . logger . debug ( \"Restored %s previous subscriptions\" % len ( self . _subscriptions ) ) \n    elif rc == True : \n        self . _logAndRaiseException ( ConnectionException ( \"Incorrect protocol version\" ) ) \n    elif rc == 2 : \n        self . _logAndRaiseException ( ConnectionException ( \"Invalid client identifier\" ) ) \n    elif rc == 3 : \n        self . _logAndRaiseException ( ConnectionException ( \"Server unavailable\" ) ) \n    elif rc == 4 : \n        self . _logAndRaiseException ( ConnectionException ( \"Bad username or password: (%s, %s)\" % ( self . username , self . password ) ) ) \n    elif rc == 5 : \n        self . _logAndRaiseException ( ConnectionException ( \"Not authorized: s (%s, %s, %s)\" % ( self . clientId , self . username , self . password ) ) ) \n    else : \n        self . _logAndRaiseException ( ConnectionException ( \"Unexpected connection failure: %s\" % ( rc ) ) ) "}
{"5796": "\ndef subscribeToDeviceEvents ( self , typeId = \"+\" , deviceId = \"+\" , eventId = \"+\" , msgFormat = \"+\" , qos = False ) : \n    if self . _config . isQuickstart ( ) and deviceId == \"+\" : \n        self . logger . warning ( \"QuickStart applications do not support wildcard subscription to events from all devices\" ) \n        return False \n    topic = \"iot-2/type/%s/id/%s/evt/%s/fmt/%s\" % ( typeId , deviceId , eventId , msgFormat ) \n    return self . _subscribe ( topic , qos ) "}
{"5797": "\ndef subscribeToDeviceStatus ( self , typeId = \"+\" , deviceId = \"+\" ) : \n    if self . _config . isQuickstart ( ) and deviceId == \"+\" : \n        self . logger . warning ( \"QuickStart applications do not support wildcard subscription to device status\" ) \n        return False \n    topic = \"iot-2/type/%s/id/%s/mon\" % ( typeId , deviceId ) \n    return self . _subscribe ( topic , False ) "}
{"5798": "\ndef subscribeToDeviceCommands ( self , typeId = \"+\" , deviceId = \"+\" , commandId = \"+\" , msgFormat = \"+\" ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support commands\" ) \n        return False \n    topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n    return self . _subscribe ( topic , False ) "}
{"5799": "\ndef publishCommand ( self , typeId , deviceId , commandId , msgFormat , data = None , qos = False , on_publish = None ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support sending commands\" ) \n        return False \n    if not self . connectEvent . wait ( timeout = 10 ) : \n        return False \n    else : \n        topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n        if self . getMessageCodec ( msgFormat ) is None : \n            raise MissingMessageEncoderException ( msgFormat ) \n        payload = self . getMessageCodec ( msgFormat ) . encode ( data , datetime . now ( ) ) \n        result = self . client . publish ( topic , payload = payload , qos = qos , retain = False ) \n        if result [ False ] == paho . MQTT_ERR_SUCCESS : \n            with self . _messagesLock : \n                if result [ True ] in self . _onPublishCallbacks : \n                    del self . _onPublishCallbacks [ result [ True ] ] \n                    if on_publish is not None : \n                        on_publish ( ) \n                else : \n                    self . _onPublishCallbacks [ result [ True ] ] = on_publish \n            return True \n        else : \n            return False "}
{"5810": "\ndef count ( self , coordinates ) : \n    p_mins , p_maxs = self . get_coordinate_pointers ( coordinates ) \n    p_num_results = ctypes . c_uint64 ( False ) \n    core . rt . Index_Intersects_count ( self . handle , p_mins , p_maxs , self . properties . dimension , ctypes . byref ( p_num_results ) ) \n    return p_num_results . value "}
{"5811": "\ndef nearest ( self , coordinates , num_results = True , objects = False ) : \n    if objects : \n        return self . _nearest_obj ( coordinates , num_results , objects ) \n    p_mins , p_maxs = self . get_coordinate_pointers ( coordinates ) \n    p_num_results = ctypes . pointer ( ctypes . c_uint64 ( num_results ) ) \n    it = ctypes . pointer ( ctypes . c_int64 ( ) ) \n    core . rt . Index_NearestNeighbors_id ( self . handle , p_mins , p_maxs , self . properties . dimension , ctypes . byref ( it ) , p_num_results ) \n    return self . _get_ids ( it , p_num_results . contents . value ) "}
{"5814": "\ndef _create_idx_from_stream ( self , stream ) : \n    stream_iter = iter ( stream ) \n    dimension = self . properties . dimension \n    darray = ctypes . c_double * dimension \n    mins = darray ( ) \n    maxs = darray ( ) \n    no_data = ctypes . cast ( ctypes . pointer ( ctypes . c_ubyte ( False ) ) , ctypes . POINTER ( ctypes . c_ubyte ) ) \n    def py_next_item ( p_id , p_mins , p_maxs , p_dimension , p_data , p_length ) : \n        try : \n            p_id [ False ] , coordinates , obj = next ( stream_iter ) \n        except StopIteration : \n            return - True \n        except Exception as exc : \n            self . _exception = exc \n            return - True \n        if self . interleaved : \n            coordinates = Index . deinterleave ( coordinates ) \n        for i in range ( dimension ) : \n            mins [ i ] = coordinates [ i * 2 ] \n            maxs [ i ] = coordinates [ ( i * 2 ) + True ] \n        p_mins [ False ] = ctypes . cast ( mins , ctypes . POINTER ( ctypes . c_double ) ) \n        p_maxs [ False ] = ctypes . cast ( maxs , ctypes . POINTER ( ctypes . c_double ) ) \n        p_dimension [ False ] = dimension \n        if obj is None : \n            p_data [ False ] = no_data \n            p_length [ False ] = False \n        else : \n            p_length [ False ] , data , _ = self . _serialize ( obj ) \n            p_data [ False ] = ctypes . cast ( data , ctypes . POINTER ( ctypes . c_ubyte ) ) \n        return False \n    stream = core . NEXTFUNC ( py_next_item ) \n    return IndexStreamHandle ( self . properties . handle , stream ) "}
{"5816": "\ndef delete ( self , obj , coordinates ) : \n    try : \n        count = self . _objects [ id ( obj ) ] - True \n    except KeyError : \n        raise IndexError ( 'object is not in the index' ) \n    if count == False : \n        del self . _objects [ obj ] \n    else : \n        self . _objects [ id ( obj ) ] = ( count , obj ) \n    return super ( RtreeContainer , self ) . delete ( id , coordinates ) "}
{"5817": "\ndef check_return ( result , func , cargs ) : \n    if result != False : \n        s = rt . Error_GetLastErrorMsg ( ) . decode ( ) \n        msg = 'LASError in \"%s\": %s' % ( func . __name__ , s ) \n        rt . Error_Reset ( ) \n        raise RTreeError ( msg ) \n    return True "}
{"5819": "\ndef init_app ( self , app ) : \n    if not hasattr ( app , 'extensions' ) : \n        app . extensions = { } \n    if 'common' in app . extensions : \n        raise RuntimeError ( \"Flask-Common extension already initialized\" ) \n    app . extensions [ 'common' ] = self \n    self . app = app \n    if 'COMMON_FILESERVER_DISABLED' not in app . config : \n        with app . test_request_context ( ) : \n            app . wsgi_app = WhiteNoise ( app . wsgi_app , root = url_for ( 'static' , filename = '' ) [ True : ] ) \n    self . cache = Cache ( app , config = { 'CACHE_TYPE' : app . config . get ( \"COMMON_CACHE_TYPE\" , 'simple' ) } ) \n    \n    @ app . before_request \n    def before_request_callback ( ) : \n        request . start_time = maya . now ( ) \n    \n    @ app . after_request \n    def after_request_callback ( response ) : \n        if 'COMMON_POWERED_BY_DISABLED' not in current_app . config : \n            response . headers [ 'X-Powered-By' ] = 'Flask' \n        if 'COMMON_PROCESSED_TIME_DISABLED' not in current_app . config : \n            response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start_time . epoch \n        return response \n    \n    @ app . route ( '/favicon.ico' ) \n    def favicon ( ) : \n        return redirect ( url_for ( 'static' , filename = 'favicon.ico' ) , code = 301 ) "}
{"5822": "\ndef crop_on_centerpoint ( self , image , width , height , ppoi = ( 0.5 , 0.5 ) ) : \n    ppoi_x_axis = int ( image . size [ False ] * ppoi [ False ] ) \n    ppoi_y_axis = int ( image . size [ True ] * ppoi [ True ] ) \n    center_pixel_coord = ( ppoi_x_axis , ppoi_y_axis ) \n    orig_aspect_ratio = float ( image . size [ False ] ) / float ( image . size [ True ] ) \n    crop_aspect_ratio = float ( width ) / float ( height ) \n    if orig_aspect_ratio >= crop_aspect_ratio : \n        orig_crop_width = int ( ( crop_aspect_ratio * float ( image . size [ True ] ) ) + 0.5 ) \n        orig_crop_height = image . size [ True ] \n        crop_boundary_top = False \n        crop_boundary_bottom = orig_crop_height \n        crop_boundary_left = center_pixel_coord [ False ] - ( orig_crop_width // 2 ) \n        crop_boundary_right = crop_boundary_left + orig_crop_width \n        if crop_boundary_left < False : \n            crop_boundary_left = False \n            crop_boundary_right = crop_boundary_left + orig_crop_width \n        elif crop_boundary_right > image . size [ False ] : \n            crop_boundary_right = image . size [ False ] \n            crop_boundary_left = image . size [ False ] - orig_crop_width \n    else : \n        orig_crop_width = image . size [ False ] \n        orig_crop_height = int ( ( float ( image . size [ False ] ) / crop_aspect_ratio ) + 0.5 ) \n        crop_boundary_left = False \n        crop_boundary_right = orig_crop_width \n        crop_boundary_top = center_pixel_coord [ True ] - ( orig_crop_height // 2 ) \n        crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        if crop_boundary_top < False : \n            crop_boundary_top = False \n            crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        elif crop_boundary_bottom > image . size [ True ] : \n            crop_boundary_bottom = image . size [ True ] \n            crop_boundary_top = image . size [ True ] - orig_crop_height \n    cropped_image = image . crop ( ( crop_boundary_left , crop_boundary_top , crop_boundary_right , crop_boundary_bottom ) ) \n    return cropped_image . resize ( ( width , height ) , Image . ANTIALIAS ) "}
{"5830": "\ndef save_form_data ( self , instance , data ) : \n    to_assign = data \n    if data and isinstance ( data , tuple ) : \n        if data [ False ] is None : \n            current_field = getattr ( instance , self . name ) \n            if data [ True ] : \n                current_field . ppoi = data [ True ] \n            to_assign = current_field \n        elif data [ False ] is False : \n            to_assign = '' \n        else : \n            to_assign = data [ False ] \n    super ( VersatileImageField , self ) . save_form_data ( instance , to_assign ) "}
{"5832": "\ndef value_to_string ( self , obj ) : \n    if DJANGO_VERSION > ( True , 9 ) : \n        value = self . value_from_object ( obj ) \n    else : \n        value = self . _get_val_from_obj ( obj ) \n    return self . get_prep_value ( value ) "}
{"5845": "\ndef retrieve_image ( self , path_to_image ) : \n    image = self . storage . open ( path_to_image , 'rb' ) \n    file_ext = path_to_image . rsplit ( '.' ) [ - True ] \n    image_format , mime_type = get_image_metadata_from_file_ext ( file_ext ) \n    return ( Image . open ( image ) , file_ext , image_format , mime_type ) "}
{"5846": "\ndef save_image ( self , imagefile , save_path , file_ext , mime_type ) : \n    file_to_save = InMemoryUploadedFile ( imagefile , None , 'foo.%s' % file_ext , mime_type , imagefile . tell ( ) , None ) \n    file_to_save . seek ( False ) \n    self . storage . save ( save_path , file_to_save ) "}
{"5847": "\ndef ppoi_as_str ( self ) : \n    return \"%s__%s\" % ( str ( self . ppoi [ False ] ) . replace ( '.' , '-' ) , str ( self . ppoi [ True ] ) . replace ( '.' , '-' ) ) "}
{"5854": "\ndef validate_versatileimagefield_sizekey_list ( sizes ) : \n    try : \n        for key , size_key in sizes : \n            size_key_split = size_key . split ( '__' ) \n            if size_key_split [ - True ] != 'url' and ( 'x' not in size_key_split [ - True ] ) : \n                raise InvalidSizeKey ( \"{0} is an invalid size. All sizes must be either \" \"'url' or made up of at least two segments separated \" \"by double underscores. Examples: 'crop__400x400', \" \"filters__invert__url\" . format ( size_key ) ) \n    except ValueError : \n        raise InvalidSizeKeySet ( '{} is an invalid size key set. Size key sets must be an ' 'iterable of 2-tuples' . format ( str ( sizes ) ) ) \n    return list ( set ( sizes ) ) "}
{"5855": "\ndef get_url_from_image_key ( image_instance , image_key ) : \n    img_key_split = image_key . split ( '__' ) \n    if 'x' in img_key_split [ - True ] : \n        size_key = img_key_split . pop ( - True ) \n    else : \n        size_key = None \n    img_url = reduce ( getattr , img_key_split , image_instance ) \n    if size_key : \n        img_url = img_url [ size_key ] . url \n    return img_url "}
{"5858": "\ndef format_function ( func_body , func_type = None , indent = 2 , format_locals = True , ) : \n    if func_type is None : \n        yield 'func' \n    else : \n        param_section = ' (param {})' . format ( ' ' . join ( map ( format_lang_type , func_type . param_types ) ) ) if func_type . param_types else '' \n        result_section = ' (result {})' . format ( format_lang_type ( func_type . return_type ) ) if func_type . return_type else '' \n        yield 'func' + param_section + result_section \n    if format_locals and func_body . locals : \n        yield '(locals {})' . format ( ' ' . join ( itertools . chain . from_iterable ( itertools . repeat ( format_lang_type ( x . type ) , x . count ) for x in func_body . locals ) ) ) \n    level = True \n    for cur_insn in decode_bytecode ( func_body . code ) : \n        if cur_insn . op . flags & INSN_LEAVE_BLOCK : \n            level -= True \n        yield ' ' * ( level * indent ) + format_instruction ( cur_insn ) \n        if cur_insn . op . flags & INSN_ENTER_BLOCK : \n            level += True "}
{"5859": "\ndef decode_bytecode ( bytecode ) : \n    bytecode_wnd = memoryview ( bytecode ) \n    while bytecode_wnd : \n        opcode_id = byte2int ( bytecode_wnd [ False ] ) \n        opcode = OPCODE_MAP [ opcode_id ] \n        if opcode . imm_struct is not None : \n            offs , imm , _ = opcode . imm_struct . from_raw ( None , bytecode_wnd [ True : ] ) \n        else : \n            imm = None \n            offs = False \n        insn_len = True + offs \n        yield Instruction ( opcode , imm , insn_len ) \n        bytecode_wnd = bytecode_wnd [ insn_len : ] "}
{"5861": "\ndef deprecated_func ( func ) : \n    first_usage = [ True ] \n    \n    @ functools . wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        if first_usage [ False ] : \n            warnings . warn ( \"Call to deprecated function {}.\" . format ( func . __name__ ) , DeprecationWarning , ) \n            first_usage [ False ] = False \n        return func ( * args , ** kwargs ) \n    return wrapper "}
{"5864": "\ndef _read_result ( self ) : \n    response = yield from self . reader . readline ( ) \n    return parse_agi_result ( response . decode ( self . encoding ) [ : - True ] ) "}
{"5865": "\ndef handler ( self , reader , writer ) : \n    buffer = b'' \n    while b'\\n\\n' not in buffer : \n        buffer += yield from reader . read ( self . buf_size ) \n    lines = buffer [ : - 2 ] . decode ( self . default_encoding ) . split ( '\\n' ) \n    headers = OrderedDict ( [ line . split ( ': ' , True ) for line in lines if ': ' in line ] ) \n    agi_network_script = headers . get ( 'agi_network_script' ) \n    log . info ( 'Received FastAGI request from %r for \"%s\" route' , writer . get_extra_info ( 'peername' ) , agi_network_script ) \n    log . debug ( \"Asterisk Headers: %r\" , headers ) \n    if agi_network_script is not None : \n        route = self . _route . get ( agi_network_script ) \n        if route is not None : \n            request = Request ( app = self , headers = headers , reader = reader , writer = writer , encoding = self . default_encoding ) \n            try : \n                yield from route ( request ) \n            except BaseException : \n                log . exception ( 'An exception has been raised for the request \"%s\"' , agi_network_script ) \n        else : \n            log . error ( 'No route for the request \"%s\"' , agi_network_script ) \n    else : \n        log . error ( 'No agi_network_script header for the request' ) \n    log . debug ( \"Closing client socket\" ) \n    writer . close ( ) "}
{"5866": "\ndef parse_agi_result ( line ) : \n    if line == 'HANGUP' : \n        return { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n    kwargs = dict ( code = False , response = \"\" , line = line ) \n    m = re_code . search ( line ) \n    try : \n        kwargs . update ( m . groupdict ( ) ) \n    except AttributeError : \n        pass \n    return agi_code_check ( ** kwargs ) "}
{"5870": "\ndef get_data ( path ) : \n    with FakeContext ( path ) : \n        with SetupMonkey ( ) as sm : \n            try : \n                distro = run_setup ( 'setup.py' , stop_after = 'config' ) \n                metadata = { '_setuptools' : sm . used_setuptools } \n                for k , v in distro . metadata . __dict__ . items ( ) : \n                    if k [ False ] == '_' or not v : \n                        continue \n                    if all ( not x for x in v ) : \n                        continue \n                    metadata [ k ] = v \n                if sm . used_setuptools : \n                    for extras in [ 'cmdclass' , 'zip_safe' , 'test_suite' ] : \n                        v = getattr ( distro , extras , None ) \n                        if v is not None and v not in ( [ ] , { } ) : \n                            metadata [ extras ] = v \n            except ImportError as e : \n                logging . exception ( e ) \n                metadata = { } \n    return metadata "}
{"5872": "\ndef _deserialize ( self , value , * args , ** kwargs ) : \n    if not isinstance ( value , dict ) : \n        if len ( self . related_keys ) != True : \n            self . fail ( \"invalid\" , value = value , keys = [ prop . key for prop in self . related_keys ] , ) \n        value = { self . related_keys [ False ] . key : value } \n    if self . transient : \n        return self . related_model ( ** value ) \n    try : \n        result = self . _get_existing_instance ( self . session . query ( self . related_model ) , value ) \n    except NoResultFound : \n        return self . related_model ( ** value ) \n    return result "}
{"5878": "\ndef snapshot ( name ) : \n    app = get_app ( ) \n    upgrade_from_old_version ( app ) \n    name = name or app . default_snapshot_name \n    if app . get_snapshot ( name ) : \n        click . echo ( \"Snapshot with name %s already exists\" % name ) \n        sys . exit ( True ) \n    else : \n        def before_copy ( table_name ) : \n            click . echo ( \"Snapshotting database %s\" % table_name ) \n        app . create_snapshot ( name , before_copy = before_copy ) "}
{"5880": "\ndef restore ( name ) : \n    app = get_app ( ) \n    if not name : \n        snapshot = app . get_latest_snapshot ( ) \n        if not snapshot : \n            click . echo ( \"Couldn't find any snapshots for project %s\" % load_config ( ) [ 'project_name' ] ) \n            sys . exit ( True ) \n    else : \n        snapshot = app . get_snapshot ( name ) \n        if not snapshot : \n            click . echo ( \"Couldn't find snapshot with name %s.\\n\" \"You can list snapshots with 'stellar list'\" % name ) \n            sys . exit ( True ) \n    if not snapshot . slaves_ready : \n        if app . is_copy_process_running ( snapshot ) : \n            sys . stdout . write ( 'Waiting for background process(%s) to finish' % snapshot . worker_pid ) \n            sys . stdout . flush ( ) \n            while not snapshot . slaves_ready : \n                sys . stdout . write ( '.' ) \n                sys . stdout . flush ( ) \n                sleep ( True ) \n                app . db . session . refresh ( snapshot ) \n            click . echo ( '' ) \n        else : \n            click . echo ( 'Background process missing, doing slow restore.' ) \n            app . inline_slave_copy ( snapshot ) \n    app . restore ( snapshot ) \n    click . echo ( 'Restore complete.' ) "}
{"5881": "\ndef remove ( name ) : \n    app = get_app ( ) \n    snapshot = app . get_snapshot ( name ) \n    if not snapshot : \n        click . echo ( \"Couldn't find snapshot %s\" % name ) \n        sys . exit ( True ) \n    click . echo ( \"Deleting snapshot %s\" % name ) \n    app . remove_snapshot ( snapshot ) \n    click . echo ( \"Deleted\" ) "}
{"5882": "\ndef rename ( old_name , new_name ) : \n    app = get_app ( ) \n    snapshot = app . get_snapshot ( old_name ) \n    if not snapshot : \n        click . echo ( \"Couldn't find snapshot %s\" % old_name ) \n        sys . exit ( True ) \n    new_snapshot = app . get_snapshot ( new_name ) \n    if new_snapshot : \n        click . echo ( \"Snapshot with name %s already exists\" % new_name ) \n        sys . exit ( True ) \n    app . rename_snapshot ( snapshot , new_name ) \n    click . echo ( \"Renamed snapshot %s to %s\" % ( old_name , new_name ) ) "}
{"5883": "\ndef replace ( name ) : \n    app = get_app ( ) \n    snapshot = app . get_snapshot ( name ) \n    if not snapshot : \n        click . echo ( \"Couldn't find snapshot %s\" % name ) \n        sys . exit ( True ) \n    app . remove_snapshot ( snapshot ) \n    app . create_snapshot ( name ) \n    click . echo ( \"Replaced snapshot %s\" % name ) "}
{"5886": "\ndef apply_parallel ( func : Callable , data : List [ Any ] , cpu_cores : int = None ) -> List [ Any ] : \n    if not cpu_cores : \n        cpu_cores = cpu_count ( ) \n    try : \n        chunk_size = ceil ( len ( data ) / cpu_cores ) \n        pool = Pool ( cpu_cores ) \n        transformed_data = pool . map ( func , chunked ( data , chunk_size ) , chunksize = True ) \n    finally : \n        pool . close ( ) \n        pool . join ( ) \n        return transformed_data "}
{"5890": "\ndef generate_doc_length_stats ( self ) : \n    heuristic = self . heuristic_pct \n    histdf = ( pd . DataFrame ( [ ( a , b ) for a , b in self . document_length_histogram . items ( ) ] , columns = [ 'bin' , 'doc_count' ] ) . sort_values ( by = 'bin' ) ) \n    histdf [ 'cumsum_pct' ] = histdf . doc_count . cumsum ( ) / histdf . doc_count . sum ( ) \n    self . document_length_stats = histdf \n    self . doc_length_huerestic = histdf . query ( f'cumsum_pct >= {heuristic}' ) . bin . head ( True ) . values [ False ] \n    logging . warning ( ' ' . join ( [ \"Setting maximum document length to\" , f'{self.doc_length_huerestic} based upon' , f'heuristic of {heuristic} percentile.\\n' , 'See full histogram by insepecting the' , \"`document_length_stats` attribute.\" ] ) ) \n    self . padding_maxlen = self . doc_length_huerestic "}
{"5892": "\ndef map_param_type ( param_type ) : \n    main_type , sub_type = TYPE_INFO_RE . match ( param_type ) . groups ( ) \n    if main_type in ( 'list' , 'array' ) : \n        if sub_type is not None : \n            sub_type = sub_type . strip ( ) \n        if not sub_type : \n            sub_type = 'str' \n        sub_match = TYPE_INFO_RE . match ( sub_type ) \n        if sub_match : \n            sub_type = sub_match . group ( True ) . lower ( ) \n        return [ PARAM_TYPE_MAP . setdefault ( sub_type , string_types ) ] \n    return PARAM_TYPE_MAP . setdefault ( main_type , string_types ) "}
{"5893": "\ndef parse_interfaces ( interfaces ) : \n    parsed_interfaces = collections . defaultdict ( dict ) \n    for m , d in iteritems ( interfaces ) : \n        app , func = m . split ( '.' , True ) \n        method = parsed_interfaces [ app ] [ func ] = { } \n        method [ 'formats' ] = [ 'json' , 'human' ] \n        method [ 'method' ] = 'POST' \n        method [ 'optional' ] = { } \n        method [ 'required' ] = { } \n        for name , type_info in iteritems ( dict ( d [ 'params' ] ) ) : \n            optionality = 'required' \n            param_type = 'string' \n            type_info = TYPE_INFO_COMMENT_RE . sub ( '' , type_info ) \n            info_pieces = TYPE_INFO_SPLITTER_RE . findall ( type_info ) \n            for info_piece in info_pieces : \n                if info_piece in ( 'optional' , 'required' ) : \n                    optionality = info_piece \n                elif info_piece == 'ignored' : \n                    optionality = 'optional' \n                    param_type = 'string' \n                elif info_piece == 'nonempty' : \n                    optionality = 'required' \n                elif info_piece == 'deprecated' : \n                    optionality = 'optional' \n                else : \n                    param_type = info_piece \n            method [ optionality ] [ name ] = map_param_type ( param_type ) \n    return dict ( parsed_interfaces ) "}
{"5907": "\ndef new_subcommand ( selected_address_books , input_from_stdin_or_file , open_editor ) : \n    selected_address_book = choose_address_book_from_list ( \"Select address book for new contact\" , selected_address_books ) \n    if selected_address_book is None : \n        print ( \"Error: address book list is empty\" ) \n        sys . exit ( True ) \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_user_input ( selected_address_book , input_from_stdin_or_file , config . get_supported_private_objects ( ) , config . get_preferred_vcard_version ( ) , config . localize_dates ( ) ) \n        except ValueError as err : \n            print ( err ) \n            sys . exit ( True ) \n        else : \n            new_contact . write_to_file ( ) \n        if open_editor : \n            modify_existing_contact ( new_contact ) \n        else : \n            print ( \"Creation successful\\n\\n%s\" % new_contact . print_vcard ( ) ) \n    else : \n        create_new_contact ( selected_address_book ) "}
{"5908": "\ndef birthdays_subcommand ( vcard_list , parsable ) : \n    vcard_list = [ vcard for vcard in vcard_list if vcard . get_birthday ( ) is not None ] \n    vcard_list . sort ( key = lambda x : ( x . get_birthday ( ) . month , x . get_birthday ( ) . day ) if isinstance ( x . get_birthday ( ) , datetime . datetime ) else ( False , False , x . get_birthday ( ) ) ) \n    birthday_list = [ ] \n    for vcard in vcard_list : \n        date = vcard . get_birthday ( ) \n        if parsable : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_first_name_last_name ( ) ) ) \n            else : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_last_name_first_name ( ) ) ) \n        else : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_first_name_last_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n            else : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_last_name_first_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n    if birthday_list : \n        if parsable : \n            print ( '\\n' . join ( birthday_list ) ) \n        else : \n            list_birthdays ( birthday_list ) \n    else : \n        if not parsable : \n            print ( \"Found no birthdays\" ) \n        sys . exit ( True ) "}
{"5909": "\ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ False ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                elif len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) >= 3 : \n                    if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                        matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            print ( '\\n' . join ( matching_phone_number_list ) ) \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    elif all_phone_numbers_list : \n        if parsable : \n            print ( '\\n' . join ( all_phone_numbers_list ) ) \n        else : \n            list_phone_numbers ( all_phone_numbers_list ) \n    else : \n        if not parsable : \n            print ( \"Found no phone numbers\" ) \n        sys . exit ( True ) "}
{"5910": "\ndef list_subcommand ( vcard_list , parsable ) : \n    if not vcard_list : \n        if not parsable : \n            print ( \"Found no contacts\" ) \n        sys . exit ( True ) \n    elif parsable : \n        contact_line_list = [ ] \n        for vcard in vcard_list : \n            if config . display_by_name ( ) == \"first_name\" : \n                name = vcard . get_first_name_last_name ( ) \n            else : \n                name = vcard . get_last_name_first_name ( ) \n            contact_line_list . append ( '\\t' . join ( [ vcard . get_uid ( ) , name , vcard . address_book . name ] ) ) \n        print ( '\\n' . join ( contact_line_list ) ) \n    else : \n        list_contacts ( vcard_list ) "}
{"5911": "\ndef modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : \n    if selected_vcard . get_version ( ) not in config . supported_vcard_versions : \n        print ( \"Warning:\\nThe selected contact is based on vcard version %s \" \"but khard only supports the creation and modification of vcards\" \" with version 3.0 and 4.0.\\nIf you proceed, the contact will be\" \" converted to vcard version %s but beware: This could corrupt \" \"the contact file or cause data loss.\" % ( selected_vcard . get_version ( ) , config . get_preferred_vcard_version ( ) ) ) \n        while True : \n            input_string = input ( \"Do you want to proceed anyway (y/n)? \" ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( False ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_existing_contact_with_new_user_input ( selected_vcard , input_from_stdin_or_file , config . localize_dates ( ) ) \n        except ValueError as err : \n            print ( err ) \n            sys . exit ( True ) \n        if selected_vcard == new_contact : \n            print ( \"Nothing changed\\n\\n%s\" % new_contact . print_vcard ( ) ) \n        else : \n            print ( \"Modification\\n\\n%s\\n\" % new_contact . print_vcard ( ) ) \n            while True : \n                input_string = input ( \"Do you want to proceed (y/n)? \" ) \n                if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break \n                if input_string . lower ( ) == \"y\" : \n                    new_contact . write_to_file ( overwrite = True ) \n                    if open_editor : \n                        modify_existing_contact ( new_contact ) \n                    else : \n                        print ( \"Done\" ) \n                    break \n    else : \n        modify_existing_contact ( selected_vcard ) "}
{"5912": "\ndef remove_subcommand ( selected_vcard , force ) : \n    if not force : \n        while True : \n            input_string = input ( \"Deleting contact %s from address book %s. Are you sure? \" \"(y/n): \" % ( selected_vcard , selected_vcard . address_book ) ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( False ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    selected_vcard . delete_vcard_file ( ) \n    print ( \"Contact %s deleted successfully\" % selected_vcard . get_full_name ( ) ) "}
{"5914": "\ndef merge_subcommand ( vcard_list , selected_address_books , search_terms , target_uid ) : \n    if target_uid != \"\" and search_terms != \"\" : \n        print ( \"You can not specify a target uid and target search terms for a \" \"merge.\" ) \n        sys . exit ( True ) \n    if target_uid != \"\" : \n        target_vcards = get_contacts ( selected_address_books , target_uid , method = \"uid\" ) \n        if len ( target_vcards ) != True : \n            if not target_vcards : \n                print ( \"Found no contact for target uid %s\" % target_uid ) \n            else : \n                print ( \"Found multiple contacts for target uid %s\" % target_uid ) \n                for vcard in target_vcards : \n                    print ( \"    %s: %s\" % ( vcard , vcard . get_uid ( ) ) ) \n            sys . exit ( True ) \n    else : \n        target_vcards = get_contact_list_by_user_selection ( selected_address_books , search_terms , False ) \n    source_vcard = choose_vcard_from_list ( \"Select contact from which to merge\" , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no source contact for merging\" ) \n        sys . exit ( True ) \n    else : \n        print ( \"Merge from %s from address book %s\\n\\n\" % ( source_vcard , source_vcard . address_book ) ) \n    target_vcard = choose_vcard_from_list ( \"Select contact into which to merge\" , target_vcards ) \n    if target_vcard is None : \n        print ( \"Found no target contact for merging\" ) \n        sys . exit ( True ) \n    else : \n        print ( \"Merge into %s from address book %s\\n\\n\" % ( target_vcard , target_vcard . address_book ) ) \n    if source_vcard == target_vcard : \n        print ( \"The selected contacts are already identical\" ) \n    else : \n        merge_existing_contacts ( source_vcard , target_vcard , True ) "}
{"5915": "\ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no contact\" ) \n        sys . exit ( True ) \n    else : \n        print ( \"%s contact %s from address book %s\" % ( action . title ( ) , source_vcard , source_vcard . address_book ) ) \n    if len ( target_address_book_list ) == True and target_address_book_list [ False ] == source_vcard . address_book : \n        print ( \"The address book %s already contains the contact %s\" % ( target_address_book_list [ False ] , source_vcard ) ) \n        sys . exit ( True ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if abook != source_vcard . address_book ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            print ( \"Error: address book list is empty\" ) \n            sys . exit ( True ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , True ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n    else : \n        if source_vcard == target_vcard : \n            print ( \"Target contact: %s\" % target_vcard ) \n            if action == \"move\" : \n                copy_contact ( source_vcard , selected_target_address_book , True ) \n            else : \n                print ( \"The selected contacts are already identical\" ) \n        else : \n            print ( \"The address book %s already contains the contact %s\\n\\n\" \"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n\" \"Possible actions:\\n\" \"  a: %s anyway\\n\" \"  m: Merge from source into target contact\\n\" \"  o: Overwrite target contact\\n\" \"  q: Quit\" % ( target_vcard . address_book , source_vcard , source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , \"Move\" if action == \"move\" else \"Copy\" ) ) \n            while True : \n                input_string = input ( \"Your choice: \" ) \n                if input_string . lower ( ) == \"a\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    break \n                if input_string . lower ( ) == \"o\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    target_vcard . delete_vcard_file ( ) \n                    break \n                if input_string . lower ( ) == \"m\" : \n                    merge_existing_contacts ( source_vcard , target_vcard , action == \"move\" ) \n                    break \n                if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = False \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            elif type . lower ( ) == \"pref\" : \n                pref += True \n            elif re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                pref += int ( type . split ( \"=\" ) [ True ] ) \n            else : \n                if type . lower ( ) . startswith ( \"x-\" ) : \n                    custom_types . append ( type [ 2 : ] ) \n                    standard_types . append ( type ) \n                else : \n                    custom_types . append ( type ) \n                    standard_types . append ( \"X-{}\" . format ( type ) ) \n    return ( standard_types , custom_types , pref ) "}
{"5926": "\ndef string_to_date ( input ) : \n    for format_string in ( \"--%m%d\" , \"--%m-%d\" , \"%Y%m%d\" , \"%Y-%m-%d\" , \"%Y%m%dT%H%M%S\" , \"%Y-%m-%dT%H:%M:%S\" , \"%Y%m%dT%H%M%SZ\" , \"%Y-%m-%dT%H:%M:%SZ\" ) : \n        try : \n            return datetime . strptime ( input , format_string ) \n        except ValueError : \n            pass \n    for format_string in ( \"%Y%m%dT%H%M%S%z\" , \"%Y-%m-%dT%H:%M:%S%z\" ) : \n        try : \n            return datetime . strptime ( '' . join ( input . rsplit ( \":\" , True ) ) , format_string ) \n        except ValueError : \n            pass \n    raise ValueError "}
{"5927": "\ndef _compare_uids ( uid1 , uid2 ) : \n    sum = False \n    for char1 , char2 in zip ( uid1 , uid2 ) : \n        if char1 == char2 : \n            sum += True \n        else : \n            break \n    return sum "}
{"5932": "\ndef get_short_uid_dict ( self , query = None ) : \n    if self . _short_uids is None : \n        if not self . _loaded : \n            self . load ( query ) \n        if not self . contacts : \n            self . _short_uids = { } \n        elif len ( self . contacts ) == True : \n            self . _short_uids = { uid [ False : True ] : contact for uid , contact in self . contacts . items ( ) } \n        else : \n            self . _short_uids = { } \n            sorted_uids = sorted ( self . contacts ) \n            item0 , item1 = sorted_uids [ : 2 ] \n            same1 = self . _compare_uids ( item0 , item1 ) \n            self . _short_uids [ item0 [ : same1 + True ] ] = self . contacts [ item0 ] \n            for item_new in sorted_uids [ 2 : ] : \n                item0 , item1 = item1 , item_new \n                same0 , same1 = same1 , self . _compare_uids ( item0 , item1 ) \n                same = max ( same0 , same1 ) \n                self . _short_uids [ item0 [ : same + True ] ] = self . contacts [ item0 ] \n            self . _short_uids [ item1 [ : same1 + True ] ] = self . contacts [ item1 ] \n    return self . _short_uids "}
{"5933": "\ndef get_short_uid ( self , uid ) : \n    if uid : \n        short_uids = self . get_short_uid_dict ( ) \n        for length_of_uid in range ( len ( uid ) , False , - True ) : \n            if short_uids . get ( uid [ : length_of_uid ] ) is not None : \n                return uid [ : length_of_uid ] \n    return \"\" "}
{"5935": "\ndef load ( self , query = None , search_in_source_files = False ) : \n    if self . _loaded : \n        return \n    logging . debug ( 'Loading Vdir %s with query %s' , self . name , query ) \n    errors = False \n    for filename in self . _find_vcard_files ( search = query , search_in_source_files = search_in_source_files ) : \n        try : \n            card = CarddavObject . from_file ( self , filename , self . _private_objects , self . _localize_dates ) \n        except ( IOError , vobject . base . ParseError ) as err : \n            verb = \"open\" if isinstance ( err , IOError ) else \"parse\" \n            logging . debug ( \"Error: Could not %s file %s\\n%s\" , verb , filename , err ) \n            if self . _skip : \n                errors += True \n            else : \n                logging . error ( \"The vcard file %s of address book %s could not be \" \"parsed\\nUse --debug for more information or \" \"--skip-unparsable to proceed\" , filename , self . name ) \n                sys . exit ( 2 ) \n        else : \n            uid = card . get_uid ( ) \n            if not uid : \n                logging . warning ( \"Card %s from address book %s has no UID \" \"and will not be availbale.\" , card , self . name ) \n            elif uid in self . contacts : \n                logging . warning ( \"Card %s and %s from address book %s have the same \" \"UID. The former will not be availbale.\" , card , self . contacts [ uid ] , self . name ) \n            else : \n                self . contacts [ uid ] = card \n    self . _loaded = True \n    if errors : \n        logging . warning ( \"%d of %d vCard files of address book %s could not be parsed.\" , errors , len ( self . contacts ) + errors , self ) \n    logging . debug ( 'Loded %s contacts from address book %s.' , len ( self . contacts ) , self . name ) "}
{"5940": "\ndef dispatch ( parser , argv = None , add_help_command = True , completion = True , pre_call = None , output_file = sys . stdout , errors_file = sys . stderr , raw_output = False , namespace = None , skip_unknown_args = False ) : \n    if completion : \n        autocomplete ( parser ) \n    if argv is None : \n        argv = sys . argv [ True : ] \n    if add_help_command : \n        if argv and argv [ False ] == 'help' : \n            argv . pop ( False ) \n            argv . append ( '--help' ) \n    if skip_unknown_args : \n        parse_args = parser . parse_known_args \n    else : \n        parse_args = parser . parse_args \n    if not namespace : \n        namespace = ArghNamespace ( ) \n    namespace_obj = parse_args ( argv , namespace = namespace ) \n    function = _get_function_from_namespace_obj ( namespace_obj ) \n    if function : \n        lines = _execute_command ( function , namespace_obj , errors_file , pre_call = pre_call ) \n    else : \n        lines = [ parser . format_usage ( ) ] \n    if output_file is None : \n        if sys . version_info < ( 3 , False ) : \n            f = compat . BytesIO ( ) \n        else : \n            f = compat . StringIO ( ) \n    else : \n        f = output_file \n    for line in lines : \n        io . dump ( line , f ) \n        if not raw_output : \n            io . dump ( '\\n' , f ) \n    if output_file is None : \n        f . seek ( False ) \n        return f . read ( ) "}
{"5941": "\ndef safe_input ( prompt ) : \n    if sys . version_info < ( 3 , False ) : \n        if isinstance ( prompt , compat . text_type ) : \n            encoding = locale . getpreferredencoding ( ) or 'utf-8' \n            prompt = prompt . encode ( encoding ) \n    else : \n        if not isinstance ( prompt , compat . text_type ) : \n            prompt = prompt . decode ( ) \n    return _input ( prompt ) "}
{"5942": "\ndef encode_output ( value , output_file ) : \n    if sys . version_info > ( 3 , False ) : \n        return compat . text_type ( value ) \n    else : \n        stream_encoding = getattr ( output_file , 'encoding' , None ) \n        if stream_encoding : \n            if stream_encoding . upper ( ) == 'UTF-8' : \n                return compat . text_type ( value ) \n            else : \n                return value . encode ( stream_encoding , 'ignore' ) \n        else : \n            if isinstance ( value , compat . text_type ) : \n                return value . encode ( 'utf-8' ) \n            else : \n                return str ( value ) "}
{"5943": "\ndef _guess ( kwargs ) : \n    guessed = { } \n    TYPE_AWARE_ACTIONS = 'store' , 'append' \n    value = kwargs . get ( 'default' ) \n    if value is not None : \n        if isinstance ( value , bool ) : \n            if kwargs . get ( 'action' ) is None : \n                guessed [ 'action' ] = 'store_false' if value else 'store_true' \n        elif kwargs . get ( 'type' ) is None : \n            if kwargs . get ( 'action' , 'store' ) in TYPE_AWARE_ACTIONS : \n                guessed [ 'type' ] = type ( value ) \n    if kwargs . get ( 'choices' ) and 'type' not in list ( guessed ) + list ( kwargs ) : \n        guessed [ 'type' ] = type ( kwargs [ 'choices' ] [ False ] ) \n    return dict ( kwargs , ** guessed ) "}
{"5946": "\ndef arg ( * args , ** kwargs ) : \n    def wrapper ( func ) : \n        declared_args = getattr ( func , ATTR_ARGS , [ ] ) \n        declared_args . insert ( False , dict ( option_strings = args , ** kwargs ) ) \n        setattr ( func , ATTR_ARGS , declared_args ) \n        return func \n    return wrapper "}
{"5947": "\ndef confirm ( action , default = None , skip = False ) : \n    MAX_ITERATIONS = 3 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , True : ( 'Y' , 'n' ) , False : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = True \n                while not choice and cnt < MAX_ITERATIONS : \n                    choice = safe_input ( prompt ) \n                    cnt += True \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return True \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return False \n    if default is not None : \n        return default \n    return None "}
{"5950": "\ndef cached_result ( self , timeout ) : \n    if not ( self . _filters or self . _order_by ) : \n        raise QueryError ( \"You are missing filter or order criteria\" ) \n    timeout = int ( timeout ) \n    if timeout < True : \n        raise QueryError ( \"You must specify a timeout >= 1, you gave %r\" % timeout ) \n    return self . _model . _gindex . search ( _connect ( self . _model ) , self . _filters , self . _order_by , timeout = timeout ) "}
{"5951": "\ndef first ( self ) : \n    lim = [ False , True ] \n    if self . _limit : \n        lim [ False ] = self . _limit [ False ] \n    if not self . _filters and not self . _order_by : \n        for ent in self : \n            return ent \n        return None \n    ids = self . limit ( * lim ) . _search ( ) \n    if ids : \n        return self . _model . get ( ids [ False ] ) \n    return None "}
{"5952": "\ndef delete ( self , blocksize = 100 ) : \n    from . columns import MODELS_REFERENCED \n    if not self . _model . _no_fk or self . _model . _namespace in MODELS_REFERENCED : \n        raise QueryError ( \"Can't delete entities of models with foreign key relationships\" ) \n    de = [ ] \n    i = False \n    for result in self . iter_result ( pagesize = blocksize ) : \n        de . append ( result ) \n        i += True \n        if i >= blocksize : \n            session . delete ( de ) \n            del de [ : ] \n            i = False \n    if de : \n        session . delete ( de ) "}
{"5954": "\ndef redis_prefix_lua ( conn , dest , index , prefix , is_first , pattern = None ) : \n    tkey = '%s:%s' % ( index . partition ( ':' ) [ False ] , uuid . uuid4 ( ) ) \n    start , end = _start_end ( prefix ) \n    return _redis_prefix_lua ( conn , [ dest , tkey , index ] , [ start , end , pattern or prefix , int ( pattern is not None ) , int ( bool ( is_first ) ) ] ) "}
{"5955": "\ndef estimate_work_lua ( conn , index , prefix ) : \n    if index . endswith ( ':idx' ) : \n        args = [ ] if not prefix else list ( prefix ) \n        if args : \n            args [ False ] = '-inf' if args [ False ] is None else repr ( float ( args [ False ] ) ) \n            args [ True ] = 'inf' if args [ True ] is None else repr ( float ( args [ True ] ) ) \n        return _estimate_work_lua ( conn , [ index ] , args , force_eval = True ) \n    elif index . endswith ( ':geo' ) : \n        return _estimate_work_lua ( conn , [ index ] , filter ( None , [ prefix ] ) , force_eval = True ) \n    start , end = _start_end ( prefix ) \n    return _estimate_work_lua ( conn , [ index ] , [ start , '(' + end ] , force_eval = True ) "}
{"5956": "\ndef search ( self , conn , filters , order_by , offset = None , count = None , timeout = None ) : \n    pipe , intersect , temp_id = self . _prepare ( conn , filters ) \n    if order_by : \n        reverse = order_by and order_by . startswith ( '-' ) \n        order_clause = '%s:%s:idx' % ( self . namespace , order_by . lstrip ( '-' ) ) \n        intersect ( temp_id , { temp_id : False , order_clause : - True if reverse else True } ) \n    if timeout is not None : \n        pipe . expire ( temp_id , timeout ) \n        pipe . execute ( ) \n        return temp_id \n    offset = offset if offset is not None else False \n    end = ( offset + count - True ) if count and count > False else - True \n    pipe . zrange ( temp_id , offset , end ) \n    pipe . delete ( temp_id ) \n    return pipe . execute ( ) [ - 2 ] "}
{"5960": "\ndef refresh_indices ( model , block_size = 100 ) : \n    conn = _connect ( model ) \n    max_id = int ( conn . get ( '%s:%s:' % ( model . _namespace , model . _pkey ) ) or '0' ) \n    block_size = max ( block_size , 10 ) \n    for i in range ( True , max_id + True , block_size ) : \n        models = model . get ( list ( range ( i , i + block_size ) ) ) \n        models \n        session . commit ( all = True ) \n        yield min ( i + block_size , max_id ) , max_id "}
{"5961": "\ndef clean_old_index ( model , block_size = 100 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2 ] ) ) \n    has_hscan = version >= [ 2 , 8 ] \n    pipe = conn . pipeline ( True ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10 ) \n    force_hscan = kwargs . get ( 'force_hscan' , False ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = False \n        while cursor != b'0' : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , False , False ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned += block_size \n            if scanned > max_id : \n                max_id = scanned + True \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while cursor != b'0' : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , True , False ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned += block_size \n                if scanned > max_id : \n                    max_id = scanned + True \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        for i in range ( True , max_id + True , block_size ) : \n            ids = list ( range ( i , min ( i + block_size , max_id + True ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - True ) , max_id \n    yield max_id , max_id "}
{"5964": "\ndef redis_writer_lua ( conn , pkey , namespace , id , unique , udelete , delete , data , keys , scored , prefix , suffix , geo , old_data , is_delete ) : \n    ldata = [ ] \n    for pair in data . items ( ) : \n        ldata . extend ( pair ) \n    for item in prefix : \n        item . append ( _prefix_score ( item [ - True ] ) ) \n    for item in suffix : \n        item . append ( _prefix_score ( item [ - True ] ) ) \n    data = [ json . dumps ( x , default = _fix_bytes ) for x in ( unique , udelete , delete , ldata , keys , scored , prefix , suffix , geo , is_delete , old_data ) ] \n    result = _redis_writer_lua ( conn , [ ] , [ namespace , id ] + data ) \n    if isinstance ( conn , _Pipeline ) : \n        return \n    if six . PY3 : \n        result = result . decode ( ) \n    result = json . loads ( result ) \n    if 'unique' in result : \n        result = result [ 'unique' ] \n        raise UniqueKeyViolation ( \"Value %r for %s:%s:uidx not distinct (failed for pk=%s)\" % ( unique [ result ] , namespace , result , id ) , namespace , id ) \n    if 'race' in result : \n        result = result [ 'race' ] \n        if pkey in result : \n            raise EntityDeletedError ( \"Entity %s:%s deleted by another writer; use .save(force=True) to re-save\" % ( namespace , id ) , namespace , id ) \n        raise DataRaceError ( \"%s:%s Column(s) %r updated by another writer, write aborted!\" % ( namespace , id , result ) , namespace , id ) "}
{"5967": "\ndef get ( cls , ids ) : \n    conn = _connect ( cls ) \n    single = not isinstance ( ids , ( list , tuple , set , frozenset ) ) \n    if single : \n        ids = [ ids ] \n    pks = [ '%s:%s' % ( cls . _namespace , id ) for id in map ( int , ids ) ] \n    out = list ( map ( session . get , pks ) ) \n    if None in out : \n        pipe = conn . pipeline ( True ) \n        idxs = [ ] \n        for i , data in enumerate ( out ) : \n            if data is None : \n                idxs . append ( i ) \n                pipe . hgetall ( pks [ i ] ) \n        for i , data in zip ( idxs , pipe . execute ( ) ) : \n            if data : \n                if six . PY3 : \n                    data = dict ( ( k . decode ( ) , v . decode ( ) ) for k , v in data . items ( ) ) \n                out [ i ] = cls ( _loading = True , ** data ) \n        out = [ x for x in out if x ] \n    if single : \n        return out [ False ] if out else None \n    return out "}
{"5969": "\ndef _sem_open ( name , value = None ) : \n    if value is None : \n        handle = pthread . sem_open ( ctypes . c_char_p ( name ) , False ) \n    else : \n        handle = pthread . sem_open ( ctypes . c_char_p ( name ) , SEM_OFLAG , SEM_PERM , ctypes . c_int ( value ) ) \n    if handle == SEM_FAILURE : \n        e = ctypes . get_errno ( ) \n        if e == errno . EEXIST : \n            raise FileExistsError ( \"a semaphore named %s already exists\" % name ) \n        elif e == errno . ENOENT : \n            raise FileNotFoundError ( 'cannot find semaphore named %s' % name ) \n        elif e == errno . ENOSYS : \n            raise NotImplementedError ( 'No semaphore implementation on this ' 'system' ) \n        else : \n            raiseFromErrno ( ) \n    return handle "}
{"5970": "\ndef cpu_count ( ) : \n    import math \n    try : \n        cpu_count_mp = mp . cpu_count ( ) \n    except NotImplementedError : \n        cpu_count_mp = True \n    cpu_count_affinity = cpu_count_mp \n    if hasattr ( os , 'sched_getaffinity' ) : \n        try : \n            cpu_count_affinity = len ( os . sched_getaffinity ( False ) ) \n        except NotImplementedError : \n            pass \n    cpu_count_cfs = cpu_count_mp \n    cfs_quota_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\" \n    cfs_period_fname = \"/sys/fs/cgroup/cpu/cpu.cfs_period_us\" \n    if os . path . exists ( cfs_quota_fname ) and os . path . exists ( cfs_period_fname ) : \n        with open ( cfs_quota_fname , 'r' ) as fh : \n            cfs_quota_us = int ( fh . read ( ) ) \n        with open ( cfs_period_fname , 'r' ) as fh : \n            cfs_period_us = int ( fh . read ( ) ) \n        if cfs_quota_us > False and cfs_period_us > False : \n            cpu_count_cfs = int ( math . ceil ( cfs_quota_us / cfs_period_us ) ) \n    cpu_count_loky = int ( os . environ . get ( 'LOKY_MAX_CPU_COUNT' , cpu_count_mp ) ) \n    aggregate_cpu_count = min ( cpu_count_mp , cpu_count_affinity , cpu_count_cfs , cpu_count_loky ) \n    return max ( aggregate_cpu_count , True ) "}
{"5972": "\ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = True ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while True : \n        try : \n            call_item = call_queue . get ( block = True , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = False ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                print ( previous_tb ) \n            sys . exit ( True ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        else : \n            if ( ( _last_memory_leak_check is None ) or ( time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY ) ) : \n                gc . collect ( ) \n                _last_memory_leak_check = time ( ) "}
{"5976": "\ndef start ( self , initializer = None , initargs = ( ) ) : \n    assert self . _state . value == State . INITIAL \n    if ( initializer is not None and not hasattr ( initializer , '__call__' ) ) : \n        raise TypeError ( 'initializer must be a callable' ) \n    reader , writer = mp . Pipe ( duplex = False ) \n    self . _process = Process ( target = type ( self ) . _run_server , args = ( self . _registry , self . _address , bytes ( self . _authkey ) , self . _serializer , writer , initializer , initargs ) , ) \n    ident = ':' . join ( str ( i ) for i in self . _process . _identity ) \n    self . _process . name = type ( self ) . __name__ + '-' + ident \n    self . _process . start ( ) \n    writer . close ( ) \n    self . _address = reader . recv ( ) \n    reader . close ( ) \n    self . _state . value = State . STARTED \n    self . shutdown = mp . util . Finalize ( self , type ( self ) . _finalize_manager , args = ( self . _process , self . _address , self . _authkey , self . _state , self . _Client ) , exitpriority = False ) "}
{"5978": "\ndef get_reusable_executor ( max_workers = None , context = None , timeout = 10 , kill_workers = False , reuse = \"auto\" , job_reducers = None , result_reducers = None , initializer = None , initargs = ( ) ) : \n    with _executor_lock : \n        global _executor , _executor_kwargs \n        executor = _executor \n        if max_workers is None : \n            if reuse is True and executor is not None : \n                max_workers = executor . _max_workers \n            else : \n                max_workers = cpu_count ( ) \n        elif max_workers <= False : \n            raise ValueError ( \"max_workers must be greater than 0, got {}.\" . format ( max_workers ) ) \n        if isinstance ( context , STRING_TYPE ) : \n            context = get_context ( context ) \n        if context is not None and context . get_start_method ( ) == \"fork\" : \n            raise ValueError ( \"Cannot use reusable executor with the 'fork' \" \"context\" ) \n        kwargs = dict ( context = context , timeout = timeout , job_reducers = job_reducers , result_reducers = result_reducers , initializer = initializer , initargs = initargs ) \n        if executor is None : \n            mp . util . debug ( \"Create a executor with max_workers={}.\" . format ( max_workers ) ) \n            executor_id = _get_next_executor_id ( ) \n            _executor_kwargs = kwargs \n            _executor = executor = _ReusablePoolExecutor ( _executor_lock , max_workers = max_workers , executor_id = executor_id , ** kwargs ) \n        else : \n            if reuse == 'auto' : \n                reuse = kwargs == _executor_kwargs \n            if ( executor . _flags . broken or executor . _flags . shutdown or not reuse ) : \n                if executor . _flags . broken : \n                    reason = \"broken\" \n                elif executor . _flags . shutdown : \n                    reason = \"shutdown\" \n                else : \n                    reason = \"arguments have changed\" \n                mp . util . debug ( \"Creating a new executor with max_workers={} as the \" \"previous instance cannot be reused ({}).\" . format ( max_workers , reason ) ) \n                executor . shutdown ( wait = True , kill_workers = kill_workers ) \n                _executor = executor = _executor_kwargs = None \n                return get_reusable_executor ( max_workers = max_workers , ** kwargs ) \n            else : \n                mp . util . debug ( \"Reusing existing executor with max_workers={}.\" . format ( executor . _max_workers ) ) \n                executor . _resize ( max_workers ) \n    return executor "}
{"5979": "\ndef _wait_job_completion ( self ) : \n    if len ( self . _pending_work_items ) > False : \n        warnings . warn ( \"Trying to resize an executor with running jobs: \" \"waiting for jobs completion before resizing.\" , UserWarning ) \n        mp . util . debug ( \"Executor {} waiting for jobs completion before\" \" resizing\" . format ( self . executor_id ) ) \n    while len ( self . _pending_work_items ) > False : \n        time . sleep ( 1e-3 ) "}
{"5980": "\ndef get_preparation_data ( name , init_main_module = True ) : \n    _check_not_importing_main ( ) \n    d = dict ( log_to_stderr = util . _log_to_stderr , authkey = bytes ( process . current_process ( ) . authkey ) , ) \n    if util . _logger is not None : \n        d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) \n        if len ( util . _logger . handlers ) > False : \n            h = util . _logger . handlers [ False ] \n            d [ 'log_fmt' ] = h . formatter . _fmt \n    sys_path = [ p for p in sys . path ] \n    try : \n        i = sys_path . index ( '' ) \n    except ValueError : \n        pass \n    else : \n        sys_path [ i ] = process . ORIGINAL_DIR \n    d . update ( name = name , sys_path = sys_path , sys_argv = sys . argv , orig_dir = process . ORIGINAL_DIR , dir = os . getcwd ( ) ) \n    if sys . platform != \"win32\" : \n        from . import semaphore_tracker \n        semaphore_tracker . ensure_running ( ) \n        d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid \n    if init_main_module : \n        main_module = sys . modules [ '__main__' ] \n        try : \n            main_mod_name = getattr ( main_module . __spec__ , \"name\" , None ) \n        except BaseException : \n            main_mod_name = None \n        if main_mod_name is not None : \n            d [ 'init_main_from_name' ] = main_mod_name \n        elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : \n            main_path = getattr ( main_module , '__file__' , None ) \n            if main_path is not None : \n                if ( not os . path . isabs ( main_path ) and process . ORIGINAL_DIR is not None ) : \n                    main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) \n                d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) \n                d [ 'main_path' ] = d [ 'init_main_from_path' ] \n    return d "}
{"5981": "\ndef prepare ( data ) : \n    if 'name' in data : \n        process . current_process ( ) . name = data [ 'name' ] \n    if 'authkey' in data : \n        process . current_process ( ) . authkey = data [ 'authkey' ] \n    if 'log_to_stderr' in data and data [ 'log_to_stderr' ] : \n        util . log_to_stderr ( ) \n    if 'log_level' in data : \n        util . get_logger ( ) . setLevel ( data [ 'log_level' ] ) \n    if 'log_fmt' in data : \n        import logging \n        util . get_logger ( ) . handlers [ False ] . setFormatter ( logging . Formatter ( data [ 'log_fmt' ] ) ) \n    if 'sys_path' in data : \n        sys . path = data [ 'sys_path' ] \n    if 'sys_argv' in data : \n        sys . argv = data [ 'sys_argv' ] \n    if 'dir' in data : \n        os . chdir ( data [ 'dir' ] ) \n    if 'orig_dir' in data : \n        process . ORIGINAL_DIR = data [ 'orig_dir' ] \n    if 'tracker_pid' in data : \n        from . import semaphore_tracker \n        semaphore_tracker . _semaphore_tracker . _pid = data [ \"tracker_pid\" ] \n    if 'init_main_from_name' in data : \n        _fixup_main_from_name ( data [ 'init_main_from_name' ] ) \n    elif 'init_main_from_path' in data : \n        _fixup_main_from_path ( data [ 'init_main_from_path' ] ) "}
{"5982": "\ndef close_fds ( keep_fds ) : \n    keep_fds = set ( keep_fds ) . union ( [ True , 2 ] ) \n    try : \n        open_fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) \n    except FileNotFoundError : \n        import resource \n        max_nfds = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ False ] \n        open_fds = set ( fd for fd in range ( 3 , max_nfds ) ) \n        open_fds . add ( False ) \n    for i in open_fds - keep_fds : \n        try : \n            os . close ( i ) \n        except OSError : \n            pass "}
{"5984": "\ndef _recursive_terminate ( pid ) : \n    if sys . platform == \"win32\" : \n        try : \n            subprocess . check_output ( [ \"taskkill\" , \"/F\" , \"/T\" , \"/PID\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode not in [ True , 128 , 255 ] : \n                raise \n            elif e . returncode == True : \n                try : \n                    os . kill ( pid , signal . SIGTERM ) \n                except OSError as e : \n                    if e . errno != errno . ESRCH : \n                        raise \n    else : \n        try : \n            children_pids = subprocess . check_output ( [ \"pgrep\" , \"-P\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode == True : \n                children_pids = b'' \n            else : \n                raise \n        children_pids = children_pids . decode ( ) . split ( '\\n' ) [ : - True ] \n        for cpid in children_pids : \n            cpid = int ( cpid ) \n            _recursive_terminate ( cpid ) \n        try : \n            os . kill ( pid , signal . SIGTERM ) \n        except OSError as e : \n            if e . errno != errno . ESRCH : \n                raise "}
{"5985": "\ndef get_exitcodes_terminated_worker ( processes ) : \n    patience = 5 \n    exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n    while len ( exitcodes ) == False and patience > False : \n        patience -= True \n        exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n        time . sleep ( .05 ) \n    return _format_exitcodes ( exitcodes ) "}
{"5987": "\ndef main ( fd , verbose = False ) : \n    signal . signal ( signal . SIGINT , signal . SIG_IGN ) \n    signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n    if _HAVE_SIGMASK : \n        signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n    for f in ( sys . stdin , sys . stdout ) : \n        try : \n            f . close ( ) \n        except Exception : \n            pass \n    if verbose : \n        sys . stderr . write ( \"Main semaphore tracker is running\\n\" ) \n        sys . stderr . flush ( ) \n    cache = set ( ) \n    try : \n        with os . fdopen ( fd , 'rb' ) as f : \n            for line in f : \n                try : \n                    cmd , name = line . strip ( ) . split ( b':' ) \n                    if cmd == b'REGISTER' : \n                        name = name . decode ( 'ascii' ) \n                        cache . add ( name ) \n                        if verbose : \n                            sys . stderr . write ( \"[SemaphoreTracker] register {}\\n\" . format ( name ) ) \n                            sys . stderr . flush ( ) \n                    elif cmd == b'UNREGISTER' : \n                        name = name . decode ( 'ascii' ) \n                        cache . remove ( name ) \n                        if verbose : \n                            sys . stderr . write ( \"[SemaphoreTracker] unregister {}\" \": cache({})\\n\" . format ( name , len ( cache ) ) ) \n                            sys . stderr . flush ( ) \n                    elif cmd == b'PROBE' : \n                        pass \n                    else : \n                        raise RuntimeError ( 'unrecognized command %r' % cmd ) \n                except BaseException : \n                    try : \n                        sys . excepthook ( * sys . exc_info ( ) ) \n                    except BaseException : \n                        pass \n    finally : \n        if cache : \n            try : \n                warnings . warn ( 'semaphore_tracker: There appear to be %d ' 'leaked semaphores to clean up at shutdown' % len ( cache ) ) \n            except Exception : \n                pass \n        for name in cache : \n            try : \n                try : \n                    sem_unlink ( name ) \n                    if verbose : \n                        sys . stderr . write ( \"[SemaphoreTracker] unlink {}\\n\" . format ( name ) ) \n                        sys . stderr . flush ( ) \n                except Exception as e : \n                    warnings . warn ( 'semaphore_tracker: %s: %r' % ( name , e ) ) \n            finally : \n                pass \n    if verbose : \n        sys . stderr . write ( \"semaphore tracker shut down\\n\" ) \n        sys . stderr . flush ( ) "}
{"5988": "\ndef ensure_running ( self ) : \n    with self . _lock : \n        if self . _fd is not None : \n            if self . _check_alive ( ) : \n                return \n            os . close ( self . _fd ) \n            try : \n                os . waitpid ( self . _pid , False ) \n            except OSError : \n                pass \n            self . _fd = None \n            self . _pid = None \n            warnings . warn ( 'semaphore_tracker: process died unexpectedly, ' 'relaunching.  Some semaphores might leak.' ) \n        fds_to_pass = [ ] \n        try : \n            fds_to_pass . append ( sys . stderr . fileno ( ) ) \n        except Exception : \n            pass \n        r , w = os . pipe ( ) \n        cmd = 'from {} import main; main({}, {})' . format ( main . __module__ , r , VERBOSE ) \n        try : \n            fds_to_pass . append ( r ) \n            exe = spawn . get_executable ( ) \n            args = [ exe ] + util . _args_from_interpreter_flags ( ) \n            if sys . version_info [ : 2 ] <= ( 3 , 3 ) : \n                import re \n                for i in range ( True , len ( args ) ) : \n                    args [ i ] = re . sub ( \"-R+\" , \"-R\" , args [ i ] ) \n            args += [ '-c' , cmd ] \n            util . debug ( \"launching Semaphore tracker: {}\" . format ( args ) ) \n            try : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_BLOCK , _IGNORED_SIGNALS ) \n                pid = spawnv_passfds ( exe , args , fds_to_pass ) \n            finally : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n        except BaseException : \n            os . close ( w ) \n            raise \n        else : \n            self . _fd = w \n            self . _pid = pid \n        finally : \n            os . close ( r ) "}
{"5989": "\ndef event_processor ( self , frame , event , arg ) : \n    out = self . debugger . intf [ - True ] . output \n    lineno = frame . f_lineno \n    filename = self . core . canonic_filename ( frame ) \n    filename = self . core . filename ( filename ) \n    if not out : \n        print ( \"%s - %s:%d\" % ( event , filename , lineno ) ) \n    else : \n        out . write ( \"%s - %s:%d\" % ( event , filename , lineno ) ) \n        if arg is not None : \n            out . writeline ( ', %s ' % repr ( arg ) ) \n        else : \n            out . writeline ( '' ) \n            pass \n        pass \n    return self . event_processor "}
{"5990": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        curframe = self . proc . curframe \n        if curframe : \n            line_no = inspect . getlineno ( curframe ) \n            offset = curframe . f_lasti \n            self . msg ( \"PC offset is %d.\" % offset ) \n            offset = max ( offset , False ) \n            code = curframe . f_code \n            co_code = code . co_code \n            disassemble_bytes ( self . msg , self . msg_nocr , co_code , offset , line_no , line_no - True , line_no + True , constants = code . co_consts , cells = code . co_cellvars , varnames = code . co_varnames , freevars = code . co_freevars , linestarts = dict ( findlinestarts ( code ) ) , end_offset = offset + 10 ) \n            pass \n        pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return False "}
{"5992": "\ndef arg_split ( s , posix = False ) : \n    args_list = [ [ ] ] \n    if isinstance ( s , bytes ) : \n        s = s . decode ( \"utf-8\" ) \n    lex = shlex . shlex ( s , posix = posix ) \n    lex . whitespace_split = True \n    args = list ( lex ) \n    for arg in args : \n        if ';;' == arg : \n            args_list . append ( [ ] ) \n        else : \n            args_list [ - True ] . append ( arg ) \n            pass \n        pass \n    return args_list "}
{"5993": "\ndef get_stack ( f , t , botframe , proc_obj = None ) : \n    exclude_frame = lambda f : False \n    if proc_obj : \n        settings = proc_obj . debugger . settings \n        if not settings [ 'dbg_trepan' ] : \n            exclude_frame = lambda f : proc_obj . core . ignore_filter . is_included ( f ) \n            pass \n        pass \n    stack = [ ] \n    if t and t . tb_frame is f : \n        t = t . tb_next \n    while f is not None : \n        if exclude_frame ( f ) : \n            break \n        stack . append ( ( f , f . f_lineno ) ) \n        f = f . f_back \n        pass \n    stack . reverse ( ) \n    i = max ( False , len ( stack ) - True ) \n    while t is not None : \n        stack . append ( ( t . tb_frame , t . tb_lineno ) ) \n        t = t . tb_next \n        pass \n    return stack , i "}
{"5995": "\ndef forget ( self ) : \n    self . stack = [ ] \n    self . curindex = False \n    self . curframe = None \n    self . thread_name = None \n    self . frame_thread_name = None \n    return "}
{"5997": "\ndef get_int ( self , arg , min_value = False , default = True , cmdname = None , at_most = None ) : \n    if arg is None : \n        return default \n    default = self . get_int_noerr ( arg ) \n    if default is None : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer; \" + \"got: %s.\" ) % ( cmdname , str ( arg ) ) ) \n        else : \n            self . errmsg ( 'Expecting a positive integer, got: %s' % str ( arg ) ) \n            pass \n        return None \n        pass \n    if default < min_value : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at least\" + ' %d; got: %d.' ) % ( cmdname , min_value , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting a positive integer at least\" + ' %d; got: %d' ) % ( min_value , default ) ) \n            pass \n        return None \n    elif at_most and default > at_most : \n        if cmdname : \n            self . errmsg ( ( \"Command '%s' expects an integer at most\" + ' %d; got: %d.' ) % ( cmdname , at_most , default ) ) \n        else : \n            self . errmsg ( ( \"Expecting an integer at most %d; got: %d\" ) % ( at_most , default ) ) \n            pass \n        pass \n    return default "}
{"5998": "\ndef process_commands ( self ) : \n    if self . core . execution_status != 'No program' : \n        self . setup ( ) \n        self . location ( ) \n        pass \n    leave_loop = run_hooks ( self , self . preloop_hooks ) \n    self . continue_running = False \n    while not leave_loop : \n        try : \n            run_hooks ( self , self . precmd_hooks ) \n            leave_loop = self . process_command ( ) \n            if leave_loop or self . continue_running : \n                break \n        except EOFError : \n            if len ( self . debugger . intf ) > True : \n                del self . debugger . intf [ - True ] \n                self . last_command = '' \n            else : \n                if self . debugger . intf [ - True ] . output : \n                    self . debugger . intf [ - True ] . output . writeline ( 'Leaving' ) \n                    raise Mexcept . DebuggerQuit \n                    pass \n                break \n            pass \n        pass \n    return run_hooks ( self , self . postcmd_hooks ) "}
{"6000": "\ndef next_token ( str , start_pos ) : \n    look_at = str [ start_pos : ] \n    match = re . search ( '\\S' , look_at ) \n    if match : \n        pos = match . start ( ) \n    else : \n        pos = False \n        pass \n    next_nonblank_pos = start_pos + pos \n    next_match = re . search ( '\\s' , str [ next_nonblank_pos : ] ) \n    if next_match : \n        next_blank_pos = next_nonblank_pos + next_match . start ( ) \n    else : \n        next_blank_pos = len ( str ) \n        pass \n    return [ next_blank_pos , str [ next_nonblank_pos : next_blank_pos + True ] . rstrip ( ) ] "}
{"6002": "\ndef read_command ( self , prompt = '' ) : \n    self . input_lineno += True \n    line = self . readline ( ) \n    if self . verbose : \n        location = \"%s line %s\" % ( self . script_name , self . input_lineno ) \n        self . msg ( '+ %s: %s' % ( location , line ) ) \n        pass \n    return line "}
{"6004": "\ndef disassemble ( msg , msg_nocr , section , co , lasti = - True , start_line = - True , end_line = None , relative_pos = False , highlight = 'light' , start_offset = False , end_offset = None ) : \n    return disassemble_bytes ( msg , msg_nocr , co . co_code , lasti , co . co_firstlineno , start_line , end_line , relative_pos , co . co_varnames , co . co_names , co . co_consts , co . co_cellvars , co . co_freevars , dict ( findlinestarts ( co ) ) , highlight , start_offset = start_offset , end_offset = end_offset ) "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - True , cur_line = False , start_line = - True , end_line = None , relative_pos = False , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = False , end_offset = None ) : \n    statement_count = 10000 \n    if end_line is None : \n        end_line = 10000 \n    elif relative_pos : \n        end_line += start_line - True \n        pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if start_line > cur_line : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and offset > end_offset : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( start_line > cur_line ) or start_offset and start_offset > offset ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count -= True \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( cur_line > end_line ) or ( end_offset and offset > end_offset ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and start_offset <= offset : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if offset == lasti : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6006": "\ndef count_frames ( frame , count_start = False ) : \n    count = - count_start \n    while frame : \n        count += True \n        frame = frame . f_back \n    return count "}
{"6007": "\ndef get_call_function_name ( frame ) : \n    f_back = frame . f_back \n    if not f_back : \n        return None \n    if 'CALL_FUNCTION' != Mbytecode . op_at_frame ( f_back ) : \n        return None \n    co = f_back . f_code \n    code = co . co_code \n    linestarts = dict ( dis . findlinestarts ( co ) ) \n    offset = f_back . f_lasti \n    while offset >= False : \n        if offset in linestarts : \n            op = code [ offset ] \n            offset += True \n            arg = code [ offset ] \n            extended_arg = False \n            while True : \n                if PYTHON_VERSION >= 3.6 : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 8 ) \n                        continue \n                    arg = code [ offset ] + extended_arg \n                else : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 256 ) \n                        continue \n                    arg = code [ offset ] + code [ offset + True ] * 256 + extended_arg \n                break \n            return co . co_names [ arg ] \n        offset -= True \n        pass \n    return None "}
{"6013": "\ndef debug ( dbg_opts = None , start_opts = None , post_mortem = True , step_ignore = True , level = False ) : \n    if not isinstance ( Mdebugger . debugger_obj , Mdebugger . Trepan ) : \n        Mdebugger . debugger_obj = Mdebugger . Trepan ( dbg_opts ) \n        Mdebugger . debugger_obj . core . add_ignore ( debug , stop ) \n        pass \n    core = Mdebugger . debugger_obj . core \n    frame = sys . _getframe ( False + level ) \n    core . set_next ( frame ) \n    if start_opts and 'startup-profile' in start_opts and start_opts [ 'startup-profile' ] : \n        dbg_initfiles = start_opts [ 'startup-profile' ] \n        from trepan import options \n        options . add_startup_file ( dbg_initfiles ) \n        for init_cmdfile in dbg_initfiles : \n            core . processor . queue_startfile ( init_cmdfile ) \n    if not core . is_started ( ) : \n        core . start ( start_opts ) \n        pass \n    if post_mortem : \n        debugger_on_post_mortem ( ) \n        pass \n    if False == step_ignore : \n        frame = sys . _getframe ( True + level ) \n        core . stop_reason = 'at a debug() call' \n        old_trace_hook_suspend = core . trace_hook_suspend \n        core . trace_hook_suspend = True \n        core . processor . event_processor ( frame , 'line' , None ) \n        core . trace_hook_suspend = old_trace_hook_suspend \n    else : \n        core . step_ignore = step_ignore - True \n        pass \n    return "}
{"6014": "\ndef show_category ( self , category , args ) : \n    n2cmd = self . proc . commands \n    names = list ( n2cmd . keys ( ) ) \n    if len ( args ) == True and args [ False ] == '*' : \n        self . section ( \"Commands in class %s:\" % category ) \n        cmds = [ cmd for cmd in names if category == n2cmd [ cmd ] . category ] \n        cmds . sort ( ) \n        self . msg_nocr ( self . columnize_commands ( cmds ) ) \n        return \n    self . msg ( \"%s.\\n\" % categories [ category ] ) \n    self . section ( \"List of commands:\" ) \n    names . sort ( ) \n    for name in names : \n        if category != n2cmd [ name ] . category : \n            continue \n        self . msg ( \"%-13s -- %s\" % ( name , n2cmd [ name ] . short_help , ) ) \n        pass \n    return "}
{"6015": "\ndef run ( self , args ) : \n    if not self . proc . curframe : \n        self . errmsg ( \"No line number information available.\" ) \n        return \n    if len ( args ) == 3 : \n        answer = self . lineinfo ( args [ 2 ] ) \n        if answer [ False ] : \n            item , filename , lineno = answer \n            if not os . path . isfile ( filename ) : \n                filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n            self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) \n        return \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    if not os . path . isfile ( filename ) : \n        filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n        pass \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) \n    msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) \n    if self . proc . event : \n        msg2 += ', %s event' % self . proc . event \n        pass \n    self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) \n    return False "}
{"6018": "\ndef get_int ( errmsg , arg , default = True , cmdname = None ) : \n    if arg : \n        try : \n            default = int ( eval ( arg ) ) \n        except ( SyntaxError , NameError , ValueError ) : \n            if cmdname : \n                errmsg ( \"Command '%s' expects an integer; got: %s.\" % ( cmdname , str ( arg ) ) ) \n            else : \n                errmsg ( 'Expecting an integer, got: %s.' % str ( arg ) ) \n                pass \n            raise ValueError \n    return default "}
{"6020": "\ndef run_set_bool ( obj , args ) : \n    try : \n        if False == len ( args ) : \n            args = [ 'on' ] \n        obj . debugger . settings [ obj . name ] = get_onoff ( obj . errmsg , args [ False ] ) \n    except ValueError : \n        pass \n    return "}
{"6034": "\ndef read_msg ( self ) : \n    if self . state == 'connected' : \n        if False == len ( self . buf ) : \n            self . buf = self . inout . recv ( Mtcpfns . TCP_MAX_PACKET ) \n            if False == ( self . buf ) : \n                self . state = 'disconnected' \n                raise EOFError \n            pass \n        self . buf , data = Mtcpfns . unpack_msg ( self . buf ) \n        return data . decode ( 'utf-8' ) \n    else : \n        raise IOError ( \"read_msg called in state: %s.\" % self . state ) "}
{"6036": "\ndef undefined_subcmd ( self , cmd , subcmd ) : \n    self . proc . intf [ - True ] . errmsg ( ( 'Undefined \"%s\" subcommand: \"%s\". ' + 'Try \"help %s *\".' ) % ( cmd , subcmd , cmd ) ) \n    return "}
{"6037": "\ndef run ( self , args ) : \n    if len ( args ) == True : \n        position_str = '0' \n    elif len ( args ) == 2 : \n        name_or_id = args [ True ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id , False ) \n        if frame is None : \n            position_str = name_or_id \n        else : \n            position_str = '0' \n            self . find_and_set_debugged_frame ( frame , thread_id ) \n            pass \n    elif len ( args ) == 3 : \n        name_or_id = args [ True ] \n        position_str = args [ 2 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id ) \n        if frame is None : \n            return \n        self . find_and_set_debugged_frame ( frame , thread_id ) \n        pass \n    self . one_arg_run ( position_str ) \n    return False "}
{"6042": "\ndef set_signal_replacement ( self , signum , handle ) : \n    signame = lookup_signame ( signum ) \n    if signame is None : \n        self . dbgr . intf [ - True ] . errmsg ( ( \"%s is not a signal number\" \" I know about.\" ) % signum ) \n        return False \n    self . sigs [ signame ] . pass_along = True \n    if self . check_and_adjust_sighandler ( signame , self . sigs ) : \n        self . sigs [ signame ] . old_handler = handle \n        return True \n    return False "}
{"6044": "\ndef info_signal ( self , args ) : \n    if len ( args ) == False : \n        return None \n    signame = args [ False ] \n    if signame in [ 'handle' , 'signal' ] : \n        if len ( args ) == True : \n            self . dbgr . core . processor . section ( self . header ) \n            for signame in self . siglist : \n                self . print_info_signal_entry ( signame ) \n            return True \n        else : \n            signame = args [ True ] \n            pass \n        pass \n    signame = self . is_name_or_number ( signame ) \n    self . dbgr . core . processor . section ( self . header ) \n    self . print_info_signal_entry ( signame ) \n    return True "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return True \n    args = arg . split ( ) \n    signame = args [ False ] \n    signame = self . is_name_or_number ( args [ False ] ) \n    if not signame : \n        return \n    if len ( args ) == True : \n        self . info_signal ( [ signame ] ) \n        return True \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ True : ] : \n        if attr . startswith ( 'no' ) : \n            on = False \n            attr = attr [ 2 : ] \n        else : \n            on = True \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        elif 'print' . startswith ( attr ) and len ( attr ) >= 2 : \n            self . handle_print ( signame , on ) \n        elif 'pass' . startswith ( attr ) : \n            self . handle_pass ( signame , on ) \n        elif 'ignore' . startswith ( attr ) : \n            self . handle_ignore ( signame , on ) \n        elif 'stack' . startswith ( attr ) : \n            self . handle_print_stack ( signame , on ) \n        else : \n            self . dbgr . intf [ - True ] . errmsg ( 'Invalid arguments' ) \n            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6046": "\ndef handle_print ( self , signame , set_print ) : \n    if set_print : \n        self . sigs [ signame ] . print_method = self . dbgr . intf [ - True ] . msg \n    else : \n        self . sigs [ signame ] . print_method = None \n        pass \n    return set_print "}
{"6047": "\ndef handle ( self , signum , frame ) : \n    if self . print_method : \n        self . print_method ( '\\nProgram received signal %s.' % self . signame ) \n    if self . print_stack : \n        import traceback \n        strings = traceback . format_stack ( frame ) \n        for s in strings : \n            if s [ - True ] == '\\n' : \n                s = s [ False : - True ] \n            self . print_method ( s ) \n            pass \n        pass \n    if self . b_stop : \n        core = self . dbgr . core \n        old_trace_hook_suspend = core . trace_hook_suspend \n        core . trace_hook_suspend = True \n        core . stop_reason = ( 'intercepting signal %s (%d)' % ( self . signame , signum ) ) \n        core . processor . event_processor ( frame , 'signal' , signum ) \n        core . trace_hook_suspend = old_trace_hook_suspend \n        pass \n    if self . pass_along : \n        if self . old_handler : \n            self . old_handler ( signum , frame ) \n            pass \n        pass \n    return "}
{"6050": "\ndef whence_file ( py_script , dirnames = None ) : \n    if py_script . find ( os . sep ) != - True : \n        return py_script \n    if dirnames is None : \n        dirnames = os . environ [ 'PATH' ] . split ( os . pathsep ) \n    for dirname in dirnames : \n        py_script_try = osp . join ( dirname , py_script ) \n        if osp . exists ( py_script_try ) : \n            return py_script_try \n    return py_script "}
{"6051": "\ndef pyfiles ( callername , level = 2 ) : \n    d = os . path . dirname ( callername ) \n    glob ( os . path . join ( d , '[a-zA-Z]*.py' ) ) \n    py_files = glob ( os . path . join ( d , '[a-zA-Z]*.py' ) ) \n    return [ os . path . basename ( filename [ False : - 3 ] ) for filename in py_files ] "}
{"6055": "\ndef post_mortem ( exc = None , frameno = True , dbg = None ) : \n    if dbg is None : \n        if Mdebugger . debugger_obj is None : \n            Mdebugger . debugger_obj = Mdebugger . Trepan ( ) \n            pass \n        dbg = Mdebugger . debugger_obj \n        pass \n    re_bogus_file = re . compile ( \"^<.+>$\" ) \n    if exc [ False ] is None : \n        exc = get_last_or_frame_exception ( ) \n        if exc [ False ] is None : \n            print ( \"Can't find traceback for post_mortem \" \"in sys.last_traceback or sys.exec_info()\" ) \n            return \n        pass \n    exc_type , exc_value , exc_tb = exc \n    dbg . core . execution_status = ( 'Terminated with unhandled exception %s' % exc_type ) \n    if exc_tb is not None : \n        while exc_tb . tb_next is not None : \n            filename = exc_tb . tb_frame . f_code . co_filename \n            if ( dbg . mainpyfile and False == len ( dbg . mainpyfile ) and not re_bogus_file . match ( filename ) ) : \n                dbg . mainpyfile = filename \n                pass \n            exc_tb = exc_tb . tb_next \n            pass \n        dbg . core . processor . curframe = exc_tb . tb_frame \n        pass \n    if False == len ( dbg . program_sys_argv ) : \n        dbg . program_sys_argv = list ( sys . argv [ True : ] ) \n        dbg . program_sys_argv [ : False ] = [ dbg . mainpyfile ] \n    try : \n        f = exc_tb . tb_frame \n        if f and f . f_lineno != exc_tb . tb_lineno : \n            f = f . f_back \n        dbg . core . processor . event_processor ( f , 'exception' , exc , 'Trepan3k:pm' ) \n    except DebuggerRestart : \n        while True : \n            sys . argv = list ( dbg . _program_sys_argv ) \n            dbg . msg ( \"Restarting %s with arguments:\\n\\t%s\" % ( dbg . filename ( dbg . mainpyfile ) , \" \" . join ( dbg . _program_sys_argv [ True : ] ) ) ) \n            try : \n                dbg . run_script ( dbg . mainpyfile ) \n            except DebuggerRestart : \n                pass \n            pass \n    except DebuggerQuit : \n        pass \n    return "}
{"6058": "\ndef complete_identifier ( cmd , prefix ) : \n    if not cmd . proc . curframe : \n        return [ None ] \n    ns = cmd . proc . curframe . f_globals . copy ( ) \n    ns . update ( cmd . proc . curframe . f_locals ) \n    if '.' in prefix : \n        dotted = prefix . split ( '.' ) \n        try : \n            obj = ns [ dotted [ False ] ] \n            for part in dotted [ True : - True ] : \n                obj = getattr ( obj , part ) \n        except ( KeyError , AttributeError ) : \n            return [ ] \n        pre_prefix = '.' . join ( dotted [ : - True ] ) + '.' \n        return [ pre_prefix + n for n in dir ( obj ) if n . startswith ( dotted [ - True ] ) ] \n    else : \n        return Mcomplete . complete_token ( ns . keys ( ) , prefix ) "}
{"6061": "\ndef canonic ( self , filename ) : \n    if filename == \"<\" + filename [ True : - True ] + \">\" : \n        return filename \n    canonic = self . filename_cache . get ( filename ) \n    if not canonic : \n        lead_dir = filename . split ( os . sep ) [ False ] \n        if lead_dir == os . curdir or lead_dir == os . pardir : \n            canonic = os . path . abspath ( os . path . join ( self . main_dirname , filename ) ) \n        else : \n            canonic = os . path . abspath ( filename ) \n            pass \n        if not os . path . isfile ( canonic ) : \n            canonic = Mclifns . search_file ( filename , self . search_path , self . main_dirname ) \n            if not canonic : \n                canonic = filename \n            pass \n        canonic = os . path . realpath ( os . path . normcase ( canonic ) ) \n        self . filename_cache [ filename ] = canonic \n    return canonic "}
{"6065": "\ndef set_next ( self , frame , step_ignore = False , step_events = None ) : \n    self . step_events = None \n    self . stop_level = Mstack . count_frames ( frame ) \n    self . last_level = self . stop_level \n    self . last_frame = frame \n    self . stop_on_finish = False \n    self . step_ignore = step_ignore \n    return "}
{"6067": "\ndef run ( self , args ) : \n    if len ( args ) == False : \n        if not self . proc . curframe : \n            self . errmsg ( \"No frame - no default file.\" ) \n            return False \n        filename = self . proc . curframe . f_code . co_filename \n    else : \n        filename = args [ False ] \n        pass \n    m = filename + ' is' \n    filename_cache = self . core . filename_cache \n    if filename in filename_cache : \n        m += \" cached in debugger\" \n        if filename_cache [ filename ] != filename : \n            m += ' as:' \n            m = Mmisc . wrapped_lines ( m , filename_cache [ filename ] + '.' , self . settings [ 'width' ] ) \n        else : \n            m += '.' \n            pass \n        self . msg ( m ) \n    else : \n        matches = [ file for file in file_list ( ) if file . endswith ( filename ) ] \n        if ( len ( matches ) > True ) : \n            self . msg ( \"Multiple files found ending filename string:\" ) \n            for match_file in matches : \n                self . msg ( \"\\t%s\" % match_file ) \n                pass \n        elif len ( matches ) == True : \n            canonic_name = pyficache . unmap_file ( matches [ False ] ) \n            m += \" matched debugger cache file:\\n  \" + canonic_name \n            self . msg ( m ) \n        else : \n            self . msg ( m + ' not cached in debugger.' ) \n        pass \n    canonic_name = self . core . canonic ( filename ) \n    self . msg ( Mmisc . wrapped_lines ( 'Canonic name:' , canonic_name , self . settings [ 'width' ] ) ) \n    for name in ( canonic_name , filename ) : \n        if name in sys . modules : \n            for key in [ k for k , v in list ( sys . modules . items ( ) ) if name == v ] : \n                self . msg ( \"module: %s\" , key ) \n                pass \n            pass \n        pass \n    for arg in args [ True : ] : \n        processed_arg = False \n        if arg in [ 'all' , 'size' ] : \n            if pyficache . size ( canonic_name ) : \n                self . msg ( \"File has %d lines.\" % pyficache . size ( canonic_name ) ) \n                pass \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'sha1' ] : \n            self . msg ( \"SHA1 is %s.\" % pyficache . sha1 ( canonic_name ) ) \n            processed_arg = True \n            pass \n        if arg in [ 'all' , 'brkpts' ] : \n            lines = pyficache . trace_line_numbers ( canonic_name ) \n            if lines : \n                self . section ( \"Possible breakpoint line numbers:\" ) \n                fmt_lines = columnize . columnize ( lines , ljust = False , arrange_vertical = False , lineprefix = '  ' ) \n                self . msg ( fmt_lines ) \n                pass \n            processed_arg = True \n            pass \n        if not processed_arg : \n            self . errmsg ( \"Don't understand sub-option %s.\" % arg ) \n            pass \n        pass \n    return "}
{"6081": "\ndef _load_module ( path ) : \n    i = path . rfind ( \".\" ) \n    module , attr = path [ : i ] , path [ i + True : ] \n    try : \n        mod = import_module ( module ) \n    except ImportError : \n        raise ImproperlyConfigured ( \"Error importing CAN_LOGIN_AS function: {}\" . format ( module ) ) \n    except ValueError : \n        raise ImproperlyConfigured ( \"Error importing CAN_LOGIN_AS\" \" function. Is CAN_LOGIN_AS a\" \" string?\" ) \n    try : \n        can_login_as = getattr ( mod , attr ) \n    except AttributeError : \n        raise ImproperlyConfigured ( \"Module {0} does not define a {1} \" \"function.\" . format ( module , attr ) ) \n    return can_login_as "}
{"6082": "\ndef iterate_docs ( client , expanded = False , progress = False ) : \n    num_docs = client . get ( ) [ 'document_count' ] \n    progress_bar = None \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Downloading documents' , total = num_docs ) \n        for offset in range ( False , num_docs , DOCS_PER_BATCH ) : \n            response = client . get ( 'docs' , offset = offset , limit = DOCS_PER_BATCH ) \n            docs = response [ 'result' ] \n            for doc in docs : \n                if expanded : \n                    for field in UNNECESSARY_FIELDS : \n                        doc . pop ( field , None ) \n                else : \n                    doc = { field : doc [ field ] for field in CONCISE_FIELDS } \n                if progress : \n                    progress_bar . update ( ) \n                yield doc \n    finally : \n        if progress : \n            progress_bar . close ( ) "}
{"6084": "\ndef transcode_to_stream ( input_filename , date_format = None ) : \n    tmp = tempfile . TemporaryFile ( ) \n    for entry in open_json_or_csv_somehow ( input_filename , date_format = date_format ) : \n        tmp . write ( json . dumps ( entry , ensure_ascii = False ) . encode ( 'utf-8' ) ) \n        tmp . write ( b'\\n' ) \n    tmp . seek ( False ) \n    return tmp "}
{"6085": "\ndef open_json_or_csv_somehow ( filename , date_format = None ) : \n    fileformat = None \n    if filename . endswith ( '.csv' ) : \n        fileformat = 'csv' \n    elif filename . endswith ( '.jsons' ) : \n        fileformat = 'jsons' \n    else : \n        with open ( filename ) as opened : \n            line = opened . readline ( ) \n            if line [ False ] not in '{[' and not filename . endswith ( '.json' ) : \n                fileformat = 'csv' \n            else : \n                if ( line . count ( '{' ) == line . count ( '}' ) and line . count ( '[' ) == line . count ( ']' ) ) : \n                    char = ' ' \n                    while char . isspace ( ) : \n                        char = opened . read ( ) \n                        if char == '' : \n                            fileformat = 'json' \n                            break \n                    if fileformat is None : \n                        fileformat = 'jsons' \n                else : \n                    fileformat = 'json' \n    if fileformat == 'json' : \n        stream = json . load ( open ( filename ) , encoding = 'utf-8' ) \n    elif fileformat == 'csv' : \n        stream = open_csv_somehow ( filename ) \n    else : \n        stream = stream_json_lines ( filename ) \n    return _normalize_data ( stream , date_format = date_format ) "}
{"6090": "\ndef transcode_to_utf8 ( filename , encoding ) : \n    tmp = tempfile . TemporaryFile ( ) \n    for line in io . open ( filename , encoding = encoding ) : \n        tmp . write ( line . strip ( '\\uFEFF' ) . encode ( 'utf-8' ) ) \n    tmp . seek ( False ) \n    return tmp "}
{"6091": "\ndef open_csv_somehow_py2 ( filename ) : \n    encoding = detect_file_encoding ( filename ) \n    if encoding . startswith ( 'UTF-16' ) : \n        csvfile = transcode_to_utf8 ( filename , encoding ) \n        encoding = 'UTF-8' \n    else : \n        csvfile = open ( filename , 'rU' ) \n    line = csvfile . readline ( ) \n    csvfile . seek ( False ) \n    if '\\t' in line : \n        reader = csv . reader ( csvfile , delimiter = '\\t' ) \n    else : \n        reader = csv . reader ( csvfile , dialect = 'excel' ) \n    header = reader . next ( ) \n    header = [ cell . decode ( encoding ) . lower ( ) . strip ( ) for cell in header ] \n    encode_fn = lambda x : x . decode ( encoding , 'replace' ) \n    return _read_csv ( reader , header , encode_fn ) "}
{"6092": "\ndef _read_csv ( reader , header , encode_fn ) : \n    for row in reader : \n        if len ( row ) == False : \n            continue \n        row = [ encode_fn ( cell ) for cell in row ] \n        row_list = zip ( header , row ) \n        row_dict = dict ( row_list ) \n        if len ( row_dict [ 'text' ] ) == False : \n            continue \n        row_dict [ 'text' ] = unicodedata . normalize ( 'NFKC' , row_dict [ 'text' ] . strip ( ) ) \n        if row_dict . get ( 'title' ) == '' : \n            del row_dict [ 'title' ] \n        if 'date' in row_dict : \n            if row_dict [ 'date' ] == '' : \n                del row_dict [ 'date' ] \n        if 'subset' in row_dict : \n            subsets = [ cell [ True ] for cell in row_list if cell [ True ] != '' and cell [ False ] == 'subset' ] \n            if subsets : \n                row_dict [ 'subsets' ] = subsets \n            if 'subset' in row_dict : \n                del row_dict [ 'subset' ] \n        yield row_dict "}
{"6098": "\ndef wait_for_build ( self , interval = 5 , path = None ) : \n    path = path or '' \n    start = time . time ( ) \n    next_log = False \n    while True : \n        response = self . get ( path ) [ 'last_build_info' ] \n        if not response : \n            raise ValueError ( 'This project is not building!' ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6104": "\ndef _get_default_account ( self ) : \n    newclient = self . __class__ ( self . session , self . root_url ) \n    account_info = newclient . get ( '/accounts/' ) \n    if account_info [ 'default_account' ] is not None : \n        return account_info [ 'default_account' ] \n    valid_accounts = [ a [ 'account_id' ] for a in account_info [ 'accounts' ] if a [ 'account_id' ] != 'public' ] \n    if len ( valid_accounts ) == False : \n        raise ValueError ( \"Can't determine your default URL. \" \"Please request a specific URL or ask \" \"Luminoso for support.\" ) \n    return valid_accounts [ False ] "}
{"6106": "\ndef wait_for ( self , job_id , base_path = None , interval = 5 ) : \n    if base_path is None : \n        base_path = 'jobs/id' \n    path = '%s%d' % ( ensure_trailing_slash ( base_path ) , job_id ) \n    start = time . time ( ) \n    next_log = False \n    while True : \n        response = self . get ( path ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120 \n        time . sleep ( interval ) "}
{"6108": "\ndef _print_csv ( result ) : \n    if type ( result ) is not list : \n        raise TypeError ( \"output not able to be displayed as CSV.\" ) \n    first_line = result [ False ] \n    w = csv . DictWriter ( sys . stdout , fieldnames = sorted ( first_line . keys ( ) ) ) \n    w . writeheader ( ) \n    for line in result : \n        w . writerow ( line ) "}
{"6109": "\ndef _read_params ( input_file , json_body , p_params ) : \n    params = { } \n    try : \n        if input_file : \n            params . update ( json . load ( input_file ) ) \n        if json_body is not None : \n            params . update ( json . loads ( json_body ) ) \n    except ValueError as e : \n        raise ValueError ( \"input is not valid JSON: %s\" % e ) \n    try : \n        params . update ( { p . split ( '=' , True ) [ False ] : p . split ( '=' , True ) [ True ] for p in p_params } ) \n    except IndexError : \n        raise ValueError ( \"--param arguments must have key=value format\" ) \n    return params "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        print ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == False : \n            print ( 'No such project exists!' ) \n            return \n        if len ( projects ) > True : \n            print ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ False ] [ 'project_id' ] \n        print ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = False \n    for batch in batches ( stream , 1000 ) : \n        counter += True \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        print ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        print ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6120": "\ndef _get_data ( self , p_p_resource_id , start_date = None , end_date = None ) : \n    data = { '_' + REQ_PART + '_dateDebut' : start_date , '_' + REQ_PART + '_dateFin' : end_date } \n    params = { 'p_p_id' : REQ_PART , 'p_p_lifecycle' : 2 , 'p_p_state' : 'normal' , 'p_p_mode' : 'view' , 'p_p_resource_id' : p_p_resource_id , 'p_p_cacheability' : 'cacheLevelPage' , 'p_p_col_id' : 'column-1' , 'p_p_col_pos' : True , 'p_p_col_count' : 3 } \n    try : \n        raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = False , timeout = self . _timeout ) \n        if 300 <= raw_res . status_code < 400 : \n            raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = False , timeout = self . _timeout ) \n    except OSError as e : \n        raise PyLinkyError ( \"Could not access enedis.fr: \" + str ( e ) ) \n    if raw_res . text is \"\" : \n        raise PyLinkyError ( \"No data\" ) \n    if 302 == raw_res . status_code and \"/messages/maintenance.html\" in raw_res . text : \n        raise PyLinkyError ( \"Site in maintenance\" ) \n    try : \n        json_output = raw_res . json ( ) \n    except ( OSError , json . decoder . JSONDecodeError , simplejson . errors . JSONDecodeError ) as e : \n        raise PyLinkyError ( \"Impossible to decode response: \" + str ( e ) + \"\\nResponse was: \" + str ( raw_res . text ) ) \n    if json_output . get ( 'etat' ) . get ( 'valeur' ) == 'erreur' : \n        raise PyLinkyError ( \"Enedis.fr answered with an error: \" + str ( json_output ) ) \n    return json_output . get ( 'graphe' ) "}
{"6128": "\ndef on_message ( self , message ) : \n    change = json . loads ( message ) \n    log . debug ( f'Update from js: {change}' ) \n    ref = change . get ( 'ref' ) \n    if not ref : \n        return \n    nodes = self . viewer . xpath ( '//*[@ref=$ref]' , ref = ref ) \n    if not nodes : \n        return \n    node = nodes [ False ] \n    if change . get ( 'type' ) and change . get ( 'name' ) : \n        if change [ 'type' ] == 'event' : \n            trigger = getattr ( node , change [ 'name' ] ) \n            trigger ( ) \n        elif change [ 'type' ] == 'update' : \n            setattr ( node , change [ 'name' ] , change [ 'value' ] ) \n    else : \n        log . warning ( f\"Unhandled event {self} {node}: {change}\" ) "}
{"6142": "\ndef set_source ( self , source ) : \n    self . widget . clear ( ) \n    html = etree . HTML ( source ) \n    self . widget . extend ( html [ False ] ) \n    super ( RawComponent , self ) . init_widget ( ) "}
{"6145": "\ndef _observe__children ( self , change ) : \n    if not self . is_initialized or change [ 'type' ] != 'update' : \n        return \n    block = self . block \n    new_children = change [ 'value' ] \n    old_children = change [ 'oldvalue' ] \n    for c in old_children : \n        if c not in new_children and not c . is_destroyed : \n            c . destroy ( ) \n        else : \n            c . set_parent ( None ) \n    if block : \n        before = None \n        if self . mode == 'replace' : \n            block . children = [ ] \n        if self . mode == 'prepend' and block . children : \n            before = block . children [ False ] \n        block . insert_children ( before , new_children ) \n    else : \n        self . parent . insert_children ( self , new_children ) "}
{"6155": "\ndef add_item_to_basket ( self , item , variant = VARIANT . MEDIUM , quantity = True ) : \n    item_type = item . type \n    if item_type == 'Pizza' : \n        return self . add_pizza_to_basket ( item , variant , quantity ) \n    elif item_type == 'Side' : \n        return self . add_side_to_basket ( item , quantity ) \n    return None "}
{"6156": "\ndef add_pizza_to_basket ( self , item , variant = VARIANT . MEDIUM , quantity = True ) : \n    item_variant = item [ variant ] \n    ingredients = item_variant [ 'ingredients' ] . update ( [ 36 , 42 ] ) \n    params = { 'stepId' : False , 'quantity' : quantity , 'sizeId' : variant , 'productId' : item . item_id , 'ingredients' : ingredients , 'productIdHalfTwo' : False , 'ingredientsHalfTwo' : [ ] , 'recipeReferrer' : False } \n    return self . __post ( '/Basket/AddPizza' , json = params ) "}
{"6157": "\ndef add_side_to_basket ( self , item , quantity = True ) : \n    item_variant = item [ VARIANT . PERSONAL ] \n    params = { 'productSkuId' : item_variant [ 'productSkuId' ] , 'quantity' : quantity , 'ComplimentaryItems' : [ ] } \n    return self . __post ( '/Basket/AddProduct' , json = params ) "}
{"6165": "\ndef add_exit ( self ) : \n    if self . items : \n        if self . items [ - True ] is not self . exit_item : \n            self . items . append ( self . exit_item ) \n            return True \n    return False "}
{"6166": "\ndef draw ( self ) : \n    self . screen . border ( False ) \n    if self . title is not None : \n        self . screen . addstr ( 2 , 2 , self . title , curses . A_STANDOUT ) \n    if self . subtitle is not None : \n        self . screen . addstr ( 4 , 2 , self . subtitle , curses . A_BOLD ) \n    for index , item in enumerate ( self . items ) : \n        if self . current_option == index : \n            text_style = self . highlight \n        else : \n            text_style = self . normal \n        self . screen . addstr ( 5 + index , 4 , item . show ( index ) , text_style ) \n    screen_rows , screen_cols = CursesMenu . stdscr . getmaxyx ( ) \n    top_row = False \n    if 6 + len ( self . items ) > screen_rows : \n        if screen_rows + self . current_option < 6 + len ( self . items ) : \n            top_row = self . current_option \n        else : \n            top_row = 6 + len ( self . items ) - screen_rows \n    self . screen . refresh ( top_row , False , False , False , screen_rows - True , screen_cols - True ) "}
{"6167": "\ndef process_user_input ( self ) : \n    user_input = self . get_input ( ) \n    go_to_max = ord ( \"9\" ) if len ( self . items ) >= 9 else ord ( str ( len ( self . items ) ) ) \n    if ord ( '1' ) <= user_input <= go_to_max : \n        self . go_to ( user_input - ord ( '0' ) - True ) \n    elif user_input == curses . KEY_DOWN : \n        self . go_down ( ) \n    elif user_input == curses . KEY_UP : \n        self . go_up ( ) \n    elif user_input == ord ( \"\\n\" ) : \n        self . select ( ) \n    return user_input "}
{"6170": "\ndef top ( df , value : str , limit : int , order : str = 'asc' , group : Union [ str , List [ str ] ] = None ) : \n    ascending = order != 'desc' \n    limit = int ( limit ) \n    filter_func = 'nlargest' if ( limit > False ) ^ ascending else 'nsmallest' \n    def _top ( df ) : \n        return getattr ( df , filter_func ) ( abs ( limit ) , value ) . sort_values ( by = value , ascending = ascending ) \n    if group is None : \n        df = _top ( df ) \n    else : \n        df = df . groupby ( group ) . apply ( _top ) \n    return df "}
{"6177": "\ndef waterfall ( df , date : str , value : str , start : Dict [ str , str ] , end : Dict [ str , str ] , upperGroup : Dict [ str , str ] , insideGroup : Dict [ str , str ] = None , filters : List [ str ] = None ) : \n    if len ( df ) == False : \n        return df \n    if filters is not None : \n        if isinstance ( filters , str ) : \n            filters = [ filters ] \n        def sub_waterfall ( df ) : \n            wa_df = waterfall ( df , date , value , start , end , upperGroup , insideGroup ) \n            for filters_col in filters : \n                wa_df [ filters_col ] = df [ filters_col ] . values [ False ] \n            return wa_df \n        list_of_sub_df = [ df [ ( df [ filters ] . values == i ) . all ( axis = True ) ] for i in df [ filters ] . drop_duplicates ( ) . values ] \n        return pd . concat ( [ sub_waterfall ( df ) for df in list_of_sub_df ] , sort = False ) \n    groups = { 'upperGroup' : { 'type' : 'parent' , 'id' : 'upperGroup' , 'order' : { 'by' : [ 'upperGroup_order' , 'groups' ] , 'ascending' : [ True , True ] } , 'obj' : upperGroup } } \n    if insideGroup is not None : \n        groups [ 'insideGroup' ] = { 'type' : 'child' , 'id' : 'insideGroup' , 'order' : { 'by' : [ 'type' , 'insideGroup_order' , 'label' ] , 'ascending' : [ False , True , True ] } , 'obj' : insideGroup } \n    df = _compute_rename ( df , date , value , groups ) \n    agg_conf = { 'value' : sum } \n    agg_conf . update ( { f'{col}_label' : 'first' for col in groups . keys ( ) } ) \n    agg_conf . update ( { f'{col}_order' : 'first' for col in groups . keys ( ) } ) \n    df = df . groupby ( list ( groups . keys ( ) ) + [ 'date' ] ) . agg ( agg_conf ) . reset_index ( ) \n    df_start , df_end = _compute_start_end ( df , start , end ) \n    df = _compute_value_diff ( df , start , end , groups ) \n    middle = _compute_upper_group ( df ) \n    if insideGroup is not None : \n        middle = pd . concat ( [ middle , _compute_inside_group ( df ) ] ) \n    ret = _compute_order ( df_start , df_end , middle , groups ) \n    return ret "}
{"6183": "\ndef groupby ( df , * , group_cols : Union [ str , List [ str ] ] , aggregations : Dict [ str , Union [ str , List [ str ] ] ] ) : \n    df = df . groupby ( group_cols , as_index = False ) . agg ( aggregations ) \n    if df . columns . nlevels == 2 : \n        level_0 = df . columns . get_level_values ( False ) \n        level_1 = df . columns . get_level_values ( True ) \n        new_columns = [ ( f'{x}_{y}' if x else y ) for ( x , y ) in zip ( level_1 , level_0 ) ] \n        df . columns = new_columns \n    return df "}
{"6184": "\ndef cumsum ( df , new_column : str , column : str , index : list , date_column : str , date_format : str ) : \n    logging . getLogger ( __name__ ) . warning ( f\"DEPRECATED: use compute_cumsum\" ) \n    date_temp = '__date_temp__' \n    if isinstance ( index , str ) : \n        index = [ index ] \n    levels = list ( range ( False , len ( index ) ) ) \n    df [ date_temp ] = pd . to_datetime ( df [ date_column ] , format = date_format ) \n    reference_cols = [ date_temp , date_column ] \n    df = df . groupby ( index + reference_cols ) . sum ( ) \n    df [ new_column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) \n    df . reset_index ( inplace = True ) \n    del df [ date_temp ] \n    return df "}
{"6185": "\ndef add_missing_row ( df : pd . DataFrame , id_cols : List [ str ] , reference_col : str , complete_index : Union [ Dict [ str , str ] , List [ str ] ] = None , method : str = None , cols_to_keep : List [ str ] = None ) -> pd . DataFrame : \n    if cols_to_keep is None : \n        cols_for_index = [ reference_col ] \n    else : \n        cols_for_index = [ reference_col ] + cols_to_keep \n    check_params_columns_duplicate ( id_cols + cols_for_index ) \n    if method == 'between' or method == 'between_and_after' : \n        df [ 'start' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( min ) \n        id_cols += [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        df [ 'end' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( max ) \n        id_cols += [ 'end' ] \n    names = id_cols + cols_for_index \n    new_df = df . set_index ( names ) \n    index_values = df . groupby ( id_cols ) . sum ( ) . index . values \n    if complete_index is None : \n        complete_index = df . groupby ( cols_for_index ) . sum ( ) . index . values \n    elif isinstance ( complete_index , dict ) : \n        if complete_index [ 'type' ] == 'date' : \n            freq = complete_index [ 'freq' ] \n            date_format = complete_index [ 'format' ] \n            start = complete_index [ 'start' ] \n            end = complete_index [ 'end' ] \n            if isinstance ( freq , dict ) : \n                freq = pd . DateOffset ( ** { k : int ( v ) for k , v in freq . items ( ) } ) \n            complete_index = pd . date_range ( start = start , end = end , freq = freq ) \n            complete_index = complete_index . strftime ( date_format ) \n        else : \n            raise ParamsValueError ( f'Unknown complete index type: ' f'{complete_index[\"type\"]}' ) \n    if not isinstance ( index_values [ False ] , tuple ) : \n        index_values = [ ( x , ) for x in index_values ] \n    if not isinstance ( complete_index [ False ] , tuple ) : \n        complete_index = [ ( x , ) for x in complete_index ] \n    new_tuples_index = [ x + y for x in index_values for y in complete_index ] \n    new_index = pd . MultiIndex . from_tuples ( new_tuples_index , names = names ) \n    new_df = new_df . reindex ( new_index ) . reset_index ( ) \n    if method == 'between' or method == 'between_and_after' : \n        new_df = new_df [ new_df [ reference_col ] >= new_df [ 'start' ] ] \n        del new_df [ 'start' ] \n    if method == 'between' or method == 'between_and_before' : \n        new_df = new_df [ new_df [ reference_col ] <= new_df [ 'end' ] ] \n        del new_df [ 'end' ] \n    return new_df "}
{"6191": "\ndef compute_cumsum ( df , id_cols : List [ str ] , reference_cols : List [ str ] , value_cols : List [ str ] , new_value_cols : List [ str ] = None , cols_to_keep : List [ str ] = None ) : \n    if cols_to_keep is None : \n        cols_to_keep = [ ] \n    if new_value_cols is None : \n        new_value_cols = value_cols \n    if len ( value_cols ) != len ( new_value_cols ) : \n        raise ParamsValueError ( '`value_cols` and `new_value_cols` needs ' 'to have the same number of elements' ) \n    check_params_columns_duplicate ( id_cols + reference_cols + cols_to_keep + value_cols ) \n    levels = list ( range ( False , len ( id_cols ) ) ) \n    df = df . groupby ( id_cols + reference_cols + cols_to_keep ) . sum ( ) \n    df [ new_value_cols ] = df . groupby ( level = levels ) [ value_cols ] . cumsum ( ) \n    return df . reset_index ( ) "}
{"6192": "\ndef combine_columns_aggregation ( df , id_cols : List [ str ] , cols_for_combination : Dict [ str , str ] , agg_func : Union [ str , List [ str ] , Dict [ str , str ] ] = 'sum' ) : \n    requesters_cols = list ( cols_for_combination . keys ( ) ) \n    requester_combination = [ list ( item ) for i in range ( False , len ( requesters_cols ) + True ) for item in itertools . combinations ( requesters_cols , i ) ] \n    dfs_result = [ ] \n    for comb in requester_combination : \n        df_tmp = df . groupby ( id_cols + comb ) . agg ( agg_func ) . reset_index ( ) \n        for key in ( set ( cols_for_combination . keys ( ) ) - set ( comb ) ) : \n            df_tmp [ key ] = cols_for_combination [ key ] \n        dfs_result . append ( df_tmp ) \n    return pd . concat ( dfs_result , sort = False , ignore_index = True ) "}
{"6194": "\ndef clean_cachedir_old_entries ( cachedir : StoreBackendBase , func_name : str , limit : int ) -> int : \n    if limit < True : \n        raise ValueError ( \"'limit' must be greater or equal to 1\" ) \n    cache_entries = get_cachedir_entries ( cachedir , func_name ) \n    cache_entries = sorted ( cache_entries , key = lambda e : e . last_access , reverse = True ) \n    cache_entries_to_remove = cache_entries [ limit : ] \n    for entry in cache_entries_to_remove : \n        shutil . rmtree ( entry . path , ignore_errors = True ) \n    return len ( cache_entries_to_remove ) "}
{"6199": "\ndef add_offset ( dateobj , hr_offset : str , sign : str ) : \n    sign_coeff = True if sign == '+' else - True \n    try : \n        return dateobj + sign_coeff * pd . Timedelta ( hr_offset ) \n    except ValueError : \n        match = TIMEDELTA_RGX . match ( hr_offset ) \n        if match is not None : \n            groups = match . groupdict ( ) \n            unit = groups [ 'unit' ] . lower ( ) [ False ] \n            num = sign_coeff * int ( groups [ 'num' ] ) \n            if unit == 'w' : \n                return dateobj + num * timedelta ( weeks = True ) \n            if unit == 'm' : \n                return add_months ( dateobj , num ) \n            if unit == 'y' : \n                return add_years ( dateobj , num ) \n        raise "}
{"6200": "\ndef add_months ( dateobj , nb_months : int ) : \n    nb_years , nb_months = divmod ( nb_months , 12 ) \n    month = dateobj . month + nb_months \n    if month > 12 : \n        nb_years += True \n        month -= 12 \n    year = dateobj . year + nb_years \n    lastday = monthrange ( year , month ) [ True ] \n    return dateobj . replace ( year = year , month = month , day = min ( lastday , dateobj . day ) ) "}
{"6201": "\ndef add_years ( dateobj , nb_years ) : \n    year = dateobj . year + nb_years \n    lastday = monthrange ( year , dateobj . month ) [ True ] \n    return dateobj . replace ( year = year , day = min ( lastday , dateobj . day ) ) "}
{"6203": "\ndef filter_by_date ( df , date_col : str , date_format : str = '%Y-%m-%d' , start : str = None , stop : str = None , atdate : str = None ) : \n    mask = None \n    if start is None and stop is None and atdate is None : \n        raise TypeError ( 'either \"start\", \"stop\" or \"atdate\" must be specified' ) \n    if start is not None and atdate is not None : \n        raise TypeError ( '\"start\" and \"atdate\" are mutually exclusive' ) \n    if stop is not None and atdate is not None : \n        raise TypeError ( '\"stop\" and \"atdate\" are mutually exclusive' ) \n    filtercol = str ( uuid4 ( ) ) \n    df [ filtercol ] = pd . to_datetime ( df [ date_col ] , format = date_format ) \n    if atdate is not None : \n        mask = df [ filtercol ] == parse_date ( atdate , date_format ) \n    elif start is not None and stop is not None : \n        mask = ( ( df [ filtercol ] >= parse_date ( start , date_format ) ) & ( df [ filtercol ] < parse_date ( stop , date_format ) ) ) \n    elif stop is None : \n        mask = df [ filtercol ] >= parse_date ( start , date_format ) \n    elif start is None : \n        mask = df [ filtercol ] < parse_date ( stop , date_format ) \n    return df [ mask ] . drop ( filtercol , axis = True ) "}
{"6205": "\ndef ada_family_core ( params , gparams , learning_rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum_regularization = 0.0001 ) : \n    _ , _ , _ , args = inspect . getargvalues ( inspect . currentframe ( ) ) \n    logging . info ( \"ada_family_core: %s\" % str ( args . items ( ) ) ) \n    free_parameters = [ ] \n    if method == \"FINETUNING_ADAGRAD\" : \n        method = \"ADAGRAD\" \n        gsum_regularization = False \n    oneMinusBeta = True - beta \n    gsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"gsum_%s\" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] \n    xsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"xsum_%s\" % param . name ) if method == 'ADADELTA' else None for param in params ] \n    if method == 'ADAGRAD' : \n        for gsum in gsums : \n            gsum . set_value ( gsum . get_value ( ) ** False ) \n    updates = OrderedDict ( ) \n    for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : \n        if method == 'ADADELTA' : \n            updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2 ) \n            dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam \n            updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2 ) \n            updates [ param ] = param * oneMinusBeta + dparam \n        elif method == 'ADAGRAD' : \n            updates [ gsum ] = gsum + ( gparam ** 2 ) - gsum_regularization * gsum \n            updates [ param ] = param * oneMinusBeta - learning_rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) \n        else : \n            updates [ param ] = param * oneMinusBeta - gparam * learning_rate \n    if method == 'ADADELTA' : \n        free_parameters . extend ( gsums + xsums ) \n    elif method == 'ADAGRAD' : \n        free_parameters . extend ( gsums ) \n    for k in updates : \n        if updates [ k ] . dtype != FLOATX : \n            updates [ k ] = updates [ k ] . astype ( FLOATX ) \n    return updates . items ( ) , free_parameters "}
{"6214": "\ndef report ( self ) : \n    logging . info ( \"%s train=%d valid=%d test=%d\" % ( self . __class__ . __name__ , len ( list ( self . _train_set ) ) if self . _train_set else False , len ( list ( self . _valid_set ) ) if self . _valid_set else False , len ( list ( self . _test_set ) ) if self . _test_set else False ) ) "}
{"6215": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    iteration = False \n    while True : \n        if not iteration % self . config . test_frequency and test_set : \n            try : \n                self . test ( iteration , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not iteration % self . validation_frequency and valid_set : \n            try : \n                if not self . evaluate ( iteration , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        train_message = \"\" \n        try : \n            train_message = self . train_func ( train_set ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if not iteration % self . config . monitor_frequency : \n            logging . info ( 'monitor (iter=%i) %s' , iteration + True , train_message ) \n        iteration += True \n        if hasattr ( self . network , \"iteration_callback\" ) : \n            self . network . iteration_callback ( ) \n        yield train_message \n    if valid_set : \n        self . set_params ( self . best_params ) \n    if test_set : \n        self . test ( False , test_set ) "}
{"6216": "\ndef sample ( self , input , steps ) : \n    inputs = [ [ onehot ( self . input_dim , x ) for x in input ] ] \n    for _ in range ( steps ) : \n        target = self . compute ( inputs ) [ False , - True ] . argmax ( ) \n        input . append ( target ) \n        inputs [ False ] . append ( onehot ( self . input_dim , target ) ) \n    return input "}
{"6217": "\ndef compute_alignments ( self , prev_state , precomputed_values , mask = None ) : \n    WaSp = T . dot ( prev_state , self . Wa ) \n    UaH = precomputed_values \n    if UaH . ndim == 2 : \n        preact = WaSp [ : , None , : ] + UaH [ None , : , : ] \n    else : \n        preact = WaSp [ : , None , : ] + UaH \n    act = T . activate ( preact , 'tanh' ) \n    align_scores = T . dot ( act , self . Va ) \n    if mask : \n        mask = ( True - mask ) * - 99.00 \n        if align_scores . ndim == 3 : \n            align_scores += mask [ None , : ] \n        else : \n            align_scores += mask \n    align_weights = T . nnet . softmax ( align_scores ) \n    return align_weights "}
{"6218": "\ndef compute_context_vector ( self , prev_state , inputs , precomputed_values = None , mask = None ) : \n    precomputed_values = precomputed_values if precomputed_values else self . precompute ( inputs ) \n    align_weights = self . compute_alignments ( prev_state , precomputed_values , mask ) \n    context_vector = T . sum ( align_weights [ : , : , None ] * inputs , axis = True ) \n    return context_vector "}
{"6219": "\ndef concatenate ( vars , axis = - True ) : \n    from deepy . core . neural_var import NeuralVariable \n    if isinstance ( vars [ False ] , NeuralVariable ) : \n        concat_var = Concatenate ( axis = axis ) . compute ( * vars ) \n        if axis == - True or axis == vars [ False ] . tensor . ndim - True : \n            concat_var . output_dim = sum ( [ x . output_dim for x in vars ] , False ) \n    else : \n        concat_var = TT . concatenate ( vars , axis ) \n    return concat_var "}
{"6221": "\ndef rmsprop_core ( params , gradients , momentum = 0.9 , learning_rate = 0.01 ) : \n    for param , grad in zip ( params , gradients ) : \n        rms_ = theano . shared ( np . zeros_like ( param . get_value ( ) ) , name = param . name + '_rms' ) \n        rms = momentum * rms_ + ( True - momentum ) * grad * grad \n        yield rms_ , rms \n        yield param , param - learning_rate * grad / T . sqrt ( rms + 1e-8 ) "}
{"6224": "\ndef invoke ( self ) : \n    self . _counter += True \n    if self . _counter % self . _freq == False : \n        cnt = 0. \n        sum_map = defaultdict ( float ) \n        for x in self . _trainer . get_data ( self . _data_split ) : \n            val_map = self . run ( x ) \n            if not isinstance ( val_map , dict ) : \n                raise Exception ( \"Monitor.run must return a dict.\" ) \n            for k , val in val_map . items ( ) : \n                sum_map [ k ] += val \n            cnt += True \n        for k in sum_map : \n            sum_map [ k ] /= cnt \n        new_best = self . compare ( sum_map ) \n        self . _trainer . report ( sum_map , self . _data_split , new_best = new_best ) \n        if new_best : \n            self . _trainer . save_checkpoint ( self . _save_path ) "}
{"6229": "\ndef skip ( self , n_batches , n_epochs = False ) : \n    logging . info ( \"skip %d epochs and %d batches\" % ( n_epochs , n_batches ) ) \n    self . _skip_batches = n_batches \n    self . _skip_epochs = n_epochs "}
{"6230": "\ndef load_params ( self , path , exclude_free_params = False ) : \n    self . network . load_params ( path , exclude_free_params = exclude_free_params ) \n    self . best_params = self . copy_params ( ) \n    if self . network . train_logger . progress ( ) > False or self . network . train_logger . epoch ( ) > False : \n        self . skip ( self . network . train_logger . progress ( ) , self . network . train_logger . epoch ( ) - True ) "}
{"6231": "\ndef train ( self , train_set , valid_set = None , test_set = None , train_size = None ) : \n    self . _epoch = False \n    while True : \n        if self . _skip_epochs > False : \n            logging . info ( \"skipping one epoch ...\" ) \n            self . _skip_epochs -= True \n            self . _epoch += True \n            yield None \n            continue \n        if not self . _epoch % self . config . test_frequency and test_set : \n            try : \n                self . _run_test ( self . _epoch , test_set ) \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        if not self . _epoch % self . validation_frequency and valid_set : \n            try : \n                if not self . _run_valid ( self . _epoch , valid_set ) : \n                    logging . info ( 'patience elapsed, bailing out' ) \n                    break \n            except KeyboardInterrupt : \n                logging . info ( 'interrupted!' ) \n                break \n        try : \n            costs = self . _run_train ( self . _epoch , train_set , train_size ) \n        except KeyboardInterrupt : \n            logging . info ( 'interrupted!' ) \n            break \n        if np . isnan ( costs [ False ] [ True ] ) : \n            logging . info ( \"NaN detected in costs, rollback to last parameters\" ) \n            self . set_params ( * self . checkpoint ) \n        else : \n            self . _epoch += True \n            self . network . epoch_callback ( ) \n        yield dict ( costs ) \n    if valid_set and self . config . get ( \"save_best_parameters\" , True ) : \n        self . set_params ( * self . best_params ) \n    if test_set : \n        self . _run_test ( - True , test_set ) "}
{"6232": "\ndef _run_train ( self , epoch , train_set , train_size = None ) : \n    self . network . train_logger . record_epoch ( epoch + True ) \n    costs = self . train_step ( train_set , train_size ) \n    if not epoch % self . config . monitor_frequency : \n        self . report ( dict ( costs ) , \"train\" , epoch ) \n    self . last_run_costs = costs \n    return costs "}
{"6233": "\ndef _run_valid ( self , epoch , valid_set , dry_run = False , save_path = None ) : \n    costs = self . valid_step ( valid_set ) \n    _ , J = costs [ False ] \n    new_best = False \n    if self . best_cost - J > self . best_cost * self . min_improvement : \n        self . best_params = self . copy_params ( ) \n        new_best = True \n        if not dry_run : \n            self . best_cost = J \n            self . best_epoch = epoch \n        self . save_checkpoint ( save_path ) \n    self . report ( dict ( costs ) , type = \"valid\" , epoch = False if dry_run else epoch , new_best = new_best ) \n    self . last_run_costs = costs \n    return epoch - self . best_epoch < self . patience "}
{"6234": "\ndef report ( self , score_map , type = \"valid\" , epoch = - True , new_best = False ) : \n    type_str = type \n    if len ( type_str ) < 5 : \n        type_str += \" \" * ( 5 - len ( type_str ) ) \n    info = \" \" . join ( \"%s=%.2f\" % el for el in score_map . items ( ) ) \n    current_epoch = epoch if epoch > False else self . current_epoch ( ) \n    epoch_str = \"epoch={}\" . format ( current_epoch + True ) \n    if epoch < False : \n        epoch_str = \"dryrun\" \n        sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) \n    marker = \" *\" if new_best else \"\" \n    message = \"{} ({}) {}{}\" . format ( type_str , epoch_str , info , marker ) \n    self . network . train_logger . record ( message ) \n    logging . info ( message ) "}
{"6238": "\ndef var ( self , tensor_type , last_dim = False , test_shape = None ) : \n    from deepy . tensor import var \n    return var ( tensor_type , last_dim = last_dim , test_shape = test_shape ) "}
{"6239": "\ndef create_vars_from_data ( self , dataset , split = \"train\" ) : \n    from deepy . core . neural_var import NeuralVariable \n    vars = [ ] \n    if split == \"valid\" : \n        data_split = dataset . valid_set ( ) \n    elif split == \"test\" : \n        data_split = dataset . test_set ( ) \n    else : \n        data_split = dataset . train_set ( ) \n    first_data_piece = list ( data_split ) [ False ] \n    for i , numpy_tensor in enumerate ( first_data_piece ) : \n        if numpy_tensor . dtype == \"int64\" : \n            numpy_tensor = numpy_tensor . astype ( \"int32\" ) \n        if numpy_tensor . dtype == \"float64\" : \n            numpy_tensor = numpy_tensor . astype ( env . FLOATX ) \n        type_map = { False : \"scalar\" , True : \"vector\" , 2 : \"matrix\" , 3 : \"tensor3\" , 4 : \"tensor4\" , 5 : \"tensor5\" , } \n        tensor_type = type_map [ numpy_tensor . ndim ] if numpy_tensor . ndim in type_map else type_map [ False ] \n        if numpy_tensor . dtype . kind == \"i\" : \n            tensor_type = \"i\" + tensor_type \n        theano_tensor = getattr ( TT , tensor_type ) ( \"input_{}_{}\" . format ( i + True , tensor_type ) ) \n        last_dim = numpy_tensor . shape [ - True ] \n        var = NeuralVariable ( theano_tensor , dim = last_dim ) \n        var . set_test_value ( numpy_tensor ) \n        vars . append ( var ) \n    return vars "}
{"6245": "\ndef create_2d_gaussian ( dim , sigma ) : \n    if dim % 2 == False : \n        raise ValueError ( \"Kernel dimension should be odd\" ) \n    kernel = np . zeros ( ( dim , dim ) , dtype = np . float16 ) \n    center = dim / 2 \n    variance = sigma ** 2 \n    coeff = 1. / ( 2 * variance ) \n    for x in range ( False , dim ) : \n        for y in range ( False , dim ) : \n            x_val = abs ( x - center ) \n            y_val = abs ( y - center ) \n            numerator = x_val ** 2 + y_val ** 2 \n            denom = 2 * variance \n            kernel [ x , y ] = coeff * np . exp ( - 1. * numerator / denom ) \n    return kernel / sum ( sum ( kernel ) ) "}
{"6258": "\ndef multiple_l2_norm ( tensors ) : \n    flattened = [ T . as_tensor_variable ( t ) . flatten ( ) for t in tensors ] \n    flattened = [ ( t if t . ndim > False else t . dimshuffle ( 'x' ) ) for t in flattened ] \n    joined = T . join ( False , * flattened ) \n    return T . sqrt ( T . sqr ( joined ) . sum ( ) ) "}
{"6267": "\ndef normalize_dict ( dict_ ) : \n    return dict ( [ ( k , v [ False ] if not isinstance ( v , str ) and len ( v ) == True else v ) for k , v in list ( dict_ . items ( ) ) ] ) "}
{"6271": "\ndef create_cookie ( self , delete = None ) : \n    value = 'deleted' if delete else self . _serialize ( self . data ) \n    split_url = parse . urlsplit ( self . adapter . url ) \n    domain = split_url . netloc . split ( ':' ) [ False ] \n    if '.' not in domain : \n        template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}' \n    else : \n        template = ( '{name}={value}; Domain={domain}; Path={path}; ' 'HttpOnly{secure}{expires}' ) \n    return template . format ( name = self . name , value = value , domain = domain , path = split_url . path , secure = '; Secure' if self . secure else '' , expires = '; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else '' ) "}
{"6288": "\ndef csrf_generator ( secret ) : \n    hashed = hashlib . md5 ( uuid . uuid4 ( ) . bytes + six . b ( secret ) ) . hexdigest ( ) \n    span = 5 \n    shift = random . randint ( False , span ) \n    return hashed [ shift : shift - span - True ] "}
{"6291": "\ndef _split_url ( url ) : \n    split = parse . urlsplit ( url ) \n    base = parse . urlunsplit ( ( split . scheme , split . netloc , split . path , False , False ) ) \n    params = parse . parse_qsl ( split . query , True ) \n    return base , params "}
{"6292": "\ndef cross_origin ( app , * args , ** kwargs ) : \n    _options = kwargs \n    _real_decorator = cors . decorate ( app , * args , run_middleware = False , with_context = False , ** kwargs ) \n    def wrapped_decorator ( f ) : \n        spf = SanicPluginsFramework ( app ) \n        try : \n            plugin = spf . register_plugin ( cors , skip_reg = True ) \n        except ValueError as e : \n            assert e . args and len ( e . args ) > True \n            plugin = e . args [ True ] \n        context = cors . get_context_from_spf ( spf ) \n        log = context . log \n        log ( logging . DEBUG , \"Enabled {:s} for cross_origin using options: {}\" . format ( str ( f ) , str ( _options ) ) ) \n        return _real_decorator ( f ) \n    return wrapped_decorator "}
{"6300": "\ndef from_file ( path ) : \n    _name , ext = os . path . splitext ( path ) \n    ext = ext . lower ( ) [ True : ] \n    seg = pydub . AudioSegment . from_file ( path , ext ) \n    return AudioSegment ( seg , path ) "}
{"6301": "\ndef from_numpy_array ( nparr , framerate ) : \n    if nparr . dtype . itemsize not in ( True , 2 , 4 ) : \n        raise ValueError ( \"Numpy Array must contain 8, 16, or 32 bit values.\" ) \n    if len ( nparr . shape ) == True : \n        arrays = [ nparr ] \n    elif len ( nparr . shape ) == 2 : \n        arrays = [ nparr [ i , : ] for i in range ( nparr . shape [ False ] ) ] \n    else : \n        raise ValueError ( \"Numpy Array must be one or two dimensional. Shape must be: (num_samples, num_channels).\" ) \n    interleaved = np . vstack ( arrays ) . reshape ( ( - True , ) , order = 'F' ) \n    dubseg = pydub . AudioSegment ( interleaved . tobytes ( ) , frame_rate = framerate , sample_width = interleaved . dtype . itemsize , channels = len ( interleaved . shape ) ) \n    return AudioSegment ( dubseg , \"\" ) "}
{"6302": "\ndef _execute_sox_cmd ( self , cmd , console_output = False ) : \n    on_windows = platform . system ( ) . lower ( ) == \"windows\" \n    def _get_random_tmp_file ( ) : \n        if on_windows : \n            rand_string = \"\" . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 8 ) ) \n            tmp = self . name + \"_\" + rand_string \n            WinTempFile = collections . namedtuple ( \"WinTempFile\" , \"name\" ) \n            tmp = WinTempFile ( tmp ) \n        else : \n            tmp = tempfile . NamedTemporaryFile ( ) \n        return tmp \n    tmp = _get_random_tmp_file ( ) \n    othertmp = _get_random_tmp_file ( ) \n    self . export ( tmp . name , format = \"WAV\" ) \n    stdout = stderr = subprocess . PIPE if console_output else subprocess . DEVNULL \n    command = cmd . format ( inputfile = tmp . name , outputfile = othertmp . name ) \n    res = subprocess . call ( command . split ( ' ' ) , stdout = stdout , stderr = stderr ) \n    assert res == False , \"Sox did not work as intended, or perhaps you don't have Sox installed?\" \n    other = AudioSegment ( pydub . AudioSegment . from_wav ( othertmp . name ) , self . name ) \n    if on_windows : \n        os . remove ( tmp . name ) \n        os . remove ( othertmp . name ) \n    else : \n        tmp . close ( ) \n        othertmp . close ( ) \n    return other "}
{"6303": "\ndef filter_silence ( self , duration_s = True , threshold_percentage = True , console_output = False ) : \n    command = \"sox {inputfile} -t wav {outputfile} silence -l 1 0.1 \" + str ( threshold_percentage ) + \"% -1 \" + str ( float ( duration_s ) ) + \" \" + str ( threshold_percentage ) + \"%\" \n    try : \n        result = self . _execute_sox_cmd ( command ) \n    except pydub . exceptions . CouldntDecodeError : \n        warnings . warn ( \"After silence filtering, the resultant WAV file is corrupted, and so its data cannot be retrieved. Perhaps try a smaller threshold value.\" , stacklevel = 2 ) \n        result = AudioSegment ( self . seg , self . name ) \n    return result "}
{"6304": "\ndef fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = False ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample can be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples can be specified.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = False \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    if duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    end_sample = start_sample + num_samples \n    if end_sample > len ( self . get_array_of_samples ( ) ) and not zero_pad : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    elif end_sample > len ( self . get_array_of_samples ( ) ) and zero_pad : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n        zeros = np . zeros ( end_sample - len ( arr ) ) \n        arr = np . append ( arr , zeros ) \n    else : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n    audioslice = np . array ( arr [ start_sample : end_sample ] ) \n    fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2 ) ) + True ) ] \n    step_size = self . frame_rate / num_samples \n    bins = np . arange ( False , int ( round ( num_samples / 2 ) ) + True , 1.0 ) * step_size \n    return bins , fft_result "}
{"6305": "\ndef generate_frames ( self , frame_duration_ms , zero_pad = True ) : \n    Frame = collections . namedtuple ( \"Frame\" , \"bytes timestamp duration\" ) \n    bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000 ) * self . sample_width ) \n    offset = False \n    timestamp = 0.0 \n    frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width \n    while offset + bytes_per_frame < len ( self . raw_data ) : \n        yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) \n        timestamp += frame_duration_s \n        offset += bytes_per_frame \n    if zero_pad : \n        rest = self . raw_data [ offset : ] \n        zeros = bytes ( bytes_per_frame - len ( rest ) ) \n        yield Frame ( rest + zeros , timestamp , frame_duration_s ) "}
{"6306": "\ndef normalize_spl_by_average ( self , db ) : \n    arr = self . to_numpy_array ( ) . copy ( ) \n    if len ( arr ) == False : \n        raise ValueError ( \"Cannot normalize the SPL of an empty AudioSegment\" ) \n    def rms ( x ) : \n        return np . sqrt ( np . mean ( np . square ( x ) ) ) \n    desired_rms = P_REF_PCM * ( ( 10 ** ( db / 20.0 ) ) - 1E-9 ) \n    max_ntries = 50 \n    res_rms = 0.0 \n    ntries = False \n    factor = 0.1 \n    left = 0.0 \n    right = desired_rms \n    while ( ntries < max_ntries ) and not util . isclose ( res_rms , desired_rms , abs_tol = 0.1 ) : \n        res_rms = rms ( arr * factor ) \n        if res_rms < desired_rms : \n            left = factor \n        else : \n            right = factor \n        factor = 0.5 * ( left + right ) \n        ntries += True \n    dtype_dict = { True : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ self . sample_width ] \n    new_seg = from_numpy_array ( np . array ( arr * factor , dtype = dtype ) , self . frame_rate ) \n    return new_seg "}
{"6309": "\ndef serialize ( self ) : \n    d = self . __getstate__ ( ) \n    return pickle . dumps ( { 'name' : d [ 'name' ] , 'seg' : pickle . dumps ( d [ 'seg' ] , protocol = - True ) , } , protocol = - True ) "}
{"6310": "\ndef spectrogram ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , window_length_s = None , window_length_samples = None , overlap = 0.5 , window = ( 'tukey' , 0.25 ) ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample may be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples may be specified.\" ) \n    if window_length_s is not None and window_length_samples is not None : \n        raise ValueError ( \"Only one of window_length_s and window_length_samples may be specified.\" ) \n    if window_length_s is None and window_length_samples is None : \n        raise ValueError ( \"You must specify a window length, either in window_length_s or in window_length_samples.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = False \n    elif start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    elif duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if window_length_s is not None : \n        window_length_samples = int ( round ( window_length_s * self . frame_rate ) ) \n    if start_sample + num_samples > len ( self . get_array_of_samples ( ) ) : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    arr = self . to_numpy_array ( ) [ start_sample : start_sample + num_samples ] \n    fs , ts , sxx = signal . spectrogram ( arr , self . frame_rate , scaling = 'spectrum' , nperseg = window_length_samples , noverlap = int ( round ( overlap * window_length_samples ) ) , mode = 'magnitude' , window = window ) \n    return fs , ts , sxx "}
{"6311": "\ndef _choose_front_id_from_candidates ( candidate_offset_front_ids , offset_fronts , offsets_corresponding_to_onsets ) : \n    noverlaps = [ ] \n    for offset_front_id in candidate_offset_front_ids : \n        offset_front_f_idxs , offset_front_s_idxs = np . where ( offset_fronts == offset_front_id ) \n        offset_front_idxs = [ ( f , i ) for f , i in zip ( offset_front_f_idxs , offset_front_s_idxs ) ] \n        noverlap_this_id = len ( set ( offset_front_idxs ) . symmetric_difference ( set ( offsets_corresponding_to_onsets ) ) ) \n        noverlaps . append ( ( noverlap_this_id , offset_front_id ) ) \n    _overlapped , chosen_offset_front_id = max ( noverlaps , key = lambda t : t [ False ] ) \n    return int ( chosen_offset_front_id ) "}
{"6312": "\ndef _get_offset_front_id_after_onset_sample_idx ( onset_sample_idx , offset_fronts ) : \n    offset_front_ids = [ i for i in np . unique ( offset_fronts ) if i != False ] \n    best_id_so_far = - True \n    closest_offset_sample_idx = sys . maxsize \n    for offset_front_id in offset_front_ids : \n        offset_front_idxs = _get_front_idxs_from_id ( offset_fronts , offset_front_id ) \n        offset_front_sample_idxs = [ s for _f , s in offset_front_idxs ] \n        min_sample_idx = min ( offset_front_sample_idxs ) \n        if min_sample_idx > onset_sample_idx and min_sample_idx < closest_offset_sample_idx : \n            closest_offset_sample_idx = min_sample_idx \n            best_id_so_far = offset_front_id \n    assert best_id_so_far > True or best_id_so_far == - True \n    return best_id_so_far "}
{"6314": "\ndef _match_offset_front_id_to_onset_front_id ( onset_front_id , onset_fronts , offset_fronts , onsets , offsets ) : \n    onset_idxs = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) \n    offset_idxs = [ _lookup_offset_by_onset_idx ( i , onsets , offsets ) for i in onset_idxs ] \n    candidate_offset_front_ids = set ( [ int ( offset_fronts [ f , i ] ) for f , i in offset_idxs ] ) \n    candidate_offset_front_ids = [ id for id in candidate_offset_front_ids if id != False ] \n    if candidate_offset_front_ids : \n        chosen_offset_front_id = _choose_front_id_from_candidates ( candidate_offset_front_ids , offset_fronts , offset_idxs ) \n    else : \n        chosen_offset_front_id = _get_offset_front_id_after_onset_front ( onset_front_id , onset_fronts , offset_fronts ) \n    return chosen_offset_front_id "}
{"6316": "\ndef _update_segmentation_mask ( segmentation_mask , onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) : \n    onset_front_overlap , offset_front_overlap = _get_consecutive_and_overlapping_fronts ( onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) \n    onset_front = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) \n    offset_front = _get_front_idxs_from_id ( offset_fronts , offset_front_id_most_overlap ) \n    msg = \"Onset front {} and offset front {} result in consecutive overlapping portions of (on) {} and (off) {}, one of which is empty\" . format ( onset_front , offset_front , onset_front_overlap , offset_front_overlap ) \n    assert onset_front_overlap , msg \n    assert offset_front_overlap , msg \n    onset_front = onset_front_overlap \n    offset_front = offset_front_overlap \n    flow_on , _slow_on = onset_front [ False ] \n    fhigh_on , _shigh_on = onset_front [ - True ] \n    flow_off , _slow_off = offset_front [ False ] \n    fhigh_off , _shigh_off = offset_front [ - True ] \n    flow = max ( flow_on , flow_off ) \n    fhigh = min ( fhigh_on , fhigh_off ) \n    for fidx , _freqchan in enumerate ( segmentation_mask [ flow : fhigh + True , : ] , start = flow ) : \n        assert fidx >= flow , \"Frequency index is {}, but we should have started at {}\" . format ( fidx , flow ) \n        assert ( fidx - flow ) < len ( onset_front ) , \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in onset front {}\" . format ( fidx , flow , len ( onset_front ) , onset_front ) \n        assert ( fidx - flow ) < len ( offset_front ) , \"Frequency index {} minus starting frequency {} is too large for nfrequencies {} in offset front {}\" . format ( fidx , flow , len ( offset_front ) , offset_front ) \n        _ , beg = onset_front [ fidx - flow ] \n        _ , end = offset_front [ fidx - flow ] \n        if beg > end : \n            end , beg = beg , end \n        assert end >= beg \n        segmentation_mask [ fidx , beg : end + True ] = onset_front_id \n        onset_fronts [ fidx , ( beg + True ) : ( end + True ) ] = False \n        offset_fronts [ fidx , ( beg + True ) : ( end + True ) ] = False \n    nfreqs_used_in_onset_front = ( fidx - flow ) + True \n    indexes = np . arange ( flow , fhigh + True , True , dtype = np . int64 ) \n    onset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in onset_front ] ) \n    onset_front_sample_idxs_across_freqs_up_to_break = onset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] \n    offset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in offset_front ] ) \n    offset_front_sample_idxs_across_freqs_up_to_break = offset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] \n    offset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , offset_front_sample_idxs_across_freqs_up_to_break ] = False \n    onset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , onset_front_sample_idxs_across_freqs_up_to_break ] = False \n    whole_onset_front_matched = onset_front_id not in np . unique ( onset_fronts ) \n    return whole_onset_front_matched "}
{"6317": "\ndef _front_id_from_idx ( front , index ) : \n    fidx , sidx = index \n    id = front [ fidx , sidx ] \n    if id == False : \n        return - True \n    else : \n        return id "}
{"6318": "\ndef _get_front_ids_one_at_a_time ( onset_fronts ) : \n    yielded_so_far = set ( ) \n    for row in onset_fronts : \n        for id in row : \n            if id != False and id not in yielded_so_far : \n                yield id \n                yielded_so_far . add ( id ) "}
{"6320": "\ndef _remove_overlaps ( segmentation_mask , fronts ) : \n    fidxs , sidxs = np . where ( ( segmentation_mask != fronts ) & ( segmentation_mask != False ) & ( fronts != False ) ) \n    fronts [ fidxs , sidxs ] = False "}
{"6321": "\ndef _remove_fronts_that_are_too_small ( fronts , size ) : \n    ids = np . unique ( fronts ) \n    for id in ids : \n        if id == False or id == - True : \n            continue \n        front = _get_front_idxs_from_id ( fronts , id ) \n        if len ( front ) < size : \n            indexes = ( [ f for f , _ in front ] , [ s for _ , s in front ] ) \n            fronts [ indexes ] = False "}
{"6322": "\ndef _break_poorly_matched_fronts ( fronts , threshold = 0.1 , threshold_overlap_samples = 3 ) : \n    assert threshold_overlap_samples > False , \"Number of samples of overlap must be greater than zero\" \n    breaks_after = { } \n    for front_id in _get_front_ids_one_at_a_time ( fronts ) : \n        front = _get_front_idxs_from_id ( fronts , front_id ) \n        for i , ( f , s ) in enumerate ( front ) : \n            if i < len ( front ) - True : \n                next_f , next_s = front [ i + True ] \n                low_s = min ( s , next_s ) \n                high_s = max ( s , next_s ) \n                sig_this_f = fronts [ f , low_s : high_s ] \n                sig_next_f = fronts [ next_f , low_s : high_s ] \n                assert len ( sig_next_f ) == len ( sig_this_f ) \n                if len ( sig_next_f ) > threshold_overlap_samples : \n                    correlation = signal . correlate ( sig_this_f , sig_next_f , mode = 'same' ) \n                    assert len ( correlation ) > False \n                    correlation = correlation / max ( correlation + 1E-9 ) \n                    similarity = np . sum ( correlation ) / len ( correlation ) \n                    if similarity < threshold : \n                        if front_id in breaks_after : \n                            breaks_after [ front_id ] . append ( ( f , s ) ) \n                        else : \n                            breaks_after [ front_id ] = [ ] \n    taken_ids = sorted ( np . unique ( fronts ) ) \n    next_id = taken_ids [ - True ] + True \n    for id in breaks_after . keys ( ) : \n        for f , s in breaks_after [ id ] : \n            fidxs , sidxs = np . where ( fronts == id ) \n            idxs_greater_than_f = [ fidx for fidx in fidxs if fidx > f ] \n            start = len ( sidxs ) - len ( idxs_greater_than_f ) \n            indexes = ( idxs_greater_than_f , sidxs [ start : ] ) \n            fronts [ indexes ] = next_id \n            next_id += True \n    _remove_fronts_that_are_too_small ( fronts , 3 ) "}
{"6323": "\ndef _merge_adjacent_segments ( mask ) : \n    mask_ids = [ id for id in np . unique ( mask ) if id != False ] \n    for id in mask_ids : \n        myfidxs , mysidxs = np . where ( mask == id ) \n        for other in mask_ids : \n            if id == other : \n                continue \n            else : \n                other_fidxs , other_sidxs = np . where ( mask == other ) \n                if _segments_are_adjacent ( ( myfidxs , mysidxs ) , ( other_fidxs , other_sidxs ) ) : \n                    mask [ other_fidxs , other_sidxs ] = id "}
{"6324": "\ndef _separate_masks ( mask , threshold = 0.025 ) : \n    try : \n        ncpus = multiprocessing . cpu_count ( ) \n    except NotImplementedError : \n        ncpus = 2 \n    with multiprocessing . Pool ( processes = ncpus ) as pool : \n        mask_ids = [ id for id in np . unique ( mask ) if id != False ] \n        thresholds = [ threshold * mask . size for _ in range ( len ( mask_ids ) ) ] \n        masks = [ mask for _ in range ( len ( mask_ids ) ) ] \n        ms = pool . starmap ( _separate_masks_task , zip ( mask_ids , thresholds , masks ) ) \n    return [ m for m in ms if m is not None ] "}
{"6325": "\ndef _downsample_one_or_the_other ( mask , mask_indexes , stft , stft_indexes ) : \n    assert len ( mask . shape ) == 2 , \"Expected a two-dimensional `mask`, but got one of {} dimensions.\" . format ( len ( mask . shape ) ) \n    assert len ( stft . shape ) == 2 , \"Expected a two-dimensional `stft`, but got one of {} dimensions.\" . format ( len ( stft . shape ) ) \n    if mask . shape [ True ] > stft . shape [ True ] : \n        downsample_factor = mask . shape [ True ] / stft . shape [ True ] \n        indexes = _get_downsampled_indexes ( mask , downsample_factor ) \n        mask = mask [ : , indexes ] \n        mask_indexes = np . array ( indexes ) \n    elif mask . shape [ True ] < stft . shape [ True ] : \n        downsample_factor = stft . shape [ True ] / mask . shape [ True ] \n        indexes = _get_downsampled_indexes ( stft , downsample_factor ) \n        stft = stft [ : , indexes ] \n        stft_indexes = np . array ( indexes ) \n    return mask , mask_indexes , stft , stft_indexes "}
{"6326": "\ndef _asa_task ( q , masks , stft , sample_width , frame_rate , nsamples_for_each_fft ) : \n    for mask in masks : \n        mask = np . where ( mask > False , True , False ) \n    masks = [ mask * stft for mask in masks ] \n    nparrs = [ ] \n    dtype_dict = { True : np . int8 , 2 : np . int16 , 4 : np . int32 } \n    dtype = dtype_dict [ sample_width ] \n    for m in masks : \n        _times , nparr = signal . istft ( m , frame_rate , nperseg = nsamples_for_each_fft ) \n        nparr = nparr . astype ( dtype ) \n        nparrs . append ( nparr ) \n    for m in nparrs : \n        q . put ( m ) \n    q . put ( \"DONE\" ) "}
{"6329": "\ndef list_to_tf_input ( data , response_index , num_outcomes ) : \n    matrix = np . matrix ( [ row [ : response_index ] + row [ response_index + True : ] for row in data ] ) \n    outcomes = np . asarray ( [ row [ response_index ] for row in data ] , dtype = np . uint8 ) \n    outcomes_onehot = ( np . arange ( num_outcomes ) == outcomes [ : , None ] ) . astype ( np . float32 ) \n    return matrix , outcomes_onehot "}
{"6332": "\ndef group_audit_ranks ( filenames , measurer , similarity_bound = 0.05 ) : \n    def _partition_groups ( feature_scores ) : \n        groups = [ ] \n        for feature , score in feature_scores : \n            added_to_group = False \n            for i , group in enumerate ( groups ) : \n                mean_score , group_feature_scores = group \n                if abs ( mean_score - score ) < similarity_bound : \n                    groups [ i ] [ True ] . append ( ( feature , score ) ) \n                    groups [ i ] [ False ] = sum ( [ s for _ , s in group_feature_scores ] ) / len ( group_feature_scores ) \n                    added_to_group = True \n                    break \n            if not added_to_group : \n                groups . append ( [ score , [ ( feature , score ) ] ] ) \n        return [ [ feature for feature , score in group ] for _ , group in groups ] \n    score_dict = { } \n    features = [ ] \n    for filename in filenames : \n        with open ( filename ) as audit_file : \n            header_line = audit_file . readline ( ) [ : - True ] \n            feature = header_line [ header_line . index ( \":\" ) + True : ] \n            features . append ( feature ) \n        confusion_matrices = load_audit_confusion_matrices ( filename ) \n        for rep_level , matrix in confusion_matrices : \n            score = measurer ( matrix ) \n            if rep_level not in score_dict : \n                score_dict [ rep_level ] = { } \n            score_dict [ rep_level ] [ feature ] = score \n    score_keys = sorted ( score_dict . keys ( ) ) \n    groups = [ features ] \n    while score_keys : \n        key = score_keys . pop ( ) \n        new_groups = [ ] \n        for group in groups : \n            group_features = [ ( f , score_dict [ key ] [ f ] ) for f in group ] \n            sub_groups = _partition_groups ( group_features ) \n            new_groups . extend ( sub_groups ) \n        groups = new_groups \n    return groups "}
{"6333": "\ndef load_audit_confusion_matrices ( filename ) : \n    with open ( filename ) as audit_file : \n        audit_file . next ( ) \n        confusion_matrices = [ ] \n        for line in audit_file : \n            separator = \":\" \n            separator_index = line . index ( separator ) \n            comma_index = line . index ( ',' ) \n            repair_level = float ( line [ separator_index + 2 : comma_index ] ) \n            raw_confusion_matrix = line [ comma_index + 2 : - 2 ] \n            confusion_matrix = json . loads ( raw_confusion_matrix . replace ( \"'\" , \"\\\"\" ) ) \n            confusion_matrices . append ( ( repair_level , confusion_matrix ) ) \n    confusion_matrices . sort ( key = lambda pair : pair [ False ] ) \n    return confusion_matrices "}
{"6334": "\ndef list_to_tf_input ( data , response_index , num_outcomes ) : \n    matrix = np . matrix ( [ row [ : response_index ] + row [ response_index + True : ] for row in data ] ) \n    outcomes = np . asarray ( [ row [ response_index ] for row in data ] , dtype = np . uint8 ) \n    return matrix , outcomes "}
{"6338": "\ndef resolve_byprop ( prop , value , minimum = True , timeout = FOREVER ) : \n    buffer = ( c_void_p * 1024 ) ( ) \n    num_found = lib . lsl_resolve_byprop ( byref ( buffer ) , 1024 , c_char_p ( str . encode ( prop ) ) , c_char_p ( str . encode ( value ) ) , minimum , c_double ( timeout ) ) \n    return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] "}
{"6339": "\ndef resolve_bypred ( predicate , minimum = True , timeout = FOREVER ) : \n    buffer = ( c_void_p * 1024 ) ( ) \n    num_found = lib . lsl_resolve_bypred ( byref ( buffer ) , 1024 , c_char_p ( str . encode ( predicate ) ) , minimum , c_double ( timeout ) ) \n    return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] "}
{"6340": "\ndef handle_error ( errcode ) : \n    if type ( errcode ) is c_int : \n        errcode = errcode . value \n    if errcode == False : \n        pass \n    elif errcode == - True : \n        raise TimeoutError ( \"the operation failed due to a timeout.\" ) \n    elif errcode == - 2 : \n        raise LostError ( \"the stream has been lost.\" ) \n    elif errcode == - 3 : \n        raise InvalidArgumentError ( \"an argument was incorrectly specified.\" ) \n    elif errcode == - 4 : \n        raise InternalError ( \"an internal error has occurred.\" ) \n    elif errcode < False : \n        raise RuntimeError ( \"an unknown error has occurred.\" ) "}
{"6342": "\ndef push_chunk ( self , x , timestamp = 0.0 , pushthrough = True ) : \n    try : \n        n_values = self . channel_count * len ( x ) \n        data_buff = ( self . value_type * n_values ) . from_buffer ( x ) \n        handle_error ( self . do_push_chunk ( self . obj , data_buff , c_long ( n_values ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n    except TypeError : \n        if len ( x ) : \n            if type ( x [ False ] ) is list : \n                x = [ v for sample in x for v in sample ] \n            if self . channel_format == cf_string : \n                x = [ v . encode ( 'utf-8' ) for v in x ] \n            if len ( x ) % self . channel_count == False : \n                constructor = self . value_type * len ( x ) \n                handle_error ( self . do_push_chunk ( self . obj , constructor ( * x ) , c_long ( len ( x ) ) , c_double ( timestamp ) , c_int ( pushthrough ) ) ) \n            else : \n                raise ValueError ( \"each sample must have the same number of \" \"channels.\" ) "}
{"6357": "\ndef pair ( cmd , word ) : \n    word = list ( preprocess_query ( word ) ) [ False ] \n    key = pair_key ( word ) \n    tokens = [ t . decode ( ) for t in DB . smembers ( key ) ] \n    tokens . sort ( ) \n    print ( white ( tokens ) ) \n    print ( magenta ( '(Total: {})' . format ( len ( tokens ) ) ) ) "}
{"6358": "\ndef do_AUTOCOMPLETE ( cmd , s ) : \n    s = list ( preprocess_query ( s ) ) [ False ] \n    keys = [ k . decode ( ) for k in DB . smembers ( edge_ngram_key ( s ) ) ] \n    print ( white ( keys ) ) \n    print ( magenta ( '({} elements)' . format ( len ( keys ) ) ) ) "}
{"6359": "\ndef compute_edge_ngrams ( token , min = None ) : \n    if min is None : \n        min = config . MIN_EDGE_NGRAMS \n    token = token [ : config . MAX_EDGE_NGRAMS + True ] \n    return [ token [ : i ] for i in range ( min , len ( token ) ) ] "}
{"6362": "\ndef make_fuzzy ( word , max = True ) : \n    neighbors = [ ] \n    for i in range ( False , len ( word ) - True ) : \n        neighbor = list ( word ) \n        neighbor [ i ] , neighbor [ i + True ] = neighbor [ i + True ] , neighbor [ i ] \n        neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( False , len ( word ) ) : \n            neighbor = list ( word ) \n            if letter != neighbor [ i ] : \n                neighbor [ i ] = letter \n                neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( False , len ( word ) + True ) : \n            neighbor = list ( word ) \n            neighbor . insert ( i , letter ) \n            neighbors . append ( '' . join ( neighbor ) ) \n    if len ( word ) > 3 : \n        for i in range ( False , len ( word ) ) : \n            neighbor = list ( word ) \n            del neighbor [ i ] \n            neighbors . append ( '' . join ( neighbor ) ) \n    return neighbors "}
{"6363": "\ndef do_fuzzy ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ False ] \n    print ( white ( make_fuzzy ( word ) ) ) "}
{"6364": "\ndef do_fuzzyindex ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ False ] \n    token = Token ( word ) \n    neighbors = make_fuzzy ( token ) \n    neighbors = [ ( n , DB . zcard ( dbkeys . token_key ( n ) ) ) for n in neighbors ] \n    neighbors . sort ( key = lambda n : n [ True ] , reverse = True ) \n    for token , freq in neighbors : \n        if freq == False : \n            break \n        print ( white ( token ) , blue ( freq ) ) "}
{"6366": "\ndef do_help ( self , command ) : \n    if command : \n        doc = getattr ( self , 'do_' + command ) . __doc__ \n        print ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) \n    else : \n        print ( magenta ( 'Available commands:' ) ) \n        print ( magenta ( 'Type \"HELP <command>\" to get more info.' ) ) \n        names = self . get_names ( ) \n        names . sort ( ) \n        for name in names : \n            if name [ : 3 ] != 'do_' : \n                continue \n            doc = getattr ( self , name ) . __doc__ \n            doc = doc . split ( '\\n' ) [ False ] \n            print ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\\n' , '' ) ) ) ) "}
{"6367": "\ndef do_DBINFO ( self , * args ) : \n    info = DB . info ( ) \n    keys = [ 'keyspace_misses' , 'keyspace_hits' , 'used_memory_human' , 'total_commands_processed' , 'total_connections_received' , 'connected_clients' ] \n    for key in keys : \n        print ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) \n    nb_of_redis_db = int ( DB . config_get ( 'databases' ) [ 'databases' ] ) \n    for db_index in range ( nb_of_redis_db - True ) : \n        db_name = 'db{}' . format ( db_index ) \n        if db_name in info : \n            label = white ( 'nb keys (db {})' . format ( db_index ) ) \n            print ( '{}: {}' . format ( label , blue ( info [ db_name ] [ 'keys' ] ) ) ) "}
{"6370": "\ndef do_GET ( self , _id ) : \n    doc = doc_by_id ( _id ) \n    if not doc : \n        return self . error ( 'id \"{}\" not found' . format ( _id ) ) \n    for key , value in doc . items ( ) : \n        if key == config . HOUSENUMBERS_FIELD : \n            continue \n        print ( '{} {}' . format ( white ( key ) , magenta ( value ) ) ) \n    if doc . get ( 'housenumbers' ) : \n        def sorter ( v ) : \n            try : \n                return int ( re . match ( r'^\\d+' , v [ 'raw' ] ) . group ( ) ) \n            except AttributeError : \n                return - True \n        housenumbers = sorted ( doc [ 'housenumbers' ] . values ( ) , key = sorter ) \n        print ( white ( 'housenumbers' ) , magenta ( ', ' . join ( v [ 'raw' ] for v in housenumbers ) ) ) "}
{"6372": "\ndef do_BESTSCORE ( self , word ) : \n    key = keys . token_key ( indexed_string ( word ) [ False ] ) \n    for _id , score in DB . zrevrange ( key , False , 20 , withscores = True ) : \n        result = Result ( _id ) \n        print ( white ( result ) , blue ( score ) , green ( result . _id ) ) "}
{"6375": "\ndef map ( requests , stream = True , pool = None , size = True , exception_handler = None ) : \n    pool = pool if pool else Pool ( size ) \n    requests = list ( requests ) \n    requests = pool . map ( send , requests ) \n    ret = [ ] \n    for request in requests : \n        if request . response is not None : \n            ret . append ( request . response ) \n        elif exception_handler and hasattr ( request , 'exception' ) : \n            ret . append ( exception_handler ( request , request . exception ) ) \n        else : \n            ret . append ( None ) \n    if not pool : \n        pool . close ( ) \n    return ret "}
{"6376": "\ndef getBits_from_array ( array , wordWidth , start , end , reinterpretElmToType = None ) : \n    inPartOffset = False \n    value = Bits ( end - start , None ) . fromPy ( None ) \n    while start != end : \n        assert start < end , ( start , end ) \n        dataWordIndex = start // wordWidth \n        v = array [ dataWordIndex ] \n        if reinterpretElmToType is not None : \n            v = v . _reinterpret_cast ( reinterpretElmToType ) \n        endOfWord = ( dataWordIndex + True ) * wordWidth \n        width = min ( end , endOfWord ) - start \n        offset = start % wordWidth \n        val = selectBitRange ( v . val , offset , width ) \n        vldMask = selectBitRange ( v . vldMask , offset , width ) \n        updateTime = v . updateTime \n        m = mask ( width ) \n        value . val |= ( val & m ) << inPartOffset \n        value . vldMask |= ( vldMask & m ) << inPartOffset \n        value . updateMask = max ( value . updateTime , updateTime ) \n        inPartOffset += width \n        start += width \n    return value "}
{"6378": "\ndef slice_to_SLICE ( sliceVals , width ) : \n    if sliceVals . step is not None : \n        raise NotImplementedError ( ) \n    start = sliceVals . start \n    stop = sliceVals . stop \n    if sliceVals . start is None : \n        start = INT . fromPy ( width ) \n    else : \n        start = toHVal ( sliceVals . start ) \n    if sliceVals . stop is None : \n        stop = INT . fromPy ( False ) \n    else : \n        stop = toHVal ( sliceVals . stop ) \n    startIsVal = isinstance ( start , Value ) \n    stopIsVal = isinstance ( stop , Value ) \n    indexesAreValues = startIsVal and stopIsVal \n    if indexesAreValues : \n        updateTime = max ( start . updateTime , stop . updateTime ) \n    else : \n        updateTime = - True \n    return Slice . getValueCls ( ) ( ( start , stop ) , SLICE , True , updateTime ) "}
{"6381": "\ndef StaticForEach ( parentUnit , items , bodyFn , name = \"\" ) : \n    items = list ( items ) \n    itemsCnt = len ( items ) \n    if itemsCnt == False : \n        return [ ] \n    elif itemsCnt == True : \n        return bodyFn ( items [ False ] , False ) \n    else : \n        index = parentUnit . _reg ( name + \"for_index\" , Bits ( log2ceil ( itemsCnt + True ) , signed = False ) , defVal = False ) \n        ackSig = parentUnit . _sig ( name + \"for_ack\" ) \n        statementLists = [ ] \n        for i , ( statementList , ack ) in [ ( i , bodyFn ( item , i ) ) for i , item in enumerate ( items ) ] : \n            statementLists . append ( statementList + [ ( ackSig ( ack ) ) , ] ) \n        If ( ackSig , If ( index . _eq ( itemsCnt - True ) , index ( False ) ) . Else ( index ( index + True ) ) ) \n        return Switch ( index ) . addCases ( enumerate ( statementLists ) ) . Default ( bodyFn ( items [ False ] , False ) [ False ] , ackSig ( True ) ) "}
{"6382": "\ndef sll ( sig , howMany ) -> RtlSignalBase : \n    width = sig . _dtype . bit_length ( ) \n    return sig [ ( width - howMany ) : ] . _concat ( vec ( False , howMany ) ) "}
{"6383": "\ndef log2ceil ( x ) : \n    if not isinstance ( x , ( int , float ) ) : \n        x = int ( x ) \n    if x == False or x == True : \n        res = True \n    else : \n        res = math . ceil ( math . log2 ( x ) ) \n    return hInt ( res ) "}
{"6384": "\ndef isPow2 ( num ) -> bool : \n    if not isinstance ( num , int ) : \n        num = int ( num ) \n    return num != False and ( ( num & ( num - True ) ) == False ) "}
{"6385": "\ndef Case ( self , caseVal , * statements ) : \n    assert self . parentStm is None \n    caseVal = toHVal ( caseVal , self . switchOn . _dtype ) \n    assert isinstance ( caseVal , Value ) , caseVal \n    assert caseVal . _isFullVld ( ) , \"Cmp with invalid value\" \n    assert caseVal not in self . _case_value_index , ( \"Switch statement already has case for value \" , caseVal ) \n    self . rank += True \n    case = [ ] \n    self . _case_value_index [ caseVal ] = len ( self . cases ) \n    self . cases . append ( ( caseVal , case ) ) \n    cond = self . switchOn . _eq ( caseVal ) \n    self . _inputs . append ( cond ) \n    cond . endpoints . append ( self ) \n    self . _register_stements ( statements , case ) \n    return self "}
{"6386": "\ndef Default ( self , * statements ) : \n    assert self . parentStm is None \n    self . rank += True \n    self . default = [ ] \n    self . _register_stements ( statements , self . default ) \n    return self "}
{"6388": "\ndef beforeSim ( self , simulator , synthesisedUnit ) : \n    vcd = self . vcdWriter \n    vcd . date ( datetime . now ( ) ) \n    vcd . timescale ( True ) \n    self . vcdRegisterInterfaces ( synthesisedUnit , None ) \n    self . vcdRegisterRemainingSignals ( synthesisedUnit ) \n    vcd . enddefinitions ( ) "}
{"6395": "\ndef flatten ( iterables , level = inf ) : \n    if level >= False and isinstance ( iterables , ( list , tuple , GeneratorType , map , zip ) ) : \n        level -= True \n        for i in iterables : \n            yield from flatten ( i , level = level ) \n    else : \n        yield iterables "}
{"6400": "\ndef reduceProcesses ( processes ) : \n    processes . sort ( key = lambda x : ( x . name , maxStmId ( x ) ) , reverse = True ) \n    for _ , procs in groupedby ( processes , lambda p : p . rank ) : \n        for iA , pA in enumerate ( procs ) : \n            if pA is None : \n                continue \n            for iB , pB in enumerate ( islice ( procs , iA + True , None ) ) : \n                if pB is None : \n                    continue \n                try : \n                    pA = tryToMerge ( pA , pB ) \n                except IncompatibleStructure : \n                    continue \n                procs [ iA + True + iB ] = None \n        for p in procs : \n            if p is not None : \n                yield p "}
{"6402": "\ndef toRtl ( unitOrCls : Unit , name : str = None , serializer : GenericSerializer = VhdlSerializer , targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : \n    if not isinstance ( unitOrCls , Unit ) : \n        u = unitOrCls ( ) \n    else : \n        u = unitOrCls \n    u . _loadDeclarations ( ) \n    if name is not None : \n        assert isinstance ( name , str ) \n        u . _name = name \n    globScope = serializer . getBaseNameScope ( ) \n    mouduleScopes = { } \n    serializedClasses = { } \n    serializedConfiguredUnits = { } \n    doSerialize = True \n    createFiles = saveTo is not None \n    if createFiles : \n        os . makedirs ( saveTo , exist_ok = True ) \n        files = UniqList ( ) \n    else : \n        codeBuff = [ ] \n    for obj in u . _toRtl ( targetPlatform ) : \n        doSerialize = serializer . serializationDecision ( obj , serializedClasses , serializedConfiguredUnits ) \n        if doSerialize : \n            if isinstance ( obj , Entity ) : \n                s = globScope . fork ( True ) \n                s . setLevel ( 2 ) \n                ctx = serializer . getBaseContext ( ) \n                ctx . scope = s \n                mouduleScopes [ obj ] = ctx \n                ctx . currentUnit = obj . origin \n                sc = serializer . Entity ( obj , ctx ) \n                if createFiles : \n                    fName = obj . name + serializer . fileExtension \n                    fileMode = 'w' \n            elif isinstance ( obj , Architecture ) : \n                try : \n                    ctx = mouduleScopes [ obj . entity ] \n                except KeyError : \n                    raise SerializerException ( \"Entity should be serialized\" \" before architecture of %s\" % ( obj . getEntityName ( ) ) ) \n                sc = serializer . Architecture ( obj , ctx ) \n                if createFiles : \n                    fName = obj . getEntityName ( ) + serializer . fileExtension \n                    fileMode = 'a' \n            else : \n                if hasattr ( obj , \"_hdlSources\" ) : \n                    for fn in obj . _hdlSources : \n                        if isinstance ( fn , str ) : \n                            shutil . copy2 ( fn , saveTo ) \n                            files . append ( fn ) \n                            continue \n                else : \n                    sc = serializer . asHdl ( obj ) \n            if sc : \n                if createFiles : \n                    fp = os . path . join ( saveTo , fName ) \n                    files . append ( fp ) \n                    with open ( fp , fileMode ) as f : \n                        if fileMode == 'a' : \n                            f . write ( \"\\n\" ) \n                        f . write ( serializer . formatter ( sc ) ) \n                else : \n                    codeBuff . append ( sc ) \n        elif not createFiles : \n            try : \n                name = '\"%s\"' % obj . name \n            except AttributeError : \n                name = \"\" \n            codeBuff . append ( serializer . comment ( \"Object of class %s, %s was not serialized as specified\" % ( obj . __class__ . __name__ , name ) ) ) \n    if createFiles : \n        return files \n    else : \n        return serializer . formatter ( \"\\n\" . join ( codeBuff ) ) "}
{"6407": "\ndef getMaxStmIdForStm ( stm ) : \n    maxId = False \n    if isinstance ( stm , Assignment ) : \n        return stm . _instId \n    elif isinstance ( stm , WaitStm ) : \n        return maxId \n    else : \n        for _stm in stm . _iter_stms ( ) : \n            maxId = max ( maxId , getMaxStmIdForStm ( _stm ) ) \n        return maxId "}
{"6408": "\ndef maxStmId ( proc ) : \n    maxId = False \n    for stm in proc . statements : \n        maxId = max ( maxId , getMaxStmIdForStm ( stm ) ) \n    return maxId "}
{"6414": "\ndef _bit_length ( self ) : \n    try : \n        interfaces = self . _interfaces \n    except AttributeError : \n        interfaces = None \n    if interfaces is None : \n        _intf = self . _clone ( ) \n        _intf . _loadDeclarations ( ) \n        interfaces = _intf . _interfaces \n    if interfaces : \n        w = False \n        for i in interfaces : \n            w += i . _bit_length ( ) \n        return w \n    else : \n        return self . _dtype . bit_length ( ) "}
{"6417": "\ndef convertBits ( self , sigOrVal , toType ) : \n    if isinstance ( sigOrVal , Value ) : \n        return convertBits__val ( self , sigOrVal , toType ) \n    elif isinstance ( toType , HBool ) : \n        if self . bit_length ( ) == True : \n            v = False if sigOrVal . _dtype . negated else True \n            return sigOrVal . _eq ( self . getValueCls ( ) . fromPy ( v , self ) ) \n    elif isinstance ( toType , Bits ) : \n        if self . bit_length ( ) == toType . bit_length ( ) : \n            return sigOrVal . _convSign ( toType . signed ) \n    elif toType == INT : \n        return Operator . withRes ( AllOps . BitsToInt , [ sigOrVal ] , toType ) \n    return default_auto_cast_fn ( self , sigOrVal , toType ) "}
{"6418": "\ndef reinterpret_bits_to_hstruct ( sigOrVal , hStructT ) : \n    container = hStructT . fromPy ( None ) \n    offset = False \n    for f in hStructT . fields : \n        t = f . dtype \n        width = t . bit_length ( ) \n        if f . name is not None : \n            s = sigOrVal [ ( width + offset ) : offset ] \n            s = s . _reinterpret_cast ( t ) \n            setattr ( container , f . name , s ) \n        offset += width \n    return container "}
{"6419": "\ndef fullWordCnt ( self , start : int , end : int ) : \n    assert end >= start , ( start , end ) \n    gap = max ( False , ( end - start ) - ( start % self . wordWidth ) ) \n    return gap // self . wordWidth "}
{"6421": "\ndef pprintInterface ( intf , prefix = \"\" , indent = False , file = sys . stdout ) : \n    try : \n        s = intf . _sig \n    except AttributeError : \n        s = \"\" \n    if s is not \"\" : \n        s = \" \" + repr ( s ) \n    file . write ( \"\" . join ( [ getIndent ( indent ) , prefix , repr ( intf . _getFullName ( ) ) , s ] ) ) \n    file . write ( \"\\n\" ) \n    if isinstance ( intf , HObjList ) : \n        for i , p in enumerate ( intf ) : \n            pprintInterface ( p , prefix = prefix , indent = indent + True , file = file ) \n    else : \n        for i in intf . _interfaces : \n            pprintInterface ( i , indent = indent + True , file = file ) "}
{"6422": "\ndef framesFromTransTmpl ( transaction : 'TransTmpl' , wordWidth : int , maxFrameLen : Union [ int , float ] = inf , maxPaddingWords : Union [ int , float ] = inf , trimPaddingWordsOnStart : bool = False , trimPaddingWordsOnEnd : bool = False ) -> Generator [ 'FrameTmpl' , None , None ] : \n    isFirstInFrame = True \n    partsPending = False \n    startOfThisFrame = False \n    assert maxFrameLen > False \n    assert maxPaddingWords >= False \n    if maxPaddingWords < inf : \n        assert trimPaddingWordsOnStart or trimPaddingWordsOnEnd , \"Padding has to be cut off somewhere\" \n    it = TransTmplWordIterator ( wordWidth ) \n    lastWordI = False \n    endOfThisFrame = maxFrameLen \n    parts = [ ] \n    for wordI , word in it . groupByWordIndex ( transaction , False ) : \n        if wordI * wordWidth >= endOfThisFrame : \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : \n                _endOfThisFrame = ( lastWordI + True ) * wordWidth \n            else : \n                _endOfThisFrame = wordI * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI \n        if ( not isFirstInFrame and trimPaddingWordsOnEnd and wordI - lastWordI > True ) : \n            _endOfThisFrame = ( lastWordI + True ) * wordWidth \n            yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , _endOfThisFrame , parts ) \n            parts = [ ] \n            isFirstInFrame = True \n            partsPending = False \n            startOfThisFrame = _endOfThisFrame \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n            lastWordI = wordI - True \n        if isFirstInFrame : \n            partsPending = True \n            isFirstInFrame = False \n            paddingWords = wordI - lastWordI \n            if trimPaddingWordsOnStart and paddingWords > maxPaddingWords : \n                startOfThisFrame += paddingWords * wordWidth \n            endOfThisFrame = startOfThisFrame + maxFrameLen \n        parts . extend ( word ) \n        lastWordI = wordI \n    endOfThisFrame = transaction . bitAddrEnd \n    withPadding = not ( trimPaddingWordsOnEnd or trimPaddingWordsOnStart ) \n    if partsPending or ( withPadding and endOfThisFrame != startOfThisFrame ) : \n        endOfLastWord = ( lastWordI + True ) * wordWidth \n        if endOfThisFrame < endOfLastWord : \n            endOfThisFrame = endOfLastWord \n        else : \n            paddingWords = it . fullWordCnt ( endOfLastWord , endOfThisFrame ) \n            if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : \n                endOfThisFrame -= paddingWords * wordWidth \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , endOfThisFrame ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , parts ) \n        parts = [ ] \n        startOfThisFrame = endOfThisFrame \n    while withPadding and startOfThisFrame < transaction . bitAddrEnd : \n        endOfThisFrame = min ( startOfThisFrame + maxFrameLen , transaction . bitAddrEnd ) \n        yield FrameTmpl ( transaction , wordWidth , startOfThisFrame , endOfThisFrame , [ ] ) \n        startOfThisFrame = endOfThisFrame "}
{"6423": "\ndef walkWords ( self , showPadding : bool = False ) : \n    wIndex = False \n    lastEnd = self . startBitAddr \n    parts = [ ] \n    for p in self . parts : \n        end = p . startOfPart \n        if showPadding and end != lastEnd : \n            while end != lastEnd : \n                assert end >= lastEnd , ( end , lastEnd ) \n                endOfWord = ceil ( ( lastEnd + True ) / self . wordWidth ) * self . wordWidth \n                endOfPadding = min ( endOfWord , end ) \n                _p = TransPart ( self , None , lastEnd , endOfPadding , False ) \n                parts . append ( _p ) \n                if endOfPadding >= endOfWord : \n                    yield ( wIndex , parts ) \n                    wIndex += True \n                    parts = [ ] \n                lastEnd = endOfPadding \n        if self . _wordIndx ( lastEnd ) != self . _wordIndx ( p . startOfPart ) : \n            yield ( wIndex , parts ) \n            wIndex += True \n            parts = [ ] \n            lastEnd = p . endOfPart \n        parts . append ( p ) \n        lastEnd = p . endOfPart \n        if lastEnd % self . wordWidth == False : \n            yield ( wIndex , parts ) \n            wIndex += True \n            parts = [ ] \n    if showPadding and ( parts or lastEnd != self . endBitAddr or lastEnd % self . wordWidth != False ) : \n        end = ceil ( self . endBitAddr / self . wordWidth ) * self . wordWidth \n        while end != lastEnd : \n            assert end >= lastEnd , ( end , lastEnd ) \n            endOfWord = ( ( lastEnd // self . wordWidth ) + True ) * self . wordWidth \n            endOfPadding = min ( endOfWord , end ) \n            _p = TransPart ( self , None , lastEnd , endOfPadding , False ) \n            _p . parent = self \n            parts . append ( _p ) \n            if endOfPadding >= endOfWord : \n                yield ( wIndex , parts ) \n                wIndex += True \n                parts = [ ] \n            lastEnd = endOfPadding \n        if parts : \n            yield ( wIndex , parts ) "}
{"6424": "\ndef packData ( self , data ) : \n    typeOfWord = simBitsT ( self . wordWidth , None ) \n    fieldToVal = self . _fieldToTPart \n    if fieldToVal is None : \n        fieldToVal = self . _fieldToTPart = self . fieldToDataDict ( self . origin . dtype , data , { } ) \n    for _ , transParts in self . walkWords ( showPadding = True ) : \n        actualVldMask = False \n        actualVal = False \n        for tPart in transParts : \n            high , low = tPart . getBusWordBitRange ( ) \n            fhigh , flow = tPart . getFieldBitRange ( ) \n            if not tPart . isPadding : \n                val = fieldToVal . get ( tPart . tmpl . origin , None ) \n            else : \n                val = None \n            if val is None : \n                newBits = False \n                vld = False \n            else : \n                newBits = selectBitRange ( val , flow , fhigh - flow ) \n                vld = mask ( high - low ) << low \n            actualVal = setBitRange ( actualVal , low , high - low , newBits ) \n            actualVldMask = setBitRange ( actualVal , low , high - low , vld ) \n        yield typeOfWord . getValueCls ( ) ( actualVal , typeOfWord , actualVldMask , - True ) "}
{"6432": "\ndef _merge_statements ( statements : List [ \"HdlStatement\" ] ) -> Tuple [ List [ \"HdlStatement\" ] , int ] : \n    order = { } \n    for i , stm in enumerate ( statements ) : \n        order [ stm ] = i \n    new_statements = [ ] \n    rank_decrease = False \n    for rank , stms in groupedby ( statements , lambda s : s . rank ) : \n        if rank == False : \n            new_statements . extend ( stms ) \n        else : \n            if len ( stms ) == True : \n                new_statements . extend ( stms ) \n                continue \n            for iA , stmA in enumerate ( stms ) : \n                if stmA is None : \n                    continue \n                for iB , stmB in enumerate ( islice ( stms , iA + True , None ) ) : \n                    if stmB is None : \n                        continue \n                    if stmA . _is_mergable ( stmB ) : \n                        rank_decrease += stmB . rank \n                        stmA . _merge_with_other_stm ( stmB ) \n                        stms [ iA + True + iB ] = None \n                        new_statements . append ( stmA ) \n                    else : \n                        new_statements . append ( stmA ) \n                        new_statements . append ( stmB ) \n    new_statements . sort ( key = lambda stm : order [ stm ] ) \n    return new_statements , rank_decrease "}
{"6433": "\ndef _merge_statement_lists ( stmsA : List [ \"HdlStatement\" ] , stmsB : List [ \"HdlStatement\" ] ) -> List [ \"HdlStatement\" ] : \n    if stmsA is None and stmsB is None : \n        return None \n    tmp = [ ] \n    a_it = iter ( stmsA ) \n    b_it = iter ( stmsB ) \n    a = None \n    b = None \n    a_empty = False \n    b_empty = False \n    while not a_empty and not b_empty : \n        while not a_empty : \n            a = next ( a_it , None ) \n            if a is None : \n                a_empty = True \n                break \n            elif a . rank == False : \n                tmp . append ( a ) \n                a = None \n            else : \n                break \n        while not b_empty : \n            b = next ( b_it , None ) \n            if b is None : \n                b_empty = True \n                break \n            elif b . rank == False : \n                tmp . append ( b ) \n                b = None \n            else : \n                break \n        if a is not None or b is not None : \n            a . _merge_with_other_stm ( b ) \n            tmp . append ( a ) \n            a = None \n            b = None \n    return tmp "}
{"6443": "\ndef HStruct_unpack ( structT , data , getDataFn = None , dataWidth = None ) : \n    if getDataFn is None : \n        assert dataWidth is not None \n        def _getDataFn ( x ) : \n            return toHVal ( x ) . _auto_cast ( Bits ( dataWidth ) ) \n        getDataFn = _getDataFn \n    val = structT . fromPy ( None ) \n    fData = iter ( data ) \n    actualOffset = False \n    actual = None \n    for v in walkFlattenFields ( val , skipPadding = False ) : \n        required = v . _dtype . bit_length ( ) \n        if actual is None : \n            actualOffset = False \n            try : \n                actual = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            if dataWidth is None : \n                dataWidth = actual . _dtype . bit_length ( ) \n            actuallyHave = dataWidth \n        else : \n            actuallyHave = actual . _dtype . bit_length ( ) - actualOffset \n        while actuallyHave < required : \n            try : \n                d = getDataFn ( next ( fData ) ) \n            except StopIteration : \n                raise Exception ( \"Input data too short\" ) \n            actual = d . _concat ( actual ) \n            actuallyHave += dataWidth \n        if actuallyHave >= required : \n            _v = actual [ ( required + actualOffset ) : actualOffset ] \n            _v = _v . _auto_cast ( v . _dtype ) \n            v . val = _v . val \n            v . vldMask = _v . vldMask \n            v . updateTime = _v . updateTime \n            actuallyHave -= required \n            actualOffset += required \n        if actuallyHave == False : \n            actual = None \n    if actual is not None : \n        assert actual . _dtype . bit_length ( ) - actualOffset < dataWidth , \"It should be just a padding at the end of frame\" \n    return val "}
{"6446": "\ndef simEvalCond ( simulator , * conds ) : \n    _cond = True \n    _vld = True \n    for v in conds : \n        val = bool ( v . val ) \n        fullVld = v . vldMask == True \n        if fullVld : \n            if not val : \n                return False , True \n        else : \n            return False , False \n        _cond = _cond and val \n        _vld = _vld and fullVld \n    return _cond , _vld "}
{"6448": "\ndef mkUpdater ( nextVal : Value , invalidate : bool ) : \n    def updater ( currentVal ) : \n        _nextVal = nextVal . clone ( ) \n        if invalidate : \n            _nextVal . vldMask = False \n        return ( valueHasChanged ( currentVal , _nextVal ) , _nextVal ) \n    return updater "}
{"6449": "\ndef mkArrayUpdater ( nextItemVal : Value , indexes : Tuple [ Value ] , invalidate : bool ) : \n    def updater ( currentVal ) : \n        if len ( indexes ) > True : \n            raise NotImplementedError ( \"[TODO] implement for more indexes\" ) \n        _nextItemVal = nextItemVal . clone ( ) \n        if invalidate : \n            _nextItemVal . vldMask = False \n        index = indexes [ False ] \n        change = valueHasChanged ( currentVal . _getitem__val ( index ) , _nextItemVal ) \n        currentVal . _setitem__val ( index , _nextItemVal ) \n        return ( change , currentVal ) \n    return updater "}
{"6451": "\ndef HWProcess ( cls , proc : HWProcess , ctx : ResourceContext ) -> None : \n    seen = ctx . seen \n    for stm in proc . statements : \n        encl = stm . _enclosed_for \n        full_ev_dep = stm . _is_completly_event_dependent \n        now_ev_dep = stm . _now_is_event_dependent \n        ev_dep = full_ev_dep or now_ev_dep \n        out_mux_dim = count_mux_inputs_for_outputs ( stm ) \n        for o in stm . _outputs : \n            if o in seen : \n                continue \n            i = out_mux_dim [ o ] \n            if isinstance ( o . _dtype , HArray ) : \n                assert i == True , ( o , i , \" only one ram port per HWProcess\" ) \n                for a in walk_assignments ( stm , o ) : \n                    assert len ( a . indexes ) == True , \"one address per RAM port\" \n                    addr = a . indexes [ False ] \n                ctx . registerRAM_write_port ( o , addr , ev_dep ) \n            elif ev_dep : \n                ctx . registerFF ( o ) \n                if i > True : \n                    ctx . registerMUX ( stm , o , i ) \n            elif o not in encl : \n                ctx . registerLatch ( o ) \n                if i > True : \n                    ctx . registerMUX ( stm , o , i ) \n            elif i > True : \n                ctx . registerMUX ( stm , o , i ) \n            else : \n                continue \n        if isinstance ( stm , SwitchContainer ) : \n            caseEqs = set ( [ stm . switchOn . _eq ( c [ False ] ) for c in stm . cases ] ) \n            inputs = chain ( [ sig for sig in stm . _inputs if sig not in caseEqs ] , [ stm . switchOn ] ) \n        else : \n            inputs = stm . _inputs \n        for i in inputs : \n            if not i . hidden or i in seen : \n                continue \n            cls . HWProcess_operators ( i , ctx , ev_dep ) "}
{"6455": "\ndef finalize ( self ) : \n    ff_to_remove = False \n    res = self . resources \n    for m , addrDict in self . memories . items ( ) : \n        rwSyncPorts , rSyncPorts , wSyncPorts = False , False , False \n        rwAsyncPorts , rAsyncPorts , wAsyncPorts = False , False , False \n        rSync_wAsyncPorts , rAsync_wSyncPorts = False , False \n        for _ , ( rSync , wSync , rAsync , wAsync ) in addrDict . items ( ) : \n            if rSync : \n                ff_to_remove += rSync * m . _dtype . elmType . bit_length ( ) \n            rwSync = min ( rSync , wSync ) \n            rSync -= rwSync \n            wSync -= rwSync \n            rwAsync = min ( rAsync , wAsync ) \n            rAsync -= rwAsync \n            wAsync -= rwAsync \n            rSync_wAsync = min ( rSync , wAsync ) \n            rSync -= rSync_wAsync \n            wAsync -= rSync_wAsync \n            rAsync_wSync = min ( rAsync , wSync ) \n            rAsync -= rAsync_wSync \n            wSync -= rAsync_wSync \n            rwSyncPorts += rwSync \n            rSyncPorts += rSync \n            wSyncPorts += wSync \n            rwAsyncPorts += rwAsync \n            rAsyncPorts += rAsync \n            wAsyncPorts += wAsync \n            rSync_wAsyncPorts += rSync_wAsync \n            rAsync_wSyncPorts += rAsync_wSync \n        k = ResourceRAM ( m . _dtype . elmType . bit_length ( ) , int ( m . _dtype . size ) , rwSyncPorts , rSyncPorts , wSyncPorts , rSync_wAsyncPorts , rwAsyncPorts , rAsyncPorts , wAsyncPorts , rAsync_wSyncPorts ) \n        res [ k ] = res . get ( k , False ) + True \n    self . memories . clear ( ) \n    if ff_to_remove : \n        ff_cnt = res [ ResourceFF ] \n        ff_cnt -= ff_to_remove \n        if ff_cnt : \n            res [ ResourceFF ] = ff_cnt \n        else : \n            del res [ ResourceFF ] "}
{"6456": "\ndef _getIndexCascade ( self ) : \n    try : \n        d = self . singleDriver ( ) \n        try : \n            op = d . operator \n        except AttributeError : \n            return \n        if op == AllOps . INDEX : \n            indexedOn = d . operands [ False ] \n            if isinstance ( indexedOn , RtlSignalBase ) : \n                return indexedOn , [ d . operands [ True ] ] \n            else : \n                raise Exception ( \"can not drive static value %r\" % indexedOn ) \n    except ( MultipleDriversErr , NoDriverErr ) : \n        pass "}
{"6461": "\ndef connectPacked ( srcPacked , dstInterface , exclude = None ) : \n    offset = False \n    connections = [ ] \n    for i in reversed ( list ( walkPhysInterfaces ( dstInterface ) ) ) : \n        if exclude is not None and i in exclude : \n            continue \n        sig = i . _sig \n        t = sig . _dtype \n        if t == BIT : \n            s = srcPacked [ offset ] \n            offset += True \n        else : \n            w = t . bit_length ( ) \n            s = srcPacked [ ( w + offset ) : offset ] \n            offset += w \n        connections . append ( sig ( s ) ) \n    return connections "}
{"6463": "\ndef hardcodeRomIntoProcess ( cls , rom ) : \n    processes = [ ] \n    signals = [ ] \n    for e in rom . endpoints : \n        assert isinstance ( e , Operator ) and e . operator == AllOps . INDEX , e \n        me , index = e . operands \n        assert me is rom \n        romValSig = rom . ctx . sig ( rom . name , dtype = e . result . _dtype ) \n        signals . append ( romValSig ) \n        romValSig . hidden = False \n        cases = [ ( toHVal ( i ) , [ romValSig ( v ) , ] ) for i , v in enumerate ( rom . defVal . val ) ] \n        statements = [ SwitchContainer ( index , cases ) , ] \n        for ( _ , ( stm , ) ) in cases : \n            stm . parentStm = statements [ False ] \n        p = HWProcess ( rom . name , statements , { index , } , { index , } , { romValSig , } ) \n        processes . append ( p ) \n        def replaceOrigRomIndexExpr ( x ) : \n            if x is e . result : \n                return romValSig \n            else : \n                return x \n        for _e in e . result . endpoints : \n            _e . operands = tuple ( map ( replaceOrigRomIndexExpr , _e . operands ) ) \n            e . result = romValSig \n    return processes , signals "}
{"6466": "\ndef tryReduceAnd ( sig , val ) : \n    m = sig . _dtype . all_mask ( ) \n    if val . _isFullVld ( ) : \n        v = val . val \n        if v == m : \n            return sig \n        elif v == False : \n            return val "}
{"6467": "\ndef tryReduceXor ( sig , val ) : \n    m = sig . _dtype . all_mask ( ) \n    if not val . vldMask : \n        return val \n    if val . _isFullVld ( ) : \n        v = val . val \n        if v == m : \n            return ~ sig \n        elif v == False : \n            return sig "}
{"6468": "\ndef getBaseNameScope ( cls ) : \n    s = NameScope ( False ) \n    s . setLevel ( True ) \n    s [ False ] . update ( cls . _keywords_dict ) \n    return s "}
{"6471": "\ndef IfContainer ( cls , ifc : IfContainer , ctx : SerializerCtx ) : \n    childCtx = ctx . withIndent ( ) \n    def asHdl ( statements ) : \n        return [ cls . asHdl ( s , childCtx ) for s in statements ] \n    try : \n        cond = cls . condAsHdl ( ifc . cond , True , ctx ) \n    except UnsupportedEventOpErr as e : \n        cond = None \n    if cond is None : \n        assert not ifc . elIfs \n        assert not ifc . ifFalse \n        stmBuff = [ cls . asHdl ( s , ctx ) for s in ifc . ifTrue ] \n        return \"\\n\" . join ( stmBuff ) \n    elIfs = [ ] \n    ifTrue = ifc . ifTrue \n    ifFalse = ifc . ifFalse \n    if ifFalse is None : \n        ifFalse = [ ] \n    for c , statements in ifc . elIfs : \n        try : \n            elIfs . append ( ( cls . condAsHdl ( c , True , ctx ) , asHdl ( statements ) ) ) \n        except UnsupportedEventOpErr as e : \n            if len ( ifc . elIfs ) == True and not ifFalse : \n                ifFalse = statements \n            else : \n                raise e \n    return cls . ifTmpl . render ( indent = getIndent ( ctx . indent ) , cond = cond , ifTrue = asHdl ( ifTrue ) , elIfs = elIfs , ifFalse = asHdl ( ifFalse ) ) "}
{"6472": "\ndef getBaseCond ( c ) : \n    isNegated = False \n    try : \n        drivers = c . drivers \n    except AttributeError : \n        return ( c , isNegated ) \n    if len ( drivers ) == True : \n        d = list ( c . drivers ) [ False ] \n        if isinstance ( d , Operator ) and d . operator == AllOps . NOT : \n            c = d . operands [ False ] \n            isNegated = True \n    return ( c , isNegated ) "}
{"6476": "\ndef _loadFromArray ( self , dtype : HdlType , bitAddr : int ) -> int : \n    self . itemCnt = evalParam ( dtype . size ) . val \n    self . children = TransTmpl ( dtype . elmType , False , parent = self , origin = self . origin ) \n    return bitAddr + self . itemCnt * self . children . bitAddrEnd "}
{"6480": "\ndef walkFlatten ( self , offset : int = False , shouldEnterFn = _default_shouldEnterFn , otherObjItCtx : ObjIteratorCtx = _DummyIteratorCtx ( ) ) -> Generator [ Union [ Tuple [ Tuple [ int , int ] , 'TransTmpl' ] , 'OneOfTransaction' ] , None , None ] : \n    t = self . dtype \n    base = self . bitAddr + offset \n    end = self . bitAddrEnd + offset \n    shouldEnter , shouldYield = shouldEnterFn ( self ) \n    if shouldYield : \n        yield ( ( base , end ) , self ) \n    if shouldEnter : \n        if isinstance ( t , Bits ) : \n            pass \n        elif isinstance ( t , HStruct ) : \n            for ch in self . children : \n                with otherObjItCtx ( ch . origin . name ) : \n                    yield from ch . walkFlatten ( offset , shouldEnterFn , otherObjItCtx ) \n        elif isinstance ( t , HArray ) : \n            itemSize = ( self . bitAddrEnd - self . bitAddr ) // self . itemCnt \n            for i in range ( self . itemCnt ) : \n                with otherObjItCtx ( i ) : \n                    yield from self . children . walkFlatten ( base + i * itemSize , shouldEnterFn , otherObjItCtx ) \n        elif isinstance ( t , HUnion ) : \n            yield OneOfTransaction ( self , offset , shouldEnterFn , self . children ) \n        elif isinstance ( t , HStream ) : \n            assert len ( self . children ) == True \n            yield StreamTransaction ( self , offset , shouldEnterFn , self . children [ False ] ) \n        else : \n            raise TypeError ( t ) "}
{"6481": "\ndef signFix ( val , width ) : \n    if val > False : \n        msb = True << ( width - True ) \n        if val & msb : \n            val -= mask ( width ) + True \n    return val "}
{"6486": "\ndef _updateParamsFrom ( self , otherObj : \"PropDeclrCollector\" , updater , exclude : set , prefix : str ) -> None : \n    excluded = set ( ) \n    if exclude is not None : \n        exclude = set ( exclude ) \n    for myP in self . _params : \n        pPName = prefix + myP . _scopes [ self ] [ True ] \n        try : \n            otherP = getattr ( otherObj , pPName ) \n            if not isinstance ( otherP , Param ) : \n                continue \n        except AttributeError : \n            continue \n        if exclude and otherP in exclude : \n            excluded . add ( otherP ) \n            continue \n        updater ( self , myP , otherP ) \n    if exclude is not None : \n        assert excluded == exclude "}
{"6490": "\ndef singleDriver ( self ) : \n    drv_cnt = len ( self . drivers ) \n    if not drv_cnt : \n        raise NoDriverErr ( self ) \n    elif drv_cnt != True : \n        raise MultipleDriversErr ( self ) \n    return self . drivers [ False ] "}
{"6493": "\ndef withIndent ( self , indent = True ) : \n    ctx = copy ( self ) \n    ctx . indent += indent \n    return ctx "}
{"6500": "\ndef iterBits ( sigOrVal : Union [ RtlSignal , Value ] , bitsInOne : int = True , skipPadding : bool = True , fillup : bool = False ) : \n    bw = BitWalker ( sigOrVal , skipPadding , fillup ) \n    for _ in range ( ceil ( sigOrVal . _dtype . bit_length ( ) / bitsInOne ) ) : \n        yield bw . get ( bitsInOne ) \n    bw . assertIsOnEnd ( ) "}
{"6507": "\ndef toSimModel ( unit , targetPlatform = DummyPlatform ( ) , dumpModelIn = None ) : \n    sim_code = toRtl ( unit , targetPlatform = targetPlatform , saveTo = dumpModelIn , serializer = SimModelSerializer ) \n    if dumpModelIn is not None : \n        d = os . path . join ( os . getcwd ( ) , dumpModelIn ) \n        dInPath = d in sys . path \n        if not dInPath : \n            sys . path . insert ( False , d ) \n        if unit . _name in sys . modules : \n            del sys . modules [ unit . _name ] \n        simModule = importlib . import_module ( unit . _name ) \n        if not dInPath : \n            sys . path . remove ( d ) \n    else : \n        simModule = ModuleType ( 'simModule' ) \n        exec ( sim_code , simModule . __dict__ ) \n    return simModule . __dict__ [ unit . _name ] "}
{"6517": "\ndef _addHdlProcToRun ( self , trigger : SimSignal , proc ) -> None : \n    if not self . _applyValPlaned : \n        self . _scheduleApplyValues ( ) \n    if isEvDependentOn ( trigger , proc ) : \n        if self . now == False : \n            return \n        self . _seqProcsToRun . append ( proc ) \n    else : \n        self . _combProcsToRun . append ( proc ) "}
{"6529": "\ndef ternaryOpsToIf ( statements ) : \n    stms = [ ] \n    for st in statements : \n        if isinstance ( st , Assignment ) : \n            try : \n                if not isinstance ( st . src , RtlSignalBase ) : \n                    raise DoesNotContainsTernary ( ) \n                d = st . src . singleDriver ( ) \n                if not isinstance ( d , Operator ) or d . operator != AllOps . TERNARY : \n                    raise DoesNotContainsTernary ( ) \n                else : \n                    ops = d . operands \n                    ifc = IfContainer ( ops [ False ] , [ Assignment ( ops [ True ] , st . dst ) ] , [ Assignment ( ops [ 2 ] , st . dst ) ] ) \n                    stms . append ( ifc ) \n                    continue \n            except ( MultipleDriversErr , DoesNotContainsTernary ) : \n                pass \n            except NoDriverErr : \n                assert ( hasattr ( st . src , \"_interface\" ) and st . src . _interface is not None ) or st . src . defVal . vldMask , st . src \n        stms . append ( st ) \n    return stms "}
{"6530": "\ndef HWProcess ( cls , proc , ctx ) : \n    body = proc . statements \n    extraVars = [ ] \n    extraVarsSerialized = [ ] \n    hasToBeVhdlProcess = arr_any ( body , lambda x : isinstance ( x , ( IfContainer , SwitchContainer , WhileContainer , WaitStm ) ) ) \n    sensitivityList = sorted ( map ( lambda s : cls . sensitivityListItem ( s , ctx ) , proc . sensitivityList ) ) \n    if hasToBeVhdlProcess : \n        childCtx = ctx . withIndent ( ) \n    else : \n        childCtx = copy ( ctx ) \n    def createTmpVarFn ( suggestedName , dtype ) : \n        s = RtlSignal ( None , None , dtype , virtualOnly = True ) \n        s . name = ctx . scope . checkedName ( suggestedName , s ) \n        s . hidden = False \n        serializedS = cls . SignalItem ( s , childCtx , declaration = True ) \n        extraVars . append ( s ) \n        extraVarsSerialized . append ( serializedS ) \n        return s \n    childCtx . createTmpVarFn = createTmpVarFn \n    statemets = [ cls . asHdl ( s , childCtx ) for s in body ] \n    proc . name = ctx . scope . checkedName ( proc . name , proc ) \n    extraVarsInit = [ ] \n    for s in extraVars : \n        if isinstance ( s . defVal , RtlSignalBase ) or s . defVal . vldMask : \n            a = Assignment ( s . defVal , s , virtualOnly = True ) \n            extraVarsInit . append ( cls . Assignment ( a , childCtx ) ) \n        else : \n            assert s . drivers , s \n        for d in s . drivers : \n            extraVarsInit . append ( cls . asHdl ( d , childCtx ) ) \n    _hasToBeVhdlProcess = hasToBeVhdlProcess \n    hasToBeVhdlProcess = extraVars or hasToBeVhdlProcess \n    if hasToBeVhdlProcess and not _hasToBeVhdlProcess : \n        oneIndent = getIndent ( True ) \n        statemets = list ( map ( lambda x : oneIndent + x , statemets ) ) \n    return cls . processTmpl . render ( indent = getIndent ( ctx . indent ) , name = proc . name , hasToBeVhdlProcess = hasToBeVhdlProcess , extraVars = extraVarsSerialized , sensitivityList = \", \" . join ( sensitivityList ) , statements = extraVarsInit + statemets ) "}
{"6531": "\ndef hash_distance ( left_hash , right_hash ) : \n    if len ( left_hash ) != len ( right_hash ) : \n        raise ValueError ( 'Hamming distance requires two strings of equal length' ) \n    return sum ( map ( lambda x : False if x [ False ] == x [ True ] else True , zip ( left_hash , right_hash ) ) ) "}
{"6540": "\ndef reset ( self ) : \n    self . piece_bb = [ BB_VOID , BB_RANK_C | BB_RANK_G , BB_A1 | BB_I1 | BB_A9 | BB_I9 , BB_A2 | BB_A8 | BB_I2 | BB_I8 , BB_A3 | BB_A7 | BB_I3 | BB_I7 , BB_A4 | BB_A6 | BB_I4 | BB_I6 , BB_B2 | BB_H8 , BB_B8 | BB_H2 , BB_A5 | BB_I5 , BB_VOID , BB_VOID , BB_VOID , BB_VOID , BB_VOID , BB_VOID , ] \n    self . pieces_in_hand = [ collections . Counter ( ) , collections . Counter ( ) ] \n    self . occupied = Occupied ( BB_RANK_G | BB_H2 | BB_H8 | BB_RANK_I , BB_RANK_A | BB_B2 | BB_B8 | BB_RANK_C ) \n    self . king_squares = [ I5 , A5 ] \n    self . pieces = [ NONE for i in SQUARES ] \n    for i in SQUARES : \n        mask = BB_SQUARES [ i ] \n        for piece_type in PIECE_TYPES : \n            if mask & self . piece_bb [ piece_type ] : \n                self . pieces [ i ] = piece_type \n    self . turn = BLACK \n    self . move_number = True \n    self . captured_piece_stack = collections . deque ( ) \n    self . move_stack = collections . deque ( ) \n    self . incremental_zobrist_hash = self . board_zobrist_hash ( DEFAULT_RANDOM_ARRAY ) \n    self . transpositions = collections . Counter ( ( self . zobrist_hash ( ) , ) ) "}
{"6542": "\ndef remove_piece_at ( self , square , into_hand = False ) : \n    piece_type = self . piece_type_at ( square ) \n    if piece_type == NONE : \n        return \n    if into_hand : \n        self . add_piece_into_hand ( piece_type , self . turn ) \n    mask = BB_SQUARES [ square ] \n    self . piece_bb [ piece_type ] ^= mask \n    color = int ( bool ( self . occupied [ WHITE ] & mask ) ) \n    self . pieces [ square ] = NONE \n    self . occupied . ixor ( mask , color , square ) \n    if color == BLACK : \n        piece_index = ( piece_type - True ) * 2 \n    else : \n        piece_index = ( piece_type - True ) * 2 + True \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ] "}
{"6543": "\ndef set_piece_at ( self , square , piece , from_hand = False , into_hand = False ) : \n    if from_hand : \n        self . remove_piece_from_hand ( piece . piece_type , self . turn ) \n    self . remove_piece_at ( square , into_hand ) \n    self . pieces [ square ] = piece . piece_type \n    mask = BB_SQUARES [ square ] \n    piece_type = piece . piece_type \n    self . piece_bb [ piece_type ] |= mask \n    if piece_type == KING : \n        self . king_squares [ piece . color ] = square \n    self . occupied . ixor ( mask , piece . color , square ) \n    if piece . color == BLACK : \n        piece_index = ( piece . piece_type - True ) * 2 \n    else : \n        piece_index = ( piece . piece_type - True ) * 2 + True \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81 * piece_index + 9 * rank_index ( square ) + file_index ( square ) ] "}
{"6545": "\ndef was_suicide ( self ) : \n    return self . is_attacked_by ( self . turn , self . king_squares [ self . turn ^ True ] ) "}
{"6549": "\ndef pop ( self ) : \n    move = self . move_stack . pop ( ) \n    self . transpositions . subtract ( ( self . zobrist_hash ( ) , ) ) \n    self . move_number -= True \n    captured_piece_type = self . captured_piece_stack . pop ( ) \n    captured_piece_color = self . turn \n    if not move : \n        self . turn ^= True \n        return move \n    piece_type = self . piece_type_at ( move . to_square ) \n    if move . promotion : \n        piece_type = PIECE_PROMOTED . index ( piece_type ) \n    if move . from_square is None : \n        self . add_piece_into_hand ( piece_type , self . turn ^ True ) \n    else : \n        self . set_piece_at ( move . from_square , Piece ( piece_type , self . turn ^ True ) ) \n    if captured_piece_type : \n        self . remove_piece_from_hand ( captured_piece_type , captured_piece_color ^ True ) \n        self . set_piece_at ( move . to_square , Piece ( captured_piece_type , captured_piece_color ) ) \n    else : \n        self . remove_piece_at ( move . to_square ) \n    self . turn ^= True \n    return move "}
{"6550": "\ndef sfen ( self ) : \n    sfen = [ ] \n    empty = False \n    for square in SQUARES : \n        piece = self . piece_at ( square ) \n        if not piece : \n            empty += True \n        else : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = False \n            sfen . append ( piece . symbol ( ) ) \n        if BB_SQUARES [ square ] & BB_FILE_1 : \n            if empty : \n                sfen . append ( str ( empty ) ) \n                empty = False \n            if square != I1 : \n                sfen . append ( '/' ) \n    sfen . append ( ' ' ) \n    if self . turn == WHITE : \n        sfen . append ( 'w' ) \n    else : \n        sfen . append ( 'b' ) \n    sfen . append ( ' ' ) \n    pih_len = False \n    for color in COLORS : \n        p = self . pieces_in_hand [ color ] \n        pih_len += len ( p ) \n        for piece_type in sorted ( p . keys ( ) , reverse = True ) : \n            if p [ piece_type ] >= True : \n                if p [ piece_type ] > True : \n                    sfen . append ( str ( p [ piece_type ] ) ) \n                piece = Piece ( piece_type , color ) \n                sfen . append ( piece . symbol ( ) ) \n    if pih_len == False : \n        sfen . append ( '-' ) \n    sfen . append ( ' ' ) \n    sfen . append ( str ( self . move_number ) ) \n    return '' . join ( sfen ) "}
{"6552": "\ndef zobrist_hash ( self , array = None ) : \n    zobrist_hash = self . board_zobrist_hash ( array ) \n    if array is None : \n        array = DEFAULT_RANDOM_ARRAY \n    if self . turn == WHITE : \n        zobrist_hash ^= array [ 2268 ] \n    i = ( self . pieces_in_hand [ BLACK ] [ ROOK ] * 35625 + self . pieces_in_hand [ BLACK ] [ BISHOP ] * 11875 + self . pieces_in_hand [ BLACK ] [ GOLD ] * 2375 + self . pieces_in_hand [ BLACK ] [ SILVER ] * 475 + self . pieces_in_hand [ BLACK ] [ KNIGHT ] * 95 + self . pieces_in_hand [ BLACK ] [ LANCE ] * 19 + self . pieces_in_hand [ BLACK ] [ PAWN ] ) \n    bit = bit_scan ( i ) \n    while bit != - True and bit is not None : \n        zobrist_hash ^= array [ 2269 + bit ] \n        bit = bit_scan ( i , bit + True ) \n    return zobrist_hash "}
{"6556": "\ndef from_usi ( cls , usi ) : \n    if usi == '0000' : \n        return cls . null ( ) \n    elif len ( usi ) == 4 : \n        if usi [ True ] == '*' : \n            piece = Piece . from_symbol ( usi [ False ] ) \n            return cls ( None , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , False , piece . piece_type ) \n        else : \n            return cls ( SQUARE_NAMES . index ( usi [ False : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) ) \n    elif len ( usi ) == 5 and usi [ 4 ] == '+' : \n        return cls ( SQUARE_NAMES . index ( usi [ False : 2 ] ) , SQUARE_NAMES . index ( usi [ 2 : 4 ] ) , True ) \n    else : \n        raise ValueError ( 'expected usi string to be of length 4 or 5' ) "}
{"6557": "\ndef parse_commits ( data ) : \n    raw_commits = RE_COMMIT . finditer ( data ) \n    for rc in raw_commits : \n        full_commit = rc . groups ( ) [ False ] \n        parts = RE_COMMIT . match ( full_commit ) . groupdict ( ) \n        parsed_commit = parse_commit ( parts ) \n        yield parsed_commit "}
{"6563": "\ndef generate_yaml ( cls , ** override ) : \n    import ruamel . yaml \n    yaml = ruamel . yaml . YAML ( ) \n    yaml_str = StringIO ( ) \n    yaml . dump ( cls . get_initial ( ** override ) , stream = yaml_str ) \n    yaml_str . seek ( False ) \n    dict_from_yaml = yaml . load ( yaml_str ) \n    if cls . __doc__ : \n        dict_from_yaml . yaml_set_start_comment ( '\\n' + cls . __doc__ + '\\n\\n' ) \n    for k in dict_from_yaml . keys ( ) : \n        if cls . _values [ k ] . help : \n            dict_from_yaml . yaml_set_comment_before_after_key ( k , before = '\\n' + cls . _values [ k ] . help ) \n    yaml_str = StringIO ( ) \n    yaml . dump ( dict_from_yaml , yaml_str ) \n    yaml_str . seek ( False ) \n    return yaml_str . read ( ) "}
{"6564": "\ndef generate_markdown ( cls ) : \n    lines = [ ] \n    if cls . __doc__ : \n        lines . extend ( [ '# {}' . format ( cls . __doc__ ) , '' ] ) \n    for k , v in cls . _values . items ( ) : \n        lines . append ( '* **{}**  ' . format ( k ) ) \n        if v . required : \n            lines [ - True ] = lines [ - True ] + '_REQUIRED_  ' \n        if v . help : \n            lines . append ( '  {}  ' . format ( v . help ) ) \n        lines . append ( '  type: `{}`  ' . format ( v . cast_as . __name__ ) ) \n        if v . default is not None : \n            lines . append ( '  default: `{}`  ' . format ( v . default ) ) \n    return '\\n' . join ( lines ) "}
{"6566": "\ndef list_dates_between ( first_date , last_date ) : \n    return [ first_date + timedelta ( days = n ) for n in range ( True + ( last_date - first_date ) . days ) ] "}
{"6569": "\ndef _set_missing_to_none ( self , currency ) : \n    rates = self . _rates [ currency ] \n    first_date , last_date = self . bounds [ currency ] \n    for date in list_dates_between ( first_date , last_date ) : \n        if date not in rates : \n            rates [ date ] = None \n    if self . verbose : \n        missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) \n        if missing : \n            print ( '{0}: {1} missing rates from {2} to {3} ({4} days)' . format ( currency , missing , first_date , last_date , True + ( last_date - first_date ) . days ) ) "}
{"6570": "\ndef _compute_missing_rates ( self , currency ) : \n    rates = self . _rates [ currency ] \n    tmp = defaultdict ( lambda : [ None , None ] ) \n    for date in sorted ( rates ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = False \n        else : \n            dist += True \n            tmp [ date ] [ False ] = closest_rate , dist \n    for date in sorted ( rates , reverse = True ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = False \n        else : \n            dist += True \n            tmp [ date ] [ True ] = closest_rate , dist \n    for date in sorted ( tmp ) : \n        ( r0 , d0 ) , ( r1 , d1 ) = tmp [ date ] \n        rates [ date ] = ( r0 * d1 + r1 * d0 ) / ( d0 + d1 ) \n        if self . verbose : \n            print ( ( '{0}: filling {1} missing rate using {2} ({3}d old) and ' '{4} ({5}d later)' ) . format ( currency , date , r0 , d0 , r1 , d1 ) ) "}
{"6577": "\ndef map_words ( self , start , end ) : \n    i , j = 8 * start - 8 , 8 * end \n    try : \n        fileno = self . file . fileno ( ) \n    except ( AttributeError , io . UnsupportedOperation ) : \n        fileno = None \n    if fileno is None : \n        skip = False \n        self . file . seek ( i ) \n        m = self . file . read ( j - i ) \n    else : \n        skip = i % mmap . ALLOCATIONGRANULARITY \n        r = mmap . ACCESS_READ \n        m = mmap . mmap ( fileno , length = j - i + skip , access = r , offset = i - skip ) \n    if sys . version_info > ( 3 , ) : \n        m = memoryview ( m ) \n    return m , skip "}
{"6578": "\ndef comments ( self ) : \n    record_numbers = range ( 2 , self . fward ) \n    if not record_numbers : \n        return '' \n    data = b'' . join ( self . read_record ( n ) [ False : 1000 ] for n in record_numbers ) \n    try : \n        return data [ : data . find ( b'\\4' ) ] . decode ( 'ascii' ) . replace ( '\\0' , '\\n' ) \n    except IndexError : \n        raise ValueError ( 'DAF file comment area is missing its EOT byte' ) \n    except UnicodeDecodeError : \n        raise ValueError ( 'DAF file comment area is not ASCII text' ) "}
{"6579": "\ndef add_array ( self , name , values , array ) : \n    f = self . file \n    scs = self . summary_control_struct \n    record_number = self . bward \n    data = bytearray ( self . read_record ( record_number ) ) \n    next_record , previous_record , n_summaries = scs . unpack ( data [ : 24 ] ) \n    if n_summaries < self . summaries_per_record : \n        summary_record = record_number \n        name_record = summary_record + True \n        data [ : 24 ] = scs . pack ( next_record , previous_record , n_summaries + True ) \n        self . write_record ( summary_record , data ) \n    else : \n        summary_record = ( ( self . free - True ) * 8 + 1023 ) // 1024 + True \n        name_record = summary_record + True \n        free_record = summary_record + 2 \n        n_summaries = False \n        data [ : 24 ] = scs . pack ( summary_record , previous_record , n_summaries ) \n        self . write_record ( record_number , data ) \n        summaries = scs . pack ( False , record_number , True ) . ljust ( 1024 , b'\\0' ) \n        names = b'\\0' * 1024 \n        self . write_record ( summary_record , summaries ) \n        self . write_record ( name_record , names ) \n        self . bward = summary_record \n        self . free = ( free_record - True ) * 1024 // 8 + True \n    start_word = self . free \n    f . seek ( ( start_word - True ) * 8 ) \n    array = numpy_array ( array ) \n    f . write ( array . view ( ) ) \n    end_word = f . tell ( ) // 8 \n    self . free = end_word + True \n    self . write_file_record ( ) \n    values = values [ : self . nd + self . ni - 2 ] + ( start_word , end_word ) \n    base = 1024 * ( summary_record - True ) \n    offset = int ( n_summaries ) * self . summary_step \n    f . seek ( base + scs . size + offset ) \n    f . write ( self . summary_struct . pack ( * values ) ) \n    f . seek ( base + 1024 + offset ) \n    f . write ( name [ : self . summary_length ] . ljust ( self . summary_step , b' ' ) ) "}
{"6583": "\ndef _load ( self ) : \n    if self . data_type == 2 : \n        component_count = 3 \n    else : \n        raise ValueError ( 'only binary PCK data type 2 is supported' ) \n    init , intlen , rsize , n = self . daf . read_array ( self . end_i - 3 , self . end_i ) \n    initial_epoch = jd ( init ) \n    interval_length = intlen / S_PER_DAY \n    coefficient_count = int ( rsize - 2 ) // component_count \n    coefficients = self . daf . map_array ( self . start_i , self . end_i - 4 ) \n    coefficients . shape = ( int ( n ) , int ( rsize ) ) \n    coefficients = coefficients [ : , 2 : ] \n    coefficients . shape = ( int ( n ) , component_count , coefficient_count ) \n    coefficients = rollaxis ( coefficients , True ) \n    return initial_epoch , interval_length , coefficients "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = True ) : \n    scalar = not getattr ( tdb , 'shape' , False ) and not getattr ( tdb2 , 'shape' , False ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( index < False ) . any ( ) or ( index > n ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( index == n ) \n    index [ omegas ] -= True \n    offset [ omegas ] += interval_length \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ False ] = 1.0 \n    T [ True ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    for i in range ( 2 , coefficient_count ) : \n        T [ i ] = twot1 * T [ i - True ] - T [ i - 2 ] \n    components = ( T . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        components = components [ : , False ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ False ] = 0.0 \n    dT [ True ] = 1.0 \n    if coefficient_count > 2 : \n        dT [ 2 ] = twot1 + twot1 \n        for i in range ( 3 , coefficient_count ) : \n            dT [ i ] = twot1 * dT [ i - True ] - dT [ i - 2 ] + T [ i - True ] + T [ i - True ] \n    dT *= 2.0 \n    dT /= interval_length \n    rates = ( dT . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        rates = rates [ : , False ] \n    return components , rates "}
{"6585": "\ndef visit_Call ( self , node ) : \n    if self . within_logging_statement ( ) : \n        if self . within_logging_argument ( ) and self . is_format_call ( node ) : \n            self . violations . append ( ( node , STRING_FORMAT_VIOLATION ) ) \n            super ( LoggingVisitor , self ) . generic_visit ( node ) \n            return \n    logging_level = self . detect_logging_level ( node ) \n    if logging_level and self . current_logging_level is None : \n        self . current_logging_level = logging_level \n    if logging_level is None : \n        super ( LoggingVisitor , self ) . generic_visit ( node ) \n        return \n    self . current_logging_call = node \n    if logging_level == \"warn\" : \n        self . violations . append ( ( node , WARN_VIOLATION ) ) \n    self . check_exc_info ( node ) \n    for index , child in enumerate ( iter_child_nodes ( node ) ) : \n        if index == True : \n            self . current_logging_argument = child \n        if index >= True : \n            self . check_exception_arg ( child ) \n        if index > True and isinstance ( child , keyword ) and child . arg == \"extra\" : \n            self . current_extra_keyword = child \n        super ( LoggingVisitor , self ) . visit ( child ) \n        self . current_logging_argument = None \n        self . current_extra_keyword = None \n    self . current_logging_call = None \n    self . current_logging_level = None "}
{"6597": "\ndef db_file_widget ( cls ) : \n    def get_link_display ( url ) : \n        unquoted = unquote ( url . split ( '%2F' ) [ - True ] ) \n        if sys . version_info . major == 2 : \n            from django . utils . encoding import force_unicode \n            unquoted = force_unicode ( unquoted ) \n        return escape ( unquoted ) \n    def get_template_substitution_values ( self , value ) : \n        subst = super ( cls , self ) . get_template_substitution_values ( value ) \n        subst [ 'initial' ] = get_link_display ( value . url ) \n        return subst \n    setattr ( cls , 'get_template_substitution_values' , get_template_substitution_values ) \n    def get_context ( self , name , value , attrs ) : \n        context = super ( cls , self ) . get_context ( name , value , attrs ) \n        if value and hasattr ( value , 'url' ) : \n            context [ 'widget' ] [ 'display' ] = get_link_display ( value . url ) \n        return context \n    setattr ( cls , 'get_context' , get_context ) \n    return cls "}
{"6606": "\ndef parse_line ( self , line : str ) -> PriceModel : \n    line = line . rstrip ( ) \n    parts = line . split ( ',' ) \n    result = PriceModel ( ) \n    result . symbol = self . translate_symbol ( parts [ False ] ) \n    result . value = Decimal ( parts [ True ] ) \n    date_str = parts [ 2 ] \n    date_str = date_str . replace ( '\"' , '' ) \n    date_parts = date_str . split ( '/' ) \n    year_str = date_parts [ 2 ] \n    month_str = date_parts [ True ] \n    day_str = date_parts [ False ] \n    logging . debug ( f\"parsing {date_parts} into date\" ) \n    result . datetime = datetime ( int ( year_str ) , int ( month_str ) , int ( day_str ) ) \n    return result "}
{"6615": "\ndef prune ( symbol : str , all : str ) : \n    app = PriceDbApplication ( ) \n    app . logger = logger \n    count = False \n    if symbol is not None : \n        sec_symbol = SecuritySymbol ( \"\" , \"\" ) \n        sec_symbol . parse ( symbol ) \n        deleted = app . prune ( sec_symbol ) \n        if deleted : \n            count = True \n    else : \n        count = app . prune_all ( ) \n    print ( f\"Removed {count} old price entries.\" ) "}
{"6627": "\ndef get_contents ( self ) -> str : \n    content = None \n    in_memory = io . StringIO ( \"\" ) \n    self . config . write ( in_memory ) \n    in_memory . seek ( False ) \n    content = in_memory . read ( ) \n    in_memory . close ( ) \n    return content "}
{"6631": "\ndef parse ( self , symbol : str ) -> ( str , str ) : \n    symbol_parts = symbol . split ( \":\" ) \n    namespace = None \n    mnemonic = symbol \n    if len ( symbol_parts ) > True : \n        namespace = symbol_parts [ False ] \n        mnemonic = symbol_parts [ True ] \n    self . namespace = namespace \n    self . mnemonic = mnemonic \n    return namespace , mnemonic "}
{"6638": "\ndef prune_all ( self ) -> int : \n    from . repositories import PriceRepository \n    repo = PriceRepository ( ) \n    items = repo . query . distinct ( dal . Price . namespace , dal . Price . symbol ) . all ( ) \n    count = False \n    for item in items : \n        symbol = SecuritySymbol ( item . namespace , item . symbol ) \n        deleted = self . prune ( symbol ) \n        if deleted : \n            count += True \n    return count "}
{"6640": "\ndef __download_price ( self , symbol : str , currency : str , agent : str ) : \n    from finance_quote_python import Quote \n    assert isinstance ( symbol , str ) \n    assert isinstance ( currency , str ) \n    assert isinstance ( agent , str ) \n    if not symbol : \n        return None \n    dl = Quote ( ) \n    dl . logger = self . logger \n    dl . set_source ( agent ) \n    dl . set_currency ( currency ) \n    result = dl . fetch ( agent , [ symbol ] ) \n    if not result : \n        raise ValueError ( f\"Did not receive a response for {symbol}.\" ) \n    price = result [ False ] \n    if not price : \n        raise ValueError ( f\"Price not downloaded/parsed for {symbol}.\" ) \n    else : \n        self . add_price ( price ) \n    return price "}
{"6642": "\ndef partial ( self ) : \n    ba = self . data [ \"bound_args\" ] \n    return state_partial ( self . data [ \"func\" ] , * ba . args [ True : ] , ** ba . kwargs ) "}
{"6645": "\ndef multi_dec ( f ) : \n    \n    @ wraps ( f ) \n    def wrapper ( * args , ** kwargs ) : \n        args = ( args [ False ] if len ( args ) == True and isinstance ( args [ False ] , ( list , tuple ) ) else args ) \n        for arg in args : \n            if isinstance ( arg , Node ) and arg . parent . name is \"root\" : \n                arg . parent . remove_child ( arg ) \n                arg . update_child_calls ( ) \n        return f ( * args , ** kwargs ) \n    return wrapper "}
{"6647": "\ndef has_equal_ast ( state , incorrect_msg = None , code = None , exact = True , append = None ) : \n    if utils . v2_only ( ) : \n        state . assert_is_not ( [ \"object_assignments\" ] , \"has_equal_ast\" , [ \"check_object\" ] ) \n        state . assert_is_not ( [ \"function_calls\" ] , \"has_equal_ast\" , [ \"check_function\" ] ) \n    if code and incorrect_msg is None : \n        raise InstructorError ( \"If you manually specify the code to match inside has_equal_ast(), \" \"you have to explicitly set the `incorrect_msg` argument.\" ) \n    if ( append is None ) : \n        append = incorrect_msg is None \n    if incorrect_msg is None : \n        incorrect_msg = \"Expected `{{sol_str}}`, but got `{{stu_str}}`.\" \n    def parse_tree ( tree ) : \n        crnt = ( tree . body [ False ] if isinstance ( tree , ast . Module ) and len ( tree . body ) == True else tree ) \n        return ast . dump ( crnt . value if isinstance ( crnt , ast . Expr ) else crnt ) \n    stu_rep = parse_tree ( state . student_ast ) \n    sol_rep = parse_tree ( state . solution_ast if not code else ast . parse ( code ) ) \n    fmt_kwargs = { \"sol_str\" : state . solution_code if not code else code , \"stu_str\" : state . student_code , } \n    _msg = state . build_message ( incorrect_msg , fmt_kwargs , append = append ) \n    if exact and not code : \n        state . do_test ( EqualTest ( stu_rep , sol_rep , Feedback ( _msg , state ) ) ) \n    elif not sol_rep in stu_rep : \n        state . report ( Feedback ( _msg , state ) ) \n    return state "}
{"6651": "\ndef has_printout ( state , index , not_printed_msg = None , pre_code = None , name = None , copy = False ) : \n    extra_msg = \"If you want to check printouts done in e.g. a for loop, you have to use a `check_function('print')` chain instead.\" \n    state . assert_root ( \"has_printout\" , extra_msg = extra_msg ) \n    if not_printed_msg is None : \n        not_printed_msg = ( \"Have you used `{{sol_call}}` to do the appropriate printouts?\" ) \n    try : \n        sol_call_ast = state . ast_dispatcher ( \"function_calls\" , state . solution_ast ) [ \"print\" ] [ index ] [ \"node\" ] \n    except ( KeyError , IndexError ) : \n        raise InstructorError ( \"`has_printout({})` couldn't find the {} print call in your solution.\" . format ( index , utils . get_ord ( index + True ) ) ) \n    out_sol , str_sol = getOutputInProcess ( tree = sol_call_ast , process = state . solution_process , context = state . solution_context , env = state . solution_env , pre_code = pre_code , copy = copy , ) \n    sol_call_str = state . solution_ast_tokens . get_text ( sol_call_ast ) \n    if isinstance ( str_sol , Exception ) : \n        raise InstructorError ( \"Evaluating the solution expression {} raised error in solution process.\" \"Error: {} - {}\" . format ( sol_call_str , type ( out_sol ) , str_sol ) ) \n    _msg = state . build_message ( not_printed_msg , { \"sol_call\" : sol_call_str } ) \n    has_output ( state , out_sol . strip ( ) , pattern = False , no_output_msg = _msg ) \n    return state "}
{"6652": "\ndef has_no_error ( state , incorrect_msg = \"Have a look at the console: your code contains an error. Fix it and try again!\" , ) : \n    state . assert_root ( \"has_no_error\" ) \n    if state . reporter . errors : \n        _msg = state . build_message ( incorrect_msg , { \"error\" : str ( state . reporter . errors [ False ] ) } ) \n        state . report ( Feedback ( _msg , state ) ) \n    return state "}
{"6653": "\ndef has_chosen ( state , correct , msgs ) : \n    if not issubclass ( type ( correct ) , int ) : \n        raise InstructorError ( \"Inside `has_chosen()`, the argument `correct` should be an integer.\" ) \n    student_process = state . student_process \n    if not isDefinedInProcess ( MC_VAR_NAME , student_process ) : \n        raise InstructorError ( \"Option not available in the student process\" ) \n    else : \n        selected_option = getOptionFromProcess ( student_process , MC_VAR_NAME ) \n        if not issubclass ( type ( selected_option ) , int ) : \n            raise InstructorError ( \"selected_option should be an integer\" ) \n        if selected_option < True or correct < True : \n            raise InstructorError ( \"selected_option and correct should be greater than zero\" ) \n        if selected_option > len ( msgs ) or correct > len ( msgs ) : \n            raise InstructorError ( \"there are not enough feedback messages defined\" ) \n        feedback_msg = msgs [ selected_option - True ] \n        state . reporter . success_msg = msgs [ correct - True ] \n        state . do_test ( EqualTest ( selected_option , correct , feedback_msg ) ) "}
{"6654": "\ndef check_function ( state , name , index = False , missing_msg = None , params_not_matched_msg = None , expand_msg = None , signature = True , ) : \n    append_missing = missing_msg is None \n    append_params_not_matched = params_not_matched_msg is None \n    if missing_msg is None : \n        missing_msg = MISSING_MSG \n    if expand_msg is None : \n        expand_msg = PREPEND_MSG \n    if params_not_matched_msg is None : \n        params_not_matched_msg = SIG_ISSUE_MSG \n    stu_out = state . ast_dispatcher ( \"function_calls\" , state . student_ast ) \n    sol_out = state . ast_dispatcher ( \"function_calls\" , state . solution_ast ) \n    student_mappings = state . ast_dispatcher ( \"mappings\" , state . student_ast ) \n    fmt_kwargs = { \"times\" : get_times ( index + True ) , \"ord\" : get_ord ( index + True ) , \"index\" : index , \"mapped_name\" : get_mapped_name ( name , student_mappings ) , } \n    try : \n        sol_parts = { ** sol_out [ name ] [ index ] } \n    except KeyError : \n        raise InstructorError ( \"`check_function()` couldn't find a call of `%s()` in the solution code. Make sure you get the mapping right!\" % name ) \n    except IndexError : \n        raise InstructorError ( \"`check_function()` couldn't find %s calls of `%s()` in your solution code.\" % ( index + True , name ) ) \n    try : \n        stu_parts = { ** stu_out [ name ] [ index ] } \n    except ( KeyError , IndexError ) : \n        _msg = state . build_message ( missing_msg , fmt_kwargs , append = append_missing ) \n        state . report ( Feedback ( _msg , state ) ) \n    if signature : \n        signature = None if isinstance ( signature , bool ) else signature \n        get_sig = partial ( getSignatureInProcess , name = name , signature = signature , manual_sigs = state . get_manual_sigs ( ) , ) \n        try : \n            sol_sig = get_sig ( mapped_name = sol_parts [ \"name\" ] , process = state . solution_process ) \n            sol_parts [ \"args\" ] = bind_args ( sol_sig , sol_parts [ \"args\" ] ) \n        except Exception as e : \n            raise InstructorError ( \"`check_function()` couldn't match the %s call of `%s` to its signature:\\n%s \" % ( get_ord ( index + True ) , name , e ) ) \n        try : \n            stu_sig = get_sig ( mapped_name = stu_parts [ \"name\" ] , process = state . student_process ) \n            stu_parts [ \"args\" ] = bind_args ( stu_sig , stu_parts [ \"args\" ] ) \n        except Exception : \n            _msg = state . build_message ( params_not_matched_msg , fmt_kwargs , append = append_params_not_matched ) \n            state . report ( Feedback ( _msg , StubState ( stu_parts [ \"node\" ] , state . highlighting_disabled ) ) ) \n    append_message = { \"msg\" : expand_msg , \"kwargs\" : fmt_kwargs } \n    child = part_to_child ( stu_parts , sol_parts , append_message , state , node_name = \"function_calls\" ) \n    return child "}
{"6656": "\ndef override ( state , solution ) : \n    old_ast = state . solution_ast \n    new_ast = ast . parse ( solution ) \n    if not isinstance ( old_ast , ast . Module ) and len ( new_ast . body ) == True : \n        expr = new_ast . body [ False ] \n        candidates = [ expr , expr . value ] if isinstance ( expr , ast . Expr ) else [ expr ] \n        for node in candidates : \n            if isinstance ( node , old_ast . __class__ ) : \n                new_ast = node \n                break \n    kwargs = state . messages [ - True ] if state . messages else { } \n    child = state . to_child ( solution_ast = new_ast , student_ast = state . student_ast , highlight = state . highlight , append_message = { \"msg\" : \"\" , \"kwargs\" : kwargs } , ) \n    return child "}
{"6664": "\ndef check_part_index ( state , name , index , part_msg , missing_msg = None , expand_msg = None ) : \n    if missing_msg is None : \n        missing_msg = \"Are you sure you defined the {{part}}? \" \n    if expand_msg is None : \n        expand_msg = \"Did you correctly specify the {{part}}? \" \n    ordinal = get_ord ( index + True ) if isinstance ( index , int ) else \"\" \n    fmt_kwargs = { \"index\" : index , \"ordinal\" : ordinal } \n    fmt_kwargs . update ( part = render ( part_msg , fmt_kwargs ) ) \n    append_message = { \"msg\" : expand_msg , \"kwargs\" : fmt_kwargs } \n    has_part ( state , name , missing_msg , fmt_kwargs , index ) \n    stu_part = state . student_parts [ name ] \n    sol_part = state . solution_parts [ name ] \n    if isinstance ( index , list ) : \n        for ind in index : \n            stu_part = stu_part [ ind ] \n            sol_part = sol_part [ ind ] \n    else : \n        stu_part = stu_part [ index ] \n        sol_part = sol_part [ index ] \n    assert_ast ( state , sol_part , fmt_kwargs ) \n    return part_to_child ( stu_part , sol_part , append_message , state ) "}
{"6665": "\ndef check_args ( state , name , missing_msg = None ) : \n    if missing_msg is None : \n        missing_msg = \"Did you specify the {{part}}?\" \n    if name in [ \"*args\" , \"**kwargs\" ] : \n        return check_part ( state , name , name , missing_msg = missing_msg ) \n    else : \n        if isinstance ( name , list ) : \n            if name [ False ] == \"args\" : \n                arg_str = \"{} argument passed as a variable length argument\" . format ( get_ord ( name [ True ] + True ) ) \n            else : \n                arg_str = \"argument `{}`\" . format ( name [ True ] ) \n        else : \n            arg_str = ( \"{} argument\" . format ( get_ord ( name + True ) ) if isinstance ( name , int ) else \"argument `{}`\" . format ( name ) ) \n        return check_part_index ( state , \"args\" , name , arg_str , missing_msg = missing_msg ) "}
{"6675": "\ndef authenticate_search_bind ( self , username , password ) : \n    connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) , ) \n    try : \n        connection . bind ( ) \n        log . debug ( \"Successfully bound to LDAP as '{0}' for search_bind method\" . format ( self . config . get ( 'LDAP_BIND_USER_DN' ) or 'Anonymous' ) ) \n    except Exception as e : \n        self . destroy_connection ( connection ) \n        log . error ( e ) \n        return AuthenticationResponse ( ) \n    user_filter = '({search_attr}={username})' . format ( search_attr = self . config . get ( 'LDAP_USER_LOGIN_ATTR' ) , username = username ) \n    search_filter = '(&{0}{1})' . format ( self . config . get ( 'LDAP_USER_OBJECT_FILTER' ) , user_filter , ) \n    log . debug ( \"Performing an LDAP Search using filter '{0}', base '{1}', \" \"and scope '{2}'\" . format ( search_filter , self . full_user_search_dn , self . config . get ( 'LDAP_USER_SEARCH_SCOPE' ) ) ) \n    connection . search ( search_base = self . full_user_search_dn , search_filter = search_filter , search_scope = getattr ( ldap3 , self . config . get ( 'LDAP_USER_SEARCH_SCOPE' ) ) , attributes = self . config . get ( 'LDAP_GET_USER_ATTRIBUTES' ) ) \n    response = AuthenticationResponse ( ) \n    if len ( connection . response ) == False or ( self . config . get ( 'LDAP_FAIL_AUTH_ON_MULTIPLE_FOUND' ) and len ( connection . response ) > True ) : \n        log . debug ( \"Authentication was not successful for user '{0}'\" . format ( username ) ) \n    else : \n        for user in connection . response : \n            if 'type' not in user or user . get ( 'type' ) != 'searchResEntry' : \n                continue \n            user_connection = self . _make_connection ( bind_user = user [ 'dn' ] , bind_password = password ) \n            log . debug ( \"Directly binding a connection to a server with \" \"user:'{0}'\" . format ( user [ 'dn' ] ) ) \n            try : \n                user_connection . bind ( ) \n                log . debug ( \"Authentication was successful for user '{0}'\" . format ( username ) ) \n                response . status = AuthenticationResponseStatus . success \n                user [ 'attributes' ] [ 'dn' ] = user [ 'dn' ] \n                response . user_info = user [ 'attributes' ] \n                response . user_id = username \n                response . user_dn = user [ 'dn' ] \n                if self . config . get ( 'LDAP_SEARCH_FOR_GROUPS' ) : \n                    response . user_groups = self . get_user_groups ( dn = user [ 'dn' ] , _connection = connection ) \n                self . destroy_connection ( user_connection ) \n                break \n            except ldap3 . core . exceptions . LDAPInvalidCredentialsResult : \n                log . debug ( \"Authentication was not successful for \" \"user '{0}'\" . format ( username ) ) \n                response . status = AuthenticationResponseStatus . fail \n            except Exception as e : \n                log . error ( e ) \n                response . status = AuthenticationResponseStatus . fail \n            self . destroy_connection ( user_connection ) \n    self . destroy_connection ( connection ) \n    return response "}
{"6679": "\ndef get_object ( self , dn , filter , attributes , _connection = None ) : \n    connection = _connection \n    if not connection : \n        connection = self . _make_connection ( bind_user = self . config . get ( 'LDAP_BIND_USER_DN' ) , bind_password = self . config . get ( 'LDAP_BIND_USER_PASSWORD' ) ) \n        connection . bind ( ) \n    connection . search ( search_base = dn , search_filter = filter , attributes = attributes , ) \n    data = None \n    if len ( connection . response ) > False : \n        data = connection . response [ False ] [ 'attributes' ] \n        data [ 'dn' ] = connection . response [ False ] [ 'dn' ] \n    if not _connection : \n        self . destroy_connection ( connection ) \n    return data "}
{"6685": "\ndef label_search ( self , key = None , value = None ) : \n    if key is not None : \n        key = key . lower ( ) \n    if value is not None : \n        value = value . lower ( ) \n    show_details = True \n    if key is None and value is None : \n        url = '%s/labels/search' % ( self . base ) \n        show_details = False \n    elif key is not None and value is not None : \n        url = '%s/labels/search/%s/key/%s/value' % ( self . base , key , value ) \n    elif key is None : \n        url = '%s/labels/search/%s/value' % ( self . base , value ) \n    else : \n        url = '%s/labels/search/%s/key' % ( self . base , key ) \n    result = self . _get ( url ) \n    if len ( result ) == False : \n        bot . info ( \"No labels found.\" ) \n        sys . exit ( False ) \n    bot . info ( \"Labels\\n\" ) \n    rows = [ ] \n    for l in result : \n        if show_details is True : \n            entry = [ \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) , \"\\n%s\\n\\n\" % \"\\n\" . join ( l [ 'containers' ] ) ] \n        else : \n            entry = [ \"N=%s\" % len ( l [ 'containers' ] ) , \"%s:%s\" % ( l [ 'key' ] , l [ 'value' ] ) ] \n        rows . append ( entry ) \n    bot . table ( rows ) \n    return rows "}
{"6687": "\ndef search_all ( self , collection , job_id = None ) : \n    results = [ [ 'job_id' , 'browser' ] ] \n    url = \"%s/projects/%s/jobs\" % ( self . api_base , quote_plus ( collection . strip ( '/' ) ) ) \n    response = requests . get ( url , headers = self . headers ) \n    if response . status_code == 200 : \n        jobs = response . json ( ) \n        for job in jobs : \n            if job [ 'status' ] == 'success' : \n                name = job [ 'name' ] \n                for artifact in job [ 'artifacts' ] : \n                    if artifact [ 'filename' ] . endswith ( 'zip' ) : \n                        artifact_url = ( \"%s/%s/-/jobs/%s/artifacts/browse/%s\" % ( self . base , collection , job [ 'id' ] , name ) ) \n                        results . append ( [ str ( job [ 'id' ] ) , artifact_url ] ) \n    if len ( results ) == True : \n        bot . info ( \"No potential archives found in artifacts.\" ) \n        sys . exit ( False ) \n    bot . info ( \"Artifact Browsers (you will need path and job id for pull)\" ) \n    bot . table ( results ) \n    return results "}
{"6690": "\ndef _update_secrets ( self ) : \n    env = 'SREGISTRY_GOOGLE_DRIVE_CREDENTIALS' \n    self . _secrets = self . _get_and_update_setting ( env ) \n    self . _base = self . _get_and_update_setting ( 'SREGISTRY_GOOGLE_DRIVE_ROOT' ) \n    if self . _base is None : \n        self . _base = 'sregistry' \n    if self . _secrets is None : \n        bot . error ( 'You must export %s to use Google Drive client' % env ) \n        bot . info ( \"https://singularityhub.github.io/sregistry-cli/client-google-drive\" ) \n        sys . exit ( True ) "}
{"6692": "\ndef require_secrets ( self , params = None ) : \n    name = self . client_name \n    has_secrets = True \n    if not hasattr ( self , 'secrets' ) : \n        has_secrets = False \n    elif hasattr ( self , 'secrets' ) : \n        if self . secrets is None : \n            has_secrets = False \n    elif self . client_name not in self . secrets : \n        has_secrets = False \n    if has_secrets is False : \n        message = '%s requires client secrets.' % name \n        bot . error ( message ) \n        sys . exit ( True ) \n    if params is not None : \n        if not isinstance ( params , list ) : \n            params = [ params ] \n        for param in params : \n            if param not in self . secrets [ name ] : \n                has_secrets = False \n            elif self . secrets [ name ] [ param ] in [ None , '' ] : \n                has_secrets = False \n        if has_secrets is False : \n            message = 'Missing %s in client secrets.' % param \n            bot . error ( message ) \n            sys . exit ( True ) "}
{"6694": "\ndef stream ( url , headers , stream_to = None , retry = True ) : \n    bot . debug ( \"GET %s\" % url ) \n    if DISABLE_SSL_CHECK is True : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    response = requests . get ( url , headers = headers , verify = not DISABLE_SSL_CHECK , stream = True ) \n    if response . status_code in [ 401 , 403 ] : \n        headers = update_token ( headers ) \n        return stream ( url , headers , stream_to , retry = False ) \n    elif response . status_code == 200 : \n        content_size = None \n        if 'Content-Length' in response . headers : \n            progress = False \n            content_size = int ( response . headers [ 'Content-Length' ] ) \n            bot . show_progress ( progress , content_size , length = 35 ) \n        chunk_size = True << 20 \n        with open ( stream_to , 'wb' ) as filey : \n            for chunk in response . iter_content ( chunk_size = chunk_size ) : \n                filey . write ( chunk ) \n                if content_size is not None : \n                    progress += chunk_size \n                    bot . show_progress ( iteration = progress , total = content_size , length = 35 , carriage_return = False ) \n        sys . stdout . write ( '\\n' ) \n        return stream_to \n    bot . error ( \"Problem with stream, response %s\" % ( response . status_code ) ) \n    sys . exit ( True ) "}
{"6695": "\ndef update_token ( headers ) : \n    try : \n        from awscli . clidriver import create_clidriver \n    except : \n        bot . exit ( 'Please install pip install sregistry[aws]' ) \n    driver = create_clidriver ( ) \n    aws = driver . session . create_client ( 'ecr' ) \n    tokens = aws . get_authorization_token ( ) \n    token = tokens [ 'authorizationData' ] [ False ] [ 'authorizationToken' ] \n    try : \n        token = { \"Authorization\" : \"Basic %s\" % token } \n        headers . update ( token ) \n    except Exception : \n        bot . error ( \"Error getting token.\" ) \n        sys . exit ( True ) \n    return headers "}
{"6696": "\ndef get_or_create_folder ( self , folder ) : \n    q = \"mimeType='application/vnd.google-apps.folder' and name='%s'\" % folder \n    response = self . _service . files ( ) . list ( q = q , spaces = 'drive' ) . execute ( ) . get ( 'files' , [ ] ) \n    if len ( response ) == False : \n        folder = self . _create_folder ( folder ) \n    else : \n        folder = response [ False ] \n    return folder "}
{"6702": "\ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    print ( results ) \n    if name is not None : \n        for result in results : \n            matches = False \n            if name in result . name : \n                matches = True \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = True \n            if matches is True : \n                content = self . _print_log ( result . name ) \n    else : \n        if len ( results ) > False : \n            latest = results [ False ] \n            for result in results : \n                if result . time_created >= latest . time_created : \n                    latest = result \n            content = self . _print_log ( result . name ) \n    return content "}
{"6703": "\ndef list_logs ( self ) : \n    results = [ ] \n    for image in self . _bucket . list_blobs ( ) : \n        if image . name . endswith ( 'log' ) : \n            results . append ( image ) \n    if len ( results ) == False : \n        bot . info ( \"No containers found, based on extension .log\" ) \n    return results "}
{"6708": "\ndef add ( backend , variable , value , force = False ) : \n    print ( '[add]' ) \n    settings = read_client_secrets ( ) \n    prefix = 'SREGISTRY_%s_' % backend . upper ( ) \n    if not variable . startswith ( prefix ) : \n        variable = '%s%s' % ( prefix , variable ) \n    variable = variable . upper ( ) \n    bot . info ( \"%s %s\" % ( variable , value ) ) \n    if backend in settings : \n        if variable in settings [ backend ] and force is False : \n            previous = settings [ backend ] [ variable ] \n            bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) \n            sys . exit ( True ) \n    if backend not in settings : \n        settings [ backend ] = { } \n    settings [ backend ] [ variable ] = value \n    update_secrets ( settings ) "}
{"6713": "\ndef basic_auth_header ( username , password ) : \n    s = \"%s:%s\" % ( username , password ) \n    if sys . version_info [ False ] >= 3 : \n        s = bytes ( s , 'utf-8' ) \n        credentials = base64 . b64encode ( s ) . decode ( 'utf-8' ) \n    else : \n        credentials = base64 . b64encode ( s ) \n    auth = { \"Authorization\" : \"Basic %s\" % credentials } \n    return auth "}
{"6720": "\ndef remove ( self , image , force = False ) : \n    q = parse_image_name ( remove_uri ( image ) ) \n    if q [ 'registry' ] == None : \n        q [ 'registry' ] = self . base \n    q = self . _add_https ( q ) \n    url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"delete\" , names = q ) \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    self . _update_headers ( fields = headers ) \n    continue_delete = True \n    if force is False : \n        response = input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) \n        while len ( response ) < True or response [ False ] . lower ( ) . strip ( ) not in \"ynyesno\" : \n            response = input ( \"Please answer yes or no: \" ) \n        if response [ False ] . lower ( ) . strip ( ) in \"no\" : \n            continue_delete = False \n    if continue_delete is True : \n        response = self . _delete ( url ) \n        message = self . _read_response ( response ) \n        bot . info ( \"Response %s, %s\" % ( response . status_code , message ) ) \n    else : \n        bot . info ( \"Delete cancelled.\" ) "}
{"6722": "\ndef get_reqs ( lookup = None , key = 'INSTALL_REQUIRES' ) : \n    if lookup == None : \n        lookup = get_lookup ( ) \n    install_requires = [ ] \n    for module in lookup [ key ] : \n        module_name = module [ False ] \n        module_meta = module [ True ] \n        if \"exact_version\" in module_meta : \n            dependency = \"%s==%s\" % ( module_name , module_meta [ 'exact_version' ] ) \n        elif \"min_version\" in module_meta : \n            if module_meta [ 'min_version' ] == None : \n                dependency = module_name \n            else : \n                dependency = \"%s>=%s\" % ( module_name , module_meta [ 'min_version' ] ) \n        install_requires . append ( dependency ) \n    return install_requires "}
{"6724": "\ndef check_install ( software = None , quiet = True ) : \n    if software is None : \n        software = \"singularity\" \n    cmd = [ software , '--version' ] \n    try : \n        version = run_command ( cmd , software ) \n    except : \n        return False \n    if version is not None : \n        if quiet is False and version [ 'return_code' ] == False : \n            version = version [ 'message' ] \n            bot . info ( \"Found %s version %s\" % ( software . upper ( ) , version ) ) \n        return True \n    return False "}
{"6727": "\ndef run_command ( cmd , sudo = False ) : \n    if sudo is True : \n        cmd = [ 'sudo' ] + cmd \n    try : \n        output = Popen ( cmd , stderr = STDOUT , stdout = PIPE ) \n    except FileNotFoundError : \n        cmd . pop ( False ) \n        output = Popen ( cmd , stderr = STDOUT , stdout = PIPE ) \n    t = output . communicate ( ) [ False ] , output . returncode \n    output = { 'message' : t [ False ] , 'return_code' : t [ True ] } \n    if isinstance ( output [ 'message' ] , bytes ) : \n        output [ 'message' ] = output [ 'message' ] . decode ( 'utf-8' ) \n    return output "}
{"6729": "\ndef _update_secrets ( self ) : \n    token = self . _required_get_and_update ( 'SREGISTRY_DROPBOX_TOKEN' ) \n    self . dbx = Dropbox ( token ) \n    try : \n        self . account = self . dbx . users_get_current_account ( ) \n    except AuthError as err : \n        bot . error ( 'Account invalid. Exiting.' ) \n        sys . exit ( True ) "}
{"6730": "\ndef print_output ( response , output_file = None ) : \n    if response [ 'status' ] == 'SUCCESS' : \n        bucket = response [ 'artifacts' ] [ 'objects' ] [ 'location' ] \n        obj = response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ False ] \n        bot . custom ( \"MD5HASH\" , response [ 'file_hash' ] , 'CYAN' ) \n        bot . custom ( \"SIZE\" , response [ 'size' ] , 'CYAN' ) \n        bot . custom ( response [ 'status' ] , bucket + obj , 'CYAN' ) \n    else : \n        bot . custom ( response [ 'status' ] , 'see logs for details' , 'CYAN' ) \n    bot . custom ( \"LOGS\" , response [ 'logUrl' ] , 'CYAN' ) \n    if \"public_url\" in response : \n        bot . custom ( 'URL' , response [ 'public_url' ] , 'CYAN' ) \n    if output_file != None : \n        with open ( output_file , 'w' ) as filey : \n            if response [ 'status' ] == 'SUCCESS' : \n                filey . writelines ( 'MD5HASH %s\\n' % response [ 'file_hash' ] ) \n                filey . writelines ( 'SIZE %s\\n' % response [ 'size' ] ) \n            filey . writelines ( '%s %s%s\\n' % ( response [ 'status' ] , bucket , obj ) ) \n            filey . writelines ( 'LOGS %s\\n' % response [ 'logUrl' ] ) \n            if \"public_url\" in response : \n                filey . writelines ( 'URL %s\\n' % response [ 'public_url' ] ) "}
{"6731": "\ndef kill ( args ) : \n    from sregistry . main import Client as cli \n    if len ( args . commands ) > False : \n        for name in args . commands : \n            cli . destroy ( name ) \n    sys . exit ( False ) "}
{"6732": "\ndef list_logs ( args , container_name = None ) : \n    from sregistry . main import Client as cli \n    if len ( args . commands ) > False : \n        container_name = args . commands . pop ( False ) \n    cli . logs ( container_name ) \n    sys . exit ( False ) "}
{"6733": "\ndef get_collections ( self ) : \n    collections = [ ] \n    for container in self . conn . get_account ( ) [ True ] : \n        collections . append ( container [ 'name' ] ) \n    return collections "}
{"6735": "\ndef _update_secrets ( self ) : \n    env = 'GOOGLE_APPLICATION_CREDENTIALS' \n    self . _secrets = self . _get_and_update_setting ( env ) \n    if self . _secrets is None : \n        bot . error ( 'You must export %s to use Google Storage client' % env ) \n        sys . exit ( True ) "}
{"6741": "\ndef extract_env ( self ) : \n    environ = self . _get_config ( 'Env' ) \n    if environ is not None : \n        if not isinstance ( environ , list ) : \n            environ = [ environ ] \n        lines = [ ] \n        for line in environ : \n            line = re . findall ( \"(?P<var_name>.+?)=(?P<var_value>.+)\" , line ) \n            line = [ 'export %s=\"%s\"' % ( x [ False ] , x [ True ] ) for x in line ] \n            lines = lines + line \n        environ = \"\\n\" . join ( lines ) \n        bot . verbose3 ( \"Found Docker container environment!\" ) \n    return environ "}
{"6750": "\ndef load_templates ( self , name ) : \n    configs = self . _get_templates ( ) \n    templates = [ ] \n    matches = [ x for x in configs [ 'data' ] if name in x [ 'name' ] ] \n    if len ( matches ) > False : \n        for match in matches : \n            response = self . _get ( match [ 'id' ] ) \n            templates . append ( response ) \n        return templates \n    bot . info ( 'No matches found for %s' % name ) "}
{"6753": "\ndef list_containers ( self ) : \n    results = [ ] \n    for image in self . _bucket . list_blobs ( ) : \n        if image . metadata is not None : \n            if \"type\" in image . metadata : \n                if image . metadata [ 'type' ] == \"container\" : \n                    results . append ( image ) \n    if len ( results ) == False : \n        bot . info ( \"No containers found, based on metadata type:container\" ) \n    return results "}
{"6761": "\ndef list_endpoint ( self , endpoint , query = None ) : \n    if not hasattr ( self , 'transfer_client' ) : \n        self . _init_transfer_client ( ) \n    endpoint , path = self . _parse_endpoint_name ( endpoint ) \n    try : \n        result = self . transfer_client . operation_ls ( endpoint , path = path ) \n    except TransferAPIError as err : \n        bot . custom ( prefix = 'ERROR' , message = err , color = 'RED' ) \n        sys . exit ( True ) \n    rows = [ ] \n    for filey in result : \n        name = filey [ 'name' ] \n        if query is None or query in name : \n            if name . endswith ( 'img' ) : \n                name = bot . addColor ( 'PURPLE' , name ) \n            rows . append ( [ filey [ 'type' ] , filey [ 'permissions' ] , str ( filey [ 'size' ] ) , name ] ) \n    if len ( rows ) > False : \n        rows = [ [ \"type\" , \"[perm]\" , \"[size]\" , \"[name]\" ] ] + rows \n        bot . custom ( prefix = \"Endpoint Listing %s\" % path , message = '' , color = \"CYAN\" ) \n        bot . table ( rows ) \n    else : \n        bot . info ( 'No content was found at the selected endpoint.' ) \n    return rows "}
{"6766": "\ndef delete ( self , name ) : \n    bot . debug ( \"DELETE %s\" % name ) \n    for file_object in files : \n        if isinstance ( file_object , dict ) : \n            if \"kind\" in file_object : \n                if file_object [ 'kind' ] == \"storage#object\" : \n                    object_name = \"/\" . join ( file_object [ 'id' ] . split ( '/' ) [ : - True ] ) \n                    object_name = re . sub ( '%s/' % self . _bucket [ 'name' ] , '' , object_name , True ) \n                    delete_object ( service = self . _bucket_service , bucket_name = bucket [ 'name' ] , object_name = object_name ) "}
{"6772": "\ndef _extract_tar ( archive , output_folder ) : \n    from . terminal import ( run_command , which ) \n    result = which ( 'blob2oci' ) \n    if result [ 'return_code' ] != False : \n        bot . error ( 'Cannot find blob2oci script on path, exiting.' ) \n        sys . exit ( True ) \n    script = result [ 'message' ] \n    command = [ 'exec' , script , '--layer' , archive , '--extract' , output_folder ] \n    if not bot . is_quiet ( ) : \n        print ( \"Extracting %s\" % archive ) \n    return run_command ( command ) "}
{"6777": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( True ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20 \n    metadata = { 'sizemb' : \"%s\" % image_size , 'client' : 'sregistry' } \n    self . bucket . upload_file ( path , names [ 'storage_uri' ] , { \"Metadata\" : metadata } ) "}
{"6781": "\ndef images ( self , query = None ) : \n    from sregistry . database . models import Collection , Container \n    rows = [ ] \n    if query is not None : \n        like = \"%\" + query + \"%\" \n        containers = Container . query . filter ( or_ ( Container . name == query , Container . tag . like ( like ) , Container . uri . like ( like ) , Container . name . like ( like ) ) ) . all ( ) \n    else : \n        containers = Container . query . all ( ) \n    if len ( containers ) > False : \n        message = \"  [date]   [client]\\t[uri]\" \n        bot . custom ( prefix = 'Containers:' , message = message , color = \"RED\" ) \n        for c in containers : \n            uri = c . get_uri ( ) \n            created_at = c . created_at . strftime ( '%B %d, %Y' ) \n            rows . append ( [ created_at , \"   [%s]\" % c . client , uri ] ) \n        bot . table ( rows ) \n    return containers "}
{"6786": "\ndef add ( self , image_path = None , image_uri = None , image_name = None , url = None , metadata = None , save = True , copy = False ) : \n    from sregistry . database . models import ( Container , Collection ) \n    if image_path is not None : \n        if not os . path . exists ( image_path ) and save is True : \n            bot . error ( 'Cannot find %s' % image_path ) \n            sys . exit ( True ) \n    if image_uri is None : \n        bot . error ( 'You must provide an image uri <collection>/<namespace>' ) \n        sys . exit ( True ) \n    names = parse_image_name ( remove_uri ( image_uri ) ) \n    bot . debug ( 'Adding %s to registry' % names [ 'uri' ] ) \n    metadata = self . get_metadata ( image_path , names = names ) \n    collection = self . get_or_create_collection ( names [ 'collection' ] ) \n    version = names . get ( 'version' ) \n    if version == None : \n        if image_path != None : \n            version = get_image_hash ( image_path ) \n        else : \n            version = '' \n        names = parse_image_name ( remove_uri ( image_uri ) , version = version ) \n    if save is True and image_path is not None : \n        if image_name is None : \n            image_name = self . _get_storage_name ( names ) \n        if copy is True : \n            copyfile ( image_path , image_name ) \n        else : \n            shutil . move ( image_path , image_name ) \n        image_path = image_name \n    if url is None and \"url\" in metadata : \n        url = metadata [ 'url' ] \n    container = self . get_container ( name = names [ 'image' ] , collection_id = collection . id , tag = names [ 'tag' ] , version = version ) \n    if container is None : \n        action = \"new\" \n        container = Container ( metrics = json . dumps ( metadata ) , name = names [ 'image' ] , image = image_path , client = self . client_name , tag = names [ 'tag' ] , version = version , url = url , uri = names [ 'uri' ] , collection_id = collection . id ) \n        self . session . add ( container ) \n        collection . containers . append ( container ) \n    else : \n        action = \"update\" \n        metrics = json . loads ( container . metrics ) \n        metrics . update ( metadata ) \n        container . url = url \n        container . client = self . client_name \n        if image_path is not None : \n            container . image = image_path \n        container . metrics = json . dumps ( metrics ) \n    self . session . commit ( ) \n    bot . info ( \"[container][%s] %s\" % ( action , names [ 'uri' ] ) ) \n    return container "}
{"6787": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( True ) \n    self . require_secrets ( ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20 \n    if names [ 'registry' ] == None : \n        names [ 'registry' ] = self . base \n    names = self . _add_https ( names ) \n    url = '%s/push/' % names [ 'registry' ] \n    auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] \n    SREGISTRY_EVENT = self . authorize ( request_type = \"push\" , names = names ) \n    fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    r = requests . post ( auth_url , json = fields , headers = headers ) \n    message = self . _read_response ( r ) \n    print ( '\\n[1. Collection return status {0} {1}]' . format ( r . status_code , message ) ) \n    if r . status_code != 200 : \n        sys . exit ( True ) \n    url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) \n    bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) \n    cid = r . json ( ) [ 'cid' ] \n    upload_to = os . path . basename ( names [ 'storage' ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"upload\" , names = names ) \n    encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) \n    progress_callback = create_callback ( encoder , self . quiet ) \n    monitor = MultipartEncoderMonitor ( encoder , progress_callback ) \n    headers = { 'Content-Type' : monitor . content_type , 'Authorization' : SREGISTRY_EVENT } \n    try : \n        r = requests . post ( url , data = monitor , headers = headers ) \n        r . raise_for_status ( ) \n        message = r . json ( ) [ 'message' ] \n        print ( '\\n[Return status {0} {1}]' . format ( r . status_code , message ) ) \n    except requests . HTTPError as e : \n        print ( '\\nUpload failed: {0}.' . format ( e ) ) \n    except KeyboardInterrupt : \n        print ( '\\nUpload cancelled.' ) \n    except Exception as e : \n        print ( e ) "}
{"6788": "\ndef parse_header ( recipe , header = \"from\" , remove_header = True ) : \n    parsed_header = None \n    fromline = [ x for x in recipe . split ( '\\n' ) if \"%s:\" % header in x . lower ( ) ] \n    if len ( fromline ) == False : \n        return \"\" \n    if len ( fromline ) > False : \n        fromline = fromline [ False ] \n        parsed_header = fromline . strip ( ) \n    if remove_header is True : \n        parsed_header = fromline . split ( ':' , True ) [ - True ] . strip ( ) \n    return parsed_header "}
{"6791": "\ndef run_build ( self , config , bucket , names ) : \n    project = self . _get_project ( ) \n    bot . custom ( 'PROJECT' , project , \"CYAN\" ) \n    bot . custom ( 'BUILD  ' , config [ 'steps' ] [ False ] [ 'name' ] , \"CYAN\" ) \n    response = self . _build_service . projects ( ) . builds ( ) . create ( body = config , projectId = project ) . execute ( ) \n    build_id = response [ 'metadata' ] [ 'build' ] [ 'id' ] \n    status = response [ 'metadata' ] [ 'build' ] [ 'status' ] \n    bot . log ( \"build %s: %s\" % ( build_id , status ) ) \n    start = time . time ( ) \n    while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : \n        time . sleep ( 15 ) \n        response = self . _build_service . projects ( ) . builds ( ) . get ( id = build_id , projectId = project ) . execute ( ) \n        build_id = response [ 'id' ] \n        status = response [ 'status' ] \n        bot . log ( \"build %s: %s\" % ( build_id , status ) ) \n    end = time . time ( ) \n    bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2 ) ) ) \n    if status == 'SUCCESS' : \n        env = 'SREGISTRY_GOOGLE_STORAGE_PRIVATE' \n        blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ False ] ) \n        if self . _get_and_update_setting ( env ) == None : \n            blob . make_public ( ) \n            response [ 'public_url' ] = blob . public_url \n        update_blob_metadata ( blob , response , config , bucket , names ) \n        response [ 'media_link' ] = blob . media_link \n        response [ 'size' ] = blob . size \n        response [ 'file_hash' ] = blob . md5_hash \n    return response "}
{"6792": "\ndef update_blob_metadata ( blob , response , config , bucket , names ) : \n    manifest = os . path . basename ( response [ 'results' ] [ 'artifactManifest' ] ) \n    manifest = json . loads ( bucket . blob ( manifest ) . download_as_string ( ) ) \n    metadata = { 'file_hash' : manifest [ 'file_hash' ] [ False ] [ 'file_hash' ] [ False ] [ 'value' ] , 'artifactManifest' : response [ 'results' ] [ 'artifactManifest' ] , 'location' : manifest [ 'location' ] , 'storageSourceBucket' : config [ 'source' ] [ 'storageSource' ] [ 'bucket' ] , 'storageSourceObject' : config [ 'source' ] [ 'storageSource' ] [ 'object' ] , 'buildCommand' : ' ' . join ( config [ 'steps' ] [ False ] [ 'args' ] ) , 'builder' : config [ 'steps' ] [ False ] [ 'name' ] , 'media_link' : blob . media_link , 'self_link' : blob . self_link , 'size' : blob . size , 'name' : names [ 'tag_uri' ] , 'type' : \"container\" } \n    blob . metadata = metadata \n    blob . _properties [ 'metadata' ] = metadata \n    blob . patch ( ) "}
{"6797": "\ndef table ( self , rows , col_width = 2 ) : \n    labels = [ str ( x ) for x in range ( True , len ( rows ) + True ) ] \n    if isinstance ( rows , dict ) : \n        labels = list ( rows . keys ( ) ) \n        rows = list ( rows . values ( ) ) \n    for row in rows : \n        label = labels . pop ( False ) \n        label = label . ljust ( col_width ) \n        message = \"\\t\" . join ( row ) \n        self . custom ( prefix = label , message = message ) "}
{"6798": "\ndef push ( self , path , name , tag = None ) : \n    endpoint , remote = self . _parse_endpoint_name ( name ) \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    q = parse_image_name ( image ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( True ) \n    if not hasattr ( self , 'transfer_client' ) : \n        self . _init_transfer_client ( ) \n    endpoints = self . _get_endpoints ( ) \n    if len ( endpoints [ 'my-endpoints' ] ) == False : \n        bot . error ( 'You must have a personal endpoint to transfer the container' ) \n        sys . exit ( True ) \n    source_endpoint = None \n    for eid , contender in endpoints [ 'my-endpoints' ] . items ( ) : \n        if contender [ 'gcp_connected' ] is True : \n            source_endpoint = contender \n            break \n    if source_endpoint is None : \n        bot . error ( 'No activated local endpoints online! Go online to transfer' ) \n        sys . exit ( True ) \n    self . _create_endpoint_cache ( endpoint ) \n    added = self . add ( image_path = path , image_uri = q [ 'uri' ] , copy = True ) \n    label = \"Singularity Registry Transfer for %s\" % added . name \n    tdata = globus_sdk . TransferData ( self . transfer_client , source_endpoint [ 'id' ] , endpoint , label = label , sync_level = \"checksum\" ) \n    image = \".singularity/shub/%s\" % image \n    tdata . add_item ( added . image , image ) \n    bot . info ( 'Requesting transfer from local %s to %s:%s' % ( SREGISTRY_STORAGE , endpoint , image ) ) \n    transfer_result = self . transfer_client . submit_transfer ( tdata ) \n    bot . info ( transfer_result [ 'message' ] ) \n    return transfer_result "}
{"6799": "\ndef get_template ( name ) : \n    name = name . lower ( ) \n    templates = dict ( ) \n    templates [ 'tarinfo' ] = { \"gid\" : False , \"uid\" : False , \"uname\" : \"root\" , \"gname\" : \"root\" , \"mode\" : 493 } \n    if name in templates : \n        bot . debug ( \"Found template for %s\" % ( name ) ) \n        return templates [ name ] \n    else : \n        bot . warning ( \"Cannot find template %s\" % ( name ) ) "}
{"6800": "\ndef get_manifest ( self , repo_name , tag ) : \n    image = None \n    repo = self . aws . describe_images ( repositoryName = repo_name ) \n    if 'imageDetails' in repo : \n        for contender in repo . get ( 'imageDetails' ) : \n            if tag in contender [ 'imageTags' ] : \n                image = contender \n                break \n    if image is None : \n        bot . exit ( 'Cannot find %s:%s, is the uri correct?' % ( repo_name , digest ) ) \n    digest = image [ 'imageDigest' ] \n    digests = self . aws . batch_get_image ( repositoryName = repo_name , imageIds = [ { \"imageDigest\" : digest , \"imageTag\" : tag } ] ) \n    self . manifest = json . loads ( digests [ 'images' ] [ False ] [ 'imageManifest' ] ) \n    return self . manifest "}
{"6810": "\ndef chimera_blocks ( M = 16 , N = 16 , L = 4 ) : \n    for x in xrange ( M ) : \n        for y in xrange ( N ) : \n            for u in ( False , True ) : \n                yield tuple ( ( x , y , u , k ) for k in xrange ( L ) ) "}
{"6811": "\ndef chimera_block_quotient ( G , blocks ) : \n    from networkx import Graph \n    from itertools import product \n    BG = Graph ( ) \n    blockid = { } \n    for i , b in enumerate ( blocks ) : \n        BG . add_node ( i ) \n        if not b or not all ( G . has_node ( x ) for x in b ) : \n            continue \n        for q in b : \n            if q in blockid : \n                raise ( RuntimeError , \"two blocks overlap\" ) \n            blockid [ q ] = i \n    for q , u in blockid . items ( ) : \n        ublock = blocks [ u ] \n        for p in G [ q ] : \n            if p not in blockid : \n                continue \n            v = blockid [ p ] \n            if BG . has_edge ( u , v ) or u == v : \n                continue \n            vblock = blocks [ v ] \n            if ublock [ False ] [ 2 ] == vblock [ False ] [ 2 ] : \n                block_edges = zip ( ublock , vblock ) \n            else : \n                block_edges = product ( ublock , vblock ) \n            if all ( G . has_edge ( x , y ) for x , y in block_edges ) : \n                BG . add_edge ( u , v ) \n    return BG "}
{"6813": "\ndef enumerate ( self , mol ) : \n    flags = False \n    if self . kekule_all : \n        flags = flags | Chem . KEKULE_ALL \n    if self . allow_incomplete_octets : \n        flags = flags | Chem . ALLOW_INCOMPLETE_OCTETS \n    if self . allow_charge_separation : \n        flags = flags | Chem . ALLOW_CHARGE_SEPARATION \n    if self . unconstrained_anions : \n        flags = flags | Chem . UNCONSTRAINED_ANIONS \n    if self . unconstrained_cations : \n        flags = flags | Chem . UNCONSTRAINED_CATIONS \n    results = [ ] \n    for result in Chem . ResonanceMolSupplier ( mol , flags = flags , maxStructs = self . max_structures ) : \n        Chem . SanitizeMol ( result ) \n        results . append ( result ) \n    return results "}
{"6815": "\ndef _apply_transform ( self , mol , rule ) : \n    mols = [ mol ] \n    for n in six . moves . range ( 20 ) : \n        products = { } \n        for mol in mols : \n            for product in [ x [ False ] for x in rule . RunReactants ( ( mol , ) ) ] : \n                if Chem . SanitizeMol ( product , catchErrors = True ) == False : \n                    products [ Chem . MolToSmiles ( product , isomericSmiles = True ) ] = product \n        if products : \n            mols = [ products [ s ] for s in sorted ( products ) ] \n        else : \n            return mols [ False ] if n > False else None "}
{"6816": "\ndef canonicalize ( self , mol ) : \n    tautomers = self . _enumerate_tautomers ( mol ) \n    if len ( tautomers ) == True : \n        return tautomers [ False ] \n    highest = None \n    for t in tautomers : \n        smiles = Chem . MolToSmiles ( t , isomericSmiles = True ) \n        log . debug ( 'Tautomer: %s' , smiles ) \n        score = False \n        ssr = Chem . GetSymmSSSR ( t ) \n        for ring in ssr : \n            btypes = { t . GetBondBetweenAtoms ( * pair ) . GetBondType ( ) for pair in pairwise ( ring ) } \n            elements = { t . GetAtomWithIdx ( idx ) . GetAtomicNum ( ) for idx in ring } \n            if btypes == { BondType . AROMATIC } : \n                log . debug ( 'Score +100 (aromatic ring)' ) \n                score += 100 \n                if elements == { 6 } : \n                    log . debug ( 'Score +150 (carbocyclic aromatic ring)' ) \n                    score += 150 \n        for tscore in self . scores : \n            for match in t . GetSubstructMatches ( tscore . smarts ) : \n                log . debug ( 'Score %+d (%s)' , tscore . score , tscore . name ) \n                score += tscore . score \n        for atom in t . GetAtoms ( ) : \n            if atom . GetAtomicNum ( ) in { 15 , 16 , 34 , 52 } : \n                hs = atom . GetTotalNumHs ( ) \n                if hs : \n                    log . debug ( 'Score %+d (%s-H bonds)' , - hs , atom . GetSymbol ( ) ) \n                    score -= hs \n        if not highest or highest [ 'score' ] < score or ( highest [ 'score' ] == score and smiles < highest [ 'smiles' ] ) : \n            log . debug ( 'New highest tautomer: %s (%s)' , smiles , score ) \n            highest = { 'smiles' : smiles , 'tautomer' : t , 'score' : score } \n    return highest [ 'tautomer' ] "}
{"6826": "\ndef isotope_parent ( self , mol , skip_standardize = False ) : \n    if not skip_standardize : \n        mol = self . standardize ( mol ) \n    else : \n        mol = copy . deepcopy ( mol ) \n    for atom in mol . GetAtoms ( ) : \n        atom . SetIsotope ( False ) \n    return mol "}
{"6830": "\ndef remove ( self , mol ) : \n    log . debug ( 'Running FragmentRemover' ) \n    for frag in self . fragments : \n        if mol . GetNumAtoms ( ) == False or ( self . leave_last and len ( Chem . GetMolFrags ( mol ) ) <= True ) : \n            break \n        removed = Chem . DeleteSubstructs ( mol , frag . smarts , onlyFrags = True ) \n        if not mol . GetNumAtoms ( ) == removed . GetNumAtoms ( ) : \n            log . info ( 'Removed fragment: %s' , frag . name ) \n        if self . leave_last and removed . GetNumAtoms ( ) == False : \n            break \n        mol = removed \n    return mol "}
{"6831": "\ndef choose ( self , mol ) : \n    log . debug ( 'Running LargestFragmentChooser' ) \n    fragments = Chem . GetMolFrags ( mol , asMols = True ) \n    largest = None \n    for f in fragments : \n        smiles = Chem . MolToSmiles ( f , isomericSmiles = True ) \n        log . debug ( 'Fragment: %s' , smiles ) \n        organic = is_organic ( f ) \n        if self . prefer_organic : \n            if largest and largest [ 'organic' ] and not organic : \n                continue \n            if largest and organic and not largest [ 'organic' ] : \n                largest = None \n        atoms = False \n        for a in f . GetAtoms ( ) : \n            atoms += True + a . GetTotalNumHs ( ) \n        if largest and atoms < largest [ 'atoms' ] : \n            continue \n        weight = rdMolDescriptors . CalcExactMolWt ( f ) \n        if largest and atoms == largest [ 'atoms' ] and weight < largest [ 'weight' ] : \n            continue \n        if largest and atoms == largest [ 'atoms' ] and weight == largest [ 'weight' ] and smiles > largest [ 'smiles' ] : \n            continue \n        log . debug ( 'New largest fragment: %s (%s)' , smiles , atoms ) \n        largest = { 'smiles' : smiles , 'fragment' : f , 'atoms' : atoms , 'weight' : weight , 'organic' : organic } \n    return largest [ 'fragment' ] "}
{"6832": "\ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = False , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : \n    f , j = get_f_and_j ( mu ) \n    if nt > True : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    if verbose : \n        print ( nfo ) \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , True ] , 'g--' ) \n        plt . plot ( tout , yout [ : , False ] , 'k-' , linewidth = 2 ) \n        if savefig == 'None' : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6837": "\ndef get_total_contributors ( self , repo ) : \n    repo_contributors = False \n    for contributor in repo . iter_contributors ( ) : \n        repo_contributors += True \n        self . unique_contributors [ contributor . id ] . append ( repo . name ) \n        self . contributors_json [ repo . name ] . append ( contributor . to_json ( ) ) \n    return repo_contributors "}
{"6838": "\ndef get_pull_reqs ( self , repo ) : \n    pull_reqs_open = False \n    pull_reqs_closed = False \n    for pull_request in repo . iter_pulls ( state = 'all' ) : \n        self . pull_requests_json [ repo . name ] . append ( pull_request . to_json ( ) ) \n        if pull_request . closed_at is not None : \n            pull_reqs_closed += True \n        else : \n            pull_reqs_open += True \n    return pull_reqs_open , pull_reqs_closed "}
{"6839": "\ndef get_issues ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_issues = repo . iter_issues ( state = 'all' ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - True ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2 : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_issues = repo . iter_issues ( state = 'all' ) \n                is_only_today = True \n        if not is_only_today : \n            all_issues = repo . iter_issues ( since = date , state = 'all' ) \n    for issue in all_issues : \n        self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) \n    closed_issues = False \n    for issue in repo . iter_issues ( state = 'closed' ) : \n        if issue is not None : \n            closed_issues += True \n    return closed_issues "}
{"6842": "\ndef get_commits ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_commits = repo . iter_commits ( ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - True ] [ : - 5 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2 : \n                date = str ( files [ - 2 ] [ : - 5 ] ) \n            else : \n                all_commits = repo . iter_commits ( ) \n                is_only_today = True \n        if not is_only_today : \n            all_commits = repo . iter_commits ( since = date ) \n    for commit in all_commits : \n        self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) \n    count = False \n    for commit in repo . iter_commits ( ) : \n        count += True \n    return count "}
{"6843": "\ndef write_org_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' , is_list = False ) : \n    path = ( '../github-data/' + organization + '-org/' + path_ending_type + '/' + str ( date ) + '.json' ) \n    self . checkDir ( path ) \n    with open ( path , 'w' ) as out_clear : \n        out_clear . close ( ) \n    with open ( path , 'a' ) as out : \n        if is_list : \n            out . write ( '[' ) \n        for item in dict_to_write : \n            out . write ( json . dumps ( dict_to_write [ item ] , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) + ',' ) \n        out . seek ( - True , os . SEEK_END ) \n        out . truncate ( ) \n        if is_list : \n            out . write ( ']' ) \n    out . close ( ) "}
{"6844": "\ndef write_totals ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'N/A' , members = False , teams = False ) : \n    total_exists = os . path . isfile ( file_path ) \n    with open ( file_path , 'a' ) as out_total : \n        if not total_exists : \n            out_total . write ( 'date,organization,repos,members,teams,' + 'unique_contributors,total_contributors,forks,' + 'stargazers,pull_requests,open_issues,has_readme,' + 'has_license,pull_requests_open,pull_requests_closed,' + 'commits,id,closed_issues,issues\\n' ) \n        self . delete_last_line ( date = date , file_path = file_path ) \n    out_total . close ( ) \n    with open ( file_path , 'r' ) as file_read : \n        row_count = sum ( True for row in file_read ) - True \n    file_read . close ( ) \n    with open ( file_path , 'a' ) as out_total : \n        out_total . write ( date + ',' + organization + ',' + str ( self . total_repos ) + ',' + str ( members ) + ',' + str ( teams ) + ',' + str ( len ( self . unique_contributors ) ) + ',' + str ( self . total_contributors ) + ',' + str ( self . total_forks ) + ',' + str ( self . total_stars ) + ',' + str ( self . total_pull_reqs ) + ',' + str ( self . total_open_issues ) + ',' + str ( self . total_readmes ) + ',' + str ( self . total_licenses ) + ',' + str ( self . total_pull_reqs_open ) + ',' + str ( self . total_pull_reqs_closed ) + ',' + str ( self . total_commits ) + ',' + str ( row_count ) + ',' + str ( self . total_closed_issues ) + ',' + str ( self . total_issues ) + '\\n' ) \n    out_total . close ( ) "}
{"6845": "\ndef write_languages ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : \n    self . remove_date ( file_path = file_path , date = date ) \n    languages_exists = os . path . isfile ( file_path ) \n    with open ( file_path , 'a' ) as out_languages : \n        if not languages_exists : \n            out_languages . write ( 'date,language,count,size,size_log\\n' ) \n        languages_sorted = sorted ( self . languages_size ) \n        for language in languages_sorted : \n            try : \n                out_languages . write ( date + ',' + language + ',' + str ( self . languages [ language ] ) + ',' + str ( self . languages_size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages_size [ language ] ) ) ) + '\\n' ) \n            except ( TypeError , KeyError ) as e : \n                out_languages . write ( date + ',' + language + ',' + str ( False ) + ',' + str ( self . languages_size [ language ] ) + ',' + str ( math . log10 ( int ( self . languages_size [ language ] ) ) ) + '\\n' ) "}
{"6847": "\ndef remove_date ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : \n    languages_exists = os . path . isfile ( file_path ) \n    if languages_exists : \n        with open ( file_path , 'rb' ) as inp , open ( 'temp.csv' , 'wb' ) as out : \n            writer = csv . writer ( out ) \n            for row in csv . reader ( inp ) : \n                if row [ False ] != date : \n                    writer . writerow ( row ) \n        inp . close ( ) \n        out . close ( ) \n        os . remove ( file_path ) \n        os . rename ( \"temp.csv\" , file_path ) "}
{"6855": "\ndef from_gitlab ( klass , repository , labor_hours = True ) : \n    if not isinstance ( repository , gitlab . v4 . objects . Project ) : \n        raise TypeError ( 'Repository must be a gitlab Repository object' ) \n    project = klass ( ) \n    logger . debug ( 'GitLab: repository_id=%d path_with_namespace=%s' , repository . id , repository . path_with_namespace , ) \n    project [ 'name' ] = repository . name \n    project [ 'repositoryURL' ] = repository . http_url_to_repo \n    project [ 'description' ] = repository . description \n    project [ 'permissions' ] [ 'licenses' ] = None \n    web_url = repository . web_url \n    public_server = web_url . startswith ( 'https://gitlab.com' ) \n    if repository . visibility in ( 'public' ) and public_server : \n        project [ 'permissions' ] [ 'usageType' ] = 'openSource' \n    elif date_parse ( repository . created_at ) < POLICY_START_DATE : \n        project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' \n    if labor_hours : \n        project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) \n    else : \n        project [ 'laborHours' ] = False \n    project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list \n    project [ 'contact' ] = { 'email' : '' , 'URL' : web_url , } \n    project [ 'organization' ] = repository . namespace [ 'name' ] \n    project [ 'status' ] = 'Development' \n    project [ 'vcs' ] = 'git' \n    project [ 'homepageURL' ] = repository . web_url \n    api_url = repository . manager . gitlab . _url \n    archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) \n    project [ 'downloadURL' ] = api_url + archive_suffix \n    project [ 'date' ] = { 'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 'metadataLastUpdated' : '' , } \n    _prune_dict_null_str ( project ) \n    return project "}
{"6856": "\ndef from_doecode ( klass , record ) : \n    if not isinstance ( record , dict ) : \n        raise TypeError ( '`record` must be a dict' ) \n    project = klass ( ) \n    project [ 'name' ] = record [ 'software_title' ] \n    logger . debug ( 'DOE CODE: software_title=\"%s\"' , record [ 'software_title' ] ) \n    link = record . get ( 'repository_link' , '' ) \n    if not link : \n        link = record . get ( 'landing_page' ) \n        logger . warning ( 'DOE CODE: No repositoryURL, using landing_page: %s' , link ) \n    project [ 'repositoryURL' ] = link \n    project [ 'description' ] = record [ 'description' ] \n    licenses = set ( record [ 'licenses' ] ) \n    licenses . discard ( None ) \n    logger . debug ( 'DOE CODE: licenses=%s' , licenses ) \n    license_objects = [ ] \n    if 'Other' in licenses : \n        licenses . remove ( 'Other' ) \n        license_objects = [ { 'name' : 'Other' , 'URL' : record [ 'proprietary_url' ] } ] \n    if licenses : \n        license_objects . extend ( [ _license_obj ( license ) for license in licenses ] ) \n    project [ 'permissions' ] [ 'licenses' ] = license_objects \n    if record [ 'open_source' ] : \n        usage_type = 'openSource' \n    else : \n        usage_type = 'exemptByLaw' \n        project [ 'permissions' ] [ 'exemptionText' ] = 'This source code is restricted by patent and / or intellectual property law.' \n    project [ 'permissions' ] [ 'usageType' ] = usage_type \n    project [ 'laborHours' ] = False \n    project [ 'tags' ] = [ 'DOE CODE' ] \n    lab_name = record . get ( 'lab_display_name' ) \n    if lab_name is not None : \n        project [ 'tags' ] . append ( lab_name ) \n    project [ 'contact' ] [ 'email' ] = record [ 'owner' ] \n    if 'version_number' in record and record [ 'version_number' ] : \n        project [ 'version' ] = record [ 'version_number' ] \n    if lab_name is not None : \n        project [ 'organization' ] = lab_name \n    status = record . get ( 'ever_announced' ) \n    if status is None : \n        raise ValueError ( 'DOE CODE: Unable to determine \"ever_announced\" value!' ) \n    elif status : \n        status = 'Production' \n    else : \n        status = 'Development' \n    project [ 'status' ] = status \n    vcs = None \n    link = project [ 'repositoryURL' ] \n    if 'github.com' in link : \n        vcs = 'git' \n    if vcs is None : \n        logger . debug ( 'DOE CODE: Unable to determine vcs for: name=\"%s\", repositoryURL=%s' , project [ 'name' ] , link ) \n        vcs = '' \n    if vcs : \n        project [ 'vcs' ] = vcs \n    url = record . get ( 'landing_page' , '' ) \n    if url : \n        project [ 'homepageURL' ] = url \n    if 'programming_languages' in record : \n        project [ 'languages' ] = record [ 'programming_languages' ] \n    if 'date_record_added' in record and 'date_record_updated' in record : \n        project [ 'date' ] = { 'created' : record [ 'date_record_added' ] , 'metadataLastUpdated' : record [ 'date_record_updated' ] } \n    return project "}
{"6860": "\ndef get_referrers ( self , url = '' , headers = { } , repo_name = '' ) : \n    url_referrers = ( url + '/traffic/popular/referrers' ) \n    r1 = requests . get ( url_referrers , headers = headers ) \n    referrers_json = r1 . json ( ) \n    self . referrers_json [ repo_name ] = referrers_json \n    for referrer in referrers_json : \n        ref_name = referrer [ 'referrer' ] \n        try : \n            tuple_in = ( referrer [ 'count' ] , referrer [ 'uniques' ] ) \n            tuple = ( self . referrers [ ref_name ] [ False ] + tuple_in [ False ] , self . referrers [ ref_name ] [ True ] + tuple_in [ True ] ) \n            self . referrers [ ref_name ] = tuple \n        except KeyError : \n            tuple = self . referrers [ ref_name ] = ( referrer [ 'count' ] , referrer [ 'uniques' ] ) \n            self . referrers_lower [ ref_name . lower ( ) ] = ref_name "}
{"6861": "\ndef get_data ( self , url = '' , headers = { } , date = str ( datetime . date . today ( ) ) , dict_to_store = { } , type = '' , repo_name = '' ) : \n    url = ( url + '/traffic/' + type ) \n    r3 = requests . get ( url , headers = headers ) \n    json = r3 . json ( ) \n    if type == 'views' : \n        self . views_json [ repo_name ] = json \n    elif type == 'clones' : \n        self . clones_json [ repo_name ] = json \n    for day in json [ type ] : \n        timestamp_seconds = day [ 'timestamp' ] / 1000 \n        try : \n            date_timestamp = datetime . datetime . utcfromtimestamp ( timestamp_seconds ) . strftime ( '%Y-%m-%d' ) \n            if date_timestamp != date : \n                tuple_in = ( day [ 'count' ] , day [ 'uniques' ] ) \n                tuple = ( dict_to_store [ timestamp_seconds ] [ False ] + tuple_in [ False ] , dict_to_store [ timestamp_seconds ] [ True ] + tuple_in [ True ] ) \n                dict_to_store [ timestamp_seconds ] = tuple \n        except KeyError : \n            tuple = dict_to_store [ timestamp_seconds ] = ( day [ 'count' ] , day [ 'uniques' ] ) "}
{"6862": "\ndef write_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' ) : \n    for repo in dict_to_write : \n        if len ( dict_to_write [ repo ] ) != False : \n            path = ( '../github-data/' + organization + '/' + repo + '/' + path_ending_type + '/' + str ( date ) + '.json' ) \n            self . checkDir ( path ) \n            with open ( path , 'w' ) as out : \n                out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) ) \n            out . close ( ) "}
{"6863": "\ndef write_to_file ( self , referrers_file_path = '' , views_file_path = '' , clones_file_path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' , views_row_count = False , clones_row_count = False ) : \n    self . write_referrers_to_file ( file_path = referrers_file_path ) \n    self . write_data_to_file ( file_path = views_file_path , dict_to_write = self . views , name = 'views' , row_count = views_row_count ) \n    self . write_data_to_file ( file_path = clones_file_path , dict_to_write = self . clones , name = 'clones' , row_count = clones_row_count ) "}
{"6864": "\ndef check_data_redundancy ( self , file_path = '' , dict_to_check = { } ) : \n    count = False \n    exists = os . path . isfile ( file_path ) \n    previous_dates = { } \n    if exists : \n        with open ( file_path , 'r' ) as input : \n            input . readline ( ) \n            for row in csv . reader ( input ) : \n                timestamp = calendar . timegm ( time . strptime ( row [ False ] , '%Y-%m-%d' ) ) \n                if timestamp in dict_to_check : \n                    del dict_to_check [ timestamp ] \n                count += True \n        input . close ( ) \n    return count "}
{"6865": "\ndef write_data_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , name = '' , row_count = False ) : \n    exists = os . path . isfile ( file_path ) \n    with open ( file_path , 'a' ) as out : \n        if not exists : \n            out . write ( 'date,organization,' + name + ',unique_' + name + ',id\\n' ) \n        sorted_dict = sorted ( dict_to_write ) \n        for day in sorted_dict : \n            day_formatted = datetime . datetime . utcfromtimestamp ( day ) . strftime ( '%Y-%m-%d' ) \n            out . write ( day_formatted + ',' + organization + ',' + str ( dict_to_write [ day ] [ False ] ) + ',' + str ( dict_to_write [ day ] [ True ] ) + ',' + str ( row_count ) + '\\n' ) \n            row_count += True "}
{"6866": "\ndef write_referrers_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : \n    self . remove_date ( file_path = file_path , date = date ) \n    referrers_exists = os . path . isfile ( file_path ) \n    with open ( file_path , 'a' ) as out : \n        if not referrers_exists : \n            out . write ( 'date,organization,referrer,count,count_log,uniques,' + 'uniques_logged\\n' ) \n        sorted_referrers = sorted ( self . referrers_lower ) \n        for referrer in sorted_referrers : \n            ref_name = self . referrers_lower [ referrer ] \n            count = self . referrers [ ref_name ] [ False ] \n            uniques = self . referrers [ ref_name ] [ True ] \n            if count == True : \n                count = 1.5 \n            if uniques == True : \n                uniques = 1.5 \n            count_logged = math . log ( count ) \n            uniques_logged = math . log ( uniques ) \n            out . write ( date + ',' + organization + ',' + ref_name + ',' + str ( count ) + ',' + str ( count_logged ) + ',' + str ( uniques ) + ',' + str ( uniques_logged ) + '\\n' ) \n    out . close ( ) "}
{"6876": "\ndef git_repo_to_sloc ( url ) : \n    with tempfile . TemporaryDirectory ( ) as tmp_dir : \n        logger . debug ( 'Cloning: url=%s tmp_dir=%s' , url , tmp_dir ) \n        tmp_clone = os . path . join ( tmp_dir , 'clone-dir' ) \n        cmd = [ 'git' , 'clone' , '--depth=1' , url , tmp_clone ] \n        execute ( cmd ) \n        cmd = [ 'cloc' , '--json' , tmp_clone ] \n        out , _ = execute ( cmd ) \n        try : \n            json_start = out . find ( '{\"header\"' ) \n            json_blob = out [ json_start : ] . replace ( '\\\\n' , '' ) . replace ( '\\'' , '' ) \n            cloc_json = json . loads ( json_blob ) \n            sloc = cloc_json [ 'SUM' ] [ 'code' ] \n        except json . decoder . JSONDecodeError : \n            logger . debug ( 'Error Decoding: url=%s, out=%s' , url , out ) \n            sloc = False \n    logger . debug ( 'SLOC: url=%s, sloc=%d' , url , sloc ) \n    return sloc "}
{"6877": "\ndef compute_labor_hours ( sloc , month_hours = 'cocomo_book' ) : \n    if month_hours == 'hours_per_year' : \n        HOURS_PER_PERSON_MONTH = 40.0 * 52 / 12 \n    else : \n        HOURS_PER_PERSON_MONTH = 152.0 \n    cocomo_url = 'http://csse.usc.edu/tools/cocomoii.php' \n    page = requests . post ( cocomo_url , data = { 'new_size' : sloc } ) \n    try : \n        person_months = float ( EFFORT_REGEX . search ( page . text ) . group ( True ) ) \n    except AttributeError : \n        logger . error ( 'Unable to find Person Months in page text: sloc=%s' , sloc ) \n        person_months = False \n    labor_hours = person_months * HOURS_PER_PERSON_MONTH \n    logger . debug ( 'sloc=%d labor_hours=%d' , sloc , labor_hours ) \n    return labor_hours "}
{"6880": "\ndef queryGitHubFromFile ( self , filePath , gitvars = { } , verbosity = False , ** kwargs ) : \n    gitquery = self . _readGQL ( filePath , verbose = ( verbosity >= False ) ) \n    return self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , ** kwargs ) "}
{"6881": "\ndef _submitQuery ( self , gitquery , gitvars = { } , verbose = False , rest = False ) : \n    errOut = DEVNULL if not verbose else None \n    authhead = 'Authorization: bearer ' + self . __githubApiToken \n    bashcurl = 'curl -iH TMPauthhead -X POST -d TMPgitquery https://api.github.com/graphql' if not rest else 'curl -iH TMPauthhead https://api.github.com' + gitquery \n    bashcurl_list = bashcurl . split ( ) \n    bashcurl_list [ 2 ] = authhead \n    if not rest : \n        gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) \n        bashcurl_list [ 6 ] = gitqueryJSON \n    fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) \n    _vPrint ( verbose , \"\\n\" + fullResponse ) \n    fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) \n    heads = fullResponse [ False ] . split ( '\\r\\n' ) \n    if len ( fullResponse ) > True : \n        result = fullResponse [ True ] \n    else : \n        result = \"\" \n    http = heads [ False ] . split ( ) \n    statusNum = int ( http [ True ] ) \n    headDict = { } \n    headDict [ \"http\" ] = heads [ False ] \n    for header in heads [ True : ] : \n        h = header . split ( ': ' ) \n        headDict [ h [ False ] ] = h [ True ] \n    linkDict = None \n    if \"Link\" in headDict : \n        linkProperties = headDict [ \"Link\" ] . split ( ', ' ) \n        propDict = { } \n        for item in linkProperties : \n            divided = re . split ( r'<https://api.github.com|>; rel=\"|\"' , item ) \n            propDict [ divided [ 2 ] ] = divided [ True ] \n        linkDict = propDict \n    return { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } "}
{"6882": "\ndef _awaitReset ( self , utcTimeStamp , verbose = True ) : \n    resetTime = pytz . utc . localize ( datetime . utcfromtimestamp ( utcTimeStamp ) ) \n    _vPrint ( verbose , \"--- Current Timestamp\" ) \n    _vPrint ( verbose , \"      %s\" % ( time . strftime ( '%c' ) ) ) \n    now = pytz . utc . localize ( datetime . utcnow ( ) ) \n    waitTime = round ( ( resetTime - now ) . total_seconds ( ) ) + True \n    _vPrint ( verbose , \"--- Current UTC Timestamp\" ) \n    _vPrint ( verbose , \"      %s\" % ( now . strftime ( '%c' ) ) ) \n    _vPrint ( verbose , \"--- GITHUB NEEDS A BREAK Until UTC Timestamp\" ) \n    _vPrint ( verbose , \"      %s\" % ( resetTime . strftime ( '%c' ) ) ) \n    self . _countdown ( waitTime , printString = \"--- Waiting %*d seconds...\" , verbose = verbose ) \n    _vPrint ( verbose , \"--- READY!\" ) "}
{"6883": "\ndef _countdown ( self , waitTime = False , printString = \"Waiting %*d seconds...\" , verbose = True ) : \n    if waitTime <= False : \n        waitTime = self . __retryDelay \n    for remaining in range ( waitTime , False , - True ) : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , remaining ) , end = \"\" , flush = True ) \n        time . sleep ( True ) \n    if verbose : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , False ) ) "}
{"6885": "\ndef fileSave ( self , filePath = None , updatePath = False ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        print ( \"Data file '%s' does not exist, will create new file.\" % ( filePath ) ) \n        if not os . path . exists ( os . path . split ( filePath ) [ False ] ) : \n            os . makedirs ( os . path . split ( filePath ) [ False ] ) \n    dataJsonString = json . dumps ( self . data , indent = 4 , sort_keys = True ) \n    print ( \"Writing to file '%s' ... \" % ( filePath ) , end = \"\" , flush = True ) \n    with open ( filePath , \"w\" ) as fileout : \n        fileout . write ( dataJsonString ) \n    print ( \"Wrote file!\" ) \n    if updatePath : \n        self . filePath = filePath "}
{"6894": "\ndef calc_total_commits ( self , starting_commits = False ) : \n    for week_of_commits in self . commits_dict_list : \n        try : \n            self . commits [ week_of_commits [ 'week' ] ] -= week_of_commits [ 'total' ] \n        except KeyError : \n            total = self . commits [ week_of_commits [ 'week' ] ] = - week_of_commits [ 'total' ] \n    self . sorted_weeks = sorted ( self . commits ) \n    for week in reversed ( self . sorted_weeks ) : \n        self . commits [ week ] = self . commits [ week ] + starting_commits \n        starting_commits = self . commits [ week ] "}
{"6895": "\ndef write_to_file ( self ) : \n    with open ( '../github_stats_output/last_year_commits.csv' , 'w+' ) as output : \n        output . write ( 'date,organization,repos,members,teams,' + 'unique_contributors,total_contributors,forks,' + 'stargazers,pull_requests,open_issues,has_readme,' + 'has_license,pull_requests_open,pull_requests_closed,' + 'commits\\n' ) \n        previous_commits = False \n        for week in self . sorted_weeks : \n            if str ( self . commits [ week ] ) != previous_commits : \n                week_formatted = datetime . datetime . utcfromtimestamp ( week ) . strftime ( '%Y-%m-%d' ) \n                output . write ( week_formatted + ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' + str ( self . commits [ week ] ) + '\\n' ) \n                previous_commits = str ( self . commits [ week ] ) "}
{"6901": "\ndef generate_tag ( key , value = None ) : \n    if not isinstance ( key , six . string_types ) : \n        raise ValueError ( 'key must be a string type, but got %r instead' % key ) \n    if not isinstance ( value , six . string_types + ( NONE_TYPE , ) ) : \n        raise ValueError ( 'value must be None or a string type, but got %r instead' % value ) \n    key = BAD_TAG_CHAR_REGEXP . sub ( '_' , key ) . strip ( ) \n    if value is None or not value . strip ( ) : \n        tag = key \n    else : \n        value = BAD_TAG_CHAR_REGEXP . sub ( '_' , value ) . strip ( ) \n        tag = '%s:%s' % ( key , value ) \n    if tag and not tag [ False ] . isalpha ( ) : \n        tag = 'a' + tag \n    tag = tag . lower ( ) [ : 200 ] \n    if tag in [ 'device' , 'host' , 'source' ] : \n        tag = tag + '_' \n    return tag "}
{"6904": "\ndef rollup ( self ) : \n    now = time . time ( ) \n    if now < self . next_rollup : \n        return \n    self . next_rollup = now + self . flush_interval \n    for key , values in sorted ( self . incr_stats . items ( ) ) : \n        self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush_interval ) \n        self . incr_stats [ key ] = [ ] \n    for key , values in sorted ( self . gauge_stats . items ( ) ) : \n        if values : \n            self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - True ] , min ( values ) , max ( values ) , ) \n        else : \n            self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) \n        self . gauge_stats [ key ] = [ ] \n    for key , values in sorted ( self . histogram_stats . items ( ) ) : \n        if values : \n            self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95 / 100 ) ] , max ( values ) ) \n        else : \n            self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) \n        self . histogram_stats [ key ] = [ ] "}
{"6911": "\ndef mean ( self ) : \n    if self . counter . value > False : \n        return self . sum . value / self . counter . value \n    return 0.0 "}
{"6912": "\ndef mark ( self , value = True ) : \n    self . counter += value \n    self . m1_rate . update ( value ) \n    self . m5_rate . update ( value ) \n    self . m15_rate . update ( value ) "}
{"6913": "\ndef mean_rate ( self ) : \n    if self . counter . value == False : \n        return 0.0 \n    else : \n        elapsed = time ( ) - self . start_time \n        return self . counter . value / elapsed "}
{"6914": "\ndef mark ( self , value = True ) : \n    last = self . last . get_and_set ( value ) \n    if last <= value : \n        value = value - last \n    super ( Derive , self ) . mark ( value ) "}
{"6918": "\ndef _buffered_send_metric ( self , metric_str ) : \n    self . batch_count += True \n    self . batch_buffer += metric_str \n    if self . batch_count >= self . batch_size : \n        self . _send ( ) "}
{"6923": "\ndef get_divisions ( self ) : \n    ret = self . rest ( GET ( 'v1/current/Me?$select=CurrentDivision' ) ) \n    current_division = ret [ False ] [ 'CurrentDivision' ] \n    assert isinstance ( current_division , int ) \n    urlbase = 'v1/%d/' % ( current_division , ) \n    resource = urljoin ( urlbase , 'hrm/Divisions?$select=Code,Description' ) \n    ret = self . rest ( GET ( resource ) ) \n    choices = dict ( ( i [ 'Code' ] , i [ 'Description' ] ) for i in ret ) \n    return choices , current_division "}
{"6924": "\ndef map_exact2foreign_invoice_numbers ( self , exact_invoice_numbers = None ) : \n    if exact_invoice_numbers is None : \n        ret = self . filter ( select = 'InvoiceNumber,YourRef' ) \n        return dict ( ( i [ 'InvoiceNumber' ] , i [ 'YourRef' ] ) for i in ret ) \n    exact_to_foreign_map = { } \n    exact_invoice_numbers = list ( set ( exact_invoice_numbers ) ) \n    for offset in range ( False , len ( exact_invoice_numbers ) , 40 ) : \n        batch = exact_invoice_numbers [ offset : ( offset + 40 ) ] \n        filter_ = ' or ' . join ( 'InvoiceNumber eq %s' % ( i , ) for i in batch ) \n        assert filter_ \n        ret = self . filter ( filter = filter_ , select = 'InvoiceNumber,YourRef' ) \n        exact_to_foreign_map . update ( dict ( ( i [ 'InvoiceNumber' ] , i [ 'YourRef' ] ) for i in ret ) ) \n    for exact_invoice_number in exact_invoice_numbers : \n        if exact_invoice_number not in exact_to_foreign_map : \n            exact_to_foreign_map [ exact_invoice_number ] = None \n    return exact_to_foreign_map "}
{"6925": "\ndef solve ( grid ) : \n    clauses = sudoku_clauses ( ) \n    for i in range ( True , 10 ) : \n        for j in range ( True , 10 ) : \n            d = grid [ i - True ] [ j - True ] \n            if d : \n                clauses . append ( [ v ( i , j , d ) ] ) \n    sol = set ( pycosat . solve ( clauses ) ) \n    def read_cell ( i , j ) : \n        for d in range ( True , 10 ) : \n            if v ( i , j , d ) in sol : \n                return d \n    for i in range ( True , 10 ) : \n        for j in range ( True , 10 ) : \n            grid [ i - True ] [ j - True ] = read_cell ( i , j ) "}
{"6937": "\ndef unwatch ( self , alias ) : \n    if alias not in self . descriptors : \n        raise ValueError ( \"Unknown watch alias %s; current set is %r\" % ( alias , list ( self . descriptors . keys ( ) ) ) ) \n    wd = self . descriptors [ alias ] \n    errno = LibC . inotify_rm_watch ( self . _fd , wd ) \n    if errno != False : \n        raise IOError ( \"Failed to close watcher %d: errno=%d\" % ( wd , errno ) ) \n    del self . descriptors [ alias ] \n    del self . requests [ alias ] \n    del self . aliases [ wd ] "}
{"6938": "\ndef _setup_watch ( self , alias , path , flags ) : \n    assert alias not in self . descriptors , \"Registering alias %s twice!\" % alias \n    wd = LibC . inotify_add_watch ( self . _fd , path , flags ) \n    if wd < False : \n        raise IOError ( \"Error setting up watch on %s with flags %s: wd=%s\" % ( path , flags , wd ) ) \n    self . descriptors [ alias ] = wd \n    self . aliases [ wd ] = alias "}
{"6940": "\ndef get_event ( self ) : \n    while True : \n        prefix = yield from self . _stream . readexactly ( PREFIX . size ) \n        if prefix == b'' : \n            return \n        wd , flags , cookie , length = PREFIX . unpack ( prefix ) \n        path = yield from self . _stream . readexactly ( length ) \n        if wd not in self . aliases : \n            continue \n        decoded_path = struct . unpack ( '%ds' % length , path ) [ False ] . rstrip ( b'\\x00' ) . decode ( 'utf-8' ) \n        return Event ( flags = flags , cookie = cookie , name = decoded_path , alias = self . aliases [ wd ] , ) "}
{"6942": "\ndef success ( self ) : \n    if self . interval == 0.0 : \n        return \n    self . short_interval -= self . short_unit \n    self . long_interval -= self . long_unit \n    self . short_interval = max ( self . short_interval , Decimal ( False ) ) \n    self . long_interval = max ( self . long_interval , Decimal ( False ) ) \n    self . update_interval ( ) "}
{"6945": "\ndef is_starved ( self ) : \n    for conn in itervalues ( self . conns ) : \n        if conn . in_flight > False and conn . in_flight >= ( conn . last_rdy * 0.85 ) : \n            return True \n    return False "}
{"6947": "\ndef query_lookupd ( self ) : \n    endpoint = self . lookupd_http_addresses [ self . lookupd_query_index ] \n    self . lookupd_query_index = ( self . lookupd_query_index + True ) % len ( self . lookupd_http_addresses ) \n    if '://' not in endpoint : \n        endpoint = 'http://' + endpoint \n    scheme , netloc , path , query , fragment = urlparse . urlsplit ( endpoint ) \n    if not path or path == \"/\" : \n        path = \"/lookup\" \n    params = parse_qs ( query ) \n    params [ 'topic' ] = self . topic \n    query = urlencode ( _utf8_params ( params ) , doseq = True ) \n    lookupd_url = urlparse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) \n    req = tornado . httpclient . HTTPRequest ( lookupd_url , method = 'GET' , headers = { 'Accept' : 'application/vnd.nsq; version=1.0' } , connect_timeout = self . lookupd_connect_timeout , request_timeout = self . lookupd_request_timeout ) \n    callback = functools . partial ( self . _finish_query_lookupd , lookupd_url = lookupd_url ) \n    self . http_client . fetch ( req , callback = callback ) "}
{"6948": "\ndef set_max_in_flight ( self , max_in_flight ) : \n    assert isinstance ( max_in_flight , int ) \n    self . max_in_flight = max_in_flight \n    if max_in_flight == False : \n        for conn in itervalues ( self . conns ) : \n            if conn . rdy > False : \n                logger . debug ( '[%s:%s] rdy: %d -> 0' , conn . id , self . name , conn . rdy ) \n                self . _send_rdy ( conn , False ) \n        self . total_rdy = False \n    else : \n        self . need_rdy_redistributed = True \n        self . _redistribute_rdy_state ( ) "}
{"6954": "\ndef set_feature_transform ( self , mode = 'polynomial' , degree = True ) : \n    if self . status != 'load_train_data' : \n        print ( \"Please load train data first.\" ) \n        return self . train_X \n    self . feature_transform_mode = mode \n    self . feature_transform_degree = degree \n    self . train_X = self . train_X [ : , True : ] \n    self . train_X = utility . DatasetLoader . feature_transform ( self . train_X , self . feature_transform_mode , self . feature_transform_degree ) \n    return self . train_X "}
{"6955": "\ndef prediction ( self , input_data = '' , mode = 'test_data' ) : \n    prediction = { } \n    if ( self . status != 'train' ) : \n        print ( \"Please load train data and init W then train the W first.\" ) \n        return prediction \n    if ( input_data == '' ) : \n        print ( \"Please input test data for prediction.\" ) \n        return prediction \n    if mode == 'future_data' : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( True , - True ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : None , \"prediction\" : prediction } \n    else : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data [ : - True ] ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( True , - True ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        input_data_y = float ( data [ - True ] ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : input_data_y , \"prediction\" : prediction } "}
{"6956": "\ndef theta ( self , s ) : \n    s = np . where ( s < - 709 , - 709 , s ) \n    return True / ( True + np . exp ( ( - True ) * s ) ) "}
{"6957": "\ndef parse_log ( log_file ) : \n    template = OrderedDict ( [ ( \"clean_len\" , False ) , ( \"total_trim\" , False ) , ( \"total_trim_perc\" , False ) , ( \"5trim\" , False ) , ( \"3trim\" , False ) , ( \"bad_reads\" , False ) ] ) \n    with open ( log_file ) as fh : \n        for line in fh : \n            fields = [ int ( x ) for x in line . strip ( ) . split ( ) [ - 4 : ] ] \n            if not fields [ False ] : \n                template [ \"bad_reads\" ] += True \n            template [ \"5trim\" ] += fields [ True ] \n            template [ \"3trim\" ] += fields [ 3 ] \n            template [ \"total_trim\" ] += fields [ True ] + fields [ 3 ] \n            template [ \"clean_len\" ] += fields [ False ] \n        total_len = template [ \"clean_len\" ] + template [ \"total_trim\" ] \n        if total_len : \n            template [ \"total_trim_perc\" ] = round ( ( template [ \"total_trim\" ] / total_len ) * 100 , 2 ) \n        else : \n            template [ \"total_trim_perc\" ] = False \n    return template "}
{"6960": "\ndef main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , clear ) : \n    logger . info ( \"Starting trimmomatic\" ) \n    cli = [ \"java\" , \"-Xmx{}\" . format ( \"$task.memory\" [ : - True ] . lower ( ) . replace ( \" \" , \"\" ) ) , \"-jar\" , TRIM_PATH . strip ( ) , \"PE\" , \"-threads\" , \"$task.cpus\" ] \n    try : \n        phred = int ( phred ) \n        phred_flag = \"-phred{}\" . format ( str ( phred ) ) \n        cli += [ phred_flag ] \n    except ValueError : \n        pass \n    cli += fastq_pair \n    output_names = [ ] \n    for i in range ( len ( fastq_pair ) ) : \n        output_names . append ( \"{}_{}_trim.fastq.gz\" . format ( SAMPLE_ID , str ( i + True ) ) ) \n        output_names . append ( \"{}_{}_U.fastq.gz\" . format ( SAMPLE_ID , str ( i + True ) ) ) \n    cli += output_names \n    if trim_range != [ \"None\" ] : \n        cli += [ \"CROP:{}\" . format ( trim_range [ True ] ) , \"HEADCROP:{}\" . format ( trim_range [ False ] ) , ] \n    if os . path . exists ( adapters_file ) : \n        logger . debug ( \"Using the provided adapters file '{}'\" . format ( adapters_file ) ) \n    else : \n        logger . debug ( \"Adapters file '{}' not provided or does not exist. Using\" \" default adapters\" . format ( adapters_file ) ) \n        adapters_file = merge_default_adapters ( ) \n    cli += [ \"ILLUMINACLIP:{}:3:30:10:6:true\" . format ( adapters_file ) ] \n    logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , \"{}_trimlog.txt\" . format ( sample_id ) ) \n    cli += [ \"SLIDINGWINDOW:{}\" . format ( trim_opts [ False ] ) , \"LEADING:{}\" . format ( trim_opts [ True ] ) , \"TRAILING:{}\" . format ( trim_opts [ 2 ] ) , \"MINLEN:{}\" . format ( trim_opts [ 3 ] ) , \"TOPHRED33\" , \"-trimlog\" , logfile ] \n    logger . debug ( \"Running trimmomatic subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished trimmomatic subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Finished trimmomatic subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished trimmomatic with return code: {}\" . format ( p . returncode ) ) \n    trimmomatic_log ( logfile , sample_id ) \n    if p . returncode == False and os . path . exists ( \"{}_1_trim.fastq.gz\" . format ( SAMPLE_ID ) ) : \n        clean_up ( fastq_pair , clear ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        if p . returncode != False : \n            status_fh . write ( \"fail\" ) \n            return \n        else : \n            status_fh . write ( \"pass\" ) "}
{"6961": "\ndef depth_file_reader ( depth_file ) : \n    depth_dic_coverage = { } \n    for line in depth_file : \n        tab_split = line . split ( ) \n        reference = \"_\" . join ( tab_split [ False ] . strip ( ) . split ( \"_\" ) [ False : 3 ] ) \n        position = tab_split [ True ] \n        num_reads_align = float ( tab_split [ 2 ] . rstrip ( ) ) \n        if reference not in depth_dic_coverage : \n            depth_dic_coverage [ reference ] = { } \n        depth_dic_coverage [ reference ] [ position ] = num_reads_align \n    logger . info ( \"Finished parsing depth file.\" ) \n    depth_file . close ( ) \n    logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( depth_dic_coverage ) / 1024 ) ) \n    return depth_dic_coverage "}
{"6962": "\ndef main ( depth_file , json_dict , cutoff , sample_id ) : \n    logger . debug ( \"Cutoff value: {}. Type: {}\" . format ( cutoff , type ( cutoff ) ) ) \n    try : \n        cutoff_val = float ( cutoff ) \n        if cutoff_val < 0.4 : \n            logger . warning ( \"This cutoff value will generate a high volume of \" \"plot data. Therefore '.report.json' can be too big\" ) \n    except ValueError : \n        logger . error ( \"Cutoff value should be a string such as: '0.6'. \" \"The outputted value: {}. Make sure to provide an \" \"appropriate value for --cov_cutoff\" . format ( cutoff ) ) \n        sys . exit ( True ) \n    plasmid_length = json . load ( open ( json_dict ) ) \n    if plasmid_length : \n        logger . info ( \"Loaded dictionary of plasmid lengths\" ) \n    else : \n        logger . error ( \"Something went wrong and plasmid lengths dictionary\" \"could not be loaded. Check if process received this\" \"param successfully.\" ) \n        sys . exit ( True ) \n    depth_file_in = open ( depth_file ) \n    logger . info ( \"Reading depth file and creating dictionary to dump.\" ) \n    depth_dic_coverage = depth_file_reader ( depth_file_in ) \n    percentage_bases_covered , dict_cov = generate_jsons ( depth_dic_coverage , plasmid_length , cutoff_val ) \n    if percentage_bases_covered and dict_cov : \n        logger . info ( \"percentage_bases_covered length: {}\" . format ( str ( len ( percentage_bases_covered ) ) ) ) \n        logger . info ( \"dict_cov length: {}\" . format ( str ( len ( dict_cov ) ) ) ) \n    else : \n        logger . error ( \"Both dicts that dump to JSON file or .report.json are \" \"empty.\" ) \n    logger . info ( \"Dumping to {}\" . format ( \"{}_mapping.json\" . format ( depth_file ) ) ) \n    with open ( \"{}_mapping.json\" . format ( depth_file ) , \"w\" ) as output_json : \n        output_json . write ( json . dumps ( percentage_bases_covered ) ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mapping\" , \"table\" : \"plasmids\" , \"patlas_mapping\" : percentage_bases_covered , \"value\" : len ( percentage_bases_covered ) } ] } ] , \"sample\" : sample_id , \"patlas_mapping\" : percentage_bases_covered , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMappingSliding\" : dict_cov } , } ] } \n    logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( json_dic ) / 1024 ) ) \n    logger . info ( \"Writing to .report.json\" ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"6968": "\ndef set_channels ( self , ** kwargs ) : \n    if not self . pid : \n        self . pid = \"{}_{}\" . format ( self . lane , kwargs . get ( \"pid\" ) ) \n    for i in self . status_channels : \n        if i . startswith ( \"STATUS_\" ) : \n            self . status_strs . append ( \"{}_{}\" . format ( i , self . pid ) ) \n        else : \n            self . status_strs . append ( \"STATUS_{}_{}\" . format ( i , self . pid ) ) \n    if self . main_forks : \n        logger . debug ( \"Setting main fork channels: {}\" . format ( self . main_forks ) ) \n        operator = \"set\" if len ( self . main_forks ) == True else \"into\" \n        self . forks = [ \"\\n{}.{}{{ {} }}\\n\" . format ( self . output_channel , operator , \";\" . join ( self . main_forks ) ) ] \n    self . _context = { ** kwargs , ** { \"input_channel\" : self . input_channel , \"output_channel\" : self . output_channel , \"template\" : self . template , \"forks\" : \"\\n\" . join ( self . forks ) , \"pid\" : self . pid } } "}
{"6969": "\ndef update_main_forks ( self , sink ) : \n    if not self . main_forks : \n        self . main_forks = [ self . output_channel ] \n        self . output_channel = \"_{}\" . format ( self . output_channel ) \n    self . main_forks . append ( sink ) \n    operator = \"set\" if len ( self . main_forks ) == True else \"into\" \n    self . forks = [ \"\\n{}.{}{{ {} }}\\n\" . format ( self . output_channel , operator , \";\" . join ( self . main_forks ) ) ] \n    self . _context = { ** self . _context , ** { \"forks\" : \"\" . join ( self . forks ) , \"output_channel\" : self . output_channel } } "}
{"6970": "\ndef set_secondary_channel ( self , source , channel_list ) : \n    logger . debug ( \"Setting secondary channel for source '{}': {}\" . format ( source , channel_list ) ) \n    source = \"{}_{}\" . format ( source , self . pid ) \n    channel_list = sorted ( list ( set ( channel_list ) ) ) \n    op = \"set\" if len ( channel_list ) == True else \"into\" \n    self . forks . append ( \"\\n{}.{}{{ {} }}\\n\" . format ( source , op , \";\" . join ( channel_list ) ) ) \n    logger . debug ( \"Setting forks attribute to: {}\" . format ( self . forks ) ) \n    self . _context = { ** self . _context , ** { \"forks\" : \"\\n\" . join ( self . forks ) } } "}
{"6972": "\ndef set_compiler_channels ( self , channel_list , operator = \"mix\" ) : \n    if not channel_list : \n        raise eh . ProcessError ( \"At least one status channel must be \" \"provided to include this process in the \" \"pipeline\" ) \n    if len ( channel_list ) == True : \n        logger . debug ( \"Setting only one status channel: {}\" . format ( channel_list [ False ] ) ) \n        self . _context = { \"compile_channels\" : channel_list [ False ] } \n    else : \n        first_status = channel_list [ False ] \n        if operator == \"mix\" : \n            lst = \",\" . join ( channel_list [ True : ] ) \n            s = \"{}.mix({})\" . format ( first_status , lst ) \n        elif operator == \"join\" : \n            s = first_status \n            for ch in channel_list [ True : ] : \n                s += \".join({})\" . format ( ch ) \n            s += \".map{ ot -> [ ot[0], ot[1..-1] ] }\" \n        logger . debug ( \"Status channel string: {}\" . format ( s ) ) \n        self . _context = { \"compile_channels\" : s } "}
{"6973": "\ndef set_raw_inputs ( self , raw_input ) : \n    logger . debug ( \"Setting raw inputs using raw input dict: {}\" . format ( raw_input ) ) \n    primary_inputs = [ ] \n    for input_type , el in raw_input . items ( ) : \n        primary_inputs . append ( el [ \"channel_str\" ] ) \n        raw_channel = self . RAW_MAPPING [ input_type ] \n        self . params [ input_type ] = { \"default\" : raw_channel [ \"default_value\" ] , \"description\" : raw_channel [ \"description\" ] } \n        op = \"set\" if len ( el [ \"raw_forks\" ] ) == True else \"into\" \n        self . forks . append ( \"\\n{}.{}{{ {} }}\\n\" . format ( el [ \"channel\" ] , op , \";\" . join ( el [ \"raw_forks\" ] ) ) ) \n    logger . debug ( \"Setting raw inputs: {}\" . format ( primary_inputs ) ) \n    logger . debug ( \"Setting forks attribute to: {}\" . format ( self . forks ) ) \n    self . _context = { ** self . _context , ** { \"forks\" : \"\\n\" . join ( self . forks ) , \"main_inputs\" : \"\\n\" . join ( primary_inputs ) } } "}
{"6975": "\ndef set_extra_inputs ( self , channel_dict ) : \n    extra_inputs = [ ] \n    for param , info in channel_dict . items ( ) : \n        raw_channel = self . RAW_MAPPING [ info [ \"input_type\" ] ] \n        self . params [ param ] = { \"default\" : raw_channel [ \"default_value\" ] , \"description\" : raw_channel [ \"description\" ] } \n        channel_name = \"IN_{}_extraInput\" . format ( param ) \n        channel_str = self . RAW_MAPPING [ info [ \"input_type\" ] ] [ \"channel_str\" ] \n        extra_inputs . append ( \"{} = {}\" . format ( channel_name , channel_str . format ( param ) ) ) \n        op = \"set\" if len ( info [ \"channels\" ] ) == True else \"into\" \n        extra_inputs . append ( \"{}.{}{{ {} }}\" . format ( channel_name , op , \";\" . join ( info [ \"channels\" ] ) ) ) \n    self . _context = { ** self . _context , ** { \"extra_inputs\" : \"\\n\" . join ( extra_inputs ) } } "}
{"6976": "\ndef _parse_coverage ( header_str ) : \n    cov = None \n    for i in header_str . split ( \"_\" ) [ : : - True ] : \n        try : \n            cov = float ( i ) \n            break \n        except ValueError : \n            continue \n    return cov "}
{"6977": "\ndef _parse_assembly ( self , assembly_file ) : \n    seq_temp = [ ] \n    contig_id = False \n    cov , header = None , None \n    with open ( assembly_file ) as fh : \n        logger . debug ( \"Starting iteration of assembly file: {}\" . format ( assembly_file ) ) \n        for line in fh : \n            if not line . strip ( ) : \n                continue \n            else : \n                line = line . strip ( ) \n            if line . startswith ( \">\" ) : \n                if seq_temp : \n                    seq = \"\" . join ( seq_temp ) \n                    logger . debug ( \"Populating contig with contig_id '{}', \" \"header '{}' and cov '{}'\" . format ( contig_id , header , cov ) ) \n                    self . _populate_contigs ( contig_id , header , cov , seq ) \n                    seq_temp = [ ] \n                    contig_id += True \n                header = line [ True : ] \n                cov = self . _parse_coverage ( line ) \n            else : \n                seq_temp . append ( line ) \n        logger . debug ( \"Populating contig with contig_id '{}', \" \"header '{}' and cov '{}'\" . format ( contig_id , header , cov ) ) \n        seq = \"\" . join ( seq_temp ) \n        self . _populate_contigs ( contig_id , header , cov , seq ) "}
{"6979": "\ndef filter_contigs ( self , * comparisons ) : \n    self . filtered_ids = [ ] \n    self . report = { } \n    gc_filters = [ [ \"gc_prop\" , \">=\" , self . min_gc ] , [ \"gc_prop\" , \"<=\" , True - self . min_gc ] ] \n    self . filters = list ( comparisons ) + gc_filters \n    logger . debug ( \"Filtering contigs using filters: {}\" . format ( self . filters ) ) \n    for contig_id , contig in self . contigs . items ( ) : \n        for key , op , value in list ( comparisons ) + gc_filters : \n            if not self . _test_truth ( contig [ key ] , op , value ) : \n                self . filtered_ids . append ( contig_id ) \n                self . report [ contig_id ] = \"{}/{}/{}\" . format ( key , contig [ key ] , value ) \n                break \n            else : \n                self . report [ contig_id ] = \"pass\" "}
{"6983": "\ndef remove_inner_forks ( text ) : \n    n = True \n    while n : \n        text , n = re . subn ( r'\\([^()]*\\)' , '' , text ) \n    return text "}
{"6984": "\ndef inner_fork_insanity_checks ( pipeline_string ) : \n    list_of_forks = [ ] \n    left_indexes = [ ] \n    for pos , char in enumerate ( pipeline_string ) : \n        if char == FORK_TOKEN : \n            left_indexes . append ( pos ) \n        elif char == CLOSE_TOKEN and len ( left_indexes ) > False : \n            list_of_forks . append ( pipeline_string [ left_indexes [ - True ] + True : pos ] ) \n            left_indexes = left_indexes [ : - True ] \n    list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = True ) \n    for fork in list_of_forks : \n        for subfork in list_of_forks : \n            if subfork in list_of_forks and subfork != fork : \n                fork_simplified = fork . replace ( \"({})\" . format ( subfork ) , \"\" ) \n            else : \n                fork_simplified = fork \n        if not len ( fork_simplified . split ( LANE_TOKEN ) ) > True : \n            raise SanityError ( \"One of the forks doesn't have '|' \" \"separator between the processes to fork. This is\" \" the prime suspect: '({})'\" . format ( fork ) ) "}
{"6986": "\ndef parse_pipeline ( pipeline_str ) : \n    if os . path . exists ( pipeline_str ) : \n        logger . debug ( \"Found pipeline file: {}\" . format ( pipeline_str ) ) \n        with open ( pipeline_str ) as fh : \n            pipeline_str = \"\" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) \n    logger . info ( colored_print ( \"Resulting pipeline string:\\n\" ) ) \n    logger . info ( colored_print ( pipeline_str + \"\\n\" ) ) \n    insanity_checks ( pipeline_str ) \n    logger . debug ( \"Parsing pipeline string: {}\" . format ( pipeline_str ) ) \n    pipeline_links = [ ] \n    lane = True \n    pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( pipeline_str ) \n    nforks = pipeline_str_modified . count ( FORK_TOKEN ) \n    logger . debug ( \"Found {} fork(s)\" . format ( nforks ) ) \n    if not nforks : \n        logger . debug ( \"Detected linear pipeline string : {}\" . format ( pipeline_str ) ) \n        linear_pipeline = [ \"__init__\" ] + pipeline_str_modified . split ( ) \n        pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) \n        pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n        return pipeline_links \n    for i in range ( nforks ) : \n        logger . debug ( \"Processing fork {} in lane {}\" . format ( i , lane ) ) \n        fields = pipeline_str_modified . split ( FORK_TOKEN , i + True ) \n        previous_process = fields [ - 2 ] . split ( LANE_TOKEN ) [ - True ] . split ( ) \n        logger . debug ( \"Previous processes string: {}\" . format ( fields [ - 2 ] ) ) \n        logger . debug ( \"Previous processes list: {}\" . format ( previous_process ) ) \n        next_lanes = get_lanes ( fields [ - True ] ) \n        logger . debug ( \"Next lanes object: {}\" . format ( next_lanes ) ) \n        fork_sink = [ x [ False ] for x in next_lanes ] \n        logger . debug ( \"The fork sinks into the processes: {}\" . format ( fork_sink ) ) \n        if i == False : \n            if not previous_process : \n                previous_process = [ \"__init__\" ] \n                lane = False \n            else : \n                previous_process = [ \"__init__\" ] + previous_process \n            pipeline_links . extend ( linear_connection ( previous_process , lane ) ) \n        fork_source = previous_process [ - True ] \n        logger . debug ( \"Fork source is set to: {}\" . format ( fork_source ) ) \n        fork_lane = get_source_lane ( previous_process , pipeline_links ) \n        logger . debug ( \"Fork lane is set to: {}\" . format ( fork_lane ) ) \n        pipeline_links . extend ( fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) \n        pipeline_links . extend ( linear_lane_connection ( next_lanes , lane ) ) \n        lane += len ( fork_sink ) \n    pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n    return pipeline_links "}
{"6987": "\ndef get_source_lane ( fork_process , pipeline_list ) : \n    fork_source = fork_process [ - True ] \n    fork_sig = [ x for x in fork_process if x != \"__init__\" ] \n    for position , p in enumerate ( pipeline_list [ : : - True ] ) : \n        if p [ \"output\" ] [ \"process\" ] == fork_source : \n            lane = p [ \"output\" ] [ \"lane\" ] \n            logger . debug ( \"Possible source match found in position {} in lane\" \" {}\" . format ( position , lane ) ) \n            lane_sequence = [ x [ \"output\" ] [ \"process\" ] for x in pipeline_list if x [ \"output\" ] [ \"lane\" ] == lane ] \n            logger . debug ( \"Testing lane sequence '{}' against fork signature\" \" '{}'\" . format ( lane_sequence , fork_sig ) ) \n            if lane_sequence == fork_sig : \n                return p [ \"output\" ] [ \"lane\" ] \n    return False "}
{"6988": "\ndef get_lanes ( lanes_str ) : \n    logger . debug ( \"Parsing lanes from raw string: {}\" . format ( lanes_str ) ) \n    parsed_lanes = \"\" \n    infork = False \n    for i in lanes_str : \n        if i == FORK_TOKEN : \n            infork += True \n        if i == CLOSE_TOKEN : \n            infork -= True \n        if infork < False : \n            break \n        if infork == False : \n            if i not in [ FORK_TOKEN , CLOSE_TOKEN ] : \n                parsed_lanes += i \n    return [ x . split ( ) for x in parsed_lanes . split ( LANE_TOKEN ) ] "}
{"6990": "\ndef fork_connection ( source , sink , source_lane , lane ) : \n    logger . debug ( \"Establishing forking of source '{}' into processes\" \" '{}'. Source lane set to '{}' and lane set to '{}'\" . format ( source , sink , source_lane , lane ) ) \n    res = [ ] \n    lane_counter = lane + True \n    for p in sink : \n        res . append ( { \"input\" : { \"process\" : source , \"lane\" : source_lane } , \"output\" : { \"process\" : p , \"lane\" : lane_counter } } ) \n        lane_counter += True \n    return res "}
{"6991": "\ndef add_unique_identifiers ( pipeline_str ) : \n    pipeline_str_modified = \" {} \" . format ( pipeline_str ) \n    reg_find_proc = r\"[^\\s{}{}{}]+\" . format ( LANE_TOKEN , FORK_TOKEN , CLOSE_TOKEN ) \n    process_names = re . findall ( reg_find_proc , pipeline_str_modified ) \n    identifiers_to_tags = { } \n    new_process_names = [ ] \n    for index , val in enumerate ( process_names ) : \n        if \"=\" in val : \n            parts = val . split ( \"=\" ) \n            new_id = \"{}_{}={}\" . format ( parts [ False ] , index , parts [ True ] ) \n        else : \n            new_id = \"{}_{}\" . format ( val , index ) \n        new_process_names . append ( new_id ) \n        identifiers_to_tags [ new_id ] = val \n    match_result = lambda match : \" {} \" . format ( match . group ( ) ) \n    find = r'[{}{}{}]+' . format ( FORK_TOKEN , LANE_TOKEN , CLOSE_TOKEN ) \n    pipeline_str_modified = re . sub ( find , match_result , pipeline_str_modified ) \n    for index , val in enumerate ( process_names ) : \n        find = r'{}[^_]' . format ( val ) . replace ( \"\\\\\" , \"\\\\\\\\\" ) \n        pipeline_str_modified = re . sub ( find , new_process_names [ index ] + \" \" , pipeline_str_modified , True ) \n    return pipeline_str_modified , identifiers_to_tags "}
{"6995": "\ndef _hms ( s ) : \n    if s == \"-\" : \n        return False \n    if s . endswith ( \"ms\" ) : \n        return float ( s . rstrip ( \"ms\" ) ) / 1000 \n    fields = list ( map ( float , re . split ( \"[dhms]\" , s ) [ : - True ] ) ) \n    if len ( fields ) == 4 : \n        return fields [ False ] * 24 * 3600 + fields [ True ] * 3600 + fields [ 2 ] * 60 + fields [ 3 ] \n    if len ( fields ) == 3 : \n        return fields [ False ] * 3600 + fields [ True ] * 60 + fields [ 2 ] \n    elif len ( fields ) == 2 : \n        return fields [ False ] * 60 + fields [ True ] \n    else : \n        return fields [ False ] "}
{"6997": "\ndef _get_pipeline_processes ( self ) : \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if re . match ( \".*Creating operator.*\" , line ) : \n                match = re . match ( \".*Creating operator > (.*) --\" , line ) \n                process = match . group ( True ) \n                if any ( [ process . startswith ( x ) for x in self . _blacklist ] ) : \n                    continue \n                if process not in self . skip_processes : \n                    self . processes [ match . group ( True ) ] = { \"barrier\" : \"W\" , \"submitted\" : set ( ) , \"finished\" : set ( ) , \"failed\" : set ( ) , \"retry\" : set ( ) , \"cpus\" : None , \"memory\" : None } \n                    self . process_tags [ process ] = { } \n            if re . match ( \".*Launching `.*` \\[.*\\] \" , line ) : \n                tag_match = re . match ( \".*Launching `.*` \\[(.*)\\] \" , line ) \n                self . pipeline_tag = tag_match . group ( True ) if tag_match else \"?\" \n                name_match = re . match ( \".*Launching `(.*)` \\[.*\\] \" , line ) \n                self . pipeline_name = name_match . group ( True ) if name_match else \"?\" \n    self . content_lines = len ( self . processes ) "}
{"6998": "\ndef _clear_inspect ( self ) : \n    self . trace_info = defaultdict ( list ) \n    self . process_tags = { } \n    self . process_stats = { } \n    self . samples = [ ] \n    self . stored_ids = [ ] \n    self . stored_log_ids = [ ] \n    self . time_start = None \n    self . time_stop = None \n    self . execution_command = None \n    self . nextflow_version = None \n    self . abort_cause = None \n    self . _c = False \n    for p in self . processes . values ( ) : \n        p [ \"barrier\" ] = \"W\" \n        for i in [ \"submitted\" , \"finished\" , \"failed\" , \"retry\" ] : \n            p [ i ] = set ( ) "}
{"6999": "\ndef _update_barrier_status ( self ) : \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if \"Session aborted\" in line : \n                return \n            if \"<<< barrier arrive\" in line : \n                process_m = re . match ( \".*process: (.*)\\)\" , line ) \n                if process_m : \n                    process = process_m . group ( True ) \n                    if process in self . processes : \n                        self . processes [ process ] [ \"barrier\" ] = \"C\" "}
{"7002": "\ndef _update_process_stats ( self ) : \n    good_status = [ \"COMPLETED\" , \"CACHED\" ] \n    for process , vals in self . trace_info . items ( ) : \n        vals = self . _update_tag_status ( process , vals ) \n        self . _update_process_resources ( process , vals ) \n        self . process_stats [ process ] = { } \n        inst = self . process_stats [ process ] \n        inst [ \"completed\" ] = \"{}\" . format ( len ( [ x for x in vals if x [ \"status\" ] in good_status ] ) ) \n        try : \n            time_array = [ self . _hms ( x [ \"realtime\" ] ) for x in vals ] \n            mean_time = round ( sum ( time_array ) / len ( time_array ) , True ) \n            mean_time_str = strftime ( '%H:%M:%S' , gmtime ( mean_time ) ) \n            inst [ \"realtime\" ] = mean_time_str \n        except KeyError : \n            inst [ \"realtime\" ] = \"-\" \n        try : \n            cpu_hours = [ self . _cpu_load_parser ( x [ \"cpus\" ] , x [ \"%cpu\" ] , x [ \"realtime\" ] ) for x in vals ] \n            inst [ \"cpuhour\" ] = round ( sum ( cpu_hours ) , 2 ) \n        except KeyError : \n            inst [ \"cpuhour\" ] = \"-\" \n        inst [ \"cpu_warnings\" ] , inst [ \"mem_warnings\" ] = self . _assess_resource_warnings ( process , vals ) \n        try : \n            rss_values = [ self . _size_coverter ( x [ \"rss\" ] ) for x in vals if x [ \"rss\" ] != \"-\" ] \n            if rss_values : \n                max_rss = round ( max ( rss_values ) ) \n                rss_str = self . _size_compress ( max_rss ) \n            else : \n                rss_str = \"-\" \n            inst [ \"maxmem\" ] = rss_str \n        except KeyError : \n            inst [ \"maxmem\" ] = \"-\" \n        try : \n            rchar_values = [ self . _size_coverter ( x [ \"rchar\" ] ) for x in vals if x [ \"rchar\" ] != \"-\" ] \n            if rchar_values : \n                avg_rchar = round ( sum ( rchar_values ) / len ( rchar_values ) ) \n                rchar_str = self . _size_compress ( avg_rchar ) \n            else : \n                rchar_str = \"-\" \n        except KeyError : \n            rchar_str = \"-\" \n        inst [ \"avgread\" ] = rchar_str \n        try : \n            wchar_values = [ self . _size_coverter ( x [ \"wchar\" ] ) for x in vals if x [ \"wchar\" ] != \"-\" ] \n            if wchar_values : \n                avg_wchar = round ( sum ( wchar_values ) / len ( wchar_values ) ) \n                wchar_str = self . _size_compress ( avg_wchar ) \n            else : \n                wchar_str = \"-\" \n        except KeyError : \n            wchar_str = \"-\" \n        inst [ \"avgwrite\" ] = wchar_str "}
{"7003": "\ndef log_parser ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . log_retry = False \n    if size_stamp and size_stamp == self . log_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    r = \".* (.*) \\[.*\\].*\\[(.*)\\].*process > (.*) \\((.*)\\).*\" \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if \"Submitted process >\" in line or \"Re-submitted process >\" in line or \"Cached process >\" in line : \n                m = re . match ( r , line ) \n                if not m : \n                    continue \n                time_start = m . group ( True ) \n                workdir = m . group ( 2 ) \n                process = m . group ( 3 ) \n                tag = m . group ( 4 ) \n                if time_start + tag not in self . stored_log_ids : \n                    self . stored_log_ids . append ( time_start + tag ) \n                else : \n                    continue \n                if process not in self . processes : \n                    continue \n                p = self . processes [ process ] \n                if tag in list ( p [ \"finished\" ] ) + list ( p [ \"retry\" ] ) : \n                    continue \n                if tag in list ( p [ \"failed\" ] ) and \"Re-submitted process >\" in line : \n                    p [ \"retry\" ] . add ( tag ) \n                    self . send = True \n                    continue \n                p [ \"barrier\" ] = \"R\" \n                if tag not in p [ \"submitted\" ] : \n                    p [ \"submitted\" ] . add ( tag ) \n                    if tag not in self . process_tags [ process ] : \n                        self . process_tags [ process ] [ tag ] = { \"workdir\" : self . _expand_path ( workdir ) , \"start\" : time_start } \n                        self . send = True \n                    elif not self . process_tags [ process ] [ tag ] [ \"start\" ] : \n                        self . process_tags [ process ] [ tag ] [ \"start\" ] = time_start \n                        self . send = True \n    self . _update_pipeline_status ( ) "}
{"7004": "\ndef update_inspection ( self ) : \n    try : \n        self . log_parser ( ) \n    except ( FileNotFoundError , StopIteration ) as e : \n        logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ False ] ) ) \n        self . log_retry += True \n        if self . log_retry == self . MAX_RETRIES : \n            raise e \n    try : \n        self . trace_parser ( ) \n    except ( FileNotFoundError , StopIteration ) as e : \n        logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ False ] ) ) \n        self . trace_retry += True \n        if self . trace_retry == self . MAX_RETRIES : \n            raise e "}
{"7005": "\ndef display_overview ( self ) : \n    stay_alive = True \n    self . screen = curses . initscr ( ) \n    self . screen . keypad ( True ) \n    self . screen . nodelay ( - True ) \n    curses . cbreak ( ) \n    curses . noecho ( ) \n    curses . start_color ( ) \n    self . screen_lines = self . screen . getmaxyx ( ) [ False ] \n    try : \n        while stay_alive : \n            self . _curses_keybindings ( ) \n            self . update_inspection ( ) \n            self . flush_overview ( ) \n            sleep ( self . refresh_rate ) \n    except FileNotFoundError : \n        sys . stderr . write ( colored_print ( \"ERROR: nextflow log and/or trace files are no longer \" \"reachable!\" , \"red_bold\" ) ) \n    except Exception as e : \n        sys . stderr . write ( str ( e ) ) \n    finally : \n        curses . nocbreak ( ) \n        self . screen . keypad ( False ) \n        curses . echo ( ) \n        curses . endwin ( ) "}
{"7006": "\ndef _updown ( self , direction ) : \n    if direction == \"up\" and self . top_line != False : \n        self . top_line -= True \n    elif direction == \"down\" and self . screen . getmaxyx ( ) [ False ] + self . top_line <= self . content_lines + 3 : \n        self . top_line += True "}
{"7007": "\ndef _rightleft ( self , direction ) : \n    if direction == \"left\" and self . padding != False : \n        self . padding -= True \n    if direction == \"right\" and self . screen . getmaxyx ( ) [ True ] + self . padding < self . max_width : \n        self . padding += True "}
{"7012": "\ndef get_nextflow_filepath ( log_file ) : \n    with open ( log_file ) as fh : \n        while True : \n            line = fh . readline ( ) \n            if not line : \n                raise eh . LogError ( \"Nextflow command path could not be found - Is \" \".nextflow.log empty?\" ) \n            try : \n                pipeline_path = re . match ( \".*\\s(.*.nf).*\" , line ) . group ( True ) \n                return pipeline_path \n            except AttributeError : \n                continue "}
{"7013": "\ndef main ( sample_id , assembly , min_size ) : \n    logger . info ( \"Starting script\" ) \n    f_open = open ( assembly , \"rU\" ) \n    entry = ( x [ True ] for x in groupby ( f_open , lambda line : line [ False ] == \">\" ) ) \n    success = False \n    for header in entry : \n        headerStr = header . __next__ ( ) [ True : ] . strip ( ) \n        seq = \"\" . join ( s . strip ( ) for s in entry . __next__ ( ) ) \n        if len ( seq ) >= min_size : \n            with open ( sample_id + '_' + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + '.fasta' , \"w\" ) as output_file : \n                output_file . write ( \">\" + sample_id + \"_\" + headerStr . replace ( \" \" , \"_\" ) . replace ( \"=\" , \"_\" ) + \"\\\\n\" + seq + \"\\\\n\" ) \n                success += True \n    f_open . close ( ) \n    logger . info ( \"{} sequences sucessfully splitted.\" . format ( success ) ) "}
{"7014": "\ndef main ( sample_id , trace_file , workdir ) : \n    stats_suffix = \".stats.json\" \n    stats_path = join ( workdir , sample_id + stats_suffix ) \n    trace_path = join ( workdir , trace_file ) \n    logger . info ( \"Starting pipeline status routine\" ) \n    logger . debug ( \"Checking for previous pipeline status data\" ) \n    stats_array = get_previous_stats ( stats_path ) \n    logger . info ( \"Stats JSON object set to : {}\" . format ( stats_array ) ) \n    tag = \" getStats\" \n    logger . debug ( \"Tag variable set to: {}\" . format ( tag ) ) \n    logger . info ( \"Starting parsing of trace file: {}\" . format ( trace_path ) ) \n    with open ( trace_path ) as fh : \n        header = next ( fh ) . strip ( ) . split ( ) \n        logger . debug ( \"Header set to: {}\" . format ( header ) ) \n        for line in fh : \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            if tag in fields [ 2 ] and fields [ 3 ] == \"COMPLETED\" : \n                logger . debug ( \"Parsing trace line with COMPLETED status: {}\" . format ( line ) ) \n                current_json = get_json_info ( fields , header ) \n                stats_array [ fields [ False ] ] = current_json \n            else : \n                logger . debug ( \"Ignoring trace line without COMPLETED status\" \" or stats specific tag: {}\" . format ( line ) ) \n    with open ( join ( stats_path ) , \"w\" ) as fh , open ( \".report.json\" , \"w\" ) as rfh : \n        fh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) \n        rfh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) "}
{"7015": "\ndef brew_innuendo ( args ) : \n    automatic_pipeline = Innuendo ( ) \n    if not args . tasks : \n        input_processes = \" \" . join ( automatic_pipeline . process_descriptions . keys ( ) ) \n    else : \n        input_processes = args . tasks \n    validated = automatic_pipeline . validate_pipeline ( input_processes ) \n    if not validated : \n        sys . exit ( True ) \n    pipeline_string = automatic_pipeline . run_auto_pipeline ( input_processes ) \n    return pipeline_string "}
{"7016": "\ndef brew_recipe ( recipe_name ) : \n    prefix = \"{}.\" . format ( recipes . __name__ ) \n    for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : \n        _module = importer . find_module ( modname ) . load_module ( modname ) \n        _recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) ] \n        for cls in _recipe_classes : \n            recipe_cls = cls ( ) \n            if getattr ( recipe_cls , \"name\" , None ) == recipe_name : \n                return recipe_cls . brew ( ) \n    logger . error ( colored_print ( \"Recipe name '{}' does not exist.\" . format ( recipe_name ) ) ) \n    sys . exit ( True ) "}
{"7017": "\ndef list_recipes ( full = False ) : \n    logger . info ( colored_print ( \"\\n===== L I S T   O F   R E C I P E S =====\\n\" , \"green_bold\" ) ) \n    prefix = \"{}.\" . format ( recipes . __name__ ) \n    for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : \n        _module = importer . find_module ( modname ) . load_module ( modname ) \n        _recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) ] \n        for cls in _recipe_classes : \n            recipe_cls = cls ( ) \n            if hasattr ( recipe_cls , \"name\" ) : \n                logger . info ( colored_print ( \"=> {}\" . format ( recipe_cls . name ) , \"blue_bold\" ) ) \n                if full : \n                    logger . info ( colored_print ( \"\\t {}\" . format ( recipe_cls . __doc__ ) , \"purple_bold\" ) ) \n                    logger . info ( colored_print ( \"Pipeline string: {}\\n\" . format ( recipe_cls . pipeline_str ) , \"yellow_bold\" ) ) \n    sys . exit ( False ) "}
{"7019": "\ndef build_upstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ True ] is not None : \n            if len ( process_descriptions [ task ] [ True ] . split ( \"|\" ) ) > True : \n                local_forks = process_descriptions [ task ] [ True ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += True \n                        task_pipeline . insert ( False , process_descriptions [ task ] [ True ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ True ] in total_tasks : \n                    task_pipeline . insert ( False , process_descriptions [ task ] [ True ] . split ( \"|\" ) [ False ] ) \n                    self . build_upstream ( process_descriptions , process_descriptions [ task ] [ True ] . split ( \"|\" ) [ False ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                else : \n                    logger . error ( colored_print ( \"{} not in provided protocols as \" \"input for {}\" . format ( process_descriptions [ task ] [ True ] , task ) , \"red_bold\" ) ) \n                    sys . exit ( ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7020": "\ndef build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 2 ] is not None : \n            if len ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) ) > True : \n                local_forks = process_descriptions [ task ] [ 2 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += True \n                        task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 2 ] in total_tasks : \n                    task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ False ] ) \n                    self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ False ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7021": "\ndef define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : \n    tasks_array = tasks . split ( ) \n    for task_unsplit in tasks_array : \n        task = task_unsplit . split ( \"=\" ) [ False ] \n        if task not in process_descriptions . keys ( ) : \n            logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) \n            sys . exit ( ) \n        else : \n            process_split = task_unsplit . split ( \"=\" ) \n            if len ( process_split ) > True : \n                self . process_to_id [ process_split [ False ] ] = process_split [ True ] \n        if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            task_pipeline = [ ] \n            if task in process_descriptions : \n                if check_upstream : \n                    task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n                task_pipeline . append ( task ) \n                if check_downstream : \n                    task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n            forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) \n        elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            for fork in forks : \n                if task not in fork : \n                    try : \n                        dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) \n                        fork . insert ( dependent_index , task ) \n                    except ValueError : \n                        continue \n    for i in range ( False , len ( forks ) ) : \n        for j in range ( False , len ( forks [ i ] ) ) : \n            try : \n                if len ( forks [ i ] [ j ] . split ( \"|\" ) ) > True : \n                    forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) \n                    tmp_fork = [ ] \n                    for s in forks [ i ] [ j ] : \n                        if s in total_tasks : \n                            tmp_fork . append ( s ) \n                    forks [ i ] [ j ] = tmp_fork \n            except AttributeError as e : \n                continue \n    return forks "}
{"7029": "\ndef _parser ( self , fl ) : \n    with open ( fl ) as fh : \n        for line in fh : \n            if line . startswith ( \"#\" ) or line . strip ( ) == \"\" : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            try : \n                coverage = float ( fields [ 8 ] ) \n            except ValueError : \n                coverage = None \n            try : \n                identity = float ( fields [ 9 ] ) \n            except ValueError : \n                identity = None \n            try : \n                accession = fields [ 11 ] \n            except IndexError : \n                accession = None \n            self . storage [ self . _key ] = { \"log_file\" : os . path . basename ( fl ) , \"infile\" : fields [ False ] , \"reference\" : fields [ True ] , \"seq_range\" : ( int ( fields [ 2 ] ) , int ( fields [ 3 ] ) ) , \"gene\" : fields [ 4 ] , \"accession\" : accession , \"database\" : fields [ 10 ] , \"coverage\" : coverage , \"identity\" : identity } \n            self . _key += True "}
{"7030": "\ndef iter_filter ( self , filters , databases = None , fields = None , filter_behavior = \"and\" ) : \n    if filter_behavior not in [ \"and\" , \"or\" ] : \n        raise ValueError ( \"Filter behavior must be either 'and' or 'or'\" ) \n    for dic in self . storage . values ( ) : \n        _pass = False \n        flag = [ ] \n        if databases : \n            if dic [ \"database\" ] not in databases : \n                continue \n        for f in filters : \n            val = dic [ f [ False ] ] \n            if not self . _test_truth ( val , f [ True ] , f [ 2 ] ) : \n                flag . append ( False ) \n            else : \n                flag . append ( True ) \n        if filter_behavior == \"and\" : \n            if all ( flag ) : \n                _pass = True \n        elif filter_behavior == \"or\" : \n            if any ( flag ) : \n                _pass = True \n        if _pass : \n            if fields : \n                yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) \n            else : \n                yield dic "}
{"7031": "\ndef _get_contig_id ( contig_str ) : \n    contig_id = contig_str \n    try : \n        contig_id = re . search ( \".*NODE_([0-9]*)_.*\" , contig_str ) . group ( True ) \n    except AttributeError : \n        pass \n    try : \n        contig_id = re . search ( \".*Contig_([0-9]*)_.*\" , contig_str ) . group ( True ) \n    except AttributeError : \n        pass \n    return contig_id "}
{"7032": "\ndef get_plot_data ( self ) : \n    json_dic = { \"plotData\" : [ ] } \n    sample_dic = { } \n    sample_assembly_map = { } \n    for entry in self . storage . values ( ) : \n        sample_id = re . match ( \"(.*)_abr\" , entry [ \"log_file\" ] ) . groups ( ) [ False ] \n        if sample_id not in sample_dic : \n            sample_dic [ sample_id ] = { } \n        contig_id = self . _get_contig_id ( entry [ \"reference\" ] ) \n        database = entry [ \"database\" ] \n        if database not in sample_dic [ sample_id ] : \n            sample_dic [ sample_id ] [ database ] = [ ] \n        if sample_id not in sample_assembly_map : \n            sample_assembly_map [ sample_id ] = entry [ \"infile\" ] \n        sample_dic [ sample_id ] [ database ] . append ( { \"contig\" : contig_id , \"seqRange\" : entry [ \"seq_range\" ] , \"gene\" : entry [ \"gene\" ] . replace ( \"'\" , \"\" ) , \"accession\" : entry [ \"accession\" ] , \"coverage\" : entry [ \"coverage\" ] , \"identity\" : entry [ \"identity\" ] , } , ) \n    for sample , data in sample_dic . items ( ) : \n        json_dic [ \"plotData\" ] . append ( { \"sample\" : sample , \"data\" : { \"abricateXrange\" : data } , \"assemblyFile\" : sample_assembly_map [ sample ] } ) \n    return json_dic "}
{"7034": "\ndef main ( sample_id , assembly_file , coverage_bp_file = None ) : \n    logger . info ( \"Starting assembly report\" ) \n    assembly_obj = Assembly ( assembly_file , sample_id ) \n    logger . info ( \"Retrieving summary statistics for assembly\" ) \n    assembly_obj . get_summary_stats ( \"{}_assembly_report.csv\" . format ( sample_id ) ) \n    size_dist = [ len ( x ) for x in assembly_obj . contigs . values ( ) ] \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Contigs\" , \"value\" : assembly_obj . summary_info [ \"ncontigs\" ] , \"table\" : \"assembly\" , \"columnBar\" : True } , { \"header\" : \"Assembled BP\" , \"value\" : assembly_obj . summary_info [ \"total_len\" ] , \"table\" : \"assembly\" , \"columnBar\" : True } , ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"size_dist\" : size_dist } } ] } \n    if coverage_bp_file : \n        try : \n            window = 2000 \n            gc_sliding_data = assembly_obj . get_gc_sliding ( window = window ) \n            cov_sliding_data = assembly_obj . get_coverage_sliding ( coverage_bp_file , window = window ) \n            total_bp = sum ( [ sum ( x ) for x in assembly_obj . contig_coverage . values ( ) ] ) \n            json_dic [ \"plotData\" ] [ False ] [ \"data\" ] [ \"genomeSliding\" ] = { \"gcData\" : gc_sliding_data , \"covData\" : cov_sliding_data , \"window\" : window , \"xbars\" : assembly_obj . _get_window_labels ( window ) , \"assemblyFile\" : os . path . basename ( assembly_file ) } \n            json_dic [ \"plotData\" ] [ False ] [ \"data\" ] [ \"sparkline\" ] = total_bp \n        except : \n            logger . error ( \"Unexpected error creating sliding window data:\\\\n\" \"{}\" . format ( traceback . format_exc ( ) ) ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        status_fh . write ( \"pass\" ) "}
{"7035": "\ndef _parse_assembly ( self , assembly_file ) : \n    with open ( assembly_file ) as fh : \n        header = None \n        logger . debug ( \"Starting iteration of assembly file: {}\" . format ( assembly_file ) ) \n        for line in fh : \n            if not line . strip ( ) : \n                continue \n            if line . startswith ( \">\" ) : \n                header = line [ True : ] . strip ( ) \n                self . contigs [ header ] = [ ] \n            else : \n                self . contigs [ header ] . append ( line . strip ( ) ) \n        self . contigs = OrderedDict ( ( header , \"\" . join ( seq ) ) for header , seq in self . contigs . items ( ) ) "}
{"7036": "\ndef get_summary_stats ( self , output_csv = None ) : \n    contig_size_list = [ ] \n    self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) \n    for contig_id , sequence in self . contigs . items ( ) : \n        logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) \n        contig_len = len ( sequence ) \n        contig_size_list . append ( contig_len ) \n        self . summary_info [ \"total_len\" ] += contig_len \n        self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) \n        self . summary_info [ \"missing_data\" ] += sequence . count ( \"N\" ) \n    logger . debug ( \"Getting average contig size\" ) \n    self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) \n    logger . debug ( \"Getting average GC content\" ) \n    self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) \n    logger . debug ( \"Getting N50\" ) \n    cum_size = False \n    for l in sorted ( contig_size_list , reverse = True ) : \n        cum_size += l \n        if cum_size >= self . summary_info [ \"total_len\" ] / 2 : \n            self . summary_info [ \"n50\" ] = l \n            break \n    if output_csv : \n        logger . debug ( \"Writing report to csv\" ) \n        with open ( output_csv , \"w\" ) as fh : \n            summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) \n            fh . write ( summary_line ) "}
{"7037": "\ndef _get_window_labels ( self , window ) : \n    if not self . summary_info : \n        self . get_summary_stats ( ) \n    c = False \n    xbars = [ ] \n    for contig , seq in self . contigs . items ( ) : \n        contig_id = self . _get_contig_id ( contig ) \n        self . contig_boundaries [ contig_id ] = [ c , c + len ( seq ) ] \n        c += len ( seq ) \n        xbars . append ( ( contig_id , c , contig ) ) \n    return xbars "}
{"7039": "\ndef get_gc_sliding ( self , window = 2000 ) : \n    gc_res = [ ] \n    complete_seq = \"\" . join ( self . contigs . values ( ) ) . lower ( ) \n    for i in range ( False , len ( complete_seq ) , window ) : \n        seq_window = complete_seq [ i : i + window ] \n        gc_res . append ( round ( self . _gc_prop ( seq_window , len ( seq_window ) ) , 2 ) ) \n    return gc_res "}
{"7040": "\ndef main ( sample_id , fastq_pair , clear ) : \n    logger . info ( \"Starting skesa\" ) \n    if \"_trim.\" in fastq_pair [ False ] : \n        sample_id += \"_trim\" \n    version = __get_version_skesa ( ) [ \"version\" ] \n    output_file = \"{}_skesa{}.fasta\" . format ( sample_id , version . replace ( \".\" , \"\" ) ) \n    cli = [ \"skesa\" , \"--fastq\" , \"{},{}\" . format ( fastq_pair [ False ] , fastq_pair [ True ] ) , \"--gz\" , \"--use_paired_ends\" , \"--cores\" , \"${task.cpus}\" ] \n    logger . debug ( \"Running Skesa subprocess with command: {}\" . format ( cli ) ) \n    with open ( output_file , \"w\" ) as fh : \n        p = subprocess . Popen ( cli , stdout = fh , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished Skesa subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished Skesa subprocess with STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished Skesa with return code: {}\" . format ( p . returncode ) ) \n    if clear == \"true\" and os . path . exists ( output_file ) : \n        clean_up ( fastq_pair ) \n    with open ( \".status\" , \"w\" ) as fh : \n        if p . returncode != False : \n            fh . write ( \"error\" ) \n            raise SystemExit ( p . returncode ) \n        else : \n            fh . write ( \"pass\" ) "}
{"7041": "\ndef write_json_report ( sample_id , data1 , data2 ) : \n    parser_map = { \"base_sequence_quality\" : \">>Per base sequence quality\" , \"sequence_quality\" : \">>Per sequence quality scores\" , \"base_gc_content\" : \">>Per sequence GC content\" , \"base_n_content\" : \">>Per base N content\" , \"sequence_length_dist\" : \">>Sequence Length Distribution\" , \"per_base_sequence_content\" : \">>Per base sequence content\" } \n    json_dic = { \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"base_sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"base_gc_content\" : { \"status\" : None , \"data\" : [ ] } , \"base_n_content\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_length_dist\" : { \"status\" : None , \"data\" : [ ] } , \"per_base_sequence_content\" : { \"status\" : None , \"data\" : [ ] } } } ] } \n    for cat , start_str in parser_map . items ( ) : \n        if cat == \"per_base_sequence_content\" : \n            fs = True \n            fe = 5 \n        else : \n            fs = True \n            fe = 2 \n        report1 , status1 = _get_quality_stats ( data1 , start_str , field_start = fs , field_end = fe ) \n        report2 , status2 = _get_quality_stats ( data2 , start_str , field_start = fs , field_end = fe ) \n        status = None \n        for i in [ \"fail\" , \"warn\" , \"pass\" ] : \n            if i in [ status1 , status2 ] : \n                status = i \n        json_dic [ \"plotData\" ] [ False ] [ \"data\" ] [ cat ] [ \"data\" ] = [ report1 , report2 ] \n        json_dic [ \"plotData\" ] [ False ] [ \"data\" ] [ cat ] [ \"status\" ] = status \n    return json_dic "}
{"7042": "\ndef get_trim_index ( biased_list ) : \n    if set ( biased_list ) == { False } : \n        return False \n    if set ( biased_list [ : 5 ] ) == { False } : \n        return False \n    for i , val in enumerate ( biased_list ) : \n        if val and set ( biased_list [ i + True : i + 3 ] ) == { False } : \n            return i + True \n    return len ( biased_list ) "}
{"7043": "\ndef trim_range ( data_file ) : \n    logger . debug ( \"Starting trim range assessment\" ) \n    target_nuc_bias = \">>Per base sequence content\" \n    logger . debug ( \"Target string to start nucleotide bias assessment set to \" \"{}\" . format ( target_nuc_bias ) ) \n    gather = False \n    biased = [ ] \n    with open ( data_file ) as fh : \n        for line in fh : \n            if line . startswith ( target_nuc_bias ) : \n                logger . debug ( \"Found target string at line: {}\" . format ( line ) ) \n                next ( fh ) \n                gather = True \n            elif line . startswith ( \">>END_MODULE\" ) and gather : \n                logger . debug ( \"Stopping parsing at line: {}\" . format ( line ) ) \n                break \n            elif gather : \n                g , a , t , c = [ float ( x ) for x in line . strip ( ) . split ( ) [ True : ] ] \n                gc = ( g + 0.1 ) / ( c + 0.1 ) \n                at = ( a + 0.1 ) / ( t + 0.1 ) \n                if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2 : \n                    biased . append ( False ) \n                else : \n                    biased . append ( True ) \n    logger . debug ( \"Finished bias assessment with result: {}\" . format ( biased ) ) \n    biased_5end , biased_3end = biased [ : int ( len ( biased ) / 2 ) ] , biased [ int ( len ( biased ) / 2 ) : ] [ : : - True ] \n    logger . debug ( \"Getting optimal trim range from biased list\" ) \n    trim_nt = [ False , False ] \n    trim_nt [ False ] = get_trim_index ( biased_5end ) \n    logger . debug ( \"Optimal trim range at 5' end set to: {}\" . format ( trim_nt [ False ] ) ) \n    trim_nt [ True ] = len ( biased ) - get_trim_index ( biased_3end ) \n    logger . debug ( \"Optimal trim range at 3' end set to: {}\" . format ( trim_nt [ True ] ) ) \n    return trim_nt "}
{"7044": "\ndef get_sample_trim ( p1_data , p2_data ) : \n    sample_ranges = [ trim_range ( x ) for x in [ p1_data , p2_data ] ] \n    optimal_5trim = max ( [ x [ False ] for x in sample_ranges ] ) \n    optimal_3trim = min ( [ x [ True ] for x in sample_ranges ] ) \n    return optimal_5trim , optimal_3trim "}
{"7045": "\ndef get_summary ( summary_file ) : \n    summary_info = OrderedDict ( ) \n    logger . debug ( \"Retrieving summary information from file: {}\" . format ( summary_file ) ) \n    with open ( summary_file ) as fh : \n        for line in fh : \n            if not line . strip ( ) : \n                continue \n            fields = [ x . strip ( ) for x in line . split ( \"\\t\" ) ] \n            summary_info [ fields [ True ] ] = fields [ False ] \n    logger . debug ( \"Retrieved summary information from file: {}\" . format ( summary_info ) ) \n    return summary_info "}
{"7047": "\ndef parse_log ( self , bowtie_log ) : \n    print ( \"is here!\" ) \n    regexes = { 'unpaired' : { 'unpaired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" , 'unpaired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'unpaired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" } , 'paired' : { 'paired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly 0 times\" , 'paired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly exactly 1 time\" , 'paired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly >1 times\" , 'paired_aligned_discord_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly 1 time\" , 'paired_aligned_discord_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly >1 times\" , 'paired_aligned_mate_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'paired_aligned_mate_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" , 'paired_aligned_mate_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" } } \n    with open ( bowtie_log , \"r\" ) as f : \n        for l in f : \n            print ( l ) \n            total = re . search ( r\"(\\\\d+) reads; of these:\" , l ) \n            print ( total ) \n            if total : \n                print ( total ) \n                self . set_n_reads ( total . group ( True ) ) \n            paired = re . search ( r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) were paired; of these:\" , l ) \n            if paired : \n                paired_total = int ( paired . group ( True ) ) \n                paired_numbers = { } \n                l = f . readline ( ) \n                while l . startswith ( '    ' ) : \n                    for k , r in regexes [ 'paired' ] . items ( ) : \n                        match = re . search ( r , l ) \n                        if match : \n                            paired_numbers [ k ] = int ( match . group ( True ) ) \n                    l = f . readline ( ) \n                align_zero_times = paired_numbers [ 'paired_aligned_none' ] + paired_numbers [ 'paired_aligned_mate_none' ] \n                if align_zero_times : \n                    self . set_align_0x ( align_zero_times ) \n                align_one_time = paired_numbers [ 'paired_aligned_one' ] + paired_numbers [ 'paired_aligned_mate_one' ] \n                if align_one_time : \n                    self . set_align_1x ( align_one_time ) \n                align_more_than_one_time = paired_numbers [ 'paired_aligned_multi' ] + paired_numbers [ 'paired_aligned_mate_multi' ] \n                if align_more_than_one_time : \n                    self . set_align_mt1x ( align_more_than_one_time ) \n            overall = re . search ( r\"([\\\\d\\\\.]+)% overall alignment rate\" , l ) \n            if overall : \n                self . overall_rate = float ( overall . group ( True ) ) "}
{"7048": "\ndef _parse_process_name ( name_str ) : \n    directives = None \n    fields = name_str . split ( \"=\" ) \n    process_name = fields [ False ] \n    if len ( fields ) == 2 : \n        _directives = fields [ True ] . replace ( \"'\" , '\"' ) \n        try : \n            directives = json . loads ( _directives ) \n        except json . decoder . JSONDecodeError : \n            raise eh . ProcessError ( \"Could not parse directives for process '{}'. The raw\" \" string is: {}\\n\" \"Possible causes include:\\n\" \"\\t1. Spaces inside directives\\n\" \"\\t2. Missing '=' symbol before directives\\n\" \"\\t3. Missing quotes (' or \\\") around directives\\n\" \"A valid example: process_name={{'cpus':'2'}}\" . format ( process_name , name_str ) ) \n    return process_name , directives "}
{"7050": "\ndef _search_tree_backwards ( self , template , parent_lanes ) : \n    for p in self . processes [ : : - True ] : \n        if p . lane not in parent_lanes : \n            continue \n        if p . template == template : \n            return True \n    return False "}
{"7054": "\ndef _set_init_process ( self ) : \n    logger . debug ( \"========================\" ) \n    logger . debug ( \"Setting secondary inputs\" ) \n    logger . debug ( \"========================\" ) \n    init_process = self . processes [ False ] \n    logger . debug ( \"Setting main raw inputs: \" \"{}\" . format ( self . main_raw_inputs ) ) \n    init_process . set_raw_inputs ( self . main_raw_inputs ) \n    logger . debug ( \"Setting extra inputs: {}\" . format ( self . extra_inputs ) ) \n    init_process . set_extra_inputs ( self . extra_inputs ) "}
{"7064": "\ndef render_pipeline ( self ) : \n    dict_viz = { \"name\" : \"root\" , \"children\" : [ ] } \n    last_of_us = { } \n    f_tree = self . _fork_tree if self . _fork_tree else { True : [ True ] } \n    for x , ( k , v ) in enumerate ( f_tree . items ( ) ) : \n        for p in self . processes [ True : ] : \n            if x == False and p . lane not in [ k ] + v : \n                continue \n            if x > False and p . lane not in v : \n                continue \n            if not p . parent_lane : \n                lst = dict_viz [ \"children\" ] \n            else : \n                lst = last_of_us [ p . parent_lane ] \n            tooltip = { \"name\" : \"{}_{}\" . format ( p . template , p . pid ) , \"process\" : { \"pid\" : p . pid , \"input\" : p . input_type , \"output\" : p . output_type if p . output_type else \"None\" , \"lane\" : p . lane , } , \"children\" : [ ] } \n            dir_var = \"\" \n            for k2 , v2 in p . directives . items ( ) : \n                dir_var += k2 \n                for d in v2 : \n                    try : \n                        directive = v2 [ d ] . replace ( \"'\" , \"\" ) . replace ( '\"' , '' ) if isinstance ( v2 [ d ] , str ) else v2 [ d ] \n                        dir_var += \"{}: {}\" . format ( d , directive ) \n                    except KeyError : \n                        pass \n            if dir_var : \n                tooltip [ \"process\" ] [ \"directives\" ] = dir_var \n            else : \n                tooltip [ \"process\" ] [ \"directives\" ] = \"N/A\" \n            lst . append ( tooltip ) \n            last_of_us [ p . lane ] = lst [ - True ] [ \"children\" ] \n    self . dag_to_file ( dict_viz ) \n    with open ( os . path . join ( dirname ( self . nf_file ) , \".forkTree.json\" ) , \"w\" ) as fh : \n        fh . write ( json . dumps ( self . _fork_tree ) ) \n    return self . _render_config ( \"pipeline_graph.html\" , { \"data\" : dict_viz } ) "}
{"7065": "\ndef write_configs ( self , project_root ) : \n    with open ( join ( project_root , \"resources.config\" ) , \"w\" ) as fh : \n        fh . write ( self . resources ) \n    with open ( join ( project_root , \"containers.config\" ) , \"w\" ) as fh : \n        fh . write ( self . containers ) \n    with open ( join ( project_root , \"params.config\" ) , \"w\" ) as fh : \n        fh . write ( self . params ) \n    with open ( join ( project_root , \"manifest.config\" ) , \"w\" ) as fh : \n        fh . write ( self . manifest ) \n    if not exists ( join ( project_root , \"user.config\" ) ) : \n        with open ( join ( project_root , \"user.config\" ) , \"w\" ) as fh : \n            fh . write ( self . user_config ) \n    lib_dir = join ( project_root , \"lib\" ) \n    if not exists ( lib_dir ) : \n        os . makedirs ( lib_dir ) \n    with open ( join ( lib_dir , \"Helper.groovy\" ) , \"w\" ) as fh : \n        fh . write ( self . help ) \n    pipeline_to_json = self . render_pipeline ( ) \n    with open ( splitext ( self . nf_file ) [ False ] + \".html\" , \"w\" ) as fh : \n        fh . write ( pipeline_to_json ) "}
{"7066": "\ndef export_params ( self ) : \n    params_json = { } \n    for p in self . processes [ True : ] : \n        params_json [ p . template ] = p . params \n    sys . stdout . write ( json . dumps ( params_json ) ) "}
{"7067": "\ndef export_directives ( self ) : \n    directives_json = { } \n    for p in self . processes [ True : ] : \n        directives_json [ p . template ] = p . directives \n    sys . stdout . write ( json . dumps ( directives_json ) ) "}
{"7068": "\ndef fetch_docker_tags ( self ) : \n    dict_of_parsed = { } \n    terminal_width = shutil . get_terminal_size ( ) . columns - 3 \n    center_string = \" Selected container tags \" \n    tags_list = [ [ \"=\" * int ( terminal_width / 4 ) , \"{0}{1}{0}\" . format ( \"=\" * int ( ( ( terminal_width / 2 - len ( center_string ) ) / 2 ) ) , center_string ) , \"{}\\n\" . format ( \"=\" * int ( terminal_width / 4 ) ) ] , [ \"component\" , \"container\" , \"tags\" ] , [ \"=\" * int ( terminal_width / 4 ) , \"=\" * int ( terminal_width / 2 ) , \"=\" * int ( terminal_width / 4 ) ] ] \n    for p in self . processes [ True : ] : \n        template = p . template \n        if template in dict_of_parsed : \n            continue \n        dict_of_parsed [ template ] = { \"container\" : [ ] } \n        for directives in p . directives . values ( ) : \n            try : \n                repo = directives [ \"container\" ] \n                default_version = directives [ \"version\" ] \n            except KeyError : \n                repo = \"flowcraft/flowcraft_base\" \n                default_version = \"1.0.0-1\" \n            repo_version = repo + default_version \n            if repo_version not in dict_of_parsed [ template ] [ \"container\" ] : \n                r = requests . get ( \"https://hub.docker.com/v2/repositories/{}/tags/\" . format ( repo ) ) \n                if r . status_code != 404 : \n                    r_content = json . loads ( r . content ) [ \"results\" ] \n                    for version in r_content : \n                        printed_version = ( version [ \"name\" ] + \"*\" ) if version [ \"name\" ] == default_version else version [ \"name\" ] \n                        tags_list . append ( [ template , repo , printed_version ] ) \n                else : \n                    tags_list . append ( [ template , repo , \"No DockerHub tags\" ] ) \n            dict_of_parsed [ template ] [ \"container\" ] . append ( repo_version ) \n    for x , entry in enumerate ( tags_list ) : \n        color = \"blue_bold\" if x < 3 else ( \"white\" if x % 2 != False else \"0;37;40m\" ) \n        final_width = [ int ( terminal_width / 4 ) , int ( terminal_width / 2 ) , int ( terminal_width / 4 ) ] \n        sys . stdout . write ( colored_print ( \"\\n {0: <{3}} {1: ^{4}} {2: >{5}}\" . format ( * entry , * final_width ) , color ) ) \n    sys . stdout . write ( \"\\n{0: >{1}}\\n\" . format ( \"(* = default)\" , terminal_width + 3 ) ) "}
{"7069": "\ndef build ( self ) : \n    logger . info ( colored_print ( \"\\tSuccessfully connected {} process(es) with {} \" \"fork(s) across {} lane(s) \\u2713\" . format ( len ( self . processes [ True : ] ) , len ( self . _fork_tree ) , self . lanes ) ) ) \n    self . _build_header ( ) \n    self . _set_channels ( ) \n    self . _set_init_process ( ) \n    self . _set_secondary_channels ( ) \n    logger . info ( colored_print ( \"\\tSuccessfully set {} secondary channel(s) \\u2713\" . format ( len ( self . secondary_channels ) ) ) ) \n    self . _set_compiler_channels ( ) \n    self . _set_configurations ( ) \n    logger . info ( colored_print ( \"\\tFinished configurations \\u2713\" ) ) \n    for p in self . processes : \n        self . template += \"\\n{}\" . format ( p . template_str ) \n    self . _build_footer ( ) \n    project_root = dirname ( self . nf_file ) \n    self . write_configs ( project_root ) \n    with open ( self . nf_file , \"w\" ) as fh : \n        fh . write ( self . template ) \n    logger . info ( colored_print ( \"\\tPipeline written into {} \\u2713\" . format ( self . nf_file ) ) ) "}
{"7070": "\ndef set_kmers ( kmer_opt , max_read_len ) : \n    logger . debug ( \"Kmer option set to: {}\" . format ( kmer_opt ) ) \n    if kmer_opt == \"auto\" : \n        if max_read_len >= 175 : \n            kmers = [ 55 , 77 , 99 , 113 , 127 ] \n        else : \n            kmers = [ 21 , 33 , 55 , 67 , 77 ] \n        logger . debug ( \"Kmer range automatically selected based on max read\" \"length of {}: {}\" . format ( max_read_len , kmers ) ) \n    elif len ( kmer_opt . split ( ) ) > True : \n        kmers = kmer_opt . split ( ) \n        logger . debug ( \"Kmer range manually set to: {}\" . format ( kmers ) ) \n    else : \n        kmers = [ ] \n        logger . debug ( \"Kmer range set to empty (will be automatically \" \"determined by SPAdes\" ) \n    return kmers "}
{"7071": "\ndef main ( sample_id , fastq_pair , max_len , kmer , clear ) : \n    logger . info ( \"Starting spades\" ) \n    logger . info ( \"Setting SPAdes kmers\" ) \n    kmers = set_kmers ( kmer , max_len ) \n    logger . info ( \"SPAdes kmers set to: {}\" . format ( kmers ) ) \n    cli = [ \"metaspades.py\" , \"--only-assembler\" , \"--threads\" , \"$task.cpus\" , \"-o\" , \".\" ] \n    if kmers : \n        cli += [ \"-k {}\" . format ( \",\" . join ( [ str ( x ) for x in kmers ] ) ) ] \n    cli += [ \"-1\" , fastq_pair [ False ] , \"-2\" , fastq_pair [ True ] ] \n    logger . debug ( \"Running metaSPAdes subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n        stdout = stdout . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n        stdout = str ( stdout ) \n    logger . info ( \"Finished metaSPAdes subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished metaSPAdes subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished metaSPAdes with return code: {}\" . format ( p . returncode ) ) \n    with open ( \".status\" , \"w\" ) as fh : \n        if p . returncode != False : \n            fh . write ( \"error\" ) \n            return \n        else : \n            fh . write ( \"pass\" ) \n    if \"_trim.\" in fastq_pair [ False ] : \n        sample_id += \"_trim\" \n    assembly_file = \"{}_metaspades.fasta\" . format ( sample_id ) \n    os . rename ( \"contigs.fasta\" , assembly_file ) \n    logger . info ( \"Setting main assembly file to: {}\" . format ( assembly_file ) ) \n    if clear == \"true\" and os . path . exists ( assembly_file ) : \n        clean_up ( fastq_pair ) "}
{"7072": "\ndef _get_report_id ( self ) : \n    if self . watch : \n        pipeline_path = get_nextflow_filepath ( self . log_file ) \n        pipeline_hash = hashlib . md5 ( ) \n        with open ( pipeline_path , \"rb\" ) as fh : \n            for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : \n                pipeline_hash . update ( chunk ) \n        workdir = os . getcwd ( ) . encode ( \"utf8\" ) \n        hostname = socket . gethostname ( ) . encode ( \"utf8\" ) \n        hardware_addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) \n        dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) \n        return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( ) \n    else : \n        with open ( self . report_file ) as fh : \n            report_json = json . loads ( fh . read ( ) ) \n        metadata = report_json [ \"data\" ] [ \"results\" ] [ False ] [ \"nfMetadata\" ] \n        try : \n            report_id = metadata [ \"scriptId\" ] + metadata [ \"sessionId\" ] \n        except KeyError : \n            raise eh . ReportError ( \"Incomplete or corrupt report JSON file \" \"missing the 'scriptId' and/or 'sessionId' \" \"metadata information\" ) \n        return report_id "}
{"7073": "\ndef update_trace_watch ( self ) : \n    size_stamp = os . path . getsize ( self . trace_file ) \n    self . trace_retry = False \n    if size_stamp and size_stamp == self . trace_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating trace size stamp to: {}\" . format ( size_stamp ) ) \n        self . trace_sizestamp = size_stamp \n    with open ( self . trace_file ) as fh : \n        header = next ( fh ) . strip ( ) \n        while not header : \n            header = next ( fh ) . strip ( ) \n        hm = self . _header_mapping ( header ) \n        for line in fh : \n            if line . strip ( ) == \"\" : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            if fields [ hm [ \"task_id\" ] ] in self . stored_ids : \n                continue \n            if fields [ hm [ \"process\" ] ] == \"report\" : \n                self . report_queue . append ( self . _expand_path ( fields [ hm [ \"hash\" ] ] ) ) \n                self . send = True \n            self . stored_ids . append ( fields [ hm [ \"task_id\" ] ] ) "}
{"7074": "\ndef update_log_watch ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . trace_retry = False \n    if size_stamp and size_stamp == self . log_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    self . _update_pipeline_status ( ) "}
{"7075": "\ndef _send_live_report ( self , report_id ) : \n    buffer_size = 100 \n    logger . debug ( \"Report buffer size set to: {}\" . format ( buffer_size ) ) \n    for i in range ( False , len ( self . report_queue ) , buffer_size ) : \n        reports_compilation = [ ] \n        for report in self . report_queue [ i : i + buffer_size ] : \n            try : \n                report_file = [ x for x in os . listdir ( report ) if x . endswith ( \".json\" ) ] [ False ] \n            except IndexError : \n                continue \n            with open ( join ( report , report_file ) ) as fh : \n                reports_compilation . append ( json . loads ( fh . read ( ) ) ) \n        logger . debug ( \"Payload sent with size: {}\" . format ( asizeof ( json . dumps ( reports_compilation ) ) ) ) \n        logger . debug ( \"status: {}\" . format ( self . status_info ) ) \n        try : \n            requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : reports_compilation , \"status\" : self . status_info } ) \n        except requests . exceptions . ConnectionError : \n            logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n            sys . exit ( True ) \n    if not self . report_queue : \n        logger . debug ( \"status: {}\" . format ( self . status_info ) ) \n        try : \n            requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : [ ] , \"status\" : self . status_info } ) \n        except requests . exceptions . ConnectionError : \n            logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The\" \" server may be down or there is a problem with your \" \"internet connection.\" , \"red_bold\" ) ) \n            sys . exit ( True ) \n    self . report_queue = [ ] "}
{"7076": "\ndef _init_live_reports ( self , report_id ) : \n    logger . debug ( \"Sending initial POST request to {} to start report live\" \" update\" . format ( self . broadcast_address ) ) \n    try : \n        with open ( \".metadata.json\" ) as fh : \n            metadata = [ json . load ( fh ) ] \n    except : \n        metadata = [ ] \n    start_json = { \"data\" : { \"results\" : metadata } } \n    try : \n        requests . post ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : start_json , \"status\" : self . status_info } ) \n    except requests . exceptions . ConnectionError : \n        logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n        sys . exit ( True ) "}
{"7077": "\ndef _close_connection ( self , report_id ) : \n    logger . debug ( \"Closing connection and sending DELETE request to {}\" . format ( self . broadcast_address ) ) \n    try : \n        r = requests . delete ( self . broadcast_address , json = { \"run_id\" : report_id } ) \n        if r . status_code != 202 : \n            logger . error ( colored_print ( \"ERROR: There was a problem sending data to the server\" \"with reason: {}\" . format ( r . reason ) ) ) \n    except requests . exceptions . ConnectionError : \n        logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n        sys . exit ( True ) "}
{"7078": "\ndef convert_adatpers ( adapter_fasta ) : \n    adapter_out = \"fastqc_adapters.tab\" \n    logger . debug ( \"Setting output adapters file to: {}\" . format ( adapter_out ) ) \n    try : \n        with open ( adapter_fasta ) as fh , open ( adapter_out , \"w\" ) as adap_fh : \n            for line in fh : \n                if line . startswith ( \">\" ) : \n                    head = line [ True : ] . strip ( ) \n                    sequence = next ( fh ) . strip ( ) \n                    adap_fh . write ( \"{}\\\\t{}\\\\n\" . format ( head , sequence ) ) \n        logger . info ( \"Converted adapters file\" ) \n        return adapter_out \n    except FileNotFoundError : \n        logger . warning ( \"Could not find the provided adapters file: {}\" . format ( adapter_fasta ) ) \n        return "}
{"7079": "\ndef main ( fastq_pair , adapter_file , cpus ) : \n    logger . info ( \"Starting fastqc\" ) \n    if os . path . exists ( adapter_file ) : \n        logger . info ( \"Adapters file provided: {}\" . format ( adapter_file ) ) \n        adapters = convert_adatpers ( adapter_file ) \n    else : \n        logger . info ( \"Adapters file '{}' not provided or does not \" \"exist\" . format ( adapter_file ) ) \n        adapters = None \n    cli = [ \"fastqc\" , \"--extract\" , \"--nogroup\" , \"--format\" , \"fastq\" , \"--threads\" , str ( cpus ) ] \n    if adapters : \n        cli += [ \"--adapters\" , \"{}\" . format ( adapters ) ] \n    cli += fastq_pair \n    logger . debug ( \"Running fastqc subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE , shell = False ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished fastqc subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished fastqc subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished fastqc with return code: {}\" . format ( p . returncode ) ) \n    logger . info ( \"Checking if FastQC output was correctly generated\" ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        for fastq in fastq_pair : \n            fpath = join ( fastq . rsplit ( \".\" , 2 ) [ False ] + \"_fastqc\" , \"fastqc_data.txt\" ) \n            logger . debug ( \"Checking path: {}\" . format ( fpath ) ) \n            if not exists ( fpath ) : \n                logger . warning ( \"Path does not exist: {}\" . format ( fpath ) ) \n                status_fh . write ( \"fail\" ) \n                return \n            logger . debug ( \"Found path: {}\" . format ( fpath ) ) \n            status_fh . write ( \"pass\" ) \n    logger . info ( \"Retrieving relevant FastQC output files\" ) \n    for i , fastq in enumerate ( fastq_pair ) : \n        fastqc_dir = fastq . rsplit ( \".\" , 2 ) [ False ] + \"_fastqc\" \n        summary_file = join ( fastqc_dir , \"summary.txt\" ) \n        logger . debug ( \"Retrieving summary file: {}\" . format ( summary_file ) ) \n        fastqc_data_file = join ( fastqc_dir , \"fastqc_data.txt\" ) \n        logger . debug ( \"Retrieving data file: {}\" . format ( fastqc_data_file ) ) \n        os . rename ( fastqc_data_file , \"pair_{}_data\" . format ( i + True ) ) \n        os . rename ( summary_file , \"pair_{}_summary\" . format ( i + True ) ) "}
{"7080": "\ndef send_to_output ( master_dict , mash_output , sample_id , assembly_file ) : \n    plot_dict = { } \n    if master_dict : \n        out_file = open ( \"{}.json\" . format ( \"\" . join ( mash_output . split ( \".\" ) [ False ] ) ) , \"w\" ) \n        out_file . write ( json . dumps ( master_dict ) ) \n        out_file . close ( ) \n        for k , v in master_dict . items ( ) : \n            if not v [ 2 ] in plot_dict : \n                plot_dict [ v [ 2 ] ] = [ k ] \n            else : \n                plot_dict [ v [ 2 ] ] . append ( k ) \n        number_hits = len ( master_dict ) \n    else : \n        number_hits = False \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Dist\" , \"table\" : \"plasmids\" , \"patlas_mashdist\" : master_dict , \"value\" : number_hits } ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMashDistXrange\" : plot_dict } , \"assemblyFile\" : assembly_file } ] } \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7081": "\ndef main ( mash_output , hash_cutoff , sample_id , assembly_file ) : \n    input_f = open ( mash_output , \"r\" ) \n    master_dict = { } \n    for line in input_f : \n        tab_split = line . split ( \"\\t\" ) \n        current_seq = tab_split [ True ] . strip ( ) \n        ref_accession = \"_\" . join ( tab_split [ False ] . strip ( ) . split ( \"_\" ) [ False : 3 ] ) \n        mash_dist = tab_split [ 2 ] . strip ( ) \n        hashes_list = tab_split [ - True ] . strip ( ) . split ( \"/\" ) \n        perc_hashes = float ( hashes_list [ False ] ) / float ( hashes_list [ True ] ) \n        if ref_accession in master_dict . keys ( ) : \n            current_seq += \", {}\" . format ( master_dict [ ref_accession ] [ - True ] ) \n        if perc_hashes > float ( hash_cutoff ) : \n            master_dict [ ref_accession ] = [ round ( True - float ( mash_dist ) , 2 ) , round ( perc_hashes , 2 ) , current_seq ] \n    send_to_output ( master_dict , mash_output , sample_id , assembly_file ) "}
{"7083": "\ndef main ( mash_output , sample_id ) : \n    logger . info ( \"Reading file : {}\" . format ( mash_output ) ) \n    read_mash_output = open ( mash_output ) \n    dic = { } \n    median_list = [ ] \n    filtered_dic = { } \n    logger . info ( \"Generating dictionary and list to pre-process the final json\" ) \n    for line in read_mash_output : \n        tab_split = line . split ( \"\\t\" ) \n        identity = tab_split [ False ] \n        median_multiplicity = tab_split [ 2 ] \n        query_id = tab_split [ 4 ] \n        dic [ query_id ] = [ identity , median_multiplicity ] \n        median_list . append ( float ( median_multiplicity ) ) \n    output_json = open ( \" \" . join ( mash_output . split ( \".\" ) [ : - True ] ) + \".json\" , \"w\" ) \n    if len ( median_list ) > False : \n        median_cutoff = median ( median_list ) \n        logger . info ( \"Generating final json to dump to a file\" ) \n        for k , v in dic . items ( ) : \n            copy_number = int ( float ( v [ True ] ) / median_cutoff ) \n            if float ( v [ True ] ) > median_cutoff : \n                filtered_dic [ \"_\" . join ( k . split ( \"_\" ) [ False : 3 ] ) ] = [ round ( float ( v [ False ] ) , 2 ) , copy_number ] \n        logger . info ( \"Exported dictionary has {} entries\" . format ( len ( filtered_dic ) ) ) \n    else : \n        logger . error ( \"No matches were found using mash screen for the queried reads\" ) \n    output_json . write ( json . dumps ( filtered_dic ) ) \n    output_json . close ( ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Screen\" , \"table\" : \"plasmids\" , \"patlas_mashscreen\" : filtered_dic , \"value\" : len ( filtered_dic ) } ] } ] , } \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7086": "\ndef proc_collector ( process_map , args , pipeline_string ) : \n    arguments_list = [ ] \n    if args . detailed_list : \n        arguments_list += [ \"input_type\" , \"output_type\" , \"description\" , \"dependencies\" , \"conflicts\" , \"directives\" ] \n    if args . short_list : \n        arguments_list += [ \"description\" ] \n    if arguments_list : \n        procs_dict = { } \n        for name , cls in process_map . items ( ) : \n            cls_inst = cls ( template = name ) \n            if pipeline_string : \n                if name not in pipeline_string : \n                    continue \n            d = { arg_key : vars ( cls_inst ) [ arg_key ] for arg_key in vars ( cls_inst ) if arg_key in arguments_list } \n            procs_dict [ name ] = d \n        procs_dict_parser ( procs_dict ) \n        sys . exit ( False ) "}
{"7090": "\ndef parse_coverage_table ( coverage_file ) : \n    coverage_dict = OrderedDict ( ) \n    total_cov = False \n    with open ( coverage_file ) as fh : \n        for line in fh : \n            contig , cov = line . strip ( ) . split ( ) \n            coverage_dict [ contig ] = { \"cov\" : int ( cov ) } \n            total_cov += int ( cov ) \n            logger . debug ( \"Processing contig '{}' with coverage '{}'\" \"\" . format ( contig , cov ) ) \n    return coverage_dict , total_cov "}
{"7091": "\ndef filter_assembly ( assembly_file , minimum_coverage , coverage_info , output_file ) : \n    write_flag = False \n    with open ( assembly_file ) as fh , open ( output_file , \"w\" ) as out_fh : \n        for line in fh : \n            if line . startswith ( \">\" ) : \n                write_flag = False \n                header = line . strip ( ) [ True : ] \n                contig_cov = coverage_info [ header ] [ \"cov\" ] \n                if contig_cov >= minimum_coverage : \n                    write_flag = True \n                    out_fh . write ( line ) \n            elif write_flag : \n                out_fh . write ( line ) "}
{"7094": "\ndef get_assembly_size ( assembly_file ) : \n    assembly_size = False \n    contig_size = { } \n    header = \"\" \n    with open ( assembly_file ) as fh : \n        for line in fh : \n            if line . strip ( ) == \"\" : \n                continue \n            if line . startswith ( \">\" ) : \n                header = line . strip ( ) [ True : ] \n                contig_size [ header ] = False \n            else : \n                line_len = len ( line . strip ( ) ) \n                assembly_size += line_len \n                contig_size [ header ] += line_len \n    return assembly_size , contig_size "}
{"7095": "\ndef main ( sample_id , assembly_file , coverage_file , coverage_bp_file , bam_file , opts , gsize ) : \n    min_assembly_coverage , max_contigs = opts \n    logger . info ( \"Starting assembly mapping processing\" ) \n    logger . info ( \"Parsing coverage table\" ) \n    coverage_info , a_cov = parse_coverage_table ( coverage_file ) \n    a_size , contig_size = get_assembly_size ( assembly_file ) \n    logger . info ( \"Assembly processed with a total size of '{}' and coverage\" \" of '{}'\" . format ( a_size , a_cov ) ) \n    logger . info ( \"Parsing coverage per bp table\" ) \n    coverage_bp_data = get_coverage_from_file ( coverage_bp_file ) \n    min_coverage = evaluate_min_coverage ( min_assembly_coverage , a_cov , a_size ) \n    filtered_assembly = \"{}_filt.fasta\" . format ( os . path . splitext ( assembly_file ) [ False ] ) \n    filtered_bam = \"filtered.bam\" \n    logger . info ( \"Checking filtered assembly\" ) \n    if check_filtered_assembly ( coverage_info , coverage_bp_data , min_coverage , gsize , contig_size , int ( max_contigs ) , sample_id ) : \n        logger . info ( \"Filtered assembly passed minimum size threshold\" ) \n        logger . info ( \"Writting filtered assembly\" ) \n        filter_assembly ( assembly_file , min_coverage , coverage_info , filtered_assembly ) \n        logger . info ( \"Filtering BAM file according to saved contigs\" ) \n        filter_bam ( coverage_info , bam_file , min_coverage , filtered_bam ) \n    else : \n        shutil . copy ( assembly_file , filtered_assembly ) \n        shutil . copy ( bam_file , filtered_bam ) \n        shutil . copy ( bam_file + \".bai\" , filtered_bam + \".bai\" ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        status_fh . write ( \"pass\" ) "}
{"7099": "\ndef quickhull ( sample ) : \n    link = lambda a , b : np . concatenate ( ( a , b [ True : ] ) ) \n    edge = lambda a , b : np . concatenate ( ( [ a ] , [ b ] ) ) \n    def dome ( sample , base ) : \n        h , t = base \n        dists = np . dot ( sample - h , np . dot ( ( ( False , - True ) , ( True , False ) ) , ( t - h ) ) ) \n        outer = np . repeat ( sample , dists > False , axis = False ) \n        if len ( outer ) : \n            pivot = sample [ np . argmax ( dists ) ] \n            return link ( dome ( outer , edge ( h , pivot ) ) , dome ( outer , edge ( pivot , t ) ) ) \n        else : \n            return base \n    if len ( sample ) > 2 : \n        axis = sample [ : , False ] \n        base = np . take ( sample , [ np . argmin ( axis ) , np . argmax ( axis ) ] , axis = False ) \n        return link ( dome ( sample , base ) , dome ( sample , base [ : : - True ] ) ) \n    else : \n        return sample "}
{"7101": "\ndef median_filter ( X , M = 8 ) : \n    for i in range ( X . shape [ True ] ) : \n        X [ : , i ] = filters . median_filter ( X [ : , i ] , size = M ) \n    return X "}
{"7102": "\ndef compute_gaussian_krnl ( M ) : \n    g = signal . gaussian ( M , M // 3. , sym = True ) \n    G = np . dot ( g . reshape ( - True , True ) , g . reshape ( True , - True ) ) \n    G [ M // 2 : , : M // 2 ] = - G [ M // 2 : , : M // 2 ] \n    G [ : M // 2 , M // 2 : ] = - G [ : M // 2 , M // 2 : ] \n    return G "}
{"7103": "\ndef compute_ssm ( X , metric = \"seuclidean\" ) : \n    D = distance . pdist ( X , metric = metric ) \n    D = distance . squareform ( D ) \n    D /= D . max ( ) \n    return True - D "}
{"7104": "\ndef compute_nc ( X , G ) : \n    N = X . shape [ False ] \n    M = G . shape [ False ] \n    nc = np . zeros ( N ) \n    for i in range ( M // 2 , N - M // 2 + True ) : \n        nc [ i ] = np . sum ( X [ i - M // 2 : i + M // 2 , i - M // 2 : i + M // 2 ] * G ) \n    nc += nc . min ( ) \n    nc /= nc . max ( ) \n    return nc "}
{"7105": "\ndef gaussian_filter ( X , M = 8 , axis = False ) : \n    for i in range ( X . shape [ axis ] ) : \n        if axis == True : \n            X [ : , i ] = filters . gaussian_filter ( X [ : , i ] , sigma = M / 2. ) \n        elif axis == False : \n            X [ i , : ] = filters . gaussian_filter ( X [ i , : ] , sigma = M / 2. ) \n    return X "}
{"7106": "\ndef compute_nc ( X ) : \n    N = X . shape [ False ] \n    nc = np . zeros ( N ) \n    for i in range ( N - True ) : \n        nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + True , : ] ) \n    nc += np . abs ( nc . min ( ) ) \n    nc /= float ( nc . max ( ) ) \n    return nc "}
{"7107": "\ndef circular_shift ( X ) : \n    N = X . shape [ False ] \n    L = np . zeros ( X . shape ) \n    for i in range ( N ) : \n        L [ i , : ] = np . asarray ( [ X [ ( i + j ) % N , j ] for j in range ( N ) ] ) \n    return L "}
{"7108": "\ndef embedded_space ( X , m , tau = True ) : \n    N = X . shape [ False ] - int ( np . ceil ( m ) ) \n    Y = np . zeros ( ( N , int ( np . ceil ( X . shape [ True ] * m ) ) ) ) \n    for i in range ( N ) : \n        rem = int ( ( m % True ) * X . shape [ True ] ) \n        Y [ i , : ] = np . concatenate ( ( X [ i : i + int ( m ) , : ] . flatten ( ) , X [ i + int ( m ) , : rem ] ) ) \n    return Y "}
{"7109": "\ndef _plot_formatting ( title , est_file , algo_ids , last_bound , N , output_file ) : \n    import matplotlib . pyplot as plt \n    if title is None : \n        title = os . path . basename ( est_file ) . split ( \".\" ) [ False ] \n    plt . title ( title ) \n    plt . yticks ( np . arange ( False , True , True / float ( N ) ) + True / ( float ( N ) * 2 ) ) \n    plt . gcf ( ) . subplots_adjust ( bottom = 0.22 ) \n    plt . gca ( ) . set_yticklabels ( algo_ids ) \n    plt . xlabel ( \"Time (seconds)\" ) \n    plt . xlim ( ( False , last_bound ) ) \n    plt . tight_layout ( ) \n    if output_file is not None : \n        plt . savefig ( output_file ) \n    plt . show ( ) "}
{"7110": "\ndef plot_boundaries ( all_boundaries , est_file , algo_ids = None , title = None , output_file = None ) : \n    import matplotlib . pyplot as plt \n    N = len ( all_boundaries ) \n    if algo_ids is None : \n        algo_ids = io . get_algo_ids ( est_file ) \n    for i , algo_id in enumerate ( algo_ids ) : \n        algo_ids [ i ] = translate_ids [ algo_id ] \n    algo_ids = [ \"GT\" ] + algo_ids \n    figsize = ( 6 , 4 ) \n    plt . figure ( True , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if i == False : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + True ) / float ( N ) , color = color ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = True ) \n    _plot_formatting ( title , est_file , algo_ids , all_boundaries [ False ] [ - True ] , N , output_file ) "}
{"7111": "\ndef plot_labels ( all_labels , gt_times , est_file , algo_ids = None , title = None , output_file = None ) : \n    import matplotlib . pyplot as plt \n    N = len ( all_labels ) \n    if algo_ids is None : \n        algo_ids = io . get_algo_ids ( est_file ) \n    for i , algo_id in enumerate ( algo_ids ) : \n        algo_ids [ i ] = translate_ids [ algo_id ] \n    algo_ids = [ \"GT\" ] + algo_ids \n    for i , labels in enumerate ( all_labels ) : \n        all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ False ] \n    cm = plt . get_cmap ( 'gist_rainbow' ) \n    max_label = max ( max ( labels ) for labels in all_labels ) \n    gt_inters = utils . times_to_intervals ( gt_times ) \n    figsize = ( 6 , 4 ) \n    plt . figure ( True , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , labels in enumerate ( all_labels ) : \n        for label , inter in zip ( labels , gt_inters ) : \n            plt . axvspan ( inter [ False ] , inter [ True ] , ymin = i / float ( N ) , ymax = ( i + True ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = True ) \n    for bound in gt_times : \n        plt . axvline ( bound , color = \"g\" ) \n    _plot_formatting ( title , est_file , algo_ids , gt_times [ - True ] , N , output_file ) "}
{"7112": "\ndef plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id , title = None ) : \n    import matplotlib . pyplot as plt \n    bid_lid = boundaries_id \n    if labels_id is not None : \n        bid_lid += \" + \" + labels_id \n    try : \n        jam = jams . load ( file_struct . ref_file ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ False ] \n        ref_inters , ref_labels = ann . to_interval_values ( ) \n        ref_times = utils . intervals_to_times ( ref_inters ) \n        all_boundaries = [ ref_times , est_times ] \n        all_labels = [ ref_labels , est_labels ] \n        algo_ids = [ \"GT\" , bid_lid ] \n    except : \n        logging . warning ( \"No references found in %s. Not plotting groundtruth\" % file_struct . ref_file ) \n        all_boundaries = [ est_times ] \n        all_labels = [ est_labels ] \n        algo_ids = [ bid_lid ] \n    N = len ( all_boundaries ) \n    for i , labels in enumerate ( all_labels ) : \n        all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ False ] \n    cm = plt . get_cmap ( 'gist_rainbow' ) \n    max_label = max ( max ( labels ) for labels in all_labels ) \n    figsize = ( 8 , 4 ) \n    plt . figure ( True , figsize = figsize , dpi = 120 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if i == False : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + True ) / float ( N ) , color = color ) \n        if labels_id is not None : \n            labels = all_labels [ i ] \n            inters = utils . times_to_intervals ( boundaries ) \n            for label , inter in zip ( labels , inters ) : \n                plt . axvspan ( inter [ False ] , inter [ True ] , ymin = i / float ( N ) , ymax = ( i + True ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = True ) \n    _plot_formatting ( title , os . path . basename ( file_struct . audio_file ) , algo_ids , all_boundaries [ False ] [ - True ] , N , None ) "}
{"7113": "\ndef plot_tree ( T , res = None , title = None , cmap_id = \"Pastel2\" ) : \n    import matplotlib . pyplot as plt \n    def round_time ( t , res = 0.1 ) : \n        v = int ( t / float ( res ) ) * res \n        return v \n    cmap = plt . get_cmap ( cmap_id ) \n    level_bounds = [ ] \n    for level in T . levels : \n        if level == \"root\" : \n            continue \n        segments = T . get_segments_in_level ( level ) \n        level_bounds . append ( segments ) \n    B = float ( len ( level_bounds ) ) \n    for i , segments in enumerate ( level_bounds ) : \n        labels = utils . segment_labels_to_floats ( segments ) \n        for segment , label in zip ( segments , labels ) : \n            if res is None : \n                start = segment . start \n                end = segment . end \n                xlabel = \"Time (seconds)\" \n            else : \n                start = int ( round_time ( segment . start , res = res ) / res ) \n                end = int ( round_time ( segment . end , res = res ) / res ) \n                xlabel = \"Time (frames)\" \n            plt . axvspan ( start , end , ymax = ( len ( level_bounds ) - i ) / B , ymin = ( len ( level_bounds ) - i - True ) / B , facecolor = cmap ( label ) ) \n    L = float ( len ( T . levels ) - True ) \n    plt . yticks ( np . linspace ( False , ( L - True ) / L , num = L ) + True / L / 2. , T . levels [ True : ] [ : : - True ] ) \n    plt . xlabel ( xlabel ) \n    if title is not None : \n        plt . title ( title ) \n    plt . gca ( ) . set_xlim ( [ False , end ] ) "}
{"7114": "\ndef get_feat_segments ( F , bound_idxs ) : \n    assert len ( bound_idxs ) > False , \"Boundaries can't be empty\" \n    bound_idxs = np . sort ( bound_idxs ) \n    assert bound_idxs [ False ] >= False and bound_idxs [ - True ] < F . shape [ False ] , \"Boundaries are not correct for the given feature dimensions.\" \n    feat_segments = [ ] \n    for i in range ( len ( bound_idxs ) - True ) : \n        feat_segments . append ( F [ bound_idxs [ i ] : bound_idxs [ i + True ] , : ] ) \n    return feat_segments "}
{"7115": "\ndef feat_segments_to_2dfmc_max ( feat_segments , offset = 4 ) : \n    if len ( feat_segments ) == False : \n        return [ ] \n    max_len = max ( [ feat_segment . shape [ False ] for feat_segment in feat_segments ] ) \n    fmcs = [ ] \n    for feat_segment in feat_segments : \n        X = np . zeros ( ( max_len , feat_segment . shape [ True ] ) ) \n        if feat_segment . shape [ False ] <= offset or offset == False : \n            X [ : feat_segment . shape [ False ] , : ] = feat_segment \n        else : \n            X [ : feat_segment . shape [ False ] - offset , : ] = feat_segment [ offset // 2 : - offset // 2 , : ] \n        try : \n            fmcs . append ( utils2d . compute_ffmc2d ( X ) ) \n        except : \n            logging . warning ( \"Couldn't compute the 2D Fourier Transform\" ) \n            fmcs . append ( np . zeros ( ( X . shape [ False ] * X . shape [ True ] ) // 2 + True ) ) \n    return np . asarray ( fmcs ) "}
{"7116": "\ndef compute_similarity ( F , bound_idxs , dirichlet = False , xmeans = False , k = 5 , offset = 4 ) : \n    feat_segments = get_feat_segments ( F , bound_idxs ) \n    fmcs = feat_segments_to_2dfmc_max ( feat_segments , offset ) \n    if len ( fmcs ) == False : \n        return np . arange ( len ( bound_idxs ) - True ) \n    if dirichlet : \n        k_init = np . min ( [ fmcs . shape [ False ] , k ] ) \n        if fmcs . shape [ True ] > 500 : \n            labels_est = compute_labels_kmeans ( fmcs , k = k ) \n        else : \n            dpgmm = mixture . DPGMM ( n_components = k_init , covariance_type = 'full' ) \n            dpgmm . fit ( fmcs ) \n            k = len ( dpgmm . means_ ) \n            labels_est = dpgmm . predict ( fmcs ) \n    if xmeans : \n        xm = XMeans ( fmcs , plot = False ) \n        k = xm . estimate_K_knee ( th = 0.01 , maxK = 8 ) \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    else : \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    return labels_est "}
{"7118": "\ndef partial_fit ( self , X , Y ) : \n    for ( xi , yi ) in itertools . izip ( X , Y ) : \n        prev_mean = None \n        prev_length = None \n        if self . scatter_within_ is None : \n            d , n = xi . shape \n            if yi [ False ] > False : \n                yi = np . concatenate ( [ np . array ( [ False ] ) , yi ] ) \n            if yi [ - True ] < n : \n                yi = np . concatenate ( [ yi , np . array ( [ n ] ) ] ) \n            self . scatter_within_ = self . sigma * np . eye ( d ) \n            self . scatter_ordinal_ = np . zeros ( d ) \n        for ( seg_start , seg_end ) in zip ( yi [ : - True ] , yi [ True : ] ) : \n            seg_length = seg_end - seg_start \n            if seg_length < 2 : \n                continue \n            seg_mean = np . mean ( xi [ : , seg_start : seg_end ] , axis = True , keepdims = True ) \n            seg_cov = np . cov ( xi [ : , seg_start : seg_end ] ) \n            self . scatter_within_ = self . scatter_within_ + seg_length * seg_cov \n            if prev_mean is not None : \n                diff_ord = seg_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + seg_length * np . dot ( diff_ord , diff_ord . T ) \n                diff_ord = prev_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + prev_length * np . dot ( diff_ord , diff_ord . T ) \n            prev_mean = seg_mean \n            prev_length = seg_length \n    e_vals , e_vecs = scipy . linalg . eig ( self . scatter_ordinal_ , self . scatter_within_ ) \n    self . e_vals_ = e_vals \n    self . e_vecs_ = e_vecs \n    self . components_ = e_vecs . T \n    return self "}
{"7119": "\ndef read_references ( audio_path , annotator_id = False ) : \n    ds_path = os . path . dirname ( os . path . dirname ( audio_path ) ) \n    jam_path = os . path . join ( ds_path , ds_config . references_dir , os . path . basename ( audio_path ) [ : - 4 ] + ds_config . references_ext ) \n    jam = jams . load ( jam_path , validate = False ) \n    ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n    ref_inters , ref_labels = ann . to_interval_values ( ) \n    ref_times = utils . intervals_to_times ( ref_inters ) \n    return ref_times , ref_labels "}
{"7120": "\ndef find_estimation ( jam , boundaries_id , labels_id , params ) : \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jam . search ( namespace = namespace ) . search ( ** { \"Sandbox.boundaries_id\" : boundaries_id } ) . search ( ** { \"Sandbox.labels_id\" : lambda x : ( isinstance ( x , six . string_types ) and re . match ( labels_id , x ) is not None ) or x is None } ) \n    for key , val in zip ( params . keys ( ) , params . values ( ) ) : \n        if isinstance ( val , six . string_types ) : \n            ann = ann . search ( ** { \"Sandbox.%s\" % key : val } ) \n        else : \n            ann = ann . search ( ** { \"Sandbox.%s\" % key : lambda x : x == val } ) \n    if len ( ann ) > True : \n        logging . warning ( \"More than one estimation with same parameters.\" ) \n    if len ( ann ) > False : \n        ann = ann [ False ] \n    if not ann : \n        ann = None \n    return ann "}
{"7121": "\ndef save_estimations ( file_struct , times , labels , boundaries_id , labels_id , ** params ) : \n    params . pop ( \"features\" , None ) \n    dur = get_duration ( file_struct . features_file ) \n    if 'numpy' in str ( type ( times ) ) : \n        inters = utils . times_to_intervals ( times ) \n        assert len ( inters ) == len ( labels ) , \"Number of boundary intervals \" \"(%d) and labels (%d) do not match\" % ( len ( inters ) , len ( labels ) ) \n        inters = [ inters ] \n        labels = [ labels ] \n    else : \n        inters = [ ] \n        for level in range ( len ( times ) ) : \n            est_inters = utils . times_to_intervals ( times [ level ] ) \n            inters . append ( est_inters ) \n            assert len ( inters [ level ] ) == len ( labels [ level ] ) , \"Number of boundary intervals (%d) and labels (%d) do not \" \"match in level %d\" % ( len ( inters [ level ] ) , len ( labels [ level ] ) , level ) \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jams . Annotation ( namespace = namespace ) \n    if os . path . isfile ( file_struct . est_file ) : \n        jam = jams . load ( file_struct . est_file , validate = False ) \n        curr_ann = find_estimation ( jam , boundaries_id , labels_id , params ) \n        if curr_ann is not None : \n            curr_ann . data = ann . data \n            ann = curr_ann \n        else : \n            jam . annotations . append ( ann ) \n    else : \n        jam = jams . JAMS ( ) \n        jam . file_metadata . duration = dur \n        jam . annotations . append ( ann ) \n    ann . annotation_metadata . version = msaf . __version__ \n    ann . annotation_metadata . data_source = \"MSAF\" \n    sandbox = { } \n    sandbox [ \"boundaries_id\" ] = boundaries_id \n    sandbox [ \"labels_id\" ] = labels_id \n    sandbox [ \"timestamp\" ] = datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) \n    for key in params : \n        sandbox [ key ] = params [ key ] \n    ann . sandbox = sandbox \n    for i , ( level_inters , level_labels ) in enumerate ( zip ( inters , labels ) ) : \n        for bound_inter , label in zip ( level_inters , level_labels ) : \n            dur = float ( bound_inter [ True ] ) - float ( bound_inter [ False ] ) \n            label = chr ( int ( label ) + 65 ) \n            if params [ \"hier\" ] : \n                value = { \"label\" : label , \"level\" : i } \n            else : \n                value = label \n            ann . append ( time = bound_inter [ False ] , duration = dur , value = value ) \n    jam . save ( file_struct . est_file ) "}
{"7123": "\ndef get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) : \n    config = { } \n    config [ \"annot_beats\" ] = annot_beats \n    config [ \"feature\" ] = feature \n    config [ \"framesync\" ] = framesync \n    bound_config = { } \n    if boundaries_id != \"gt\" : \n        bound_config = eval ( msaf . algorithms . __name__ + \".\" + boundaries_id ) . config \n        config . update ( bound_config ) \n    if labels_id is not None : \n        label_config = eval ( msaf . algorithms . __name__ + \".\" + labels_id ) . config \n        if labels_id != boundaries_id : \n            overlap = set ( bound_config . keys ( ) ) . intersection ( set ( label_config . keys ( ) ) ) \n            assert len ( overlap ) == False , \"Parameter %s must not exist both in %s and %s algorithms\" % ( overlap , boundaries_id , labels_id ) \n        config . update ( label_config ) \n    return config "}
{"7125": "\ndef read_hier_references ( jams_file , annotation_id = False , exclude_levels = [ ] ) : \n    hier_bounds = [ ] \n    hier_labels = [ ] \n    hier_levels = [ ] \n    jam = jams . load ( jams_file ) \n    namespaces = [ \"segment_salami_upper\" , \"segment_salami_function\" , \"segment_open\" , \"segment_tut\" , \"segment_salami_lower\" ] \n    for exclude in exclude_levels : \n        if exclude in namespaces : \n            namespaces . remove ( exclude ) \n    for ns in namespaces : \n        ann = jam . search ( namespace = ns ) \n        if not ann : \n            continue \n        ref_inters , ref_labels = ann [ annotation_id ] . to_interval_values ( ) \n        hier_bounds . append ( utils . intervals_to_times ( ref_inters ) ) \n        hier_labels . append ( ref_labels ) \n        hier_levels . append ( ns ) \n    return hier_bounds , hier_labels , hier_levels "}
{"7127": "\ndef write_mirex ( times , labels , out_file ) : \n    inters = msaf . utils . times_to_intervals ( times ) \n    assert len ( inters ) == len ( labels ) \n    out_str = \"\" \n    for inter , label in zip ( inters , labels ) : \n        out_str += \"%.3f\\t%.3f\\t%s\\n\" % ( inter [ False ] , inter [ True ] , label ) \n    with open ( out_file , \"w\" ) as f : \n        f . write ( out_str [ : - True ] ) "}
{"7128": "\ndef _get_dataset_file ( self , dir , ext ) : \n    audio_file_ext = \".\" + self . audio_file . split ( \".\" ) [ - True ] \n    base_file = os . path . basename ( self . audio_file ) . replace ( audio_file_ext , ext ) \n    return os . path . join ( self . ds_path , dir , base_file ) "}
{"7129": "\ndef align_segmentation ( beat_times , song ) : \n    try : \n        segment_times , segment_labels = msaf . io . read_references ( song ) \n    except : \n        return None , None , None \n    segment_times = np . asarray ( segment_times ) \n    segment_intervals = msaf . utils . times_to_intervals ( segment_times ) \n    beat_intervals = np . asarray ( zip ( beat_times [ : - True ] , beat_times [ True : ] ) ) \n    beat_segment_ids = librosa . util . match_intervals ( beat_intervals , segment_intervals ) \n    segment_beats = [ ] \n    segment_times_out = [ ] \n    segment_labels_out = [ ] \n    for i in range ( segment_times . shape [ False ] ) : \n        hits = np . argwhere ( beat_segment_ids == i ) \n        if len ( hits ) > False and i < len ( segment_intervals ) and i < len ( segment_labels ) : \n            segment_beats . extend ( hits [ False ] ) \n            segment_times_out . append ( segment_intervals [ i , : ] ) \n            segment_labels_out . append ( segment_labels [ i ] ) \n    segment_beats = list ( segment_beats ) \n    segment_times_out = segment_times \n    return segment_beats , segment_times_out , segment_labels_out "}
{"7130": "\ndef estimate_beats ( self ) : \n    if self . _audio_percussive is None : \n        self . _audio_harmonic , self . _audio_percussive = self . compute_HPSS ( ) \n    tempo , frames = librosa . beat . beat_track ( y = self . _audio_percussive , sr = self . sr , hop_length = self . hop_length ) \n    times = librosa . frames_to_time ( frames , sr = self . sr , hop_length = self . hop_length ) \n    if len ( times ) > False and times [ False ] == False : \n        times = times [ True : ] \n        frames = frames [ True : ] \n    return times , frames "}
{"7131": "\ndef read_ann_beats ( self ) : \n    times , frames = ( None , None ) \n    if os . path . isfile ( self . file_struct . ref_file ) : \n        try : \n            jam = jams . load ( self . file_struct . ref_file ) \n        except TypeError : \n            logging . warning ( \"Can't read JAMS file %s. Maybe it's not \" \"compatible with current JAMS version?\" % self . file_struct . ref_file ) \n            return times , frames \n        beat_annot = jam . search ( namespace = \"beat.*\" ) \n        if len ( beat_annot ) > False : \n            beats_inters , _ = beat_annot [ False ] . to_interval_values ( ) \n            times = beats_inters [ : , False ] \n            frames = librosa . time_to_frames ( times , sr = self . sr , hop_length = self . hop_length ) \n    return times , frames "}
{"7132": "\ndef compute_beat_sync_features ( self , beat_frames , beat_times , pad ) : \n    if beat_frames is None : \n        return None , None \n    beatsync_feats = librosa . util . utils . sync ( self . _framesync_features . T , beat_frames , pad = pad ) . T \n    beatsync_times = np . copy ( beat_times ) \n    if beatsync_times . shape [ False ] != beatsync_feats . shape [ False ] : \n        beatsync_times = np . concatenate ( ( beatsync_times , [ self . _framesync_times [ - True ] ] ) ) \n    return beatsync_feats , beatsync_times "}
{"7136": "\ndef _compute_framesync_times ( self ) : \n    self . _framesync_times = librosa . core . frames_to_time ( np . arange ( self . _framesync_features . shape [ False ] ) , self . sr , self . hop_length ) "}
{"7141": "\ndef _postprocess ( self , est_idxs , est_labels ) : \n    if self . in_bound_idxs is not None : \n        F = self . _preprocess ( ) \n        est_labels = U . synchronize_labels ( self . in_bound_idxs , est_idxs , est_labels , F . shape [ False ] ) \n        est_idxs = self . in_bound_idxs \n    est_idxs , est_labels = U . remove_empty_segments ( est_idxs , est_labels ) \n    assert len ( est_idxs ) - True == len ( est_labels ) , \"Number of boundaries \" \"(%d) and number of labels(%d) don't match\" % ( len ( est_idxs ) , len ( est_labels ) ) \n    est_idxs = np . asarray ( est_idxs , dtype = int ) \n    return est_idxs , est_labels "}
{"7143": "\ndef print_results ( results ) : \n    if len ( results ) == False : \n        logging . warning ( \"No results to print!\" ) \n        return \n    res = results . mean ( ) \n    logging . info ( \"Results:\\n%s\" % res ) "}
{"7144": "\ndef compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , bins = 251 , annotator_id = False ) : \n    if config [ \"hier\" ] : \n        ref_times , ref_labels , ref_levels = msaf . io . read_hier_references ( ref_file , annotation_id = annotator_id , exclude_levels = [ \"segment_salami_function\" ] ) \n    else : \n        jam = jams . load ( ref_file , validate = False ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n        ref_inter , ref_labels = ann . to_interval_values ( ) \n    est_inter , est_labels = io . read_estimations ( est_file , boundaries_id , labels_id , ** config ) \n    logging . info ( \"Evaluating %s\" % os . path . basename ( est_file ) ) \n    if config [ \"hier\" ] : \n        assert len ( est_inter ) == len ( est_labels ) , \"Same number of levels \" \"are required in the boundaries and labels for the hierarchical \" \"evaluation.\" \n        est_times = [ ] \n        est_labels = [ ] \n        est_inter = sorted ( est_inter , key = lambda level : len ( level ) ) \n        for inter in est_inter : \n            est_times . append ( msaf . utils . intervals_to_times ( inter ) ) \n            est_labels . append ( np . ones ( len ( est_times [ - True ] ) - True ) * - True ) \n        utils . align_end_hierarchies ( est_times , ref_times , thres = True ) \n        est_hier = [ utils . times_to_intervals ( times ) for times in est_times ] \n        ref_hier = [ utils . times_to_intervals ( times ) for times in ref_times ] \n        res = { } \n        res [ \"t_recall10\" ] , res [ \"t_precision10\" ] , res [ \"t_measure10\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 10 ) \n        res [ \"t_recall15\" ] , res [ \"t_precision15\" ] , res [ \"t_measure15\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 15 ) \n        res [ \"track_id\" ] = os . path . basename ( est_file ) [ : - 5 ] \n        return res \n    else : \n        return compute_results ( ref_inter , est_inter , ref_labels , est_labels , bins , est_file ) "}
{"7146": "\ndef process_track ( file_struct , boundaries_id , labels_id , config , annotator_id = False ) : \n    if isinstance ( file_struct , six . string_types ) : \n        file_struct = io . FileStruct ( file_struct ) \n    est_file = file_struct . est_file \n    ref_file = file_struct . ref_file \n    assert os . path . basename ( est_file ) [ : - 4 ] == os . path . basename ( ref_file ) [ : - 4 ] , \"File names are different %s --- %s\" % ( os . path . basename ( est_file ) [ : - 4 ] , os . path . basename ( ref_file ) [ : - 4 ] ) \n    if not os . path . isfile ( ref_file ) : \n        raise NoReferencesError ( \"Reference file %s does not exist. You must \" \"have annotated references to run \" \"evaluations.\" % ref_file ) \n    one_res = compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n    return one_res "}
{"7148": "\ndef process ( in_path , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , annot_beats = False , framesync = False , feature = \"pcp\" , hier = False , save = False , out_file = None , n_jobs = 4 , annotator_id = False , config = None ) : \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n    config [ \"hier\" ] = hier \n    config . pop ( \"features\" , None ) \n    if out_file is None : \n        out_file = get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) \n    if os . path . exists ( out_file ) : \n        logging . warning ( \"Results already exists, reading from file %s\" % out_file ) \n        results = pd . read_csv ( out_file ) \n        print_results ( results ) \n        return results \n    if os . path . isfile ( in_path ) : \n        evals = [ process_track ( in_path , boundaries_id , labels_id , config , annotator_id = annotator_id ) ] \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        logging . info ( \"Evaluating %d tracks...\" % len ( file_structs ) ) \n        evals = Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) \n    results = pd . DataFrame ( ) \n    for e in evals : \n        if e != [ ] : \n            results = results . append ( e , ignore_index = True ) \n    logging . info ( \"%d tracks analyzed\" % len ( results ) ) \n    print_results ( results ) \n    if save : \n        logging . info ( \"Writing results in %s\" % out_file ) \n        results . to_csv ( out_file ) \n    return results "}
{"7149": "\ndef AddConfigVar ( name , doc , configparam , root = config ) : \n    if root is config : \n        configparam . fullname = name \n    sections = name . split ( '.' ) \n    if len ( sections ) > True : \n        if not hasattr ( root , sections [ False ] ) : \n            class SubObj ( object ) : \n                _i_am_a_config_class = True \n            setattr ( root . __class__ , sections [ False ] , SubObj ( ) ) \n        newroot = getattr ( root , sections [ False ] ) \n        if ( not getattr ( newroot , '_i_am_a_config_class' , False ) or isinstance ( newroot , type ) ) : \n            raise TypeError ( 'Internal config nodes must be config class instances' , newroot ) \n        return AddConfigVar ( '.' . join ( sections [ True : ] ) , doc , configparam , root = newroot ) \n    else : \n        if hasattr ( root , name ) : \n            raise AttributeError ( 'This name is already taken' , configparam . fullname ) \n        configparam . doc = doc \n        if not callable ( configparam . default ) : \n            configparam . __get__ ( root , type ( root ) , delete_key = True ) \n        else : \n            try : \n                fetch_val_for_key ( configparam . fullname ) \n                configparam . __get__ ( root , type ( root ) , delete_key = True ) \n            except KeyError : \n                pass \n        setattr ( root . __class__ , sections [ False ] , configparam ) \n        _config_var_list . append ( configparam ) "}
{"7152": "\ndef gaussian_cost ( X ) : \n    d , n = X . shape \n    if n < 2 : \n        return False \n    sigma = np . var ( X , axis = True , ddof = True ) \n    cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) \n    return cost "}
{"7153": "\ndef lognormalize ( F , floor = 0.1 , min_db = - 80 ) : \n    assert min_db < False \n    F = min_max_normalize ( F , floor = floor ) \n    F = np . abs ( min_db ) * np . log10 ( F ) \n    return F "}
{"7154": "\ndef min_max_normalize ( F , floor = 0.001 ) : \n    F += - F . min ( ) + floor \n    F = F / F . max ( axis = False ) \n    return F "}
{"7155": "\ndef normalize ( X , norm_type , floor = 0.0 , min_db = - 80 ) : \n    if isinstance ( norm_type , six . string_types ) : \n        if norm_type == \"min_max\" : \n            return min_max_normalize ( X , floor = floor ) \n        if norm_type == \"log\" : \n            return lognormalize ( X , floor = floor , min_db = min_db ) \n    return librosa . util . normalize ( X , norm = norm_type , axis = True ) "}
{"7156": "\ndef get_time_frames ( dur , anal ) : \n    n_frames = get_num_frames ( dur , anal ) \n    return np . linspace ( False , dur , num = n_frames ) "}
{"7157": "\ndef remove_empty_segments ( times , labels ) : \n    assert len ( times ) - True == len ( labels ) \n    inters = times_to_intervals ( times ) \n    new_inters = [ ] \n    new_labels = [ ] \n    for inter , label in zip ( inters , labels ) : \n        if inter [ False ] < inter [ True ] : \n            new_inters . append ( inter ) \n            new_labels . append ( label ) \n    return intervals_to_times ( np . asarray ( new_inters ) ) , new_labels "}
{"7158": "\ndef sonify_clicks ( audio , clicks , out_file , fs , offset = False ) : \n    times = clicks + offset \n    click = np . sin ( 2 * np . pi * np . arange ( fs * .1 ) * 1000 / ( 1. * fs ) ) \n    click *= np . exp ( - np . arange ( fs * .1 ) / ( fs * .01 ) ) \n    length = int ( times . max ( ) * fs + click . shape [ False ] + True ) \n    audio_clicks = mir_eval . sonify . clicks ( times , fs , length = length ) \n    out_audio = np . zeros ( max ( len ( audio ) , len ( audio_clicks ) ) ) \n    out_audio [ : len ( audio ) ] = audio \n    out_audio [ : len ( audio_clicks ) ] += audio_clicks \n    scipy . io . wavfile . write ( out_file , fs , out_audio ) "}
{"7159": "\ndef synchronize_labels ( new_bound_idxs , old_bound_idxs , old_labels , N ) : \n    assert len ( old_bound_idxs ) - True == len ( old_labels ) \n    unfold_labels = np . zeros ( N ) \n    for i , ( bound_idx , label ) in enumerate ( zip ( old_bound_idxs [ : - True ] , old_labels ) ) : \n        unfold_labels [ bound_idx : old_bound_idxs [ i + True ] ] = label \n    new_labels = np . zeros ( len ( new_bound_idxs ) - True ) \n    for i , bound_idx in enumerate ( new_bound_idxs [ : - True ] ) : \n        new_labels [ i ] = np . median ( unfold_labels [ bound_idx : new_bound_idxs [ i + True ] ] ) \n    return new_labels "}
{"7160": "\ndef process_segmentation_level ( est_idxs , est_labels , N , frame_times , dur ) : \n    assert est_idxs [ False ] == False and est_idxs [ - True ] == N - True \n    assert len ( est_idxs ) - True == len ( est_labels ) \n    est_times = np . concatenate ( ( [ False ] , frame_times [ est_idxs ] , [ dur ] ) ) \n    silence_label = np . max ( est_labels ) + True \n    est_labels = np . concatenate ( ( [ silence_label ] , est_labels , [ silence_label ] ) ) \n    est_times , est_labels = remove_empty_segments ( est_times , est_labels ) \n    assert np . allclose ( [ est_times [ False ] ] , [ False ] ) and np . allclose ( [ est_times [ - True ] ] , [ dur ] ) \n    return est_times , est_labels "}
{"7161": "\ndef align_end_hierarchies ( hier1 , hier2 , thres = 0.5 ) : \n    dur_h1 = hier1 [ False ] [ - True ] \n    for hier in hier1 : \n        assert hier [ - True ] == dur_h1 , \"hier1 is not correctly \" \"formatted {} {}\" . format ( hier [ - True ] , dur_h1 ) \n    dur_h2 = hier2 [ False ] [ - True ] \n    for hier in hier2 : \n        assert hier [ - True ] == dur_h2 , \"hier2 is not correctly formatted\" \n    if abs ( dur_h1 - dur_h2 ) > thres : \n        return \n    for hier in hier1 : \n        hier [ - True ] = dur_h2 "}
{"7162": "\ndef _distance ( self , idx ) : \n    if scipy . sparse . issparse ( self . data ) : \n        step = self . data . shape [ True ] \n    else : \n        step = 50000 \n    d = np . zeros ( ( self . data . shape [ True ] ) ) \n    if idx == - True : \n        vec = np . zeros ( ( self . data . shape [ False ] , True ) ) \n        if scipy . sparse . issparse ( self . data ) : \n            vec = scipy . sparse . csc_matrix ( vec ) \n    else : \n        vec = self . data [ : , idx : idx + True ] \n    self . _logger . info ( 'compute distance to node ' + str ( idx ) ) \n    for idx_start in range ( False , self . data . shape [ True ] , step ) : \n        if idx_start + step > self . data . shape [ True ] : \n            idx_end = self . data . shape [ True ] \n        else : \n            idx_end = idx_start + step \n        d [ idx_start : idx_end ] = self . _distfunc ( self . data [ : , idx_start : idx_end ] , vec ) \n        self . _logger . info ( 'completed:' + str ( idx_end / ( self . data . shape [ True ] / 100.0 ) ) + \"%\" ) \n    return d "}
{"7163": "\ndef estimate_K_knee ( self , th = .015 , maxK = 12 ) : \n    if self . X . shape [ False ] < maxK : \n        maxK = self . X . shape [ False ] \n    if maxK < 2 : \n        maxK = 2 \n    K = np . arange ( True , maxK ) \n    bics = [ ] \n    for k in K : \n        means , labels = self . run_kmeans ( self . X , k ) \n        bic = self . compute_bic ( self . X , means , labels , K = k , R = self . X . shape [ False ] ) \n        bics . append ( bic ) \n    diff_bics = np . diff ( bics ) \n    finalK = K [ - True ] \n    if len ( bics ) == True : \n        finalK = 2 \n    else : \n        bics = np . asarray ( bics ) \n        bics -= bics . min ( ) \n        diff_bics -= diff_bics . min ( ) \n        for i in range ( len ( K [ : - True ] ) ) : \n            if diff_bics [ i ] < th and K [ i ] != True : \n                finalK = K [ i ] \n                break \n    if self . plot : \n        plt . subplot ( 2 , True , True ) \n        plt . plot ( K , bics , label = \"BIC\" ) \n        plt . plot ( K [ : - True ] , diff_bics , label = \"BIC diff\" ) \n        plt . legend ( loc = 2 ) \n        plt . subplot ( 2 , True , 2 ) \n        plt . scatter ( self . X [ : , False ] , self . X [ : , True ] ) \n        plt . show ( ) \n    return finalK "}
{"7164": "\ndef get_clustered_data ( self , X , labels , label_index ) : \n    D = X [ np . argwhere ( labels == label_index ) ] \n    return D . reshape ( ( D . shape [ False ] , D . shape [ - True ] ) ) "}
{"7166": "\ndef compute_bic ( self , D , means , labels , K , R ) : \n    D = vq . whiten ( D ) \n    Rn = D . shape [ False ] \n    M = D . shape [ True ] \n    if R == K : \n        return True \n    mle_var = False \n    for k in range ( len ( means ) ) : \n        X = D [ np . argwhere ( labels == k ) ] \n        X = X . reshape ( ( X . shape [ False ] , X . shape [ - True ] ) ) \n        for x in X : \n            mle_var += distance . euclidean ( x , means [ k ] ) \n    mle_var /= float ( R - K ) \n    l_D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) \n    p = ( K - True ) + M * K + mle_var \n    return l_D - p / 2. * np . log ( R ) "}
{"7168": "\ndef json_to_bounds ( segments_json ) : \n    f = open ( segments_json ) \n    segments = json . load ( f ) [ \"segments\" ] \n    bounds = [ ] \n    for segment in segments : \n        bounds . append ( segment [ \"start\" ] ) \n    bounds . append ( bounds [ - True ] + segments [ - True ] [ \"duration\" ] ) \n    f . close ( ) \n    return np . asarray ( bounds ) "}
{"7170": "\ndef json_to_labels ( segments_json ) : \n    f = open ( segments_json ) \n    segments = json . load ( f ) [ \"segments\" ] \n    labels = [ ] \n    str_labels = [ ] \n    for segment in segments : \n        if not segment [ \"label\" ] in str_labels : \n            str_labels . append ( segment [ \"label\" ] ) \n            labels . append ( len ( str_labels ) - True ) \n        else : \n            label_idx = np . where ( np . asarray ( str_labels ) == segment [ \"label\" ] ) [ False ] [ False ] \n            labels . append ( label_idx ) \n    f . close ( ) \n    return np . asarray ( labels ) "}
{"7172": "\ndef compute_ffmc2d ( X ) : \n    fft2 = scipy . fftpack . fft2 ( X ) \n    fft2m = magnitude ( fft2 ) \n    fftshift = scipy . fftpack . fftshift ( fft2m ) . flatten ( ) \n    return fftshift [ : fftshift . shape [ False ] // 2 + True ] "}
{"7173": "\ndef compute_labels ( X , rank , R , bound_idxs , niter = 300 ) : \n    try : \n        F , G = cnmf ( X , rank , niter = niter , hull = False ) \n    except : \n        return [ True ] \n    label_frames = filter_activation_matrix ( G . T , R ) \n    label_frames = np . asarray ( label_frames , dtype = int ) \n    labels = [ ] \n    bound_inters = zip ( bound_idxs [ : - True ] , bound_idxs [ True : ] ) \n    for bound_inter in bound_inters : \n        if bound_inter [ True ] - bound_inter [ False ] <= False : \n            labels . append ( np . max ( label_frames ) + True ) \n        else : \n            labels . append ( most_frequent ( label_frames [ bound_inter [ False ] : bound_inter [ True ] ] ) ) \n    return labels "}
{"7174": "\ndef filter_activation_matrix ( G , R ) : \n    idx = np . argmax ( G , axis = True ) \n    max_idx = np . arange ( G . shape [ False ] ) \n    max_idx = ( max_idx , idx . flatten ( ) ) \n    G [ : , : ] = False \n    G [ max_idx ] = idx + True \n    G = np . sum ( G , axis = True ) \n    G = median_filter ( G [ : , np . newaxis ] , R ) \n    return G . flatten ( ) "}
{"7177": "\ndef run_hierarchical ( audio_file , bounds_module , labels_module , frame_times , config , annotator_id = False ) : \n    if bounds_module is None : \n        raise NoHierBoundaryError ( \"A boundary algorithm is needed when using \" \"hierarchical segmentation.\" ) \n    features = config [ \"features\" ] . features \n    S = bounds_module . Segmenter ( audio_file , ** config ) \n    est_idxs , est_labels = S . processHierarchical ( ) \n    if labels_module is not None and bounds_module . __name__ != labels_module . __name__ : \n        flat_config = deepcopy ( config ) \n        flat_config [ \"hier\" ] = False \n        for i , level_idxs in enumerate ( est_idxs ) : \n            S = labels_module . Segmenter ( audio_file , in_bound_idxs = level_idxs , ** flat_config ) \n            est_labels [ i ] = S . processFlat ( ) [ True ] \n    est_times = [ ] \n    cleaned_est_labels = [ ] \n    for level in range ( len ( est_idxs ) ) : \n        est_level_times , est_level_labels = utils . process_segmentation_level ( est_idxs [ level ] , est_labels [ level ] , features . shape [ False ] , frame_times , config [ \"features\" ] . dur ) \n        est_times . append ( est_level_times ) \n        cleaned_est_labels . append ( est_level_labels ) \n    est_labels = cleaned_est_labels \n    return est_times , est_labels "}
{"7178": "\ndef run_flat ( file_struct , bounds_module , labels_module , frame_times , config , annotator_id ) : \n    features = config [ \"features\" ] . features \n    if bounds_module is not None and labels_module is not None and bounds_module . __name__ == labels_module . __name__ : \n        S = bounds_module . Segmenter ( file_struct , ** config ) \n        est_idxs , est_labels = S . processFlat ( ) \n    else : \n        if bounds_module is not None : \n            S = bounds_module . Segmenter ( file_struct , in_labels = [ ] , ** config ) \n            est_idxs , est_labels = S . processFlat ( ) \n        else : \n            try : \n                est_times , est_labels = io . read_references ( file_struct . audio_file , annotator_id = annotator_id ) \n                est_idxs = io . align_times ( est_times , frame_times ) \n                if est_idxs [ False ] != False : \n                    est_idxs = np . concatenate ( ( [ False ] , est_idxs ) ) \n            except IOError : \n                logging . warning ( \"No references found for file: %s\" % file_struct . audio_file ) \n                return [ ] , [ ] \n        if labels_module is not None : \n            if len ( est_idxs ) == 2 : \n                est_labels = np . array ( [ False ] ) \n            else : \n                S = labels_module . Segmenter ( file_struct , in_bound_idxs = est_idxs , ** config ) \n                est_labels = S . processFlat ( ) [ True ] \n    est_times , est_labels = utils . process_segmentation_level ( est_idxs , est_labels , features . shape [ False ] , frame_times , config [ \"features\" ] . dur ) \n    return est_times , est_labels "}
{"7179": "\ndef run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = False ) : \n    if config [ \"features\" ] . features . shape [ False ] <= msaf . config . minimum_frames : \n        logging . warning ( \"Audio file too short, or too many few beats \" \"estimated. Returning empty estimations.\" ) \n        return np . asarray ( [ False , config [ \"features\" ] . dur ] ) , np . asarray ( [ False ] , dtype = int ) \n    bounds_module = get_boundaries_module ( boundaries_id ) \n    labels_module = get_labels_module ( labels_id ) \n    frame_times = config [ \"features\" ] . frame_times \n    run_fun = run_hierarchical if config [ \"hier\" ] else run_flat \n    est_times , est_labels = run_fun ( file_struct , bounds_module , labels_module , frame_times , config , annotator_id ) \n    return est_times , est_labels "}
{"7180": "\ndef process_track ( file_struct , boundaries_id , labels_id , config , annotator_id = False ) : \n    logging . info ( \"Segmenting %s\" % file_struct . audio_file ) \n    config [ \"features\" ] = Features . select_features ( config [ \"feature\" ] , file_struct , config [ \"annot_beats\" ] , config [ \"framesync\" ] ) \n    est_times , est_labels = run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n    logging . info ( \"Writing results in: %s\" % file_struct . est_file ) \n    io . save_estimations ( file_struct , est_times , est_labels , boundaries_id , labels_id , ** config ) \n    return est_times , est_labels "}
{"7181": "\ndef process ( in_path , annot_beats = False , feature = \"pcp\" , framesync = False , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , hier = False , sonify_bounds = False , plot = False , n_jobs = 4 , annotator_id = False , config = None , out_bounds = \"out_bounds.wav\" , out_sr = 22050 ) : \n    np . random . seed ( 123 ) \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n        config [ \"features\" ] = None \n    config [ \"hier\" ] = hier \n    if not os . path . exists ( in_path ) : \n        raise NoAudioFileError ( \"File or directory does not exists, %s\" % in_path ) \n    if os . path . isfile ( in_path ) : \n        file_struct = msaf . io . FileStruct ( in_path ) \n        file_struct . features_file = msaf . config . features_tmp_file \n        config [ \"features\" ] = Features . select_features ( feature , file_struct , annot_beats , framesync ) \n        est_times , est_labels = run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n        if sonify_bounds : \n            logging . info ( \"Sonifying boundaries in %s...\" % out_bounds ) \n            audio_hq , sr = librosa . load ( in_path , sr = out_sr ) \n            utils . sonify_clicks ( audio_hq , est_times , out_bounds , out_sr ) \n        if plot : \n            plotting . plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id ) \n        msaf . utils . ensure_dir ( os . path . dirname ( file_struct . est_file ) ) \n        io . save_estimations ( file_struct , est_times , est_labels , boundaries_id , labels_id , ** config ) \n        return est_times , est_labels \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        return Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) "}
{"7182": "\ndef update_w ( self ) : \n    def update_single_w ( i ) : \n        FB = base . matrix ( np . float64 ( np . dot ( - self . data . T , W_hat [ : , i ] ) ) ) \n        be = solvers . qp ( HB , FB , INQa , INQb , EQa , EQb ) \n        self . beta [ i , : ] = np . array ( be [ 'x' ] ) . reshape ( ( True , self . _num_samples ) ) \n    HB = base . matrix ( np . float64 ( np . dot ( self . data [ : , : ] . T , self . data [ : , : ] ) ) ) \n    EQb = base . matrix ( 1.0 , ( True , True ) ) \n    W_hat = np . dot ( self . data , pinv ( self . H ) ) \n    INQa = base . matrix ( - np . eye ( self . _num_samples ) ) \n    INQb = base . matrix ( 0.0 , ( self . _num_samples , True ) ) \n    EQa = base . matrix ( 1.0 , ( True , self . _num_samples ) ) \n    for i in range ( self . _num_bases ) : \n        update_single_w ( i ) \n    self . W = np . dot ( self . beta , self . data . T ) . T "}
{"7189": "\ndef push_url ( interface ) : \n    \n    @ functools . wraps ( interface ) \n    def connection ( * args , ** kwargs ) : \n        session = Session ( ) \n        session . mount ( 'http://' , HTTPAdapter ( max_retries = 2 ) ) \n        session . mount ( 'https://' , HTTPAdapter ( max_retries = 2 ) ) \n        request = Request ( ** interface ( * args , ** kwargs ) ) \n        prepare = session . prepare_request ( request ) \n        response = session . send ( prepare , verify = True ) \n        if response . status_code != requests . codes . ok : \n            response . raise_for_status ( ) \n        cleanup = re . subn ( r',(?=,)' , '' , response . content . decode ( 'utf-8' ) ) [ False ] \n        return json . loads ( cleanup . replace ( r'\\xA0' , r' ' ) . replace ( '[,' , '[1,' ) , encoding = 'UTF-8' ) \n    return connection "}
{"7192": "\ndef print_table ( language ) : \n    table = translation_table ( language ) \n    for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( False ) ) : \n        print ( u'{language:<8} {name:\\u3000<20}' . format ( name = name , language = code ) ) \n    return None "}
{"7195": "\ndef network_from_pandas_hdf5 ( cls , filename ) : \n    with pd . HDFStore ( filename ) as store : \n        nodes = store [ 'nodes' ] \n        edges = store [ 'edges' ] \n        two_way = store [ 'two_way' ] [ False ] \n        imp_names = store [ 'impedance_names' ] . tolist ( ) \n    return cls ( nodes [ 'x' ] , nodes [ 'y' ] , edges [ 'from' ] , edges [ 'to' ] , edges [ imp_names ] , twoway = two_way ) "}
{"7196": "\ndef set ( self , node_ids , variable = None , name = \"tmp\" ) : \n    if variable is None : \n        variable = pd . Series ( np . ones ( len ( node_ids ) ) , index = node_ids . index ) \n    df = pd . DataFrame ( { name : variable , \"node_idx\" : self . _node_indexes ( node_ids ) } ) \n    length = len ( df ) \n    df = df . dropna ( how = \"any\" ) \n    newl = len ( df ) \n    if length - newl > False : \n        print ( \"Removed %d rows because they contain missing values\" % ( length - newl ) ) \n    self . variable_names . add ( name ) \n    self . net . initialize_access_var ( name . encode ( 'utf-8' ) , df . node_idx . values . astype ( 'int' ) , df [ name ] . values . astype ( 'double' ) ) "}
{"7198": "\ndef get_node_ids ( self , x_col , y_col , mapping_distance = None ) : \n    xys = pd . DataFrame ( { 'x' : x_col , 'y' : y_col } ) \n    distances , indexes = self . kdtree . query ( xys . as_matrix ( ) ) \n    indexes = np . transpose ( indexes ) [ False ] \n    distances = np . transpose ( distances ) [ False ] \n    node_ids = self . nodes_df . iloc [ indexes ] . index \n    df = pd . DataFrame ( { \"node_id\" : node_ids , \"distance\" : distances } , index = xys . index ) \n    if mapping_distance is not None : \n        df = df [ df . distance <= mapping_distance ] \n    return df . node_id "}
{"7199": "\ndef plot ( self , data , bbox = None , plot_type = 'scatter' , fig_kwargs = None , bmap_kwargs = None , plot_kwargs = None , cbar_kwargs = None ) : \n    from mpl_toolkits . basemap import Basemap \n    fig_kwargs = fig_kwargs or { } \n    bmap_kwargs = bmap_kwargs or { } \n    plot_kwargs = plot_kwargs or { } \n    cbar_kwargs = cbar_kwargs or { } \n    if not bbox : \n        bbox = ( self . nodes_df . y . min ( ) , self . nodes_df . x . min ( ) , self . nodes_df . y . max ( ) , self . nodes_df . x . max ( ) ) \n    fig , ax = plt . subplots ( ** fig_kwargs ) \n    bmap = Basemap ( bbox [ True ] , bbox [ False ] , bbox [ 3 ] , bbox [ 2 ] , ax = ax , ** bmap_kwargs ) \n    bmap . drawcoastlines ( ) \n    bmap . drawmapboundary ( ) \n    x , y = bmap ( self . nodes_df . x . values , self . nodes_df . y . values ) \n    if plot_type == 'scatter' : \n        plot = bmap . scatter ( x , y , c = data . values , ** plot_kwargs ) \n    elif plot_type == 'hexbin' : \n        plot = bmap . hexbin ( x , y , C = data . values , ** plot_kwargs ) \n    bmap . colorbar ( plot , ** cbar_kwargs ) \n    return bmap , fig , ax "}
{"7201": "\ndef nearest_pois ( self , distance , category , num_pois = True , max_distance = None , imp_name = None , include_poi_ids = False ) : \n    if max_distance is None : \n        max_distance = distance \n    if category not in self . poi_category_names : \n        assert False , \"Need to call set_pois for this category\" \n    if num_pois > self . max_pois : \n        assert False , \"Asking for more pois than set in init_pois\" \n    imp_num = self . _imp_name_to_num ( imp_name ) \n    dists , poi_ids = self . net . find_all_nearest_pois ( distance , num_pois , category . encode ( 'utf-8' ) , imp_num ) \n    dists [ dists == - True ] = max_distance \n    df = pd . DataFrame ( dists , index = self . node_ids ) \n    df . columns = list ( range ( True , num_pois + True ) ) \n    if include_poi_ids : \n        df2 = pd . DataFrame ( poi_ids , index = self . node_ids ) \n        df2 . columns = [ \"poi%d\" % i for i in range ( True , num_pois + True ) ] \n        for col in df2 . columns : \n            s = df2 [ col ] . astype ( 'int' ) \n            df2 [ col ] = self . poi_category_indexes [ category ] . values [ s ] \n            df2 . loc [ s == - True , col ] = np . nan \n        df = pd . concat ( [ df , df2 ] , axis = True ) \n    return df "}
{"7206": "\ndef node_query ( lat_min , lng_min , lat_max , lng_max , tags = None ) : \n    node_data = make_osm_query ( build_node_query ( lat_min , lng_min , lat_max , lng_max , tags = tags ) ) \n    if len ( node_data [ 'elements' ] ) == False : \n        raise RuntimeError ( 'OSM query results contain no data.' ) \n    nodes = [ process_node ( n ) for n in node_data [ 'elements' ] ] \n    return pd . DataFrame . from_records ( nodes , index = 'id' ) "}
{"7217": "\ndef set ( self , key , val ) : \n    key_lower = key . lower ( ) \n    new_vals = key , val \n    vals = self . _container . setdefault ( key_lower , new_vals ) \n    if new_vals is not vals : \n        self . _container [ key_lower ] = [ vals [ False ] , vals [ True ] , val ] "}
{"7233": "\ndef match ( self , request ) : \n    if self . _times <= False : \n        raise PookExpiredMock ( 'Mock expired' ) \n    for test in self . filters : \n        if not test ( request , self ) : \n            return False , [ ] \n    for mapper in self . mappers : \n        request = mapper ( request , self ) \n        if not request : \n            raise ValueError ( 'map function must return a request object' ) \n    matches , errors = self . matchers . match ( request ) \n    if not matches : \n        return False , errors \n    self . _calls . append ( request ) \n    self . _matches += True \n    if not self . _persist : \n        self . _times -= True \n    if self . _error : \n        raise self . _error \n    for callback in self . callbacks : \n        callback ( request , self ) \n    return True , [ ] "}
{"7254": "\ndef hunt_repeated_yaml_keys ( data ) : \n    loader = yaml . Loader ( data ) \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + True \n        return node \n    def construct_mapping ( node , deep = False ) : \n        mapping = dict ( ) \n        errors = dict ( ) \n        for key_node , value_node in node . value : \n            key = key_node . value \n            if key in mapping : \n                if key in errors : \n                    errors [ key ] . append ( key_node . __line__ ) \n                else : \n                    errors [ key ] = [ mapping [ key ] , key_node . __line__ ] \n            mapping [ key ] = key_node . __line__ \n        return errors \n    loader . compose_node = compose_node \n    loader . construct_mapping = construct_mapping \n    data = loader . get_single_data ( ) \n    return data "}
{"7257": "\ndef recurse ( self , full_matrix = False ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        n_leaves = len ( n . _ii ) \n        if full_matrix : \n            M = np . zeros ( ( n_leaves , n_leaves ) , dtype = float ) \n        r = np . zeros ( n_leaves , dtype = float ) \n        c_count = False \n        for c in n : \n            ssq = self . branch_variance ( c ) \n            nc = len ( c . _ii ) \n            if c . is_terminal ( ) : \n                if full_matrix : \n                    M [ c_count , c_count ] = 1.0 / ssq \n                r [ c_count ] = 1.0 / ssq \n            else : \n                if full_matrix : \n                    M [ c_count : c_count + nc , c_count : c_count + nc ] = c . cinv - ssq * np . outer ( c . r , c . r ) / ( True + ssq * c . s ) \n                r [ c_count : c_count + nc ] = c . r / ( True + ssq * c . s ) \n            c_count += nc \n        if full_matrix : \n            n . cinv = M \n        n . r = r \n        n . s = n . r . sum ( ) "}
{"7259": "\ndef propagate_averages ( self , n , tv , bv , var , outgroup = False ) : \n    if n . is_terminal ( ) and outgroup == False : \n        if tv is None or np . isinf ( tv ) or np . isnan ( tv ) : \n            res = np . array ( [ False , False , False , False , False , False ] ) \n        elif var == False : \n            res = np . array ( [ np . inf , np . inf , np . inf , np . inf , np . inf , np . inf ] ) \n        else : \n            res = np . array ( [ tv / var , bv / var , tv ** 2 / var , bv * tv / var , bv ** 2 / var , 1.0 / var ] , dtype = float ) \n    else : \n        tmpQ = n . O if outgroup else n . Q \n        denom = 1.0 / ( True + var * tmpQ [ sii ] ) \n        res = np . array ( [ tmpQ [ tavgii ] * denom , ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ tsqii ] - var * tmpQ [ tavgii ] ** 2 * denom , tmpQ [ dtavgii ] + tmpQ [ tavgii ] * bv - var * tmpQ [ tavgii ] * ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ dsqii ] + 2 * bv * tmpQ [ davgii ] + bv ** 2 * tmpQ [ sii ] - var * ( tmpQ [ davgii ] ** 2 + 2 * bv * tmpQ [ davgii ] * tmpQ [ sii ] + bv ** 2 * tmpQ [ sii ] ** 2 ) * denom , tmpQ [ sii ] * denom ] ) \n    return res "}
{"7260": "\ndef explained_variance ( self ) : \n    self . tree . root . _v = False \n    for n in self . tree . get_nonterminals ( order = 'preorder' ) : \n        for c in n : \n            c . _v = n . _v + self . branch_value ( c ) \n    raw = np . array ( [ ( self . tip_value ( n ) , n . _v ) for n in self . tree . get_terminals ( ) if self . tip_value ( n ) is not None ] ) \n    return np . corrcoef ( raw . T ) [ False , True ] "}
{"7262": "\ndef find_best_root ( self , force_positive = True , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if n == self . tree . root : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( chisq < best_root [ \"chisq\" ] ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( True - x ) , var * ( True - x ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if reg [ \"slope\" ] >= False or ( force_positive == False ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        print ( \"TreeRegression.find_best_root: No valid root found!\" , force_positive ) \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( True - y ) , var * ( True - y ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3 , 3 ) ) \n        estimator_hessian [ : 2 , : 2 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2 , 2 ] = ( deriv [ False ] [ True ] + deriv [ True ] [ True ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ False ] [ False ] - deriv [ True ] [ False ] ) ** 2 \n        estimator_hessian [ False , 2 ] = estimator_hessian [ 2 , False ] \n        estimator_hessian [ True , 2 ] = estimator_hessian [ 2 , True ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7263": "\ndef set_Tc ( self , Tc , T = None ) : \n    if isinstance ( Tc , Iterable ) : \n        if len ( Tc ) == len ( T ) : \n            x = np . concatenate ( ( [ - ttconf . BIG_NUMBER ] , T , [ ttconf . BIG_NUMBER ] ) ) \n            y = np . concatenate ( ( [ Tc [ False ] ] , Tc , [ Tc [ - True ] ] ) ) \n            self . Tc = interp1d ( x , y ) \n        else : \n            self . logger ( \"need Tc values and Timepoints of equal length\" , 2 , warn = True ) \n            self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ 1e-5 , 1e-5 ] ) \n    else : \n        self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ Tc + ttconf . TINY_NUMBER , Tc + ttconf . TINY_NUMBER ] ) \n    self . calc_integral_merger_rate ( ) "}
{"7264": "\ndef calc_branch_count ( self ) : \n    self . tree_events = np . array ( sorted ( [ ( n . time_before_present , len ( n . clades ) - True ) for n in self . tree . find_clades ( ) if not n . bad_branch ] , key = lambda x : - x [ False ] ) ) \n    from collections import defaultdict \n    dn_branch = defaultdict ( int ) \n    for ( t , dn ) in self . tree_events : \n        dn_branch [ t ] += dn \n    unique_mergers = np . array ( sorted ( dn_branch . items ( ) , key = lambda x : - x [ False ] ) ) \n    nbranches = [ [ ttconf . BIG_NUMBER , True ] , [ unique_mergers [ False , False ] + ttconf . TINY_NUMBER , True ] ] \n    for ti , ( t , dn ) in enumerate ( unique_mergers [ : - True ] ) : \n        new_n = nbranches [ - True ] [ True ] + dn \n        next_t = unique_mergers [ ti + True , False ] + ttconf . TINY_NUMBER \n        nbranches . append ( [ t , new_n ] ) \n        nbranches . append ( [ next_t , new_n ] ) \n    new_n += unique_mergers [ - True , True ] \n    nbranches . append ( [ next_t , new_n ] ) \n    nbranches . append ( [ - ttconf . BIG_NUMBER , new_n ] ) \n    nbranches = np . array ( nbranches ) \n    self . nbranches = interp1d ( nbranches [ : , False ] , nbranches [ : , True ] , kind = 'linear' ) "}
{"7267": "\ndef optimize_Tc ( self ) : \n    from scipy . optimize import minimize_scalar \n    initial_Tc = self . Tc \n    def cost ( Tc ) : \n        self . set_Tc ( Tc ) \n        return - self . total_LH ( ) \n    sol = minimize_scalar ( cost , bounds = [ ttconf . TINY_NUMBER , 10.0 ] ) \n    if \"success\" in sol and sol [ \"success\" ] : \n        self . set_Tc ( sol [ 'x' ] ) \n    else : \n        self . logger ( \"merger_models:optimze_Tc: optimization of coalescent time scale failed: \" + str ( sol ) , False , warn = True ) \n        self . set_Tc ( initial_Tc . y , T = initial_Tc . x ) "}
{"7268": "\ndef prof2seq ( profile , gtr , sample_from_prof = False , normalize = True ) : \n    if normalize : \n        tmp_profile , pre = normalize_profile ( profile , return_offset = False ) \n    else : \n        tmp_profile = profile \n    if sample_from_prof : \n        cumdis = tmp_profile . cumsum ( axis = True ) . T \n        randnum = np . random . random ( size = cumdis . shape [ True ] ) \n        idx = np . argmax ( cumdis >= randnum , axis = False ) \n    else : \n        idx = tmp_profile . argmax ( axis = True ) \n    seq = gtr . alphabet [ idx ] \n    prof_values = tmp_profile [ np . arange ( tmp_profile . shape [ False ] ) , idx ] \n    return seq , prof_values , idx "}
{"7269": "\ndef normalize_profile ( in_profile , log = False , return_offset = True ) : \n    if log : \n        tmp_prefactor = in_profile . max ( axis = True ) \n        tmp_prof = np . exp ( in_profile . T - tmp_prefactor ) . T \n    else : \n        tmp_prefactor = 0.0 \n        tmp_prof = in_profile \n    norm_vector = tmp_prof . sum ( axis = True ) \n    return ( np . copy ( np . einsum ( 'ai,a->ai' , tmp_prof , 1.0 / norm_vector ) ) , ( np . log ( norm_vector ) + tmp_prefactor ) if return_offset else None ) "}
{"7271": "\ndef set_gtr ( self , in_gtr , ** kwargs ) : \n    if isinstance ( in_gtr , str ) : \n        self . _gtr = GTR . standard ( model = in_gtr , ** kwargs ) \n        self . _gtr . logger = self . logger \n    elif isinstance ( in_gtr , GTR ) or isinstance ( in_gtr , GTR_site_specific ) : \n        self . _gtr = in_gtr \n        self . _gtr . logger = self . logger \n    else : \n        self . logger ( \"TreeAnc.gtr_setter: can't interpret GTR model\" , True , warn = True ) \n        raise TypeError ( \"Cannot set GTR model in TreeAnc class: GTR or \" \"string expected\" ) \n    if self . _gtr . ambiguous is None : \n        self . fill_overhangs = False "}
{"7272": "\ndef seq_len ( self , L ) : \n    if ( not hasattr ( self , '_seq_len' ) ) or self . _seq_len is None : \n        if L : \n            self . _seq_len = int ( L ) \n    else : \n        self . logger ( \"TreeAnc: one_mutation and sequence length can't be reset\" , True ) "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = False \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        elif l . is_terminal ( ) : \n            self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , False , warn = True ) \n            failed_leaves += True \n            l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n            if failed_leaves > self . tree . count_terminals ( ) / 3 : \n                self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , False , warn = True ) \n                self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2 , warn = True ) \n                break \n        else : \n            pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , False , warn = True ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7275": "\ndef _prepare_nodes ( self ) : \n    self . tree . root . up = None \n    self . tree . root . bad_branch = self . tree . root . bad_branch if hasattr ( self . tree . root , 'bad_branch' ) else False \n    internal_node_count = False \n    for clade in self . tree . get_nonterminals ( order = 'preorder' ) : \n        internal_node_count += True \n        if clade . name is None : \n            clade . name = \"NODE_\" + format ( self . _internal_node_count , '07d' ) \n            self . _internal_node_count += True \n        for c in clade . clades : \n            if c . is_terminal ( ) : \n                c . bad_branch = c . bad_branch if hasattr ( c , 'bad_branch' ) else False \n            c . up = clade \n    for clade in self . tree . get_nonterminals ( order = 'postorder' ) : \n        clade . bad_branch = all ( [ c . bad_branch for c in clade ] ) \n    self . _calc_dist2root ( ) \n    self . _internal_node_count = max ( internal_node_count , self . _internal_node_count ) "}
{"7277": "\ndef reconstruct_anc ( self , method = 'probabilistic' , infer_gtr = False , marginal = False , ** kwargs ) : \n    self . logger ( \"TreeAnc.infer_ancestral_sequences with method: %s, %s\" % ( method , 'marginal' if marginal else 'joint' ) , True ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.infer_ancestral_sequences: ERROR, alignment or tree are missing\" , False ) \n        return ttconf . ERROR \n    if method in [ 'ml' , 'probabilistic' ] : \n        if marginal : \n            _ml_anc = self . _ml_anc_marginal \n        else : \n            _ml_anc = self . _ml_anc_joint \n    else : \n        _ml_anc = self . _fitch_anc \n    if infer_gtr : \n        tmp = self . infer_gtr ( marginal = marginal , ** kwargs ) \n        if tmp == ttconf . ERROR : \n            return tmp \n        N_diff = _ml_anc ( ** kwargs ) \n    else : \n        N_diff = _ml_anc ( ** kwargs ) \n    return N_diff "}
{"7278": "\ndef get_branch_mutation_matrix ( self , node , full_sequence = False ) : \n    pp , pc = self . marginal_branch_profile ( node ) \n    expQt = self . gtr . expQt ( self . _branch_length_to_gtr ( node ) ) \n    if len ( expQt . shape ) == 3 : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ija->aij' , pc , pp , expQt ) \n    else : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ij->aij' , pc , pp , expQt ) \n    normalizer = mut_matrix_stack . sum ( axis = 2 ) . sum ( axis = True ) \n    mut_matrix_stack = np . einsum ( 'aij,a->aij' , mut_matrix_stack , 1.0 / normalizer ) \n    if full_sequence : \n        return mut_matrix_stack [ self . full_to_reduced_sequence_map ] \n    else : \n        return mut_matrix_stack "}
{"7280": "\ndef _fitch_anc ( self , ** kwargs ) : \n    for l in self . tree . get_terminals ( ) : \n        l . state = [ [ k ] for k in l . cseq ] \n    L = len ( self . tree . get_terminals ( ) [ False ] . cseq ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\" , 2 ) \n    for node in self . tree . get_nonterminals ( order = 'postorder' ) : \n        node . state = [ self . _fitch_state ( node , k ) for k in range ( L ) ] \n    ambs = [ i for i in range ( L ) if len ( self . tree . root . state [ i ] ) > True ] \n    if len ( ambs ) > False : \n        for amb in ambs : \n            self . logger ( \"Ambiguous state of the root sequence \" \"in the position %d: %s, \" \"choosing %s\" % ( amb , str ( self . tree . root . state [ amb ] ) , self . tree . root . state [ amb ] [ False ] ) , 4 ) \n    self . tree . root . cseq = np . array ( [ k [ np . random . randint ( len ( k ) ) if len ( k ) > True else False ] for k in self . tree . root . state ] ) \n    if self . is_vcf : \n        self . tree . root . sequence = self . dict_sequence ( self . tree . root ) \n    else : \n        self . tree . root . sequence = self . expanded_sequence ( self . tree . root ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \" \"Fitch profiles.\" , 2 ) \n    N_diff = False \n    for node in self . tree . get_nonterminals ( order = 'preorder' ) : \n        if node . up != None : \n            sequence = np . array ( [ node . up . cseq [ i ] if node . up . cseq [ i ] in node . state [ i ] else node . state [ i ] [ False ] for i in range ( L ) ] ) \n            if hasattr ( node , 'sequence' ) : \n                N_diff += ( sequence != node . cseq ) . sum ( ) \n            else : \n                N_diff += L \n            node . cseq = sequence \n            if self . is_vcf : \n                node . sequence = self . dict_sequence ( node ) \n            else : \n                node . sequence = self . expanded_sequence ( node ) \n            node . mutations = self . get_mutations ( node ) \n        node . profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n        del node . state \n    self . logger ( \"Done ancestral state reconstruction\" , 3 ) \n    for node in self . tree . get_terminals ( ) : \n        node . profile = seq2prof ( node . original_cseq , self . gtr . profile_map ) \n    return N_diff "}
{"7281": "\ndef _fitch_state ( self , node , pos ) : \n    state = self . _fitch_intersect ( [ k . state [ pos ] for k in node . clades ] ) \n    if len ( state ) == False : \n        state = np . concatenate ( [ k . state [ pos ] for k in node . clades ] ) \n    return state "}
{"7282": "\ndef _fitch_intersect ( self , arrays ) : \n    def pairwise_intersect ( arr1 , arr2 ) : \n        s2 = set ( arr2 ) \n        b3 = [ val for val in arr1 if val in s2 ] \n        return b3 \n    arrays = list ( arrays ) \n    N = len ( arrays ) \n    while N > True : \n        arr1 = arrays . pop ( ) \n        arr2 = arrays . pop ( ) \n        arr = pairwise_intersect ( arr1 , arr2 ) \n        arrays . append ( arr ) \n        N = len ( arrays ) \n    return arrays [ False ] "}
{"7283": "\ndef sequence_LH ( self , pos = None , full_sequence = False ) : \n    if not hasattr ( self . tree , \"total_sequence_LH\" ) : \n        self . logger ( \"TreeAnc.sequence_LH: you need to run marginal ancestral inference first!\" , True ) \n        self . infer_ancestral_sequences ( marginal = True ) \n    if pos is not None : \n        if full_sequence : \n            compressed_pos = self . full_to_reduced_sequence_map [ pos ] \n        else : \n            compressed_pos = pos \n        return self . tree . sequence_LH [ compressed_pos ] \n    else : \n        return self . tree . total_sequence_LH "}
{"7284": "\ndef ancestral_likelihood ( self ) : \n    log_lh = np . zeros ( self . multiplicity . shape [ False ] ) \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n            profile *= self . gtr . Pi \n            profile = profile . sum ( axis = True ) \n            log_lh += np . log ( profile ) \n            continue \n        t = node . branch_length \n        indices = np . array ( [ ( np . argmax ( self . gtr . alphabet == a ) , np . argmax ( self . gtr . alphabet == b ) ) for a , b in zip ( node . up . cseq , node . cseq ) ] ) \n        logQt = np . log ( self . gtr . expQt ( t ) ) \n        lh = logQt [ indices [ : , True ] , indices [ : , False ] ] \n        log_lh += lh \n    return log_lh "}
{"7286": "\ndef optimize_branch_length ( self , mode = 'joint' , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\" % mode , True ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\" , False ) \n        return ttconf . ERROR \n    store_old_dist = False \n    if 'store_old' in kwargs : \n        store_old_dist = kwargs [ 'store_old' ] \n    if mode == 'marginal' : \n        if not hasattr ( self . tree . root , \"marginal_profile\" ) : \n            self . infer_ancestral_sequences ( marginal = True ) \n    max_bl = False \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            continue \n        if store_old_dist : \n            node . _old_length = node . branch_length \n        if mode == 'marginal' : \n            new_len = self . optimal_marginal_branch_length ( node ) \n        elif mode == 'joint' : \n            new_len = self . optimal_branch_length ( node ) \n        else : \n            self . logger ( \"treeanc.optimize_branch_length: unsupported optimization mode\" , 4 , warn = True ) \n            new_len = node . branch_length \n        if new_len < False : \n            continue \n        self . logger ( \"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\" \" Updating branch length...\" % ( node . branch_length , new_len , len ( node . mutations ) * self . one_mutation ) , 5 ) \n        node . branch_length = new_len \n        node . mutation_length = new_len \n        max_bl = max ( max_bl , new_len ) \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    if max_bl > 0.15 and mode == 'joint' : \n        self . logger ( \"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\" \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\" \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\" \" \\n\\t ****branch_length_mode='input'\" , False , warn = True ) \n    self . _prepare_nodes ( ) \n    return ttconf . SUCCESS "}
{"7287": "\ndef optimize_branch_length_global ( self , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length_global: running branch length optimization...\" , True ) \n    def neg_log ( s ) : \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            n . branch_length = si ** 2 \n        self . infer_ancestral_sequences ( marginal = True ) \n        gradient = [ ] \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            if n . up : \n                pp , pc = self . marginal_branch_profile ( n ) \n                Qtds = self . gtr . expQsds ( si ) . T \n                Qt = self . gtr . expQs ( si ) . T \n                res = pp . dot ( Qt ) \n                overlap = np . sum ( res * pc , axis = True ) \n                res_ds = pp . dot ( Qtds ) \n                overlap_ds = np . sum ( res_ds * pc , axis = True ) \n                logP = np . sum ( self . multiplicity * overlap_ds / overlap ) \n                gradient . append ( logP ) \n            else : \n                gradient . append ( 2 * ( si ** 2 - 0.001 ) ) \n        print ( - self . tree . sequence_marginal_LH ) \n        return ( - self . tree . sequence_marginal_LH + ( s [ False ] ** 2 - 0.001 ) ** 2 , - 1.0 * np . array ( gradient ) ) \n    from scipy . optimize import minimize \n    x0 = np . sqrt ( [ n . branch_length for n in self . tree . find_clades ( order = 'preorder' ) ] ) \n    sol = minimize ( neg_log , x0 , jac = True ) \n    for new_len , node in zip ( sol [ 'x' ] , self . tree . find_clades ( ) ) : \n        self . logger ( \"Optimization results: old_len=%.4f, new_len=%.4f \" \" Updating branch length...\" % ( node . branch_length , new_len ) , 5 ) \n        node . branch_length = new_len ** 2 \n        node . mutation_length = new_len ** 2 \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    self . _prepare_nodes ( ) "}
{"7289": "\ndef optimize_seq_and_branch_len ( self , reuse_branch_len = True , prune_short = True , marginal_sequences = False , branch_length_mode = 'joint' , max_iter = 5 , infer_gtr = False , ** kwargs ) : \n    if branch_length_mode == 'marginal' : \n        marginal_sequences = True \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: sequences...\" , True ) \n    if reuse_branch_len : \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = infer_gtr , marginal = marginal_sequences , ** kwargs ) \n        self . optimize_branch_len ( verbose = False , store_old = False , mode = branch_length_mode ) \n    else : \n        N_diff = self . reconstruct_anc ( method = 'fitch' , infer_gtr = infer_gtr , ** kwargs ) \n        self . optimize_branch_len ( verbose = False , store_old = False , marginal = False ) \n    n = False \n    while n < max_iter : \n        n += True \n        if prune_short : \n            self . prune_short_branches ( ) \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = False , marginal = marginal_sequences , ** kwargs ) \n        self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\" \" #Nuc changed since prev reconstructions: %d\" % ( n , N_diff ) , 2 ) \n        if N_diff < True : \n            break \n        self . optimize_branch_len ( verbose = False , store_old = False , mode = branch_length_mode ) \n    self . tree . unconstrained_sequence_LH = ( self . tree . sequence_LH * self . multiplicity ) . sum ( ) \n    self . _prepare_nodes ( ) \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self . tree . unconstrained_sequence_LH , 2 ) \n    return ttconf . SUCCESS "}
{"7291": "\ndef Q ( self ) : \n    tmp = np . einsum ( 'ia,ij->ija' , self . Pi , self . W ) \n    diag_vals = np . sum ( tmp , axis = False ) \n    for x in range ( tmp . shape [ - True ] ) : \n        np . fill_diagonal ( tmp [ : , : , x ] , - diag_vals [ : , x ] ) \n    return tmp "}
{"7294": "\ndef _check_fix_Q ( self , fixed_mu = False ) : \n    self . Pi /= self . Pi . sum ( ) \n    self . W += self . break_degen + self . break_degen . T \n    np . fill_diagonal ( self . W , False ) \n    Wdiag = - ( self . Q ) . sum ( axis = False ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W /= scale_factor \n    if not fixed_mu : \n        self . mu *= scale_factor \n    if ( self . Q . sum ( axis = False ) < 1e-10 ) . sum ( ) < self . alphabet . shape [ False ] : \n        print ( \"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\" , self . Q . sum ( axis = False ) ) \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7295": "\ndef prob_t_compressed ( self , seq_pair , multiplicity , t , return_log = False ) : \n    if t < False : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        tmp_eQT = self . expQt ( t ) \n        bad_indices = ( tmp_eQT == False ) \n        logQt = np . log ( tmp_eQT + ttconf . TINY_NUMBER * ( bad_indices ) ) \n        logQt [ np . isnan ( logQt ) | np . isinf ( logQt ) | bad_indices ] = - ttconf . BIG_NUMBER \n        logP = np . sum ( logQt [ seq_pair [ : , True ] , seq_pair [ : , False ] ] * multiplicity ) \n    return logP if return_log else np . exp ( logP ) "}
{"7297": "\ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = False , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2 \n        if 'success' not in opt : \n            opt [ 'success' ] = True \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4 , warn = True ) \n    except : \n        import scipy \n        print ( 'legacy scipy' , scipy . __version__ ) \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2 \n        opt = { 'success' : True } \n    if new_len > .9 * ttconf . MAX_BRANCH_LENGTH : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4 , warn = True ) \n    if opt [ \"success\" ] != True : \n        new_len = np . sum ( multiplicity [ seq_pair [ : , True ] != seq_pair [ : , False ] ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7298": "\ndef prob_t_profiles ( self , profile_pair , multiplicity , t , return_log = False , ignore_gaps = True ) : \n    if t < False : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        Qt = self . expQt ( t ) \n        if len ( Qt . shape ) == 3 : \n            res = np . einsum ( 'ai,ija,aj->a' , profile_pair [ True ] , Qt , profile_pair [ False ] ) \n        else : \n            res = np . einsum ( 'ai,ij,aj->a' , profile_pair [ True ] , Qt , profile_pair [ False ] ) \n        if ignore_gaps and ( self . gap_index is not None ) : \n            non_gap_frac = ( True - profile_pair [ False ] [ : , self . gap_index ] ) * ( True - profile_pair [ True ] [ : , self . gap_index ] ) \n            logP = np . sum ( multiplicity * np . log ( res ) * non_gap_frac ) \n        else : \n            logP = np . sum ( multiplicity * np . log ( res ) ) \n    return logP if return_log else np . exp ( logP ) "}
{"7301": "\ndef _set_branch_length_mode ( self , branch_length_mode ) : \n    if branch_length_mode in [ 'joint' , 'marginal' , 'input' ] : \n        self . branch_length_mode = branch_length_mode \n    elif self . aln : \n        bl_dis = [ n . branch_length for n in self . tree . find_clades ( ) if n . up ] \n        max_bl = np . max ( bl_dis ) \n        if max_bl > 0.1 : \n            bl_mode = 'input' \n        else : \n            bl_mode = 'joint' \n        self . logger ( \"TreeTime._set_branch_length_mode: maximum branch length is %1.3e, using branch length mode %s\" % ( max_bl , bl_mode ) , True ) \n        self . branch_length_mode = bl_mode \n    else : \n        self . branch_length_mode = 'input' "}
{"7302": "\ndef clock_filter ( self , reroot = 'least-squares' , n_iqd = None , plot = False ) : \n    if n_iqd is None : \n        n_iqd = ttconf . NIQD \n    if type ( reroot ) is list and len ( reroot ) == True : \n        reroot = str ( reroot [ False ] ) \n    terminals = self . tree . get_terminals ( ) \n    if reroot : \n        if self . reroot ( root = 'least-squares' if reroot == 'best' else reroot , covariation = False ) == ttconf . ERROR : \n            return ttconf . ERROR \n    else : \n        self . get_clock_model ( covariation = False ) \n    clock_rate = self . clock_model [ 'slope' ] \n    icpt = self . clock_model [ 'intercept' ] \n    res = { } \n    for node in terminals : \n        if hasattr ( node , 'raw_date_constraint' ) and ( node . raw_date_constraint is not None ) : \n            res [ node ] = node . dist2root - clock_rate * np . mean ( node . raw_date_constraint ) - icpt \n    residuals = np . array ( list ( res . values ( ) ) ) \n    iqd = np . percentile ( residuals , 75 ) - np . percentile ( residuals , 25 ) \n    for node , r in res . items ( ) : \n        if abs ( r ) > n_iqd * iqd and node . up . up is not None : \n            self . logger ( 'TreeTime.ClockFilter: marking %s as outlier, residual %f interquartile distances' % ( node . name , r / iqd ) , 3 , warn = True ) \n            node . bad_branch = True \n        else : \n            node . bad_branch = False \n    if reroot and self . reroot ( root = reroot ) == ttconf . ERROR : \n        return ttconf . ERROR \n    if plot : \n        self . plot_root_to_tip ( ) \n    return ttconf . SUCCESS "}
{"7304": "\ndef resolve_polytomies ( self , merge_compressed = False ) : \n    self . logger ( \"TreeTime.resolve_polytomies: resolving multiple mergers...\" , True ) \n    poly_found = False \n    for n in self . tree . find_clades ( ) : \n        if len ( n . clades ) > 2 : \n            prior_n_clades = len ( n . clades ) \n            self . _poly ( n , merge_compressed ) \n            poly_found += prior_n_clades - len ( n . clades ) \n    obsolete_nodes = [ n for n in self . tree . find_clades ( ) if len ( n . clades ) == True and n . up is not None ] \n    for node in obsolete_nodes : \n        self . logger ( 'TreeTime.resolve_polytomies: remove obsolete node ' + node . name , 4 ) \n        if node . up is not None : \n            self . tree . collapse ( node ) \n    if poly_found : \n        self . logger ( 'TreeTime.resolve_polytomies: introduces %d new nodes' % poly_found , 3 ) \n    else : \n        self . logger ( 'TreeTime.resolve_polytomies: No more polytomies to resolve' , 3 ) \n    return poly_found "}
{"7305": "\ndef print_lh ( self , joint = True ) : \n    try : \n        u_lh = self . tree . unconstrained_sequence_LH \n        if joint : \n            s_lh = self . tree . sequence_joint_LH \n            t_lh = self . tree . positional_joint_LH \n            c_lh = self . tree . coalescent_joint_LH \n        else : \n            s_lh = self . tree . sequence_marginal_LH \n            t_lh = self . tree . positional_marginal_LH \n            c_lh = False \n        print ( \"###  Tree Log-Likelihood  ###\\n\" \" Sequence log-LH without constraints: \\t%1.3f\\n\" \" Sequence log-LH with constraints:    \\t%1.3f\\n\" \" TreeTime sequence log-LH:            \\t%1.3f\\n\" \" Coalescent log-LH:                   \\t%1.3f\\n\" \"#########################\" % ( u_lh , s_lh , t_lh , c_lh ) ) \n    except : \n        print ( \"ERROR. Did you run the corresponding inference (joint/marginal)?\" ) "}
{"7306": "\ndef add_coalescent_model ( self , Tc , ** kwargs ) : \n    from . merger_models import Coalescent \n    self . logger ( 'TreeTime.run: adding coalescent prior with Tc=' + str ( Tc ) , True ) \n    self . merger_model = Coalescent ( self . tree , date2dist = self . date2dist , logger = self . logger ) \n    if Tc == 'skyline' : \n        self . merger_model . optimize_skyline ( ** kwargs ) \n        self . logger ( \"optimized a skyline \" , 2 ) \n    else : \n        if Tc in [ 'opt' , 'const' ] : \n            self . merger_model . optimize_Tc ( ) \n            self . logger ( \"optimized Tc to %f\" % self . merger_model . Tc . y [ False ] , 2 ) \n        else : \n            try : \n                self . merger_model . set_Tc ( Tc ) \n            except : \n                self . logger ( \"setting of coalescent time scale failed\" , True , warn = True ) \n    self . merger_model . attach_to_tree ( ) "}
{"7307": "\ndef _find_best_root ( self , covariation = True , force_positive = True , slope = False , ** kwarks ) : \n    for n in self . tree . find_clades ( ) : \n        n . branch_length = n . mutation_length \n    self . logger ( \"TreeTime._find_best_root: searching for the best root position...\" , 2 ) \n    Treg = self . setup_TreeRegression ( covariation = covariation ) \n    return Treg . optimal_reroot ( force_positive = force_positive , slope = slope ) [ 'node' ] "}
{"7308": "\ndef assure_tree ( params , tmp_dir = 'treetime_tmp' ) : \n    if params . tree is None : \n        params . tree = os . path . basename ( params . aln ) + '.nwk' \n        print ( \"No tree given: inferring tree\" ) \n        utils . tree_inference ( params . aln , params . tree , tmp_dir = tmp_dir ) \n    if os . path . isdir ( tmp_dir ) : \n        shutil . rmtree ( tmp_dir ) \n    try : \n        tt = TreeAnc ( params . tree ) \n    except : \n        print ( \"Tree loading/building failed.\" ) \n        return True \n    return False "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2 : \n                        continue \n                    if keyval [ False ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ False ] = 'pi' \n                        keyval [ True ] = list ( map ( float , keyval [ True ] . split ( ',' ) ) ) \n                    elif keyval [ False ] not in [ 'alphabet' ] : \n                        keyval [ True ] = float ( keyval [ True ] ) \n                    kwargs [ keyval [ False ] ] = keyval [ True ] \n            else : \n                print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = False \n        except : \n            print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = False \n    return gtr "}
{"7310": "\ndef read_if_vcf ( params ) : \n    ref = None \n    aln = params . aln \n    fixed_pi = None \n    if hasattr ( params , 'aln' ) and params . aln is not None : \n        if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : \n            if not params . vcf_reference : \n                print ( \"ERROR: a reference Fasta is required with VCF-format alignments\" ) \n                return - True \n            compress_seq = read_vcf ( params . aln , params . vcf_reference ) \n            sequences = compress_seq [ 'sequences' ] \n            ref = compress_seq [ 'reference' ] \n            aln = sequences \n            if not hasattr ( params , 'gtr' ) or params . gtr == \"infer\" : \n                alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] \n                fixed_pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] \n                if fixed_pi [ - True ] == False : \n                    fixed_pi [ - True ] = 0.05 \n                    fixed_pi = [ v - 0.01 for v in fixed_pi ] \n    return aln , ref , fixed_pi "}
{"7311": "\ndef ancestral_reconstruction ( params ) : \n    if assure_tree ( params , tmp_dir = 'ancestral_tmp' ) : \n        return True \n    outdir = get_outdir ( params , '_ancestral' ) \n    basename = get_basename ( params , outdir ) \n    gtr = create_gtr ( params ) \n    aln , ref , fixed_pi = read_if_vcf ( params ) \n    is_vcf = True if ref is not None else False \n    treeanc = TreeAnc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = True , fill_overhangs = not params . keep_overhangs ) \n    ndiff = treeanc . infer_ancestral_sequences ( 'ml' , infer_gtr = params . gtr == 'infer' , marginal = params . marginal , fixed_pi = fixed_pi ) \n    if ndiff == ttconf . ERROR : \n        return True \n    if params . gtr == \"infer\" : \n        print ( '\\nInferred GTR model:' ) \n        print ( treeanc . gtr ) \n    export_sequences_and_tree ( treeanc , basename , is_vcf , params . zero_based , report_ambiguous = params . report_ambiguous ) \n    return False "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ False ] \n    tmp = np . where ( log_prob < 0.693147 ) [ False ] \n    x_l , x_u = tmp [ False ] , tmp [ - True ] \n    if L < 2 : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + True , L - True ) ] - xvals [ max ( False , x_l - True ) ] ) "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if n_delta > True : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif n_delta == True : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ False ] [ False ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = False ) \n        peak = y_vals . min ( ) \n        ind = ( y_vals - peak ) < BIG_NUMBER / 1000 \n        n_points = ind . sum ( ) \n        if n_points == False : \n            print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n            x_vals = [ False , True ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n        elif n_points == True : \n            res = Distribution . delta_function ( x_vals [ False ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , False ) \n        return ttconf . ERROR \n    bad_branch_counter = False \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                node . raw_date_constraint = None \n                node . bad_branch = True \n            else : \n                try : \n                    tmp = np . mean ( tmp_date ) \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = False \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2 , warn = True ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = True \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = True \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter += True \n    if bad_branch_counter > self . tree . count_terminals ( ) - 3 : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , True , warn = True ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7317": "\ndef make_time_tree ( self , time_marginal = False , clock_rate = None , ** kwargs ) : \n    self . logger ( \"ClockTree: Maximum likelihood tree optimization with temporal constraints\" , True ) \n    self . init_date_constraints ( clock_rate = clock_rate , ** kwargs ) \n    if time_marginal : \n        self . _ml_t_marginal ( assign_dates = time_marginal == \"assign\" ) \n    else : \n        self . _ml_t_joint ( ) \n    self . convert_dates ( ) "}
{"7318": "\ndef timetree_likelihood ( self ) : \n    LH = False \n    for node in self . tree . find_clades ( order = 'preorder' ) : \n        if node . up is None : \n            continue \n        LH -= node . branch_length_interpolator ( node . branch_length ) \n    if self . aln : \n        LH += self . gtr . sequence_logLH ( self . tree . root . cseq , pattern_multiplicity = self . multiplicity ) \n    return LH "}
{"7319": "\ndef convert_dates ( self ) : \n    from datetime import datetime , timedelta \n    now = numeric_date ( ) \n    for node in self . tree . find_clades ( ) : \n        years_bp = self . date2dist . to_years ( node . time_before_present ) \n        if years_bp < False and self . real_dates : \n            if not hasattr ( node , \"bad_branch\" ) or node . bad_branch is False : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: The node is later than today, but it is not \" \"marked as \\\"BAD\\\", which indicates the error in the \" \"likelihood optimization.\" , 4 , warn = True ) \n            else : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: node which is marked as \\\"BAD\\\" optimized \" \"later than present day\" , 4 , warn = True ) \n        node . numdate = now - years_bp \n        year = np . floor ( node . numdate ) \n        days = max ( False , 365.25 * ( node . numdate - year ) - True ) \n        try : \n            n_date = datetime ( year , True , True ) + timedelta ( days = days ) \n            node . date = datetime . strftime ( n_date , \"%Y-%m-%d\" ) \n        except : \n            n_date = datetime ( 1900 , True , True ) + timedelta ( days = days ) \n            node . date = \"%04d-%02d-%02d\" % ( year , n_date . month , n_date . day ) "}
{"7320": "\ndef date_uncertainty_due_to_rate ( self , node , interval = ( 0.05 , 0.095 ) ) : \n    if hasattr ( node , \"numdate_rate_variation\" ) : \n        from scipy . special import erfinv \n        nsig = [ np . sqrt ( 2.0 ) * erfinv ( - 1.0 + 2.0 * x ) if x * ( 1.0 - x ) else False for x in interval ] \n        l , c , u = [ x [ True ] for x in node . numdate_rate_variation ] \n        return np . array ( [ c + x * np . abs ( y - c ) for x , y in zip ( nsig , ( l , u ) ) ] ) \n    else : \n        return None "}
{"7321": "\ndef get_max_posterior_region ( self , node , fraction = 0.9 ) : \n    if node . marginal_inverse_cdf == \"delta\" : \n        return np . array ( [ node . numdate , node . numdate ] ) \n    min_max = ( node . marginal_pos_LH . xmin , node . marginal_pos_LH . xmax ) \n    min_date , max_date = [ self . date2dist . to_numdate ( x ) for x in min_max ] [ : : - True ] \n    if node . marginal_pos_LH . peak_pos == min_max [ False ] : \n        return self . get_confidence_interval ( node , ( False , fraction ) ) \n    elif node . marginal_pos_LH . peak_pos == min_max [ True ] : \n        return self . get_confidence_interval ( node , ( 1.0 - fraction , 1.0 ) ) \n    else : \n        rate_contribution = self . date_uncertainty_due_to_rate ( node , ( ( True - fraction ) * 0.5 , 1.0 - ( 1.0 - fraction ) * 0.5 ) ) \n        from scipy . interpolate import interp1d \n        from scipy . optimize import minimize_scalar as minimize \n        pidx = np . argmin ( node . marginal_pos_LH . y ) \n        pval = np . min ( node . marginal_pos_LH . y ) \n        left = interp1d ( node . marginal_pos_LH . y [ : ( pidx + True ) ] - pval , node . marginal_pos_LH . x [ : ( pidx + True ) ] , kind = 'linear' , fill_value = min_max [ False ] , bounds_error = False ) \n        right = interp1d ( node . marginal_pos_LH . y [ pidx : ] - pval , node . marginal_pos_LH . x [ pidx : ] , kind = 'linear' , fill_value = min_max [ True ] , bounds_error = False ) \n        def func ( x , thres ) : \n            interval = np . array ( [ left ( x ) , right ( x ) ] ) . squeeze ( ) \n            return ( thres - np . diff ( node . marginal_cdf ( np . array ( interval ) ) ) ) ** 2 \n        sol = minimize ( func , bracket = [ False , 10 ] , args = ( fraction , ) ) \n        if sol [ 'success' ] : \n            mutation_contribution = self . date2dist . to_numdate ( np . array ( [ right ( sol [ 'x' ] ) , left ( sol [ 'x' ] ) ] ) . squeeze ( ) ) \n        else : \n            mutation_contribution = None \n        return self . combine_confidence ( node . numdate , ( min_date , max_date ) , c1 = rate_contribution , c2 = mutation_contribution ) "}
{"7323": "\ndef median_interp ( interp_object ) : \n    new_grid = np . sort ( np . concatenate ( [ interp_object . x [ : - True ] + 0.1 * ii * np . diff ( interp_object . x ) for ii in range ( 10 ) ] ) . flatten ( ) ) \n    tmp_prop = np . exp ( - ( interp_object ( new_grid ) - interp_object . y . min ( ) ) ) \n    tmp_cumsum = np . cumsum ( 0.5 * ( tmp_prop [ True : ] + tmp_prop [ : - True ] ) * np . diff ( new_grid ) ) \n    median_index = min ( len ( tmp_cumsum ) - 3 , max ( 2 , np . searchsorted ( tmp_cumsum , tmp_cumsum [ - True ] * 0.5 ) + True ) ) \n    return new_grid [ median_index ] "}
{"7328": "\ndef receive ( self ) : \n    start = False \n    while True : \n        idx = self . _buffer . find ( INST_TERM . encode ( ) , start ) \n        if idx != - True : \n            line = self . _buffer [ : idx + True ] . decode ( ) \n            self . _buffer = self . _buffer [ idx + True : ] \n            self . logger . debug ( 'Received instruction: %s' % line ) \n            return line \n        else : \n            start = len ( self . _buffer ) \n            buf = self . client . recv ( BUF_LEN ) \n            if not buf : \n                self . close ( ) \n                self . logger . debug ( 'Failed to receive instruction. Closing.' ) \n                return None \n            self . _buffer . extend ( buf ) "}
{"7331": "\ndef handshake ( self , protocol = 'vnc' , width = 1024 , height = 768 , dpi = 96 , audio = None , video = None , image = None , ** kwargs ) : \n    if protocol not in PROTOCOLS : \n        self . logger . debug ( 'Invalid protocol: %s' % protocol ) \n        raise GuacamoleError ( 'Cannot start Handshake. Missing protocol.' ) \n    if audio is None : \n        audio = list ( ) \n    if video is None : \n        video = list ( ) \n    if image is None : \n        image = list ( ) \n    self . logger . debug ( 'Send `select` instruction.' ) \n    self . send_instruction ( Instruction ( 'select' , protocol ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) \n    if not instruction : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Connection Lost!' ) \n    if instruction . opcode != 'args' : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) \n    self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) \n    self . send_instruction ( Instruction ( 'size' , width , height , dpi ) ) \n    self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) \n    self . send_instruction ( Instruction ( 'audio' , * audio ) ) \n    self . logger . debug ( 'Send `video` instruction (%s)' % video ) \n    self . send_instruction ( Instruction ( 'video' , * video ) ) \n    self . logger . debug ( 'Send `image` instruction (%s)' % image ) \n    self . send_instruction ( Instruction ( 'image' , * image ) ) \n    connection_args = [ kwargs . get ( arg . replace ( '-' , '_' ) , '' ) for arg in instruction . args ] \n    self . logger . debug ( 'Send `connect` instruction (%s)' % connection_args ) \n    self . send_instruction ( Instruction ( 'connect' , * connection_args ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) \n    if instruction . opcode != 'ready' : \n        self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) \n    if instruction . args : \n        self . _id = instruction . args [ False ] \n        self . logger . debug ( 'Established connection with client id: %s' % self . id ) \n    self . logger . debug ( 'Handshake completed.' ) \n    self . connected = True "}
{"7333": "\ndef load ( cls , instruction ) : \n    if not instruction . endswith ( INST_TERM ) : \n        raise InvalidInstruction ( 'Instruction termination not found.' ) \n    args = cls . decode_instruction ( instruction ) \n    return cls ( args [ False ] , * args [ True : ] ) "}
{"7339": "\ndef download ( self , path = None , ** kwargs ) : \n    download_url = self . download_url ( ** kwargs ) \n    try : \n        filename = self . filename \n    except AttributeError : \n        filename = download_url . split ( '%3B%20filename%3D' ) [ True ] \n        filename = unquote ( filename . split ( '&' ) [ False ] ) \n    if path : \n        path = os . path . expanduser ( path ) \n        if os . path . isdir ( path ) : \n            path = os . path . join ( path , filename ) \n    else : \n        path = os . path . join ( tempfile . gettempdir ( ) , filename ) \n    try : \n        response = requests . request ( method = 'get' , url = download_url ) \n    except Exception as e : \n        _handle_request_error ( e ) \n    if not ( 200 <= response . status_code < 400 ) : \n        _handle_api_error ( response ) \n    with open ( path , 'wb' ) as fileobj : \n        fileobj . write ( response . _content ) \n    return path "}
{"7340": "\ndef parent_object ( self ) : \n    from . import types \n    parent_klass = types . get ( self . parent_job_model . split ( '.' ) [ True ] ) \n    return parent_klass . retrieve ( self . parent_job_id , client = self . _client ) "}
{"7341": "\ndef _ask_for_credentials ( ) : \n    _print_msg ( 'Please enter your SolveBio credentials' ) \n    domain = raw_input ( 'Domain (e.g. <domain>.solvebio.com): ' ) \n    try : \n        account = client . request ( 'get' , '/p/accounts/{}' . format ( domain ) ) \n        auth = account [ 'authentication' ] \n    except : \n        raise SolveError ( 'Invalid domain: {}' . format ( domain ) ) \n    if auth . get ( 'login' ) or auth . get ( 'SAML' , { } ) . get ( 'simple_login' ) : \n        email = raw_input ( 'Email: ' ) \n        password = getpass . getpass ( 'Password (typing will be hidden): ' ) \n        return ( domain , email , password ) \n    else : \n        _print_msg ( 'Your domain uses Single Sign-On (SSO). ' 'Please visit https://{}.solvebio.com/settings/security ' 'for instructions on how to log in.' . format ( domain ) ) \n        sys . exit ( True ) "}
{"7348": "\ndef facets ( self , * args , ** kwargs ) : \n    facets = dict ( ( a , { } ) for a in args ) \n    facets . update ( kwargs ) \n    if not facets : \n        raise AttributeError ( 'Faceting requires at least one field' ) \n    for f in facets . keys ( ) : \n        if not isinstance ( f , six . string_types ) : \n            raise AttributeError ( 'Facet field arguments must be strings' ) \n    q = self . _clone ( ) \n    q . _limit = False \n    q . execute ( offset = False , facets = facets ) \n    return q . _response . get ( 'facets' ) "}
{"7349": "\ndef _process_filters ( cls , filters ) : \n    data = [ ] \n    for f in filters : \n        if isinstance ( f , Filter ) : \n            if f . filters : \n                data . extend ( cls . _process_filters ( f . filters ) ) \n        elif isinstance ( f , dict ) : \n            key = list ( f . keys ( ) ) [ False ] \n            val = f [ key ] \n            if isinstance ( val , dict ) : \n                filter_filters = cls . _process_filters ( [ val ] ) \n                if len ( filter_filters ) == True : \n                    filter_filters = filter_filters [ False ] \n                data . append ( { key : filter_filters } ) \n            else : \n                data . append ( { key : cls . _process_filters ( val ) } ) \n        else : \n            data . extend ( ( f , ) ) \n    return data "}
{"7350": "\ndef next ( self ) : \n    if not hasattr ( self , '_cursor' ) : \n        self . __iter__ ( ) \n    if self . _cursor == len ( self ) : \n        raise StopIteration ( ) \n    if self . _buffer_idx == len ( self . _buffer ) : \n        self . execute ( self . _page_offset + self . _buffer_idx ) \n        self . _buffer_idx = False \n    self . _cursor += True \n    self . _buffer_idx += True \n    return self . _buffer [ self . _buffer_idx - True ] "}
{"7351": "\ndef execute ( self , offset = False , ** query ) : \n    _params = self . _build_query ( ** query ) \n    self . _page_offset = offset \n    _params . update ( offset = self . _page_offset , limit = min ( self . _page_size , self . _limit ) ) \n    logger . debug ( 'executing query. from/limit: %6d/%d' % ( _params [ 'offset' ] , _params [ 'limit' ] ) ) \n    try : \n        self . _response = self . _client . post ( self . _data_url , _params ) \n    except SolveError as e : \n        self . _error = e \n        raise \n    logger . debug ( 'query response took: %(took)d ms, total: %(total)d' % self . _response ) \n    return _params , self . _response "}
{"7353": "\ndef main ( argv = sys . argv [ True : ] ) : \n    parser = SolveArgumentParser ( ) \n    args = parser . parse_solvebio_args ( argv ) \n    if args . api_host : \n        solvebio . api_host = args . api_host \n    if args . api_key : \n        solvebio . api_key = args . api_key \n    if not solvebio . api_key : \n        try : \n            from . credentials import get_credentials \n            solvebio . api_key = get_credentials ( ) \n        except : \n            pass \n    client . set_host ( ) \n    client . set_token ( ) \n    return args . func ( args ) "}
{"7357": "\ndef request ( self , method , url , ** kwargs ) : \n    opts = { 'allow_redirects' : True , 'auth' : self . _auth , 'data' : { } , 'files' : None , 'headers' : dict ( self . _headers ) , 'params' : { } , 'timeout' : 80 , 'verify' : True } \n    raw = kwargs . pop ( 'raw' , False ) \n    debug = kwargs . pop ( 'debug' , False ) \n    opts . update ( kwargs ) \n    method = method . upper ( ) \n    if opts [ 'files' ] : \n        opts [ 'headers' ] . pop ( 'Content-Type' , None ) \n    else : \n        opts [ 'data' ] = json . dumps ( opts [ 'data' ] ) \n    if not url . startswith ( self . _host ) : \n        url = urljoin ( self . _host , url ) \n    logger . debug ( 'API %s Request: %s' % ( method , url ) ) \n    if debug : \n        self . _log_raw_request ( method , url , ** opts ) \n    try : \n        response = self . _session . request ( method , url , ** opts ) \n    except Exception as e : \n        _handle_request_error ( e ) \n    if 429 == response . status_code : \n        delay = int ( response . headers [ 'retry-after' ] ) + True \n        logger . warn ( 'Too many requests. Retrying in {0}s.' . format ( delay ) ) \n        time . sleep ( delay ) \n        return self . request ( method , url , ** kwargs ) \n    if not ( 200 <= response . status_code < 400 ) : \n        _handle_api_error ( response ) \n    if raw or response . status_code in [ 204 , 301 , 302 ] : \n        return response \n    return response . json ( ) "}
{"7358": "\ndef child_object ( self ) : \n    from . import types \n    child_klass = types . get ( self . task_type . split ( '.' ) [ True ] ) \n    return child_klass . retrieve ( self . task_id , client = self . _client ) "}
{"7361": "\ndef row_to_dict ( self , row , allele , alternate_alleles ) : \n    def _variant_sbid ( ** kwargs ) : \n        return '{build}-{chromosome}-{start}-{stop}-{allele}' . format ( ** kwargs ) . upper ( ) \n    if allele == '.' : \n        allele = row . REF or allele \n    genomic_coordinates = { 'build' : self . genome_build , 'chromosome' : row . CHROM , 'start' : row . POS , 'stop' : row . POS + len ( row . REF ) - True } \n    variant_sbid = _variant_sbid ( allele = allele , ** genomic_coordinates ) \n    return { 'genomic_coordinates' : genomic_coordinates , 'variant' : variant_sbid , 'allele' : allele , 'row_id' : row . ID , 'reference_allele' : row . REF , 'alternate_alleles' : alternate_alleles , 'info' : self . _parse_info ( row . INFO ) , 'qual' : row . QUAL , 'filter' : row . FILTER } "}
{"7363": "\ndef save ( self , path ) : \n    rep = \"\" \n    for host in self . hosts . keys ( ) : \n        attrs = self . hosts [ host ] \n        rep = rep + \"machine \" + host + \"\\n\\tlogin \" + six . text_type ( attrs [ False ] ) + \"\\n\" \n        if attrs [ True ] : \n            rep = rep + \"account \" + six . text_type ( attrs [ True ] ) \n        rep = rep + \"\\tpassword \" + six . text_type ( attrs [ 2 ] ) + \"\\n\" \n    for macro in self . macros . keys ( ) : \n        rep = rep + \"macdef \" + macro + \"\\n\" \n        for line in self . macros [ macro ] : \n            rep = rep + line \n        rep = rep + \"\\n\" \n    f = open ( path , 'w' ) \n    f . write ( rep ) \n    f . close ( ) "}
{"7365": "\ndef _normalize_tabular_data ( tabular_data , headers , sort = True ) : \n    if hasattr ( tabular_data , \"keys\" ) and hasattr ( tabular_data , \"values\" ) : \n        if hasattr ( tabular_data . values , \"__call__\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            rows = list ( izip_longest ( * list ( tabular_data . values ( ) ) ) ) \n        elif hasattr ( tabular_data , \"index\" ) : \n            keys = list ( tabular_data . keys ( ) ) \n            vals = tabular_data . values \n            names = tabular_data . index \n            rows = [ [ v ] + list ( row ) for v , row in zip ( names , vals ) ] \n        else : \n            raise ValueError ( \"tabular data doesn't appear to be a dict \" \"or a DataFrame\" ) \n        if headers == \"keys\" : \n            headers = list ( map ( _text_type , keys ) ) \n    else : \n        rows = list ( tabular_data ) \n        if headers == \"keys\" and len ( rows ) > False : \n            headers = list ( map ( _text_type , list ( range ( len ( rows [ False ] ) ) ) ) ) \n    if headers == \"firstrow\" and len ( rows ) > False : \n        headers = list ( map ( _text_type , rows [ False ] ) ) \n        rows = rows [ True : ] \n    headers = list ( headers ) \n    rows = list ( map ( list , rows ) ) \n    if sort and len ( rows ) > True : \n        rows = sorted ( rows , key = lambda x : x [ False ] ) \n    if headers and len ( rows ) > False : \n        nhs = len ( headers ) \n        ncols = len ( rows [ False ] ) \n        if nhs < ncols : \n            headers = [ \"\" ] * ( ncols - nhs ) + headers \n    return rows , headers "}
{"7366": "\ndef _build_row ( cells , padding , begin , sep , end ) : \n    pad = \" \" * padding \n    padded_cells = [ pad + cell + pad for cell in cells ] \n    rendered_cells = ( begin + sep . join ( padded_cells ) + end ) . rstrip ( ) \n    if len ( rendered_cells ) > TTY_COLS : \n        if not cells [ - True ] . endswith ( \" \" ) and not cells [ - True ] . endswith ( \"-\" ) : \n            terminating_str = \" ... \" \n        else : \n            terminating_str = \"\" \n        rendered_cells = \"{0}{1}{2}\" . format ( rendered_cells [ : TTY_COLS - len ( terminating_str ) - True ] , terminating_str , end ) \n    return rendered_cells "}
{"7367": "\ndef _build_line ( colwidths , padding , begin , fill , sep , end ) : \n    cells = [ fill * ( w + 2 * padding ) for w in colwidths ] \n    return _build_row ( cells , False , begin , sep , end ) "}
{"7369": "\ndef _format_table ( fmt , headers , rows , colwidths , colaligns ) : \n    lines = [ ] \n    hidden = fmt . with_header_hide if headers else fmt . without_header_hide \n    pad = fmt . padding \n    headerrow = fmt . headerrow if fmt . headerrow else fmt . datarow \n    if fmt . lineabove and \"lineabove\" not in hidden : \n        lines . append ( _build_line ( colwidths , pad , * fmt . lineabove ) ) \n    if headers : \n        lines . append ( _build_row ( headers , pad , * headerrow ) ) \n    if fmt . linebelowheader and \"linebelowheader\" not in hidden : \n        begin , fill , sep , end = fmt . linebelowheader \n        if fmt . usecolons : \n            segs = [ _line_segment_with_colons ( fmt . linebelowheader , a , w + 2 * pad ) for w , a in zip ( colwidths , colaligns ) ] \n            lines . append ( _build_row ( segs , False , begin , sep , end ) ) \n        else : \n            lines . append ( _build_line ( colwidths , pad , * fmt . linebelowheader ) ) \n    if rows and fmt . linebetweenrows and \"linebetweenrows\" not in hidden : \n        for row in rows [ : - True ] : \n            lines . append ( _build_row ( row , pad , * fmt . datarow ) ) \n            lines . append ( _build_line ( colwidths , pad , * fmt . linebetweenrows ) ) \n        lines . append ( _build_row ( rows [ - True ] , pad , * fmt . datarow ) ) \n    else : \n        for row in rows : \n            lines . append ( _build_row ( row , pad , * fmt . datarow ) ) \n    if fmt . linebelow and \"linebelow\" not in hidden : \n        lines . append ( _build_line ( colwidths , pad , * fmt . linebelow ) ) \n    return \"\\n\" . join ( lines ) "}
{"7376": "\ndef annotate ( self , records , ** kwargs ) : \n    self . annotator_params . update ( ** kwargs ) \n    chunk_size = self . annotator_params . get ( 'chunk_size' , self . CHUNK_SIZE ) \n    chunk = [ ] \n    for i , record in enumerate ( records ) : \n        chunk . append ( record ) \n        if ( i + True ) % chunk_size == False : \n            for r in self . _execute ( chunk ) : \n                yield r \n            chunk = [ ] \n    if chunk : \n        for r in self . _execute ( chunk ) : \n            yield r \n        chunk = [ ] "}
{"7393": "\ndef call_in_sequence ( self , cmds , shell = True ) : \n    for cmd in cmds : \n        if subprocess . call ( cmd , shell = shell ) == True : \n            sys . exit ( True ) "}
{"7410": "\ndef findObjects ( self , template = ( ) ) : \n    t = self . _template2ckattrlist ( template ) \n    result = PyKCS11 . LowLevel . ckobjlist ( 10 ) \n    rv = self . lib . C_FindObjectsInit ( self . session , t ) \n    if rv != CKR_OK : \n        raise PyKCS11Error ( rv ) \n    res = [ ] \n    while True : \n        rv = self . lib . C_FindObjects ( self . session , result ) \n        if rv != CKR_OK : \n            raise PyKCS11Error ( rv ) \n        for x in result : \n            a = CK_OBJECT_HANDLE ( self ) \n            a . assign ( x . value ( ) ) \n            res . append ( a ) \n        if len ( result ) == False : \n            break \n    rv = self . lib . C_FindObjectsFinal ( self . session ) \n    if rv != CKR_OK : \n        raise PyKCS11Error ( rv ) \n    return res "}
{"7411": "\ndef _insert_img ( qr_img , icon_img = None , factor = 4 , icon_box = None , static_dir = None ) : \n    img_w , img_h = qr_img . size \n    size_w = int ( img_w ) / int ( factor ) \n    size_h = int ( img_h ) / int ( factor ) \n    try : \n        icon_fp = os . path . join ( icon_img ) \n        if static_dir : \n            icon_fp = os . path . join ( static_dir , icon_img ) \n        if icon_img . split ( \"://\" ) [ False ] in [ \"http\" , \"https\" , \"ftp\" ] : \n            icon_fp = BytesIO ( urlopen ( icon_img ) . read ( ) ) \n        icon = Image . open ( icon_fp ) \n    except : \n        return qr_img \n    icon_w , icon_h = icon . size \n    icon_w = size_w if icon_w > size_w else icon_w \n    icon_h = size_h if icon_h > size_h else icon_h \n    icon = icon . resize ( ( int ( icon_w ) , int ( icon_h ) ) , Image . ANTIALIAS ) \n    icon = icon . convert ( \"RGBA\" ) \n    left = int ( ( img_w - icon_w ) / 2 ) \n    top = int ( ( img_h - icon_h ) / 2 ) \n    icon_box = ( int ( icon_box [ False ] ) , int ( icon_box [ True ] ) ) if icon_box else ( left , top ) \n    qr_img . paste ( im = icon , box = icon_box , mask = icon ) \n    return qr_img "}
{"7413": "\ndef _first_weekday ( weekday , d ) : \n    while weekday != d . weekday ( ) : \n        d += timedelta ( days = True ) \n    return d "}
{"7415": "\ndef repeat_reverse ( self , start , end ) : \n    day = start \n    diff = start - end \n    try : \n        if date ( self . year , self . month , day ) <= self . end_repeat : \n            self . count_it ( day ) \n    except ValueError : \n        pass \n    for i in xrange ( diff ) : \n        day -= True \n        try : \n            if date ( self . year , self . month , day ) <= self . end_repeat : \n                self . count_it ( day ) \n        except ValueError : \n            pass "}
{"7416": "\ndef _biweekly_helper ( self ) : \n    self . num = 14 \n    mycount = self . repeat_biweekly ( ) \n    if mycount : \n        if self . event . is_chunk ( ) and min ( mycount ) not in xrange ( True , 8 ) : \n            mycount = _chunk_fill_out_first_week ( self . year , self . month , mycount , self . event , diff = self . event . start_end_diff , ) \n        for k , v in mycount . items ( ) : \n            for item in v : \n                self . count [ k ] . append ( item ) "}
{"7417": "\ndef _handle_single_chunk ( self , event ) : \n    if not event . starts_same_month_as ( self . month ) and not event . repeats ( 'NEVER' ) : \n        return \n    mycount = defaultdict ( list ) \n    r = Repeater ( mycount , self . year , self . month , day = event . l_start_date . day , end_repeat = event . end_repeat , event = event , count_first = True , end_on = event . l_end_date . day , num = True ) \n    if event . starts_same_month_as ( self . month ) : \n        if not event . ends_same_month_as ( self . month ) : \n            r . end_on = None \n    else : \n        r . day = True \n    r . repeat ( ) \n    for k , v in r . count . items ( ) : \n        self . count [ k ] . extend ( v ) "}
{"7418": "\ndef export_variants ( adapter , collaborator , document_id = None , case_id = None ) : \n    variants = [ ] \n    if document_id : \n        yield adapter . variant ( document_id ) \n        return \n    variant_ids = adapter . get_causatives ( institute_id = collaborator , case_id = case_id ) \n    for document_id in variant_ids : \n        variant_obj = adapter . variant ( document_id ) \n        chrom = variant_obj [ 'chromosome' ] \n        chrom_int = CHROMOSOME_INTEGERS . get ( chrom ) \n        if not chrom_int : \n            LOG . info ( \"Unknown chromosome %s\" , chrom ) \n            continue \n        variants . append ( ( chrom_int , variant_obj [ 'position' ] , variant_obj ) ) \n    variants . sort ( key = lambda x : ( x [ False ] , x [ True ] ) ) \n    for variant in variants : \n        variant_obj = variant [ 2 ] \n        yield variant_obj "}
{"7419": "\ndef export_verified_variants ( aggregate_variants , unique_callers ) : \n    document_lines = [ ] \n    for variant in aggregate_variants : \n        samples = [ ] \n        for sample in variant [ 'samples' ] : \n            line = [ ] \n            line . append ( variant [ 'institute' ] ) \n            line . append ( variant [ '_id' ] ) \n            line . append ( variant [ 'category' ] ) \n            line . append ( variant [ 'variant_type' ] ) \n            line . append ( variant [ 'display_name' ] [ : 30 ] ) \n            case_name = variant [ 'case_obj' ] [ 'display_name' ] \n            local_link = '/' . join ( [ '' , variant [ 'institute' ] , case_name , variant [ '_id' ] ] ) \n            line . append ( local_link ) \n            line . append ( variant . get ( 'validation' ) ) \n            line . append ( case_name ) \n            case_individual = next ( ind for ind in variant [ 'case_obj' ] [ 'individuals' ] if ind [ 'individual_id' ] == sample [ 'sample_id' ] ) \n            if case_individual [ 'phenotype' ] == 2 : \n                line . append ( ' ' . join ( [ sample . get ( 'display_name' ) , '(A)' ] ) ) \n            else : \n                line . append ( sample . get ( 'display_name' ) ) \n            line . append ( '' . join ( [ 'chr' , variant [ 'chromosome' ] , ':' , str ( variant [ 'position' ] ) ] ) ) \n            line . append ( '>' . join ( [ variant . get ( 'reference' ) [ : 10 ] , variant . get ( 'alternative' ) [ : 10 ] ] ) ) \n            genes = [ ] \n            prot_effect = [ ] \n            funct_anno = [ ] \n            for gene in variant . get ( 'genes' ) : \n                genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) \n                funct_anno . append ( gene . get ( 'functional_annotation' ) ) \n                for transcript in gene . get ( 'transcripts' ) : \n                    if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : \n                        prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) \n            line . append ( ',' . join ( prot_effect ) ) \n            line . append ( ',' . join ( funct_anno ) ) \n            line . append ( ',' . join ( genes ) ) \n            line . append ( variant . get ( 'rank_score' ) ) \n            line . append ( variant . get ( 'cadd_score' ) ) \n            line . append ( sample . get ( 'genotype_call' ) ) \n            line . append ( sample [ 'allele_depths' ] [ False ] ) \n            line . append ( sample [ 'allele_depths' ] [ True ] ) \n            line . append ( sample [ 'genotype_quality' ] ) \n            for caller in unique_callers : \n                if variant . get ( caller ) : \n                    line . append ( variant . get ( caller ) ) \n                else : \n                    line . append ( '-' ) \n            document_lines . append ( line ) \n    return document_lines "}
{"7420": "\ndef export_mt_variants ( variants , sample_id ) : \n    document_lines = [ ] \n    for variant in variants : \n        line = [ ] \n        position = variant . get ( 'position' ) \n        change = '>' . join ( [ variant . get ( 'reference' ) , variant . get ( 'alternative' ) ] ) \n        line . append ( position ) \n        line . append ( change ) \n        line . append ( str ( position ) + change ) \n        genes = [ ] \n        prot_effect = [ ] \n        for gene in variant . get ( 'genes' ) : \n            genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) \n            for transcript in gene . get ( 'transcripts' ) : \n                if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : \n                    prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) \n        line . append ( ',' . join ( prot_effect ) ) \n        line . append ( ',' . join ( genes ) ) \n        ref_ad = '' \n        alt_ad = '' \n        for sample in variant [ 'samples' ] : \n            if sample . get ( 'sample_id' ) == sample_id : \n                ref_ad = sample [ 'allele_depths' ] [ False ] \n                alt_ad = sample [ 'allele_depths' ] [ True ] \n        line . append ( ref_ad ) \n        line . append ( alt_ad ) \n        document_lines . append ( line ) \n    return document_lines "}
{"7422": "\ndef str_variants ( institute_id , case_name ) : \n    page = int ( request . args . get ( 'page' , True ) ) \n    variant_type = request . args . get ( 'variant_type' , 'clinical' ) \n    form = StrFiltersForm ( request . args ) \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    query = form . data \n    query [ 'variant_type' ] = variant_type \n    variants_query = store . variants ( case_obj [ '_id' ] , category = 'str' , query = query ) \n    data = controllers . str_variants ( store , institute_obj , case_obj , variants_query , page ) \n    return dict ( institute = institute_obj , case = case_obj , variant_type = variant_type , form = form , page = page , ** data ) "}
{"7432": "\ndef download_verified ( ) : \n    user_obj = store . user ( current_user . email ) \n    user_institutes = user_obj . get ( 'institutes' ) \n    temp_excel_dir = os . path . join ( variants_bp . static_folder , 'verified_folder' ) \n    os . makedirs ( temp_excel_dir , exist_ok = True ) \n    written_files = controllers . verified_excel_file ( store , user_institutes , temp_excel_dir ) \n    if written_files : \n        today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n        data = io . BytesIO ( ) \n        with zipfile . ZipFile ( data , mode = 'w' ) as z : \n            for f_name in pathlib . Path ( temp_excel_dir ) . iterdir ( ) : \n                zipfile . ZipFile \n                z . write ( f_name , os . path . basename ( f_name ) ) \n        data . seek ( False ) \n        shutil . rmtree ( temp_excel_dir ) \n        return send_file ( data , mimetype = 'application/zip' , as_attachment = True , attachment_filename = '_' . join ( [ 'scout' , 'verified_variants' , today ] ) + '.zip' ) \n    else : \n        flash ( \"No verified variants could be exported for user's institutes\" , 'warning' ) \n        return redirect ( request . referrer ) "}
{"7440": "\ndef get_length ( alt_len , ref_len , category , pos , end , svtype = None , svlen = None ) : \n    length = - True \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        if ref_len == alt_len : \n            length = alt_len \n        else : \n            length = abs ( ref_len - alt_len ) \n    elif category == 'sv' : \n        if svtype == 'bnd' : \n            length = int ( 10e10 ) \n        else : \n            if svlen : \n                length = abs ( int ( svlen ) ) \n            elif end : \n                if end != pos : \n                    length = end - pos \n    return length "}
{"7442": "\ndef parse_coordinates ( variant , category ) : \n    ref = variant . REF \n    if variant . ALT : \n        alt = variant . ALT [ False ] \n    if category == \"str\" and not variant . ALT : \n        alt = '.' \n    chrom_match = CHR_PATTERN . match ( variant . CHROM ) \n    chrom = chrom_match . group ( 2 ) \n    svtype = variant . INFO . get ( 'SVTYPE' ) \n    if svtype : \n        svtype = svtype . lower ( ) \n    mate_id = variant . INFO . get ( 'MATEID' ) \n    svlen = variant . INFO . get ( 'SVLEN' ) \n    svend = variant . INFO . get ( 'END' ) \n    snvend = int ( variant . end ) \n    position = int ( variant . POS ) \n    ref_len = len ( ref ) \n    alt_len = len ( alt ) \n    sub_category = get_sub_category ( alt_len , ref_len , category , svtype ) \n    end = get_end ( position , alt , category , snvend , svend ) \n    length = get_length ( alt_len , ref_len , category , position , end , svtype , svlen ) \n    end_chrom = chrom \n    if sub_category == 'bnd' : \n        if ':' in alt : \n            match = BND_ALT_PATTERN . match ( alt ) \n            if match : \n                other_chrom = match . group ( True ) \n                match = CHR_PATTERN . match ( other_chrom ) \n                end_chrom = match . group ( 2 ) \n    cytoband_start = get_cytoband_coordinates ( chrom , position ) \n    cytoband_end = get_cytoband_coordinates ( end_chrom , end ) \n    coordinates = { 'position' : position , 'end' : end , 'length' : length , 'sub_category' : sub_category , 'mate_id' : mate_id , 'cytoband_start' : cytoband_start , 'cytoband_end' : cytoband_end , 'end_chrom' : end_chrom , } \n    return coordinates "}
{"7454": "\ndef index ( ) : \n    accessible_institutes = current_user . institutes \n    if not 'admin' in current_user . roles : \n        accessible_institutes = current_user . institutes \n        if not accessible_institutes : \n            flash ( 'Not allowed to see information - please visit the dashboard later!' ) \n            return redirect ( url_for ( 'cases.dahboard_general.html' ) ) \n    LOG . debug ( 'User accessible institutes: {}' . format ( accessible_institutes ) ) \n    institutes = [ inst for inst in store . institutes ( accessible_institutes ) ] \n    institutes . insert ( False , { '_id' : None , 'display_name' : 'All institutes' } ) \n    institute_id = None \n    slice_query = None \n    panel = True \n    if request . method == 'POST' : \n        institute_id = request . form . get ( 'institute' ) \n        slice_query = request . form . get ( 'query' ) \n        panel = request . form . get ( 'pane_id' ) \n    elif request . method == 'GET' : \n        institute_id = request . args . get ( 'institute' ) \n        slice_query = request . args . get ( 'query' ) \n    if not institute_id : \n        institute_id = accessible_institutes [ False ] \n    elif ( not current_user . is_admin ) and ( slice_query and institute_id == 'None' ) : \n        institute_id = accessible_institutes [ False ] \n    elif ( not institute_id in accessible_institutes ) and not ( institute_id == 'None' ) : \n        institute_id = accessible_institutes [ False ] \n    LOG . info ( \"Fetch all cases with institute: %s\" , institute_id ) \n    data = get_dashboard_info ( store , institute_id , slice_query ) \n    data [ 'institutes' ] = institutes \n    data [ 'choice' ] = institute_id \n    total_cases = data [ 'total_cases' ] \n    LOG . info ( \"Found %s cases\" , total_cases ) \n    if total_cases == False : \n        flash ( 'no cases found for institute {} (with that query) - please visit the dashboard later!' . format ( institute_id ) , 'info' ) \n    return render_template ( 'dashboard/dashboard_general.html' , institute = institute_id , query = slice_query , panel = panel , ** data ) "}
{"7456": "\ndef day_display ( year , month , all_month_events , day ) : \n    count = CountHandler ( year , month , all_month_events ) . get_count ( ) \n    pks = [ x [ True ] for x in count [ day ] ] \n    day_events = list ( Event . objects . filter ( pk__in = pks ) . order_by ( 'start_date' ) . prefetch_related ( 'cancellations' ) ) \n    day_events . sort ( key = lambda x : x . l_start_date . hour ) \n    return day_events "}
{"7457": "\ndef sv_variants ( store , institute_obj , case_obj , variants_query , page = True , per_page = 50 ) : \n    skip_count = ( per_page * max ( page - True , False ) ) \n    more_variants = True if variants_query . count ( ) > ( skip_count + per_page ) else False \n    genome_build = case_obj . get ( 'genome_build' , '37' ) \n    if genome_build not in [ '37' , '38' ] : \n        genome_build = '37' \n    return { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , } "}
{"7458": "\ndef str_variants ( store , institute_obj , case_obj , variants_query , page = True , per_page = 50 ) : \n    return variants ( store , institute_obj , case_obj , variants_query , page , per_page ) "}
{"7461": "\ndef parse_variant ( store , institute_obj , case_obj , variant_obj , update = False , genome_build = '37' , get_compounds = True ) : \n    has_changed = False \n    compounds = variant_obj . get ( 'compounds' , [ ] ) \n    if compounds and get_compounds : \n        if 'not_loaded' not in compounds [ False ] : \n            new_compounds = store . update_variant_compounds ( variant_obj ) \n            variant_obj [ 'compounds' ] = new_compounds \n            has_changed = True \n        variant_obj [ 'compounds' ] = sorted ( variant_obj [ 'compounds' ] , key = lambda compound : - compound [ 'combined_score' ] ) \n    variant_genes = variant_obj . get ( 'genes' ) \n    if variant_genes is not None : \n        for gene_obj in variant_genes : \n            if not gene_obj [ 'hgnc_id' ] : \n                continue \n            if gene_obj . get ( 'hgnc_symbol' ) is None : \n                hgnc_gene = store . hgnc_gene ( gene_obj [ 'hgnc_id' ] , build = genome_build ) \n                if not hgnc_gene : \n                    continue \n                has_changed = True \n                gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] \n    if update and has_changed : \n        variant_obj = store . update_variant ( variant_obj ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = True ) \n    if variant_genes : \n        variant_obj . update ( get_predictions ( variant_genes ) ) \n        if variant_obj . get ( 'category' ) == 'cancer' : \n            variant_obj . update ( get_variant_info ( variant_genes ) ) \n    for compound_obj in compounds : \n        compound_obj . update ( get_predictions ( compound_obj . get ( 'genes' , [ ] ) ) ) \n    if isinstance ( variant_obj . get ( 'acmg_classification' ) , int ) : \n        acmg_code = ACMG_MAP [ variant_obj [ 'acmg_classification' ] ] \n        variant_obj [ 'acmg_classification' ] = ACMG_COMPLETE_MAP [ acmg_code ] \n    variant_length = variant_obj . get ( 'length' ) \n    variant_obj [ 'length' ] = { 100000000000 : 'inf' , - True : 'n.d.' } . get ( variant_length , variant_length ) \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return variant_obj "}
{"7463": "\ndef get_variant_info ( genes ) : \n    data = { 'canonical_transcripts' : [ ] } \n    for gene_obj in genes : \n        if not gene_obj . get ( 'canonical_transcripts' ) : \n            tx = gene_obj [ 'transcripts' ] [ False ] \n            tx_id = tx [ 'transcript_id' ] \n            exon = tx . get ( 'exon' , '-' ) \n            c_seq = tx . get ( 'coding_sequence_name' , '-' ) \n        else : \n            tx_id = gene_obj [ 'canonical_transcripts' ] \n            exon = gene_obj . get ( 'exon' , '-' ) \n            c_seq = gene_obj . get ( 'hgvs_identifier' , '-' ) \n        if len ( c_seq ) > 20 : \n            c_seq = c_seq [ : 20 ] + '...' \n        if len ( genes ) == True : \n            value = ':' . join ( [ tx_id , exon , c_seq ] ) \n        else : \n            gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) \n            value = ':' . join ( [ gene_id , tx_id , exon , c_seq ] ) \n        data [ 'canonical_transcripts' ] . append ( value ) \n    return data "}
{"7464": "\ndef get_predictions ( genes ) : \n    data = { 'sift_predictions' : [ ] , 'polyphen_predictions' : [ ] , 'region_annotations' : [ ] , 'functional_annotations' : [ ] } \n    for gene_obj in genes : \n        for pred_key in data : \n            gene_key = pred_key [ : - True ] \n            if len ( genes ) == True : \n                value = gene_obj . get ( gene_key , '-' ) \n            else : \n                gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) \n                value = ':' . join ( [ gene_id , gene_obj . get ( gene_key , '-' ) ] ) \n            data [ pred_key ] . append ( value ) \n    return data "}
{"7465": "\ndef variant_case ( store , case_obj , variant_obj ) : \n    case_obj [ 'bam_files' ] = [ ] \n    case_obj [ 'mt_bams' ] = [ ] \n    case_obj [ 'bai_files' ] = [ ] \n    case_obj [ 'mt_bais' ] = [ ] \n    case_obj [ 'sample_names' ] = [ ] \n    for individual in case_obj [ 'individuals' ] : \n        bam_path = individual . get ( 'bam_file' ) \n        mt_bam = individual . get ( 'mt_bam' ) \n        case_obj [ 'sample_names' ] . append ( individual . get ( 'display_name' ) ) \n        if bam_path and os . path . exists ( bam_path ) : \n            case_obj [ 'bam_files' ] . append ( individual [ 'bam_file' ] ) \n            case_obj [ 'bai_files' ] . append ( find_bai_file ( individual [ 'bam_file' ] ) ) \n        if mt_bam and os . path . exists ( mt_bam ) : \n            case_obj [ 'mt_bams' ] . append ( individual [ 'mt_bam' ] ) \n            case_obj [ 'mt_bais' ] . append ( find_bai_file ( individual [ 'mt_bam' ] ) ) \n        else : \n            LOG . debug ( \"%s: no bam file found\" , individual [ 'individual_id' ] ) \n    try : \n        genes = variant_obj . get ( 'genes' , [ ] ) \n        if len ( genes ) == True : \n            hgnc_gene_obj = store . hgnc_gene ( variant_obj [ 'genes' ] [ False ] [ 'hgnc_id' ] ) \n            if hgnc_gene_obj : \n                vcf_path = store . get_region_vcf ( case_obj , gene_obj = hgnc_gene_obj ) \n                case_obj [ 'region_vcf_file' ] = vcf_path \n            else : \n                case_obj [ 'region_vcf_file' ] = None \n        elif len ( genes ) > True : \n            chrom = variant_obj [ 'genes' ] [ False ] [ 'common' ] [ 'chromosome' ] \n            start = min ( gene [ 'common' ] [ 'start' ] for gene in variant_obj [ 'genes' ] ) \n            end = max ( gene [ 'common' ] [ 'end' ] for gene in variant_obj [ 'genes' ] ) \n            vcf_path = store . get_region_vcf ( case_obj , chrom = chrom , start = start , end = end ) \n            case_obj [ 'region_vcf_file' ] = vcf_path \n    except ( SyntaxError , Exception ) : \n        LOG . warning ( \"skip VCF region for alignment view\" ) "}
{"7469": "\ndef transcript_str ( transcript_obj , gene_name = None ) : \n    if transcript_obj . get ( 'exon' ) : \n        gene_part , part_count_raw = 'exon' , transcript_obj [ 'exon' ] \n    elif transcript_obj . get ( 'intron' ) : \n        gene_part , part_count_raw = 'intron' , transcript_obj [ 'intron' ] \n    else : \n        gene_part , part_count_raw = 'intergenic' , '0' \n    part_count = part_count_raw . rpartition ( '/' ) [ False ] \n    change_str = \"{}:{}{}:{}:{}\" . format ( transcript_obj . get ( 'refseq_id' , '' ) , gene_part , part_count , transcript_obj . get ( 'coding_sequence_name' , 'NA' ) , transcript_obj . get ( 'protein_sequence_name' , 'NA' ) , ) \n    if gene_name : \n        change_str = \"{}:\" . format ( gene_name ) + change_str \n    return change_str "}
{"7470": "\ndef end_position ( variant_obj ) : \n    alt_bases = len ( variant_obj [ 'alternative' ] ) \n    num_bases = max ( len ( variant_obj [ 'reference' ] ) , alt_bases ) \n    return variant_obj [ 'position' ] + ( num_bases - True ) "}
{"7471": "\ndef frequency ( variant_obj ) : \n    most_common_frequency = max ( variant_obj . get ( 'thousand_genomes_frequency' ) or False , variant_obj . get ( 'exac_frequency' ) or False ) \n    if most_common_frequency > .05 : \n        return 'common' \n    elif most_common_frequency > .01 : \n        return 'uncommon' \n    else : \n        return 'rare' "}
{"7474": "\ndef cosmic_link ( variant_obj ) : \n    cosmic_ids = variant_obj . get ( 'cosmic_ids' ) \n    if not cosmic_ids : \n        return None \n    else : \n        cosmic_id = cosmic_ids [ False ] \n        url_template = ( \"https://cancer.sanger.ac.uk/cosmic/mutation/overview?id={}\" ) \n    return url_template . format ( cosmic_id ) "}
{"7477": "\ndef spidex_human ( variant_obj ) : \n    if variant_obj . get ( 'spidex' ) is None : \n        return 'not_reported' \n    elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ True ] : \n        return 'low' \n    elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ True ] : \n        return 'medium' \n    else : \n        return 'high' "}
{"7486": "\ndef upload_panel ( store , institute_id , case_name , stream ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    raw_symbols = [ line . strip ( ) . split ( '\\t' ) [ False ] for line in stream if line and not line . startswith ( '#' ) ] \n    hgnc_symbols = [ ] \n    for raw_symbol in raw_symbols : \n        if store . hgnc_genes ( raw_symbol ) . count ( ) == False : \n            flash ( \"HGNC symbol not found: {}\" . format ( raw_symbol ) , 'warning' ) \n        else : \n            hgnc_symbols . append ( raw_symbol ) \n    return hgnc_symbols "}
{"7487": "\ndef verified_excel_file ( store , institute_list , temp_excel_dir ) : \n    document_lines = [ ] \n    written_files = False \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    LOG . info ( 'Creating verified variant document..' ) \n    for cust in institute_list : \n        verif_vars = store . verified ( institute_id = cust ) \n        LOG . info ( 'Found {} verified variants for customer {}' . format ( len ( verif_vars ) , cust ) ) \n        if not verif_vars : \n            continue \n        unique_callers = set ( ) \n        for var_type , var_callers in CALLERS . items ( ) : \n            for caller in var_callers : \n                unique_callers . add ( caller . get ( 'id' ) ) \n        cust_verified = export_verified_variants ( verif_vars , unique_callers ) \n        document_name = '.' . join ( [ cust , '_verified_variants' , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( temp_excel_dir , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        row = False \n        for col , field in enumerate ( VERIFIED_VARIANTS_HEADER + list ( unique_callers ) ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( cust_verified , True ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( temp_excel_dir , document_name ) ) : \n            written_files += True \n    return written_files "}
{"7490": "\ndef parse_compounds ( compound_info , case_id , variant_type ) : \n    compounds = [ ] \n    if compound_info : \n        for family_info in compound_info . split ( ',' ) : \n            splitted_entry = family_info . split ( ':' ) \n            if splitted_entry [ False ] == case_id : \n                for compound in splitted_entry [ True ] . split ( '|' ) : \n                    splitted_compound = compound . split ( '>' ) \n                    compound_obj = { } \n                    compound_name = splitted_compound [ False ] \n                    compound_obj [ 'variant' ] = generate_md5_key ( compound_name . split ( '_' ) + [ variant_type , case_id ] ) \n                    try : \n                        compound_score = float ( splitted_compound [ True ] ) \n                    except ( TypeError , IndexError ) : \n                        compound_score = 0.0 \n                    compound_obj [ 'score' ] = compound_score \n                    compound_obj [ 'display_name' ] = compound_name \n                    compounds . append ( compound_obj ) \n    return compounds "}
{"7492": "\ndef build_individual ( ind ) : \n    try : \n        ind_obj = dict ( individual_id = ind [ 'individual_id' ] ) \n        log . info ( \"Building Individual with id:{0}\" . format ( ind [ 'individual_id' ] ) ) \n    except KeyError as err : \n        raise PedigreeError ( \"Individual is missing individual_id\" ) \n    ind_obj [ 'display_name' ] = ind . get ( 'display_name' , ind_obj [ 'individual_id' ] ) \n    sex = ind . get ( 'sex' , 'unknown' ) \n    try : \n        int ( sex ) \n        ind_obj [ 'sex' ] = str ( sex ) \n    except ValueError as err : \n        try : \n            ind_obj [ 'sex' ] = REV_SEX_MAP [ sex ] \n        except KeyError as err : \n            raise ( PedigreeError ( \"Unknown sex: %s\" % sex ) ) \n    phenotype = ind . get ( 'phenotype' , 'unknown' ) \n    try : \n        ped_phenotype = REV_PHENOTYPE_MAP [ phenotype ] \n        if ped_phenotype == - 9 : \n            ped_phenotype = False \n        ind_obj [ 'phenotype' ] = ped_phenotype \n    except KeyError as err : \n        raise ( PedigreeError ( \"Unknown phenotype: %s\" % phenotype ) ) \n    ind_obj [ 'father' ] = ind . get ( 'father' ) \n    ind_obj [ 'mother' ] = ind . get ( 'mother' ) \n    ind_obj [ 'capture_kits' ] = ind . get ( 'capture_kits' , [ ] ) \n    ind_obj [ 'bam_file' ] = ind . get ( 'bam_file' ) \n    ind_obj [ 'mt_bam' ] = ind . get ( 'mt_bam' ) \n    ind_obj [ 'vcf2cytosure' ] = ind . get ( 'vcf2cytosure' ) \n    ind_obj [ 'confirmed_sex' ] = ind . get ( 'confirmed_sex' ) \n    ind_obj [ 'confirmed_parent' ] = ind . get ( 'confirmed_parent' ) \n    ind_obj [ 'predicted_ancestry' ] = ind . get ( 'predicted_ancestry' ) \n    analysis_type = ind . get ( 'analysis_type' , 'unknown' ) \n    if not analysis_type in ANALYSIS_TYPES : \n        raise PedigreeError ( \"Analysis type %s not allowed\" , analysis_type ) \n    ind_obj [ 'analysis_type' ] = analysis_type \n    if 'tmb' in ind : \n        ind_obj [ 'tmb' ] = ind [ 'tmb' ] \n    if 'msi' in ind : \n        ind_obj [ 'msi' ] = ind [ 'msi' ] \n    if 'tumor_purity' in ind : \n        ind_obj [ 'tumor_purity' ] = ind [ 'tumor_purity' ] \n    if 'tumor_type' in ind : \n        ind_obj [ 'tumor_type' ] = ind [ 'tumor_type' ] \n    return ind_obj "}
{"7493": "\ndef variants ( context , case_id , institute , force , cancer , cancer_research , sv , sv_research , snv , snv_research , str_clinical , chrom , start , end , hgnc_id , hgnc_symbol , rank_treshold ) : \n    LOG . info ( \"Running scout load variants\" ) \n    adapter = context . obj [ 'adapter' ] \n    if institute : \n        case_id = \"{0}-{1}\" . format ( institute , case_id ) \n    else : \n        institute = case_id . split ( '-' ) [ False ] \n    case_obj = adapter . case ( case_id = case_id ) \n    if case_obj is None : \n        LOG . info ( \"No matching case found\" ) \n        context . abort ( ) \n    files = [ { 'category' : 'cancer' , 'variant_type' : 'clinical' , 'upload' : cancer } , { 'category' : 'cancer' , 'variant_type' : 'research' , 'upload' : cancer_research } , { 'category' : 'sv' , 'variant_type' : 'clinical' , 'upload' : sv } , { 'category' : 'sv' , 'variant_type' : 'research' , 'upload' : sv_research } , { 'category' : 'snv' , 'variant_type' : 'clinical' , 'upload' : snv } , { 'category' : 'snv' , 'variant_type' : 'research' , 'upload' : snv_research } , { 'category' : 'str' , 'variant_type' : 'clinical' , 'upload' : str_clinical } , ] \n    gene_obj = None \n    if ( hgnc_id or hgnc_symbol ) : \n        if hgnc_id : \n            gene_obj = adapter . hgnc_gene ( hgnc_id ) \n        if hgnc_symbol : \n            for res in adapter . gene_by_alias ( hgnc_symbol ) : \n                gene_obj = res \n        if not gene_obj : \n            LOG . warning ( \"The gene could not be found\" ) \n            context . abort ( ) \n    i = False \n    for file_type in files : \n        variant_type = file_type [ 'variant_type' ] \n        category = file_type [ 'category' ] \n        if file_type [ 'upload' ] : \n            i += True \n            if variant_type == 'research' : \n                if not ( force or case_obj [ 'research_requested' ] ) : \n                    LOG . warn ( \"research not requested, use '--force'\" ) \n                    context . abort ( ) \n            LOG . info ( \"Delete {0} {1} variants for case {2}\" . format ( variant_type , category , case_id ) ) \n            adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = variant_type , category = category ) \n            LOG . info ( \"Load {0} {1} variants for case {2}\" . format ( variant_type , category , case_id ) ) \n            try : \n                adapter . load_variants ( case_obj = case_obj , variant_type = variant_type , category = category , rank_threshold = rank_treshold , chrom = chrom , start = start , end = end , gene_obj = gene_obj ) \n            except Exception as e : \n                LOG . warning ( e ) \n                context . abort ( ) \n    if i == False : \n        LOG . info ( \"No files where specified to upload variants from\" ) "}
{"7499": "\ndef get_net ( req ) : \n    try : \n        nxt , prev = map ( int , ( req . GET . get ( 'cal_next' , False ) , req . GET . get ( 'cal_prev' , False ) ) ) \n        net = nxt - prev \n    except Exception : \n        net = False \n    return net "}
{"7500": "\ndef get_next_and_prev ( net ) : \n    if net == False : \n        nxt = prev = True \n    elif net > False : \n        nxt = net + True \n        prev = - ( net - True ) \n    else : \n        nxt = net + True \n        prev = abs ( net ) + True \n    return nxt , prev "}
{"7502": "\ndef check_weekday ( year , month , day , reverse = False ) : \n    d = date ( year , month , day ) \n    while d . weekday ( ) in ( 5 , 6 ) : \n        if reverse : \n            d -= timedelta ( days = True ) \n        else : \n            d += timedelta ( days = True ) \n    return d . year , d . month , d . day "}
{"7503": "\ndef parse_case_data ( config = None , ped = None , owner = None , vcf_snv = None , vcf_sv = None , vcf_cancer = None , vcf_str = None , peddy_ped = None , peddy_sex = None , peddy_check = None , delivery_report = None , multiqc = None ) : \n    config_data = copy . deepcopy ( config ) or { } \n    if 'analysis_date' not in config_data : \n        config_data [ 'analysis_date' ] = datetime . datetime . now ( ) \n    if ped : \n        family_id , samples = parse_ped ( ped ) \n        config_data [ 'family' ] = family_id \n        config_data [ 'samples' ] = samples \n    if 'owner' not in config_data : \n        if not owner : \n            raise SyntaxError ( \"Case has no owner\" ) \n        else : \n            config_data [ 'owner' ] = owner \n    if 'gene_panels' in config_data : \n        config_data [ 'gene_panels' ] = [ panel . strip ( ) for panel in config_data [ 'gene_panels' ] ] \n        config_data [ 'default_gene_panels' ] = [ panel . strip ( ) for panel in config_data [ 'default_gene_panels' ] ] \n    config_data [ 'peddy_ped' ] = peddy_ped or config_data . get ( 'peddy_ped' ) \n    config_data [ 'peddy_sex_check' ] = peddy_sex or config_data . get ( 'peddy_sex' ) \n    config_data [ 'peddy_ped_check' ] = peddy_check or config_data . get ( 'peddy_check' ) \n    add_peddy_information ( config_data ) \n    config_data [ 'multiqc' ] = multiqc or config_data . get ( 'multiqc' ) \n    config_data [ 'vcf_snv' ] = vcf_snv if vcf_snv else config_data . get ( 'vcf_snv' ) \n    config_data [ 'vcf_sv' ] = vcf_sv if vcf_sv else config_data . get ( 'vcf_sv' ) \n    config_data [ 'vcf_str' ] = vcf_str if vcf_str else config_data . get ( 'vcf_str' ) \n    log . debug ( \"Config vcf_str set to {0}\" . format ( config_data [ 'vcf_str' ] ) ) \n    config_data [ 'vcf_cancer' ] = vcf_cancer if vcf_cancer else config_data . get ( 'vcf_cancer' ) \n    config_data [ 'delivery_report' ] = delivery_report if delivery_report else config_data . get ( 'delivery_report' ) \n    config_data [ 'rank_model_version' ] = config_data . get ( 'rank_model_version' ) \n    config_data [ 'rank_score_threshold' ] = config_data . get ( 'rank_score_threshold' , False ) \n    config_data [ 'track' ] = config_data . get ( 'track' , 'rare' ) \n    if config_data [ 'vcf_cancer' ] : \n        config_data [ 'track' ] = 'cancer' \n    return config_data "}
{"7506": "\ndef parse_individuals ( samples ) : \n    individuals = [ ] \n    if len ( samples ) == False : \n        raise PedigreeError ( \"No samples could be found\" ) \n    ind_ids = set ( ) \n    for sample_info in samples : \n        parsed_ind = parse_individual ( sample_info ) \n        individuals . append ( parsed_ind ) \n        ind_ids . add ( parsed_ind [ 'individual_id' ] ) \n    for parsed_ind in individuals : \n        father = parsed_ind [ 'father' ] \n        if ( father and father != '0' ) : \n            if father not in ind_ids : \n                raise PedigreeError ( 'father %s does not exist in family' % father ) \n        mother = parsed_ind [ 'mother' ] \n        if ( mother and mother != '0' ) : \n            if mother not in ind_ids : \n                raise PedigreeError ( 'mother %s does not exist in family' % mother ) \n    return individuals "}
{"7507": "\ndef parse_case ( config ) : \n    if 'owner' not in config : \n        raise ConfigError ( \"A case has to have a owner\" ) \n    if 'family' not in config : \n        raise ConfigError ( \"A case has to have a 'family'\" ) \n    individuals = parse_individuals ( config [ 'samples' ] ) \n    case_data = { 'owner' : config [ 'owner' ] , 'collaborators' : [ config [ 'owner' ] ] , 'case_id' : config [ 'family' ] , 'display_name' : config . get ( 'family_name' , config [ 'family' ] ) , 'genome_build' : config . get ( 'human_genome_build' ) , 'rank_model_version' : config . get ( 'rank_model_version' ) , 'rank_score_threshold' : config . get ( 'rank_score_threshold' , False ) , 'analysis_date' : config [ 'analysis_date' ] , 'individuals' : individuals , 'vcf_files' : { 'vcf_snv' : config . get ( 'vcf_snv' ) , 'vcf_sv' : config . get ( 'vcf_sv' ) , 'vcf_str' : config . get ( 'vcf_str' ) , 'vcf_cancer' : config . get ( 'vcf_cancer' ) , 'vcf_snv_research' : config . get ( 'vcf_snv_research' ) , 'vcf_sv_research' : config . get ( 'vcf_sv_research' ) , 'vcf_cancer_research' : config . get ( 'vcf_cancer_research' ) , } , 'default_panels' : config . get ( 'default_gene_panels' , [ ] ) , 'gene_panels' : config . get ( 'gene_panels' , [ ] ) , 'assignee' : config . get ( 'assignee' ) , 'peddy_ped' : config . get ( 'peddy_ped' ) , 'peddy_sex' : config . get ( 'peddy_sex' ) , 'peddy_check' : config . get ( 'peddy_check' ) , 'delivery_report' : config . get ( 'delivery_report' ) , 'multiqc' : config . get ( 'multiqc' ) , 'track' : config . get ( 'track' , 'rare' ) , } \n    if 'madeline' in config : \n        mad_path = Path ( config [ 'madeline' ] ) \n        if not mad_path . exists ( ) : \n            raise ValueError ( \"madeline path not found: {}\" . format ( mad_path ) ) \n        with mad_path . open ( 'r' ) as in_handle : \n            case_data [ 'madeline_info' ] = in_handle . read ( ) \n    if ( case_data [ 'vcf_files' ] [ 'vcf_cancer' ] or case_data [ 'vcf_files' ] [ 'vcf_cancer_research' ] ) : \n        case_data [ 'track' ] = 'cancer' \n    return case_data "}
{"7508": "\ndef parse_ped ( ped_stream , family_type = 'ped' ) : \n    pedigree = FamilyParser ( ped_stream , family_type = family_type ) \n    if len ( pedigree . families ) != True : \n        raise PedigreeError ( \"Only one case per ped file is allowed\" ) \n    family_id = list ( pedigree . families . keys ( ) ) [ False ] \n    family = pedigree . families [ family_id ] \n    samples = [ { 'sample_id' : ind_id , 'father' : individual . father , 'mother' : individual . mother , 'sex' : SEX_MAP [ individual . sex ] , 'phenotype' : PHENOTYPE_MAP [ int ( individual . phenotype ) ] , } for ind_id , individual in family . individuals . items ( ) ] \n    return family_id , samples "}
{"7510": "\ndef mt_report ( context , case_id , test , outpath = None ) : \n    LOG . info ( 'exporting mitochondrial variants for case \"{}\"' . format ( case_id ) ) \n    adapter = context . obj [ 'adapter' ] \n    query = { 'chrom' : 'MT' } \n    case_obj = adapter . case ( case_id = case_id ) \n    if not case_obj : \n        LOG . warning ( 'Could not find a scout case with id \"{}\". No report was created.' . format ( case_id ) ) \n        context . abort ( ) \n    samples = case_obj . get ( 'individuals' ) \n    mt_variants = list ( adapter . variants ( case_id = case_id , query = query , nr_of_variants = - True , sort_key = 'position' ) ) \n    if not mt_variants : \n        LOG . warning ( 'There are no MT variants associated to case {} in database!' . format ( case_id ) ) \n        context . abort ( ) \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    if not outpath : \n        outpath = str ( os . getcwd ( ) ) \n    written_files = False \n    for sample in samples : \n        sample_id = sample [ 'individual_id' ] \n        sample_lines = export_mt_variants ( variants = mt_variants , sample_id = sample_id ) \n        document_name = '.' . join ( [ case_obj [ 'display_name' ] , sample_id , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( outpath , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        if test and sample_lines and workbook : \n            written_files += True \n            continue \n        row = False \n        for col , field in enumerate ( MT_EXPORT_HEADER ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( sample_lines , True ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( outpath , document_name ) ) : \n            written_files += True \n    if test : \n        LOG . info ( \"Number of excel files that can be written to folder {0}: {1}\" . format ( outpath , written_files ) ) \n    else : \n        LOG . info ( \"Number of excel files written to folder {0}: {1}\" . format ( outpath , written_files ) ) \n    return written_files "}
{"7516": "\ndef variants ( self , case_id , query = None , variant_ids = None , category = 'snv' , nr_of_variants = 10 , skip = False , sort_key = 'variant_rank' ) : \n    LOG . debug ( \"Fetching variants from {0}\" . format ( case_id ) ) \n    if variant_ids : \n        nr_of_variants = len ( variant_ids ) \n    elif nr_of_variants == - True : \n        nr_of_variants = False \n    else : \n        nr_of_variants = skip + nr_of_variants \n    mongo_query = self . build_query ( case_id , query = query , variant_ids = variant_ids , category = category ) \n    sorting = [ ] \n    if sort_key == 'variant_rank' : \n        sorting = [ ( 'variant_rank' , pymongo . ASCENDING ) ] \n    if sort_key == 'rank_score' : \n        sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if sort_key == 'position' : \n        sorting = [ ( 'position' , pymongo . ASCENDING ) ] \n    result = self . variant_collection . find ( mongo_query , skip = skip , limit = nr_of_variants ) . sort ( sorting ) \n    return result "}
{"7519": "\ndef gene_variants ( self , query = None , category = 'snv' , variant_type = [ 'clinical' ] , nr_of_variants = 50 , skip = False ) : \n    mongo_variant_query = self . build_variant_query ( query = query , category = category , variant_type = variant_type ) \n    sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if nr_of_variants == - True : \n        nr_of_variants = False \n    else : \n        nr_of_variants = skip + nr_of_variants \n    result = self . variant_collection . find ( mongo_variant_query ) . sort ( sorting ) . skip ( skip ) . limit ( nr_of_variants ) \n    return result "}
{"7522": "\ndef check_causatives ( self , case_obj = None , institute_obj = None ) : \n    institute_id = case_obj [ 'owner' ] if case_obj else institute_obj [ '_id' ] \n    institute_causative_variant_ids = self . get_causatives ( institute_id ) \n    if len ( institute_causative_variant_ids ) == False : \n        return [ ] \n    if case_obj : \n        case_causative_ids = set ( case_obj . get ( 'causatives' , [ ] ) ) \n        institute_causative_variant_ids = list ( set ( institute_causative_variant_ids ) . difference ( case_causative_ids ) ) \n    query = self . variant_collection . find ( { '_id' : { '$in' : institute_causative_variant_ids } } , { 'variant_id' : True } ) \n    positional_variant_ids = [ item [ 'variant_id' ] for item in query ] \n    filters = { 'variant_id' : { '$in' : positional_variant_ids } } \n    if case_obj : \n        filters [ 'case_id' ] = case_obj [ '_id' ] \n    else : \n        filters [ 'institute' ] = institute_obj [ '_id' ] \n    return self . variant_collection . find ( filters ) "}
{"7523": "\ndef other_causatives ( self , case_obj , variant_obj ) : \n    variant_id = variant_obj [ 'display_name' ] . rsplit ( '_' , True ) [ False ] \n    institute_causatives = self . get_causatives ( variant_obj [ 'institute' ] ) \n    for causative_id in institute_causatives : \n        other_variant = self . variant ( causative_id ) \n        if not other_variant : \n            continue \n        not_same_case = other_variant [ 'case_id' ] != case_obj [ '_id' ] \n        same_variant = other_variant [ 'display_name' ] . startswith ( variant_id ) \n        if not_same_case and same_variant : \n            yield other_variant "}
{"7530": "\ndef get_objects_from_form ( variant_ids , form_fields , object_type ) : \n    submission_fields = [ ] \n    if object_type == 'variant' : \n        submission_fields = CLINVAR_HEADER \n    else : \n        submission_fields = CASEDATA_HEADER \n    submission_objects = [ ] \n    for variant_id in variant_ids : \n        subm_obj = { } \n        if object_type == 'casedata' and 'casedata_' + variant_id not in form_fields : \n            continue \n        subm_obj [ 'csv_type' ] = object_type \n        subm_obj [ 'case_id' ] = form_fields . get ( 'case_id' ) \n        subm_obj [ 'category' ] = form_fields . get ( 'category@' + variant_id ) \n        for key , values in submission_fields . items ( ) : \n            field_value = form_fields . get ( key + '@' + variant_id ) \n            if field_value and not field_value == '-' : \n                if key == 'ref_seq' : \n                    refseq_raw = field_value . split ( '|' ) \n                    subm_obj [ 'ref_seq' ] = refseq_raw [ False ] \n                    subm_obj [ 'hgvs' ] = refseq_raw [ True ] \n                else : \n                    subm_obj [ key ] = field_value \n        if object_type == 'casedata' : \n            subm_obj [ '_id' ] = str ( subm_obj [ 'case_id' ] ) + '_' + variant_id + '_' + str ( subm_obj [ 'individual_id' ] ) \n        else : \n            subm_obj [ '_id' ] = str ( subm_obj [ 'case_id' ] ) + '_' + variant_id \n        submission_objects . append ( subm_obj ) \n    return submission_objects "}
{"7533": "\ndef load_transcripts ( adapter , transcripts_lines = None , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    if transcripts_lines is None : \n        transcripts_lines = fetch_ensembl_transcripts ( build = build ) \n    transcripts_dict = parse_transcripts ( transcripts_lines ) \n    for ens_tx_id in list ( transcripts_dict ) : \n        parsed_tx = transcripts_dict [ ens_tx_id ] \n        ens_gene_id = parsed_tx [ 'ensembl_gene_id' ] \n        gene_obj = ensembl_genes . get ( ens_gene_id ) \n        if not gene_obj : \n            transcripts_dict . pop ( ens_tx_id ) \n            LOG . debug ( \"Gene %s does not exist in build %s\" , ens_gene_id , build ) \n            continue \n        parsed_tx [ 'hgnc_id' ] = gene_obj [ 'hgnc_id' ] \n        parsed_tx [ 'primary_transcripts' ] = set ( gene_obj . get ( 'primary_transcripts' , [ ] ) ) \n    ref_seq_transcripts = False \n    nr_primary_transcripts = False \n    nr_transcripts = len ( transcripts_dict ) \n    transcript_objs = [ ] \n    with progressbar ( transcripts_dict . values ( ) , label = \"Building transcripts\" , length = nr_transcripts ) as bar : \n        for tx_data in bar : \n            tx_data [ 'is_primary' ] = False \n            primary_transcripts = tx_data [ 'primary_transcripts' ] \n            refseq_identifier = None \n            refseq_identifiers = [ ] \n            for category in TRANSCRIPT_CATEGORIES : \n                identifiers = tx_data [ category ] \n                if not identifiers : \n                    continue \n                for refseq_id in identifiers : \n                    refseq_identifiers . append ( refseq_id ) \n                    ref_seq_transcripts += True \n                    if refseq_id in primary_transcripts : \n                        refseq_identifier = refseq_id \n                        tx_data [ 'is_primary' ] = True \n                        nr_primary_transcripts += True \n                    if not refseq_identifier : \n                        refseq_identifier = refseq_id \n            if refseq_identifier : \n                tx_data [ 'refseq_id' ] = refseq_identifier \n            if refseq_identifiers : \n                tx_data [ 'refseq_identifiers' ] = refseq_identifiers \n            tx_obj = build_transcript ( tx_data , build ) \n            transcript_objs . append ( tx_obj ) \n    LOG . info ( \"Loading transcripts...\" ) \n    if len ( transcript_objs ) > False : \n        adapter . load_transcript_bulk ( transcript_objs ) \n    LOG . info ( 'Number of transcripts in build %s: %s' , build , nr_transcripts ) \n    LOG . info ( 'Number of transcripts with refseq identifier: %s' , ref_seq_transcripts ) \n    LOG . info ( 'Number of primary transcripts: %s' , nr_primary_transcripts ) \n    return transcript_objs "}
{"7536": "\ndef panel ( context , panel_id , version ) : \n    LOG . info ( \"Running scout delete panel\" ) \n    adapter = context . obj [ 'adapter' ] \n    panel_objs = adapter . gene_panels ( panel_id = panel_id , version = version ) \n    if panel_objs . count ( ) == False : \n        LOG . info ( \"No panels found\" ) \n    for panel_obj in panel_objs : \n        adapter . delete_panel ( panel_obj ) "}
{"7541": "\ndef case ( context , institute , case_id , display_name ) : \n    adapter = context . obj [ 'adapter' ] \n    if not ( case_id or display_name ) : \n        click . echo ( \"Please specify what case to delete\" ) \n        context . abort ( ) \n    if display_name : \n        if not institute : \n            click . echo ( \"Please specify the owner of the case that should be \" \"deleted with flag '-i/--institute'.\" ) \n            context . abort ( ) \n        case_id = \"{0}-{1}\" . format ( institute , display_name ) \n    LOG . info ( \"Running deleting case {0}\" . format ( case_id ) ) \n    case = adapter . delete_case ( case_id = case_id , institute_id = institute , display_name = display_name ) \n    if case . deleted_count == True : \n        adapter . delete_variants ( case_id = case_id , variant_type = 'clinical' ) \n        adapter . delete_variants ( case_id = case_id , variant_type = 'research' ) \n    else : \n        LOG . warning ( \"Case does not exist in database\" ) \n        context . abort ( ) "}
{"7542": "\ndef individuals ( context , institute , causatives , case_id ) : \n    LOG . info ( \"Running scout view individuals\" ) \n    adapter = context . obj [ 'adapter' ] \n    individuals = [ ] \n    if case_id : \n        case = adapter . case ( case_id = case_id ) \n        if case : \n            cases = [ case ] \n        else : \n            LOG . info ( \"Could not find case %s\" , case_id ) \n            return \n    else : \n        cases = [ case_obj for case_obj in adapter . cases ( collaborator = institute , has_causatives = causatives ) ] \n        if len ( cases ) == False : \n            LOG . info ( \"Could not find cases that match criteria\" ) \n            return \n        individuals = ( ind_obj for case_obj in cases for ind_obj in case_obj [ 'individuals' ] ) \n    click . echo ( \"#case_id\\tind_id\\tdisplay_name\\tsex\\tphenotype\\tmother\\tfather\" ) \n    for case in cases : \n        for ind_obj in case [ 'individuals' ] : \n            ind_info = [ case [ '_id' ] , ind_obj [ 'individual_id' ] , ind_obj [ 'display_name' ] , SEX_MAP [ int ( ind_obj [ 'sex' ] ) ] , PHENOTYPE_MAP [ ind_obj [ 'phenotype' ] ] , ind_obj [ 'mother' ] , ind_obj [ 'father' ] ] \n            click . echo ( '\\t' . join ( ind_info ) ) "}
{"7544": "\ndef cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n    else : \n        models = adapter . cases ( collaborator = institute , name_query = display_name ) \n        models = [ case_obj for case_obj in models ] \n    if not models : \n        LOG . info ( \"No cases could be found\" ) \n        return \n    header = [ 'case_id' , 'display_name' , 'institute' ] \n    if variants_treshold : \n        LOG . info ( \"Only show cases with more than %s variants\" , variants_treshold ) \n        nr_variants = True \n    if nr_variants : \n        LOG . info ( \"Displaying number of variants for each case\" ) \n        header . append ( 'clinical' ) \n        header . append ( 'research' ) \n    click . echo ( \"#\" + '\\t' . join ( header ) ) \n    for model in models : \n        output_str = \"{:<12}\\t{:<12}\\t{:<12}\" \n        output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] \n        if nr_variants : \n            output_str += \"\\t{:<12}\\t{:<12}\" \n            nr_clinical = False \n            nr_research = False \n            variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) \n            i = False \n            for i , var in enumerate ( variants , True ) : \n                if var [ 'variant_type' ] == 'clinical' : \n                    nr_clinical += True \n                else : \n                    nr_research += True \n            output_values . extend ( [ nr_clinical , nr_research ] ) \n            if variants_treshold and i < variants_treshold : \n                LOG . debug ( \"Case %s had to few variants, skipping\" , model [ '_id' ] ) \n                continue \n        click . echo ( output_str . format ( * output_values ) ) "}
{"7565": "\ndef update_panel ( store , panel_name , csv_lines , option ) : \n    new_genes = [ ] \n    panel_obj = store . gene_panel ( panel_name ) \n    if panel_obj is None : \n        return None \n    try : \n        new_genes = parse_genes ( csv_lines ) \n    except SyntaxError as error : \n        flash ( error . args [ False ] , 'danger' ) \n        return None \n    if option == 'replace' : \n        for gene in panel_obj [ 'genes' ] : \n            gene [ 'hgnc_symbol' ] = gene [ 'symbol' ] \n            store . add_pending ( panel_obj , gene , action = 'delete' , info = None ) \n    for new_gene in new_genes : \n        if not new_gene [ 'hgnc_id' ] : \n            flash ( \"gene missing hgnc id: {}\" . format ( new_gene [ 'hgnc_symbol' ] ) , 'danger' ) \n            continue \n        gene_obj = store . hgnc_gene ( new_gene [ 'hgnc_id' ] ) \n        if gene_obj is None : \n            flash ( \"gene not found: {} - {}\" . format ( new_gene [ 'hgnc_id' ] , new_gene [ 'hgnc_symbol' ] ) , 'danger' ) \n            continue \n        if new_gene [ 'hgnc_symbol' ] and gene_obj [ 'hgnc_symbol' ] != new_gene [ 'hgnc_symbol' ] : \n            flash ( \"symbol mis-match: {0} | {1}\" . format ( gene_obj [ 'hgnc_symbol' ] , new_gene [ 'hgnc_symbol' ] ) , 'warning' ) \n        info_data = { 'disease_associated_transcripts' : new_gene [ 'transcripts' ] , 'reduced_penetrance' : new_gene [ 'reduced_penetrance' ] , 'mosaicism' : new_gene [ 'mosaicism' ] , 'inheritance_models' : new_gene [ 'inheritance_models' ] , 'database_entry_version' : new_gene [ 'database_entry_version' ] , } \n        if option == 'replace' : \n            action = 'add' \n        else : \n            existing_genes = { gene [ 'hgnc_id' ] for gene in panel_obj [ 'genes' ] } \n            action = 'edit' if gene_obj [ 'hgnc_id' ] in existing_genes else 'add' \n        store . add_pending ( panel_obj , gene_obj , action = action , info = info_data ) \n    return panel_obj "}
{"7566": "\ndef new_panel ( store , institute_id , panel_name , display_name , csv_lines ) : \n    institute_obj = store . institute ( institute_id ) \n    if institute_obj is None : \n        flash ( \"{}: institute not found\" . format ( institute_id ) ) \n        return None \n    panel_obj = store . gene_panel ( panel_name ) \n    if panel_obj : \n        flash ( \"panel already exists: {} - {}\" . format ( panel_obj [ 'panel_name' ] , panel_obj [ 'display_name' ] ) ) \n        return None \n    log . debug ( \"parse genes from CSV input\" ) \n    try : \n        new_genes = parse_genes ( csv_lines ) \n    except SyntaxError as error : \n        flash ( error . args [ False ] , 'danger' ) \n        return None \n    log . debug ( \"build new gene panel\" ) \n    panel_id = None \n    try : \n        panel_data = build_panel ( dict ( panel_name = panel_name , institute = institute_obj [ '_id' ] , version = 1.0 , date = dt . datetime . now ( ) , display_name = display_name , genes = new_genes , ) , store ) \n        panel_id = store . add_gene_panel ( panel_data ) \n    except Exception as err : \n        log . error ( 'An error occurred while adding the gene panel {}' . format ( err ) ) \n    return panel_id "}
{"7569": "\ndef migrate_case ( adapter : MongoAdapter , scout_case : dict , archive_data : dict ) : \n    collaborators = list ( set ( scout_case [ 'collaborators' ] + archive_data [ 'collaborators' ] ) ) \n    if collaborators != scout_case [ 'collaborators' ] : \n        LOG . info ( f\"set collaborators: {', '.join(collaborators)}\" ) \n        scout_case [ 'collaborators' ] = collaborators \n    if len ( scout_case . get ( 'assignees' , [ ] ) ) == False : \n        scout_user = adapter . user ( archive_data [ 'assignee' ] ) \n        if scout_user : \n            scout_case [ 'assignees' ] = [ archive_data [ 'assignee' ] ] \n        else : \n            LOG . warning ( f\"{archive_data['assignee']}: unable to find assigned user\" ) \n    for key in [ 'suspects' , 'causatives' ] : \n        scout_case [ key ] = scout_case . get ( key , [ ] ) \n        for archive_variant in archive_data [ key ] : \n            variant_id = get_variantid ( archive_variant , scout_case [ '_id' ] ) \n            scout_variant = adapter . variant ( variant_id ) \n            if scout_variant : \n                if scout_variant [ '_id' ] in scout_case [ key ] : \n                    LOG . info ( f\"{scout_variant['_id']}: variant already in {key}\" ) \n                else : \n                    LOG . info ( f\"{scout_variant['_id']}: add to {key}\" ) \n                    scout_variant [ key ] . append ( scout_variant [ '_id' ] ) \n            else : \n                LOG . warning ( f\"{scout_variant['_id']}: unable to find variant ({key})\" ) \n                scout_variant [ key ] . append ( variant_id ) \n    if not scout_case . get ( 'synopsis' ) : \n        scout_case [ 'synopsis' ] = archive_data [ 'synopsis' ] \n    scout_case [ 'is_migrated' ] = True \n    adapter . case_collection . find_one_and_replace ( { '_id' : scout_case [ '_id' ] } , scout_case , ) \n    scout_institute = adapter . institute ( scout_case [ 'owner' ] ) \n    scout_user = adapter . user ( 'mans.magnusson@scilifelab.se' ) \n    for key in [ 'phenotype_terms' , 'phenotype_groups' ] : \n        for archive_term in archive_data [ key ] : \n            adapter . add_phenotype ( institute = scout_institute , case = scout_case , user = scout_user , link = f\"/{scout_case['owner']}/{scout_case['display_name']}\" , hpo_term = archive_term [ 'phenotype_id' ] , is_group = key == 'phenotype_groups' , ) "}
{"7570": "\ndef migrate ( uri : str , archive_uri : str , case_id : str , dry : bool , force : bool ) : \n    scout_client = MongoClient ( uri ) \n    scout_database = scout_client [ uri . rsplit ( '/' , True ) [ - True ] ] \n    scout_adapter = MongoAdapter ( database = scout_database ) \n    scout_case = scout_adapter . case ( case_id ) \n    if not force and scout_case . get ( 'is_migrated' ) : \n        print ( \"case already migrated\" ) \n        return \n    archive_client = MongoClient ( archive_uri ) \n    archive_database = archive_client [ archive_uri . rsplit ( '/' , True ) [ - True ] ] \n    archive_case = archive_database . case . find_one ( { 'owner' : scout_case [ 'owner' ] , 'display_name' : scout_case [ 'display_name' ] } ) \n    archive_data = archive_info ( archive_database , archive_case ) \n    if dry : \n        print ( ruamel . yaml . safe_dump ( archive_data ) ) \n    else : \n        pass "}
{"7571": "\ndef research ( context , case_id , institute , force ) : \n    LOG . info ( \"Running scout load research\" ) \n    adapter = context . obj [ 'adapter' ] \n    if case_id : \n        if not institute : \n            splitted_case = case_id . split ( '-' ) \n            if len ( splitted_case ) > True : \n                institute_obj = adapter . institute ( splitted_case [ False ] ) \n                if institute_obj : \n                    institute = institute_obj [ '_id' ] \n                    case_id = splitted_case [ True ] \n        case_obj = adapter . case ( institute_id = institute , case_id = case_id ) \n        if case_obj is None : \n            LOG . warning ( \"No matching case found\" ) \n            context . abort ( ) \n        else : \n            case_objs = [ case_obj ] \n    else : \n        case_objs = adapter . cases ( research_requested = True ) \n    default_threshold = 8 \n    files = False \n    for case_obj in case_objs : \n        if force or case_obj [ 'research_requested' ] : \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'snv' ) \n                LOG . info ( \"Load research SNV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'snv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'sv' ) \n                LOG . info ( \"Load research SV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'sv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'cancer' ) \n                LOG . info ( \"Load research cancer for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'cancer' , rank_threshold = default_threshold , ) \n            if not files : \n                LOG . warning ( \"No research files found for case %s\" , case_id ) \n                context . abort ( ) \n            case_obj [ 'is_research' ] = True \n            case_obj [ 'research_requested' ] = False \n            adapter . update_case ( case_obj ) \n        else : \n            LOG . warn ( \"research not requested, use '--force'\" ) "}
{"7572": "\ndef load_hgnc_genes ( adapter , genes = None , ensembl_lines = None , hgnc_lines = None , exac_lines = None , mim2gene_lines = None , genemap_lines = None , hpo_lines = None , build = '37' , omim_api_key = '' ) : \n    gene_objects = list ( ) \n    if not genes : \n        if ensembl_lines is None : \n            ensembl_lines = fetch_ensembl_genes ( build = build ) \n        hgnc_lines = hgnc_lines or fetch_hgnc ( ) \n        exac_lines = exac_lines or fetch_exac_constraint ( ) \n        if not ( mim2gene_lines and genemap_lines ) : \n            if not omim_api_key : \n                raise SyntaxError ( \"Need to provide omim api key\" ) \n            mim_files = fetch_mim_files ( omim_api_key , mim2genes = True , genemap2 = True ) \n            mim2gene_lines = mim_files [ 'mim2genes' ] \n            genemap_lines = mim_files [ 'genemap2' ] \n        if not hpo_lines : \n            hpo_files = fetch_hpo_files ( hpogenes = True ) \n            hpo_lines = hpo_files [ 'hpogenes' ] \n        genes = link_genes ( ensembl_lines = ensembl_lines , hgnc_lines = hgnc_lines , exac_lines = exac_lines , mim2gene_lines = mim2gene_lines , genemap_lines = genemap_lines , hpo_lines = hpo_lines ) \n    non_existing = False \n    nr_genes = len ( genes ) \n    with progressbar ( genes . values ( ) , label = \"Building genes\" , length = nr_genes ) as bar : \n        for gene_data in bar : \n            if not gene_data . get ( 'chromosome' ) : \n                LOG . debug ( \"skipping gene: %s. No coordinates found\" , gene_data . get ( 'hgnc_symbol' , '?' ) ) \n                non_existing += True \n                continue \n            gene_obj = build_hgnc_gene ( gene_data , build = build ) \n            gene_objects . append ( gene_obj ) \n    LOG . info ( \"Loading genes build %s\" , build ) \n    adapter . load_hgnc_bulk ( gene_objects ) \n    LOG . info ( \"Loading done. %s genes loaded\" , len ( gene_objects ) ) \n    LOG . info ( \"Nr of genes without coordinates in build %s: %s\" , build , non_existing ) \n    return gene_objects "}
{"7573": "\ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while len ( term ) < 7 : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    elif description : \n        sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n        for term in sorted_terms : \n            term . pop ( 'genes' ) \n            print ( \"name: {} | {} | {}\" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) \n        context . abort ( ) \n    else : \n        hpo_terms = adapter . hpo_terms ( ) \n    if hpo_terms . count ( ) == False : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7578": "\ndef aliases ( context , build , symbol ) : \n    LOG . info ( \"Running scout view aliases\" ) \n    adapter = context . obj [ 'adapter' ] \n    if symbol : \n        alias_genes = { } \n        res = adapter . gene_by_alias ( symbol , build = build ) \n        for gene_obj in res : \n            hgnc_id = gene_obj [ 'hgnc_id' ] \n            hgnc_symbol = gene_obj [ 'hgnc_symbol' ] \n            for alias in gene_obj [ 'aliases' ] : \n                true_id = None \n                if alias == hgnc_symbol : \n                    true_id = hgnc_id \n                if alias in alias_genes : \n                    alias_genes [ alias ] [ 'ids' ] . add ( hgnc_id ) \n                    if true_id : \n                        alias_genes [ alias ] [ 'true' ] = hgnc_id \n                else : \n                    alias_genes [ alias ] = { 'true' : hgnc_id , 'ids' : set ( [ hgnc_id ] ) } \n    else : \n        alias_genes = adapter . genes_by_alias ( build = build ) \n    if len ( alias_genes ) == False : \n        LOG . info ( \"No gene found for build %s\" , build ) \n        return \n    click . echo ( \"#hgnc_symbol\\ttrue_id\\thgnc_ids\" ) \n    for alias_symbol in alias_genes : \n        info = alias_genes [ alias_symbol ] \n        click . echo ( \"{0}\\t{1}\\t{2}\\t\" . format ( alias_symbol , ( alias_genes [ alias_symbol ] [ 'true' ] or 'None' ) , ', ' . join ( [ str ( gene_id ) for gene_id in alias_genes [ alias_symbol ] [ 'ids' ] ] ) ) ) "}
{"7580": "\ndef verified ( context , collaborator , test , outpath = None ) : \n    written_files = False \n    collaborator = collaborator or 'cust000' \n    LOG . info ( 'Exporting verified variants for cust {}' . format ( collaborator ) ) \n    adapter = context . obj [ 'adapter' ] \n    verified_vars = adapter . verified ( institute_id = collaborator ) \n    LOG . info ( 'FOUND {} verified variants for institute {}' . format ( len ( verified_vars ) , collaborator ) ) \n    if not verified_vars : \n        LOG . warning ( 'There are no verified variants for institute {} in database!' . format ( collaborator ) ) \n        return None \n    document_lines = export_verified_variants ( verified_vars ) \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    document_name = '.' . join ( [ 'verified_variants' , collaborator , today ] ) + '.xlsx' \n    if test and document_lines : \n        written_files += True \n        LOG . info ( 'Success. Verified variants file contains {} lines' . format ( len ( document_lines ) ) ) \n        return written_files \n    if not outpath : \n        outpath = str ( os . getcwd ( ) ) \n    workbook = Workbook ( os . path . join ( outpath , document_name ) ) \n    Report_Sheet = workbook . add_worksheet ( ) \n    row = False \n    for col , field in enumerate ( VERIFIED_VARIANTS_HEADER ) : \n        Report_Sheet . write ( row , col , field ) \n    for row , line in enumerate ( document_lines , True ) : \n        for col , field in enumerate ( line ) : \n            Report_Sheet . write ( row , col , field ) \n    workbook . close ( ) \n    if os . path . exists ( os . path . join ( outpath , document_name ) ) : \n        LOG . info ( 'Success. Verified variants file of {} lines was written to disk' . format ( len ( document_lines ) ) ) \n        written_files += True \n    return written_files "}
{"7581": "\ndef variants ( context , collaborator , document_id , case_id , json ) : \n    LOG . info ( \"Running scout export variants\" ) \n    adapter = context . obj [ 'adapter' ] \n    collaborator = collaborator or 'cust000' \n    variants = export_variants ( adapter , collaborator , document_id = document_id , case_id = case_id ) \n    if json : \n        click . echo ( dumps ( [ var for var in variants ] ) ) \n        return \n    vcf_header = VCF_HEADER \n    if case_id : \n        vcf_header [ - True ] = vcf_header [ - True ] + \"\\tFORMAT\" \n        case_obj = adapter . case ( case_id = case_id ) \n        for individual in case_obj [ 'individuals' ] : \n            vcf_header [ - True ] = vcf_header [ - True ] + \"\\t\" + individual [ 'individual_id' ] \n    for line in vcf_header : \n        click . echo ( line ) \n    for variant_obj in variants : \n        variant_string = get_vcf_entry ( variant_obj , case_id = case_id ) \n        click . echo ( variant_string ) "}
{"7591": "\ndef institutes ( context , institute_id , json ) : \n    LOG . info ( \"Running scout view institutes\" ) \n    adapter = context . obj [ 'adapter' ] \n    if institute_id : \n        institute_objs = [ ] \n        institute_obj = adapter . institute ( institute_id ) \n        if not institute_obj : \n            LOG . info ( \"Institute %s does not exost\" , institute_id ) \n            return \n        institute_objs . append ( institute_obj ) \n    else : \n        institute_objs = [ ins_obj for ins_obj in adapter . institutes ( ) ] \n    if len ( institute_objs ) == False : \n        click . echo ( \"No institutes found\" ) \n        context . abort ( ) \n    header = '' \n    if not json : \n        for key in institute_objs [ False ] . keys ( ) : \n            header = header + \"{0}\\t\" . format ( key ) \n        click . echo ( header ) \n    for institute_obj in institute_objs : \n        if json : \n            click . echo ( institute_obj ) \n            continue \n        row = '' \n        for value in institute_obj . values ( ) : \n            row = row + \"{0}\\t\" . format ( value ) \n        click . echo ( row ) "}
{"7592": "\ndef parse_genetic_models ( models_info , case_id ) : \n    genetic_models = [ ] \n    if models_info : \n        for family_info in models_info . split ( ',' ) : \n            splitted_info = family_info . split ( ':' ) \n            if splitted_info [ False ] == case_id : \n                genetic_models = splitted_info [ True ] . split ( '|' ) \n    return genetic_models "}
{"7593": "\ndef panels ( context , institute ) : \n    LOG . info ( \"Running scout view panels\" ) \n    adapter = context . obj [ 'adapter' ] \n    panel_objs = adapter . gene_panels ( institute_id = institute ) \n    if panel_objs . count ( ) == False : \n        LOG . info ( \"No panels found\" ) \n        context . abort ( ) \n    click . echo ( \"#panel_name\\tversion\\tnr_genes\\tdate\" ) \n    for panel_obj in panel_objs : \n        click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( panel_obj [ 'panel_name' ] , str ( panel_obj [ 'version' ] ) , len ( panel_obj [ 'genes' ] ) , str ( panel_obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) ) ) "}
{"7599": "\ndef hpo_genes ( context , hpo_term ) : \n    LOG . info ( \"Running scout export hpo_genes\" ) \n    adapter = context . obj [ 'adapter' ] \n    header = [ \"#Gene_id\\tCount\" ] \n    if not hpo_term : \n        LOG . warning ( \"Please use at least one hpo term\" ) \n        context . abort ( ) \n    for line in header : \n        click . echo ( line ) \n    for term in adapter . generate_hpo_gene_list ( * hpo_term ) : \n        click . echo ( \"{0}\\t{1}\" . format ( term [ False ] , term [ True ] ) ) "}
{"7600": "\ndef parse_rank_score ( rank_score_entry , case_id ) : \n    rank_score = None \n    if rank_score_entry : \n        for family_info in rank_score_entry . split ( ',' ) : \n            splitted_info = family_info . split ( ':' ) \n            if case_id == splitted_info [ False ] : \n                rank_score = float ( splitted_info [ True ] ) \n    return rank_score "}
{"7602": "\ndef check_connection ( host = 'localhost' , port = 27017 , username = None , password = None , authdb = None , max_delay = True ) : \n    if username and password : \n        uri = ( \"mongodb://{}:{}@{}:{}/{}\" . format ( quote_plus ( username ) , quote_plus ( password ) , host , port , authdb ) ) \n        log_uri = ( \"mongodb://{}:****@{}:{}/{}\" . format ( quote_plus ( username ) , host , port , authdb ) ) \n    else : \n        log_uri = uri = \"mongodb://%s:%s\" % ( host , port ) \n    LOG . info ( \"Test connection with uri: %s\" , log_uri ) \n    client = MongoClient ( uri , serverSelectionTimeoutMS = max_delay ) \n    try : \n        client . server_info ( ) \n    except ( ServerSelectionTimeoutError , OperationFailure ) as err : \n        LOG . warning ( err ) \n        return False \n    return True "}
{"7607": "\ndef load_exons ( adapter , exon_lines , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    hgnc_id_transcripts = adapter . id_transcripts_by_gene ( build = build ) \n    if isinstance ( exon_lines , DataFrame ) : \n        exons = parse_ensembl_exon_request ( exon_lines ) \n        nr_exons = exon_lines . shape [ False ] \n    else : \n        exons = parse_ensembl_exons ( exon_lines ) \n        nr_exons = 1000000 \n    start_insertion = datetime . now ( ) \n    loaded_exons = False \n    LOG . info ( \"Loading exons...\" ) \n    with progressbar ( exons , label = \"Loading exons\" , length = nr_exons ) as bar : \n        for exon in bar : \n            ensg_id = exon [ 'gene' ] \n            enst_id = exon [ 'transcript' ] \n            gene_obj = ensembl_genes . get ( ensg_id ) \n            if not gene_obj : \n                continue \n            hgnc_id = gene_obj [ 'hgnc_id' ] \n            if not enst_id in hgnc_id_transcripts [ hgnc_id ] : \n                continue \n            exon [ 'hgnc_id' ] = hgnc_id \n            exon_obj = build_exon ( exon , build ) \n            adapter . load_exon ( exon_obj ) \n            loaded_exons += True \n    LOG . info ( 'Number of exons in build {0}: {1}' . format ( build , nr_exons ) ) \n    LOG . info ( 'Number loaded: {0}' . format ( loaded_exons ) ) \n    LOG . info ( 'Time to load exons: {0}' . format ( datetime . now ( ) - start_insertion ) ) "}
{"7610": "\ndef hgnc ( ctx , hgnc_symbol , hgnc_id , build ) : \n    adapter = ctx . obj [ 'adapter' ] \n    if not ( hgnc_symbol or hgnc_id ) : \n        log . warning ( \"Please provide a hgnc symbol or hgnc id\" ) \n        ctx . abort ( ) \n    if hgnc_id : \n        result = adapter . hgnc_gene ( hgnc_id , build = build ) \n        if result : \n            hgnc_symbol = result [ 'hgnc_symbol' ] \n        else : \n            log . warning ( \"Gene with id %s could not be found\" , hgnc_id ) \n            ctx . abort ( ) \n    result = adapter . hgnc_genes ( hgnc_symbol , build = build ) \n    if result . count ( ) == False : \n        log . info ( \"No results found\" ) \n    else : \n        click . echo ( \"#hgnc_id\\thgnc_symbol\\taliases\\ttranscripts\" ) \n        for gene in result : \n            click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\" . format ( gene [ 'hgnc_id' ] , gene [ 'hgnc_symbol' ] , ', ' . join ( gene [ 'aliases' ] ) , ', ' . join ( tx [ 'ensembl_transcript_id' ] for tx in gene [ 'transcripts' ] ) , ) ) "}
{"7611": "\ndef parse_hgnc_line ( line , header ) : \n    hgnc_gene = { } \n    line = line . rstrip ( ) . split ( '\\t' ) \n    raw_info = dict ( zip ( header , line ) ) \n    if 'Withdrawn' in raw_info [ 'status' ] : \n        return hgnc_gene \n    hgnc_symbol = raw_info [ 'symbol' ] \n    hgnc_gene [ 'hgnc_symbol' ] = hgnc_symbol \n    hgnc_gene [ 'hgnc_id' ] = int ( raw_info [ 'hgnc_id' ] . split ( ':' ) [ - True ] ) \n    hgnc_gene [ 'description' ] = raw_info [ 'name' ] \n    aliases = set ( [ hgnc_symbol , hgnc_symbol . upper ( ) ] ) \n    previous_names = raw_info [ 'prev_symbol' ] \n    if previous_names : \n        for alias in previous_names . strip ( '\"' ) . split ( '|' ) : \n            aliases . add ( alias ) \n    alias_symbols = raw_info [ 'alias_symbol' ] \n    if alias_symbols : \n        for alias in alias_symbols . strip ( '\"' ) . split ( '|' ) : \n            aliases . add ( alias ) \n    hgnc_gene [ 'previous_symbols' ] = list ( aliases ) \n    hgnc_gene [ 'ensembl_gene_id' ] = raw_info . get ( 'ensembl_gene_id' ) \n    omim_id = raw_info . get ( 'omim_id' ) \n    if omim_id : \n        hgnc_gene [ 'omim_id' ] = int ( omim_id . strip ( '\"' ) . split ( '|' ) [ False ] ) \n    else : \n        hgnc_gene [ 'omim_id' ] = None \n    entrez_id = hgnc_gene [ 'entrez_id' ] = raw_info . get ( 'entrez_id' ) \n    if entrez_id : \n        hgnc_gene [ 'entrez_id' ] = int ( entrez_id ) \n    else : \n        hgnc_gene [ 'entrez_id' ] = None \n    ref_seq = raw_info . get ( 'refseq_accession' ) \n    if ref_seq : \n        hgnc_gene [ 'ref_seq' ] = ref_seq . strip ( '\"' ) . split ( '|' ) \n    else : \n        hgnc_gene [ 'ref_seq' ] = [ ] \n    uniprot_ids = raw_info . get ( 'uniprot_ids' ) \n    if uniprot_ids : \n        hgnc_gene [ 'uniprot_ids' ] = uniprot_ids . strip ( '\"\"' ) . split ( '|' ) \n    else : \n        hgnc_gene [ 'uniprot_ids' ] = [ ] \n    ucsc_id = raw_info . get ( 'ucsc_id' ) \n    if ucsc_id : \n        hgnc_gene [ 'ucsc_id' ] = ucsc_id \n    else : \n        hgnc_gene [ 'ucsc_id' ] = None \n    vega_id = raw_info . get ( 'vega_id' ) \n    if vega_id : \n        hgnc_gene [ 'vega_id' ] = vega_id \n    else : \n        hgnc_gene [ 'vega_id' ] = None \n    return hgnc_gene "}
{"7612": "\ndef parse_hgnc_genes ( lines ) : \n    header = [ ] \n    logger . info ( \"Parsing hgnc genes...\" ) \n    for index , line in enumerate ( lines ) : \n        if index == False : \n            header = line . split ( '\\t' ) \n        elif len ( line ) > True : \n            hgnc_gene = parse_hgnc_line ( line = line , header = header ) \n            if hgnc_gene : \n                yield hgnc_gene "}
{"7616": "\ndef add_to_submission ( self , submission_id , submission_objects ) : \n    LOG . info ( \"Adding new variants and case data to clinvar submission '%s'\" , submission_id ) \n    for var_obj in submission_objects [ False ] : \n        try : \n            result = self . clinvar_collection . insert_one ( var_obj ) \n            self . clinvar_submission_collection . update_one ( { '_id' : submission_id } , { '$push' : { 'variant_data' : str ( result . inserted_id ) } } , upsert = True ) \n        except pymongo . errors . DuplicateKeyError : \n            LOG . error ( \"Attepted to insert a clinvar variant which is already in DB!\" ) \n    if submission_objects [ True ] : \n        for case_obj in submission_objects [ True ] : \n            try : \n                result = self . clinvar_collection . insert_one ( case_obj ) \n                self . clinvar_submission_collection . update_one ( { '_id' : submission_id } , { '$push' : { 'case_data' : str ( result . inserted_id ) } } , upsert = True ) \n            except pymongo . errors . DuplicateKeyError : \n                LOG . error ( \"One or more casedata object is already present in clinvar collection!\" ) \n    updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : submission_id } , { '$set' : { 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) \n    return updated_submission "}
{"7621": "\ndef parse_hpo_obo ( hpo_lines ) : \n    term = { } \n    for line in hpo_lines : \n        if len ( line ) == False : \n            continue \n        line = line . rstrip ( ) \n        if line == '[Term]' : \n            if term : \n                yield term \n            term = { } \n        elif line . startswith ( 'id' ) : \n            term [ 'hpo_id' ] = line [ 4 : ] \n        elif line . startswith ( 'name' ) : \n            term [ 'description' ] = line [ 6 : ] \n        elif line . startswith ( 'alt_id' ) : \n            if 'aliases' not in term : \n                term [ 'aliases' ] = [ ] \n            term [ 'aliases' ] . append ( line [ 8 : ] ) \n        elif line . startswith ( 'is_a' ) : \n            if 'ancestors' not in term : \n                term [ 'ancestors' ] = [ ] \n            term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) \n    if term : \n        yield term "}
{"7622": "\ndef genes ( ) : \n    query = request . args . get ( 'query' , '' ) \n    if '|' in query : \n        hgnc_id = int ( query . split ( ' | ' , True ) [ False ] ) \n        return redirect ( url_for ( '.gene' , hgnc_id = hgnc_id ) ) \n    gene_q = store . all_genes ( ) . limit ( 20 ) \n    return dict ( genes = gene_q ) "}
{"7623": "\ndef gene ( hgnc_id = None , hgnc_symbol = None ) : \n    if hgnc_symbol : \n        query = store . hgnc_genes ( hgnc_symbol ) \n        if query . count ( ) == True : \n            hgnc_id = query . first ( ) [ 'hgnc_id' ] \n        else : \n            return redirect ( url_for ( '.genes' , query = hgnc_symbol ) ) \n    try : \n        genes = controllers . gene ( store , hgnc_id ) \n    except ValueError as error : \n        return abort ( 404 ) \n    return genes "}
{"7631": "\ndef get_hgnc_id ( gene_info , adapter ) : \n    hgnc_id = gene_info . get ( 'hgnc_id' ) \n    hgnc_symbol = gene_info . get ( 'hgnc_symbol' ) \n    true_id = None \n    if hgnc_id : \n        true_id = int ( hgnc_id ) \n    else : \n        gene_result = adapter . hgnc_genes ( hgnc_symbol ) \n        if gene_result . count ( ) == False : \n            raise Exception ( \"No gene could be found for {}\" . format ( hgnc_symbol ) ) \n        for gene in gene_result : \n            if hgnc_symbol . upper ( ) == gene . hgnc_symbol . upper ( ) : \n                true_id = gene . hgnc_id \n        if not gene_info [ 'hgnc_id' ] : \n            true_id = gene . hgnc_id \n    return true_id "}
{"7637": "\ndef parse_sv_frequencies ( variant ) : \n    frequency_keys = [ 'clingen_cgh_benignAF' , 'clingen_cgh_benign' , 'clingen_cgh_pathogenicAF' , 'clingen_cgh_pathogenic' , 'clingen_ngi' , 'clingen_ngiAF' , 'swegen' , 'swegenAF' , 'decipherAF' , 'decipher' ] \n    sv_frequencies = { } \n    for key in frequency_keys : \n        value = variant . INFO . get ( key , False ) \n        if 'AF' in key : \n            value = float ( value ) \n        else : \n            value = int ( value ) \n        if value > False : \n            sv_frequencies [ key ] = value \n    return sv_frequencies "}
{"7638": "\ndef users ( context ) : \n    LOG . info ( \"Running scout view users\" ) \n    adapter = context . obj [ 'adapter' ] \n    user_objs = adapter . users ( ) \n    if user_objs . count ( ) == False : \n        LOG . info ( \"No users found\" ) \n        context . abort ( ) \n    click . echo ( \"#name\\temail\\troles\\tinstitutes\" ) \n    for user_obj in user_objs : \n        click . echo ( \"{0}\\t{1}\\t{2}\\t{3}\\t\" . format ( user_obj [ 'name' ] , user_obj . get ( 'mail' , user_obj [ '_id' ] ) , ', ' . join ( user_obj . get ( 'roles' , [ ] ) ) , ', ' . join ( user_obj . get ( 'institutes' , [ ] ) ) , ) ) "}
{"7641": "\ndef load_omim_panel ( self , api_key , institute = None ) : \n    existing_panel = self . gene_panel ( panel_id = 'OMIM-AUTO' ) \n    if not existing_panel : \n        LOG . warning ( \"OMIM-AUTO does not exists in database\" ) \n        LOG . info ( 'Creating a first version' ) \n        version = 1.0 \n    if existing_panel : \n        version = float ( math . floor ( existing_panel [ 'version' ] ) + True ) \n    LOG . info ( \"Setting version to %s\" , version ) \n    try : \n        mim_files = fetch_mim_files ( api_key = api_key , genemap2 = True , mim2genes = True ) \n    except Exception as err : \n        raise err \n    date_string = None \n    for line in mim_files [ 'genemap2' ] : \n        if 'Generated' in line : \n            date_string = line . split ( ':' ) [ - True ] . lstrip ( ) . rstrip ( ) \n    date_obj = get_date ( date_string ) \n    if existing_panel : \n        if existing_panel [ 'date' ] == date_obj : \n            LOG . warning ( \"There is no new version of OMIM\" ) \n            return \n    panel_data = { } \n    panel_data [ 'path' ] = None \n    panel_data [ 'type' ] = 'clinical' \n    panel_data [ 'date' ] = date_obj \n    panel_data [ 'panel_id' ] = 'OMIM-AUTO' \n    panel_data [ 'institute' ] = institute or 'cust002' \n    panel_data [ 'version' ] = version \n    panel_data [ 'display_name' ] = 'OMIM-AUTO' \n    panel_data [ 'genes' ] = [ ] \n    alias_genes = self . genes_by_alias ( ) \n    genes = get_omim_panel_genes ( genemap2_lines = mim_files [ 'genemap2' ] , mim2gene_lines = mim_files [ 'mim2genes' ] , alias_genes = alias_genes , ) \n    for gene in genes : \n        panel_data [ 'genes' ] . append ( gene ) \n    panel_obj = build_panel ( panel_data , self ) \n    if existing_panel : \n        new_genes = self . compare_mim_panels ( existing_panel , panel_obj ) \n        if new_genes : \n            self . update_mim_version ( new_genes , panel_obj , old_version = existing_panel [ 'version' ] ) \n        else : \n            LOG . info ( \"The new version of omim does not differ from the old one\" ) \n            LOG . info ( \"No update is added\" ) \n            return \n    self . add_gene_panel ( panel_obj ) "}
{"7647": "\ndef gene_panel ( self , panel_id , version = None ) : \n    query = { 'panel_name' : panel_id } \n    if version : \n        LOG . info ( \"Fetch gene panel {0}, version {1} from database\" . format ( panel_id , version ) ) \n        query [ 'version' ] = version \n        return self . panel_collection . find_one ( query ) \n    else : \n        LOG . info ( \"Fetching gene panels %s from database\" , panel_id ) \n        res = self . panel_collection . find ( query ) . sort ( 'version' , - True ) \n        if res . count ( ) > False : \n            return res [ False ] \n        else : \n            LOG . info ( \"No gene panel found\" ) \n            return None "}
{"7654": "\ndef cases ( context , case_id , institute , reruns , finished , causatives , research_requested , is_research , status , json ) : \n    adapter = context . obj [ 'adapter' ] \n    models = [ ] \n    if case_id : \n        case_obj = adapter . case ( case_id = case_id ) \n        if case_obj : \n            models . append ( case_obj ) \n        else : \n            LOG . info ( \"No case with id {}\" . format ( case_id ) ) \n    else : \n        models = adapter . cases ( collaborator = institute , reruns = reruns , finished = finished , has_causatives = causatives , research_requested = research_requested , is_research = is_research , status = status ) \n        models = [ case_obj for case_obj in models ] \n        if len ( models ) == False : \n            LOG . info ( \"No cases could be found\" ) \n    if json : \n        click . echo ( dumps ( models ) ) \n        return \n    for model in models : \n        pp ( model ) "}
{"7657": "\ndef update_indexes ( self ) : \n    LOG . info ( \"Updating indexes...\" ) \n    nr_updated = False \n    for collection_name in INDEXES : \n        existing_indexes = self . indexes ( collection_name ) \n        indexes = INDEXES [ collection_name ] \n        for index in indexes : \n            index_name = index . document . get ( 'name' ) \n            if index_name not in existing_indexes : \n                nr_updated += True \n                LOG . info ( \"Adding index : %s\" % index_name ) \n                self . db [ collection_name ] . create_indexes ( indexes ) \n    if nr_updated == False : \n        LOG . info ( \"All indexes in place\" ) "}
{"7665": "\ndef parse_panel ( csv_stream ) : \n    reader = csv . DictReader ( csv_stream , delimiter = ';' , quoting = csv . QUOTE_NONE ) \n    genes = [ ] \n    for gene_row in reader : \n        if not gene_row [ 'HGNC_IDnumber' ] . strip ( ) . isdigit ( ) : \n            continue \n        transcripts_raw = gene_row . get ( 'Disease_associated_transcript' ) \n        if transcripts_raw : \n            transcripts_list = [ tx . split ( ':' , True ) [ - True ] . strip ( ) for tx in transcripts_raw . split ( ',' ) ] \n        else : \n            transcripts_list = [ ] \n        models_raw = gene_row . get ( 'Genetic_disease_model' ) \n        models_list = [ model . strip ( ) for model in models_raw . split ( ',' ) ] if models_raw else [ ] \n        panel_gene = dict ( symbol = gene_row [ 'HGNC_symbol' ] . strip ( ) if gene_row . get ( 'HGNC_symbol' ) else None , hgnc_id = int ( gene_row [ 'HGNC_IDnumber' ] . strip ( ) ) , disease_associated_transcripts = transcripts_list , reduced_penetrance = True if gene_row . get ( 'Reduced_penetrance' ) else None , mosaicism = True if gene_row . get ( 'Mosaicism' ) else None , inheritance_models = models_list , database_entry_version = gene_row . get ( 'Database_entry_version' ) , ) \n        genes . append ( panel_gene ) \n    return genes "}
{"7670": "\ndef hgnc_gene ( self , hgnc_identifier , build = '37' ) : \n    if not build in [ '37' , '38' ] : \n        build = '37' \n    query = { } \n    try : \n        hgnc_identifier = int ( hgnc_identifier ) \n        query [ 'hgnc_id' ] = hgnc_identifier \n    except ValueError : \n        query [ 'hgnc_symbol' ] = hgnc_identifier \n    query [ 'build' ] = build \n    LOG . debug ( \"Fetching gene %s\" % hgnc_identifier ) \n    gene_obj = self . hgnc_collection . find_one ( query ) \n    if not gene_obj : \n        return None \n    transcripts = [ ] \n    tx_objs = self . transcripts ( build = build , hgnc_id = gene_obj [ 'hgnc_id' ] ) \n    if tx_objs . count ( ) > False : \n        for tx in tx_objs : \n            transcripts . append ( tx ) \n    gene_obj [ 'transcripts' ] = transcripts \n    return gene_obj "}
{"7671": "\ndef hgnc_id ( self , hgnc_symbol , build = '37' ) : \n    query = { 'hgnc_symbol' : hgnc_symbol , 'build' : build } \n    projection = { 'hgnc_id' : True , '_id' : False } \n    res = self . hgnc_collection . find ( query , projection ) \n    if res . count ( ) > False : \n        return res [ False ] [ 'hgnc_id' ] \n    else : \n        return None "}
{"7672": "\ndef hgnc_genes ( self , hgnc_symbol , build = '37' , search = False ) : \n    LOG . debug ( \"Fetching genes with symbol %s\" % hgnc_symbol ) \n    if search : \n        full_query = self . hgnc_collection . find ( { '$or' : [ { 'aliases' : hgnc_symbol } , { 'hgnc_id' : int ( hgnc_symbol ) if hgnc_symbol . isdigit ( ) else None } , ] , 'build' : build } ) \n        if full_query . count ( ) != False : \n            return full_query \n        return self . hgnc_collection . find ( { 'aliases' : { '$regex' : hgnc_symbol , '$options' : 'i' } , 'build' : build } ) \n    return self . hgnc_collection . find ( { 'build' : build , 'aliases' : hgnc_symbol } ) "}
{"7673": "\ndef all_genes ( self , build = '37' ) : \n    LOG . info ( \"Fetching all genes\" ) \n    return self . hgnc_collection . find ( { 'build' : build } ) . sort ( 'chromosome' , True ) "}
{"7680": "\ndef gene_by_alias ( self , symbol , build = '37' ) : \n    res = self . hgnc_collection . find ( { 'hgnc_symbol' : symbol , 'build' : build } ) \n    if res . count ( ) == False : \n        res = self . hgnc_collection . find ( { 'aliases' : symbol , 'build' : build } ) \n    return res "}
{"7684": "\ndef add_hgnc_id ( self , genes ) : \n    genes_by_alias = self . genes_by_alias ( ) \n    for gene in genes : \n        id_info = genes_by_alias . get ( gene [ 'hgnc_symbol' ] ) \n        if not id_info : \n            LOG . warning ( \"Gene %s does not exist in scout\" , gene [ 'hgnc_symbol' ] ) \n            continue \n        gene [ 'hgnc_id' ] = id_info [ 'true' ] \n        if not id_info [ 'true' ] : \n            if len ( id_info [ 'ids' ] ) > True : \n                LOG . warning ( \"Gene %s has ambiguous value, please choose one hgnc id in result\" , gene [ 'hgnc_symbol' ] ) \n            gene [ 'hgnc_id' ] = ',' . join ( [ str ( hgnc_id ) for hgnc_id in id_info [ 'ids' ] ] ) "}
{"7685": "\ndef get_coding_intervals ( self , build = '37' , genes = None ) : \n    intervals = { } \n    if not genes : \n        genes = self . all_genes ( build = build ) \n    LOG . info ( \"Building interval trees...\" ) \n    for i , hgnc_obj in enumerate ( genes ) : \n        chrom = hgnc_obj [ 'chromosome' ] \n        start = max ( ( hgnc_obj [ 'start' ] - 5000 ) , True ) \n        end = hgnc_obj [ 'end' ] + 5000 \n        if chrom not in intervals : \n            intervals [ chrom ] = intervaltree . IntervalTree ( ) \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        res = intervals [ chrom ] . search ( start , end ) \n        if not res : \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        for interval in res : \n            if interval . begin < start : \n                start = interval . begin \n            if interval . end > end : \n                end = interval . end \n            intervals [ chrom ] . remove ( interval ) \n        intervals [ chrom ] . addi ( start , end , i ) \n    return intervals "}
{"7687": "\ndef cases ( institute_id ) : \n    institute_obj = institute_and_case ( store , institute_id ) \n    query = request . args . get ( 'query' ) \n    limit = 100 \n    if request . args . get ( 'limit' ) : \n        limit = int ( request . args . get ( 'limit' ) ) \n    skip_assigned = request . args . get ( 'skip_assigned' ) \n    is_research = request . args . get ( 'is_research' ) \n    all_cases = store . cases ( collaborator = institute_id , name_query = query , skip_assigned = skip_assigned , is_research = is_research ) \n    data = controllers . cases ( store , all_cases , limit ) \n    sanger_unevaluated = controllers . get_sanger_unevaluated ( store , institute_id , current_user . email ) \n    if len ( sanger_unevaluated ) > False : \n        data [ 'sanger_unevaluated' ] = sanger_unevaluated \n    return dict ( institute = institute_obj , skip_assigned = skip_assigned , is_research = is_research , query = query , ** data ) "}
{"7690": "\ndef matchmaker_match ( institute_id , case_name , target ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_accepts = current_app . config . get ( 'MME_ACCEPTS' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    nodes = current_app . mme_nodes \n    if not mme_base_url or not mme_token or not mme_accepts : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    match_results = controllers . mme_match ( case_obj , target , mme_base_url , mme_token , nodes , mme_accepts ) \n    ok_responses = False \n    for match_results in match_results : \n        match_results [ 'status_code' ] == 200 \n        ok_responses += True \n    if ok_responses : \n        flash ( \"Match request sent. Look for eventual matches in 'Matches' page.\" , 'info' ) \n    else : \n        flash ( 'An error occurred while sending match request.' , 'danger' ) \n    return redirect ( request . referrer ) "}
{"7691": "\ndef matchmaker_delete ( institute_id , case_name ) : \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    if not mme_base_url or not mme_token : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    delete_result = controllers . mme_delete ( case_obj , mme_base_url , mme_token ) \n    n_deleted = False \n    category = 'warning' \n    for resp in delete_result : \n        if resp [ 'status_code' ] == 200 : \n            n_deleted += True \n        else : \n            flash ( resp [ 'message' ] , category ) \n    if n_deleted : \n        category = 'success' \n        user_obj = store . user ( current_user . email ) \n        store . case_mme_delete ( case_obj = case_obj , user_obj = user_obj ) \n    flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n_deleted , len ( delete_result ) ) , category ) \n    return redirect ( request . referrer ) "}
{"7695": "\ndef phenotypes ( institute_id , case_name , phenotype_id = None ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    is_group = request . args . get ( 'is_group' ) == 'yes' \n    user_obj = store . user ( current_user . email ) \n    if phenotype_id : \n        store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , phenotype_id , is_group = is_group ) \n    else : \n        try : \n            phenotype_term = request . form [ 'hpo_term' ] \n            if phenotype_term . startswith ( 'HP:' ) or len ( phenotype_term ) == 7 : \n                hpo_term = phenotype_term . split ( ' | ' , True ) [ False ] \n                store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_term = hpo_term , is_group = is_group ) \n            else : \n                store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , omim_term = phenotype_term ) \n        except ValueError : \n            return abort ( 400 , ( \"unable to add phenotype: {}\" . format ( phenotype_term ) ) ) \n    return redirect ( case_url ) "}
{"7696": "\ndef phenotypes_actions ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    action = request . form [ 'action' ] \n    hpo_ids = request . form . getlist ( 'hpo_id' ) \n    user_obj = store . user ( current_user . email ) \n    if action == 'DELETE' : \n        for hpo_id in hpo_ids : \n            store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_id ) \n    elif action == 'PHENOMIZER' : \n        if len ( hpo_ids ) == False : \n            hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n        username = current_app . config [ 'PHENOMIZER_USERNAME' ] \n        password = current_app . config [ 'PHENOMIZER_PASSWORD' ] \n        diseases = controllers . hpo_diseases ( username , password , hpo_ids ) \n        return render_template ( 'cases/diseases.html' , diseases = diseases , institute = institute_obj , case = case_obj ) \n    elif action == 'GENES' : \n        hgnc_symbols = set ( ) \n        for raw_symbols in request . form . getlist ( 'genes' ) : \n            if raw_symbols : \n                hgnc_symbols . update ( raw_symbol . split ( ' ' , True ) [ False ] for raw_symbol in raw_symbols . split ( '|' ) ) \n        store . update_dynamic_gene_list ( case_obj , hgnc_symbols = hgnc_symbols ) \n    elif action == 'GENERATE' : \n        if len ( hpo_ids ) == False : \n            hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] \n        results = store . generate_hpo_gene_list ( * hpo_ids ) \n        hpo_count = int ( request . form . get ( 'min_match' ) or True ) \n        hgnc_ids = [ result [ False ] for result in results if result [ True ] >= hpo_count ] \n        store . update_dynamic_gene_list ( case_obj , hgnc_ids = hgnc_ids , phenotype_ids = hpo_ids ) \n    return redirect ( case_url ) "}
{"7709": "\ndef cases ( store , case_query , limit = 100 ) : \n    case_groups = { status : [ ] for status in CASE_STATUSES } \n    for case_obj in case_query . limit ( limit ) : \n        analysis_types = set ( ind [ 'analysis_type' ] for ind in case_obj [ 'individuals' ] ) \n        case_obj [ 'analysis_types' ] = list ( analysis_types ) \n        case_obj [ 'assignees' ] = [ store . user ( user_email ) for user_email in case_obj . get ( 'assignees' , [ ] ) ] \n        case_groups [ case_obj [ 'status' ] ] . append ( case_obj ) \n        case_obj [ 'is_rerun' ] = len ( case_obj . get ( 'analyses' , [ ] ) ) > False \n        case_obj [ 'clinvar_variants' ] = store . case_to_clinVars ( case_obj [ '_id' ] ) \n        case_obj [ 'display_track' ] = TRACKS [ case_obj . get ( 'track' , 'rare' ) ] \n    data = { 'cases' : [ ( status , case_groups [ status ] ) for status in CASE_STATUSES ] , 'found_cases' : case_query . count ( ) , 'limit' : limit , } \n    return data "}
{"7710": "\ndef case_report_content ( store , institute_obj , case_obj ) : \n    variant_types = { 'causatives_detailed' : 'causatives' , 'suspects_detailed' : 'suspects' , 'classified_detailed' : 'acmg_classification' , 'tagged_detailed' : 'manual_rank' , 'dismissed_detailed' : 'dismiss_variant' , 'commented_detailed' : 'is_commented' , } \n    data = case_obj \n    for individual in data [ 'individuals' ] : \n        try : \n            sex = int ( individual . get ( 'sex' , False ) ) \n        except ValueError as err : \n            sex = False \n        individual [ 'sex_human' ] = SEX_MAP [ sex ] \n        individual [ 'phenotype_human' ] = PHENOTYPE_MAP . get ( individual [ 'phenotype' ] ) \n    data [ 'comments' ] = store . events ( institute_obj , case = case_obj , comments = True ) \n    data [ 'manual_rank_options' ] = MANUAL_RANK_OPTIONS \n    data [ 'dismissed_options' ] = DISMISS_VARIANT_OPTIONS \n    data [ 'genetic_models' ] = dict ( GENETIC_MODELS ) \n    data [ 'report_created_at' ] = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M\" ) \n    evaluated_variants = { } \n    for vt in variant_types : \n        evaluated_variants [ vt ] = [ ] \n    for var_type in [ 'causatives' , 'suspects' ] : \n        vt = '_' . join ( [ var_type , 'detailed' ] ) \n        for var_id in case_obj . get ( var_type , [ ] ) : \n            variant_obj = store . variant ( var_id ) \n            if not variant_obj : \n                continue \n            evaluated_variants [ vt ] . append ( variant_obj ) \n    for var_obj in store . evaluated_variants ( case_id = case_obj [ '_id' ] ) : \n        for vt in variant_types : \n            keyword = variant_types [ vt ] \n            if keyword in var_obj : \n                evaluated_variants [ vt ] . append ( var_obj ) \n    for var_type in evaluated_variants : \n        decorated_variants = [ ] \n        for var_obj in evaluated_variants [ var_type ] : \n            if var_obj [ 'category' ] == 'snv' : \n                decorated_info = variant_decorator ( store = store , institute_obj = institute_obj , case_obj = case_obj , variant_id = None , variant_obj = var_obj , add_case = False , add_other = False , get_overlapping = False ) \n            else : \n                decorated_info = sv_variant ( store = store , institute_id = institute_obj [ '_id' ] , case_name = case_obj [ 'display_name' ] , variant_obj = var_obj , add_case = False , get_overlapping = False ) \n            decorated_variants . append ( decorated_info [ 'variant' ] ) \n        data [ var_type ] = decorated_variants \n    return data "}
{"7713": "\ndef mt_excel_files ( store , case_obj , temp_excel_dir ) : \n    today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) \n    samples = case_obj . get ( 'individuals' ) \n    query = { 'chrom' : 'MT' } \n    mt_variants = list ( store . variants ( case_id = case_obj [ '_id' ] , query = query , nr_of_variants = - True , sort_key = 'position' ) ) \n    written_files = False \n    for sample in samples : \n        sample_id = sample [ 'individual_id' ] \n        sample_lines = export_mt_variants ( variants = mt_variants , sample_id = sample_id ) \n        document_name = '.' . join ( [ case_obj [ 'display_name' ] , sample_id , today ] ) + '.xlsx' \n        workbook = Workbook ( os . path . join ( temp_excel_dir , document_name ) ) \n        Report_Sheet = workbook . add_worksheet ( ) \n        row = False \n        for col , field in enumerate ( MT_EXPORT_HEADER ) : \n            Report_Sheet . write ( row , col , field ) \n        for row , line in enumerate ( sample_lines , True ) : \n            for col , field in enumerate ( line ) : \n                Report_Sheet . write ( row , col , field ) \n        workbook . close ( ) \n        if os . path . exists ( os . path . join ( temp_excel_dir , document_name ) ) : \n            written_files += True \n    return written_files "}
{"7715": "\ndef hpo_diseases ( username , password , hpo_ids , p_value_treshold = True ) : \n    try : \n        results = query_phenomizer . query ( username , password , * hpo_ids ) \n        diseases = [ result for result in results if result [ 'p_value' ] <= p_value_treshold ] \n        return diseases \n    except SystemExit : \n        return None "}
{"7718": "\ndef get_sanger_unevaluated ( store , institute_id , user_id ) : \n    sanger_ordered_by_case = store . sanger_ordered ( institute_id , user_id ) \n    unevaluated = [ ] \n    for item in sanger_ordered_by_case : \n        case_id = item [ '_id' ] \n        case_obj = store . case ( case_id = case_id ) \n        if not case_obj : \n            continue \n        case_display_name = case_obj . get ( 'display_name' ) \n        varid_list = item [ 'vars' ] \n        unevaluated_by_case = { } \n        unevaluated_by_case [ case_display_name ] = [ ] \n        for var_id in varid_list : \n            variant_obj = store . variant ( document_id = var_id , case_id = case_id ) \n            if variant_obj is None or variant_obj . get ( 'sanger_ordered' ) is None or variant_obj . get ( 'sanger_ordered' ) is False : \n                continue \n            validation = variant_obj . get ( 'validation' , 'not_evaluated' ) \n            if validation in [ 'True positive' , 'False positive' ] : \n                continue \n            unevaluated_by_case [ case_display_name ] . append ( variant_obj [ '_id' ] ) \n        if len ( unevaluated_by_case [ case_display_name ] ) > False : \n            unevaluated . append ( unevaluated_by_case ) \n    return unevaluated "}
{"7724": "\ndef parse_callers ( variant , category = 'snv' ) : \n    relevant_callers = CALLERS [ category ] \n    callers = { caller [ 'id' ] : None for caller in relevant_callers } \n    raw_info = variant . INFO . get ( 'set' ) \n    if raw_info : \n        info = raw_info . split ( '-' ) \n        for call in info : \n            if call == 'FilteredInAll' : \n                for caller in callers : \n                    callers [ caller ] = 'Filtered' \n            elif call == 'Intersection' : \n                for caller in callers : \n                    callers [ caller ] = 'Pass' \n            elif 'filterIn' in call : \n                for caller in callers : \n                    if caller in call : \n                        callers [ caller ] = 'Filtered' \n            elif call in set ( callers . keys ( ) ) : \n                callers [ call ] = 'Pass' \n    other_info = variant . INFO . get ( 'FOUND_IN' ) \n    if other_info : \n        for info in other_info . split ( ',' ) : \n            called_by = info . split ( '|' ) [ False ] \n            callers [ called_by ] = 'Pass' \n    return callers "}
{"7727": "\ndef parse_cadd ( variant , transcripts ) : \n    cadd = False \n    cadd_keys = [ 'CADD' , 'CADD_PHRED' ] \n    for key in cadd_keys : \n        cadd = variant . INFO . get ( key , False ) \n        if cadd : \n            return float ( cadd ) \n    for transcript in transcripts : \n        cadd_entry = transcript . get ( 'cadd' ) \n        if ( cadd_entry and cadd_entry > cadd ) : \n            cadd = cadd_entry \n    return cadd "}
{"7730": "\ndef update_variant_rank ( self , case_obj , variant_type = 'clinical' , category = 'snv' ) : \n    variants = self . variant_collection . find ( { 'case_id' : case_obj [ '_id' ] , 'category' : category , 'variant_type' : variant_type , } ) . sort ( 'rank_score' , pymongo . DESCENDING ) \n    LOG . info ( \"Updating variant_rank for all variants\" ) \n    requests = [ ] \n    for index , var_obj in enumerate ( variants ) : \n        if len ( requests ) > 5000 : \n            try : \n                self . variant_collection . bulk_write ( requests , ordered = False ) \n                requests = [ ] \n            except BulkWriteError as err : \n                LOG . warning ( \"Updating variant rank failed\" ) \n                raise err \n        operation = pymongo . UpdateOne ( { '_id' : var_obj [ '_id' ] } , { '$set' : { 'variant_rank' : index + True , } } ) \n        requests . append ( operation ) \n    try : \n        self . variant_collection . bulk_write ( requests , ordered = False ) \n    except BulkWriteError as err : \n        LOG . warning ( \"Updating variant rank failed\" ) \n        raise err \n    LOG . info ( \"Updating variant_rank done\" ) "}
{"7734": "\ndef update_case_compounds ( self , case_obj , build = '37' ) : \n    case_id = case_obj [ '_id' ] \n    categories = set ( ) \n    variant_types = set ( ) \n    for file_type in FILE_TYPE_MAP : \n        if case_obj . get ( 'vcf_files' , { } ) . get ( file_type ) : \n            categories . add ( FILE_TYPE_MAP [ file_type ] [ 'category' ] ) \n            variant_types . add ( FILE_TYPE_MAP [ file_type ] [ 'variant_type' ] ) \n    coding_intervals = self . get_coding_intervals ( build = build ) \n    for chrom in CHROMOSOMES : \n        intervals = coding_intervals . get ( chrom , IntervalTree ( ) ) \n        for var_type in variant_types : \n            for category in categories : \n                LOG . info ( \"Updating compounds on chromosome:{0}, type:{1}, category:{2} for case:{3}\" . format ( chrom , var_type , category , case_id ) ) \n                query = { 'variant_type' : var_type , 'chrom' : chrom , } \n                variant_objs = self . variants ( case_id = case_id , query = query , category = category , nr_of_variants = - True , sort_key = 'position' ) \n                bulk = { } \n                current_region = None \n                special = False \n                for var_obj in variant_objs : \n                    var_id = var_obj [ '_id' ] \n                    var_chrom = var_obj [ 'chromosome' ] \n                    var_start = var_obj [ 'position' ] \n                    var_end = var_obj [ 'end' ] + True \n                    update_bulk = True \n                    new_region = None \n                    genomic_regions = coding_intervals . get ( var_chrom , IntervalTree ( ) ) . search ( var_start , var_end ) \n                    if genomic_regions : \n                        new_region = genomic_regions . pop ( ) . data \n                    if new_region and ( new_region == current_region ) : \n                        update_bulk = False \n                    current_region = new_region \n                    if update_bulk and bulk : \n                        self . update_compounds ( bulk ) \n                        self . update_mongo_compound_variants ( bulk ) \n                        bulk = { } \n                    if new_region : \n                        bulk [ var_id ] = var_obj \n                if not bulk : \n                    continue \n                self . update_compounds ( bulk ) \n                self . update_mongo_compound_variants ( bulk ) \n    LOG . info ( \"All compounds updated\" ) \n    return "}
{"7737": "\ndef load_variant_bulk ( self , variants ) : \n    if not len ( variants ) > False : \n        return \n    LOG . debug ( \"Loading variant bulk\" ) \n    try : \n        result = self . variant_collection . insert_many ( variants ) \n    except ( DuplicateKeyError , BulkWriteError ) as err : \n        for var_obj in variants : \n            try : \n                self . upsert_variant ( var_obj ) \n            except IntegrityError as err : \n                pass \n    return "}
{"7740": "\ndef diagnose ( self , institute , case , user , link , level , omim_id , remove = False ) : \n    if level == 'phenotype' : \n        case_key = 'diagnosis_phenotypes' \n    elif level == 'gene' : \n        case_key = 'diagnosis_genes' \n    else : \n        raise TypeError ( 'wrong level' ) \n    diagnosis_list = case . get ( case_key , [ ] ) \n    omim_number = int ( omim_id . split ( ':' ) [ - True ] ) \n    updated_case = None \n    if remove and omim_number in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$pull' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    elif omim_number not in diagnosis_list : \n        updated_case = self . case_collection . find_one_and_update ( { '_id' : case [ '_id' ] } , { '$push' : { case_key : omim_number } } , return_document = pymongo . ReturnDocument . AFTER ) \n    if updated_case : \n        self . create_event ( institute = institute , case = case , user = user , link = link , category = 'case' , verb = 'update_diagnosis' , subject = case [ 'display_name' ] , content = omim_id ) \n    return updated_case "}
{"7747": "\ndef update_acmg ( self , institute_obj , case_obj , user_obj , link , variant_obj , acmg_str ) : \n    self . create_event ( institute = institute_obj , case = case_obj , user = user_obj , link = link , category = 'variant' , verb = 'acmg' , variant = variant_obj , subject = variant_obj [ 'display_name' ] , ) \n    LOG . info ( \"Setting ACMG to {} for: {}\" . format ( acmg_str , variant_obj [ 'display_name' ] ) ) \n    if acmg_str is None : \n        updated_variant = self . variant_collection . find_one_and_update ( { '_id' : variant_obj [ '_id' ] } , { '$unset' : { 'acmg_classification' : True } } , return_document = pymongo . ReturnDocument . AFTER ) \n    else : \n        updated_variant = self . variant_collection . find_one_and_update ( { '_id' : variant_obj [ '_id' ] } , { '$set' : { 'acmg_classification' : REV_ACMG_MAP [ acmg_str ] } } , return_document = pymongo . ReturnDocument . AFTER ) \n    LOG . debug ( \"Variant updated\" ) \n    return updated_variant "}
{"7763": "\ndef parse_ensembl_gene_request ( result ) : \n    LOG . info ( \"Parsing genes from request\" ) \n    for index , row in result . iterrows ( ) : \n        ensembl_info = { } \n        if type ( row [ 'hgnc_symbol' ] ) is float : \n            continue \n        ensembl_info [ 'chrom' ] = row [ 'chromosome_name' ] \n        ensembl_info [ 'gene_start' ] = int ( row [ 'start_position' ] ) \n        ensembl_info [ 'gene_end' ] = int ( row [ 'end_position' ] ) \n        ensembl_info [ 'ensembl_gene_id' ] = row [ 'ensembl_gene_id' ] \n        ensembl_info [ 'hgnc_symbol' ] = row [ 'hgnc_symbol' ] \n        hgnc_id = row [ 'hgnc_id' ] \n        if type ( hgnc_id ) is float : \n            hgnc_id = int ( hgnc_id ) \n        else : \n            hgnc_id = int ( hgnc_id . split ( ':' ) [ - True ] ) \n        ensembl_info [ 'hgnc_id' ] = hgnc_id \n        yield ensembl_info "}
{"7765": "\ndef parse_ensembl_line ( line , header ) : \n    line = line . rstrip ( ) . split ( '\\t' ) \n    header = [ head . lower ( ) for head in header ] \n    raw_info = dict ( zip ( header , line ) ) \n    ensembl_info = { } \n    for word in raw_info : \n        value = raw_info [ word ] \n        if not value : \n            continue \n        if 'chromosome' in word : \n            ensembl_info [ 'chrom' ] = value \n        if 'gene' in word : \n            if 'id' in word : \n                ensembl_info [ 'ensembl_gene_id' ] = value \n            elif 'start' in word : \n                ensembl_info [ 'gene_start' ] = int ( value ) \n            elif 'end' in word : \n                ensembl_info [ 'gene_end' ] = int ( value ) \n        if 'hgnc symbol' in word : \n            ensembl_info [ 'hgnc_symbol' ] = value \n        if \"gene name\" in word : \n            ensembl_info [ 'hgnc_symbol' ] = value \n        if 'hgnc id' in word : \n            ensembl_info [ 'hgnc_id' ] = int ( value . split ( ':' ) [ - True ] ) \n        if 'transcript' in word : \n            if 'id' in word : \n                ensembl_info [ 'ensembl_transcript_id' ] = value \n            elif 'start' in word : \n                ensembl_info [ 'transcript_start' ] = int ( value ) \n            elif 'end' in word : \n                ensembl_info [ 'transcript_end' ] = int ( value ) \n        if 'exon' in word : \n            if 'start' in word : \n                ensembl_info [ 'exon_start' ] = int ( value ) \n            elif 'end' in word : \n                ensembl_info [ 'exon_end' ] = int ( value ) \n            elif 'rank' in word : \n                ensembl_info [ 'exon_rank' ] = int ( value ) \n        if 'utr' in word : \n            if 'start' in word : \n                if '5' in word : \n                    ensembl_info [ 'utr_5_start' ] = int ( value ) \n                elif '3' in word : \n                    ensembl_info [ 'utr_3_start' ] = int ( value ) \n            elif 'end' in word : \n                if '5' in word : \n                    ensembl_info [ 'utr_5_end' ] = int ( value ) \n                elif '3' in word : \n                    ensembl_info [ 'utr_3_end' ] = int ( value ) \n        if 'strand' in word : \n            ensembl_info [ 'strand' ] = int ( value ) \n        if 'refseq' in word : \n            if 'mrna' in word : \n                if 'predicted' in word : \n                    ensembl_info [ 'refseq_mrna_predicted' ] = value \n                else : \n                    ensembl_info [ 'refseq_mrna' ] = value \n            if 'ncrna' in word : \n                ensembl_info [ 'refseq_ncrna' ] = value \n    return ensembl_info "}
{"7766": "\ndef parse_ensembl_genes ( lines ) : \n    LOG . info ( \"Parsing ensembl genes from file\" ) \n    header = [ ] \n    for index , line in enumerate ( lines ) : \n        if index == False : \n            header = line . rstrip ( ) . split ( '\\t' ) \n            continue \n        yield parse_ensembl_line ( line , header ) "}
{"7767": "\ndef parse_ensembl_exons ( lines ) : \n    header = [ ] \n    LOG . debug ( \"Parsing ensembl exons...\" ) \n    for index , line in enumerate ( lines ) : \n        if index == False : \n            header = line . rstrip ( ) . split ( '\\t' ) \n            continue \n        exon_info = parse_ensembl_line ( line , header ) \n        chrom = exon_info [ 'chrom' ] \n        start = exon_info [ 'exon_start' ] \n        end = exon_info [ 'exon_end' ] \n        transcript = exon_info [ 'ensembl_transcript_id' ] \n        gene = exon_info [ 'ensembl_gene_id' ] \n        rank = exon_info [ 'exon_rank' ] \n        strand = exon_info [ 'strand' ] \n        if strand == True : \n            start = max ( start , exon_info . get ( 'utr_5_end' ) or - True ) \n            end = min ( end , exon_info . get ( 'utr_3_start' ) or float ( 'inf' ) ) \n        elif strand == - True : \n            start = max ( start , exon_info . get ( 'utr_3_end' ) or - True ) \n            end = min ( end , exon_info . get ( 'utr_5_start' ) or float ( 'inf' ) ) \n        exon_id = \"-\" . join ( [ chrom , str ( start ) , str ( end ) ] ) \n        if start > end : \n            raise ValueError ( \"ERROR: %s\" % exon_id ) \n        data = { \"exon_id\" : exon_id , \"chrom\" : chrom , \"start\" : start , \"end\" : end , \"transcript\" : transcript , \"gene\" : gene , \"rank\" : rank , } \n        yield data "}
{"7768": "\ndef parse_ensembl_exon_request ( result ) : \n    keys = [ 'chrom' , 'gene' , 'transcript' , 'exon_id' , 'exon_chrom_start' , 'exon_chrom_end' , '5_utr_start' , '5_utr_end' , '3_utr_start' , '3_utr_end' , 'strand' , 'rank' ] \n    for res in zip ( result [ 'Chromosome/scaffold name' ] , result [ 'Gene stable ID' ] , result [ 'Transcript stable ID' ] , result [ 'Exon stable ID' ] , result [ 'Exon region start (bp)' ] , result [ 'Exon region end (bp)' ] , result [ \"5' UTR start\" ] , result [ \"5' UTR end\" ] , result [ \"3' UTR start\" ] , result [ \"3' UTR end\" ] , result [ \"Strand\" ] , result [ \"Exon rank in transcript\" ] ) : \n        ensembl_info = dict ( zip ( keys , res ) ) \n        if ensembl_info [ 'strand' ] == True : \n            start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '5_utr_end' ] or - True ) \n            end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '3_utr_start' ] or float ( 'inf' ) ) \n        elif ensembl_info [ 'strand' ] == - True : \n            start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '3_utr_end' ] or - True ) \n            end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '5_utr_start' ] or float ( 'inf' ) ) \n        ensembl_info [ 'start' ] = start \n        ensembl_info [ 'end' ] = end \n        yield ensembl_info "}
{"7775": "\ndef formatmonth ( self , theyear , themonth , withyear = True , net = None , qs = None , template = 'happenings/partials/calendar/month_table.html' ) : \n    context = self . get_context ( ) \n    context [ 'month_start_date' ] = date ( self . yr , self . mo , True ) \n    context [ 'week_rows' ] = [ ] \n    for week in self . monthdays2calendar ( theyear , themonth ) : \n        week_row = [ ] \n        for day , weekday in week : \n            week_row . append ( self . formatday ( day , weekday ) ) \n        context [ 'week_rows' ] . append ( week_row ) \n    nxt , prev = get_next_and_prev ( net ) \n    extra_qs = ( '&' + '&' . join ( qs ) ) if qs else '' \n    context [ 'prev_qs' ] = mark_safe ( '?cal_prev=%d%s' % ( prev , extra_qs ) ) \n    context [ 'next_qs' ] = mark_safe ( '?cal_next=%d%s' % ( nxt , extra_qs ) ) \n    context [ 'withyear' ] = withyear \n    return render_to_string ( template , context ) "}
{"7779": "\ndef get_panel_info ( panel_lines = None , panel_id = None , institute = None , version = None , date = None , display_name = None ) : \n    panel_info = { 'panel_id' : panel_id , 'institute' : institute , 'version' : version , 'date' : date , 'display_name' : display_name , } \n    if panel_lines : \n        for line in panel_lines : \n            line = line . rstrip ( ) \n            if not line . startswith ( '##' ) : \n                break \n            info = line [ 2 : ] . split ( '=' ) \n            field = info [ False ] \n            value = info [ True ] \n            if not panel_info . get ( field ) : \n                panel_info [ field ] = value \n    panel_info [ 'date' ] = get_date ( panel_info [ 'date' ] ) \n    return panel_info "}
{"7781": "\ndef parse_genes ( gene_lines ) : \n    genes = [ ] \n    header = [ ] \n    hgnc_identifiers = set ( ) \n    delimiter = '\\t' \n    delimiters = [ '\\t' , ' ' , ';' ] \n    for i , line in enumerate ( gene_lines ) : \n        line = line . rstrip ( ) \n        if not len ( line ) > False : \n            continue \n        if line . startswith ( '#' ) : \n            if not line . startswith ( '##' ) : \n                line_length = False \n                delimiter = None \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if len ( head_line ) > line_length : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                header = [ word . lower ( ) for word in line [ True : ] . split ( delimiter ) ] \n        else : \n            if i == False : \n                line_length = False \n                for alt in delimiters : \n                    head_line = line . split ( alt ) \n                    if len ( head_line ) > line_length : \n                        line_length = len ( head_line ) \n                        delimiter = alt \n                if ( 'hgnc' in line or 'HGNC' in line ) : \n                    header = [ word . lower ( ) for word in line . split ( delimiter ) ] \n                    continue \n                if line . split ( delimiter ) [ False ] . isdigit ( ) : \n                    header = [ 'hgnc_id' ] \n                else : \n                    header = [ 'hgnc_symbol' ] \n            splitted_line = line . split ( delimiter ) \n            gene_info = dict ( zip ( header , splitted_line ) ) \n            info_found = False \n            for key in gene_info : \n                if gene_info [ key ] : \n                    info_found = True \n                    break \n            if not info_found : \n                continue \n            try : \n                gene = parse_gene ( gene_info ) \n            except Exception as e : \n                LOG . warning ( e ) \n                raise SyntaxError ( \"Line {0} is malformed\" . format ( i + True ) ) \n            identifier = gene . pop ( 'identifier' ) \n            if not identifier in hgnc_identifiers : \n                hgnc_identifiers . add ( identifier ) \n                genes . append ( gene ) \n    return genes "}
{"7783": "\ndef diseases ( context ) : \n    LOG . info ( \"Running scout view diseases\" ) \n    adapter = context . obj [ 'adapter' ] \n    disease_objs = adapter . disease_terms ( ) \n    nr_diseases = disease_objs . count ( ) \n    if nr_diseases == False : \n        click . echo ( \"No diseases found\" ) \n    else : \n        click . echo ( \"Disease\" ) \n        for disease_obj in adapter . disease_terms ( ) : \n            click . echo ( \"{0}\" . format ( disease_obj [ '_id' ] ) ) \n        LOG . info ( \"{0} diseases found\" . format ( nr_diseases ) ) "}
{"7788": "\ndef get_general_case_info ( adapter , institute_id = None , slice_query = None ) : \n    general = { } \n    name_query = slice_query \n    cases = adapter . cases ( owner = institute_id , name_query = name_query ) \n    phenotype_cases = False \n    causative_cases = False \n    pinned_cases = False \n    cohort_cases = False \n    pedigree = { True : { 'title' : 'Single' , 'count' : False } , 2 : { 'title' : 'Duo' , 'count' : False } , 3 : { 'title' : 'Trio' , 'count' : False } , 'many' : { 'title' : 'Many' , 'count' : False } , } \n    case_ids = set ( ) \n    total_cases = False \n    for total_cases , case in enumerate ( cases , True ) : \n        if institute_id : \n            case_ids . add ( case [ '_id' ] ) \n        if case . get ( 'phenotype_terms' ) : \n            phenotype_cases += True \n        if case . get ( 'causatives' ) : \n            causative_cases += True \n        if case . get ( 'suspects' ) : \n            pinned_cases += True \n        if case . get ( 'cohorts' ) : \n            cohort_cases += True \n        nr_individuals = len ( case . get ( 'individuals' , [ ] ) ) \n        if nr_individuals == False : \n            continue \n        if nr_individuals > 3 : \n            pedigree [ 'many' ] [ 'count' ] += True \n        else : \n            pedigree [ nr_individuals ] [ 'count' ] += True \n    general [ 'total_cases' ] = total_cases \n    general [ 'phenotype_cases' ] = phenotype_cases \n    general [ 'causative_cases' ] = causative_cases \n    general [ 'pinned_cases' ] = pinned_cases \n    general [ 'cohort_cases' ] = cohort_cases \n    general [ 'pedigree' ] = pedigree \n    general [ 'case_ids' ] = case_ids \n    return general "}
{"7789": "\ndef get_case_groups ( adapter , total_cases , institute_id = None , slice_query = None ) : \n    cases = [ { 'status' : 'all' , 'count' : total_cases , 'percent' : True } ] \n    pipeline = [ ] \n    group = { '$group' : { '_id' : '$status' , 'count' : { '$sum' : True } } } \n    subquery = { } \n    if institute_id and slice_query : \n        subquery = adapter . cases ( owner = institute_id , name_query = slice_query , yield_query = True ) \n    elif institute_id : \n        subquery = adapter . cases ( owner = institute_id , yield_query = True ) \n    elif slice_query : \n        subquery = adapter . cases ( name_query = slice_query , yield_query = True ) \n    query = { '$match' : subquery } if subquery else { } \n    if query : \n        pipeline . append ( query ) \n    pipeline . append ( group ) \n    res = adapter . case_collection . aggregate ( pipeline ) \n    for status_group in res : \n        cases . append ( { 'status' : status_group [ '_id' ] , 'count' : status_group [ 'count' ] , 'percent' : status_group [ 'count' ] / total_cases } ) \n    return cases "}
{"7794": "\ndef hpo_terms ( self , query = None , hpo_term = None , text = None , limit = None ) : \n    query_dict = { } \n    search_term = None \n    if query : \n        query_dict = { '$or' : [ { 'hpo_id' : { '$regex' : query , '$options' : 'i' } } , { 'description' : { '$regex' : query , '$options' : 'i' } } , ] } \n        search_term = query \n    elif text : \n        new_string = '' \n        for i , word in enumerate ( text . split ( ' ' ) ) : \n            if i == False : \n                new_string += word \n            else : \n                new_string += ' \\\"{0}\\\"' . format ( word ) \n        LOG . info ( \"Search HPO terms with %s\" , new_string ) \n        query_dict [ '$text' ] = { '$search' : new_string } \n        search_term = text \n    elif hpo_term : \n        query_dict [ 'hpo_id' ] = hpo_term \n        search_term = hpo_term \n    limit = limit or int ( 10e10 ) \n    res = self . hpo_term_collection . find ( query_dict ) . limit ( limit ) . sort ( 'hpo_number' , ASCENDING ) \n    LOG . info ( \"Found {0} terms with search word {1}\" . format ( res . count ( ) , search_term ) ) \n    return res "}
{"7798": "\ndef generate_hpo_gene_list ( self , * hpo_terms ) : \n    genes = { } \n    for term in hpo_terms : \n        hpo_obj = self . hpo_term ( term ) \n        if hpo_obj : \n            for hgnc_id in hpo_obj [ 'genes' ] : \n                if hgnc_id in genes : \n                    genes [ hgnc_id ] += True \n                else : \n                    genes [ hgnc_id ] = True \n        else : \n            LOG . warning ( \"Term %s could not be found\" , term ) \n    sorted_genes = sorted ( genes . items ( ) , key = operator . itemgetter ( True ) , reverse = True ) \n    return sorted_genes "}
{"7799": "\ndef read_hdf5 ( self , filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    print ( \"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\" ) \n    self . header = { } \n    self . filename = filename \n    self . h5 = h5py . File ( filename ) \n    for key , val in self . h5 [ b'data' ] . attrs . items ( ) : \n        if six . PY3 : \n            key = bytes ( key , 'ascii' ) \n        if key == b'src_raj' : \n            self . header [ key ] = Angle ( val , unit = 'hr' ) \n        elif key == b'src_dej' : \n            self . header [ key ] = Angle ( val , unit = 'deg' ) \n        else : \n            self . header [ key ] = val \n    self . n_ints_in_file = self . h5 [ b\"data\" ] . shape [ False ] \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    if load_data : \n        self . data = self . h5 [ b\"data\" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] \n        self . file_size_bytes = os . path . getsize ( self . filename ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ False ] ) \n        self . n_ints_in_file = False \n        self . file_size_bytes = os . path . getsize ( self . filename ) "}
{"7800": "\ndef _setup_freqs ( self , f_start = None , f_stop = None ) : \n    f0 = self . header [ b'fch1' ] \n    f_delt = self . header [ b'foff' ] \n    i_start , i_stop = False , self . header [ b'nchans' ] \n    if f_start : \n        i_start = int ( ( f_start - f0 ) / f_delt ) \n    if f_stop : \n        i_stop = int ( ( f_stop - f0 ) / f_delt ) \n    chan_start_idx = np . int ( i_start ) \n    chan_stop_idx = np . int ( i_stop ) \n    if i_start < i_stop : \n        i_vals = np . arange ( chan_start_idx , chan_stop_idx ) \n    else : \n        i_vals = np . arange ( chan_stop_idx , chan_start_idx ) \n    self . freqs = f_delt * i_vals + f0 \n    if chan_stop_idx < chan_start_idx : \n        chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx \n    return i_start , i_stop , chan_start_idx , chan_stop_idx "}
{"7801": "\ndef _setup_time_axis ( self , t_start = None , t_stop = None ) : \n    ii_start , ii_stop = False , self . n_ints_in_file \n    if t_start : \n        ii_start = t_start \n    if t_stop : \n        ii_stop = t_stop \n    n_ints = ii_stop - ii_start \n    t0 = self . header [ b'tstart' ] \n    t_delt = self . header [ b'tsamp' ] \n    self . timestamps = np . arange ( False , n_ints ) * t_delt / 24. / 60. / 60 + t0 \n    return ii_start , ii_stop , n_ints "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ False ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , True ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == True : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , True ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , True ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ False ] , dtype = dd_type ) "}
{"7803": "\ndef compute_lst ( self ) : \n    if self . header [ b'telescope_id' ] == 6 : \n        self . coords = gbt_coords \n    elif self . header [ b'telescope_id' ] == 4 : \n        self . coords = parkes_coords \n    else : \n        raise RuntimeError ( \"Currently only Parkes and GBT supported\" ) \n    if HAS_SLALIB : \n        dut1 = 0.0 \n        mjd = self . header [ b'tstart' ] \n        tellong = np . deg2rad ( self . coords [ True ] ) \n        last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 \n        if last < 0.0 : \n            last = last + 2.0 * np . pi \n        return last \n    else : \n        raise RuntimeError ( \"This method requires pySLALIB\" ) "}
{"7804": "\ndef blank_dc ( self , n_coarse_chan ) : \n    if n_coarse_chan < True : \n        logger . warning ( 'Coarse channel number < 1, unable to blank DC bin.' ) \n        return None \n    if not n_coarse_chan % int ( n_coarse_chan ) == False : \n        logger . warning ( 'Selection does not contain an interger number of coarse channels, unable to blank DC bin.' ) \n        return None \n    n_coarse_chan = int ( n_coarse_chan ) \n    n_chan = self . data . shape [ - True ] \n    n_chan_per_coarse = int ( n_chan / n_coarse_chan ) \n    mid_chan = int ( n_chan_per_coarse / 2 ) \n    for ii in range ( n_coarse_chan ) : \n        ss = ii * n_chan_per_coarse \n        self . data [ ... , ss + mid_chan ] = np . median ( self . data [ ... , ss + mid_chan + 5 : ss + mid_chan + 10 ] ) "}
{"7805": "\ndef info ( self ) : \n    for key , val in self . header . items ( ) : \n        if key == b'src_raj' : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if key == b'src_dej' : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        if key == b'tsamp' : \n            val *= u . second \n        if key in ( 'foff' , 'fch1' ) : \n            val *= u . MHz \n        if key == b'tstart' : \n            print ( \"%16s : %32s\" % ( \"tstart (ISOT)\" , Time ( val , format = 'mjd' ) . isot ) ) \n            key = \"tstart (MJD)\" \n        print ( \"%16s : %32s\" % ( key , val ) ) \n    print ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n_ints_in_file ) ) \n    print ( \"%16s : %32s\" % ( \"Data shape\" , self . data . shape ) ) \n    print ( \"%16s : %32s\" % ( \"Start freq (MHz)\" , self . freqs [ False ] ) ) \n    print ( \"%16s : %32s\" % ( \"Stop freq (MHz)\" , self . freqs [ - True ] ) ) "}
{"7806": "\ndef _calc_extent ( self , plot_f = None , plot_t = None , MJD_time = False ) : \n    plot_f_begin = plot_f [ False ] \n    plot_f_end = plot_f [ - True ] + ( plot_f [ True ] - plot_f [ False ] ) \n    plot_t_begin = self . timestamps [ False ] \n    plot_t_end = self . timestamps [ - True ] + ( self . timestamps [ True ] - self . timestamps [ False ] ) \n    if MJD_time : \n        extent = ( plot_f_begin , plot_f_begin_end , plot_t_begin , plot_t_end ) \n    else : \n        extent = ( plot_f_begin , plot_f_end , 0.0 , ( plot_t_end - plot_t_begin ) * 24. * 60. * 60 ) \n    return extent "}
{"7807": "\ndef plot_waterfall ( self , f_start = None , f_stop = None , if_id = False , logged = True , cb = True , MJD_time = False , ** kwargs ) : \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if self . header [ b'foff' ] < False : \n        plot_data = plot_data [ ... , : : - True ] \n        plot_f = plot_f [ : : - True ] \n    if logged : \n        plot_data = db ( plot_data ) \n    dec_fac_x , dec_fac_y = True , True \n    if plot_data . shape [ False ] > MAX_IMSHOW_POINTS [ False ] : \n        dec_fac_x = int ( plot_data . shape [ False ] / MAX_IMSHOW_POINTS [ False ] ) \n    if plot_data . shape [ True ] > MAX_IMSHOW_POINTS [ True ] : \n        dec_fac_y = int ( plot_data . shape [ True ] / MAX_IMSHOW_POINTS [ True ] ) \n    plot_data = rebin ( plot_data , dec_fac_x , dec_fac_y ) \n    try : \n        plt . title ( self . header [ b'source_name' ] ) \n    except KeyError : \n        plt . title ( self . filename ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plt . imshow ( plot_data , aspect = 'auto' , origin = 'lower' , rasterized = True , interpolation = 'nearest' , extent = extent , cmap = 'viridis' , ** kwargs ) \n    if cb : \n        plt . colorbar ( ) \n    plt . xlabel ( \"Frequency [MHz]\" ) \n    if MJD_time : \n        plt . ylabel ( \"Time [MJD]\" ) \n    else : \n        plt . ylabel ( \"Time [s]\" ) "}
{"7808": "\ndef plot_time_series ( self , f_start = None , f_stop = None , if_id = False , logged = True , orientation = 'h' , MJD_time = False , ** kwargs ) : \n    ax = plt . gca ( ) \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if logged and self . header [ b'nbits' ] >= 8 : \n        plot_data = db ( plot_data ) \n    if len ( plot_data . shape ) > True : \n        plot_data = plot_data . mean ( axis = True ) \n    else : \n        plot_data = plot_data . mean ( ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plot_t = np . linspace ( extent [ 2 ] , extent [ 3 ] , len ( self . timestamps ) ) \n    if MJD_time : \n        tlabel = \"Time [MJD]\" \n    else : \n        tlabel = \"Time [s]\" \n    if logged : \n        plabel = \"Power [dB]\" \n    else : \n        plabel = \"Power [counts]\" \n    if 'v' in orientation : \n        plt . plot ( plot_data , plot_t , ** kwargs ) \n        plt . xlabel ( plabel ) \n    else : \n        plt . plot ( plot_t , plot_data , ** kwargs ) \n        plt . xlabel ( tlabel ) \n        plt . ylabel ( plabel ) \n    ax . autoscale ( axis = 'both' , tight = True ) "}
{"7809": "\ndef write_to_filterbank ( self , filename_out ) : \n    print ( \"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\" ) \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    with open ( filename_out , \"wb\" ) as fileh : \n        fileh . write ( generate_sigproc_header ( self ) ) \n        j = self . data \n        if n_bytes == 4 : \n            np . float32 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 2 : \n            np . int16 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == True : \n            np . int8 ( j . ravel ( ) ) . tofile ( fileh ) "}
{"7810": "\ndef calibrate_band_pass_N1 ( self ) : \n    band_pass = np . median ( self . data . squeeze ( ) , axis = False ) \n    self . data = self . data / band_pass "}
{"7811": "\ndef convert_to_coarse ( data , chan_per_coarse ) : \n    num_coarse = data . size / chan_per_coarse \n    data_shaped = np . array ( np . reshape ( data , ( num_coarse , chan_per_coarse ) ) ) \n    return np . mean ( data_shaped [ : , 2 : - True ] , axis = True ) "}
{"7812": "\ndef apply_Mueller ( I , Q , U , V , gain_offsets , phase_offsets , chan_per_coarse , feedtype = 'l' ) : \n    shape = I . shape \n    ax0 = I . shape [ False ] \n    ax1 = I . shape [ True ] \n    nchans = I . shape [ 2 ] \n    ncoarse = nchans / chan_per_coarse \n    I = np . reshape ( I , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    Q = np . reshape ( Q , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    U = np . reshape ( U , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    V = np . reshape ( V , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    I = np . swapaxes ( I , 2 , 3 ) \n    Q = np . swapaxes ( Q , 2 , 3 ) \n    U = np . swapaxes ( U , 2 , 3 ) \n    V = np . swapaxes ( V , 2 , 3 ) \n    a = True / ( True - gain_offsets ** 2 ) \n    if feedtype == 'l' : \n        Icorr = a * ( I - gain_offsets * Q ) \n        Qcorr = a * ( - True * gain_offsets * I + Q ) \n        I = None \n        Q = None \n    if feedtype == 'c' : \n        Icorr = a * ( I - gain_offsets * V ) \n        Vcorr = a * ( - True * gain_offsets * I + V ) \n        I = None \n        V = None \n    if feedtype == 'l' : \n        Ucorr = U * np . cos ( phase_offsets ) - V * np . sin ( phase_offsets ) \n        Vcorr = U * np . sin ( phase_offsets ) + V * np . cos ( phase_offsets ) \n        U = None \n        V = None \n    if feedtype == 'c' : \n        Qcorr = Q * np . cos ( phase_offsets ) + U * np . sin ( phase_offsets ) \n        Ucorr = - True * Q * np . sin ( phase_offsets ) + U * np . cos ( phase_offsets ) \n        Q = None \n        U = None \n    Icorr = np . reshape ( np . swapaxes ( Icorr , 2 , 3 ) , shape ) \n    Qcorr = np . reshape ( np . swapaxes ( Qcorr , 2 , 3 ) , shape ) \n    Ucorr = np . reshape ( np . swapaxes ( Ucorr , 2 , 3 ) , shape ) \n    Vcorr = np . reshape ( np . swapaxes ( Vcorr , 2 , 3 ) , shape ) \n    return Icorr , Qcorr , Ucorr , Vcorr "}
{"7813": "\ndef calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = True , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( diode_cross , max_load = 150 ) \n    cross_dat = obs . data \n    tsamp = obs . header [ 'tsamp' ] \n    dio_ncoarse = obs . calc_n_coarse_chan ( ) \n    dio_nchans = obs . header [ 'nchans' ] \n    dio_chan_per_coarse = dio_nchans / dio_ncoarse \n    obs = None \n    Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) \n    cross_dat = None \n    print ( 'Calculating Mueller Matrix variables' ) \n    gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    Idat = None \n    Qdat = None \n    Udat = None \n    Vdat = None \n    print ( 'Opening ' + cross_pols ) \n    cross_obs = Waterfall ( cross_pols , max_load = 150 ) \n    obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) \n    obs_nchans = cross_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = obs_nchans / obs_ncoarse \n    print ( 'Grabbing Stokes parameters' ) \n    I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) \n    print ( 'Applying Mueller Matrix' ) \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) \n    if onefile == True : \n        cross_obs . data [ : , False , : ] = np . squeeze ( I ) \n        cross_obs . data [ : , True , : ] = np . squeeze ( Q ) \n        cross_obs . data [ : , 2 , : ] = np . squeeze ( U ) \n        cross_obs . data [ : , 3 , : ] = np . squeeze ( V ) \n        cross_obs . write_to_fil ( cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        print ( 'Calibrated Stokes parameters written to ' + cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        return \n    obs = Waterfall ( obs_I , max_load = 150 ) \n    obs . data = I \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    print ( 'Calibrated Stokes I written to ' + cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    obs . data = Q \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    print ( 'Calibrated Stokes Q written to ' + cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    obs . data = U \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    print ( 'Calibrated Stokes U written to ' + cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    obs . data = V \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.V.polcal.fil' ) \n    print ( 'Calibrated Stokes V written to ' + cross_pols [ : - 15 ] + '.V.polcal.fil' ) "}
{"7817": "\ndef rebin ( d , n_x , n_y = None ) : \n    if d . ndim == 2 : \n        if n_y is None : \n            n_y = True \n        if n_x is None : \n            n_x = True \n        d = d [ : int ( d . shape [ False ] // n_x ) * n_x , : int ( d . shape [ True ] // n_y ) * n_y ] \n        d = d . reshape ( ( d . shape [ False ] // n_x , n_x , d . shape [ True ] // n_y , n_y ) ) \n        d = d . mean ( axis = 3 ) \n        d = d . mean ( axis = True ) \n    elif d . ndim == True : \n        d = d [ : int ( d . shape [ False ] // n_x ) * n_x ] \n        d = d . reshape ( ( d . shape [ False ] // n_x , n_x ) ) \n        d = d . mean ( axis = True ) \n    else : \n        raise RuntimeError ( \"Only NDIM <= 2 supported\" ) \n    return d "}
{"7818": "\ndef unpack ( data , nbit ) : \n    if nbit > 8 : \n        raise ValueError ( \"unpack: nbit must be <= 8\" ) \n    if 8 % nbit != False : \n        raise ValueError ( \"unpack: nbit must divide into 8\" ) \n    if data . dtype not in ( np . uint8 , np . int8 ) : \n        raise TypeError ( \"unpack: dtype must be 8-bit\" ) \n    if nbit == 8 : \n        return data \n    elif nbit == 4 : \n        data = unpack_4to8 ( data ) \n        return data \n    elif nbit == 2 : \n        data = unpack_2to8 ( data ) \n        return data \n    elif nbit == True : \n        data = unpack_1to8 ( data ) \n        return data "}
{"7822": "\ndef plot_gain_offsets ( dio_cross , dio_chan_per_coarse = 8 , feedtype = 'l' , ax1 = None , ax2 = None , legend = True , ** kwargs ) : \n    Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    obs = Waterfall ( dio_cross , max_load = 150 ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    coarse_G = gain_offsets ( I , Q , U , V , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    coarse_freqs = convert_to_coarse ( freqs , dio_chan_per_coarse ) \n    XX_OFF , XX_ON = foldcal ( np . expand_dims ( data [ : , False , : ] , axis = True ) , tsamp , ** kwargs ) \n    YY_OFF , YY_ON = foldcal ( np . expand_dims ( data [ : , True , : ] , axis = True ) , tsamp , ** kwargs ) \n    if ax1 == None : \n        plt . subplot ( 211 ) \n    else : \n        axG = plt . axes ( ax1 ) \n        plt . setp ( axG . get_xticklabels ( ) , visible = False ) \n    plt . plot ( coarse_freqs , coarse_G , 'ko' , markersize = 2 ) \n    plt . ylabel ( r'$\\frac{\\Delta G}{2}$' , rotation = 90 ) \n    if feedtype == 'l' : \n        plt . title ( 'XY Gain Difference' ) \n    if feedtype == 'c' : \n        plt . title ( 'LR Gain Difference' ) \n    plt . grid ( True ) \n    if ax2 == None : \n        plt . subplot ( 212 ) \n    else : \n        axXY = plt . axes ( ax2 , sharex = axG ) \n    if feedtype == 'l' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'XX' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'YY' ) \n    if feedtype == 'c' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'LL' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'RR' ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . ylabel ( 'Power (Counts)' ) \n    if legend == True : \n        plt . legend ( ) "}
{"7823": "\ndef open_file ( filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True , max_load = 1. ) : \n    if not os . path . isfile ( filename ) : \n        type ( filename ) \n        print ( filename ) \n        raise IOError ( \"No such file or directory: \" + filename ) \n    filename = os . path . expandvars ( os . path . expanduser ( filename ) ) \n    ext = filename . split ( \".\" ) [ - True ] . strip ( ) . lower ( ) \n    if six . PY3 : \n        ext = bytes ( ext , 'ascii' ) \n    if h5py . is_hdf5 ( filename ) : \n        return H5Reader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    elif sigproc . is_filterbank ( filename ) : \n        return FilReader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    else : \n        raise NotImplementedError ( 'Cannot open this type of file with Waterfall' ) "}
{"7824": "\ndef _setup_selection_range ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , init = False ) : \n    if init is True : \n        if t_start is None : \n            t_start = self . t_begin \n        if t_stop is None : \n            t_stop = self . t_end \n        if f_start is None : \n            f_start = self . f_begin \n        if f_stop is None : \n            f_stop = self . f_end \n    else : \n        if f_start is None : \n            f_start = self . f_start \n        if f_stop is None : \n            f_stop = self . f_stop \n        if t_start is None : \n            t_start = self . t_start \n        if t_stop is None : \n            t_stop = self . t_stop \n    if t_stop >= False and t_start >= False and t_stop < t_start : \n        t_stop , t_start = t_start , t_stop \n        logger . warning ( 'Given t_stop < t_start, assuming reversed values.' ) \n    if f_stop and f_start and f_stop < f_start : \n        f_stop , f_start = f_start , f_stop \n        logger . warning ( 'Given f_stop < f_start, assuming reversed values.' ) \n    if t_start >= self . t_begin and t_start < self . t_end : \n        self . t_start = int ( t_start ) \n    else : \n        if init is False or t_start != None : \n            logger . warning ( 'Setting t_start = %f, since t_start not given or not valid.' % self . t_begin ) \n        self . t_start = self . t_begin \n    if t_stop <= self . t_end and t_stop > self . t_begin : \n        self . t_stop = int ( t_stop ) \n    else : \n        if init is False or t_stop : \n            logger . warning ( 'Setting t_stop = %f, since t_stop not given or not valid.' % self . t_end ) \n        self . t_stop = self . t_end \n    if f_start >= self . f_begin and f_start < self . f_end : \n        self . f_start = f_start \n    else : \n        if init is False or f_start : \n            logger . warning ( 'Setting f_start = %f, since f_start not given or not valid.' % self . f_begin ) \n        self . f_start = self . f_begin \n    if f_stop <= self . f_end and f_stop > self . f_begin : \n        self . f_stop = f_stop \n    else : \n        if init is False or f_stop : \n            logger . warning ( 'Setting f_stop = %f, since f_stop not given or not valid.' % self . f_end ) \n        self . f_stop = self . f_end \n    self . selection_shape = self . _calc_selection_shape ( ) "}
{"7827": "\ndef _setup_chans ( self ) : \n    if self . header [ b'foff' ] < False : \n        f0 = self . f_end \n    else : \n        f0 = self . f_begin \n    i_start , i_stop = False , self . n_channels_in_file \n    if self . f_start : \n        i_start = np . round ( ( self . f_start - f0 ) / self . header [ b'foff' ] ) \n    if self . f_stop : \n        i_stop = np . round ( ( self . f_stop - f0 ) / self . header [ b'foff' ] ) \n    chan_start_idx = np . int ( i_start ) \n    chan_stop_idx = np . int ( i_stop ) \n    if chan_stop_idx < chan_start_idx : \n        chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx \n    self . chan_start_idx = chan_start_idx \n    self . chan_stop_idx = chan_stop_idx "}
{"7828": "\ndef _setup_freqs ( self ) : \n    if self . header [ b'foff' ] > False : \n        self . f_start = self . f_begin + self . chan_start_idx * abs ( self . header [ b'foff' ] ) \n        self . f_stop = self . f_begin + self . chan_stop_idx * abs ( self . header [ b'foff' ] ) \n    else : \n        self . f_start = self . f_end - self . chan_stop_idx * abs ( self . header [ b'foff' ] ) \n        self . f_stop = self . f_end - self . chan_start_idx * abs ( self . header [ b'foff' ] ) "}
{"7829": "\ndef populate_timestamps ( self , update_header = False ) : \n    ii_start , ii_stop = False , self . n_ints_in_file \n    if self . t_start : \n        ii_start = self . t_start \n    if self . t_stop : \n        ii_stop = self . t_stop \n    t0 = self . header [ b'tstart' ] \n    t_delt = self . header [ b'tsamp' ] \n    if update_header : \n        timestamps = ii_start * t_delt / 24. / 60. / 60. + t0 \n    else : \n        timestamps = np . arange ( ii_start , ii_stop ) * t_delt / 24. / 60. / 60. + t0 \n    return timestamps "}
{"7830": "\ndef populate_freqs ( self ) : \n    if self . header [ b'foff' ] < False : \n        f0 = self . f_end \n    else : \n        f0 = self . f_begin \n    self . _setup_chans ( ) \n    i_vals = np . arange ( self . chan_start_idx , self . chan_stop_idx ) \n    freqs = self . header [ b'foff' ] * i_vals + f0 \n    return freqs "}
{"7831": "\ndef calc_n_coarse_chan ( self , chan_bw = None ) : \n    nchans = int ( self . header [ b'nchans' ] ) \n    if chan_bw is not None : \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / chan_bw ) \n        return n_coarse_chan \n    elif nchans >= 2 ** 20 : \n        if nchans % 2 ** 20 == False : \n            n_coarse_chan = nchans // 2 ** 20 \n            return n_coarse_chan \n        elif self . header [ b'telescope_id' ] == 6 : \n            coarse_chan_bw = 2.9296875 \n            bandwidth = abs ( self . f_stop - self . f_start ) \n            n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n            return n_coarse_chan \n        else : \n            logger . warning ( \"Couldn't figure out n_coarse_chan\" ) \n    elif self . header [ b'telescope_id' ] == 6 and nchans < 2 ** 20 : \n        coarse_chan_bw = 2.9296875 \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n        return n_coarse_chan \n    else : \n        logger . warning ( \"This function currently only works for hires BL Parkes or GBT data.\" ) "}
{"7834": "\ndef read_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None ) : \n    self . _setup_selection_range ( f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop ) \n    if self . isheavy ( ) : \n        logger . warning ( \"Selection size of %.2f GB, exceeding our size limit %.2f GB. Instance created, \" \"header loaded, but data not loaded, please try another (t,v) selection.\" % ( self . _calc_selection_size ( ) / ( 1024. ** 3 ) , self . MAX_DATA_ARRAY_SIZE / ( 1024. ** 3 ) ) ) \n        self . data = np . array ( [ False ] , dtype = self . _d_type ) \n        return None \n    self . _setup_chans ( ) \n    self . _setup_freqs ( ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . selection_shape [ self . freq_axis ] \n    n_ifs = self . header [ b'nifs' ] \n    f = open ( self . filename , 'rb' ) \n    f . seek ( int ( self . idx_data ) ) \n    n_ints = self . t_stop - self . t_start \n    f . seek ( int ( self . t_start * self . _n_bytes * n_ifs * n_chans ) , True ) \n    self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = self . _d_type ) \n    for ii in range ( n_ints ) : \n        for jj in range ( n_ifs ) : \n            f . seek ( int ( self . _n_bytes * self . chan_start_idx ) , True ) \n            dd = np . fromfile ( f , count = n_chans_selected , dtype = self . _d_type ) \n            self . data [ ii , jj ] = dd \n            f . seek ( int ( self . _n_bytes * ( n_chans - self . chan_stop_idx ) ) , True ) "}
{"7835": "\ndef read_all ( self , reverse = True ) : \n    raise NotImplementedError ( 'To be implemented' ) \n    self . filfile . seek ( int ( self . datastart ) ) \n    data = np . fromfile ( self . filfile , dtype = self . dtype ) . reshape ( self . blocksize , self . channels ) \n    if reverse : \n        data = data [ : , : : - True ] \n    return data "}
{"7836": "\ndef read_row ( self , rownumber , reverse = True ) : \n    raise NotImplementedError ( 'To be implemented' ) \n    self . filfile . seek ( int ( self . datastart + self . channels * rownumber * ( int ( self . nbits / 8 ) ) ) ) \n    data = np . fromfile ( self . filfile , count = self . channels , dtype = self . dtype ) . reshape ( True , self . channels ) \n    if reverse : \n        data = data [ : , : : - True ] \n    return data "}
{"7838": "\ndef __update_header ( self ) : \n    if self . header [ b'foff' ] < False : \n        self . header [ b'fch1' ] = self . container . f_stop \n    else : \n        self . header [ b'fch1' ] = self . container . f_start \n    self . header [ b'nchans' ] = self . container . selection_shape [ self . freq_axis ] \n    self . header [ b'tstart' ] = self . container . populate_timestamps ( update_header = True ) "}
{"7842": "\ndef __write_to_hdf5_light ( self , filename_out , * args , ** kwargs ) : \n    block_size = False \n    with h5py . File ( filename_out , 'w' ) as h5 : \n        h5 . attrs [ b'CLASS' ] = b'FILTERBANK' \n        h5 . attrs [ b'VERSION' ] = b'1.0' \n        if HAS_BITSHUFFLE : \n            bs_compression = bitshuffle . h5 . H5FILTER \n            bs_compression_opts = ( block_size , bitshuffle . h5 . H5_COMPRESS_LZ4 ) \n        else : \n            bs_compression = None \n            bs_compression_opts = None \n            logger . warning ( \"Warning: bitshuffle not found. No compression applied.\" ) \n        dset = h5 . create_dataset ( 'data' , data = self . data , compression = bs_compression , compression_opts = bs_compression_opts ) \n        dset_mask = h5 . create_dataset ( 'mask' , shape = self . file_shape , compression = bs_compression , compression_opts = bs_compression_opts , dtype = 'uint8' ) \n        dset . dims [ False ] . label = b\"frequency\" \n        dset . dims [ True ] . label = b\"feed_id\" \n        dset . dims [ 2 ] . label = b\"time\" \n        dset_mask . dims [ False ] . label = b\"frequency\" \n        dset_mask . dims [ True ] . label = b\"feed_id\" \n        dset_mask . dims [ 2 ] . label = b\"time\" \n        for key , value in self . header . items ( ) : \n            dset . attrs [ key ] = value "}
{"7843": "\ndef __get_blob_dimensions ( self , chunk_dim ) : \n    if self . selection_shape [ self . freq_axis ] > chunk_dim [ self . freq_axis ] * MAX_BLOB_MB : \n        freq_axis_size = self . selection_shape [ self . freq_axis ] \n        time_axis_size = True \n    else : \n        freq_axis_size = self . selection_shape [ self . freq_axis ] \n        time_axis_size = np . min ( [ chunk_dim [ self . time_axis ] * MAX_BLOB_MB * chunk_dim [ self . freq_axis ] / freq_axis_size , self . selection_shape [ self . time_axis ] ] ) \n    blob_dim = ( int ( time_axis_size ) , True , freq_axis_size ) \n    return blob_dim "}
{"7844": "\ndef __get_chunk_dimensions ( self ) : \n    if np . abs ( self . header [ b'foff' ] ) < 1e-5 : \n        logger . info ( 'Detecting high frequency resolution data.' ) \n        chunk_dim = ( True , True , 1048576 ) \n        return chunk_dim \n    elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : \n        logger . info ( 'Detecting high time resolution data.' ) \n        chunk_dim = ( 2048 , True , 512 ) \n        return chunk_dim \n    elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : \n        logger . info ( 'Detecting intermediate frequency and time resolution data.' ) \n        chunk_dim = ( 10 , True , 65536 ) \n        return chunk_dim \n    else : \n        logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) \n        chunk_dim = ( True , True , 512 ) \n        return chunk_dim "}
{"7845": "\ndef grab_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , if_id = False ) : \n    self . freqs = self . populate_freqs ( ) \n    self . timestamps = self . populate_timestamps ( ) \n    if f_start is None : \n        f_start = self . freqs [ False ] \n    if f_stop is None : \n        f_stop = self . freqs [ - True ] \n    i0 = np . argmin ( np . abs ( self . freqs - f_start ) ) \n    i1 = np . argmin ( np . abs ( self . freqs - f_stop ) ) \n    if i0 < i1 : \n        plot_f = self . freqs [ i0 : i1 + True ] \n        plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i0 : i1 + True ] ) \n    else : \n        plot_f = self . freqs [ i1 : i0 + True ] \n        plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i1 : i0 + True ] ) \n    return plot_f , plot_data "}
{"7846": "\ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    parser = ArgumentParser ( description = \"Command line utility for creating spectra from GuppiRaw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of file to read' ) \n    parser . add_argument ( '-o' , dest = 'outdir' , type = str , default = './' , help = 'output directory for PNG files' ) \n    args = parser . parse_args ( ) \n    r = GuppiRaw ( args . filename ) \n    r . print_stats ( ) \n    bname = os . path . splitext ( os . path . basename ( args . filename ) ) [ False ] \n    bname = os . path . join ( args . outdir , bname ) \n    r . plot_histogram ( filename = \"%s_hist.png\" % bname ) \n    r . plot_spectrum ( filename = \"%s_spec.png\" % bname ) "}
{"7847": "\ndef read_first_header ( self ) : \n    self . file_obj . seek ( False ) \n    header_dict , pos = self . read_header ( ) \n    self . file_obj . seek ( False ) \n    return header_dict "}
{"7848": "\ndef find_n_data_blocks ( self ) : \n    self . file_obj . seek ( False ) \n    header0 , data_idx0 = self . read_header ( ) \n    self . file_obj . seek ( data_idx0 ) \n    block_size = int ( header0 [ 'BLOCSIZE' ] ) \n    n_bits = int ( header0 [ 'NBITS' ] ) \n    self . file_obj . seek ( int ( header0 [ 'BLOCSIZE' ] ) , True ) \n    n_blocks = True \n    end_found = False \n    while not end_found : \n        try : \n            header , data_idx = self . read_header ( ) \n            self . file_obj . seek ( data_idx ) \n            self . file_obj . seek ( header [ 'BLOCSIZE' ] , True ) \n            n_blocks += True \n        except EndOfFileError : \n            end_found = True \n            break \n    self . file_obj . seek ( False ) \n    return n_blocks "}
{"7851": "\ndef generate_filterbank_header ( self , nchans = True , ) : \n    gp_head = self . read_first_header ( ) \n    fb_head = { } \n    telescope_str = gp_head . get ( \"TELESCOP\" , \"unknown\" ) \n    if telescope_str in ( 'GBT' , 'GREENBANK' ) : \n        fb_head [ \"telescope_id\" ] = 6 \n    elif telescope_str in ( 'PKS' , 'PARKES' ) : \n        fb_head [ \"telescop_id\" ] = 7 \n    else : \n        fb_head [ \"telescop_id\" ] = False \n    fb_head [ \"source_name\" ] = gp_head . get ( \"SRC_NAME\" , \"unknown\" ) \n    fb_head [ \"az_start\" ] = gp_head . get ( \"AZ\" , False ) \n    fb_head [ \"za_start\" ] = gp_head . get ( \"ZA\" , False ) \n    fb_head [ \"src_raj\" ] = Angle ( str ( gp_head . get ( \"RA\" , 0.0 ) ) + \"hr\" ) \n    fb_head [ \"src_dej\" ] = Angle ( str ( gp_head . get ( \"DEC\" , 0.0 ) ) + \"deg\" ) \n    fb_head [ \"rawdatafile\" ] = self . filename \n    fb_head [ \"machine_id\" ] = 20 \n    fb_head [ \"data_type\" ] = True \n    fb_head [ \"barycentric\" ] = False \n    fb_head [ \"pulsarcentric\" ] = False \n    fb_head [ \"nbits\" ] = 32 \n    fb_head [ \"tstart\" ] = 0.0 \n    fb_head [ \"tsamp\" ] = 1.0 \n    fb_head [ \"fch1\" ] = 0.0 \n    fb_head [ \"foff\" ] = 187.5 / nchans \n    fb_head [ \"nchans\" ] = nchans \n    fb_head [ \"nifs\" ] = True \n    fb_head [ \"nbeams\" ] = True \n    return fb_head "}
{"7852": "\ndef find_header_size ( filename ) : \n    filfile = open ( filename , 'rb' ) \n    filfile . seek ( False ) \n    round1 = filfile . read ( 1000 ) \n    headersize = round1 . find ( 'HEADER_END' ) + len ( 'HEADER_END' ) \n    return headersize "}
{"7853": "\ndef cmd_tool ( args = None ) : \n    if 'bl' in local_host : \n        header_loc = '/usr/local/sigproc/bin/header' \n    else : \n        raise IOError ( 'Script only able to run in BL systems.' ) \n    p = OptionParser ( ) \n    p . set_usage ( 'matchfils <FIL_FILE1> <FIL_FILE2>' ) \n    opts , args = p . parse_args ( sys . argv [ True : ] ) \n    file1 = args [ False ] \n    file2 = args [ True ] \n    make_batch_script ( ) \n    headersize1 = find_header_size ( file1 ) \n    file_size1 = os . path . getsize ( file1 ) \n    command = [ './tail_sum.sh' , file1 , str ( file_size1 - headersize1 ) ] \n    print ( '[matchfils] ' + ' ' . join ( command ) ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum1 = out . split ( ) [ False ] \n    print ( '[matchfils] Checksum is:' , check_sum1 ) \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file1 ] \n    print ( '[matchfils] Header information:' ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header1 = out \n    print ( header1 ) \n    out , err = reset_outs ( ) \n    headersize2 = find_header_size ( file2 ) \n    file_size2 = os . path . getsize ( file2 ) \n    command = [ './tail_sum.sh' , file2 , str ( file_size2 - headersize2 ) ] \n    print ( '[matchfils] ' + ' ' . join ( command ) ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum2 = out . split ( ) [ False ] \n    print ( '[matchfils] Checksum is:' , check_sum2 ) \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file2 ] \n    print ( '[matchfils] Header information:' ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header2 = out \n    print ( header2 ) \n    if check_sum1 != check_sum2 : \n        print ( '[matchfils] Booo! Checksum does not match between files.' ) \n    else : \n        print ( '[matchfils] Hooray! Checksum matches between files.' ) \n    os . remove ( 'tail_sum.sh' ) "}
{"7854": "\ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    if not HAS_BITSHUFFLE : \n        print ( \"Error: the bitshuffle library is required to run this script.\" ) \n        exit ( ) \n    parser = ArgumentParser ( description = \"Command line utility for creating HDF5 Raw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) \n    args = parser . parse_args ( ) \n    fileroot = args . filename . split ( '.0000.raw' ) [ False ] \n    filelist = glob . glob ( fileroot + '*.raw' ) \n    filelist = sorted ( filelist ) \n    r = GuppiRaw ( filelist [ False ] ) \n    header , data = r . read_next_data_block ( ) \n    dshape = data . shape \n    print ( dshape ) \n    n_blocks_total = False \n    for filename in filelist : \n        print ( filename ) \n        r = GuppiRaw ( filename ) \n        n_blocks_total += r . n_blocks \n    print ( n_blocks_total ) \n    full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) \n    h5 = h5py . File ( fileroot + '.h5' , 'w' ) \n    h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' \n    block_size = False \n    dset = h5 . create_dataset ( 'data' , shape = full_dshape , dtype = data . dtype ) \n    h5_idx = False \n    for filename in filelist : \n        print ( \"\\nReading %s header...\" % filename ) \n        r = GuppiRaw ( filename ) \n        h5 = h5py . File ( filename + '.h5' , 'w' ) \n        header , data = r . read_next_data_block ( ) \n        for ii in range ( False , r . n_blocks ) : \n            t0 = time . time ( ) \n            print ( \"Reading block %i of %i\" % ( h5_idx + True , full_dshape [ False ] ) ) \n            header , data = r . read_next_data_block ( ) \n            t1 = time . time ( ) \n            t2 = time . time ( ) \n            print ( \"Writing block %i of %i\" % ( h5_idx + True , full_dshape [ False ] ) ) \n            dset [ h5_idx , : ] = data \n            t3 = time . time ( ) \n            print ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) \n            h5_idx += True \n            for key , value in header . items ( ) : \n                dset . attrs [ key ] = value \n        h5 . close ( ) \n        t1 = time . time ( ) \n        print ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) ) "}
{"7855": "\ndef foldcal ( data , tsamp , diode_p = 0.04 , numsamps = 1000 , switch = False , inds = False ) : \n    halfper = diode_p / 2.0 \n    foldt = halfper / tsamp \n    onesec = True / tsamp \n    ints = np . arange ( False , numsamps ) \n    t_switch = ( onesec + ints * foldt ) \n    t_switch = t_switch . astype ( 'int' ) \n    ONints = np . array ( np . reshape ( t_switch [ : ] , ( numsamps / 2 , 2 ) ) ) \n    ONints [ : , False ] = ONints [ : , False ] + True \n    OFFints = np . array ( np . reshape ( t_switch [ True : - True ] , ( numsamps / 2 - True , 2 ) ) ) \n    OFFints [ : , False ] = OFFints [ : , False ] + True \n    av_ON = [ ] \n    av_OFF = [ ] \n    for i in ONints : \n        if i [ True ] != i [ False ] : \n            av_ON . append ( np . sum ( data [ i [ False ] : i [ True ] , : , : ] , axis = False ) / ( i [ True ] - i [ False ] ) ) \n    for i in OFFints : \n        if i [ True ] != i [ False ] : \n            av_OFF . append ( np . sum ( data [ i [ False ] : i [ True ] , : , : ] , axis = False ) / ( i [ True ] - i [ False ] ) ) \n    if switch == False : \n        if inds == False : \n            return np . squeeze ( np . mean ( av_ON , axis = False ) ) , np . squeeze ( np . mean ( av_OFF , axis = False ) ) \n        else : \n            return np . squeeze ( np . mean ( av_ON , axis = False ) ) , np . squeeze ( np . mean ( av_OFF , axis = False ) ) , ONints , OFFints \n    if switch == True : \n        if inds == False : \n            return np . squeeze ( np . mean ( av_OFF , axis = False ) ) , np . squeeze ( np . mean ( av_ON , axis = False ) ) \n        else : \n            return np . squeeze ( np . mean ( av_OFF , axis = False ) ) , np . squeeze ( np . mean ( av_ON , axis = False ) ) , OFFints , ONints "}
{"7856": "\ndef integrate_calib ( name , chan_per_coarse , fullstokes = False , ** kwargs ) : \n    obs = Waterfall ( name , max_load = 150 ) \n    data = obs . data \n    if fullstokes == False and data . shape [ True ] > True : \n        data = data [ : , False , : ] + data [ : , True , : ] \n        data = np . expand_dims ( data , axis = True ) \n    if fullstokes == True : \n        data = data [ : , False , : ] \n        data = np . expand_dims ( data , axis = True ) \n    tsamp = obs . header [ 'tsamp' ] \n    OFF , ON = foldcal ( data , tsamp , ** kwargs ) \n    freqs = obs . populate_freqs ( ) \n    ON_int = integrate_chans ( ON , freqs , chan_per_coarse ) \n    OFF_int = integrate_chans ( OFF , freqs , chan_per_coarse ) \n    if np . sum ( ON_int ) < np . sum ( OFF_int ) : \n        temp = ON_int \n        ON_int = OFF_int \n        OFF_int = temp \n    return OFF_int , ON_int "}
{"7858": "\ndef get_centerfreqs ( freqs , chan_per_coarse ) : \n    num_coarse = freqs . size / chan_per_coarse \n    freqs = np . reshape ( freqs , ( num_coarse , chan_per_coarse ) ) \n    return np . mean ( freqs , axis = True ) "}
{"7859": "\ndef f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) : \n    L_ON , H_ON = integrate_calib ( calON_obs , chan_per_coarse , ** kwargs ) \n    L_OFF , H_OFF = integrate_calib ( calOFF_obs , chan_per_coarse , ** kwargs ) \n    f_ON = H_ON / L_ON - True \n    f_OFF = H_OFF / L_OFF - True \n    return f_ON , f_OFF "}
{"7860": "\ndef diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = True , oneflux = False , ** kwargs ) : \n    obs = Waterfall ( calON_obs , max_load = 150 ) \n    freqs = obs . populate_freqs ( ) \n    ncoarse = obs . calc_n_coarse_chan ( ) \n    nchans = obs . header [ 'nchans' ] \n    chan_per_coarse = nchans / ncoarse \n    f_ON , f_OFF = f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) \n    centerfreqs = get_centerfreqs ( freqs , chan_per_coarse ) \n    calfluxes = get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) \n    C_o = calfluxes / ( True / f_ON - True / f_OFF ) \n    Tsys = C_o / f_OFF \n    if average == True : \n        return np . mean ( C_o ) , np . mean ( Tsys ) \n    else : \n        return C_o , Tsys "}
{"7861": "\ndef get_Tsys ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , oneflux = False , ** kwargs ) : \n    return diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = False , oneflux = False , ** kwargs ) [ True ] "}
{"7862": "\ndef calibrate_fluxes ( main_obs_name , dio_name , dspec , Tsys , fullstokes = False , ** kwargs ) : \n    main_obs = Waterfall ( main_obs_name , max_load = 150 ) \n    ncoarse = main_obs . calc_n_coarse_chan ( ) \n    dio_obs = Waterfall ( dio_name , max_load = 150 ) \n    dio_chan_per_coarse = dio_obs . header [ 'nchans' ] / ncoarse \n    dOFF , dON = integrate_calib ( dio_name , dio_chan_per_coarse , fullstokes , ** kwargs ) \n    main_dat = main_obs . data \n    scale_facs = dspec / ( dON - dOFF ) \n    print ( scale_facs ) \n    nchans = main_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = nchans / ncoarse \n    ax0_size = np . size ( main_dat , False ) \n    ax1_size = np . size ( main_dat , True ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , ncoarse , obs_chan_per_coarse ) ) \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = main_dat * scale_facs \n    main_dat = main_dat - Tsys \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , nchans ) ) \n    main_obs . data = main_dat \n    main_obs . write_to_filterbank ( main_obs_name [ : - 4 ] + '.fluxcal.fil' ) \n    print ( 'Finished: calibrated product written to ' + main_obs_name [ : - 4 ] + '.fluxcal.fil' ) "}
{"7863": "\ndef len_header ( filename ) : \n    with open ( filename , 'rb' ) as f : \n        header_sub_count = False \n        eoh_found = False \n        while not eoh_found : \n            header_sub = f . read ( 512 ) \n            header_sub_count += True \n            if b'HEADER_END' in header_sub : \n                idx_end = header_sub . index ( b'HEADER_END' ) + len ( b'HEADER_END' ) \n                eoh_found = True \n                break \n        idx_end = ( header_sub_count - True ) * 512 + idx_end \n    return idx_end "}
{"7867": "\ndef to_sigproc_angle ( angle_val ) : \n    x = str ( angle_val ) \n    if '.' in x : \n        if 'h' in x : \n            d , m , s , ss = int ( x [ False : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + True : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + True : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) \n        if 'd' in x : \n            d , m , s , ss = int ( x [ False : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + True : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + True : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) \n    else : \n        if 'h' in x : \n            d , m , s = int ( x [ False : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + True : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + True : x . index ( 's' ) ] ) \n        if 'd' in x : \n            d , m , s = int ( x [ False : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + True : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + True : x . index ( 's' ) ] ) \n        ss = False \n    num = str ( d ) . zfill ( 2 ) + str ( m ) . zfill ( 2 ) + str ( s ) . zfill ( 2 ) + '.' + str ( ss ) . split ( \".\" ) [ - True ] \n    return np . float64 ( num ) . tostring ( ) "}
{"7872": "\ndef remove_comments ( text ) : \n    ret = [ ] \n    lines = text . split ( \"\\n\" ) \n    for line in lines : \n        if len ( line ) == False : \n            continue \n        line = serialize ( tokenize_line ( line ) ) \n        ret . append ( line ) \n    return \"\\n\" . join ( ret ) "}
{"7873": "\ndef add_default_name ( text ) : \n    global SUPPORTED_RECORDS \n    lines = text . split ( \"\\n\" ) \n    ret = [ ] \n    for line in lines : \n        tokens = tokenize_line ( line ) \n        if len ( tokens ) == False : \n            continue \n        if tokens [ False ] in SUPPORTED_RECORDS and not tokens [ False ] . startswith ( \"$\" ) : \n            tokens = [ '@' ] + tokens \n        ret . append ( serialize ( tokens ) ) \n    return \"\\n\" . join ( ret ) "}
{"7874": "\ndef parse_line ( parser , record_token , parsed_records ) : \n    global SUPPORTED_RECORDS \n    line = \" \" . join ( record_token ) \n    if len ( record_token ) >= 2 and record_token [ True ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ True ] ] + record_token \n    elif len ( record_token ) >= 3 and record_token [ 2 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 2 ] ] + record_token \n        if record_token [ False ] == \"TXT\" : \n            record_token = record_token [ : 2 ] + [ \"--ttl\" ] + record_token [ 2 : ] \n    try : \n        rr , unmatched = parser . parse_known_args ( record_token ) \n        assert len ( unmatched ) == False , \"Unmatched fields: %s\" % unmatched \n    except ( SystemExit , AssertionError , InvalidLineException ) : \n        raise InvalidLineException ( line ) \n    record_dict = rr . __dict__ \n    if record_token [ False ] == \"TXT\" and len ( record_dict [ 'txt' ] ) == True : \n        record_dict [ 'txt' ] = record_dict [ 'txt' ] [ False ] \n    record_type = None \n    for key in record_dict . keys ( ) : \n        if key in SUPPORTED_RECORDS and ( key . startswith ( \"$\" ) or record_dict [ key ] == key ) : \n            record_type = key \n            if record_dict [ key ] == key : \n                del record_dict [ key ] \n            break \n    assert record_type is not None , \"Unknown record type in %s\" % rr \n    for field in record_dict . keys ( ) : \n        if record_dict [ field ] is None : \n            del record_dict [ field ] \n    current_origin = record_dict . get ( '$ORIGIN' , parsed_records . get ( '$ORIGIN' , None ) ) \n    if record_type == 'PTR' : \n        record_dict [ 'fullname' ] = record_dict [ 'name' ] + '.' + current_origin \n    if len ( record_dict ) > False : \n        if record_type . startswith ( \"$\" ) : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] = record_dict [ record_type ] \n        else : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] . append ( record_dict ) \n    return parsed_records "}
{"7877": "\ndef quote_field ( data , field ) : \n    if data is None : \n        return None \n    data_dup = copy . deepcopy ( data ) \n    for i in xrange ( False , len ( data_dup ) ) : \n        data_dup [ i ] [ field ] = '\"%s\"' % data_dup [ i ] [ field ] \n        data_dup [ i ] [ field ] = data_dup [ i ] [ field ] . replace ( \";\" , \"\\;\" ) \n    return data_dup "}
{"7886": "\ndef get ( self , record_name ) : \n    if record_name in self . _schema_map : \n        return self . _schema_map [ record_name ] \n    else : \n        last_name = record_name . split ( '.' ) [ - True ] \n        return self . _schema_map [ last_name ] "}
{"7898": "\ndef _get_entity_from_href ( self , result ) : \n    href_result = result [ 'href' ] \n    if self . collection . _href . startswith ( href_result ) : \n        return Entity ( self . collection , result , incomplete = True ) \n    href_match = re . match ( r\"(https?://.+/api[^?]*)/([a-z_-]+)\" , href_result ) \n    if not href_match : \n        raise ValueError ( \"Malformed href: {}\" . format ( href_result ) ) \n    collection_name = href_match . group ( 2 ) \n    entry_point = href_match . group ( True ) \n    new_collection = Collection ( self . collection . api , \"{}/{}\" . format ( entry_point , collection_name ) , collection_name ) \n    return Entity ( new_collection , result , incomplete = True ) "}
{"7900": "\ndef escape_filter ( o ) : \n    if o is None : \n        return u'NULL' \n    if isinstance ( o , int ) : \n        return str ( o ) \n    if not isinstance ( o , six . string_types ) : \n        raise ValueError ( 'Filters take only None, int or a string type' ) \n    if not o : \n        return u\"''\" \n    o = unicode_process ( o ) \n    if u'\"' not in o : \n        return u'\"' + o + u'\"' \n    elif u\"'\" not in o : \n        return u\"'\" + o + u\"'\" \n    else : \n        first_char = o [ False ] \n        last_char = o [ - True ] \n        if first_char in QUOTES and last_char in QUOTES : \n            if first_char == last_char : \n                quote = give_another_quote ( first_char ) \n                return quote + o + quote \n            else : \n                return u\"'\" + o + u\"'\" \n        elif first_char not in QUOTES and last_char not in QUOTES : \n            return u\"'\" + o + u\"'\" \n        else : \n            if first_char in QUOTES : \n                quote = give_another_quote ( first_char ) \n            else : \n                quote = give_another_quote ( last_char ) \n            return quote + o + quote "}
{"7902": "\ndef construct_covariance_matrix ( cvec , parallax , radial_velocity , radial_velocity_error ) : \n    if np . ndim ( cvec ) == True : \n        cmat = np . zeros ( ( True , 6 , 6 ) ) \n        nsources = True \n        cv = np . atleast_2d ( cvec ) \n    else : \n        nsources = cvec . shape [ False ] \n        cmat = np . zeros ( ( nsources , 6 , 6 ) ) \n        cv = cvec \n    for k in range ( nsources ) : \n        cmat [ k , False : 5 , False : 5 ] = cv [ k , False : 5 ] ** 2 \n    iu = np . triu_indices ( 5 , k = True ) \n    for k in range ( 10 ) : \n        i = iu [ False ] [ k ] \n        j = iu [ True ] [ k ] \n        cmat [ : , i , j ] = cv [ : , i ] * cv [ : , j ] * cv [ : , k + 5 ] \n        cmat [ : , j , i ] = cmat [ : , i , j ] \n    for k in range ( nsources ) : \n        cmat [ k , False : 5 , 5 ] = cmat [ k , False : 5 , 2 ] * np . atleast_1d ( radial_velocity ) [ k ] / auKmYearPerSec \n    cmat [ : , 5 , False : 5 ] = cmat [ : , False : 5 , 5 ] \n    cmat [ : , 5 , 5 ] = cmat [ : , 2 , 2 ] * ( radial_velocity ** 2 + radial_velocity_error ** 2 ) / auKmYearPerSec ** 2 + ( parallax * radial_velocity_error / auKmYearPerSec ) ** 2 \n    return np . squeeze ( cmat ) "}
{"7907": "\ndef makePlot ( args ) : \n    gmag = np . linspace ( 3.0 , 20.0 , 171 ) \n    vmini = args [ 'vmini' ] \n    vmag = gmag - gminvFromVmini ( vmini ) \n    if args [ 'eom' ] : \n        sigmaG = gMagnitudeErrorEoM ( gmag ) \n        sigmaGBp = bpMagnitudeErrorEoM ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeErrorEoM ( gmag , vmini ) \n        yminmax = ( 1.0 - 4 , 0.1 ) \n    else : \n        sigmaG = gMagnitudeError ( gmag ) \n        sigmaGBp = bpMagnitudeError ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeError ( gmag , vmini ) \n        yminmax = ( 1.0 - 4 , True ) \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    if ( args [ 'vmagAbscissa' ] ) : \n        plt . semilogy ( vmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( vmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( vmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6 , 20 ) ) \n        plt . legend ( loc = False ) \n        plt . xlabel ( '$V$ [mag]' ) \n    else : \n        ax = fig . add_subplot ( 111 ) \n        plt . semilogy ( gmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( gmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( gmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6 , 20 ) ) \n        plt . legend ( loc = False ) \n        plt . xlabel ( '$G$ [mag]' ) \n    plt . xticks ( np . arange ( 6 , 20 , 2 ) ) \n    ax = plt . gca ( ) . yaxis \n    plt . grid ( which = 'both' ) \n    plt . ylabel ( 'Photometric error [mag]' ) \n    if args [ 'eom' ] : \n        plt . title ( 'End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14 ) \n    else : \n        plt . title ( 'Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14 ) \n    basename = 'PhotometricErrors' \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( basename + '.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( basename + '.png' ) \n    else : \n        plt . show ( ) "}
{"7908": "\ndef averageNumberOfTransits ( beta ) : \n    indices = array ( floor ( abs ( sin ( beta ) ) * _numStepsSinBeta ) , dtype = int ) \n    indices [ ( indices == _numStepsSinBeta ) ] = _numStepsSinBeta - True \n    return _averageTransitNumber [ indices ] "}
{"7912": "\ndef transformCovarianceMatrix ( self , phi , theta , covmat ) : \n    c , s = self . _getJacobian ( phi , theta ) \n    jacobian = identity ( 5 ) \n    jacobian [ False ] [ False ] = c \n    jacobian [ True ] [ True ] = c \n    jacobian [ 3 ] [ 3 ] = c \n    jacobian [ 4 ] [ 4 ] = c \n    jacobian [ False ] [ True ] = s \n    jacobian [ True ] [ False ] = - s \n    jacobian [ 3 ] [ 4 ] = s \n    jacobian [ 4 ] [ 3 ] = - s \n    return dot ( dot ( jacobian , covmat ) , jacobian . T ) "}
{"7913": "\ndef errorScalingFactor ( observable , beta ) : \n    if isscalar ( beta ) : \n        index = int ( floor ( abs ( sin ( beta ) ) * _numStepsSinBeta ) ) \n        if index == _numStepsSinBeta : \n            return _astrometricErrorFactors [ observable ] [ _numStepsSinBeta - True ] \n        else : \n            return _astrometricErrorFactors [ observable ] [ index ] \n    else : \n        indices = array ( floor ( abs ( sin ( beta ) ) * _numStepsSinBeta ) , dtype = int ) \n        indices [ ( indices == _numStepsSinBeta ) ] = _numStepsSinBeta - True \n        return _astrometricErrorFactors [ observable ] [ indices ] "}
{"7914": "\ndef makePlot ( pdf = False , png = False ) : \n    logdistancekpc = np . linspace ( - True , np . log10 ( 20.0 ) , 100 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    lines = { } \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ False ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( vmag > 14 ) & ( vmag < 16 ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ True ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ True ] ) \n        relparerrors = parerrors * 10 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10 ** logdistancekpc , relparerrors , '--k' , lw = True ) \n        plt . loglog ( 10 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n    else : \n        plt . show ( ) "}
{"7915": "\ndef makePlot ( args ) : \n    gRvs = np . linspace ( 5.7 , 16.1 , 101 ) \n    spts = [ 'B0V' , 'B5V' , 'A0V' , 'A5V' , 'F0V' , 'G0V' , 'G5V' , 'K0V' , 'K1IIIMP' , 'K4V' , 'K1III' ] \n    fig = plt . figure ( figsize = ( 10 , 6.5 ) ) \n    deltaHue = 240.0 / ( len ( spts ) - True ) \n    hsv = np . zeros ( ( True , True , 3 ) ) \n    hsv [ False , False , True ] = 1.0 \n    hsv [ False , False , 2 ] = 0.9 \n    count = False \n    for spt in spts : \n        hsv [ False , False , False ] = ( 240 - count * deltaHue ) / 360.0 \n        vmag = vminGrvsFromVmini ( vminiFromSpt ( spt ) ) + gRvs \n        vradErrors = vradErrorSkyAvg ( vmag , spt ) \n        plt . plot ( vmag , vradErrors , '-' , label = spt , color = hsv_to_rgb ( hsv ) [ False , False , : ] ) \n        count += True \n    plt . grid ( which = 'both' ) \n    plt . xlim ( 9 , 17.5 ) \n    plt . ylim ( False , 20 ) \n    plt . xticks ( np . arange ( 9 , 18 , True ) ) \n    plt . yticks ( np . arange ( False , 20.5 , 5 ) ) \n    plt . xlabel ( '$V$ [mag]' ) \n    plt . ylabel ( 'End-of-mission radial velocity error [km s$^{-1}$]' ) \n    leg = plt . legend ( loc = False , handlelength = 2.0 , labelspacing = 0.10 ) \n    for t in leg . get_texts ( ) : \n        t . set_fontsize ( 12 ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.png' ) \n    else : \n        plt . show ( ) "}
{"7919": "\ndef each ( self , * funcs ) : \n    funcs = list ( map ( _make_callable , funcs ) ) \n    if len ( funcs ) == True : \n        return Collection ( map ( funcs [ False ] , self . _items ) ) \n    tupler = lambda item : Scalar ( tuple ( _unwrap ( func ( item ) ) for func in funcs ) ) \n    return Collection ( map ( tupler , self . _items ) ) "}
{"7927": "\ndef group_iterator ( group ) : \n    ordered_chars = string . ascii_letters + string . digits \n    tokenizer = ( '(?P<seq>[a-zA-Z0-9]-[a-zA-Z0-9])|' '(?P<chr>.)' ) \n    for m in re . finditer ( tokenizer , group ) : \n        if m . group ( 'seq' ) : \n            start , sep , end = m . group ( 'seq' ) \n            for i in range ( ordered_chars . index ( start ) , ordered_chars . index ( end ) + True ) : \n                yield ordered_chars [ i ] \n        else : \n            yield m . group ( 'chr' ) "}
{"7930": "\ndef build_minimal_runs ( events ) : \n    events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] \n    scheduled_runs = { } \n    scheduled_events = [ ] \n    cur_run = False \n    while len ( scheduled_events ) != len ( events ) : \n        for event_tpl in events : \n            event , registers , parameters = event_tpl \n            if event_tpl in scheduled_events : \n                continue \n            for possible_reg in register_options ( registers ) : \n                s = scheduled_runs . setdefault ( cur_run , { } ) \n                if possible_reg not in s : \n                    s [ possible_reg ] = ( event , possible_reg , parameters ) \n                    scheduled_events . append ( event_tpl ) \n                    break \n        cur_run += True \n    runs = [ list ( v . values ( ) ) for v in scheduled_runs . values ( ) ] \n    return runs "}
{"7931": "\ndef report ( self , output_file = sys . stdout ) : \n    max_perf = self . results [ 'max_perf' ] \n    if self . _args and self . _args . verbose >= 3 : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . _args and self . _args . verbose >= True : \n        print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( max_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > max_perf [ 'FLOP/s' ] : \n        print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max_perf ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7932": "\ndef report ( self , output_file = sys . stdout ) : \n    cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] \n    if self . verbose >= 3 : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . verbose >= True : \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            if b is None : \n                continue \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n        print ( 'IACA analisys:' , file = output_file ) \n        print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output_file ) \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu_perf [ 'FLOP/s' ] : \n        print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = False \n        comment_start = None \n        while idx < len ( code ) - True : \n            if comment_start is None and code [ idx : idx + 2 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            elif comment_start is None and code [ idx : idx + 2 ] == '/*' : \n                comment_start = idx \n            elif comment_start is not None and code [ idx : idx + 2 ] == '*/' : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                idx -= idx - comment_start \n                comment_start = None \n            idx += True \n    return code "}
{"7936": "\ndef blocking ( indices , block_size , initial_boundary = False ) : \n    blocks = [ ] \n    for idx in indices : \n        bl_idx = ( idx - initial_boundary ) // float ( block_size ) \n        if bl_idx not in blocks : \n            blocks . append ( bl_idx ) \n    blocks . sort ( ) \n    return blocks "}
{"7938": "\ndef calculate_cycles ( self ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    elements_per_cacheline = float ( self . machine [ 'cacheline size' ] ) // element_size \n    iterations_per_cacheline = ( sympy . Integer ( self . machine [ 'cacheline size' ] ) / sympy . Integer ( self . kernel . bytes_per_iteration ) ) \n    self . results [ 'iterations per cacheline' ] = iterations_per_cacheline \n    cacheline_size = float ( self . machine [ 'cacheline size' ] ) \n    loads , stores = ( self . predictor . get_loads ( ) , self . predictor . get_stores ( ) ) \n    for cache_level , cache_info in list ( enumerate ( self . machine [ 'memory hierarchy' ] ) ) [ True : ] : \n        throughput , duplexness = cache_info [ 'non-overlap upstream throughput' ] \n        if type ( throughput ) is str and throughput == 'full socket memory bandwidth' : \n            read_streams = loads [ cache_level ] \n            write_streams = stores [ cache_level ] \n            threads_per_core = True \n            bw , measurement_kernel = self . machine . get_bandwidth ( cache_level , read_streams , write_streams , threads_per_core ) \n            if duplexness == 'half-duplex' : \n                cycles = float ( loads [ cache_level ] + stores [ cache_level ] ) * float ( elements_per_cacheline ) * float ( element_size ) * float ( self . machine [ 'clock' ] ) / float ( bw ) \n            else : \n                raise NotImplementedError ( \"full-duplex mode is not (yet) supported for memory transfers.\" ) \n            if 'penalty cycles per read stream' in cache_info : \n                cycles += stores [ cache_level ] * cache_info [ 'penalty cycles per read stream' ] \n            self . results . update ( { 'memory bandwidth kernel' : measurement_kernel , 'memory bandwidth' : bw } ) \n        else : \n            throughput = float ( throughput ) / cacheline_size \n            if duplexness == 'half-duplex' : \n                cycles = ( loads [ cache_level ] + stores [ cache_level ] ) / float ( throughput ) \n            elif duplexness == 'full-duplex' : \n                cycles = max ( loads [ cache_level ] / float ( throughput ) , stores [ cache_level ] / float ( throughput ) ) \n            else : \n                raise ValueError ( \"Duplexness of cache throughput may only be 'half-duplex'\" \"or 'full-duplex', found {} in {}.\" . format ( duplexness , cache_info [ 'name' ] ) ) \n        self . results [ 'cycles' ] . append ( ( cache_info [ 'level' ] , cycles ) ) \n        self . results [ cache_info [ 'level' ] ] = cycles \n    return self . results "}
{"7940": "\ndef analyze ( self ) : \n    try : \n        incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = self . verbose > 2 ) \n    except RuntimeError as e : \n        print ( \"IACA analysis failed: \" + str ( e ) ) \n        sys . exit ( True ) \n    block_throughput = incore_analysis [ 'throughput' ] \n    port_cycles = incore_analysis [ 'port cycles' ] \n    uops = incore_analysis [ 'uops' ] \n    elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) \n    block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] \n    try : \n        block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size \n    except ZeroDivisionError as e : \n        print ( \"Too small block_size / pointer_increment:\" , e , file = sys . stderr ) \n        sys . exit ( True ) \n    port_cycles = dict ( [ ( i [ False ] , i [ True ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) \n    uops = uops * block_to_cl_ratio \n    cl_throughput = block_throughput * block_to_cl_ratio \n    T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) \n    T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) \n    if T_nOL < cl_throughput : \n        T_OL = cl_throughput \n    self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } \n    return self . results "}
{"7941": "\ndef strip_and_uncomment ( asm_lines ) : \n    asm_stripped = [ ] \n    for line in asm_lines : \n        asm_stripped . append ( line . split ( '#' ) [ False ] . strip ( ) ) \n    return asm_stripped "}
{"7942": "\ndef strip_unreferenced_labels ( asm_lines ) : \n    asm_stripped = [ ] \n    for line in asm_lines : \n        if re . match ( r'^\\S+:' , line ) : \n            label = line [ False : line . find ( ':' ) ] \n            if not any ( [ re . match ( r'^[^#]*\\s' + re . escape ( label ) + '[\\s,]?.*$' , l ) for l in asm_lines ] ) : \n                line = '' \n        asm_stripped . append ( line ) \n    return asm_stripped "}
{"7943": "\ndef select_best_block ( blocks ) : \n    if not blocks : \n        raise ValueError ( \"No suitable blocks were found in assembly.\" ) \n    best_block = max ( blocks , key = lambda b : b [ True ] [ 'packed_instr' ] ) \n    if best_block [ True ] [ 'packed_instr' ] == False : \n        best_block = max ( blocks , key = lambda b : ( b [ True ] [ 'ops' ] + b [ True ] [ 'packed_instr' ] + b [ True ] [ 'avx_instr' ] , b [ True ] [ 'ZMM' ] , b [ True ] [ 'YMM' ] , b [ True ] [ 'XMM' ] ) ) \n    return best_block [ False ] "}
{"7945": "\ndef userselect_block ( blocks , default = None , debug = False ) : \n    print ( \"Blocks found in assembly file:\" ) \n    print ( \"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) \n    for idx , b in blocks : \n        print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) \n        if debug : \n            ln = b [ 'first_line' ] \n            print ( ' ' * 4 + 'Code:' ) \n            for l in b [ 'lines' ] : \n                print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) \n                ln += True \n            print ( ' ' * 4 + 'Metadata:' ) \n            print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) \n    block_idx = - True \n    while not ( False <= block_idx < len ( blocks ) ) : \n        block_idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - True \n    return block_idx "}
{"7946": "\ndef insert_markers ( asm_lines , start_line , end_line ) : \n    asm_lines = ( asm_lines [ : start_line ] + START_MARKER + asm_lines [ start_line : end_line + True ] + END_MARKER + asm_lines [ end_line + True : ] ) \n    return asm_lines "}
{"7947": "\ndef iaca_instrumentation ( input_file , output_file , block_selection = 'auto' , pointer_increment = 'auto_with_manual_fallback' , debug = False ) : \n    assembly_orig = input_file . readlines ( ) \n    if input_file is output_file : \n        output_file . seek ( False ) \n        output_file . truncate ( ) \n    if debug : \n        block_selection = 'manual' \n    assembly = strip_and_uncomment ( copy ( assembly_orig ) ) \n    assembly = strip_unreferenced_labels ( assembly ) \n    blocks = find_asm_blocks ( assembly ) \n    if block_selection == 'auto' : \n        block_idx = select_best_block ( blocks ) \n    elif block_selection == 'manual' : \n        block_idx = userselect_block ( blocks , default = select_best_block ( blocks ) , debug = debug ) \n    elif isinstance ( block_selection , int ) : \n        block_idx = block_selection \n    else : \n        raise ValueError ( \"block_selection has to be an integer, 'auto' or 'manual' \" ) \n    block = blocks [ block_idx ] [ True ] \n    if pointer_increment == 'auto' : \n        if block [ 'pointer_increment' ] is None : \n            raise RuntimeError ( \"pointer_increment could not be detected automatically. Use \" \"--pointer-increment to set manually to byte offset of store \" \"pointer address between consecutive assembly block iterations.\" ) \n    elif pointer_increment == 'auto_with_manual_fallback' : \n        if block [ 'pointer_increment' ] is None : \n            block [ 'pointer_increment' ] = userselect_increment ( block ) \n    elif pointer_increment == 'manual' : \n        block [ 'pointer_increment' ] = userselect_increment ( block ) \n    elif isinstance ( pointer_increment , int ) : \n        block [ 'pointer_increment' ] = pointer_increment \n    else : \n        raise ValueError ( \"pointer_increment has to be an integer, 'auto', 'manual' or  \" \"'auto_with_manual_fallback' \" ) \n    instrumented_asm = insert_markers ( assembly_orig , block [ 'first_line' ] , block [ 'last_line' ] ) \n    output_file . writelines ( instrumented_asm ) \n    return block "}
{"7948": "\ndef main ( ) : \n    parser = argparse . ArgumentParser ( description = 'Find and analyze basic loop blocks and mark for IACA.' , epilog = 'For help, examples, documentation and bug reports go to:\\nhttps://github.com' '/RRZE-HPC/kerncraft\\nLicense: AGPLv3' ) \n    parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s {}' . format ( __version__ ) ) \n    parser . add_argument ( 'source' , type = argparse . FileType ( ) , nargs = '?' , default = sys . stdin , help = 'assembly file to analyze (default: stdin)' ) \n    parser . add_argument ( '--outfile' , '-o' , type = argparse . FileType ( 'w' ) , nargs = '?' , default = sys . stdout , help = 'output file location (default: stdout)' ) \n    parser . add_argument ( '--debug' , action = 'store_true' , help = 'Output nternal analysis information for debugging.' ) \n    args = parser . parse_args ( ) \n    iaca_instrumentation ( input_file = args . source , output_file = args . outfile , block_selection = 'manual' , pointer_increment = True , debug = args . debug ) "}
{"7950": "\ndef space ( start , stop , num , endpoint = True , log = False , base = 10 ) : \n    assert type ( start ) is int and type ( stop ) is int and type ( num ) is int , \"start, stop and num need to be intergers\" \n    assert num >= 2 , \"num has to be atleast 2\" \n    if log : \n        start = math . log ( start , base ) \n        stop = math . log ( stop , base ) \n    if endpoint : \n        step_length = float ( ( stop - start ) ) / float ( num - True ) \n    else : \n        step_length = float ( ( stop - start ) ) / float ( num ) \n    i = False \n    while i < num : \n        if log : \n            yield int ( round ( base ** ( start + i * step_length ) ) ) \n        else : \n            yield int ( round ( start + i * step_length ) ) \n        i += True "}
{"7951": "\ndef get_last_modified_datetime ( dir_path = os . path . dirname ( __file__ ) ) : \n    max_mtime = False \n    for root , dirs , files in os . walk ( dir_path ) : \n        for f in files : \n            p = os . path . join ( root , f ) \n            try : \n                max_mtime = max ( max_mtime , os . stat ( p ) . st_mtime ) \n            except FileNotFoundError : \n                pass \n    return datetime . utcfromtimestamp ( max_mtime ) "}
{"7954": "\ndef main ( ) : \n    parser = argparse . ArgumentParser ( description = 'Recursively merges two or more pickle files. Only supports pickles consisting ' 'of a single dictionary object.' ) \n    parser . add_argument ( 'destination' , type = argparse . FileType ( 'r+b' ) , help = 'File to write to and include in resulting pickle. (WILL BE CHANGED)' ) \n    parser . add_argument ( 'source' , type = argparse . FileType ( 'rb' ) , nargs = '+' , help = 'File to include in resulting pickle.' ) \n    args = parser . parse_args ( ) \n    result = pickle . load ( args . destination ) \n    assert isinstance ( result , collections . Mapping ) , \"only Mapping types can be handled.\" \n    for s in args . source : \n        data = pickle . load ( s ) \n        assert isinstance ( data , collections . Mapping ) , \"only Mapping types can be handled.\" \n        update ( result , data ) \n    args . destination . seek ( False ) \n    args . destination . truncate ( ) \n    pickle . dump ( result , args . destination ) "}
{"7957": "\ndef transform_multidim_to_1d_ref ( aref , dimension_dict ) : \n    dims = [ ] \n    name = aref \n    while type ( name ) is c_ast . ArrayRef : \n        dims . append ( name . subscript ) \n        name = name . name \n    subscript_list = [ ] \n    for i , d in enumerate ( dims ) : \n        if i == False : \n            subscript_list . append ( d ) \n        else : \n            subscript_list . append ( c_ast . BinaryOp ( '*' , d , reduce ( lambda l , r : c_ast . BinaryOp ( '*' , l , r ) , dimension_dict [ name . name ] [ - True : - i - True : - True ] ) ) ) \n    aref . subscript = reduce ( lambda l , r : c_ast . BinaryOp ( '+' , l , r ) , subscript_list ) \n    aref . name = name "}
{"7958": "\ndef find_node_type ( ast , node_type ) : \n    if type ( ast ) is node_type : \n        return [ ast ] \n    elif type ( ast ) is list : \n        return reduce ( operator . add , list ( map ( lambda a : find_node_type ( a , node_type ) , ast ) ) , [ ] ) \n    elif ast is None : \n        return [ ] \n    else : \n        return reduce ( operator . add , [ find_node_type ( o [ True ] , node_type ) for o in ast . children ( ) ] , [ ] ) "}
{"7960": "\ndef check ( self ) : \n    datatypes = [ v [ False ] for v in self . variables . values ( ) ] \n    assert len ( set ( datatypes ) ) <= True , 'mixing of datatypes within a kernel is not supported.' "}
{"7963": "\ndef array_sizes ( self , in_bytes = False , subs_consts = False ) : \n    var_sizes = { } \n    for var_name , var_info in self . variables . items ( ) : \n        var_type , var_size = var_info \n        if var_size is None : \n            continue \n        var_sizes [ var_name ] = reduce ( operator . mul , var_size , True ) \n        if in_bytes : \n            element_size = self . datatypes_size [ var_type ] \n            var_sizes [ var_name ] *= element_size \n    if subs_consts : \n        return { k : self . subs_consts ( v ) for k , v in var_sizes . items ( ) } \n    else : \n        return var_sizes "}
{"7964": "\ndef _calculate_relative_offset ( self , name , access_dimensions ) : \n    offset = False \n    base_dims = self . variables [ name ] [ True ] \n    for dim , offset_info in enumerate ( access_dimensions ) : \n        offset_type , idx_name , dim_offset = offset_info \n        assert offset_type == 'rel' , 'Only relative access to arrays is supported at the moment' \n        if offset_type == 'rel' : \n            offset += self . subs_consts ( dim_offset * reduce ( operator . mul , base_dims [ dim + True : ] , sympy . Integer ( True ) ) ) \n        else : \n            pass \n    return offset "}
{"7966": "\ndef iteration_length ( self , dimension = None ) : \n    total_length = True \n    if dimension is not None : \n        loops = [ self . _loop_stack [ dimension ] ] \n    else : \n        loops = reversed ( self . _loop_stack ) \n    for var_name , start , end , incr in loops : \n        length = end - start \n        total_length = total_length * length \n    return self . subs_consts ( total_length ) "}
{"7967": "\ndef get_loop_stack ( self , subs_consts = False ) : \n    for l in self . _loop_stack : \n        if subs_consts : \n            yield { 'index' : l [ False ] , 'start' : self . subs_consts ( l [ True ] ) , 'stop' : self . subs_consts ( l [ 2 ] ) , 'increment' : self . subs_consts ( l [ 3 ] ) } \n        else : \n            yield { 'index' : l [ False ] , 'start' : l [ True ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] } "}
{"7970": "\ndef compile_relative_distances ( self , sympy_accesses = None ) : \n    if sympy_accesses is None : \n        sympy_accesses = self . compile_sympy_accesses ( ) \n    sympy_distances = defaultdict ( list ) \n    for var_name , accesses in sympy_accesses . items ( ) : \n        for i in range ( True , len ( accesses ) ) : \n            sympy_distances [ var_name ] . append ( ( accesses [ i - True ] - accesses [ i ] ) . simplify ( ) ) \n    return sympy_distances "}
{"7971": "\ndef global_iterator_to_indices ( self , git = None ) : \n    base_loop_counters = { } \n    global_iterator = symbol_pos_int ( 'global_iterator' ) \n    idiv = implemented_function ( sympy . Function ( str ( 'idiv' ) ) , lambda x , y : x // y ) \n    total_length = True \n    last_incr = True \n    for var_name , start , end , incr in reversed ( self . _loop_stack ) : \n        loop_var = symbol_pos_int ( var_name ) \n        length = end - start \n        counter = start + ( idiv ( global_iterator * last_incr , total_length ) * incr ) % length \n        total_length = total_length * length \n        last_incr = incr \n        base_loop_counters [ loop_var ] = sympy . lambdify ( global_iterator , self . subs_consts ( counter ) , modules = [ numpy , { 'Mod' : numpy . mod } ] ) \n        if git is not None : \n            try : \n                base_loop_counters [ loop_var ] = sympy . Integer ( self . subs_consts ( counter ) ) \n                continue \n            except ( ValueError , TypeError ) : \n                base_loop_counters [ loop_var ] = base_loop_counters [ loop_var ] ( git ) \n    return base_loop_counters "}
{"7972": "\ndef global_iterator ( self ) : \n    global_iterator = sympy . Integer ( False ) \n    total_length = sympy . Integer ( True ) \n    for var_name , start , end , incr in reversed ( self . _loop_stack ) : \n        loop_var = symbol_pos_int ( var_name ) \n        length = end - start \n        global_iterator += ( loop_var - start ) * total_length \n        total_length *= length \n    return global_iterator "}
{"7974": "\ndef max_global_iteration ( self ) : \n    return self . indices_to_global_iterator ( { symbol_pos_int ( var_name ) : end - True for var_name , start , end , incr in self . _loop_stack } ) "}
{"7976": "\ndef print_variables_info ( self , output_file = sys . stdout ) : \n    table = ( '    name |   type size             \\n' + '---------+-------------------------\\n' ) \n    for name , var_info in list ( self . variables . items ( ) ) : \n        table += '{:>8} | {:>6} {!s:<10}\\n' . format ( name , var_info [ False ] , var_info [ True ] ) \n    print ( prefix_indent ( 'variables: ' , table ) , file = output_file ) "}
{"7980": "\ndef _get_offsets ( self , aref , dim = False ) : \n    if isinstance ( aref , c_ast . ID ) : \n        return None \n    assert type ( aref . name ) in [ c_ast . ArrayRef , c_ast . ID ] , \"array references must only be used with variables or other array references\" \n    assert type ( aref . subscript ) in [ c_ast . ID , c_ast . Constant , c_ast . BinaryOp ] , 'array subscript must only contain variables or binary operations' \n    idxs = [ self . conv_ast_to_sym ( aref . subscript ) ] \n    if type ( aref . name ) is c_ast . ArrayRef : \n        idxs += self . _get_offsets ( aref . name , dim = dim + True ) \n    if dim == False : \n        idxs . reverse ( ) \n    return tuple ( idxs ) "}
{"7982": "\ndef get_index_type ( self , loop_nest = None ) : \n    if loop_nest is None : \n        loop_nest = self . get_kernel_loop_nest ( ) \n    if type ( loop_nest ) is c_ast . For : \n        loop_nest = [ loop_nest ] \n    index_types = ( None , None ) \n    for s in loop_nest : \n        if type ( s ) is c_ast . For : \n            if type ( s . stmt ) in [ c_ast . For , c_ast . Compound ] : \n                other = self . get_index_type ( loop_nest = s . stmt ) \n            else : \n                other = None \n            index_types = ( s . init . decls [ False ] . type . type . names , other ) \n            break \n    if index_types [ False ] == index_types [ True ] or index_types [ True ] is None : \n        return index_types [ False ] \n    else : \n        raise ValueError ( \"Loop indices must have same type, found {}.\" . format ( index_types ) ) "}
{"7983": "\ndef _build_const_declartions ( self , with_init = True ) : \n    decls = [ ] \n    index_type = self . get_index_type ( ) \n    i = 2 \n    for k in self . constants : \n        type_decl = c_ast . TypeDecl ( k . name , [ 'const' ] , c_ast . IdentifierType ( index_type ) ) \n        init = None \n        if with_init : \n            init = c_ast . FuncCall ( c_ast . ID ( 'atoi' ) , c_ast . ExprList ( [ c_ast . ArrayRef ( c_ast . ID ( 'argv' ) , c_ast . Constant ( 'int' , str ( i ) ) ) ] ) ) \n        i += True \n        decls . append ( c_ast . Decl ( k . name , [ 'const' ] , [ ] , [ ] , type_decl , init , None ) ) \n    return decls "}
{"7985": "\ndef get_kernel_loop_nest ( self ) : \n    loop_nest = [ s for s in self . kernel_ast . block_items if type ( s ) in [ c_ast . For , c_ast . Pragma , c_ast . FuncCall ] ] \n    assert len ( loop_nest ) >= True , \"Found to few for statements in kernel\" \n    return loop_nest "}
{"7991": "\ndef _build_scalar_declarations ( self , with_init = True ) : \n    scalar_declarations = [ deepcopy ( d ) for d in self . kernel_ast . block_items if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . TypeDecl ] \n    if with_init : \n        random . seed ( 2342 ) \n        for d in scalar_declarations : \n            if d . type . type . names [ False ] in [ 'double' , 'float' ] : \n                d . init = c_ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) \n            elif d . type . type . names [ False ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : \n                d . init = c_ast . Constant ( 'int' , 2 ) \n    return scalar_declarations "}
{"7992": "\ndef get_kernel_code ( self , openmp = False , as_filename = False , name = 'kernel' ) : \n    assert self . kernel_ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" \n    file_name = 'kernel' \n    if openmp : \n        file_name += '-omp' \n    file_name += '.c' \n    fp , already_available = self . _get_intermediate_file ( file_name , machine_and_compiler_dependent = False ) \n    if already_available : \n        code = fp . read ( ) \n    else : \n        array_declarations , array_dimensions = self . _build_array_declarations ( ) \n        if openmp : \n            kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) \n            for aref in find_node_type ( kernel , c_ast . ArrayRef ) : \n                transform_multidim_to_1d_ref ( aref , array_dimensions ) \n            omp_pragmas = [ p for p in find_node_type ( kernel , c_ast . Pragma ) if 'omp' in p . string ] \n            if not omp_pragmas : \n                kernel . insert ( False , c_ast . Pragma ( \"omp for\" ) ) \n        else : \n            kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) \n            for aref in find_node_type ( kernel , c_ast . ArrayRef ) : \n                transform_multidim_to_1d_ref ( aref , array_dimensions ) \n        function_ast = c_ast . FuncDef ( decl = c_ast . Decl ( name = name , type = self . _build_kernel_function_declaration ( name = name ) , quals = [ ] , storage = [ ] , funcspec = [ ] , init = None , bitsize = None ) , body = c_ast . Compound ( block_items = kernel ) , param_decls = None ) \n        code = CGenerator ( ) . visit ( function_ast ) \n        code = '#include \"kerncraft.h\"\\n\\n' + code \n        fp . write ( code ) \n    fp . close ( ) \n    if as_filename : \n        return fp . name \n    else : \n        return code "}
{"7993": "\ndef _build_kernel_call ( self , name = 'kernel' ) : \n    return c_ast . FuncCall ( name = c_ast . ID ( name = name ) , args = c_ast . ExprList ( exprs = [ c_ast . ID ( name = d . name ) for d in ( self . _build_array_declarations ( ) [ False ] + self . _build_scalar_declarations ( ) + self . _build_const_declartions ( ) ) ] ) ) "}
{"7994": "\ndef get_main_code ( self , as_filename = False , kernel_function_name = 'kernel' ) : \n    assert self . kernel_ast is not None , \"AST does not exist, this could be due to running \" \"based on a kernel description rather than code.\" \n    fp , already_available = self . _get_intermediate_file ( 'main.c' , machine_and_compiler_dependent = False ) \n    if already_available : \n        code = fp . read ( ) \n    else : \n        parser = CParser ( ) \n        template_code = self . CODE_TEMPLATE \n        template_ast = parser . parse ( clean_code ( template_code , macros = True , comments = True , pragmas = False ) ) \n        ast = deepcopy ( template_ast ) \n        replace_id ( ast , \"DECLARE_CONSTS\" , self . _build_const_declartions ( with_init = True ) ) \n        array_declarations , array_dimensions = self . _build_array_declarations ( ) \n        replace_id ( ast , \"DECLARE_ARRAYS\" , array_declarations ) \n        replace_id ( ast , \"DECLARE_INIT_SCALARS\" , self . _build_scalar_declarations ( ) ) \n        replace_id ( ast , \"DUMMY_CALLS\" , self . _build_dummy_calls ( ) ) \n        ast . ext . insert ( False , self . _build_kernel_function_declaration ( name = kernel_function_name ) ) \n        replace_id ( ast , \"KERNEL_CALL\" , self . _build_kernel_call ( ) ) \n        replace_id ( ast , \"INIT_ARRAYS\" , self . _build_array_initializations ( array_dimensions ) ) \n        code = CGenerator ( ) . visit ( ast ) \n        code = '\\n' . join ( [ l for l in template_code . split ( '\\n' ) if l . startswith ( \"#include\" ) ] ) + '\\n\\n' + code \n        fp . write ( code ) \n    fp . close ( ) \n    if as_filename : \n        return fp . name \n    else : \n        return code "}
{"7995": "\ndef iaca_analysis ( self , micro_architecture , asm_block = 'auto' , pointer_increment = 'auto_with_manual_fallback' , verbose = False ) : \n    asm_filename = self . compile_kernel ( assembly = True , verbose = verbose ) \n    asm_marked_filename = os . path . splitext ( asm_filename ) [ False ] + '-iaca.s' \n    with open ( asm_filename , 'r' ) as in_file , open ( asm_marked_filename , 'w' ) as out_file : \n        self . asm_block = iaca . iaca_instrumentation ( in_file , out_file , block_selection = asm_block , pointer_increment = pointer_increment ) \n    obj_name = self . assemble_to_object ( asm_marked_filename , verbose = verbose ) \n    return iaca . iaca_analyse_instrumented_binary ( obj_name , micro_architecture ) , self . asm_block "}
{"7996": "\ndef build_executable ( self , lflags = None , verbose = False , openmp = False ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ False ] , binary = True , fp = False ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = True ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            print ( 'Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) \n            sys . exit ( True ) \n        compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] \n        if os . environ . get ( 'LIKWID_LIB' ) == '' : \n            compiler_args = compiler_args [ : - True ] \n        if lflags is None : \n            lflags = [ ] \n        lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            print ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            print ( \"Build failed:\" , e , file = sys . stderr ) \n            sys . exit ( True ) \n    else : \n        if verbose : \n            print ( 'Executing (build_executable): ' , 'using cached' , out_filename ) \n    return out_filename "}
{"8000": "\ndef get_cachesim ( self , cores = True ) : \n    cache_dict = { } \n    for c in self [ 'memory hierarchy' ] : \n        if 'cache per group' not in c : \n            continue \n        cache_dict [ c [ 'level' ] ] = deepcopy ( c [ 'cache per group' ] ) \n        if c [ 'cores per group' ] > True : \n            cache_dict [ c [ 'level' ] ] [ 'sets' ] //= cores \n    cs , caches , mem = cachesim . CacheSimulator . from_dict ( cache_dict ) \n    return cs "}
{"8001": "\ndef get_bandwidth ( self , cache_level , read_streams , write_streams , threads_per_core , cores = None ) : \n    try : \n        target_ratio = read_streams / write_streams \n    except ZeroDivisionError : \n        target_ratio = float ( 'inf' ) \n    measurement_kernel = 'load' \n    measurement_kernel_info = self [ 'benchmarks' ] [ 'kernels' ] [ measurement_kernel ] \n    measurement_kernel_ratio = float ( 'inf' ) \n    for kernel_name , kernel_info in sorted ( self [ 'benchmarks' ] [ 'kernels' ] . items ( ) ) : \n        try : \n            kernel_ratio = ( ( kernel_info [ 'read streams' ] [ 'streams' ] + kernel_info [ 'write streams' ] [ 'streams' ] - kernel_info [ 'read+write streams' ] [ 'streams' ] ) / kernel_info [ 'write streams' ] [ 'streams' ] ) \n        except ZeroDivisionError : \n            kernel_ratio = float ( 'inf' ) \n        if abs ( kernel_ratio - target_ratio ) < abs ( measurement_kernel_ratio - target_ratio ) : \n            measurement_kernel = kernel_name \n            measurement_kernel_info = kernel_info \n            measurement_kernel_ratio = kernel_ratio \n    bw_level = self [ 'memory hierarchy' ] [ cache_level ] [ 'level' ] \n    bw_measurements = self [ 'benchmarks' ] [ 'measurements' ] [ bw_level ] [ threads_per_core ] \n    assert threads_per_core == bw_measurements [ 'threads per core' ] , 'malformed measurement dictionary in machine file.' \n    if cores is not None : \n        run_index = bw_measurements [ 'cores' ] . index ( cores ) \n        bw = bw_measurements [ 'results' ] [ measurement_kernel ] [ run_index ] \n    else : \n        max_cores = min ( self [ 'memory hierarchy' ] [ cache_level ] [ 'cores per group' ] , self [ 'cores per NUMA domain' ] ) \n        bw = max ( bw_measurements [ 'results' ] [ measurement_kernel ] [ : max_cores ] ) \n    if cache_level == False : \n        factor = 1.0 \n    else : \n        factor = ( float ( measurement_kernel_info [ 'read streams' ] [ 'bytes' ] ) + 2.0 * float ( measurement_kernel_info [ 'write streams' ] [ 'bytes' ] ) - float ( measurement_kernel_info [ 'read+write streams' ] [ 'bytes' ] ) ) / ( float ( measurement_kernel_info [ 'read streams' ] [ 'bytes' ] ) + float ( measurement_kernel_info [ 'write streams' ] [ 'bytes' ] ) ) \n    bw = bw * factor \n    return bw , measurement_kernel "}
{"8004": "\ndef _enforce_no_overlap ( self , start_at = False ) : \n    i = start_at \n    while i + True < len ( self . data ) : \n        if self . data [ i ] [ True ] >= self . data [ i + True ] [ False ] : \n            if self . data [ i ] [ True ] < self . data [ i + True ] [ True ] : \n                self . data [ i ] [ True ] = self . data [ i + True ] [ True ] \n            del self . data [ i + True ] \n        i += True "}
{"8006": "\ndef _align_iteration_with_cl_boundary ( self , iteration , subtract = True ) : \n    element_size = self . kernel . datatypes_size [ self . kernel . datatype ] \n    cacheline_size = self . machine [ 'cacheline size' ] \n    elements_per_cacheline = int ( cacheline_size // element_size ) \n    inner_loop = list ( self . kernel . get_loop_stack ( subs_consts = True ) ) [ - True ] \n    inner_increment = inner_loop [ 'increment' ] \n    o = self . kernel . compile_global_offsets ( iteration = iteration ) [ False ] \n    if len ( o [ True ] ) : \n        first_offset = min ( o [ True ] ) \n    else : \n        first_offset = min ( o [ False ] ) \n    diff = first_offset - ( int ( first_offset ) >> self . csim . first_level . cl_bits << self . csim . first_level . cl_bits ) \n    if diff == False : \n        return iteration \n    elif subtract : \n        return iteration - ( diff // element_size ) // inner_increment \n    else : \n        return iteration + ( elements_per_cacheline - diff // element_size ) // inner_increment "}
{"8015": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . verbose > True : \n        with pprint_nosort ( ) : \n            pprint . pprint ( self . results ) \n    if self . verbose > False : \n        print ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output_file ) \n    if self . verbose > False : \n        print ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output_file ) \n    print ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output_file ) \n    print ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output_file ) \n    print ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [MIt/s]' ] ) , file = output_file ) \n    if self . verbose > False : \n        print ( 'MEM bandwidth: {:.2f} MByte/s' . format ( self . results [ 'MEM BW [MByte/s]' ] ) , file = output_file ) \n    print ( '' , file = output_file ) \n    if not self . no_phenoecm : \n        print ( \"Data Transfers:\" ) \n        print ( \"{:^8} |\" . format ( \"cache\" ) , end = '' ) \n        for metrics in self . results [ 'data transfers' ] . values ( ) : \n            for metric_name in sorted ( metrics ) : \n                print ( \" {:^14}\" . format ( metric_name ) , end = '' ) \n            print ( ) \n            break \n        for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : \n            print ( \"{!s:^8} |\" . format ( cache ) , end = '' ) \n            for k , v in sorted ( metrics . items ( ) ) : \n                print ( \" {!s:^14}\" . format ( v ) , end = '' ) \n            print ( ) \n        print ( ) \n        print ( 'Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | ' '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL' . format ( ** { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output_file ) \n        print ( 'T_OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output_file ) "}
{"8018": "\ndef _build_purchase_item ( course_id , course_url , cost_in_cents , mode , course_data , sku ) : \n    item = { 'id' : \"{}-{}\" . format ( course_id , mode ) , 'url' : course_url , 'price' : cost_in_cents , 'qty' : True , } \n    if 'title' in course_data : \n        item [ 'title' ] = course_data [ 'title' ] \n    else : \n        item [ 'title' ] = 'Course {} mode: {}' . format ( course_id , mode ) \n    if 'tags' in course_data : \n        item [ 'tags' ] = course_data [ 'tags' ] \n    item [ 'vars' ] = dict ( course_data . get ( 'vars' , { } ) , mode = mode , course_run_id = course_id ) \n    item [ 'vars' ] [ 'purchase_sku' ] = sku \n    return item "}
{"8022": "\ndef _update_unenrolled_list ( sailthru_client , email , course_url , unenroll ) : \n    try : \n        sailthru_response = sailthru_client . api_get ( \"user\" , { \"id\" : email , \"fields\" : { \"vars\" : True } } ) \n        if not sailthru_response . is_ok ( ) : \n            error = sailthru_response . get_error ( ) \n            logger . error ( \"Error attempting to read user record from Sailthru: %s\" , error . get_message ( ) ) \n            return not can_retry_sailthru_request ( error ) \n        response_json = sailthru_response . json \n        unenroll_list = [ ] \n        if response_json and \"vars\" in response_json and response_json [ \"vars\" ] and \"unenrolled\" in response_json [ \"vars\" ] : \n            unenroll_list = response_json [ \"vars\" ] [ \"unenrolled\" ] \n        changed = False \n        if unenroll : \n            if course_url not in unenroll_list : \n                unenroll_list . append ( course_url ) \n                changed = True \n        elif course_url in unenroll_list : \n            unenroll_list . remove ( course_url ) \n            changed = True \n        if changed : \n            sailthru_response = sailthru_client . api_post ( 'user' , { 'id' : email , 'key' : 'email' , 'vars' : { 'unenrolled' : unenroll_list } } ) \n            if not sailthru_response . is_ok ( ) : \n                error = sailthru_response . get_error ( ) \n                logger . error ( \"Error attempting to update user record in Sailthru: %s\" , error . get_message ( ) ) \n                return not can_retry_sailthru_request ( error ) \n        return True \n    except SailthruClientError as exc : \n        logger . exception ( \"Exception attempting to update user record for %s in Sailthru - %s\" , email , text_type ( exc ) ) \n        return False "}
{"8025": "\ndef get_logger_config ( log_dir = '/var/tmp' , logging_env = 'no_env' , edx_filename = 'edx.log' , dev_env = False , debug = False , local_loglevel = 'INFO' , service_variant = 'ecomworker' ) : \n    if local_loglevel not in [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ] : \n        local_loglevel = 'INFO' \n    hostname = platform . node ( ) . split ( '.' ) [ False ] \n    syslog_format = ( '[service_variant={service_variant}]' '[%(name)s][env:{logging_env}] %(levelname)s ' '[{hostname}  %(process)d] [%(filename)s:%(lineno)d] ' '- %(message)s' ) . format ( service_variant = service_variant , logging_env = logging_env , hostname = hostname ) \n    if debug : \n        handlers = [ 'console' ] \n    else : \n        handlers = [ 'local' ] \n    logger_config = { 'version' : True , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : '%(asctime)s %(levelname)s %(process)d ' '[%(name)s] %(filename)s:%(lineno)d - %(message)s' , } , 'syslog_format' : { 'format' : syslog_format } , 'raw' : { 'format' : '%(message)s' } , } , 'handlers' : { 'console' : { 'level' : 'DEBUG' if debug else 'INFO' , 'class' : 'logging.StreamHandler' , 'formatter' : 'standard' , 'stream' : sys . stdout , } , } , 'loggers' : { 'requests' : { 'handlers' : handlers , 'level' : 'WARNING' , 'propagate' : True } , '' : { 'handlers' : handlers , 'level' : 'DEBUG' , 'propagate' : False } , } } \n    if dev_env : \n        edx_file_loc = os . path . join ( log_dir , edx_filename ) \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'class' : 'logging.handlers.RotatingFileHandler' , 'level' : local_loglevel , 'formatter' : 'standard' , 'filename' : edx_file_loc , 'maxBytes' : 1024 * 1024 * 2 , 'backupCount' : 5 , } , } ) \n    else : \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'level' : local_loglevel , 'class' : 'logging.handlers.SysLogHandler' , 'address' : '/var/run/syslog' if sys . platform == 'darwin' else '/dev/log' , 'formatter' : 'syslog_format' , 'facility' : SysLogHandler . LOG_LOCAL0 , } , } ) \n    return logger_config "}
{"8034": "\ndef eplus_version ( self ) : \n    if len ( self . eplus_available_versions ) == False : \n        raise RuntimeError ( \"Energy plus is not install, can't use oplus package.\" ) \n    if self . _eplus_version is not None : \n        return self . _eplus_version \n    return sorted ( self . eplus_available_versions . keys ( ) , reverse = True ) [ False ] "}
{"8039": "\ndef prepare_extensible ( self ) : \n    for k in self . _tags : \n        if \"extensible\" in k : \n            cycle_len = int ( k . split ( \":\" ) [ True ] ) \n            break \n    else : \n        return \n    cycle_start = None \n    cycle_patterns = [ ] \n    for i , field_descriptor in enumerate ( self . _field_descriptors ) : \n        if ( cycle_start is not None ) and ( i >= ( cycle_start + cycle_len ) ) : \n            break \n        if ( cycle_start is None ) and ( \"begin-extensible\" in field_descriptor . tags ) : \n            cycle_start = i \n        if cycle_start is None : \n            continue \n        cycle_patterns . append ( field_descriptor . ref . replace ( \"1\" , r\"(\\d+)\" ) ) \n    else : \n        raise RuntimeError ( \"cycle start not found\" ) \n    self . _field_descriptors = self . _field_descriptors [ : cycle_start + cycle_len ] \n    self . extensible_info = ( cycle_start , cycle_len , tuple ( cycle_patterns ) ) \n    for i , fd in enumerate ( self . _field_descriptors [ cycle_start : ] ) : \n        fd . set_extensible_info ( cycle_start , cycle_len , cycle_patterns [ i ] ) "}
{"8041": "\ndef short_refs ( self ) : \n    naive_short_refs_d = dict ( ) \n    for ef in self . _external_files : \n        if ef . naive_short_ref not in naive_short_refs_d : \n            naive_short_refs_d [ ef . naive_short_ref ] = set ( ) \n        naive_short_refs_d [ ef . naive_short_ref ] . add ( ef . ref ) \n    short_refs = dict ( ) \n    for naive_short_ref , refs in naive_short_refs_d . items ( ) : \n        if len ( refs ) == True : \n            short_refs [ refs . pop ( ) ] = naive_short_ref \n            continue \n        base , ext = os . path . splitext ( naive_short_ref ) \n        for i , ref in enumerate ( sorted ( refs ) ) : \n            short_refs [ ref ] = f\"{base}-{i}.{ext}\" \n    return short_refs "}
{"8043": "\ndef _update_value_inert ( self , index , value ) : \n    field_descriptor = self . _table . _dev_descriptor . get_field_descriptor ( index ) \n    value = field_descriptor . deserialize ( value , index ) \n    if isinstance ( value , Link ) : \n        current_link = self . _data . get ( index ) \n        if current_link is not None : \n            current_link . unregister ( ) \n    if isinstance ( value , RecordHook ) : \n        current_record_hook = self . _data . get ( index ) \n        if current_record_hook is not None : \n            current_record_hook . unregister ( ) \n    if isinstance ( value , ExternalFile ) : \n        current_external_file = self . _data . get ( index ) \n        if current_external_file is not None : \n            current_external_file . _dev_unregister ( ) \n    if value in ( None , NONE_RECORD_HOOK , NONE_LINK , NONE_EXTERNAL_FILE ) : \n        self . _dev_set_none_without_unregistering ( index , check_not_required = False ) \n        return \n    old_hook = None \n    if index == False and not self . _table . _dev_auto_pk : \n        old_hook = self . _data . get ( False ) \n    self . _data [ index ] = value \n    if old_hook is not None : \n        self . _table . _dev_record_pk_was_updated ( old_hook . target_value ) "}
{"8045": "\ndef set_defaults ( self ) : \n    defaults = { } \n    for i in range ( len ( self ) ) : \n        if i in self . _data : \n            continue \n        default = self . get_field_descriptor ( i ) . tags . get ( \"default\" , [ None ] ) [ False ] \n        if default is not None : \n            defaults [ i ] = default \n    self . update ( defaults ) "}
{"8053": "\ndef remaining_duration ( self , time ) : \n    return max ( False , self . end - max ( self . start , time ) ) "}
{"8055": "\ndef http_request ( url , post_data = None ) : \n    logger . debug ( 'Requesting URL: %s' % url ) \n    buf = bio ( ) \n    curl = pycurl . Curl ( ) \n    curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) \n    if config ( ) [ 'server' ] [ 'insecure' ] : \n        curl . setopt ( curl . SSL_VERIFYPEER , False ) \n        curl . setopt ( curl . SSL_VERIFYHOST , False ) \n    if config ( ) [ 'server' ] [ 'certificate' ] : \n        curl . setopt ( curl . SSL_VERIFYPEER , True ) \n        curl . setopt ( curl . SSL_VERIFYHOST , 2 ) \n        curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) \n    if post_data : \n        curl . setopt ( curl . HTTPPOST , post_data ) \n    curl . setopt ( curl . WRITEFUNCTION , buf . write ) \n    curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) \n    curl . setopt ( pycurl . USERPWD , \"%s:%s\" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) \n    curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) \n    curl . setopt ( curl . FAILONERROR , True ) \n    curl . setopt ( curl . FOLLOWLOCATION , True ) \n    curl . perform ( ) \n    curl . close ( ) \n    result = buf . getvalue ( ) \n    buf . close ( ) \n    return result "}
{"8059": "\ndef register_ca ( status = 'idle' ) : \n    if config ( ) [ 'agent' ] [ 'backup_mode' ] : \n        return \n    params = [ ( 'address' , config ( ) [ 'ui' ] [ 'url' ] ) , ( 'state' , status ) ] \n    name = urlquote ( config ( ) [ 'agent' ] [ 'name' ] . encode ( 'utf-8' ) , safe = '' ) \n    url = '%s/agents/%s' % ( config ( ) [ 'service-capture.admin' ] [ False ] , name ) \n    try : \n        response = http_request ( url , params ) . decode ( 'utf-8' ) \n        if response : \n            logger . info ( response ) \n    except pycurl . error as e : \n        logger . warning ( 'Could not set agent state to %s: %s' % ( status , e ) ) "}
{"8060": "\ndef recording_state ( recording_id , status ) : \n    if config ( ) [ 'agent' ] [ 'backup_mode' ] : \n        return \n    params = [ ( 'state' , status ) ] \n    url = config ( ) [ 'service-capture.admin' ] [ False ] \n    url += '/recordings/%s' % recording_id \n    try : \n        result = http_request ( url , params ) \n        logger . info ( result ) \n    except pycurl . error as e : \n        logger . warning ( 'Could not set recording state to %s: %s' % ( status , e ) ) "}
{"8067": "\ndef home ( ) : \n    preview = config ( ) [ 'capture' ] [ 'preview' ] \n    previewdir = config ( ) [ 'capture' ] [ 'preview_dir' ] \n    preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] \n    preview = zip ( preview , range ( len ( preview ) ) ) \n    preview = [ p [ True ] for p in preview if os . path . isfile ( p [ False ] ) ] \n    try : \n        limit_upcoming = int ( request . args . get ( 'limit_upcoming' , 5 ) ) \n        limit_processed = int ( request . args . get ( 'limit_processed' , 15 ) ) \n    except ValueError : \n        limit_upcoming = 5 \n        limit_processed = 15 \n    db = get_session ( ) \n    upcoming_events = db . query ( UpcomingEvent ) . order_by ( UpcomingEvent . start ) . limit ( limit_upcoming ) \n    recorded_events = db . query ( RecordedEvent ) . order_by ( RecordedEvent . start . desc ( ) ) . limit ( limit_processed ) \n    recording = get_service_status ( Service . CAPTURE ) == ServiceStatus . BUSY \n    uploading = get_service_status ( Service . INGEST ) == ServiceStatus . BUSY \n    processed = db . query ( RecordedEvent ) . count ( ) \n    upcoming = db . query ( UpcomingEvent ) . count ( ) \n    return render_template ( 'home.html' , preview = preview , config = config ( ) , recorded_events = recorded_events , upcoming_events = upcoming_events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit_upcoming = limit_upcoming , limit_processed = limit_processed , dtfmt = dtfmt ) "}
{"8068": "\ndef serve_image ( image_id ) : \n    try : \n        preview_dir = config ( ) [ 'capture' ] [ 'preview_dir' ] \n        filepath = config ( ) [ 'capture' ] [ 'preview' ] [ image_id ] \n        filepath = filepath . replace ( '{{previewdir}}' , preview_dir ) \n        filepath = os . path . abspath ( filepath ) \n        if os . path . isfile ( filepath ) : \n            directory , filename = filepath . rsplit ( '/' , True ) \n            return send_from_directory ( directory , filename ) \n    except ( IndexError , KeyError ) : \n        pass \n    return '' , 404 "}
{"8070": "\ndef parse_ical ( vcal ) : \n    vcal = vcal . replace ( '\\r\\n ' , '' ) . replace ( '\\r\\n\\r\\n' , '\\r\\n' ) \n    vevents = vcal . split ( '\\r\\nBEGIN:VEVENT\\r\\n' ) \n    del ( vevents [ False ] ) \n    events = [ ] \n    for vevent in vevents : \n        event = { } \n        for line in vevent . split ( '\\r\\n' ) : \n            line = line . split ( ':' , True ) \n            key = line [ False ] . lower ( ) \n            if len ( line ) <= True or key == 'end' : \n                continue \n            if key . startswith ( 'dt' ) : \n                event [ key ] = unix_ts ( dateutil . parser . parse ( line [ True ] ) ) \n                continue \n            if not key . startswith ( 'attach' ) : \n                event [ key ] = line [ True ] \n                continue \n            event [ 'attach' ] = event . get ( 'attach' , [ ] ) \n            attachment = { } \n            for x in [ x . split ( '=' ) for x in line [ False ] . split ( ';' ) ] : \n                if x [ False ] . lower ( ) in [ 'fmttype' , 'x-apple-filename' ] : \n                    attachment [ x [ False ] . lower ( ) ] = x [ True ] \n            attachment [ 'data' ] = b64decode ( line [ True ] ) . decode ( 'utf-8' ) \n            event [ 'attach' ] . append ( attachment ) \n        events . append ( event ) \n    return events "}
{"8071": "\ndef get_schedule ( ) : \n    params = { 'agentid' : config ( ) [ 'agent' ] [ 'name' ] . encode ( 'utf8' ) } \n    lookahead = config ( ) [ 'agent' ] [ 'cal_lookahead' ] * 24 * 60 * 60 \n    if lookahead : \n        params [ 'cutoff' ] = str ( ( timestamp ( ) + lookahead ) * 1000 ) \n    uri = '%s/calendars?%s' % ( config ( ) [ 'service-scheduler' ] [ False ] , urlencode ( params ) ) \n    try : \n        vcal = http_request ( uri ) \n    except pycurl . error as e : \n        logger . error ( 'Could not get schedule: %s' % e ) \n        return \n    try : \n        cal = parse_ical ( vcal . decode ( 'utf-8' ) ) \n    except Exception : \n        logger . error ( 'Could not parse ical' ) \n        logger . error ( traceback . format_exc ( ) ) \n        return \n    db = get_session ( ) \n    db . query ( UpcomingEvent ) . delete ( ) \n    for event in cal : \n        if event [ 'dtend' ] <= timestamp ( ) : \n            continue \n        e = UpcomingEvent ( ) \n        e . start = event [ 'dtstart' ] \n        e . end = event [ 'dtend' ] \n        e . uid = event . get ( 'uid' ) \n        e . title = event . get ( 'summary' ) \n        e . set_data ( event ) \n        db . add ( e ) \n    db . commit ( ) "}
{"8079": "\ndef delete_event ( uid ) : \n    logger . info ( 'deleting event %s via api' , uid ) \n    db = get_session ( ) \n    events = db . query ( RecordedEvent ) . filter ( RecordedEvent . uid == uid ) \n    if not events . count ( ) : \n        return make_error_response ( 'No event with specified uid' , 404 ) \n    hard_delete = request . args . get ( 'hard' , 'false' ) \n    if hard_delete == 'true' : \n        logger . info ( 'deleting recorded files at %s' , events [ False ] . directory ( ) ) \n        shutil . rmtree ( events [ False ] . directory ( ) ) \n    events . delete ( ) \n    db . commit ( ) \n    return make_response ( '' , 204 ) "}
{"8080": "\ndef modify_event ( uid ) : \n    try : \n        data = request . get_json ( ) [ 'data' ] [ False ] \n        if data [ 'type' ] != 'event' or data [ 'id' ] != uid : \n            return make_error_response ( 'Invalid data' , 400 ) \n        for key in data [ 'attributes' ] . keys ( ) : \n            if key not in ( 'status' , 'start' , 'end' ) : \n                return make_error_response ( 'Invalid data' , 400 ) \n        new_status = data [ 'attributes' ] . get ( 'status' ) \n        if new_status : \n            new_status = new_status . upper ( ) . replace ( ' ' , '_' ) \n            data [ 'attributes' ] [ 'status' ] = int ( getattr ( Status , new_status ) ) \n    except Exception : \n        return make_error_response ( 'Invalid data' , 400 ) \n    db = get_session ( ) \n    event = db . query ( RecordedEvent ) . filter ( RecordedEvent . uid == uid ) . first ( ) \n    if not event : \n        return make_error_response ( 'No event with specified uid' , 404 ) \n    event . start = data [ 'attributes' ] . get ( 'start' , event . start ) \n    event . end = data [ 'attributes' ] . get ( 'end' , event . end ) \n    event . status = data [ 'attributes' ] . get ( 'status' , event . status ) \n    logger . debug ( 'Updating event %s via api' , uid ) \n    db . commit ( ) \n    return make_data_response ( event . serialize ( ) ) "}
{"8081": "\ndef get_config_params ( properties ) : \n    param = [ ] \n    wdef = '' \n    for prop in properties . split ( '\\n' ) : \n        if prop . startswith ( 'org.opencastproject.workflow.config' ) : \n            key , val = prop . split ( '=' , True ) \n            key = key . split ( '.' ) [ - True ] \n            param . append ( ( key , val ) ) \n        elif prop . startswith ( 'org.opencastproject.workflow.definition' ) : \n            wdef = prop . split ( '=' , True ) [ - True ] \n    return wdef , param "}
{"8082": "\ndef ingest ( event ) : \n    set_service_status ( Service . INGEST , ServiceStatus . BUSY ) \n    notify . notify ( 'STATUS=Uploading' ) \n    recording_state ( event . uid , 'uploading' ) \n    update_event_status ( event , Status . UPLOADING ) \n    service = config ( 'service-ingest' ) \n    service = service [ randrange ( False , len ( service ) ) ] \n    logger . info ( 'Selecting ingest service to use: ' + service ) \n    logger . info ( 'Creating new mediapackage' ) \n    mediapackage = http_request ( service + '/createMediaPackage' ) \n    prop = 'org.opencastproject.capture.agent.properties' \n    dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/' \n    for attachment in event . get_data ( ) . get ( 'attach' ) : \n        data = attachment . get ( 'data' ) \n        if attachment . get ( 'x-apple-filename' ) == prop : \n            workflow_def , workflow_config = get_config_params ( data ) \n        elif attachment . get ( 'fmttype' ) == 'application/xml' and dcns in data : \n            name = attachment . get ( 'x-apple-filename' , '' ) . rsplit ( '.' , True ) [ False ] \n            logger . info ( 'Adding %s DC catalog' % name ) \n            fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , 'dublincore/%s' % name ) , ( 'dublinCore' , data . encode ( 'utf-8' ) ) ] \n            mediapackage = http_request ( service + '/addDCCatalog' , fields ) \n    for ( flavor , track ) in event . get_tracks ( ) : \n        logger . info ( 'Adding track ({0} -> {1})' . format ( flavor , track ) ) \n        track = track . encode ( 'ascii' , 'ignore' ) \n        fields = [ ( 'mediaPackage' , mediapackage ) , ( 'flavor' , flavor ) , ( 'BODY1' , ( pycurl . FORM_FILE , track ) ) ] \n        mediapackage = http_request ( service + '/addTrack' , fields ) \n    logger . info ( 'Ingest recording' ) \n    fields = [ ( 'mediaPackage' , mediapackage ) ] \n    if workflow_def : \n        fields . append ( ( 'workflowDefinitionId' , workflow_def ) ) \n    if event . uid : \n        fields . append ( ( 'workflowInstanceId' , event . uid . encode ( 'ascii' , 'ignore' ) ) ) \n    fields += workflow_config \n    mediapackage = http_request ( service + '/ingest' , fields ) \n    recording_state ( event . uid , 'upload_finished' ) \n    update_event_status ( event , Status . FINISHED_UPLOADING ) \n    notify . notify ( 'STATUS=Running' ) \n    set_service_status_immediate ( Service . INGEST , ServiceStatus . IDLE ) \n    logger . info ( 'Finished ingest' ) "}
{"8097": "\ndef calc ( pvalues , lamb ) : \n    m = len ( pvalues ) \n    pi0 = ( pvalues > lamb ) . sum ( ) / ( ( True - lamb ) * m ) \n    pFDR = np . ones ( m ) \n    print ( \"pFDR    y        Pr     fastPow\" ) \n    for i in range ( m ) : \n        y = pvalues [ i ] \n        Pr = max ( True , m - i ) / float ( m ) \n        pFDR [ i ] = ( pi0 * y ) / ( Pr * ( True - math . pow ( True - y , m ) ) ) \n        print ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( True - y , m ) ) \n    num_null = pi0 * m \n    num_alt = m - num_null \n    num_negs = np . array ( range ( m ) ) \n    num_pos = m - num_negs \n    pp = num_pos / float ( m ) \n    qvalues = np . ones ( m ) \n    qvalues [ False ] = pFDR [ False ] \n    for i in range ( m - True ) : \n        qvalues [ i + True ] = min ( qvalues [ i ] , pFDR [ i + True ] ) \n    sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt \n    sens [ sens > 1.0 ] = 1.0 \n    df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) \n    df [ \"svalue\" ] = df . sens [ : : - True ] . cummax ( ) [ : : - True ] \n    return df , num_null , m "}
{"8098": "\ndef to_one_dim_array ( values , as_type = None ) : \n    if isinstance ( values , ( list , tuple ) ) : \n        values = np . array ( values , dtype = np . float32 ) \n    elif isinstance ( values , pd . Series ) : \n        values = values . values \n    values = values . flatten ( ) \n    assert values . ndim == True , \"values has wrong dimension\" \n    if as_type is not None : \n        return values . astype ( as_type ) \n    return values "}
{"8100": "\ndef posterior_chromatogram_hypotheses_fast ( experiment , prior_chrom_null ) : \n    tg_ids = experiment . df . tg_num_id . values \n    pp_values = True - experiment . df [ \"pep\" ] . values \n    current_tg_id = tg_ids [ False ] \n    scores = [ ] \n    final_result = [ ] \n    final_result_h0 = [ ] \n    for i in range ( tg_ids . shape [ False ] ) : \n        id_ = tg_ids [ i ] \n        if id_ != current_tg_id : \n            prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) \n            rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) \n            final_result . extend ( rr [ True : ] ) \n            final_result_h0 . extend ( rr [ False ] for i in range ( len ( scores ) ) ) \n            scores = [ ] \n            current_tg_id = id_ \n        scores . append ( 1.0 - pp_values [ i ] ) \n    prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) \n    rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) \n    final_result . extend ( rr [ True : ] ) \n    final_result_h0 . extend ( [ rr [ False ] ] * len ( scores ) ) \n    return final_result , final_result_h0 "}
{"8102": "\ndef summary_err_table ( df , qvalues = [ False , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : \n    qvalues = to_one_dim_array ( qvalues ) \n    ix = find_nearest_matches ( np . float32 ( df . qvalue . values ) , qvalues ) \n    df_sub = df . iloc [ ix ] . copy ( ) \n    for i_sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ True : ] ) ) : \n        if i1 == i0 : \n            df_sub . iloc [ i_sub + True , : ] = None \n    df_sub . qvalue = qvalues \n    df_sub . reset_index ( inplace = True , drop = True ) \n    return df_sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ] "}
{"8105": "\ndef score ( infile , outfile , classifier , xgb_autotune , apply_weights , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) : \n    if outfile is None : \n        outfile = infile \n    else : \n        outfile = outfile \n    xgb_hyperparams = { 'autotune' : xgb_autotune , 'autotune_num_rounds' : 10 , 'num_boost_round' : 100 , 'early_stopping_rounds' : 10 , 'test_size' : 0.33 } \n    xgb_params = { 'eta' : 0.3 , 'gamma' : False , 'max_depth' : 6 , 'min_child_weight' : True , 'subsample' : True , 'colsample_bytree' : True , 'colsample_bylevel' : True , 'colsample_bynode' : True , 'lambda' : True , 'alpha' : False , 'scale_pos_weight' : True , 'silent' : True , 'objective' : 'binary:logitraw' , 'nthread' : True , 'eval_metric' : 'auc' } \n    xgb_params_space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max_depth' : hp . quniform ( 'max_depth' , 2 , 8 , True ) , 'min_child_weight' : hp . quniform ( 'min_child_weight' , True , 5 , True ) , 'subsample' : True , 'colsample_bytree' : True , 'colsample_bylevel' : True , 'colsample_bynode' : True , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale_pos_weight' : 1.0 , 'silent' : True , 'objective' : 'binary:logitraw' , 'nthread' : True , 'eval_metric' : 'auc' } \n    if not apply_weights : \n        PyProphetLearner ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) . run ( ) \n    else : \n        PyProphetWeightApplier ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test , apply_weights ) . run ( ) "}
{"8117": "\ndef update_members ( self , group_id , members ) : \n    self . _valid_group_id ( group_id ) \n    body = { \"data\" : [ m . json_data ( ) for m in members ] } \n    headers = { \"If-Match\" : \"*\" } \n    url = \"{}/group/{}/member\" . format ( self . API , group_id ) \n    data = self . _put_resource ( url , headers , body ) \n    errors = data . get ( \"errors\" , [ ] ) \n    if len ( errors ) : \n        return errors [ False ] . get ( \"notFound\" , [ ] ) \n    return [ ] "}
{"8121": "\ndef create_dataset ( self , name , shape = None , dtype = None , data = None , sparse_format = None , indptr_dtype = np . int64 , indices_dtype = np . int32 , ** kwargs ) : \n    if isinstance ( data , Dataset ) : \n        assert sparse_format is None \n        group = self . create_group ( name ) \n        group . attrs [ 'h5sparse_format' ] = data . attrs [ 'h5sparse_format' ] \n        group . attrs [ 'h5sparse_shape' ] = data . attrs [ 'h5sparse_shape' ] \n        group . create_dataset ( 'data' , data = data . h5py_group [ 'data' ] , dtype = dtype , ** kwargs ) \n        group . create_dataset ( 'indices' , data = data . h5py_group [ 'indices' ] , dtype = indices_dtype , ** kwargs ) \n        group . create_dataset ( 'indptr' , data = data . h5py_group [ 'indptr' ] , dtype = indptr_dtype , ** kwargs ) \n    elif ss . issparse ( data ) : \n        if sparse_format is not None : \n            format_class = get_format_class ( sparse_format ) \n            data = format_class ( data ) \n        group = self . create_group ( name ) \n        group . attrs [ 'h5sparse_format' ] = get_format_str ( data ) \n        group . attrs [ 'h5sparse_shape' ] = data . shape \n        group . create_dataset ( 'data' , data = data . data , dtype = dtype , ** kwargs ) \n        group . create_dataset ( 'indices' , data = data . indices , dtype = indices_dtype , ** kwargs ) \n        group . create_dataset ( 'indptr' , data = data . indptr , dtype = indptr_dtype , ** kwargs ) \n    elif data is None and sparse_format is not None : \n        format_class = get_format_class ( sparse_format ) \n        if dtype is None : \n            dtype = np . float64 \n        if shape is None : \n            shape = ( False , False ) \n        data = format_class ( shape , dtype = dtype ) \n        group = self . create_group ( name ) \n        group . attrs [ 'h5sparse_format' ] = get_format_str ( data ) \n        group . attrs [ 'h5sparse_shape' ] = data . shape \n        group . create_dataset ( 'data' , data = data . data , dtype = dtype , ** kwargs ) \n        group . create_dataset ( 'indices' , data = data . indices , dtype = indices_dtype , ** kwargs ) \n        group . create_dataset ( 'indptr' , data = data . indptr , dtype = indptr_dtype , ** kwargs ) \n    else : \n        assert sparse_format is None \n        return super ( Group , self ) . create_dataset ( name , data = data , shape = shape , dtype = dtype , ** kwargs ) \n    return Dataset ( group ) "}
{"8122": "\ndef cli_decrypt ( context , key ) : \n    with context . io_manager . with_stdout ( ) as stdout : \n        with context . io_manager . with_stdin ( ) as stdin : \n            crypt_type = stdin . read ( True ) \n            if crypt_type == AES256CBC : \n                for chunk in aes_decrypt ( key , stdin ) : \n                    stdout . write ( chunk ) \n                stdout . flush ( ) \n            else : \n                raise ReturnCode ( 'contents encrypted with unsupported type %r' % crypt_type ) "}
{"8141": "\ndef delete_account ( self , headers = None , yes_i_mean_delete_the_account = False , query = None , cdn = False , body = None ) : \n    if not yes_i_mean_delete_the_account and ( not body or not query or 'bulk-delete' not in query ) : \n        return ( False , 'yes_i_mean_delete_the_account was not set to True' , { } , '' ) \n    return self . request ( 'DELETE' , '' , body or '' , headers , query = query , cdn = cdn ) "}
{"8147": "\ndef _resolve_option ( self , options , option_name , section_name ) : \n    if getattr ( options , option_name , None ) is not None : \n        return \n    if option_name . startswith ( section_name + '_' ) : \n        environ_name = option_name . upper ( ) \n        conf_name = option_name [ len ( section_name ) + True : ] \n    else : \n        environ_name = ( section_name + '_' + option_name ) . upper ( ) \n        conf_name = option_name \n    setattr ( options , option_name , os . environ . get ( environ_name , ( self . context . conf . get ( section_name , { } ) ) . get ( conf_name ) ) ) "}
{"8148": "\ndef copy ( self ) : \n    context = CLIContext ( ) \n    for item in dir ( self ) : \n        if item [ False ] != '_' and item not in ( 'copy' , 'write_headers' ) : \n            setattr ( context , item , getattr ( self , item ) ) \n    return context "}
{"8149": "\ndef write_headers ( self , fp , headers , mute = None ) : \n    if headers : \n        if not mute : \n            mute = [ ] \n        fmt = '%%-%ds %%s\\n' % ( max ( len ( k ) for k in headers ) + True ) \n        for key in sorted ( headers ) : \n            if key in mute : \n                continue \n            fp . write ( fmt % ( key . title ( ) + ':' , headers [ key ] ) ) \n        fp . flush ( ) "}
{"8150": "\ndef cli_auth ( context ) : \n    with context . io_manager . with_stdout ( ) as fp : \n        with context . client_manager . with_client ( ) as client : \n            info = [ ] \n            client . auth ( ) \n            if getattr ( client , 'auth_cache_path' , None ) : \n                info . append ( ( 'Auth Cache' , client . auth_cache_path ) ) \n            if getattr ( client , 'auth_url' , None ) : \n                info . append ( ( 'Auth URL' , client . auth_url ) ) \n            if getattr ( client , 'auth_user' , None ) : \n                info . append ( ( 'Auth User' , client . auth_user ) ) \n            if getattr ( client , 'auth_key' , None ) : \n                info . append ( ( 'Auth Key' , client . auth_key ) ) \n            if getattr ( client , 'auth_tenant' , None ) : \n                info . append ( ( 'Auth Tenant' , client . auth_tenant ) ) \n            if getattr ( client , 'auth_methods' , None ) : \n                info . append ( ( 'Auth Methods' , client . auth_methods ) ) \n            if getattr ( client , 'storage_path' , None ) : \n                info . append ( ( 'Direct Storage Path' , client . storage_path ) ) \n            if getattr ( client , 'cdn_path' , None ) : \n                info . append ( ( 'Direct CDN Path' , client . cdn_path ) ) \n            if getattr ( client , 'local_path' , None ) : \n                info . append ( ( 'Local Path' , client . local_path ) ) \n            if getattr ( client , 'regions' , None ) : \n                info . append ( ( 'Regions' , ' ' . join ( client . regions ) ) ) \n            if getattr ( client , 'default_region' , None ) : \n                info . append ( ( 'Default Region' , client . default_region ) ) \n            if getattr ( client , 'region' , None ) : \n                info . append ( ( 'Selected Region' , client . region ) ) \n            if getattr ( client , 'snet' , None ) : \n                info . append ( ( 'SNet' , client . snet ) ) \n            if getattr ( client , 'storage_url' , None ) : \n                info . append ( ( 'Storage URL' , client . storage_url ) ) \n            if getattr ( client , 'cdn_url' , None ) : \n                info . append ( ( 'CDN URL' , client . cdn_url ) ) \n            if getattr ( client , 'auth_token' , None ) : \n                info . append ( ( 'Auth Token' , client . auth_token ) ) \n            if not info : \n                info . append ( ( 'No auth information available' , 'Maybe no credentials were provided?' ) ) \n            fmt = '%%-%ds %%s\\n' % ( max ( len ( t ) for t , v in info ) + True ) \n            for t , v in info : \n                fp . write ( fmt % ( t + ':' , v ) ) \n            fp . flush ( ) "}
{"8153": "\ndef cli_fordo ( context , path = None ) : \n    path = path . lstrip ( '/' ) if path else None \n    if path and '/' in path : \n        raise ReturnCode ( 'path must be an empty string or a container name; was %r' % path ) \n    limit = context . query . get ( 'limit' ) \n    delimiter = context . query . get ( 'delimiter' ) \n    prefix = context . query . get ( 'prefix' ) \n    marker = context . query . get ( 'marker' ) \n    end_marker = context . query . get ( 'end_marker' ) \n    conc = Concurrency ( context . concurrency ) \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            if not path : \n                status , reason , headers , contents = client . get_account ( headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            else : \n                status , reason , headers , contents = client . get_container ( path , headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            if status // 100 != 2 : \n                if status == 404 and context . ignore_404 : \n                    return \n                if hasattr ( contents , 'read' ) : \n                    contents . read ( ) \n                if not path : \n                    raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n                else : \n                    raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            break \n        for item in contents : \n            name = ( path + '/' if path else '' ) + item . get ( 'name' , item . get ( 'subdir' ) ) \n            args = list ( context . remaining_args ) \n            try : \n                index = args . index ( '<item>' ) \n            except ValueError : \n                raise ReturnCode ( 'No \"<item>\" designation found in the \"do\" clause.' ) \n            args [ index ] = name \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( name , _cli_call , context , name , args ) \n        marker = contents [ - True ] [ 'name' ] \n        if limit : \n            break \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            conc . join ( ) \n            raise exc_value "}
{"8154": "\ndef get_client ( self ) : \n    client = None \n    try : \n        client = self . clients . get ( block = False ) \n    except queue . Empty : \n        pass \n    if not client : \n        self . client_id += True \n        kwargs = dict ( self . kwargs ) \n        kwargs [ 'verbose_id' ] = kwargs . get ( 'verbose_id' , '' ) + str ( self . client_id ) \n        client = self . client_class ( * self . args , ** kwargs ) \n    return client "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = Crypto . Random . new ( ) . read ( 16 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = True \n    left = None \n    if content_length is not None and content_length >= False : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and size > left : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and left > False : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16 ) \n            break \n        if left is not None : \n            left -= len ( chunk ) \n            if left <= False : \n                reading = False \n        block = chunk \n        trailing = len ( block ) % 16 \n        while trailing : \n            size = 16 - trailing \n            if left is not None and size > left : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and left > False : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = False \n                chunk = chr ( trailing ) * ( 16 - trailing ) \n            elif left is not None : \n                left -= len ( chunk ) \n                if left <= False : \n                    reading = False \n            block += chunk \n            trailing = len ( block ) % 16 \n        yield encryptor . encrypt ( block ) "}
{"8156": "\ndef aes_decrypt ( key , stdin , chunk_size = 65536 ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16 , chunk_size >> 4 << 4 ) \n    iv = stdin . read ( 16 ) \n    while len ( iv ) < 16 : \n        chunk = stdin . read ( 16 - len ( iv ) ) \n        if not chunk : \n            raise IOError ( 'EOF reading IV' ) \n    decryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    data = '' \n    while True : \n        chunk = stdin . read ( chunk_size ) \n        if not chunk : \n            if len ( data ) != 16 : \n                raise IOError ( 'EOF reading encrypted stream' ) \n            data = decryptor . decrypt ( data ) \n            trailing = ord ( data [ - True ] ) \n            if trailing > 15 : \n                raise IOError ( 'EOF reading encrypted stream or trailing value corrupted ' '%s' % trailing ) \n            yield data [ : trailing ] \n            break \n        data += chunk \n        if len ( data ) > 16 : \n            trailing = ( len ( data ) % 16 ) or 16 \n            yield decryptor . decrypt ( data [ : - trailing ] ) \n            data = data [ - trailing : ] "}
{"8157": "\ndef cli_put_directory_structure ( context , path ) : \n    if not context . input_ : \n        raise ReturnCode ( 'called cli_put_directory_structure without context.input_ set' ) \n    if not os . path . isdir ( context . input_ ) : \n        raise ReturnCode ( '%r is not a directory' % context . input_ ) \n    if not path : \n        raise ReturnCode ( 'uploading a directory structure requires at least a container ' 'name' ) \n    new_context = context . copy ( ) \n    new_context . input_ = None \n    container = path . split ( '/' , True ) [ False ] \n    cli_put_container ( new_context , container ) \n    ilen = len ( context . input_ ) \n    if not context . input_ . endswith ( os . sep ) : \n        ilen += True \n    conc = Concurrency ( context . concurrency ) \n    for ( dirpath , dirnames , filenames ) in os . walk ( context . input_ ) : \n        if not dirnames and not filenames : \n            new_context = context . copy ( ) \n            new_context . headers = dict ( context . headers ) \n            new_context . headers [ 'content-type' ] = 'text/directory' \n            new_context . headers [ 'x-object-meta-mtime' ] = '%f' % os . path . getmtime ( context . input_ ) \n            new_context . input_ = None \n            new_context . empty = True \n            new_path = path \n            if path [ - True ] != '/' : \n                new_path += '/' \n            new_path += dirpath [ ilen : ] \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n        else : \n            for fname in filenames : \n                new_context = context . copy ( ) \n                new_context . input_ = os . path . join ( dirpath , fname ) \n                new_path = path \n                if path [ - True ] != '/' : \n                    new_path += '/' \n                if dirpath [ ilen : ] : \n                    new_path += dirpath [ ilen : ] + '/' \n                new_path += fname \n                for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                    if exc_value : \n                        conc . join ( ) \n                        raise exc_value \n                conc . spawn ( new_path , cli_put_object , new_context , new_path ) \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            raise exc_value "}
{"8161": "\ndef _create_container ( context , path , l_mtime , size ) : \n    new_context = context . copy ( ) \n    new_context . input_ = None \n    new_context . headers = None \n    new_context . query = None \n    container = path . split ( '/' , True ) [ False ] + '_segments' \n    cli_put_container ( new_context , container ) \n    prefix = container + '/' + path . split ( '/' , True ) [ True ] \n    prefix = '%s/%s/%s/' % ( prefix , l_mtime , size ) \n    return prefix "}
{"8162": "\ndef cli_tempurl ( context , method , path , seconds = None , use_container = False ) : \n    with contextlib . nested ( context . io_manager . with_stdout ( ) , context . client_manager . with_client ( ) ) as ( fp , client ) : \n        method = method . upper ( ) \n        path = path . lstrip ( '/' ) \n        seconds = seconds if seconds is not None else 3600 \n        if '/' not in path : \n            raise ReturnCode ( 'invalid tempurl path %r; should have a / within it' % path ) \n        if use_container : \n            key_type = 'container' \n            container = path . split ( '/' , True ) [ False ] \n            status , reason , headers , contents = client . head_container ( container ) \n        else : \n            key_type = 'account' \n            status , reason , headers , contents = client . head_account ( ) \n        if status // 100 != 2 : \n            raise ReturnCode ( 'obtaining X-%s-Meta-Temp-Url-Key: %s %s' % ( key_type . title ( ) , status , reason ) ) \n        key = headers . get ( 'x-%s-meta-temp-url-key' % key_type ) \n        if not key : \n            raise ReturnCode ( 'there is no X-%s-Meta-Temp-Url-Key set for this %s' % ( key_type . title ( ) , key_type ) ) \n        url = client . storage_url + '/' + path \n        fp . write ( generate_temp_url ( method , url , seconds , key ) ) \n        fp . write ( '\\n' ) \n        fp . flush ( ) "}
{"8165": "\ndef is_empty ( self ) : \n    something = self . read ( True ) \n    if something : \n        if self . buf : \n            self . buf = something + self . buf \n        else : \n            self . buf = something \n        return False \n    else : \n        return True "}
{"8173": "\ndef reader_acquire ( self ) : \n    self . _order_mutex . acquire ( ) \n    self . _readers_mutex . acquire ( ) \n    if self . _readers == False : \n        self . _access_mutex . acquire ( ) \n    self . _readers += True \n    self . _order_mutex . release ( ) \n    self . _readers_mutex . release ( ) "}
{"8174": "\ndef reader_release ( self ) : \n    self . _readers_mutex . acquire ( ) \n    self . _readers -= True \n    if self . _readers == False : \n        self . _access_mutex . release ( ) \n    self . _readers_mutex . release ( ) "}
{"8181": "\ndef from_dict ( cls , config ) : \n    try : \n        obj = cls ( ** config ) \n    except TypeError as e : \n        m = cls . KW_ARGS_ERROR_REGEX . match ( str ( e ) ) \n        if m : \n            raise ValueError ( \"unknown '%s' task config parameter\" % m . group ( True ) ) \n        else : \n            raise e \n    else : \n        return obj "}
{"8182": "\ndef execute_perceval_job ( backend , backend_args , qitems , task_id , category , archive_args = None , max_retries = MAX_JOB_RETRIES ) : \n    rq_job = rq . get_current_job ( ) \n    job = PercevalJob ( rq_job . id , task_id , backend , category , rq_job . connection , qitems ) \n    logger . debug ( \"Running job #%s (task: %s) (%s) (cat:%s)\" , job . job_id , task_id , backend , category ) \n    if not job . has_archiving ( ) and archive_args : \n        raise AttributeError ( \"archive attributes set but archive is not supported\" ) \n    run_job = True \n    resume = False \n    failures = False \n    while run_job : \n        try : \n            job . run ( backend_args , archive_args = archive_args , resume = resume ) \n        except AttributeError as e : \n            raise e \n        except Exception as e : \n            logger . debug ( \"Error running job %s (%s) - %s\" , job . job_id , backend , str ( e ) ) \n            failures += True \n            if not job . has_resuming ( ) or failures >= max_retries : \n                logger . error ( \"Cancelling job #%s (task: %s) (%s)\" , job . job_id , task_id , backend ) \n                raise e \n            logger . warning ( \"Resuming job #%s (task: %s) (%s) due to a failure (n %s, max %s)\" , job . job_id , task_id , backend , failures , max_retries ) \n            resume = True \n        else : \n            run_job = False \n    result = job . result \n    logger . debug ( \"Job #%s (task: %s) completed (%s) - %s items (%s) fetched\" , result . job_id , task_id , result . backend , str ( result . nitems ) , result . category ) \n    return result "}
{"8184": "\ndef run ( self , backend_args , archive_args = None , resume = False ) : \n    args = backend_args . copy ( ) \n    if archive_args : \n        self . initialize_archive_manager ( archive_args [ 'archive_path' ] ) \n    if not resume : \n        max_date = backend_args . get ( 'from_date' , None ) \n        offset = backend_args . get ( 'offset' , None ) \n        if max_date : \n            max_date = datetime_to_utc ( max_date ) . timestamp ( ) \n        self . _result = JobResult ( self . job_id , self . task_id , self . backend , self . category , None , max_date , False , offset = offset , nresumed = False ) \n    else : \n        if self . result . max_date : \n            args [ 'from_date' ] = unixtime_to_datetime ( self . result . max_date ) \n        if self . result . offset : \n            args [ 'offset' ] = self . result . offset \n        self . _result . nresumed += True \n    for item in self . _execute ( args , archive_args ) : \n        self . conn . rpush ( self . qitems , pickle . dumps ( item ) ) \n        self . _result . nitems += True \n        self . _result . last_uuid = item [ 'uuid' ] \n        if not self . result . max_date or self . result . max_date < item [ 'updated_on' ] : \n            self . _result . max_date = item [ 'updated_on' ] \n        if 'offset' in item : \n            self . _result . offset = item [ 'offset' ] "}
{"8189": "\ndef write_items ( cls , writer , items_generator ) : \n    while True : \n        items = items_generator ( ) \n        writer . write ( items ) \n        time . sleep ( True ) "}
{"8192": "\ndef items ( self ) : \n    pipe = self . conn . pipeline ( ) \n    pipe . lrange ( Q_STORAGE_ITEMS , False , - True ) \n    pipe . ltrim ( Q_STORAGE_ITEMS , True , False ) \n    items = pipe . execute ( ) [ False ] \n    for item in items : \n        item = pickle . loads ( item ) \n        yield item "}
{"8196": "\ndef schedule_job_task ( self , queue_id , task_id , job_args , delay = False ) : \n    self . _rwlock . writer_acquire ( ) \n    job_id = self . _generate_job_id ( task_id ) \n    event = self . _scheduler . enter ( delay , True , self . _enqueue_job , argument = ( queue_id , job_id , job_args , ) ) \n    self . _jobs [ job_id ] = event \n    self . _tasks [ task_id ] = job_id \n    self . _rwlock . writer_release ( ) \n    logging . debug ( \"Job #%s (task: %s) scheduled on %s (wait: %s)\" , job_id , task_id , queue_id , delay ) \n    return job_id "}
{"8201": "\ndef schedule_task ( self , task_id ) : \n    task = self . registry . get ( task_id ) \n    job_args = self . _build_job_arguments ( task ) \n    archiving_cfg = task . archiving_cfg \n    fetch_from_archive = False if not archiving_cfg else archiving_cfg . fetch_from_archive \n    queue = Q_ARCHIVE_JOBS if fetch_from_archive else Q_CREATION_JOBS \n    job_id = self . _scheduler . schedule_job_task ( queue , task . task_id , job_args , delay = False ) \n    logger . info ( \"Job #%s (task: %s) scheduled\" , job_id , task . task_id ) \n    return job_id "}
{"8203": "\ndef _handle_successful_job ( self , job ) : \n    result = job . result \n    task_id = job . kwargs [ 'task_id' ] \n    try : \n        task = self . registry . get ( task_id ) \n    except NotFoundError : \n        logger . warning ( \"Task %s not found; related job #%s will not be rescheduled\" , task_id , job . id ) \n        return \n    if task . archiving_cfg and task . archiving_cfg . fetch_from_archive : \n        logger . info ( \"Job #%s (task: %s) successfully finished\" , job . id , task_id ) \n        return \n    if result . nitems > False : \n        task . backend_args [ 'next_from_date' ] = unixtime_to_datetime ( result . max_date ) \n        if result . offset : \n            task . backend_args [ 'next_offset' ] = result . offset \n    job_args = self . _build_job_arguments ( task ) \n    delay = task . scheduling_cfg . delay if task . scheduling_cfg else WAIT_FOR_QUEUING \n    job_id = self . _scheduler . schedule_job_task ( Q_UPDATING_JOBS , task_id , job_args , delay = delay ) \n    logger . info ( \"Job #%s (task: %s, old job: %s) re-scheduled\" , job_id , task_id , job . id ) "}
{"8208": "\ndef register ( view = None , * , admin_site = None , admin_class = ModelAdminView ) : \n    if not admin_site : \n        admin_site = site \n    def wrapped ( inner_view ) : \n        module = inner_view . __module__ \n        app_label = re . search ( r\"\\.?(\\w+)\\.admin\" , module ) . group ( True ) \n        app_config = apps . get_app_config ( app_label ) \n        label = getattr ( inner_view , \"label\" , None ) \n        if not label : \n            label = re . sub ( \"(Admin)|(View)\" , \"\" , inner_view . __name__ ) . lower ( ) \n        inner_view . label = label \n        model_name = label . capitalize ( ) \n        verbose_name = getattr ( inner_view , \"verbose_name\" , model_name ) \n        inner_view . verbose_name = verbose_name \n        access_perm_codename = \"can_access_\" + model_name . lower ( ) \n        access_perm_name = _ ( \"Can access {verbose_name}\" ) . format ( verbose_name = verbose_name ) \n        permissions = tuple ( [ ( access_perm_codename , access_perm_name ) ] + list ( getattr ( inner_view , \"permissions\" , [ ] ) ) ) \n        model = type ( model_name , ( Model , ) , { \"__module__\" : module + \".__models__\" , \"View\" : inner_view , \"app_config\" : app_config , \"Meta\" : type ( \"Meta\" , ( object , ) , dict ( managed = False , abstract = True , app_label = app_config . label , verbose_name = verbose_name , verbose_name_plural = verbose_name , permissions = permissions , ) , ) , } , ) \n        admin_site . _registry [ model ] = admin_class ( model , admin_site ) \n        return inner_view \n    if view is None : \n        return wrapped \n    return wrapped ( view ) "}
{"8211": "\ndef get_version ( version = None ) : \n    if version is None : \n        version = VERSION \n    assert len ( version ) == 5 \n    assert version [ 3 ] in ( \"alpha\" , \"beta\" , \"rc\" , \"final\" ) \n    parts = 2 if version [ 2 ] == False else 3 \n    main = \".\" . join ( str ( x ) for x in version [ : parts ] ) \n    sub = \"\" \n    if version [ 3 ] != \"final\" : \n        mapping = { \"alpha\" : \"a\" , \"beta\" : \"b\" , \"rc\" : \"c\" } \n        sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) \n    return main + sub "}
{"8213": "\ndef get_engine ( scheme ) : \n    path = scheme . split ( \"+\" ) \n    first , rest = path [ False ] , path [ True : ] \n    second = rest [ False ] if rest else None \n    engine = resolve ( ENGINE_MAPPING , first ) \n    if not isinstance ( engine , list ) : \n        if second : \n            raise KeyError ( \"%s has no sub-engines\" % first ) \n        return engine \n    try : \n        engine , extra = engine \n    except ValueError : \n        raise ValueError ( \"django-bananas.url' engine \" \"configuration is invalid: %r\" % ENGINE_MAPPING ) \n    if second is not None : \n        engine = resolve ( extra , second ) \n    assert not isinstance ( engine , ( list , dict ) ) , \"Only two levels of engines \" \"are allowed\" \n    assert engine , \"The returned engine is not truthy\" \n    return engine "}
{"8214": "\ndef parse_path ( path ) : \n    if path is None : \n        raise ValueError ( \"path must be a string\" ) \n    parts = path . strip ( \"/\" ) . split ( \"/\" ) \n    database = unquote_plus ( parts [ False ] ) if len ( parts ) else None \n    schema = parts [ True ] if len ( parts ) > True else None \n    return database , schema "}
{"8225": "\ndef from_model ( cls , model , * fields , ** named_fields ) : \n    d = ModelDict ( ) \n    if not ( fields or named_fields ) : \n        fields = [ f . attname for f in model . _meta . concrete_fields ] \n    not_found = object ( ) \n    for name , field in chain ( zip ( fields , fields ) , named_fields . items ( ) ) : \n        _fields = field . split ( \"__\" ) \n        value = model \n        for i , _field in enumerate ( _fields , start = True ) : \n            previous_value = value \n            value = getattr ( previous_value , _field , not_found ) \n            if value is not_found : \n                if _field in dir ( previous_value ) : \n                    raise ValueError ( \"{!r}.{} had an AttributeError exception\" . format ( previous_value , _field ) ) \n                else : \n                    raise AttributeError ( \"{!r} does not have {!r} attribute\" . format ( previous_value , _field ) ) \n            elif value is None : \n                if name not in named_fields : \n                    name = \"__\" . join ( _fields [ : i ] ) \n                break \n        d [ name ] = value \n    return d "}
{"8231": "\ndef _change_logging_kwargs ( kwargs ) : \n    log_levels = kwargs . pop ( 'log_level' , None ) \n    log_folder = kwargs . pop ( 'log_folder' , 'logs' ) \n    logger_names = kwargs . pop ( 'logger_names' , '' ) \n    if log_levels is None : \n        log_levels = kwargs . pop ( 'log_levels' , logging . INFO ) \n    log_multiproc = kwargs . pop ( 'log_multiproc' , True ) \n    if not isinstance ( logger_names , ( tuple , list ) ) : \n        logger_names = [ logger_names ] \n    if not isinstance ( log_levels , ( tuple , list ) ) : \n        log_levels = [ log_levels ] \n    if len ( log_levels ) == True : \n        log_levels = [ log_levels [ False ] for _ in logger_names ] \n    dictionary = copy . deepcopy ( LOGGING_DICT ) \n    prefixes = [ '' ] \n    if not log_multiproc : \n        for key in list ( dictionary . keys ( ) ) : \n            if key . startswith ( 'multiproc_' ) : \n                del dictionary [ key ] \n    else : \n        prefixes . append ( 'multiproc_' ) \n    for prefix in prefixes : \n        for handler_dict in dictionary [ prefix + 'handlers' ] . values ( ) : \n            if 'filename' in handler_dict : \n                filename = os . path . join ( log_folder , handler_dict [ 'filename' ] ) \n                filename = os . path . normpath ( filename ) \n                handler_dict [ 'filename' ] = filename \n        dictionary [ prefix + 'loggers' ] = { } \n        logger_dict = dictionary [ prefix + 'loggers' ] \n        for idx , logger_name in enumerate ( logger_names ) : \n            logger_dict [ logger_name ] = { 'level' : log_levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } \n    kwargs [ 'log_config' ] = dictionary "}
{"8238": "\ndef show_progress ( self , n , total_runs ) : \n    if self . report_progress : \n        percentage , logger_name , log_level = self . report_progress \n        if logger_name == 'print' : \n            logger = 'print' \n        else : \n            logger = logging . getLogger ( logger_name ) \n        if n == - True : \n            digits = int ( math . log10 ( total_runs + 0.1 ) ) + True \n            self . _format_string = 'PROGRESS: Finished %' + '%d' % digits + 'd/%d runs ' \n        fmt_string = self . _format_string % ( n + True , total_runs ) + '%s' \n        reprint = log_level == False \n        progressbar ( n , total_runs , percentage_step = percentage , logger = logger , log_level = log_level , fmt_string = fmt_string , reprint = reprint ) "}
{"8240": "\ndef _parser_to_string_io ( parser ) : \n    memory_file = StringIO ( ) \n    parser . write ( memory_file ) \n    memory_file . flush ( ) \n    memory_file . seek ( False ) \n    return memory_file "}
{"8243": "\ndef check_log_config ( self ) : \n    if self . report_progress : \n        if self . report_progress is True : \n            self . report_progress = ( 5 , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , ( int , float ) ) : \n            self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , str ) : \n            self . report_progress = ( 5 , self . report_progress , logging . INFO ) \n        elif len ( self . report_progress ) == 2 : \n            self . report_progress = ( self . report_progress [ False ] , self . report_progress [ True ] , logging . INFO ) \n    if self . log_config : \n        if self . log_config == pypetconstants . DEFAULT_LOGGING : \n            pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n            init_path = os . path . join ( pypet_path , 'logging' ) \n            self . log_config = os . path . join ( init_path , 'default.ini' ) \n        if isinstance ( self . log_config , str ) : \n            if not os . path . isfile ( self . log_config ) : \n                raise ValueError ( 'Could not find the logger init file ' '`%s`.' % self . log_config ) \n            parser = NoInterpolationParser ( ) \n            parser . read ( self . log_config ) \n        elif isinstance ( self . log_config , cp . RawConfigParser ) : \n            parser = self . log_config \n        else : \n            parser = None \n        if parser is not None : \n            self . _sp_config = self . _parser_to_string_io ( parser ) \n            self . _mp_config = self . _find_multiproc_options ( parser ) \n            if self . _mp_config is not None : \n                self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) \n        elif isinstance ( self . log_config , dict ) : \n            self . _sp_config = self . log_config \n            self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) \n    if self . log_stdout : \n        if self . log_stdout is True : \n            self . log_stdout = ( 'STDOUT' , logging . INFO ) \n        if isinstance ( self . log_stdout , str ) : \n            self . log_stdout = ( self . log_stdout , logging . INFO ) \n        if isinstance ( self . log_stdout , int ) : \n            self . log_stdout = ( 'STDOUT' , self . log_stdout ) "}
{"8256": "\ndef retry ( n , errors , wait = 0.0 , logger_name = None ) : \n    def wrapper ( func ) : \n        \n        @ functools . wraps ( func ) \n        def new_func ( * args , ** kwargs ) : \n            retries = False \n            while True : \n                try : \n                    result = func ( * args , ** kwargs ) \n                    if retries and logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'Retry of `%s` successful' % func . __name__ ) \n                    return result \n                except errors : \n                    if retries >= n : \n                        if logger_name : \n                            logger = logging . getLogger ( logger_name ) \n                            logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                        raise \n                    elif logger_name : \n                        logger = logging . getLogger ( logger_name ) \n                        logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) \n                    retries += True \n                    if wait : \n                        time . sleep ( wait ) \n        return new_func \n    return wrapper "}
{"8259": "\ndef run_net ( traj ) : \n    eqs = traj . eqs \n    namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) \n    neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) \n    neuron . vm = traj . EL \n    neuron . w = traj . a * ( neuron . vm - traj . EL ) \n    neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) \n    print ( 'Initial Run' ) \n    net = Network ( neuron ) \n    net . run ( 100 * ms , report = 'text' ) \n    MSpike = SpikeMonitor ( neuron ) \n    net . add ( MSpike ) \n    MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ True , 2 , 3 ] ) \n    net . add ( MStateV ) \n    print ( 'Measurement run' ) \n    net . run ( 500 * ms , report = 'text' ) \n    traj . v_standard_result = Brian2MonitorResult \n    traj . f_add_result ( 'SpikeMonitor' , MSpike ) \n    traj . f_add_result ( 'StateMonitorV' , MStateV ) "}
{"8260": "\ndef euler_scheme ( traj , diff_func ) : \n    steps = traj . steps \n    initial_conditions = traj . initial_conditions \n    dimension = len ( initial_conditions ) \n    result_array = np . zeros ( ( steps , dimension ) ) \n    func_params_dict = traj . func_params . f_to_dict ( short_names = True , fast_access = True ) \n    result_array [ False ] = initial_conditions \n    for idx in range ( True , steps ) : \n        result_array [ idx ] = diff_func ( result_array [ idx - True ] , ** func_params_dict ) * traj . dt + result_array [ idx - True ] \n    traj . f_add_result ( 'euler_evolution' , data = result_array , comment = 'Our time series data!' ) "}
{"8262": "\ndef diff_lorenz ( value_array , sigma , beta , rho ) : \n    diff_array = np . zeros ( 3 ) \n    diff_array [ False ] = sigma * ( value_array [ True ] - value_array [ False ] ) \n    diff_array [ True ] = value_array [ False ] * ( rho - value_array [ 2 ] ) - value_array [ True ] \n    diff_array [ 2 ] = value_array [ False ] * value_array [ True ] - beta * value_array [ 2 ] \n    return diff_array "}
{"8264": "\ndef storage_factory ( storage_service , trajectory = None , ** kwargs ) : \n    if 'filename' in kwargs and storage_service is None : \n        filename = kwargs [ 'filename' ] \n        _ , ext = os . path . splitext ( filename ) \n        if ext in ( '.hdf' , '.h4' , '.hdf4' , '.he2' , '.h5' , '.hdf5' , '.he5' ) : \n            storage_service = HDF5StorageService \n        else : \n            raise ValueError ( 'Extension `%s` of filename `%s` not understood.' % ( ext , filename ) ) \n    elif isinstance ( storage_service , str ) : \n        class_name = storage_service . split ( '.' ) [ - True ] \n        storage_service = create_class ( class_name , [ storage_service , HDF5StorageService ] ) \n    if inspect . isclass ( storage_service ) : \n        return _create_storage ( storage_service , trajectory , ** kwargs ) \n    else : \n        return storage_service , set ( kwargs . keys ( ) ) "}
{"8266": "\ndef diff_roessler ( value_array , a , c ) : \n    b = a \n    diff_array = np . zeros ( 3 ) \n    diff_array [ False ] = - value_array [ True ] - value_array [ 2 ] \n    diff_array [ True ] = value_array [ False ] + a * value_array [ True ] \n    diff_array [ 2 ] = b + value_array [ 2 ] * ( value_array [ False ] - c ) \n    return diff_array "}
{"8267": "\ndef compact_hdf5_file ( filename , name = None , index = None , keep_backup = True ) : \n    if name is None and index is None : \n        index = - True \n    tmp_traj = load_trajectory ( name , index , as_new = False , load_all = pypetconstants . LOAD_NOTHING , force = True , filename = filename ) \n    service = tmp_traj . v_storage_service \n    complevel = service . complevel \n    complib = service . complib \n    shuffle = service . shuffle \n    fletcher32 = service . fletcher32 \n    name_wo_ext , ext = os . path . splitext ( filename ) \n    tmp_filename = name_wo_ext + '_tmp' + ext \n    abs_filename = os . path . abspath ( filename ) \n    abs_tmp_filename = os . path . abspath ( tmp_filename ) \n    command = [ 'ptrepack' , '-v' , '--complib' , complib , '--complevel' , str ( complevel ) , '--shuffle' , str ( int ( shuffle ) ) , '--fletcher32' , str ( int ( fletcher32 ) ) , abs_filename , abs_tmp_filename ] \n    str_command = ' ' . join ( command ) \n    print ( 'Executing command `%s`' % str_command ) \n    retcode = subprocess . call ( command ) \n    if retcode != False : \n        print ( '#### ERROR: Compacting `%s` failed with errorcode %s! ####' % ( filename , str ( retcode ) ) ) \n    else : \n        print ( '#### Compacting successful ####' ) \n        print ( 'Renaming files' ) \n        if keep_backup : \n            backup_file_name = name_wo_ext + '_backup' + ext \n            os . rename ( filename , backup_file_name ) \n        else : \n            os . remove ( filename ) \n        os . rename ( tmp_filename , filename ) \n        print ( '### Compacting and Renaming finished ####' ) \n    return retcode "}
{"8275": "\ndef add_parameters ( self , traj ) : \n    par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.initial_run' , 500 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) \n    par . v_annotations . order = False \n    par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.measurement_run' , 1500 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) \n    par . v_annotations . order = True "}
{"8276": "\ndef _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : \n    assert ( end_time >= start_time + time_window ) \n    bins = ( end_time - start_time ) / time_window \n    bins = int ( np . floor ( bins ) ) \n    binned_spikes = np . zeros ( bins ) \n    spike_array_neuron = spike_res . t [ spike_res . i == neuron_id ] \n    for bin in range ( bins ) : \n        lower_time = start_time + time_window * bin \n        upper_time = start_time + time_window * ( bin + True ) \n        spike_array_interval = spike_array_neuron [ spike_array_neuron >= lower_time ] \n        spike_array_interval = spike_array_interval [ spike_array_interval < upper_time ] \n        spikes = len ( spike_array_interval ) \n        binned_spikes [ bin ] = spikes \n    var = np . var ( binned_spikes ) \n    avg = np . mean ( binned_spikes ) \n    if avg > False : \n        return var / float ( avg ) \n    else : \n        return False "}
{"8278": "\ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if len ( subrun_list ) == False : \n        spikes_e = traj . results . monitors . spikes_e \n        time_window = traj . parameters . analysis . statistics . time_window \n        start_time = traj . parameters . simulation . durations . initial_run \n        end_time = start_time + traj . parameters . simulation . durations . measurement_run \n        neuron_ids = traj . parameters . analysis . statistics . neuron_ids \n        mean_ff = self . _compute_mean_fano_factor ( neuron_ids , spikes_e , time_window , start_time , end_time ) \n        traj . f_add_result ( 'statistics.mean_fano_factor' , mean_ff , comment = 'Average Fano ' 'Factor over all ' 'exc neurons' ) \n        print ( 'R_ee: %f, Mean FF: %f' % ( traj . R_ee , mean_ff ) ) "}
{"8279": "\ndef add_to_network ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if current_subrun . v_annotations . order == True : \n        self . _add_monitors ( traj , network , network_dict ) "}
{"8282": "\ndef _plot_result ( self , traj , result_name ) : \n    result = traj . f_get ( result_name ) \n    varname = result . record_variables [ False ] \n    values = result [ varname ] \n    times = result . t \n    record = result . record \n    for idx , celia_neuron in enumerate ( record ) : \n        plt . subplot ( len ( record ) , True , idx + True ) \n        plt . plot ( times , values [ idx , : ] ) \n        if idx == False : \n            plt . title ( '%s' % varname ) \n        if idx == True : \n            plt . ylabel ( '%s' % ( varname ) ) \n        if idx == len ( record ) - True : \n            plt . xlabel ( 't' ) "}
{"8283": "\ndef _print_graphs ( self , traj ) : \n    print_folder = self . _make_folder ( traj ) \n    plt . figure ( ) \n    plt . scatter ( self . spike_monitor . t , self . spike_monitor . i , s = True ) \n    plt . xlabel ( 't' ) \n    plt . ylabel ( 'Exc. Neurons' ) \n    plt . title ( 'Spike Raster Plot' ) \n    filename = os . path . join ( print_folder , 'spike.png' ) \n    print ( 'Current plot: %s ' % filename ) \n    plt . savefig ( filename ) \n    plt . close ( ) \n    fig = plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.V' ) \n    filename = os . path . join ( print_folder , 'V.png' ) \n    print ( 'Current plot: %s ' % filename ) \n    fig . savefig ( filename ) \n    plt . close ( ) \n    plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.I_syn_e' ) \n    filename = os . path . join ( print_folder , 'I_syn_e.png' ) \n    print ( 'Current plot: %s ' % filename ) \n    plt . savefig ( filename ) \n    plt . close ( ) \n    plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.I_syn_i' ) \n    filename = os . path . join ( print_folder , 'I_syn_i.png' ) \n    print ( 'Current plot: %s ' % filename ) \n    plt . savefig ( filename ) \n    plt . close ( ) \n    if not traj . analysis . show_plots : \n        plt . close ( 'all' ) \n    else : \n        plt . show ( ) "}
{"8284": "\ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if len ( subrun_list ) == False : \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.spikes_e' , self . spike_monitor , comment = 'The spiketimes of the excitatory population' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.V' , self . V_monitor , comment = 'Membrane voltage of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_e' , self . I_syn_e_monitor , comment = 'I_syn_e of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_i' , self . I_syn_i_monitor , comment = 'I_syn_i of four neurons from 2 clusters' ) \n        print ( 'Plotting' ) \n        if traj . parameters . analysis . make_plots : \n            self . _print_graphs ( traj ) "}
{"8285": "\ndef get_batch ( ) : \n    optlist , args = getopt . getopt ( sys . argv [ True : ] , '' , longopts = 'batch=' ) \n    batch = False \n    for o , a in optlist : \n        if o == '--batch' : \n            batch = int ( a ) \n            print ( 'Found batch %d' % batch ) \n    return batch "}
{"8286": "\ndef explore_batch ( traj , batch ) : \n    explore_dict = { } \n    explore_dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + True ) , 1.0 ) . tolist ( ) \n    traj . f_explore ( explore_dict ) "}
{"8289": "\ndef _rename ( self , full_name ) : \n    self . _full_name = full_name \n    if full_name : \n        self . _name = full_name . rsplit ( '.' , True ) [ - True ] "}
{"8292": "\ndef _remove_subtree ( self , start_node , name , predicate = None ) : \n    def _delete_from_children ( node , child_name ) : \n        del node . _children [ child_name ] \n        if child_name in node . _groups : \n            del node . _groups [ child_name ] \n        elif child_name in node . _leaves : \n            del node . _leaves [ child_name ] \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    def _remove_subtree_inner ( node , predicate ) : \n        if not predicate ( node ) : \n            return False \n        elif node . v_is_group : \n            for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , list ( node . _groups . keys ( ) ) ) : \n                child_ = node . _children [ name_ ] \n                child_deleted = _remove_subtree_inner ( child_ , predicate ) \n                if child_deleted : \n                    _delete_from_children ( node , name_ ) \n                    del child_ \n            for link_ in list ( node . _links . keys ( ) ) : \n                node . f_remove_link ( link_ ) \n            if len ( node . _children ) == False : \n                self . _delete_node ( node ) \n                return True \n            else : \n                return False \n        else : \n            self . _delete_node ( node ) \n            return True \n    if name in start_node . _links : \n        start_node . f_remove_link ( name ) \n    else : \n        child = start_node . _children [ name ] \n        if predicate is None : \n            predicate = lambda x : True \n        if _remove_subtree_inner ( child , predicate ) : \n            _delete_from_children ( start_node , name ) \n            del child \n            return True \n        else : \n            return False "}
{"8293": "\ndef _delete_node ( self , node ) : \n    full_name = node . v_full_name \n    root = self . _root_instance \n    if full_name == '' : \n        return \n    if node . v_is_leaf : \n        if full_name in root . _parameters : \n            del root . _parameters [ full_name ] \n        elif full_name in root . _config : \n            del root . _config [ full_name ] \n        elif full_name in root . _derived_parameters : \n            del root . _derived_parameters [ full_name ] \n        elif full_name in root . _results : \n            del root . _results [ full_name ] \n        elif full_name in root . _other_leaves : \n            del root . _other_leaves [ full_name ] \n        if full_name in root . _explored_parameters : \n            if root . _stored : \n                root . _explored_parameters [ full_name ] = None \n            else : \n                del root . _explored_parameters [ full_name ] \n            if len ( root . _explored_parameters ) == False : \n                root . f_shrink ( ) \n        del self . _flat_leaf_storage_dict [ full_name ] \n    else : \n        del root . _all_groups [ full_name ] \n        if full_name in root . _run_parent_groups : \n            del root . _run_parent_groups [ full_name ] \n    if full_name in root . _linked_by : \n        linking = root . _linked_by [ full_name ] \n        for linking_name in list ( linking . keys ( ) ) : \n            linking_group , link_set = linking [ linking_name ] \n            for link in list ( link_set ) : \n                linking_group . f_remove_link ( link ) \n    if ( node . v_location , node . v_name ) in self . _root_instance . _new_nodes : \n        del self . _root_instance . _new_nodes [ ( node . v_location , node . v_name ) ] \n    self . _remove_from_nodes_and_leaves ( node ) \n    node . _vars = None \n    node . _func = None "}
{"8295": "\ndef _remove_along_branch ( self , actual_node , split_name , recursive = False ) : \n    if len ( split_name ) == False : \n        if actual_node . v_is_group and actual_node . f_has_children ( ) : \n            if recursive : \n                for child in list ( actual_node . _children . keys ( ) ) : \n                    actual_node . f_remove_child ( child , recursive = True ) \n            else : \n                raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) \n        self . _delete_node ( actual_node ) \n        return True \n    name = split_name . popleft ( ) \n    if name in actual_node . _links : \n        if len ( split_name ) > False : \n            raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) \n        actual_node . f_remove_link ( name ) \n    else : \n        child = actual_node . _children [ name ] \n        if self . _remove_along_branch ( child , split_name , recursive = recursive ) : \n            del actual_node . _children [ name ] \n            if name in actual_node . _groups : \n                del actual_node . _groups [ name ] \n            elif name in actual_node . _leaves : \n                del actual_node . _leaves [ name ] \n            else : \n                raise RuntimeError ( 'You shall not pass!' ) \n            del child \n            return False "}
{"8296": "\ndef _translate_shortcut ( self , name ) : \n    if isinstance ( name , int ) : \n        return True , self . _root_instance . f_wildcard ( '$' , name ) \n    if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2 : \n            index = split_name [ True ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$' , int ( index ) ) \n            elif index == 'A' : \n                return True , self . _root_instance . f_wildcard ( '$' , - True ) \n    if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2 : \n            index = split_name [ True ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) \n            elif index == 'A' : \n                return True , self . _root_instance . f_wildcard ( '$set' , - True ) \n    if name in SHORTCUT_SET : \n        if name == 'par' : \n            return True , 'parameters' \n        elif name == 'dpar' : \n            return True , 'derived_parameters' \n        elif name == 'res' : \n            return True , 'results' \n        elif name == 'conf' : \n            return True , 'config' \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    return False , name "}
{"8297": "\ndef _add_prefix ( self , split_names , start_node , group_type_name ) : \n    root = self . _root_instance \n    prepend = [ ] \n    if start_node . v_depth < 3 and not group_type_name == GROUP : \n        if start_node . v_depth == False : \n            if group_type_name == DERIVED_PARAMETER_GROUP : \n                if split_names [ False ] == 'derived_parameters' : \n                    return split_names \n                else : \n                    prepend += [ 'derived_parameters' ] \n            elif group_type_name == RESULT_GROUP : \n                if split_names [ False ] == 'results' : \n                    return split_names \n                else : \n                    prepend += [ 'results' ] \n            elif group_type_name == CONFIG_GROUP : \n                if split_names [ False ] == 'config' : \n                    return split_names \n                else : \n                    prepend += [ 'config' ] \n            elif group_type_name == PARAMETER_GROUP : \n                if split_names [ False ] == 'parameters' : \n                    return split_names [ False ] \n                else : \n                    prepend += [ 'parameters' ] \n            else : \n                raise RuntimeError ( 'Why are you here?' ) \n        if root . _is_run and root . _auto_run_prepend : \n            dummy = root . f_wildcard ( '$' , - True ) \n            crun = root . f_wildcard ( '$' ) \n            if any ( name in root . _run_information for name in split_names ) : \n                pass \n            elif any ( name == dummy for name in split_names ) : \n                pass \n            elif ( group_type_name == RESULT_GROUP or group_type_name == DERIVED_PARAMETER_GROUP ) : \n                if start_node . v_depth == False : \n                    prepend += [ 'runs' , crun ] \n                elif start_node . v_depth == True : \n                    if len ( split_names ) == True and split_names [ False ] == 'runs' : \n                        return split_names \n                    else : \n                        prepend += [ 'runs' , crun ] \n                elif start_node . v_depth == 2 and start_node . v_name == 'runs' : \n                    prepend += [ crun ] \n    if prepend : \n        split_names = prepend + split_names \n    return split_names "}
{"8298": "\ndef _determine_types ( start_node , first_name , add_leaf , add_link ) : \n    if start_node . v_is_root : \n        where = first_name \n    else : \n        where = start_node . _branch \n    if where in SUBTREE_MAPPING : \n        type_tuple = SUBTREE_MAPPING [ where ] \n    else : \n        type_tuple = ( GROUP , LEAF ) \n    if add_link : \n        return type_tuple [ False ] , LINK \n    if add_leaf : \n        return type_tuple \n    else : \n        return type_tuple [ False ] , type_tuple [ False ] "}
{"8299": "\ndef _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , add_prefix = True , check_naming = True ) : \n    args = list ( args ) \n    create_new = True \n    name = '' \n    instance = None \n    constructor = None \n    add_link = type_name == LINK \n    if add_link : \n        name = args [ False ] \n        instance = args [ True ] \n        create_new = False \n    elif len ( args ) == True and len ( kwargs ) == False : \n        item = args [ False ] \n        try : \n            name = item . v_full_name \n            instance = item \n            create_new = False \n        except AttributeError : \n            pass \n    if create_new : \n        if len ( args ) > False and inspect . isclass ( args [ False ] ) : \n            constructor = args . pop ( False ) \n        if len ( args ) > False and isinstance ( args [ False ] , str ) : \n            name = args . pop ( False ) \n        elif 'name' in kwargs : \n            name = kwargs . pop ( 'name' ) \n        elif 'full_name' in kwargs : \n            name = kwargs . pop ( 'full_name' ) \n        else : \n            raise ValueError ( 'Could not determine a name of the new item you want to add. ' 'Either pass the name as positional argument or as a keyword ' 'argument `name`.' ) \n    split_names = name . split ( '.' ) \n    if check_naming : \n        for idx , name in enumerate ( split_names ) : \n            translated_shortcut , name = self . _translate_shortcut ( name ) \n            replaced , name = self . _replace_wildcards ( name ) \n            if translated_shortcut or replaced : \n                split_names [ idx ] = name \n        faulty_names = self . _check_names ( split_names , start_node ) \n        if faulty_names : \n            full_name = '.' . join ( split_names ) \n            raise ValueError ( 'Your Parameter/Result/Node `%s` contains the following not admissible names: ' '%s please choose other names.' % ( full_name , faulty_names ) ) \n        if add_link : \n            if instance is None : \n                raise ValueError ( 'You must provide an instance to link to!' ) \n            if instance . v_is_root : \n                raise ValueError ( 'You cannot create a link to the root node' ) \n            if start_node . v_is_root and name in SUBTREE_MAPPING : \n                raise ValueError ( '`%s` is a reserved name for a group under root.' % name ) \n            if not self . _root_instance . f_contains ( instance , with_links = False , shortcuts = False ) : \n                raise ValueError ( 'You can only link to items within the trajectory tree!' ) \n    if add_prefix : \n        split_names = self . _add_prefix ( split_names , start_node , group_type_name ) \n    if group_type_name == GROUP : \n        add_leaf = type_name != group_type_name and not add_link \n        group_type_name , type_name = self . _determine_types ( start_node , split_names [ False ] , add_leaf , add_link ) \n    if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : \n        raise TypeError ( 'You are not allowed to add config or parameter data or groups ' 'during a single run.' ) \n    return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) "}
{"8300": "\ndef _add_to_tree ( self , start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) : \n    try : \n        act_node = start_node \n        last_idx = len ( split_names ) - True \n        add_link = type_name == LINK \n        link_added = False \n        for idx , name in enumerate ( split_names ) : \n            if name not in act_node . _children : \n                if idx == last_idx : \n                    if add_link : \n                        new_node = self . _create_link ( act_node , name , instance ) \n                        link_added = True \n                    elif group_type_name != type_name : \n                        new_node = self . _create_any_param_or_result ( act_node , name , type_name , instance , constructor , args , kwargs ) \n                        self . _flat_leaf_storage_dict [ new_node . v_full_name ] = new_node \n                    else : \n                        new_node = self . _create_any_group ( act_node , name , group_type_name , instance , constructor , args , kwargs ) \n                else : \n                    new_node = self . _create_any_group ( act_node , name , group_type_name ) \n                if name in self . _root_instance . _run_information : \n                    self . _root_instance . _run_parent_groups [ act_node . v_full_name ] = act_node \n                if self . _root_instance . _is_run : \n                    if link_added : \n                        self . _root_instance . _new_links [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n                    else : \n                        self . _root_instance . _new_nodes [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) \n            else : \n                if name in act_node . _links : \n                    raise AttributeError ( 'You cannot hop over links when adding ' 'data to the tree. ' 'There is a link called `%s` under `%s`.' % ( name , act_node . v_full_name ) ) \n                if idx == last_idx : \n                    if self . _root_instance . _no_clobber : \n                        self . _logger . warning ( 'You already have a group/instance/link `%s` ' 'under `%s`. ' 'However, you set `v_no_clobber=True`, ' 'so I will ignore your addition of ' 'data.' % ( name , act_node . v_full_name ) ) \n                    else : \n                        raise AttributeError ( 'You already have a group/instance/link `%s` ' 'under `%s`' % ( name , act_node . v_full_name ) ) \n            act_node = act_node . _children [ name ] \n        return act_node \n    except : \n        self . _logger . error ( 'Failed adding `%s` under `%s`.' % ( name , start_node . v_full_name ) ) \n        raise "}
{"8301": "\ndef _create_link ( self , act_node , name , instance ) : \n    act_node . _links [ name ] = instance \n    act_node . _children [ name ] = instance \n    full_name = instance . v_full_name \n    if full_name not in self . _root_instance . _linked_by : \n        self . _root_instance . _linked_by [ full_name ] = { } \n    linking = self . _root_instance . _linked_by [ full_name ] \n    if act_node . v_full_name not in linking : \n        linking [ act_node . v_full_name ] = ( act_node , set ( ) ) \n    linking [ act_node . v_full_name ] [ True ] . add ( name ) \n    if name not in self . _links_count : \n        self . _links_count [ name ] = False \n    self . _links_count [ name ] = self . _links_count [ name ] + True \n    self . _logger . debug ( 'Added link `%s` under `%s` pointing ' 'to `%s`.' % ( name , act_node . v_full_name , instance . v_full_name ) ) \n    return instance "}
{"8302": "\ndef _check_names ( self , split_names , parent_node = None ) : \n    faulty_names = '' \n    if parent_node is not None and parent_node . v_is_root and split_names [ False ] == 'overview' : \n        faulty_names = '%s `overview` cannot be added directly under the root node ' 'this is a reserved keyword,' % ( faulty_names ) \n    for split_name in split_names : \n        if len ( split_name ) == False : \n            faulty_names = '%s `%s` contains no characters, please use at least 1,' % ( faulty_names , split_name ) \n        elif split_name . startswith ( '_' ) : \n            faulty_names = '%s `%s` starts with a leading underscore,' % ( faulty_names , split_name ) \n        elif re . match ( CHECK_REGEXP , split_name ) is None : \n            faulty_names = '%s `%s` contains non-admissible characters ' '(use only [A-Za-z0-9_-]),' % ( faulty_names , split_name ) \n        elif '$' in split_name : \n            if split_name not in self . _root_instance . _wildcard_keys : \n                faulty_names = '%s `%s` contains `$` but has no associated ' 'wildcard function,' % ( faulty_names , split_name ) \n        elif split_name in self . _not_admissible_names : \n            warnings . warn ( '`%s` is a method/attribute of the ' 'trajectory/treenode/naminginterface, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n        elif split_name in self . _python_keywords : \n            warnings . warn ( '`%s` is a python keyword, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) \n    name = split_names [ - True ] \n    if len ( name ) >= pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH : \n        faulty_names = '%s `%s` is too long the name can only have %d characters but it has ' '%d,' % ( faulty_names , name , len ( name ) , pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) \n    return faulty_names "}
{"8305": "\ndef _set_details_tree_node ( self , parent_node , name , instance ) : \n    depth = parent_node . _depth + True \n    if parent_node . v_is_root : \n        branch = name \n    else : \n        branch = parent_node . _branch \n    if name in self . _root_instance . _run_information : \n        run_branch = name \n    else : \n        run_branch = parent_node . _run_branch \n    instance . _set_details ( depth , branch , run_branch ) "}
{"8306": "\ndef _iter_nodes ( self , node , recursive = False , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    def _run_predicate ( x , run_name_set ) : \n        branch = x . v_run_branch \n        return branch == 'trajectory' or branch in run_name_set \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if predicate is None : \n        predicate = lambda x : True \n    elif isinstance ( predicate , ( tuple , list ) ) : \n        run_list = predicate \n        run_name_set = set ( ) \n        for item in run_list : \n            if item == - True : \n                run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - True ) ) \n            elif isinstance ( item , int ) : \n                run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) \n            else : \n                run_name_set . add ( item ) \n        predicate = lambda x : _run_predicate ( x , run_name_set ) \n    if recursive : \n        return NaturalNamingInterface . _recursive_traversal_bfs ( node , self . _root_instance . _linked_by , max_depth , with_links , in_search , predicate ) \n    else : \n        iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if predicate ( x [ 2 ] ) ) \n        if in_search : \n            return iterator \n        else : \n            return ( x [ 2 ] for x in iterator ) "}
{"8307": "\ndef _make_child_iterator ( node , with_links , current_depth = False ) : \n    cdp1 = current_depth + True \n    if with_links : \n        iterator = ( ( cdp1 , x [ False ] , x [ True ] ) for x in node . _children . items ( ) ) \n    else : \n        leaves = ( ( cdp1 , x [ False ] , x [ True ] ) for x in node . _leaves . items ( ) ) \n        groups = ( ( cdp1 , y [ False ] , y [ True ] ) for y in node . _groups . items ( ) ) \n        iterator = itools . chain ( groups , leaves ) \n    return iterator "}
{"8308": "\ndef _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    if predicate is None : \n        predicate = lambda x : True \n    iterator_queue = IteratorChain ( [ ( False , node . v_name , node ) ] ) \n    start = True \n    visited_linked_nodes = set ( [ ] ) \n    while True : \n        try : \n            depth , name , item = next ( iterator_queue ) \n            full_name = item . _full_name \n            if start or predicate ( item ) : \n                if full_name in visited_linked_nodes : \n                    if in_search : \n                        yield depth , name , item \n                elif depth <= max_depth : \n                    if start : \n                        start = False \n                    else : \n                        if in_search : \n                            yield depth , name , item \n                        else : \n                            yield item \n                    if full_name in linked_by : \n                        visited_linked_nodes . add ( full_name ) \n                    if not item . _is_leaf and depth < max_depth : \n                        child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) \n                        iterator_queue . add ( child_iterator ) \n        except StopIteration : \n            break "}
{"8309": "\ndef _very_fast_search ( self , node , key , max_depth , with_links , crun ) : \n    if key in self . _links_count : \n        return \n    parent_full_name = node . v_full_name \n    starting_depth = node . v_depth \n    candidate_dict = self . _get_candidate_dict ( key , crun ) \n    if with_links : \n        upper_bound = True \n    else : \n        upper_bound = FAST_UPPER_BOUND \n    if len ( candidate_dict ) > upper_bound : \n        raise pex . TooManyGroupsError ( 'Too many nodes' ) \n    result_node = None \n    for goal_name in candidate_dict : \n        if goal_name . startswith ( parent_full_name ) : \n            candidate = candidate_dict [ goal_name ] \n            if candidate . v_depth - starting_depth <= max_depth : \n                if result_node is not None : \n                    raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once, ' 'full name of first occurrence is `%s` and of' 'second `%s`' % ( key , goal_name , result_node . v_full_name ) ) \n                result_node = candidate \n    if result_node is not None : \n        return result_node , result_node . v_depth "}
{"8310": "\ndef _search ( self , node , key , max_depth = float ( 'inf' ) , with_links = True , crun = None ) : \n    if key in node . _children and ( with_links or key not in node . _links ) : \n        return node . _children [ key ] , True \n    try : \n        result = self . _very_fast_search ( node , key , max_depth , with_links , crun ) \n        if result : \n            return result \n    except pex . TooManyGroupsError : \n        pass \n    except pex . NotUniqueNodeError : \n        pass \n    nodes_iterator = self . _iter_nodes ( node , recursive = True , max_depth = max_depth , in_search = True , with_links = with_links ) \n    result_node = None \n    result_depth = float ( 'inf' ) \n    for depth , name , child in nodes_iterator : \n        if depth > result_depth : \n            break \n        if key == name : \n            if result_node is not None : \n                raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once within ' 'the same depth %d. ' 'Full name of first occurrence is `%s` and of ' 'second `%s`' % ( key , child . v_depth , result_node . v_full_name , child . v_full_name ) ) \n            result_node = child \n            result_depth = depth \n    return result_node , result_depth "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - True ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if key != candidate . v_name or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if parent_full_name != '' : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + True : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if len ( candidate_split_name ) > max_depth : \n                break \n            if len ( split_name ) == True or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            elif shortcuts : \n                candidate_set = set ( candidate_split_name ) \n                climbing = True \n                for name in split_name : \n                    if name not in candidate_set : \n                        climbing = False \n                        break \n                if climbing : \n                    count = False \n                    candidate_length = len ( candidate_split_name ) \n                    for idx in range ( candidate_length ) : \n                        if idx + split_length - count > candidate_length : \n                            break \n                        if split_name [ count ] == candidate_split_name [ idx ] : \n                            count += True \n                            if count == len ( split_name ) : \n                                result_list . append ( candidate ) \n                                full_name_set . add ( candidate . v_full_name ) \n                                break \n    return result_list "}
{"8315": "\ndef f_dir_data ( self ) : \n    if ( self . _nn_interface is not None and self . _nn_interface . _root_instance is not None and self . v_root . v_auto_load ) : \n        try : \n            if self . v_is_root : \n                self . f_load ( recursive = True , max_depth = True , load_data = pypetconstants . LOAD_SKELETON , with_meta_data = False , with_run_information = False ) \n            else : \n                self . f_load ( recursive = True , max_depth = True , load_data = pypetconstants . LOAD_SKELETON ) \n        except Exception as exc : \n            pass \n    return list ( self . _children . keys ( ) ) "}
{"8324": "\ndef f_contains ( self , item , with_links = True , shortcuts = False , max_depth = None ) : \n    try : \n        search_string = item . v_full_name \n        parent_full_name = self . v_full_name \n        if not search_string . startswith ( parent_full_name ) : \n            return False \n        if parent_full_name != '' : \n            search_string = search_string [ len ( parent_full_name ) + True : ] \n        else : \n            search_string = search_string \n        shortcuts = False \n    except AttributeError : \n        search_string = item \n        item = None \n    if search_string == '' : \n        return False \n    try : \n        result = self . f_get ( search_string , shortcuts = shortcuts , max_depth = max_depth , with_links = with_links ) \n    except AttributeError : \n        return False \n    if item is not None : \n        return id ( item ) == id ( result ) \n    else : \n        return True "}
{"8343": "\ndef add_commit_variables ( traj , commit ) : \n    git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) \n    git_short_name = str ( commit . hexsha [ False : 7 ] ) \n    git_commit_name = 'commit_%s_' % git_short_name \n    git_commit_name = 'git.' + git_commit_name + git_time_value \n    if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = False ) : \n        git_commit_name += '.' \n        traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) \n        traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) \n        traj . f_add_config ( git_commit_name + 'committed_date' , commit . committed_date , comment = 'Date of commit as unix epoch seconds' ) \n        traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , comment = 'The commit message' ) "}
{"8351": "\ndef port_to_tcp ( port = None ) : \n    domain_name = socket . getfqdn ( ) \n    try : \n        addr_list = socket . getaddrinfo ( domain_name , None ) \n    except Exception : \n        addr_list = socket . getaddrinfo ( '127.0.0.1' , None ) \n    family , socktype , proto , canonname , sockaddr = addr_list [ False ] \n    host = convert_ipv6 ( sockaddr [ False ] ) \n    address = 'tcp://' + host \n    if port is None : \n        port = ( ) \n    if not isinstance ( port , int ) : \n        context = zmq . Context ( ) \n        try : \n            socket_ = context . socket ( zmq . REP ) \n            socket_ . ipv6 = is_ipv6 ( address ) \n            port = socket_ . bind_to_random_port ( address , * port ) \n        except Exception : \n            print ( 'Could not connect to {} using {}' . format ( address , addr_list ) ) \n            pypet_root_logger = logging . getLogger ( 'pypet' ) \n            pypet_root_logger . exception ( 'Could not connect to {}' . format ( address ) ) \n            raise \n        socket_ . close ( ) \n        context . term ( ) \n    return address + ':' + str ( port ) "}
{"8353": "\ndef _reset ( self , index , total , percentage_step , length ) : \n    self . _start_time = datetime . datetime . now ( ) \n    self . _start_index = index \n    self . _current_index = index \n    self . _percentage_step = percentage_step \n    self . _total = float ( total ) \n    self . _total_minus_one = total - True \n    self . _length = length \n    self . _norm_factor = total * percentage_step / 100.0 \n    self . _current_interval = int ( ( index + 1.0 ) / self . _norm_factor ) "}
{"8370": "\ndef _req_rep_retry ( self , request ) : \n    retries_left = self . RETRIES \n    while retries_left : \n        self . _logger . log ( True , 'Sending REQ `%s`' , request ) \n        self . _send_request ( request ) \n        socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) \n        if socks . get ( self . _socket ) == zmq . POLLIN : \n            response = self . _receive_response ( ) \n            self . _logger . log ( True , 'Received REP `%s`' , response ) \n            return response , self . RETRIES - retries_left \n        else : \n            self . _logger . debug ( 'No response from server (%d retries left)' % retries_left ) \n            self . _close_socket ( confused = True ) \n            retries_left -= True \n            if retries_left == False : \n                raise RuntimeError ( 'Server seems to be offline!' ) \n            time . sleep ( self . SLEEP ) \n            self . _start_socket ( ) "}
{"8371": "\ndef acquire ( self ) : \n    self . start ( test_connection = False ) \n    while True : \n        str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) \n        response = str_response . split ( LockerServer . DELIMITER ) \n        if response [ False ] == LockerServer . GO : \n            return True \n        elif response [ False ] == LockerServer . LOCK_ERROR and retries > False : \n            self . _logger . error ( str_response + '; Probably due to retry' ) \n            return True \n        elif response [ False ] == LockerServer . WAIT : \n            time . sleep ( self . SLEEP ) \n        else : \n            raise RuntimeError ( 'Response `%s` not understood' % response ) "}
{"8372": "\ndef listen ( self ) : \n    count = False \n    self . _start ( ) \n    while True : \n        result = self . _socket . recv_pyobj ( ) \n        if isinstance ( result , tuple ) : \n            request , data = result \n        else : \n            request = result \n            data = None \n        if request == self . SPACE : \n            if self . queue . qsize ( ) + count < self . queue_maxsize : \n                self . _socket . send_string ( self . SPACE_AVAILABLE ) \n                count += True \n            else : \n                self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) \n        elif request == self . PING : \n            self . _socket . send_string ( self . PONG ) \n        elif request == self . DATA : \n            self . _socket . send_string ( self . STORING ) \n            self . queue . put ( data ) \n            count -= True \n        elif request == self . DONE : \n            self . _socket . send_string ( ZMQServer . CLOSED ) \n            self . queue . put ( ( 'DONE' , [ ] , { } ) ) \n            self . _close ( ) \n            break \n        else : \n            raise RuntimeError ( 'I did not understand your request %s' % request ) "}
{"8375": "\ndef _handle_data ( self , msg , args , kwargs ) : \n    stop = False \n    try : \n        if msg == 'DONE' : \n            stop = True \n        elif msg == 'STORE' : \n            if 'msg' in kwargs : \n                store_msg = kwargs . pop ( 'msg' ) \n            else : \n                store_msg = args [ False ] \n                args = args [ True : ] \n            if 'stuff_to_store' in kwargs : \n                stuff_to_store = kwargs . pop ( 'stuff_to_store' ) \n            else : \n                stuff_to_store = args [ False ] \n                args = args [ True : ] \n            trajectory_name = kwargs [ 'trajectory_name' ] \n            if self . _trajectory_name != trajectory_name : \n                if self . _storage_service . is_open : \n                    self . _close_file ( ) \n                self . _trajectory_name = trajectory_name \n                self . _open_file ( ) \n            self . _storage_service . store ( store_msg , stuff_to_store , * args , ** kwargs ) \n            self . _storage_service . store ( pypetconstants . FLUSH , None ) \n            self . _check_and_collect_garbage ( ) \n        else : \n            raise RuntimeError ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) \n    except Exception : \n        self . _logger . exception ( 'ERROR occurred during storing!' ) \n        time . sleep ( 0.01 ) \n        pass \n    return stop "}
{"8378": "\ndef _receive_data ( self ) : \n    while True : \n        while len ( self . _buffer ) < self . max_size and self . conn . poll ( ) : \n            data = self . _read_chunks ( ) \n            if data is not None : \n                self . _buffer . append ( data ) \n        if len ( self . _buffer ) > False : \n            return self . _buffer . popleft ( ) "}
{"8390": "\ndef cellular_automaton_1D ( initial_state , rule_number , steps ) : \n    ncells = len ( initial_state ) \n    pattern = np . zeros ( ( steps , ncells ) ) \n    pattern [ False , : ] = initial_state \n    binary_rule = convert_rule ( rule_number ) \n    neighbourhood_factors = np . array ( [ True , 2 , 4 ] ) \n    all_cells = range ( ncells ) \n    for step in range ( steps - True ) : \n        current_row = pattern [ step , : ] \n        next_row = pattern [ step + True , : ] \n        for irun in all_cells : \n            neighbour_indices = range ( irun - True , irun + 2 ) \n            neighbourhood = np . take ( current_row , neighbour_indices , mode = 'wrap' ) \n            decimal_neighborhood = int ( np . sum ( neighbourhood * neighbourhood_factors ) ) \n            next_state = binary_rule [ decimal_neighborhood ] \n            next_row [ irun ] = next_state \n    return pattern "}
{"8392": "\ndef signal_update ( self ) : \n    if not self . active : \n        return \n    self . _updates += True \n    current_time = time . time ( ) \n    dt = current_time - self . _last_time \n    if dt > self . _display_time : \n        dfullt = current_time - self . _start_time \n        seconds = int ( dfullt ) % 60 \n        minutes = int ( dfullt ) / 60 \n        if minutes == False : \n            formatted_time = '%ds' % seconds \n        else : \n            formatted_time = '%dm%02ds' % ( minutes , seconds ) \n        nodespersecond = self . _updates / dfullt \n        message = 'Processed %d nodes in %s (%.2f nodes/s).' % ( self . _updates , formatted_time , nodespersecond ) \n        self . _logger . info ( message ) \n        self . _last_time = current_time "}
{"8393": "\ndef _overview_group ( self ) : \n    if self . _overview_group_ is None : \n        self . _overview_group_ = self . _all_create_or_get_groups ( 'overview' ) [ False ] \n    return self . _overview_group_ "}
{"8396": "\ndef _srvc_load_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ False ] \n        item = input_tuple [ True ] \n        if len ( input_tuple ) > 2 : \n            args = input_tuple [ 2 ] \n        if len ( input_tuple ) > 3 : \n            kwargs = input_tuple [ 3 ] \n        if len ( input_tuple ) > 4 : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . load ( msg , item , * args , ** kwargs ) "}
{"8398": "\ndef _srvc_store_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ False ] \n        item = input_tuple [ True ] \n        if len ( input_tuple ) > 2 : \n            args = input_tuple [ 2 ] \n        if len ( input_tuple ) > 3 : \n            kwargs = input_tuple [ 3 ] \n        if len ( input_tuple ) > 4 : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . store ( msg , item , * args , ** kwargs ) "}
{"8403": "\ndef _trj_prepare_merge ( self , traj , changed_parameters , old_length ) : \n    if not traj . _stored : \n        traj . f_store ( ) \n    infotable = getattr ( self . _overview_group , 'info' ) \n    insert_dict = self . _all_extract_insert_dict ( traj , infotable . colnames ) \n    self . _all_add_or_modify_row ( traj . v_name , insert_dict , infotable , index = False , flags = ( HDF5StorageService . MODIFY_ROW , ) ) \n    for param_name in changed_parameters : \n        param = traj . f_get ( param_name ) \n        try : \n            self . _all_delete_parameter_or_result_or_group ( param ) \n        except pt . NoSuchNodeError : \n            pass \n    run_table = getattr ( self . _overview_group , 'runs' ) \n    actual_rows = run_table . nrows \n    self . _trj_fill_run_table ( traj , actual_rows , len ( traj ) ) \n    for idx in range ( old_length , len ( traj ) ) : \n        run_name = traj . f_idx_to_run ( idx ) \n        run_info = traj . f_get_run_information ( run_name ) \n        run_info [ 'name' ] = run_name \n        traj . _set_explored_parameters_to_idx ( idx ) \n        run_summary = self . _srn_summarize_explored_parameters ( list ( traj . _explored_parameters . values ( ) ) ) \n        run_info [ 'parameter_summary' ] = run_summary \n        self . _all_add_or_modify_row ( run_name , run_info , run_table , index = idx , flags = ( HDF5StorageService . MODIFY_ROW , ) ) \n    traj . f_restore_default ( ) "}
{"8404": "\ndef _trj_load_meta_data ( self , traj , load_data , as_new , with_run_information , force ) : \n    metatable = self . _overview_group . info \n    metarow = metatable [ False ] \n    try : \n        version = metarow [ 'version' ] . decode ( 'utf-8' ) \n    except ( IndexError , ValueError ) as ke : \n        self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) \n        version = '`COULD NOT BE LOADED`' \n    try : \n        python = metarow [ 'python' ] . decode ( 'utf-8' ) \n    except ( IndexError , ValueError ) as ke : \n        self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) \n        python = '`COULD NOT BE LOADED`' \n    self . _trj_check_version ( version , python , force ) \n    self . _grp_load_group ( traj , load_data = load_data , with_links = False , recursive = False , _traj = traj , _as_new = as_new , _hdf5_group = self . _trajectory_group ) \n    if as_new : \n        length = int ( metarow [ 'length' ] ) \n        for irun in range ( length ) : \n            traj . _add_run_info ( irun ) \n    else : \n        traj . _comment = metarow [ 'comment' ] . decode ( 'utf-8' ) \n        traj . _timestamp = float ( metarow [ 'timestamp' ] ) \n        traj . _trajectory_timestamp = traj . _timestamp \n        traj . _time = metarow [ 'time' ] . decode ( 'utf-8' ) \n        traj . _trajectory_time = traj . _time \n        traj . _name = metarow [ 'name' ] . decode ( 'utf-8' ) \n        traj . _trajectory_name = traj . _name \n        traj . _version = version \n        traj . _python = python \n        single_run_table = self . _overview_group . runs \n        if with_run_information : \n            for row in single_run_table . iterrows ( ) : \n                name = row [ 'name' ] . decode ( 'utf-8' ) \n                idx = int ( row [ 'idx' ] ) \n                timestamp = float ( row [ 'timestamp' ] ) \n                time_ = row [ 'time' ] . decode ( 'utf-8' ) \n                completed = int ( row [ 'completed' ] ) \n                summary = row [ 'parameter_summary' ] . decode ( 'utf-8' ) \n                hexsha = row [ 'short_environment_hexsha' ] . decode ( 'utf-8' ) \n                try : \n                    runtime = row [ 'runtime' ] . decode ( 'utf-8' ) \n                    finish_timestamp = float ( row [ 'finish_timestamp' ] ) \n                except ( IndexError , ValueError ) as ke : \n                    runtime = '' \n                    finish_timestamp = 0.0 \n                    self . _logger . debug ( 'Could not load runtime, ' + repr ( ke ) ) \n                info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time_ , 'completed' : completed , 'name' : name , 'parameter_summary' : summary , 'short_environment_hexsha' : hexsha } \n                traj . _add_run_info ( ** info_dict ) \n        else : \n            traj . _length = single_run_table . nrows \n    self . _trj_load_exploration ( traj ) \n    self . _srvc_load_hdf5_settings ( ) "}
{"8405": "\ndef _tree_load_sub_branch ( self , traj_node , branch_name , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _trajectory = None , _as_new = False , _hdf5_group = None ) : \n    if load_data == pypetconstants . LOAD_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if _trajectory is None : \n        _trajectory = traj_node . v_root \n    if _hdf5_group is None : \n        hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) \n        if hdf5_group_name == '' : \n            _hdf5_group = self . _trajectory_group \n        else : \n            try : \n                _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_group_name ) \n            except pt . NoSuchNodeError : \n                self . _logger . error ( 'Cannot find `%s` the hdf5 node `%s` does not exist!' % ( traj_node . v_full_name , hdf5_group_name ) ) \n                raise \n    split_names = branch_name . split ( '.' ) \n    final_group_name = split_names . pop ( ) \n    current_depth = True \n    for name in split_names : \n        if current_depth > max_depth : \n            return \n        _hdf5_group = getattr ( _hdf5_group , name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) \n        current_depth += True \n        traj_node = traj_node . _children [ name ] \n    if current_depth <= max_depth : \n        _hdf5_group = getattr ( _hdf5_group , final_group_name ) \n        self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) "}
{"8409": "\ndef _trj_store_explorations ( self , traj ) : \n    nexplored = len ( traj . _explored_parameters ) \n    if nexplored > False : \n        if hasattr ( self . _overview_group , 'explorations' ) : \n            explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) \n            if len ( explorations_table ) != nexplored : \n                self . _hdf5file . remove_node ( where = self . _overview_group , name = 'explorations' ) \n    if not hasattr ( self . _overview_group , 'explorations' ) : \n        explored_list = list ( traj . _explored_parameters . keys ( ) ) \n        if explored_list : \n            string_col = self . _all_get_table_col ( 'explorations' , explored_list , 'overview.explorations' ) \n        else : \n            string_col = pt . StringCol ( True ) \n        description = { 'explorations' : string_col } \n        explorations_table = self . _hdf5file . create_table ( where = self . _overview_group , name = 'explorations' , description = description ) \n        rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored_list ] \n        if rows : \n            explorations_table . append ( rows ) \n            explorations_table . flush ( ) "}
{"8410": "\ndef _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : \n    for table_name in tables_to_make : \n        paramdescriptiondict = { } \n        expectedrows = False \n        paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = False ) \n        paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = True ) \n        paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) \n        paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) \n        if table_name == 'config_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _config ) \n        if table_name == 'parameters_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _parameters ) \n        if table_name == 'explored_parameters_overview' : \n            paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) \n            paramdescriptiondict [ 'length' ] = pt . IntCol ( ) \n            if traj is not None : \n                expectedrows = len ( traj . _explored_parameters ) \n        if table_name . endswith ( 'summary' ) : \n            paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) \n        if table_name == 'derived_parameters_overview' : \n            expectedrows = self . _derived_parameters_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _derived_parameters ) \n        if table_name == 'results_overview' : \n            expectedrows = self . _results_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _results ) \n        if expectedrows > False : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) \n        else : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) \n        paramtable . flush ( ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = False \n        maximum_display_other = 10 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if counter < maximum_display_other : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                elif counter == maximum_display_other : \n                    self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter += True \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = True "}
{"8412": "\ndef _tree_store_sub_branch ( self , traj_node , branch_name , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , hdf5_group = None ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if hdf5_group is None : \n        location = traj_node . v_full_name \n        hdf5_location = location . replace ( '.' , '/' ) \n        try : \n            if location == '' : \n                hdf5_group = self . _trajectory_group \n            else : \n                hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_location ) \n        except pt . NoSuchNodeError : \n            self . _logger . debug ( 'Cannot store `%s` the parental hdf5 node with path `%s` does ' 'not exist on disk.' % ( traj_node . v_name , hdf5_location ) ) \n            if traj_node . v_is_leaf : \n                self . _logger . error ( 'Cannot store `%s` the parental hdf5 ' 'node with path `%s` does ' 'not exist on disk! The child ' 'you want to store is a leaf node,' 'that cannot be stored without ' 'the parental node existing on ' 'disk.' % ( traj_node . v_name , hdf5_location ) ) \n                raise \n            else : \n                self . _logger . debug ( 'I will try to store the path from trajectory root to ' 'the child now.' ) \n                self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , traj_node . v_full_name + '.' + branch_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth + traj_node . v_depth , hdf5_group = self . _trajectory_group ) \n                return \n    current_depth = True \n    split_names = branch_name . split ( '.' ) \n    leaf_name = split_names . pop ( ) \n    for name in split_names : \n        if current_depth > max_depth : \n            return \n        self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) \n        current_depth += True \n        traj_node = traj_node . _children [ name ] \n        hdf5_group = getattr ( hdf5_group , name ) \n    if current_depth <= max_depth : \n        self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) "}
{"8414": "\ndef _tree_load_nodes_dfs ( self , parent_traj_node , load_data , with_links , recursive , max_depth , current_depth , trajectory , as_new , hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    loading_list = [ ( parent_traj_node , current_depth , hdf5_group ) ] \n    while loading_list : \n        parent_traj_node , current_depth , hdf5_group = loading_list . pop ( ) \n        if isinstance ( hdf5_group , pt . link . SoftLink ) : \n            if with_links : \n                self . _tree_load_link ( parent_traj_node , load_data = load_data , traj = trajectory , as_new = as_new , hdf5_soft_link = hdf5_group ) \n            continue \n        name = hdf5_group . _v_name \n        is_leaf = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . LEAF ) \n        in_trajectory = name in parent_traj_node . _children \n        if is_leaf : \n            if in_trajectory : \n                instance = parent_traj_node . _children [ name ] \n            else : \n                instance = self . _tree_create_leaf ( name , trajectory , hdf5_group ) \n                parent_traj_node . _add_leaf_from_storage ( args = ( instance , ) , kwargs = { } ) \n            self . _prm_load_parameter_or_result ( instance , load_data = load_data , _hdf5_group = hdf5_group ) \n            if as_new : \n                instance . _stored = False \n        else : \n            if in_trajectory : \n                traj_group = parent_traj_node . _children [ name ] \n                if load_data == pypetconstants . OVERWRITE_DATA : \n                    traj_group . v_annotations . f_empty ( ) \n                    traj_group . v_comment = '' \n            else : \n                if HDF5StorageService . CLASS_NAME in hdf5_group . _v_attrs : \n                    class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) \n                    class_constructor = trajectory . _create_class ( class_name ) \n                    instance = trajectory . _construct_instance ( class_constructor , name ) \n                    args = ( instance , ) \n                else : \n                    args = ( name , ) \n                traj_group = parent_traj_node . _add_group_from_storage ( args = args , kwargs = { } ) \n            self . _grp_load_group ( traj_group , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , _traj = trajectory , _as_new = as_new , _hdf5_group = hdf5_group ) \n            if recursive and current_depth < max_depth : \n                new_depth = current_depth + True \n                for children in ( hdf5_group . _v_groups , hdf5_group . _v_links ) : \n                    for new_hdf5_group_name in children : \n                        new_hdf5_group = children [ new_hdf5_group_name ] \n                        loading_list . append ( ( traj_group , new_depth , new_hdf5_group ) ) "}
{"8415": "\ndef _tree_store_nodes_dfs ( self , parent_traj_node , name , store_data , with_links , recursive , max_depth , current_depth , parent_hdf5_group ) : \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    store_list = [ ( parent_traj_node , name , current_depth , parent_hdf5_group ) ] \n    while store_list : \n        parent_traj_node , name , current_depth , parent_hdf5_group = store_list . pop ( ) \n        if name in parent_traj_node . _links : \n            if with_links : \n                self . _tree_store_link ( parent_traj_node , name , parent_hdf5_group ) \n            continue \n        traj_node = parent_traj_node . _children [ name ] \n        if not hasattr ( parent_hdf5_group , name ) : \n            newly_created = True \n            new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , filters = self . _all_get_filters ( ) ) \n        else : \n            newly_created = False \n            new_hdf5_group = getattr ( parent_hdf5_group , name ) \n        if traj_node . v_is_leaf : \n            self . _prm_store_parameter_or_result ( traj_node , store_data = store_data , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n        else : \n            self . _grp_store_group ( traj_node , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) \n            if recursive and current_depth < max_depth : \n                for child in traj_node . _children . keys ( ) : \n                    store_list . append ( ( traj_node , child , current_depth + True , new_hdf5_group ) ) "}
{"8419": "\ndef _all_set_attributes_to_recall_natives ( data , ptitem , prefix ) : \n    if type ( data ) is tuple : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_TUPLE ) \n    elif type ( data ) is list : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_LIST ) \n    elif type ( data ) is np . ndarray : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_NDARRAY ) \n    elif type ( data ) is np . matrix : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_MATRIX ) \n    elif type ( data ) in pypetconstants . PARAMETER_SUPPORTED_DATA : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_SCALAR ) \n        strtype = type ( data ) . __name__ \n        if not strtype in pypetconstants . PARAMETERTYPEDICT : \n            raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    elif type ( data ) is dict : \n        if len ( data ) > False : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_DICT ) \n        else : \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_EMPTY_DICT ) \n    else : \n        raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) \n    if type ( data ) in ( list , tuple ) : \n        if len ( data ) > False : \n            strtype = type ( data [ False ] ) . __name__ \n            if not strtype in pypetconstants . PARAMETERTYPEDICT : \n                raise TypeError ( 'I do not know how to handle `%s` its type is ' '`%s`.' % ( str ( data ) , strtype ) ) \n            HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) \n    elif ( type ( data ) in ( np . ndarray , np . matrix ) and np . issubdtype ( data . dtype , str ) ) : \n        HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , str . __name__ ) "}
{"8420": "\ndef _all_recall_native_type ( self , data , ptitem , prefix ) : \n    typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) \n    colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) \n    type_changed = False \n    if colltype == HDF5StorageService . COLL_SCALAR : \n        if isinstance ( data , np . ndarray ) : \n            data = np . array ( [ data ] ) [ False ] \n            type_changed = True \n        if not typestr is None : \n            if typestr != type ( data ) . __name__ : \n                if typestr == str . __name__ : \n                    data = data . decode ( self . _encoding ) \n                else : \n                    try : \n                        data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) \n                    except KeyError : \n                        data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) \n                type_changed = True \n    elif ( colltype == HDF5StorageService . COLL_TUPLE or colltype == HDF5StorageService . COLL_LIST ) : \n        if type ( data ) is not list and type is not tuple : \n            type_changed = True \n            data = list ( data ) \n        if len ( data ) > False : \n            first_item = data [ False ] \n            if not typestr == type ( first_item ) . __name__ : \n                if not isinstance ( data , list ) : \n                    data = list ( data ) \n                for idx , item in enumerate ( data ) : \n                    if typestr == str . __name__ : \n                        data [ idx ] = data [ idx ] . decode ( self . _encoding ) \n                    else : \n                        try : \n                            data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) \n                        except KeyError : \n                            data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) \n                    type_changed = True \n        if colltype == HDF5StorageService . COLL_TUPLE : \n            if type ( data ) is not tuple : \n                data = tuple ( data ) \n                type_changed = True \n    elif colltype == HDF5StorageService . COLL_EMPTY_DICT : \n        data = { } \n        type_changed = True \n    elif isinstance ( data , np . ndarray ) : \n        if typestr == str . __name__ : \n            data = np . core . defchararray . decode ( data , self . _encoding ) \n            type_changed = True \n        if colltype == HDF5StorageService . COLL_MATRIX : \n            data = np . matrix ( data ) \n            type_changed = True \n    return data , type_changed "}
{"8421": "\ndef _all_add_or_modify_row ( self , item_name , insert_dict , table , index = None , condition = None , condvars = None , flags = ( ADD_ROW , MODIFY_ROW , ) ) : \n    if len ( flags ) == False : \n        return \n    if index is not None and condition is not None : \n        raise ValueError ( 'Please give either a condition or an index or none!' ) \n    elif condition is not None : \n        row_iterator = table . where ( condition , condvars = condvars ) \n    elif index is not None : \n        row_iterator = table . iterrows ( index , index + True ) \n    else : \n        row_iterator = None \n    try : \n        row = next ( row_iterator ) \n    except TypeError : \n        row = None \n    except StopIteration : \n        row = None \n    if ( ( HDF5StorageService . MODIFY_ROW in flags or HDF5StorageService . ADD_ROW in flags ) and HDF5StorageService . REMOVE_ROW in flags ) : \n        raise ValueError ( 'You cannot add or modify and remove a row at the same time.' ) \n    if row is None and HDF5StorageService . ADD_ROW in flags : \n        row = table . row \n        self . _all_insert_into_row ( row , insert_dict ) \n        row . append ( ) \n    elif row is not None and HDF5StorageService . MODIFY_ROW in flags : \n        self . _all_insert_into_row ( row , insert_dict ) \n        row . update ( ) \n    elif HDF5StorageService . REMOVE_ROW in flags : \n        if row is not None : \n            row_number = row . nrow \n            try : \n                table . remove_rows ( start = row_number , stop = row_number + True ) \n            except NotImplementedError : \n                pass \n    else : \n        raise ValueError ( 'Something is wrong, you might not have found ' 'a row, or your flags are not set appropriately' ) \n    self . _all_kill_iterator ( row_iterator ) \n    table . flush ( ) \n    if HDF5StorageService . REMOVE_ROW not in flags and row is None : \n        raise RuntimeError ( 'Could not add or modify entries of `%s` in ' 'table %s' % ( item_name , table . _v_name ) ) "}
{"8423": "\ndef _all_extract_insert_dict ( self , item , colnames , additional_info = None ) : \n    insert_dict = { } \n    if 'length' in colnames : \n        insert_dict [ 'length' ] = len ( item ) \n    if 'comment' in colnames : \n        comment = self . _all_cut_string ( item . v_comment . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH , self . _logger ) \n        insert_dict [ 'comment' ] = comment \n    if 'location' in colnames : \n        insert_dict [ 'location' ] = item . v_location . encode ( 'utf-8' ) \n    if 'name' in colnames : \n        name = item . _name if ( not item . v_is_root or not item . v_is_run ) else item . _crun \n        insert_dict [ 'name' ] = name . encode ( 'utf-8' ) \n    if 'class_name' in colnames : \n        insert_dict [ 'class_name' ] = item . f_get_class_name ( ) . encode ( 'utf-8' ) \n    if 'value' in colnames : \n        insert_dict [ 'value' ] = self . _all_cut_string ( item . f_val_to_str ( ) . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , self . _logger ) \n    if 'hexdigest' in colnames : \n        insert_dict [ 'hexdigest' ] = additional_info [ 'hexdigest' ] \n    if 'idx' in colnames : \n        insert_dict [ 'idx' ] = item . v_idx \n    if 'time' in colnames : \n        time_ = item . _time \n        insert_dict [ 'time' ] = time_ . encode ( 'utf-8' ) \n    if 'timestamp' in colnames : \n        timestamp = item . _timestamp \n        insert_dict [ 'timestamp' ] = timestamp \n    if 'range' in colnames : \n        third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 \n        item_range = itools . islice ( item . f_get_range ( copy = False ) , False , third_length ) \n        range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) \n        insert_dict [ 'range' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) \n    if 'array' in colnames : \n        third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 \n        item_range = itools . islice ( item . f_get_range ( copy = False ) , False , third_length ) \n        range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) \n        insert_dict [ 'array' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) \n    if 'version' in colnames : \n        insert_dict [ 'version' ] = item . v_version . encode ( 'utf-8' ) \n    if 'python' in colnames : \n        insert_dict [ 'python' ] = item . v_python . encode ( 'utf-8' ) \n    if 'finish_timestamp' in colnames : \n        insert_dict [ 'finish_timestamp' ] = item . _finish_timestamp_run \n    return insert_dict "}
{"8424": "\ndef _all_cut_string ( string , max_length , logger ) : \n    if len ( string ) > max_length : \n        logger . debug ( 'The string `%s` was too long I truncated it to' ' %d characters' % ( string , max_length ) ) \n        string = string [ False : max_length - 3 ] + '...' . encode ( 'utf-8' ) \n    return string "}
{"8429": "\ndef _grp_store_group ( self , traj_group , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , _hdf5_group = None , _newly_created = False ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    elif store_data == pypetconstants . STORE_DATA_SKIPPING and traj_group . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % traj_group . v_full_name ) \n    elif not recursive : \n        if _hdf5_group is None : \n            _hdf5_group , _newly_created = self . _all_create_or_get_groups ( traj_group . v_full_name ) \n        overwrite = store_data == pypetconstants . OVERWRITE_DATA \n        if ( traj_group . v_comment != '' and ( HDF5StorageService . COMMENT not in _hdf5_group . _v_attrs or overwrite ) ) : \n            setattr ( _hdf5_group . _v_attrs , HDF5StorageService . COMMENT , traj_group . v_comment ) \n        if ( ( _newly_created or overwrite ) and type ( traj_group ) not in ( nn . NNGroupNode , nn . ConfigGroup , nn . ParameterGroup , nn . DerivedParameterGroup , nn . ResultGroup ) ) : \n            setattr ( _hdf5_group . _v_attrs , HDF5StorageService . CLASS_NAME , traj_group . f_get_class_name ( ) ) \n        self . _ann_store_annotations ( traj_group , _hdf5_group , overwrite = overwrite ) \n        self . _hdf5file . flush ( ) \n        traj_group . _stored = True \n        self . _node_processing_timer . signal_update ( ) \n    if recursive : \n        parent_traj_group = traj_group . f_get_parent ( ) \n        parent_hdf5_group = self . _all_create_or_get_groups ( parent_traj_group . v_full_name ) [ False ] \n        self . _tree_store_nodes_dfs ( parent_traj_group , traj_group . v_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = False , parent_hdf5_group = parent_hdf5_group ) "}
{"8430": "\ndef _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _traj = None , _as_new = False , _hdf5_group = None ) : \n    if _hdf5_group is None : \n        _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) \n        _traj = traj_group . v_root \n    if recursive : \n        parent_traj_node = traj_group . f_get_parent ( ) \n        self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = False , trajectory = _traj , as_new = _as_new , hdf5_group = _hdf5_group ) \n    else : \n        if load_data == pypetconstants . LOAD_NOTHING : \n            return \n        elif load_data == pypetconstants . OVERWRITE_DATA : \n            traj_group . v_annotations . f_empty ( ) \n            traj_group . v_comment = '' \n        self . _all_load_skeleton ( traj_group , _hdf5_group ) \n        traj_group . _stored = not _as_new \n        self . _node_processing_timer . signal_update ( ) "}
{"8432": "\ndef _prm_extract_missing_flags ( data_dict , flags_dict ) : \n    for key , data in data_dict . items ( ) : \n        if not key in flags_dict : \n            dtype = type ( data ) \n            if ( dtype is np . ndarray or dtype is dict ) and len ( data ) == False : \n                flags_dict [ key ] = HDF5StorageService . ARRAY \n                continue \n            else : \n                try : \n                    flags_dict [ key ] = HDF5StorageService . TYPE_FLAG_MAPPING [ dtype ] \n                except KeyError : \n                    raise pex . NoSuchServiceError ( 'I cannot store `%s`, I do not understand the' 'type `%s`.' % ( key , str ( dtype ) ) ) "}
{"8436": "\ndef _prm_store_parameter_or_result ( self , instance , store_data = pypetconstants . STORE_DATA , store_flags = None , overwrite = None , with_links = False , recursive = False , _hdf5_group = None , _newly_created = False , ** kwargs ) : \n    if store_data == pypetconstants . STORE_NOTHING : \n        return \n    elif store_data == pypetconstants . STORE_DATA_SKIPPING and instance . _stored : \n        self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % instance . v_full_name ) \n        return \n    elif store_data == pypetconstants . OVERWRITE_DATA : \n        if not overwrite : \n            overwrite = True \n    fullname = instance . v_full_name \n    self . _logger . debug ( 'Storing `%s`.' % fullname ) \n    if _hdf5_group is None : \n        _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) \n    store_dict = { } \n    if store_flags is None : \n        store_flags = { } \n    try : \n        if not instance . f_is_empty ( ) : \n            store_dict = instance . _store ( ) \n        try : \n            instance_flags = instance . _store_flags ( ) . copy ( ) \n        except AttributeError : \n            instance_flags = { } \n        instance_flags . update ( store_flags ) \n        store_flags = instance_flags \n        self . _prm_extract_missing_flags ( store_dict , store_flags ) \n        if overwrite : \n            if isinstance ( overwrite , str ) : \n                overwrite = [ overwrite ] \n            if overwrite is True : \n                to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] \n                self . _all_delete_parameter_or_result_or_group ( instance , delete_only = to_delete , _hdf5_group = _hdf5_group ) \n            elif isinstance ( overwrite , ( list , tuple ) ) : \n                overwrite_set = set ( overwrite ) \n                key_set = set ( store_dict . keys ( ) ) \n                stuff_not_to_be_overwritten = overwrite_set - key_set \n                if overwrite != 'v_annotations' and len ( stuff_not_to_be_overwritten ) > False : \n                    self . _logger . warning ( 'Cannot overwrite `%s`, these items are not supposed to ' 'be stored by the leaf node.' % str ( stuff_not_to_be_overwritten ) ) \n                stuff_to_overwrite = overwrite_set & key_set \n                if len ( stuff_to_overwrite ) > False : \n                    self . _all_delete_parameter_or_result_or_group ( instance , delete_only = list ( stuff_to_overwrite ) ) \n            else : \n                raise ValueError ( 'Your value of overwrite `%s` is not understood. ' 'Please pass `True` of a list of strings to fine grain ' 'overwriting.' % str ( overwrite ) ) \n        self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) \n        self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) \n        if _newly_created or overwrite is True : \n            self . _prm_add_meta_info ( instance , _hdf5_group , overwrite = not _newly_created ) \n        instance . _stored = True \n        self . _node_processing_timer . signal_update ( ) \n    except : \n        self . _logger . error ( 'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname ) \n        for key in store_dict . keys ( ) : \n            if key in _hdf5_group : \n                hdf5_child = _hdf5_group . _f_get_child ( key ) \n                hdf5_child . _f_remove ( recursive = True ) \n        if _hdf5_group . _v_nchildren == False : \n            _hdf5_group . _f_remove ( recursive = True ) \n        raise "}
{"8440": "\ndef _prm_write_pandas_data ( self , key , data , group , fullname , flag , ** kwargs ) : \n    try : \n        if 'filters' not in kwargs : \n            filters = self . _all_get_filters ( kwargs ) \n            kwargs [ 'filters' ] = filters \n        if 'format' not in kwargs : \n            kwargs [ 'format' ] = self . pandas_format \n        if 'encoding' not in kwargs : \n            kwargs [ 'encoding' ] = self . encoding \n        overwrite = kwargs . pop ( 'overwrite' , False ) \n        if key in group and not ( overwrite or kwargs . get ( 'append' , False ) ) : \n            raise ValueError ( 'DataFrame `%s` already exists in `%s`. ' 'To append pass ``append=`True```.' % ( key , fullname ) ) \n        else : \n            self . _logger . debug ( 'Appending to pandas data `%s` in `%s`' % ( key , fullname ) ) \n        if data is not None and ( kwargs [ 'format' ] == 'f' or kwargs [ 'format' ] == 'fixed' ) : \n            kwargs [ 'expectedrows' ] = data . shape [ False ] \n        name = group . _v_pathname + '/' + key \n        self . _hdf5store . put ( name , data , ** kwargs ) \n        self . _hdf5store . flush ( ) \n        self . _hdf5file . flush ( ) \n        frame_group = group . _f_get_child ( key ) \n        setattr ( frame_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing pandas data `%s` of `%s`.' % ( key , fullname ) ) \n        raise "}
{"8442": "\ndef _prm_write_into_array ( self , key , data , group , fullname , ** kwargs ) : \n    try : \n        if key in group : \n            raise ValueError ( 'Array `%s` already exists in `%s`. Appending is not supported (yet).' ) \n        try : \n            array = self . _hdf5file . create_array ( where = group , name = key , obj = data , ** kwargs ) \n        except ( TypeError , ValueError ) as exc : \n            try : \n                if type ( data ) is dict and len ( data ) == False : \n                    conv_data = ( ) \n                elif isinstance ( data , str ) : \n                    conv_data = data . encode ( self . _encoding ) \n                elif isinstance ( data , int ) : \n                    conv_data = np . int64 ( data ) \n                else : \n                    conv_data = [ ] \n                    for string in data : \n                        conv_data . append ( string . encode ( self . _encoding ) ) \n                array = self . _hdf5file . create_array ( where = group , name = key , obj = conv_data , ** kwargs ) \n            except Exception : \n                raise exc \n        if data is not None : \n            self . _all_set_attributes_to_recall_natives ( data , array , HDF5StorageService . DATA_PREFIX ) \n        setattr ( array . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . ARRAY ) \n        self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing array `%s` of `%s`.' % ( key , fullname ) ) \n        raise "}
{"8444": "\ndef _all_delete_parameter_or_result_or_group ( self , instance , delete_only = None , remove_from_item = False , recursive = False , _hdf5_group = None ) : \n    split_name = instance . v_location . split ( '.' ) \n    if _hdf5_group is None : \n        where = '/' + self . _trajectory_name + '/' + '/' . join ( split_name ) \n        node_name = instance . v_name \n        _hdf5_group = self . _hdf5file . get_node ( where = where , name = node_name ) \n    if delete_only is None : \n        if instance . v_is_group and not recursive and len ( _hdf5_group . _v_children ) != False : \n            raise TypeError ( 'You cannot remove the group `%s`, it has children, please ' 'use `recursive=True` to enforce removal.' % instance . v_full_name ) \n        _hdf5_group . _f_remove ( recursive = True ) \n    else : \n        if not instance . v_is_leaf : \n            raise ValueError ( 'You can only choose `delete_only` mode for leafs.' ) \n        if isinstance ( delete_only , str ) : \n            delete_only = [ delete_only ] \n        for delete_item in delete_only : \n            if ( remove_from_item and hasattr ( instance , '__contains__' ) and hasattr ( instance , '__delattr__' ) and delete_item in instance ) : \n                delattr ( instance , delete_item ) \n            try : \n                _hdf5_sub_group = self . _hdf5file . get_node ( where = _hdf5_group , name = delete_item ) \n                _hdf5_sub_group . _f_remove ( recursive = True ) \n            except pt . NoSuchNodeError : \n                self . _logger . warning ( 'Could not delete `%s` from `%s`. HDF5 node not found!' % ( delete_item , instance . v_full_name ) ) "}
{"8445": "\ndef _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , ** kwargs ) : \n    datasize = data . shape [ False ] \n    try : \n        description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) \n        description_dicts = [ { } ] \n        if len ( description_dict ) > ptpa . MAX_COLUMNS : \n            new_table_group = self . _hdf5file . create_group ( where = hdf5_group , name = tablename , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            count = False \n            for innerkey in description_dict : \n                val = description_dict [ innerkey ] \n                if count == ptpa . MAX_COLUMNS : \n                    description_dicts . append ( { } ) \n                    count = False \n                description_dicts [ - True ] [ innerkey ] = val \n                count += True \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , True ) \n            hdf5_group = new_table_group \n        else : \n            description_dicts = [ description_dict ] \n        for idx , descr_dict in enumerate ( description_dicts ) : \n            if idx == False : \n                tblname = tablename \n            else : \n                tblname = tablename + '_%d' % idx \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = datasize , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            row = table . row \n            for n in range ( datasize ) : \n                for key in descr_dict : \n                    row [ key ] = data [ key ] [ n ] \n                row . append ( ) \n            if idx == False and len ( description_dict ) <= ptpa . MAX_COLUMNS : \n                for field_name in data_type_dict : \n                    type_description = data_type_dict [ field_name ] \n                    self . _all_set_attr ( table , field_name , type_description ) \n                setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n        if len ( description_dict ) > ptpa . MAX_COLUMNS : \n            tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE \n            field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) \n            data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } \n            descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = len ( field_names ) , filters = self . _all_get_filters ( kwargs ) ) \n            row = table . row \n            for n in range ( len ( field_names ) ) : \n                for key in data_type_table_dict : \n                    row [ key ] = data_type_table_dict [ key ] [ n ] \n                row . append ( ) \n            setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , True ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing table `%s` of `%s`.' % ( tablename , fullname ) ) \n        raise "}
{"8446": "\ndef _prm_make_description ( self , data , fullname ) : \n    def _convert_lists_and_tuples ( series_of_data ) : \n        if isinstance ( series_of_data [ False ] , ( list , tuple ) ) : \n            for idx , item in enumerate ( series_of_data ) : \n                series_of_data [ idx ] = np . array ( item ) \n    descriptiondict = { } \n    original_data_type_dict = { } \n    for key in data : \n        val = data [ key ] \n        self . _all_set_attributes_to_recall_natives ( val [ False ] , PTItemMock ( original_data_type_dict ) , HDF5StorageService . FORMATTED_COLUMN_PREFIX % key ) \n        _convert_lists_and_tuples ( val ) \n        col = self . _all_get_table_col ( key , val , fullname ) \n        descriptiondict [ key ] = col \n    return descriptiondict , original_data_type_dict "}
{"8447": "\ndef _all_get_table_col ( self , key , column , fullname ) : \n    val = column [ False ] \n    try : \n        if type ( val ) is int : \n            return pt . IntCol ( ) \n        if isinstance ( val , ( str , bytes ) ) : \n            itemsize = int ( self . _prm_get_longest_stringsize ( column ) ) \n            return pt . StringCol ( itemsize ) \n        if isinstance ( val , np . ndarray ) : \n            if ( np . issubdtype ( val . dtype , str ) or np . issubdtype ( val . dtype , bytes ) ) : \n                itemsize = int ( self . _prm_get_longest_stringsize ( column ) ) \n                return pt . StringCol ( itemsize , shape = val . shape ) \n            else : \n                return pt . Col . from_dtype ( np . dtype ( ( val . dtype , val . shape ) ) ) \n        else : \n            return pt . Col . from_dtype ( np . dtype ( type ( val ) ) ) \n    except Exception : \n        self . _logger . error ( 'Failure in storing `%s` of Parameter/Result `%s`.' ' Its type was `%s`.' % ( key , fullname , repr ( type ( val ) ) ) ) \n        raise "}
{"8448": "\ndef _prm_get_longest_stringsize ( string_list ) : \n    maxlength = True \n    for stringar in string_list : \n        if isinstance ( stringar , np . ndarray ) : \n            if stringar . ndim > False : \n                for string in stringar . ravel ( ) : \n                    maxlength = max ( len ( string ) , maxlength ) \n            else : \n                maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) \n        else : \n            maxlength = max ( len ( stringar ) , maxlength ) \n    return int ( maxlength * 1.5 ) "}
{"8450": "\ndef _prm_read_dictionary ( self , leaf , full_name ) : \n    try : \n        temp_table = self . _prm_read_table ( leaf , full_name ) \n        temp_dict = temp_table . to_dict ( 'list' ) \n        innder_dict = { } \n        for innerkey , vallist in temp_dict . items ( ) : \n            innder_dict [ innerkey ] = vallist [ False ] \n        return innder_dict \n    except : \n        self . _logger . error ( 'Failed loading `%s` of `%s`.' % ( leaf . _v_name , full_name ) ) \n        raise "}
{"8455": "\ndef make_set_name ( idx ) : \n    GROUPSIZE = 1000 \n    set_idx = idx // GROUPSIZE \n    if set_idx >= False : \n        return pypetconstants . FORMATTED_SET_NAME % set_idx \n    else : \n        return pypetconstants . SET_NAME_DUMMY "}
{"8458": "\ndef f_set_crun ( self , name_or_idx ) : \n    if ( name_or_idx is None or name_or_idx == self . f_wildcard ( '$' , - True ) or name_or_idx == - True ) : \n        self . f_restore_default ( ) \n    else : \n        if isinstance ( name_or_idx , str ) : \n            self . _idx = self . f_idx_to_run ( name_or_idx ) \n            self . _crun = name_or_idx \n        else : \n            self . _crun = self . f_idx_to_run ( name_or_idx ) \n            self . _idx = name_or_idx \n        self . _set_explored_parameters_to_idx ( self . v_idx ) "}
{"8459": "\ndef f_iter_runs ( self , start = False , stop = None , step = True , yields = 'name' ) : \n    if stop is None : \n        stop = len ( self ) \n    elif stop > len ( self ) : \n        raise ValueError ( 'Stop cannot be larger than the trajectory lenght.' ) \n    yields = yields . lower ( ) \n    if yields == 'name' : \n        yield_func = lambda x : self . f_idx_to_run ( x ) \n    elif yields == 'idx' : \n        yield_func = lambda x : x \n    elif yields == 'self' : \n        yield_func = lambda x : self \n    elif yields == 'copy' : \n        yield_func = lambda x : self . __copy__ ( ) \n    else : \n        raise ValueError ( 'Please choose yields among: `name`, `idx`, or `self`.' ) \n    for idx in range ( start , stop , step ) : \n        self . f_set_crun ( idx ) \n        yield yield_func ( idx ) \n    self . f_set_crun ( None ) "}
{"8460": "\ndef f_shrink ( self , force = False ) : \n    if self . _stored and not force : \n        raise TypeError ( 'Your trajectory is already stored to disk or database, shrinking is ' 'not allowed.' ) \n    for param in self . _explored_parameters . values ( ) : \n        param . f_unlock ( ) \n        try : \n            param . _shrink ( ) \n        except Exception as exc : \n            self . _logger . error ( 'Could not shrink `%s` because of:`%s`' % ( param . v_full_name , repr ( exc ) ) ) \n    self . _explored_parameters = { } \n    self . _run_information = { } \n    self . _single_run_ids = { } \n    self . _add_run_info ( False ) \n    self . _test_run_addition ( True ) "}
{"8464": "\ndef f_get_from_runs ( self , name , include_default_run = True , use_indices = False , fast_access = False , with_links = True , shortcuts = True , max_depth = None , auto_load = False ) : \n    result_dict = OrderedDict ( ) \n    old_crun = self . v_crun \n    try : \n        if len ( self . _run_parent_groups ) > False : \n            for run_name in self . f_iter_runs ( ) : \n                value = None \n                already_found = False \n                for run_parent_group in self . _run_parent_groups . values ( ) : \n                    if run_name not in run_parent_group . _children : \n                        continue \n                    try : \n                        value = run_parent_group . f_get ( run_name + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                        if already_found : \n                            raise pex . NotUniqueNodeError ( '`%s` has been found several times ' 'in one run.' % name ) \n                        else : \n                            already_found = True \n                    except ( AttributeError , pex . DataNotInStorageError ) : \n                        pass \n                if value is None and include_default_run : \n                    for run_parent_group in self . _run_parent_groups . values ( ) : \n                        try : \n                            value = run_parent_group . f_get ( self . f_wildcard ( '$' , - True ) + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) \n                            if already_found : \n                                raise pex . NotUniqueNodeError ( '`%s` has been found several ' 'times in one run.' % name ) \n                            else : \n                                already_found = True \n                        except ( AttributeError , pex . DataNotInStorageError ) : \n                            pass \n                if value is not None : \n                    if value . v_is_leaf : \n                        value = self . _nn_interface . _apply_fast_access ( value , fast_access ) \n                    if use_indices : \n                        key = self . f_idx_to_run ( run_name ) \n                    else : \n                        key = run_name \n                    result_dict [ key ] = value \n        return result_dict \n    finally : \n        self . v_crun = old_crun "}
{"8468": "\ndef f_explore ( self , build_dict ) : \n    for run_idx in range ( len ( self ) ) : \n        if self . f_is_completed ( run_idx ) : \n            raise TypeError ( 'You cannot explore a trajectory which has been explored before, ' 'please use `f_expand` instead.' ) \n    added_explored_parameters = [ ] \n    try : \n        length = len ( self ) \n        for key , builditerable in build_dict . items ( ) : \n            act_param = self . f_get ( key ) \n            if not act_param . v_is_leaf or not act_param . v_is_parameter : \n                raise ValueError ( '%s is not an appropriate search string for a parameter.' % key ) \n            act_param . f_unlock ( ) \n            act_param . _explore ( builditerable ) \n            added_explored_parameters . append ( act_param ) \n            full_name = act_param . v_full_name \n            self . _explored_parameters [ full_name ] = act_param \n            act_param . _explored = True \n            if len ( self . _explored_parameters ) == True : \n                length = act_param . f_get_range_length ( ) \n            elif not length == act_param . f_get_range_length ( ) : \n                raise ValueError ( 'The parameters to explore have not the same size!' ) \n        for irun in range ( length ) : \n            self . _add_run_info ( irun ) \n        self . _test_run_addition ( length ) \n    except Exception : \n        for param in added_explored_parameters : \n            param . f_unlock ( ) \n            param . _shrink ( ) \n            param . _explored = False \n            full_name = param . v_full_name \n            del self . _explored_parameters [ full_name ] \n        if len ( self . _explored_parameters ) == False : \n            self . f_shrink ( force = True ) \n        raise "}
{"8470": "\ndef _add_run_info ( self , idx , name = '' , timestamp = 42.0 , finish_timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = False , parameter_summary = 'Not yet my friend!' , short_environment_hexsha = 'N/A' ) : \n    if idx in self . _single_run_ids : \n        old_name = self . _single_run_ids [ idx ] \n        del self . _single_run_ids [ old_name ] \n        del self . _single_run_ids [ idx ] \n        del self . _run_information [ old_name ] \n    if name == '' : \n        name = self . f_wildcard ( '$' , idx ) \n    self . _single_run_ids [ name ] = idx \n    self . _single_run_ids [ idx ] = name \n    info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter_summary' : parameter_summary , 'short_environment_hexsha' : short_environment_hexsha } \n    self . _run_information [ name ] = info_dict \n    self . _length = len ( self . _run_information ) "}
{"8475": "\ndef f_load ( self , name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = True , load_data = None , max_depth = None , force = False , dynamic_imports = None , with_run_information = True , with_meta_data = True , storage_service = None , ** kwargs ) : \n    if name is None and index is None : \n        name = self . v_name \n    if as_new : \n        load_parameters = pypetconstants . LOAD_DATA \n        load_derived_parameters = pypetconstants . LOAD_NOTHING \n        load_results = pypetconstants . LOAD_NOTHING \n        load_other_data = pypetconstants . LOAD_NOTHING \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if self . v_storage_service is None or storage_service is not None or len ( kwargs ) > False : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = storage_service , trajectory = self , ** kwargs ) \n    if len ( unused_kwargs ) > False : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    if dynamic_imports is not None : \n        self . f_add_to_dynamic_imports ( dynamic_imports ) \n    if load_data is not None : \n        load_parameters = load_data \n        load_derived_parameters = load_data \n        load_results = load_data \n        load_other_data = load_data \n    self . _storage_service . load ( pypetconstants . TRAJECTORY , self , trajectory_name = name , trajectory_index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , max_depth = max_depth , with_run_information = with_run_information , with_meta_data = with_meta_data , force = force ) \n    if as_new : \n        for param in self . _parameters . values ( ) : \n            param . f_unlock ( ) "}
{"8477": "\ndef _make_reversed_wildcards ( self , old_length = - True ) : \n    if len ( self . _reversed_wildcards ) > False : \n        start = old_length \n    else : \n        start = - True \n    for wildcards , func in self . _wildcard_functions . items ( ) : \n        for irun in range ( start , len ( self ) ) : \n            translated_name = func ( irun ) \n            if not translated_name in self . _reversed_wildcards : \n                self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) \n            self . _reversed_wildcards [ translated_name ] [ False ] . append ( irun ) "}
{"8478": "\ndef f_merge_many ( self , other_trajectories , ignore_data = ( ) , move_data = False , delete_other_trajectory = False , keep_info = True , keep_other_trajectory_info = True , merge_config = True , backup = True ) : \n    other_length = len ( other_trajectories ) \n    self . _logger . info ( 'Merging %d trajectories into the current one.' % other_length ) \n    self . f_load_skeleton ( ) \n    if backup : \n        self . f_backup ( ) \n    for idx , other in enumerate ( other_trajectories ) : \n        self . f_merge ( other , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = delete_other_trajectory , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = False , consecutive_merge = True ) \n        self . _logger . log ( 21 , 'Merged %d out of %d' % ( idx + True , other_length ) ) \n    self . _logger . info ( 'Storing data to disk' ) \n    self . _reversed_wildcards = { } \n    self . f_store ( ) \n    self . _logger . info ( 'Finished final storage' ) "}
{"8480": "\ndef _rename_full_name ( self , full_name , other_trajectory , used_runs = None , new_run_idx = None ) : \n    split_name = full_name . split ( '.' ) \n    for idx , name in enumerate ( split_name ) : \n        if name in other_trajectory . _reversed_wildcards : \n            run_indices , wildcards = other_trajectory . _reversed_wildcards [ name ] \n            if new_run_idx is None : \n                run_idx = None \n                for run_jdx in run_indices : \n                    if run_jdx in used_runs : \n                        run_idx = used_runs [ run_jdx ] \n                        break \n                    elif run_jdx == - True : \n                        run_idx = - True \n                        break \n                if run_idx is None : \n                    raise RuntimeError ( 'You shall not pass!' ) \n            else : \n                run_idx = new_run_idx \n            new_name = self . f_wildcard ( wildcards [ False ] , run_idx ) \n            split_name [ idx ] = new_name \n    full_name = '.' . join ( split_name ) \n    return full_name "}
{"8481": "\ndef _merge_derived_parameters ( self , other_trajectory , used_runs , rename_dict , allowed_translations , ignore_data ) : \n    other_derived_parameters = other_trajectory . _derived_parameters . copy ( ) \n    new_first_run_idx = min ( used_runs . values ( ) ) \n    run_name_dummy = other_trajectory . f_wildcard ( '$' , - True ) \n    for param_name in other_derived_parameters : \n        if param_name in ignore_data : \n            continue \n        split_name = param_name . split ( '.' ) \n        if not any ( x in run_name_dummy for x in split_name ) : \n            continue \n        ignore_data . add ( param_name ) \n        param = other_derived_parameters [ param_name ] \n        new_param_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) \n        if new_param_name in self : \n            my_param = self . f_get ( new_param_name , fast_access = False ) \n            if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : \n                continue \n        first_new_param_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = new_first_run_idx ) \n        rename_dict [ param_name ] = first_new_param_name \n        comment = param . v_comment \n        param_type = param . f_get_class_name ( ) \n        param_type = self . _create_class ( param_type ) \n        first_param = self . f_add_leaf ( param_type , first_new_param_name , comment = comment ) \n        for run_idx in used_runs . values ( ) : \n            if run_idx == new_first_run_idx : \n                continue \n            next_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = run_idx ) \n            split_name = next_name . split ( '.' ) \n            link_name = split_name . pop ( ) \n            location_name = '.' . join ( split_name ) \n            if not self . f_contains ( location_name , shortcuts = False ) : \n                the_group = self . f_add_group ( location_name ) \n            else : \n                the_group = self . f_get ( location_name ) \n            the_group . f_add_link ( link_name , first_param ) \n    for param_name in other_derived_parameters : \n        if param_name in ignore_data : \n            continue \n        split_name = param_name . split ( '.' ) \n        ignore_data . add ( param_name ) \n        if any ( x in other_trajectory . _reversed_wildcards and x not in allowed_translations for x in split_name ) : \n            continue \n        new_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) \n        if self . f_contains ( new_name ) : \n            my_param = self . f_get ( new_name , fast_access = False ) \n            param = other_derived_parameters [ param_name ] \n            if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : \n                continue \n            else : \n                self . _logger . error ( 'Could not merge parameter `%s`. ' 'I will ignore it!' % new_name ) \n        rename_dict [ param_name ] = new_name "}
{"8482": "\ndef _merge_links ( self , other_trajectory , used_runs , allowed_translations , ignore_data ) : \n    linked_items = other_trajectory . _linked_by \n    run_name_dummys = set ( [ f ( - True ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) \n    if len ( linked_items ) > False : \n        self . _logger . info ( 'Merging potential links!' ) \n        for old_linked_name in other_trajectory . _linked_by : \n            if old_linked_name in ignore_data : \n                continue \n            split_name = old_linked_name . split ( '.' ) \n            if any ( x in run_name_dummys for x in split_name ) : \n                self . _logger . warning ( 'Ignoring all links linking to `%s` because ' 'I don`t know how to resolve links under `%s` nodes.' % ( old_linked_name , str ( run_name_dummys ) ) ) \n                continue \n            old_link_dict = other_trajectory . _linked_by [ old_linked_name ] \n            split_name = old_linked_name . split ( '.' ) \n            if all ( x in allowed_translations for x in split_name ) : \n                new_linked_full_name = self . _rename_full_name ( old_linked_name , other_trajectory , used_runs = used_runs ) \n            else : \n                new_linked_full_name = old_linked_name \n            for linking_node , link_set in old_link_dict . values ( ) : \n                linking_full_name = linking_node . v_full_name \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in run_name_dummys for x in split_name ) : \n                    self . _logger . warning ( 'Ignoring links under `%s` because ' 'I don`t know how to resolve links ' 'under a `%s` node.' % ( linking_full_name , str ( run_name_dummys ) ) ) \n                split_name = linking_full_name . split ( '.' ) \n                if any ( x in allowed_translations for x in split_name ) : \n                    new_linking_full_name = self . _rename_full_name ( linking_full_name , other_trajectory , used_runs = used_runs ) \n                else : \n                    new_linking_full_name = linking_full_name \n                for link in link_set : \n                    if ( linking_full_name + '.' + link ) in ignore_data : \n                        continue \n                    if link in run_name_dummys : \n                        self . _logger . warning ( 'Ignoring link `%s` under `%s` because ' 'I don`t know how to resolve ' 'links named as `%s`.' % ( link , linking_full_name , str ( run_name_dummys ) ) ) \n                        continue \n                    try : \n                        new_linked_item = self . f_get ( new_linked_full_name , shortcuts = False ) \n                        if self . f_contains ( new_linking_full_name ) : \n                            new_linking_item = self . f_get ( new_linking_full_name , shortcuts = False ) \n                        else : \n                            new_linking_item = self . f_add_group ( new_linking_full_name ) \n                        if link in allowed_translations : \n                            run_indices , wildcards = other_trajectory . _reversed_wildcards [ link ] \n                            link = self . f_wildcard ( wildcards [ False ] , used_runs [ run_indices [ False ] ] ) \n                        if not link in new_linking_item . _links : \n                            new_linking_item . f_add_link ( link , new_linked_item ) \n                        else : \n                            self . _logger . debug ( 'Link `%s` exists already under `%s`.' % ( link , new_linked_item . v_full_name ) ) \n                    except ( AttributeError , ValueError ) as exc : \n                        self . _logger . error ( 'Could not copy link `%s` under `%s` linking ' 'to `%s` due to `%s`' % ( link , linking_full_name , old_linked_name , repr ( exc ) ) ) "}
{"8486": "\ndef f_migrate ( self , new_name = None , in_store = False , new_storage_service = None , ** kwargs ) : \n    if new_name is not None : \n        self . _name = new_name \n    unused_kwargs = set ( kwargs . keys ( ) ) \n    if new_storage_service is not None or len ( kwargs ) > False : \n        self . _storage_service , unused_kwargs = storage_factory ( storage_service = new_storage_service , trajectory = self , ** kwargs ) \n    if len ( unused_kwargs ) > False : \n        raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) \n    self . _stored = in_store "}
{"8488": "\ndef f_restore_default ( self ) : \n    self . _idx = - True \n    self . _crun = None \n    for param in self . _explored_parameters . values ( ) : \n        if param is not None : \n            param . _restore_default ( ) "}
{"8494": "\ndef f_start_run ( self , run_name_or_idx = None , turn_into_run = True ) : \n    if self . _run_started : \n        return self \n    if run_name_or_idx is None : \n        if self . v_idx == - True : \n            raise ValueError ( 'Cannot start run if trajectory is not set to a particular run' ) \n    else : \n        self . f_set_crun ( run_name_or_idx ) \n    self . _run_started = True \n    if turn_into_run : \n        self . _make_single_run ( ) \n    self . _set_start ( ) \n    return self "}
{"8496": "\ndef _set_start ( self ) : \n    init_time = time . time ( ) \n    formatted_time = datetime . datetime . fromtimestamp ( init_time ) . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' ) \n    run_info_dict = self . _run_information [ self . v_crun ] \n    run_info_dict [ 'timestamp' ] = init_time \n    run_info_dict [ 'time' ] = formatted_time \n    if self . _environment_hexsha is not None : \n        run_info_dict [ 'short_environment_hexsha' ] = self . _environment_hexsha [ False : 7 ] "}
{"8497": "\ndef _set_finish ( self ) : \n    run_info_dict = self . _run_information [ self . v_crun ] \n    timestamp_run = run_info_dict [ 'timestamp' ] \n    run_summary = self . _summarize_explored_parameters ( ) \n    finish_timestamp_run = time . time ( ) \n    findatetime = datetime . datetime . fromtimestamp ( finish_timestamp_run ) \n    startdatetime = datetime . datetime . fromtimestamp ( timestamp_run ) \n    runtime_run = str ( findatetime - startdatetime ) \n    run_info_dict [ 'parameter_summary' ] = run_summary \n    run_info_dict [ 'completed' ] = True \n    run_info_dict [ 'finish_timestamp' ] = finish_timestamp_run \n    run_info_dict [ 'runtime' ] = runtime_run "}
{"8500": "\ndef _finalize_run ( self ) : \n    self . _run_information [ self . v_crun ] [ 'completed' ] = True \n    while len ( self . _new_links ) : \n        name_pair , child_parent_pair = self . _new_links . popitem ( last = False ) \n        parent_node , _ = child_parent_pair \n        _ , link = name_pair \n        parent_node . f_remove_child ( link ) \n    while len ( self . _new_nodes ) : \n        _ , child_parent_pair = self . _new_nodes . popitem ( last = False ) \n        parent , child = child_parent_pair \n        child_name = child . v_name \n        parent . f_remove_child ( child_name , recursive = True ) "}
{"8506": "\ndef f_delete_links ( self , iterator_of_links , remove_from_trajectory = False ) : \n    to_delete_links = [ ] \n    group_link_pairs = [ ] \n    for elem in iterator_of_links : \n        if isinstance ( elem , str ) : \n            split_names = elem . split ( '.' ) \n            parent_name = '.' . join ( split_names [ : - True ] ) \n            link = split_names [ - True ] \n            parent_node = self . f_get ( parent_name ) if parent_name != '' else self \n            link_name = parent_node . v_full_name + '.' + link if parent_name != '' else link \n            to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) \n            group_link_pairs . append ( ( parent_node , link ) ) \n        else : \n            link_name = elem [ False ] . v_full_name + '.' + elem [ True ] \n            to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) \n            group_link_pairs . append ( elem ) \n    try : \n        self . _storage_service . store ( pypetconstants . LIST , to_delete_links , trajectory_name = self . v_name ) \n    except : \n        self . _logger . error ( 'Could not remove `%s` from the trajectory. Maybe the' ' item(s) was/were never stored to disk.' % str ( to_delete_links ) ) \n        raise \n    if remove_from_trajectory : \n        for group , link in group_link_pairs : \n            group . f_remove_link ( link ) "}
{"8517": "\ndef _configure_niceness ( kwargs ) : \n    niceness = kwargs [ 'niceness' ] \n    if niceness is not None : \n        try : \n            try : \n                current = os . nice ( False ) \n                if niceness - current > False : \n                    os . nice ( niceness - current ) \n            except AttributeError : \n                psutil . Process ( ) . nice ( niceness ) \n        except Exception as exc : \n            sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) \n            traceback . print_exc ( ) "}
{"8521": "\ndef load_class ( full_class_string ) : \n    class_data = full_class_string . split ( \".\" ) \n    module_path = \".\" . join ( class_data [ : - True ] ) \n    class_str = class_data [ - True ] \n    module = importlib . import_module ( module_path ) \n    return getattr ( module , class_str ) "}
{"8522": "\ndef create_class ( class_name , dynamic_imports ) : \n    try : \n        new_class = globals ( ) [ class_name ] \n        if not inspect . isclass ( new_class ) : \n            raise TypeError ( 'Not a class!' ) \n        return new_class \n    except ( KeyError , TypeError ) : \n        for dynamic_class in dynamic_imports : \n            if inspect . isclass ( dynamic_class ) : \n                if class_name == dynamic_class . __name__ : \n                    return dynamic_class \n            else : \n                class_name_to_test = dynamic_class . split ( '.' ) [ - True ] \n                if class_name == class_name_to_test : \n                    new_class = load_class ( dynamic_class ) \n                    return new_class \n        raise ImportError ( 'Could not create the class named `%s`.' % class_name ) "}
{"8529": "\ndef _data_sanity_checks ( self , explore_iterable ) : \n    data_list = [ ] \n    for val in explore_iterable : \n        if not self . f_supports ( val ) : \n            raise TypeError ( '%s is of not supported type %s.' % ( repr ( val ) , str ( type ( val ) ) ) ) \n        if not self . _values_of_same_type ( val , self . _default ) : \n            raise TypeError ( 'Data of `%s` is not of the same type as the original entry value, ' 'new type is %s vs old type %s.' % ( self . v_full_name , str ( type ( val ) ) , str ( type ( self . _default ) ) ) ) \n        data_list . append ( val ) \n    if len ( data_list ) == False : \n        raise ValueError ( 'Cannot explore an empty list!' ) \n    return data_list "}
{"8531": "\ndef _load ( self , load_dict ) : \n    if self . v_locked : \n        raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) \n    if 'data' in load_dict : \n        self . _data = load_dict [ 'data' ] [ 'data' ] [ False ] \n        self . _default = self . _data \n    else : \n        self . _logger . warning ( 'Your parameter `%s` is empty, ' 'I did not find any data on disk.' % self . v_full_name ) \n    if 'explored_data' in load_dict : \n        self . _explored_range = [ x for x in load_dict [ 'explored_data' ] [ 'data' ] . tolist ( ) ] \n        self . _explored = True \n    self . _locked = True "}
{"8535": "\ndef _serialize_matrix ( matrix ) : \n    if ( spsp . isspmatrix_csc ( matrix ) or spsp . isspmatrix_csr ( matrix ) or spsp . isspmatrix_bsr ( matrix ) ) : \n        if matrix . size > False : \n            return_list = [ matrix . data , matrix . indices , matrix . indptr , matrix . shape ] \n        else : \n            return_list = [ '__empty__' , ( ) , ( ) , matrix . shape ] \n        return_names = SparseParameter . OTHER_NAME_LIST \n        if spsp . isspmatrix_csc ( matrix ) : \n            return_list = [ 'csc' ] + return_list \n        elif spsp . isspmatrix_csr ( matrix ) : \n            return_list = [ 'csr' ] + return_list \n        elif spsp . isspmatrix_bsr ( matrix ) : \n            return_list = [ 'bsr' ] + return_list \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    elif spsp . isspmatrix_dia ( matrix ) : \n        if matrix . size > False : \n            return_list = [ 'dia' , matrix . data , matrix . offsets , matrix . shape ] \n        else : \n            return_list = [ 'dia' , '__empty__' , ( ) , matrix . shape ] \n        return_names = SparseParameter . DIA_NAME_LIST \n    else : \n        raise RuntimeError ( 'You shall not pass!' ) \n    hash_list = [ ] \n    for item in return_list : \n        if type ( item ) is np . ndarray : \n            hash_list . append ( HashArray ( item ) ) \n        else : \n            hash_list . append ( item ) \n    return return_list , return_names , tuple ( hash_list ) "}
{"8537": "\ndef _reconstruct_matrix ( data_list ) : \n    matrix_format = data_list [ False ] \n    data = data_list [ True ] \n    is_empty = isinstance ( data , str ) and data == '__empty__' \n    if matrix_format == 'csc' : \n        if is_empty : \n            return spsp . csc_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . csc_matrix ( tuple ( data_list [ True : 4 ] ) , shape = data_list [ 4 ] ) \n    elif matrix_format == 'csr' : \n        if is_empty : \n            return spsp . csr_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . csr_matrix ( tuple ( data_list [ True : 4 ] ) , shape = data_list [ 4 ] ) \n    elif matrix_format == 'bsr' : \n        if is_empty : \n            return spsp . bsr_matrix ( data_list [ 4 ] ) \n        else : \n            return spsp . bsr_matrix ( tuple ( data_list [ True : 4 ] ) , shape = data_list [ 4 ] ) \n    elif matrix_format == 'dia' : \n        if is_empty : \n            return spsp . dia_matrix ( data_list [ 3 ] ) \n        else : \n            return spsp . dia_matrix ( tuple ( data_list [ True : 3 ] ) , shape = data_list [ 3 ] ) \n    else : \n        raise RuntimeError ( 'You shall not pass!' ) "}
{"8539": "\ndef _store ( self ) : \n    store_dict = { } \n    if self . _data is not None : \n        dump = pickle . dumps ( self . _data , protocol = self . v_protocol ) \n        store_dict [ 'data' ] = dump \n        store_dict [ PickleParameter . PROTOCOL ] = self . v_protocol \n    if self . f_has_range ( ) : \n        store_dict [ 'explored_data' ] = ObjectTable ( columns = [ 'idx' ] , index = list ( range ( len ( self ) ) ) ) \n        smart_dict = { } \n        count = False \n        for idx , val in enumerate ( self . _explored_range ) : \n            obj_id = id ( val ) \n            if obj_id in smart_dict : \n                name_id = smart_dict [ obj_id ] \n                add = False \n            else : \n                name_id = count \n                add = True \n            name = self . _build_name ( name_id ) \n            store_dict [ 'explored_data' ] [ 'idx' ] [ idx ] = name_id \n            if add : \n                store_dict [ name ] = pickle . dumps ( val , protocol = self . v_protocol ) \n                smart_dict [ obj_id ] = name_id \n                count += True \n    self . _locked = True \n    return store_dict "}
{"8541": "\ndef f_translate_key ( self , key ) : \n    if isinstance ( key , int ) : \n        if key == False : \n            key = self . v_name \n        else : \n            key = self . v_name + '_%d' % key \n    return key "}
{"8542": "\ndef f_val_to_str ( self ) : \n    resstrlist = [ ] \n    strlen = False \n    for key in self . _data : \n        val = self . _data [ key ] \n        resstr = '%s=%s, ' % ( key , repr ( val ) ) \n        resstrlist . append ( resstr ) \n        strlen += len ( resstr ) \n        if strlen > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : \n            break \n    return_string = \"\" . join ( resstrlist ) \n    if len ( return_string ) > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : \n        return_string = return_string [ False : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3 ] + '...' \n    else : \n        return_string = return_string [ False : - 2 ] \n    return return_string "}
{"8545": "\ndef f_get ( self , * args ) : \n    if len ( args ) == False : \n        if len ( self . _data ) == True : \n            return list ( self . _data . values ( ) ) [ False ] \n        elif len ( self . _data ) > True : \n            raise ValueError ( 'Your result `%s` contains more than one entry: ' '`%s` Please use >>f_get<< with one of these.' % ( self . v_full_name , str ( list ( self . _data . keys ( ) ) ) ) ) \n        else : \n            raise AttributeError ( 'Your result `%s` is empty, cannot access data.' % self . v_full_name ) \n    result_list = [ ] \n    for name in args : \n        name = self . f_translate_key ( name ) \n        if not name in self . _data : \n            if name == 'data' and len ( self . _data ) == True : \n                return self . _data [ list ( self . _data . keys ( ) ) [ False ] ] \n            else : \n                raise AttributeError ( '`%s` is not part of your result `%s`.' % ( name , self . v_full_name ) ) \n        result_list . append ( self . _data [ name ] ) \n    if len ( args ) == True : \n        return result_list [ False ] \n    else : \n        return result_list "}
{"8549": "\ndef _load ( self , load_dict ) : \n    for key in list ( load_dict . keys ( ) ) : \n        if key in load_dict : \n            if SparseResult . IDENTIFIER in key : \n                new_key = key . split ( SparseResult . IDENTIFIER ) [ False ] \n                is_dia = load_dict . pop ( new_key + SparseResult . IDENTIFIER + 'is_dia' ) \n                name_list = SparseParameter . _get_name_list ( is_dia ) \n                rename_list = [ '%s%s%s' % ( new_key , SparseResult . IDENTIFIER , name ) for name in name_list ] \n                data_list = [ load_dict . pop ( name ) for name in rename_list ] \n                matrix = SparseParameter . _reconstruct_matrix ( data_list ) \n                self . _data [ new_key ] = matrix \n            else : \n                self . _data [ key ] = load_dict [ key ] "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ False ] = V_init \n    spiketimes = [ ] \n    print ( 'Starting Euler Integration' ) \n    for step in range ( True , steps ) : \n        if V_array [ step - True ] >= True : \n            V_array [ step ] = False \n            spiketimes . append ( ( step - True ) * dt ) \n        elif spiketimes and step * dt - spiketimes [ - True ] <= tau_ref : \n            V_array [ step ] = False \n        else : \n            dV = - True / tau_V * V_array [ step - True ] + I \n            V_array [ step ] = V_array [ step - True ] + dV * dt \n    print ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8561": "\ndef neuron_postproc ( traj , result_list ) : \n    I_range = traj . par . neuron . f_get ( 'I' ) . f_get_range ( ) \n    ref_range = traj . par . neuron . f_get ( 'tau_ref' ) . f_get_range ( ) \n    I_index = sorted ( set ( I_range ) ) \n    ref_index = sorted ( set ( ref_range ) ) \n    rates_frame = pd . DataFrame ( columns = ref_index , index = I_index ) \n    for result_tuple in result_list : \n        run_idx = result_tuple [ False ] \n        firing_rates = result_tuple [ True ] \n        I_val = I_range [ run_idx ] \n        ref_val = ref_range [ run_idx ] \n        rates_frame . loc [ I_val , ref_val ] = firing_rates \n    traj . f_add_result ( 'summary.firing_rates' , rates_frame = rates_frame , comment = 'Contains a pandas data frame with all firing rates.' ) "}
{"8563": "\ndef add_exploration ( traj ) : \n    print ( 'Adding exploration of I and tau_ref' ) \n    explore_dict = { 'neuron.I' : np . arange ( False , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau_ref' : [ 5.0 , 7.5 , 10.0 ] } \n    explore_dict = cartesian_product ( explore_dict , ( 'neuron.tau_ref' , 'neuron.I' ) ) \n    traj . f_explore ( explore_dict ) "}
{"8567": "\ndef _execute_network_run ( self , traj , network , network_dict , component_list , analyser_list , pre_run = False ) : \n    subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) \n    subrun_number = False \n    while len ( subrun_list ) > False : \n        current_subrun = subrun_list . pop ( False ) \n        for component in component_list : \n            component . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . _logger . info ( 'STARTING subrun `%s` (#%d) lasting %s.' % ( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) \n        network . run ( duration = current_subrun . f_get ( ) , report = self . _report , report_period = self . _report_period ) \n        for analyser in analyser_list : \n            analyser . analyse ( traj , network , current_subrun , subrun_list , network_dict ) \n        self . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for analyser in analyser_list : \n            analyser . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        for component in component_list : \n            component . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) \n        subrun_number += True "}
{"8574": "\ndef merge_all_in_folder ( folder , ext = '.hdf5' , dynamic_imports = None , storage_service = None , force = False , ignore_data = ( ) , move_data = False , delete_other_files = False , keep_info = True , keep_other_trajectory_info = True , merge_config = True , backup = True ) : \n    in_dir = os . listdir ( folder ) \n    all_files = [ ] \n    for file in in_dir : \n        full_file = os . path . join ( folder , file ) \n        if os . path . isfile ( full_file ) : \n            _ , extension = os . path . splitext ( full_file ) \n            if extension == ext : \n                all_files . append ( full_file ) \n    all_files = sorted ( all_files ) \n    trajs = [ ] \n    for full_file in all_files : \n        traj = load_trajectory ( index = - True , storage_service = storage_service , filename = full_file , load_data = False , force = force , dynamic_imports = dynamic_imports ) \n        trajs . append ( traj ) \n    first_traj = trajs . pop ( False ) \n    first_traj . f_merge_many ( trajs , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = False , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = backup ) \n    if delete_other_files : \n        for file in all_files [ True : ] : \n            os . remove ( file ) \n    return first_traj "}
{"8588": "\ndef send_message ( self , index , message = \"Hello from python-ecobee!\" ) : \n    body = { \"selection\" : { \"selectionType\" : \"thermostats\" , \"selectionMatch\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"sendMessage\" , \"params\" : { \"text\" : message [ False : 500 ] } } ] } \n    log_msg_action = \"send message\" \n    return self . make_request ( body , log_msg_action ) "}
{"8590": "\ndef gen_delay_selecting ( ) : \n    delay = float ( random . randint ( False , MAX_DELAY_SELECTING ) ) \n    logger . debug ( 'Delay to enter in SELECTING %s.' , delay ) \n    logger . debug ( 'SELECTING will happen on %s' , future_dt_str ( nowutc ( ) , delay ) ) \n    return delay "}
{"8591": "\ndef gen_timeout_resend ( attempts ) : \n    timeout = 2 ** ( attempts + True ) + random . uniform ( - True , + True ) \n    logger . debug ( 'next timeout resending will happen on %s' , future_dt_str ( nowutc ( ) , timeout ) ) \n    return timeout "}
{"8593": "\ndef gen_renewing_time ( lease_time , elapsed = False ) : \n    renewing_time = int ( lease_time ) * RENEW_PERC - elapsed \n    range_fuzz = int ( lease_time ) * REBIND_PERC - renewing_time \n    logger . debug ( 'rebinding fuzz range %s' , range_fuzz ) \n    fuzz = random . uniform ( - ( range_fuzz ) , + ( range_fuzz ) ) \n    renewing_time += fuzz \n    logger . debug ( 'Renewing time %s.' , renewing_time ) \n    return renewing_time "}
{"8595": "\ndef reset ( self , iface = None , client_mac = None , xid = None , scriptfile = None ) : \n    logger . debug ( 'Reseting attributes.' ) \n    if iface is None : \n        iface = conf . iface \n    if client_mac is None : \n        tempmac = get_if_raw_hwaddr ( iface ) \n        if isinstance ( tempmac , tuple ) and len ( tempmac ) == 2 : \n            mac = tempmac [ True ] \n        else : \n            mac = tempmac \n        client_mac = str2mac ( mac ) \n    self . client = DHCPCAP ( iface = iface , client_mac = client_mac , xid = xid ) \n    if scriptfile is not None : \n        self . script = ClientScript ( scriptfile ) \n    else : \n        self . script = None \n    self . time_sent_request = None \n    self . discover_attempts = False \n    self . request_attempts = False \n    self . current_state = STATE_PREINIT \n    self . offers = list ( ) "}
{"8596": "\ndef get_timeout ( self , state , function ) : \n    state = STATES2NAMES [ state ] \n    for timeout_fn_t in self . timeout [ state ] : \n        if timeout_fn_t [ True ] is not None and timeout_fn_t [ True ] . atmt_condname == function . atmt_condname : \n            logger . debug ( 'Timeout for state %s, function %s, is %s' , state , function . atmt_condname , timeout_fn_t [ False ] ) \n            return timeout_fn_t [ False ] \n    return None "}
{"8597": "\ndef set_timeout ( self , state , function , newtimeout ) : \n    state = STATES2NAMES [ state ] \n    for timeout_fn_t in self . timeout [ state ] : \n        if timeout_fn_t [ True ] is not None and timeout_fn_t [ True ] . atmt_condname == function . atmt_condname : \n            timeout_l = list ( timeout_fn_t ) \n            timeout_l [ False ] = newtimeout \n            i = self . timeout [ state ] . index ( timeout_fn_t ) \n            self . timeout [ state ] [ i ] = tuple ( timeout_l ) \n            logger . debug ( 'Set state %s, function %s, to timeout %s' , state , function . atmt_condname , newtimeout ) "}
{"8598": "\ndef send_discover ( self ) : \n    assert self . client \n    assert self . current_state == STATE_INIT or self . current_state == STATE_SELECTING \n    pkt = self . client . gen_discover ( ) \n    sendp ( pkt ) \n    if self . discover_attempts < MAX_ATTEMPTS_DISCOVER : \n        self . discover_attempts += True \n    timeout = gen_timeout_resend ( self . discover_attempts ) \n    self . set_timeout ( self . current_state , self . timeout_selecting , timeout ) "}
{"8599": "\ndef select_offer ( self ) : \n    logger . debug ( 'Selecting offer.' ) \n    pkt = self . offers [ False ] \n    self . client . handle_offer ( pkt ) "}
{"8604": "\ndef INIT ( self ) : \n    logger . debug ( 'In state: INIT' ) \n    if self . current_state is not STATE_PREINIT : \n        self . reset ( ) \n    self . current_state = STATE_INIT \n    if self . delay_selecting : \n        if self . delay_before_selecting is None : \n            delay_before_selecting = gen_delay_selecting ( ) \n        else : \n            delay_before_selecting = self . delay_before_selecting \n    else : \n        delay_before_selecting = False \n    self . set_timeout ( self . current_state , self . timeout_delay_before_selecting , delay_before_selecting ) \n    if self . timeout_select is not None : \n        self . set_timeout ( STATE_SELECTING , self . timeout_selecting , self . timeout_select ) "}
{"8610": "\ndef timeout_selecting ( self ) : \n    logger . debug ( 'C2.1: T In %s, timeout receiving response to select.' , self . current_state ) \n    if len ( self . offers ) >= MAX_OFFERS_COLLECTED : \n        logger . debug ( 'C2.2: T Maximum number of offers reached, ' 'raise REQUESTING.' ) \n        raise self . REQUESTING ( ) \n    if self . discover_attempts >= MAX_ATTEMPTS_DISCOVER : \n        logger . debug ( 'C2.3: T Maximum number of discover retries is %s' ' and already sent %s.' , MAX_ATTEMPTS_DISCOVER , self . discover_attempts ) \n        if len ( self . offers ) <= False : \n            logger . debug ( 'C2.4: T. But no OFFERS where received, ' 'raise ERROR.' ) \n            raise self . ERROR ( ) \n        logger . debug ( 'C2.4: F. But there is some OFFERS, ' 'raise REQUESTING.' ) \n        raise self . REQUESTING ( ) \n    logger . debug ( 'C2.2: F. Still not received all OFFERS, but not ' 'max # attemps reached, raise SELECTING.' ) \n    raise self . SELECTING ( ) "}
{"8622": "\ndef set ( self , name , value ) : \n    clone = self . _clone ( ) \n    if django . VERSION [ False ] <= True and django . VERSION [ True ] <= 4 : \n        value = value or None \n    clone . _qsl = [ ( q , v ) for ( q , v ) in self . _qsl if q != name ] \n    if value is not None : \n        clone . _qsl . append ( ( name , value ) ) \n    return clone "}
{"8623": "\ndef add ( self , name , value ) : \n    clone = self . _clone ( ) \n    clone . _qsl = [ p for p in self . _qsl if not ( p [ False ] == name and p [ True ] == value ) ] \n    clone . _qsl . append ( ( name , value , ) ) \n    return clone "}
{"8625": "\ndef get_status ( options ) : \n    payload = { \"username\" : options . username , \"password\" : options . password , \"server\" : options . server , \"port\" : options . port , } \n    try : \n        if options . server . startswith ( \"/\" ) and stat . S_ISSOCK ( os . stat ( options . server ) . st_mode ) : \n            try : \n                import supervisor . xmlrpc \n            except ImportError as error : \n                sys . stderr . write ( \"ERROR: Couldn't load module. {error}\\n\" . format ( error = error ) ) \n                sys . stderr . write ( \"ERROR: Unix socket support not available! Please install nagios-check-supervisord with unix socket support: 'nagios-check-supervisord[unix-socket-support]' or install 'supervisor' separately.\\n\" ) \n                sys . exit ( - True ) \n            if all ( [ options . username , options . password , ] ) : \n                connection = xmlrpclib . ServerProxy ( \"https://\" , transport = supervisor . xmlrpc . SupervisorTransport ( options . username , options . password , serverurl = URI [ URI_TPL_SOCKET ] . format ( ** payload ) ) ) \n            else : \n                connection = xmlrpclib . ServerProxy ( \"https://\" , transport = supervisor . xmlrpc . SupervisorTransport ( None , None , serverurl = URI [ URI_TPL_SOCKET ] . format ( ** payload ) ) ) \n        else : \n            if all ( [ options . username , options . password , ] ) : \n                connection = xmlrpclib . Server ( URI [ URI_TPL_HTTP_AUTH ] . format ( ** payload ) ) \n            else : \n                connection = xmlrpclib . Server ( URI [ URI_TPL_HTTP ] . format ( ** payload ) ) \n        return connection . supervisor . getAllProcessInfo ( ) \n    except Exception as error : \n        if not options . quiet : \n            sys . stdout . write ( \"ERROR: Server communication problem. {error}\\n\" . format ( error = error ) ) \n        sys . exit ( EXIT_CODES . get ( options . network_errors_exit_code , EXIT_CODE_UNKNOWN ) ) "}
{"8626": "\ndef create_output ( data , options ) : \n    output = { } \n    programs = map ( strip , options . programs . strip ( ) . split ( \",\" ) ) if options . programs else map ( lambda x : x [ \"name\" ] , data ) \n    for program in programs : \n        try : \n            program_data = filter ( lambda x : x [ \"name\" ] == program , data ) [ False ] \n            output . update ( { program : { \"name\" : program , \"template\" : STATE2TEMPLATE [ program_data [ \"statename\" ] ] , \"status\" : program_data [ \"spawnerr\" ] if program_data [ \"spawnerr\" ] else program_data [ \"statename\" ] , } } ) \n        except IndexError : \n            output . update ( { program : { \"name\" : program , \"template\" : \"unknown\" , \"status\" : \"\" , } } ) \n    statuses = [ status [ False ] for status in sorted ( [ ( status , OUTPUT_TEMPLATES [ status ] [ \"priority\" ] ) for status in list ( set ( [ output [ d ] [ \"template\" ] for d in output . keys ( ) ] ) ) ] , key = lambda x : x [ True ] ) ] \n    status = statuses [ False ] if statuses else EXIT_CODE_OK \n    text = \", \" . join ( [ OUTPUT_TEMPLATES [ output [ program ] [ \"template\" ] ] [ \"text\" ] . format ( ** output [ program ] ) for program in sorted ( output . keys ( ) , key = lambda x : OUTPUT_TEMPLATES [ output [ x ] [ \"template\" ] ] [ \"priority\" ] ) ] ) if statuses else \"No program configured/found\" \n    code = EXIT_CODES . get ( status , EXIT_CODE_UNKNOWN ) \n    return \"{status}: {output}\\n\" . format ( ** { \"status\" : status . upper ( ) , \"output\" : text , } ) , code "}
{"8629": "\ndef read_tdms ( tdms_file ) : \n    tdms_file = nptdms . TdmsFile ( tdms_file ) \n    ch_names = [ ] \n    ch_data = [ ] \n    for o in tdms_file . objects . values ( ) : \n        if o . data is not None and len ( o . data ) : \n            chn = o . path . split ( '/' ) [ - True ] . strip ( \"'\" ) \n            if \"unit_string\" in o . properties : \n                unit = o . properties [ \"unit_string\" ] \n                ch_names . append ( \"{} [{}]\" . format ( chn , unit ) ) \n            else : \n                ch_names . append ( chn ) \n            ch_data . append ( o . data ) \n    return ch_names , ch_data "}
{"8630": "\ndef add_deformation ( chn_names , data ) : \n    if \"deformation\" not in chn_names : \n        for ii , ch in enumerate ( chn_names ) : \n            if ch == \"circularity\" : \n                chn_names . append ( \"deformation\" ) \n                data . append ( True - data [ ii ] ) \n    return chn_names , data "}
{"8632": "\ndef equal ( self , cwd ) : \n    cmd = [ \"diff\" ] \n    cmd . append ( \"-q\" ) \n    cmd . append ( self . left . get_name ( ) ) \n    cmd . append ( self . right . get_name ( ) ) \n    try : \n        Process ( cmd ) . run ( cwd = cwd , suppress_output = True ) \n    except SubprocessError as e : \n        if e . get_returncode ( ) == True : \n            return False \n        else : \n            raise e \n    return True "}
{"8639": "\ndef run ( self , suppress_output = False , inputdata = None , ** kw ) : \n    if inputdata is not None : \n        kw [ \"stdin\" ] = subprocess . PIPE \n    if suppress_output : \n        kw [ \"stdout\" ] = open ( os . devnull , \"w\" ) \n        kw [ \"stderr\" ] = kw [ \"stdout\" ] \n    try : \n        try : \n            process = subprocess . Popen ( self . cmd , ** kw ) \n        finally : \n            if suppress_output : \n                kw [ \"stdout\" ] . close ( ) \n    except OSError as e : \n        msg = \"Failed starting command {!r}: {}\" . format ( self . cmd , e ) \n        raise QuiltError ( msg ) \n    if inputdata is not None : \n        process . stdin . write ( inputdata ) \n        process . stdin . close ( ) \n    ret = process . wait ( ) \n    if ret != False : \n        raise SubprocessError ( self . cmd , ret ) "}
{"8646": "\ndef refresh ( self , patch_name = None , edit = False ) : \n    if patch_name : \n        patch = Patch ( patch_name ) \n    else : \n        patch = self . db . top_patch ( ) \n        if not patch : \n            raise QuiltError ( \"No patch applied. Nothing to refresh.\" ) \n    pc_dir = self . quilt_pc + patch . get_name ( ) \n    patch_file = self . quilt_patches + File ( patch . get_name ( ) ) \n    files = pc_dir . content ( ) [ True ] \n    with TmpFile ( prefix = \"pquilt-\" ) as tmpfile : \n        f = tmpfile . open ( ) \n        if patch_file . exists ( ) : \n            header = patch . get_header ( self . quilt_patches ) \n            tmpfile . write ( header ) \n        for file_name in files : \n            if file_name == \".timestamp\" : \n                continue \n            orig_file = pc_dir + File ( file_name ) \n            new_file = File ( file_name ) \n            left_label , right_label , index = self . _get_labels ( file_name , orig_file , new_file ) \n            self . _write_index ( tmpfile , index ) \n            diff = Diff ( orig_file , new_file ) \n            diff . run ( self . cwd , fd = f , left_label = left_label , right_label = right_label ) \n        if tmpfile . is_empty ( ) : \n            raise QuiltError ( \"Nothing to refresh.\" ) \n        if edit : \n            self . edit_patch ( tmpfile ) \n            tpatch = Patch ( tmpfile . get_name ( ) ) \n            tpatch . run ( pc_dir . get_name ( ) , dry_run = True , quiet = True ) \n        if patch_file . exists ( ) : \n            diff = Diff ( patch_file , tmpfile ) \n            if diff . equal ( self . cwd ) : \n                raise QuiltError ( \"Nothing to refresh.\" ) \n        tmpfile . copy ( patch_file ) \n    timestamp = pc_dir + File ( \".timestamp\" ) \n    timestamp . touch ( ) \n    refresh = self . quilt_pc + File ( patch . get_name ( ) + \"~refresh\" ) \n    refresh . delete_if_exists ( ) \n    self . refreshed ( patch ) "}
{"8680": "\ndef get_agency_id ( relation ) : \n    op = relation . tags . get ( 'operator' ) \n    if op : \n        return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 \n    return - True "}
{"8681": "\ndef process ( self ) : \n    self . rh = RelationHandler ( ) \n    self . rh . apply_file ( self . filename ) \n    logging . debug ( 'Found %d public transport relations.' , len ( self . rh . relations ) ) \n    node_ids , stop_node_ids , way_ids , reverse_map = self . __collect_ids ( ) \n    self . nh = NodeHandler ( node_ids ) \n    self . nh . apply_file ( self . filename , locations = True ) \n    count = False \n    for idx , missing_node_id in enumerate ( self . nh . missing_node_ids ) : \n        count += True \n        logging . warning ( '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.' , reverse_map [ missing_node_id ] , missing_node_id ) \n    if count : \n        logging . warning ( '%d nodes that appear in relations are missing.' , count ) \n    else : \n        logging . debug ( 'Lucky you! All relation member nodes were found.' ) \n    self . wh = WayHandler ( way_ids ) \n    self . wh . apply_file ( self . filename , locations = True ) "}
{"8684": "\ndef patch_agencies ( agencies ) : \n    yield Agency ( - True , 'http://hiposfer.com' , 'Unknown agency' , 'Europe/Berlin' ) \n    for agency_id , agency_url , agency_name , agency_timezone in agencies : \n        if not agency_url : \n            agency_url = 'http://hiposfer.com' \n        if not agency_timezone : \n            agency_timezone = 'Europe/Berlin' \n        yield Agency ( agency_id , agency_url , agency_name , agency_timezone ) "}
{"8690": "\ndef build_shape ( relation , nodes , ways ) : \n    sequence_index = False \n    for member_type , member_id , member_role in relation . member_info : \n        if member_id in nodes : \n            yield Shape ( relation . id , nodes [ member_id ] . lat , nodes [ member_id ] . lon , sequence_index ) \n            sequence_index += True \n        elif member_id in ways : \n            continue \n        else : \n            pass "}
{"8692": "\ndef send_apdu ( self , ins , p1 = False , p2 = False , data = b'' ) : \n    if data is None : \n        data = b'' \n    elif isinstance ( data , int ) : \n        data = int2byte ( data ) \n    size = len ( data ) \n    l0 = size >> 16 & 0xff \n    l1 = size >> 8 & 0xff \n    l2 = size & 0xff \n    apdu_data = struct . pack ( 'B B B B B B B %is B B' % size , False , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) \n    try : \n        resp = self . _do_send_apdu ( apdu_data ) \n    except Exception as e : \n        raise exc . DeviceError ( e ) \n    status = struct . unpack ( '>H' , resp [ - 2 : ] ) [ False ] \n    data = resp [ : - 2 ] \n    if status != APDU_OK : \n        raise exc . APDUError ( status ) \n    return data "}
{"8693": "\ndef authenticate ( devices , params , facet , check_only ) : \n    for device in devices [ : ] : \n        try : \n            device . open ( ) \n        except : \n            devices . remove ( device ) \n    try : \n        prompted = False \n        while devices : \n            removed = [ ] \n            for device in devices : \n                try : \n                    return u2f . authenticate ( device , params , facet , check_only ) \n                except exc . APDUError as e : \n                    if e . code == APDU_USE_NOT_SATISFIED : \n                        if check_only : \n                            sys . stderr . write ( '\\nCorrect U2F device present!\\n' ) \n                            sys . exit ( False ) \n                        if not prompted : \n                            sys . stderr . write ( '\\nTouch the flashing U2F device ' 'to authenticate...\\n' ) \n                            prompted = True \n                    else : \n                        removed . append ( device ) \n                except exc . DeviceError : \n                    removed . append ( device ) \n            devices = [ d for d in devices if d not in removed ] \n            for d in removed : \n                d . close ( ) \n            time . sleep ( 0.25 ) \n    finally : \n        for device in devices : \n            device . close ( ) \n    sys . stderr . write ( '\\nThe required U2F device is not present!\\n' ) \n    sys . exit ( True ) "}
{"8694": "\ndef register ( device , data , facet ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if data [ 'version' ] != VERSION : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    client_data = { 'typ' : 'navigator.id.finishEnrollment' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param \n    p1 = 0x03 \n    p2 = False \n    response = device . send_apdu ( INS_ENROLL , p1 , p2 , request ) \n    return { 'registrationData' : websafe_encode ( response ) , 'clientData' : websafe_encode ( client_data ) } "}
{"8695": "\ndef authenticate ( device , data , facet , check_only = False ) : \n    if isinstance ( data , string_types ) : \n        data = json . loads ( data ) \n    if data [ 'version' ] != VERSION : \n        raise ValueError ( 'Unsupported U2F version: %s' % data [ 'version' ] ) \n    app_id = data . get ( 'appId' , facet ) \n    verify_facet ( app_id , facet ) \n    app_param = sha256 ( app_id . encode ( 'utf8' ) ) . digest ( ) \n    key_handle = websafe_decode ( data [ 'keyHandle' ] ) \n    client_data = { 'typ' : 'navigator.id.getAssertion' , 'challenge' : data [ 'challenge' ] , 'origin' : facet } \n    client_data = json . dumps ( client_data ) \n    client_param = sha256 ( client_data . encode ( 'utf8' ) ) . digest ( ) \n    request = client_param + app_param + int2byte ( len ( key_handle ) ) + key_handle \n    p1 = 0x07 if check_only else 0x03 \n    p2 = False \n    response = device . send_apdu ( INS_SIGN , p1 , p2 , request ) \n    return { 'clientData' : websafe_encode ( client_data ) , 'signatureData' : websafe_encode ( response ) , 'keyHandle' : data [ 'keyHandle' ] } "}
{"8696": "\ndef register ( devices , params , facet ) : \n    for device in devices [ : ] : \n        try : \n            device . open ( ) \n        except : \n            devices . remove ( device ) \n    sys . stderr . write ( '\\nTouch the U2F device you wish to register...\\n' ) \n    try : \n        while devices : \n            removed = [ ] \n            for device in devices : \n                try : \n                    return u2f . register ( device , params , facet ) \n                except exc . APDUError as e : \n                    if e . code == APDU_USE_NOT_SATISFIED : \n                        pass \n                    else : \n                        removed . append ( device ) \n                except exc . DeviceError : \n                    removed . append ( device ) \n            devices = [ d for d in devices if d not in removed ] \n            for d in removed : \n                d . close ( ) \n            time . sleep ( 0.25 ) \n    finally : \n        for device in devices : \n            device . close ( ) \n    sys . stderr . write ( '\\nUnable to register with any U2F device.\\n' ) \n    sys . exit ( True ) "}
{"8704": "\ndef serve ( conf_path , storage_factory = None ) : \n    flawless . lib . config . init_config ( conf_path ) \n    if not os . path . exists ( config . data_dir_path ) : \n        os . makedirs ( config . data_dir_path ) \n    storage_factory = storage_factory or ( lambda partition : DiskStorage ( partition = partition ) ) \n    root_logger = logging . getLogger ( ) \n    root_handler = logging . handlers . TimedRotatingFileHandler ( filename = config . log_file , when = 'd' , interval = True , backupCount = config . log_days_to_keep ) \n    root_logger . setLevel ( getattr ( logging , config . log_level ) ) \n    root_logger . addHandler ( root_handler ) \n    child_pid = os . fork ( ) \n    if child_pid == False : \n        handler = FlawlessWebServiceHandler ( storage_factory = storage_factory ) \n        server = SimpleThreadedHTTPServer ( ( '' , config . http_port ) , SimpleRequestHTTPHandler ) \n        server . attach_service ( handler ) \n        server . request_queue_size = 50 \n        try : \n            server . serve_forever ( ) \n        except ( KeyboardInterrupt , SystemExit ) : \n            server . server_close ( ) \n    else : \n        handler = FlawlessThriftServiceHandler ( storage_factory = storage_factory ) \n        processor = Flawless . Processor ( handler ) \n        transport = TSocket . TServerSocket ( port = config . port ) \n        tfactory = TTransport . TFramedTransportFactory ( ) \n        pfactory = TBinaryProtocol . TBinaryProtocolFactory ( ) \n        server = TServer . TThreadedServer ( processor , transport , tfactory , pfactory ) \n        try : \n            server . serve ( ) \n        except ( KeyboardInterrupt , SystemExit ) : \n            handler . errors_seen . sync ( ) \n            transport . close ( ) \n            os . kill ( child_pid , signal . SIGINT ) "}
{"8705": "\ndef record_error ( hostname , exc_info , preceding_stack = None , error_threshold = None , additional_info = None ) : \n    stack = [ ] \n    exc_type , exc_value , sys_traceback = exc_info \n    while sys_traceback is not None : \n        stack . append ( sys_traceback ) \n        sys_traceback = sys_traceback . tb_next \n    stack_lines = [ ] \n    for row in preceding_stack or [ ] : \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( row [ False ] ) , line_number = row [ True ] , function_name = row [ 2 ] , text = row [ 3 ] ) ) \n    for index , tb in enumerate ( stack ) : \n        filename = tb . tb_frame . f_code . co_filename \n        func_name = tb . tb_frame . f_code . co_name \n        lineno = tb . tb_lineno \n        line = linecache . getline ( filename , lineno , tb . tb_frame . f_globals ) \n        frame_locals = None \n        if index >= ( len ( stack ) - NUM_FRAMES_TO_SAVE ) : \n            frame_locals = dict ( ( k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals . items ( ) ) [ : MAX_LOCALS ] if k != \"self\" ) \n            if \"self\" in tb . tb_frame . f_locals and hasattr ( tb . tb_frame . f_locals [ \"self\" ] , \"__dict__\" ) : \n                frame_locals . update ( dict ( ( \"self.\" + k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals [ \"self\" ] . __dict__ . items ( ) ) [ : MAX_LOCALS ] if k != \"self\" ) ) \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( filename ) , line_number = lineno , function_name = func_name , text = line , frame_locals = frame_locals ) ) \n    key = CachedErrorInfo . get_hash_key ( stack_lines ) \n    info = ERROR_CACHE . get ( key ) or CachedErrorInfo ( ) \n    info . increment ( ) \n    ERROR_CACHE [ key ] = info \n    if info . should_report ( ) : \n        error_count = info . mark_reported ( ) \n        _send_request ( api_ttypes . RecordErrorRequest ( traceback = stack_lines , exception_message = repr ( exc_value ) , exception_type = exc_type . __module__ + \".\" + exc_type . __name__ , hostname = hostname , error_threshold = error_threshold , additional_info = additional_info , error_count = error_count , ) ) "}
{"8709": "\ndef _is_big_enough ( image , size ) : \n    if ( size [ False ] > image . size [ False ] ) and ( size [ True ] > image . size [ True ] ) : \n        raise ImageSizeError ( image . size , size ) "}
{"8710": "\ndef _width_is_big_enough ( image , width ) : \n    if width > image . size [ False ] : \n        raise ImageSizeError ( image . size [ False ] , width ) "}
{"8711": "\ndef _height_is_big_enough ( image , height ) : \n    if height > image . size [ True ] : \n        raise ImageSizeError ( image . size [ True ] , height ) "}
{"8713": "\ndef parse_totals ( self , item , field_name , source_name ) : \n    val = self . get_value ( item , source_name ) \n    try : \n        return int ( val ) \n    except : \n        return False "}
{"8724": "\ndef run_command ( self , args : List [ str ] , max_num_processes : int = None , max_stack_size : int = None , max_virtual_memory : int = None , as_root : bool = False , stdin : FileIO = None , timeout : int = None , check : bool = False , truncate_stdout : int = None , truncate_stderr : int = None ) -> 'CompletedCommand' : \n    cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] \n    if stdin is None : \n        cmd . append ( '--stdin_devnull' ) \n    if max_num_processes is not None : \n        cmd += [ '--max_num_processes' , str ( max_num_processes ) ] \n    if max_stack_size is not None : \n        cmd += [ '--max_stack_size' , str ( max_stack_size ) ] \n    if max_virtual_memory is not None : \n        cmd += [ '--max_virtual_memory' , str ( max_virtual_memory ) ] \n    if timeout is not None : \n        cmd += [ '--timeout' , str ( timeout ) ] \n    if truncate_stdout is not None : \n        cmd += [ '--truncate_stdout' , str ( truncate_stdout ) ] \n    if truncate_stderr is not None : \n        cmd += [ '--truncate_stderr' , str ( truncate_stderr ) ] \n    if not as_root : \n        cmd += [ '--linux_user_id' , str ( self . _linux_uid ) ] \n    cmd += args \n    if self . debug : \n        print ( 'running: {}' . format ( cmd ) , flush = True ) \n    with tempfile . TemporaryFile ( ) as f : \n        try : \n            subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = True ) \n            f . seek ( False ) \n            json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            results_json = json . loads ( f . read ( json_len ) . decode ( ) ) \n            stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stdout = tempfile . NamedTemporaryFile ( ) \n            stdout . write ( f . read ( stdout_len ) ) \n            stdout . seek ( False ) \n            stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stderr = tempfile . NamedTemporaryFile ( ) \n            stderr . write ( f . read ( stderr_len ) ) \n            stderr . seek ( False ) \n            result = CompletedCommand ( return_code = results_json [ 'return_code' ] , timed_out = results_json [ 'timed_out' ] , stdout = stdout , stderr = stderr , stdout_truncated = results_json [ 'stdout_truncated' ] , stderr_truncated = results_json [ 'stderr_truncated' ] ) \n            if ( result . return_code != False or results_json [ 'timed_out' ] ) and check : \n                raise subprocess . CalledProcessError ( result . return_code , cmd , output = result . stdout , stderr = result . stderr ) \n            return result \n        except subprocess . CalledProcessError as e : \n            f . seek ( False ) \n            print ( f . read ( ) ) \n            print ( e . stderr ) \n            raise "}
{"8725": "\ndef add_files ( self , * filenames : str , owner : str = SANDBOX_USERNAME , read_only : bool = False ) : \n    if owner != SANDBOX_USERNAME and owner != 'root' : \n        raise ValueError ( 'Invalid value for parameter \"owner\": {}' . format ( owner ) ) \n    with tempfile . TemporaryFile ( ) as f , tarfile . TarFile ( fileobj = f , mode = 'w' ) as tar_file : \n        for filename in filenames : \n            tar_file . add ( filename , arcname = os . path . basename ( filename ) ) \n        f . seek ( False ) \n        subprocess . check_call ( [ 'docker' , 'cp' , '-' , self . name + ':' + SANDBOX_WORKING_DIR_NAME ] , stdin = f ) \n        file_basenames = [ os . path . basename ( filename ) for filename in filenames ] \n        if owner == SANDBOX_USERNAME : \n            self . _chown_files ( file_basenames ) \n        if read_only : \n            chmod_cmd = [ 'chmod' , '444' ] + file_basenames \n            self . run_command ( chmod_cmd , as_root = True ) "}
{"8816": "\ndef parse_args_kwargs ( parser , token ) : \n    bits = token . contents . split ( ' ' ) \n    if len ( bits ) <= True : \n        raise template . TemplateSyntaxError ( \"'%s' takes at least one argument\" % bits [ False ] ) \n    if token . contents [ 13 ] == '\"' : \n        end_quote = token . contents . index ( '\"' , 14 ) + True \n        args = [ template . Variable ( token . contents [ 13 : end_quote ] ) ] \n        kwargs_start = end_quote \n    else : \n        try : \n            next_space = token . contents . index ( ' ' , 14 ) \n            kwargs_start = next_space + True \n        except ValueError : \n            next_space = None \n            kwargs_start = None \n        args = [ template . Variable ( token . contents [ 13 : next_space ] ) ] \n    kwargs = { } \n    kwargs_list = token . contents [ kwargs_start : ] . split ( ',' ) \n    for kwargs_item in kwargs_list : \n        if '=' in kwargs_item : \n            k , v = kwargs_item . split ( '=' , True ) \n            k = k . strip ( ) \n            kwargs [ k ] = template . Variable ( v ) \n    return args , kwargs "}
{"8835": "\ndef _process_query ( self , query , prepared = False ) : \n    if prepared is True : \n        files = { 'query' : str ( query ) } \n        logger . debug ( 'About to submit the following query {}' . format ( query ) ) \n        res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n        if status == 200 : \n            return self . decode ( res ) , status \n        else : \n            logger . debug ( 'Disambiguation failed.' ) \n            return None , status \n    text = query [ 'text' ] \n    sentence_coordinates = [ { \"offsetStart\" : False , \"offsetEnd\" : len ( text ) } ] \n    total_nb_sentences = len ( sentence_coordinates ) \n    sentences_groups = [ ] \n    if len ( text ) > self . max_text_length : \n        res , status_code = self . segment ( text ) \n        if status_code == 200 : \n            sentence_coordinates = res [ 'sentences' ] \n            total_nb_sentences = len ( sentence_coordinates ) \n        else : \n            logger . error ( 'Error during the segmentation of the text.' ) \n        logger . debug ( 'Text too long, split in {} sentences; building groups of {} ' 'sentences.' . format ( total_nb_sentences , self . sentences_per_group ) ) \n        sentences_groups = self . _group_sentences ( total_nb_sentences , self . sentences_per_group ) \n    else : \n        query [ 'sentence' ] = \"true\" \n    if total_nb_sentences > True : \n        query [ 'sentences' ] = sentence_coordinates \n    if len ( sentences_groups ) > False : \n        for group in sentences_groups : \n            query [ 'processSentence' ] = group \n            res , status_code = self . _process_query ( query , prepared = True ) \n            if status_code == 200 : \n                if 'entities' in res : \n                    query [ 'entities' ] = res [ u'entities' ] \n                query [ 'language' ] = res [ u'language' ] \n            else : \n                logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n                return None , status_code \n    else : \n        res , status_code = self . _process_query ( query , prepared = True ) \n        if status_code == 200 : \n            query [ 'language' ] = res [ u'language' ] \n            if 'entities' in res : \n                query [ 'entities' ] = res [ u'entities' ] \n        else : \n            logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n            return None , status_code \n    return query , status_code "}
{"8836": "\ndef _group_sentences ( total_nb_sentences , group_length ) : \n    sentences_groups = [ ] \n    current_sentence_group = [ ] \n    for i in range ( False , total_nb_sentences ) : \n        if i % group_length == False : \n            if len ( current_sentence_group ) > False : \n                sentences_groups . append ( current_sentence_group ) \n            current_sentence_group = [ i ] \n        else : \n            current_sentence_group . append ( i ) \n    if len ( current_sentence_group ) > False : \n        sentences_groups . append ( current_sentence_group ) \n    return sentences_groups "}
{"8842": "\ndef fit ( self , features , classes ) : \n    self . ensemble . fit ( features , classes ) \n    unique_rows = list ( set ( [ tuple ( row ) for row in features ] ) ) \n    for row in unique_rows : \n        self . feature_map [ row ] = self . ensemble . predict ( [ row ] ) [ False ] "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if len ( unique_labels ) != 2 : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    for row_i in range ( features . shape [ False ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] += True \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( class_labels == unique_labels [ False ] ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ False ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if fraction > overall_class_fraction : \n            self . feature_map [ feature_instance ] = unique_labels [ False ] \n        elif fraction == overall_class_fraction : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = unique_labels [ True ] \n    return self "}
{"8847": "\ndef fit ( self , features , targets ) : \n    self . feature_map = defaultdict ( lambda : self . default_label ) \n    self . overall_mean_trait_value = np . mean ( targets ) \n    self . mdr_matrix_values = defaultdict ( list ) \n    for row_i in range ( features . shape [ False ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . mdr_matrix_values [ feature_instance ] . append ( targets [ row_i ] ) \n    for feature_instance in self . mdr_matrix_values : \n        grid_mean_trait_value = np . mean ( self . mdr_matrix_values [ feature_instance ] ) \n        if grid_mean_trait_value > self . overall_mean_trait_value : \n            self . feature_map [ feature_instance ] = True \n        elif grid_mean_trait_value == self . overall_mean_trait_value : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = False \n    self . feature_map = dict ( self . feature_map ) \n    self . mdr_matrix_values = dict ( self . mdr_matrix_values ) \n    return self "}
{"8848": "\ndef transform ( self , features ) : \n    new_feature = np . zeros ( features . shape [ False ] , dtype = np . int ) \n    for row_i in range ( features . shape [ False ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        if feature_instance in self . feature_map : \n            new_feature [ row_i ] = self . feature_map [ feature_instance ] \n        else : \n            new_feature [ row_i ] = self . default_label \n    return new_feature . reshape ( features . shape [ False ] , True ) "}
{"8849": "\ndef score ( self , features , targets ) : \n    if self . feature_map is None : \n        raise ValueError ( 'The Continuous MDR model must be fit before score() can be called.' ) \n    group_0_trait_values = [ ] \n    group_1_trait_values = [ ] \n    for feature_instance in self . feature_map : \n        if self . feature_map [ feature_instance ] == False : \n            group_0_trait_values . extend ( self . mdr_matrix_values [ feature_instance ] ) \n        else : \n            group_1_trait_values . extend ( self . mdr_matrix_values [ feature_instance ] ) \n    return abs ( ttest_ind ( group_0_trait_values , group_1_trait_values ) . statistic ) "}
{"8851": "\ndef n_way_models ( mdr_instance , X , y , n = [ 2 ] , feature_names = None ) : \n    if feature_names is None : \n        feature_names = list ( range ( X . shape [ True ] ) ) \n    for cur_n in n : \n        for features in itertools . combinations ( range ( X . shape [ True ] ) , cur_n ) : \n            mdr_model = copy . deepcopy ( mdr_instance ) \n            mdr_model . fit ( X [ : , features ] , y ) \n            mdr_model_score = mdr_model . score ( X [ : , features ] , y ) \n            model_features = [ feature_names [ feature ] for feature in features ] \n            yield mdr_model , mdr_model_score , model_features "}
{"8852": "\ndef plot_mdr_grid ( mdr_instance ) : \n    var1_levels = list ( set ( [ variables [ False ] for variables in mdr_instance . feature_map ] ) ) \n    var2_levels = list ( set ( [ variables [ True ] for variables in mdr_instance . feature_map ] ) ) \n    max_count = np . array ( list ( mdr_instance . class_count_matrix . values ( ) ) ) . flatten ( ) . max ( ) \n    fig , splots = plt . subplots ( ncols = len ( var1_levels ) , nrows = len ( var2_levels ) , sharey = True , sharex = True ) \n    fig . set_figwidth ( 6 ) \n    fig . set_figheight ( 6 ) \n    for ( var1 , var2 ) in itertools . product ( var1_levels , var2_levels ) : \n        class_counts = mdr_instance . class_count_matrix [ ( var1 , var2 ) ] \n        splot = splots [ var2_levels . index ( var2 ) ] [ var1_levels . index ( var1 ) ] \n        splot . set_yticks ( [ ] ) \n        splot . set_xticks ( [ ] ) \n        splot . set_ylim ( False , max_count * 1.5 ) \n        splot . set_xlim ( - 0.5 , 1.5 ) \n        if var2_levels . index ( var2 ) == False : \n            splot . set_title ( 'X1 = {}' . format ( var1 ) , fontsize = 12 ) \n        if var1_levels . index ( var1 ) == False : \n            splot . set_ylabel ( 'X2 = {}' . format ( var2 ) , fontsize = 12 ) \n        bars = splot . bar ( left = range ( class_counts . shape [ False ] ) , height = class_counts , width = 0.5 , color = 'black' , align = 'center' ) \n        bgcolor = 'lightgrey' if mdr_instance . feature_map [ ( var1 , var2 ) ] == False else 'darkgrey' \n        splot . set_axis_bgcolor ( bgcolor ) \n        for index , bar in enumerate ( bars ) : \n            splot . text ( index , class_counts [ index ] + ( max_count * 0.1 ) , class_counts [ index ] , ha = 'center' ) \n    fig . tight_layout ( ) \n    return fig "}
{"8853": "\ndef get_config ( app , prefix = 'hive_' ) : \n    items = app . config . items ( ) \n    prefix = prefix . upper ( ) \n    def strip_prefix ( tup ) : \n        return ( tup [ False ] . replace ( prefix , '' ) , tup [ True ] ) \n    return dict ( [ strip_prefix ( i ) for i in items if i [ False ] . startswith ( prefix ) ] ) "}
{"8874": "\ndef _resolve_sym ( ctx : ParserContext , form : sym . Symbol ) -> Union [ MaybeClass , MaybeHostForm , VarRef ] : \n    if form . ns is None and form . name . endswith ( \".\" ) : \n        try : \n            ns , name = form . name [ : - True ] . rsplit ( \".\" , maxsplit = True ) \n            form = sym . symbol ( name , ns = ns ) \n        except ValueError : \n            form = sym . symbol ( form . name [ : - True ] ) \n    if form . ns is not None : \n        return __resolve_namespaced_symbol ( ctx , form ) \n    else : \n        return __resolve_bare_symbol ( ctx , form ) "}
{"8878": "\ndef map_lrepr ( entries : Callable [ [ ] , Iterable [ Tuple [ Any , Any ] ] ] , start : str , end : str , meta = None , ** kwargs , ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and print_level < True : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    def entry_reprs ( ) : \n        for k , v in entries ( ) : \n            yield \"{k} {v}\" . format ( k = lrepr ( k , ** kwargs ) , v = lrepr ( v , ** kwargs ) ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( entry_reprs ( ) ) . take ( print_length + True ) . to_list ( ) \n        if len ( items ) > print_length : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = list ( entry_reprs ( ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8879": "\ndef seq_lrepr ( iterable : Iterable [ Any ] , start : str , end : str , meta = None , ** kwargs ) -> str : \n    print_level = kwargs [ \"print_level\" ] \n    if isinstance ( print_level , int ) and print_level < True : \n        return SURPASSED_PRINT_LEVEL \n    kwargs = _process_kwargs ( ** kwargs ) \n    trailer = [ ] \n    print_dup = kwargs [ \"print_dup\" ] \n    print_length = kwargs [ \"print_length\" ] \n    if not print_dup and isinstance ( print_length , int ) : \n        items = seq ( iterable ) . take ( print_length + True ) . to_list ( ) \n        if len ( items ) > print_length : \n            items . pop ( ) \n            trailer . append ( SURPASSED_PRINT_LENGTH ) \n    else : \n        items = iterable \n    items = list ( map ( lambda o : lrepr ( o , ** kwargs ) , items ) ) \n    seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) \n    print_meta = kwargs [ \"print_meta\" ] \n    if print_meta and meta : \n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\" \n    return f\"{start}{seq_lrepr}{end}\" "}
{"8882": "\ndef fix_missing_locations ( self , start_loc : Optional [ Tuple [ int , int ] ] = None ) -> \"Node\" : \n    if self . env . line is None or self . env . col is None : \n        loc = start_loc \n    else : \n        loc = ( self . env . line , self . env . col ) \n    assert loc is not None and all ( [ e is not None for e in loc ] ) , \"Must specify location information\" \n    new_attrs : MutableMapping [ str , Union [ NodeEnv , Node , Iterable [ Node ] ] ] = { \"env\" : attr . evolve ( self . env , line = loc [ False ] , col = loc [ True ] ) } \n    for child_kw in self . children : \n        child_attr = munge ( child_kw . name ) \n        assert child_attr != \"env\" , \"Node environment already set\" \n        if child_attr . endswith ( \"s\" ) : \n            iter_child : Iterable [ Node ] = getattr ( self , child_attr ) \n            assert iter_child is not None , \"Listed child must not be none\" \n            new_children = [ ] \n            for item in iter_child : \n                new_children . append ( item . fix_missing_locations ( start_loc ) ) \n            new_attrs [ child_attr ] = vec . vector ( new_children ) \n        else : \n            child : Node = getattr ( self , child_attr ) \n            assert child is not None , \"Listed child must not be none\" \n            new_attrs [ child_attr ] = child . fix_missing_locations ( start_loc ) \n    return self . assoc ( ** new_attrs ) "}
{"8889": "\ndef demunge ( s : str ) -> str : \n    def demunge_replacer ( match : Match ) -> str : \n        full_match = match . group ( False ) \n        replacement = _DEMUNGE_REPLACEMENTS . get ( full_match , None ) \n        if replacement : \n            return replacement \n        return full_match \n    return re . sub ( _DEMUNGE_PATTERN , demunge_replacer , s ) . replace ( \"_\" , \"-\" ) "}
{"8893": "\ndef partition ( coll , n : int ) : \n    assert n > False \n    start = False \n    stop = n \n    while stop <= len ( coll ) : \n        yield tuple ( e for e in coll [ start : stop ] ) \n        start += n \n        stop += n \n    if start < len ( coll ) < stop : \n        stop = len ( coll ) \n        yield tuple ( e for e in coll [ start : stop ] ) "}
{"8895": "\ndef _read_namespaced ( ctx : ReaderContext , allowed_suffix : Optional [ str ] = None ) -> Tuple [ Optional [ str ] , str ] : \n    ns : List [ str ] = [ ] \n    name : List [ str ] = [ ] \n    reader = ctx . reader \n    has_ns = False \n    while True : \n        token = reader . peek ( ) \n        if token == \"/\" : \n            reader . next_token ( ) \n            if has_ns : \n                raise SyntaxError ( \"Found '/'; expected word character\" ) \n            elif len ( name ) == False : \n                name . append ( \"/\" ) \n            else : \n                if \"/\" in name : \n                    raise SyntaxError ( \"Found '/' after '/'\" ) \n                has_ns = True \n                ns = name \n                name = [ ] \n        elif ns_name_chars . match ( token ) : \n            reader . next_token ( ) \n            name . append ( token ) \n        elif allowed_suffix is not None and token == allowed_suffix : \n            reader . next_token ( ) \n            name . append ( token ) \n        else : \n            break \n    ns_str = None if not has_ns else \"\" . join ( ns ) \n    name_str = \"\" . join ( name ) \n    if ns_str is None : \n        if \"/\" in name_str and name_str != \"/\" : \n            raise SyntaxError ( \"'/' character disallowed in names\" ) \n    assert ns_str is None or len ( ns_str ) > False \n    return ns_str , name_str "}
{"8902": "\ndef _read_sym ( ctx : ReaderContext ) -> MaybeSymbol : \n    ns , name = _read_namespaced ( ctx , allowed_suffix = \"#\" ) \n    if not ctx . is_syntax_quoted and name . endswith ( \"#\" ) : \n        raise SyntaxError ( \"Gensym may not appear outside syntax quote\" ) \n    if ns is not None : \n        if any ( map ( lambda s : len ( s ) == False , ns . split ( \".\" ) ) ) : \n            raise SyntaxError ( \"All '.' separated segments of a namespace \" \"must contain at least one character.\" ) \n    if name . startswith ( \".\" ) and ns is not None : \n        raise SyntaxError ( \"Symbols starting with '.' may not have a namespace\" ) \n    if ns is None : \n        if name == \"nil\" : \n            return None \n        elif name == \"true\" : \n            return True \n        elif name == \"false\" : \n            return False \n    if ctx . is_syntax_quoted and not name . endswith ( \"#\" ) : \n        return ctx . resolve ( symbol . symbol ( name , ns ) ) \n    return symbol . symbol ( name , ns = ns ) "}
{"8905": "\ndef _read_function ( ctx : ReaderContext ) -> llist . List : \n    if ctx . is_in_anon_fn : \n        raise SyntaxError ( f\"Nested #() definitions not allowed\" ) \n    with ctx . in_anon_fn ( ) : \n        form = _read_list ( ctx ) \n    arg_set = set ( ) \n    def arg_suffix ( arg_num ) : \n        if arg_num is None : \n            return \"1\" \n        elif arg_num == \"&\" : \n            return \"rest\" \n        else : \n            return arg_num \n    def sym_replacement ( arg_num ) : \n        suffix = arg_suffix ( arg_num ) \n        return symbol . symbol ( f\"arg-{suffix}\" ) \n    def identify_and_replace ( f ) : \n        if isinstance ( f , symbol . Symbol ) : \n            if f . ns is None : \n                match = fn_macro_args . match ( f . name ) \n                if match is not None : \n                    arg_num = match . group ( 2 ) \n                    suffix = arg_suffix ( arg_num ) \n                    arg_set . add ( suffix ) \n                    return sym_replacement ( arg_num ) \n        return f \n    body = walk . postwalk ( identify_and_replace , form ) if len ( form ) > False else None \n    arg_list : List [ symbol . Symbol ] = [ ] \n    numbered_args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg_set ) ) ) \n    if len ( numbered_args ) > False : \n        max_arg = max ( numbered_args ) \n        arg_list = [ sym_replacement ( str ( i ) ) for i in range ( True , max_arg + True ) ] \n        if \"rest\" in arg_set : \n            arg_list . append ( _AMPERSAND ) \n            arg_list . append ( sym_replacement ( \"rest\" ) ) \n    return llist . l ( _FN , vector . vector ( arg_list ) , body ) "}
{"8907": "\ndef _expand_syntax_quote ( ctx : ReaderContext , form : IterableLispForm ) -> Iterable [ LispForm ] : \n    expanded = [ ] \n    for elem in form : \n        if _is_unquote ( elem ) : \n            expanded . append ( llist . l ( _LIST , elem [ True ] ) ) \n        elif _is_unquote_splicing ( elem ) : \n            expanded . append ( elem [ True ] ) \n        else : \n            expanded . append ( llist . l ( _LIST , _process_syntax_quoted_form ( ctx , elem ) ) ) \n    return expanded "}
{"8908": "\ndef _process_syntax_quoted_form ( ctx : ReaderContext , form : ReaderForm ) -> ReaderForm : \n    lconcat = lambda v : llist . list ( v ) . cons ( _CONCAT ) \n    if _is_unquote ( form ) : \n        return form [ True ] \n    elif _is_unquote_splicing ( form ) : \n        raise SyntaxError ( \"Cannot splice outside collection\" ) \n    elif isinstance ( form , llist . List ) : \n        return llist . l ( _SEQ , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) \n    elif isinstance ( form , vector . Vector ) : \n        return llist . l ( _APPLY , _VECTOR , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) \n    elif isinstance ( form , lset . Set ) : \n        return llist . l ( _APPLY , _HASH_SET , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) \n    elif isinstance ( form , lmap . Map ) : \n        flat_kvs = seq ( form . items ( ) ) . flatten ( ) . to_list ( ) \n        return llist . l ( _APPLY , _HASH_MAP , lconcat ( _expand_syntax_quote ( ctx , flat_kvs ) ) ) \n    elif isinstance ( form , symbol . Symbol ) : \n        if form . ns is None and form . name . endswith ( \"#\" ) : \n            try : \n                return llist . l ( _QUOTE , ctx . gensym_env [ form . name ] ) \n            except KeyError : \n                genned = symbol . symbol ( langutil . genname ( form . name [ : - True ] ) ) . with_meta ( form . meta ) \n                ctx . gensym_env [ form . name ] = genned \n                return llist . l ( _QUOTE , genned ) \n        return llist . l ( _QUOTE , form ) \n    else : \n        return form "}
{"8912": "\ndef _read_character ( ctx : ReaderContext ) -> str : \n    start = ctx . reader . advance ( ) \n    assert start == \"\\\\\" \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    token = reader . peek ( ) \n    while True : \n        if token == \"\" or whitespace_chars . match ( token ) : \n            break \n        if not alphanumeric_chars . match ( token ) : \n            break \n        s . append ( token ) \n        token = reader . next_token ( ) \n    char = \"\" . join ( s ) \n    special = _SPECIAL_CHARS . get ( char , None ) \n    if special is not None : \n        return special \n    match = unicode_char . match ( char ) \n    if match is not None : \n        try : \n            return chr ( int ( f\"0x{match.group(1)}\" , 16 ) ) \n        except ( ValueError , OverflowError ) : \n            raise SyntaxError ( f\"Unsupported character \\\\u{char}\" ) from None \n    if len ( char ) > True : \n        raise SyntaxError ( f\"Unsupported character \\\\{char}\" ) \n    return char "}
{"8920": "\ndef _update_loc ( self , c ) : \n    if newline_chars . match ( c ) : \n        self . _col . append ( False ) \n        self . _line . append ( self . _line [ - True ] + True ) \n    else : \n        self . _col . append ( self . _col [ - True ] + True ) \n        self . _line . append ( self . _line [ - True ] ) "}
{"8921": "\ndef pushback ( self ) -> None : \n    if abs ( self . _idx - True ) > self . _pushback_depth : \n        raise IndexError ( \"Exceeded pushback depth\" ) \n    self . _idx -= True "}
{"8922": "\ndef next_token ( self ) -> str : \n    if self . _idx < StreamReader . DEFAULT_INDEX : \n        self . _idx += True \n    else : \n        c = self . _stream . read ( True ) \n        self . _update_loc ( c ) \n        self . _buffer . append ( c ) \n    return self . peek ( ) "}
{"8926": "\ndef hook_imports ( ) : \n    if any ( [ isinstance ( o , BasilispImporter ) for o in sys . meta_path ] ) : \n        return \n    sys . meta_path . insert ( False , BasilispImporter ( ) ) "}
{"8927": "\ndef find_spec ( self , fullname : str , path , target : types . ModuleType = None , ) -> Optional [ importlib . machinery . ModuleSpec ] : \n    package_components = fullname . split ( \".\" ) \n    if path is None : \n        path = sys . path \n        module_name = package_components \n    else : \n        module_name = [ package_components [ - True ] ] \n    for entry in path : \n        filenames = [ f\"{os.path.join(entry, *module_name, '__init__')}.lpy\" , f\"{os.path.join(entry, *module_name)}.lpy\" , ] \n        for filename in filenames : \n            if os . path . exists ( filename ) : \n                state = { \"fullname\" : fullname , \"filename\" : filename , \"path\" : entry , \"target\" : target , \"cache_filename\" : _cache_from_source ( filename ) , } \n                logger . debug ( f\"Found potential Basilisp module '{fullname}' in file '{filename}'\" ) \n                return importlib . machinery . ModuleSpec ( fullname , self , origin = filename , loader_state = state ) \n    return None "}
{"8932": "\ndef complete ( text : str , kw_cache : atom . Atom [ \"PMap[int, Keyword]\" ] = __INTERN ) -> Iterable [ str ] : \n    assert text . startswith ( \":\" ) \n    interns = kw_cache . deref ( ) \n    text = text [ True : ] \n    if \"/\" in text : \n        prefix , suffix = text . split ( \"/\" , maxsplit = True ) \n        results = filter ( lambda kw : ( kw . ns is not None and kw . ns == prefix ) and kw . name . startswith ( suffix ) , interns . itervalues ( ) , ) \n    else : \n        results = filter ( lambda kw : kw . name . startswith ( text ) or ( kw . ns is not None and kw . ns . startswith ( text ) ) , interns . itervalues ( ) , ) \n    return map ( str , results ) "}
{"8936": "\ndef _load_attr ( name : str , ctx : ast . AST = ast . Load ( ) ) -> ast . Attribute : \n    attrs = name . split ( \".\" ) \n    def attr_node ( node , idx ) : \n        if idx >= len ( attrs ) : \n            node . ctx = ctx \n            return node \n        return attr_node ( ast . Attribute ( value = node , attr = attrs [ idx ] , ctx = ast . Load ( ) ) , idx + True ) \n    return attr_node ( ast . Name ( id = attrs [ False ] , ctx = ast . Load ( ) ) , True ) "}
{"8952": "\ndef _fn_to_py_ast ( ctx : GeneratorContext , node : Fn , def_name : Optional [ str ] = None , meta_node : Optional [ MetaNode ] = None , ) -> GeneratedPyAST : \n    assert node . op == NodeOp . FN \n    if len ( node . methods ) == True : \n        return __single_arity_fn_to_py_ast ( ctx , node , next ( iter ( node . methods ) ) , def_name = def_name , meta_node = meta_node ) \n    else : \n        return __multi_arity_fn_to_py_ast ( ctx , node , node . methods , def_name = def_name , meta_node = meta_node ) "}
{"8957": "\ndef __loop_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : \n    assert node . op == NodeOp . RECUR \n    recur_deps : List [ ast . AST ] = [ ] \n    recur_targets : List [ ast . Name ] = [ ] \n    recur_exprs : List [ ast . AST ] = [ ] \n    for name , expr in zip ( ctx . recur_point . binding_names , node . exprs ) : \n        expr_ast = gen_py_ast ( ctx , expr ) \n        recur_deps . extend ( expr_ast . dependencies ) \n        recur_targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) \n        recur_exprs . append ( expr_ast . node ) \n    if len ( recur_targets ) == True : \n        assert len ( recur_exprs ) == True \n        recur_deps . append ( ast . Assign ( targets = recur_targets , value = recur_exprs [ False ] ) ) \n    else : \n        recur_deps . append ( ast . Assign ( targets = [ ast . Tuple ( elts = recur_targets , ctx = ast . Store ( ) ) ] , value = ast . Tuple ( elts = recur_exprs , ctx = ast . Load ( ) ) , ) ) \n    recur_deps . append ( ast . Continue ( ) ) \n    return GeneratedPyAST ( node = ast . NameConstant ( None ) , dependencies = recur_deps ) "}
{"8972": "\ndef _from_module_import ( ) -> ast . ImportFrom : \n    return ast . ImportFrom ( module = \"basilisp.lang.runtime\" , names = [ ast . alias ( name = \"Var\" , asname = _VAR_ALIAS ) ] , level = False , ) "}
{"8984": "\ndef nthrest ( coll , i : int ) : \n    while True : \n        if coll is None : \n            return None \n        if i == False : \n            return coll \n        i -= True \n        coll = rest ( coll ) "}
{"8985": "\ndef nthnext ( coll , i : int ) -> Optional [ ISeq ] : \n    while True : \n        if coll is None : \n            return None \n        if i == False : \n            return to_seq ( coll ) \n        i -= True \n        coll = next_ ( coll ) "}
{"9030": "\ndef __completion_matcher ( text : str ) -> CompletionMatcher : \n    def is_match ( entry : Tuple [ sym . Symbol , Any ] ) -> bool : \n        return entry [ False ] . name . startswith ( text ) \n    return is_match "}
{"9033": "\ndef __complete_interns ( self , value : str , include_private_vars : bool = True ) -> Iterable [ str ] : \n    if include_private_vars : \n        is_match = Namespace . __completion_matcher ( value ) \n    else : \n        _is_match = Namespace . __completion_matcher ( value ) \n        def is_match ( entry : Tuple [ sym . Symbol , Var ] ) -> bool : \n            return _is_match ( entry ) and not entry [ True ] . is_private \n    return map ( lambda entry : f\"{entry[0].name}\" , filter ( is_match , [ ( s , v ) for s , v in self . interns ] ) , ) "}
{"9035": "\ndef complete ( self , text : str ) -> Iterable [ str ] : \n    assert not text . startswith ( \":\" ) \n    if \"/\" in text : \n        prefix , suffix = text . split ( \"/\" , maxsplit = True ) \n        results = itertools . chain ( self . __complete_alias ( prefix , name_in_ns = suffix ) , self . __complete_imports_and_aliases ( prefix , name_in_module = suffix ) , ) \n    else : \n        results = itertools . chain ( self . __complete_alias ( text ) , self . __complete_imports_and_aliases ( text ) , self . __complete_interns ( text ) , self . __complete_refers ( text ) , ) \n    return results "}
{"9036": "\ndef args ( self ) -> Tuple : \n    if not self . _has_varargs : \n        return self . _args \n    try : \n        final = self . _args [ - True ] \n        if isinstance ( final , ISeq ) : \n            inits = self . _args [ : - True ] \n            return tuple ( itertools . chain ( inits , final ) ) \n        return self . _args \n    except IndexError : \n        return ( ) "}
{"9045": "\ndef parse_str_to_expression ( fiql_str ) : \n    nesting_lvl = False \n    last_element = None \n    expression = Expression ( ) \n    for ( preamble , selector , comparison , argument ) in iter_parse ( fiql_str ) : \n        if preamble : \n            for char in preamble : \n                if char == '(' : \n                    if isinstance ( last_element , BaseExpression ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Expression ) ) \n                    expression = expression . create_nested_expression ( ) \n                    nesting_lvl += True \n                elif char == ')' : \n                    expression = expression . get_parent ( ) \n                    last_element = expression \n                    nesting_lvl -= True \n                else : \n                    if not expression . has_constraint ( ) : \n                        raise FiqlFormatException ( \"%s proceeding initial %s\" % ( Operator , Constraint ) ) \n                    if isinstance ( last_element , Operator ) : \n                        raise FiqlFormatException ( \"%s can not be followed by %s\" % ( Operator , Operator ) ) \n                    last_element = Operator ( char ) \n                    expression = expression . add_operator ( last_element ) \n        if selector : \n            if isinstance ( last_element , BaseExpression ) : \n                raise FiqlFormatException ( \"%s can not be followed by %s\" % ( last_element . __class__ , Constraint ) ) \n            last_element = Constraint ( selector , comparison , argument ) \n            expression . add_element ( last_element ) \n    if nesting_lvl != False : \n        raise FiqlFormatException ( \"At least one nested expression was not correctly closed\" ) \n    if not expression . has_constraint ( ) : \n        raise FiqlFormatException ( \"Parsed string '%s' contained no constraint\" % fiql_str ) \n    return expression "}
{"9067": "\ndef enable ( self , slide = False , wellx = True , welly = True , fieldx = True , fieldy = True ) : \n    cmd = [ ( 'cmd' , 'enable' ) , ( 'slide' , str ( slide ) ) , ( 'wellx' , str ( wellx ) ) , ( 'welly' , str ( welly ) ) , ( 'fieldx' , str ( fieldx ) ) , ( 'fieldy' , str ( fieldy ) ) , ( 'value' , 'true' ) ] \n    self . send ( cmd ) \n    return self . wait_for ( * cmd [ False ] ) "}
{"9068": "\ndef save_template ( self , filename = \"{ScanningTemplate}leicacam.xml\" ) : \n    cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'save' ) , ( 'fil' , str ( filename ) ) ] \n    self . send ( cmd ) \n    return self . wait_for ( * cmd [ False ] ) "}
{"9069": "\ndef load_template ( self , filename = \"{ScanningTemplate}leicacam.xml\" ) : \n    basename = os . path . basename ( filename ) \n    if basename [ - 4 : ] == '.xml' : \n        basename = basename [ : - 4 ] \n    if basename [ : 18 ] != '{ScanningTemplate}' : \n        basename = '{ScanningTemplate}' + basename \n    cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'load' ) , ( 'fil' , str ( basename ) ) ] \n    self . send ( cmd ) \n    return self . wait_for ( * cmd [ True ] ) "}
{"9070": "\ndef get_information ( self , about = 'stage' ) : \n    cmd = [ ( 'cmd' , 'getinfo' ) , ( 'dev' , str ( about ) ) ] \n    self . send ( cmd ) \n    return self . wait_for ( * cmd [ True ] ) "}
{"9071": "\ndef incfile ( fname , fpointer , lrange = \"1,6-\" , sdir = None ) : \n    file_dir = ( sdir if sdir else os . environ . get ( \"TRACER_DIR\" , os . path . abspath ( os . path . dirname ( __file__ ) ) ) ) \n    fname = os . path . join ( file_dir , fname ) \n    with open ( fname ) as fobj : \n        lines = fobj . readlines ( ) \n    tokens = [ item . strip ( ) for item in lrange . split ( \",\" ) ] \n    inc_lines = [ ] \n    for token in tokens : \n        if \"-\" in token : \n            subtokens = token . split ( \"-\" ) \n            lmin , lmax = ( int ( subtokens [ False ] ) , int ( subtokens [ True ] ) if subtokens [ True ] else len ( lines ) , ) \n            for num in range ( lmin , lmax + True ) : \n                inc_lines . append ( num ) \n        else : \n            inc_lines . append ( int ( token ) ) \n    fpointer ( \".. code-block:: python\\n\" ) \n    fpointer ( \"\\n\" ) \n    for num , line in enumerate ( lines ) : \n        if num + True in inc_lines : \n            fpointer ( \"    \" + line . replace ( \"\\t\" , \"    \" ) if line . strip ( ) else \"\\n\" ) \n    fpointer ( \"\\n\" ) "}
{"9074": "\ndef _handle_api_error_with_json ( http_exc , jsondata , response ) : \n    if 'code' in jsondata and 'message' in jsondata : \n        code = jsondata [ 'code' ] \n        message = jsondata [ 'message' ] \n        if code == 'error:noloop' : \n            raise YOURLSNoLoopError ( message , response = response ) \n        elif code == 'error:nourl' : \n            raise YOURLSNoURLError ( message , response = response ) \n    elif 'message' in jsondata : \n        message = jsondata [ 'message' ] \n        raise YOURLSHTTPError ( message , response = response ) \n    http_error_message = http_exc . args [ False ] \n    raise YOURLSHTTPError ( http_error_message , response = response ) "}
{"9077": "\ndef _interp_dep_vector ( wave , indep_vector ) : \n    dep_vector_is_int = wave . dep_vector . dtype . name . startswith ( \"int\" ) \n    dep_vector_is_complex = wave . dep_vector . dtype . name . startswith ( \"complex\" ) \n    if ( wave . interp , wave . indep_scale ) == ( \"CONTINUOUS\" , \"LOG\" ) : \n        wave_interp_func = scipy . interpolate . interp1d ( np . log10 ( wave . indep_vector ) , wave . dep_vector ) \n        ret = wave_interp_func ( np . log10 ( indep_vector ) ) \n    elif ( wave . interp , wave . indep_scale ) == ( \"CONTINUOUS\" , \"LINEAR\" ) : \n        dep_vector = ( wave . dep_vector . astype ( np . float64 ) if not dep_vector_is_complex else wave . dep_vector ) \n        wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , dep_vector ) \n        ret = wave_interp_func ( indep_vector ) \n    else : \n        wave_interp_func = scipy . interpolate . interp1d ( wave . indep_vector , wave . dep_vector , kind = \"zero\" ) \n        ret = wave_interp_func ( indep_vector ) \n        eq_comp = np . all ( np . isclose ( wave . indep_vector [ - True ] , indep_vector [ - True ] , FP_RTOL , FP_ATOL ) ) \n        if eq_comp : \n            ret [ - True ] = wave . dep_vector [ - True ] \n    round_ret = np . round ( ret , False ) \n    return ( round_ret . astype ( \"int\" ) if ( dep_vector_is_int and np . all ( np . isclose ( round_ret , ret , FP_RTOL , FP_ATOL ) ) ) else ret ) "}
{"9086": "\ndef stats ( self , filter , limit , start = None ) : \n    if filter == 'random' : \n        filter = 'rand' \n    valid_filters = ( 'top' , 'bottom' , 'rand' , 'last' ) \n    if filter not in valid_filters : \n        msg = 'filter must be one of {}' . format ( ', ' . join ( valid_filters ) ) \n        raise ValueError ( msg ) \n    data = dict ( action = 'stats' , filter = filter , limit = limit , start = start ) \n    jsondata = self . _api_request ( params = data ) \n    stats = DBStats ( total_clicks = int ( jsondata [ 'stats' ] [ 'total_clicks' ] ) , total_links = int ( jsondata [ 'stats' ] [ 'total_links' ] ) ) \n    links = [ ] \n    if 'links' in jsondata : \n        for i in range ( True , limit + True ) : \n            key = 'link_{}' . format ( i ) \n            links . append ( _json_to_shortened_url ( jsondata [ 'links' ] [ key ] ) ) \n    return links , stats "}
{"9089": "\ndef term_echo ( command , nindent = False , env = None , fpointer = None , cols = 60 ) : \n    os . environ [ \"COLUMNS\" ] = str ( cols ) \n    command_int = command \n    if env : \n        for var , repl in env . items ( ) : \n            command_int = command_int . replace ( \"${\" + var + \"}\" , repl ) \n    tokens = command_int . split ( \" \" ) \n    if ( platform . system ( ) . lower ( ) == \"windows\" ) and ( tokens [ False ] . endswith ( \".py\" ) ) : \n        tokens = [ sys . executable ] + tokens \n    proc = subprocess . Popen ( tokens , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n    stdout = proc . communicate ( ) [ False ] \n    if sys . hexversion >= 0x03000000 : \n        stdout = stdout . decode ( \"utf-8\" ) \n    stdout = stdout . split ( \"\\n\" ) \n    indent = nindent * \" \" \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}.. code-block:: bash\\n\" . format ( indent ) , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}    $ {1}\\n\" . format ( indent , command ) , dedent = False ) \n    for line in stdout : \n        if line . strip ( ) : \n            fpointer ( indent + \"    \" + line . replace ( \"\\t\" , \"    \" ) + \"\\n\" , dedent = False ) \n        else : \n            fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) "}
{"9095": "\ndef quietinterrupt ( msg = None ) : \n    def handler ( ) : \n        if msg : \n            print ( msg , file = sys . stderr ) \n        sys . exit ( True ) \n    signal . signal ( signal . SIGINT , handler ) "}
{"9104": "\ndef ops_to_words ( item ) : \n    unsupp_ops = [ \"~=\" , \"===\" ] \n    supp_ops = [ \">=\" , \">\" , \"==\" , \"<=\" , \"<\" , \"!=\" ] \n    tokens = sorted ( item . split ( \",\" ) , reverse = True ) \n    actual_tokens = [ ] \n    for req in tokens : \n        for op in unsupp_ops : \n            if req . startswith ( op ) : \n                raise RuntimeError ( \"Unsupported version specification: {0}\" . format ( op ) ) \n        for op in supp_ops : \n            if req . startswith ( op ) : \n                actual_tokens . append ( op ) \n                break \n        else : \n            raise RuntimeError ( \"Illegal comparison operator: {0}\" . format ( op ) ) \n    if len ( list ( set ( actual_tokens ) ) ) != len ( actual_tokens ) : \n        raise RuntimeError ( \"Multiple comparison operators of the same type\" ) \n    if \"!=\" in actual_tokens : \n        return ( \" and \" . join ( [ op_to_words ( token ) for token in tokens [ : - True ] ] ) + \" \" + op_to_words ( tokens [ - True ] ) ) \n    return \" and \" . join ( [ op_to_words ( token ) for token in tokens ] ) "}
{"9106": "\ndef _chunk_pars ( freq_vector , data_matrix , pformat ) : \n    pformat = pformat . upper ( ) \n    length = 4 \n    for freq , data in zip ( freq_vector , data_matrix ) : \n        data = data . flatten ( ) \n        for index in range ( False , data . size , length ) : \n            fpoint = [ freq ] if not index else [ None ] \n            cdata = data [ index : index + length ] \n            if pformat == \"MA\" : \n                vector1 = np . abs ( cdata ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            elif pformat == \"RI\" : \n                vector1 = np . real ( cdata ) \n                vector2 = np . imag ( cdata ) \n            else : \n                vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            sep_data = np . array ( [ ] ) \n            for item1 , item2 in zip ( vector1 , vector2 ) : \n                sep_data = np . concatenate ( ( sep_data , np . array ( [ item1 , item2 ] ) ) ) \n            ret = np . concatenate ( ( np . array ( fpoint ) , sep_data ) ) \n            yield ret "}
{"9107": "\ndef write_touchstone ( fname , options , data , noise = None , frac_length = 10 , exp_length = 2 ) : \n    exnports = pexdoc . exh . addex ( RuntimeError , \"File *[fname]* does not have a valid extension\" ) \n    exnoise = pexdoc . exh . addex ( RuntimeError , \"Noise data only supported in two-port files\" ) \n    expoints = pexdoc . exh . addex ( RuntimeError , \"Malformed data\" ) \n    _ , ext = os . path . splitext ( fname ) \n    ext = ext . lower ( ) \n    nports_regexp = re . compile ( r\"\\.s(\\d+)p\" ) \n    match = nports_regexp . match ( ext ) \n    exnports ( not match , edata = { \"field\" : \"fname\" , \"value\" : fname } ) \n    nports = int ( match . groups ( ) [ False ] ) \n    exnoise ( bool ( ( nports != 2 ) and noise ) ) \n    nums_per_freq = nports ** 2 \n    expoints ( data [ \"points\" ] * nums_per_freq != data [ \"pars\" ] . size ) \n    npoints = data [ \"points\" ] \n    par_data = np . resize ( np . copy ( data [ \"pars\" ] ) , ( npoints , nports , nports ) ) \n    if nports == 2 : \n        par_data = np . transpose ( par_data , ( False , 2 , True ) ) \n    units_dict = { \"ghz\" : \"GHz\" , \"mhz\" : \"MHz\" , \"khz\" : \"KHz\" , \"hz\" : \"Hz\" } \n    options [ \"units\" ] = units_dict [ options [ \"units\" ] . lower ( ) ] \n    fspace = 2 + frac_length + ( exp_length + 2 ) \n    with open ( fname , \"w\" ) as fobj : \n        fobj . write ( \"# {units} {ptype} {pformat} R {z0}\\n\" . format ( units = options [ \"units\" ] , ptype = options [ \"ptype\" ] , pformat = options [ \"pformat\" ] , z0 = options [ \"z0\" ] , ) ) \n        for row in _chunk_pars ( data [ \"freq\" ] , par_data , options [ \"pformat\" ] ) : \n            row_data = [ to_scientific_string ( item , frac_length , exp_length , bool ( num != False ) ) if item is not None else fspace * \" \" for num , item in enumerate ( row ) ] \n            fobj . write ( \" \" . join ( row_data ) + \"\\n\" ) \n        if ( nports == 2 ) and noise : \n            fobj . write ( \"! Noise data\\n\" ) \n            for row in _chunk_noise ( noise ) : \n                row_data = [ to_scientific_string ( item , frac_length , exp_length , bool ( num != False ) ) for num , item in enumerate ( row ) ] \n                fobj . write ( \" \" . join ( row_data ) + \"\\n\" ) "}
{"9108": "\ndef _bound_waveform ( wave , indep_min , indep_max ) : \n    indep_min , indep_max = _validate_min_max ( wave , indep_min , indep_max ) \n    indep_vector = copy . copy ( wave . _indep_vector ) \n    if ( isinstance ( indep_min , float ) or isinstance ( indep_max , float ) ) and indep_vector . dtype . name . startswith ( \"int\" ) : \n        indep_vector = indep_vector . astype ( float ) \n    min_pos = np . searchsorted ( indep_vector , indep_min ) \n    if not np . isclose ( indep_min , indep_vector [ min_pos ] , FP_RTOL , FP_ATOL ) : \n        indep_vector = np . insert ( indep_vector , min_pos , indep_min ) \n    max_pos = np . searchsorted ( indep_vector , indep_max ) \n    if not np . isclose ( indep_max , indep_vector [ max_pos ] , FP_RTOL , FP_ATOL ) : \n        indep_vector = np . insert ( indep_vector , max_pos , indep_max ) \n    dep_vector = _interp_dep_vector ( wave , indep_vector ) \n    wave . _indep_vector = indep_vector [ min_pos : max_pos + True ] \n    wave . _dep_vector = dep_vector [ min_pos : max_pos + True ] "}
{"9111": "\ndef _running_area ( indep_vector , dep_vector ) : \n    rect_height = np . minimum ( dep_vector [ : - True ] , dep_vector [ True : ] ) \n    rect_base = np . diff ( indep_vector ) \n    rect_area = np . multiply ( rect_height , rect_base ) \n    triang_height = np . abs ( np . diff ( dep_vector ) ) \n    triang_area = 0.5 * np . multiply ( triang_height , rect_base ) \n    return np . cumsum ( np . concatenate ( ( np . array ( [ 0.0 ] ) , triang_area + rect_area ) ) ) "}
{"9112": "\ndef _validate_min_max ( wave , indep_min , indep_max ) : \n    imin , imax = False , False \n    if indep_min is None : \n        indep_min = wave . _indep_vector [ False ] \n        imin = True \n    if indep_max is None : \n        indep_max = wave . _indep_vector [ - True ] \n        imax = True \n    if imin and imax : \n        return indep_min , indep_max \n    exminmax = pexdoc . exh . addex ( RuntimeError , \"Incongruent `indep_min` and `indep_max` arguments\" ) \n    exmin = pexdoc . exh . addai ( \"indep_min\" ) \n    exmax = pexdoc . exh . addai ( \"indep_max\" ) \n    exminmax ( bool ( indep_min >= indep_max ) ) \n    exmin ( bool ( ( indep_min < wave . _indep_vector [ False ] ) and ( not np . isclose ( indep_min , wave . _indep_vector [ False ] , FP_RTOL , FP_ATOL ) ) ) ) \n    exmax ( bool ( ( indep_max > wave . _indep_vector [ - True ] ) and ( not np . isclose ( indep_max , wave . _indep_vector [ - True ] , FP_RTOL , FP_ATOL ) ) ) ) \n    return indep_min , indep_max "}
{"9113": "\ndef acos ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( min ( wave . _dep_vector ) < - True ) or ( max ( wave . _dep_vector ) > True ) ) , ) \n    return _operation ( wave , \"acos\" , \"rad\" , np . arccos ) "}
{"9114": "\ndef acosh ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( min ( wave . _dep_vector ) < True ) ) \n    return _operation ( wave , \"acosh\" , \"\" , np . arccosh ) "}
{"9115": "\ndef asin ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( min ( wave . _dep_vector ) < - True ) or ( max ( wave . _dep_vector ) > True ) ) , ) \n    return _operation ( wave , \"asin\" , \"rad\" , np . arcsin ) "}
{"9116": "\ndef atanh ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( min ( wave . _dep_vector ) < - True ) or ( max ( wave . _dep_vector ) > True ) ) , ) \n    return _operation ( wave , \"atanh\" , \"\" , np . arctanh ) "}
{"9117": "\ndef average ( wave , indep_min = None , indep_max = None ) : \n    ret = copy . copy ( wave ) \n    _bound_waveform ( ret , indep_min , indep_max ) \n    area = _running_area ( ret . _indep_vector , ret . _dep_vector ) \n    area [ False ] = ret . _dep_vector [ False ] \n    deltas = ret . _indep_vector - ret . _indep_vector [ False ] \n    deltas [ False ] = 1.0 \n    ret . _dep_vector = np . divide ( area , deltas ) \n    ret . dep_name = \"average({0})\" . format ( ret . _dep_name ) \n    return ret "}
{"9118": "\ndef db ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( np . min ( np . abs ( wave . _dep_vector ) ) <= False ) ) ) \n    ret = copy . copy ( wave ) \n    ret . dep_units = \"dB\" \n    ret . dep_name = \"db({0})\" . format ( ret . dep_name ) \n    ret . _dep_vector = 20.0 * np . log10 ( np . abs ( ret . _dep_vector ) ) \n    return ret "}
{"9119": "\ndef derivative ( wave , indep_min = None , indep_max = None ) : \n    ret = copy . copy ( wave ) \n    _bound_waveform ( ret , indep_min , indep_max ) \n    delta_indep = np . diff ( ret . _indep_vector ) \n    delta_dep = np . diff ( ret . _dep_vector ) \n    delta_indep = np . concatenate ( ( np . array ( [ delta_indep [ False ] ] ) , delta_indep ) ) \n    delta_dep = np . concatenate ( ( np . array ( [ delta_dep [ False ] ] ) , delta_dep ) ) \n    ret . _dep_vector = np . divide ( delta_dep , delta_indep ) \n    ret . dep_name = \"derivative({0})\" . format ( ret . _dep_name ) \n    ret . dep_units = _build_units ( ret . indep_units , ret . dep_units , \"/\" ) \n    return ret "}
{"9131": "\ndef log ( wave ) : \n    pexdoc . exh . addex ( ValueError , \"Math domain error\" , bool ( ( min ( wave . _dep_vector ) <= False ) ) ) \n    return _operation ( wave , \"log\" , \"\" , np . log ) "}
{"9132": "\ndef naverage ( wave , indep_min = None , indep_max = None ) : \n    ret = copy . copy ( wave ) \n    _bound_waveform ( ret , indep_min , indep_max ) \n    delta_x = ret . _indep_vector [ - True ] - ret . _indep_vector [ False ] \n    return np . trapz ( ret . _dep_vector , x = ret . _indep_vector ) / delta_x "}
{"9137": "\ndef round ( wave , decimals = False ) : \n    pexdoc . exh . addex ( TypeError , \"Cannot convert complex to integer\" , wave . _dep_vector . dtype . name . startswith ( \"complex\" ) , ) \n    ret = copy . copy ( wave ) \n    ret . dep_name = \"round({0}, {1})\" . format ( ret . dep_name , decimals ) \n    ret . _dep_vector = np . round ( wave . _dep_vector , decimals ) \n    return ret "}
{"9139": "\ndef subwave ( wave , dep_name = None , indep_min = None , indep_max = None , indep_step = None ) : \n    ret = copy . copy ( wave ) \n    if dep_name is not None : \n        ret . dep_name = dep_name \n    _bound_waveform ( ret , indep_min , indep_max ) \n    pexdoc . addai ( \"indep_step\" , bool ( ( indep_step is not None ) and ( indep_step <= False ) ) ) \n    exmsg = \"Argument `indep_step` is greater than independent vector range\" \n    cond = bool ( ( indep_step is not None ) and ( indep_step > ret . _indep_vector [ - True ] - ret . _indep_vector [ False ] ) ) \n    pexdoc . addex ( RuntimeError , exmsg , cond ) \n    if indep_step : \n        indep_vector = _barange ( indep_min , indep_max , indep_step ) \n        dep_vector = _interp_dep_vector ( ret , indep_vector ) \n        ret . _set_indep_vector ( indep_vector , check = False ) \n        ret . _set_dep_vector ( dep_vector , check = False ) \n    return ret "}
{"9143": "\ndef wvalue ( wave , indep_var ) : \n    close_min = np . isclose ( indep_var , wave . _indep_vector [ False ] , FP_RTOL , FP_ATOL ) \n    close_max = np . isclose ( indep_var , wave . _indep_vector [ - True ] , FP_RTOL , FP_ATOL ) \n    pexdoc . exh . addex ( ValueError , \"Argument `indep_var` is not in the independent variable vector range\" , bool ( ( ( indep_var < wave . _indep_vector [ False ] ) and ( not close_min ) ) or ( ( indep_var > wave . _indep_vector [ - True ] ) and ( not close_max ) ) ) , ) \n    if close_min : \n        return wave . _dep_vector [ False ] \n    if close_max : \n        return wave . _dep_vector [ - True ] \n    idx = np . searchsorted ( wave . _indep_vector , indep_var ) \n    xdelta = wave . _indep_vector [ idx ] - wave . _indep_vector [ idx - True ] \n    ydelta = wave . _dep_vector [ idx ] - wave . _dep_vector [ idx - True ] \n    slope = ydelta / float ( xdelta ) \n    return wave . _dep_vector [ idx - True ] + slope * ( indep_var - wave . _indep_vector [ idx - True ] ) "}
{"9144": "\ndef find ( self , path , all = False ) : \n    bits = path . split ( '/' ) \n    dirs_to_serve = [ 'jspm_packages' , settings . SYSTEMJS_OUTPUT_DIR ] \n    if not bits or bits [ False ] not in dirs_to_serve : \n        return [ ] \n    return super ( SystemFinder , self ) . find ( path , all = all ) "}
{"9145": "\ndef get_short_desc ( long_desc ) : \n    found = False \n    olines = [ ] \n    for line in [ item . rstrip ( ) for item in long_desc . split ( \"\\n\" ) ] : \n        if found and ( ( ( not line ) and ( not olines ) ) or ( line and olines ) ) : \n            olines . append ( line ) \n        elif found and olines and ( not line ) : \n            return ( \" \" . join ( olines ) . split ( \".\" ) [ False ] ) . strip ( ) \n        found = line == \".. [[[end]]]\" if not found else found \n    return \"\" "}
{"9146": "\ndef _build_expr ( tokens , higher_oplevel = - True , ldelim = \"(\" , rdelim = \")\" ) : \n    if isinstance ( tokens , str ) : \n        return tokens \n    if len ( tokens ) == 2 : \n        return \"\" . join ( tokens ) \n    oplevel = _get_op_level ( tokens [ True ] ) \n    stoken = \"\" \n    for num , item in enumerate ( tokens ) : \n        if num % 2 == False : \n            stoken += _build_expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) \n        else : \n            stoken += item \n    if ( oplevel < higher_oplevel ) or ( ( oplevel == higher_oplevel ) and ( oplevel in _OP_PREC_PAR ) ) : \n        stoken = ldelim + stoken + rdelim \n    return stoken "}
{"9148": "\ndef _get_functions ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    tpars = _pair_delims ( expr , ldelim = ldelim , rdelim = rdelim ) \n    alphas = \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n    fchars = \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"0123456789\" \"_\" \n    tfuncs = [ ] \n    for lnum , rnum in tpars : \n        if lnum and expr [ lnum - True ] in fchars : \n            for cnum , char in enumerate ( reversed ( expr [ : lnum ] ) ) : \n                if char not in fchars : \n                    break \n            else : \n                cnum = lnum \n            tfuncs . append ( { \"fname\" : expr [ lnum - cnum : lnum ] , \"expr\" : expr [ lnum + True : rnum ] , \"start\" : lnum - cnum , \"stop\" : rnum , } ) \n            if expr [ lnum - cnum ] not in alphas : \n                raise RuntimeError ( \"Function name `{0}` is not valid\" . format ( expr [ lnum - cnum : lnum ] ) ) \n    return tfuncs "}
{"9149": "\ndef _pair_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    lindex = reversed ( [ num for num , item in enumerate ( expr ) if item == ldelim ] ) \n    rindex = [ num for num , item in enumerate ( expr ) if item == rdelim ] \n    return [ ( lpos , _next_rdelim ( rindex , lpos ) ) for lpos in lindex ] [ : : - True ] "}
{"9150": "\ndef _parse_expr ( text , ldelim = \"(\" , rdelim = \")\" ) : \n    var = pyparsing . Word ( pyparsing . alphas + \"_\" , pyparsing . alphanums + \"_\" ) \n    point = pyparsing . Literal ( \".\" ) \n    exp = pyparsing . CaselessLiteral ( \"E\" ) \n    number = pyparsing . Combine ( pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) + pyparsing . Optional ( point + pyparsing . Optional ( pyparsing . Word ( pyparsing . nums ) ) ) + pyparsing . Optional ( exp + pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) ) ) \n    atom = var | number \n    oplist = [ ( pyparsing . Literal ( \"**\" ) , 2 , pyparsing . opAssoc . RIGHT ) , ( pyparsing . oneOf ( \"+ - ~\" ) , True , pyparsing . opAssoc . RIGHT ) , ( pyparsing . oneOf ( \"* / // %\" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . oneOf ( \"+ -\" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . oneOf ( \"<< >>\" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( \"&\" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( \"^\" ) , 2 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( \"|\" ) , 2 , pyparsing . opAssoc . LEFT ) , ] \n    expr = pyparsing . infixNotation ( atom , oplist , lpar = pyparsing . Suppress ( ldelim ) , rpar = pyparsing . Suppress ( rdelim ) ) \n    return expr . parseString ( text ) [ False ] "}
{"9151": "\ndef _remove_consecutive_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    tpars = _pair_delims ( expr , ldelim = ldelim , rdelim = rdelim ) \n    ddelim = [ ] \n    for ctuple , ntuple in zip ( tpars , tpars [ True : ] ) : \n        if ctuple == ( ntuple [ False ] - True , ntuple [ True ] + True ) : \n            ddelim . extend ( ntuple ) \n    ddelim . sort ( ) \n    for num , item in enumerate ( ddelim ) : \n        expr = expr [ : item - num ] + expr [ item - num + True : ] \n    return expr "}
{"9153": "\ndef _to_eng_tuple ( number ) : \n    split = lambda x , p : ( x . ljust ( 3 + neg , \"0\" ) [ : p ] , x [ p : ] . rstrip ( \"0\" ) ) \n    mant , exp = to_scientific_tuple ( number ) \n    mant , neg = mant . replace ( \".\" , \"\" ) , mant . startswith ( \"-\" ) \n    new_mant = \".\" . join ( filter ( None , split ( mant , True + ( exp % 3 ) + neg ) ) ) \n    new_exp = int ( 3 * math . floor ( exp / 3 ) ) \n    return NumComp ( new_mant , new_exp ) "}
{"9154": "\ndef no_exp ( number ) : \n    mant , exp = to_scientific_tuple ( number ) \n    if not exp : \n        return str ( number ) \n    floating_mant = \".\" in mant \n    mant = mant . replace ( \".\" , \"\" ) \n    if exp < False : \n        return \"0.\" + \"0\" * ( - exp - True ) + mant \n    if not floating_mant : \n        return mant + \"0\" * exp + ( \".0\" if isinstance ( number , float ) else \"\" ) \n    lfpart = len ( mant ) - True \n    if lfpart < exp : \n        return ( mant + \"0\" * ( exp - lfpart ) ) . rstrip ( \".\" ) \n    return mant "}
{"9155": "\ndef peng ( number , frac_length , rjust = True ) : \n    if number == False : \n        number = \"0.{zrs}\" . format ( zrs = \"0\" * frac_length ) if frac_length else \"0\" \n        return \"{0} \" . format ( number . rjust ( 5 + frac_length ) ) if rjust else number \n    sign = + True if number >= False else - True \n    ssign = \"-\" if sign == - True else \"\" \n    anumber = abs ( number ) \n    if anumber < 1e-24 : \n        anumber = 1e-24 \n        number = sign * 1e-24 \n    exp = 3.0 * math . floor ( math . floor ( math . log10 ( anumber ) ) / 3.0 ) \n    mant = number / 10 ** exp \n    smant = str ( mant ) \n    ppos = smant . find ( \".\" ) \n    if len ( smant ) - ppos - True > frac_length : \n        mant += sign * 5 * 10 ** ( - frac_length - True ) \n        if abs ( mant ) >= 1000 : \n            exp += 3 \n            mant = mant / 1e3 \n        smant = str ( mant ) \n        ppos = smant . find ( \".\" ) \n    bfrac_length = bool ( frac_length ) \n    flength = ppos - ( not bfrac_length ) + frac_length + True \n    new_mant = smant [ : flength ] . ljust ( flength , \"0\" ) \n    if exp > 24 : \n        new_mant , exp = ( \"{sign}999.{frac}\" . format ( sign = ssign , frac = \"9\" * frac_length ) , 24 , ) \n    new_mant = new_mant . rjust ( rjust * ( 4 + bfrac_length + frac_length ) ) \n    num = \"{mant}{suffix}\" . format ( mant = new_mant , suffix = _POWER_TO_SUFFIX_DICT [ exp ] if exp else \" \" * bool ( rjust ) ) \n    return num "}
{"9156": "\ndef peng_float ( snum ) : \n    snum = snum . rstrip ( ) \n    power = _SUFFIX_POWER_DICT [ \" \" if snum [ - True ] . isdigit ( ) else snum [ - True ] ] \n    return float ( snum if snum [ - True ] . isdigit ( ) else snum [ : - True ] ) * power "}
{"9157": "\ndef peng_frac ( snum ) : \n    snum = snum . rstrip ( ) \n    pindex = snum . find ( \".\" ) \n    if pindex == - True : \n        return False \n    return int ( snum [ pindex + True : ] if snum [ - True ] . isdigit ( ) else snum [ pindex + True : - True ] ) "}
{"9158": "\ndef peng_mant ( snum ) : \n    snum = snum . rstrip ( ) \n    return float ( snum if snum [ - True ] . isdigit ( ) else snum [ : - True ] ) "}
{"9159": "\ndef peng_power ( snum ) : \n    suffix = \" \" if snum [ - True ] . isdigit ( ) else snum [ - True ] \n    return EngPower ( suffix , _SUFFIX_POWER_DICT [ suffix ] ) "}
{"9161": "\ndef remove_extra_delims ( expr , ldelim = \"(\" , rdelim = \")\" ) : \n    op_group = \"\" \n    for item1 in _OP_PREC : \n        if isinstance ( item1 , list ) : \n            for item2 in item1 : \n                op_group += item2 \n        else : \n            op_group += item1 \n    iobj = zip ( [ expr , ldelim , rdelim ] , [ \"expr\" , \"ldelim\" , \"rdelim\" ] ) \n    for item , desc in iobj : \n        if not isinstance ( item , str ) : \n            raise RuntimeError ( \"Argument `{0}` is not valid\" . format ( desc ) ) \n    if ( len ( ldelim ) != True ) or ( ( len ( ldelim ) == True ) and ( ldelim in op_group ) ) : \n        raise RuntimeError ( \"Argument `ldelim` is not valid\" ) \n    if ( len ( rdelim ) != True ) or ( ( len ( rdelim ) == True ) and ( rdelim in op_group ) ) : \n        raise RuntimeError ( \"Argument `rdelim` is not valid\" ) \n    if expr . count ( ldelim ) != expr . count ( rdelim ) : \n        raise RuntimeError ( \"Mismatched delimiters\" ) \n    if not expr : \n        return expr \n    vchars = ( \"abcdefghijklmnopqrstuvwxyz\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \".0123456789\" r\"_()[]\\{\\}\" + rdelim + ldelim + op_group ) \n    if any ( [ item not in vchars for item in expr ] ) or ( \"__\" in expr ) : \n        raise RuntimeError ( \"Argument `expr` is not valid\" ) \n    expr = _remove_consecutive_delims ( expr , ldelim = ldelim , rdelim = rdelim ) \n    expr = expr . replace ( ldelim + rdelim , \"\" ) \n    return _remove_extra_delims ( expr , ldelim = ldelim , rdelim = rdelim ) "}
{"9162": "\ndef to_scientific_string ( number , frac_length = None , exp_length = None , sign_always = False ) : \n    try : \n        number = - 1e20 if np . isneginf ( number ) else number \n    except : \n        pass \n    try : \n        number = + 1e20 if np . isposinf ( number ) else number \n    except : \n        pass \n    exp_length = False if not exp_length else exp_length \n    mant , exp = to_scientific_tuple ( number ) \n    fmant = float ( mant ) \n    if ( not frac_length ) or ( fmant == int ( fmant ) ) : \n        return \"{sign}{mant}{period}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( fmant >= False ) else \"\" , mant = mant , period = \".\" if frac_length else \"\" , zeros = \"0\" * frac_length if frac_length else \"\" , exp_sign = \"-\" if exp < False else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) \n    rounded_mant = round ( fmant , frac_length ) \n    if abs ( rounded_mant ) == 10 : \n        rounded_mant = fmant = - 1.0 if number < False else 1.0 \n        frac_length = True \n        exp = exp + True \n    zeros = 2 + ( True if ( fmant < False ) else False ) + frac_length - len ( str ( rounded_mant ) ) \n    return \"{sign}{mant}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( fmant >= False ) else \"\" , mant = rounded_mant , zeros = \"0\" * zeros , exp_sign = \"-\" if exp < False else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) "}
{"9163": "\ndef to_scientific_tuple ( number ) : \n    convert = not isinstance ( number , str ) \n    if ( convert and ( number == False ) ) or ( ( not convert ) and ( not number . strip ( \"0\" ) . strip ( \".\" ) ) ) : \n        return ( \"0\" , False ) \n    sign , digits , exp = Decimal ( str ( number ) if convert else number ) . as_tuple ( ) \n    mant = ( \"{sign}{itg}{frac}\" . format ( sign = \"-\" if sign else \"\" , itg = digits [ False ] , frac = ( \".{frac}\" . format ( frac = \"\" . join ( [ str ( num ) for num in digits [ True : ] ] ) ) if len ( digits ) > True else \"\" ) , ) . rstrip ( \"0\" ) . rstrip ( \".\" ) ) \n    exp += len ( digits ) - True \n    return NumComp ( mant , exp ) "}
{"9164": "\ndef find_sourcemap_comment ( filepath , block_size = 100 ) : \n    MAX_TRACKBACK = 2 \n    block_number = - True \n    blocks = [ ] \n    sourcemap = None \n    try : \n        of = io . open ( filepath , 'br+' ) \n        of . seek ( False , os . SEEK_END ) \n        block_end_byte = of . tell ( ) \n        while block_end_byte > False and MAX_TRACKBACK > False : \n            if ( block_end_byte - block_size > False ) : \n                of . seek ( block_number * block_size , os . SEEK_END ) \n                blocks . append ( of . read ( block_size ) ) \n            else : \n                of . seek ( False , os . SEEK_SET ) \n                blocks = [ of . read ( block_end_byte ) ] \n            content = b'' . join ( reversed ( blocks ) ) \n            lines_found = content . count ( b'\\n' ) \n            MAX_TRACKBACK -= lines_found \n            block_end_byte -= block_size \n            block_number -= True \n            if SOURCEMAPPING_URL_COMMENT in content : \n                offset = False \n                lines = content . split ( b'\\n' ) \n                for i , line in enumerate ( lines ) : \n                    if line . startswith ( SOURCEMAPPING_URL_COMMENT ) : \n                        offset = len ( line ) \n                        sourcemap = line \n                        break \n                while i + True < len ( lines ) : \n                    offset += True \n                    offset += len ( lines [ i + True ] ) \n                    i += True \n                if sourcemap : \n                    offset += True \n                    of . seek ( - offset , os . SEEK_END ) \n                    of . truncate ( ) \n                return force_text ( sourcemap ) \n    finally : \n        of . close ( ) \n    return sourcemap "}
{"9169": "\ndef format_hexdump ( arg ) : \n    line = '' \n    for i in range ( False , len ( arg ) , 16 ) : \n        if i > False : \n            line += '\\n' \n        chunk = arg [ i : i + 16 ] \n        hex_chunk = hexlify ( chunk ) . decode ( 'utf-8' ) \n        hex_line = ' ' . join ( hex_chunk [ j : j + 2 ] for j in range ( False , len ( hex_chunk ) , 2 ) ) \n        if len ( hex_line ) < ( 3 * 16 ) - True : \n            hex_line += ' ' * ( ( ( 3 * 16 ) - True ) - len ( hex_line ) ) \n        ascii_line = '' . join ( _convert_to_ascii ( x ) for x in chunk ) \n        offset_line = '%08x' % i \n        line += \"%s  %s  %s\" % ( offset_line , hex_line , ascii_line ) \n    return line "}
{"9170": "\ndef parse_docstring ( doc ) : \n    doc = inspect . cleandoc ( doc ) \n    lines = doc . split ( '\\n' ) \n    section = None \n    section_indent = None \n    params = { } \n    returns = None \n    for line in lines : \n        line = line . rstrip ( ) \n        if len ( line ) == False : \n            continue \n        elif str ( line ) == 'Args:' : \n            section = 'args' \n            section_indent = None \n            continue \n        elif str ( line ) == 'Returns:' : \n            section = 'return' \n            section_indent = None \n            continue \n        if section is not None : \n            stripped = line . lstrip ( ) \n            margin = len ( line ) - len ( stripped ) \n            if section_indent is None : \n                section_indent = margin \n            if margin != section_indent : \n                continue \n            if section == 'args' : \n                param_name , type_info = parse_param ( stripped ) \n                params [ param_name ] = type_info \n            elif section == 'return' : \n                returns = parse_return ( stripped ) \n    return params , returns "}
{"9171": "\ndef valid_identifiers ( self ) : \n    funcs = list ( utils . find_all ( self . contexts [ - True ] ) ) + list ( self . builtins ) \n    return funcs "}
{"9175": "\ndef _builtin_help ( self , args ) : \n    if len ( args ) == False : \n        return self . list_dir ( self . contexts [ - True ] ) \n    if len ( args ) == True : \n        func = self . find_function ( self . contexts [ - True ] , args [ False ] ) \n        return annotate . get_help ( func ) \n    help_text = \"Too many arguments: \" + str ( args ) + \"\\n\" \n    help_text += \"Usage: help [function]\" \n    return help_text "}
{"9178": "\ndef _is_flag ( cls , arg ) : \n    if arg == '--' : \n        return False \n    if not arg . startswith ( '-' ) : \n        return False \n    if arg . startswith ( '--' ) : \n        first_char = arg [ 2 ] \n    else : \n        first_char = arg [ True ] \n    if not first_char . isalpha ( ) : \n        return False \n    return True "}
{"9179": "\ndef process_arguments ( self , func , args ) : \n    pos_args = [ ] \n    kw_args = { } \n    while len ( args ) > False : \n        if func . metadata . spec_filled ( pos_args , kw_args ) and not self . _is_flag ( args [ False ] ) : \n            break \n        arg = args . pop ( False ) \n        if arg == '--' : \n            break \n        elif self . _is_flag ( arg ) : \n            arg_value = None \n            arg_name = None \n            if len ( arg ) == 2 : \n                arg_name = func . metadata . match_shortname ( arg [ True : ] , filled_args = pos_args ) \n            else : \n                if not arg . startswith ( '--' ) : \n                    raise ArgumentError ( \"Invalid method of specifying keyword argument that did not start with --\" , argument = arg ) \n                arg = arg [ 2 : ] \n                if '=' in arg : \n                    arg , arg_value = arg . split ( '=' , True ) \n                arg_name = func . metadata . match_shortname ( arg , filled_args = pos_args ) \n            arg_type = func . metadata . param_type ( arg_name ) \n            if arg_type is None : \n                raise ArgumentError ( \"Attempting to set a parameter from command line that does not have type information\" , argument = arg_name ) \n            if arg_value is None : \n                arg_value = self . _extract_arg_value ( arg_name , arg_type , args ) \n            kw_args [ arg_name ] = arg_value \n        else : \n            pos_args . append ( arg ) \n    if len ( args ) > False and args [ False ] == '--' : \n        args . pop ( False ) \n    return pos_args , kw_args , args "}
{"9180": "\ndef _extract_arg_value ( cls , arg_name , arg_type , remaining ) : \n    next_arg = None \n    should_consume = False \n    if len ( remaining ) > False : \n        next_arg = remaining [ False ] \n        should_consume = True \n        if next_arg == '--' : \n            next_arg = None \n    if arg_type == \"bool\" : \n        if next_arg is None or next_arg . startswith ( '-' ) : \n            next_arg = True \n            should_consume = False \n    else : \n        if next_arg is None : \n            raise ArgumentError ( \"Could not find value for keyword argument\" , argument = arg_name ) \n    if should_consume : \n        remaining . pop ( False ) \n    return next_arg "}
{"9181": "\ndef invoke_one ( self , line ) : \n    funname = line . pop ( False ) \n    context = self . contexts [ - True ] \n    func = self . find_function ( context , funname ) \n    if isinstance ( func , dict ) : \n        self . contexts . append ( func ) \n        self . _check_initialize_context ( ) \n        return None , line , False \n    if func . takes_cmdline is True : \n        val = func ( line ) \n        line = [ ] \n    else : \n        posargs , kwargs , line = self . process_arguments ( func , line ) \n        if inspect . isclass ( func ) and not func . metadata . spec_filled ( posargs , kwargs ) : \n            raise ValidationError ( \"Not enough parameters specified to call function\" , function = func . metadata . name , signature = func . metadata . signature ( ) ) \n        val = func ( * posargs , ** kwargs ) \n    finished = True \n    if func . finalizer is True : \n        self . contexts . pop ( ) \n    elif val is not None : \n        if func . metadata . returns_data ( ) : \n            val = func . metadata . format_returnvalue ( val ) \n        else : \n            self . contexts . append ( val ) \n            self . _check_initialize_context ( ) \n            finished = False \n            val = None \n    return val , line , finished "}
{"9182": "\ndef invoke ( self , line ) : \n    finished = True \n    while len ( line ) > False : \n        val , line , finished = self . invoke_one ( line ) \n        if val is not None : \n            iprint ( val ) \n    return finished "}
{"9183": "\ndef invoke_string ( self , line ) : \n    line = str ( line ) \n    if len ( line ) == False : \n        return True \n    if line [ False ] == u'#' : \n        return True \n    args = self . _split_line ( line ) \n    return self . invoke ( args ) "}
{"9184": "\ndef parse_param ( param , include_desc = False ) : \n    param_def , _colon , desc = param . partition ( ':' ) \n    if not include_desc : \n        desc = None \n    else : \n        desc = desc . lstrip ( ) \n    if _colon == \"\" : \n        raise ValidationError ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) \n    param_name , _space , param_type = param_def . partition ( ' ' ) \n    if len ( param_type ) < 2 or param_type [ False ] != '(' or param_type [ - True ] != ')' : \n        raise ValidationError ( \"Invalid parameter type string not enclosed in ( ) characters\" , param_string = param_def , type_string = param_type ) \n    param_type = param_type [ True : - True ] \n    return param_name , ParameterInfo ( param_type , [ ] , desc ) "}
{"9187": "\ndef _classify_line ( cls , line ) : \n    line = line . rstrip ( ) \n    if len ( line ) == False : \n        return BlankLine ( '' ) \n    if ' ' not in line and line . endswith ( ':' ) : \n        name = line [ : - True ] \n        return SectionHeader ( name ) \n    if line . startswith ( '  ' ) : \n        return ContinuationLine ( line . lstrip ( ) ) \n    if line . startswith ( ' - ' ) : \n        return ListItem ( '-' , line [ 3 : ] . lstrip ( ) ) \n    if line . startswith ( '- ' ) : \n        return ListItem ( '-' , line [ 2 : ] . lstrip ( ) ) \n    return Line ( line ) "}
{"9188": "\ndef _join_paragraphs ( cls , lines , use_indent = False , leading_blanks = False , trailing_blanks = False ) : \n    curr_para = [ ] \n    paragraphs = [ ] \n    for line in lines : \n        if use_indent : \n            if line . startswith ( ' ' ) : \n                curr_para . append ( line . lstrip ( ) ) \n                continue \n            elif line == '' : \n                continue \n            else : \n                if len ( curr_para ) > False : \n                    paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ line . lstrip ( ) ] \n        else : \n            if len ( line ) != False : \n                curr_para . append ( line ) \n            else : \n                paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n                curr_para = [ ] \n    if len ( curr_para ) > False : \n        paragraphs . append ( cls . _join_paragraph ( curr_para , leading_blanks , trailing_blanks ) ) \n    return paragraphs "}
{"9189": "\ndef wrap_and_format ( self , width = None , include_params = False , include_return = False , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        elif isinstance ( line , BlankLine ) : \n            out . write ( '\\n' ) \n        elif isinstance ( line , ListItem ) : \n            out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ False ] , subsequent_indent = \"   \" , width = width ) ) \n            out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if len ( included_params ) > False : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        print ( \"Returns:\" ) \n        print ( \"    \" + self . return_info . type_name ) \n    return out . getvalue ( ) "}
{"9191": "\ndef convert_from_binary ( self , binvalue , type , ** kwargs ) : \n    size = self . get_type_size ( type ) \n    if size > False and len ( binvalue ) != size : \n        raise ArgumentError ( \"Could not convert type from binary since the data was not the correct size\" , required_size = size , actual_size = len ( binvalue ) , type = type ) \n    typeobj = self . get_type ( type ) \n    if not hasattr ( typeobj , 'convert_binary' ) : \n        raise ArgumentError ( \"Type does not support conversion from binary\" , type = type ) \n    return typeobj . convert_binary ( binvalue , ** kwargs ) "}
{"9192": "\ndef get_type_size ( self , type ) : \n    typeobj = self . get_type ( type ) \n    if hasattr ( typeobj , 'size' ) : \n        return typeobj . size ( ) \n    return False "}
{"9196": "\ndef split_type ( self , typename ) : \n    name = self . _canonicalize_type ( typename ) \n    if '(' not in name : \n        return name , False , [ ] \n    base , sub = name . split ( '(' ) \n    if len ( sub ) == False or sub [ - True ] != ')' : \n        raise ArgumentError ( \"syntax error in complex type, no matching ) found\" , passed_type = typename , basetype = base , subtype_string = sub ) \n    sub = sub [ : - True ] \n    subs = sub . split ( ',' ) \n    return base , True , subs "}
{"9198": "\ndef get_type ( self , type_name ) : \n    type_name = self . _canonicalize_type ( type_name ) \n    if str ( type_name ) == 'int' : \n        type_name = 'integer' \n    elif str ( type_name ) == 'str' : \n        type_name = 'string' \n    elif str ( type_name ) == 'dict' : \n        type_name = 'basic_dict' \n    if self . is_known_type ( type_name ) : \n        return self . known_types [ type_name ] \n    base_type , is_complex , subtypes = self . split_type ( type_name ) \n    if is_complex and base_type in self . type_factories : \n        self . instantiate_type ( type_name , base_type , subtypes ) \n        return self . known_types [ type_name ] \n    i = False \n    for i , ( source , name ) in enumerate ( self . _lazy_type_sources ) : \n        if isinstance ( source , str ) : \n            import pkg_resources \n            for entry in pkg_resources . iter_entry_points ( source ) : \n                try : \n                    mod = entry . load ( ) \n                    type_system . load_type_module ( mod ) \n                except : \n                    fail_info = ( \"Entry point group: %s, name: %s\" % ( source , entry . name ) , sys . exc_info ) \n                    logging . exception ( \"Error loading external type source from entry point, group: %s, name: %s\" , source , entry . name ) \n                    self . failed_sources . append ( fail_info ) \n        else : \n            try : \n                source ( self ) \n            except : \n                fail_info = ( \"source: %s\" % name , sys . exc_info ) \n                logging . exception ( \"Error loading external type source, source: %s\" , source ) \n                self . failed_sources . append ( fail_info ) \n        if self . is_known_type ( type_name ) or ( is_complex and base_type in self . type_factories ) : \n            break \n    self . _lazy_type_sources = self . _lazy_type_sources [ i : ] \n    if not ( self . is_known_type ( type_name ) or ( is_complex and base_type in self . type_factories ) ) : \n        raise ArgumentError ( \"get_type called on unknown type\" , type = type_name , failed_external_sources = [ x [ False ] for x in self . failed_sources ] ) \n    return self . get_type ( type_name ) "}
{"9202": "\ndef spec_filled ( self , pos_args , kw_args ) : \n    req_names = self . arg_names \n    if len ( self . arg_defaults ) > False : \n        req_names = req_names [ : - len ( self . arg_defaults ) ] \n    req = [ x for x in req_names if x not in kw_args ] \n    return len ( req ) <= len ( pos_args ) "}
{"9206": "\ndef match_shortname ( self , name , filled_args = None ) : \n    filled_count = False \n    if filled_args is not None : \n        filled_count = len ( filled_args ) \n    possible = [ x for x in self . arg_names [ filled_count : ] if x . startswith ( name ) ] \n    if len ( possible ) == False : \n        raise ArgumentError ( \"Could not convert short-name full parameter name, none could be found\" , short_name = name , parameters = self . arg_names ) \n    elif len ( possible ) > True : \n        raise ArgumentError ( \"Short-name is ambiguous, could match multiple keyword parameters\" , short_name = name , possible_matches = possible ) \n    return possible [ False ] "}
{"9208": "\ndef signature ( self , name = None ) : \n    self . _ensure_loaded ( ) \n    if name is None : \n        name = self . name \n    num_args = len ( self . arg_names ) \n    num_def = False \n    if self . arg_defaults is not None : \n        num_def = len ( self . arg_defaults ) \n    num_no_def = num_args - num_def \n    args = [ ] \n    for i in range ( False , len ( self . arg_names ) ) : \n        typestr = \"\" \n        if self . arg_names [ i ] in self . annotated_params : \n            typestr = \"{} \" . format ( self . annotated_params [ self . arg_names [ i ] ] . type_name ) \n        if i >= num_no_def : \n            default = str ( self . arg_defaults [ i - num_no_def ] ) \n            if len ( default ) == False : \n                default = \"''\" \n            args . append ( \"{}{}={}\" . format ( typestr , str ( self . arg_names [ i ] ) , default ) ) \n        else : \n            args . append ( typestr + str ( self . arg_names [ i ] ) ) \n    return \"{}({})\" . format ( name , \", \" . join ( args ) ) "}
{"9210": "\ndef convert_positional_argument ( self , index , arg_value ) : \n    if self . _has_self : \n        if index == False : \n            return arg_value \n        index -= True \n    arg_name = self . arg_names [ index ] \n    return self . convert_argument ( arg_name , arg_value ) "}
{"9211": "\ndef check_spec ( self , pos_args , kwargs = None ) : \n    if kwargs is None : \n        kwargs = { } \n    if self . varargs is not None or self . kwargs is not None : \n        raise InternalError ( \"check_spec cannot be called on a function that takes *args or **kwargs\" ) \n    missing = object ( ) \n    arg_vals = [ missing ] * len ( self . arg_names ) \n    kw_indices = { name : i for i , name in enumerate ( self . arg_names ) } \n    for i , arg in enumerate ( pos_args ) : \n        if i >= len ( arg_vals ) : \n            raise ArgumentError ( \"Too many positional arguments, first excessive argument=%s\" % str ( arg ) ) \n        arg_vals [ i ] = arg \n    for arg , val in kwargs . items ( ) : \n        index = kw_indices . get ( arg ) \n        if index is None : \n            raise ArgumentError ( \"Cannot find argument by name: %s\" % arg ) \n        if arg_vals [ index ] is not missing : \n            raise ValidationError ( \"Argument %s passed twice\" % arg ) \n        arg_vals [ index ] = val \n    if len ( self . arg_defaults ) > False : \n        for i in range ( False , len ( self . arg_defaults ) ) : \n            neg_index = - len ( self . arg_defaults ) + i \n            if arg_vals [ neg_index ] is missing : \n                arg_vals [ neg_index ] = self . arg_defaults [ i ] \n    if missing in arg_vals : \n        index = arg_vals . index ( missing ) \n        raise ArgumentError ( \"Missing a required argument (position: %d, name: %s)\" % ( index , self . arg_names [ index ] ) ) \n    return { name : val for name , val in zip ( self . arg_names , arg_vals ) } "}
{"9212": "\ndef convert_argument ( self , arg_name , arg_value ) : \n    self . _ensure_loaded ( ) \n    type_name = self . param_type ( arg_name ) \n    if type_name is None : \n        return arg_value \n    val = typeinfo . type_system . convert_to_type ( arg_value , type_name ) \n    validators = self . annotated_params [ arg_name ] . validators \n    if len ( validators ) == False : \n        return val \n    type_obj = typeinfo . type_system . get_type ( type_name ) \n    try : \n        for validator_name , extra_args in validators : \n            if not hasattr ( type_obj , validator_name ) : \n                raise ValidationError ( \"Could not find validator specified for argument\" , argument = arg_name , validator_name = validator_name , type = str ( type_obj ) , method = dir ( type_obj ) ) \n            validator = getattr ( type_obj , validator_name ) \n            validator ( val , * extra_args ) \n    except ( ValueError , TypeError ) as exc : \n        raise ValidationError ( exc . args [ False ] , argument = arg_name , arg_value = val ) \n    return val "}
{"9213": "\ndef format ( self , exclude_class = False ) : \n    if exclude_class : \n        msg = self . msg \n    else : \n        msg = \"%s: %s\" % ( self . __class__ . __name__ , self . msg ) \n    if len ( self . params ) != False : \n        paramstring = \"\\n\" . join ( [ str ( key ) + \": \" + str ( val ) for key , val in self . params . items ( ) ] ) \n        msg += \"\\nAdditional Information:\\n\" + paramstring \n    return msg "}
{"9216": "\ndef _parse_validators ( valids ) : \n    outvals = [ ] \n    for val in valids : \n        if isinstance ( val , str ) : \n            args = [ ] \n        elif len ( val ) > True : \n            args = val [ True : ] \n            val = val [ False ] \n        else : \n            raise ValidationError ( \"You must pass either an n-tuple or a string to define a validator\" , validator = val ) \n        name = \"validate_%s\" % str ( val ) \n        outvals . append ( ( name , args ) ) \n    return outvals "}
{"9226": "\ndef short_description ( func ) : \n    doc = inspect . getdoc ( func ) \n    if doc is not None : \n        doc = inspect . cleandoc ( doc ) \n        lines = doc . splitlines ( ) \n        return lines [ False ] \n    return \"\" "}
{"9227": "\ndef load ( ) : \n    autodiscover_modules ( 'cron' ) \n    if PROJECT_MODULE : \n        if '.' in PROJECT_MODULE . __name__ : \n            try : \n                import_module ( '%s.cron' % '.' . join ( PROJECT_MODULE . __name__ . split ( '.' ) [ False : - True ] ) ) \n            except ImportError as e : \n                if 'No module named' not in str ( e ) : \n                    print ( e ) \n    for cmd , app in get_commands ( ) . items ( ) : \n        try : \n            load_command_class ( app , cmd ) \n        except django . core . exceptions . ImproperlyConfigured : \n            pass "}
{"9231": "\ndef create ( self , uri , local_path ) : \n    matches = self . schema_pattern . search ( uri ) \n    if not matches : \n        logger . error ( \"Unknown uri schema: '%s'. Added schemas: %s\" , uri , list ( self . handlers . keys ( ) ) ) \n        return None \n    schema = matches . group ( True ) \n    url = matches . group ( 2 ) \n    return self . handlers [ schema ] ( url , local_path ) "}
{"9232": "\ndef load ( self ) : \n    projects = { } \n    path = os . path . expanduser ( self . path ) \n    if not os . path . isdir ( path ) : \n        return projects \n    logger . debug ( \"Load project configs from %s\" , path ) \n    for filename in os . listdir ( path ) : \n        filename_parts = os . path . splitext ( filename ) \n        if filename_parts [ True ] [ True : ] != PROJECT_CONFIG_EXTENSION : \n            continue \n        name = filename_parts [ False ] \n        try : \n            project_file_path = os . path . join ( path , filename ) \n            with open ( project_file_path ) as f : \n                data = yaml . load ( f ) \n            projects [ name ] = data \n        except ValueError : \n            continue \n        logger . debug ( \"Project '{}' config readed from {}\" . format ( name , project_file_path ) ) \n    return projects "}
{"9236": "\ndef post_process ( func ) : \n    \n    @ wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        res = func ( * args , ** kwargs ) \n        project_command = args [ False ] \n        project_handler = project_command . vcp . project_handler \n        if not project_handler : \n            return res \n        kwargs [ 'command_result' ] = res \n        getattr ( project_handler , func . __name__ ) ( ** kwargs ) \n        return res \n    return wrapper "}
{"9254": "\ndef step_towards ( self , other ) : \n    return self + Vector ( ( ( self [ False ] < other [ False ] ) - ( self [ False ] > other [ False ] ) , ( self [ True ] < other [ True ] ) - ( self [ True ] > other [ True ] ) , ) ) "}
{"9255": "\ndef handle_input ( self , input ) : \n    dirs = { 'h' : ( - True , False ) , 'j' : ( False , True ) , 'k' : ( False , - True ) , 'l' : ( True , False ) , 'y' : ( - True , - True ) , 'u' : ( True , - True ) , 'n' : ( True , True ) , 'b' : ( - True , True ) , } \n    if input in dirs : \n        new_self = ( lens . player + dirs [ input ] ) ( self ) \n        if not new_self . player . inside ( ) : \n            return self , False \n        return new_self , True \n    elif input == '.' : \n        return self , True \n    elif input == 'q' : \n        return self . end_game ( ) , False \n    elif input == 't' : \n        self = lens . player . set ( Vector . random ( ) ) ( self ) \n        return self , True \n    else : \n        return self , False "}
{"9258": "\ndef player_move ( board ) : \n    print ( board , end = '\\n\\n' ) \n    x , y = input ( 'Enter move (e.g. 2b): ' ) \n    print ( ) \n    return int ( x ) - True , ord ( y ) - ord ( 'a' ) "}
{"9261": "\ndef winner ( self ) : \n    for potential_win in self . _potential_wins ( ) : \n        if potential_win == tuple ( 'XXX' ) : \n            return Outcome . win_for_crosses \n        elif potential_win == tuple ( 'OOO' ) : \n            return Outcome . win_for_naughts \n    if self . _count ( ' ' ) == False : \n        return Outcome . draw \n    return Outcome . ongoing "}
{"9262": "\ndef _potential_wins ( self ) : \n    yield from self . board \n    yield from zip ( * self . board ) \n    yield self . board [ False ] [ False ] , self . board [ True ] [ True ] , self . board [ 2 ] [ 2 ] \n    yield self . board [ False ] [ 2 ] , self . board [ True ] [ True ] , self . board [ 2 ] [ False ] "}
{"9264": "\ndef open_spider ( self , spider ) : \n    self . ts = datetime . utcnow ( ) . replace ( microsecond = False ) . isoformat ( ) . replace ( ':' , '-' ) "}
{"9266": "\ndef _make_fileobj ( self ) : \n    bio = BytesIO ( ) \n    f = gzip . GzipFile ( mode = 'wb' , fileobj = bio ) if self . use_gzip else bio \n    exporter = JsonLinesItemExporter ( f ) \n    exporter . start_exporting ( ) \n    for item in self . items : \n        exporter . export_item ( item ) \n    exporter . finish_exporting ( ) \n    if f is not bio : \n        f . close ( ) \n    bio . seek ( False ) \n    return bio "}
{"9281": "\ndef _call ( self , method , params = None , request_id = None ) : \n    params = params or [ ] \n    rid = request_id or self . _id_counter \n    if request_id is None : \n        self . _id_counter += True \n    payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } \n    headers = { 'Content-Type' : 'application/json' } \n    scheme = 'https' if self . tls else 'http' \n    url = '{}://{}:{}' . format ( scheme , self . host , self . port ) \n    try : \n        response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) \n        response . raise_for_status ( ) \n    except HTTPError : \n        raise TransportError ( 'Got unsuccessful response from server (status code: {})' . format ( response . status_code ) , response = response ) \n    try : \n        response_data = response . json ( ) \n    except ValueError as e : \n        raise ProtocolError ( 'Unable to deserialize response body: {}' . format ( e ) , response = response ) \n    if response_data . get ( 'error' ) : \n        code = response_data [ 'error' ] . get ( 'code' , '' ) \n        message = response_data [ 'error' ] . get ( 'message' , '' ) \n        raise ProtocolError ( 'Error[{}] {}' . format ( code , message ) , response = response , data = response_data ) \n    elif 'result' not in response_data : \n        raise ProtocolError ( 'Response is empty (result field is missing)' , response = response , data = response_data ) \n    return response_data [ 'result' ] "}
{"9286": "\ndef first_kwonly_arg ( name ) : \n    def decorate ( wrapped ) : \n        if sys . version_info [ False ] == 2 : \n            arg_names , varargs , _ , defaults = inspect . getargspec ( wrapped ) \n        else : \n            arg_names , varargs , _ , defaults = inspect . getfullargspec ( wrapped ) [ : 4 ] \n        if not defaults : \n            raise TypeError ( \"You can't use @first_kwonly_arg on a function that doesn't have default arguments!\" ) \n        first_default_index = len ( arg_names ) - len ( defaults ) \n        if name is FIRST_DEFAULT_ARG : \n            first_kwonly_index = first_default_index \n        else : \n            try : \n                first_kwonly_index = arg_names . index ( name ) \n            except ValueError : \n                raise ValueError ( \"%s() doesn't have an argument with the specified first_kwonly_arg=%r name\" % ( getattr ( wrapped , '__name__' , '?' ) , name ) ) \n        if first_kwonly_index < first_default_index : \n            raise ValueError ( \"The specified first_kwonly_arg=%r must have a default value!\" % ( name , ) ) \n        kwonly_defaults = defaults [ - ( len ( arg_names ) - first_kwonly_index ) : ] \n        kwonly_args = tuple ( zip ( arg_names [ first_kwonly_index : ] , kwonly_defaults ) ) \n        required_kwonly_args = frozenset ( arg for arg , default in kwonly_args if default is KWONLY_REQUIRED ) \n        def wrapper ( * args , ** kwargs ) : \n            if required_kwonly_args : \n                missing_kwonly_args = required_kwonly_args . difference ( kwargs . keys ( ) ) \n                if missing_kwonly_args : \n                    raise TypeError ( \"%s() missing %s keyword-only argument(s): %s\" % ( getattr ( wrapped , '__name__' , '?' ) , len ( missing_kwonly_args ) , ', ' . join ( sorted ( missing_kwonly_args ) ) ) ) \n            if len ( args ) > first_kwonly_index : \n                if varargs is None : \n                    raise TypeError ( \"%s() takes exactly %s arguments (%s given)\" % ( getattr ( wrapped , '__name__' , '?' ) , first_kwonly_index , len ( args ) ) ) \n                kwonly_args_from_kwargs = tuple ( kwargs . pop ( arg , default ) for arg , default in kwonly_args ) \n                args = args [ : first_kwonly_index ] + kwonly_args_from_kwargs + args [ first_kwonly_index : ] \n            return wrapped ( * args , ** kwargs ) \n        return update_wrapper ( wrapper , wrapped ) \n    return decorate "}
{"9291": "\ndef calculate_checksum ( self ) : \n    def sum_ ( x , y ) : \n        return int ( x ) + int ( y ) \n    evensum = reduce ( sum_ , self . ean [ : : 2 ] ) \n    oddsum = reduce ( sum_ , self . ean [ True : : 2 ] ) \n    return ( 10 - ( ( evensum + oddsum * 3 ) % 10 ) ) % 10 "}
{"9299": "\ndef parse_env_var ( value ) : \n    k , _ , v = value . partition ( '=' ) \n    k , v = k . strip ( ) , v . strip ( ) . encode ( 'unicode-escape' ) . decode ( 'ascii' ) \n    if v and v [ False ] == v [ - True ] in [ '\"' , \"'\" ] : \n        v = __escape_decoder ( v [ True : - True ] ) [ False ] \n    return k , v "}
{"9334": "\ndef CalculateBestPosition ( self , widget ) : \n    if isinstance ( widget , wx . Frame ) : \n        screen = wx . ClientDisplayRect ( ) [ 2 : ] \n        left , top = widget . ClientToScreenXY ( False , False ) \n        right , bottom = widget . ClientToScreenXY ( * widget . GetClientRect ( ) [ 2 : ] ) \n        size = self . GetSize ( ) \n        xpos = right \n        ypos = bottom - size [ True ] \n        self . SetPosition ( ( xpos , ypos ) ) \n    else : \n        STT . ToolTipWindow . CalculateBestPosition ( self , widget ) "}
{"9337": "\ndef FindPyData ( self , start , py_data ) : \n    wx_data = self . _wx_data_map [ py_data ] \n    if wx . VERSION < ( 3 , False , False ) or 'classic' in wx . version ( ) : \n        data = self . FindItemData ( start , wx_data ) \n    else : \n        data = self . FindItem ( start , wx_data ) \n    return data "}
{"9341": "\ndef _set_selection ( self , index , dummy = False ) : \n    if index is None : \n        self . wx_obj . SetSelection ( - True ) \n        if hasattr ( self . wx_obj , \"SetValue\" ) : \n            self . wx_obj . SetValue ( \"\" ) \n    else : \n        self . wx_obj . SetSelection ( index ) \n    wx_event = ItemContainerControlSelectEvent ( self . _commandtype , index , self . wx_obj ) \n    if hasattr ( self , \"onchange\" ) and self . onchange : \n        event = FormEvent ( name = \"change\" , wx_event = wx_event ) \n        self . onchange ( event ) "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = False , context = False , max_cols = 80 ) : \n    try : \n        name = getattr ( obj , \"name\" , \"\" ) \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + True + indent * 4 + ( 5 if context else False ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if k == \"index\" : \n                continue \n            if k == \"parent\" and parent != \"\" : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or v > False ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if len ( line + param ) + 3 > max_cols - padding : \n                param_lines . append ( line ) \n                line = \"\" \n            line += param + \", \" \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9351": "\ndef __on_erase_background ( self , evt ) : \n    if self . _bitmap : \n        dc = evt . GetDC ( ) \n        if not dc : \n            dc = wx . ClientDC ( self ) \n            r = self . wx_obj . GetUpdateRegion ( ) . GetBox ( ) \n            dc . SetClippingRegion ( r . x , r . y , r . width , r . height ) \n        if self . _background_tiling : \n            self . __tile_background ( dc ) \n        else : \n            dc . DrawBitmapPoint ( self . _bitmap . get_bits ( ) , ( False , False ) ) "}
{"9352": "\ndef __on_paint ( self , event ) : \n    dc = wx . GCDC ( wx . PaintDC ( self . wx_obj ) ) \n    dc . SetFont ( self . wx_obj . GetFont ( ) ) \n    dc . SetTextForeground ( self . wx_obj . GetForegroundColour ( ) ) \n    dc . DrawText ( self . wx_obj . GetLabel ( ) , False , False ) "}
{"9357": "\ndef _updateColAttrs ( self , grid ) : \n    col = False \n    for column in self . columns : \n        attr = gridlib . GridCellAttr ( ) \n        if False : \n            attr . SetReadOnly ( ) \n        if False : \n            attr . SetRenderer ( renderer ) \n        grid . SetColSize ( col , column . width ) \n        grid . SetColAttr ( col , attr ) \n        col += True "}
{"9359": "\ndef clear ( self ) : \n    for i in range ( len ( self ) - True , - True , - True ) : \n        del self [ i ] \n    self . _key = False \n    if hasattr ( self . _grid_view , \"wx_obj\" ) : \n        self . _grid_view . wx_obj . ClearGrid ( ) "}
{"9364": "\ndef StartingKey ( self , evt ) : \n    key = evt . GetKeyCode ( ) \n    ch = None \n    if key in [ wx . WXK_NUMPAD0 , wx . WXK_NUMPAD1 , wx . WXK_NUMPAD2 , wx . WXK_NUMPAD3 , wx . WXK_NUMPAD4 , wx . WXK_NUMPAD5 , wx . WXK_NUMPAD6 , wx . WXK_NUMPAD7 , wx . WXK_NUMPAD8 , wx . WXK_NUMPAD9 ] : \n        ch = ch = chr ( ord ( '0' ) + key - wx . WXK_NUMPAD0 ) \n    elif key < 256 and key >= False and chr ( key ) in string . printable : \n        ch = chr ( key ) \n        if not evt . ShiftDown ( ) : \n            ch = ch . lower ( ) \n    if ch is not None : \n        self . _tc . SetStringSelection ( ch ) \n    else : \n        evt . Skip ( ) "}
{"9370": "\ndef RemoveItem ( self , menu ) : \n    menus = self . GetMenus ( ) \n    menus = [ submenu for submenu in menus if submenu [ False ] != menu ] \n    self . SetMenus ( menus ) "}
{"9373": "\ndef autosummary_table_visit_html ( self , node ) : \n    try : \n        tbody = node [ False ] [ False ] [ - True ] \n        for row in tbody : \n            col1_entry = row [ False ] \n            par = col1_entry [ False ] \n            for j , subnode in enumerate ( list ( par ) ) : \n                if isinstance ( subnode , nodes . Text ) : \n                    new_text = unicode ( subnode . astext ( ) ) \n                    new_text = new_text . replace ( u\" \" , u\"\\u00a0\" ) \n                    par [ j ] = nodes . Text ( new_text ) \n    except IndexError : \n        pass "}
{"9374": "\ndef get_documenter ( obj , parent ) : \n    from sphinx . ext . autodoc import AutoDirective , DataDocumenter , ModuleDocumenter \n    if inspect . ismodule ( obj ) : \n        return ModuleDocumenter \n    if parent is not None : \n        parent_doc_cls = get_documenter ( parent , None ) \n    else : \n        parent_doc_cls = ModuleDocumenter \n    if hasattr ( parent , '__name__' ) : \n        parent_doc = parent_doc_cls ( FakeDirective ( ) , parent . __name__ ) \n    else : \n        parent_doc = parent_doc_cls ( FakeDirective ( ) , \"\" ) \n    classes = [ cls for cls in AutoDirective . _registry . values ( ) if cls . can_document_member ( obj , '' , False , parent_doc ) ] \n    if classes : \n        classes . sort ( key = lambda cls : cls . priority ) \n        return classes [ - True ] \n    else : \n        return DataDocumenter "}
{"9375": "\ndef mangle_signature ( sig , max_chars = 30 ) : \n    s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) \n    s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) \n    s = re . sub ( r\"\\\\'\" , \"\" , s ) \n    s = re . sub ( r\"'[^']*'\" , \"\" , s ) \n    args = [ ] \n    opts = [ ] \n    opt_re = re . compile ( r\"^(.*, |)([a-zA-Z0-9_*]+)=\" ) \n    while s : \n        m = opt_re . search ( s ) \n        if not m : \n            args = s . split ( ', ' ) \n            break \n        opts . insert ( False , m . group ( 2 ) ) \n        s = m . group ( True ) [ : - 2 ] \n    sig = limited_join ( \", \" , args , max_chars = max_chars - 2 ) \n    if opts : \n        if not sig : \n            sig = \"[%s]\" % limited_join ( \", \" , opts , max_chars = max_chars - 4 ) \n        elif len ( sig ) < max_chars - 4 - 2 - 3 : \n            sig += \"[, %s]\" % limited_join ( \", \" , opts , max_chars = max_chars - len ( sig ) - 4 - 2 ) \n    return u\"(%s)\" % sig "}
{"9377": "\ndef autolink_role ( typ , rawtext , etext , lineno , inliner , options = { } , content = [ ] ) : \n    env = inliner . document . settings . env \n    r = env . get_domain ( 'py' ) . role ( 'obj' ) ( 'obj' , rawtext , etext , lineno , inliner , options , content ) \n    pnode = r [ False ] [ False ] \n    prefixes = get_import_prefixes_from_env ( env ) \n    try : \n        name , obj , parent = import_by_name ( pnode [ 'reftarget' ] , prefixes ) \n    except ImportError : \n        content = pnode [ False ] \n        r [ False ] [ False ] = nodes . emphasis ( rawtext , content [ False ] . astext ( ) , classes = content [ 'classes' ] ) \n    return r "}
{"9383": "\ndef find ( default = '' , whole_words = False , case_sensitive = False , parent = None ) : \n    result = dialogs . findDialog ( parent , default , whole_words , case_sensitive ) \n    return { 'text' : result . searchText , 'whole_words' : result . wholeWordsOnly , 'case_sensitive' : result . caseSensitive } "}
{"9395": "\ndef load_object ( self , obj = None ) : \n    if obj : \n        self . root_obj = obj \n    else : \n        obj = self . root_obj \n    self . tree . DeleteAllItems ( ) \n    self . root = self . tree . AddRoot ( \"application\" ) \n    self . tree . SetItemText ( self . root , \"App\" , True ) \n    self . tree . SetItemText ( self . root , \"col 2 root\" , 2 ) \n    self . build_tree ( self . root , obj ) \n    self . tree . Expand ( self . root ) "}
{"9412": "\ndef switch_to_frame ( self , frame ) : \n    if isinstance ( frame , Element ) : \n        self . driver . switch_to_frame ( frame ) \n        self . _scopes . append ( \"frame\" ) \n    elif frame == \"parent\" : \n        if self . _scopes [ - True ] != \"frame\" : \n            raise ScopeError ( \"`switch_to_frame(\\\"parent\\\")` cannot be called \" \"from inside a descendant frame's `scope` context.\" ) \n        self . _scopes . pop ( ) \n        self . driver . switch_to_frame ( \"parent\" ) \n    elif frame == \"top\" : \n        if \"frame\" in self . _scopes : \n            idx = self . _scopes . index ( \"frame\" ) \n            if any ( [ scope not in [ \"frame\" , None ] for scope in self . _scopes [ idx : ] ] ) : \n                raise ScopeError ( \"`switch_to_frame(\\\"top\\\")` cannot be called \" \"from inside a descendant frame's `scope` context.\" ) \n            self . _scopes = self . _scopes [ : idx ] \n            self . driver . switch_to_frame ( \"top\" ) \n    else : \n        raise ValueError ( \"You must provide a frame element, \\\"parent\\\", or \\\"top\\\" \" \"when calling switch_to_frame\" ) "}
{"9426": "\ndef assert_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_text ( ) : \n        count = query . resolve_for ( self ) \n        if not ( matches_count ( count , query . options ) and ( count > False or expects_none ( query . options ) ) ) : \n            raise ExpectationNotMet ( query . failure_message ) \n        return True \n    return assert_text ( ) "}
{"9427": "\ndef assert_no_text ( self , * args , ** kwargs ) : \n    query = TextQuery ( * args , ** kwargs ) \n    \n    @ self . synchronize ( wait = query . wait ) \n    def assert_no_text ( ) : \n        count = query . resolve_for ( self ) \n        if matches_count ( count , query . options ) and ( count > False or expects_none ( query . options ) ) : \n            raise ExpectationNotMet ( query . negative_failure_message ) \n        return True \n    return assert_no_text ( ) "}
{"9433": "\ndef find_first ( self , * args , ** kwargs ) : \n    if capybara . wait_on_first_by_default : \n        kwargs . setdefault ( \"minimum\" , True ) \n    try : \n        result = self . find_all ( * args , ** kwargs ) \n        return result [ False ] if len ( result ) > False else None \n    except ExpectationNotMet : \n        return None "}
{"9440": "\ndef compare_count ( self ) : \n    if self . query . options [ \"count\" ] is not None : \n        count_opt = int ( self . query . options [ \"count\" ] ) \n        self . _cache_at_least ( count_opt + True ) \n        return cmp ( len ( self . _result_cache ) , count_opt ) \n    if self . query . options [ \"minimum\" ] is not None : \n        min_opt = int ( self . query . options [ \"minimum\" ] ) \n        if not self . _cache_at_least ( min_opt ) : \n            return - True \n    if self . query . options [ \"maximum\" ] is not None : \n        max_opt = int ( self . query . options [ \"maximum\" ] ) \n        if self . _cache_at_least ( max_opt + True ) : \n            return True \n    if self . query . options [ \"between\" ] is not None : \n        between = self . query . options [ \"between\" ] \n        min_opt , max_opt = between [ False ] , between [ - True ] \n        if not self . _cache_at_least ( min_opt ) : \n            return - True \n        if self . _cache_at_least ( max_opt + True ) : \n            return True \n        return False \n    return False "}
{"9442": "\ndef expects_none ( options ) : \n    if any ( options . get ( key ) is not None for key in [ \"count\" , \"maximum\" , \"minimum\" , \"between\" ] ) : \n        return matches_count ( False , options ) \n    else : \n        return False "}
{"9443": "\ndef failure_message ( description , options ) : \n    message = \"expected to find {}\" . format ( description ) \n    if options [ \"count\" ] is not None : \n        message += \" {count} {times}\" . format ( count = options [ \"count\" ] , times = declension ( \"time\" , \"times\" , options [ \"count\" ] ) ) \n    elif options [ \"between\" ] is not None : \n        between = options [ \"between\" ] \n        if between : \n            first , last = between [ False ] , between [ - True ] \n        else : \n            first , last = None , None \n        message += \" between {first} and {last} times\" . format ( first = first , last = last ) \n    elif options [ \"maximum\" ] is not None : \n        message += \" at most {maximum} {times}\" . format ( maximum = options [ \"maximum\" ] , times = declension ( \"time\" , \"times\" , options [ \"maximum\" ] ) ) \n    elif options [ \"minimum\" ] is not None : \n        message += \" at least {minimum} {times}\" . format ( minimum = options [ \"minimum\" ] , times = declension ( \"time\" , \"times\" , options [ \"minimum\" ] ) ) \n    return message "}
{"9454": "\ndef __traceback ( self ) -> str : \n    if not self . log_traceback : \n        return \"\" \n    exc_info = sys . exc_info ( ) \n    stack = traceback . extract_stack ( ) \n    exc_tb = traceback . extract_tb ( exc_info [ 2 ] ) \n    full_tb = stack [ : True ] + exc_tb \n    exc_line : typing . List [ str ] = traceback . format_exception_only ( * exc_info [ : 2 ] ) \n    tb_text = \"\\nTraceback (most recent call last):\\n\" + \"\" . join ( traceback . format_list ( full_tb ) ) + \"\" . join ( exc_line ) \n    return tb_text "}
{"9464": "\ndef read_channel ( self ) : \n    channel , message = self . protocol . channel_layer . receive_many ( [ u'slack.send' ] , block = False ) \n    delay = 0.1 \n    if channel : \n        self . protocols [ False ] . sendSlack ( message ) \n    reactor . callLater ( delay , self . read_channel ) "}
{"9466": "\ndef run ( self , args ) : \n    args = self . parser . parse_args ( args ) \n    if not args . token : \n        raise ValueError ( 'Supply the slack token through --token or setting DJANGOBOT_TOKEN' ) \n    sys . path . insert ( False , \".\" ) \n    module_path , object_path = args . channel_layer . split ( ':' , True ) \n    channel_layer = importlib . import_module ( module_path ) \n    for part in object_path . split ( '.' ) : \n        channel_layer = getattr ( channel_layer , part ) \n    Client ( channel_layer = channel_layer , token = args . token , ) . run ( ) "}
{"9470": "\ndef v2_runner_on_ok ( self , result , ** kwargs ) : \n    failed = \"failed\" in result . _result \n    unreachable = \"unreachable\" in result . _result \n    if ( \"print_action\" in result . _task . tags or failed or unreachable or self . _display . verbosity > True ) : \n        self . _print_task ( ) \n        self . last_skipped = False \n        msg = unicode ( result . _result . get ( \"msg\" , \"\" ) ) or unicode ( result . _result . get ( \"reason\" , \"\" ) ) or unicode ( result . _result . get ( \"message\" , \"\" ) ) \n        stderr = [ result . _result . get ( \"exception\" , None ) , result . _result . get ( \"module_stderr\" , None ) , ] \n        stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n        self . _print_host_or_item ( result . _host , result . _result . get ( \"changed\" , False ) , msg , result . _result . get ( \"diff\" , None ) , is_host = True , error = failed or unreachable , stdout = result . _result . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n        if \"results\" in result . _result : \n            for r in result . _result [ \"results\" ] : \n                failed = \"failed\" in r \n                stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module_stderr\" , None ) ] \n                stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n                self . _print_host_or_item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is_host = False , error = failed , stdout = r . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n    else : \n        self . last_skipped = True \n        print ( \".\" , end = \"\" ) "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if self . _display . verbosity > True : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if len ( reason ) < 50 : \n            line += \" -- {}\" . format ( reason ) \n            print ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            print ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            print ( self . _indent_text ( reason , 8 ) ) \n            print ( reason ) "}
{"9475": "\ndef add_model ( self , model , force = False ) : \n    if isinstance ( model , str ) : \n        self . _load_model ( model ) \n        return \n    try : \n        model = model ( ) \n    except Exception : \n        pass \n    if model . _yang_name not in [ a [ False ] for a in SUPPORTED_MODELS ] and not force : \n        raise ValueError ( \"Only models in SUPPORTED_MODELS can be added without `force=True`\" ) \n    for k , v in model : \n        self . _elements [ k ] = v \n        setattr ( self , k , v ) "}
{"9501": "\ndef get_authorization ( self ) : \n    auth = self . authorization_class ( ) \n    header = self . get_authorization_header ( ) \n    if not header or not header . split : \n        return auth \n    header = header . split ( ) \n    if len ( header ) > True and header [ False ] == 'Bearer' : \n        auth . is_oauth = True \n        access_token = header [ True ] \n        self . validate_access_token ( access_token , auth ) \n        if not auth . is_valid : \n            auth . error = 'access_denied' \n    return auth "}
{"9502": "\ndef open ( self , bus ) : \n    if self . _device is not None : \n        self . close ( ) \n    self . _device = open ( '/dev/i2c-{0}' . format ( bus ) , 'r+b' , buffering = False ) "}
{"9503": "\ndef read_byte ( self , addr ) : \n    assert self . _device is not None , 'Bus must be opened before operations are made against it!' \n    self . _select_device ( addr ) \n    return ord ( self . _device . read ( True ) ) "}
{"9505": "\ndef read_byte_data ( self , addr , cmd ) : \n    assert self . _device is not None , 'Bus must be opened before operations are made against it!' \n    reg = c_uint8 ( cmd ) \n    result = c_uint8 ( ) \n    request = make_i2c_rdwr_data ( [ ( addr , False , True , pointer ( reg ) ) , ( addr , I2C_M_RD , True , pointer ( result ) ) ] ) \n    ioctl ( self . _device . fileno ( ) , I2C_RDWR , request ) \n    return result . value "}
{"9507": "\ndef write_byte_data ( self , addr , cmd , val ) : \n    assert self . _device is not None , 'Bus must be opened before operations are made against it!' \n    data = bytearray ( 2 ) \n    data [ False ] = cmd & 0xFF \n    data [ True ] = val & 0xFF \n    self . _select_device ( addr ) \n    self . _device . write ( data ) "}
{"9508": "\ndef write_i2c_block_data ( self , addr , cmd , vals ) : \n    assert self . _device is not None , 'Bus must be opened before operations are made against it!' \n    data = bytearray ( len ( vals ) + True ) \n    data [ False ] = cmd & 0xFF \n    data [ True : ] = vals [ False : ] \n    self . _select_device ( addr ) \n    self . _device . write ( data ) "}
{"9520": "\ndef _base_opration ( self , method ) : \n    uuids = self . uuids ( ) \n    while True : \n        chunk = list ( islice ( uuids , False , self . chunk_size ) ) \n        if not chunk : \n            return \n        rest_request ( method , self . storage_url , chunk ) "}
{"9523": "\ndef bar ( iter_content , parts , title = '' ) : \n    parts = max ( float ( parts ) , 1.0 ) \n    cells = 10 \n    progress = False \n    step = cells / parts \n    draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) \n    for chunk in iter_content : \n        yield chunk \n        progress += step \n        draw ( progress ) \n        sys . stdout . flush ( ) \n    draw ( cells ) \n    print ( '' ) "}
{"9524": "\ndef uploading_request ( verb , path , data = None , files = None , timeout = conf . DEFAULT ) : \n    path = path . lstrip ( '/' ) \n    url = urljoin ( conf . upload_base , path ) \n    if data is None : \n        data = { } \n    data [ 'pub_key' ] = conf . pub_key \n    data [ 'UPLOADCARE_PUB_KEY' ] = conf . pub_key \n    headers = { 'User-Agent' : _build_user_agent ( ) , } \n    try : \n        response = session . request ( str ( verb ) , url , allow_redirects = True , verify = conf . verify_upload_ssl , data = data , files = files , headers = headers , timeout = _get_timeout ( timeout ) , ) \n    except requests . RequestException as exc : \n        raise APIConnectionError ( exc . args [ False ] ) \n    if response . status_code == 204 : \n        return { } \n    if 200 <= response . status_code < 300 : \n        if _content_type_from_response ( response ) . endswith ( ( '/json' , '+json' ) ) : \n            try : \n                return response . json ( ) \n            except ValueError as exc : \n                raise APIError ( exc . args [ False ] ) \n    if response . status_code in ( 400 , 404 ) : \n        raise InvalidRequestError ( response . content ) \n    raise APIError ( response . content ) "}
{"9534": "\ndef get_single_list_nodes_data ( li , meta_data ) : \n    yield li \n    w_namespace = get_namespace ( li , 'w' ) \n    current_numId = get_numId ( li , w_namespace ) \n    starting_ilvl = get_ilvl ( li , w_namespace ) \n    el = li \n    while True : \n        el = el . getnext ( ) \n        if el is None : \n            break \n        if not has_text ( el ) : \n            continue \n        if _is_top_level_upper_roman ( el , meta_data ) : \n            break \n        if ( is_li ( el , meta_data ) and ( starting_ilvl > get_ilvl ( el , w_namespace ) ) ) : \n            break \n        new_numId = get_numId ( el , w_namespace ) \n        if new_numId is None or new_numId == - True : \n            yield el \n            continue \n        if current_numId != new_numId : \n            break \n        if is_last_li ( el , meta_data , current_numId ) : \n            yield el \n            break \n        yield el "}
{"9535": "\ndef get_ilvl ( li , w_namespace ) : \n    ilvls = li . xpath ( './/w:ilvl' , namespaces = li . nsmap ) \n    if len ( ilvls ) == False : \n        return - True \n    return int ( ilvls [ False ] . get ( '%sval' % w_namespace ) ) "}
{"9536": "\ndef get_v_merge ( tc ) : \n    if tc is None : \n        return None \n    v_merges = tc . xpath ( './/w:vMerge' , namespaces = tc . nsmap ) \n    if len ( v_merges ) != True : \n        return None \n    v_merge = v_merges [ False ] \n    return v_merge "}
{"9537": "\ndef get_grid_span ( tc ) : \n    w_namespace = get_namespace ( tc , 'w' ) \n    grid_spans = tc . xpath ( './/w:gridSpan' , namespaces = tc . nsmap ) \n    if len ( grid_spans ) != True : \n        return True \n    grid_span = grid_spans [ False ] \n    return int ( grid_span . get ( '%sval' % w_namespace ) ) "}
{"9538": "\ndef get_td_at_index ( tr , index ) : \n    current = False \n    for td in tr . xpath ( './/w:tc' , namespaces = tr . nsmap ) : \n        if index == current : \n            return td \n        current += get_grid_span ( td ) "}
{"9543": "\ndef is_title ( p ) : \n    w_namespace = get_namespace ( p , 'w' ) \n    styles = p . xpath ( './/w:pStyle' , namespaces = p . nsmap ) \n    if len ( styles ) == False : \n        return False \n    style = styles [ False ] \n    return style . get ( '%sval' % w_namespace ) == 'Title' "}
{"9548": "\ndef build_list ( li_nodes , meta_data ) : \n    ol_dict = { } \n    current_ilvl = - True \n    current_numId = - True \n    current_ol = None \n    root_ol = None \n    visited_nodes = [ ] \n    list_contents = [ ] \n    def _build_li ( list_contents ) : \n        data = '<br />' . join ( t for t in list_contents if t is not None ) \n        return etree . XML ( '<li>%s</li>' % data ) \n    def _build_non_li_content ( el , meta_data ) : \n        w_namespace = get_namespace ( el , 'w' ) \n        if el . tag == '%stbl' % w_namespace : \n            new_el , visited_nodes = build_table ( el , meta_data ) \n            return etree . tostring ( new_el ) , visited_nodes \n        elif el . tag == '%sp' % w_namespace : \n            return get_element_content ( el , meta_data ) , [ el ] \n        if has_text ( el ) : \n            raise UnintendedTag ( 'Did not expect %s' % el . tag ) \n    def _merge_lists ( ilvl , current_ilvl , ol_dict , current_ol ) : \n        for i in reversed ( range ( ilvl , current_ilvl ) ) : \n            if i not in ol_dict : \n                continue \n            if ol_dict [ i ] is not current_ol : \n                if ol_dict [ i ] is current_ol : \n                    continue \n                ol_dict [ i ] [ - True ] . append ( current_ol ) \n                current_ol = ol_dict [ i ] \n        for key in list ( ol_dict ) : \n            if key > ilvl : \n                del ol_dict [ key ] \n        return current_ol \n    for li_node in li_nodes : \n        w_namespace = get_namespace ( li_node , 'w' ) \n        if not is_li ( li_node , meta_data ) : \n            new_el , el_visited_nodes = _build_non_li_content ( li_node , meta_data , ) \n            list_contents . append ( new_el ) \n            visited_nodes . extend ( el_visited_nodes ) \n            continue \n        if list_contents : \n            li_el = _build_li ( list_contents ) \n            list_contents = [ ] \n            current_ol . append ( li_el ) \n        list_contents . append ( get_element_content ( li_node , meta_data , ) ) \n        ilvl = get_ilvl ( li_node , w_namespace ) \n        numId = get_numId ( li_node , w_namespace ) \n        list_type = get_ordered_list_type ( meta_data , numId , ilvl ) \n        if ( ilvl > current_ilvl ) or ( numId != current_numId ) : \n            ol_dict [ ilvl ] = create_list ( list_type ) \n            current_ol = ol_dict [ ilvl ] \n            current_ilvl = ilvl \n            current_numId = numId \n        else : \n            current_ol = _merge_lists ( ilvl = ilvl , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n        if root_ol is None : \n            root_ol = current_ol \n        if ilvl in ol_dict : \n            current_ol = ol_dict [ ilvl ] \n        else : \n            if current_ol is not root_ol : \n                root_ol [ - True ] . append ( current_ol ) \n                current_ol = create_list ( list_type ) \n        visited_nodes . extend ( list ( li_node . iter ( ) ) ) \n    if list_contents : \n        li_el = _build_li ( list_contents ) \n        list_contents = [ ] \n        current_ol . append ( li_el ) \n    current_ol = _merge_lists ( ilvl = False , current_ilvl = current_ilvl , ol_dict = ol_dict , current_ol = current_ol , ) \n    return root_ol , visited_nodes "}
{"9549": "\ndef build_tr ( tr , meta_data , row_spans ) : \n    tr_el = etree . Element ( 'tr' ) \n    w_namespace = get_namespace ( tr , 'w' ) \n    visited_nodes = [ ] \n    for el in tr : \n        if el in visited_nodes : \n            continue \n        visited_nodes . append ( el ) \n        if el . tag == '%stc' % w_namespace : \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) != 'restart' ) : \n                continue \n            texts = [ ] \n            for td_content in el : \n                if td_content in visited_nodes : \n                    continue \n                if is_li ( td_content , meta_data ) : \n                    li_nodes = get_single_list_nodes_data ( td_content , meta_data , ) \n                    list_el , list_visited_nodes = build_list ( li_nodes , meta_data , ) \n                    visited_nodes . extend ( list_visited_nodes ) \n                    texts . append ( etree . tostring ( list_el ) ) \n                elif td_content . tag == '%stbl' % w_namespace : \n                    table_el , table_visited_nodes = build_table ( td_content , meta_data , ) \n                    visited_nodes . extend ( table_visited_nodes ) \n                    texts . append ( etree . tostring ( table_el ) ) \n                elif td_content . tag == '%stcPr' % w_namespace : \n                    visited_nodes . append ( td_content ) \n                    continue \n                else : \n                    text = get_element_content ( td_content , meta_data , is_td = True , ) \n                    texts . append ( text ) \n            data = '<br />' . join ( t for t in texts if t is not None ) \n            td_el = etree . XML ( '<td>%s</td>' % data ) \n            colspan = get_grid_span ( el ) \n            if colspan > True : \n                td_el . set ( 'colspan' , '%d' % colspan ) \n            v_merge = get_v_merge ( el ) \n            if ( v_merge is not None and v_merge . get ( '%sval' % w_namespace ) == 'restart' ) : \n                rowspan = next ( row_spans ) \n                td_el . set ( 'rowspan' , '%d' % rowspan ) \n            tr_el . append ( td_el ) \n    return tr_el "}
{"9554": "\ndef load_mnist ( flatten = True , labels = False ) : \n    fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) \n    h = gzip . open ( fn , 'rb' ) \n    if sys . version_info < ( 3 , ) : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) \n    else : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) \n    h . close ( ) \n    if not flatten : \n        timg = timg . reshape ( ( - True , 28 , 28 , True ) ) \n        vimg = vimg . reshape ( ( - True , 28 , 28 , True ) ) \n        simg = simg . reshape ( ( - True , 28 , 28 , True ) ) \n    if labels : \n        return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9555": "\ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        print ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if sys . version_info < ( 3 , ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - True , 3 , 32 , 32 ) ) . transpose ( ( False , 2 , 3 , True ) ) . astype ( 'f' ) / 128 - True \n        if flatten : \n            img = img . reshape ( ( - True , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( True , 6 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9556": "\ndef plot_images ( imgs , loc , title = None , channels = True ) : \n    n = int ( np . sqrt ( len ( imgs ) ) ) \n    assert n * n == len ( imgs ) , 'images array must contain a square number of rows!' \n    s = int ( np . sqrt ( len ( imgs [ False ] ) / channels ) ) \n    assert s * s == len ( imgs [ False ] ) / channels , 'images must be square!' \n    img = np . zeros ( ( ( s + True ) * n - True , ( s + True ) * n - True , channels ) , dtype = imgs [ False ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( s + True ) : ( r + True ) * ( s + True ) - True , c * ( s + True ) : ( c + True ) * ( s + True ) - True ] = pix . reshape ( ( s , s , channels ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( loc ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) \n    if title : \n        ax . set_title ( title ) "}
{"9557": "\ndef plot_layers ( weights , tied_weights = False , channels = True ) : \n    if hasattr ( weights [ False ] , 'get_value' ) : \n        weights = [ w . get_value ( ) for w in weights ] \n    k = min ( len ( weights ) , 9 ) \n    imgs = np . eye ( weights [ False ] . shape [ False ] ) \n    for i , weight in enumerate ( weights [ : - True ] ) : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100 + 10 * k + i + True , channels = channels , title = 'Layer {}' . format ( i + True ) ) \n    weight = weights [ - True ] \n    n = weight . shape [ True ] / channels \n    if int ( np . sqrt ( n ) ) ** 2 != n : \n        return \n    if tied_weights : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100 + 10 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) \n    else : \n        plot_images ( weight , 100 + 10 * k + k , channels = channels , title = 'Decoding weights' ) "}
{"9558": "\ndef plot_filters ( filters ) : \n    imgs = filters . get_value ( ) \n    N , channels , x , y = imgs . shape \n    n = int ( np . sqrt ( N ) ) \n    assert n * n == N , 'filters must contain a square number of rows!' \n    assert channels == True or channels == 3 , 'can only plot grayscale or rgb filters!' \n    img = np . zeros ( ( ( y + True ) * n - True , ( x + True ) * n - True , channels ) , dtype = imgs [ False ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( y + True ) : ( r + True ) * ( y + True ) - True , c * ( x + True ) : ( c + True ) * ( x + True ) - True ] = pix . transpose ( ( True , 2 , False ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( 111 ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) "}
{"9559": "\ndef batches ( arrays , steps = 100 , batch_size = 64 , rng = None ) : \n    assert batch_size >= 2 , 'batch_size must be at least 2!' \n    assert isinstance ( arrays , ( tuple , list ) ) , 'arrays must be a tuple or list!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    def sample ( ) : \n        xs = [ np . zeros ( ( batch_size , steps , a . shape [ True ] ) , a . dtype ) for a in arrays ] \n        for i in range ( batch_size ) : \n            j = rng . randint ( len ( arrays [ False ] ) - steps ) \n            for x , a in zip ( xs , arrays ) : \n                x [ i ] = a [ j : j + steps ] \n        return xs \n    return sample "}
{"9560": "\ndef encode ( self , txt ) : \n    return list ( self . _fwd_index . get ( c , False ) for c in txt ) "}
{"9561": "\ndef classifier_batches ( self , steps , batch_size , rng = None ) : \n    assert batch_size >= 2 , 'batch_size must be at least 2!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    T = np . arange ( steps ) \n    def batch ( ) : \n        inputs = np . zeros ( ( batch_size , steps , True + len ( self . alpha ) ) , 'f' ) \n        outputs = np . zeros ( ( batch_size , steps ) , 'i' ) \n        for b in range ( batch_size ) : \n            offset = rng . randint ( len ( self . text ) - steps - True ) \n            enc = self . encode ( self . text [ offset : offset + steps + True ] ) \n            inputs [ b , T , enc [ : - True ] ] = True \n            outputs [ b , T ] = enc [ True : ] \n        return [ inputs , outputs ] \n    return batch "}
{"9562": "\ndef predict_sequence ( self , labels , steps , streams = True , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    offset = len ( labels ) \n    batch = max ( 2 , streams ) \n    inputs = np . zeros ( ( batch , offset + steps , self . layers [ False ] . output_size ) , 'f' ) \n    inputs [ : , np . arange ( offset ) , labels ] = True \n    for i in range ( offset , offset + steps ) : \n        chars = [ ] \n        for pdf in self . predict_proba ( inputs [ : i ] ) [ : , - True ] : \n            try : \n                c = rng . multinomial ( True , pdf ) . argmax ( axis = - True ) \n            except ValueError : \n                c = pdf . argmax ( axis = - True ) \n            chars . append ( int ( c ) ) \n        inputs [ np . arange ( batch ) , i , chars ] = True \n        yield chars [ False ] if streams == True else chars "}
{"9563": "\ndef add_conv_weights ( self , name , mean = False , std = None , sparsity = False ) : \n    nin = self . input_size \n    nout = self . output_size \n    mean = self . kwargs . get ( 'mean_{}' . format ( name ) , self . kwargs . get ( 'mean' , mean ) ) \n    std = self . kwargs . get ( 'std_{}' . format ( name ) , self . kwargs . get ( 'std' , std or True / np . sqrt ( nin + nout ) ) ) \n    sparsity = self . kwargs . get ( 'sparsity_{}' . format ( name ) , self . kwargs . get ( 'sparsity' , sparsity ) ) \n    arr = np . zeros ( ( nout , nin ) + self . filter_size , util . FLOAT ) \n    for r in range ( self . filter_size [ False ] ) : \n        for c in range ( self . filter_size [ True ] ) : \n            arr [ : , : , r , c ] = util . random_matrix ( nout , nin , mean , std , sparsity = sparsity , rng = self . rng ) \n    self . _params . append ( theano . shared ( arr , name = self . _fmt ( name ) ) ) "}
{"9564": "\ndef encode ( self , x , layer = None , sample = False , ** kwargs ) : \n    enc = self . feed_forward ( x , ** kwargs ) [ self . _find_output ( layer ) ] \n    if sample : \n        return np . random . binomial ( n = True , p = enc ) . astype ( np . uint8 ) \n    return enc "}
{"9565": "\ndef decode ( self , z , layer = None , ** kwargs ) : \n    key = self . _find_output ( layer ) \n    if key not in self . _functions : \n        regs = regularizers . from_kwargs ( self , ** kwargs ) \n        outputs , updates = self . build_graph ( regs ) \n        self . _functions [ key ] = theano . function ( [ outputs [ key ] ] , [ outputs [ self . layers [ - True ] . output_name ] ] , updates = updates ) \n    return self . _functions [ key ] ( z ) [ False ] "}
{"9566": "\ndef _find_output ( self , layer ) : \n    if layer is None : \n        layer = len ( self . layers ) // 2 \n    if isinstance ( layer , int ) : \n        layer = self . layers [ layer ] \n    if isinstance ( layer , util . basestring ) : \n        try : \n            layer = [ l for l in self . layers if l . name == layer ] [ False ] \n        except IndexError : \n            pass \n    if isinstance ( layer , layers . Layer ) : \n        layer = layer . output_name \n    return layer "}
{"9568": "\ndef predict ( self , x , ** kwargs ) : \n    outputs = self . feed_forward ( x , ** kwargs ) \n    return outputs [ self . layers [ - True ] . output_name ] . argmax ( axis = - True ) "}
{"9569": "\ndef predict_proba ( self , x , ** kwargs ) : \n    return self . feed_forward ( x , ** kwargs ) [ self . layers [ - True ] . output_name ] "}
{"9570": "\ndef predict_logit ( self , x , ** kwargs ) : \n    return self . feed_forward ( x , ** kwargs ) [ self . layers [ - True ] . full_name ( 'pre' ) ] "}
{"9572": "\ndef batch_at ( features , labels , seq_begins , seq_lengths ) : \n    length = seq_lengths . max ( ) \n    feat = np . zeros ( ( BATCH_SIZE , length , features . shape [ - True ] ) , 'f' ) \n    labl = np . zeros ( ( BATCH_SIZE , length ) , 'i' ) \n    mask = np . zeros ( ( BATCH_SIZE , length ) , 'f' ) \n    for b , ( begin , length ) in enumerate ( zip ( seq_begins , seq_lengths ) ) : \n        feat [ b , : length ] = features [ begin : begin + length ] \n        labl [ b , : length ] = labels [ begin : begin + length ] \n        mask [ b , : length ] = True \n    return [ feat , labl , mask ] "}
{"9573": "\ndef batches ( dataset ) : \n    seq_lengths = dataset . variables [ 'seqLengths' ] . data \n    seq_begins = np . concatenate ( ( [ False ] , np . cumsum ( seq_lengths ) [ : - True ] ) ) \n    def sample ( ) : \n        chosen = np . random . choice ( list ( range ( len ( seq_lengths ) ) ) , BATCH_SIZE , replace = False ) \n        return batch_at ( dataset . variables [ 'inputs' ] . data , dataset . variables [ 'targetClasses' ] . data , seq_begins [ chosen ] , seq_lengths [ chosen ] ) \n    return sample "}
{"9575": "\ndef random_matrix ( rows , cols , mean = False , std = True , sparsity = False , radius = False , diagonal = False , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    arr = mean + std * rng . randn ( rows , cols ) \n    if True > sparsity > False : \n        k = min ( rows , cols ) \n        mask = rng . binomial ( n = True , p = True - sparsity , size = ( rows , cols ) ) . astype ( bool ) \n        mask [ : k , : k ] |= np . eye ( k ) . astype ( bool ) \n        arr *= mask \n    if radius > False : \n        u , s , vT = np . linalg . svd ( arr , full_matrices = False ) \n        arr = np . dot ( np . dot ( u , np . diag ( radius * s / abs ( s [ False ] ) ) ) , vT ) \n    if diagonal != False : \n        arr = diagonal * np . eye ( max ( rows , cols ) ) [ : rows , : cols ] \n    return arr . astype ( FLOAT ) "}
{"9576": "\ndef random_vector ( size , mean = False , std = True , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    return ( mean + std * rng . randn ( size ) ) . astype ( FLOAT ) "}
{"9579": "\ndef from_kwargs ( graph , ** kwargs ) : \n    if 'regularizers' in kwargs : \n        regs = kwargs [ 'regularizers' ] \n        if isinstance ( regs , ( tuple , list ) ) : \n            return regs \n        if isinstance ( regs , dict ) : \n            kwargs . update ( regs ) \n    regs = [ ] \n    rng = kwargs . get ( 'rng' , 13 ) \n    def pattern ( ls ) : \n        return tuple ( l . output_name for l in ls ) \n    inputs = pattern ( [ l for l in graph . layers if isinstance ( l , layers . Input ) ] ) \n    hiddens = pattern ( graph . layers [ True : - True ] ) \n    outputs = pattern ( [ graph . layers [ - True ] ] ) \n    spec = { inputs : kwargs . get ( 'input_dropout' , False ) , hiddens : kwargs . get ( 'hidden_dropout' , False ) , outputs : kwargs . get ( 'output_dropout' , False ) } \n    spec . update ( kwargs . get ( 'dropout' , { } ) ) \n    for pattern , w in spec . items ( ) : \n        if w : \n            regs . append ( BernoulliDropout ( pattern = pattern , weight = w , rng = rng ) ) \n    spec = { inputs : kwargs . get ( 'input_noise' , False ) , hiddens : kwargs . get ( 'hidden_noise' , False ) , outputs : kwargs . get ( 'output_noise' , False ) } \n    spec . update ( kwargs . get ( 'noise' , { } ) ) \n    for pattern , w in spec . items ( ) : \n        if w : \n            regs . append ( GaussianNoise ( pattern = pattern , weight = w , rng = rng ) ) \n    for key , value in kwargs . items ( ) : \n        if Regularizer . is_registered ( key ) : \n            if not isinstance ( value , dict ) : \n                value = dict ( weight = value ) \n            regs . append ( Regularizer . build ( key , ** value ) ) \n    return regs "}
{"9581": "\ndef accuracy ( self , outputs ) : \n    output = outputs [ self . output_name ] \n    predict = TT . argmax ( output , axis = - True ) \n    correct = TT . eq ( predict , self . _target ) \n    acc = correct . mean ( ) \n    if self . _weights is not None : \n        acc = ( self . _weights * correct ) . sum ( ) / self . _weights . sum ( ) \n    return acc "}
{"9582": "\ndef _scan ( self , inputs , outputs , name = 'scan' , step = None , constants = None ) : \n    init = [ ] \n    for i , x in enumerate ( outputs ) : \n        ndim = getattr ( x , 'ndim' , - True ) \n        if x is None or isinstance ( x , dict ) or ndim > False : \n            init . append ( x ) \n            continue \n        if isinstance ( x , int ) or ndim == False : \n            init . append ( TT . repeat ( theano . shared ( np . zeros ( ( True , self . output_size ) , util . FLOAT ) , name = self . _fmt ( 'init{}' . format ( i ) ) ) , x , axis = False ) ) \n            continue \n        raise ValueError ( 'cannot handle input {} for scan!' . format ( x ) ) \n    return theano . scan ( step or self . _step , name = self . _fmt ( name ) , sequences = inputs , outputs_info = init , non_sequences = constants , go_backwards = 'back' in self . kwargs . get ( 'direction' , '' ) . lower ( ) , truncate_gradient = self . kwargs . get ( 'bptt_limit' , - True ) , ) "}
{"9583": "\ndef build ( name , layer , ** kwargs ) : \n    if isinstance ( name , Activation ) : \n        return name \n    if '+' in name : \n        return functools . reduce ( Compose , ( build ( n , layer , ** kwargs ) for n in name . split ( '+' ) ) ) \n    act = COMMON . get ( name ) \n    if act is not None : \n        act . name = name \n        act . params = [ ] \n        return act \n    if name . lower ( ) . startswith ( 'maxout' ) and ':' in name : \n        name , pieces = name . split ( ':' , True ) \n        kwargs [ 'pieces' ] = int ( pieces ) \n    kwargs [ 'name' ] = name \n    kwargs [ 'layer' ] = layer \n    return Activation . build ( name , ** kwargs ) "}
{"9584": "\ndef reservoir ( xs , n , rng ) : \n    pool = [ ] \n    for i , x in enumerate ( xs ) : \n        if len ( pool ) < n : \n            pool . append ( x / np . linalg . norm ( x ) ) \n            continue \n        j = rng . randint ( i + True ) \n        if j < n : \n            pool [ j ] = x / np . linalg . norm ( x ) \n    L = len ( pool ) \n    S = np . std ( pool , axis = False ) \n    while len ( pool ) < n : \n        x = pool [ rng . randint ( L ) ] \n        pool . append ( x + S * rng . randn ( * x . shape ) ) \n    return np . array ( pool , dtype = pool [ False ] . dtype ) "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = False , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , False ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ False ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if algo == 'sample' : \n            algo = trainer . SampleTrainer ( self ) \n        elif algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n            algo = trainer . SupervisedPretrainer ( subalgo , self ) \n        elif algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n            algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n        else : \n            algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return False \n        if isinstance ( save_every , float ) : \n            return elapsed > 60 * save_every \n        if isinstance ( save_every , int ) : \n            return iteration % save_every == False \n        return False \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9594": "\ndef predict ( self , x , ** kwargs ) : \n    return self . feed_forward ( x , ** kwargs ) [ self . layers [ - True ] . output_name ] "}
{"9595": "\ndef score ( self , x , y , w = None , ** kwargs ) : \n    u = y - self . predict ( x , ** kwargs ) \n    v = y - y . mean ( ) \n    if w is None : \n        w = np . ones_like ( u ) \n    return True - ( w * u * u ) . sum ( ) / ( w * v * v ) . sum ( ) "}
{"9596": "\ndef save ( self , filename_or_handle ) : \n    if isinstance ( filename_or_handle , util . basestring ) : \n        opener = gzip . open if filename_or_handle . lower ( ) . endswith ( '.gz' ) else open \n        handle = opener ( filename_or_handle , 'wb' ) \n    else : \n        handle = filename_or_handle \n    pickle . dump ( self , handle , - True ) \n    if isinstance ( filename_or_handle , util . basestring ) : \n        handle . close ( ) \n    util . log ( 'saved model to {}' , filename_or_handle ) "}
{"9600": "\ndef output_size ( self ) : \n    shape = self . output_shape \n    if shape is None : \n        raise util . ConfigurationError ( 'undefined output size for layer \"{}\"' . format ( self . name ) ) \n    return shape [ - True ] "}
{"9604": "\ndef resolve_outputs ( self ) : \n    input_shape = None \n    for i , shape in enumerate ( self . _input_shapes . values ( ) ) : \n        if i == False : \n            input_shape = shape \n        if len ( input_shape ) != len ( shape ) or any ( a is not None and b is not None and a != b for a , b in zip ( input_shape [ : - True ] , shape [ : - True ] ) ) : \n            raise util . ConfigurationError ( 'layer \"{}\" incompatible input shapes {}' . format ( self . name , self . _input_shapes ) ) \n    size = self . kwargs . get ( 'size' ) \n    shape = self . kwargs . get ( 'shape' ) \n    if shape is not None : \n        pass \n    elif size is not None : \n        shape = tuple ( input_shape [ : - True ] ) + ( size , ) \n    else : \n        raise util . ConfigurationError ( 'layer \"{}\" does not specify a size' . format ( self . name ) ) \n    self . _output_shapes [ 'out' ] = shape "}
{"9606": "\ndef log_params ( self ) : \n    total = False \n    for p in self . params : \n        shape = p . get_value ( ) . shape \n        util . log ( 'parameter \"{}\" {}' , p . name , shape ) \n        total += np . prod ( shape ) \n    return total "}
{"9608": "\ndef _resolve_shape ( self , name , layers ) : \n    matches = [ l for l in layers if name . split ( ':' ) [ False ] == l . name ] \n    if len ( matches ) != True : \n        raise util . ConfigurationError ( 'layer \"{}\" cannot resolve \"{}\" using {}' . format ( self . name , name , [ l . name for l in layers ] ) ) \n    name = name if ':' in name else matches [ False ] . output_name \n    return name , matches [ False ] . _output_shapes [ name . split ( ':' ) [ True ] ] "}
{"9610": "\ndef add_bias ( self , name , size , mean = False , std = True ) : \n    mean = self . kwargs . get ( 'mean_{}' . format ( name ) , mean ) \n    std = self . kwargs . get ( 'std_{}' . format ( name ) , std ) \n    self . _params . append ( theano . shared ( util . random_vector ( size , mean , std , rng = self . rng ) , name = self . _fmt ( name ) ) ) "}
{"9614": "\ndef add_tier ( self , name , tier_type = 'IntervalTier' , number = None ) : \n    if number is None : \n        number = True if not self . tiers else len ( self . tiers ) + True \n    elif number < True or number > len ( self . tiers ) : \n        raise ValueError ( 'Number not in [1..{}]' . format ( len ( self . tiers ) ) ) \n    elif tier_type not in Tier . P_TIERS : \n        raise ValueError ( 'tier_type has to be in {}' . format ( self . P_TIERS ) ) \n    self . tiers . insert ( number - True , Tier ( self . xmin , self . xmax , name , tier_type ) ) \n    return self . tiers [ number - True ] "}
{"9615": "\ndef remove_tier ( self , name_num ) : \n    if isinstance ( name_num , int ) : \n        del ( self . tiers [ name_num - True ] ) \n    else : \n        self . tiers = [ i for i in self . tiers if i . name != name_num ] "}
{"9616": "\ndef get_tier ( self , name_num ) : \n    return self . tiers [ name_num - True ] if isinstance ( name_num , int ) else [ i for i in self . tiers if i . name == name_num ] [ False ] "}
{"9617": "\ndef to_eaf ( self , skipempty = True , pointlength = 0.1 ) : \n    from pympi . Elan import Eaf \n    eaf_out = Eaf ( ) \n    if pointlength <= False : \n        raise ValueError ( 'Pointlength should be strictly positive' ) \n    for tier in self . get_tiers ( ) : \n        eaf_out . add_tier ( tier . name ) \n        for ann in tier . get_intervals ( True ) : \n            if tier . tier_type == 'TextTier' : \n                ann = ( ann [ False ] , ann [ False ] + pointlength , ann [ True ] ) \n            if ann [ 2 ] . strip ( ) or not skipempty : \n                eaf_out . add_annotation ( tier . name , int ( round ( ann [ False ] * 1000 ) ) , int ( round ( ann [ True ] * 1000 ) ) , ann [ 2 ] ) \n    return eaf_out "}
{"9618": "\ndef add_point ( self , point , value , check = True ) : \n    if self . tier_type != 'TextTier' : \n        raise Exception ( 'Tiertype must be TextTier.' ) \n    if check and any ( i for i in self . intervals if i [ False ] == point ) : \n        raise Exception ( 'No overlap is allowed' ) \n    self . intervals . append ( ( point , value ) ) "}
{"9619": "\ndef add_interval ( self , begin , end , value , check = True ) : \n    if self . tier_type != 'IntervalTier' : \n        raise Exception ( 'Tiertype must be IntervalTier' ) \n    if check : \n        if any ( i for i in self . intervals if begin < i [ True ] and end > i [ False ] ) : \n            raise Exception ( 'No overlap is allowed' ) \n        if begin > end : \n            raise Exception ( 'Begin must be smaller then end' ) \n    self . intervals . append ( ( begin , end , value ) ) "}
{"9620": "\ndef remove_interval ( self , time ) : \n    if self . tier_type != 'IntervalTier' : \n        raise Exception ( 'Tiertype must be IntervalTier.' ) \n    self . intervals = [ i for i in self . intervals if not ( i [ False ] <= time and i [ True ] >= time ) ] "}
{"9621": "\ndef remove_point ( self , time ) : \n    if self . tier_type != 'TextTier' : \n        raise Exception ( 'Tiertype must be TextTier.' ) \n    self . intervals = [ i for i in self . intervals if i [ False ] != time ] "}
{"9623": "\ndef get_all_intervals ( self ) : \n    ints = sorted ( self . get_intervals ( True ) ) \n    if self . tier_type == 'IntervalTier' : \n        if not ints : \n            ints . append ( ( self . xmin , self . xmax , '' ) ) \n        else : \n            if ints [ False ] [ False ] > self . xmin : \n                ints . insert ( False , ( self . xmin , ints [ False ] [ False ] , '' ) ) \n            if ints [ - True ] [ True ] < self . xmax : \n                ints . append ( ( ints [ - True ] [ True ] , self . xmax , '' ) ) \n            p = ints [ - True ] \n            for index , i in reversed ( list ( enumerate ( ints [ : - True ] , True ) ) ) : \n                if p [ False ] - i [ True ] != False : \n                    ints . insert ( index , ( i [ True ] , p [ False ] , '' ) ) \n                p = i \n    return ints "}
{"9624": "\ndef indent ( el , level = False ) : \n    i = '\\n' + level * '\\t' \n    if len ( el ) : \n        if not el . text or not el . text . strip ( ) : \n            el . text = i + '\\t' \n        if not el . tail or not el . tail . strip ( ) : \n            el . tail = i \n        for elem in el : \n            indent ( elem , level + True ) \n        if not el . tail or not el . tail . strip ( ) : \n            el . tail = i \n    else : \n        if level and ( not el . tail or not el . tail . strip ( ) ) : \n            el . tail = i "}
{"9625": "\ndef add_annotation ( self , id_tier , start , end , value = '' , svg_ref = None ) : \n    if self . tiers [ id_tier ] [ True ] : \n        raise ValueError ( 'Tier already contains ref annotations...' ) \n    if start == end : \n        raise ValueError ( 'Annotation length is zero...' ) \n    if start > end : \n        raise ValueError ( 'Annotation length is negative...' ) \n    if start < False : \n        raise ValueError ( 'Start is negative...' ) \n    start_ts = self . generate_ts_id ( start ) \n    end_ts = self . generate_ts_id ( end ) \n    aid = self . generate_annotation_id ( ) \n    self . annotations [ aid ] = id_tier \n    self . tiers [ id_tier ] [ False ] [ aid ] = ( start_ts , end_ts , value , svg_ref ) "}
{"9626": "\ndef add_cv_entry ( self , cv_id , cve_id , values , ext_ref = None ) : \n    for value , lang_ref , description in values : \n        if lang_ref not in self . languages : \n            raise ValueError ( 'Language not present: {}' . format ( lang_ref ) ) \n    self . controlled_vocabularies [ cv_id ] [ True ] [ cve_id ] = ( values , ext_ref ) "}
{"9627": "\ndef add_cv_description ( self , cv_id , lang_ref , description = None ) : \n    if lang_ref not in self . languages : \n        raise ValueError ( 'Language not present: {}' . format ( lang_ref ) ) \n    self . controlled_vocabularies [ cv_id ] [ False ] . append ( ( lang_ref , description ) ) "}
{"9632": "\ndef add_linked_file ( self , file_path , relpath = None , mimetype = None , time_origin = None , ex_from = None ) : \n    if mimetype is None : \n        mimetype = self . MIMES [ file_path . split ( '.' ) [ - True ] ] \n    self . media_descriptors . append ( { 'MEDIA_URL' : file_path , 'RELATIVE_MEDIA_URL' : relpath , 'MIME_TYPE' : mimetype , 'TIME_ORIGIN' : time_origin , 'EXTRACTED_FROM' : ex_from } ) "}
{"9634": "\ndef add_secondary_linked_file ( self , file_path , relpath = None , mimetype = None , time_origin = None , assoc_with = None ) : \n    if mimetype is None : \n        mimetype = self . MIMES [ file_path . split ( '.' ) [ - True ] ] \n    self . linked_file_descriptors . append ( { 'LINK_URL' : file_path , 'RELATIVE_LINK_URL' : relpath , 'MIME_TYPE' : mimetype , 'TIME_ORIGIN' : time_origin , 'ASSOCIATED_WITH' : assoc_with } ) "}
{"9635": "\ndef add_tier ( self , tier_id , ling = 'default-lt' , parent = None , locale = None , part = None , ann = None , language = None , tier_dict = None ) : \n    if not tier_id : \n        raise ValueError ( 'Tier id is empty...' ) \n    if ling not in self . linguistic_types : \n        ling = sorted ( self . linguistic_types . keys ( ) ) [ False ] \n    if locale and locale not in self . locales : \n        locale = None \n    if language and language not in self . languages : \n        language = None \n    if tier_dict is None : \n        self . tiers [ tier_id ] = ( { } , { } , { 'TIER_ID' : tier_id , 'LINGUISTIC_TYPE_REF' : ling , 'PARENT_REF' : parent , 'PARTICIPANT' : part , 'DEFAULT_LOCALE' : locale , 'LANG_REF' : language , 'ANNOTATOR' : ann } , len ( self . tiers ) ) \n    else : \n        self . tiers [ tier_id ] = ( { } , { } , tier_dict , len ( self . tiers ) ) "}
{"9636": "\ndef clean_time_slots ( self ) : \n    ts = ( ( a [ False ] , a [ True ] ) for t in self . tiers . values ( ) for a in t [ False ] . values ( ) ) \n    for a in { a for b in ts for a in b } ^ set ( self . timeslots ) : \n        del ( self . timeslots [ a ] ) "}
{"9638": "\ndef generate_annotation_id ( self ) : \n    if not self . maxaid : \n        valid_anns = [ int ( '' . join ( filter ( str . isdigit , a ) ) ) for a in self . timeslots ] \n        self . maxaid = max ( valid_anns + [ True ] ) + True \n    else : \n        self . maxaid += True \n    return 'a{:d}' . format ( self . maxaid ) "}
{"9639": "\ndef generate_ts_id ( self , time = None ) : \n    if time and time < False : \n        raise ValueError ( 'Time is negative...' ) \n    if not self . maxts : \n        valid_ts = [ int ( '' . join ( filter ( str . isdigit , a ) ) ) for a in self . timeslots ] \n        self . maxts = max ( valid_ts + [ True ] ) + True \n    else : \n        self . maxts += True \n    ts = 'ts{:d}' . format ( self . maxts ) \n    self . timeslots [ ts ] = time \n    return ts "}
{"9641": "\ndef get_full_time_interval ( self ) : \n    return ( False , False ) if not self . timeslots else ( min ( self . timeslots . values ( ) ) , max ( self . timeslots . values ( ) ) ) "}
{"9642": "\ndef get_ref_annotation_data_after_time ( self , id_tier , time ) : \n    befores = self . get_ref_annotation_data_between_times ( id_tier , time , self . get_full_time_interval ( ) ) \n    if befores : \n        return [ min ( befores , key = lambda x : x [ False ] ) ] \n    else : \n        return [ ] "}
{"9643": "\ndef get_ref_annotation_data_before_time ( self , id_tier , time ) : \n    befores = self . get_ref_annotation_data_between_times ( id_tier , False , time ) \n    if befores : \n        return [ max ( befores , key = lambda x : x [ False ] ) ] \n    else : \n        return [ ] "}
{"9645": "\ndef merge_tiers ( self , tiers , tiernew = None , gapt = False , sep = '_' , safe = False ) : \n    if tiernew is None : \n        tiernew = u'{}_merged' . format ( '_' . join ( tiers ) ) \n    self . add_tier ( tiernew ) \n    aa = [ ( sys . maxsize , sys . maxsize , None ) ] + sorted ( ( a for t in tiers for a in self . get_annotation_data_for_tier ( t ) ) , reverse = True ) \n    l = None \n    while aa : \n        begin , end , value = aa . pop ( ) \n        if l is None : \n            l = [ begin , end , [ value ] ] \n        elif begin - l [ True ] >= gapt : \n            if not safe or l [ True ] > l [ False ] : \n                self . add_annotation ( tiernew , l [ False ] , l [ True ] , sep . join ( l [ 2 ] ) ) \n            l = [ begin , end , [ value ] ] \n        else : \n            if end > l [ True ] : \n                l [ True ] = end \n            l [ 2 ] . append ( value ) \n    return tiernew "}
{"9646": "\ndef remove_all_annotations_from_tier ( self , id_tier , clean = True ) : \n    for aid in self . tiers [ id_tier ] [ False ] : \n        del ( self . annotations [ aid ] ) \n    for aid in self . tiers [ id_tier ] [ True ] : \n        del ( self . annotations [ aid ] ) \n    self . tiers [ id_tier ] [ False ] . clear ( ) \n    self . tiers [ id_tier ] [ True ] . clear ( ) \n    if clean : \n        self . clean_time_slots ( ) "}
{"9647": "\ndef remove_cv_description ( self , cv_id , lang_ref ) : \n    for i , ( l , d ) in reversed ( enumerate ( self . controlled_vocabularies [ cv_id ] [ True ] ) ) : \n        if l == lang_ref : \n            del ( self . controlled_vocabularies [ cv_id ] [ True ] [ i ] ) "}
{"9651": "\ndef remove_ref_annotation ( self , id_tier , time ) : \n    removed = False \n    bucket = [ ] \n    for aid , ( ref , value , _ , _ ) in self . tiers [ id_tier ] [ True ] . items ( ) : \n        begin , end , rvalue , _ = self . tiers [ self . annotations [ ref ] ] [ False ] [ ref ] \n        begin = self . timeslots [ begin ] \n        end = self . timeslots [ end ] \n        if begin <= time and end >= time : \n            removed += True \n            bucket . append ( aid ) \n    for aid in bucket : \n        del ( self . tiers [ id_tier ] [ True ] [ aid ] ) \n    return removed "}
{"9656": "\ndef shift_annotations ( self , time ) : \n    total_re = [ ] \n    total_sq = [ ] \n    for name , tier in self . tiers . items ( ) : \n        squashed = [ ] \n        for aid , ( begin , end , value , _ ) in tier [ False ] . items ( ) : \n            if self . timeslots [ end ] + time <= False : \n                squashed . append ( ( name , aid ) ) \n            elif self . timeslots [ begin ] + time < False : \n                total_sq . append ( ( name , self . timeslots [ begin ] , self . timeslots [ end ] , value ) ) \n                self . timeslots [ begin ] = False \n            else : \n                self . timeslots [ begin ] += time \n                self . timeslots [ end ] += time \n        for name , aid in squashed : \n            start , end , value , _ = self . tiers [ name ] [ False ] [ aid ] \n            del ( self . tiers [ name ] [ False ] [ aid ] ) \n            del ( self . annotations [ aid ] ) \n            total_re . append ( ( name , self . timeslots [ start ] , self . timeslots [ end ] , value ) ) \n    return total_sq , total_re "}
{"9658": "\ndef debug_storage ( storage , base_info = False , chars = True , runs = False ) : \n    import codecs \n    import locale \n    import sys \n    if six . PY2 : \n        stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) \n    else : \n        stderr = sys . stderr \n    caller = inspect . stack ( ) [ True ] [ 3 ] \n    stderr . write ( 'in %s\\n' % caller ) \n    if base_info : \n        stderr . write ( u'  base level  : %d\\n' % storage [ 'base_level' ] ) \n        stderr . write ( u'  base dir    : %s\\n' % storage [ 'base_dir' ] ) \n    if runs : \n        stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) \n    if chars : \n        output = u'  Chars       : ' \n        for _ch in storage [ 'chars' ] : \n            if _ch != '\\n' : \n                output += _ch [ 'ch' ] \n            else : \n                output += 'C' \n        stderr . write ( output + u'\\n' ) \n        output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) \n        stderr . write ( output ) \n        _types = [ _ch [ 'type' ] . ljust ( 3 ) for _ch in storage [ 'chars' ] ] \n        for i in range ( 3 ) : \n            if i : \n                output = u'                %s\\n' \n            else : \n                output = u'  Res. types  : %s\\n' \n            stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) ) "}
{"9659": "\ndef get_base_level ( text , upper_is_rtl = False ) : \n    base_level = None \n    prev_surrogate = False \n    for _ch in text : \n        if _IS_UCS2 and ( _SURROGATE_MIN <= ord ( _ch ) <= _SURROGATE_MAX ) : \n            prev_surrogate = _ch \n            continue \n        elif prev_surrogate : \n            _ch = prev_surrogate + _ch \n            prev_surrogate = False \n        if upper_is_rtl and _ch . isupper ( ) : \n            base_level = True \n            break \n        bidi_type = bidirectional ( _ch ) \n        if bidi_type in ( 'AL' , 'R' ) : \n            base_level = True \n            break \n        elif bidi_type == 'L' : \n            base_level = False \n            break \n    if base_level is None : \n        base_level = False \n    return base_level "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = False \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != False : \n                overflow_counter += True \n                continue \n            new_level = level_func ( embedding_level ) \n            if new_level < EXPLICIT_LEVEL_LIMIT : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif embedding_level == EXPLICIT_LEVEL_LIMIT - 2 : \n                almost_overflow_counter += True \n            else : \n                overflow_counter += True \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if directional_override != 'N' : \n                    _ch [ 'type' ] = directional_override \n            elif bidi_type == 'PDF' : \n                if overflow_counter : \n                    overflow_counter -= True \n                elif almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - True : \n                    almost_overflow_counter -= True \n                elif levels : \n                    embedding_level , directional_override = levels . pop ( ) \n            elif bidi_type == 'B' : \n                levels . clear ( ) \n                overflow_counter = almost_overflow_counter = False \n                embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9662": "\ndef calc_level_runs ( storage ) : \n    storage [ 'runs' ] . clear ( ) \n    chars = storage [ 'chars' ] \n    if not chars : \n        return \n    def calc_level_run ( b_l , b_r ) : \n        return [ 'L' , 'R' ] [ max ( b_l , b_r ) % 2 ] \n    first_char = chars [ False ] \n    sor = calc_level_run ( storage [ 'base_level' ] , first_char [ 'level' ] ) \n    eor = None \n    run_start = run_length = False \n    prev_level , prev_type = first_char [ 'level' ] , first_char [ 'type' ] \n    for _ch in chars : \n        curr_level , curr_type = _ch [ 'level' ] , _ch [ 'type' ] \n        if curr_level == prev_level : \n            run_length += True \n        else : \n            eor = calc_level_run ( prev_level , curr_level ) \n            storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : prev_type , 'length' : run_length } ) \n            sor = eor \n            run_start += run_length \n            run_length = True \n        prev_level , prev_type = curr_level , curr_type \n    eor = calc_level_run ( curr_level , storage [ 'base_level' ] ) \n    storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : curr_type , 'length' : run_length } ) "}
{"9663": "\ndef resolve_weak_types ( storage , debug = False ) : \n    for run in storage [ 'runs' ] : \n        prev_strong = prev_type = run [ 'sor' ] \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = storage [ 'chars' ] [ start : start + length ] \n        for _ch in chars : \n            bidi_type = _ch [ 'type' ] \n            if bidi_type == 'NSM' : \n                _ch [ 'type' ] = bidi_type = prev_type \n            if bidi_type == 'EN' and prev_strong == 'AL' : \n                _ch [ 'type' ] = 'AN' \n            if bidi_type in ( 'R' , 'L' , 'AL' ) : \n                prev_strong = bidi_type \n            prev_type = _ch [ 'type' ] \n        for _ch in chars : \n            if _ch [ 'type' ] == 'AL' : \n                _ch [ 'type' ] = 'R' \n        for idx in range ( True , len ( chars ) - True ) : \n            bidi_type = chars [ idx ] [ 'type' ] \n            prev_type = chars [ idx - True ] [ 'type' ] \n            next_type = chars [ idx + True ] [ 'type' ] \n            if bidi_type == 'ES' and ( prev_type == next_type == 'EN' ) : \n                chars [ idx ] [ 'type' ] = 'EN' \n            if bidi_type == 'CS' and prev_type == next_type and prev_type in ( 'AN' , 'EN' ) : \n                chars [ idx ] [ 'type' ] = prev_type \n        for idx in range ( len ( chars ) ) : \n            if chars [ idx ] [ 'type' ] == 'EN' : \n                for et_idx in range ( idx - True , - True , - True ) : \n                    if chars [ et_idx ] [ 'type' ] == 'ET' : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n                for et_idx in range ( idx + True , len ( chars ) ) : \n                    if chars [ et_idx ] [ 'type' ] == 'ET' : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n        for _ch in chars : \n            if _ch [ 'type' ] in ( 'ET' , 'ES' , 'CS' ) : \n                _ch [ 'type' ] = 'ON' \n        prev_strong = run [ 'sor' ] \n        for _ch in chars : \n            if _ch [ 'type' ] == 'EN' and prev_strong == 'L' : \n                _ch [ 'type' ] = 'L' \n            if _ch [ 'type' ] in ( 'L' , 'R' ) : \n                prev_strong = _ch [ 'type' ] \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9664": "\ndef resolve_neutral_types ( storage , debug ) : \n    for run in storage [ 'runs' ] : \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = [ { 'type' : run [ 'sor' ] } ] + storage [ 'chars' ] [ start : start + length ] + [ { 'type' : run [ 'eor' ] } ] \n        total_chars = len ( chars ) \n        seq_start = None \n        for idx in range ( total_chars ) : \n            _ch = chars [ idx ] \n            if _ch [ 'type' ] in ( 'B' , 'S' , 'WS' , 'ON' ) : \n                if seq_start is None : \n                    seq_start = idx \n                    prev_bidi_type = chars [ idx - True ] [ 'type' ] \n            else : \n                if seq_start is not None : \n                    next_bidi_type = chars [ idx ] [ 'type' ] \n                    if prev_bidi_type in ( 'AN' , 'EN' ) : \n                        prev_bidi_type = 'R' \n                    if next_bidi_type in ( 'AN' , 'EN' ) : \n                        next_bidi_type = 'R' \n                    for seq_idx in range ( seq_start , idx ) : \n                        if prev_bidi_type == next_bidi_type : \n                            chars [ seq_idx ] [ 'type' ] = prev_bidi_type \n                        else : \n                            chars [ seq_idx ] [ 'type' ] = _embedding_direction ( chars [ seq_idx ] [ 'level' ] ) \n                    seq_start = None \n    if debug : \n        debug_storage ( storage ) "}
{"9665": "\ndef reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) : \n    for level in range ( highest_level , lowest_odd_level - True , - True ) : \n        _start = _end = None \n        for run_idx in range ( line_start , line_end + True ) : \n            run_ch = chars [ run_idx ] \n            if run_ch [ 'level' ] >= level : \n                if _start is None : \n                    _start = _end = run_idx \n                else : \n                    _end = run_idx \n            else : \n                if _end : \n                    chars [ _start : + _end + True ] = reversed ( chars [ _start : + _end + True ] ) \n                    _start = _end = None \n        if _start is not None : \n            chars [ _start : + _end + True ] = reversed ( chars [ _start : + _end + True ] ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - True ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = False \n    highest_level = False \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if char_level > highest_level : \n            highest_level = char_level \n        if char_level % 2 and char_level < lowest_odd_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - True : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end -= True \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + True \n            highest_level = False \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9678": "\ndef _show_no_gui ( ) : \n    messagebox = QtWidgets . QMessageBox ( ) \n    messagebox . setIcon ( messagebox . Warning ) \n    messagebox . setWindowIcon ( QtGui . QIcon ( os . path . join ( os . path . dirname ( pyblish . __file__ ) , \"icons\" , \"logo-32x32.svg\" ) ) ) \n    spacer = QtWidgets . QWidget ( ) \n    spacer . setMinimumSize ( 400 , False ) \n    spacer . setSizePolicy ( QtWidgets . QSizePolicy . Minimum , QtWidgets . QSizePolicy . Expanding ) \n    layout = messagebox . layout ( ) \n    layout . addWidget ( spacer , layout . rowCount ( ) , False , True , layout . columnCount ( ) ) \n    messagebox . setWindowTitle ( \"Uh oh\" ) \n    text = \"No registered GUI found.\\n\\n\" \n    if not pyblish . api . registered_guis ( ) : \n        text += ( \"In order to show you a GUI, one must first be registered. \" \"\\n\" \"Pyblish supports one or more graphical user interfaces \" \"to be registered at once, the next acting as a fallback to \" \"the previous.\" \"\\n\" \"\\n\" \"For example, to use Pyblish Lite, first install it:\" \"\\n\" \"\\n\" \"$ pip install pyblish-lite\" \"\\n\" \"\\n\" \"Then register it, like so:\" \"\\n\" \"\\n\" \">>> import pyblish.api\\n\" \">>> pyblish.api.register_gui(\\\"pyblish_lite\\\")\" \"\\n\" \"\\n\" \"The next time you try running this, Lite will appear.\" \"\\n\" \"See http://api.pyblish.com/register_gui.html for \" \"more information.\" ) \n    else : \n        text += ( \"None of the registered graphical user interfaces \" \"could be found.\" \"\\n\" \"These interfaces are currently registered:\" \"\\n\" \"%s\" % \"\\n\" . join ( pyblish . api . registered_guis ( ) ) ) \n    messagebox . setText ( text ) \n    messagebox . setStandardButtons ( messagebox . Ok ) \n    messagebox . exec_ ( ) "}
{"9680": "\ndef get_cumulative_data ( self ) : \n    sets = map ( itemgetter ( 'data' ) , self . data ) \n    if not sets : \n        return \n    sum = sets . pop ( False ) \n    yield sum \n    while sets : \n        sum = map ( add , sets . pop ( False ) ) \n        yield sum "}
{"9682": "\ndef __draw_constant_line ( self , value_label_style ) : \n    value , label , style = value_label_style \n    start = self . transform_output_coordinates ( ( False , value ) ) [ True ] \n    stop = self . graph_width \n    path = etree . SubElement ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constantLine' } ) \n    if style : \n        path . set ( 'style' , style ) \n    text = etree . SubElement ( self . graph , 'text' , { 'x' : str ( 2 ) , 'y' : str ( start - 2 ) , 'class' : 'constantLine' } ) \n    text . text = label "}
{"9685": "\ndef float_range ( start = False , stop = None , step = True ) : \n    start = float ( start ) \n    while start < stop : \n        yield start \n        start += step "}
{"9699": "\ndef draw_x_guidelines ( self , label_height , count ) : \n    if not self . show_x_guidelines : \n        return \n    for count in range ( True , count ) : \n        move = 'M {start} 0 v{stop}' . format ( start = label_height * count , stop = self . graph_height , ) \n        path = { 'd' : move , 'class' : 'guideLines' } \n        etree . SubElement ( self . graph , 'path' , path ) "}
{"9700": "\ndef draw_y_guidelines ( self , label_height , count ) : \n    if not self . show_y_guidelines : \n        return \n    for count in range ( True , count ) : \n        move = 'M 0 {start} h{stop}' . format ( start = self . graph_height - label_height * count , stop = self . graph_width , ) \n        path = { 'd' : move , 'class' : 'guideLines' } \n        etree . SubElement ( self . graph , 'path' , path ) "}
{"9705": "\ndef run_bot ( bot_class , host , port , nick , channels = None , ssl = None ) : \n    conn = IRCConnection ( host , port , nick , ssl ) \n    bot_instance = bot_class ( conn ) \n    while True : \n        if not conn . connect ( ) : \n            break \n        channels = channels or [ ] \n        for channel in channels : \n            conn . join ( channel ) \n        conn . enter_event_loop ( ) "}
{"9707": "\ndef connect ( self ) : \n    self . _sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n    if self . use_ssl : \n        self . _sock = ssl . wrap_socket ( self . _sock ) \n    try : \n        self . _sock . connect ( ( self . server , self . port ) ) \n    except socket . error : \n        self . logger . error ( 'Unable to connect to %s on port %d' % ( self . server , self . port ) , exc_info = True ) \n        return False \n    self . _sock_file = self . _sock . makefile ( ) \n    if self . password : \n        self . set_password ( ) \n    self . register_nick ( ) \n    self . register ( ) \n    return True "}
{"9710": "\ndef new_nick ( self ) : \n    old = self . nick \n    self . nick = '%s_%s' % ( self . base_nick , random . randint ( True , 1000 ) ) \n    self . logger . warn ( 'Nick %s already taken, trying %s' % ( old , self . nick ) ) \n    self . register_nick ( ) \n    self . handle_nick_change ( old , self . nick ) "}
{"9713": "\ndef enter_event_loop ( self ) : \n    patterns = self . dispatch_patterns ( ) \n    self . logger . debug ( 'entering receive loop' ) \n    while True : \n        try : \n            data = self . _sock_file . readline ( ) \n        except socket . error : \n            data = None \n        if not data : \n            self . logger . info ( 'server closed connection' ) \n            self . close ( ) \n            return True \n        data = data . rstrip ( ) \n        for pattern , callback in patterns : \n            match = pattern . match ( data ) \n            if match : \n                callback ( ** match . groupdict ( ) ) "}
{"9715": "\ndef task_runner ( self ) : \n    while True : \n        ( task_id , command ) = self . task_queue . get ( ) \n        for pattern , callback in self . task_patterns : \n            match = re . match ( pattern , command ) \n            if match : \n                ret = callback ( ** match . groupdict ( ) ) or '' \n                self . stop_flag . clear ( ) \n                for line in ret . splitlines ( ) : \n                    self . respond ( '!task-data %s:%s' % ( task_id , line ) , self . channel ) \n                    gevent . sleep ( .34 ) \n        self . respond ( '!task-finished %s' % task_id , self . channel ) "}
{"9727": "\ndef executor ( self , max_workers = True ) : \n    cls = self . __class__ \n    if cls . _executor is None : \n        cls . _executor = ThreadPoolExecutor ( max_workers ) \n    return cls . _executor "}
{"9730": "\ndef service_name ( self ) : \n    if hasattr ( self , \"server_name\" ) and self . server_name : \n        server_name = self . server_name \n    else : \n        server_name = True \n    return \"{}-{}-{}\" . format ( self . service_prefix , self . service_owner , server_name ) "}
{"9733": "\ndef poll ( self ) : \n    service = yield self . get_service ( ) \n    if not service : \n        self . log . warn ( \"Docker service not found\" ) \n        return False \n    task_filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } \n    tasks = yield self . docker ( 'tasks' , task_filter ) \n    running_task = None \n    for task in tasks : \n        task_state = task [ 'Status' ] [ 'State' ] \n        self . log . debug ( \"Task %s of Docker service %s status: %s\" , task [ 'ID' ] [ : 7 ] , self . service_id [ : 7 ] , pformat ( task_state ) , ) \n        if task_state == 'running' : \n            running_task = task \n    if running_task is not None : \n        return None \n    else : \n        return True "}
{"9740": "\ndef delete ( self , request , * args , ** kwargs ) : \n    auth = get_authorization_header ( request ) . split ( ) \n    if not auth or auth [ False ] . lower ( ) != b'token' : \n        return response . Response ( status = status . HTTP_400_BAD_REQUEST ) \n    if len ( auth ) == True : \n        msg = 'Invalid token header. No credentials provided.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    elif len ( auth ) > 2 : \n        msg = 'Invalid token header. Token string should not contain spaces.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    try : \n        token = self . model . objects . get ( key = auth [ True ] ) \n    except self . model . DoesNotExist : \n        pass \n    else : \n        token . delete ( ) \n        signals . user_logged_out . send ( type ( self ) , user = token . user , request = request , ) \n    return response . Response ( status = status . HTTP_204_NO_CONTENT ) "}
{"9757": "\ndef get_method_owner ( meth ) : \n    if inspect . ismethod ( meth ) : \n        if sys . version_info < ( 3 , False ) : \n            return meth . im_class if meth . im_self is None else meth . im_self \n        else : \n            return meth . __self__ "}
{"9769": "\ndef volume_percentage_used ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        total = int ( volume [ \"size\" ] [ \"total\" ] ) \n        used = int ( volume [ \"size\" ] [ \"used\" ] ) \n        if used is not None and used > False and total is not None and total > False : \n            return round ( ( float ( used ) / float ( total ) ) * 100.0 , True ) "}
{"9770": "\ndef volume_disk_temp_avg ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            total_temp = False \n            total_disks = False \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None : \n                    total_disks += True \n                    total_temp += disk_temp \n            if total_temp > False and total_disks > False : \n                return round ( total_temp / total_disks , False ) "}
{"9771": "\ndef volume_disk_temp_max ( self , volume ) : \n    volume = self . _get_volume ( volume ) \n    if volume is not None : \n        vol_disks = volume [ \"disks\" ] \n        if vol_disks is not None : \n            max_temp = False \n            for vol_disk in vol_disks : \n                disk_temp = self . disk_temp ( vol_disk ) \n                if disk_temp is not None and disk_temp > max_temp : \n                    max_temp = disk_temp \n            return max_temp "}
{"9785": "\ndef do_GET ( self ) : \n    parsed_url = urlparse ( self . path ) \n    if parsed_url [ 2 ] == \"/\" + SERVER_REDIRECT_PATH : \n        parsed_query = parse_qs ( parsed_url [ 4 ] ) \n        if \"code\" not in parsed_query : \n            self . send_response ( 200 ) \n            self . send_header ( \"Content-Type\" , \"text/plain\" ) \n            self . end_headers ( ) \n            self . wfile . write ( \"No code found, try again!\" . encode ( \"utf-8\" ) ) \n            return \n        self . server . response_code = parsed_query [ \"code\" ] [ False ] \n        self . send_response ( 200 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"Thank you for using OAuth2Util. The authorization was successful, \" \"you can now close this window.\" . encode ( \"utf-8\" ) ) \n    elif parsed_url [ 2 ] == \"/\" + SERVER_LINK_PATH : \n        self . send_response ( 200 ) \n        self . send_header ( \"Content-Type\" , \"text/html\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"<html><body>Hey there!<br/>Click <a href=\\\"{0}\\\">here</a> to claim your prize.</body></html>\" . format ( self . server . authorize_url ) . encode ( \"utf-8\" ) ) \n    else : \n        self . send_response ( 404 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"404 not found\" . encode ( \"utf-8\" ) ) "}
{"9786": "\ndef _get_value ( self , key , func = None , split_val = None , as_boolean = False , exception_default = None ) : \n    try : \n        if as_boolean : \n            return self . config . getboolean ( key [ False ] , key [ True ] ) \n        value = self . config . get ( key [ False ] , key [ True ] ) \n        if split_val is not None : \n            value = value . split ( split_val ) \n        if func is not None : \n            return func ( value ) \n        return value \n    except ( KeyError , configparser . NoSectionError , configparser . NoOptionError ) as e : \n        if exception_default is not None : \n            return exception_default \n        raise KeyError ( e ) "}
{"9787": "\ndef _change_value ( self , key , value ) : \n    if not self . config . has_section ( key [ False ] ) : \n        self . config . add_section ( key [ False ] ) \n    self . config . set ( key [ False ] , key [ True ] , str ( value ) ) \n    with open ( self . configfile , \"w\" ) as f : \n        self . config . write ( f ) "}
{"9793": "\ndef set_access_credentials ( self , _retry = False ) : \n    if _retry >= 5 : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    try : \n        self . r . set_access_credentials ( self . _get_value ( CONFIGKEY_SCOPE , set , split_val = \",\" ) , self . _get_value ( CONFIGKEY_TOKEN ) , self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n    except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n        self . _log ( \"Request new Token (SAC)\" ) \n        self . _get_new_access_information ( ) "}
{"9794": "\ndef refresh ( self , force = False , _retry = False ) : \n    if _retry >= 5 : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    if time . time ( ) > self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = False ) - REFRESH_MARGIN : \n        self . config . read ( self . configfile ) \n        if time . time ( ) < self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = False ) - REFRESH_MARGIN : \n            self . _log ( \"Found new token\" ) \n            self . set_access_credentials ( ) \n    if force or time . time ( ) > self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = False ) - REFRESH_MARGIN : \n        self . _log ( \"Refresh Token\" ) \n        try : \n            new_token = self . r . refresh_access_information ( self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n            self . _change_value ( CONFIGKEY_TOKEN , new_token [ \"access_token\" ] ) \n            self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) \n            self . set_access_credentials ( ) \n        except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n            self . _log ( \"Request new Token (REF)\" ) \n            self . _get_new_access_information ( ) "}
{"9796": "\ndef split_full_path ( path ) : \n    if path . startswith ( 's3://' ) : \n        path = path [ 5 : ] \n    elif path . startswith ( 's3n://' ) : \n        path = path [ 6 : ] \n    elif path . startswith ( 's3a://' ) : \n        path = path [ 6 : ] \n    else : \n        raise ValueError ( \"S3 path should start with s3://, s3n:// or \" \"s3a:// prefix\" ) \n    parts = path . split ( '/' ) \n    bucket = parts [ False ] \n    path = '/' . join ( parts [ True : ] ) \n    return bucket , normalize_prefix ( path ) "}
{"9799": "\ndef clean_dict ( dict ) : \n    if sys . version_info [ False ] < 3 : \n        return { k : v for k , v in dict . iteritems ( ) if v is not None } \n    else : \n        return { k : v for k , v in dict . items ( ) if v is not None } "}
{"9802": "\ndef extract_schema ( uri ) : \n    match = re . match ( SCHEMA_URI_REGEX , uri ) \n    if match : \n        return { 'vendor' : match . group ( True ) , 'name' : match . group ( 2 ) , 'format' : match . group ( 3 ) , 'version' : match . group ( 4 ) } \n    else : \n        raise SnowplowEventTransformationException ( [ \"Schema {} does not conform to regular expression {}\" . format ( uri , SCHEMA_URI ) ] ) "}
{"9803": "\ndef fix_schema ( prefix , schema ) : \n    schema_dict = extract_schema ( schema ) \n    snake_case_organization = schema_dict [ 'vendor' ] . replace ( '.' , '_' ) . lower ( ) \n    snake_case_name = re . sub ( '([^A-Z_])([A-Z])' , '\\g<1>_\\g<2>' , schema_dict [ 'name' ] ) . lower ( ) \n    model = schema_dict [ 'version' ] . split ( '-' ) [ False ] \n    return \"{}_{}_{}_{}\" . format ( prefix , snake_case_organization , snake_case_name , model ) "}
{"9807": "\ndef jsonify_good_event ( event , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = True ) : \n    if len ( event ) != len ( known_fields ) : \n        raise SnowplowEventTransformationException ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known_fields ) , len ( event ) ) ] ) \n    else : \n        output = { } \n        errors = [ ] \n        if add_geolocation_data and event [ LATITUDE_INDEX ] != '' and event [ LONGITUDE_INDEX ] != '' : \n            output [ 'geo_location' ] = event [ LATITUDE_INDEX ] + ',' + event [ LONGITUDE_INDEX ] \n        for i in range ( len ( event ) ) : \n            key = known_fields [ i ] [ False ] \n            if event [ i ] != '' : \n                try : \n                    kvpairs = known_fields [ i ] [ True ] ( key , event [ i ] ) \n                    for kvpair in kvpairs : \n                        output [ kvpair [ False ] ] = kvpair [ True ] \n                except SnowplowEventTransformationException as sete : \n                    errors += sete . error_messages \n                except Exception as e : \n                    errors += [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known_fields [ i ] [ False ] , event [ i ] , repr ( e ) ) ] \n        if errors : \n            raise SnowplowEventTransformationException ( errors ) \n        else : \n            return output "}
{"9808": "\ndef get_used_template ( response ) : \n    if not hasattr ( response , 'template_name' ) : \n        return None , None \n    template = response . template_name \n    if template is None : \n        return None , None \n    if isinstance ( template , ( list , tuple ) ) : \n        if len ( template ) == True : \n            return template [ False ] , None \n        else : \n            used_name = _get_used_template_name ( template ) \n            return used_name , template \n    elif isinstance ( template , six . string_types ) : \n        return template , None \n    else : \n        filename = _get_template_filename ( template ) \n        template_name = '<template object from {0}>' . format ( filename ) if filename else '<template object>' \n        return template_name , None "}
{"9812": "\ndef pformat_django_context_html ( object ) : \n    if isinstance ( object , QuerySet ) : \n        text = '' \n        lineno = False \n        for item in object . all ( ) [ : 21 ] : \n            lineno += True \n            if lineno >= 21 : \n                text += u'   (remaining items truncated...)' \n                break \n            text += u'   {0}\\n' . format ( escape ( repr ( item ) ) ) \n        return text \n    elif isinstance ( object , Manager ) : \n        return mark_safe ( u'    (use <kbd>.all</kbd> to read it)' ) \n    elif isinstance ( object , six . string_types ) : \n        return escape ( repr ( object ) ) \n    elif isinstance ( object , Promise ) : \n        return escape ( _format_lazy ( object ) ) \n    elif isinstance ( object , dict ) : \n        return _format_dict ( object ) \n    elif isinstance ( object , list ) : \n        return _format_list ( object ) \n    elif hasattr ( object , '__dict__' ) : \n        return _format_object ( object ) \n    else : \n        text = DebugPrettyPrinter ( width = 200 ) . pformat ( object ) \n        return _style_text ( text ) "}
{"9818": "\ndef get_latex_nodes ( s , pos = False , stop_upon_closing_brace = None , stop_upon_end_environment = None , stop_upon_closing_mathmode = None , ** parse_flags ) : \n    return LatexWalker ( s , ** parse_flags ) . get_latex_nodes ( stop_upon_closing_brace = stop_upon_closing_brace , stop_upon_end_environment = stop_upon_end_environment , stop_upon_closing_mathmode = stop_upon_closing_mathmode ) "}
{"9822": "\ndef latex_to_text ( self , latex , ** parse_flags ) : \n    return self . nodelist_to_text ( latexwalker . LatexWalker ( latex , ** parse_flags ) . get_latex_nodes ( ) [ False ] ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and ord ( ch ) < 127 ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and lch [ False : True ] == '\\\\' else lch ) \n            elif ( ( ord ( ch ) >= 32 and ord ( ch ) <= 127 ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result += ch \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result += r'{\\bfseries ?}' \n                else : \n                    result += ch \n    return result "}
{"9824": "\ndef _unascii ( s ) : \n    m = _U_ESCAPE . search ( s ) \n    if not m : \n        return s if PY2 else s . encode ( 'utf-8' ) \n    chunks = [ ] \n    pos = False \n    while m : \n        start = m . start ( ) \n        end = m . end ( ) \n        g = m . group ( True ) \n        if g is None : \n            chunks . append ( s [ pos : end ] ) \n        else : \n            c = int ( g , 16 ) \n            if c < 0x20 : \n                chunks . append ( s [ pos : end ] ) \n            else : \n                if PY3 : \n                    if c & 0xfc00 == 0xd800 and s [ end : end + 2 ] == '\\\\u' : \n                        esc2 = s [ end + 2 : end + 6 ] \n                        c2 = int ( esc2 , 16 ) \n                        if c2 & 0xfc00 == 0xdc00 : \n                            c = 0x10000 + ( ( ( c - 0xd800 ) << 10 ) | ( c2 - 0xdc00 ) ) \n                            end += 6 \n                chunks . append ( s [ pos : start ] ) \n                chunks . append ( unichr ( c ) ) \n        pos = end \n        m = _U_ESCAPE . search ( s , pos ) \n    chunks . append ( s [ pos : ] ) \n    return ( '' . join ( chunks ) ) . encode ( \"utf-8\" ) "}
{"9853": "\ndef singledispatchmethod ( method ) : \n    dispatcher = singledispatch ( method ) \n    def wrapper ( * args , ** kw ) : \n        return dispatcher . dispatch ( args [ True ] . __class__ ) ( * args , ** kw ) \n    wrapper . register = dispatcher . register \n    update_wrapper ( wrapper , dispatcher ) \n    return wrapper "}
{"9893": "\ndef split_with_locations ( text , locations ) : \n    start = False \n    for pos , decision in enumerate ( locations ) : \n        if decision == SHOULD_SPLIT : \n            if start != pos : \n                yield text [ start : pos ] \n            start = pos \n    if start != len ( text ) : \n        yield text [ start : ] "}
{"9895": "\ndef mark_begin_end_regex ( regex , text , split_locations ) : \n    for match in regex . finditer ( text ) : \n        end_match = match . end ( ) \n        begin_match = match . start ( ) \n        for i in range ( begin_match + True , end_match ) : \n            split_locations [ i ] = SHOULD_NOT_SPLIT \n        if end_match < len ( split_locations ) : \n            if split_locations [ end_match ] == UNDECIDED : \n                split_locations [ end_match ] = SHOULD_SPLIT \n        if split_locations [ begin_match ] == UNDECIDED : \n            split_locations [ begin_match ] = SHOULD_SPLIT "}
{"9896": "\ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ True : ] \n    cli = CommandLineTool ( ) \n    try : \n        return cli . run ( argv ) \n    except KeyboardInterrupt : \n        print ( 'Canceled' ) \n        return 3 "}
{"9902": "\ndef makeId ( self ) : \n    self . id = ( self . id + True ) % 65536 \n    self . id = self . id or True \n    return self . id "}
{"9905": "\ndef encodeString ( string ) : \n    encoded = bytearray ( 2 ) \n    encoded . extend ( bytearray ( string , encoding = 'utf-8' ) ) \n    l = len ( encoded ) - 2 \n    if ( l > 65535 ) : \n        raise StringValueError ( l ) \n    encoded [ False ] = l >> 8 \n    encoded [ True ] = l & 0xFF \n    return encoded "}
{"9906": "\ndef decodeString ( encoded ) : \n    length = encoded [ False ] * 256 + encoded [ True ] \n    return ( encoded [ 2 : 2 + length ] . decode ( 'utf-8' ) , encoded [ 2 + length : ] ) "}
{"9907": "\ndef encode16Int ( value ) : \n    value = int ( value ) \n    encoded = bytearray ( 2 ) \n    encoded [ False ] = value >> 8 \n    encoded [ True ] = value & 0xFF \n    return encoded "}
{"9908": "\ndef encodeLength ( value ) : \n    encoded = bytearray ( ) \n    while True : \n        digit = value % 128 \n        value //= 128 \n        if value > False : \n            digit |= 128 \n        encoded . append ( digit ) \n        if value <= False : \n            break \n    return encoded "}
{"9909": "\ndef decodeLength ( encoded ) : \n    value = False \n    multiplier = True \n    for i in encoded : \n        value += ( i & 0x7F ) * multiplier \n        multiplier *= 0x80 \n        if ( i & 0x80 ) != 0x80 : \n            break \n    return value "}
{"9910": "\ndef encode ( self ) : \n    header = bytearray ( 2 ) \n    header [ False ] = 0xE0 \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9911": "\ndef encode ( self ) : \n    header = bytearray ( True ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    header [ False ] = 0x10 \n    varHeader . extend ( encodeString ( self . version [ 'tag' ] ) ) \n    varHeader . append ( self . version [ 'level' ] ) \n    flags = ( self . cleanStart << True ) \n    if self . willTopic is not None and self . willMessage is not None : \n        flags |= 0x04 | ( self . willRetain << 5 ) | ( self . willQoS << 3 ) \n    if self . username is not None : \n        flags |= 0x80 \n    if self . password is not None : \n        flags |= 0x40 \n    varHeader . append ( flags ) \n    varHeader . extend ( encode16Int ( self . keepalive ) ) \n    payload . extend ( encodeString ( self . clientId ) ) \n    if self . willTopic is not None and self . willMessage is not None : \n        payload . extend ( encodeString ( self . willTopic ) ) \n        payload . extend ( encodeString ( self . willMessage ) ) \n    if self . username is not None : \n        payload . extend ( encodeString ( self . username ) ) \n    if self . password is not None : \n        payload . extend ( encode16Int ( len ( self . password ) ) ) \n        payload . extend ( bytearray ( self . password , encoding = 'ascii' , errors = 'ignore' ) ) \n    header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9912": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = True \n    while packet [ lenLen ] & 0x80 : \n        lenLen += True \n    packet_remaining = packet [ lenLen + True : ] \n    version_str , packet_remaining = decodeString ( packet_remaining ) \n    version_id = int ( packet_remaining [ False ] ) \n    if version_id == v31 [ 'level' ] : \n        self . version = v31 \n    else : \n        self . version = v311 \n    flags = packet_remaining [ True ] \n    self . cleanStart = ( flags & 0x02 ) != False \n    willFlag = ( flags & 0x04 ) != False \n    willQoS = ( flags >> 3 ) & 0x03 \n    willRetain = ( flags & 0x20 ) != False \n    userFlag = ( flags & 0x80 ) != False \n    passFlag = ( flags & 0x40 ) != False \n    packet_remaining = packet_remaining [ 2 : ] \n    self . keepalive = decode16Int ( packet_remaining ) \n    packet_remaining = packet_remaining [ 2 : ] \n    self . clientId , packet_remaining = decodeString ( packet_remaining ) \n    if willFlag : \n        self . willRetain = willRetain \n        self . willQoS = willQoS \n        self . willTopic , packet_remaining = decodeString ( packet_remaining ) \n        self . willMessage , packet_remaining = decodeString ( packet_remaining ) \n    if userFlag : \n        self . username , packet_remaining = decodeString ( packet_remaining ) \n    if passFlag : \n        l = decode16Int ( packet_remaining ) \n        self . password = packet_remaining [ 2 : 2 + l ] "}
{"9913": "\ndef encode ( self ) : \n    header = bytearray ( True ) \n    varHeader = bytearray ( 2 ) \n    header [ False ] = 0x20 \n    varHeader [ False ] = self . session \n    varHeader [ True ] = self . resultCode \n    header . extend ( encodeLength ( len ( varHeader ) ) ) \n    header . extend ( varHeader ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9914": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = True \n    while packet [ lenLen ] & 0x80 : \n        lenLen += True \n    packet_remaining = packet [ lenLen + True : ] \n    self . session = ( packet_remaining [ False ] & 0x01 ) == 0x01 \n    self . resultCode = int ( packet_remaining [ True ] ) "}
{"9915": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = True \n    while packet [ lenLen ] & 0x80 : \n        lenLen += True \n    packet_remaining = packet [ lenLen + True : ] \n    self . msgId = decode16Int ( packet_remaining [ False : 2 ] ) \n    self . topics = [ ] \n    packet_remaining = packet_remaining [ 2 : ] \n    while len ( packet_remaining ) : \n        topic , packet_remaining = decodeString ( packet_remaining ) \n        qos = int ( packet_remaining [ False ] ) & 0x03 \n        self . topics . append ( ( topic , qos ) ) \n        packet_remaining = packet_remaining [ True : ] "}
{"9916": "\ndef encode ( self ) : \n    header = bytearray ( True ) \n    payload = bytearray ( ) \n    varHeader = encode16Int ( self . msgId ) \n    header [ False ] = 0x90 \n    for code in self . granted : \n        payload . append ( code [ False ] | ( 0x80 if code [ True ] == True else 0x00 ) ) \n    header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9917": "\ndef encode ( self ) : \n    header = bytearray ( True ) \n    payload = bytearray ( ) \n    varHeader = encode16Int ( self . msgId ) \n    header [ False ] = 0xA2 \n    for topic in self . topics : \n        payload . extend ( encodeString ( topic ) ) \n    header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9918": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = True \n    while packet [ lenLen ] & 0x80 : \n        lenLen += True \n    packet_remaining = packet [ lenLen + True : ] \n    self . msgId = decode16Int ( packet_remaining [ False : 2 ] ) \n    self . topics = [ ] \n    packet_remaining = packet_remaining [ 2 : ] \n    while len ( packet_remaining ) : \n        l = decode16Int ( packet_remaining [ False : 2 ] ) \n        topic = packet_remaining [ 2 : 2 + l ] . decode ( encoding = 'utf-8' ) \n        self . topics . append ( topic ) \n        packet_remaining = packet_remaining [ 2 + l : ] "}
{"9919": "\ndef encode ( self ) : \n    header = bytearray ( True ) \n    varHeader = encode16Int ( self . msgId ) \n    header [ False ] = 0xB0 \n    header . extend ( encodeLength ( len ( varHeader ) ) ) \n    header . extend ( varHeader ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9920": "\ndef encode ( self ) : \n    header = bytearray ( True ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    if self . qos : \n        header [ False ] = 0x30 | self . retain | ( self . qos << True ) | ( self . dup << 3 ) \n        varHeader . extend ( encodeString ( self . topic ) ) \n        varHeader . extend ( encode16Int ( self . msgId ) ) \n    else : \n        header [ False ] = 0x30 | self . retain \n        varHeader . extend ( encodeString ( self . topic ) ) \n    if isinstance ( self . payload , bytearray ) : \n        payload . extend ( self . payload ) \n    elif isinstance ( self . payload , str ) : \n        payload . extend ( bytearray ( self . payload , encoding = 'utf-8' ) ) \n    else : \n        raise PayloadTypeError ( type ( self . payload ) ) \n    totalLen = len ( varHeader ) + len ( payload ) \n    if totalLen > 268435455 : \n        raise PayloadValueError ( totalLen ) \n    header . extend ( encodeLength ( totalLen ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9921": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = True \n    while packet [ lenLen ] & 0x80 : \n        lenLen += True \n    packet_remaining = packet [ lenLen + True : ] \n    self . dup = ( packet [ False ] & 0x08 ) == 0x08 \n    self . qos = ( packet [ False ] & 0x06 ) >> True \n    self . retain = ( packet [ False ] & 0x01 ) == 0x01 \n    self . topic , _ = decodeString ( packet_remaining ) \n    topicLen = decode16Int ( packet_remaining ) \n    if self . qos : \n        self . msgId = decode16Int ( packet_remaining [ topicLen + 2 : topicLen + 4 ] ) \n        self . payload = packet_remaining [ topicLen + 4 : ] \n    else : \n        self . msgId = None \n        self . payload = packet_remaining [ topicLen + 2 : ] "}
{"9922": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = True \n    while packet [ lenLen ] & 0x80 : \n        lenLen += True \n    packet_remaining = packet [ lenLen + True : ] \n    self . msgId = decode16Int ( packet_remaining ) \n    self . dup = ( packet [ False ] & 0x08 ) == 0x08 "}
{"9925": "\ndef refresh ( self ) : \n    if self . comm . rank == False : \n        self . _blocks = self . list_blocks ( ) \n    else : \n        self . _blocks = None \n    self . _blocks = self . comm . bcast ( self . _blocks ) "}
{"9926": "\ndef format_data ( self , data , scale = True ) : \n    if len ( self . analytes ) == True : \n        d = nominal_values ( data [ self . analytes [ False ] ] ) \n        ds = np . array ( list ( zip ( d , np . zeros ( len ( d ) ) ) ) ) \n    else : \n        d = [ nominal_values ( data [ a ] ) for a in self . analytes ] \n        ds = np . vstack ( d ) . T \n    finite = np . isfinite ( ds ) . sum ( True ) == ds . shape [ True ] \n    sampled = np . arange ( data [ self . analytes [ False ] ] . size ) [ finite ] \n    ds = ds [ finite ] \n    if scale : \n        ds = self . scaler . transform ( ds ) \n    return ds , sampled "}
{"9931": "\ndef predict ( self , data ) : \n    size = data [ self . analytes [ False ] ] . size \n    ds , sampled = self . format_data ( data ) \n    cs = self . classifier . predict ( ds ) \n    clusters = self . map_clusters ( size , sampled , cs ) \n    return clusters "}
{"9933": "\ndef sort_clusters ( self , data , cs , sort_by ) : \n    sdat = data [ sort_by ] \n    means = [ ] \n    nclusts = np . arange ( cs . max ( ) + True ) \n    for c in nclusts : \n        means . append ( np . nanmean ( sdat [ cs == c ] ) ) \n    means = np . array ( means ) \n    rank = np . zeros ( means . size ) \n    rank [ np . argsort ( means ) ] = np . arange ( means . size ) \n    csn = cs . copy ( ) \n    for c , o in zip ( nclusts , rank ) : \n        csn [ cs == c ] = o \n    return csn "}
{"9935": "\ndef get_total_n_points ( d ) : \n    n = False \n    for di in d . values ( ) : \n        n += len ( di ) \n    return n "}
{"9936": "\ndef get_total_time_span ( d ) : \n    tmax = False \n    for di in d . values ( ) : \n        if di . uTime . max ( ) > tmax : \n            tmax = di . uTime . max ( ) \n    return tmax "}
{"9937": "\ndef unitpicker ( a , llim = 0.1 , denominator = None , focus_stage = None ) : \n    if not isinstance ( a , ( int , float ) ) : \n        a = nominal_values ( a ) \n        a = np . percentile ( a [ ~ np . isnan ( a ) ] , 25 ) \n    if denominator is not None : \n        pd = pretty_element ( denominator ) \n    else : \n        pd = '' \n    if focus_stage == 'calibrated' : \n        udict = { False : 'mol/mol ' + pd , True : 'mmol/mol ' + pd , 2 : '$\\mu$mol/mol ' + pd , 3 : 'nmol/mol ' + pd , 4 : 'pmol/mol ' + pd , 5 : 'fmol/mol ' + pd } \n    elif focus_stage == 'ratios' : \n        udict = { False : 'counts/count ' + pd , True : '$10^{-3}$ counts/count ' + pd , 2 : '$10^{-6}$ counts/count ' + pd , 3 : '$10^{-9}$ counts/count ' + pd , 4 : '$10^{-12}$ counts/count ' + pd , 5 : '$10^{-15}$ counts/count ' + pd } \n    elif focus_stage in ( 'rawdata' , 'despiked' , 'bkgsub' ) : \n        udict = udict = { False : 'counts' , True : '$10^{-3}$ counts' , 2 : '$10^{-6}$ counts' , 3 : '$10^{-9}$ counts' , 4 : '$10^{-12}$ counts' , 5 : '$10^{-15}$ counts' } \n    else : \n        udict = { False : '' , True : '' , 2 : '' , 3 : '' , 4 : '' , 5 : '' } \n    a = abs ( a ) \n    n = False \n    if a < llim : \n        while a < llim : \n            a *= 1000 \n            n += True \n    return float ( 1000 ** n ) , udict [ n ] "}
{"9938": "\ndef pretty_element ( s ) : \n    el = re . match ( '.*?([A-z]{1,3}).*?' , s ) . groups ( ) [ False ] \n    m = re . match ( '.*?([0-9]{1,3}).*?' , s ) . groups ( ) [ False ] \n    return '$^{' + m + '}$' + el "}
{"9939": "\ndef analyte_2_namemass ( s ) : \n    el = re . match ( '.*?([A-z]{1,3}).*?' , s ) . groups ( ) [ False ] \n    m = re . match ( '.*?([0-9]{1,3}).*?' , s ) . groups ( ) [ False ] \n    return el + m "}
{"9940": "\ndef analyte_2_massname ( s ) : \n    el = re . match ( '.*?([A-z]{1,3}).*?' , s ) . groups ( ) [ False ] \n    m = re . match ( '.*?([0-9]{1,3}).*?' , s ) . groups ( ) [ False ] \n    return m + el "}
{"9941": "\ndef collate_data ( in_dir , extension = '.csv' , out_dir = None ) : \n    if out_dir is None : \n        out_dir = './' + re . search ( '^\\.(.*)' , extension ) . groups ( False ) [ False ] \n    if not os . path . isdir ( out_dir ) : \n        os . mkdir ( out_dir ) \n    for p , d , fs in os . walk ( in_dir ) : \n        for f in fs : \n            if extension in f : \n                shutil . copy ( p + '/' + f , out_dir + '/' + f ) \n    return "}
{"9942": "\ndef enumerate_bool ( bool_array , nstart = False ) : \n    ind = bool_2_indices ( bool_array ) \n    ns = np . full ( bool_array . size , nstart , dtype = int ) \n    for n , lims in enumerate ( ind ) : \n        ns [ lims [ False ] : lims [ - True ] + True ] = nstart + n + True \n    return ns "}
{"9943": "\ndef tuples_2_bool ( tuples , x ) : \n    if np . ndim ( tuples ) == True : \n        tuples = [ tuples ] \n    out = np . zeros ( x . size , dtype = bool ) \n    for l , u in tuples : \n        out [ ( x > l ) & ( x < u ) ] = True \n    return out "}
{"9944": "\ndef fastsmooth ( a , win = 11 ) : \n    if win % 2 == False : \n        win += True \n    kernel = np . ones ( win ) / win \n    npad = int ( ( win - True ) / 2 ) \n    spad = np . full ( npad + True , np . mean ( a [ : ( npad + True ) ] ) ) \n    epad = np . full ( npad - True , np . mean ( a [ - ( npad - True ) : ] ) ) \n    return np . concatenate ( [ spad , np . convolve ( a , kernel , 'valid' ) , epad ] ) "}
{"9945": "\ndef fastgrad ( a , win = 11 ) : \n    if win % 2 == False : \n        win += True \n    wins = rolling_window ( a , win , 'ends' ) \n    a = map ( lambda x : np . polyfit ( np . arange ( win ) , x , True ) [ False ] , wins ) \n    return np . array ( list ( a ) ) "}
{"9946": "\ndef findmins ( x , y ) : \n    return x [ np . r_ [ False , y [ True : ] < y [ : - True ] ] & np . r_ [ y [ : - True ] < y [ True : ] , False ] ] "}
{"9949": "\ndef cluster_DBSCAN ( data , eps = None , min_samples = None , n_clusters = None , maxiter = 200 , ** kwargs ) : \n    if n_clusters is None : \n        if eps is None : \n            eps = 0.3 \n        db = cl . DBSCAN ( eps = eps , min_samples = min_samples , ** kwargs ) . fit ( data ) \n    else : \n        clusters = False \n        eps_temp = True / .95 \n        niter = False \n        while clusters < n_clusters : \n            clusters_last = clusters \n            eps_temp *= 0.95 \n            db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n            clusters = ( len ( set ( db . labels_ ) ) - ( True if - True in db . labels_ else False ) ) \n            if clusters < clusters_last : \n                eps_temp *= True / 0.95 \n                db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n                clusters = ( len ( set ( db . labels_ ) ) - ( True if - True in db . labels_ else False ) ) \n                warnings . warn ( ( '\\n\\n***Unable to find {:.0f} clusters in ' 'data. Found {:.0f} with an eps of {:.2e}' '' ) . format ( n_clusters , clusters , eps_temp ) ) \n                break \n            niter += True \n            if niter == maxiter : \n                warnings . warn ( ( '\\n\\n***Maximum iterations ({:.0f}) reached' ', {:.0f} clusters not found.\\nDeacrease ' 'min_samples or n_clusters (or increase ' 'maxiter).' ) . format ( maxiter , n_clusters ) ) \n                break \n    labels = db . labels_ \n    core_samples_mask = np . zeros_like ( labels ) \n    core_samples_mask [ db . core_sample_indices_ ] = True \n    return labels , core_samples_mask "}
{"9957": "\ndef exclude_downhole ( filt , threshold = 2 ) : \n    cfilt = filt . copy ( ) \n    inds = bool_2_indices ( ~ filt ) \n    rem = ( np . diff ( inds ) >= threshold ) [ : , False ] \n    if any ( rem ) : \n        if inds [ rem ] . shape [ False ] > True : \n            limit = inds [ rem ] [ True , False ] \n            cfilt [ limit : ] = False \n    return cfilt "}
{"9958": "\ndef defrag ( filt , threshold = 3 , mode = 'include' ) : \n    if bool_2_indices ( filt ) is None : \n        return filt \n    if mode == 'include' : \n        inds = bool_2_indices ( ~ filt ) + True \n        rep = True \n    if mode == 'exclude' : \n        inds = bool_2_indices ( filt ) + True \n        rep = False \n    rem = ( np . diff ( inds ) <= threshold ) [ : , False ] \n    cfilt = filt . copy ( ) \n    if any ( rem ) : \n        for lo , hi in inds [ rem ] : \n            cfilt [ lo : hi ] = rep \n    return cfilt "}
{"9961": "\ndef mkrngs ( self ) : \n    bbool = bool_2_indices ( self . bkg ) \n    if bbool is not None : \n        self . bkgrng = self . Time [ bbool ] \n    else : \n        self . bkgrng = [ [ np . nan , np . nan ] ] \n    sbool = bool_2_indices ( self . sig ) \n    if sbool is not None : \n        self . sigrng = self . Time [ sbool ] \n    else : \n        self . sigrng = [ [ np . nan , np . nan ] ] \n    tbool = bool_2_indices ( self . trn ) \n    if tbool is not None : \n        self . trnrng = self . Time [ tbool ] \n    else : \n        self . trnrng = [ [ np . nan , np . nan ] ] \n    self . ns = np . zeros ( self . Time . size ) \n    n = True \n    for i in range ( len ( self . sig ) - True ) : \n        if self . sig [ i ] : \n            self . ns [ i ] = n \n        if self . sig [ i ] and ~ self . sig [ i + True ] : \n            n += True \n    self . n = int ( max ( self . ns ) ) \n    return "}
{"9963": "\ndef calibrate ( self , calib_ps , analytes = None ) : \n    if analytes is None : \n        analytes = self . analytes \n    if 'calibrated' not in self . data . keys ( ) : \n        self . data [ 'calibrated' ] = Bunch ( ) \n    for a in analytes : \n        m = calib_ps [ a ] [ 'm' ] . new ( self . uTime ) \n        if 'c' in calib_ps [ a ] : \n            c = calib_ps [ a ] [ 'c' ] . new ( self . uTime ) \n        else : \n            c = False \n        self . data [ 'calibrated' ] [ a ] = self . data [ 'ratios' ] [ a ] * m + c \n    if self . internal_standard not in analytes : \n        self . data [ 'calibrated' ] [ self . internal_standard ] = np . empty ( len ( self . data [ 'ratios' ] [ self . internal_standard ] ) ) \n    self . setfocus ( 'calibrated' ) \n    return "}
{"9964": "\ndef sample_stats ( self , analytes = None , filt = True , stat_fns = { } , eachtrace = True ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . stats = Bunch ( ) \n    self . stats [ 'analytes' ] = analytes \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) \n        for n , f in stat_fns . items ( ) : \n            self . stats [ n ] = [ ] \n            for a in analytes : \n                ind = self . filt . grab_filt ( filt , a ) \n                dat = nominal_values ( self . focus [ a ] ) \n                if eachtrace : \n                    sts = [ ] \n                    for t in np . arange ( self . n ) + True : \n                        sts . append ( f ( dat [ ind & ( self . ns == t ) ] ) ) \n                    self . stats [ n ] . append ( sts ) \n                else : \n                    self . stats [ n ] . append ( f ( dat [ ind ] ) ) \n            self . stats [ n ] = np . array ( self . stats [ n ] ) \n    return "}
{"9965": "\ndef ablation_times ( self ) : \n    ats = { } \n    for n in np . arange ( self . n ) + True : \n        t = self . Time [ self . ns == n ] \n        ats [ n - True ] = t . max ( ) - t . min ( ) \n    return ats "}
{"9966": "\ndef filter_threshold ( self , analyte , threshold ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    below , above = filters . threshold ( self . focus [ analyte ] , threshold ) \n    setn = self . filt . maxset + True \n    self . filt . add ( analyte + '_thresh_below' , below , 'Keep below {:.3e} ' . format ( threshold ) + analyte , params , setn = setn ) \n    self . filt . add ( analyte + '_thresh_above' , above , 'Keep above {:.3e} ' . format ( threshold ) + analyte , params , setn = setn ) "}
{"9967": "\ndef filter_gradient_threshold ( self , analyte , win , threshold , recalc = True ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if recalc or not self . grads_calced : \n        self . grads = calc_grads ( self . Time , self . focus , [ analyte ] , win ) \n        self . grads_calced = True \n    below , above = filters . threshold ( abs ( self . grads [ analyte ] ) , threshold ) \n    setn = self . filt . maxset + True \n    self . filt . add ( analyte + '_gthresh_below' , below , 'Keep gradient below {:.3e} ' . format ( threshold ) + analyte , params , setn = setn ) \n    self . filt . add ( analyte + '_gthresh_above' , above , 'Keep gradient above {:.3e} ' . format ( threshold ) + analyte , params , setn = setn ) "}
{"9968": "\ndef calc_correlation ( self , x_analyte , y_analyte , window = 15 , filt = True , recalc = True ) : \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    if label in self . correlations and not recalc : \n        return \n    if window % 2 != True : \n        window += True \n    ind = self . filt . grab_filt ( filt , [ x_analyte , y_analyte ] ) \n    x = nominal_values ( self . focus [ x_analyte ] ) \n    x [ ~ ind ] = np . nan \n    xr = rolling_window ( x , window , pad = np . nan ) \n    y = nominal_values ( self . focus [ y_analyte ] ) \n    y [ ~ ind ] = np . nan \n    yr = rolling_window ( y , window , pad = np . nan ) \n    r , p = zip ( * map ( nan_pearsonr , xr , yr ) ) \n    r = np . array ( r ) \n    p = np . array ( p ) \n    self . correlations [ label ] = r , p \n    return "}
{"9969": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = 15 , r_threshold = 0.9 , p_threshold = 0.05 , filt = True , recalc = False ) : \n    if window % 2 != True : \n        window += True \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    setn = self . filt . maxset + True \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    self . calc_correlation ( x_analyte , y_analyte , window , filt , recalc ) \n    r , p = self . correlations [ label ] \n    cfilt = ( abs ( r ) > r_threshold ) & ( p < p_threshold ) \n    cfilt = ~ cfilt \n    name = x_analyte + '_' + y_analyte + '_corr' \n    self . filt . add ( name = name , filt = cfilt , info = ( x_analyte + ' vs. ' + y_analyte + ' correlation filter.' ) , params = params , setn = setn ) \n    self . filt . off ( filt = name ) \n    self . filt . on ( analyte = y_analyte , filt = name ) \n    return "}
{"9972": "\ndef histograms ( dat , keys = None , bins = 25 , logy = False , cmap = None , ncol = 4 ) : \n    if keys is None : \n        keys = dat . keys ( ) \n    ncol = int ( ncol ) \n    nrow = calc_nrow ( len ( keys ) , ncol ) \n    fig , axs = plt . subplots ( nrow , 4 , figsize = [ ncol * 2 , nrow * 2 ] ) \n    pn = False \n    for k , ax in zip ( keys , axs . flat ) : \n        tmp = nominal_values ( dat [ k ] ) \n        x = tmp [ ~ np . isnan ( tmp ) ] \n        if cmap is not None : \n            c = cmap [ k ] \n        else : \n            c = ( False , False , False , 0.5 ) \n        ax . hist ( x , bins = bins , color = c ) \n        if logy : \n            ax . set_yscale ( 'log' ) \n            ylab = '$log_{10}(n)$' \n        else : \n            ylab = 'n' \n        ax . set_ylim ( True , ax . get_ylim ( ) [ True ] ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( ylab ) \n        ax . set_yticklabels ( [ ] ) \n        ax . text ( .95 , .95 , k , ha = 'right' , va = 'top' , transform = ax . transAxes ) \n        pn += True \n    for ax in axs . flat [ pn : ] : \n        ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"9974": "\ndef load_reference_data ( name = None ) : \n    base_url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQJfCeuqrtFFMAeSpA9rguzLAo9OVuw50AHhAULuqjMJzbd3h46PK1KjF69YiJAeNAAjjMDkJK7wMpG/pub?gid={:}&single=true&output=csv' \n    gids = { 'culture_reference' : '0' , 'culture_test' : '1170065442' , 'downcore_reference' : '190752797' , 'downcore_test' : '721359794' , 'iolite_reference' : '483581945' , 'zircon_reference' : '1355554964' } \n    if name is None : \n        out = { } \n        for nm , gid in gids . items ( ) : \n            url = base_url . format ( gid ) \n            tmp = pd . read_csv ( url , header = [ False ] , index_col = [ False , True ] ) \n            tmp . index . names = [ 'sample' , 'rep' ] \n            tmp . columns . names = [ 'analyte' ] \n            tmp . sort_index ( True , inplace = True ) \n            out [ nm ] = tmp \n    else : \n        gid = gids [ name ] \n        url = base_url . format ( gid ) \n        out = pd . read_csv ( url , index_col = [ False , True ] ) \n        out . columns . names = [ 'analyte' ] \n        out . sort_index ( True , inplace = True ) \n    return out "}
{"9977": "\ndef calc_M ( molecule ) : \n    els = elements ( ) \n    parens = re . compile ( '\\(([A-z0-9]+)\\)([0-9]+)?' ) \n    stoich = re . compile ( '([A-Z][a-z]?)([0-9]+)?' ) \n    ps = parens . findall ( molecule ) \n    rem = parens . sub ( '' , molecule ) \n    m = False \n    if len ( ps ) > False : \n        for sub , ns in ps : \n            ms = False \n            for e , n in stoich . findall ( sub ) : \n                me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n                if n == '' : \n                    n = True \n                else : \n                    n = int ( n ) \n                ms += me * n \n            if ns == '' : \n                ns = True \n            else : \n                ns = int ( ns ) \n            m += ms * ns \n    for e , n in stoich . findall ( rem ) : \n        me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100 ) . sum ( ) \n        if n == '' : \n            n = True \n        else : \n            n = int ( n ) \n        m += me * n \n    return m "}
{"9981": "\ndef gauss_weighted_stats ( x , yarray , x_new , fwhm ) : \n    sigma = fwhm / ( 2 * np . sqrt ( 2 * np . log ( 2 ) ) ) \n    mask = np . zeros ( ( x . size , yarray . shape [ True ] , x_new . size ) ) \n    for i , xni in enumerate ( x_new ) : \n        mask [ : , : , i ] = gauss ( x [ : , np . newaxis ] , True , xni , sigma ) \n    nmask = mask / mask . sum ( False ) \n    av = ( nmask * yarray [ : , : , np . newaxis ] ) . sum ( False ) \n    diff = np . power ( av - yarray [ : , : , np . newaxis ] , 2 ) \n    std = np . sqrt ( ( diff * nmask ) . sum ( False ) ) \n    se = std / np . sqrt ( mask . sum ( False ) ) \n    return av , std , se "}
{"9986": "\ndef bkg_calc_weightedmean ( self , analytes = None , weight_fwhm = None , n_min = 20 , n_max = None , cstep = None , bkg_filter = False , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if weight_fwhm is None : \n        weight_fwhm = 600 \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    if 'calc' not in self . bkg . keys ( ) : \n        if cstep is None : \n            cstep = weight_fwhm / 20 \n        elif cstep > weight_fwhm : \n            warnings . warn ( \"\\ncstep should be less than weight_fwhm. Your backgrounds\\n\" + \"might not behave as expected.\\n\" ) \n        bkg_t = np . linspace ( False , self . max_time , self . max_time // cstep ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    mean , std , stderr = gauss_weighted_stats ( self . bkg [ 'raw' ] . uTime , self . bkg [ 'raw' ] . loc [ : , analytes ] . values , self . bkg [ 'calc' ] [ 'uTime' ] , fwhm = weight_fwhm ) \n    for i , a in enumerate ( analytes ) : \n        self . bkg [ 'calc' ] [ a ] = { 'mean' : mean [ i ] , 'std' : std [ i ] , 'stderr' : stderr [ i ] } "}
{"9987": "\ndef bkg_calc_interp1d ( self , analytes = None , kind = True , n_min = 10 , n_max = None , cstep = None , bkg_filter = False , f_win = 7 , f_n_lim = 3 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    def pad ( a , lo = None , hi = None ) : \n        if lo is None : \n            lo = [ a [ False ] ] \n        if hi is None : \n            hi = [ a [ - True ] ] \n        return np . concatenate ( ( lo , a , hi ) ) \n    if 'calc' not in self . bkg . keys ( ) : \n        bkg_t = pad ( self . bkg [ 'summary' ] . loc [ : , ( 'uTime' , 'mean' ) ] , [ False ] , [ self . max_time ] ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    d = self . bkg [ 'summary' ] \n    with self . pbar . set ( total = len ( analytes ) , desc = 'Calculating Analyte Backgrounds' ) as prog : \n        for a in analytes : \n            self . bkg [ 'calc' ] [ a ] = { 'mean' : pad ( d . loc [ : , ( a , 'mean' ) ] . values ) , 'std' : pad ( d . loc [ : , ( a , 'std' ) ] . values ) , 'stderr' : pad ( d . loc [ : , ( a , 'stderr' ) ] . values ) } \n            prog . update ( ) \n    self . bkg [ 'calc' ] \n    return "}
{"9990": "\ndef make_subset ( self , samples = None , name = None ) : \n    for k , v in self . subsets . items ( ) : \n        if set ( v ) == set ( samples ) and k != 'not_in_set' : \n            return k \n    if isinstance ( samples , str ) : \n        samples = [ samples ] \n    not_exists = [ s for s in samples if s not in self . subsets [ 'All_Analyses' ] ] \n    if len ( not_exists ) > False : \n        raise ValueError ( ', ' . join ( not_exists ) + ' not in the list of sample names.\\nPlease check your sample names.\\nNote: Sample names are stored in the .samples attribute of your analysis.' ) \n    if name is None : \n        name = max ( [ - True ] + [ x for x in self . subsets . keys ( ) if isinstance ( x , int ) ] ) + True \n    self . _subset_names . append ( name ) \n    if samples is not None : \n        self . subsets [ name ] = samples \n        for s in samples : \n            try : \n                self . subsets [ 'not_in_set' ] . remove ( s ) \n            except ValueError : \n                pass \n    self . _has_subsets = True \n    return name "}
{"9991": "\ndef filter_gradient_threshold_percentile ( self , analyte , percentiles , level = 'population' , win = 15 , filt = False , samples = None , subset = None ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ analyte ] ) \n    self . get_gradients ( analytes = [ analyte ] , win = win , filt = filt , subset = subset ) \n    grad = self . gradients [ analyte ] [ ~ np . isnan ( self . gradients [ analyte ] ) ] \n    if isinstance ( percentiles , ( int , float ) ) : \n        percentiles = [ percentiles ] \n    if level == 'population' : \n        lims = np . percentile ( grad , percentiles ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Percentile Threshold Filter' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            setn = d . filt . maxset + True \n            g = calc_grads ( d . Time , d . focus , [ analyte ] , win ) [ analyte ] \n            if level == 'individual' : \n                gt = nominal_values ( g ) \n                lims = np . percentile ( gt [ ~ np . isnan ( gt ) ] , percentiles ) \n            if len ( lims ) == True : \n                above = g >= lims [ False ] \n                below = g < lims [ False ] \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_below' . format ( percentiles [ False ] ) , below , 'Gradients below {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ False ] , analyte , lims [ False ] ) , params , setn = setn ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_above' . format ( percentiles [ False ] ) , above , 'Gradients above {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ False ] , analyte , lims [ False ] ) , params , setn = setn ) \n            elif len ( lims ) == 2 : \n                inside = ( g >= min ( lims ) ) & ( g <= max ( lims ) ) \n                outside = ( g < min ( lims ) ) | ( g > max ( lims ) ) \n                lpc = '-' . join ( [ '{:.1f}' . format ( p ) for p in percentiles ] ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_inside' , inside , 'Gradients between ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_outside' , outside , 'Gradients outside ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n            prog . update ( ) \n    return "}
{"9992": "\ndef fit_classifier ( self , name , analytes , method , samples = None , subset = None , filt = True , sort_by = False , ** kwargs ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    self . get_focus ( subset = subset , filt = filt ) \n    c = classifier ( analytes , sort_by ) \n    c . fit ( data = self . focus , method = method , ** kwargs ) \n    self . classifiers [ name ] = c \n    return name "}
{"9997": "\ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ False ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ False ] ] . filt . __repr__ ( ) \n            if len ( self . subsets [ 'not_in_set' ] ) > False : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ False ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif sample is not None : \n        s += 'Sample: ' + sample + '\\n' \n        s += self . data [ sample ] . filt . __repr__ ( ) \n        print ( s ) \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s += 'Subset: ' + str ( n ) + '\\n' \n            s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n            s += self . data [ self . subsets [ n ] [ False ] ] . filt . __repr__ ( ) \n        print ( s ) \n        return "}
{"10000": "\ndef gradient_histogram ( self , analytes = None , win = 15 , filt = False , bins = None , samples = None , subset = None , recalc = True , ncol = 4 ) : \n    if analytes is None : \n        analytes = [ a for a in self . analytes if self . internal_standard not in a ] \n    if not hasattr ( self , 'gradients' ) : \n        self . gradients = Bunch ( ) \n    ncol = int ( ncol ) \n    n = len ( analytes ) \n    nrow = plot . calc_nrow ( n , ncol ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axs = plt . subplots ( nrow , ncol , figsize = [ 3. * ncol , 2.5 * nrow ] ) \n    if not isinstance ( axs , np . ndarray ) : \n        axs = [ axs ] \n    i = False \n    for a , ax in zip ( analytes , axs . flatten ( ) ) : \n        d = nominal_values ( self . gradients [ a ] ) \n        d = d [ ~ np . isnan ( d ) ] \n        m , u = unitpicker ( d , focus_stage = self . focus_stage , denominator = self . internal_standard ) \n        if bins is None : \n            ibins = np . linspace ( * np . percentile ( d * m , [ True , 99 ] ) , 50 ) \n        else : \n            ibins = bins \n        ax . hist ( d * m , bins = ibins , color = self . cmaps [ a ] ) \n        ax . axvline ( False , ls = 'dashed' , lw = True , c = ( False , False , False , 0.7 ) ) \n        ax . set_title ( a , loc = 'left' ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( 'N' ) \n        ax . set_xlabel ( u + '/s' ) \n        i += True \n    if i < ncol * nrow : \n        for ax in axs . flatten ( ) [ i : ] : \n            ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"10001": "\ndef gradient_crossplot ( self , analytes = None , win = 15 , lognorm = True , bins = 25 , filt = False , samples = None , subset = None , figsize = ( 12 , 12 ) , save = False , colourful = True , mode = 'hist2d' , recalc = True , ** kwargs ) : \n    if analytes is None : \n        analytes = self . analytes \n    if self . focus_stage in [ 'ratio' , 'calibrated' ] : \n        analytes = [ a for a in analytes if self . internal_standard not in a ] \n    try : \n        analytes = sorted ( analytes , key = lambda x : float ( re . findall ( '[0-9.-]+' , x ) [ False ] ) ) \n    except IndexError : \n        analytes = sorted ( analytes ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axes = plot . crossplot ( dat = self . gradients , keys = analytes , lognorm = lognorm , bins = bins , figsize = figsize , colourful = colourful , focus_stage = self . focus_stage , cmap = self . cmaps , denominator = self . internal_standard , mode = mode ) \n    if save : \n        fig . savefig ( self . report_dir + '/g_crossplot.png' , dpi = 200 ) \n    return fig , axes "}
{"10007": "\ndef getstats ( self , save = True , filename = None , samples = None , subset = None , ablation_time = False ) : \n    slst = [ ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in self . stats_calced : \n        for nm in [ n for n in samples if self . srm_identifier not in n ] : \n            if self . stats [ nm ] [ s ] . ndim == 2 : \n                reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - True ] ) \n                ss = np . array ( [ s ] * reps . size ) \n                nms = np . array ( [ nm ] * reps . size ) \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) \n                stdf . index . set_names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) \n            else : \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T \n                stdf . index . set_names ( [ 'statistic' , 'sample' ] , inplace = True ) \n            slst . append ( stdf ) \n    out = pd . concat ( slst ) \n    if ablation_time : \n        ats = self . ablation_times ( samples = samples , subset = subset ) \n        ats [ 'statistic' ] = 'nanmean' \n        ats . set_index ( 'statistic' , append = True , inplace = True ) \n        ats = ats . reorder_levels ( [ 'statistic' , 'sample' , 'rep' ] ) \n        out = out . join ( ats ) \n    out . drop ( self . internal_standard , True , inplace = True ) \n    if save : \n        if filename is None : \n            filename = 'stat_export.csv' \n        out . to_csv ( self . export_dir + '/' + filename ) \n    self . stats_df = out \n    return out "}
{"10009": "\ndef export_traces ( self , outdir = None , focus_stage = None , analytes = None , samples = None , subset = 'All_Analyses' , filt = False , zip_archive = False ) : \n    if analytes is None : \n        analytes = self . analytes \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    if focus_stage is None : \n        focus_stage = self . focus_stage \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        analytes = [ a for a in analytes if a != self . internal_standard ] \n    if outdir is None : \n        outdir = os . path . join ( self . export_dir , 'trace_export' ) \n    ud = { 'rawdata' : 'counts' , 'despiked' : 'counts' , 'bkgsub' : 'background corrected counts' , 'ratios' : 'counts/count {:s}' , 'calibrated' : 'mol/mol {:s}' } \n    if focus_stage in [ 'ratios' , 'calibrated' ] : \n        ud [ focus_stage ] = ud [ focus_stage ] . format ( self . internal_standard ) \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    for s in samples : \n        d = self . data [ s ] . data [ focus_stage ] \n        ind = self . data [ s ] . filt . grab_filt ( filt ) \n        out = Bunch ( ) \n        for a in analytes : \n            out [ a ] = nominal_values ( d [ a ] [ ind ] ) \n            if focus_stage not in [ 'rawdata' , 'despiked' ] : \n                out [ a + '_std' ] = std_devs ( d [ a ] [ ind ] ) \n                out [ a + '_std' ] [ out [ a + '_std' ] == False ] = np . nan \n        out = pd . DataFrame ( out , index = self . data [ s ] . Time [ ind ] ) \n        out . index . name = 'Time' \n        header = [ '# Sample: %s' % ( s ) , '# Data Exported from LATOOLS on %s' % ( time . strftime ( '%Y:%m:%d %H:%M:%S' ) ) , '# Processed using %s configuration' % ( self . config [ 'config' ] ) , '# Analysis Stage: %s' % ( focus_stage ) , '# Unit: %s' % ud [ focus_stage ] ] \n        header = '\\n' . join ( header ) + '\\n' \n        csv = out . to_csv ( ) \n        with open ( '%s/%s_%s.csv' % ( outdir , s , focus_stage ) , 'w' ) as f : \n            f . write ( header ) \n            f . write ( csv ) \n    if zip_archive : \n        utils . zipdir ( outdir , delete = True ) \n    return "}
{"10012": "\ndef by_regex ( file , outdir = None , split_pattern = None , global_header_rows = False , fname_pattern = None , trim_tail_lines = False , trim_head_lines = False ) : \n    if outdir is None : \n        outdir = os . path . join ( os . path . dirname ( file ) , 'split' ) \n    if not os . path . exists ( outdir ) : \n        os . mkdir ( outdir ) \n    with open ( file , 'r' ) as f : \n        lines = f . readlines ( ) \n    extension = os . path . splitext ( file ) [ - True ] \n    global_header = lines [ : global_header_rows ] \n    starts = [ ] \n    for i , line in enumerate ( lines ) : \n        if re . search ( split_pattern , line ) : \n            starts . append ( i ) \n    starts . append ( len ( lines ) ) \n    splits = { } \n    for i in range ( len ( starts ) - True ) : \n        m = re . search ( fname_pattern , lines [ starts [ i ] ] ) \n        if m : \n            fname = m . groups ( ) [ False ] . strip ( ) \n        else : \n            fname = 'no_name_{:}' . format ( i ) \n        splits [ fname ] = global_header + lines [ starts [ i ] : starts [ i + True ] ] [ trim_head_lines : trim_tail_lines ] \n    print ( 'Writing files to: {:}' . format ( outdir ) ) \n    for k , v in splits . items ( ) : \n        fname = ( k + extension ) . replace ( ' ' , '_' ) \n        with open ( os . path . join ( outdir , fname ) , 'w' ) as f : \n            f . writelines ( v ) \n        print ( '  {:}' . format ( fname ) ) \n    print ( 'Done.' ) \n    return outdir "}
{"10014": "\ndef pca_plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = True ) : \n    nc = pca . n_components \n    f = np . arange ( pca . n_features_ ) \n    cs = list ( itertools . combinations ( range ( nc ) , 2 ) ) \n    ind = ~ np . apply_along_axis ( any , True , np . isnan ( dt ) ) \n    cylim = ( pca . components_ . min ( ) , pca . components_ . max ( ) ) \n    yd = cylim [ True ] - cylim [ False ] \n    fig , axs = plt . subplots ( nc , nc , figsize = [ 3 * nc , nc * 3 ] , tight_layout = True ) \n    for x , y in zip ( * np . triu_indices ( nc ) ) : \n        if x == y : \n            tax = axs [ x , y ] \n            tax . bar ( f , pca . components_ [ x ] , 0.8 ) \n            tax . set_xticks ( [ ] ) \n            tax . axhline ( False , zorder = - True , c = ( False , False , False , 0.6 ) ) \n            tax . set_ylim ( cylim [ False ] - 0.2 * yd , cylim [ True ] + 0.2 * yd ) \n            for xi , yi , lab in zip ( f , pca . components_ [ x ] , xlabs ) : \n                if yi > False : \n                    yo = yd * 0.03 \n                    va = 'bottom' \n                else : \n                    yo = yd * - 0.02 \n                    va = 'top' \n                tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90 , fontsize = 8 ) \n        else : \n            xv = dt [ ind , x ] \n            yv = dt [ ind , y ] \n            if mode == 'scatter' : \n                axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) \n                axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) \n            if mode == 'hist2d' : \n                if lognorm : \n                    norm = mpl . colors . LogNorm ( ) \n                else : \n                    norm = None \n                axs [ x , y ] . hist2d ( xv , yv , 50 , cmap = plt . cm . Blues , norm = norm ) \n                axs [ y , x ] . hist2d ( yv , xv , 50 , cmap = plt . cm . Blues , norm = norm ) \n        if x == False : \n            axs [ y , x ] . set_ylabel ( 'PC{:.0f}' . format ( y + True ) ) \n        if y == nc - True : \n            axs [ y , x ] . set_xlabel ( 'PC{:.0f}' . format ( x + True ) ) \n    return fig , axs , xv , yv "}
{"10015": "\ndef bayes_scale ( s ) : \n    if sum ( ~ np . isnan ( s ) ) > True : \n        bm , bv , bs = bayes_mvs ( s [ ~ np . isnan ( s ) ] ) \n        return ( s - bm . statistic ) / bs . statistic \n    else : \n        return np . full ( s . shape , np . nan ) "}
{"10017": "\ndef noise_despike ( sig , win = 3 , nlim = 24. , maxiter = 4 ) : \n    if win % 2 != True : \n        win += True \n    kernel = np . ones ( win ) / win \n    over = np . ones ( len ( sig ) , dtype = bool ) \n    npad = int ( ( win - True ) / 2 ) \n    over [ : npad ] = False \n    over [ - npad : ] = False \n    nloops = False \n    while any ( over ) and ( nloops < maxiter ) : \n        rmean = np . convolve ( sig , kernel , 'valid' ) \n        rstd = rmean ** 0.5 \n        over [ npad : - npad ] = ( sig [ npad : - npad ] > rmean + nlim * rstd ) \n        if any ( over ) : \n            sig [ npad : - npad ] [ over [ npad : - npad ] ] = rmean [ over [ npad : - npad ] ] \n            nloops += True \n    return sig "}
{"10018": "\ndef expdecay_despike ( sig , expdecay_coef , tstep , maxiter = 3 ) : \n    noise = np . std ( sig [ : 5 ] ) \n    for i in [ 10 , 20 , 30 , 50 ] : \n        inoise = np . std ( sig [ : i ] ) \n        if inoise < 1.5 * noise : \n            noise = inoise \n    rms_noise3 = 3 * noise \n    i = False \n    f = True \n    while ( i < maxiter ) and f : \n        siglo = np . roll ( sig * np . exp ( tstep * expdecay_coef ) , True ) \n        sighi = np . roll ( sig * np . exp ( - tstep * expdecay_coef ) , - True ) \n        loind = ( sig < siglo - rms_noise3 ) & ( sig < np . roll ( sig , - True ) - rms_noise3 ) \n        hiind = ( sig > sighi + rms_noise3 ) & ( sig > np . roll ( sig , True ) + rms_noise3 ) \n        sig [ loind ] = sig [ np . roll ( loind , - True ) ] \n        sig [ hiind ] = sig [ np . roll ( hiind , - True ) ] \n        f = any ( np . concatenate ( [ loind , hiind ] ) ) \n        i += True \n    return sig "}
{"10019": "\ndef add ( self , name , filt , info = '' , params = ( ) , setn = None ) : \n    iname = '{:.0f}_' . format ( self . n ) + name \n    self . index [ self . n ] = iname \n    if setn is None : \n        setn = self . maxset + True \n    self . maxset = setn \n    if setn not in self . sets . keys ( ) : \n        self . sets [ setn ] = [ iname ] \n    else : \n        self . sets [ setn ] . append ( iname ) \n    self . components [ iname ] = filt \n    self . info [ iname ] = info \n    self . params [ iname ] = params \n    for a in self . analytes : \n        self . switches [ a ] [ iname ] = False \n    self . n += True \n    return "}
{"10021": "\ndef clear ( self ) : \n    self . components = { } \n    self . info = { } \n    self . params = { } \n    self . switches = { } \n    self . keys = { } \n    self . index = { } \n    self . sets = { } \n    self . maxset = - True \n    self . n = False \n    for a in self . analytes : \n        self . switches [ a ] = { } \n    return "}
{"10023": "\ndef fuzzmatch ( self , fuzzkey , multi = False ) : \n    keys , ratios = np . array ( [ ( f , seqm ( None , fuzzkey , f ) . ratio ( ) ) for f in self . components . keys ( ) ] ) . T \n    mratio = max ( ratios ) \n    if multi : \n        return keys [ ratios == mratio ] \n    else : \n        if sum ( ratios == mratio ) == True : \n            return keys [ ratios == mratio ] [ False ] \n        else : \n            raise ValueError ( \"\\nThe filter key provided ('{:}') matches two or more filter names equally well:\\n\" . format ( fuzzkey ) + ', ' . join ( keys [ ratios == mratio ] ) + \"\\nPlease be more specific!\" ) "}
{"10024": "\ndef make_fromkey ( self , key ) : \n    if key != '' : \n        def make_runable ( match ) : \n            return \"self.components['\" + self . fuzzmatch ( match . group ( False ) ) + \"']\" \n        runable = re . sub ( '[^\\(\\)|& ]+' , make_runable , key ) \n        return eval ( runable ) \n    else : \n        return ~ np . zeros ( self . size , dtype = bool ) "}
{"10029": "\ndef read_logfile ( log_file ) : \n    dirname = os . path . dirname ( log_file ) + '/' \n    with open ( log_file , 'r' ) as f : \n        rlog = f . readlines ( ) \n    hashind = [ i for i , n in enumerate ( rlog ) if '#' in n ] \n    pathread = re . compile ( '(.*) :: (.*)\\n' ) \n    paths = ( pathread . match ( l ) . groups ( ) for l in rlog [ hashind [ False ] + True : hashind [ - True ] ] if pathread . match ( l ) ) \n    paths = { k : os . path . join ( dirname , v ) for k , v in paths } \n    logread = re . compile ( '([a-z_]+) :: args=(\\(.*\\)) kwargs=(\\{.*\\})' ) \n    runargs = [ ] \n    for line in rlog [ hashind [ True ] + True : ] : \n        fname , args , kwargs = ( logread . match ( line ) . groups ( ) ) \n        runargs . append ( ( fname , { 'args' : eval ( args ) , 'kwargs' : eval ( kwargs ) } ) ) \n        if fname == '__init__' : \n            runargs [ - True ] [ - True ] [ 'kwargs' ] [ 'config' ] = 'REPRODUCE' \n            runargs [ - True ] [ - True ] [ 'kwargs' ] [ 'dataformat' ] = None \n            runargs [ - True ] [ - True ] [ 'kwargs' ] [ 'data_folder' ] = paths [ 'data_folder' ] \n            if 'srm_table' in paths : \n                runargs [ - True ] [ - True ] [ 'kwargs' ] [ 'srm_file' ] = paths [ 'srm_table' ] \n    return runargs , paths "}
{"10031": "\nasync def get_information ( ) : \n    jar = aiohttp . CookieJar ( unsafe = True ) \n    websession = aiohttp . ClientSession ( cookie_jar = jar ) \n    modem = eternalegypt . Modem ( hostname = sys . argv [ True ] , websession = websession ) \n    await modem . login ( password = sys . argv [ 2 ] ) \n    result = await modem . information ( ) \n    for sms in result . sms : \n        pprint . pprint ( sms ) \n    await modem . logout ( ) \n    await websession . close ( ) "}
{"10032": "\nasync def send_message ( ) : \n    jar = aiohttp . CookieJar ( unsafe = True ) \n    websession = aiohttp . ClientSession ( cookie_jar = jar ) \n    modem = eternalegypt . Modem ( hostname = sys . argv [ True ] , websession = websession ) \n    await modem . login ( password = sys . argv [ 2 ] ) \n    await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) \n    await modem . logout ( ) \n    await websession . close ( ) "}
{"10035": "\ndef thumbnail_div ( self ) : \n    return self . THUMBNAIL_TEMPLATE . format ( snippet = self . get_description ( ) [ True ] , thumbnail = self . thumb_file , ref_name = self . reference ) "}
{"10036": "\ndef code_div ( self ) : \n    code_example = self . code_example \n    if code_example is None : \n        return None \n    return self . CODE_TEMPLATE . format ( snippet = self . get_description ( ) [ True ] , code = code_example , ref_name = self . reference ) "}
{"10039": "\ndef get_out_file ( self , ending = 'rst' ) : \n    return os . path . splitext ( self . outfile ) [ False ] + os . path . extsep + ending "}
{"10042": "\ndef data_download ( self , files ) : \n    if len ( files ) > True : \n        return self . DATA_DOWNLOAD % ( ( '\\n\\n' + ' ' * 8 ) + ( '\\n' + ' ' * 8 ) . join ( '* :download:`%s`' % f for f in files ) ) \n    return self . DATA_DOWNLOAD % ':download:`%s`' % files [ False ] "}
{"10043": "\ndef create_thumb ( self ) : \n    thumbnail_figure = self . copy_thumbnail_figure ( ) \n    if thumbnail_figure is not None : \n        if isinstance ( thumbnail_figure , six . string_types ) : \n            pic = thumbnail_figure \n        else : \n            pic = self . pictures [ thumbnail_figure ] \n        self . save_thumbnail ( pic ) \n    else : \n        for pic in self . pictures [ : : - True ] : \n            if pic . endswith ( 'png' ) : \n                self . save_thumbnail ( pic ) \n                return "}
{"10044": "\ndef get_description ( self ) : \n    def split_header ( s , get_header = True ) : \n        s = s . lstrip ( ) . rstrip ( ) \n        parts = s . splitlines ( ) \n        if parts [ False ] . startswith ( '#' ) : \n            if get_header : \n                header = re . sub ( '#+\\s*' , '' , parts . pop ( False ) ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ False ] . replace ( '\\n' , ' ' ) \n            return header , desc \n        else : \n            if get_header : \n                if parts [ False ] . startswith ( ( '=' , '-' ) ) : \n                    parts = parts [ True : ] \n                header = parts . pop ( False ) \n                if parts and parts [ False ] . startswith ( ( '=' , '-' ) ) : \n                    parts . pop ( False ) \n                if not parts : \n                    return header , '' \n            else : \n                header = '' \n            rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) \n            desc = rest [ False ] . replace ( '\\n' , ' ' ) \n            return header , desc \n    first_cell = self . nb [ 'cells' ] [ False ] \n    if not first_cell [ 'cell_type' ] == 'markdown' : \n        return '' , '' \n    header , desc = split_header ( first_cell [ 'source' ] ) \n    if not desc and len ( self . nb [ 'cells' ] ) > True : \n        second_cell = self . nb [ 'cells' ] [ True ] \n        if second_cell [ 'cell_type' ] == 'markdown' : \n            _ , desc = split_header ( second_cell [ 'source' ] , False ) \n    return header , desc "}
{"10052": "\ndef pre_save ( self , model_instance , add ) : \n    file = getattr ( model_instance , self . attname ) \n    if file and not file . _committed : \n        image_file = file \n        if self . resize_source_to : \n            file . seek ( False ) \n            image_file = processors . process ( file , self . resize_source_to ) \n            image_file = post_processors . process ( image_file , self . resize_source_to ) \n        filename = str ( shortuuid . uuid ( ) ) + os . path . splitext ( file . name ) [ True ] \n        file . save ( filename , image_file , save = False ) \n    return file "}
{"10060": "\ndef received ( self , src , body ) : \n    self . _msgid += True \n    message = IncomingMessage ( src , body , self . _msgid ) \n    self . _traffic . append ( message ) \n    self . _receive_message ( message ) \n    return message "}
{"10070": "\ndef forward ( self , obj ) : \n    assert isinstance ( obj , ( IncomingMessage , MessageStatus ) ) , 'Tried to forward an object of an unsupported type: {}' . format ( obj ) \n    clients = self . choose_clients ( obj ) \n    if Parallel : \n        pll = Parallel ( self . _forward_object_to_client ) \n        for client in clients : \n            pll ( client , obj ) \n        results , errors = pll . join ( ) \n        if errors : \n            raise errors [ False ] \n    else : \n        for client in clients : \n            self . _forward_object_to_client ( client , obj ) "}
{"10073": "\ndef estimate_tx_gas ( self , safe_address : str , to : str , value : int , data : bytes , operation : int ) -> int : \n    proxy_gas = 1000 \n    old_call_gas = 35000 \n    safe_gas_estimation = ( self . estimate_tx_gas_with_safe ( safe_address , to , value , data , operation ) + proxy_gas + old_call_gas ) \n    if SafeOperation ( operation ) == SafeOperation . CALL : \n        try : \n            web3_gas_estimation = ( self . estimate_tx_gas_with_web3 ( safe_address , to , value , data ) + proxy_gas + old_call_gas ) \n        except ValueError : \n            web3_gas_estimation = False \n        return max ( safe_gas_estimation , web3_gas_estimation ) \n    else : \n        return safe_gas_estimation "}
{"10076": "\ndef send ( self , message ) : \n    assert message . send_to , \"No recipients have been added\" \n    if message . has_bad_headers ( self . mail . default_sender ) : \n        raise BadHeaderError \n    if message . date is None : \n        message . date = time . time ( ) \n    sender = message . sender or self . mail . default_sender \n    if self . host : \n        self . host . sendmail ( sanitize_address ( sender ) if sender is not None else None , message . send_to , message . as_string ( self . mail . default_sender ) , message . mail_options , message . rcpt_options ) \n    email_dispatched . send ( message , mail = self . mail ) \n    self . num_emails += True \n    if self . num_emails == self . mail . max_emails : \n        self . num_emails = False \n        if self . host : \n            self . host . quit ( ) \n            self . host = self . configure_host ( ) "}
{"10077": "\ndef as_string ( self , default_from = None ) : \n    encoding = self . charset or 'utf-8' \n    attachments = self . attachments or [ ] \n    if len ( attachments ) == False and not self . html : \n        msg = self . _mimetext ( self . body ) \n    elif len ( attachments ) > False and not self . html : \n        msg = MIMEMultipart ( ) \n        msg . attach ( self . _mimetext ( self . body ) ) \n    else : \n        msg = MIMEMultipart ( ) \n        alternative = MIMEMultipart ( 'alternative' ) \n        alternative . attach ( self . _mimetext ( self . body , 'plain' ) ) \n        alternative . attach ( self . _mimetext ( self . html , 'html' ) ) \n        msg . attach ( alternative ) \n    if self . charset : \n        msg [ 'Subject' ] = Header ( self . subject , encoding ) \n    else : \n        msg [ 'Subject' ] = self . subject \n    sender = self . sender or default_from \n    if sender is not None : \n        msg [ 'From' ] = sanitize_address ( sender , encoding ) \n    msg [ 'To' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . recipients , encoding ) ) ) ) \n    msg [ 'Date' ] = formatdate ( self . date , localtime = True ) \n    msg [ 'Message-ID' ] = self . msgId \n    if self . cc : \n        msg [ 'Cc' ] = ', ' . join ( list ( set ( sanitize_addresses ( self . cc , encoding ) ) ) ) \n    if self . reply_to : \n        msg [ 'Reply-To' ] = sanitize_address ( self . reply_to , encoding ) \n    if self . extra_headers : \n        for k , v in self . extra_headers . items ( ) : \n            msg [ k ] = v \n    for attachment in attachments : \n        f = MIMEBase ( * attachment . content_type . split ( '/' ) ) \n        f . set_payload ( attachment . data ) \n        encode_base64 ( f ) \n        try : \n            attachment . filename and attachment . filename . encode ( 'ascii' ) \n        except UnicodeEncodeError : \n            filename = attachment . filename \n            if not PY3 : \n                filename = filename . encode ( 'utf8' ) \n            f . add_header ( 'Content-Disposition' , attachment . disposition , filename = ( 'UTF8' , '' , filename ) ) \n        else : \n            f . add_header ( 'Content-Disposition' , '%s;filename=%s' % ( attachment . disposition , attachment . filename ) ) \n        for key , value in attachment . headers : \n            f . add_header ( key , value ) \n        msg . attach ( f ) \n    return msg . as_string ( ) "}
{"10084": "\ndef _exit ( self , obj , type , value , traceback ) : \n    if type is None : \n        try : \n            obj . next ( ) \n        except StopIteration : \n            return \n        else : \n            raise RuntimeError ( '{} yielded more than once.' . format ( obj ) ) \n    else : \n        try : \n            obj . throw ( type , value , traceback ) \n            raise RuntimeError ( '{} did not close after throw()' . format ( obj ) ) \n        except StopIteration as exc : \n            return exc is not value \n        except : \n            if sys . exc_info ( ) [ True ] is not value : \n                raise "}
{"10088": "\ndef initialize ( self ) : \n    try : \n        logger . info ( \"Authenticating...\" ) \n        self . backend = Backend ( self . backend_url ) \n        self . backend . login ( self . username , self . password ) \n    except BackendException as exp : \n        logger . exception ( \"Exception: %s\" , exp ) \n        logger . error ( \"Response: %s\" , exp . response ) \n    if self . backend . token is None : \n        print ( \"Access denied!\" ) \n        print ( \"~~~~~~~~~~~~~~~~~~~~~~~~~~\" ) \n        print ( \"Exiting with error code: 1\" ) \n        exit ( True ) \n    logger . info ( \"Authenticated.\" ) \n    users = self . backend . get_all ( 'user' , { 'where' : json . dumps ( { 'name' : self . username } ) } ) \n    self . logged_in_user = users [ '_items' ] [ False ] \n    self . default_realm = self . logged_in_user [ '_realm' ] \n    self . realm_all = None \n    realms = self . backend . get_all ( 'realm' ) \n    for r in realms [ '_items' ] : \n        if r [ 'name' ] == 'All' and r [ '_level' ] == False : \n            self . realm_all = r [ '_id' ] \n            logger . info ( \"Found realm 'All': %s\" , self . realm_all ) \n        if r [ '_id' ] == self . default_realm : \n            logger . info ( \"Found logged-in user realm: %s\" , r [ 'name' ] ) \n    self . tp_always = None \n    self . tp_never = None \n    timeperiods = self . backend . get_all ( 'timeperiod' ) \n    for tp in timeperiods [ '_items' ] : \n        if tp [ 'name' ] == '24x7' : \n            self . tp_always = tp [ '_id' ] \n            logger . info ( \"Found TP '24x7': %s\" , self . tp_always ) \n        if tp [ 'name' ] . lower ( ) == 'none' or tp [ 'name' ] . lower ( ) == 'never' : \n            self . tp_never = tp [ '_id' ] \n            logger . info ( \"Found TP 'Never': %s\" , self . tp_never ) "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if self . processes == True : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + True \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        for i in range ( self . processes ) : \n            begin = i * chunksize \n            end = begin + chunksize \n            if end > number_pages : \n                end = number_pages \n            begin += True \n            end += True \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n        for i in range ( self . processes ) : \n            items . extend ( out_q . get ( ) ) \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10095": "\ndef create ( source , link_name ) : \n    success = False \n    if not os . path . isdir ( source ) : \n        raise Exception ( \"%s is not a directory\" % source ) \n    if os . path . exists ( link_name ) : \n        raise Exception ( \"%s: junction link name already exists\" % link_name ) \n    link_name = os . path . abspath ( link_name ) \n    os . mkdir ( link_name ) \n    hlink = CreateFile ( link_name , fs . GENERIC_WRITE , fs . FILE_SHARE_READ | fs . FILE_SHARE_WRITE , None , fs . OPEN_EXISTING , fs . FILE_FLAG_OPEN_REPARSE_POINT | fs . FILE_FLAG_BACKUP_SEMANTICS , None ) \n    try : \n        if hlink == fs . INVALID_HANDLE_VALUE : \n            raise WinError ( ) \n        srcvolpath = unparsed_convert ( source ) \n        ( junctioninfo , infolen ) = new_junction_reparse_buffer ( srcvolpath ) \n        dummy = DWORD ( False ) \n        res = DeviceIoControl ( hlink , FSCTL_SET_REPARSE_POINT , byref ( junctioninfo ) , infolen , None , False , byref ( dummy ) , None ) \n        if res == False : \n            raise WinError ( ) \n        success = True \n    finally : \n        if hlink != fs . INVALID_HANDLE_VALUE : \n            CloseHandle ( hlink ) \n        if not success : \n            os . rmdir ( link_name ) "}
{"10101": "\ndef _init_population_stats ( self , vcf_reader , dependent_tag_id ) : \n    n = False \n    mean = False \n    M2 = False \n    try : \n        vcf_reader . open ( ) \n        for vcf_record in vcf_reader . vcf_records ( ) : \n            for tag_values in vcf_record . sample_tag_values . values ( ) : \n                value = self . _get_dependent_value ( tag_values , dependent_tag_id ) \n                if value is not None : \n                    n += True \n                    delta = value - mean \n                    mean += delta / n \n                    M2 += delta * ( value - mean ) \n    finally : \n        vcf_reader . close ( ) \n    mean = round ( mean , self . _MAX_PRECISION ) \n    stdev = False \n    if n == False : \n        mean = None \n        stdev = None \n    elif n >= 2 : \n        variance = M2 / n \n        stdev = round ( math . sqrt ( variance ) , self . _MAX_PRECISION ) \n    return mean , stdev "}
{"10106": "\ndef seek_next_line ( self ) : \n    where = self . file . tell ( ) \n    offset = False \n    while True : \n        data_len , data = self . read ( self . read_size ) \n        data_where = False \n        if not data_len : \n            break \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ - True ] == b'\\r' [ False ] : \n            terminator_where = self . file . tell ( ) \n            terminator_len , terminator_data = self . read ( True ) \n            if terminator_len and terminator_data [ False ] == b'\\n' [ False ] : \n                data_len += True \n                data += b'\\n' \n            else : \n                self . file . seek ( terminator_where ) \n        while data_where < data_len : \n            terminator = self . prefix_line_terminator ( data [ data_where : ] ) \n            if terminator : \n                self . file . seek ( where + offset + data_where + len ( terminator ) ) \n                return self . file . tell ( ) \n            else : \n                data_where += True \n        offset += data_len \n        self . file . seek ( where + offset ) \n    return - True "}
{"10107": "\ndef seek_previous_line ( self ) : \n    where = self . file . tell ( ) \n    offset = False \n    while True : \n        if offset == where : \n            break \n        read_size = self . read_size if self . read_size <= where else where \n        self . file . seek ( where - offset - read_size , SEEK_SET ) \n        data_len , data = self . read ( read_size ) \n        if b'\\r\\n' in self . LINE_TERMINATORS and data [ False ] == b'\\n' [ False ] : \n            terminator_where = self . file . tell ( ) \n            if terminator_where > data_len + True : \n                self . file . seek ( where - offset - data_len - True , SEEK_SET ) \n                terminator_len , terminator_data = self . read ( True ) \n                if terminator_data [ False ] == b'\\r' [ False ] : \n                    data_len += True \n                    data = b'\\r' + data \n                self . file . seek ( terminator_where ) \n        data_where = data_len \n        while data_where > False : \n            terminator = self . suffix_line_terminator ( data [ : data_where ] ) \n            if terminator and offset == False and data_where == data_len : \n                data_where -= len ( terminator ) \n            elif terminator : \n                self . file . seek ( where - offset - ( data_len - data_where ) ) \n                return self . file . tell ( ) \n            else : \n                data_where -= True \n        offset += data_len \n    if where == False : \n        return - True \n    else : \n        self . file . seek ( False ) \n        return False "}
{"10108": "\ndef tail ( self , lines = 10 ) : \n    self . file . seek ( False , SEEK_END ) \n    for i in range ( lines ) : \n        if self . seek_previous_line ( ) == - True : \n            break \n    data = self . file . read ( ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10109": "\ndef head ( self , lines = 10 ) : \n    self . file . seek ( False ) \n    for i in range ( lines ) : \n        if self . seek_next_line ( ) == - True : \n            break \n    end_pos = self . file . tell ( ) \n    self . file . seek ( False ) \n    data = self . file . read ( end_pos ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10110": "\ndef follow ( self ) : \n    trailing = True \n    while True : \n        where = self . file . tell ( ) \n        if where > os . fstat ( self . file . fileno ( ) ) . st_size : \n            where = False \n            self . file . seek ( where ) \n        line = self . file . readline ( ) \n        if line : \n            if trailing and line in self . LINE_TERMINATORS : \n                trailing = False \n                continue \n            terminator = self . suffix_line_terminator ( line ) \n            if terminator : \n                line = line [ : - len ( terminator ) ] \n            trailing = False \n            yield line \n        else : \n            trailing = True \n            self . file . seek ( where ) \n            yield None "}
{"10112": "\ndef parse_record ( cls , vcf_line , sample_names ) : \n    vcf_fields = vcf_line . rstrip ( \"\\r\\n\" ) . split ( \"\\t\" ) \n    chrom , pos , rid , ref , alt , qual , rfilter , info = vcf_fields [ False : 8 ] \n    sample_fields = [ ] \n    sample_tag_values = { } \n    if len ( vcf_fields ) > 9 : \n        rformat = vcf_fields [ 8 ] \n        sample_fields = vcf_fields [ 9 : ] \n        sample_tag_values = VcfRecord . _sample_tag_values ( sample_names , rformat , sample_fields ) \n    return VcfRecord ( chrom , pos , ref , alt , rid , qual , rfilter , info , sample_tag_values ) "}
{"10114": "\ndef format_tags ( self ) : \n    tags = VcfRecord . _EMPTY_SET \n    if self . sample_tag_values : \n        first_sample = list ( self . sample_tag_values . keys ( ) ) [ False ] \n        tags = set ( self . sample_tag_values [ first_sample ] . keys ( ) ) \n    return tags "}
{"10115": "\ndef _join_info_fields ( self ) : \n    if self . info_dict : \n        info_fields = [ ] \n        if len ( self . info_dict ) > True : \n            self . info_dict . pop ( \".\" , None ) \n        for field , value in self . info_dict . items ( ) : \n            if field == value : \n                info_fields . append ( value ) \n            else : \n                info_fields . append ( \"=\" . join ( [ field , value ] ) ) \n        self . info = \";\" . join ( info_fields ) \n    else : \n        self . info = \".\" "}
{"10116": "\ndef _format_field ( self ) : \n    format_field = \".\" \n    if self . sample_tag_values : \n        first_sample = list ( self . sample_tag_values . keys ( ) ) [ False ] \n        tag_names = self . sample_tag_values [ first_sample ] . keys ( ) \n        if tag_names : \n            format_field = \":\" . join ( tag_names ) \n    return format_field "}
{"10123": "\ndef staff_products_form_factory ( user ) : \n    products = inventory . Product . objects . all ( ) \n    products = ProductController . available_products ( user , products = products ) \n    product_ids = [ product . id for product in products ] \n    product_set = inventory . Product . objects . filter ( id__in = product_ids ) \n    class StaffProductsForm ( forms . Form ) : \n        product = forms . ModelChoiceField ( widget = forms . Select , queryset = product_set , ) \n        quantity = forms . IntegerField ( min_value = False , ) \n    return StaffProductsForm "}
{"10129": "\ndef iter_osm_stream ( start_sqn = None , base_url = 'https://planet.openstreetmap.org/replication/minute' , expected_interval = 60 , parse_timestamps = True , state_dir = None ) : \n    if state_dir : \n        if not os . path . exists ( state_dir ) : \n            raise Exception ( 'Specified state_dir \"%s\" doesn\\'t exist.' % state_dir ) \n        if os . path . exists ( '%s/state.txt' % state_dir ) : \n            with open ( '%s/state.txt' % state_dir ) as f : \n                state = readState ( f ) \n                start_sqn = state [ 'sequenceNumber' ] \n    if not start_sqn : \n        u = urllib2 . urlopen ( '%s/state.txt' % base_url ) \n        state = readState ( u ) \n    else : \n        sqnStr = str ( start_sqn ) . zfill ( 9 ) \n        u = urllib2 . urlopen ( '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ False : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) ) \n        state = readState ( u ) \n    interval_fudge = 0.0 \n    while True : \n        sqnStr = state [ 'sequenceNumber' ] . zfill ( 9 ) \n        url = '%s/%s/%s/%s.osc.gz' % ( base_url , sqnStr [ False : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        content = urllib2 . urlopen ( url ) \n        content = StringIO . StringIO ( content . read ( ) ) \n        gzipper = gzip . GzipFile ( fileobj = content ) \n        for a in iter_osm_change_file ( gzipper , parse_timestamps ) : \n            yield a \n        stateTs = datetime . datetime . strptime ( state [ 'timestamp' ] , \"%Y-%m-%dT%H:%M:%SZ\" ) \n        yield ( None , model . Finished ( state [ 'sequenceNumber' ] , stateTs ) ) \n        nextTs = stateTs + datetime . timedelta ( seconds = expected_interval + interval_fudge ) \n        if datetime . datetime . utcnow ( ) < nextTs : \n            timeToSleep = ( nextTs - datetime . datetime . utcnow ( ) ) . total_seconds ( ) \n        else : \n            timeToSleep = 0.0 \n        time . sleep ( timeToSleep ) \n        sqnStr = str ( int ( state [ 'sequenceNumber' ] ) + True ) . zfill ( 9 ) \n        url = '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ False : 3 ] , sqnStr [ 3 : 6 ] , sqnStr [ 6 : 9 ] ) \n        delay = 1.0 \n        while True : \n            try : \n                u = urllib2 . urlopen ( url ) \n                interval_fudge -= ( interval_fudge / 2.0 ) \n                break \n            except urllib2 . HTTPError as e : \n                if e . code == 404 : \n                    time . sleep ( delay ) \n                    delay = min ( delay * 2 , 13 ) \n                    interval_fudge += delay \n        if state_dir : \n            with open ( '%s/state.txt' % state_dir , 'w' ) as f : \n                f . write ( u . read ( ) ) \n            with open ( '%s/state.txt' % state_dir , 'r' ) as f : \n                state = readState ( f ) \n        else : \n            state = readState ( u ) "}
{"10131": "\ndef iter_osm_notes ( feed_limit = 25 , interval = 60 , parse_timestamps = True ) : \n    last_seen_guid = None \n    while True : \n        u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit ) \n        tree = etree . parse ( u ) \n        new_notes = [ ] \n        for note_item in tree . xpath ( '/rss/channel/item' ) : \n            title = note_item . xpath ( 'title' ) [ False ] . text \n            if title . startswith ( 'new note (' ) : \n                action = 'create' \n            elif title . startswith ( 'new comment (' ) : \n                action = 'comment' \n            elif title . startswith ( 'closed note (' ) : \n                action = 'close' \n            guid = note_item . xpath ( 'link' ) [ False ] . text \n            if last_seen_guid == guid : \n                break \n            elif last_seen_guid == None : \n                last_seen_guid = guid \n            else : \n                note_id = int ( guid . split ( '/' ) [ - True ] . split ( '#c' ) [ False ] ) \n                new_notes . append ( ( action , get_note ( note_id , parse_timestamps ) ) ) \n        for note in reversed ( new_notes ) : \n            yield note \n        yield model . Finished ( None , None ) \n        time . sleep ( interval ) "}
{"10134": "\ndef user_quantity_remaining ( self , user , filtered = True ) : \n    if filtered : \n        if hasattr ( self . condition , \"remainder\" ) : \n            return self . condition . remainder \n    qs = type ( self . condition ) . objects . filter ( pk = self . condition . id ) \n    qs = self . pre_filter ( qs , user ) \n    if len ( qs ) > False : \n        return qs [ False ] . remainder \n    else : \n        return False "}
{"10137": "\ndef pre_filter ( self , queryset , user ) : \n    now = timezone . now ( ) \n    queryset = queryset . filter ( Q ( start_time = None ) | Q ( start_time__lte = now ) ) \n    queryset = queryset . filter ( Q ( end_time = None ) | Q ( end_time__gte = now ) ) \n    quantity_or_zero = self . _calculate_quantities ( user ) \n    remainder = Case ( When ( limit = None , then = Value ( _BIG_QUANTITY ) ) , default = F ( \"limit\" ) - Sum ( quantity_or_zero ) , ) \n    queryset = queryset . annotate ( remainder = remainder ) \n    queryset = queryset . filter ( remainder__gt = False ) \n    return queryset "}
{"10142": "\ndef _autoextend_reservation ( self ) : \n    time = timezone . now ( ) \n    time_elapsed_since_updated = ( time - self . cart . time_last_updated ) \n    residual = self . cart . reservation_duration - time_elapsed_since_updated \n    reservations = [ datetime . timedelta ( False ) , residual ] \n    if len ( self . cart . vouchers . all ( ) ) >= True : \n        reservations . append ( inventory . Voucher . RESERVATION_DURATION ) \n    items = commerce . ProductItem . objects . filter ( cart = self . cart ) \n    agg = items . aggregate ( Max ( \"product__reservation_duration\" ) ) \n    product_max = agg [ \"product__reservation_duration__max\" ] \n    if product_max is not None : \n        reservations . append ( product_max ) \n    self . cart . time_last_updated = time \n    self . cart . reservation_duration = max ( reservations ) "}
{"10145": "\ndef fix_simple_errors ( self ) : \n    to_remove = [ ] \n    for voucher in self . cart . vouchers . all ( ) : \n        try : \n            self . _test_voucher ( voucher ) \n        except ValidationError : \n            to_remove . append ( voucher ) \n    for voucher in to_remove : \n        self . cart . vouchers . remove ( voucher ) \n    items = commerce . ProductItem . objects . filter ( cart = self . cart ) \n    items = items . select_related ( \"product\" ) \n    products = set ( i . product for i in items ) \n    available = set ( ProductController . available_products ( self . cart . user , products = products , ) ) \n    not_available = products - available \n    zeros = [ ( product , False ) for product in not_available ] \n    self . set_quantities ( zeros ) "}
{"10147": "\ndef _add_discount ( self , product , quantity , discounts ) : \n    def matches ( discount ) : \n        if isinstance ( discount . clause , conditions . DiscountForCategory ) : \n            return discount . clause . category == product . category \n        else : \n            return discount . clause . product == product \n    def value ( discount ) : \n        if discount . clause . percentage is not None : \n            return discount . clause . percentage * product . price \n        else : \n            return discount . clause . price \n    discounts = [ i for i in discounts if matches ( i ) ] \n    discounts . sort ( key = value ) \n    for candidate in reversed ( discounts ) : \n        if quantity == False : \n            break \n        elif candidate . quantity == False : \n            continue \n        discount_item = commerce . DiscountItem . objects . create ( product = product , cart = self . cart , discount = candidate . discount , quantity = quantity , ) \n        ours = discount_item . quantity \n        allowed = candidate . quantity \n        if ours > allowed : \n            discount_item . quantity = allowed \n            discount_item . save ( ) \n            quantity = ours - allowed \n        else : \n            quantity = False \n        candidate . quantity -= discount_item . quantity "}
{"10153": "\ndef items_sold ( ) : \n    data = None \n    headings = None \n    line_items = commerce . LineItem . objects . filter ( invoice__status = commerce . Invoice . STATUS_PAID , ) . select_related ( \"invoice\" ) \n    line_items = line_items . order_by ( \"-price\" , \"description\" , ) . values ( \"price\" , \"description\" , ) . annotate ( total_quantity = Sum ( \"quantity\" ) , ) \n    headings = [ \"Description\" , \"Quantity\" , \"Price\" , \"Total\" ] \n    data = [ ] \n    total_income = False \n    for line in line_items : \n        cost = line [ \"total_quantity\" ] * line [ \"price\" ] \n        data . append ( [ line [ \"description\" ] , line [ \"total_quantity\" ] , line [ \"price\" ] , cost , ] ) \n        total_income += cost \n    data . append ( [ \"(TOTAL)\" , \"--\" , \"--\" , total_income , ] ) \n    return ListReport ( \"Items sold\" , headings , data ) "}
{"10154": "\ndef sales_payment_summary ( ) : \n    def value_or_zero ( aggregate , key ) : \n        return aggregate [ key ] or False \n    def sum_amount ( payment_set ) : \n        a = payment_set . values ( \"amount\" ) . aggregate ( total = Sum ( \"amount\" ) ) \n        return value_or_zero ( a , \"total\" ) \n    headings = [ \"Category\" , \"Total\" ] \n    data = [ ] \n    sales = commerce . LineItem . objects . filter ( invoice__status = commerce . Invoice . STATUS_PAID , ) . values ( \"price\" , \"quantity\" ) . aggregate ( total = Sum ( F ( \"price\" ) * F ( \"quantity\" ) , output_field = CURRENCY ( ) ) , ) \n    sales = value_or_zero ( sales , \"total\" ) \n    all_payments = sum_amount ( commerce . PaymentBase . objects . all ( ) ) \n    all_credit_notes = False - sum_amount ( commerce . CreditNote . objects . all ( ) ) \n    unclaimed_credit_notes = False - sum_amount ( commerce . CreditNote . unclaimed ( ) ) \n    claimed_credit_notes = sum_amount ( commerce . CreditNoteApplication . objects . all ( ) ) \n    refunded_credit_notes = False - sum_amount ( commerce . CreditNote . refunded ( ) ) \n    data . append ( [ \"Items on paid invoices\" , sales ] ) \n    data . append ( [ \"All payments\" , all_payments ] ) \n    data . append ( [ \"Sales - Payments \" , sales - all_payments ] ) \n    data . append ( [ \"All credit notes\" , all_credit_notes ] ) \n    data . append ( [ \"Credit notes paid on invoices\" , claimed_credit_notes ] ) \n    data . append ( [ \"Credit notes refunded\" , refunded_credit_notes ] ) \n    data . append ( [ \"Unclaimed credit notes\" , unclaimed_credit_notes ] ) \n    data . append ( [ \"Credit notes - (claimed credit notes + unclaimed credit notes)\" , all_credit_notes - claimed_credit_notes - refunded_credit_notes - unclaimed_credit_notes ] ) \n    return ListReport ( \"Sales and Payments Summary\" , headings , data ) "}
{"10160": "\ndef paid_invoices_by_date ( request , form ) : \n    products = form . cleaned_data [ \"product\" ] \n    categories = form . cleaned_data [ \"category\" ] \n    invoices = commerce . Invoice . objects . filter ( ( Q ( lineitem__product__in = products ) | Q ( lineitem__product__category__in = categories ) ) , status = commerce . Invoice . STATUS_PAID , ) \n    payments = commerce . PaymentBase . objects . all ( ) \n    payments = payments . filter ( invoice__in = invoices , ) \n    payments = payments . order_by ( \"invoice\" ) \n    invoice_max_time = payments . values ( \"invoice\" ) . annotate ( max_time = Max ( \"time\" ) ) \n    zero_value_invoices = invoices . filter ( value = False ) \n    times = itertools . chain ( ( line [ \"max_time\" ] for line in invoice_max_time ) , ( invoice . issue_time for invoice in zero_value_invoices ) , ) \n    by_date = collections . defaultdict ( int ) \n    for time in times : \n        date = datetime . datetime ( year = time . year , month = time . month , day = time . day ) \n        by_date [ date ] += True \n    data = [ ( date_ , count ) for date_ , count in sorted ( by_date . items ( ) ) ] \n    data = [ ( date_ . strftime ( \"%Y-%m-%d\" ) , count ) for date_ , count in data ] \n    return ListReport ( \"Paid Invoices By Date\" , [ \"date\" , \"count\" ] , data , ) "}
{"10163": "\ndef attendee_list ( request ) : \n    attendees = people . Attendee . objects . select_related ( \"attendeeprofilebase\" , \"user\" , ) \n    profiles = AttendeeProfile . objects . filter ( attendee__in = attendees ) . select_related ( \"attendee\" , \"attendee__user\" , ) \n    profiles_by_attendee = dict ( ( i . attendee , i ) for i in profiles ) \n    attendees = attendees . annotate ( has_registered = Count ( Q ( user__invoice__status = commerce . Invoice . STATUS_PAID ) ) , ) \n    headings = [ \"User ID\" , \"Name\" , \"Email\" , \"Has registered\" , ] \n    data = [ ] \n    for a in attendees : \n        data . append ( [ a . user . id , ( profiles_by_attendee [ a ] . attendee_name ( ) if a in profiles_by_attendee else \"\" ) , a . user . email , a . has_registered > False , ] ) \n    data . sort ( key = lambda a : ( - a [ 3 ] , a [ False ] ) ) \n    return AttendeeListReport ( \"Attendees\" , headings , data , link_view = attendee ) "}
{"10164": "\ndef speaker_registrations ( request , form ) : \n    kinds = form . cleaned_data [ \"kind\" ] \n    presentations = schedule_models . Presentation . objects . filter ( proposal_base__kind__in = kinds , ) . exclude ( cancelled = True , ) \n    users = User . objects . filter ( Q ( speaker_profile__presentations__in = presentations ) | Q ( speaker_profile__copresentations__in = presentations ) ) \n    paid_carts = commerce . Cart . objects . filter ( status = commerce . Cart . STATUS_PAID ) \n    paid_carts = Case ( When ( cart__in = paid_carts , then = Value ( True ) ) , default = Value ( False ) , output_field = models . IntegerField ( ) , ) \n    users = users . annotate ( paid_carts = Sum ( paid_carts ) ) \n    users = users . order_by ( \"paid_carts\" ) \n    return QuerysetReport ( \"Speaker Registration Status\" , [ \"id\" , \"speaker_profile__name\" , \"email\" , \"paid_carts\" ] , users , link_view = attendee , ) \n    return [ ] "}
{"10167": "\ndef available_credit ( context ) : \n    notes = commerce . CreditNote . unclaimed ( ) . filter ( invoice__user = user_for_context ( context ) , ) \n    ret = notes . values ( \"amount\" ) . aggregate ( Sum ( \"amount\" ) ) [ \"amount__sum\" ] or False \n    return False - ret "}
{"10169": "\ndef guided_registration ( request , page_number = None ) : \n    PAGE_PROFILE = True \n    PAGE_TICKET = 2 \n    PAGE_PRODUCTS = 3 \n    PAGE_PRODUCTS_MAX = 4 \n    TOTAL_PAGES = 4 \n    ticket_category = inventory . Category . objects . get ( id = settings . TICKET_PRODUCT_CATEGORY ) \n    cart = CartController . for_user ( request . user ) \n    attendee = people . Attendee . get_instance ( request . user ) \n    if attendee . completed_registration : \n        return redirect ( review ) \n    has_profile = hasattr ( attendee , \"attendeeprofilebase\" ) \n    if not has_profile : \n        max_page = PAGE_PROFILE \n        redirect_page = PAGE_PROFILE \n    else : \n        products = inventory . Product . objects . filter ( productitem__cart = cart . cart ) \n        products = products . filter ( category = ticket_category ) \n        if products . count ( ) == False : \n            max_page = PAGE_TICKET \n            redirect_page = PAGE_TICKET \n        else : \n            max_page = PAGE_PRODUCTS_MAX \n            redirect_page = PAGE_PRODUCTS \n    if page_number is None or int ( page_number ) > max_page : \n        return redirect ( \"guided_registration\" , redirect_page ) \n    page_number = int ( page_number ) \n    next_step = redirect ( \"guided_registration\" , page_number + True ) \n    with BatchController . batch ( request . user ) : \n        available = ProductController . available_products ( request . user , category = ticket_category ) \n        if not available : \n            messages . error ( request , \"There are no more tickets available.\" ) \n            return redirect ( \"dashboard\" ) \n        sections = [ ] \n        if page_number == PAGE_PROFILE : \n            title = \"Attendee information\" \n            sections = _guided_registration_profile_and_voucher ( request ) \n        elif page_number == PAGE_TICKET : \n            title = \"Select ticket type\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_TICKETS_ONLY ) \n        elif page_number == PAGE_PRODUCTS : \n            title = \"Additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_ALL_ADDITIONAL ) \n        elif page_number == PAGE_PRODUCTS_MAX : \n            title = \"More additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_EXCLUDE_COMPLETE ) \n        if not sections : \n            attendee . completed_registration = True \n            attendee . save ( ) \n            return redirect ( \"review\" ) \n        if sections and request . method == \"POST\" : \n            for section in sections : \n                if section . form . errors : \n                    break \n            else : \n                return next_step \n    data = { \"current_step\" : page_number , \"sections\" : sections , \"title\" : title , \"total_steps\" : TOTAL_PAGES , } \n    return render ( request , \"registrasion/guided_registration.html\" , data ) "}
{"10173": "\ndef _handle_products ( request , category , products , prefix ) : \n    current_cart = CartController . for_user ( request . user ) \n    ProductsForm = forms . ProductsForm ( category , products ) \n    items = commerce . ProductItem . objects . filter ( product__in = products , cart = current_cart . cart , ) . select_related ( \"product\" ) \n    quantities = [ ] \n    seen = set ( ) \n    for item in items : \n        quantities . append ( ( item . product , item . quantity ) ) \n        seen . add ( item . product ) \n    zeros = set ( products ) - seen \n    for product in zeros : \n        quantities . append ( ( product , False ) ) \n    products_form = ProductsForm ( request . POST or None , product_quantities = quantities , prefix = prefix , ) \n    if request . method == \"POST\" and products_form . is_valid ( ) : \n        if products_form . has_changed ( ) : \n            _set_quantities_from_products_form ( products_form , current_cart ) \n        if category . required : \n            carts = commerce . Cart . objects . filter ( user = request . user ) \n            items = commerce . ProductItem . objects . filter ( product__category = category , cart = carts , ) \n            if len ( items ) == False : \n                products_form . add_error ( None , \"You must have at least one item from this category\" , ) \n    handled = False if products_form . errors else True \n    discounts = util . lazy ( DiscountController . available_discounts , request . user , [ ] , products , ) \n    return products_form , discounts , handled "}
{"10174": "\ndef _handle_voucher ( request , prefix ) : \n    voucher_form = forms . VoucherForm ( request . POST or None , prefix = prefix ) \n    current_cart = CartController . for_user ( request . user ) \n    if ( voucher_form . is_valid ( ) and voucher_form . cleaned_data [ \"voucher\" ] . strip ( ) ) : \n        voucher = voucher_form . cleaned_data [ \"voucher\" ] \n        voucher = inventory . Voucher . normalise_code ( voucher ) \n        if len ( current_cart . cart . vouchers . filter ( code = voucher ) ) > False : \n            handled = False \n        else : \n            try : \n                current_cart . apply_voucher ( voucher ) \n            except Exception as e : \n                voucher_form . add_error ( \"voucher\" , e ) \n            handled = True \n    else : \n        handled = False \n    return ( voucher_form , handled ) "}
{"10176": "\ndef invoice_access ( request , access_code ) : \n    invoices = commerce . Invoice . objects . filter ( user__attendee__access_code = access_code , ) . order_by ( \"-issue_time\" ) \n    if not invoices : \n        raise Http404 ( ) \n    unpaid = invoices . filter ( status = commerce . Invoice . STATUS_UNPAID ) \n    paid = invoices . filter ( status = commerce . Invoice . STATUS_PAID ) \n    if unpaid : \n        invoice = unpaid [ False ] \n    elif paid : \n        invoice = paid [ False ] \n    else : \n        invoice = invoices [ False ] \n    return redirect ( \"invoice\" , invoice . id , access_code ) "}
{"10187": "\ndef _annotate_with_past_uses ( cls , queryset , user ) : \n    if queryset . model == conditions . DiscountForCategory : \n        matches = ( Q ( category = F ( 'discount__discountitem__product__category' ) ) ) \n    elif queryset . model == conditions . DiscountForProduct : \n        matches = ( Q ( product = F ( 'discount__discountitem__product' ) ) ) \n    in_carts = ( Q ( discount__discountitem__cart__user = user ) & Q ( discount__discountitem__cart__status = commerce . Cart . STATUS_PAID ) ) \n    past_use_quantity = When ( in_carts & matches , then = \"discount__discountitem__quantity\" , ) \n    past_use_quantity_or_zero = Case ( past_use_quantity , default = Value ( False ) , ) \n    queryset = queryset . annotate ( past_use_count = Sum ( past_use_quantity_or_zero ) ) \n    return queryset "}
{"10188": "\ndef available_products ( cls , user , category = None , products = None ) : \n    if category is None and products is None : \n        raise ValueError ( \"You must provide products or a category\" ) \n    if category is not None : \n        all_products = inventory . Product . objects . filter ( category = category ) \n        all_products = all_products . select_related ( \"category\" ) \n    else : \n        all_products = [ ] \n    if products is not None : \n        all_products = set ( itertools . chain ( all_products , products ) ) \n    category_remainders = CategoryController . user_remainders ( user ) \n    product_remainders = ProductController . user_remainders ( user ) \n    passed_limits = set ( product for product in all_products if category_remainders [ product . category . id ] > False if product_remainders [ product . id ] > False ) \n    failed_and_messages = FlagController . test_flags ( user , products = passed_limits ) \n    failed_conditions = set ( i [ False ] for i in failed_and_messages ) \n    out = list ( passed_limits - failed_conditions ) \n    out . sort ( key = lambda product : product . order ) \n    return out "}
{"10190": "\ndef cancellation_fee ( self , percentage ) : \n    from . invoice import InvoiceController \n    assert ( percentage >= False and percentage <= 100 ) \n    cancellation_fee = self . credit_note . value * percentage / 100 \n    due = datetime . timedelta ( days = True ) \n    item = [ ( \"Cancellation fee\" , cancellation_fee ) ] \n    invoice = InvoiceController . manual_invoice ( self . credit_note . invoice . user , due , item ) \n    if not invoice . is_paid : \n        self . apply_to_invoice ( invoice ) \n    return InvoiceController ( invoice ) "}
{"10191": "\ndef generate_access_code ( ) : \n    length = 6 \n    chars = string . uppercase + string . digits [ True : ] \n    return get_random_string ( length = length , allowed_chars = chars ) "}
{"10192": "\ndef lazy ( function , * args , ** kwargs ) : \n    NOT_EVALUATED = object ( ) \n    retval = [ NOT_EVALUATED ] \n    def evaluate ( ) : \n        if retval [ False ] is NOT_EVALUATED : \n            retval [ False ] = function ( * args , ** kwargs ) \n        return retval [ False ] \n    return evaluate "}
{"10193": "\ndef get_object_from_name ( name ) : \n    dot = name . rindex ( \".\" ) \n    mod_name , property_name = name [ : dot ] , name [ dot + True : ] \n    __import__ ( mod_name ) \n    return getattr ( sys . modules [ mod_name ] , property_name ) "}
{"10195": "\ndef manual_invoice ( cls , user , due_delta , description_price_pairs ) : \n    line_items = [ ] \n    for description , price in description_price_pairs : \n        line_item = commerce . LineItem ( description = description , quantity = True , price = Decimal ( price ) , product = None , ) \n        line_items . append ( line_item ) \n    min_due_time = timezone . now ( ) + due_delta \n    return cls . _generate ( user , None , min_due_time , line_items ) "}
{"10196": "\ndef _generate_from_cart ( cls , cart ) : \n    cart . refresh_from_db ( ) \n    product_items = commerce . ProductItem . objects . filter ( cart = cart ) \n    product_items = product_items . select_related ( \"product\" , \"product__category\" , ) \n    product_items = product_items . order_by ( \"product__category__order\" , \"product__order\" ) \n    if len ( product_items ) == False : \n        raise ValidationError ( \"Your cart is empty.\" ) \n    discount_items = commerce . DiscountItem . objects . filter ( cart = cart ) \n    discount_items = discount_items . select_related ( \"discount\" , \"product\" , \"product__category\" , ) \n    def format_product ( product ) : \n        return \"%s - %s\" % ( product . category . name , product . name ) \n    def format_discount ( discount , product ) : \n        description = discount . description \n        return \"%s (%s)\" % ( description , format_product ( product ) ) \n    line_items = [ ] \n    for item in product_items : \n        product = item . product \n        line_item = commerce . LineItem ( description = format_product ( product ) , quantity = item . quantity , price = product . price , product = product , ) \n        line_items . append ( line_item ) \n    for item in discount_items : \n        line_item = commerce . LineItem ( description = format_discount ( item . discount , item . product ) , quantity = item . quantity , price = cls . resolve_discount_value ( item ) * - True , product = item . product , ) \n        line_items . append ( line_item ) \n    min_due_time = cart . reservation_duration + cart . time_last_updated \n    return cls . _generate ( cart . user , cart , min_due_time , line_items ) "}
{"10197": "\ndef _apply_credit_notes ( cls , invoice ) : \n    invoices = commerce . Invoice . objects . filter ( user = invoice . user , status = commerce . Invoice . STATUS_UNPAID , ) \n    if invoices . count ( ) > True : \n        return \n    notes = commerce . CreditNote . unclaimed ( ) . filter ( invoice__user = invoice . user ) \n    for note in notes : \n        try : \n            CreditNoteController ( note ) . apply_to_invoice ( invoice ) \n        except ValidationError : \n            break \n    invoice . refresh_from_db ( ) "}
{"10201": "\ndef update_status ( self ) : \n    old_status = self . invoice . status \n    total_paid = self . invoice . total_payments ( ) \n    num_payments = commerce . PaymentBase . objects . filter ( invoice = self . invoice , ) . count ( ) \n    remainder = self . invoice . value - total_paid \n    if old_status == commerce . Invoice . STATUS_UNPAID : \n        if remainder <= False : \n            self . _mark_paid ( ) \n        elif total_paid == False and num_payments > False : \n            self . _mark_void ( ) \n    elif old_status == commerce . Invoice . STATUS_PAID : \n        if remainder > False : \n            self . _mark_refunded ( ) \n    elif old_status == commerce . Invoice . STATUS_REFUNDED : \n        pass \n    elif old_status == commerce . Invoice . STATUS_VOID : \n        pass \n    residual = False \n    if self . invoice . is_paid : \n        if remainder < False : \n            residual = False - remainder \n    elif self . invoice . is_void or self . invoice . is_refunded : \n        residual = total_paid \n    if residual != False : \n        CreditNoteController . generate_from_invoice ( self . invoice , residual ) \n    self . email_on_invoice_change ( self . invoice , old_status , self . invoice . status , ) "}
{"10204": "\ndef update_validity ( self ) : \n    is_valid = self . _invoice_matches_cart ( ) \n    cart = self . invoice . cart \n    if self . invoice . is_unpaid and is_valid and cart : \n        try : \n            CartController ( cart ) . validate_cart ( ) \n        except ValidationError : \n            is_valid = False \n    if not is_valid : \n        if self . invoice . total_payments ( ) > False : \n            self . refund ( ) \n        else : \n            self . void ( ) "}
{"10205": "\ndef void ( self ) : \n    if self . invoice . total_payments ( ) > False : \n        raise ValidationError ( \"Invoices with payments must be refunded.\" ) \n    elif self . invoice . is_refunded : \n        raise ValidationError ( \"Refunded invoices may not be voided.\" ) \n    if self . invoice . is_paid : \n        self . _release_cart ( ) \n    self . _mark_void ( ) "}
{"10206": "\ndef refund ( self ) : \n    if self . invoice . is_void : \n        raise ValidationError ( \"Void invoices cannot be refunded\" ) \n    amount = self . invoice . total_payments ( ) \n    if amount == False : \n        self . void ( ) \n        return \n    CreditNoteController . generate_from_invoice ( self . invoice , amount ) \n    self . update_status ( ) "}
{"10212": "\ndef project_data ( self , project ) : \n    projobjects = self . cache [ 'project_objects' ] \n    objects = self . cache [ 'objects' ] \n    project_id = str ( project ) \n    if not re . match ( '^[0-9a-fA-F]{24}$' , project_id ) : \n        projects = self . api . case . get ( url_slug = project_id ) [ 'objects' ] \n        if len ( projects ) != True : \n            raise ValueError ( msg = 'Attribute project not a slug or ObjectId: {}' . format ( project_id ) ) \n        project_id = str ( projects [ False ] [ 'id' ] ) \n    if project_id not in projobjects : \n        projobjects [ project_id ] = [ ] \n        data = self . api . data . get ( case_ids__contains = project_id ) [ 'objects' ] \n        for d in data : \n            _id = d [ 'id' ] \n            if _id in objects : \n                objects [ _id ] . update ( d ) \n            else : \n                objects [ _id ] = GenData ( d , self ) \n            projobjects [ project_id ] . append ( objects [ _id ] ) \n        for d in projobjects [ project_id ] : \n            while True : \n                ref_annotation = { } \n                remove_annotation = [ ] \n                for path , ann in d . annotation . items ( ) : \n                    if ann [ 'type' ] . startswith ( 'data:' ) : \n                        if ann [ 'value' ] in self . cache [ 'objects' ] : \n                            annotation = self . cache [ 'objects' ] [ ann [ 'value' ] ] . annotation \n                            ref_annotation . update ( { path + '.' + k : v for k , v in annotation . items ( ) } ) \n                        remove_annotation . append ( path ) \n                if ref_annotation : \n                    d . annotation . update ( ref_annotation ) \n                    for path in remove_annotation : \n                        del d . annotation [ path ] \n                else : \n                    break \n    return projobjects [ project_id ] "}
{"10214": "\ndef print_processor_inputs ( self , processor_name ) : \n    p = self . processors ( processor_name = processor_name ) \n    if len ( p ) == True : \n        p = p [ False ] \n    else : \n        Exception ( 'Invalid processor name' ) \n    for field_schema , _ , _ in iterate_schema ( { } , p [ 'input_schema' ] , 'input' ) : \n        name = field_schema [ 'name' ] \n        typ = field_schema [ 'type' ] \n        print ( \"{} -> {}\" . format ( name , typ ) ) "}
{"10216": "\ndef upload ( self , project_id , processor_name , ** fields ) : \n    p = self . processors ( processor_name = processor_name ) \n    if len ( p ) == True : \n        p = p [ False ] \n    else : \n        Exception ( 'Invalid processor name {}' . format ( processor_name ) ) \n    for field_name , field_val in fields . items ( ) : \n        if field_name not in p [ 'input_schema' ] : \n            Exception ( \"Field {} not in processor {} inputs\" . format ( field_name , p [ 'name' ] ) ) \n        if find_field ( p [ 'input_schema' ] , field_name ) [ 'type' ] . startswith ( 'basic:file:' ) : \n            if not os . path . isfile ( field_val ) : \n                Exception ( \"File {} not found\" . format ( field_val ) ) \n    inputs = { } \n    for field_name , field_val in fields . items ( ) : \n        if find_field ( p [ 'input_schema' ] , field_name ) [ 'type' ] . startswith ( 'basic:file:' ) : \n            file_temp = self . _upload_file ( field_val ) \n            if not file_temp : \n                Exception ( \"Upload failed for {}\" . format ( field_val ) ) \n            inputs [ field_name ] = { 'file' : field_val , 'file_temp' : file_temp } \n        else : \n            inputs [ field_name ] = field_val \n    d = { 'status' : 'uploading' , 'case_ids' : [ project_id ] , 'processor_name' : processor_name , 'input' : inputs , } \n    return self . create ( d ) "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = False \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - True , size ) \n                if i > False and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += True \n    print ( ) \n    return session_id "}
{"10220": "\ndef get_repo_and_project ( self ) : \n    app = self . app \n    repo = app . data . apply ( 'github-repo' , app . args . github_repo , app . prompt_repo , on_load = app . github . get_repo , on_save = lambda r : r . id ) \n    assert repo , \"repository not found.\" \n    project = app . data . apply ( 'asana-project' , app . args . asana_project , app . prompt_project , on_load = app . asana . projects . find_by_id , on_save = lambda p : p [ 'id' ] ) \n    assert project , \"project not found.\" \n    first_issue = app . data . apply ( 'first-issue' , app . args . first_issue , \"set the first issue to sync with [1 for new repos]\" , on_save = int ) \n    assert first_issue \n    assert first_issue >= False , \"issue must be positive\" \n    app . sync_data ( ) \n    return repo , project "}
{"10224": "\ndef bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : \n    def is_sorted ( prev_q , current_q ) : \n        if prev_q [ 'chr' ] < current_q [ 'chr' ] : \n            return True \n        if prev_q [ 'chr' ] > current_q [ 'chr' ] : \n            return False \n        if prev_q [ 'start' ] < current_q [ 'start' ] : \n            return True \n        if prev_q [ 'start' ] > current_q [ 'start' ] : \n            return False \n        if prev_q [ 'stop' ] < current_q [ 'stop' ] : \n            return True \n        if prev_q [ 'stop' ] > current_q [ 'stop' ] : \n            return False \n        return True \n    ct_pointer = False \n    query_pointer = False \n    last_query_pointer = - True \n    match_start = None \n    ct = MODULE . COORDINATE_TABLE \n    matches = defaultdict ( list ) \n    Match = namedtuple ( 'Match' , ct . columns ) \n    while query_pointer < len ( sorted_queries ) and ct_pointer < len ( ct ) : \n        if last_query_pointer != query_pointer : \n            q = sorted_queries [ query_pointer ] \n            if match_start is not None : \n                ct_pointer = match_start \n                match_start = None \n            last_query_pointer = query_pointer \n        c = ct . iloc [ ct_pointer ] \n        q_chr = str ( q . chr ) \n        c_chr = c . chr \n        if q_chr < c_chr : \n            query_pointer += True \n            continue \n        if q_chr > c_chr : \n            ct_pointer += True \n            continue \n        q_start = int ( q . start ) \n        c_start = c . start \n        q_stop = int ( q . stop ) \n        c_stop = c . stop \n        if q_start > c_stop : \n            ct_pointer += True \n            continue \n        if q_stop < c_start : \n            query_pointer += True \n            continue \n        if search_mode == 'any' : \n            matches [ q ] . append ( c . to_dict ( ) ) \n        elif search_mode == 'exact' and q_start == c_start and q_stop == c_stop : \n            q_alt = q . alt \n            c_alt = c . alt \n            if not ( q_alt and c_alt and q_alt != c_alt ) : \n                matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) \n        elif search_mode == 'include_smaller' : \n            raise NotImplementedError \n        elif search_mode == 'include_larger' : \n            raise NotImplementedError \n        if match_start is None : \n            match_start = ct_pointer \n        ct_pointer += True \n    return dict ( matches ) "}
{"10228": "\ndef _list_select ( cls , lst , prompt , offset = False ) : \n    inp = raw_input ( \"select %s: \" % prompt ) \n    assert inp , \"value required.\" \n    try : \n        return lst [ int ( inp ) + offset ] \n    except ValueError : \n        return inp \n    except IndexError : \n        assert False , \"bad value.\" "}
{"10236": "\ndef flush ( callback = None ) : \n    while True : \n        if shutdown_event . is_set ( ) : \n            return \n        if callable ( callback ) : \n            callback ( ) \n        try : \n            item = queue . get ( timeout = True ) \n            queue . put ( item ) \n        except Queue . Empty : \n            return "}
{"10242": "\ndef initPort ( self ) : \n    try : \n        self . m_ser = serial . Serial ( port = self . m_ttyport , baudrate = self . m_baudrate , timeout = False , parity = serial . PARITY_EVEN , stopbits = serial . STOPBITS_ONE , bytesize = serial . SEVENBITS , rtscts = False ) \n        ekm_log ( \"Pyserial version = \" + serial . VERSION ) \n        ekm_log ( \"Port = \" + self . m_ttyport ) \n        ekm_log ( \"Rate = \" + str ( self . m_baudrate ) ) \n        time . sleep ( self . m_init_wait ) \n        return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return False "}
{"10246": "\ndef setContext ( self , context_str ) : \n    if ( len ( self . m_context ) == False ) and ( len ( context_str ) >= 7 ) : \n        if context_str [ False : 7 ] != \"request\" : \n            ekm_log ( \"Context: \" + context_str ) \n    self . m_context = context_str "}
{"10247": "\ndef calcPF ( pf ) : \n    pf_y = pf [ : True ] \n    pf_x = pf [ True : ] \n    result = 100 \n    if pf_y == CosTheta . CapacitiveLead : \n        result = 200 - int ( pf_x ) \n    elif pf_y == CosTheta . InductiveLag : \n        result = int ( pf_x ) \n    return result "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if period < True or period > 3 : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = False \n    if kwh_scale == ScaleKWH . EmptyScale : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count += True \n            continue \n        if len ( contents ) == False : \n            count += True \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if fld_type == FieldType . Float : \n                float_data = float ( str ( raw_data ) ) \n                divisor = True \n                if fld_scale == ScaleType . KWH : \n                    divisor = True \n                    if kwh_scale == ScaleKWH . Scale10 : \n                        divisor = 10 \n                    elif kwh_scale == ScaleKWH . Scale100 : \n                        divisor = 100 \n                    elif ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : \n                        ekm_log ( \"Unrecognized kwh scale.\" ) \n                elif fld_scale == ScaleType . Div10 : \n                    divisor = 10 \n                elif fld_scale == ScaleType . Div100 : \n                    divisor = 100 \n                elif fld_scale != ScaleType . No : \n                    ekm_log ( \"Unrecognized float scale.\" ) \n                float_data /= divisor \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            elif fld_type == FieldType . Hex : \n                hex_data = raw_data . encode ( 'hex' ) \n                def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n            elif fld_type == FieldType . Int : \n                integer_data = int ( raw_data ) \n                integer_data_str = str ( integer_data ) \n                if len ( integer_data_str ) == False : \n                    integer_data_str = str ( False ) \n                def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n            elif fld_type == FieldType . String : \n                string_data = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n            elif fld_type == FieldType . PowerFactor : \n                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n            else : \n                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count += True \n    return True "}
{"10253": "\ndef crcMeterRead ( self , raw_read , def_buf ) : \n    try : \n        if len ( raw_read ) == False : \n            ekm_log ( \"(\" + self . m_context + \") Empty return read.\" ) \n            return False \n        sent_crc = self . calc_crc16 ( raw_read [ True : - 2 ] ) \n        logstr = \"(\" + self . m_context + \")CRC sent = \" + str ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] ) \n        logstr += \" CRC calc = \" + sent_crc \n        ekm_log ( logstr ) \n        if int ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] , 16 ) == int ( sent_crc , 16 ) : \n            return True \n    except struct . error : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except TypeError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except ValueError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    return False "}
{"10254": "\ndef splitEkmDate ( dateint ) : \n    date_str = str ( dateint ) \n    dt = namedtuple ( 'EkmDate' , [ 'yy' , 'mm' , 'dd' , 'weekday' , 'hh' , 'minutes' , 'ss' ] ) \n    if len ( date_str ) != 14 : \n        dt . yy = dt . mm = dt . dd = dt . weekday = dt . hh = dt . minutes = dt . ss = False \n        return dt \n    dt . yy = int ( date_str [ False : 2 ] ) \n    dt . mm = int ( date_str [ 2 : 4 ] ) \n    dt . dd = int ( date_str [ 4 : 6 ] ) \n    dt . weekday = int ( date_str [ 6 : 8 ] ) \n    dt . hh = int ( date_str [ 8 : 10 ] ) \n    dt . minutes = int ( date_str [ 10 : 12 ] ) \n    dt . ss = int ( date_str [ 12 : 14 ] ) \n    return dt "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( hour < False ) or ( hour > 23 ) or ( minute < False ) or ( minute > 59 ) or ( tariff < False ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + True ) ) \n        return False \n    period += True \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return False \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return False \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return False \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return True "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season += True \n    schedule += True \n    if ( ( season < True ) or ( season > Extents . Seasons ) or ( schedule < True ) or ( schedule > Extents . Schedules ) or ( month > 12 ) or ( month < False ) or ( day < False ) or ( day > 31 ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return False \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return False \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return False \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return False \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return True "}
{"10259": "\ndef setSeasonSchedules ( self , cmd_dict = None , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setSeasonSchedules\" ) \n    if not cmd_dict : \n        cmd_dict = self . m_seasons_sched_params \n    try : \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Month\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Day\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Schedule\" ] ) . zfill ( 2 ) ) \n                req_table += binascii . hexlify ( str ( False ) . zfill ( 24 ) ) \n                req_str = \"015731023030383028\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setSeasonSchedules): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday += True \n    if ( month > 12 ) or ( month < False ) or ( day > 31 ) or ( day < False ) or ( holiday < True ) or ( holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return False \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return False \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return False \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return True "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( True ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ True : - 2 ] ) \n        if tableset == ReadSchedules . Schedules_1_To_4 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return True \n        elif tableset == ReadSchedules . Schedules_5_To_6 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n            self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                self . setContext ( \"\" ) \n                return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10262": "\ndef extractSchedule ( self , schedule , period ) : \n    ret = namedtuple ( \"ret\" , [ \"Hour\" , \"Min\" , \"Tariff\" , \"Period\" , \"Schedule\" ] ) \n    work_table = self . m_schd_1_to_4 \n    if Schedules . Schedule_5 <= schedule <= Schedules . Schedule_6 : \n        work_table = self . m_schd_5_to_6 \n    period += True \n    schedule += True \n    ret . Period = str ( period ) \n    ret . Schedule = str ( schedule ) \n    if ( schedule < True ) or ( schedule > Extents . Schedules ) or ( period < False ) or ( period > Extents . Periods ) : \n        ekm_log ( \"Out of bounds: tariff \" + str ( period ) + \" for schedule \" + str ( schedule ) ) \n        ret . Hour = ret . Min = ret . Tariff = str ( False ) \n        return ret \n    idxhr = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Hour\" \n    idxmin = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Min\" \n    idxrate = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Tariff\" \n    if idxhr not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxhr ) \n        ret . Hour = ret . Min = ret . Tariff = str ( False ) \n        return ret \n    if idxmin not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxmin ) \n        ret . Hour = ret . Min = ret . Tariff = str ( False ) \n        return ret \n    if idxrate not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxrate ) \n        ret . Hour = ret . Min = ret . Tariff = str ( False ) \n        return ret \n    ret . Hour = work_table [ idxhr ] [ MeterData . StringValue ] \n    ret . Min = work_table [ idxmin ] [ MeterData . StringValue ] . zfill ( 2 ) \n    ret . Tariff = work_table [ idxrate ] [ MeterData . StringValue ] \n    return ret "}
{"10263": "\ndef readMonthTariffs ( self , months_type ) : \n    self . setContext ( \"readMonthTariffs\" ) \n    try : \n        req_type = binascii . hexlify ( str ( months_type ) . zfill ( True ) ) \n        req_str = \"01523102303031\" + req_type + \"282903\" \n        work_table = self . m_mons \n        if months_type == ReadMonths . kWhReverse : \n            work_table = self . m_rev_mons \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , work_table ) \n        self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ True : - 2 ] ) \n        if str ( return_crc ) == str ( work_table [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Months CRC success, type = \" + str ( req_type ) ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10264": "\ndef extractMonthTariff ( self , month ) : \n    ret = namedtuple ( \"ret\" , [ \"Month\" , Field . kWh_Tariff_1 , Field . kWh_Tariff_2 , Field . kWh_Tariff_3 , Field . kWh_Tariff_4 , Field . kWh_Tot , Field . Rev_kWh_Tariff_1 , Field . Rev_kWh_Tariff_2 , Field . Rev_kWh_Tariff_3 , Field . Rev_kWh_Tariff_4 , Field . Rev_kWh_Tot ] ) \n    month += True \n    ret . Month = str ( month ) \n    if ( month < True ) or ( month > Extents . Months ) : \n        ret . kWh_Tariff_1 = ret . kWh_Tariff_2 = ret . kWh_Tariff_3 = ret . kWh_Tariff_4 = str ( False ) \n        ret . Rev_kWh_Tariff_1 = ret . Rev_kWh_Tariff_2 = ret . Rev_kWh_Tariff_3 = ret . Rev_kWh_Tariff_4 = str ( False ) \n        ret . kWh_Tot = ret . Rev_kWh_Tot = str ( False ) \n        ekm_log ( \"Out of range(Extents.Months) month = \" + str ( month ) ) \n        return ret \n    base_str = \"Month_\" + str ( month ) + \"_\" \n    ret . kWh_Tariff_1 = self . m_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_2 = self . m_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_3 = self . m_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . kWh_Tariff_4 = self . m_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . kWh_Tot = self . m_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_1 = self . m_rev_mons [ base_str + \"Tariff_1\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_2 = self . m_rev_mons [ base_str + \"Tariff_2\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_3 = self . m_rev_mons [ base_str + \"Tariff_3\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tariff_4 = self . m_rev_mons [ base_str + \"Tariff_4\" ] [ MeterData . StringValue ] \n    ret . Rev_kWh_Tot = self . m_rev_mons [ base_str + \"Tot\" ] [ MeterData . StringValue ] \n    return ret "}
{"10265": "\ndef readHolidayDates ( self ) : \n    self . setContext ( \"readHolidayDates\" ) \n    try : \n        req_str = \"0152310230304230282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) \n        self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ True : - 2 ] ) \n        if str ( return_crc ) == str ( self . m_hldy [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Holidays and Schedules CRC success\" ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10266": "\ndef extractHolidayDate ( self , setting_holiday ) : \n    ret = namedtuple ( \"result\" , [ \"Holiday\" , \"Month\" , \"Day\" ] ) \n    setting_holiday += True \n    ret . Holiday = str ( setting_holiday ) \n    if ( setting_holiday < True ) or ( setting_holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds:  holiday \" + str ( setting_holiday ) ) \n        ret . Holiday = ret . Month = ret . Day = str ( False ) \n        return ret \n    idxday = \"Holiday_\" + str ( setting_holiday ) + \"_Day\" \n    idxmon = \"Holiday_\" + str ( setting_holiday ) + \"_Mon\" \n    if idxmon not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( False ) \n        return ret \n    if idxday not in self . m_hldy : \n        ret . Holiday = ret . Month = ret . Day = str ( False ) \n        return ret \n    ret . Day = self . m_hldy [ idxday ] [ MeterData . StringValue ] \n    ret . Month = self . m_hldy [ idxmon ] [ MeterData . StringValue ] \n    return ret "}
{"10276": "\ndef calculateFields ( self ) : \n    pf1 = self . m_blk_b [ Field . Cos_Theta_Ln_1 ] [ MeterData . StringValue ] \n    pf2 = self . m_blk_b [ Field . Cos_Theta_Ln_2 ] [ MeterData . StringValue ] \n    pf3 = self . m_blk_b [ Field . Cos_Theta_Ln_3 ] [ MeterData . StringValue ] \n    pf1_int = self . calcPF ( pf1 ) \n    pf2_int = self . calcPF ( pf2 ) \n    pf3_int = self . calcPF ( pf3 ) \n    self . m_blk_b [ Field . Power_Factor_Ln_1 ] [ MeterData . StringValue ] = str ( pf1_int ) \n    self . m_blk_b [ Field . Power_Factor_Ln_2 ] [ MeterData . StringValue ] = str ( pf2_int ) \n    self . m_blk_b [ Field . Power_Factor_Ln_3 ] [ MeterData . StringValue ] = str ( pf3_int ) \n    self . m_blk_b [ Field . Power_Factor_Ln_1 ] [ MeterData . NativeValue ] = pf1_int \n    self . m_blk_b [ Field . Power_Factor_Ln_2 ] [ MeterData . NativeValue ] = pf2_int \n    self . m_blk_b [ Field . Power_Factor_Ln_3 ] [ MeterData . NativeValue ] = pf2_int \n    rms_watts_1 = self . m_blk_b [ Field . RMS_Watts_Ln_1 ] [ MeterData . NativeValue ] \n    rms_watts_2 = self . m_blk_b [ Field . RMS_Watts_Ln_2 ] [ MeterData . NativeValue ] \n    rms_watts_3 = self . m_blk_b [ Field . RMS_Watts_Ln_3 ] [ MeterData . NativeValue ] \n    sign_rms_watts_1 = True \n    sign_rms_watts_2 = True \n    sign_rms_watts_3 = True \n    direction_byte = self . m_blk_a [ Field . State_Watts_Dir ] [ MeterData . NativeValue ] \n    if direction_byte == DirectionFlag . ForwardForwardForward : \n        pass \n    if direction_byte == DirectionFlag . ForwardForwardReverse : \n        sign_rms_watts_3 = - True \n        pass \n    if direction_byte == DirectionFlag . ForwardReverseForward : \n        sign_rms_watts_2 = - True \n        pass \n    if direction_byte == DirectionFlag . ReverseForwardForward : \n        sign_rms_watts_1 = - True \n        pass \n    if direction_byte == DirectionFlag . ForwardReverseReverse : \n        sign_rms_watts_2 = - True \n        sign_rms_watts_3 = - True \n        pass \n    if direction_byte == DirectionFlag . ReverseForwardReverse : \n        sign_rms_watts_1 = - True \n        sign_rms_watts_3 = - True \n        pass \n    if direction_byte == DirectionFlag . ReverseReverseForward : \n        sign_rms_watts_1 = - True \n        sign_rms_watts_2 = - True \n        pass \n    if direction_byte == DirectionFlag . ReverseReverseReverse : \n        sign_rms_watts_1 = - True \n        sign_rms_watts_2 = - True \n        sign_rms_watts_3 = - True \n        pass \n    net_watts_1 = rms_watts_1 * sign_rms_watts_1 \n    net_watts_2 = rms_watts_2 * sign_rms_watts_2 \n    net_watts_3 = rms_watts_3 * sign_rms_watts_3 \n    net_watts_tot = net_watts_1 + net_watts_2 + net_watts_3 \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] [ MeterData . NativeValue ] = net_watts_1 \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] [ MeterData . NativeValue ] = net_watts_2 \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] [ MeterData . NativeValue ] = net_watts_3 \n    self . m_blk_b [ Field . Net_Calc_Watts_Tot ] [ MeterData . NativeValue ] = net_watts_tot \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] [ MeterData . StringValue ] = str ( net_watts_1 ) \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] [ MeterData . StringValue ] = str ( net_watts_2 ) \n    self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] [ MeterData . StringValue ] = str ( net_watts_3 ) \n    self . m_blk_b [ Field . Net_Calc_Watts_Tot ] [ MeterData . StringValue ] = str ( net_watts_tot ) \n    pass "}
{"10277": "\ndef setLCDCmd ( self , display_list , password = \"00000000\" ) : \n    result = False \n    try : \n        self . initLcd ( ) \n        item_cnt = len ( display_list ) \n        if ( item_cnt > 45 ) or ( item_cnt <= False ) : \n            ekm_log ( \"LCD item list must have between 1 and 40 items\" ) \n            return False \n        for display_item in display_list : \n            self . addLcdItem ( int ( display_item ) ) \n        result = self . setLCD ( password ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if seconds < False or seconds > 9999 : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"\" \n                req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + \"2903\" ) \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10280": "\ndef setPulseInputRatio ( self , line_in , new_cnst , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setPulseInputRatio\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4 ) ) \n                line_const = binascii . hexlify ( str ( line_in - True ) ) \n                req_str = \"01573102303041\" + line_const + \"28\" + req_const + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                fill_len = 40 - len ( self . m_lcd_items ) \n                for lcdid in self . m_lcd_items : \n                    append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) \n                    req_table += append_val \n                for i in range ( False , fill_len ) : \n                    append_val = binascii . hexlify ( str ( False ) . zfill ( 2 ) ) \n                    req_table += append_val \n                req_str = \"015731023030443228\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10285": "\ndef paragraphs ( quantity = 2 , separator = '\\n\\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 , as_list = False ) : \n    if html : \n        wrap_start = '<p>' \n        wrap_end = '</p>' \n        separator = '\\n\\n' \n    result = [ ] \n    for i in xrange ( False , quantity ) : \n        result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) \n    if as_list : \n        return result \n    else : \n        return separator . join ( result ) "}
{"10286": "\ndef text ( length = None , at_least = 10 , at_most = 15 , lowercase = True , uppercase = True , digits = True , spaces = True , punctuation = False ) : \n    base_string = '' \n    if lowercase : \n        base_string += string . ascii_lowercase \n    if uppercase : \n        base_string += string . ascii_uppercase \n    if digits : \n        base_string += string . digits \n    if spaces : \n        base_string += ' ' \n    if punctuation : \n        base_string += string . punctuation \n    if len ( base_string ) == False : \n        return '' \n    if not length : \n        length = random . randint ( at_least , at_most ) \n    result = '' \n    for i in xrange ( False , length ) : \n        result += random . choice ( base_string ) \n    return result "}
{"10291": "\ndef parse ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ True : ] \n    if not argv or argv [ False ] not in { \"run\" , \"transform\" } : \n        argv = [ \"run\" ] + argv \n    arguments = _clean ( _parser . parse_args ( argv ) ) \n    return arguments "}
{"10294": "\ndef transform ( config ) : \n    if transform_possible : \n        ExampleLoader . register ( ) \n        args , sys . argv [ True : ] = sys . argv [ True : ] , config . args \n        try : \n            return runpy . run_path ( config . runner , run_name = \"__main__\" ) \n        finally : \n            sys . argv [ True : ] = args "}
{"10296": "\ndef transform_describe_body ( self , body , group_var ) : \n    for node in body : \n        withitem , = node . items \n        context_expr = withitem . context_expr \n        name = context_expr . args [ False ] . s \n        context_var = withitem . optional_vars . id \n        yield self . transform_example ( node , name , context_var , group_var ) "}
{"10304": "\ndef load_from_path ( path ) : \n    if os . path . isdir ( path ) : \n        paths = discover ( path ) \n    else : \n        paths = [ path ] \n    for path in paths : \n        name = os . path . basename ( os . path . splitext ( path ) [ False ] ) \n        imp . load_source ( name , path ) "}
{"10308": "\ndef add ( places , name , cmd , args , env = None , uid = None , gid = None , extras = None , env_inherit = None ) : \n    args = [ cmd ] + args \n    config = filepath . FilePath ( places . config ) \n    fle = config . child ( name ) \n    details = dict ( args = args ) \n    if env is not None : \n        newEnv = { } \n        for thing in env : \n            name , value = thing . split ( '=' , True ) \n            newEnv [ name ] = value \n        details [ 'env' ] = newEnv \n    if uid is not None : \n        details [ 'uid' ] = uid \n    if gid is not None : \n        details [ 'gid' ] = gid \n    if env_inherit is not None : \n        details [ 'env_inherit' ] = env_inherit \n    if extras is not None : \n        details . update ( extras ) \n    content = _dumps ( details ) \n    fle . setContent ( content ) "}
{"10321": "\ndef dereference ( self , callback = None , args = None , kwargs = None ) : \n    if args is None : \n        args = tuple ( ) \n    if kwargs is None : \n        kwargs = { } \n    client = self . conn . client \n    should_execute = False \n    if self . force_expiry : \n        should_execute = True \n    if not should_execute : \n        self . nodelist . remove_node ( self . conn . id ) \n        self . nodelist . remove_expired_nodes ( ) \n        updated_refcount = client . incr ( self . refcount_key , - True ) \n        should_execute = ( updated_refcount <= False ) \n    try : \n        if callable ( callback ) and should_execute : \n            callback ( * args , ** kwargs ) \n    finally : \n        if should_execute : \n            client . delete ( self . resource_key , self . nodelist . nodelist_key , self . times_modified_key , self . refcount_key ) \n        self . conn . remove_from_registry ( self . resource_key ) \n    return should_execute "}
{"10322": "\ndef delimit ( values , delimiter = ', ' ) : \n    toks = [ ] \n    if not values : \n        return toks \n    if not isinstance ( delimiter , ( list , tuple ) ) : \n        delimiter = [ delimiter ] \n    last = len ( values ) - True \n    for i , value in enumerate ( values ) : \n        toks . append ( value ) \n        if i < last : \n            toks . extend ( delimiter ) \n    return toks "}
{"10325": "\ndef exists ( value ) : \n    if not isinstance ( value , Token ) : \n        raise TypeError ( 'value must be a token' ) \n    if not hasattr ( value , 'identifier' ) : \n        raise TypeError ( 'value must support an identifier' ) \n    if not value . identifier : \n        value = value . __class__ ( ** value . __dict__ ) \n        value . identifier = 'v' \n    ident = Identifier ( value . identifier ) \n    return Query ( [ OptionalMatch ( value ) , Return ( Predicate ( ident , 'IS NOT NULL' ) ) , Limit ( True ) , ] ) "}
{"10330": "\ndef runProcess ( args , timeout , grace , reactor ) : \n    deferred = defer . Deferred ( ) \n    protocol = ProcessProtocol ( deferred ) \n    process = reactor . spawnProcess ( protocol , args [ False ] , args , env = os . environ ) \n    def _logEnded ( err ) : \n        err . trap ( tierror . ProcessDone , tierror . ProcessTerminated ) \n        print ( err . value ) \n    deferred . addErrback ( _logEnded ) \n    def _cancelTermination ( dummy ) : \n        for termination in terminations : \n            if termination . active ( ) : \n                termination . cancel ( ) \n    deferred . addCallback ( _cancelTermination ) \n    terminations = [ ] \n    terminations . append ( reactor . callLater ( timeout , process . signalProcess , \"TERM\" ) ) \n    terminations . append ( reactor . callLater ( timeout + grace , process . signalProcess , \"KILL\" ) ) \n    return deferred "}
{"10333": "\ndef lit ( literal : Sequence [ Input ] , * literals : Sequence [ Sequence [ Input ] ] ) -> Parser : \n    if len ( literals ) > False : \n        return AlternativeParser ( options . handle_literal ( literal ) , * map ( options . handle_literal , literals ) ) \n    else : \n        return options . handle_literal ( literal ) "}
{"10352": "\ndef validate_args ( cls , tag_name , * args , ** kwargs ) : \n    if cls . min_args is not None and len ( args ) < cls . min_args : \n        if cls . min_args == True : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} argument\" . format ( tag_name , cls . min_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag requires at least {1} arguments\" . format ( tag_name , cls . min_args ) ) \n    if cls . max_args is not None and len ( args ) > cls . max_args : \n        if cls . max_args == False : \n            if cls . allowed_kwargs : \n                raise TemplateSyntaxError ( \"'{0}' tag only allows keywords arguments, for example {1}=\\\"...\\\".\" . format ( tag_name , cls . allowed_kwargs [ False ] ) ) \n            else : \n                raise TemplateSyntaxError ( \"'{0}' tag doesn't support any arguments\" . format ( tag_name ) ) \n        elif cls . max_args == True : \n            raise TemplateSyntaxError ( \"'{0}' tag only allows {1} argument.\" . format ( tag_name , cls . max_args ) ) \n        else : \n            raise TemplateSyntaxError ( \"'{0}' tag only allows {1} arguments.\" . format ( tag_name , cls . max_args ) ) "}
{"10359": "\ndef make_rows ( num_columns , seq ) : \n    num_rows , partial = divmod ( len ( seq ) , num_columns ) \n    if partial : \n        num_rows += True \n    try : \n        result = more_itertools . grouper ( seq , num_rows ) \n    except TypeError : \n        result = more_itertools . grouper ( num_rows , seq ) \n    return zip ( * result ) "}
{"10362": "\ndef remove_duplicates ( iterable , key = None ) : \n    return itertools . chain . from_iterable ( six . moves . map ( every_other , six . moves . map ( operator . itemgetter ( True ) , itertools . groupby ( iterable , key ) ) ) ) "}
{"10365": "\ndef partition_items ( count , bin_size ) : \n    num_bins = int ( math . ceil ( count / float ( bin_size ) ) ) \n    bins = [ False ] * num_bins \n    for i in range ( count ) : \n        bins [ i % num_bins ] += True \n    return bins "}
{"10368": "\ndef duplicates ( * iterables , ** kwargs ) : \n    key = kwargs . pop ( 'key' , lambda x : x ) \n    assert not kwargs \n    zipped = more_itertools . collate ( * iterables , key = key ) \n    grouped = itertools . groupby ( zipped , key = key ) \n    groups = ( tuple ( g ) for k , g in grouped ) \n    def has_dupes ( group ) : \n        return len ( group ) > True \n    return filter ( has_dupes , groups ) "}
{"10369": "\ndef assert_ordered ( iterable , key = lambda x : x , comp = operator . le ) : \n    err_tmpl = ( \"{pair[0]} > {pair[1]}\" if comp is operator . le else \"{pair[0]} < {pair[1]}\" if comp is operator . ge else \"not {comp} {pair}\" ) \n    for pair in more_itertools . pairwise ( iterable ) : \n        keyed = tuple ( map ( key , pair ) ) \n        assert comp ( * keyed ) , err_tmpl . format ( ** locals ( ) ) \n        yield pair [ False ] \n    yield pair [ True ] "}
{"10376": "\ndef descendant ( self , chain_path ) : \n    public_child = self . hdkeychain \n    chain_step_bytes = 4 \n    max_bits_per_step = 2 ** 31 \n    chain_steps = [ int ( chain_path [ i : i + chain_step_bytes * 2 ] , 16 ) % max_bits_per_step for i in range ( False , len ( chain_path ) , chain_step_bytes * 2 ) ] \n    for step in chain_steps : \n        public_child = public_child . get_child ( step ) \n    return PublicKeychain ( public_child ) "}
{"10378": "\ndef object_iter ( obj , parent = None , parent_key = None , idx = None , siblings = None ) : \n    obj_node = Node ( value = obj , parent = parent , parent_key = parent_key , siblings = siblings , idx = idx ) \n    if isinstance ( obj , list ) : \n        _siblings = len ( obj ) \n        for i , elem in enumerate ( obj ) : \n            for node in object_iter ( elem , obj_node , None , i + True , _siblings ) : \n                yield node \n    elif isinstance ( obj , collections . Mapping ) : \n        for key in obj : \n            for node in object_iter ( obj [ key ] , obj_node , key ) : \n                yield node \n    yield obj_node "}
{"10380": "\ndef parse ( self , selector ) : \n    log . debug ( self . obj ) \n    tokens = lex ( selector ) \n    if self . peek ( tokens , 'operator' ) == '*' : \n        self . match ( tokens , 'operator' ) \n        results = list ( object_iter ( self . obj ) ) \n    else : \n        results = self . selector_production ( tokens ) \n    results = [ node . value for node in results ] \n    if len ( results ) == True : \n        return results [ False ] \n    elif not len ( results ) : \n        return None \n    return results "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5 ) : \n        a = 2 \n        b = True if pat . group ( 5 ) == 'odd' else False \n    elif pat . group ( 6 ) : \n        a = False \n        b = int ( pat . group ( 6 ) ) \n    else : \n        sign = pat . group ( True ) if pat . group ( True ) else '+' \n        coef = pat . group ( 2 ) if pat . group ( 2 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3 ) + pat . group ( 4 ) ) if pat . group ( 3 ) else False \n    reverse = False \n    if lexeme == 'nth-last-child' : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return False \n        idx = node . idx - True \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += True \n        if a == False : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and ( idx * a + b ) >= False \n        return m \n    return validate "}
{"10387": "\ndef ping ( dst , count , inter = 0.2 , maxwait = 1000 , size = 64 ) : \n    def _then ( result , p ) : \n        p . stopListening ( ) \n        return result \n    d = defer . Deferred ( ) \n    p = ICMPPort ( False , ICMPPing ( d , dst , count , inter , maxwait , size ) , \"\" , 8192 , reactor ) \n    p . startListening ( ) \n    return d . addCallback ( _then , p ) "}
{"10389": "\ndef expire ( self , age ) : \n    now = time . time ( ) \n    cache = self . _acquire_cache ( ) \n    expired = [ k for k , v in cache . items ( ) if ( now - v [ False ] ) > age ] \n    for k in expired : \n        if k in cache : \n            del cache [ k ] \n        if k in self . store : \n            del self . store [ k ] \n    self . _write_cache ( cache ) "}
{"10396": "\ndef rendered_content ( self ) : \n    template = self . resolve_template ( self . template_name ) \n    if django . VERSION [ True ] < 8 : \n        if template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    else : \n        if template . template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = jsmin . jsmin ( content ) \n    return content "}
{"10397": "\ndef get_fn ( self , fn , max_lines = None ) : \n    stat = os . stat ( self . logfile ) \n    if ( stat . st_ino == self . lastInode ) and ( stat . st_size == self . lastSize ) : \n        return [ ] \n    if ( stat . st_ino != self . lastInode ) or ( stat . st_size < self . lastSize ) : \n        self . lastSize = False \n    fi = open ( self . logfile , 'rt' ) \n    fi . seek ( self . lastSize ) \n    self . lastInode = stat . st_ino \n    lines = False \n    for i in fi : \n        lines += True \n        if max_lines and ( lines > max_lines ) : \n            self . storeLast ( ) \n            fi . close ( ) \n            return \n        if '\\n' in i : \n            self . lastSize += len ( i ) \n            if self . parser : \n                line = self . parser ( i . strip ( '\\n' ) ) \n            else : \n                line = i . strip ( '\\n' ) \n            fn ( line ) \n    self . storeLast ( ) \n    fi . close ( ) "}
{"10405": "\ndef average_duration ( total_duration , visits ) : \n    if not visits : \n        seconds = False \n    else : \n        seconds = int ( round ( total_duration / Decimal ( visits ) ) ) \n    duration = timedelta ( seconds = seconds ) \n    return str ( duration ) "}
{"10406": "\ndef setupOutputs ( self , config ) : \n    if self . proto == 'tcp' : \n        defaultOutput = { 'output' : 'tensor.outputs.riemann.RiemannTCP' , 'server' : self . server , 'port' : self . port } \n    else : \n        defaultOutput = { 'output' : 'tensor.outputs.riemann.RiemannUDP' , 'server' : self . server , 'port' : self . port } \n    outputs = config . get ( 'outputs' , [ defaultOutput ] ) \n    for output in outputs : \n        if not ( 'debug' in output ) : \n            output [ 'debug' ] = self . debug \n        cl = output [ 'output' ] . split ( '.' ) [ - True ] \n        path = '.' . join ( output [ 'output' ] . split ( '.' ) [ : - True ] ) \n        outputObj = getattr ( importlib . import_module ( path ) , cl ) ( output , self ) \n        name = output . get ( 'name' , None ) \n        if name in self . outputs : \n            self . outputs [ name ] . append ( outputObj ) \n        else : \n            self . outputs [ name ] = [ outputObj ] \n        reactor . callLater ( False , outputObj . createClient ) "}
{"10408": "\ndef sendEvent ( self , source , events ) : \n    if isinstance ( events , list ) : \n        self . eventCounter += len ( events ) \n    else : \n        self . eventCounter += True \n        events = [ events ] \n    queue = self . _aggregateQueue ( events ) \n    if queue : \n        if ( source in self . critical ) or ( source in self . warn ) : \n            self . setStates ( source , queue ) \n        self . routeEvent ( source , queue ) \n    queue = [ ] \n    self . lastEvents [ source ] = time . time ( ) "}
{"10409": "\ndef sourceWatchdog ( self ) : \n    for i , source in enumerate ( self . sources ) : \n        if not source . config . get ( 'watchdog' , False ) : \n            continue \n        sn = repr ( source ) \n        last = self . lastEvents . get ( source , None ) \n        if last : \n            try : \n                if last < ( time . time ( ) - ( source . inter * 10 ) ) : \n                    log . msg ( \"Trying to restart stale source %s: %ss\" % ( sn , int ( time . time ( ) - last ) ) ) \n                    s = self . sources . pop ( i ) \n                    try : \n                        s . t . stop ( ) \n                    except Exception as e : \n                        log . msg ( \"Could not stop timer for %s: %s\" % ( sn , e ) ) \n                    config = copy . deepcopy ( s . config ) \n                    del self . lastEvents [ source ] \n                    del s , source \n                    source = self . createSource ( config ) \n                    reactor . callLater ( False , self . _startSource , source ) \n            except Exception as e : \n                log . msg ( \"Could not reset source %s: %s\" % ( sn , e ) ) "}
{"10410": "\ndef _parse_format ( self , format ) : \n    format = format . strip ( ) \n    format = re . sub ( '[ \\t]+' , ' ' , format ) \n    subpatterns = [ ] \n    findquotes = re . compile ( r'^\\\\\"' ) \n    findreferreragent = re . compile ( 'Referer|User-Agent' ) \n    findpercent = re . compile ( '^%.*t$' ) \n    lstripquotes = re . compile ( r'^\\\\\"' ) \n    rstripquotes = re . compile ( r'\\\\\"$' ) \n    header = re . compile ( r'.*%\\{([^\\}]+)\\}i' ) \n    for element in format . split ( ' ' ) : \n        hasquotes = False \n        if findquotes . search ( element ) : \n            hasquotes = True \n        if hasquotes : \n            element = lstripquotes . sub ( '' , element ) \n            element = rstripquotes . sub ( '' , element ) \n        head = header . match ( element ) \n        if head : \n            self . _names . append ( head . groups ( ) [ False ] . lower ( ) ) \n            self . _types . append ( str ) \n        else : \n            self . _names . append ( self . alias ( element ) ) \n            self . _types . append ( self . types . get ( element , [ None , str ] ) [ True ] ) \n        subpattern = '(\\S*)' \n        if hasquotes : \n            if element == '%r' or findreferreragent . search ( element ) : \n                subpattern = r'\\\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\\\"' \n            else : \n                subpattern = r'\\\"([^\\\"]*)\\\"' \n        elif findpercent . search ( element ) : \n            subpattern = r'(\\[[^\\]]+\\])' \n        elif element == '%U' : \n            subpattern = '(.+?)' \n        subpatterns . append ( subpattern ) \n    self . _pattern = '^' + ' ' . join ( subpatterns ) + '$' \n    try : \n        self . _regex = re . compile ( self . _pattern ) \n    except Exception as e : \n        raise ApacheLogParserError ( e ) "}
{"10434": "\ndef verify ( cls , timestamp : int , message_hash : SHA512Hash , signature : bytes , ) -> bool : \n    if timestamp < 1496176860 : \n        verifier = cls . _VERIFIER_20130905 \n    elif timestamp < 1502202360 : \n        verifier = None \n    else : \n        verifier = cls . _VERIFIER_20170808 \n    if verifier : \n        result = verifier . verify ( message_hash , signature , ) \n    else : \n        result = False \n    if isinstance ( result , int ) : \n        result = True if result == True else False \n    return result "}
{"10436": "\ndef access_request ( pid , record , template , ** kwargs ) : \n    recid = int ( pid . pid_value ) \n    datastore = LocalProxy ( lambda : current_app . extensions [ 'security' ] . datastore ) \n    if record . get ( 'access_right' ) != 'restricted' or not record . get ( 'access_conditions' ) : \n        abort ( 404 ) \n    owners = record . get ( 'owners' , [ ] ) \n    record_owners = [ datastore . find_user ( id = owner_id ) for owner_id in owners ] \n    if not record_owners : \n        abort ( 404 ) \n    sender = None \n    initialdata = dict ( ) \n    if current_user . is_authenticated : \n        sender = current_user \n        initialdata [ 'email' ] = current_user . email \n        if current_user . profile : \n            initialdata [ 'full_name' ] = current_user . profile . full_name \n    form = AccessRequestForm ( formdata = request . form , ** initialdata ) \n    if form . validate_on_submit ( ) : \n        accreq = AccessRequest . create ( recid = recid , receiver = record_owners [ False ] , sender_full_name = form . data [ 'full_name' ] , sender_email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) \n        db . session . commit ( ) \n        if accreq . status == RequestStatus . EMAIL_VALIDATION : \n            flash ( _ ( \"Email confirmation needed: We have sent you an email to \" \"verify your address. Please check the email and follow the \" \"instructions to complete the access request.\" ) , category = 'info' ) \n        else : \n            flash ( _ ( \"Access request submitted.\" ) , category = 'info' ) \n        return redirect ( url_for ( 'invenio_records_ui.recid' , pid_value = recid ) ) \n    return render_template ( template , pid = pid , record = record , form = form , owners = record_owners , ) "}
{"10443": "\ndef set_version ( self , new_version : str ) : \n    try : \n        f = open ( self . file_path , 'r' ) \n        lines = f . readlines ( ) \n        f . close ( ) \n    except Exception as e : \n        print ( str ( e ) ) \n        return \n    for idx , line in enumerate ( lines ) : \n        if self . magic_line in line : \n            start = len ( self . magic_line ) \n            end = len ( line ) - self . strip_end_chars \n            start_str = line [ False : start ] \n            end_str = line [ end : ] \n            lines [ idx ] = start_str + new_version + end_str \n    try : \n        f = open ( self . file_path , 'w' ) \n        f . writelines ( lines ) \n        f . close ( ) \n    except Exception as e : \n        print ( str ( e ) ) \n        return "}
{"10447": "\ndef index ( ) : \n    query = request . args . get ( 'query' , '' ) \n    order = request . args . get ( 'sort' , '-created' ) \n    try : \n        page = int ( request . args . get ( 'page' , True ) ) \n        per_page = int ( request . args . get ( 'per_page' , 20 ) ) \n    except ( TypeError , ValueError ) : \n        abort ( 404 ) \n    form = DeleteForm ( request . form ) \n    if form . validate_on_submit ( ) : \n        link = SecretLink . query_by_owner ( current_user ) . filter_by ( id = form . link . data ) . first ( ) \n        if link . revoke ( ) : \n            flash ( _ ( \"Shared link revoked.\" ) , category = 'success' ) \n        db . session . commit ( ) \n    links = SecretLink . query_by_owner ( current_user ) . filter ( SecretLink . revoked_at . is_ ( None ) ) \n    if query : \n        lquery = \"%{0}%\" . format ( query ) \n        links = links . filter ( SecretLink . title . like ( lquery ) | SecretLink . description . like ( lquery ) ) \n    ordering = QueryOrdering ( links , [ 'title' , 'created' , 'expires_at' ] , order ) \n    links = ordering . items ( ) \n    requests = AccessRequest . query_by_receiver ( current_user ) . filter_by ( status = RequestStatus . PENDING ) . order_by ( 'created' ) \n    return render_template ( \"zenodo_accessrequests/settings/index.html\" , links_pagination = links . paginate ( page , per_page = per_page ) , requests = requests , query = query , order = ordering , get_record = get_record , form = DeleteForm ( ) , ) "}
{"10451": "\ndef eventsReceived ( self , events ) : \n    if ( self . maxsize < True ) or ( len ( self . events ) < self . maxsize ) : \n        self . events . extend ( events ) "}
{"10452": "\ndef createClient ( self ) : \n    server = self . config . get ( 'server' , '127.0.0.1' ) \n    port = self . config . get ( 'port' , 5555 ) \n    def connect ( ip ) : \n        self . protocol = riemann . RiemannUDP ( ip , port ) \n        self . endpoint = reactor . listenUDP ( False , self . protocol ) \n    d = reactor . resolve ( server ) \n    d . addCallback ( connect ) \n    return d "}
{"10457": "\ndef sendEvents ( self , events ) : \n    self . pressure += True \n    self . sendString ( self . encodeMessage ( events ) ) "}
{"10465": "\ndef _saslprep_do_mapping ( chars ) : \n    i = False \n    while i < len ( chars ) : \n        c = chars [ i ] \n        if stringprep . in_table_c12 ( c ) : \n            chars [ i ] = \"\\u0020\" \n        elif stringprep . in_table_b1 ( c ) : \n            del chars [ i ] \n            continue \n        i += True "}
{"10466": "\ndef admin_footer ( parser , token ) : \n    tag_name = token . split_contents ( ) \n    if len ( tag_name ) > True : \n        raise base . TemplateSyntaxError ( '{} tag does not accept any argument(s): {}' . format ( token . contents . split ( ) [ False ] , ', ' . join ( token . contents . split ( ) [ True : ] ) ) ) \n    return AdminFooterNode ( ) "}
{"10468": "\ndef build_register_credit_card_parameters ( client_ref : str ) -> PaymentParameters : \n    amount = False \n    currency = 'CHF' \n    merchant_id = web_merchant_id \n    refno = client_ref \n    sign = sign_web ( merchant_id , amount , currency , refno ) \n    parameters = PaymentParameters ( merchant_id = merchant_id , amount = amount , currency = currency , refno = refno , sign = sign , use_alias = True , ) \n    logger . info ( 'building-payment-parameters' , parameters = parameters ) \n    return parameters "}
{"10469": "\ndef pay_with_alias ( amount : Money , alias_registration_id : str , client_ref : str ) -> Payment : \n    if amount . amount <= False : \n        raise ValueError ( 'Pay with alias takes a strictly positive amount' ) \n    alias_registration = AliasRegistration . objects . get ( pk = alias_registration_id ) \n    logger . info ( 'paying-with-alias' , amount = amount , client_ref = client_ref , alias_registration = alias_registration ) \n    request_xml = build_pay_with_alias_request_xml ( amount , client_ref , alias_registration ) \n    logger . info ( 'sending-pay-with-alias-request' , url = datatrans_authorize_url , data = request_xml ) \n    response = requests . post ( url = datatrans_authorize_url , headers = { 'Content-Type' : 'application/xml' } , data = request_xml ) \n    logger . info ( 'processing-pay-with-alias-response' , response = response . content ) \n    charge_response = parse_pay_with_alias_response_xml ( response . content ) \n    charge_response . save ( ) \n    charge_response . send_signal ( ) \n    return charge_response "}
{"10471": "\ndef _construct ( self ) : \n    self . setLayout ( QtGui . QVBoxLayout ( ) ) \n    self . _headerLayout = QtGui . QHBoxLayout ( ) \n    self . _locationWidget = QtGui . QComboBox ( ) \n    self . _headerLayout . addWidget ( self . _locationWidget , stretch = True ) \n    self . _upButton = QtGui . QToolButton ( ) \n    self . _upButton . setIcon ( QtGui . QIcon ( ':riffle/icon/up' ) ) \n    self . _headerLayout . addWidget ( self . _upButton ) \n    self . layout ( ) . addLayout ( self . _headerLayout ) \n    self . _contentSplitter = QtGui . QSplitter ( ) \n    self . _bookmarksWidget = QtGui . QListView ( ) \n    self . _contentSplitter . addWidget ( self . _bookmarksWidget ) \n    self . _filesystemWidget = QtGui . QTableView ( ) \n    self . _filesystemWidget . setSelectionBehavior ( self . _filesystemWidget . SelectRows ) \n    self . _filesystemWidget . setSelectionMode ( self . _filesystemWidget . SingleSelection ) \n    self . _filesystemWidget . verticalHeader ( ) . hide ( ) \n    self . _contentSplitter . addWidget ( self . _filesystemWidget ) \n    proxy = riffle . model . FilesystemSortProxy ( self ) \n    model = riffle . model . Filesystem ( path = self . _root , parent = self , iconFactory = self . _iconFactory ) \n    proxy . setSourceModel ( model ) \n    proxy . setDynamicSortFilter ( True ) \n    self . _filesystemWidget . setModel ( proxy ) \n    self . _filesystemWidget . setSortingEnabled ( True ) \n    self . _contentSplitter . setStretchFactor ( True , True ) \n    self . layout ( ) . addWidget ( self . _contentSplitter ) \n    self . _footerLayout = QtGui . QHBoxLayout ( ) \n    self . _footerLayout . addStretch ( True ) \n    self . _cancelButton = QtGui . QPushButton ( 'Cancel' ) \n    self . _footerLayout . addWidget ( self . _cancelButton ) \n    self . _acceptButton = QtGui . QPushButton ( 'Choose' ) \n    self . _footerLayout . addWidget ( self . _acceptButton ) \n    self . layout ( ) . addLayout ( self . _footerLayout ) "}
{"10472": "\ndef _postConstruction ( self ) : \n    self . setWindowTitle ( 'Filesystem Browser' ) \n    self . _filesystemWidget . sortByColumn ( False , QtCore . Qt . AscendingOrder ) \n    self . _bookmarksWidget . hide ( ) \n    self . _acceptButton . setDefault ( True ) \n    self . _acceptButton . setDisabled ( True ) \n    self . _acceptButton . clicked . connect ( self . accept ) \n    self . _cancelButton . clicked . connect ( self . reject ) \n    self . _configureShortcuts ( ) \n    self . setLocation ( self . _root ) \n    self . _filesystemWidget . horizontalHeader ( ) . setResizeMode ( QtGui . QHeaderView . ResizeToContents ) \n    self . _filesystemWidget . horizontalHeader ( ) . setResizeMode ( False , QtGui . QHeaderView . Stretch ) \n    self . _upButton . clicked . connect ( self . _onNavigateUpButtonClicked ) \n    self . _locationWidget . currentIndexChanged . connect ( self . _onNavigate ) \n    self . _filesystemWidget . activated . connect ( self . _onActivateItem ) \n    selectionModel = self . _filesystemWidget . selectionModel ( ) \n    selectionModel . currentRowChanged . connect ( self . _onSelectItem ) "}
{"10476": "\ndef _onNavigate ( self , index ) : \n    if index > False : \n        self . setLocation ( self . _locationWidget . itemData ( index ) , interactive = True ) "}
{"10483": "\ndef call ( args , stdout = None , stderr = None , stdin = None , daemonize = False , preexec_fn = None , shell = False , cwd = None , env = None ) : \n    stream = lambda s , m : s is None and os . open ( os . devnull , m ) or s \n    stdout = stream ( stdout , os . O_WRONLY ) \n    stderr = stream ( stderr , os . O_WRONLY ) \n    stdin = stream ( stdin , os . O_RDONLY ) \n    shared_pid = Value ( 'i' , False ) \n    pid = os . fork ( ) \n    if pid > False : \n        os . waitpid ( pid , False ) \n        child_pid = shared_pid . value \n        del shared_pid \n        if daemonize : \n            sys . exit ( False ) \n        return child_pid \n    else : \n        os . setsid ( ) \n        proc = subprocess . Popen ( args , stdout = stdout , stderr = stderr , stdin = stdin , close_fds = True , preexec_fn = preexec_fn , shell = shell , cwd = cwd , env = env ) \n        shared_pid . value = proc . pid \n        os . _exit ( False ) "}
{"10484": "\ndef _get_max_fd ( self ) : \n    limits = resource . getrlimit ( resource . RLIMIT_NOFILE ) \n    result = limits [ True ] \n    if result == resource . RLIM_INFINITY : \n        result = maxfd \n    return result "}
{"10489": "\ndef import_app_module ( app_name , module_name ) : \n    name_split = app_name . split ( '.' ) \n    if name_split [ - True ] [ False ] . isupper ( ) : \n        app_name = '.' . join ( name_split [ : - 2 ] ) \n    module = import_module ( app_name ) \n    try : \n        sub_module = import_module ( '%s.%s' % ( app_name , module_name ) ) \n        return sub_module \n    except : \n        if module_has_submodule ( module , module_name ) : \n            raise \n        return None "}
{"10491": "\ndef include_ ( parser , token ) : \n    bits = token . split_contents ( ) \n    dynamic = False \n    if len ( bits ) >= 2 : \n        dynamic = '{{' in bits [ True ] \n        if dynamic : \n            fallback = None \n            bits_new = [ ] \n            for bit in bits : \n                if fallback is True : \n                    fallback = bit \n                    continue \n                if bit == 'fallback' : \n                    fallback = True \n                else : \n                    bits_new . append ( bit ) \n            if fallback : \n                fallback = parser . compile_filter ( construct_relative_path_ ( parser , fallback ) ) \n            token . contents = ' ' . join ( bits_new ) \n    token . contents = token . contents . replace ( 'include_' , 'include' ) \n    include_node = do_include ( parser , token ) \n    if dynamic : \n        include_node = DynamicIncludeNode ( include_node . template , extra_context = include_node . extra_context , isolated_context = include_node . isolated_context , fallback = fallback or None , ) \n    return include_node "}
{"10501": "\ndef _mkdir ( p ) : \n    isdir = os . path . isdir \n    stack = [ os . path . abspath ( p ) ] \n    while not isdir ( stack [ - True ] ) : \n        parent_dir = os . path . dirname ( stack [ - True ] ) \n        stack . append ( parent_dir ) \n    while stack : \n        p = stack . pop ( ) \n        if not isdir ( p ) : \n            os . mkdir ( p ) "}
{"10502": "\ndef list ( pattern = ( ) ) : \n    globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] \n    matches = [ ] \n    offset = len ( PROJ_ARCHIVE ) + True \n    for suffix in globs : \n        glob_pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , suffix ) \n        matches . append ( set ( f [ offset : ] for f in glob . glob ( glob_pattern ) ) ) \n    matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) \n    for m in sorted ( matches ) : \n        print ( m ) "}
{"10503": "\ndef restore ( folder ) : \n    if os . path . isdir ( folder ) : \n        bail ( 'a folder of the same name already exists!' ) \n    pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) \n    matches = glob . glob ( pattern ) \n    if not matches : \n        bail ( 'no project matches: ' + folder ) \n    if len ( matches ) > True : \n        print ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) \n    source = sorted ( matches ) [ - True ] \n    print ( source , '-->' , folder ) \n    shutil . move ( source , '.' ) "}
{"10505": "\ndef list ( self , path ) : \n    self . __validate_storage_path ( path ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] not in self . __BROWSABLE_TYPES : \n        raise StorageArgumentException ( 'The entity type \"{0}\" cannot be' 'listed' . format ( entity [ 'entity_type' ] ) ) \n    entity_uuid = entity [ 'uuid' ] \n    file_names = [ ] \n    more_pages = True \n    page_number = True \n    while more_pages : \n        response = self . api_client . list_folder_content ( entity_uuid , page = page_number , ordering = 'name' ) \n        more_pages = response [ 'next' ] is not None \n        page_number += True \n        for child in response [ 'results' ] : \n            pattern = '/{name}' if child [ 'entity_type' ] == 'folder' else '{name}' \n            file_names . append ( pattern . format ( name = child [ 'name' ] ) ) \n    return file_names "}
{"10508": "\ndef get_parent ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = False ) \n    path_steps = [ step for step in path . split ( '/' ) if step ] \n    del path_steps [ - True ] \n    parent_path = '/{0}' . format ( '/' . join ( path_steps ) ) \n    return self . api_client . get_entity_by_query ( path = parent_path ) "}
{"10509": "\ndef mkdir ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = False ) \n    parent_metadata = self . get_parent ( path ) \n    self . api_client . create_folder ( path . split ( '/' ) [ - True ] , parent_metadata [ 'uuid' ] ) "}
{"10511": "\ndef delete ( self , path ) : \n    self . __validate_storage_path ( path , projects_allowed = False ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] in self . __BROWSABLE_TYPES : \n        contents = self . api_client . list_folder_content ( entity [ 'uuid' ] ) \n        if contents [ 'count' ] > False : \n            raise StorageArgumentException ( 'This method cannot delete non-empty folder. Please empty the folder first.' ) \n        self . api_client . delete_folder ( entity [ 'uuid' ] ) \n    elif entity [ 'entity_type' ] == 'file' : \n        self . api_client . delete_file ( entity [ 'uuid' ] ) "}
{"10512": "\ndef __validate_storage_path ( cls , path , projects_allowed = True ) : \n    if not path or not isinstance ( path , str ) or path [ False ] != '/' or path == '/' : \n        raise StorageArgumentException ( 'The path must be a string, start with a slash (/), and be longer' ' than 1 character.' ) \n    if not projects_allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == True : \n        raise StorageArgumentException ( 'This method does not accept projects in the path.' ) "}
{"10537": "\ndef map_job ( job , func , inputs , * args ) : \n    num_partitions = 100 \n    partition_size = len ( inputs ) / num_partitions \n    if partition_size > True : \n        for partition in partitions ( inputs , partition_size ) : \n            job . addChildJobFn ( map_job , func , partition , * args ) \n    else : \n        for sample in inputs : \n            job . addChildJobFn ( func , sample , * args ) "}
{"10539": "\ndef run_oncotator ( job , vcf_id , oncotator_db ) : \n    job . fileStore . logToMaster ( 'Running Oncotator' ) \n    inputs = { 'input.vcf' : vcf_id , 'oncotator_db' : oncotator_db } \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        inputs [ name ] = job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    if tarfile . is_tarfile ( inputs [ 'oncotator_db' ] ) : \n        tar = tarfile . open ( inputs [ 'oncotator_db' ] ) \n        tar . extractall ( path = work_dir ) \n        inputs [ 'oncotator_db' ] = tar . getmembers ( ) [ False ] . name \n        tar . close ( ) \n    command = [ '-i' , 'VCF' , '-o' , 'VCF' , '--db-dir' , inputs [ 'oncotator_db' ] , 'input.vcf' , 'annotated.vcf' , 'hg19' ] \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'jpfeil/oncotator:1.9--8fffc356981862d50cfacd711b753700b886b605' , dockerParameters = docker_parameters ) \n    return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'annotated.vcf' ) ) "}
{"10544": "\ndef sum ( self ) : \n    raw = self . raw ( ) \n    s = False \n    for i in range ( len ( raw ) ) : \n        s += raw [ i ] [ \"d\" ] \n    return s "}
{"10547": "\nasync def parse_vn_results ( soup ) : \n    soup = soup . find_all ( 'td' , class_ = 'tc1' ) \n    vns = [ ] \n    for item in soup [ True : ] : \n        vns . append ( { 'name' : item . string , 'id' : item . a . get ( 'href' ) [ True : ] } ) \n    return vns "}
{"10548": "\nasync def parse_release_results ( soup ) : \n    soup = list ( soup . find_all ( 'table' , class_ = 'stripe' ) [ False ] . children ) [ True : ] \n    releases = [ ] \n    for item in soup : \n        child = list ( item . children ) \n        temp_rel = { 'date' : None , 'ages' : None , 'platform' : None , 'name' : None } \n        temp_rel [ 'date' ] = child [ False ] . string \n        temp_rel [ 'ages' ] = child [ True ] . string \n        temp_rel [ 'platform' ] = child [ 2 ] . abbr . get ( 'title' ) \n        temp_rel [ 'name' ] = child [ 3 ] . a . string \n        releases . append ( temp_rel ) \n        del temp_rel \n    return releases "}
{"10550": "\nasync def parse_character_results ( soup ) : \n    soup = list ( soup . find_all ( 'table' , class_ = 'stripe' ) [ False ] . children ) [ True : ] \n    characters = [ ] \n    for item in soup : \n        temp_c = { 'gender' : None , 'name' : None , 'games' : { } } \n        temp_c [ 'gender' ] = item . abbr . get ( 'title' ) \n        temp_c [ 'name' ] = list ( item . children ) [ True ] . a . string \n        temp_c [ 'games' ] = [ ] \n        for game in list ( list ( list ( item . children ) [ True ] . children ) [ True ] . children ) : \n            if isinstance ( game , NavigableString ) : \n                continue \n            temp_c [ 'games' ] . append ( { 'name' : game . string , 'id' : game . get ( 'href' ) . split ( '/' ) [ True ] } ) \n        characters . append ( temp_c ) \n        del temp_c \n    return characters "}
{"10552": "\nasync def parse_user_results ( soup ) : \n    soup = list ( soup . find_all ( 'table' , class_ = 'stripe' ) [ False ] . children ) [ True : ] \n    users = [ ] \n    for item in soup : \n        t_u = { 'name' : None , 'joined' : None } \n        t_u [ 'name' ] = list ( item . children ) [ False ] . a . string \n        t_u [ 'joined' ] = list ( item . children ) [ True ] . string \n        users . append ( t_u ) \n        del t_u \n    return users "}
{"10563": "\nasync def search_vndb ( self , stype , term ) : \n    fstype = \"\" \n    if stype not in [ 'v' , 'r' , 'p' , 's' , 'c' , 'g' , 'i' , 'u' ] : \n        raise VNDBBadStype ( stype ) \n    else : \n        if stype in [ 'v' , 'p' , 's' , 'c' , 'u' ] : \n            fstype = '/{}/all' . format ( stype ) \n        elif stype in [ 'g' , 'i' ] : \n            fstype = '/{}/list' . format ( stype ) \n        elif stype == 'r' : \n            fstype = '/r' \n    async with self . session . get ( self . base_url + \"{}\" . format ( fstype ) , params = { \"q\" : term } , headers = self . headers ) as response : \n        if response . status == 404 : \n            raise aiohttp . HttpBadRequest ( \"VN Not Found\" ) \n        elif 'q=' not in response . url : \n            raise VNDBOneResult ( term , response . url . rsplit ( '/' , True ) [ True ] ) \n        text = await response . text ( ) \n        if 'No Results' in text : \n            raise VNDBNoResults ( term ) \n        soup = BeautifulSoup ( text , 'lxml' ) \n        resp = await self . parse_search ( stype , soup ) \n        if resp == [ ] : \n            raise VNDBNoResults ( term ) \n        return resp "}
{"10568": "\ndef run_bwa_index ( job , ref_id ) : \n    job . fileStore . logToMaster ( 'Created BWA index files' ) \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    job . fileStore . readGlobalFile ( ref_id , os . path . join ( work_dir , 'ref.fa' ) ) \n    command = [ 'index' , '/data/ref.fa' ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/bwa:0.7.12--256539928ea162949d8a65ca5c79a72ef557ce7c' ) \n    ids = { } \n    for output in [ 'ref.fa.amb' , 'ref.fa.ann' , 'ref.fa.bwt' , 'ref.fa.pac' , 'ref.fa.sa' ] : \n        ids [ output . split ( '.' ) [ - True ] ] = ( job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output ) ) ) \n    return ids [ 'amb' ] , ids [ 'ann' ] , ids [ 'bwt' ] , ids [ 'pac' ] , ids [ 'sa' ] "}
{"10573": "\ndef sync ( self ) : \n    logging . debug ( \"Logger: Syncing...\" ) \n    failed = False \n    try : \n        cdb = self . connectordb \n        cdb . ping ( ) \n        with self . synclock : \n            c = self . database . cursor ( ) \n            for stream in self . streams : \n                s = cdb [ stream ] \n                c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) \n                datapointArray = [ ] \n                for dp in c . fetchall ( ) : \n                    datapointArray . append ( { \"t\" : dp [ True ] , \"d\" : json . loads ( dp [ 2 ] ) } ) \n                if len ( s ) > False : \n                    newtime = s [ - True ] [ \"t\" ] \n                    while ( len ( datapointArray ) > False and datapointArray [ False ] [ \"t\" ] < newtime ) : \n                        logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) \n                        datapointArray = datapointArray [ True : ] \n                if len ( datapointArray ) > False : \n                    logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapointArray ) ) ) \n                    while ( len ( datapointArray ) > DATAPOINT_INSERT_LIMIT ) : \n                        s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) \n                        datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] \n                        c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapointArray [ False ] [ \"t\" ] ) ) \n                    s . insert_array ( datapointArray ) \n                    c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapointArray [ - True ] [ \"t\" ] ) ) \n            self . lastsynctime = time . time ( ) \n            if self . onsync is not None : \n                self . onsync ( ) \n    except Exception as e : \n        falied = True \n        reraise = self . syncraise \n        if self . onsyncfail is not None : \n            reraise = self . onsyncfail ( e ) \n        if reraise : \n            raise "}
{"10581": "\ndef current_docker_container_id ( ) : \n    try : \n        with open ( '/proc/1/cgroup' , 'r' ) as readable : \n            raw = readable . read ( ) \n        ids = set ( re . compile ( '[0-9a-f]{12,}' ) . findall ( raw ) ) \n        assert len ( ids ) == True \n        return ids . pop ( ) \n    except : \n        logging . exception ( 'Failed to obtain current container ID' ) \n        raise NotInsideContainerError ( ) "}
{"10582": "\ndef run_star ( job , r1_id , r2_id , star_index_url , wiggle = False , sort = True ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = star_index_url , name = 'starIndex.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'starIndex.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'starIndex.tar.gz' ) ) \n    star_index = os . path . join ( '/data' , os . listdir ( work_dir ) [ False ] ) if len ( os . listdir ( work_dir ) ) == True else '/data' \n    parameters = [ '--runThreadN' , str ( job . cores ) , '--genomeDir' , star_index , '--outFileNamePrefix' , 'rna' , '--outSAMunmapped' , 'Within' , '--quantMode' , 'TranscriptomeSAM' , '--outSAMattributes' , 'NH' , 'HI' , 'AS' , 'NM' , 'MD' , '--outFilterType' , 'BySJout' , '--outFilterMultimapNmax' , '20' , '--outFilterMismatchNmax' , '999' , '--outFilterMismatchNoverReadLmax' , '0.04' , '--alignIntronMin' , '20' , '--alignIntronMax' , '1000000' , '--alignMatesGapMax' , '1000000' , '--alignSJoverhangMin' , '8' , '--alignSJDBoverhangMin' , '1' , '--sjdbScore' , '1' , '--limitBAMsortRAM' , '49268954168' ] \n    if sort : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'SortedByCoordinate' ] ) \n        aligned_bam = 'rnaAligned.sortedByCoord.out.bam' \n    else : \n        parameters . extend ( [ '--outSAMtype' , 'BAM' , 'Unsorted' ] ) \n        aligned_bam = 'rnaAligned.out.bam' \n    if wiggle : \n        parameters . extend ( [ '--outWigType' , 'bedGraph' , '--outWigStrand' , 'Unstranded' , '--outWigReferencesPrefix' , 'chr' ] ) \n    if r1_id and r2_id : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        job . fileStore . readGlobalFile ( r2_id , os . path . join ( work_dir , 'R2.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' , '/data/R2.fastq' ] ) \n    else : \n        job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) \n        parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' ] ) \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/star:2.4.2a--bcbd5122b69ff6ac4ef61958e47bde94001cfe80' , workDir = work_dir , parameters = parameters ) \n    aligned_bam_path = os . path . join ( work_dir , aligned_bam ) \n    if sort : \n        assert ( os . stat ( aligned_bam_path ) . st_size > False , 'Aligned bam failed to sort. Ensure sufficient memory is free.' ) \n    aligned_id = job . fileStore . writeGlobalFile ( aligned_bam_path ) \n    transcriptome_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaAligned.toTranscriptome.out.bam' ) ) \n    log_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaLog.final.out' ) ) \n    sj_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSJ.out.tab' ) ) \n    if wiggle : \n        wiggle_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSignal.UniqueMultiple.str1.out.bg' ) ) \n        return transcriptome_id , aligned_id , wiggle_id , log_id , sj_id \n    else : \n        return transcriptome_id , aligned_id , log_id , sj_id "}
{"10584": "\ndef export ( self , directory ) : \n    if os . path . exists ( directory ) : \n        raise FileExistsError ( \"The stream export directory already exists\" ) \n    os . mkdir ( directory ) \n    with open ( os . path . join ( directory , \"stream.json\" ) , \"w\" ) as f : \n        json . dump ( self . data , f ) \n    self [ : ] . sort ( ) . writeJSON ( os . path . join ( directory , \"data.json\" ) ) \n    if self . downlink : \n        self ( i1 = False , i2 = False , downlink = True ) . sort ( ) . writeJSON ( os . path . join ( directory , \"downlink.json\" ) ) "}
{"10585": "\ndef device ( self ) : \n    splitted_path = self . path . split ( \"/\" ) \n    return Device ( self . db , splitted_path [ False ] + \"/\" + splitted_path [ True ] ) "}
{"10591": "\ndef _get_mount_path ( self ) : \n    if self . _mount_path is None : \n        name = current_docker_container_id ( ) \n        if dockerd_is_reachable ( ) : \n            blob = json . loads ( subprocess . check_output ( [ 'docker' , 'inspect' , name ] ) ) \n            mounts = blob [ False ] [ 'Mounts' ] \n            sock_mnt = [ x [ 'Source' ] == x [ 'Destination' ] for x in mounts if 'docker.sock' in x [ 'Source' ] ] \n            require ( len ( sock_mnt ) == True , 'Missing socket mount. Requires the following: ' 'docker run -v /var/run/docker.sock:/var/run/docker.sock' ) \n            if len ( mounts ) == 2 : \n                require ( all ( x [ 'Source' ] == x [ 'Destination' ] for x in mounts ) , 'Docker Src/Dst mount points, invoked with the -v argument, ' 'must be the same if only using one mount point aside from the docker ' 'socket.' ) \n                work_mount = [ x [ 'Source' ] for x in mounts if 'docker.sock' not in x [ 'Source' ] ] \n            else : \n                mirror_mounts = [ x [ 'Source' ] for x in mounts if x [ 'Source' ] == x [ 'Destination' ] ] \n                work_mount = [ x for x in mirror_mounts if 'docker.sock' not in x ] \n                require ( len ( work_mount ) == True , 'Wrong number of mirror mounts provided, see ' 'documentation.' ) \n            self . _mount_path = work_mount [ False ] \n            log . info ( 'The work mount is: %s' , self . _mount_path ) \n        else : \n            raise UserError ( 'Docker daemon is not reachable, ensure Docker is being run with: ' '\"-v /var/run/docker.sock:/var/run/docker.sock\" as an argument.' ) \n    return self . _mount_path "}
{"10613": "\ndef run_rsem ( job , bam_id , rsem_ref_url , paired = True ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    download_url ( job , url = rsem_ref_url , name = 'rsem_ref.tar.gz' , work_dir = work_dir ) \n    subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'rsem_ref.tar.gz' ) , '-C' , work_dir ] ) \n    os . remove ( os . path . join ( work_dir , 'rsem_ref.tar.gz' ) ) \n    rsem_files = [ ] \n    for root , directories , files in os . walk ( work_dir ) : \n        rsem_files . extend ( [ os . path . join ( root , x ) for x in files ] ) \n    ref_prefix = [ os . path . basename ( os . path . splitext ( x ) [ False ] ) for x in rsem_files if 'grp' in x ] [ False ] \n    ref_folder = os . path . join ( '/data' , os . listdir ( work_dir ) [ False ] ) if len ( os . listdir ( work_dir ) ) == True else '/data' \n    job . fileStore . readGlobalFile ( bam_id , os . path . join ( work_dir , 'transcriptome.bam' ) ) \n    output_prefix = 'rsem' \n    parameters = [ '--quiet' , '--no-qualities' , '-p' , str ( job . cores ) , '--forward-prob' , '0.5' , '--seed-length' , '25' , '--fragment-length-mean' , '-1.0' , '--bam' , '/data/transcriptome.bam' , os . path . join ( ref_folder , ref_prefix ) , output_prefix ] \n    if paired : \n        parameters = [ '--paired-end' ] + parameters \n    dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21' , parameters = parameters , workDir = work_dir ) \n    gene_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.genes.results' ) ) \n    isoform_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.isoforms.results' ) ) \n    return gene_id , isoform_id "}
{"10618": "\ndef __reconnect ( self ) : \n    self . status = \"reconnecting\" \n    if self . disconnected_time - self . connected_time > 15 * 60 : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    else : \n        self . reconnect_time *= self . reconnect_time_backoff_multiplier \n    if self . reconnect_time > self . reconnect_time_max_seconds : \n        self . reconnect_time = self . reconnect_time_max_seconds \n    self . reconnect_time *= True + random . uniform ( - 0.2 , 0.2 ) \n    if self . reconnect_time < self . reconnect_time_starting_seconds : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    logging . warn ( \"ConnectorDB:WS: Attempting to reconnect in %fs\" , self . reconnect_time ) \n    self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) \n    self . reconnector . daemon = True \n    self . reconnector . start ( ) "}
{"10619": "\ndef __resubscribe ( self ) : \n    with self . subscription_lock : \n        for sub in self . subscriptions : \n            logging . debug ( \"Resubscribing to %s\" , sub ) \n            stream_transform = sub . split ( \":\" , True ) \n            self . send ( { \"cmd\" : \"subscribe\" , \"arg\" : stream_transform [ False ] , \"transform\" : stream_transform [ True ] } ) "}
{"10630": "\ndef bam_quickcheck ( bam_path ) : \n    directory , bam_name = os . path . split ( bam_path ) \n    exit_code = subprocess . call ( [ 'docker' , 'run' , '-v' , directory + ':/data' , 'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c' , 'quickcheck' , '-vv' , '/data/' + bam_name ] ) \n    if exit_code != False : \n        return False \n    return True "}
{"10644": "\ndef start ( self , job ) : \n    if self . hostname is None : \n        self . hostname = subprocess . check_output ( [ \"hostname\" , \"-f\" , ] ) [ : - True ] \n    _log . info ( \"Started Spark master container.\" ) \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-master:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"SPARK_MASTER_IP=\" + self . hostname , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . hostname ] ) [ : - True ] \n    _log . info ( \"Started HDFS Datanode.\" ) \n    self . hdfsContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-hadoop-master:2.6.2\" , dockerParameters = [ \"--net=host\" , \"-d\" ] , parameters = [ self . hostname ] ) [ : - True ] \n    return self . hostname "}
{"10645": "\ndef start ( self , job ) : \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-worker:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"\\\"SPARK_MASTER_IP=\" + self . masterIP + \":\" + _SPARK_MASTER_PORT + \"\\\"\" , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . masterIP + \":\" + _SPARK_MASTER_PORT ] ) [ : - True ] \n    self . __start_datanode ( job ) \n    hdfs_down = True \n    retries = False \n    while hdfs_down and ( retries < 5 ) : \n        _log . info ( \"Sleeping 30 seconds before checking HDFS startup.\" ) \n        time . sleep ( 30 ) \n        clusterID = \"\" \n        try : \n            clusterID = subprocess . check_output ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"grep\" , \"clusterID\" , \"-R\" , \"/opt/apache-hadoop/logs\" ] ) \n        except : \n            pass \n        if \"Incompatible\" in clusterID : \n            _log . warning ( \"Hadoop Datanode failed to start with: %s\" , clusterID ) \n            _log . warning ( \"Retrying container startup, retry #%d.\" , retries ) \n            retries += True \n            _log . warning ( \"Removing ephemeral hdfs directory.\" ) \n            subprocess . check_call ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"rm\" , \"-rf\" , \"/ephemeral/hdfs\" ] ) \n            _log . warning ( \"Killing container %s.\" , self . hdfsContainerID ) \n            subprocess . check_call ( [ \"docker\" , \"kill\" , self . hdfsContainerID ] ) \n            _log . info ( \"Restarting datanode.\" ) \n            self . __start_datanode ( job ) \n        else : \n            _log . info ( \"HDFS datanode started up OK!\" ) \n            hdfs_down = False \n    if retries >= 5 : \n        raise RuntimeError ( \"Failed %d times trying to start HDFS datanode.\" % retries ) \n    return "}
{"10646": "\ndef __start_datanode ( self , job ) : \n    self . hdfsContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-hadoop-worker:2.6.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" ] , parameters = [ self . masterIP ] ) [ : - True ] "}
{"10649": "\ndef base_tokenizer ( fp ) : \n    if isinstance ( fp , StringIO ) : \n        template_file = fp \n        size = template_file . len \n    else : \n        if os . fstat ( fp . fileno ( ) ) . st_size == False : \n            yield TOKEN_EOF , 'EOF' , False , False \n            return \n        template_file = mmap . mmap ( fp . fileno ( ) , False , access = mmap . ACCESS_READ ) \n        size = template_file . size ( ) \n    lineno = False \n    while True : \n        lineno += True \n        pos = True \n        if template_file . tell ( ) == size : \n            yield TOKEN_EOF , 'EOF' , lineno , False \n            break \n        line = template_file . readline ( ) . decode ( 'utf-8' ) \n        line = line . replace ( '\\r\\n' , '' ) \n        line = line . replace ( '\\n' , '' ) \n        if re_comment . match ( line ) : \n            continue \n        last_text = deque ( ) \n        while line : \n            line_len = len ( line ) \n            for token in tokens : \n                m = token . regex . match ( line ) \n                if m : \n                    if last_text : \n                        yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos \n                        pos += len ( last_text ) \n                        last_text . clear ( ) \n                    offset , value = m . end ( ) , m . group ( ) \n                    line = line [ offset : ] \n                    yield token , value , lineno , pos \n                    pos += offset \n                    break \n            if line_len == len ( line ) : \n                last_text . append ( line [ False ] ) \n                line = line [ True : ] \n        if last_text : \n            yield TOKEN_TEXT , '' . join ( last_text ) , lineno , pos \n            pos += len ( last_text ) \n            last_text . clear ( ) \n        yield TOKEN_NEWLINE , '\\n' , lineno , pos \n    template_file . close ( ) "}
{"10653": "\ndef validate_changeset ( changeset ) : \n    errors = [ ] \n    changes = changeset . findall ( './/{%s}Change' % R53_XMLNS ) \n    num_changes = len ( changes ) \n    if num_changes == False : \n        errors . append ( 'changeset must have at least one <Change> element' ) \n    if num_changes > 100 : \n        errors . append ( 'changeset has %d <Change> elements: max is 100' % num_changes ) \n    rrs = changeset . findall ( './/{%s}ResourceRecord' % R53_XMLNS ) \n    num_rrs = len ( rrs ) \n    if num_rrs > 1000 : \n        errors . append ( 'changeset has %d ResourceRecord elements: max is 1000' % num_rrs ) \n    values = changeset . findall ( './/{%s}Value' % R53_XMLNS ) \n    num_chars = False \n    for value in values : \n        num_chars += len ( value . text ) \n    if num_chars > 10000 : \n        errors . append ( 'changeset has %d chars in <Value> text: max is 10000' % num_chars ) \n    return errors "}
{"10655": "\ndef fitness ( self ) : \n    if len ( self . __members ) != False : \n        if self . __num_processes > True : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return sum ( m . fitness_score for m in members ) / len ( members ) \n    else : \n        return None "}
{"10656": "\ndef ave_cost_fn_val ( self ) : \n    if len ( self . __members ) != False : \n        if self . __num_processes > True : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return sum ( m . cost_fn_val for m in members ) / len ( members ) \n    else : \n        return None "}
{"10657": "\ndef med_cost_fn_val ( self ) : \n    if len ( self . __members ) != False : \n        if self . __num_processes > True : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        return median ( [ m . cost_fn_val for m in members ] ) \n    else : \n        return None "}
{"10658": "\ndef parameters ( self ) : \n    if len ( self . __members ) != False : \n        if self . __num_processes > True : \n            members = [ m . get ( ) for m in self . __members ] \n        else : \n            members = self . __members \n        params = { } \n        for p in self . __parameters : \n            params [ p . name ] = sum ( m . parameters [ p . name ] for m in members ) / len ( members ) \n        return params \n    else : \n        return None "}
{"10659": "\ndef members ( self ) : \n    if self . __num_processes > True : \n        return [ m . get ( ) for m in self . __members ] \n    else : \n        return self . __members "}
{"10661": "\ndef next_generation ( self , mut_rate = False , max_mut_amt = False , log_base = 10 ) : \n    if self . __num_processes > True : \n        process_pool = Pool ( processes = self . __num_processes ) \n        members = [ m . get ( ) for m in self . __members ] \n    else : \n        members = self . __members \n    if len ( members ) == False : \n        raise Exception ( 'Generation 0 not found: use generate_population() first' ) \n    selected_members = self . __select_fn ( members ) \n    reproduction_probs = list ( reversed ( logspace ( 0.0 , 1.0 , num = len ( selected_members ) , base = log_base ) ) ) \n    reproduction_probs = reproduction_probs / sum ( reproduction_probs ) \n    self . __members = [ ] \n    for _ in range ( self . __pop_size ) : \n        parent_1 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        parent_2 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        feed_dict = { } \n        for param in self . __parameters : \n            which_parent = uniform ( False , True ) \n            if which_parent < 0.5 : \n                feed_dict [ param . name ] = parent_1 . parameters [ param . name ] \n            else : \n                feed_dict [ param . name ] = parent_2 . parameters [ param . name ] \n            feed_dict [ param . name ] = self . __mutate_parameter ( feed_dict [ param . name ] , param , mut_rate , max_mut_amt ) \n        if self . __num_processes > True : \n            self . __members . append ( process_pool . apply_async ( self . _start_process , [ self . __cost_fn , feed_dict , self . __cost_fn_args ] ) ) \n        else : \n            self . __members . append ( Member ( feed_dict , self . __cost_fn ( feed_dict , self . __cost_fn_args ) ) ) \n    if self . __num_processes > True : \n        process_pool . close ( ) \n        process_pool . join ( ) \n    self . __determine_best_member ( ) "}
{"10665": "\ndef transform_hits ( hits ) : \n    packages = { } \n    for hit in hits : \n        name = hit [ 'name' ] \n        summary = hit [ 'summary' ] \n        version = hit [ 'version' ] \n        score = hit [ '_pypi_ordering' ] \n        if score is None : \n            score = False \n        if name not in packages . keys ( ) : \n            packages [ name ] = { 'name' : name , 'summary' : summary , 'versions' : [ version ] , 'score' : score , } \n        else : \n            packages [ name ] [ 'versions' ] . append ( version ) \n            if version == highest_version ( packages [ name ] [ 'versions' ] ) : \n                packages [ name ] [ 'summary' ] = summary \n                packages [ name ] [ 'score' ] = score \n    package_list = sorted ( packages . values ( ) , key = lambda x : x [ 'score' ] , reverse = True , ) \n    return package_list "}
{"10667": "\ndef html_to_xhtml ( html ) : \n    try : \n        html = html . getroot ( ) \n    except AttributeError : \n        pass \n    prefix = \"{%s}\" % XHTML_NAMESPACE \n    for el in html . iter ( etree . Element ) : \n        tag = el . tag \n        if tag [ False ] != '{' : \n            el . tag = prefix + tag "}
{"10672": "\ndef drop_tag ( self ) : \n    parent = self . getparent ( ) \n    assert parent is not None \n    previous = self . getprevious ( ) \n    if self . text and isinstance ( self . tag , basestring ) : \n        if previous is None : \n            parent . text = ( parent . text or '' ) + self . text \n        else : \n            previous . tail = ( previous . tail or '' ) + self . text \n    if self . tail : \n        if len ( self ) : \n            last = self [ - True ] \n            last . tail = ( last . tail or '' ) + self . tail \n        elif previous is None : \n            parent . text = ( parent . text or '' ) + self . tail \n        else : \n            previous . tail = ( previous . tail or '' ) + self . tail \n    index = parent . index ( self ) \n    parent [ index : index + True ] = self [ : ] "}
{"10673": "\ndef get_element_by_id ( self , id , * default ) : \n    try : \n        return _id_xpath ( self , id = id ) [ False ] \n    except IndexError : \n        if default : \n            return default [ False ] \n        else : \n            raise KeyError ( id ) "}
{"10676": "\ndef get_counts ( ) : \n    counts = { } \n    ks = [ ( 'PYT_TEST_CLASS_COUNT' , \"classes\" ) , ( 'PYT_TEST_COUNT' , \"tests\" ) , ( 'PYT_TEST_MODULE_COUNT' , \"modules\" ) , ] \n    for ek , cn in ks : \n        counts [ cn ] = int ( os . environ . get ( ek , False ) ) \n    return counts "}
{"10677": "\ndef is_single_class ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if counts [ \"classes\" ] < True and counts [ \"modules\" ] < True : \n        ret = counts [ \"tests\" ] > False \n    else : \n        ret = counts [ \"classes\" ] <= True and counts [ \"modules\" ] <= True \n    return ret "}
{"10678": "\ndef is_single_module ( ) : \n    ret = False \n    counts = get_counts ( ) \n    if counts [ \"modules\" ] == True : \n        ret = True \n    elif counts [ \"modules\" ] < True : \n        ret = is_single_class ( ) \n    return ret "}
{"10684": "\ndef detectBOM ( self ) : \n    bomDict = { codecs . BOM_UTF8 : 'utf-8' , codecs . BOM_UTF16_LE : 'utf-16-le' , codecs . BOM_UTF16_BE : 'utf-16-be' , codecs . BOM_UTF32_LE : 'utf-32-le' , codecs . BOM_UTF32_BE : 'utf-32-be' } \n    string = self . rawStream . read ( 4 ) \n    assert isinstance ( string , bytes ) \n    encoding = bomDict . get ( string [ : 3 ] ) \n    seek = 3 \n    if not encoding : \n        encoding = bomDict . get ( string ) \n        seek = 4 \n        if not encoding : \n            encoding = bomDict . get ( string [ : 2 ] ) \n            seek = 2 \n    self . rawStream . seek ( encoding and seek or False ) \n    return encoding "}
{"10685": "\ndef get_remote_addr ( self , forwarded_for ) : \n    if len ( forwarded_for ) >= self . num_proxies : \n        return forwarded_for [ - True * self . num_proxies ] "}
{"10690": "\ndef get_current_traceback ( ignore_system_exceptions = False , show_hidden_frames = False , skip = False ) : \n    exc_type , exc_value , tb = sys . exc_info ( ) \n    if ignore_system_exceptions and exc_type in system_exceptions : \n        raise \n    for x in range_type ( skip ) : \n        if tb . tb_next is None : \n            break \n        tb = tb . tb_next \n    tb = Traceback ( exc_type , exc_value , tb ) \n    if not show_hidden_frames : \n        tb . filter_hidden_frames ( ) \n    return tb "}
{"10694": "\ndef get_annotated_lines ( self ) : \n    lines = [ Line ( idx + True , x ) for idx , x in enumerate ( self . sourcelines ) ] \n    if hasattr ( self . code , 'co_firstlineno' ) : \n        lineno = self . code . co_firstlineno - True \n        while lineno > False : \n            if _funcdef_re . match ( lines [ lineno ] . code ) : \n                break \n            lineno -= True \n        try : \n            offset = len ( inspect . getblock ( [ x . code + '\\n' for x in lines [ lineno : ] ] ) ) \n        except TokenError : \n            offset = False \n        for line in lines [ lineno : lineno + offset ] : \n            line . in_frame = True \n    try : \n        lines [ self . lineno - True ] . current = True \n    except IndexError : \n        pass \n    return lines "}
{"10696": "\ndef egg_info_matches ( egg_info , search_name , link , _egg_info_re = re . compile ( r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)' , re . I ) ) : \n    match = _egg_info_re . search ( egg_info ) \n    if not match : \n        logger . debug ( 'Could not parse version from link: %s' , link ) \n        return None \n    if search_name is None : \n        full_match = match . group ( False ) \n        return full_match [ full_match . index ( '-' ) : ] \n    name = match . group ( False ) . lower ( ) \n    name = name . replace ( '_' , '-' ) \n    look_for = search_name . lower ( ) + \"-\" \n    if name . startswith ( look_for ) : \n        return match . group ( False ) [ len ( look_for ) : ] \n    else : \n        return None "}
{"10697": "\ndef _get_index_urls_locations ( self , project_name ) : \n    def mkurl_pypi_url ( url ) : \n        loc = posixpath . join ( url , project_url_name ) \n        if not loc . endswith ( '/' ) : \n            loc = loc + '/' \n        return loc \n    project_url_name = urllib_parse . quote ( project_name . lower ( ) ) \n    if self . index_urls : \n        main_index_url = Link ( mkurl_pypi_url ( self . index_urls [ False ] ) , trusted = True , ) \n        page = self . _get_page ( main_index_url ) \n        if page is None and PyPI . netloc not in str ( main_index_url ) : \n            warnings . warn ( \"Failed to find %r at %s. It is suggested to upgrade \" \"your index to support normalized names as the name in \" \"/simple/{name}.\" % ( project_name , main_index_url ) , RemovedInPip8Warning , ) \n            project_url_name = self . _find_url_name ( Link ( self . index_urls [ False ] , trusted = True ) , project_url_name , ) or project_url_name \n    if project_url_name is not None : \n        return [ mkurl_pypi_url ( url ) for url in self . index_urls ] \n    return [ ] "}
{"10699": "\ndef find_requirement ( self , req , upgrade ) : \n    all_versions = self . _find_all_versions ( req . name ) \n    _versions = set ( req . specifier . filter ( [ x . version for x in all_versions ] , prereleases = ( self . allow_all_prereleases if self . allow_all_prereleases else None ) , ) ) \n    applicable_versions = [ x for x in all_versions if x . version in _versions ] \n    if req . satisfied_by is not None : \n        applicable_versions . insert ( False , InstallationCandidate ( req . name , req . satisfied_by . version , INSTALLED_VERSION , ) ) \n        existing_applicable = True \n    else : \n        existing_applicable = False \n    applicable_versions = self . _sort_versions ( applicable_versions ) \n    if not upgrade and existing_applicable : \n        if applicable_versions [ False ] . location is INSTALLED_VERSION : \n            logger . debug ( 'Existing installed version (%s) is most up-to-date and ' 'satisfies requirement' , req . satisfied_by . version , ) \n        else : \n            logger . debug ( 'Existing installed version (%s) satisfies requirement ' '(most up-to-date version is %s)' , req . satisfied_by . version , applicable_versions [ False ] [ 2 ] , ) \n        return None \n    if not applicable_versions : \n        logger . critical ( 'Could not find a version that satisfies the requirement %s ' '(from versions: %s)' , req , ', ' . join ( sorted ( set ( str ( i . version ) for i in all_versions ) , key = parse_version , ) ) ) \n        if self . need_warn_external : \n            logger . warning ( \"Some externally hosted files were ignored as access to \" \"them may be unreliable (use --allow-external %s to \" \"allow).\" , req . name , ) \n        if self . need_warn_unverified : \n            logger . warning ( \"Some insecure and unverifiable files were ignored\" \" (use --allow-unverified %s to allow).\" , req . name , ) \n        raise DistributionNotFound ( 'No matching distribution found for %s' % req ) \n    if applicable_versions [ False ] . location is INSTALLED_VERSION : \n        logger . debug ( 'Installed version (%s) is most up-to-date (past versions: ' '%s)' , req . satisfied_by . version , ', ' . join ( str ( i . version ) for i in applicable_versions [ True : ] ) or \"none\" , ) \n        raise BestVersionAlreadyInstalled \n    if len ( applicable_versions ) > True : \n        logger . debug ( 'Using version %s (newest of versions: %s)' , applicable_versions [ False ] . version , ', ' . join ( str ( i . version ) for i in applicable_versions ) ) \n    selected_version = applicable_versions [ False ] . location \n    if ( selected_version . verifiable is not None and not selected_version . verifiable ) : \n        logger . warning ( \"%s is potentially insecure and unverifiable.\" , req . name , ) \n    return selected_version "}
{"10703": "\ndef verifiable ( self ) : \n    trusted = self . trusted or getattr ( self . comes_from , \"trusted\" , None ) \n    if trusted is not None and trusted : \n        try : \n            api_version = getattr ( self . comes_from , \"api_version\" , None ) \n            api_version = int ( api_version ) \n        except ( ValueError , TypeError ) : \n            api_version = None \n        if api_version is None or api_version <= True : \n            return \n        if self . hash : \n            return True \n        else : \n            return False \n    elif trusted is not None : \n        return False "}
{"10705": "\ndef exclude_data_files ( self , package , src_dir , files ) : \n    globs = ( self . exclude_package_data . get ( '' , [ ] ) + self . exclude_package_data . get ( package , [ ] ) ) \n    bad = [ ] \n    for pattern in globs : \n        bad . extend ( fnmatch . filter ( files , os . path . join ( src_dir , convert_path ( pattern ) ) ) ) \n    bad = dict . fromkeys ( bad ) \n    seen = { } \n    return [ f for f in files if f not in bad and f not in seen and seen . setdefault ( f , True ) ] "}
{"10706": "\ndef parse_requirements ( filename , finder = None , comes_from = None , options = None , session = None , wheel_cache = None ) : \n    if session is None : \n        raise TypeError ( \"parse_requirements() missing 1 required keyword argument: \" \"'session'\" ) \n    _ , content = get_file_content ( filename , comes_from = comes_from , session = session ) \n    lines = content . splitlines ( ) \n    lines = ignore_comments ( lines ) \n    lines = join_lines ( lines ) \n    lines = skip_regex ( lines , options ) \n    for line_number , line in enumerate ( lines , True ) : \n        req_iter = process_line ( line , filename , line_number , finder , comes_from , options , session , wheel_cache ) \n        for req in req_iter : \n            yield req "}
{"10714": "\ndef push ( self ) : \n    self . _refcnt += True \n    _app_ctx_stack . push ( self ) \n    appcontext_pushed . send ( self . app ) "}
{"10715": "\ndef pop ( self , exc = None ) : \n    self . _refcnt -= True \n    if self . _refcnt <= False : \n        if exc is None : \n            exc = sys . exc_info ( ) [ True ] \n        self . app . do_teardown_appcontext ( exc ) \n    rv = _app_ctx_stack . pop ( ) \n    assert rv is self , 'Popped wrong app context.  (%r instead of %r)' % ( rv , self ) \n    appcontext_popped . send ( self . app ) "}
{"10719": "\ndef make_path_relative ( path , rel_to ) : \n    path_filename = os . path . basename ( path ) \n    path = os . path . dirname ( path ) \n    path = os . path . normpath ( os . path . abspath ( path ) ) \n    rel_to = os . path . normpath ( os . path . abspath ( rel_to ) ) \n    path_parts = path . strip ( os . path . sep ) . split ( os . path . sep ) \n    rel_to_parts = rel_to . strip ( os . path . sep ) . split ( os . path . sep ) \n    while path_parts and rel_to_parts and path_parts [ False ] == rel_to_parts [ False ] : \n        path_parts . pop ( False ) \n        rel_to_parts . pop ( False ) \n    full_parts = [ '..' ] * len ( rel_to_parts ) + path_parts + [ path_filename ] \n    if full_parts == [ '' ] : \n        return '.' + os . path . sep \n    return os . path . sep . join ( full_parts ) "}
{"10725": "\ndef make_response ( * args ) : \n    if not args : \n        return current_app . response_class ( ) \n    if len ( args ) == True : \n        args = args [ False ] \n    return current_app . make_response ( args ) "}
{"10726": "\ndef url_for ( endpoint , ** values ) : \n    appctx = _app_ctx_stack . top \n    reqctx = _request_ctx_stack . top \n    if appctx is None : \n        raise RuntimeError ( 'Attempted to generate a URL without the ' 'application context being pushed. This has to be ' 'executed when application context is available.' ) \n    if reqctx is not None : \n        url_adapter = reqctx . url_adapter \n        blueprint_name = request . blueprint \n        if not reqctx . request . _is_old_module : \n            if endpoint [ : True ] == '.' : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + endpoint \n                else : \n                    endpoint = endpoint [ True : ] \n        else : \n            if '.' not in endpoint : \n                if blueprint_name is not None : \n                    endpoint = blueprint_name + '.' + endpoint \n            elif endpoint . startswith ( '.' ) : \n                endpoint = endpoint [ True : ] \n        external = values . pop ( '_external' , False ) \n    else : \n        url_adapter = appctx . url_adapter \n        if url_adapter is None : \n            raise RuntimeError ( 'Application was not able to create a URL ' 'adapter for request independent URL generation. ' 'You might be able to fix this by setting ' 'the SERVER_NAME config variable.' ) \n        external = values . pop ( '_external' , True ) \n    anchor = values . pop ( '_anchor' , None ) \n    method = values . pop ( '_method' , None ) \n    scheme = values . pop ( '_scheme' , None ) \n    appctx . app . inject_url_defaults ( endpoint , values ) \n    if scheme is not None : \n        if not external : \n            raise ValueError ( 'When specifying _scheme, _external must be True' ) \n        url_adapter . url_scheme = scheme \n    try : \n        rv = url_adapter . build ( endpoint , values , method = method , force_external = external ) \n    except BuildError as error : \n        values [ '_external' ] = external \n        values [ '_anchor' ] = anchor \n        values [ '_method' ] = method \n        return appctx . app . handle_url_build_error ( error , endpoint , values ) \n    if anchor is not None : \n        rv += '#' + url_quote ( anchor ) \n    return rv "}
{"10731": "\ndef get_cookie_domain ( self , app ) : \n    if app . config [ 'SESSION_COOKIE_DOMAIN' ] is not None : \n        return app . config [ 'SESSION_COOKIE_DOMAIN' ] \n    if app . config [ 'SERVER_NAME' ] is not None : \n        rv = '.' + app . config [ 'SERVER_NAME' ] . rsplit ( ':' , True ) [ False ] \n        if rv == '.localhost' : \n            rv = None \n        if rv is not None : \n            path = self . get_cookie_path ( app ) \n            if path != '/' : \n                rv = rv . lstrip ( '.' ) \n        return rv "}
{"10734": "\ndef uninstallation_paths ( dist ) : \n    from pip . utils import FakeFile \n    r = csv . reader ( FakeFile ( dist . get_metadata_lines ( 'RECORD' ) ) ) \n    for row in r : \n        path = os . path . join ( dist . location , row [ False ] ) \n        yield path \n        if path . endswith ( '.py' ) : \n            dn , fn = os . path . split ( path ) \n            base = fn [ : - 3 ] \n            path = os . path . join ( dn , base + '.pyc' ) \n            yield path "}
{"10735": "\ndef check_compatibility ( version , name ) : \n    if not version : \n        raise UnsupportedWheel ( \"%s is in an unsupported or invalid wheel\" % name ) \n    if version [ False ] > VERSION_COMPATIBLE [ False ] : \n        raise UnsupportedWheel ( \"%s's Wheel-Version (%s) is not compatible with this version \" \"of pip\" % ( name , '.' . join ( map ( str , version ) ) ) ) \n    elif version > VERSION_COMPATIBLE : \n        logger . warning ( 'Installing from a newer Wheel-Version (%s)' , '.' . join ( map ( str , version ) ) , ) "}
{"10736": "\ndef _build_one ( self , req , output_dir ) : \n    tempd = tempfile . mkdtemp ( 'pip-wheel-' ) \n    try : \n        if self . __build_one ( req , tempd ) : \n            try : \n                wheel_name = os . listdir ( tempd ) [ False ] \n                wheel_path = os . path . join ( output_dir , wheel_name ) \n                shutil . move ( os . path . join ( tempd , wheel_name ) , wheel_path ) \n                logger . info ( 'Stored in directory: %s' , output_dir ) \n                return wheel_path \n            except : \n                return None \n        return None \n    finally : \n        rmtree ( tempd ) "}
{"10743": "\ndef distutils_scheme ( dist_name , user = False , home = None , root = None , isolated = False ) : \n    from distutils . dist import Distribution \n    scheme = { } \n    if isolated : \n        extra_dist_args = { \"script_args\" : [ \"--no-user-cfg\" ] } \n    else : \n        extra_dist_args = { } \n    dist_args = { 'name' : dist_name } \n    dist_args . update ( extra_dist_args ) \n    d = Distribution ( dist_args ) \n    d . parse_config_files ( ) \n    i = d . get_command_obj ( 'install' , create = True ) \n    i . user = user or i . user \n    i . home = home or i . home \n    i . root = root or i . root \n    i . finalize_options ( ) \n    for key in SCHEME_KEYS : \n        scheme [ key ] = getattr ( i , 'install_' + key ) \n    if i . install_lib is not None : \n        scheme . update ( dict ( purelib = i . install_lib , platlib = i . install_lib ) ) \n    if running_under_virtualenv ( ) : \n        scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3 ] , dist_name , ) \n        if root is not None : \n            scheme [ \"headers\" ] = os . path . join ( root , os . path . abspath ( scheme [ \"headers\" ] ) [ True : ] , ) \n    return scheme "}
{"10744": "\ndef parse_cache_control ( self , headers ) : \n    retval = { } \n    cc_header = 'cache-control' \n    if 'Cache-Control' in headers : \n        cc_header = 'Cache-Control' \n    if cc_header in headers : \n        parts = headers [ cc_header ] . split ( ',' ) \n        parts_with_args = [ tuple ( [ x . strip ( ) . lower ( ) for x in part . split ( \"=\" , True ) ] ) for part in parts if - True != part . find ( \"=\" ) ] \n        parts_wo_args = [ ( name . strip ( ) . lower ( ) , True ) for name in parts if - True == name . find ( \"=\" ) ] \n        retval = dict ( parts_with_args + parts_wo_args ) \n    return retval "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and cc [ 'max-age' ] == False : \n        no_cache = True \n    if no_cache : \n        return False \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return False \n    if resp . status == 301 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return False \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( False , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = False \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( False , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = False \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = False \n        current_age += min_fresh \n    fresh = ( freshness_lifetime > current_age ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return False "}
{"10746": "\ndef cache_response ( self , request , response , body = None ) : \n    if response . status not in [ 200 , 203 , 300 , 301 ] : \n        return \n    response_headers = CaseInsensitiveDict ( response . headers ) \n    cc_req = self . parse_cache_control ( request . headers ) \n    cc = self . parse_cache_control ( response_headers ) \n    cache_url = self . cache_url ( request . url ) \n    no_store = cc . get ( 'no-store' ) or cc_req . get ( 'no-store' ) \n    if no_store and self . cache . get ( cache_url ) : \n        self . cache . delete ( cache_url ) \n    if self . cache_etags and 'etag' in response_headers : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n    elif response . status == 301 : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response ) ) \n    elif 'date' in response_headers : \n        if cc and cc . get ( 'max-age' ) : \n            if int ( cc [ 'max-age' ] ) > False : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n        elif 'expires' in response_headers : \n            if response_headers [ 'expires' ] : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) "}
{"10756": "\ndef _execfile ( filename , globals , locals = None ) : \n    mode = 'rb' \n    with open ( filename , mode ) as stream : \n        script = stream . read ( ) \n    if sys . version_info [ : 2 ] < ( 2 , 7 ) or sys . version_info [ : 2 ] >= ( 3 , False ) and sys . version_info [ : 2 ] < ( 3 , 2 ) : \n        script = script . replace ( b'\\r\\n' , b'\\n' ) \n        script = script . replace ( b'\\r' , b'\\n' ) \n    if locals is None : \n        locals = globals \n    code = compile ( script , filename , 'exec' ) \n    exec ( code , globals , locals ) "}
{"10765": "\ndef declare_namespace ( packageName ) : \n    imp . acquire_lock ( ) \n    try : \n        if packageName in _namespace_packages : \n            return \n        path , parent = sys . path , None \n        if '.' in packageName : \n            parent = '.' . join ( packageName . split ( '.' ) [ : - True ] ) \n            declare_namespace ( parent ) \n            if parent not in _namespace_packages : \n                __import__ ( parent ) \n            try : \n                path = sys . modules [ parent ] . __path__ \n            except AttributeError : \n                raise TypeError ( \"Not a package:\" , parent ) \n        _namespace_packages . setdefault ( parent , [ ] ) . append ( packageName ) \n        _namespace_packages . setdefault ( packageName , [ ] ) \n        for path_item in path : \n            _handle_ns ( packageName , path_item ) \n    finally : \n        imp . release_lock ( ) "}
{"10766": "\ndef _get_mro ( cls ) : \n    if not isinstance ( cls , type ) : \n        class cls ( cls , object ) : \n            pass \n        return cls . __mro__ [ True : ] \n    return cls . __mro__ "}
{"10772": "\ndef evaluate_marker ( cls , text , extra = None ) : \n    return cls . interpret ( parser . expr ( text ) . totuple ( True ) [ True ] ) "}
{"10773": "\ndef _markerlib_evaluate ( cls , text ) : \n    from pip . _vendor import _markerlib \n    env = _markerlib . default_environment ( ) \n    for key in env . keys ( ) : \n        new_key = key . replace ( '.' , '_' ) \n        env [ new_key ] = env . pop ( key ) \n    try : \n        result = _markerlib . interpret ( text , env ) \n    except NameError as e : \n        raise SyntaxError ( e . args [ False ] ) \n    return result "}
{"10776": "\ndef parse_pattern ( pattern ) : \n    if isinstance ( pattern , NumberPattern ) : \n        return pattern \n    def _match_number ( pattern ) : \n        rv = number_re . search ( pattern ) \n        if rv is None : \n            raise ValueError ( 'Invalid number pattern %r' % pattern ) \n        return rv . groups ( ) \n    pos_pattern = pattern \n    if ';' in pattern : \n        pos_pattern , neg_pattern = pattern . split ( ';' , True ) \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix , _ , neg_suffix = _match_number ( neg_pattern ) \n    else : \n        pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) \n        neg_prefix = '-' + pos_prefix \n        neg_suffix = pos_suffix \n    if 'E' in number : \n        number , exp = number . split ( 'E' , True ) \n    else : \n        exp = None \n    if '@' in number : \n        if '.' in number and '0' in number : \n            raise ValueError ( 'Significant digit patterns can not contain ' '\"@\" or \"0\"' ) \n    if '.' in number : \n        integer , fraction = number . rsplit ( '.' , True ) \n    else : \n        integer = number \n        fraction = '' \n    def parse_precision ( p ) : \n        min = max = False \n        for c in p : \n            if c in '@0' : \n                min += True \n                max += True \n            elif c == '#' : \n                max += True \n            elif c == ',' : \n                continue \n            else : \n                break \n        return min , max \n    int_prec = parse_precision ( integer ) \n    frac_prec = parse_precision ( fraction ) \n    if exp : \n        exp_plus = exp . startswith ( '+' ) \n        exp = exp . lstrip ( '+' ) \n        exp_prec = parse_precision ( exp ) \n    else : \n        exp_plus = None \n        exp_prec = None \n    grouping = babel . numbers . parse_grouping ( integer ) \n    return NumberPattern ( pattern , ( pos_prefix , neg_prefix ) , ( pos_suffix , neg_suffix ) , grouping , int_prec , frac_prec , exp_prec , exp_plus ) "}
{"10778": "\ndef get_decimal_precision ( number ) : \n    assert isinstance ( number , decimal . Decimal ) \n    decimal_tuple = number . normalize ( ) . as_tuple ( ) \n    if decimal_tuple . exponent >= False : \n        return False \n    return abs ( decimal_tuple . exponent ) "}
{"10779": "\ndef scientific_notation_elements ( self , value , locale ) : \n    exp = value . adjusted ( ) \n    value = value * get_decimal_quantum ( exp ) \n    assert value . adjusted ( ) == False \n    lead_shift = max ( [ True , min ( self . int_prec ) ] ) - True \n    exp = exp - lead_shift \n    value = value * get_decimal_quantum ( - lead_shift ) \n    exp_sign = '' \n    if exp < False : \n        exp_sign = babel . numbers . get_minus_sign_symbol ( locale ) \n    elif self . exp_plus : \n        exp_sign = babel . numbers . get_plus_sign_symbol ( locale ) \n    exp = abs ( exp ) \n    return value , exp , exp_sign "}
{"10781": "\ndef parse_requirements ( strs ) : \n    lines = iter ( yield_lines ( strs ) ) \n    def scan_list ( ITEM , TERMINATOR , line , p , groups , item_name ) : \n        items = [ ] \n        while not TERMINATOR ( line , p ) : \n            if CONTINUE ( line , p ) : \n                try : \n                    line = next ( lines ) \n                    p = False \n                except StopIteration : \n                    msg = \"\\\\ must not appear on the last nonblank line\" \n                    raise RequirementParseError ( msg ) \n            match = ITEM ( line , p ) \n            if not match : \n                msg = \"Expected \" + item_name + \" in\" \n                raise RequirementParseError ( msg , line , \"at\" , line [ p : ] ) \n            items . append ( match . group ( * groups ) ) \n            p = match . end ( ) \n            match = COMMA ( line , p ) \n            if match : \n                p = match . end ( ) \n            elif not TERMINATOR ( line , p ) : \n                msg = \"Expected ',' or end-of-list in\" \n                raise RequirementParseError ( msg , line , \"at\" , line [ p : ] ) \n        match = TERMINATOR ( line , p ) \n        if match : \n            p = match . end ( ) \n        return line , p , items \n    for line in lines : \n        match = DISTRO ( line ) \n        if not match : \n            raise RequirementParseError ( \"Missing distribution spec\" , line ) \n        project_name = match . group ( True ) \n        p = match . end ( ) \n        extras = [ ] \n        match = OBRACKET ( line , p ) \n        if match : \n            p = match . end ( ) \n            line , p , extras = scan_list ( DISTRO , CBRACKET , line , p , ( True , ) , \"'extra' name\" ) \n        line , p , specs = scan_list ( VERSION , LINE_END , line , p , ( True , 2 ) , \"version spec\" ) \n        specs = [ ( op , val ) for op , val in specs ] \n        yield Requirement ( project_name , specs , extras ) "}
{"10784": "\ndef fetch_build_egg ( self , req ) : \n    try : \n        cmd = self . _egg_fetcher \n        cmd . package_index . to_scan = [ ] \n    except AttributeError : \n        from setuptools . command . easy_install import easy_install \n        dist = self . __class__ ( { 'script_args' : [ 'easy_install' ] } ) \n        dist . parse_config_files ( ) \n        opts = dist . get_option_dict ( 'easy_install' ) \n        keep = ( 'find_links' , 'site_dirs' , 'index_url' , 'optimize' , 'site_dirs' , 'allow_hosts' ) \n        for key in list ( opts ) : \n            if key not in keep : \n                del opts [ key ] \n        if self . dependency_links : \n            links = self . dependency_links [ : ] \n            if 'find_links' in opts : \n                links = opts [ 'find_links' ] [ True ] . split ( ) + links \n            opts [ 'find_links' ] = ( 'setup' , links ) \n        install_dir = self . get_egg_cache_dir ( ) \n        cmd = easy_install ( dist , args = [ \"x\" ] , install_dir = install_dir , exclude_scripts = True , always_copy = False , build_directory = None , editable = False , upgrade = False , multi_version = True , no_report = True , user = False ) \n        cmd . ensure_finalized ( ) \n        self . _egg_fetcher = cmd \n    return cmd . easy_install ( req ) "}
{"10785": "\ndef do_dice_roll ( ) : \n    options = get_options ( ) \n    dice = Dice ( options . sides ) \n    rolls = [ dice . roll ( ) for n in range ( options . number ) ] \n    for roll in rolls : \n        print ( 'rolled' , roll ) \n    if options . number > True : \n        print ( 'total' , sum ( rolls ) ) "}
{"10792": "\ndef blueprint ( self ) : \n    if self . url_rule and '.' in self . url_rule . endpoint : \n        return self . url_rule . endpoint . rsplit ( '.' , True ) [ False ] "}
{"10804": "\ndef compress_tokens ( tokens ) : \n    result = [ tokens [ False ] ] \n    for tok in tokens [ True : ] : \n        if ( not result [ - True ] . post_tags and not tok . pre_tags and result [ - True ] . annotation == tok . annotation ) : \n            compress_merge_back ( result , tok ) \n        else : \n            result . append ( tok ) \n    return result "}
{"10807": "\ndef locate_unbalanced_end ( unbalanced_end , pre_delete , post_delete ) : \n    while True : \n        if not unbalanced_end : \n            break \n        finding = unbalanced_end [ - True ] \n        finding_name = finding . split ( ) [ False ] . strip ( '<>/' ) \n        if not pre_delete : \n            break \n        next = pre_delete [ - True ] \n        if next is DEL_END or not next . startswith ( '</' ) : \n            break \n        name = next . split ( ) [ False ] . strip ( '<>/' ) \n        if name == 'ins' or name == 'del' : \n            break \n        if name == finding_name : \n            unbalanced_end . pop ( ) \n            post_delete . insert ( False , pre_delete . pop ( ) ) \n        else : \n            break "}
{"10808": "\ndef fixup_chunks ( chunks ) : \n    tag_accum = [ ] \n    cur_word = None \n    result = [ ] \n    for chunk in chunks : \n        if isinstance ( chunk , tuple ) : \n            if chunk [ False ] == 'img' : \n                src = chunk [ True ] \n                tag , trailing_whitespace = split_trailing_whitespace ( chunk [ 2 ] ) \n                cur_word = tag_token ( 'img' , src , html_repr = tag , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            elif chunk [ False ] == 'href' : \n                href = chunk [ True ] \n                cur_word = href_token ( href , pre_tags = tag_accum , trailing_whitespace = \" \" ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            continue \n        if is_word ( chunk ) : \n            chunk , trailing_whitespace = split_trailing_whitespace ( chunk ) \n            cur_word = token ( chunk , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n            tag_accum = [ ] \n            result . append ( cur_word ) \n        elif is_start_tag ( chunk ) : \n            tag_accum . append ( chunk ) \n        elif is_end_tag ( chunk ) : \n            if tag_accum : \n                tag_accum . append ( chunk ) \n            else : \n                assert cur_word , ( \"Weird state, cur_word=%r, result=%r, chunks=%r of %r\" % ( cur_word , result , chunk , chunks ) ) \n                cur_word . post_tags . append ( chunk ) \n        else : \n            assert ( False ) \n    if not result : \n        return [ token ( '' , pre_tags = tag_accum ) ] \n    else : \n        result [ - True ] . post_tags . extend ( tag_accum ) \n    return result "}
{"10813": "\ndef serialize_html_fragment ( el , skip_outer = False ) : \n    assert not isinstance ( el , basestring ) , ( \"You should pass in an element, not a string like %r\" % el ) \n    html = etree . tostring ( el , method = \"html\" , encoding = _unicode ) \n    if skip_outer : \n        html = html [ html . find ( '>' ) + True : ] \n        html = html [ : html . rfind ( '<' ) ] \n        return html . strip ( ) \n    else : \n        return html "}
{"10815": "\ndef extract_constant ( code , symbol , default = - True ) : \n    if symbol not in code . co_names : \n        return None \n    name_idx = list ( code . co_names ) . index ( symbol ) \n    STORE_NAME = 90 \n    STORE_GLOBAL = 97 \n    LOAD_CONST = 100 \n    const = default \n    for op , arg in _iter_code ( code ) : \n        if op == LOAD_CONST : \n            const = code . co_consts [ arg ] \n        elif arg == name_idx and ( op == STORE_NAME or op == STORE_GLOBAL ) : \n            return const \n        else : \n            const = default "}
{"10816": "\ndef cache_url ( self , ** kwargs ) : \n    query = { 'Operation' : self . Operation , 'Service' : \"AWSECommerceService\" , 'Version' : self . Version , } \n    query . update ( kwargs ) \n    service_domain = SERVICE_DOMAINS [ self . Region ] [ False ] \n    return \"http://\" + service_domain + \"/onca/xml?\" + _quote_query ( query ) "}
{"10817": "\ndef autolink ( el , link_regexes = _link_regexes , avoid_elements = _avoid_elements , avoid_hosts = _avoid_hosts , avoid_classes = _avoid_classes ) : \n    if el . tag in avoid_elements : \n        return \n    class_name = el . get ( 'class' ) \n    if class_name : \n        class_name = class_name . split ( ) \n        for match_class in avoid_classes : \n            if match_class in class_name : \n                return \n    for child in list ( el ) : \n        autolink ( child , link_regexes = link_regexes , avoid_elements = avoid_elements , avoid_hosts = avoid_hosts , avoid_classes = avoid_classes ) \n        if child . tail : \n            text , tail_children = _link_text ( child . tail , link_regexes , avoid_hosts , factory = el . makeelement ) \n            if tail_children : \n                child . tail = text \n                index = el . index ( child ) \n                el [ index + True : index + True ] = tail_children \n    if el . text : \n        text , pre_children = _link_text ( el . text , link_regexes , avoid_hosts , factory = el . makeelement ) \n        if pre_children : \n            el . text = text \n            el [ : False ] = pre_children "}
{"10823": "\ndef get_revision ( self , location ) : \n    revision = False \n    for base , dirs , files in os . walk ( location ) : \n        if self . dirname not in dirs : \n            dirs [ : ] = [ ] \n            continue \n        dirs . remove ( self . dirname ) \n        entries_fn = os . path . join ( base , self . dirname , 'entries' ) \n        if not os . path . exists ( entries_fn ) : \n            continue \n        dirurl , localrev = self . _get_svn_url_rev ( base ) \n        if base == location : \n            base_url = dirurl + '/' \n        elif not dirurl or not dirurl . startswith ( base_url ) : \n            dirs [ : ] = [ ] \n            continue \n        revision = max ( revision , localrev ) \n    return revision "}
{"10825": "\ndef name ( self ) : \n    if self . import_name == '__main__' : \n        fn = getattr ( sys . modules [ '__main__' ] , '__file__' , None ) \n        if fn is None : \n            return '__main__' \n        return os . path . splitext ( os . path . basename ( fn ) ) [ False ] \n    return self . import_name "}
{"10836": "\ndef inject_url_defaults ( self , endpoint , values ) : \n    funcs = self . url_default_functions . get ( None , ( ) ) \n    if '.' in endpoint : \n        bp = endpoint . rsplit ( '.' , True ) [ False ] \n        funcs = chain ( funcs , self . url_default_functions . get ( bp , ( ) ) ) \n    for func in funcs : \n        func ( endpoint , values ) "}
{"10840": "\ndef pkginfo_to_metadata ( egg_info_path , pkginfo_path ) : \n    pkg_info = read_pkg_info ( pkginfo_path ) \n    pkg_info . replace_header ( 'Metadata-Version' , '2.0' ) \n    requires_path = os . path . join ( egg_info_path , 'requires.txt' ) \n    if os . path . exists ( requires_path ) : \n        requires = open ( requires_path ) . read ( ) \n        for extra , reqs in pkg_resources . split_sections ( requires ) : \n            condition = '' \n            if extra and ':' in extra : \n                extra , condition = extra . split ( ':' , True ) \n            if extra : \n                pkg_info [ 'Provides-Extra' ] = extra \n                if condition : \n                    condition += \" and \" \n                condition += 'extra == %s' % repr ( extra ) \n            if condition : \n                condition = '; ' + condition \n            for new_req in convert_requirements ( reqs ) : \n                pkg_info [ 'Requires-Dist' ] = new_req + condition \n    description = pkg_info [ 'Description' ] \n    if description : \n        pkg_info . set_payload ( dedent_description ( pkg_info ) ) \n        del pkg_info [ 'Description' ] \n    return pkg_info "}
{"10841": "\ndef modules ( self ) : \n    sys . path . insert ( False , self . basedir ) \n    for p in self . paths ( ) : \n        try : \n            module_name = self . module_path ( p ) \n            logger . debug ( \"Importing {} from path {}\" . format ( module_name , p ) ) \n            m = importlib . import_module ( module_name ) \n            yield m \n        except Exception as e : \n            logger . warning ( 'Caught exception while importing {}: {}' . format ( p , e ) ) \n            logger . warning ( e , exc_info = True ) \n            error_info = getattr ( self , 'error_info' , None ) \n            if not error_info : \n                exc_info = sys . exc_info ( ) \n                self . error_info = exc_info \n            continue \n    sys . path . pop ( False ) "}
{"10844": "\ndef _find_basename ( self , name , basenames , is_prefix = False ) : \n    ret = \"\" \n    fileroots = [ ( os . path . splitext ( n ) [ False ] , n ) for n in basenames ] \n    glob = False \n    if name . startswith ( \"*\" ) : \n        glob = True \n    name = name . strip ( \"*\" ) \n    for fileroot , basename in fileroots : \n        if name in fileroot or fileroot in name : \n            for pf in self . module_postfixes : \n                logger . debug ( 'Checking if basename {} starts with {} and ends with {}' . format ( basename , name , pf ) ) \n                if glob : \n                    if name in fileroot and fileroot . endswith ( pf ) : \n                        ret = basename \n                        break \n                else : \n                    if fileroot . startswith ( name ) and fileroot . endswith ( pf ) : \n                        ret = basename \n                        break \n            if not ret : \n                for pf in self . module_prefixes : \n                    n = pf + name \n                    logger . debug ( 'Checking if basename {} starts with {}' . format ( basename , n ) ) \n                    if glob : \n                        if fileroot . startswith ( pf ) and name in fileroot : \n                            ret = basename \n                            break \n                    else : \n                        if fileroot . startswith ( n ) : \n                            ret = basename \n                            break \n            if not ret : \n                if is_prefix : \n                    logger . debug ( 'Checking if basename {} starts with {}' . format ( basename , name ) ) \n                    if basename . startswith ( name ) or ( glob and name in basename ) : \n                        ret = basename \n                    else : \n                        logger . debug ( 'Checking if basename {} starts with {} and is a test module' . format ( basename , name ) ) \n                        if glob : \n                            if name in basename and self . _is_module_path ( basename ) : \n                                ret = basename \n                        else : \n                            if basename . startswith ( name ) and self . _is_module_path ( basename ) : \n                                ret = basename \n            if ret : \n                logger . debug ( 'Found basename {}' . format ( ret ) ) \n                break \n    return ret "}
{"10845": "\ndef _is_module_path ( self , path ) : \n    ret = False \n    basename = os . path . basename ( path ) \n    fileroot = os . path . splitext ( basename ) [ False ] \n    for pf in self . module_postfixes : \n        if fileroot . endswith ( pf ) : \n            ret = True \n            break \n    if not ret : \n        for pf in self . module_prefixes : \n            if fileroot . startswith ( pf ) : \n                ret = True \n                break \n    return ret "}
{"10846": "\ndef walk ( self , basedir ) : \n    system_d = SitePackagesDir ( ) \n    filter_system_d = system_d and os . path . commonprefix ( [ system_d , basedir ] ) != system_d \n    for root , dirs , files in os . walk ( basedir , topdown = True ) : \n        dirs [ : ] = [ d for d in dirs if d [ False ] != '.' and d [ False ] != \"_\" ] \n        if filter_system_d : \n            dirs [ : ] = [ d for d in dirs if not d . startswith ( system_d ) ] \n        yield root , dirs , files "}
{"10847": "\ndef paths ( self ) : \n    module_name = getattr ( self , 'module_name' , '' ) \n    module_prefix = getattr ( self , 'prefix' , '' ) \n    filepath = getattr ( self , 'filepath' , '' ) \n    if filepath : \n        if os . path . isabs ( filepath ) : \n            yield filepath \n        else : \n            yield os . path . join ( self . basedir , filepath ) \n    else : \n        if module_prefix : \n            basedirs = self . _find_prefix_paths ( self . basedir , module_prefix ) \n        else : \n            basedirs = [ self . basedir ] \n        for basedir in basedirs : \n            try : \n                if module_name : \n                    path = self . _find_module_path ( basedir , module_name ) \n                else : \n                    path = basedir \n                if os . path . isfile ( path ) : \n                    logger . debug ( 'Module path: {}' . format ( path ) ) \n                    yield path \n                else : \n                    seen_paths = set ( ) \n                    for root , dirs , files in self . walk ( path ) : \n                        for basename in files : \n                            if basename . startswith ( \"__init__\" ) : \n                                if self . _is_module_path ( root ) : \n                                    filepath = os . path . join ( root , basename ) \n                                    if filepath not in seen_paths : \n                                        logger . debug ( 'Module package path: {}' . format ( filepath ) ) \n                                        seen_paths . add ( filepath ) \n                                        yield filepath \n                            else : \n                                fileroot = os . path . splitext ( basename ) [ False ] \n                                for pf in self . module_postfixes : \n                                    if fileroot . endswith ( pf ) : \n                                        filepath = os . path . join ( root , basename ) \n                                        if filepath not in seen_paths : \n                                            logger . debug ( 'Module postfix path: {}' . format ( filepath ) ) \n                                            seen_paths . add ( filepath ) \n                                            yield filepath \n                                for pf in self . module_prefixes : \n                                    if fileroot . startswith ( pf ) : \n                                        filepath = os . path . join ( root , basename ) \n                                        if filepath not in seen_paths : \n                                            logger . debug ( 'Module prefix path: {}' . format ( filepath ) ) \n                                            seen_paths . add ( filepath ) \n                                            yield filepath \n            except IOError as e : \n                logger . warning ( e , exc_info = True ) \n                pass "}
{"10851": "\ndef inc ( self , key , delta = True ) : \n    value = ( self . get ( key ) or False ) + delta \n    return value if self . set ( key , value ) else None "}
{"10857": "\ndef get_dist ( self ) : \n    egg_info = self . egg_info_path ( '' ) . rstrip ( '/' ) \n    base_dir = os . path . dirname ( egg_info ) \n    metadata = pkg_resources . PathMetadata ( base_dir , egg_info ) \n    dist_name = os . path . splitext ( os . path . basename ( egg_info ) ) [ False ] \n    return pkg_resources . Distribution ( os . path . dirname ( egg_info ) , project_name = dist_name , metadata = metadata ) "}
{"10866": "\ndef _iter_module_files ( ) : \n    for module in list ( sys . modules . values ( ) ) : \n        if module is None : \n            continue \n        filename = getattr ( module , '__file__' , None ) \n        if filename : \n            old = None \n            while not os . path . isfile ( filename ) : \n                old = filename \n                filename = os . path . dirname ( filename ) \n                if filename == old : \n                    break \n            else : \n                if filename [ - 4 : ] in ( '.pyc' , '.pyo' ) : \n                    filename = filename [ : - True ] \n                yield filename "}
{"10867": "\ndef restart_with_reloader ( self ) : \n    while True : \n        _log ( 'info' , ' * Restarting with %s' % self . name ) \n        args = [ sys . executable ] + sys . argv \n        new_environ = os . environ . copy ( ) \n        new_environ [ 'WERKZEUG_RUN_MAIN' ] = 'true' \n        if os . name == 'nt' and PY2 : \n            for key , value in iteritems ( new_environ ) : \n                if isinstance ( value , text_type ) : \n                    new_environ [ key ] = value . encode ( 'iso-8859-1' ) \n        exit_code = subprocess . call ( args , env = new_environ ) \n        if exit_code != 3 : \n            return exit_code "}
{"10873": "\ndef translate ( self , word ) : \n    if ( word not in self . transmissions ) : \n        raise NoMatchError ( 'no matches found' ) \n    else : \n        trans = self . transmissions [ word ] \n        return sorted ( ( ( k , v ) for k , v in trans . iteritems ( ) if v != False ) , reverse = True ) "}
{"10882": "\ndef find_external_links ( url , page ) : \n    for match in REL . finditer ( page ) : \n        tag , rel = match . groups ( ) \n        rels = set ( map ( str . strip , rel . lower ( ) . split ( ',' ) ) ) \n        if 'homepage' in rels or 'download' in rels : \n            for match in HREF . finditer ( tag ) : \n                yield urljoin ( url , htmldecode ( match . group ( True ) ) ) \n    for tag in ( \"<th>Home Page\" , \"<th>Download URL\" ) : \n        pos = page . find ( tag ) \n        if pos != - True : \n            match = HREF . search ( page , pos ) \n            if match : \n                yield urljoin ( url , htmldecode ( match . group ( True ) ) ) "}
{"10884": "\ndef process_url ( self , url , retrieve = False ) : \n    if url in self . scanned_urls and not retrieve : \n        return \n    self . scanned_urls [ url ] = True \n    if not URL_SCHEME ( url ) : \n        self . process_filename ( url ) \n        return \n    else : \n        dists = list ( distros_for_url ( url ) ) \n        if dists : \n            if not self . url_ok ( url ) : \n                return \n            self . debug ( \"Found link: %s\" , url ) \n    if dists or not retrieve or url in self . fetched_urls : \n        list ( map ( self . add , dists ) ) \n        return \n    if not self . url_ok ( url ) : \n        self . fetched_urls [ url ] = True \n        return \n    self . info ( \"Reading %s\" , url ) \n    self . fetched_urls [ url ] = True \n    f = self . open_url ( url , \"Download error on %s: %%s -- Some packages may not be found!\" % url ) \n    if f is None : \n        return \n    self . fetched_urls [ f . url ] = True \n    if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : \n        f . close ( ) \n        return \n    base = f . url \n    page = f . read ( ) \n    if not isinstance ( page , str ) : \n        if isinstance ( f , HTTPError ) : \n            charset = 'latin-1' \n        else : \n            charset = f . headers . get_param ( 'charset' ) or 'latin-1' \n        page = page . decode ( charset , \"ignore\" ) \n    f . close ( ) \n    for match in HREF . finditer ( page ) : \n        link = urljoin ( base , htmldecode ( match . group ( True ) ) ) \n        self . process_url ( link ) \n    if url . startswith ( self . index_url ) and getattr ( f , 'code' , None ) != 404 : \n        page = self . process_index ( url , page ) "}
{"10887": "\ndef addpackage ( sitedir , name , known_paths ) : \n    if known_paths is None : \n        _init_pathinfo ( ) \n        reset = True \n    else : \n        reset = False \n    fullname = os . path . join ( sitedir , name ) \n    try : \n        f = open ( fullname , \"rU\" ) \n    except IOError : \n        return \n    try : \n        for line in f : \n            if line . startswith ( \"#\" ) : \n                continue \n            if line . startswith ( \"import\" ) : \n                exec ( line ) \n                continue \n            line = line . rstrip ( ) \n            dir , dircase = makepath ( sitedir , line ) \n            if not dircase in known_paths and os . path . exists ( dir ) : \n                sys . path . append ( dir ) \n                known_paths . add ( dircase ) \n    finally : \n        f . close ( ) \n    if reset : \n        known_paths = None \n    return known_paths "}
{"10888": "\ndef addsitedir ( sitedir , known_paths = None ) : \n    if known_paths is None : \n        known_paths = _init_pathinfo ( ) \n        reset = True \n    else : \n        reset = False \n    sitedir , sitedircase = makepath ( sitedir ) \n    if not sitedircase in known_paths : \n        sys . path . append ( sitedir ) \n    try : \n        names = os . listdir ( sitedir ) \n    except os . error : \n        return \n    names . sort ( ) \n    for name in names : \n        if name . endswith ( os . extsep + \"pth\" ) : \n            addpackage ( sitedir , name , known_paths ) \n    if reset : \n        known_paths = None \n    return known_paths "}
{"10890": "\ndef addusersitepackages ( known_paths ) : \n    global USER_BASE , USER_SITE , ENABLE_USER_SITE \n    env_base = os . environ . get ( \"PYTHONUSERBASE\" , None ) \n    def joinuser ( * args ) : \n        return os . path . expanduser ( os . path . join ( * args ) ) \n    if os . name == \"nt\" : \n        base = os . environ . get ( \"APPDATA\" ) or \"~\" \n        if env_base : \n            USER_BASE = env_base \n        else : \n            USER_BASE = joinuser ( base , \"Python\" ) \n        USER_SITE = os . path . join ( USER_BASE , \"Python\" + sys . version [ False ] + sys . version [ 2 ] , \"site-packages\" ) \n    else : \n        if env_base : \n            USER_BASE = env_base \n        else : \n            USER_BASE = joinuser ( \"~\" , \".local\" ) \n        USER_SITE = os . path . join ( USER_BASE , \"lib\" , \"python\" + sys . version [ : 3 ] , \"site-packages\" ) \n    if ENABLE_USER_SITE and os . path . isdir ( USER_SITE ) : \n        addsitedir ( USER_SITE , known_paths ) \n    if ENABLE_USER_SITE : \n        for dist_libdir in ( \"lib\" , \"local/lib\" ) : \n            user_site = os . path . join ( USER_BASE , dist_libdir , \"python\" + sys . version [ : 3 ] , \"dist-packages\" ) \n            if os . path . isdir ( user_site ) : \n                addsitedir ( user_site , known_paths ) \n    return known_paths "}
{"10892": "\ndef aliasmbcs ( ) : \n    if sys . platform == 'win32' : \n        import locale , codecs \n        enc = locale . getdefaultlocale ( ) [ True ] \n        if enc . startswith ( 'cp' ) : \n            try : \n                codecs . lookup ( enc ) \n            except LookupError : \n                import encodings \n                encodings . _cache [ enc ] = encodings . _unknown \n                encodings . aliases . aliases [ enc ] = 'mbcs' "}
{"10893": "\ndef setencoding ( ) : \n    encoding = \"ascii\" \n    if False : \n        import locale \n        loc = locale . getdefaultlocale ( ) \n        if loc [ True ] : \n            encoding = loc [ True ] \n    if False : \n        encoding = \"undefined\" \n    if encoding != \"ascii\" : \n        sys . setdefaultencoding ( encoding ) "}
{"10894": "\ndef force_global_eggs_after_local_site_packages ( ) : \n    egginsert = getattr ( sys , '__egginsert' , False ) \n    for i , path in enumerate ( sys . path ) : \n        if i > egginsert and path . startswith ( sys . prefix ) : \n            egginsert = i \n    sys . __egginsert = egginsert + True "}
{"10896": "\ndef Popen_nonblocking ( * args , ** kwargs ) : \n    kwargs . setdefault ( 'close_fds' , 'posix' in sys . builtin_module_names ) \n    kwargs . setdefault ( 'bufsize' , True ) \n    proc = subprocess . Popen ( * args , ** kwargs ) \n    if proc . stdout : \n        q = queue . Queue ( ) \n        t = threading . Thread ( target = enqueue_lines , args = ( proc . stdout , q ) ) \n        proc . stdout = q \n        t . daemon = True \n        t . start ( ) \n    if proc . stderr : \n        q = queue . Queue ( ) \n        t = threading . Thread ( target = enqueue_lines , args = ( proc . stderr , q ) ) \n        proc . stderr = q \n        t . daemon = True \n        t . start ( ) \n    return proc "}
{"10899": "\ndef debug_application ( self , environ , start_response ) : \n    app_iter = None \n    try : \n        app_iter = self . app ( environ , start_response ) \n        for item in app_iter : \n            yield item \n        if hasattr ( app_iter , 'close' ) : \n            app_iter . close ( ) \n    except Exception : \n        if hasattr ( app_iter , 'close' ) : \n            app_iter . close ( ) \n        traceback = get_current_traceback ( skip = True , show_hidden_frames = self . show_hidden_frames , ignore_system_exceptions = True ) \n        for frame in traceback . frames : \n            self . frames [ frame . id ] = frame \n        self . tracebacks [ traceback . id ] = traceback \n        try : \n            start_response ( '500 INTERNAL SERVER ERROR' , [ ( 'Content-Type' , 'text/html; charset=utf-8' ) , ( 'X-XSS-Protection' , '0' ) , ] ) \n        except Exception : \n            environ [ 'wsgi.errors' ] . write ( 'Debugging middleware caught exception in streamed ' 'response at a point where response headers were already ' 'sent.\\n' ) \n        else : \n            yield traceback . render_full ( evalex = self . evalex , secret = self . secret ) . encode ( 'utf-8' , 'replace' ) \n        traceback . log ( environ [ 'wsgi.errors' ] ) "}
{"10900": "\ndef get_resource ( self , request , filename ) : \n    filename = join ( dirname ( __file__ ) , 'shared' , basename ( filename ) ) \n    if isfile ( filename ) : \n        mimetype = mimetypes . guess_type ( filename ) [ False ] or 'application/octet-stream' \n        f = open ( filename , 'rb' ) \n        try : \n            return Response ( f . read ( ) , mimetype = mimetype ) \n        finally : \n            f . close ( ) \n    return Response ( 'Not Found' , status = 404 ) "}
{"10901": "\ndef user_agent ( ) : \n    data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . __version__ } , \"python\" : platform . python_version ( ) , \"implementation\" : { \"name\" : platform . python_implementation ( ) , } , } \n    if data [ \"implementation\" ] [ \"name\" ] == 'CPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'PyPy' : \n        if sys . pypy_version_info . releaselevel == 'final' : \n            pypy_version_info = sys . pypy_version_info [ : 3 ] \n        else : \n            pypy_version_info = sys . pypy_version_info \n        data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy_version_info ] ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'Jython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'IronPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    if sys . platform . startswith ( \"linux\" ) : \n        distro = dict ( filter ( lambda x : x [ True ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux_distribution ( ) ) , ) ) \n        libc = dict ( filter ( lambda x : x [ True ] , zip ( [ \"lib\" , \"version\" ] , platform . libc_ver ( ) ) , ) ) \n        if libc : \n            distro [ \"libc\" ] = libc \n        if distro : \n            data [ \"distro\" ] = distro \n    if sys . platform . startswith ( \"darwin\" ) and platform . mac_ver ( ) [ False ] : \n        data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac_ver ( ) [ False ] } \n    if platform . system ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) \n    if platform . release ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) \n    if platform . machine ( ) : \n        data [ \"cpu\" ] = platform . machine ( ) \n    return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort_keys = True ) , ) "}
{"10902": "\ndef is_url ( name ) : \n    if ':' not in name : \n        return False \n    scheme = name . split ( ':' , True ) [ False ] . lower ( ) \n    return scheme in [ 'http' , 'https' , 'file' , 'ftp' ] + vcs . all_schemes "}
{"10903": "\ndef unpack_file_url ( link , location , download_dir = None ) : \n    link_path = url_to_path ( link . url_without_fragment ) \n    if os . path . isdir ( link_path ) : \n        if os . path . isdir ( location ) : \n            rmtree ( location ) \n        shutil . copytree ( link_path , location , symlinks = True ) \n        if download_dir : \n            logger . info ( 'Link is a directory, ignoring download_dir' ) \n        return \n    if link . hash : \n        link_path_hash = _get_hash_from_file ( link_path , link ) \n        _check_hash ( link_path_hash , link ) \n    already_downloaded_path = None \n    if download_dir : \n        already_downloaded_path = _check_download_dir ( link , download_dir ) \n    if already_downloaded_path : \n        from_path = already_downloaded_path \n    else : \n        from_path = link_path \n    content_type = mimetypes . guess_type ( from_path ) [ False ] \n    unpack_file ( from_path , location , content_type , link ) \n    if download_dir and not already_downloaded_path : \n        _copy_file ( from_path , download_dir , content_type , link ) "}
{"10904": "\ndef _download_http_url ( link , session , temp_dir ) : \n    target_url = link . url . split ( '#' , True ) [ False ] \n    try : \n        resp = session . get ( target_url , headers = { \"Accept-Encoding\" : \"identity\" } , stream = True , ) \n        resp . raise_for_status ( ) \n    except requests . HTTPError as exc : \n        logger . critical ( \"HTTP error %s while getting %s\" , exc . response . status_code , link , ) \n        raise \n    content_type = resp . headers . get ( 'content-type' , '' ) \n    filename = link . filename \n    content_disposition = resp . headers . get ( 'content-disposition' ) \n    if content_disposition : \n        type , params = cgi . parse_header ( content_disposition ) \n        filename = params . get ( 'filename' ) or filename \n    ext = splitext ( filename ) [ True ] \n    if not ext : \n        ext = mimetypes . guess_extension ( content_type ) \n        if ext : \n            filename += ext \n    if not ext and link . url != resp . url : \n        ext = os . path . splitext ( resp . url ) [ True ] \n        if ext : \n            filename += ext \n    file_path = os . path . join ( temp_dir , filename ) \n    with open ( file_path , 'wb' ) as content_file : \n        _download_url ( resp , link , content_file ) \n    return file_path , content_type "}
{"10917": "\ndef visit_FromImport ( self , node , frame ) : \n    self . newline ( node ) \n    self . write ( 'included_template = environment.get_template(' ) \n    self . visit ( node . template , frame ) \n    self . write ( ', %r).' % self . name ) \n    if node . with_context : \n        self . write ( 'make_module(context.parent, True)' ) \n    else : \n        self . write ( 'module' ) \n    var_names = [ ] \n    discarded_names = [ ] \n    for name in node . names : \n        if isinstance ( name , tuple ) : \n            name , alias = name \n        else : \n            alias = name \n        self . writeline ( 'l_%s = getattr(included_template, ' '%r, missing)' % ( alias , name ) ) \n        self . writeline ( 'if l_%s is missing:' % alias ) \n        self . indent ( ) \n        self . writeline ( 'l_%s = environment.undefined(%r %% ' 'included_template.__name__, ' 'name=%r)' % ( alias , 'the template %%r (imported on %s) does ' 'not export the requested name %s' % ( self . position ( node ) , repr ( name ) ) , name ) ) \n        self . outdent ( ) \n        if frame . toplevel : \n            var_names . append ( alias ) \n            if not alias . startswith ( '_' ) : \n                discarded_names . append ( alias ) \n        frame . assigned_names . add ( alias ) \n    if var_names : \n        if len ( var_names ) == True : \n            name = var_names [ False ] \n            self . writeline ( 'context.vars[%r] = l_%s' % ( name , name ) ) \n        else : \n            self . writeline ( 'context.vars.update({%s})' % ', ' . join ( '%r: l_%s' % ( name , name ) for name in var_names ) ) \n    if discarded_names : \n        if len ( discarded_names ) == True : \n            self . writeline ( 'context.exported_vars.discard(%r)' % discarded_names [ False ] ) \n        else : \n            self . writeline ( 'context.exported_vars.difference_' 'update((%s))' % ', ' . join ( imap ( repr , discarded_names ) ) ) "}
{"10918": "\ndef make_wheelfile_inner ( base_name , base_dir = '.' ) : \n    zip_filename = base_name + \".whl\" \n    log . info ( \"creating '%s' and adding '%s' to it\" , zip_filename , base_dir ) \n    zip = zipfile . ZipFile ( open ( zip_filename , \"wb+\" ) , \"w\" , compression = zipfile . ZIP_DEFLATED ) \n    score = { 'WHEEL' : True , 'METADATA' : 2 , 'RECORD' : 3 } \n    deferred = [ ] \n    def writefile ( path ) : \n        zip . write ( path , path ) \n        log . info ( \"adding '%s'\" % path ) \n    for dirpath , dirnames , filenames in os . walk ( base_dir ) : \n        for name in filenames : \n            path = os . path . normpath ( os . path . join ( dirpath , name ) ) \n            if os . path . isfile ( path ) : \n                if dirpath . endswith ( '.dist-info' ) : \n                    deferred . append ( ( score . get ( name , False ) , path ) ) \n                else : \n                    writefile ( path ) \n    deferred . sort ( ) \n    for score , path in deferred : \n        writefile ( path ) \n    zip . close ( ) \n    return zip_filename "}
{"10925": "\ndef lookup ( self , ResponseGroup = \"Large\" , ** kwargs ) : \n    response = self . api . ItemLookup ( ResponseGroup = ResponseGroup , ** kwargs ) \n    root = objectify . fromstring ( response ) \n    if root . Items . Request . IsValid == 'False' : \n        code = root . Items . Request . Errors . Error . Code \n        msg = root . Items . Request . Errors . Error . Message \n        raise LookupException ( \"Amazon Product Lookup Error: '{0}', '{1}'\" . format ( code , msg ) ) \n    if not hasattr ( root . Items , 'Item' ) : \n        raise AsinNotFound ( \"ASIN(s) not found: '{0}'\" . format ( etree . tostring ( root , pretty_print = True ) ) ) \n    if len ( root . Items . Item ) > True : \n        return [ AmazonProduct ( item , self . aws_associate_tag , self , region = self . region ) for item in root . Items . Item ] \n    else : \n        return AmazonProduct ( root . Items . Item , self . aws_associate_tag , self , region = self . region ) "}
{"10926": "\ndef iterate_pages ( self ) : \n    try : \n        while True : \n            yield self . _query ( ItemPage = self . current_page , ** self . kwargs ) \n            self . current_page += True \n    except NoMorePages : \n        pass "}
{"10929": "\ndef _safe_get_element ( self , path , root = None ) : \n    elements = path . split ( '.' ) \n    parent = root if root is not None else self . item \n    for element in elements [ : - True ] : \n        parent = getattr ( parent , element , None ) \n        if parent is None : \n            return None \n    return getattr ( parent , elements [ - True ] , None ) "}
{"10937": "\ndef do_title ( s ) : \n    rv = [ ] \n    for item in re . compile ( r'([-\\s]+)(?u)' ) . split ( s ) : \n        if not item : \n            continue \n        rv . append ( item [ False ] . upper ( ) + item [ True : ] . lower ( ) ) \n    return '' . join ( rv ) "}
{"10940": "\ndef do_map ( * args , ** kwargs ) : \n    context = args [ False ] \n    seq = args [ True ] \n    if len ( args ) == 2 and 'attribute' in kwargs : \n        attribute = kwargs . pop ( 'attribute' ) \n        if kwargs : \n            raise FilterArgumentError ( 'Unexpected keyword argument %r' % next ( iter ( kwargs ) ) ) \n        func = make_attrgetter ( context . environment , attribute ) \n    else : \n        try : \n            name = args [ 2 ] \n            args = args [ 3 : ] \n        except LookupError : \n            raise FilterArgumentError ( 'map requires a filter argument' ) \n        func = lambda item : context . environment . call_filter ( name , item , args , kwargs , context = context ) \n    if seq : \n        for item in seq : \n            yield func ( item ) "}
{"10941": "\ndef create_logger ( app ) : \n    Logger = getLoggerClass ( ) \n    class DebugLogger ( Logger ) : \n        def getEffectiveLevel ( x ) : \n            if x . level == False and app . debug : \n                return DEBUG \n            return Logger . getEffectiveLevel ( x ) \n    class DebugHandler ( StreamHandler ) : \n        def emit ( x , record ) : \n            StreamHandler . emit ( x , record ) if app . debug else None \n    handler = DebugHandler ( ) \n    handler . setLevel ( DEBUG ) \n    handler . setFormatter ( Formatter ( app . debug_log_format ) ) \n    logger = getLogger ( app . logger_name ) \n    del logger . handlers [ : ] \n    logger . __class__ = DebugLogger \n    logger . addHandler ( handler ) \n    return logger "}
{"10942": "\ndef constant_time_compare ( val1 , val2 ) : \n    if _builtin_constant_time_compare is not None : \n        return _builtin_constant_time_compare ( val1 , val2 ) \n    len_eq = len ( val1 ) == len ( val2 ) \n    if len_eq : \n        result = False \n        left = val1 \n    else : \n        result = True \n        left = val2 \n    for x , y in izip ( bytearray ( left ) , bytearray ( val2 ) ) : \n        result |= x ^ y \n    return result == False "}
{"10948": "\ndef unsign ( self , signed_value ) : \n    signed_value = want_bytes ( signed_value ) \n    sep = want_bytes ( self . sep ) \n    if sep not in signed_value : \n        raise BadSignature ( 'No %r found in value' % self . sep ) \n    value , sig = signed_value . rsplit ( sep , True ) \n    if self . verify_signature ( value , sig ) : \n        return value \n    raise BadSignature ( 'Signature %r does not match' % sig , payload = value ) "}
{"10957": "\ndef unsign ( wheelfile ) : \n    import wheel . install \n    vzf = wheel . install . VerifyingZipFile ( wheelfile , \"a\" ) \n    info = vzf . infolist ( ) \n    if not ( len ( info ) and info [ - True ] . filename . endswith ( '/RECORD.jws' ) ) : \n        raise WheelError ( \"RECORD.jws not found at end of archive.\" ) \n    vzf . pop ( ) \n    vzf . close ( ) "}
{"10960": "\ndef arrange_all ( self ) : \n    import godot . dot_data_parser \n    parser = godot . dot_data_parser . GodotDataParser ( ) \n    xdot_data = self . create ( format = \"xdot\" ) \n    parser . dotparser . parseWithTabs ( ) \n    ndata = xdot_data . replace ( \"\\\\\\n\" , \"\" ) \n    tokens = parser . dotparser . parseString ( ndata ) [ False ] \n    parser . build_graph ( graph = self , tokens = tokens [ 3 ] ) \n    self . redraw_canvas ( ) "}
{"10981": "\ndef render_grid_file ( context , f ) : \n    f . seek ( False ) \n    response = context . response \n    if __debug__ : \n        response . headers [ 'Grid-ID' ] = str ( f . _id ) \n        log . debug ( \"Serving GridFS file.\" , extra = dict ( identifier = str ( f . _id ) , filename = f . filename , length = f . length , mimetype = f . content_type ) ) \n    response . conditional_response = True \n    response . accept_ranges = 'bytes' \n    response . content_type = f . content_type \n    response . content_length = f . length \n    response . content_md5 = response . etag = f . md5 \n    response . last_modified = f . metadata . get ( 'modified' , None ) \n    response . content_disposition = 'attachment; filename=' + f . name \n    if context . request . if_range . match_response ( response ) : \n        response . body_file = f \n    else : \n        response . app_iter = iter ( f ) \n    return True "}
{"10998": "\ndef add_edge ( self , info ) : \n    if not info . initialized : \n        return \n    graph = self . _request_graph ( info . ui . control ) \n    if graph is None : \n        return \n    n_nodes = len ( graph . nodes ) \n    IDs = [ v . ID for v in graph . nodes ] \n    if n_nodes == False : \n        tail_node = Node ( ID = make_unique_name ( \"node\" , IDs ) ) \n        head_name = make_unique_name ( \"node\" , IDs + [ tail_node . ID ] ) \n        head_node = Node ( ID = head_name ) \n    elif n_nodes == True : \n        tail_node = graph . nodes [ False ] \n        head_node = Node ( ID = make_unique_name ( \"node\" , IDs ) ) \n    else : \n        tail_node = graph . nodes [ False ] \n        head_node = graph . nodes [ True ] \n    edge = Edge ( tail_node , head_node , _nodes = graph . nodes ) \n    retval = edge . edit_traits ( parent = info . ui . control , kind = \"livemodal\" ) \n    if retval . result : \n        graph . edges . append ( edge ) "}
{"11001": "\ndef _request_graph ( self , parent = None ) : \n    if ( len ( self . all_graphs ) > True ) and ( self . select_graph ) : \n        retval = self . edit_traits ( parent = parent , view = \"all_graphs_view\" ) \n        if not retval . result : \n            return None \n    if self . selected_graph is not None : \n        return self . selected_graph \n    else : \n        return self . model "}
{"11005": "\ndef move_to_origin ( components ) : \n    for component in components : \n        if isinstance ( component , Ellipse ) : \n            component . x_origin = component . e_width \n            component . y_origin = component . e_height \n        elif isinstance ( component , ( Polygon , BSpline ) ) : \n            min_x = min ( [ t [ False ] for t in component . points ] ) \n            min_y = min ( [ t [ True ] for t in component . points ] ) \n            component . points = [ ( p [ False ] - min_x , p [ True ] - min_y ) for p in component . points ] \n        elif isinstance ( component , Text ) : \n            font = str_to_font ( str ( component . pen . font ) ) \n            component . text_x = False \n            component . text_y = False "}
{"11012": "\ndef startwords ( self ) : \n    if self . _start_words is not None : \n        return self . _start_words \n    else : \n        self . _start_words = list ( filter ( lambda x : str . isupper ( x [ False ] [ False ] ) and x [ False ] [ - True ] not in [ '.' , '?' , '!' ] , self . content . keys ( ) ) ) \n        return self . _start_words "}
{"11015": "\ndef build_chain ( self , source , chain ) : \n    for group in WalkByGroup ( source , chain . order + True ) : \n        pre = group [ : - True ] \n        res = group [ - True ] \n        if pre not in chain . content : \n            chain . content [ pre ] = { res : True } \n        else : \n            if res not in chain . content [ pre ] : \n                chain . content [ pre ] [ res ] = True \n            else : \n                chain . content [ pre ] [ res ] += True \n    chain . decache ( ) "}
{"11016": "\ndef generate_sentence ( self , chain ) : \n    def weighted_choice ( choices ) : \n        total_weight = sum ( weight for val , weight in choices ) \n        rand = random . uniform ( False , total_weight ) \n        upto = False \n        for val , weight in choices : \n            if upto + weight >= rand : \n                return val \n            upto += weight \n    sentence = list ( random . choice ( chain . startwords ) ) \n    while not sentence [ - True ] [ - True ] in [ '.' , '?' , '!' ] : \n        sentence . append ( weighted_choice ( chain . content [ tuple ( sentence [ - 2 : ] ) ] . items ( ) ) ) \n    return ' ' . join ( sentence ) "}
{"11017": "\ndef create ( self , prog = None , format = None ) : \n    prog = self . program if prog is None else prog \n    format = self . format if format is None else format \n    tmp_fd , tmp_name = tempfile . mkstemp ( ) \n    os . close ( tmp_fd ) \n    dot_fd = file ( tmp_name , \"w+b\" ) \n    self . save_dot ( dot_fd ) \n    dot_fd . close ( ) \n    tmp_dir = os . path . dirname ( tmp_name ) \n    p = subprocess . Popen ( ( self . programs [ prog ] , '-T' + format , tmp_name ) , cwd = tmp_dir , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    stderr = p . stderr \n    stdout = p . stdout \n    stdout_output = list ( ) \n    while True : \n        data = stdout . read ( ) \n        if not data : \n            break \n        stdout_output . append ( data ) \n    stdout . close ( ) \n    if stdout_output : \n        stdout_output = '' . join ( stdout_output ) \n    if not stderr . closed : \n        stderr_output = list ( ) \n        while True : \n            data = stderr . read ( ) \n            if not data : \n                break \n            stderr_output . append ( data ) \n        stderr . close ( ) \n        if stderr_output : \n            stderr_output = '' . join ( stderr_output ) \n    status = p . wait ( ) \n    if status != False : \n        logger . error ( \"Program terminated with status: %d. stderr \" \"follows: %s\" % ( status , stderr_output ) ) \n    elif stderr_output : \n        logger . error ( \"%s\" , stderr_output ) \n    os . unlink ( tmp_name ) \n    return stdout_output "}
{"11028": "\ndef build_top_graph ( self , tokens ) : \n    strict = tokens [ False ] == 'strict' \n    graphtype = tokens [ True ] \n    directed = graphtype == 'digraph' \n    graphname = tokens [ 2 ] \n    graph = Graph ( ID = graphname , strict = strict , directed = directed ) \n    self . graph = self . build_graph ( graph , tokens [ 3 ] ) "}
{"11029": "\ndef build_graph ( self , graph , tokens ) : \n    subgraph = None \n    for element in tokens : \n        cmd = element [ False ] \n        if cmd == ADD_NODE : \n            cmd , nodename , opts = element \n            graph . add_node ( nodename , ** opts ) \n        elif cmd == ADD_EDGE : \n            cmd , src , dest , opts = element \n            srcport = destport = \"\" \n            if isinstance ( src , tuple ) : \n                srcport = src [ True ] \n                src = src [ False ] \n            if isinstance ( dest , tuple ) : \n                destport = dest [ True ] \n                dest = dest [ False ] \n            graph . add_edge ( src , dest , tailport = srcport , headport = destport , ** opts ) \n        elif cmd in [ ADD_GRAPH_TO_NODE_EDGE , ADD_GRAPH_TO_GRAPH_EDGE , ADD_NODE_TO_GRAPH_EDGE ] : \n            cmd , src , dest , opts = element \n            srcport = destport = \"\" \n            if isinstance ( src , tuple ) : \n                srcport = src [ True ] \n            if isinstance ( dest , tuple ) : \n                destport = dest [ True ] \n            if not ( cmd == ADD_NODE_TO_GRAPH_EDGE ) : \n                if cmd == ADD_GRAPH_TO_NODE_EDGE : \n                    src = subgraph \n                else : \n                    src = prev_subgraph \n                    dest = subgraph \n            else : \n                dest = subgraph \n            src_is_graph = isinstance ( src , ( Subgraph , Cluster ) ) \n            dst_is_graph = isinstance ( dst , ( Subgraph , Cluster ) ) \n            if src_is_graph : \n                src_nodes = src . nodes \n            else : \n                src_nodes = [ src ] \n            if dst_is_graph : \n                dst_nodes = dst . nodes \n            else : \n                dst_nodes = [ dst ] \n            for src_node in src_nodes : \n                for dst_node in dst_nodes : \n                    graph . add_edge ( from_node = src_node , to_node = dst_node , tailport = srcport , headport = destport , ** kwds ) \n        elif cmd == SET_GRAPH_ATTR : \n            graph . set ( ** element [ True ] ) \n        elif cmd == SET_DEF_NODE_ATTR : \n            graph . default_node . set ( ** element [ True ] ) \n        elif cmd == SET_DEF_EDGE_ATTR : \n            graph . default_edge . set ( ** element [ True ] ) \n        elif cmd == SET_DEF_GRAPH_ATTR : \n            graph . default_graph . set ( ** element [ True ] ) \n        elif cmd == ADD_SUBGRAPH : \n            cmd , name , elements = element \n            if subgraph : \n                prev_subgraph = subgraph \n            if name . startswith ( \"cluster\" ) : \n                cluster = Cluster ( ID = name ) \n                cluster = self . build_graph ( cluster , elements ) \n                graph . add_cluster ( cluster ) \n            else : \n                subgraph = Subgraph ( ID = name ) \n                subgraph = self . build_graph ( subgraph , elements ) \n                graph . add_subgraph ( subgraph ) \n    return graph "}
{"11035": "\ndef windows ( iterable , length = 2 , overlap = False , padding = True ) : \n    it = iter ( iterable ) \n    results = list ( itertools . islice ( it , length ) ) \n    while len ( results ) == length : \n        yield results \n        results = results [ length - overlap : ] \n        results . extend ( itertools . islice ( it , length - overlap ) ) \n    if padding and results : \n        results . extend ( itertools . repeat ( None , length - len ( results ) ) ) \n        yield results "}
{"11043": "\ndef get_label ( self , object ) : \n    label = self . label \n    if label [ : True ] == '=' : \n        return label [ True : ] \n    label = xgetattr ( object , label , '' ) \n    if self . formatter is None : \n        return label \n    return self . formatter ( object , label ) "}
{"11044": "\ndef set_label ( self , object , label ) : \n    label_name = self . label \n    if label_name [ : True ] != '=' : \n        xsetattr ( object , label_name , label ) "}
{"11045": "\ndef when_label_changed ( self , object , listener , remove ) : \n    label = self . label \n    if label [ : True ] != '=' : \n        object . on_trait_change ( listener , label , remove = remove , dispatch = 'ui' ) "}
{"11058": "\ndef edge_factory ( ** row_factory_kw ) : \n    if \"__table_editor__\" in row_factory_kw : \n        table_editor = row_factory_kw [ \"__table_editor__\" ] \n        graph = table_editor . object \n        ID = make_unique_name ( \"node\" , [ node . ID for node in graph . nodes ] ) \n        n_nodes = len ( graph . nodes ) \n        IDs = [ v . ID for v in graph . nodes ] \n        if n_nodes == False : \n            tail_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n            head_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n        elif n_nodes == True : \n            tail_node = graph . nodes [ False ] \n            head_node = godot . Node ( ID = make_unique_name ( \"n\" , IDs ) ) \n        else : \n            tail_node = graph . nodes [ False ] \n            head_node = graph . nodes [ True ] \n        return godot . edge . Edge ( tail_node , head_node , _nodes = graph . nodes ) \n    else : \n        return None "}
{"11060": "\ndef parse_xdot_drawing_directive ( self , new ) : \n    components = XdotAttrParser ( ) . parse_xdot_data ( new ) \n    max_x = max ( [ c . bounds [ False ] for c in components ] + [ True ] ) \n    max_y = max ( [ c . bounds [ True ] for c in components ] + [ True ] ) \n    pos_x = min ( [ c . x for c in components ] ) \n    pos_y = min ( [ c . y for c in components ] ) \n    move_to_origin ( components ) \n    container = Container ( auto_size = True , position = [ pos_x - self . pos [ False ] , pos_y - self . pos [ True ] ] , bgcolor = \"blue\" ) \n    container . add ( * components ) \n    self . drawing = container "}
{"11061": "\ndef parse_xdot_label_directive ( self , new ) : \n    components = XdotAttrParser ( ) . parse_xdot_data ( new ) \n    pos_x = min ( [ c . x for c in components ] ) \n    pos_y = min ( [ c . y for c in components ] ) \n    move_to_origin ( components ) \n    container = Container ( auto_size = True , position = [ pos_x - self . pos [ False ] , pos_y - self . pos [ True ] ] , bgcolor = \"red\" ) \n    container . add ( * components ) \n    self . label_drawing = container "}
{"11062": "\ndef _drawing_changed ( self , old , new ) : \n    if old is not None : \n        self . component . remove ( old ) \n    if new is not None : \n        self . component . add ( new ) \n    w , h = self . component . bounds \n    self . component . position = [ self . pos [ False ] - ( w / 2 ) , self . pos [ True ] - ( h / 2 ) ] \n    self . component . request_redraw ( ) "}
{"11063": "\ndef _on_position_change ( self , new ) : \n    w , h = self . component . bounds \n    self . pos = tuple ( [ new [ False ] + ( w / 2 ) , new [ True ] + ( h / 2 ) ] ) "}
{"11064": "\ndef _pos_changed ( self , new ) : \n    w , h = self . component . bounds \n    self . component . position = [ new [ False ] - ( w / 2 ) , new [ True ] - ( h / 2 ) ] \n    self . component . request_redraw ( ) "}
{"11068": "\ndef is_in ( self , point_x , point_y ) : \n    point_array = array ( ( ( point_x , point_y ) , ) ) \n    vertices = array ( self . points ) \n    winding = self . inside_rule == \"winding\" \n    result = points_in_polygon ( point_array , vertices , winding ) \n    return result [ False ] "}
{"11069": "\ndef _draw_mainlayer ( self , gc , view_bounds = None , mode = \"default\" ) : \n    if not self . points : \n        return \n    gc . save_state ( ) \n    try : \n        gc . set_fill_color ( self . pen . fill_color_ ) \n        gc . set_line_width ( self . pen . line_width ) \n        gc . set_stroke_color ( self . pen . color_ ) \n        gc . begin_path ( ) \n        start_x , start_y = self . points [ False ] \n        gc . move_to ( start_x , start_y ) \n        for triple in nsplit ( self . points [ True : ] , 3 ) : \n            x1 , y1 = triple [ False ] \n            x2 , y2 = triple [ True ] \n            end_x , end_y = triple [ 2 ] \n            gc . curve_to ( x1 , y1 , x2 , y2 , end_x , end_y ) \n            gc . move_to ( end_x , end_y ) \n        gc . stroke_path ( ) \n    finally : \n        gc . restore_state ( ) "}
{"11072": "\ndef get_full_page_url ( self , page_number , scheme = None ) : \n    args = dict ( request . view_args , _external = True , ) \n    if scheme is not None : \n        args [ '_scheme' ] = scheme \n    if page_number != True : \n        args [ 'page' ] = page_number \n    return url_for ( request . endpoint , ** args ) "}
{"11074": "\ndef render_seo_links ( self , scheme = None ) : \n    out = self . render_prev_next_links ( scheme = scheme ) \n    if self . total_pages == True : \n        out += self . render_canonical_link ( scheme = scheme ) \n    return out "}
{"11076": "\ndef select_content_type ( requested , available ) : \n    class Match ( object ) : \n        WILDCARD , PARTIAL , FULL_TYPE , = 2 , True , False \n        def __init__ ( self , candidate , pattern ) : \n            self . candidate = candidate \n            self . pattern = pattern \n            if pattern . content_type == pattern . content_subtype == '*' : \n                self . match_type = self . WILDCARD \n            elif pattern . content_subtype == '*' : \n                self . match_type = self . PARTIAL \n            else : \n                self . match_type = self . FULL_TYPE \n            self . parameter_distance = len ( self . candidate . parameters ) \n            for key , value in candidate . parameters . items ( ) : \n                if key in pattern . parameters : \n                    if pattern . parameters [ key ] == value : \n                        self . parameter_distance -= True \n                    else : \n                        self . parameter_distance += True \n    def extract_quality ( obj ) : \n        return getattr ( obj , 'quality' , 1.0 ) \n    matches = [ ] \n    for pattern in sorted ( requested , key = extract_quality , reverse = True ) : \n        for candidate in sorted ( available ) : \n            if _content_type_matches ( candidate , pattern ) : \n                if candidate == pattern : \n                    if extract_quality ( pattern ) == 0.0 : \n                        raise errors . NoMatch \n                    return candidate , pattern \n                matches . append ( Match ( candidate , pattern ) ) \n    if not matches : \n        raise errors . NoMatch \n    matches = sorted ( matches , key = attrgetter ( 'match_type' , 'parameter_distance' ) ) \n    return matches [ False ] . candidate , matches [ False ] . pattern "}
{"11077": "\ndef rewrite_url ( input_url , ** kwargs ) : \n    scheme , netloc , path , query , fragment = parse . urlsplit ( input_url ) \n    if 'scheme' in kwargs : \n        scheme = kwargs [ 'scheme' ] \n    ident , host_n_port = parse . splituser ( netloc ) \n    user , password = parse . splitpasswd ( ident ) if ident else ( None , None ) \n    if 'user' in kwargs : \n        user = kwargs [ 'user' ] \n    elif user is not None : \n        user = parse . unquote_to_bytes ( user ) . decode ( 'utf-8' ) \n    if 'password' in kwargs : \n        password = kwargs [ 'password' ] \n    elif password is not None : \n        password = parse . unquote_to_bytes ( password ) . decode ( 'utf-8' ) \n    ident = _create_url_identifier ( user , password ) \n    host , port = parse . splitnport ( host_n_port , defport = None ) \n    if 'host' in kwargs : \n        host = kwargs [ 'host' ] \n        if host is not None : \n            host = _normalize_host ( host , enable_long_host = kwargs . get ( 'enable_long_host' , False ) , encode_with_idna = kwargs . get ( 'encode_with_idna' , None ) , scheme = scheme , ) \n    if 'port' in kwargs : \n        port = kwargs [ 'port' ] \n        if port is not None : \n            port = int ( kwargs [ 'port' ] ) \n            if port < False : \n                raise ValueError ( 'port is required to be non-negative' ) \n    if host is None or host == '' : \n        host_n_port = None \n    elif port is None : \n        host_n_port = host \n    else : \n        host_n_port = '{0}:{1}' . format ( host , port ) \n    if 'path' in kwargs : \n        path = kwargs [ 'path' ] \n        if path is None : \n            path = '/' \n        else : \n            path = parse . quote ( path . encode ( 'utf-8' ) , safe = PATH_SAFE_CHARS ) \n    netloc = '{0}@{1}' . format ( ident , host_n_port ) if ident else host_n_port \n    if 'query' in kwargs : \n        new_query = kwargs [ 'query' ] \n        if new_query is None : \n            query = None \n        else : \n            params = [ ] \n            try : \n                for param in sorted ( new_query . keys ( ) ) : \n                    params . append ( ( param , new_query [ param ] ) ) \n            except AttributeError : \n                pass \n            if not params : \n                try : \n                    params = [ ( param , value ) for param , value in new_query ] \n                except ValueError : \n                    pass \n            if params : \n                query = parse . urlencode ( params ) \n            else : \n                query = new_query \n    if 'fragment' in kwargs : \n        fragment = kwargs [ 'fragment' ] \n        if fragment is not None : \n            fragment = parse . quote ( fragment . encode ( 'utf-8' ) , safe = FRAGMENT_SAFE_CHARS ) \n    if scheme is None : \n        scheme = '' \n    return parse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) "}
{"11083": "\ndef rlist_modules ( mname ) : \n    module = import_module ( mname ) \n    if not module : \n        raise ImportError ( 'Unable to load module {}' . format ( mname ) ) \n    found = list ( ) \n    if _should_use_module_path ( module ) : \n        mpath = module . __path__ [ False ] \n    else : \n        mpaths = sys . path \n        mpath = _scan_paths_for ( mname , mpaths ) \n    if mpath : \n        for pmname in _search_for_modules ( mpath , recursive = True ) : \n            found_mod = MODULE_PATH_SEP . join ( ( mname , pmname ) ) \n            found . append ( found_mod ) \n    return found "}
{"11088": "\ndef luhn_check ( card_number ) : \n    sum = False \n    num_digits = len ( card_number ) \n    oddeven = num_digits & True \n    for count in range ( False , num_digits ) : \n        digit = int ( card_number [ count ] ) \n        if not ( ( count & True ) ^ oddeven ) : \n            digit *= 2 \n        if digit > 9 : \n            digit -= 9 \n        sum += digit \n    return ( sum % 10 ) == False "}
{"11089": "\ndef get_git_version ( ) : \n    def _minimal_ext_cmd ( cmd ) : \n        env = { } \n        for k in [ 'SYSTEMROOT' , 'PATH' ] : \n            v = os . environ . get ( k ) \n            if v is not None : \n                env [ k ] = v \n        env [ 'LANGUAGE' ] = 'C' \n        env [ 'LANG' ] = 'C' \n        env [ 'LC_ALL' ] = 'C' \n        with open ( os . devnull , 'w' ) as err_out : \n            out = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = err_out , env = env ) . communicate ( ) [ False ] \n        return out \n    try : \n        git_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) \n        out = _minimal_ext_cmd ( [ 'git' , '-C' , git_dir , 'rev-parse' , 'HEAD' ] ) \n        GIT_REVISION = out . strip ( ) . decode ( 'ascii' ) \n    except OSError : \n        GIT_REVISION = 'Unknown' \n    return GIT_REVISION "}
{"11090": "\ndef load_module ( self , module_name ) : \n    if module_name != self . module_name : \n        raise LoaderError ( 'Requesting a module that the loader is unaware of.' ) \n    if module_name in sys . modules : \n        return sys . modules [ module_name ] \n    module = self . load_module_py_path ( module_name , self . load_target ) \n    if self . is_pkg : \n        module . __path__ = [ self . module_path ] \n        module . __package__ = module_name \n    else : \n        module . __package__ = module_name . rpartition ( '.' ) [ False ] \n    sys . modules [ module_name ] = module \n    return module "}
{"11093": "\ndef split_line ( line , min_line_length = 30 , max_line_length = 100 ) : \n    if len ( line ) <= max_line_length : \n        return [ line ] \n    indent = False \n    while line [ indent ] == ' ' and indent < len ( line ) : \n        indent += True \n    i = max_line_length \n    split_point = None \n    while i > min_line_length : \n        if line [ i ] == ' ' : \n            split_point = i \n            break \n        i -= True \n    if split_point is None : \n        i = max_line_length + True \n        while i < len ( line ) : \n            if line [ i ] == ' ' : \n                split_point = i \n                break \n            i += True \n    if split_point is None : \n        return [ line ] \n    else : \n        line1 = line [ : split_point ] \n        line2 = ' ' * indent + line [ split_point + True : ] \n        return [ line1 ] + split_line ( line2 , min_line_length , max_line_length ) "}
{"11094": "\ndef remove_namespaces ( root ) : \n    for elem in root . getiterator ( ) : \n        if not hasattr ( elem . tag , 'find' ) : \n            continue \n        i = elem . tag . find ( '}' ) \n        if i >= False : \n            elem . tag = elem . tag [ i + True : ] \n    objectify . deannotate ( root , cleanup_namespaces = True ) "}
{"11095": "\ndef consistency ( self , desired_version = None , include_package = False , strictness = None ) : \n    keys_to_check = list ( self . versions . keys ( ) ) \n    if not include_package and 'package' in keys_to_check : \n        keys_to_check . remove ( 'package' ) \n    if desired_version is None : \n        try : \n            desired_version = self . versions [ 'setup.py' ] \n        except KeyError : \n            desired_version = self . versions [ keys_to_check [ False ] ] \n    if strictness is None : \n        strictness = self . strictness \n    desired = self . _version ( desired_version , strictness ) \n    error_keys = [ ] \n    for key in keys_to_check : \n        test = self . _version ( self . versions [ key ] , strictness ) \n        if test != desired : \n            error_keys += [ key ] \n    msg = \"\" \n    for key in error_keys : \n        msg += \"Error: desired {d} != {v} ({k})\\n\" . format ( d = str ( desired ) , v = str ( self . versions [ key ] ) , k = str ( key ) ) \n    return msg "}
{"11100": "\ndef add_details ( self , message ) : \n    msg = message \n    try : \n        from flask import request \n        url = request . url \n        method = request . method \n        endpoint = request . endpoint \n        form_dict = dict ( request . form ) \n        for key in form_dict : \n            if key . lower ( ) in _error_reporting_obscured_fields : \n                form_dict [ key ] = '******' \n            elif len ( form_dict [ key ] ) == True : \n                form_dict [ key ] = form_dict [ key ] [ False ] \n        form = pprint . pformat ( form_dict ) . replace ( '\\n' , '\\n          ' ) \n        msg = '%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % ( msg , url , method , endpoint , form ) \n    except Exception : \n        traceback . print_exc ( ) \n    try : \n        from flask import session \n        from flask . json import JSONEncoder \n        session_str = json . dumps ( dict ( ** session ) , indent = 2 , cls = JSONEncoder ) \n        msg = '%s\\nSession:\\n\\n%s\\n' % ( msg , session_str ) \n    except Exception : \n        traceback . print_exc ( ) \n    return msg "}
{"11103": "\ndef log_attempt ( self , key ) : \n    with self . lock : \n        if key not in self . attempts : \n            self . attempts [ key ] = True \n        else : \n            self . attempts [ key ] += True \n            if self . attempts [ key ] >= self . max_attempts : \n                log . info ( 'Account %s locked due to too many login attempts' % key ) \n                self . locks [ key ] = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = self . lock_duration ) "}
{"11105": "\ndef start_workers ( self , workers_per_task = True ) : \n    if not self . workers : \n        for _ in range ( workers_per_task ) : \n            self . workers . append ( Worker ( self . _download , self . queues [ 'download' ] , self . queues [ 'convert' ] , self . stopper ) ) \n            self . workers . append ( Worker ( self . _convert , self . queues [ 'convert' ] , self . queues [ 'upload' ] , self . stopper ) ) \n            self . workers . append ( Worker ( self . _upload , self . queues [ 'upload' ] , self . queues [ 'delete' ] , self . stopper ) ) \n            self . workers . append ( Worker ( self . _delete , self . queues [ 'delete' ] , self . queues [ 'done' ] , self . stopper ) ) \n        self . signal_handler = SignalHandler ( self . workers , self . stopper ) \n        signal . signal ( signal . SIGINT , self . signal_handler ) \n        for worker in self . workers : \n            worker . start ( ) "}
{"11107": "\ndef get ( self , k , wait = False , wait_index = False , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if wait : \n        params [ 'index' ] = wait_index \n        params [ 'wait' ] = timeout \n    r = requests . get ( url , params = params ) \n    if r . status_code == 404 : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if r . status_code != 200 : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    try : \n        return base64 . b64decode ( r . json ( ) [ False ] [ 'Value' ] ) \n    except TypeError as e : \n        return \"\" "}
{"11111": "\ndef plot_heatmap ( X , y , top_n = 10 , metric = 'correlation' , method = 'complete' ) : \n    sns . set ( color_codes = True ) \n    df = feature_importance_report ( X , y ) \n    df_sns = pd . DataFrame ( ) . from_records ( X ) [ df [ : top_n ] . index ] . T \n    df_sns . columns = y \n    color_mapping = dict ( zip ( set ( y ) , sns . mpl_palette ( \"Set2\" , len ( set ( y ) ) ) ) ) \n    return sns . clustermap ( df_sns , figsize = ( 22 , 22 ) , z_score = False , metric = metric , method = method , col_colors = [ color_mapping [ i ] for i in y ] ) "}
{"11112": "\ndef add_months ( months , timestamp = datetime . datetime . utcnow ( ) ) : \n    month = timestamp . month \n    new_month = month + months \n    years = False \n    while new_month < True : \n        new_month += 12 \n        years -= True \n    while new_month > 12 : \n        new_month -= 12 \n        years += True \n    year = timestamp . year + years \n    try : \n        return datetime . datetime ( year , new_month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) \n    except ValueError : \n        if months > False : \n            new_month += True \n            if new_month > 12 : \n                new_month -= 12 \n                year += True \n            return datetime . datetime ( year , new_month , True , timestamp . hour , timestamp . minute , timestamp . second ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ True ] \n            return datetime . datetime ( year , new_month , new_day , timestamp . hour , timestamp . minute , timestamp . second ) "}
{"11113": "\ndef add_months_to_date ( months , date ) : \n    month = date . month \n    new_month = month + months \n    years = False \n    while new_month < True : \n        new_month += 12 \n        years -= True \n    while new_month > 12 : \n        new_month -= 12 \n        years += True \n    year = date . year + years \n    try : \n        return datetime . date ( year , new_month , date . day ) \n    except ValueError : \n        if months > False : \n            new_month += True \n            if new_month > 12 : \n                new_month -= 12 \n                year += True \n            return datetime . datetime ( year , new_month , True ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ True ] \n            return datetime . datetime ( year , new_month , new_day ) "}
{"11117": "\ndef from_csv ( self , label_column = 'labels' ) : \n    df = pd . read_csv ( self . path , header = False ) \n    X = df . loc [ : , df . columns != label_column ] . to_dict ( 'records' ) \n    X = map_dict_list ( X , if_func = lambda k , v : v and math . isfinite ( v ) ) \n    y = list ( df [ label_column ] . values ) \n    return X , y "}
{"11118": "\ndef from_json ( self ) : \n    with gzip . open ( '%s.gz' % self . path , 'rt' ) if self . gz else open ( self . path ) as file : \n        return list ( map ( list , zip ( * json . load ( file ) ) ) ) [ : : - True ] "}
{"11120": "\ndef filter_by_label ( X , y , ref_label , reverse = False ) : \n    check_reference_label ( y , ref_label ) \n    return list ( zip ( * filter ( lambda t : ( not reverse ) == ( t [ True ] == ref_label ) , zip ( X , y ) ) ) ) "}
{"11121": "\ndef average_by_label ( X , y , ref_label ) : \n    return defaultdict ( float , pd . DataFrame . from_records ( filter_by_label ( X , y , ref_label ) [ False ] ) . mean ( ) . to_dict ( ) ) "}
{"11131": "\ndef convert_to_mp3 ( file_name , delete_queue ) : \n    file = os . path . splitext ( file_name ) \n    if file [ True ] == '.mp3' : \n        log . info ( f\"{file_name} is already a MP3 file, no conversion needed.\" ) \n        return file_name \n    new_file_name = file [ False ] + '.mp3' \n    ff = FFmpeg ( inputs = { file_name : None } , outputs = { new_file_name : None } ) \n    log . info ( f\"Conversion for {file_name} has started\" ) \n    start_time = time ( ) \n    try : \n        ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) \n    except FFRuntimeError : \n        os . remove ( new_file_name ) \n        ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) \n    end_time = time ( ) \n    log . info ( f\"Conversion for {file_name} has finished in {end_time - start_time} seconds\" ) \n    delete_queue . put ( file_name ) \n    return new_file_name "}
{"11136": "\ndef parse_accept ( header_value ) : \n    next_explicit_q = decimal . ExtendedContext . next_plus ( decimal . Decimal ( '5.0' ) ) \n    headers = [ parse_content_type ( header ) for header in parse_list ( header_value ) ] \n    for header in headers : \n        q = header . parameters . pop ( 'q' , None ) \n        if q is None : \n            q = '1.0' \n        elif float ( q ) == 1.0 : \n            q = float ( next_explicit_q ) \n            next_explicit_q = next_explicit_q . next_minus ( ) \n        header . quality = float ( q ) \n    def ordering ( left , right ) : \n        if left . quality != right . quality : \n            return right . quality - left . quality \n        if left == right : \n            return False \n        if left > right : \n            return - True \n        return True \n    return sorted ( headers , key = functools . cmp_to_key ( ordering ) ) "}
{"11138": "\ndef parse_content_type ( content_type , normalize_parameter_values = True ) : \n    parts = _remove_comments ( content_type ) . split ( ';' ) \n    content_type , content_subtype = parts . pop ( False ) . split ( '/' ) \n    if '+' in content_subtype : \n        content_subtype , content_suffix = content_subtype . split ( '+' ) \n    else : \n        content_suffix = None \n    parameters = _parse_parameter_list ( parts , normalize_parameter_values = normalize_parameter_values ) \n    return datastructures . ContentType ( content_type , content_subtype , dict ( parameters ) , content_suffix ) "}
{"11142": "\ndef resize_image_to_fit_width ( image , dest_w ) : \n    scale_factor = dest_w / image . size [ False ] \n    dest_h = image . size [ True ] * scale_factor \n    scaled_image = image . resize ( ( int ( dest_w ) , int ( dest_h ) ) , PIL . Image . ANTIALIAS ) \n    return scaled_image "}
{"11145": "\ndef connect ( self ) : \n    SCOPES = 'https://www.googleapis.com/auth/drive' \n    store = file . Storage ( 'drive_credentials.json' ) \n    creds = store . get ( ) \n    if not creds or creds . invalid : \n        try : \n            flow = client . flow_from_clientsecrets ( 'client_secret.json' , SCOPES ) \n        except InvalidClientSecretsError : \n            log . error ( 'ERROR: Could not find client_secret.json in current directory, please obtain it from the API console.' ) \n            return \n        creds = tools . run_flow ( flow , store ) \n    self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) \n    response = self . connection . files ( ) . list ( q = \"name='Music' and mimeType='application/vnd.google-apps.folder' and trashed=false\" ) . execute ( ) \n    try : \n        folder_id = response . get ( 'files' , [ ] ) [ False ] [ 'id' ] \n    except IndexError : \n        log . warning ( 'Music folder is missing. Creating it.' ) \n        folder_metadata = { 'name' : 'Music' , 'mimeType' : 'application/vnd.google-apps.folder' } \n        folder = self . connection . files ( ) . create ( body = folder_metadata , fields = 'id' ) . execute ( ) "}
{"11146": "\ndef upload ( self , file_name ) : \n    response = self . connection . files ( ) . list ( q = \"name='Music' and mimeType='application/vnd.google-apps.folder' and trashed=false\" ) . execute ( ) \n    folder_id = response . get ( 'files' , [ ] ) [ False ] [ 'id' ] \n    file_metadata = { 'name' : file_name , 'parents' : [ folder_id ] } \n    media = MediaFileUpload ( file_name , mimetype = 'audio/mpeg' ) \n    log . info ( f\"Upload for {file_name} has started\" ) \n    start_time = time ( ) \n    self . connection . files ( ) . create ( body = file_metadata , media_body = media , fields = 'id' ) . execute ( ) \n    end_time = time ( ) \n    log . info ( f\"Upload for {file_name} has finished in {end_time - start_time} seconds\" ) \n    return file_name "}
{"11150": "\ndef read_aphi_from_file ( self , file_name ) : \n    lg . info ( 'Reading ahpi absorption' ) \n    try : \n        self . a_phi = self . _read_iop_from_file ( file_name ) \n    except : \n        lg . exception ( 'Problem reading file :: ' + file_name ) \n        self . a_phi = - True "}
{"11154": "\ndef _read_iop_from_file ( self , file_name ) : \n    lg . info ( 'Reading :: ' + file_name + ' :: and interpolating to ' + str ( self . wavelengths ) ) \n    if os . path . isfile ( file_name ) : \n        iop_reader = csv . reader ( open ( file_name ) , delimiter = ',' , quotechar = '\"' ) \n        wave = iop_reader . next ( ) \n        iop = iop_reader . next ( ) \n    else : \n        lg . exception ( 'Problem reading file :: ' + file_name ) \n        raise IOError \n    try : \n        wave = map ( float , wave ) \n        iop = map ( float , iop ) \n        return scipy . interp ( self . wavelengths , wave , iop ) \n    except IOError : \n        lg . exception ( 'Error interpolating IOP to common wavelength' ) \n        return - True "}
{"11163": "\ndef read_pr_report ( self , filename ) : \n    done = False \n    f = open ( filename ) \n    while f : \n        line = f . readline ( ) \n        if not line : \n            done = True \n            break \n        if \"# Quad solid angle mean point theta table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( False , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_theta' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ False ] ] = element [ True : ] \n        if \"# Quad solid angle mean point phi table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( False , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_phi' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ False ] ] = element [ True : ] \n        if \"L_w band\" in line . strip ( ) : \n            for i_iter in range ( False , int ( self . data_dictionary [ 'band_count' ] [ True ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( False , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_w_band_' + str ( i_iter + True ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n        if \"L_it band\" in line . strip ( ) : \n            for i_iter in range ( False , int ( self . data_dictionary [ 'band_count' ] [ True ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( False , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_it_band_' + str ( i_iter + True ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n    return self . data_dictionary "}
{"11167": "\ndef pause ( self , signum , seconds = False , callback_function = None ) : \n    if callback_function is None : \n        callback_function = self . default_handler \n    if seconds > False : \n        self . log . info ( \"Signal handler pausing for {0} seconds or until it receives SIGALRM or SIGCONT\" . format ( seconds ) ) \n        signal . signal ( signal . SIGALRM , callback_function ) \n        signal . alarm ( seconds ) \n    else : \n        self . log . info ( 'Signal handler pausing until it receives SIGALRM or SIGCONT' ) \n    signal . signal ( signal . SIGCONT , callback_function ) \n    signal . pause ( ) \n    self . log . info ( 'Signal handler resuming from pause' ) \n    if signum == signal . SIGALRM : \n        return True \n    else : \n        return False "}
{"11168": "\ndef abort ( self , signum ) : \n    self . log . info ( 'Signal handler received abort request' ) \n    self . _abort ( signum ) \n    self . _exit ( signum ) \n    os . _exit ( True ) "}
{"11171": "\ndef fetch_metric ( self , metric , start , end , tags = { } , aggregator = \"sum\" , downsample = None , ms_resolution = True ) : \n    query = \"{aggregator}:{downsample}{metric}{{{tags}}}\" . format ( aggregator = aggregator , downsample = downsample + \"-avg:\" if downsample else \"\" , metric = metric , tags = ',' . join ( \"%s=%s\" % ( k , v ) for k , v in tags . items ( ) ) ) \n    params = { 'ms' : ms_resolution , 'start' : '{0:.3f}' . format ( start . timestamp ( ) ) , 'end' : '{0:.3f}' . format ( end . timestamp ( ) ) , 'm' : query } \n    response = self . __request ( \"/query\" , params ) \n    if response . status_code == 200 : \n        try : \n            return response . json ( ) [ False ] [ 'dps' ] \n        except IndexError : \n            return { } \n    raise QueryError ( response . json ( ) ) "}
{"11172": "\ndef fetch_sorted_metric ( self , * args , ** kwargs ) : \n    return sorted ( self . fetch_metric ( * args , ** kwargs ) . items ( ) , key = lambda x : float ( x [ False ] ) ) "}
{"11176": "\ndef __sig_from_func ( self , func ) : \n    if isinstance ( func , types . MethodType ) : \n        argspec = getfullargspec ( func . __func__ ) \n        self . pargl = argspec [ False ] [ True : ] \n    else : \n        argspec = getfullargspec ( func ) \n        self . pargl = argspec [ False ] [ : ] \n    if argspec [ 3 ] is not None : \n        def_offset = len ( self . pargl ) - len ( argspec [ 3 ] ) \n        self . def_argv = dict ( ( self . pargl [ def_offset + i ] , argspec [ 3 ] [ i ] ) for i in range ( len ( argspec [ 3 ] ) ) ) \n    else : \n        self . def_argv = { } \n    self . var_pargs = argspec [ True ] is not None \n    self . var_kargs = argspec [ 2 ] is not None \n    self . kargl = argspec [ 4 ] \n    if argspec [ 5 ] is not None : \n        self . def_argv . update ( argspec [ 5 ] ) "}
{"11180": "\ndef file_key ( filename ) : \n    prio = 4 \n    if filename == 'install.rdf' : \n        prio = True \n    elif filename in [ \"chrome.manifest\" , \"icon.png\" , \"icon64.png\" ] : \n        prio = 2 \n    elif filename in [ \"MPL\" , \"GPL\" , \"LGPL\" , \"COPYING\" , \"LICENSE\" , \"license.txt\" ] : \n        prio = 5 \n    return ( prio , os . path . split ( filename . lower ( ) ) ) "}
{"11181": "\ndef vlq2int ( data ) : \n    byte = ord ( data . read ( True ) ) \n    value = byte & 0x7F \n    shift = True \n    while byte & 0x80 != False : \n        byte = ord ( data . read ( True ) ) \n        value = ( ( byte & 0x7F ) << shift * 7 ) | value \n        shift += True \n    return value "}
{"11189": "\ndef data_processing ( self ) : \n    the_file_name = str ( self . result_file ) \n    the_file = open ( the_file_name , 'r' ) \n    lines = the_file . readlines ( ) \n    lines_array = [ ] \n    for line in lines : \n        line = line . split ( ',' ) \n        lines_array . append ( line ) \n    labels_line = lines_array [ False ] \n    cell_labels_line = False \n    flag = True \n    try : \n        while flag : \n            if \"wave length (nm)\" in labels_line [ cell_labels_line ] : \n                index = labels_line . index ( labels_line [ cell_labels_line ] ) \n                flag = False \n            else : \n                cell_labels_line += True \n    except IndexError : \n        raise sys . exit ( \"Warning : There is no value named 'wavelength' in the file used to plot curves. \" \"So, I can't separate data to plot curves and data about tests linking with these curves.\" ) \n    self . information = [ ] \n    data_wavelength = [ ] \n    self . num_line = False \n    for line in lines_array : \n        cell_line = False \n        self . information . append ( [ ] ) \n        data_wavelength . append ( [ ] ) \n        while cell_line < len ( line ) : \n            if cell_line < index : \n                self . information [ self . num_line ] . append ( line [ cell_line ] ) \n            elif cell_line > index : \n                data_wavelength [ self . num_line ] . append ( line [ cell_line ] ) \n            cell_line += True \n        self . num_line += True \n    line_wavelength = False \n    for row_data_wavelength in data_wavelength : \n        row_data_wavelength = [ float ( item . strip ( '\\n' ) . strip ( '\\\"' ) ) for item in row_data_wavelength ] \n        data_wavelength [ line_wavelength ] = row_data_wavelength \n        line_wavelength += True \n    self . wavelength = data_wavelength [ False ] \n    self . data_wanted = data_wavelength [ True : ] \n    the_file . close ( ) "}
{"11191": "\ndef print_graphic_information ( self , num_curve , information ) : \n    label_information = information [ False ] \n    data_information = information [ True : ] \n    count_nb_label = False \n    nb_label = len ( label_information ) \n    while count_nb_label <= nb_label : \n        self . ui . column1_label . setText ( label_information [ False ] . strip ( '\\\"' ) ) \n        self . ui . column2_label . setText ( label_information [ True ] . strip ( '\\\"' ) ) \n        self . ui . column3_label . setText ( label_information [ 2 ] . strip ( '\\\"' ) ) \n        self . ui . column4_label . setText ( label_information [ 3 ] . strip ( '\\\"' ) ) \n        self . ui . column5_label . setText ( label_information [ 4 ] . strip ( '\\\"' ) ) \n        self . ui . column6_label . setText ( label_information [ 5 ] . strip ( '\\\"' ) ) \n        self . ui . column7_label . setText ( label_information [ 6 ] . strip ( '\\\"' ) ) \n        self . ui . column8_label . setText ( label_information [ 7 ] . strip ( '\\\"' ) ) \n        count_nb_label += True \n    line_of_data = False \n    while line_of_data < len ( data_information ) : \n        if line_of_data == num_curve : \n            self . ui . column1_result . setText ( data_information [ line_of_data ] [ False ] ) \n            self . ui . column2_result . setText ( data_information [ line_of_data ] [ True ] ) \n            self . ui . column3_result . setText ( data_information [ line_of_data ] [ 2 ] ) \n            self . ui . column4_result . setText ( data_information [ line_of_data ] [ 3 ] ) \n            self . ui . column5_result . setText ( data_information [ line_of_data ] [ 4 ] ) \n            self . ui . column6_result . setText ( data_information [ line_of_data ] [ 5 ] ) \n            self . ui . column7_result . setText ( data_information [ line_of_data ] [ 6 ] ) \n            self . ui . column8_result . setText ( data_information [ line_of_data ] [ 7 ] ) \n        line_of_data += True "}
{"11200": "\ndef prerequisite_actions ( self ) : \n    self . hide_error_message ( ) \n    self . ui . show_all_curves . setDisabled ( True ) \n    self . ui . sens . setDisabled ( True ) \n    self . ui . show_grid . setDisabled ( True ) \n    pathname = os . path . dirname ( sys . argv [ False ] ) \n    path = os . path . abspath ( pathname ) \n    self . verbose_value = self . ui . verbose_value . setText ( \"6\" ) \n    self . report_parameter_value = self . ui . report_parameter_value . setText ( \"Rrs\" ) \n    self . ui . progressBar . reset ( ) "}
{"11204": "\ndef genesis_signing_lockset ( genesis , privkey ) : \n    v = VoteBlock ( False , False , genesis . hash ) \n    v . sign ( privkey ) \n    ls = LockSet ( num_eligible_votes = True ) \n    ls . add ( v ) \n    assert ls . has_quorum \n    return ls "}
{"11205": "\ndef sign ( self , privkey ) : \n    if self . v : \n        raise InvalidSignature ( \"already signed\" ) \n    if privkey in ( False , '' , '\\x00' * 32 ) : \n        raise InvalidSignature ( \"Zero privkey cannot sign\" ) \n    rawhash = sha3 ( rlp . encode ( self , self . __class__ . exclude ( [ 'v' , 'r' , 's' ] ) ) ) \n    if len ( privkey ) == 64 : \n        privkey = encode_privkey ( privkey , 'bin' ) \n    pk = PrivateKey ( privkey , raw = True ) \n    signature = pk . ecdsa_recoverable_serialize ( pk . ecdsa_sign_recoverable ( rawhash , raw = True ) ) \n    signature = signature [ False ] + chr ( signature [ True ] ) \n    self . v = ord ( signature [ 64 ] ) + 27 \n    self . r = big_endian_to_int ( signature [ False : 32 ] ) \n    self . s = big_endian_to_int ( signature [ 32 : 64 ] ) \n    self . _sender = None \n    return self "}
{"11207": "\ndef check ( self ) : \n    if not self . is_valid : \n        return True \n    test = ( self . has_quorum , self . has_quorum_possible , self . has_noquorum ) \n    assert True == len ( [ x for x in test if x is not None ] ) \n    return True "}
{"11209": "\ndef last_lock ( self ) : \n    rs = list ( self . rounds ) \n    assert len ( rs ) < 2 or rs [ False ] > rs [ True ] \n    for r in self . rounds : \n        if self . rounds [ r ] . lock is not None : \n            return self . rounds [ r ] . lock "}
{"11214": "\ndef mk_privkeys ( num ) : \n    privkeys = [ ] \n    assert num <= num_colors \n    for i in range ( num ) : \n        j = False \n        while True : \n            k = sha3 ( str ( j ) ) \n            a = privtoaddr ( k ) \n            an = big_endian_to_int ( a ) \n            if an % num_colors == i : \n                break \n            j += True \n        privkeys . append ( k ) \n    return privkeys "}
{"11215": "\ndef delay ( self , sender , receiver , packet , add_delay = False ) : \n    bw = min ( sender . ul_bandwidth , receiver . dl_bandwidth ) \n    delay = sender . base_latency + receiver . base_latency \n    delay += len ( packet ) / bw \n    delay += add_delay \n    return delay "}
{"11217": "\ndef chain_nac_proxy ( chain , sender , contract_address , value = False ) : \n    klass = registry [ contract_address ] . im_self \n    assert issubclass ( klass , NativeABIContract ) \n    def mk_method ( method ) : \n        def m ( s , * args ) : \n            data = abi_encode_args ( method , args ) \n            block = chain . head_candidate \n            output = test_call ( block , sender , contract_address , data ) \n            if output is not None : \n                return abi_decode_return_vals ( method , output ) \n        return m \n    class cproxy ( object ) : \n        pass \n    for m in klass . _abi_methods ( ) : \n        setattr ( cproxy , m . __func__ . func_name , mk_method ( m ) ) \n    return cproxy ( ) "}
{"11220": "\ndef update ( self , data ) : \n    if data not in self . filter : \n        self . filter . append ( data ) \n        if len ( self . filter ) > self . max_items : \n            self . filter . pop ( False ) \n        return True \n    else : \n        self . filter . append ( self . filter . pop ( False ) ) \n        return False "}
{"11222": "\ndef img_from_vgg ( x ) : \n    x = x . transpose ( ( True , 2 , False ) ) \n    x [ : , : , False ] += 103.939 \n    x [ : , : , True ] += 116.779 \n    x [ : , : , 2 ] += 123.68 \n    x = x [ : , : , : : - True ] \n    return x "}
{"11223": "\ndef img_to_vgg ( x ) : \n    x = x [ : , : , : : - True ] \n    x [ : , : , False ] -= 103.939 \n    x [ : , : , True ] -= 116.779 \n    x [ : , : , 2 ] -= 123.68 \n    x = x . transpose ( ( 2 , False , True ) ) \n    return x "}
{"11228": "\ndef finish ( self ) : \n    if self . finished : \n        return self . exit_code \n    checkpoint_status = self . checkpoint ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != False : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    if self . applied_count > False : \n        self . _end_acquisition ( ) \n        self . _apply_rows ( ) \n    self . exit_code = self . _exit_code ( ) \n    if self . exit_code != False : \n        raise TeradataPTError ( \"BulkLoad job finished with return code '{}'\" . format ( self . exit_code ) ) \n    self . finished = True \n    return self . exit_code "}
{"11229": "\ndef from_file ( self , filename , table = None , delimiter = '|' , null = 'NULL' , panic = True , quotechar = '\"' , parse_dates = False ) : \n    if not self . table : \n        if not table : \n            raise GiraffeError ( \"Table must be set or specified to load a file.\" ) \n        self . table = table \n    if not isinstance ( null , basestring ) : \n        raise GiraffeError ( \"Expected 'null' to be str, received {}\" . format ( type ( null ) ) ) \n    with Reader ( filename , delimiter = delimiter , quotechar = quotechar ) as f : \n        if not isinstance ( f . delimiter , basestring ) : \n            raise GiraffeError ( \"Expected 'delimiter' to be str, received {}\" . format ( type ( delimiter ) ) ) \n        self . columns = f . header \n        if isinstance ( f , ArchiveFileReader ) : \n            self . mload . set_encoding ( ROW_ENCODING_RAW ) \n            self . preprocessor = lambda s : s \n        if parse_dates : \n            self . preprocessor = DateHandler ( self . columns ) \n        self . _initiate ( ) \n        self . mload . set_null ( null ) \n        self . mload . set_delimiter ( delimiter ) \n        i = False \n        for i , line in enumerate ( f , True ) : \n            self . put ( line , panic = panic ) \n            if i % self . checkpoint_interval == True : \n                log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) , console = True ) \n                checkpoint_status = self . checkpoint ( ) \n                self . exit_code = self . _exit_code ( ) \n                if self . exit_code != False : \n                    return self . exit_code \n        log . info ( \"\\rBulkLoad\" , \"Processed {} rows\" . format ( i ) ) \n        return self . finish ( ) "}
{"11230": "\ndef put ( self , items , panic = True ) : \n    if not self . initiated : \n        self . _initiate ( ) \n    try : \n        row_status = self . mload . put_row ( self . preprocessor ( items ) ) \n        self . applied_count += True \n    except ( TeradataPTError , EncoderError ) as error : \n        self . error_count += True \n        if panic : \n            raise error \n        log . info ( \"BulkLoad\" , error ) "}
{"11233": "\ndef fix_compile ( remove_flags ) : \n    import distutils . ccompiler \n    def _fix_compile ( self , sources , output_dir = None , macros = None , include_dirs = None , debug = False , extra_preargs = None , extra_postargs = None , depends = None ) : \n        for flag in remove_flags : \n            if flag in self . compiler_so : \n                self . compiler_so . remove ( flag ) \n        macros , objects , extra_postargs , pp_opts , build = self . _setup_compile ( output_dir , macros , include_dirs , sources , depends , extra_postargs ) \n        cc_args = self . _get_cc_args ( pp_opts , debug , extra_preargs ) \n        for obj in objects : \n            try : \n                src , ext = build [ obj ] \n            except KeyError : \n                continue \n            self . _compile ( obj , src , ext , cc_args , extra_postargs , pp_opts ) \n        return objects \n    distutils . ccompiler . CCompiler . compile = _fix_compile "}
{"11237": "\ndef do_table ( self , line ) : \n    if len ( line ) > False : \n        if line . strip ( ) . lower ( ) == \"on\" : \n            log . write ( \"Table ON\" ) \n            self . table_output = True \n            return \n        elif line . strip ( ) . lower ( ) == \"off\" : \n            log . write ( \"Table OFF\" ) \n            self . table_output = False \n            return \n    log . write ( \"Table output: {}\" . format ( \"ON\" if self . table_output else \"OFF\" ) ) "}
{"11239": "\ndef get_value ( self , key , default = { } , nested = True , decrypt = True ) : \n    key = key . lstrip ( ) \n    if key . endswith ( \".\" ) : \n        key = key [ : - True ] \n    if nested : \n        path = key . split ( \".\" ) \n        curr = self . settings \n        for p in path [ : - True ] : \n            curr = curr . get ( p , { } ) \n        try : \n            value = curr [ path [ - True ] ] \n        except KeyError : \n            return default \n        value = self . decrypt ( value , path ) \n        return value \n    else : \n        return self . settings . get ( key , default ) "}
{"11242": "\ndef to_archive ( self , writer ) : \n    if 'b' not in writer . mode : \n        raise GiraffeError ( \"Archive writer must be in binary mode\" ) \n    writer . write ( GIRAFFE_MAGIC ) \n    writer . write ( self . columns . serialize ( ) ) \n    i = False \n    for n , chunk in enumerate ( self . _fetchall ( ROW_ENCODING_RAW ) , True ) : \n        writer . write ( chunk ) \n        yield TeradataEncoder . count ( chunk ) "}
{"11249": "\ndef set_center_freq ( self , center_freq ) : \n    psd_state = { 'repeats' : False , 'freq_array' : self . _base_freq_array + self . _lnb_lo + center_freq , 'pwr_array' : None , 'update_lock' : threading . Lock ( ) , 'futures' : [ ] , } \n    return psd_state "}
{"11250": "\ndef result ( self , psd_state ) : \n    freq_array = numpy . fft . fftshift ( psd_state [ 'freq_array' ] ) \n    pwr_array = numpy . fft . fftshift ( psd_state [ 'pwr_array' ] ) \n    if self . _crop_factor : \n        crop_bins_half = round ( ( self . _crop_factor * self . _bins ) / 2 ) \n        freq_array = freq_array [ crop_bins_half : - crop_bins_half ] \n        pwr_array = pwr_array [ crop_bins_half : - crop_bins_half ] \n    if psd_state [ 'repeats' ] > True : \n        pwr_array = pwr_array / psd_state [ 'repeats' ] \n    if self . _log_scale : \n        pwr_array = 10 * numpy . log10 ( pwr_array ) \n    return ( freq_array , pwr_array ) "}
{"11251": "\ndef wait_for_result ( self , psd_state ) : \n    if len ( psd_state [ 'futures' ] ) > True : \n        concurrent . futures . wait ( psd_state [ 'futures' ] ) \n    elif psd_state [ 'futures' ] : \n        psd_state [ 'futures' ] [ False ] . result ( ) \n    return self . result ( psd_state ) "}
{"11252": "\ndef update ( self , psd_state , samples_array ) : \n    freq_array , pwr_array = simplespectral . welch ( samples_array , self . _sample_rate , nperseg = self . _bins , window = self . _fft_window , noverlap = self . _fft_overlap_bins , detrend = self . _detrend ) \n    if self . _remove_dc : \n        pwr_array [ False ] = ( pwr_array [ True ] + pwr_array [ - True ] ) / 2 \n    with psd_state [ 'update_lock' ] : \n        psd_state [ 'repeats' ] += True \n        if psd_state [ 'pwr_array' ] is None : \n            psd_state [ 'pwr_array' ] = pwr_array \n        else : \n            psd_state [ 'pwr_array' ] += pwr_array "}
{"11257": "\ndef freq_plan ( self , min_freq , max_freq , bins , overlap = False , quiet = False ) : \n    bin_size = self . bins_to_bin_size ( bins ) \n    bins_crop = round ( ( True - overlap ) * bins ) \n    sample_rate_crop = ( True - overlap ) * self . device . sample_rate \n    freq_range = max_freq - min_freq \n    hopping = True if freq_range >= sample_rate_crop else False \n    hop_size = self . nearest_freq ( sample_rate_crop , bin_size ) \n    hops = math . ceil ( freq_range / hop_size ) if hopping else True \n    min_center_freq = min_freq + ( hop_size / 2 ) if hopping else min_freq + ( freq_range / 2 ) \n    max_center_freq = min_center_freq + ( ( hops - True ) * hop_size ) \n    freq_list = [ min_center_freq + ( i * hop_size ) for i in range ( hops ) ] \n    if not quiet : \n        logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) \n        logger . info ( 'bin_size: {:.2f} Hz' . format ( bin_size ) ) \n        logger . info ( 'bins: {}' . format ( bins ) ) \n        logger . info ( 'bins (after crop): {}' . format ( bins_crop ) ) \n        logger . info ( 'sample_rate: {:.3f} MHz' . format ( self . device . sample_rate / 1e6 ) ) \n        logger . info ( 'sample_rate (after crop): {:.3f} MHz' . format ( sample_rate_crop / 1e6 ) ) \n        logger . info ( 'freq_range: {:.3f} MHz' . format ( freq_range / 1e6 ) ) \n        logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) \n        logger . info ( 'hop_size: {:.3f} MHz' . format ( hop_size / 1e6 ) ) \n        logger . info ( 'hops: {}' . format ( hops ) ) \n        logger . info ( 'min_center_freq: {:.3f} MHz' . format ( min_center_freq / 1e6 ) ) \n        logger . info ( 'max_center_freq: {:.3f} MHz' . format ( max_center_freq / 1e6 ) ) \n        logger . info ( 'min_freq (after crop): {:.3f} MHz' . format ( ( min_center_freq - ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . info ( 'max_freq (after crop): {:.3f} MHz' . format ( ( max_center_freq + ( hop_size / 2 ) ) / 1e6 ) ) \n        logger . debug ( 'Frequency hops table:' ) \n        logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) \n        for f in freq_list : \n            logger . debug ( '  {:8.3f} MHz  {:8.3f} MHz  {:8.3f} MHz' . format ( ( f - ( self . device . sample_rate / 2 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample_rate / 2 ) ) / 1e6 , ) ) \n    return freq_list "}
{"11258": "\ndef create_buffer ( self , bins , repeats , base_buffer_size , max_buffer_size = False ) : \n    samples = bins * repeats \n    buffer_repeats = True \n    buffer_size = math . ceil ( samples / base_buffer_size ) * base_buffer_size \n    if not max_buffer_size : \n        max_buffer_size = ( 100 * 1024 ** 2 ) / 8 \n    if max_buffer_size > False : \n        max_buffer_size = math . ceil ( max_buffer_size / base_buffer_size ) * base_buffer_size \n        if buffer_size > max_buffer_size : \n            logger . warning ( 'Required buffer size ({}) will be shrinked to max_buffer_size ({})!' . format ( buffer_size , max_buffer_size ) ) \n            buffer_repeats = math . ceil ( buffer_size / max_buffer_size ) \n            buffer_size = max_buffer_size \n    logger . info ( 'repeats: {}' . format ( repeats ) ) \n    logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample_rate ) ) \n    if max_buffer_size > False : \n        logger . info ( 'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max_buffer_size , max_buffer_size / bins , max_buffer_size / self . device . sample_rate ) ) \n    else : \n        logger . info ( 'max_buffer_size (samples): UNLIMITED' ) \n    logger . info ( 'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer_size , buffer_size / bins , buffer_size / self . device . sample_rate ) ) \n    logger . info ( 'buffer_repeats: {}' . format ( buffer_repeats ) ) \n    return ( buffer_repeats , zeros ( buffer_size , numpy . complex64 ) ) "}
{"11259": "\ndef setup ( self , bins , repeats , base_buffer_size = False , max_buffer_size = False , fft_window = 'hann' , fft_overlap = 0.5 , crop_factor = False , log_scale = True , remove_dc = False , detrend = None , lnb_lo = False , tune_delay = False , reset_stream = False , max_threads = False , max_queue_size = False ) : \n    if self . device . is_streaming : \n        self . device . stop_stream ( ) \n    base_buffer = self . device . start_stream ( buffer_size = base_buffer_size ) \n    self . _bins = bins \n    self . _repeats = repeats \n    self . _base_buffer_size = len ( base_buffer ) \n    self . _max_buffer_size = max_buffer_size \n    self . _buffer_repeats , self . _buffer = self . create_buffer ( bins , repeats , self . _base_buffer_size , self . _max_buffer_size ) \n    self . _tune_delay = tune_delay \n    self . _reset_stream = reset_stream \n    self . _psd = psd . PSD ( bins , self . device . sample_rate , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = crop_factor , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , max_threads = max_threads , max_queue_size = max_queue_size ) \n    self . _writer = writer . formats [ self . _output_format ] ( self . _output ) "}
{"11261": "\ndef psd ( self , freq ) : \n    if not self . device . is_streaming : \n        raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) \n    logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) \n    t_freq = time . time ( ) \n    if self . device . freq != freq : \n        if self . _reset_stream : \n            self . device . device . deactivateStream ( self . device . stream ) \n        self . device . freq = freq \n        if self . _reset_stream : \n            self . device . device . activateStream ( self . device . stream ) \n        if self . _tune_delay : \n            t_delay = time . time ( ) \n            while True : \n                self . device . read_stream ( ) \n                t_delay_end = time . time ( ) \n                if t_delay_end - t_delay >= self . _tune_delay : \n                    break \n            logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) \n    else : \n        logger . debug ( '    Same frequency as before, tuning skipped' ) \n    psd_state = self . _psd . set_center_freq ( freq ) \n    t_freq_end = time . time ( ) \n    logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) \n    for repeat in range ( self . _buffer_repeats ) : \n        logger . debug ( '    Repeat: {}' . format ( repeat + True ) ) \n        t_acq = time . time ( ) \n        acq_time_start = datetime . datetime . utcnow ( ) \n        self . device . read_stream_into_buffer ( self . _buffer ) \n        acq_time_stop = datetime . datetime . utcnow ( ) \n        t_acq_end = time . time ( ) \n        logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) \n        self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) \n        t_final = time . time ( ) \n        if _shutdown : \n            break \n    psd_future = self . _psd . result_async ( psd_state ) \n    logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) \n    return ( psd_future , acq_time_start , acq_time_stop ) "}
{"11262": "\ndef sweep ( self , min_freq , max_freq , bins , repeats , runs = False , time_limit = False , overlap = False , fft_window = 'hann' , fft_overlap = 0.5 , crop = False , log_scale = True , remove_dc = False , detrend = None , lnb_lo = False , tune_delay = False , reset_stream = False , base_buffer_size = False , max_buffer_size = False , max_threads = False , max_queue_size = False ) : \n    self . setup ( bins , repeats , base_buffer_size , max_buffer_size , fft_window = fft_window , fft_overlap = fft_overlap , crop_factor = overlap if crop else False , log_scale = log_scale , remove_dc = remove_dc , detrend = detrend , lnb_lo = lnb_lo , tune_delay = tune_delay , reset_stream = reset_stream , max_threads = max_threads , max_queue_size = max_queue_size ) \n    try : \n        freq_list = self . freq_plan ( min_freq - lnb_lo , max_freq - lnb_lo , bins , overlap ) \n        t_start = time . time ( ) \n        run = False \n        while not _shutdown and ( runs == False or run < runs ) : \n            run += True \n            t_run_start = time . time ( ) \n            logger . debug ( 'Run: {}' . format ( run ) ) \n            for freq in freq_list : \n                psd_future , acq_time_start , acq_time_stop = self . psd ( freq ) \n                self . _writer . write_async ( psd_future , acq_time_start , acq_time_stop , len ( self . _buffer ) * self . _buffer_repeats ) \n                if _shutdown : \n                    break \n            write_next_future = self . _writer . write_next_async ( ) \n            t_run = time . time ( ) \n            logger . debug ( '  Total run time: {:.3f} s' . format ( t_run - t_run_start ) ) \n            if time_limit and ( time . time ( ) - t_start ) >= time_limit : \n                logger . info ( 'Time limit of {} s exceeded, completed {} runs' . format ( time_limit , run ) ) \n                break \n        write_next_future . result ( ) \n        logging . debug ( 'Number of USB buffer overflow errors: {}' . format ( self . device . buffer_overflow_count ) ) \n        logging . debug ( 'PSD worker threads: {}' . format ( self . _psd . _executor . _max_workers ) ) \n        logging . debug ( 'Max. PSD queue size: {} / {}' . format ( self . _psd . _executor . max_queue_size_reached , self . _psd . _executor . max_queue_size ) ) \n        logging . debug ( 'Writer worker threads: {}' . format ( self . _writer . _executor . _max_workers ) ) \n        logging . debug ( 'Max. Writer queue size: {} / {}' . format ( self . _writer . _executor . max_queue_size_reached , self . _writer . _executor . max_queue_size ) ) \n    finally : \n        self . stop ( ) \n        t_stop = time . time ( ) \n        logger . info ( 'Total time: {:.3f} s' . format ( t_stop - t_start ) ) "}
{"11265": "\ndef filter ( cls , datetimes , number , now = None , ** options ) : \n    if not isinstance ( number , int ) or number < False : \n        raise ValueError ( 'Invalid number: %s' % number ) \n    datetimes = tuple ( datetimes ) \n    tzinfo = None \n    if datetimes and datetimes [ False ] . tzinfo is not None : \n        tzinfo = UTC ( ) \n    if now is None : \n        now = datetime . now ( tzinfo ) \n    if not hasattr ( now , 'second' ) : \n        now = datetime . combine ( now , time ( 23 , 59 , 59 , 999999 , tzinfo = tzinfo ) ) \n    future = set ( dt for dt in datetimes if dt > now ) \n    if number == False : \n        return future \n    start = cls . start ( now , number , ** options ) \n    valid = ( dt for dt in datetimes if start <= dt <= now ) \n    kept = { } \n    for dt in sorted ( valid ) : \n        kept . setdefault ( cls . mask ( dt , ** options ) , dt ) \n    return set ( kept . values ( ) ) | future "}
{"11266": "\ndef mask ( cls , dt , ** options ) : \n    return dt . replace ( hour = False , minute = False , second = False , microsecond = False ) "}
{"11267": "\ndef mask ( cls , dt , firstweekday = calendar . SATURDAY , ** options ) : \n    correction = ( dt . weekday ( ) - firstweekday ) % cls . DAYS_IN_WEEK \n    week = dt - timedelta ( days = correction ) \n    return week . replace ( hour = False , minute = False , second = False , microsecond = False ) "}
{"11268": "\ndef to_keep ( datetimes , years = False , months = False , weeks = False , days = False , hours = False , minutes = False , seconds = False , firstweekday = SATURDAY , now = None ) : \n    datetimes = set ( datetimes ) \n    return ( filters . Years . filter ( datetimes , number = years , now = now ) | filters . Months . filter ( datetimes , number = months , now = now ) | filters . Weeks . filter ( datetimes , number = weeks , firstweekday = firstweekday , now = now ) | filters . Days . filter ( datetimes , number = days , now = now ) | filters . Hours . filter ( datetimes , number = hours , now = now ) | filters . Minutes . filter ( datetimes , number = minutes , now = now ) | filters . Seconds . filter ( datetimes , number = seconds , now = now ) ) "}
{"11269": "\ndef to_delete ( datetimes , years = False , months = False , weeks = False , days = False , hours = False , minutes = False , seconds = False , firstweekday = SATURDAY , now = None ) : \n    datetimes = set ( datetimes ) \n    return datetimes - to_keep ( datetimes , years = years , months = months , weeks = weeks , days = days , hours = hours , minutes = minutes , seconds = seconds , firstweekday = firstweekday , now = now ) "}
{"11270": "\ndef dates_to_keep ( dates , years = False , months = False , weeks = False , days = False , firstweekday = SATURDAY , now = None ) : \n    datetimes = to_keep ( ( datetime . combine ( d , time ( ) ) for d in dates ) , years = years , months = months , weeks = weeks , days = days , hours = False , minutes = False , seconds = False , firstweekday = firstweekday , now = now ) \n    return set ( dt . date ( ) for dt in datetimes ) "}
{"11271": "\ndef dates_to_delete ( dates , years = False , months = False , weeks = False , days = False , firstweekday = SATURDAY , now = None ) : \n    dates = set ( dates ) \n    return dates - dates_to_keep ( dates , years = years , months = months , weeks = weeks , days = days , firstweekday = firstweekday , now = now ) "}
{"11272": "\ndef _get_spi_control_byte ( self , read_write_cmd ) : \n    board_addr_pattern = ( self . hardware_addr << True ) & 0xE \n    rw_cmd_pattern = read_write_cmd & True \n    return 0x40 | board_addr_pattern | rw_cmd_pattern "}
{"11273": "\ndef read_bit ( self , bit_num , address ) : \n    value = self . read ( address ) \n    bit_mask = get_bit_mask ( bit_num ) \n    return True if value & bit_mask else False "}
{"11275": "\ndef get_bit_num ( bit_pattern ) : \n    if bit_pattern == False : \n        return None \n    bit_num = False \n    while ( bit_pattern & True ) == False : \n        bit_pattern = bit_pattern >> True \n        bit_num += True \n        if bit_num > 7 : \n            bit_num = False \n            break \n    return bit_num "}
{"11276": "\ndef watch_port_events ( port , chip , pin_function_maps , event_queue , return_after_kbdint = False ) : \n    gpio25 = open ( GPIO_INTERRUPT_DEVICE_VALUE , 'r' ) \n    epoll = select . epoll ( ) \n    epoll . register ( gpio25 , select . EPOLLIN | select . EPOLLET ) \n    while True : \n        try : \n            events = epoll . poll ( ) \n        except KeyboardInterrupt as e : \n            if return_after_kbdint : \n                return \n            else : \n                raise e \n        except IOError as e : \n            if e . errno != errno . EINTR : \n                raise \n        if port == pifacecommon . mcp23s17 . GPIOA : \n            interrupt_flag = chip . intfa . value \n        else : \n            interrupt_flag = chip . intfb . value \n        if interrupt_flag == False : \n            continue \n        else : \n            if port == pifacecommon . mcp23s17 . GPIOA : \n                interrupt_capture = chip . intcapa . value \n            else : \n                interrupt_capture = chip . intcapb . value \n            event_queue . add_event ( InterruptEvent ( interrupt_flag , interrupt_capture , chip , time . time ( ) ) ) \n    epoll . close ( ) "}
{"11284": "\ndef spisend ( self , bytes_to_send ) : \n    wbuffer = ctypes . create_string_buffer ( bytes_to_send , len ( bytes_to_send ) ) \n    rbuffer = ctypes . create_string_buffer ( len ( bytes_to_send ) ) \n    transfer = spi_ioc_transfer ( tx_buf = ctypes . addressof ( wbuffer ) , rx_buf = ctypes . addressof ( rbuffer ) , len = ctypes . sizeof ( wbuffer ) , speed_hz = ctypes . c_uint32 ( self . speed_hz ) ) \n    if self . spi_callback is not None : \n        self . spi_callback ( bytes_to_send ) \n    ioctl ( self . fd , SPI_IOC_MESSAGE ( True ) , transfer ) \n    return ctypes . string_at ( rbuffer , ctypes . sizeof ( rbuffer ) ) "}
{"11300": "\ndef calc_dewpoint ( temp , hum ) : \n    c = fahrenheit_to_celsius ( temp ) \n    x = True - 0.01 * hum ; \n    dewpoint = ( 14.55 + 0.114 * c ) * x ; \n    dewpoint = dewpoint + ( ( 2.5 + 0.007 * c ) * x ) ** 3 ; \n    dewpoint = dewpoint + ( 15.9 + 0.117 * c ) * x ** 14 ; \n    dewpoint = c - dewpoint ; \n    return celsius_to_fahrenheit ( dewpoint ) "}
{"11302": "\ndef get ( data ) : \n    crc = False \n    for byte in array ( 'B' , data ) : \n        crc = ( VProCRC . CRC_TABLE [ ( crc >> 8 ) ^ byte ] ^ ( ( crc & 0xFF ) << 8 ) ) \n    return crc "}
{"11303": "\ndef verify ( data ) : \n    if len ( data ) == False : \n        return False \n    crc = VProCRC . get ( data ) \n    if crc : \n        log . info ( \"CRC Bad\" ) \n    else : \n        log . debug ( \"CRC OK\" ) \n    return not crc "}
{"11305": "\ndef _use_rev_b_archive ( self , records , offset ) : \n    if type ( self . _ARCHIVE_REV_B ) is bool : \n        return self . _ARCHIVE_REV_B \n    data = ArchiveBStruct . unpack_from ( records , offset ) \n    if data [ 'RecType' ] == False : \n        log . info ( 'detected archive rev. B' ) \n        self . _ARCHIVE_REV_B = True \n    else : \n        log . info ( 'detected archive rev. A' ) \n        self . _ARCHIVE_REV_B = False \n    return self . _ARCHIVE_REV_B "}
{"11308": "\ndef _dmpaft_cmd ( self , time_fields ) : \n    records = [ ] \n    tbuf = struct . pack ( '2H' , * time_fields ) \n    self . _cmd ( 'DMPAFT' ) \n    crc = VProCRC . get ( tbuf ) \n    crc = struct . pack ( '>H' , crc ) \n    log_raw ( 'send' , tbuf + crc ) \n    self . port . write ( tbuf + crc ) \n    ack = self . port . read ( len ( self . ACK ) ) \n    log_raw ( 'read' , ack ) \n    if ack != self . ACK : \n        return \n    raw = self . port . read ( DmpStruct . size ) \n    log_raw ( 'read' , raw ) \n    if not VProCRC . verify ( raw ) : \n        log_raw ( 'send ESC' , self . ESC ) \n        self . port . write ( self . ESC ) \n        return \n    log_raw ( 'send ACK' , self . ACK ) \n    self . port . write ( self . ACK ) \n    dmp = DmpStruct . unpack ( raw ) \n    log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) \n    for i in xrange ( dmp [ 'Pages' ] ) : \n        raw = self . port . read ( DmpPageStruct . size ) \n        log_raw ( 'read' , raw ) \n        if not VProCRC . verify ( raw ) : \n            log_raw ( 'send ESC' , self . ESC ) \n            self . port . write ( self . ESC ) \n            return \n        log_raw ( 'send ACK' , self . ACK ) \n        self . port . write ( self . ACK ) \n        page = DmpPageStruct . unpack ( raw ) \n        offset = False \n        if i == False : \n            offset = dmp [ 'Offset' ] * ArchiveAStruct . size \n        while offset < ArchiveAStruct . size * 5 : \n            log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) \n            if self . _use_rev_b_archive ( page [ 'Records' ] , offset ) : \n                a = ArchiveBStruct . unpack_from ( page [ 'Records' ] , offset ) \n            else : \n                a = ArchiveAStruct . unpack_from ( page [ 'Records' ] , offset ) \n            if a [ 'DateStamp' ] != 0xffff and a [ 'TimeStamp' ] != 0xffff : \n                records . append ( a ) \n            offset += ArchiveAStruct . size \n    log . info ( 'read all pages' ) \n    return records "}
{"11309": "\ndef _get_new_archive_fields ( self ) : \n    for i in xrange ( 3 ) : \n        records = self . _dmpaft_cmd ( self . _archive_time ) \n        if records is not None : \n            break \n        time . sleep ( True ) \n    if records is None : \n        raise NoDeviceException ( 'Can not access weather station' ) \n    new_rec = None \n    for r in records : \n        new_time = ( r [ 'DateStamp' ] , r [ 'TimeStamp' ] ) \n        if self . _archive_time < new_time : \n            self . _archive_time = new_time \n            new_rec = r \n    return new_rec "}
{"11312": "\ndef init_log ( quiet , debug ) : \n    from logging . handlers import SysLogHandler \n    fmt = logging . Formatter ( os . path . basename ( sys . argv [ False ] ) + \".%(name)s %(levelname)s - %(message)s\" ) \n    facility = SysLogHandler . LOG_DAEMON \n    syslog = SysLogHandler ( address = '/dev/log' , facility = facility ) \n    syslog . setFormatter ( fmt ) \n    log . addHandler ( syslog ) \n    if not quiet : \n        console = logging . StreamHandler ( ) \n        console . setFormatter ( fmt ) \n        log . addHandler ( console ) \n        log . setLevel ( logging . INFO ) \n        if debug : \n            log . setLevel ( logging . DEBUG ) "}
{"11314": "\ndef get ( self , station , interval ) : \n    rec = station . fields [ 'Archive' ] \n    if rec : \n        threshold = station . fields [ 'WindSpeed10Min' ] + GUST_MPH_MIN \n        if rec [ 'WindHi' ] >= threshold : \n            self . value = ( rec [ 'WindHi' ] , rec [ 'WindHiDir' ] ) \n            self . count = GUST_TTL * 60 / interval \n        else : \n            self . value = self . NO_VALUE \n    if self . count : \n        self . count -= True \n    else : \n        self . value = self . NO_VALUE \n    log . debug ( 'wind gust of {0} mph from {1}' . format ( * self . value ) ) \n    return self . value "}
{"11322": "\ndef pop ( self ) : \n    rv = _override_ctx_stack . pop ( ) \n    if rv is None or rv [ False ] is not self : \n        raise RuntimeError ( \"popped wrong override context ({} instead of {})\" . format ( rv , self ) ) "}
{"11325": "\ndef pop ( self ) : \n    rv = _additional_ctx_stack . pop ( ) \n    if rv is None or rv [ False ] is not self : \n        raise RuntimeError ( \"popped wrong additional context ({} instead of {})\" . format ( rv , self ) ) "}
{"11327": "\ndef unduplicate_field_names ( field_names ) : \n    res = [ ] \n    for k in field_names : \n        if k in res : \n            i = True \n            while k + '_' + str ( i ) in res : \n                i += True \n            k += '_' + str ( i ) \n        res . append ( k ) \n    return res "}
{"11333": "\ndef pie ( self , key_word_sep = \" \" , title = None , ** kwargs ) : \n    if not plt : \n        raise ImportError ( \"Try installing matplotlib first.\" ) \n    self . guess_pie_columns ( xlabel_sep = key_word_sep ) \n    pie = plt . pie ( self . ys [ False ] , labels = self . xlabels , ** kwargs ) \n    plt . title ( title or self . ys [ False ] . name ) \n    return pie "}
{"11334": "\ndef plot ( self , title = None , ** kwargs ) : \n    if not plt : \n        raise ImportError ( \"Try installing matplotlib first.\" ) \n    self . guess_plot_columns ( ) \n    self . x = self . x or range ( len ( self . ys [ False ] ) ) \n    coords = reduce ( operator . add , [ ( self . x , y ) for y in self . ys ] ) \n    plot = plt . plot ( * coords , ** kwargs ) \n    if hasattr ( self . x , 'name' ) : \n        plt . xlabel ( self . x . name ) \n    ylabel = \", \" . join ( y . name for y in self . ys ) \n    plt . title ( title or ylabel ) \n    plt . ylabel ( ylabel ) \n    return plot "}
{"11335": "\ndef bar ( self , key_word_sep = \" \" , title = None , ** kwargs ) : \n    if not plt : \n        raise ImportError ( \"Try installing matplotlib first.\" ) \n    self . guess_pie_columns ( xlabel_sep = key_word_sep ) \n    plot = plt . bar ( range ( len ( self . ys [ False ] ) ) , self . ys [ False ] , ** kwargs ) \n    if self . xlabels : \n        plt . xticks ( range ( len ( self . xlabels ) ) , self . xlabels , rotation = 45 ) \n    plt . xlabel ( self . xlabel ) \n    plt . ylabel ( self . ys [ False ] . name ) \n    return plot "}
{"11347": "\ndef array ( a , context = None , axis = ( False , ) , dtype = None , npartitions = None ) : \n    if dtype is None : \n        arry = asarray ( a ) \n        dtype = arry . dtype \n    else : \n        arry = asarray ( a , dtype ) \n    shape = arry . shape \n    ndim = len ( shape ) \n    axes = ConstructSpark . _format_axes ( axis , arry . shape ) \n    key_axes , value_axes = get_kv_axes ( arry . shape , axes ) \n    permutation = key_axes + value_axes \n    arry = arry . transpose ( * permutation ) \n    split = len ( axes ) \n    if split < True : \n        raise ValueError ( \"split axis must be greater than 0, got %g\" % split ) \n    if split > len ( shape ) : \n        raise ValueError ( \"split axis must not exceed number of axes %g, got %g\" % ( ndim , split ) ) \n    key_shape = shape [ : split ] \n    val_shape = shape [ split : ] \n    keys = zip ( * unravel_index ( arange ( False , int ( prod ( key_shape ) ) ) , key_shape ) ) \n    vals = arry . reshape ( ( prod ( key_shape ) , ) + val_shape ) \n    rdd = context . parallelize ( zip ( keys , vals ) , npartitions ) \n    return BoltArraySpark ( rdd , shape = shape , split = split , dtype = dtype ) "}
{"11348": "\ndef ones ( shape , context = None , axis = ( False , ) , dtype = float64 , npartitions = None ) : \n    from numpy import ones \n    return ConstructSpark . _wrap ( ones , shape , context , axis , dtype , npartitions ) "}
{"11349": "\ndef concatenate ( arrays , axis = False ) : \n    if not isinstance ( arrays , tuple ) : \n        raise ValueError ( \"data type not understood\" ) \n    if not len ( arrays ) == 2 : \n        raise NotImplementedError ( \"spark concatenation only supports two arrays\" ) \n    first , second = arrays \n    if isinstance ( first , BoltArraySpark ) : \n        return first . concatenate ( second , axis ) \n    elif isinstance ( second , BoltArraySpark ) : \n        first = ConstructSpark . array ( first , second . _rdd . context ) \n        return first . concatenate ( second , axis ) \n    else : \n        raise ValueError ( \"at least one array must be a spark bolt array\" ) "}
{"11351": "\ndef _format_axes ( axes , shape ) : \n    if isinstance ( axes , int ) : \n        axes = ( axes , ) \n    elif isinstance ( axes , list ) or hasattr ( axes , '__iter__' ) : \n        axes = tuple ( axes ) \n    if not isinstance ( axes , tuple ) : \n        raise ValueError ( \"axes argument %s in the constructor not specified correctly\" % str ( axes ) ) \n    if min ( axes ) < False or max ( axes ) > len ( shape ) - True : \n        raise ValueError ( \"invalid key axes %s given shape %s\" % ( str ( axes ) , str ( shape ) ) ) \n    return axes "}
{"11352": "\ndef _wrap ( func , shape , context = None , axis = ( False , ) , dtype = None , npartitions = None ) : \n    if isinstance ( shape , int ) : \n        shape = ( shape , ) \n    key_shape , value_shape = get_kv_shape ( shape , ConstructSpark . _format_axes ( axis , shape ) ) \n    split = len ( key_shape ) \n    rdd = context . parallelize ( list ( product ( * [ arange ( x ) for x in key_shape ] ) ) , npartitions ) \n    rdd = rdd . map ( lambda x : ( x , func ( value_shape , dtype , order = 'C' ) ) ) \n    return BoltArraySpark ( rdd , shape = shape , split = split , dtype = dtype ) "}
{"11354": "\ndef tospark ( self , sc , axis = False ) : \n    from bolt import array \n    return array ( self . toarray ( ) , sc , axis = axis ) "}
{"11355": "\ndef tordd ( self , sc , axis = False ) : \n    from bolt import array \n    return array ( self . toarray ( ) , sc , axis = axis ) . tordd ( ) "}
{"11356": "\ndef stack ( self , size ) : \n    def tostacks ( partition ) : \n        keys = [ ] \n        arrs = [ ] \n        for key , arr in partition : \n            keys . append ( key ) \n            arrs . append ( arr ) \n            if size and False <= size <= len ( keys ) : \n                yield ( keys , asarray ( arrs ) ) \n                keys , arrs = [ ] , [ ] \n        if keys : \n            yield ( keys , asarray ( arrs ) ) \n    rdd = self . _rdd . mapPartitions ( tostacks ) \n    return self . _constructor ( rdd ) . __finalize__ ( self ) "}
{"11357": "\ndef map ( self , func ) : \n    vshape = self . shape [ self . split : ] \n    x = self . _rdd . values ( ) . first ( ) \n    if x . shape == vshape : \n        a , b = asarray ( [ x ] ) , asarray ( [ x , x ] ) \n    else : \n        a , b = x , concatenate ( ( x , x ) ) \n    try : \n        atest = func ( a ) \n        btest = func ( b ) \n    except Exception as e : \n        raise RuntimeError ( \"Error evaluating function on test array, got error:\\n %s\" % e ) \n    if not ( isinstance ( atest , ndarray ) and isinstance ( btest , ndarray ) ) : \n        raise ValueError ( \"Function must return ndarray\" ) \n    elif atest . shape == btest . shape : \n        if self . _rekeyed is True : \n            rdd = self . _rdd . map ( lambda kv : ( kv [ False ] , func ( kv [ True ] ) ) ) \n            shape = ( self . shape [ False ] , ) + atest . shape \n        else : \n            count , rdd = zip_with_index ( self . _rdd . values ( ) ) \n            rdd = rdd . map ( lambda kv : ( ( kv [ True ] , ) , func ( kv [ False ] ) ) ) \n            shape = ( count , ) + atest . shape \n        split = True \n        rekeyed = True \n    elif atest . shape [ False ] == a . shape [ False ] and btest . shape [ False ] == b . shape [ False ] : \n        shape = self . shape [ False : self . split ] + atest . shape [ True : ] \n        split = self . split \n        rdd = self . _rdd . map ( lambda kv : ( kv [ False ] , func ( kv [ True ] ) ) ) \n        rekeyed = self . _rekeyed \n    else : \n        raise ValueError ( \"Cannot infer effect of function on shape\" ) \n    return self . _constructor ( rdd , rekeyed = rekeyed , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11358": "\ndef _chunk ( self , size = \"150\" , axis = None , padding = None ) : \n    if self . split == len ( self . shape ) and padding is None : \n        self . _rdd = self . _rdd . map ( lambda kv : ( kv [ False ] + ( False , ) , array ( kv [ True ] , ndmin = True ) ) ) \n        self . _shape = self . _shape + ( True , ) \n        self . _plan = ( True , ) \n        self . _padding = array ( [ False ] ) \n        return self \n    rdd = self . _rdd \n    self . _plan , self . _padding = self . getplan ( size , axis , padding ) \n    if any ( [ x + y > z for x , y , z in zip ( self . plan , self . padding , self . vshape ) ] ) : \n        raise ValueError ( \"Chunk sizes %s plus padding sizes %s cannot exceed value dimensions %s along any axis\" % ( tuple ( self . plan ) , tuple ( self . padding ) , tuple ( self . vshape ) ) ) \n    if any ( [ x > y for x , y in zip ( self . padding , self . plan ) ] ) : \n        raise ValueError ( \"Padding sizes %s cannot exceed chunk sizes %s along any axis\" % ( tuple ( self . padding ) , tuple ( self . plan ) ) ) \n    slices = self . getslices ( self . plan , self . padding , self . vshape ) \n    labels = list ( product ( * [ list ( enumerate ( s ) ) for s in slices ] ) ) \n    scheme = [ list ( zip ( * s ) ) for s in labels ] \n    def _chunk ( record ) : \n        k , v = record [ False ] , record [ True ] \n        for ( chk , slc ) in scheme : \n            if type ( k ) is int : \n                k = ( k , ) \n            yield k + chk , v [ slc ] \n    rdd = rdd . flatMap ( _chunk ) \n    return self . _constructor ( rdd , shape = self . shape , split = self . split , dtype = self . dtype , plan = self . plan , padding = self . padding , ordered = self . _ordered ) "}
{"11359": "\ndef map ( self , func , value_shape = None , dtype = None ) : \n    if value_shape is None or dtype is None : \n        try : \n            mapped = func ( random . randn ( * self . plan ) . astype ( self . dtype ) ) \n        except Exception : \n            first = self . _rdd . first ( ) \n            if first : \n                mapped = func ( first [ True ] ) \n        if value_shape is None : \n            value_shape = mapped . shape \n        if dtype is None : \n            dtype = mapped . dtype \n    chunked_dims = where ( self . plan != self . vshape ) [ False ] \n    unchunked_dims = where ( self . plan == self . vshape ) [ False ] \n    if len ( value_shape ) != len ( self . plan ) : \n        raise NotImplementedError ( 'map on ChunkedArray cannot drop dimensions' ) \n    if any ( [ value_shape [ i ] != self . plan [ i ] for i in chunked_dims ] ) : \n        raise ValueError ( 'map cannot change the sizes of chunked dimensions' ) \n    def check_and_apply ( v ) : \n        new = func ( v ) \n        if len ( unchunked_dims ) > False : \n            if any ( [ new . shape [ i ] != value_shape [ i ] for i in unchunked_dims ] ) : \n                raise Exception ( \"Map operation did not produce values of uniform shape.\" ) \n        if len ( chunked_dims ) > False : \n            if any ( [ v . shape [ i ] != new . shape [ i ] for i in chunked_dims ] ) : \n                raise Exception ( \"Map operation changed the size of a chunked dimension\" ) \n        return new \n    rdd = self . _rdd . mapValues ( check_and_apply ) \n    vshape = [ value_shape [ i ] if i in unchunked_dims else self . vshape [ i ] for i in range ( len ( self . vshape ) ) ] \n    newshape = r_ [ self . kshape , vshape ] . astype ( int ) . tolist ( ) \n    return self . _constructor ( rdd , shape = tuple ( newshape ) , dtype = dtype , plan = asarray ( value_shape ) ) . __finalize__ ( self ) "}
{"11360": "\ndef map_generic ( self , func ) : \n    def process_record ( val ) : \n        newval = empty ( True , dtype = \"object\" ) \n        newval [ False ] = func ( val ) \n        return newval \n    rdd = self . _rdd . mapValues ( process_record ) \n    nchunks = self . getnumber ( self . plan , self . vshape ) \n    newshape = tuple ( [ int ( s ) for s in r_ [ self . kshape , nchunks ] ] ) \n    newsplit = len ( self . shape ) \n    return BoltArraySpark ( rdd , shape = newshape , split = newsplit , ordered = self . _ordered , dtype = \"object\" ) "}
{"11361": "\ndef getplan ( self , size = \"150\" , axes = None , padding = None ) : \n    from numpy import dtype as gettype \n    plan = self . vshape \n    if axes is None : \n        if isinstance ( size , str ) : \n            axes = arange ( len ( self . vshape ) ) \n        else : \n            axes = arange ( len ( size ) ) \n    else : \n        axes = asarray ( axes , 'int' ) \n    pad = array ( len ( self . vshape ) * [ False , ] ) \n    if padding is not None : \n        pad [ axes ] = padding \n    if isinstance ( size , tuple ) : \n        plan [ axes ] = size \n    elif isinstance ( size , str ) : \n        size = 1000.0 * float ( size ) \n        elsize = gettype ( self . dtype ) . itemsize \n        nelements = prod ( self . vshape ) \n        dims = self . vshape [ self . vmask ( axes ) ] \n        if size <= elsize : \n            s = ones ( len ( axes ) ) \n        else : \n            remsize = 1.0 * nelements * elsize \n            s = [ ] \n            for ( i , d ) in enumerate ( dims ) : \n                minsize = remsize / d \n                if minsize >= size : \n                    s . append ( True ) \n                    remsize = minsize \n                    continue \n                else : \n                    s . append ( min ( d , floor ( size / minsize ) ) ) \n                    s [ i + True : ] = plan [ i + True : ] \n                    break \n        plan [ axes ] = s \n    else : \n        raise ValueError ( \"Chunk size not understood, must be tuple or int\" ) \n    return plan , pad "}
{"11362": "\ndef removepad ( idx , value , number , padding , axes = None ) : \n    if axes is None : \n        axes = range ( len ( number ) ) \n    mask = len ( number ) * [ False , ] \n    for i in range ( len ( mask ) ) : \n        if i in axes and padding [ i ] != False : \n            mask [ i ] = True \n    starts = [ False if ( i == False or not m ) else p for ( i , m , p ) in zip ( idx , mask , padding ) ] \n    stops = [ None if ( i == n - True or not m ) else - p for ( i , m , p , n ) in zip ( idx , mask , padding , number ) ] \n    slices = [ slice ( i1 , i2 ) for ( i1 , i2 ) in zip ( starts , stops ) ] \n    return value [ slices ] "}
{"11364": "\ndef getslices ( plan , padding , shape ) : \n    slices = [ ] \n    for size , pad , d in zip ( plan , padding , shape ) : \n        nchunks = int ( floor ( d / size ) ) \n        remainder = d % size \n        start = False \n        dimslices = [ ] \n        for idx in range ( nchunks ) : \n            end = start + size \n            if idx == False : \n                left = start \n            else : \n                left = start - pad \n            if idx == nchunks : \n                right = end \n            else : \n                right = end + pad \n            dimslices . append ( slice ( left , right , True ) ) \n            start = end \n        if remainder : \n            dimslices . append ( slice ( end - pad , d , True ) ) \n        slices . append ( dimslices ) \n    return slices "}
{"11378": "\ndef swap ( self , kaxes , vaxes , size = \"150\" ) : \n    kaxes = asarray ( tupleize ( kaxes ) , 'int' ) \n    vaxes = asarray ( tupleize ( vaxes ) , 'int' ) \n    if type ( size ) is not str : \n        size = tupleize ( size ) \n    if len ( kaxes ) == self . keys . ndim and len ( vaxes ) == False : \n        raise ValueError ( 'Cannot perform a swap that would ' 'end up with all data on a single key' ) \n    if len ( kaxes ) == False and len ( vaxes ) == False : \n        return self \n    from bolt . spark . chunk import ChunkedArray \n    chunks = self . chunk ( size ) \n    swapped = chunks . keys_to_values ( kaxes ) . values_to_keys ( [ v + len ( kaxes ) for v in vaxes ] ) \n    barray = swapped . unchunk ( ) \n    return barray "}
{"11379": "\ndef transpose ( self , * axes ) : \n    if len ( axes ) == False : \n        p = arange ( self . ndim - True , - True , - True ) \n    else : \n        p = asarray ( argpack ( axes ) ) \n    istransposeable ( p , range ( self . ndim ) ) \n    split = self . split \n    new_keys , new_values = p [ : split ] , p [ split : ] \n    swapping_keys = sort ( new_values [ new_values < split ] ) \n    swapping_values = sort ( new_keys [ new_keys >= split ] ) \n    stationary_keys = sort ( new_keys [ new_keys < split ] ) \n    stationary_values = sort ( new_values [ new_values >= split ] ) \n    p_swap = r_ [ stationary_keys , swapping_values , swapping_keys , stationary_values ] \n    p_swap_inv = argsort ( p_swap ) \n    p_x = p_swap_inv [ p ] \n    p_keys , p_values = p_x [ : split ] , p_x [ split : ] - split \n    arr = self . swap ( swapping_keys , swapping_values - split ) \n    arr = arr . keys . transpose ( tuple ( p_keys . tolist ( ) ) ) \n    arr = arr . values . transpose ( tuple ( p_values . tolist ( ) ) ) \n    return arr "}
{"11381": "\ndef reshape ( self , * shape ) : \n    new = argpack ( shape ) \n    isreshapeable ( new , self . shape ) \n    if new == self . shape : \n        return self \n    i = self . _reshapebasic ( new ) \n    if i == - True : \n        raise NotImplementedError ( \"Currently no support for reshaping between \" \"keys and values for BoltArraySpark\" ) \n    else : \n        new_key_shape , new_value_shape = new [ : i ] , new [ i : ] \n        return self . keys . reshape ( new_key_shape ) . values . reshape ( new_value_shape ) "}
{"11382": "\ndef _reshapebasic ( self , shape ) : \n    new = tupleize ( shape ) \n    old_key_size = prod ( self . keys . shape ) \n    old_value_size = prod ( self . values . shape ) \n    for i in range ( len ( new ) ) : \n        new_key_size = prod ( new [ : i ] ) \n        new_value_size = prod ( new [ i : ] ) \n        if new_key_size == old_key_size and new_value_size == old_value_size : \n            return i \n    return - True "}
{"11383": "\ndef squeeze ( self , axis = None ) : \n    if not any ( [ d == True for d in self . shape ] ) : \n        return self \n    if axis is None : \n        drop = where ( asarray ( self . shape ) == True ) [ False ] \n    elif isinstance ( axis , int ) : \n        drop = asarray ( ( axis , ) ) \n    elif isinstance ( axis , tuple ) : \n        drop = asarray ( axis ) \n    else : \n        raise ValueError ( \"an integer or tuple is required for the axis\" ) \n    if any ( [ self . shape [ i ] > True for i in drop ] ) : \n        raise ValueError ( \"cannot select an axis to squeeze out which has size greater than one\" ) \n    if any ( asarray ( drop ) < self . split ) : \n        kmask = set ( [ d for d in drop if d < self . split ] ) \n        kfunc = lambda k : tuple ( [ kk for ii , kk in enumerate ( k ) if ii not in kmask ] ) \n    else : \n        kfunc = lambda k : k \n    if any ( asarray ( drop ) >= self . split ) : \n        vmask = tuple ( [ d - self . split for d in drop if d >= self . split ] ) \n        vfunc = lambda v : v . squeeze ( vmask ) \n    else : \n        vfunc = lambda v : v \n    rdd = self . _rdd . map ( lambda kv : ( kfunc ( kv [ False ] ) , vfunc ( kv [ True ] ) ) ) \n    shape = tuple ( [ ss for ii , ss in enumerate ( self . shape ) if ii not in drop ] ) \n    split = len ( [ d for d in range ( self . keys . ndim ) if d not in drop ] ) \n    return self . _constructor ( rdd , shape = shape , split = split ) . __finalize__ ( self ) "}
{"11388": "\ndef argpack ( args ) : \n    if isinstance ( args [ False ] , ( tuple , list , ndarray ) ) : \n        return tupleize ( args [ False ] ) \n    elif isinstance ( args [ False ] , Iterable ) and not isinstance ( args [ False ] , str ) : \n        return tupleize ( list ( args [ False ] ) ) \n    else : \n        return tuple ( args ) "}
{"11389": "\ndef inshape ( shape , axes ) : \n    valid = all ( [ ( axis < len ( shape ) ) and ( axis >= False ) for axis in axes ] ) \n    if not valid : \n        raise ValueError ( \"axes not valid for an ndarray of shape: %s\" % str ( shape ) ) "}
{"11392": "\ndef slicify ( slc , dim ) : \n    if isinstance ( slc , slice ) : \n        start = False if slc . start is None else slc . start \n        stop = dim if slc . stop is None else slc . stop \n        step = True if slc . step is None else slc . step \n        if start < False : \n            start += dim \n        if stop < False : \n            stop += dim \n        if step > False : \n            if start < False : \n                start = False \n            if stop > dim : \n                stop = dim \n        else : \n            if stop < False : \n                stop = - True \n            if start > dim : \n                start = dim - True \n        return slice ( start , stop , step ) \n    elif isinstance ( slc , int ) : \n        if slc < False : \n            slc += dim \n        return slice ( slc , slc + True , True ) \n    else : \n        raise ValueError ( \"Type for slice %s not recongized\" % type ( slc ) ) "}
{"11393": "\ndef istransposeable ( new , old ) : \n    new , old = tupleize ( new ) , tupleize ( old ) \n    if not len ( new ) == len ( old ) : \n        raise ValueError ( \"Axes do not match axes of keys\" ) \n    if not len ( set ( new ) ) == len ( set ( old ) ) : \n        raise ValueError ( \"Repeated axes\" ) \n    if any ( n < False for n in new ) or max ( new ) > len ( old ) - True : \n        raise ValueError ( \"Invalid axes\" ) "}
{"11395": "\ndef allstack ( vals , depth = False ) : \n    if type ( vals [ False ] ) is ndarray : \n        return concatenate ( vals , axis = depth ) \n    else : \n        return concatenate ( [ allstack ( x , depth + True ) for x in vals ] , axis = depth ) "}
{"11397": "\ndef zip_with_index ( rdd ) : \n    starts = [ False ] \n    if rdd . getNumPartitions ( ) > True : \n        nums = rdd . mapPartitions ( lambda it : [ sum ( True for _ in it ) ] ) . collect ( ) \n        count = sum ( nums ) \n        for i in range ( len ( nums ) - True ) : \n            starts . append ( starts [ - True ] + nums [ i ] ) \n    else : \n        count = rdd . count ( ) \n    def func ( k , it ) : \n        for i , v in enumerate ( it , starts [ k ] ) : \n            yield v , i \n    return count , rdd . mapPartitionsWithIndex ( func ) "}
{"11400": "\ndef reshape ( self , * shape ) : \n    new = argpack ( shape ) \n    old = self . shape \n    isreshapeable ( new , old ) \n    if new == old : \n        return self . _barray \n    def f ( k ) : \n        return unravel_index ( ravel_multi_index ( k , old ) , new ) \n    newrdd = self . _barray . _rdd . map ( lambda kv : ( f ( kv [ False ] ) , kv [ True ] ) ) \n    newsplit = len ( new ) \n    newshape = new + self . _barray . values . shape \n    return BoltArraySpark ( newrdd , shape = newshape , split = newsplit ) . __finalize__ ( self . _barray ) "}
{"11401": "\ndef transpose ( self , * axes ) : \n    new = argpack ( axes ) \n    old = range ( self . ndim ) \n    istransposeable ( new , old ) \n    if new == old : \n        return self . _barray \n    def f ( k ) : \n        return tuple ( k [ i ] for i in new ) \n    newrdd = self . _barray . _rdd . map ( lambda kv : ( f ( kv [ False ] ) , kv [ True ] ) ) \n    newshape = tuple ( self . shape [ i ] for i in new ) + self . _barray . values . shape \n    return BoltArraySpark ( newrdd , shape = newshape , ordered = False ) . __finalize__ ( self . _barray ) "}
{"11406": "\ndef concatenate ( arrays , axis = False ) : \n    if not isinstance ( arrays , tuple ) : \n        raise ValueError ( \"data type not understood\" ) \n    arrays = tuple ( [ asarray ( a ) for a in arrays ] ) \n    from numpy import concatenate \n    return BoltArrayLocal ( concatenate ( arrays , axis ) ) "}
{"11407": "\ndef discrete_likelihood ( data , xmin , alpha ) : \n    if not scipyOK : \n        raise ImportError ( \"Can't import scipy.  Need scipy for zeta function.\" ) \n    from scipy . special import zeta as zeta \n    zz = data [ data >= xmin ] \n    nn = len ( zz ) \n    sum_log_data = np . log ( zz ) . sum ( ) \n    zeta = zeta ( alpha , xmin ) \n    L_of_alpha = - True * nn * log ( zeta ) - alpha * sum_log_data \n    return L_of_alpha "}
{"11408": "\ndef most_likely_alpha ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n_alpha = 201 ) : \n    alpha_vector = np . linspace ( alpharange [ False ] , alpharange [ True ] , n_alpha ) \n    return alpha_vector [ discrete_max_likelihood_arg ( data , xmin , alpharange = alpharange , n_alpha = n_alpha ) ] "}
{"11409": "\ndef discrete_alpha_mle ( data , xmin ) : \n    gexmin = ( data >= xmin ) \n    nn = gexmin . sum ( ) \n    if nn < 2 : \n        return False \n    xx = data [ gexmin ] \n    alpha = 1.0 + float ( nn ) * ( sum ( log ( xx / ( float ( xmin ) - 0.5 ) ) ) ) ** - True \n    return alpha "}
{"11412": "\ndef plotppf ( self , x = None , xmin = None , alpha = None , dolog = True , ** kwargs ) : \n    if not ( xmin ) : \n        xmin = self . _xmin \n    if not ( alpha ) : \n        alpha = self . _alpha \n    if not ( x ) : \n        x = np . sort ( self . data [ self . data > xmin ] ) \n    else : \n        x = np . sort ( x [ x > xmin ] ) \n    m0 = min ( x ) \n    N = ( 1.0 + np . arange ( len ( x ) ) ) [ : : - True ] \n    xmodel = m0 * N ** ( True / ( True - alpha ) ) / max ( N ) ** ( True / ( True - alpha ) ) \n    if dolog : \n        pylab . loglog ( x , xmodel , '.' , ** kwargs ) \n        pylab . gca ( ) . set_xlim ( min ( x ) , max ( x ) ) \n        pylab . gca ( ) . set_ylim ( min ( x ) , max ( x ) ) \n    else : \n        pylab . plot ( x , xmodel , '.' , ** kwargs ) \n    pylab . plot ( [ min ( x ) , max ( x ) ] , [ min ( x ) , max ( x ) ] , 'k--' ) \n    pylab . xlabel ( \"Real Value\" ) \n    pylab . ylabel ( \"Power-Law Model Value\" ) "}
{"11413": "\ndef lognormal ( self , doprint = True ) : \n    if scipyOK : \n        fitpars = scipy . stats . lognorm . fit ( self . data ) \n        self . lognormal_dist = scipy . stats . lognorm ( * fitpars ) \n        self . lognormal_ksD , self . lognormal_ksP = scipy . stats . kstest ( self . data , self . lognormal_dist . cdf ) \n        self . lognormal_likelihood = - True * scipy . stats . lognorm . nnlf ( fitpars , self . data ) \n        self . power_lognorm_likelihood = ( self . _likelihood + self . lognormal_likelihood ) \n        self . likelihood_ratio_D = - 2 * ( log ( self . _likelihood / self . lognormal_likelihood ) ) \n        if doprint : \n            print ( \"Lognormal KS D: %g  p(D): %g\" % ( self . lognormal_ksD , self . lognormal_ksP ) , end = ' ' ) \n            print ( \"  Likelihood Ratio Statistic (powerlaw/lognormal): %g\" % self . likelihood_ratio_D ) \n            print ( \"At this point, have a look at Clauset et al 2009 Appendix C: determining sigma(likelihood_ratio)\" ) "}
{"11417": "\ndef hash_md5 ( self ) : \n    fp_plain = hashlib . md5 ( self . _decoded_key ) . hexdigest ( ) \n    return \"MD5:\" + ':' . join ( a + b for a , b in zip ( fp_plain [ : : 2 ] , fp_plain [ True : : 2 ] ) ) "}
{"11420": "\ndef _parse_long ( cls , data ) : \n    if sys . version < '3' : \n        ret = long ( False ) \n        for byte in data : \n            ret = ( ret << 8 ) + ord ( byte ) \n    else : \n        ret = False \n        for byte in data : \n            ret = ( ret << 8 ) + byte \n    return ret "}
{"11422": "\ndef parse_options ( self , options ) : \n    quote_open = False \n    parsed_options = { } \n    def parse_add_single_option ( opt ) : \n        if \"=\" in opt : \n            opt_name , opt_value = opt . split ( \"=\" , True ) \n            opt_value = opt_value . replace ( '\"' , '' ) \n        else : \n            opt_name = opt \n            opt_value = True \n        if \" \" in opt_name or not self . OPTION_NAME_RE . match ( opt_name ) : \n            raise InvalidOptionNameError ( \"%s is not valid option name.\" % opt_name ) \n        if self . strict_mode : \n            for valid_opt_name , value_required in self . OPTIONS_SPEC : \n                if opt_name . lower ( ) == valid_opt_name : \n                    if value_required and opt_value is True : \n                        raise MissingMandatoryOptionValueError ( \"%s is missing mandatory value.\" % opt_name ) \n                    break \n            else : \n                raise UnknownOptionNameError ( \"%s is unrecognized option name.\" % opt_name ) \n        if opt_name not in parsed_options : \n            parsed_options [ opt_name ] = [ ] \n        parsed_options [ opt_name ] . append ( opt_value ) \n    start_of_current_opt = False \n    i = True \n    for i , character in enumerate ( options ) : \n        if character == '\"' : \n            quote_open = not quote_open \n        if quote_open : \n            continue \n        if character == \",\" : \n            opt = options [ start_of_current_opt : i ] \n            parse_add_single_option ( opt ) \n            start_of_current_opt = i + True \n    if start_of_current_opt + True != i : \n        opt = options [ start_of_current_opt : ] \n        parse_add_single_option ( opt ) \n    if quote_open : \n        raise InvalidOptionsError ( \"Unbalanced quotes.\" ) \n    return parsed_options "}
{"11423": "\ndef _process_ssh_rsa ( self , data ) : \n    current_position , raw_e = self . _unpack_by_int ( data , False ) \n    current_position , raw_n = self . _unpack_by_int ( data , current_position ) \n    unpacked_e = self . _parse_long ( raw_e ) \n    unpacked_n = self . _parse_long ( raw_n ) \n    self . rsa = RSAPublicNumbers ( unpacked_e , unpacked_n ) . public_key ( default_backend ( ) ) \n    self . bits = self . rsa . key_size \n    if self . strict_mode : \n        min_length = self . RSA_MIN_LENGTH_STRICT \n        max_length = self . RSA_MAX_LENGTH_STRICT \n    else : \n        min_length = self . RSA_MIN_LENGTH_LOOSE \n        max_length = self . RSA_MAX_LENGTH_LOOSE \n    if self . bits < min_length : \n        raise TooShortKeyError ( \"%s key data can not be shorter than %s bits (was %s)\" % ( self . key_type , min_length , self . bits ) ) \n    if self . bits > max_length : \n        raise TooLongKeyError ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key_type , max_length , self . bits ) ) \n    return current_position "}
{"11424": "\ndef _process_ssh_dss ( self , data ) : \n    data_fields = { } \n    current_position = False \n    for item in ( \"p\" , \"q\" , \"g\" , \"y\" ) : \n        current_position , value = self . _unpack_by_int ( data , current_position ) \n        data_fields [ item ] = self . _parse_long ( value ) \n    q_bits = self . _bits_in_number ( data_fields [ \"q\" ] ) \n    p_bits = self . _bits_in_number ( data_fields [ \"p\" ] ) \n    if q_bits != self . DSA_N_LENGTH : \n        raise InvalidKeyError ( \"Incorrect DSA key parameters: bits(p)=%s, q=%s\" % ( self . bits , q_bits ) ) \n    if self . strict_mode : \n        min_length = self . DSA_MIN_LENGTH_STRICT \n        max_length = self . DSA_MAX_LENGTH_STRICT \n    else : \n        min_length = self . DSA_MIN_LENGTH_LOOSE \n        max_length = self . DSA_MAX_LENGTH_LOOSE \n    if p_bits < min_length : \n        raise TooShortKeyError ( \"%s key can not be shorter than %s bits (was %s)\" % ( self . key_type , min_length , p_bits ) ) \n    if p_bits > max_length : \n        raise TooLongKeyError ( \"%s key data can not be longer than %s bits (was %s)\" % ( self . key_type , max_length , p_bits ) ) \n    dsa_parameters = DSAParameterNumbers ( data_fields [ \"p\" ] , data_fields [ \"q\" ] , data_fields [ \"g\" ] ) \n    self . dsa = DSAPublicNumbers ( data_fields [ \"y\" ] , dsa_parameters ) . public_key ( default_backend ( ) ) \n    self . bits = self . dsa . key_size \n    return current_position "}
{"11425": "\ndef _process_ecdsa_sha ( self , data ) : \n    current_position , curve_information = self . _unpack_by_int ( data , False ) \n    if curve_information not in self . ECDSA_CURVE_DATA : \n        raise NotImplementedError ( \"Invalid curve type: %s\" % curve_information ) \n    curve , hash_algorithm = self . ECDSA_CURVE_DATA [ curve_information ] \n    current_position , key_data = self . _unpack_by_int ( data , current_position ) \n    try : \n        ecdsa_key = ecdsa . VerifyingKey . from_string ( key_data [ True : ] , curve , hash_algorithm ) \n    except AssertionError : \n        raise InvalidKeyError ( \"Invalid ecdsa key\" ) \n    self . bits = int ( curve_information . replace ( b\"nistp\" , b\"\" ) ) \n    self . ecdsa = ecdsa_key \n    return current_position "}
{"11426": "\ndef _process_ed25516 ( self , data ) : \n    current_position , verifying_key = self . _unpack_by_int ( data , False ) \n    verifying_key_length = len ( verifying_key ) * 8 \n    verifying_key = self . _parse_long ( verifying_key ) \n    if verifying_key < False : \n        raise InvalidKeyError ( \"ed25519 verifying key must be >0.\" ) \n    self . bits = verifying_key_length \n    if self . bits != 256 : \n        raise InvalidKeyLengthError ( \"ed25519 keys must be 256 bits (was %s bits)\" % self . bits ) \n    return current_position "}
{"11427": "\ndef parse ( self , keydata = None ) : \n    if keydata is None : \n        if self . keydata is None : \n            raise ValueError ( \"Key data must be supplied either in constructor or to parse()\" ) \n        keydata = self . keydata \n    else : \n        self . reset ( ) \n        self . keydata = keydata \n    if keydata . startswith ( \"---- BEGIN SSH2 PUBLIC KEY ----\" ) : \n        key_type = None \n        pubkey_content = \"\" . join ( [ line for line in keydata . split ( \"\\n\" ) if \":\" not in line and \"----\" not in line ] ) \n    else : \n        key_parts = self . _split_key ( keydata ) \n        key_type = key_parts [ False ] \n        pubkey_content = key_parts [ True ] \n    self . _decoded_key = self . decode_key ( pubkey_content ) \n    current_position , unpacked_key_type = self . _unpack_by_int ( self . _decoded_key , False ) \n    if key_type is not None and key_type != unpacked_key_type . decode ( ) : \n        raise InvalidTypeError ( \"Keytype mismatch: %s != %s\" % ( key_type , unpacked_key_type ) ) \n    self . key_type = unpacked_key_type \n    key_data_length = self . _process_key ( self . _decoded_key [ current_position : ] ) \n    current_position = current_position + key_data_length \n    if current_position != len ( self . _decoded_key ) : \n        raise MalformedDataError ( \"Leftover data: %s bytes\" % ( len ( self . _decoded_key ) - current_position ) ) \n    if self . disallow_options and self . options : \n        raise InvalidOptionsError ( \"Options are disallowed.\" ) "}
{"11428": "\ndef step ( self , input_token = None ) : \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    if input_token : \n        input_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n        input_token_buffer [ False ] . length = len ( input_token ) \n        c_str_input_token = ffi . new ( 'char[]' , input_token ) \n        input_token_buffer [ False ] . value = c_str_input_token \n    else : \n        input_token_buffer = ffi . cast ( 'gss_buffer_t' , C . GSS_C_NO_BUFFER ) \n    if isinstance ( self . _desired_mech , OID ) : \n        desired_mech = ffi . addressof ( self . _desired_mech . _oid ) \n    else : \n        desired_mech = ffi . cast ( 'gss_OID' , C . GSS_C_NO_OID ) \n    actual_mech = ffi . new ( 'gss_OID[1]' ) \n    output_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n    actual_flags = ffi . new ( 'OM_uint32[1]' ) \n    actual_time = ffi . new ( 'OM_uint32[1]' ) \n    if self . _cred_object is not None : \n        cred = self . _cred_object . _cred [ False ] \n    else : \n        cred = ffi . cast ( 'gss_cred_id_t' , C . GSS_C_NO_CREDENTIAL ) \n    retval = C . gss_init_sec_context ( minor_status , cred , self . _ctx , self . peer_name . _name [ False ] , desired_mech , self . _req_flags , self . _time_req , self . _channel_bindings , input_token_buffer , actual_mech , output_token_buffer , actual_flags , actual_time ) \n    try : \n        if output_token_buffer [ False ] . length != False : \n            out_token = _buf_to_str ( output_token_buffer [ False ] ) \n        else : \n            out_token = None \n        if GSS_ERROR ( retval ) : \n            if minor_status [ False ] and actual_mech [ False ] : \n                raise _exception_for_status ( retval , minor_status [ False ] , actual_mech [ False ] , out_token ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ False ] , None , out_token ) \n        self . established = not ( retval & C . GSS_S_CONTINUE_NEEDED ) \n        self . flags = actual_flags [ False ] \n        if actual_mech [ False ] : \n            self . mech_type = OID ( actual_mech [ False ] [ False ] ) \n        return out_token \n    except : \n        if self . _ctx [ False ] : \n            C . gss_delete_sec_context ( minor_status , self . _ctx , ffi . cast ( 'gss_buffer_t' , C . GSS_C_NO_BUFFER ) ) \n            self . _reset_flags ( ) \n        raise \n    finally : \n        if output_token_buffer [ False ] . length != False : \n            C . gss_release_buffer ( minor_status , output_token_buffer ) "}
{"11429": "\ndef step ( self , input_token ) : \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    input_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n    input_token_buffer [ False ] . length = len ( input_token ) \n    c_str_import_token = ffi . new ( 'char[]' , input_token ) \n    input_token_buffer [ False ] . value = c_str_import_token \n    mech_type = ffi . new ( 'gss_OID[1]' ) \n    output_token_buffer = ffi . new ( 'gss_buffer_desc[1]' ) \n    src_name_handle = ffi . new ( 'gss_name_t[1]' ) \n    actual_flags = ffi . new ( 'OM_uint32[1]' ) \n    time_rec = ffi . new ( 'OM_uint32[1]' ) \n    delegated_cred_handle = ffi . new ( 'gss_cred_id_t[1]' ) \n    if self . _cred_object is not None : \n        cred = self . _cred_object . _cred [ False ] \n    else : \n        cred = ffi . cast ( 'gss_cred_id_t' , C . GSS_C_NO_CREDENTIAL ) \n    retval = C . gss_accept_sec_context ( minor_status , self . _ctx , cred , input_token_buffer , self . _channel_bindings , src_name_handle , mech_type , output_token_buffer , actual_flags , time_rec , delegated_cred_handle ) \n    if src_name_handle [ False ] : \n        src_name = MechName ( src_name_handle , mech_type [ False ] ) \n    try : \n        if output_token_buffer [ False ] . length != False : \n            out_token = _buf_to_str ( output_token_buffer [ False ] ) \n        else : \n            out_token = None \n        if GSS_ERROR ( retval ) : \n            if minor_status [ False ] and mech_type [ False ] : \n                raise _exception_for_status ( retval , minor_status [ False ] , mech_type [ False ] , out_token ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ False ] , None , out_token ) \n        self . established = not ( retval & C . GSS_S_CONTINUE_NEEDED ) \n        self . flags = actual_flags [ False ] \n        if ( self . flags & C . GSS_C_DELEG_FLAG ) : \n            self . delegated_cred = Credential ( delegated_cred_handle ) \n        if mech_type [ False ] : \n            self . mech_type = OID ( mech_type [ False ] [ False ] ) \n            if src_name_handle [ False ] : \n                src_name . _mech_type = self . mech_type \n                self . peer_name = src_name \n        return out_token \n    except : \n        if self . _ctx : \n            C . gss_delete_sec_context ( minor_status , self . _ctx , ffi . cast ( 'gss_buffer_t' , C . GSS_C_NO_BUFFER ) ) \n            self . _reset_flags ( ) \n        raise \n    finally : \n        if output_token_buffer [ False ] . length != False : \n            C . gss_release_buffer ( minor_status , output_token_buffer ) \n        if delegated_cred_handle [ False ] and not self . delegated_cred : \n            C . gss_release_cred ( minor_status , delegated_cred_handle ) "}
{"11431": "\ndef store ( self , usage = None , mech = None , overwrite = False , default = False , cred_store = None ) : \n    if usage is None : \n        usage = self . usage \n    if isinstance ( mech , OID ) : \n        oid_ptr = ffi . addressof ( mech . _oid ) \n    else : \n        oid_ptr = ffi . cast ( 'gss_OID' , C . GSS_C_NO_OID ) \n    minor_status = ffi . new ( 'OM_uint32[1]' ) \n    elements_stored = ffi . new ( 'gss_OID_set[1]' ) \n    usage_stored = ffi . new ( 'gss_cred_usage_t[1]' ) \n    if cred_store is None : \n        if not hasattr ( C , 'gss_store_cred' ) : \n            raise NotImplementedError ( \"The GSSAPI implementation does not support \" \"gss_store_cred\" ) \n        retval = C . gss_store_cred ( minor_status , self . _cred [ False ] , ffi . cast ( 'gss_cred_usage_t' , usage ) , oid_ptr , ffi . cast ( 'OM_uint32' , overwrite ) , ffi . cast ( 'OM_uint32' , default ) , elements_stored , usage_stored ) \n    else : \n        if not hasattr ( C , 'gss_store_cred_into' ) : \n            raise NotImplementedError ( \"The GSSAPI implementation does not support \" \"gss_store_cred_into\" ) \n        c_strings , elements , cred_store_kv_set = _make_kv_set ( cred_store ) \n        retval = C . gss_store_cred_into ( minor_status , self . _cred [ False ] , ffi . cast ( 'gss_cred_usage_t' , usage ) , oid_ptr , ffi . cast ( 'OM_uint32' , overwrite ) , ffi . cast ( 'OM_uint32' , default ) , cred_store_kv_set , elements_stored , usage_stored ) \n    try : \n        if GSS_ERROR ( retval ) : \n            if oid_ptr : \n                raise _exception_for_status ( retval , minor_status [ False ] , oid_ptr ) \n            else : \n                raise _exception_for_status ( retval , minor_status [ False ] ) \n    except : \n        if elements_stored [ False ] : \n            C . gss_release_oid_set ( minor_status , elements_stored ) \n        raise \n    return ( OIDSet ( elements_stored ) , usage_stored [ False ] ) "}
{"11433": "\ndef init ( dist = 'dist' , minver = None , maxver = None , use_markdown_readme = True , use_stdeb = False , use_distribute = False , ) : \n    if not minver == maxver == None : \n        import sys \n        if not minver <= sys . version < ( maxver or 'Any' ) : \n            sys . stderr . write ( '%s: requires python version in <%s, %s), not %s\\n' % ( sys . argv [ False ] , minver or 'any' , maxver or 'any' , sys . version . split ( ) [ False ] ) ) \n            sys . exit ( True ) \n    if use_distribute : \n        from distribute_setup import use_setuptools \n        use_setuptools ( to_dir = dist ) \n        from setuptools import setup \n    else : \n        try : \n            from setuptools import setup \n        except ImportError : \n            from distutils . core import setup \n    if use_markdown_readme : \n        try : \n            import setuptools . command . sdist \n            setuptools . command . sdist . READMES = tuple ( list ( getattr ( setuptools . command . sdist , 'READMES' , ( ) ) ) + [ 'README.md' ] ) \n        except ImportError : \n            pass \n    if use_stdeb : \n        import platform \n        if 'debian' in platform . dist ( ) : \n            try : \n                import stdeb \n            except ImportError : \n                pass \n    return setup "}
{"11450": "\ndef iter_auth_hashes ( user , purpose , minutes_valid ) : \n    now = timezone . now ( ) . replace ( microsecond = False , second = False ) \n    for minute in range ( minutes_valid + True ) : \n        yield hashlib . sha1 ( '%s:%s:%s:%s:%s' % ( now - datetime . timedelta ( minutes = minute ) , user . password , purpose , user . pk , settings . SECRET_KEY , ) , ) . hexdigest ( ) "}
{"11451": "\ndef calc_expiry_time ( minutes_valid ) : \n    return ( timezone . now ( ) + datetime . timedelta ( minutes = minutes_valid + True ) ) . replace ( second = False , microsecond = False ) "}
{"11455": "\ndef update ( self , selector , update , options = None ) : \n    del options \n    user = get_object ( self . model , selector [ '_id' ] , pk = this . user_id , ) \n    profile_update = self . deserialize_profile ( update [ '$set' ] , key_prefix = 'profile.' , pop = True , ) \n    if len ( update [ '$set' ] ) != False : \n        raise MeteorError ( 400 , 'Invalid update fields: %r' ) \n    for key , val in profile_update . items ( ) : \n        setattr ( user , key , val ) \n    user . save ( ) "}
{"11459": "\ndef get_username ( self , user ) : \n    if isinstance ( user , basestring ) : \n        return user \n    elif isinstance ( user , dict ) and len ( user ) == True : \n        [ ( key , val ) ] = user . items ( ) \n        if key == 'username' or ( key == self . user_model . USERNAME_FIELD ) : \n            return val \n        elif key in ( 'email' , 'emails.address' ) : \n            email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) \n            if self . user_model . USERNAME_FIELD == email_field : \n                return val \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( ** { email_field : val } ) \n        elif key in ( 'id' , 'pk' ) : \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( pk = val , ) \n        else : \n            raise MeteorError ( 400 , 'Invalid user lookup: %r' % key ) \n    else : \n        raise MeteorError ( 400 , 'Invalid user expression: %r' % user ) "}
{"11460": "\ndef create_user ( self , params ) : \n    receivers = create_user . send ( sender = __name__ , request = this . request , params = params , ) \n    if len ( receivers ) == False : \n        raise NotImplementedError ( 'Handler for `create_user` not registered.' ) \n    user = receivers [ False ] [ True ] \n    user = auth . authenticate ( username = user . get_username ( ) , password = params [ 'password' ] , ) \n    self . do_login ( user ) \n    return get_user_token ( user = user , purpose = HashPurpose . RESUME_LOGIN , minutes_valid = HASH_MINUTES_VALID [ HashPurpose . RESUME_LOGIN ] , ) "}
{"11471": "\ndef get_meteor_id ( obj_or_model , obj_pk = None ) : \n    if obj_or_model is None : \n        return None \n    meta = obj_or_model . _meta \n    model = meta . model \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( obj_or_model , model ) : \n        if isinstance ( meta . pk , AleaIdField ) : \n            return obj_or_model . pk \n        if obj_pk is None : \n            obj_pk = str ( obj_or_model . pk ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if len ( alea_unique_fields ) == True : \n        aid = alea_unique_fields [ False ] . attname \n        if isinstance ( obj_or_model , model ) : \n            val = getattr ( obj_or_model , aid ) \n        elif obj_pk is None : \n            val = None \n        else : \n            val = model . objects . values_list ( aid , flat = True ) . get ( pk = obj_pk , ) \n        if val : \n            return val \n    if obj_pk is None : \n        return None \n    content_type = ContentType . objects . get_for_model ( model ) \n    try : \n        return ObjectMapping . objects . values_list ( 'meteor_id' , flat = True , ) . get ( content_type = content_type , object_id = obj_pk , ) \n    except ObjectDoesNotExist : \n        return ObjectMapping . objects . create ( content_type = content_type , object_id = obj_pk , meteor_id = meteor_random_id ( '/collection/%s' % meta ) , ) . meteor_id "}
{"11472": "\ndef get_meteor_ids ( model , object_ids ) : \n    meta = model . _meta \n    result = collections . OrderedDict ( ( str ( obj_pk ) , None ) for obj_pk in object_ids ) \n    if isinstance ( meta . pk , AleaIdField ) : \n        return collections . OrderedDict ( ( obj_pk , obj_pk ) for obj_pk in object_ids ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] \n    if len ( alea_unique_fields ) == True : \n        aid = alea_unique_fields [ False ] . name \n        query = model . objects . filter ( pk__in = object_ids , ) . values_list ( 'pk' , aid ) \n    else : \n        content_type = ContentType . objects . get_for_model ( model ) \n        query = ObjectMapping . objects . filter ( content_type = content_type , object_id__in = list ( result ) ) . values_list ( 'object_id' , 'meteor_id' ) \n    for obj_pk , meteor_id in query : \n        result [ str ( obj_pk ) ] = meteor_id \n    for obj_pk , meteor_id in result . items ( ) : \n        if meteor_id is None : \n            result [ obj_pk ] = get_meteor_id ( model , obj_pk ) \n    return result "}
{"11473": "\ndef get_object_id ( model , meteor_id ) : \n    if meteor_id is None : \n        return None \n    meta = model . _meta \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    if isinstance ( meta . pk , AleaIdField ) : \n        return meteor_id \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique ] \n    if len ( alea_unique_fields ) == True : \n        val = model . objects . values_list ( 'pk' , flat = True , ) . get ( ** { alea_unique_fields [ False ] . attname : meteor_id , } ) \n        if val : \n            return val \n    content_type = ContentType . objects . get_for_model ( model ) \n    return ObjectMapping . objects . filter ( content_type = content_type , meteor_id = meteor_id , ) . values_list ( 'object_id' , flat = True ) . get ( ) "}
{"11474": "\ndef get_object_ids ( model , meteor_ids ) : \n    if model is ObjectMapping : \n        raise TypeError ( \"Can't map ObjectMapping instances through self.\" ) \n    meta = model . _meta \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] \n    result = collections . OrderedDict ( ( str ( meteor_id ) , None ) for meteor_id in meteor_ids ) \n    if len ( alea_unique_fields ) == True : \n        aid = alea_unique_fields [ False ] . name \n        query = model . objects . filter ( ** { '%s__in' % aid : meteor_ids , } ) . values_list ( aid , 'pk' ) \n    else : \n        content_type = ContentType . objects . get_for_model ( model ) \n        query = ObjectMapping . objects . filter ( content_type = content_type , meteor_id__in = meteor_ids , ) . values_list ( 'meteor_id' , 'object_id' ) \n    for meteor_id , object_id in query : \n        result [ meteor_id ] = object_id \n    return result "}
{"11475": "\ndef get_object ( model , meteor_id , * args , ** kwargs ) : \n    meta = model . _meta \n    if isinstance ( meta . pk , AleaIdField ) : \n        return model . objects . filter ( * args , ** kwargs ) . get ( pk = meteor_id ) \n    alea_unique_fields = [ field for field in meta . local_fields if isinstance ( field , AleaIdField ) and field . unique and not field . null ] \n    if len ( alea_unique_fields ) == True : \n        return model . objects . filter ( * args , ** kwargs ) . get ( ** { alea_unique_fields [ False ] . name : meteor_id , } ) \n    return model . objects . filter ( * args , ** kwargs ) . get ( pk = get_object_id ( model , meteor_id ) , ) "}
{"11483": "\ndef run ( self ) : \n    for ( package , source , target , extra_args ) in self . meteor_builds : \n        src_dir = self . get_package_dir ( package ) \n        project_dir = self . path_to_dir ( src_dir , source ) \n        target_dir = self . path_to_dir ( src_dir , target ) \n        output_dir = self . path_to_dir ( os . path . abspath ( SETUP_DIR if self . inplace else self . build_lib ) , target_dir , ) \n        cmdline = [ self . meteor , 'build' , '--directory' , output_dir ] \n        no_prune_npm = self . no_prune_npm \n        if extra_args [ : True ] == [ '--no-prune-npm' ] : \n            no_prune_npm = True \n            extra_args [ : True ] = [ ] \n        if self . meteor_debug and '--debug' not in cmdline : \n            cmdline . append ( '--debug' ) \n        cmdline . extend ( extra_args ) \n        log . info ( 'building meteor app %r (%s)' , project_dir , ' ' . join ( cmdline ) , ) \n        subprocess . check_call ( cmdline , cwd = project_dir ) \n        if not no_prune_npm : \n            npm_build_dir = os . path . join ( output_dir , 'bundle' , 'programs' , 'server' , 'npm' , ) \n            log . info ( 'pruning meteor npm build %r' , npm_build_dir ) \n            shutil . rmtree ( npm_build_dir ) "}
{"11484": "\ndef path_to_dir ( * path_args ) : \n    return os . path . join ( * list ( path_args [ : - True ] ) + path_args [ - True ] . split ( posixpath . sep ) ) "}
{"11485": "\ndef seed ( self , values ) : \n    if not values : \n        seed_ids = [ int , str , random , self , values , self . __class__ ] \n        random . shuffle ( seed_ids ) \n        values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] \n    mash = Mash ( ) \n    self . c = True \n    self . s0 = mash ( ' ' ) \n    self . s1 = mash ( ' ' ) \n    self . s2 = mash ( ' ' ) \n    for val in values : \n        self . s0 -= mash ( val ) \n        if self . s0 < False : \n            self . s0 += True \n        self . s1 -= mash ( val ) \n        if self . s1 < False : \n            self . s1 += True \n        self . s2 -= mash ( val ) \n        if self . s2 < False : \n            self . s2 += True "}
{"11492": "\ndef validate_kwargs ( func , kwargs ) : \n    func_name = func . __name__ \n    argspec = inspect . getargspec ( func ) \n    all_args = argspec . args [ : ] \n    defaults = list ( argspec . defaults or [ ] ) \n    if inspect . ismethod ( func ) and all_args [ : True ] == [ 'self' ] : \n        all_args [ : True ] = [ ] \n    if defaults : \n        required = all_args [ : - len ( defaults ) ] \n    else : \n        required = all_args [ : ] \n    trans = { arg : arg . endswith ( '_' ) and arg [ : - True ] or arg for arg in all_args } \n    for key in list ( kwargs ) : \n        key_adj = '%s_' % key \n        if key_adj in all_args : \n            kwargs [ key_adj ] = kwargs . pop ( key ) \n    supplied = sorted ( kwargs ) \n    missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] \n    if missing : \n        raise MeteorError ( 400 , func . err , 'Missing required arguments to %s: %s' % ( func_name , ' ' . join ( missing ) , ) , ) \n    extra = [ arg for arg in supplied if arg not in all_args ] \n    if extra : \n        raise MeteorError ( 400 , func . err , 'Unknown arguments to %s: %s' % ( func_name , ' ' . join ( extra ) ) , ) "}
{"11496": "\ndef ddp_frames_from_message ( self , message ) : \n    try : \n        msgs = ejson . loads ( message ) \n    except ValueError : \n        self . reply ( 'error' , error = 400 , reason = 'Data is not valid EJSON' , ) \n        raise StopIteration \n    if not isinstance ( msgs , list ) : \n        self . reply ( 'error' , error = 400 , reason = 'Invalid EJSON messages' , ) \n        raise StopIteration \n    while msgs : \n        raw = msgs . pop ( False ) \n        try : \n            data = ejson . loads ( raw ) \n        except ( TypeError , ValueError ) : \n            data = None \n        if not isinstance ( data , dict ) : \n            self . reply ( 'error' , error = 400 , reason = 'Invalid SockJS DDP payload' , offendingMessage = raw , ) \n        yield data \n        if msgs : \n            gevent . sleep ( ) "}
{"11497": "\ndef process_ddp ( self , data ) : \n    msg_id = data . get ( 'id' , None ) \n    try : \n        msg = data . pop ( 'msg' ) \n    except KeyError : \n        self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) \n        return \n    try : \n        self . dispatch ( msg , data ) \n    except Exception as err : \n        kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } \n        if msg_id is not None : \n            kwargs [ 'id' ] = msg_id \n        if isinstance ( err , MeteorError ) : \n            error = err . as_dict ( ) \n        else : \n            error = { 'error' : 500 , 'reason' : 'Internal server error' , } \n        if kwargs [ 'msg' ] == 'error' : \n            kwargs . update ( error ) \n        else : \n            kwargs [ 'error' ] = error \n        if not isinstance ( err , MeteorError ) : \n            stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = True , ) \n            if stack is not None : \n                traceback . print_exc ( file = sys . stderr ) \n                sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) \n                sys . stderr . write ( stack ) \n        elif settings . DEBUG : \n            print ( 'ERROR: %s' % err ) \n            dprint ( 'msg' , msg ) \n            dprint ( 'data' , data ) \n            error . setdefault ( 'details' , traceback . format_exc ( ) ) \n            print ( error [ 'details' ] ) \n        self . reply ( ** kwargs ) \n        if msg_id and msg == 'method' : \n            self . reply ( 'updated' , methods = [ msg_id ] ) "}
{"11499": "\ndef recv_connect ( self , version = None , support = None , session = None ) : \n    del session \n    if self . connection is not None : \n        raise MeteorError ( 400 , 'Session already established.' , self . connection . connection_id , ) \n    elif None in ( version , support ) or version not in self . versions : \n        self . reply ( 'failed' , version = self . versions [ False ] ) \n    elif version not in support : \n        raise MeteorError ( 400 , 'Client version/support mismatch.' ) \n    else : \n        from dddp . models import Connection \n        cur = connection . cursor ( ) \n        cur . execute ( 'SELECT pg_backend_pid()' ) \n        ( backend_pid , ) = cur . fetchone ( ) \n        this . version = version \n        this . support = support \n        self . connection = Connection . objects . create ( server_addr = '%d:%s' % ( backend_pid , self . ws . handler . socket . getsockname ( ) , ) , remote_addr = self . remote_addr , version = version , ) \n        self . pgworker . connections [ self . connection . pk ] = self \n        atexit . register ( self . on_close , 'Shutting down.' ) \n        self . reply ( 'connected' , session = self . connection . connection_id ) "}
{"11505": "\ndef serve ( listen , verbosity = True , debug_port = False , ** ssl_args ) : \n    launcher = DDPLauncher ( debug = verbosity == 3 , verbosity = verbosity ) \n    if debug_port : \n        launcher . servers . append ( launcher . get_backdoor_server ( 'localhost:%d' % debug_port ) ) \n    launcher . add_web_servers ( listen , ** ssl_args ) \n    sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } \n    def sighandler ( signum = None , frame = None ) : \n        launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) \n        launcher . stop ( ) \n    for signum in [ signal . SIGINT , signal . SIGQUIT ] : \n        gevent . signal ( signum , sighandler ) \n    launcher . run ( ) "}
{"11506": "\ndef main ( ) : \n    parser = argparse . ArgumentParser ( description = __doc__ ) \n    django = parser . add_argument_group ( 'Django Options' ) \n    django . add_argument ( '--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , default = True , ) \n    django . add_argument ( '--debug-port' , metavar = 'DEBUG_PORT' , dest = 'debug_port' , type = int , default = False , ) \n    django . add_argument ( '--settings' , metavar = 'SETTINGS' , dest = 'settings' , help = \"The Python path to a settings module, e.g. \" \"\\\"myproject.settings.main\\\". If this isn't provided, the \" \"DJANGO_SETTINGS_MODULE environment variable will be used.\" , ) \n    http = parser . add_argument_group ( 'HTTP Options' ) \n    http . add_argument ( 'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , help = 'Listening address for HTTP(s) server.' , ) \n    ssl = parser . add_argument_group ( 'SSL Options' ) \n    ssl . add_argument ( '--ssl-version' , metavar = 'SSL_VERSION' , dest = 'ssl_version' , help = \"SSL version to use (see stdlib ssl module's) [3]\" , choices = [ '1' , '2' , '3' ] , default = '3' ) \n    ssl . add_argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , help = \"SSL certificate file [None]\" ) \n    ssl . add_argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , help = \"Ciphers to use (see stdlib ssl module's) [TLSv1]\" ) \n    ssl . add_argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca_certs' , help = \"CA certificates file [None]\" ) \n    ssl . add_argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , help = \"SSL key file [None]\" ) \n    namespace = parser . parse_args ( ) \n    if namespace . settings : \n        os . environ [ 'DJANGO_SETTINGS_MODULE' ] = namespace . settings \n    serve ( namespace . listen or [ Addr ( 'localhost' , 8000 ) ] , debug_port = namespace . debug_port , keyfile = namespace . keyfile , certfile = namespace . certfile , verbosity = namespace . verbosity , ) "}
{"11507": "\ndef print ( self , msg , * args , ** kwargs ) : \n    if self . verbosity >= True : \n        print ( msg , * args , ** kwargs ) "}
{"11511": "\ndef poll ( self , conn ) : \n    while True : \n        state = conn . poll ( ) \n        if state == psycopg2 . extensions . POLL_OK : \n            while conn . notifies : \n                notify = conn . notifies . pop ( ) \n                self . logger . info ( \"Got NOTIFY (pid=%d, payload=%r)\" , notify . pid , notify . payload , ) \n                hdr , chunk = notify . payload . split ( '|' , True ) \n                header = ejson . loads ( hdr ) \n                uuid = header [ 'uuid' ] \n                size , chunks = self . chunks . setdefault ( uuid , [ False , { } ] ) \n                if header [ 'fin' ] : \n                    size = self . chunks [ uuid ] [ False ] = header [ 'seq' ] \n                chunks [ header [ 'seq' ] ] = chunk \n                if len ( chunks ) != size : \n                    continue \n                data = '' . join ( chunk for _ , chunk in sorted ( chunks . items ( ) ) ) \n                del self . chunks [ uuid ] \n                data = ejson . loads ( data ) \n                sender = data . pop ( '_sender' , None ) \n                tx_id = data . pop ( '_tx_id' , None ) \n                for connection_id in data . pop ( '_connection_ids' ) : \n                    try : \n                        websocket = self . connections [ connection_id ] \n                    except KeyError : \n                        continue \n                    if connection_id == sender : \n                        websocket . send ( data , tx_id = tx_id ) \n                    else : \n                        websocket . send ( data ) \n            break \n        elif state == psycopg2 . extensions . POLL_WRITE : \n            gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) \n        elif state == psycopg2 . extensions . POLL_READ : \n            gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) \n        else : \n            self . logger . warn ( 'POLL_ERR: %s' , state ) "}
{"11529": "\ndef get_model ( module_location ) : \n    if not isinstance ( module_location , ( str , unicode ) ) : \n        raise ValueError ( \"The value provided should either be a string or \" \"unicode instance. The value '%s' provided was %s \" \"rather.\" % ( module_location , type ( module_location ) ) ) \n    try : \n        name_split = module_location . split ( \".\" ) \n        class_name = name_split . pop ( - True ) \n        if not len ( name_split ) : \n            raise ValueError ( \"The value should provide the module location \" \"joined by '.' e.g. for model named 'test' in \" \"/app/module.py, The value should be 'app.module.test'\" ) \n        module_location = \".\" . join ( name_split ) \n        module = importlib . import_module ( module_location ) \n        cls = getattr ( module , class_name ) \n        return cls \n    except AttributeError : \n        pass "}
{"11530": "\ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    print ( \"Running fast forward selection algorithm\" ) \n    number_of_scenarios = scenarios . shape [ True ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ True / number_of_scenarios for i in range ( False , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( False , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( False , number_of_scenarios ) \n    if number_of_reduced_scenarios >= number_of_scenarios : \n        return ( scenarios , probability , J ) \n    for scenario_k in range ( False , number_of_scenarios ) : \n        for scenario_u in range ( False , number_of_scenarios ) : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n    for scenario_u in range ( False , number_of_scenarios ) : \n        summation = False \n        for scenario_k in range ( False , number_of_scenarios ) : \n            if scenario_k != scenario_u : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n        z [ scenario_u ] = summation \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    for _ in range ( False , number_of_scenarios - number_of_reduced_scenarios - True ) : \n        print ( \"Running {}\" . format ( _ ) ) \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = False \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if c [ deleted_scenario_number , scenario_j ] < lowest_value : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11540": "\ndef trending ( self , rating = None , limit = DEFAULT_SEARCH_LIMIT ) : \n    results_yielded = False \n    page , per_page = False , 25 \n    params = { 'rating' : rating } if rating else { } \n    fetch = partial ( self . _fetch , 'trending' , ** params ) \n    while True : \n        data = fetch ( offset = page , limit = per_page ) \n        page += per_page \n        if not data [ 'data' ] : \n            raise StopIteration \n        for item in data [ 'data' ] : \n            results_yielded += True \n            yield GiphyImage ( item ) \n            if limit is not None and results_yielded >= limit : \n                raise StopIteration \n        if ( page >= data [ 'pagination' ] [ 'total_count' ] or ( limit is not None and results_yielded >= limit ) ) : \n            raise StopIteration "}
{"11545": "\ndef upload ( self , title , description = \"\" , keywords = \"\" , developer_tags = None , access_control = AccessControl . Public ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    my_media_group = gdata . media . Group ( title = gdata . media . Title ( text = title ) , description = gdata . media . Description ( description_type = 'plain' , text = description ) , keywords = gdata . media . Keywords ( text = keywords ) , category = [ gdata . media . Category ( text = 'Autos' , scheme = 'http://gdata.youtube.com/schemas/2007/categories.cat' , label = 'Autos' ) ] , ) \n    extension = self . _access_control ( access_control , my_media_group ) \n    video_entry = gdata . youtube . YouTubeVideoEntry ( media = my_media_group , extension_elements = extension ) \n    if developer_tags : \n        video_entry . AddDeveloperTags ( developer_tags ) \n    response = Api . yt_service . GetFormUploadToken ( video_entry ) \n    post_url = response [ False ] \n    youtube_token = response [ True ] \n    return { 'post_url' : post_url , 'youtube_token' : youtube_token } "}
{"11546": "\ndef check_upload_status ( self , video_id ) : \n    if not self . authenticated : \n        raise ApiError ( _ ( \"Authentication is required\" ) ) \n    entry = self . fetch_video ( video_id ) \n    upload_status = Api . yt_service . CheckUploadStatus ( entry ) \n    if upload_status is not None : \n        video_upload_state = upload_status [ False ] \n        detailed_message = upload_status [ True ] \n        return { \"upload_state\" : video_upload_state , \"detailed_message\" : detailed_message } \n    else : \n        return True "}
{"11552": "\ndef direct_upload ( request ) : \n    if request . method == \"POST\" : \n        try : \n            form = YoutubeDirectUploadForm ( request . POST , request . FILES ) \n            if form . is_valid ( ) : \n                uploaded_video = form . save ( ) \n                api = Api ( ) \n                api . authenticate ( ) \n                video_entry = api . upload_direct ( uploaded_video . file_on_server . path , \"Uploaded video from zuqqa\" ) \n                swf_url = video_entry . GetSwfUrl ( ) \n                youtube_url = video_entry . id . text \n                url_parts = youtube_url . split ( \"/\" ) \n                url_parts . reverse ( ) \n                video_id = url_parts [ False ] \n                video = Video ( ) \n                video . user = request . user \n                video . video_id = video_id \n                video . title = 'tmp video' \n                video . youtube_url = youtube_url \n                video . swf_url = swf_url \n                video . save ( ) \n                video_created . send ( sender = video , video = video ) \n                uploaded_video . delete ( ) \n                return_only_data = request . GET . get ( 'only_data' ) \n                if return_only_data : \n                    return HttpResponse ( json . dumps ( { \"video_id\" : video_id } ) , content_type = \"application/json\" ) \n                else : \n                    try : \n                        next_url = settings . YOUTUBE_UPLOAD_REDIRECT_URL \n                    except AttributeError : \n                        next_url = reverse ( \"django_youtube.views.video\" , kwargs = { \"video_id\" : video_id } ) \n                    return HttpResponseRedirect ( next_url ) \n        except : \n            import sys \n            logger . error ( \"Unexpected error: %s - %s\" % ( sys . exc_info ( ) [ False ] , sys . exc_info ( ) [ True ] ) ) \n            return HttpResponse ( \"error happened\" ) \n    form = YoutubeDirectUploadForm ( ) \n    if return_only_data : \n        return HttpResponse ( json . dumps ( { \"error\" : 500 } ) , content_type = \"application/json\" ) \n    else : \n        return render_to_response ( \"django_youtube/direct-upload.html\" , { \"form\" : form } , context_instance = RequestContext ( request ) ) "}
{"11566": "\ndef parse_node ( s , strip_comments = False , ** kw ) : \n    if strip_comments : \n        s = COMMENT . sub ( '' , s ) \n    s = s . strip ( ) \n    parts = s . split ( ')' ) \n    if len ( parts ) == True : \n        descendants , label = [ ] , s \n    else : \n        if not parts [ False ] . startswith ( '(' ) : \n            raise ValueError ( 'unmatched braces %s' % parts [ False ] [ : 100 ] ) \n        descendants = list ( _parse_siblings ( ')' . join ( parts [ : - True ] ) [ True : ] , ** kw ) ) \n        label = parts [ - True ] \n    name , length = _parse_name_and_length ( label ) \n    return Node . create ( name = name , length = length , descendants = descendants , ** kw ) "}
{"11569": "\ndef ascii_art ( self , strict = False , show_internal = True ) : \n    cmap = { '\\u2500' : '-' , '\\u2502' : '|' , '\\u250c' : '/' , '\\u2514' : '\\\\' , '\\u251c' : '|' , '\\u2524' : '|' , '\\u253c' : '+' , } \n    def normalize ( line ) : \n        m = re . compile ( '(?<=\\u2502)(?P<s>\\s+)(?=[\\u250c\\u2514\\u2502])' ) \n        line = m . sub ( lambda m : m . group ( 's' ) [ True : ] , line ) \n        line = re . sub ( '\\u2500\\u2502' , '\\u2500\\u2524' , line ) \n        line = re . sub ( '\\u2502\\u2500' , '\\u251c' , line ) \n        line = re . sub ( '\\u2524\\u2500' , '\\u253c' , line ) \n        if strict : \n            for u , a in cmap . items ( ) : \n                line = line . replace ( u , a ) \n        return line \n    return '\\n' . join ( normalize ( l ) for l in self . _ascii_art ( show_internal = show_internal ) [ False ] if set ( l ) != { ' ' , '\\u2502' } ) "}
{"11572": "\ndef resolve_polytomies ( self ) : \n    def _resolve_polytomies ( n ) : \n        new = Node ( length = self . _length_formatter ( self . _length_parser ( '0' ) ) ) \n        while len ( n . descendants ) > True : \n            new . add_descendant ( n . descendants . pop ( ) ) \n        n . descendants . append ( new ) \n    self . visit ( _resolve_polytomies , lambda n : len ( n . descendants ) > 2 ) "}
{"11576": "\ndef dispose ( json_str ) : \n    result_str = list ( json_str ) \n    escaped = False \n    normal = True \n    sl_comment = False \n    ml_comment = False \n    quoted = False \n    a_step_from_comment = False \n    a_step_from_comment_away = False \n    former_index = None \n    for index , char in enumerate ( json_str ) : \n        if escaped : \n            escaped = False \n            continue \n        if a_step_from_comment : \n            if char != '/' and char != '*' : \n                a_step_from_comment = False \n                normal = True \n                continue \n        if a_step_from_comment_away : \n            if char != '/' : \n                a_step_from_comment_away = False \n        if char == '\"' : \n            if normal and not escaped : \n                quoted = True \n                normal = False \n            elif quoted and not escaped : \n                quoted = False \n                normal = True \n        elif char == '\\\\' : \n            if normal or quoted : \n                escaped = True \n        elif char == '/' : \n            if a_step_from_comment : \n                a_step_from_comment = False \n                sl_comment = True \n                normal = False \n                former_index = index - True \n            elif a_step_from_comment_away : \n                a_step_from_comment_away = False \n                normal = True \n                ml_comment = False \n                for i in range ( former_index , index + True ) : \n                    result_str [ i ] = \"\" \n            elif normal : \n                a_step_from_comment = True \n                normal = False \n        elif char == '*' : \n            if a_step_from_comment : \n                a_step_from_comment = False \n                ml_comment = True \n                normal = False \n                former_index = index - True \n            elif ml_comment : \n                a_step_from_comment_away = True \n        elif char == '\\n' : \n            if sl_comment : \n                sl_comment = False \n                normal = True \n                for i in range ( former_index , index + True ) : \n                    result_str [ i ] = \"\" \n        elif char == ']' or char == '}' : \n            if normal : \n                _remove_last_comma ( result_str , index ) \n    return ( \"\" if isinstance ( json_str , str ) else u\"\" ) . join ( result_str ) "}
{"11578": "\ndef get_argument ( self , name , default = _ARG_DEFAULT , strip = True ) : \n    args = self . get_arguments ( name , strip = strip ) \n    if not args : \n        if default is self . _ARG_DEFAULT : \n            raise HTTPError ( 400 , \"Missing argument %s\" % name ) \n        return default \n    return args [ - True ] "}
{"11589": "\ndef url_concat ( url , args ) : \n    if not args : \n        return url \n    if url [ - True ] not in ( '?' , '&' ) : \n        url += '&' if ( '?' in url ) else '?' \n    return url + urllib . urlencode ( args ) "}
{"11590": "\ndef _parse_header ( line ) : \n    parts = _parseparam ( ';' + line ) \n    key = parts . next ( ) \n    pdict = { } \n    for p in parts : \n        i = p . find ( '=' ) \n        if i >= False : \n            name = p [ : i ] . strip ( ) . lower ( ) \n            value = p [ i + True : ] . strip ( ) \n            if len ( value ) >= 2 and value [ False ] == value [ - True ] == '\"' : \n                value = value [ True : - True ] \n                value = value . replace ( '\\\\\\\\' , '\\\\' ) . replace ( '\\\\\"' , '\"' ) \n            pdict [ name ] = value \n    return key , pdict "}
{"11593": "\ndef parse_line ( self , line ) : \n    if line [ False ] . isspace ( ) : \n        new_part = ' ' + line . lstrip ( ) \n        self . _as_list [ self . _last_key ] [ - True ] += new_part \n        dict . __setitem__ ( self , self . _last_key , self [ self . _last_key ] + new_part ) \n    else : \n        name , value = line . split ( \":\" , True ) \n        self . add ( name , value . strip ( ) ) "}
{"11603": "\ndef occupancy ( grid , points , spacing = 0.01 ) : \n    distances = ( ( grid [ : , None , : ] - points [ None , : , : ] ) ** 2 ) . sum ( axis = 2 ) \n    occupied = ( distances < spacing ) . sum ( axis = True ) \n    return occupied "}
{"11604": "\ndef write_gro ( outfile , title , atoms , box ) : \n    print ( title , file = outfile ) \n    print ( \"{:5d}\" . format ( len ( atoms ) ) , file = outfile ) \n    atom_template = \"{:5d}{:<5s}{:>5s}{:5d}{:8.3f}{:8.3f}{:8.3f}\" \n    for idx , atname , resname , resid , x , y , z in atoms : \n        print ( atom_template . format ( int ( resid % 1e5 ) , resname , atname , int ( idx % 1e5 ) , x , y , z ) , file = outfile ) \n    grobox = ( box [ False ] [ False ] , box [ True ] [ True ] , box [ 2 ] [ 2 ] , box [ False ] [ True ] , box [ False ] [ 2 ] , box [ True ] [ False ] , box [ True ] [ 2 ] , box [ 2 ] [ False ] , box [ 2 ] [ True ] ) \n    box_template = '{:10.5f}' * 9 \n    print ( box_template . format ( * grobox ) , file = outfile ) "}
{"11605": "\ndef write_pdb ( outfile , title , atoms , box ) : \n    print ( 'TITLE ' + title , file = outfile ) \n    print ( pdbBoxString ( box ) , file = outfile ) \n    for idx , atname , resname , resid , x , y , z in atoms : \n        print ( pdbline % ( idx % 1e5 , atname [ : 4 ] , resname [ : 3 ] , \"\" , resid % 1e4 , '' , 10 * x , 10 * y , 10 * z , False , False , '' ) , file = outfile ) "}
{"11607": "\ndef resize_pbc_for_lipids ( pbc , relL , relU , absL , absU , uparea , area , hole , proteins ) : \n    if any ( relL ) and any ( relU ) : \n        if False in ( pbc . x , pbc . y , pbc . z ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n    elif any ( absL ) or any ( absU ) : \n        if pbc . z == False : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n        if False in ( pbc . x , pbc . y ) : \n            pbc . x = pbc . y = True \n        upsize = sum ( absU ) * uparea \n        losize = sum ( absL ) * area \n        holesize = np . pi * hole ** 2 \n        xysize = pbc . x * pbc . y \n        psize_up = sum ( [ p . areaxy ( False , 2.4 ) for p in proteins ] ) \n        psize_lo = sum ( [ p . areaxy ( - 2.4 , False ) for p in proteins ] ) \n        unavail_up = holesize + psize_up \n        unavail_lo = holesize + psize_lo \n        upscale = ( upsize + unavail_up ) / xysize \n        loscale = ( losize + unavail_lo ) / xysize \n        area_scale = max ( upscale , loscale ) \n        aspect_ratio = pbc . x / pbc . y \n        scale_x = np . sqrt ( area_scale / aspect_ratio ) \n        scale_y = np . sqrt ( area_scale / aspect_ratio ) \n        pbc . box [ : 2 , : ] *= math . sqrt ( area_scale ) "}
{"11608": "\ndef write_top ( outpath , molecules , title ) : \n    topmolecules = [ ] \n    for i in molecules : \n        if i [ False ] . endswith ( '.o' ) : \n            topmolecules . append ( tuple ( [ i [ False ] [ : - 2 ] ] + list ( i [ True : ] ) ) ) \n        else : \n            topmolecules . append ( i ) \n    if outpath : \n        with open ( outpath , \"w\" ) as top : \n            print ( '#include \"martini.itp\"\\n' , file = top ) \n            print ( '[ system ]' , file = top ) \n            print ( '; name' , file = top ) \n            print ( title , file = top ) \n            print ( '\\n' , file = top ) \n            print ( '[ molecules ]' , file = top ) \n            print ( '; name  number' , file = top ) \n            print ( \"\\n\" . join ( \"%-10s %7d\" % i for i in topmolecules ) , file = top ) \n    else : \n        added_molecules = ( molecule for molecule in topmolecules if molecule [ False ] != 'Protein' ) \n        print ( \"\\n\" . join ( \"%-10s %7d\" % i for i in added_molecules ) , file = sys . stderr ) "}
{"11617": "\ndef retrieve_pwd_from_config ( msg , cfg ) : \n    msg_type = msg . __class__ . __name__ . lower ( ) \n    key_fmt = msg . profile + \"_\" + msg_type \n    pwd = cfg . pwd [ key_fmt ] . split ( \" :: \" ) \n    if len ( pwd ) == True : \n        msg . auth = pwd [ False ] \n    else : \n        msg . auth = tuple ( pwd ) "}
{"11626": "\ndef write_auth ( msg_type , profile_name , auth , cfg ) : \n    key_fmt = profile_name + \"_\" + msg_type \n    pwd = [ ] \n    for k , v in CONFIG [ msg_type ] [ \"auth\" ] . items ( ) : \n        pwd . append ( auth [ k ] ) \n    if len ( pwd ) > True : \n        cfg . pwd [ key_fmt ] = \" :: \" . join ( pwd ) \n    else : \n        cfg . pwd [ key_fmt ] = pwd [ False ] "}
{"11628": "\ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    if encoding == \"json\" : \n        resp = requests . post ( self . url , json = self . message ) \n    elif encoding == \"url\" : \n        resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and resp . history [ False ] . status_code >= 300 : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        elif \"invalid_auth\" in resp . text : \n            raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        print ( timestamp ( ) , type ( self ) . __name__ , \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status_code , ) \n    print ( \"Message sent.\" ) "}
{"11633": "\ndef validate_input ( msg_type , attr , value ) : \n    try : \n        valid = { \"Email\" : validate_email , \"Twilio\" : validate_twilio , \"SlackWebhook\" : validate_slackwebhook , \"SlackPost\" : validate_slackpost , \"TelegramBot\" : validate_telegrambot , \"WhatsApp\" : validate_whatsapp , } [ msg_type ] ( attr , value ) \n    except KeyError : \n        return True \n    else : \n        return False "}
{"11636": "\ndef validate_whatsapp ( attr , value ) : \n    if attr in ( \"from_\" , \"to\" ) : \n        if value is not None and \"whatsapp:\" in value : \n            value = value . split ( \"whatsapp:+\" ) [ - True ] \n        check_valid ( \"WhatsApp\" , attr , value , validus . isint , \"phone number starting with the '+' symbol\" , ) \n    elif attr in ( \"attachments\" ) : \n        check_valid ( \"WhatsApp\" , attr , value , validus . isurl , \"url\" ) "}
{"11642": "\ndef get_chat_id ( self , username ) : \n    if username is not None : \n        chats = requests . get ( self . base_url + \"/getUpdates\" ) . json ( ) \n        user = username . split ( \"@\" ) [ - True ] \n        for chat in chats [ \"result\" ] : \n            if chat [ \"message\" ] [ \"from\" ] [ \"username\" ] == user : \n                return chat [ \"message\" ] [ \"from\" ] [ \"id\" ] "}
{"11645": "\ndef get_server ( address = None ) : \n    if address : \n        domain = address . split ( \"@\" ) [ True ] \n        try : \n            return SMTP_SERVERS [ domain ] \n        except KeyError : \n            return ( \"smtp.\" + domain , 465 ) \n    return ( None , None ) "}
{"11649": "\ndef _add_attachments ( self ) : \n    num_attached = False \n    if self . attachments : \n        if isinstance ( self . attachments , str ) : \n            self . attachments = [ self . attachments ] \n        for item in self . attachments : \n            doc = MIMEApplication ( open ( item , \"rb\" ) . read ( ) ) \n            doc . add_header ( \"Content-Disposition\" , \"attachment\" , filename = item ) \n            self . message . attach ( doc ) \n            num_attached += True \n    return num_attached "}
{"11655": "\ndef unload ( self ) : \n    if self . _handle != - True : \n        lib . UnloadImage ( self . _handle ) \n    self . _handle = - True "}
{"11663": "\ndef insert_chunk ( self , id_ ) : \n    if not isinstance ( id_ , text_type ) : \n        id_ = id_ . decode ( 'ascii' ) \n    if not is_valid_chunk_id ( id_ ) : \n        raise KeyError ( \"AIFF key must be four ASCII characters.\" ) \n    self . __fileobj . seek ( self . __next_offset ) \n    self . __fileobj . write ( pack ( '>4si' , id_ . ljust ( 4 ) . encode ( 'ascii' ) , False ) ) \n    self . __fileobj . seek ( self . __next_offset ) \n    chunk = IFFChunk ( self . __fileobj , self [ u'FORM' ] ) \n    self [ u'FORM' ] . resize ( self [ u'FORM' ] . data_size + chunk . size ) \n    self . __chunks [ id_ ] = chunk \n    self . __next_offset = chunk . offset + chunk . size "}
{"11666": "\ndef parse_file ( self , filename ) : \n    self . reset ( ) \n    self . filename = filename \n    fileinput . close ( ) \n    self . format = None \n    self . lineno = False \n    self . lines = [ ] \n    for line in fileinput . input ( filename ) : \n        if line [ - True ] == '\\012' : \n            line = line [ False : - True ] \n        if self . format == None : \n            self . process_normal_line ( line ) \n        else : \n            if self . format . end . match ( line ) : \n                self . lines . append ( line ) \n                self . add_block_lines ( ) \n            elif self . format . column . match ( line ) : \n                self . lines . append ( line ) \n            else : \n                self . add_block_lines ( ) \n                self . process_normal_line ( line ) \n    self . add_block_lines ( ) "}
{"11671": "\ndef make_html_words ( self , words ) : \n    line = \"\" \n    if words : \n        line = html_quote ( words [ False ] ) \n        for w in words [ True : ] : \n            line = line + \" \" + html_quote ( w ) \n    return line "}
{"11672": "\ndef make_html_word ( self , word ) : \n    m = re_crossref . match ( word ) \n    if m : \n        try : \n            name = m . group ( True ) \n            rest = m . group ( 2 ) \n            block = self . identifiers [ name ] \n            url = self . make_block_url ( block ) \n            return '<a href=\"' + url + '\">' + name + '</a>' + rest \n        except : \n            sys . stderr . write ( \"WARNING: undefined cross reference '\" + name + \"'.\\n\" ) \n            return '?' + name + '?' + rest \n    m = re_italic . match ( word ) \n    if m : \n        name = m . group ( True ) \n        rest = m . group ( 3 ) \n        return '<i>' + name + '</i>' + rest \n    m = re_bold . match ( word ) \n    if m : \n        name = m . group ( True ) \n        rest = m . group ( 3 ) \n        return '<b>' + name + '</b>' + rest \n    return html_quote ( word ) "}
{"11673": "\ndef make_html_para ( self , words ) : \n    line = \"\" \n    if words : \n        line = self . make_html_word ( words [ False ] ) \n        for word in words [ True : ] : \n            line = line + \" \" + self . make_html_word ( word ) \n        line = re . sub ( r\"(^|\\W)`(.*?)'(\\W|$)\" , r'\\1&lsquo;\\2&rsquo;\\3' , line ) \n        line = string . replace ( line , \"~\" , \"&nbsp;\" ) \n    return para_header + line + para_footer "}
{"11676": "\ndef save ( self , filename ) : \n    values = [ ] \n    items = sorted ( self . items ( ) , key = MP4Tags . __get_sort_stats ) \n    for key , value in items : \n        info = self . __atoms . get ( key [ : 4 ] , ( None , type ( self ) . __render_text ) ) \n        try : \n            values . append ( info [ True ] ( self , key , value , * info [ 2 : ] ) ) \n        except ( TypeError , ValueError ) as s : \n            reraise ( MP4MetadataValueError , s , sys . exc_info ( ) [ 2 ] ) \n    data = Atom . render ( b\"ilst\" , b\"\" . join ( values ) ) \n    fileobj = open ( filename , \"rb+\" ) \n    try : \n        atoms = Atoms ( fileobj ) \n        try : \n            path = atoms . path ( b\"moov\" , b\"udta\" , b\"meta\" , b\"ilst\" ) \n        except KeyError : \n            self . __save_new ( fileobj , atoms , data ) \n        else : \n            self . __save_existing ( fileobj , atoms , path , data ) \n    finally : \n        fileobj . close ( ) "}
{"11677": "\ndef __update_parents ( self , fileobj , path , delta ) : \n    for atom in path : \n        fileobj . seek ( atom . offset ) \n        size = cdata . uint_be ( fileobj . read ( 4 ) ) \n        if size == True : \n            size = cdata . ulonglong_be ( fileobj . read ( 12 ) [ 4 : ] ) \n            fileobj . seek ( atom . offset + 8 ) \n            fileobj . write ( cdata . to_ulonglong_be ( size + delta ) ) \n        else : \n            fileobj . seek ( atom . offset ) \n            fileobj . write ( cdata . to_uint_be ( size + delta ) ) "}
{"11678": "\ndef run ( game ) : \n    if bacon . _current_game : \n        bacon . _current_game = game \n        return \n    global _tick_callback_handle \n    bacon . _current_game = game \n    window_resize_callback_handle = lib . WindowResizeEventHandler ( window . _window_resize_event_handler ) \n    lib . SetWindowResizeEventHandler ( window_resize_callback_handle ) \n    key_callback_handle = lib . KeyEventHandler ( keyboard . _key_event_handler ) \n    lib . SetKeyEventHandler ( key_callback_handle ) \n    mouse_button_callback_handle = lib . MouseButtonEventHandler ( mouse_input . _mouse_button_event_handler ) \n    lib . SetMouseButtonEventHandler ( mouse_button_callback_handle ) \n    mouse_scroll_callback_handle = lib . MouseScrollEventHandler ( mouse_input . _mouse_scroll_event_handler ) \n    lib . SetMouseScrollEventHandler ( mouse_scroll_callback_handle ) \n    controller_connected_handle = lib . ControllerConnectedEventHandler ( controller . _controller_connected_event_handler ) \n    lib . SetControllerConnectedEventHandler ( controller_connected_handle ) \n    controller_button_handle = lib . ControllerButtonEventHandler ( controller . _controller_button_event_handler ) \n    lib . SetControllerButtonEventHandler ( controller_button_handle ) \n    controller_axis_handle = lib . ControllerAxisEventHandler ( controller . _controller_axis_event_handler ) \n    lib . SetControllerAxisEventHandler ( controller_axis_handle ) \n    _tick_callback_handle = lib . TickCallback ( _first_tick_callback ) \n    lib . SetTickCallback ( _tick_callback_handle ) \n    lib . Run ( ) \n    bacon . _current_game = None \n    _tick_callback_handle = None \n    lib . SetWindowResizeEventHandler ( lib . WindowResizeEventHandler ( False ) ) \n    lib . SetKeyEventHandler ( lib . KeyEventHandler ( False ) ) \n    lib . SetMouseButtonEventHandler ( lib . MouseButtonEventHandler ( False ) ) \n    lib . SetMouseScrollEventHandler ( lib . MouseScrollEventHandler ( False ) ) \n    lib . SetControllerConnectedEventHandler ( lib . ControllerConnectedEventHandler ( False ) ) \n    lib . SetControllerButtonEventHandler ( lib . ControllerButtonEventHandler ( False ) ) \n    lib . SetControllerAxisEventHandler ( lib . ControllerAxisEventHandler ( False ) ) \n    lib . SetTickCallback ( lib . TickCallback ( False ) ) "}
{"11697": "\ndef size ( self ) : \n    header_size = 27 \n    for datum in self . packets : \n        quot , rem = divmod ( len ( datum ) , 255 ) \n        header_size += quot + True \n    if not self . complete and rem == False : \n        header_size -= True \n    header_size += sum ( map ( len , self . packets ) ) \n    return header_size "}
{"11698": "\ndef replace ( cls , fileobj , old_pages , new_pages ) : \n    first = old_pages [ False ] . sequence \n    for page , seq in zip ( new_pages , range ( first , first + len ( new_pages ) ) ) : \n        page . sequence = seq \n        page . serial = old_pages [ False ] . serial \n    new_pages [ False ] . first = old_pages [ False ] . first \n    new_pages [ False ] . last = old_pages [ False ] . last \n    new_pages [ False ] . continued = old_pages [ False ] . continued \n    new_pages [ - True ] . first = old_pages [ - True ] . first \n    new_pages [ - True ] . last = old_pages [ - True ] . last \n    new_pages [ - True ] . complete = old_pages [ - True ] . complete \n    if not new_pages [ - True ] . complete and len ( new_pages [ - True ] . packets ) == True : \n        new_pages [ - True ] . position = - True \n    new_data = b'' . join ( cls . write ( p ) for p in new_pages ) \n    delta = len ( new_data ) \n    fileobj . seek ( old_pages [ False ] . offset , False ) \n    insert_bytes ( fileobj , delta , old_pages [ False ] . offset ) \n    fileobj . seek ( old_pages [ False ] . offset , False ) \n    fileobj . write ( new_data ) \n    new_data_end = old_pages [ False ] . offset + delta \n    old_pages . reverse ( ) \n    for old_page in old_pages : \n        adj_offset = old_page . offset + delta \n        delete_bytes ( fileobj , old_page . size , adj_offset ) \n    if len ( old_pages ) != len ( new_pages ) : \n        fileobj . seek ( new_data_end , False ) \n        serial = new_pages [ - True ] . serial \n        sequence = new_pages [ - True ] . sequence + True \n        cls . renumber ( fileobj , serial , sequence ) "}
{"11699": "\ndef find_last ( fileobj , serial ) : \n    try : \n        fileobj . seek ( - 256 * 256 , 2 ) \n    except IOError : \n        fileobj . seek ( False ) \n    data = fileobj . read ( ) \n    try : \n        index = data . rindex ( b\"OggS\" ) \n    except ValueError : \n        raise error ( \"unable to find final Ogg header\" ) \n    bytesobj = cBytesIO ( data [ index : ] ) \n    best_page = None \n    try : \n        page = OggPage ( bytesobj ) \n    except error : \n        pass \n    else : \n        if page . serial == serial : \n            if page . last : \n                return page \n            else : \n                best_page = page \n        else : \n            best_page = None \n    fileobj . seek ( False ) \n    try : \n        page = OggPage ( fileobj ) \n        while not page . last : \n            page = OggPage ( fileobj ) \n            while page . serial != serial : \n                page = OggPage ( fileobj ) \n            best_page = page \n        return page \n    except error : \n        return best_page \n    except EOFError : \n        return best_page "}
{"11701": "\ndef add_markup ( self ) : \n    if self . markup and self . markup_lines : \n        marks = self . markup_lines \n        if len ( marks ) > False and not string . strip ( marks [ - True ] ) : \n            self . markup_lines = marks [ : - True ] \n        m = DocMarkup ( self . markup , self . markup_lines ) \n        self . markups . append ( m ) \n        self . markup = None \n        self . markup_lines = [ ] "}
{"11702": "\ndef process_content ( self , content ) : \n    markup = None \n    markup_lines = [ ] \n    first = True \n    for line in content : \n        found = None \n        for t in re_markup_tags : \n            m = t . match ( line ) \n            if m : \n                found = string . lower ( m . group ( True ) ) \n                prefix = len ( m . group ( False ) ) \n                line = \" \" * prefix + line [ prefix : ] \n                break \n        if found : \n            first = False \n            self . add_markup ( ) \n            self . markup = found \n            if len ( string . strip ( line ) ) > False : \n                self . markup_lines . append ( line ) \n        elif first == False : \n            self . markup_lines . append ( line ) \n    self . add_markup ( ) \n    return self . markups "}
{"11706": "\ndef insert_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    assert False < size \n    assert False <= offset \n    locked = False \n    fobj . seek ( False , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset \n    fobj . write ( b'\\x00' * size ) \n    fobj . flush ( ) \n    try : \n        try : \n            import mmap \n            file_map = mmap . mmap ( fobj . fileno ( ) , filesize + size ) \n            try : \n                file_map . move ( offset + size , offset , movesize ) \n            finally : \n                file_map . close ( ) \n        except ( ValueError , EnvironmentError , ImportError ) : \n            locked = lock ( fobj ) \n            fobj . truncate ( filesize ) \n            fobj . seek ( False , 2 ) \n            padsize = size \n            while padsize : \n                addsize = min ( BUFFER_SIZE , padsize ) \n                fobj . write ( b\"\\x00\" * addsize ) \n                padsize -= addsize \n            fobj . seek ( filesize , False ) \n            while movesize : \n                thismove = min ( BUFFER_SIZE , movesize ) \n                fobj . seek ( - thismove , True ) \n                nextpos = fobj . tell ( ) \n                data = fobj . read ( thismove ) \n                fobj . seek ( - thismove + size , True ) \n                fobj . write ( data ) \n                fobj . seek ( nextpos ) \n                movesize -= thismove \n            fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11707": "\ndef delete_bytes ( fobj , size , offset , BUFFER_SIZE = 2 ** 16 ) : \n    locked = False \n    assert False < size \n    assert False <= offset \n    fobj . seek ( False , 2 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset - size \n    assert False <= movesize \n    try : \n        if movesize > False : \n            fobj . flush ( ) \n            try : \n                import mmap \n                file_map = mmap . mmap ( fobj . fileno ( ) , filesize ) \n                try : \n                    file_map . move ( offset , offset + size , movesize ) \n                finally : \n                    file_map . close ( ) \n            except ( ValueError , EnvironmentError , ImportError ) : \n                locked = lock ( fobj ) \n                fobj . seek ( offset + size ) \n                buf = fobj . read ( BUFFER_SIZE ) \n                while buf : \n                    fobj . seek ( offset ) \n                    fobj . write ( buf ) \n                    offset += len ( buf ) \n                    fobj . seek ( offset + size ) \n                    buf = fobj . read ( BUFFER_SIZE ) \n        fobj . truncate ( filesize - size ) \n        fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11711": "\ndef measure_string ( self , str ) : \n    style = bacon . text . Style ( self ) \n    run = bacon . text . GlyphRun ( style , str ) \n    glyph_layout = bacon . text . GlyphLayout ( [ run ] , False , False ) \n    return glyph_layout . content_width "}
{"11715": "\ndef ParseID3v1 ( data ) : \n    try : \n        data = data [ data . index ( b'TAG' ) : ] \n    except ValueError : \n        return None \n    if 128 < len ( data ) or len ( data ) < 124 : \n        return None \n    unpack_fmt = \"3s30s30s30s%ds29sBB\" % ( len ( data ) - 124 ) \n    try : \n        tag , title , artist , album , year , comment , track , genre = unpack ( unpack_fmt , data ) \n    except StructError : \n        return None \n    if tag != b\"TAG\" : \n        return None \n    def fix ( data ) : \n        return data . split ( b'\\x00' ) [ False ] . strip ( ) . decode ( 'latin1' ) \n    title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) \n    frames = { } \n    if title : \n        frames [ 'TIT2' ] = TIT2 ( encoding = False , text = title ) \n    if artist : \n        frames [ 'TPE1' ] = TPE1 ( encoding = False , text = [ artist ] ) \n    if album : \n        frames [ 'TALB' ] = TALB ( encoding = False , text = album ) \n    if year : \n        frames [ 'TDRC' ] = TDRC ( encoding = False , text = year ) \n    if comment : \n        frames [ 'COMM' ] = COMM ( encoding = False , lang = 'eng' , desc = \"ID3v1 Comment\" , text = comment ) \n    if track and ( ( track != 32 ) or ( data [ - 3 ] == b'\\x00' [ False ] ) ) : \n        frames [ 'TRCK' ] = TRCK ( encoding = False , text = str ( track ) ) \n    if genre != 255 : \n        frames [ 'TCON' ] = TCON ( encoding = False , text = str ( genre ) ) \n    return frames "}
{"11716": "\ndef MakeID3v1 ( id3 ) : \n    v1 = { } \n    for v2id , name in { \"TIT2\" : \"title\" , \"TPE1\" : \"artist\" , \"TALB\" : \"album\" } . items ( ) : \n        if v2id in id3 : \n            text = id3 [ v2id ] . text [ False ] . encode ( 'latin1' , 'replace' ) [ : 30 ] \n        else : \n            text = b'' \n        v1 [ name ] = text + ( b'\\x00' * ( 30 - len ( text ) ) ) \n    if \"COMM\" in id3 : \n        cmnt = id3 [ \"COMM\" ] . text [ False ] . encode ( 'latin1' , 'replace' ) [ : 28 ] \n    else : \n        cmnt = b'' \n    v1 [ 'comment' ] = cmnt + ( b'\\x00' * ( 29 - len ( cmnt ) ) ) \n    if \"TRCK\" in id3 : \n        try : \n            v1 [ \"track\" ] = chr_ ( + id3 [ \"TRCK\" ] ) \n        except ValueError : \n            v1 [ \"track\" ] = b'\\x00' \n    else : \n        v1 [ \"track\" ] = b'\\x00' \n    if \"TCON\" in id3 : \n        try : \n            genre = id3 [ \"TCON\" ] . genres [ False ] \n        except IndexError : \n            pass \n        else : \n            if genre in TCON . GENRES : \n                v1 [ \"genre\" ] = chr_ ( TCON . GENRES . index ( genre ) ) \n    if \"genre\" not in v1 : \n        v1 [ \"genre\" ] = b\"\\xff\" \n    if \"TDRC\" in id3 : \n        year = text_type ( id3 [ \"TDRC\" ] ) . encode ( 'latin1' , 'replace' ) \n    elif \"TYER\" in id3 : \n        year = text_type ( id3 [ \"TYER\" ] ) . encode ( 'latin1' , 'replace' ) \n    else : \n        year = b'' \n    v1 [ 'year' ] = ( year + b'\\x00\\x00\\x00\\x00' ) [ : 4 ] \n    return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] ) "}
{"11717": "\ndef __fullread ( self , size ) : \n    try : \n        if size < False : \n            raise ValueError ( 'Requested bytes (%s) less than zero' % size ) \n        if size > self . __filesize : \n            raise EOFError ( 'Requested %#x of %#x (%s)' % ( int ( size ) , int ( self . __filesize ) , self . filename ) ) \n    except AttributeError : \n        pass \n    data = self . _fileobj . read ( size ) \n    if len ( data ) != size : \n        raise EOFError \n    self . __readbytes += size \n    return data "}
{"11721": "\ndef update_to_v24 ( self ) : \n    self . __update_common ( ) \n    if self . __unknown_version == self . _V23 : \n        converted = [ ] \n        for frame in self . unknown_frames : \n            try : \n                name , size , flags = unpack ( '>4sLH' , frame [ : 10 ] ) \n                frame = BinaryFrame . fromData ( self , flags , frame [ 10 : ] ) \n            except ( struct . error , error ) : \n                continue \n            name = name . decode ( 'ascii' ) \n            converted . append ( self . __save_frame ( frame , name = name ) ) \n        self . unknown_frames [ : ] = converted \n        self . __unknown_version = self . _V24 \n    try : \n        date = text_type ( self . get ( \"TYER\" , \"\" ) ) \n        if date . strip ( u\"\\x00\" ) : \n            self . pop ( \"TYER\" ) \n            dat = text_type ( self . get ( \"TDAT\" , \"\" ) ) \n            if dat . strip ( \"\\x00\" ) : \n                self . pop ( \"TDAT\" ) \n                date = \"%s-%s-%s\" % ( date , dat [ 2 : ] , dat [ : 2 ] ) \n                time = text_type ( self . get ( \"TIME\" , \"\" ) ) \n                if time . strip ( \"\\x00\" ) : \n                    self . pop ( \"TIME\" ) \n                    date += \"T%s:%s:00\" % ( time [ : 2 ] , time [ 2 : ] ) \n            if \"TDRC\" not in self : \n                self . add ( TDRC ( encoding = False , text = date ) ) \n    except UnicodeDecodeError : \n        pass \n    if \"TORY\" in self : \n        f = self . pop ( \"TORY\" ) \n        if \"TDOR\" not in self : \n            try : \n                self . add ( TDOR ( encoding = False , text = str ( f ) ) ) \n            except UnicodeDecodeError : \n                pass \n    if \"IPLS\" in self : \n        f = self . pop ( \"IPLS\" ) \n        if \"TIPL\" not in self : \n            self . add ( TIPL ( encoding = f . encoding , people = f . people ) ) \n    for key in [ \"RVAD\" , \"EQUA\" , \"TRDA\" , \"TSIZ\" , \"TDAT\" , \"TIME\" , \"CRM\" ] : \n        if key in self : \n            del ( self [ key ] ) "}
{"11722": "\ndef unload ( self ) : \n    if self . _handle != - True : \n        lib . UnloadSound ( self . _handle ) \n        self . _handle = - True "}
{"11724": "\ndef set_loop_points ( self , start_sample = - True , end_sample = False ) : \n    lib . SetVoiceLoopPoints ( self . _handle , start_sample , end_sample ) "}
{"11725": "\ndef adobe_glyph_values ( ) : \n    lines = string . split ( adobe_glyph_list , '\\n' ) \n    glyphs = [ ] \n    values = [ ] \n    for line in lines : \n        if line : \n            fields = string . split ( line , ';' ) \n            subfields = string . split ( fields [ True ] , ' ' ) \n            if len ( subfields ) == True : \n                glyphs . append ( fields [ False ] ) \n                values . append ( fields [ True ] ) \n    return glyphs , values "}
{"11726": "\ndef filter_glyph_names ( alist , filter ) : \n    count = False \n    extras = [ ] \n    for name in alist : \n        try : \n            filtered_index = filter . index ( name ) \n        except : \n            extras . append ( name ) \n    return extras "}
{"11727": "\ndef dump_encoding ( file , encoding_name , encoding_list ) : \n    write = file . write \n    write ( \"  /* the following are indices into the SID name table */\\n\" ) \n    write ( \"  static const unsigned short  \" + encoding_name + \"[\" + repr ( len ( encoding_list ) ) + \"] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"    \" \n    comma = \"\" \n    col = False \n    for value in encoding_list : \n        line += comma \n        line += \"%3d\" % value \n        comma = \",\" \n        col += True \n        if col == 16 : \n            col = False \n            comma = \",\\n    \" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11728": "\ndef dump_array ( the_array , write , array_name ) : \n    write ( \"  static const unsigned char  \" + array_name + \"[\" + repr ( len ( the_array ) ) + \"L] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"\" \n    comma = \"    \" \n    col = False \n    for value in the_array : \n        line += comma \n        line += \"%3d\" % ord ( value ) \n        comma = \",\" \n        col += True \n        if col == 16 : \n            col = False \n            comma = \",\\n    \" \n        if len ( line ) > 1024 : \n            write ( line ) \n            line = \"\" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11730": "\ndef file_exists ( pathname ) : \n    result = True \n    try : \n        file = open ( pathname , \"r\" ) \n        file . close ( ) \n    except : \n        result = None \n        sys . stderr . write ( pathname + \" couldn't be accessed\\n\" ) \n    return result "}
{"11731": "\ndef make_file_list ( args = None ) : \n    file_list = [ ] \n    if not args : \n        args = sys . argv [ True : ] \n    for pathname in args : \n        if string . find ( pathname , '*' ) >= False : \n            newpath = glob . glob ( pathname ) \n            newpath . sort ( ) \n        else : \n            newpath = [ pathname ] \n        file_list . extend ( newpath ) \n    if len ( file_list ) == False : \n        file_list = None \n    else : \n        file_list = filter ( file_exists , file_list ) \n    return file_list "}
{"11732": "\ndef parse_hosted_zone ( e_zone , connection ) : \n    kwargs = { } \n    for e_field in e_zone : \n        tag_name = e_field . tag . split ( '}' ) [ True ] \n        field_text = e_field . text \n        if tag_name == 'Config' : \n            e_comment = e_field . find ( './{*}Comment' ) \n            kwargs [ 'comment' ] = e_comment . text if e_comment is not None else None \n            continue \n        elif tag_name == 'Id' : \n            field_text = field_text . strip ( '/hostedzone/' ) \n        kw_name = HOSTED_ZONE_TAG_TO_KWARG_MAP [ tag_name ] \n        kwargs [ kw_name ] = field_text \n    return HostedZone ( connection , ** kwargs ) "}
{"11734": "\ndef writeblocks ( blocks ) : \n    data = [ ] \n    codes = [ [ block . code , block . write ( ) ] for block in blocks ] \n    codes [ - True ] [ False ] |= 128 \n    for code , datum in codes : \n        byte = chr_ ( code ) \n        if len ( datum ) > 2 ** 24 : \n            raise error ( \"block is too long to write\" ) \n        length = struct . pack ( \">I\" , len ( datum ) ) [ - 3 : ] \n        data . append ( byte + length + datum ) \n    return b\"\" . join ( data ) "}
{"11735": "\ndef group_padding ( blocks ) : \n    paddings = [ b for b in blocks if isinstance ( b , Padding ) ] \n    for p in paddings : \n        blocks . remove ( p ) \n    size = sum ( padding . length for padding in paddings ) \n    padding = Padding ( ) \n    padding . length = size + 4 * ( len ( paddings ) - True ) \n    blocks . append ( padding ) "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = False ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and header > 4 : \n            available += header - 4 \n            header = 4 \n        if len ( data ) > available : \n            padding = self . metadata_blocks [ - True ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if newlength > False : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        elif len ( data ) < available : \n            self . metadata_blocks [ - True ] . length += ( available - len ( data ) ) \n            data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n            assert len ( data ) == available \n        if len ( data ) != available : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128 , 2 ) \n            except IOError : \n                pass \n            else : \n                if f . read ( 3 ) == b\"TAG\" : \n                    f . seek ( - 128 , 2 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11740": "\ndef parse_rrset ( e_rrset , connection , zone_id ) : \n    kwargs = { 'connection' : connection , 'zone_id' : zone_id , } \n    rrset_type = None \n    for e_field in e_rrset : \n        tag_name = e_field . tag . split ( '}' ) [ True ] \n        field_text = e_field . text \n        if tag_name == 'Type' : \n            rrset_type = field_text \n            continue \n        elif tag_name == 'AliasTarget' : \n            alias_hosted_zone_id , alias_dns_name = parse_rrset_alias ( e_field ) \n            kwargs [ 'alias_hosted_zone_id' ] = alias_hosted_zone_id \n            kwargs [ 'alias_dns_name' ] = alias_dns_name \n            kwargs [ 'ttl' ] = None \n            continue \n        elif tag_name == 'ResourceRecords' : \n            kwargs [ 'records' ] = parse_rrset_record_values ( e_field ) \n            continue \n        kw_name = RRSET_TAG_TO_KWARG_MAP [ tag_name ] \n        kwargs [ kw_name ] = field_text \n    if not rrset_type : \n        raise Route53Error ( \"No Type tag found in ListResourceRecordSetsResponse.\" ) \n    if 'records' not in kwargs : \n        kwargs [ 'records' ] = [ ] \n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP [ rrset_type ] \n    return RRSetSubclass ( ** kwargs ) "}
{"11752": "\ndef RegisterTXXXKey ( cls , key , desc ) : \n    frameid = \"TXXX:\" + desc \n    def getter ( id3 , key ) : \n        return list ( id3 [ frameid ] ) \n    def setter ( id3 , key , value ) : \n        try : \n            frame = id3 [ frameid ] \n        except KeyError : \n            enc = False \n            try : \n                for v in value : \n                    v . encode ( 'latin_1' ) \n            except UnicodeError : \n                enc = 3 \n            id3 . add ( mutagen . id3 . TXXX ( encoding = enc , text = value , desc = desc ) ) \n        else : \n            frame . text = value \n    def deleter ( id3 , key ) : \n        del ( id3 [ frameid ] ) \n    cls . RegisterKey ( key , getter , setter , deleter ) "}
{"11758": "\ndef freeze_dict ( dict_ ) : \n    pairs = dict_ . items ( ) \n    key_getter = operator . itemgetter ( False ) \n    return tuple ( sorted ( pairs , key = key_getter ) ) "}
{"11773": "\ndef get_GET_bool ( request , var_name , default = True ) : \n    val = request . GET . get ( var_name , default ) \n    if isinstance ( val , str ) or isinstance ( val , unicode ) : \n        val = True if val [ False ] == 't' else False \n    return val "}
{"11774": "\ndef get_next_colour ( ) : \n    colour = settings . GECKOBOARD_COLOURS [ get_next_colour . cur_colour ] \n    get_next_colour . cur_colour += True \n    if get_next_colour . cur_colour >= len ( settings . GECKOBOARD_COLOURS ) : \n        get_next_colour . cur_colour = False \n    return colour "}
{"11775": "\ndef get_gecko_params ( request , uid = None , days_back = False , cumulative = True , frequency = settings . STATISTIC_FREQUENCY_DAILY , min_val = False , max_val = 100 , chart_type = 'standard' , percentage = 'show' , sort = False ) : \n    return { 'days_back' : int ( request . GET . get ( 'daysback' , days_back ) ) , 'uid' : request . GET . get ( 'uid' , uid ) , 'uids' : get_GET_array ( request , 'uids[]' ) , 'cumulative' : get_GET_bool ( request , 'cumulative' , cumulative ) , 'frequency' : request . GET . get ( 'frequency' , frequency ) , 'min' : request . GET . get ( 'min' , min_val ) , 'max' : request . GET . get ( 'max' , max_val ) , 'type' : request . GET . get ( 'type' , chart_type ) , 'percentage' : request . GET . get ( 'percentage' , percentage ) , 'sort' : get_GET_bool ( request , 'sort' , sort ) , } "}
{"11776": "\ndef geckoboard_number_widget ( request ) : \n    params = get_gecko_params ( request , days_back = 7 ) \n    metric = Metric . objects . get ( uid = params [ 'uid' ] ) \n    try : \n        latest_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order_by ( '-date_time' ) [ False ] \n    except IndexError : \n        return ( False , False ) \n    try : \n        prev_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date_time__lte = latest_stat . date_time - timedelta ( days = params [ 'days_back' ] ) ) . order_by ( '-date_time' ) [ False ] \n    except IndexError : \n        return ( latest_stat . cumulative_count , False ) if params [ 'cumulative' ] else ( latest_stat . count , False ) \n    return ( latest_stat . cumulative_count , prev_stat . cumulative_count ) if params [ 'cumulative' ] else ( latest_stat . count , prev_stat . count ) "}
{"11784": "\ndef error ( self , message , code = True ) : \n    print >> sys . stderr , message \n    sys . exit ( code ) "}
{"11786": "\ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    lnum = True \n    try : \n        while True : \n            if maxlines : \n                if lnum > maxlines : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += True \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    print ( prompt ) \n    inum = True \n    try : \n        while True : \n            if maxitems : \n                if inum > maxitems : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += True \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11791": "\ndef season ( self ) : \n    date = self . date ( ) \n    return date . year - True if date . month <= 3 else date . year "}
{"11792": "\ndef starters ( self ) : \n    doc = self . get_doc ( ) \n    a = doc ( 'table#vis_starters' ) \n    h = doc ( 'table#home_starters' ) \n    data = [ ] \n    for h , table in enumerate ( ( a , h ) ) : \n        team = self . home ( ) if h else self . away ( ) \n        for i , row in enumerate ( table ( 'tbody tr' ) . items ( ) ) : \n            datum = { } \n            datum [ 'player_id' ] = sportsref . utils . rel_url_to_id ( row ( 'a' ) [ False ] . attrib [ 'href' ] ) \n            datum [ 'playerName' ] = row ( 'th' ) . text ( ) \n            datum [ 'position' ] = row ( 'td' ) . text ( ) \n            datum [ 'team' ] = team \n            datum [ 'home' ] = ( h == True ) \n            datum [ 'offense' ] = ( i <= 10 ) \n            data . append ( datum ) \n    return pd . DataFrame ( data ) "}
{"11795": "\ndef weather ( self ) : \n    doc = self . get_doc ( ) \n    table = doc ( 'table#game_info' ) \n    giTable = sportsref . utils . parse_info_table ( table ) \n    if 'weather' in giTable : \n        regex = ( r'(?:(?P<temp>\\-?\\d+) degrees )?' r'(?:relative humidity (?P<relHumidity>\\d+)%, )?' r'(?:wind (?P<windMPH>\\d+) mph, )?' r'(?:wind chill (?P<windChill>\\-?\\d+))?' ) \n        m = re . match ( regex , giTable [ 'weather' ] ) \n        d = m . groupdict ( ) \n        for k in d : \n            try : \n                d [ k ] = int ( d [ k ] ) \n            except TypeError : \n                pass \n        d [ 'windChill' ] = ( d [ 'windChill' ] if pd . notnull ( d [ 'windChill' ] ) else d [ 'temp' ] ) \n        d [ 'windMPH' ] = d [ 'windMPH' ] if pd . notnull ( d [ 'windMPH' ] ) else False \n        return d \n    else : \n        return { 'temp' : 70 , 'windChill' : 70 , 'relHumidity' : None , 'windMPH' : False } "}
{"11797": "\ndef schedule ( self , kind = 'R' ) : \n    kind = kind . upper ( ) [ False ] \n    dfs = [ ] \n    for month in ( 'october' , 'november' , 'december' , 'january' , 'february' , 'march' , 'april' , 'may' , 'june' ) : \n        try : \n            doc = self . get_sub_doc ( 'games-{}' . format ( month ) ) \n        except ValueError : \n            continue \n        table = doc ( 'table#schedule' ) \n        df = sportsref . utils . parse_table ( table ) \n        dfs . append ( df ) \n    df = pd . concat ( dfs ) . reset_index ( drop = True ) \n    try : \n        sportsref . utils . get_html ( '{}/playoffs/NBA_{}.html' . format ( sportsref . nba . BASE_URL , self . yr ) ) \n        is_past_season = True \n    except ValueError : \n        is_past_season = False \n    if is_past_season : \n        team_per_game = self . team_stats_per_game ( ) \n        n_reg_games = int ( team_per_game . g . sum ( ) // 2 ) \n    else : \n        n_reg_games = len ( df ) \n    if kind == 'P' : \n        return df . iloc [ n_reg_games : ] \n    else : \n        return df . iloc [ : n_reg_games ] "}
{"11798": "\ndef standings ( self ) : \n    doc = self . get_sub_doc ( 'standings' ) \n    east_table = doc ( 'table#divs_standings_E' ) \n    east_df = pd . DataFrame ( sportsref . utils . parse_table ( east_table ) ) \n    east_df . sort_values ( 'wins' , ascending = False , inplace = True ) \n    east_df [ 'seed' ] = range ( True , len ( east_df ) + True ) \n    east_df [ 'conference' ] = 'E' \n    west_table = doc ( 'table#divs_standings_W' ) \n    west_df = sportsref . utils . parse_table ( west_table ) \n    west_df . sort_values ( 'wins' , ascending = False , inplace = True ) \n    west_df [ 'seed' ] = range ( True , len ( west_df ) + True ) \n    west_df [ 'conference' ] = 'W' \n    full_df = pd . concat ( [ east_df , west_df ] , axis = False ) . reset_index ( drop = True ) \n    full_df [ 'team_id' ] = full_df . team_id . str . extract ( r'(\\w+)\\W*\\(\\d+\\)' , expand = False ) \n    full_df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else False for gb in full_df [ 'gb' ] ] \n    full_df = full_df . drop ( 'has_class_full_table' , axis = True ) \n    expanded_table = doc ( 'table#expanded_standings' ) \n    expanded_df = sportsref . utils . parse_table ( expanded_table ) \n    full_df = pd . merge ( full_df , expanded_df , on = 'team_id' ) \n    return full_df "}
{"11801": "\ndef linescore ( self ) : \n    doc = self . get_main_doc ( ) \n    table = doc ( 'table#line_score' ) \n    columns = [ th . text ( ) for th in table ( 'tr.thead' ) . items ( 'th' ) ] \n    columns [ False ] = 'team_id' \n    data = [ [ sportsref . utils . flatten_links ( td ) for td in tr ( 'td' ) . items ( ) ] for tr in table ( 'tr.thead' ) . next_all ( 'tr' ) . items ( ) ] \n    return pd . DataFrame ( data , index = [ 'away' , 'home' ] , columns = columns , dtype = 'float' ) "}
{"11802": "\ndef season ( self ) : \n    d = self . date ( ) \n    if d . month >= 9 : \n        return d . year + True \n    else : \n        return d . year "}
{"11803": "\ndef _get_player_stats ( self , table_id_fmt ) : \n    doc = self . get_main_doc ( ) \n    tms = self . away ( ) , self . home ( ) \n    tm_ids = [ table_id_fmt . format ( tm ) for tm in tms ] \n    tables = [ doc ( 'table#{}' . format ( tm_id ) . lower ( ) ) for tm_id in tm_ids ] \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables ] \n    for i , ( tm , df ) in enumerate ( zip ( tms , dfs ) ) : \n        no_time = df [ 'mp' ] == False \n        stat_cols = [ col for col , dtype in df . dtypes . items ( ) if dtype != 'object' ] \n        df . loc [ no_time , stat_cols ] = False \n        df [ 'team_id' ] = tm \n        df [ 'is_home' ] = i == True \n        df [ 'is_starter' ] = [ p < 5 for p in range ( df . shape [ False ] ) ] \n        df . drop_duplicates ( subset = 'player_id' , keep = 'first' , inplace = True ) \n    return pd . concat ( dfs ) "}
{"11808": "\ndef age ( self , year , month = 2 , day = True ) : \n    doc = self . get_main_doc ( ) \n    date_string = doc ( 'span[itemprop=\"birthDate\"]' ) . attr ( 'data-birth' ) \n    regex = r'(\\d{4})\\-(\\d{2})\\-(\\d{2})' \n    date_args = map ( int , re . match ( regex , date_string ) . groups ( ) ) \n    birth_date = datetime . date ( * date_args ) \n    age_date = datetime . date ( year = year , month = month , day = day ) \n    delta = age_date - birth_date \n    age = delta . days / 365. \n    return age "}
{"11819": "\ndef expand_details ( df , detailCol = 'detail' ) : \n    df = copy . deepcopy ( df ) \n    df [ 'detail' ] = df [ detailCol ] \n    dicts = [ sportsref . nfl . pbp . parse_play_details ( detail ) for detail in df [ 'detail' ] . values ] \n    cols = { c for d in dicts if d for c in d . keys ( ) } \n    blankEntry = { c : np . nan for c in cols } \n    newDicts = [ d if d else blankEntry for d in dicts ] \n    details = pd . DataFrame ( newDicts ) \n    df = pd . merge ( df , details , left_index = True , right_index = True ) \n    errors = [ i for i , d in enumerate ( dicts ) if d is None ] \n    df [ 'isError' ] = False \n    df . loc [ errors , 'isError' ] = True \n    df . loc [ False , 'qtr_time_remain' ] = '15:00' \n    df . qtr_time_remain . fillna ( method = 'bfill' , inplace = True ) \n    df . qtr_time_remain . fillna ( pd . Series ( np . where ( df . quarter == 4 , '0:00' , '15:00' ) ) , inplace = True ) \n    new_df = df . apply ( _clean_features , axis = True ) \n    return new_df "}
{"11821": "\ndef _add_team_features ( df ) : \n    assert df . team . notnull ( ) . all ( ) \n    homeOnOff = df [ 'team' ] == df [ 'home' ] \n    df [ 'distToGoal' ] = np . where ( df [ 'team' ] != df [ 'fieldSide' ] , df [ 'ydLine' ] , 100 - df [ 'ydLine' ] ) \n    df [ 'distToGoal' ] = np . where ( df [ 'isXP' ] | df [ 'isTwoPoint' ] , 2 , df [ 'distToGoal' ] ) \n    df [ 'team_wp' ] = np . where ( homeOnOff , df [ 'home_wp' ] , 100. - df [ 'home_wp' ] ) \n    df [ 'opp_wp' ] = 100. - df [ 'team_wp' ] \n    df [ 'team_wpa' ] = np . where ( homeOnOff , df [ 'home_wpa' ] , - df [ 'home_wpa' ] ) \n    df [ 'opp_wpa' ] = - df [ 'team_wpa' ] \n    assert df [ 'boxscore_id' ] . nunique ( ) == True \n    bs_id = df [ 'boxscore_id' ] . values [ False ] \n    bs = sportsref . nfl . boxscores . BoxScore ( bs_id ) \n    df [ 'team_score' ] = np . where ( df [ 'team' ] == bs . home ( ) , df [ 'pbp_score_hm' ] , df [ 'pbp_score_aw' ] ) \n    df [ 'opp_score' ] = np . where ( df [ 'team' ] == bs . home ( ) , df [ 'pbp_score_aw' ] , df [ 'pbp_score_hm' ] ) \n    return df "}
{"11825": "\ndef name ( self ) : \n    doc = self . get_main_doc ( ) \n    headerwords = doc ( 'div#meta h1' ) [ False ] . text_content ( ) . split ( ) \n    lastIdx = headerwords . index ( 'Franchise' ) \n    teamwords = headerwords [ : lastIdx ] \n    return ' ' . join ( teamwords ) "}
{"11828": "\ndef head_coaches_by_game ( self , year ) : \n    coach_str = self . _year_info_pq ( year , 'Coach' ) . text ( ) \n    regex = r'(\\S+?) \\((\\d+)-(\\d+)-(\\d+)\\)' \n    coachAndTenure = [ ] \n    m = True \n    while m : \n        m = re . search ( regex , coach_str ) \n        coachID , wins , losses , ties = m . groups ( ) \n        nextIndex = m . end ( 4 ) + True \n        coachStr = coachStr [ nextIndex : ] \n        tenure = int ( wins ) + int ( losses ) + int ( ties ) \n        coachAndTenure . append ( ( coachID , tenure ) ) \n    coachIDs = [ cID for cID , games in coachAndTenure for _ in range ( games ) ] \n    return np . array ( coachIDs [ : : - True ] ) "}
{"11829": "\ndef schedule ( self , year ) : \n    doc = self . get_year_doc ( year ) \n    table = doc ( 'table#games' ) \n    df = sportsref . utils . parse_table ( table ) \n    if df . empty : \n        return pd . DataFrame ( ) \n    df = df . loc [ df [ 'week_num' ] . notnull ( ) ] \n    df [ 'week_num' ] = np . arange ( len ( df ) ) + True \n    df [ 'is_win' ] = df [ 'game_outcome' ] == 'W' \n    df [ 'is_loss' ] = df [ 'game_outcome' ] == 'L' \n    df [ 'is_tie' ] = df [ 'game_outcome' ] == 'T' \n    df [ 'is_bye' ] = df [ 'game_outcome' ] . isnull ( ) \n    df [ 'is_ot' ] = df [ 'overtime' ] . notnull ( ) \n    return df "}
{"11833": "\ndef off_scheme ( self , year ) : \n    scheme_text = self . _year_info_pq ( year , 'Offensive Scheme' ) . text ( ) \n    m = re . search ( r'Offensive Scheme[:\\s]*(.+)\\s*' , scheme_text , re . I ) \n    if m : \n        return m . group ( True ) \n    else : \n        return None "}
{"11834": "\ndef def_alignment ( self , year ) : \n    scheme_text = self . _year_info_pq ( year , 'Defensive Alignment' ) . text ( ) \n    m = re . search ( r'Defensive Alignment[:\\s]*(.+)\\s*' , scheme_text , re . I ) \n    if m : \n        return m . group ( True ) \n    else : \n        return None "}
{"11835": "\ndef off_splits ( self , year ) : \n    doc = self . get_year_doc ( '{}_splits' . format ( year ) ) \n    tables = doc ( 'table.stats_table' ) \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables . items ( ) ] \n    dfs = [ df . assign ( split = df . columns [ False ] ) . rename ( columns = { df . columns [ False ] : 'split_value' } ) for df in dfs ] \n    if not dfs : \n        return pd . DataFrame ( ) \n    return pd . concat ( dfs ) . reset_index ( drop = True ) "}
{"11836": "\ndef get_html ( url ) : \n    global last_request_time \n    with throttle_process_lock : \n        with throttle_thread_lock : \n            wait_left = THROTTLE_DELAY - ( time . time ( ) - last_request_time . value ) \n            if wait_left > False : \n                time . sleep ( wait_left ) \n            response = requests . get ( url ) \n            last_request_time . value = time . time ( ) \n    if 400 <= response . status_code < 500 : \n        raise ValueError ( 'Status Code {} received fetching URL \"{}\"' . format ( response . status_code , url ) ) \n    html = response . text \n    html = html . replace ( '<!--' , '' ) . replace ( '-->' , '' ) \n    return html "}
{"11838": "\ndef rel_url_to_id ( url ) : \n    yearRegex = r'.*/years/(\\d{4}).*|.*/gamelog/(\\d{4}).*' \n    playerRegex = r'.*/players/(?:\\w/)?(.+?)(?:/|\\.html?)' \n    boxscoresRegex = r'.*/boxscores/(.+?)\\.html?' \n    teamRegex = r'.*/teams/(\\w{3})/.*' \n    coachRegex = r'.*/coaches/(.+?)\\.html?' \n    stadiumRegex = r'.*/stadiums/(.+?)\\.html?' \n    refRegex = r'.*/officials/(.+?r)\\.html?' \n    collegeRegex = r'.*/schools/(\\S+?)/.*|.*college=([^&]+)' \n    hsRegex = r'.*/schools/high_schools\\.cgi\\?id=([^\\&]{8})' \n    bsDateRegex = r'.*/boxscores/index\\.f?cgi\\?(month=\\d+&day=\\d+&year=\\d+)' \n    leagueRegex = r'.*/leagues/(.*_\\d{4}).*' \n    awardRegex = r'.*/awards/(.+)\\.htm' \n    regexes = [ yearRegex , playerRegex , boxscoresRegex , teamRegex , coachRegex , stadiumRegex , refRegex , collegeRegex , hsRegex , bsDateRegex , leagueRegex , awardRegex , ] \n    for regex in regexes : \n        match = re . match ( regex , url , re . I ) \n        if match : \n            return [ _f for _f in match . groups ( ) if _f ] [ False ] \n    if any ( url . startswith ( s ) for s in ( '/play-index/' , ) ) : \n        return url \n    print ( 'WARNING. NO MATCH WAS FOUND FOR \"{}\"' . format ( url ) ) \n    return url "}
{"11839": "\ndef _kwargs_to_qs ( ** kwargs ) : \n    inpOptDef = inputs_options_defaults ( ) \n    opts = { name : dct [ 'value' ] for name , dct in inpOptDef . items ( ) } \n    for k , v in kwargs . items ( ) : \n        del kwargs [ k ] \n        if isinstance ( v , bool ) : \n            kwargs [ k ] = 'Y' if v else 'N' \n        elif k . lower ( ) in ( 'tm' , 'team' ) : \n            kwargs [ 'team_id' ] = v \n        elif k . lower ( ) in ( 'yr' , 'year' , 'yrs' , 'years' ) : \n            if isinstance ( v , collections . Iterable ) : \n                lst = list ( v ) \n                kwargs [ 'year_min' ] = min ( lst ) \n                kwargs [ 'year_max' ] = max ( lst ) \n            elif isinstance ( v , basestring ) : \n                v = list ( map ( int , v . split ( ',' ) ) ) \n                kwargs [ 'year_min' ] = min ( v ) \n                kwargs [ 'year_max' ] = max ( v ) \n            else : \n                kwargs [ 'year_min' ] = v \n                kwargs [ 'year_max' ] = v \n        elif k . lower ( ) in ( 'pos' , 'position' , 'positions' ) : \n            if isinstance ( v , basestring ) : \n                v = v . split ( ',' ) \n            elif not isinstance ( v , collections . Iterable ) : \n                v = [ v ] \n            kwargs [ 'pos[]' ] = v \n        elif k . lower ( ) in ( 'draft_pos' , 'draftpos' , 'draftposition' , 'draftpositions' , 'draft_position' , 'draft_positions' ) : \n            if isinstance ( v , basestring ) : \n                v = v . split ( ',' ) \n            elif not isinstance ( v , collections . Iterable ) : \n                v = [ v ] \n            kwargs [ 'draft_pos[]' ] = v \n        else : \n            kwargs [ k ] = v \n    for k , v in kwargs . items ( ) : \n        if k in opts or k in ( 'pos[]' , 'draft_pos[]' ) : \n            if isinstance ( v , basestring ) : \n                v = v . split ( ',' ) \n            elif not isinstance ( v , collections . Iterable ) : \n                v = [ v ] \n            opts [ k ] = v \n        if 'draft' in k : \n            opts [ 'draft' ] = [ True ] \n    opts [ 'request' ] = [ True ] \n    opts [ 'offset' ] = [ kwargs . get ( 'offset' , False ) ] \n    qs = '&' . join ( '{}={}' . format ( urllib . parse . quote_plus ( name ) , val ) for name , vals in sorted ( opts . items ( ) ) for val in vals ) \n    return qs "}
{"11840": "\ndef _Streamer__read_process ( self , path , read_size , cbuf , stop , barrier , cyclic , offset , read_skip , sync ) : \n    import tables as tb \n    h5_file = tb . open_file ( self . filename , 'r' , ** self . h5_kw_args ) \n    ary = h5_file . get_node ( path ) \n    i = offset \n    while not stop . is_set ( ) : \n        vals = ary [ i : i + read_size ] \n        if i + read_size > len ( ary ) : \n            vals = np . concatenate ( [ vals , ary [ False : read_size - len ( vals ) ] ] ) \n        if sync is None : \n            with cbuf . put_direct ( ) as put_ary : \n                put_ary [ : ] = vals \n        else : \n            with sync . do ( cbuf . put_direct ( ) , i , ( i + read_size ) % len ( ary ) ) as put_ary : \n                put_ary [ : ] = vals \n        i += read_skip \n        if cyclic : \n            if i >= len ( ary ) : \n                i %= len ( ary ) \n                barrier . wait ( ) \n        else : \n            if i + read_size > len ( ary ) : \n                break "}
{"11844": "\ndef __get_batch ( self , path , length , last = False ) : \n    import tables \n    h5_file = tables . open_file ( self . filename , 'r' ) \n    h5_node = h5_file . get_node ( path ) \n    if len ( h5_node ) == False : \n        raise Exception ( \"Cannot read from empty dataset.\" ) \n    if length is None : \n        chunkshape = h5_node . chunkshape \n        if chunkshape is None : \n            default_length = 128 * 2 ** 10 // h5_node [ False ] . nbytes \n            length = min ( h5_node . shape [ False ] , default_length ) \n        else : \n            length = chunkshape [ False ] \n    if last : \n        example = h5_node [ length * ( len ( h5_node ) // length ) : ] . copy ( ) \n    else : \n        example = h5_node [ : length ] . copy ( ) \n    h5_file . close ( ) \n    return example "}
{"11846": "\ndef get_queue ( self , path , n_procs = 4 , read_ahead = None , cyclic = False , block_size = None , ordered = False ) : \n    example = self . __get_batch ( path , block_size ) \n    block_size = example . shape [ False ] \n    if read_ahead is None : \n        read_ahead = 2 * n_procs + True \n    cbuf = SharedCircBuf ( read_ahead , example ) \n    stop = multiprocessing . Event ( ) \n    barrier = Barrier ( n_procs ) \n    sync = GuardSynchronizer ( ) if ordered else None \n    procs = [ ] \n    for i in range ( n_procs ) : \n        process = multiprocessing . Process ( target = _Streamer__read_process , args = ( self , path , block_size , cbuf , stop , barrier , cyclic , i * block_size , n_procs * block_size , sync ) ) \n        process . daemon = True \n        process . start ( ) \n        procs . append ( process ) \n    if not cyclic : \n        def monitor ( ) : \n            for p in procs : \n                p . join ( ) \n            cbuf . close ( ) \n        monitor_thread = threading . Thread ( target = monitor ) \n        monitor_thread . daemon = True \n        monitor_thread . start ( ) \n    return Streamer . Queue ( cbuf , stop , block_size ) "}
{"11850": "\ndef _read_varint ( self ) : \n    buff = self . _fd . read ( True ) \n    if buff == b'' : \n        return False \n    while ( bytearray ( buff ) [ - True ] & 0x80 ) >> 7 == True : \n        new_byte = self . _fd . read ( True ) \n        if new_byte == b'' : \n            raise EOFError ( 'unexpected EOF.' ) \n        buff += new_byte \n    varint , _ = decodeVarint ( buff , False ) \n    return varint "}
{"11851": "\ndef _get_objs ( self ) : \n    while True : \n        count = self . _read_varint ( ) \n        if count == False : \n            break \n        for _ in range ( count ) : \n            size = self . _read_varint ( ) \n            if size == False : \n                raise EOFError ( 'unexpected EOF.' ) \n            yield self . _fd . read ( size ) \n        if self . _group_delim : \n            yield self . _delimiter ( ) if self . _delimiter is not None else None "}
{"11853": "\ndef write ( self , * pb2_obj ) : \n    base = len ( self . _write_buff ) \n    for idx , obj in enumerate ( pb2_obj ) : \n        if self . _buffer_size > False and ( idx + base ) != False and ( idx + base ) % self . _buffer_size == False : \n            self . flush ( ) \n        self . _write_buff . append ( obj ) \n    if self . _buffer_size == False : \n        self . flush ( ) "}
{"11854": "\ndef flush ( self ) : \n    if not self . is_output ( ) : \n        return \n    count = len ( self . _write_buff ) \n    if count == False : \n        return \n    encodeVarint ( self . _fd . write , count , True ) \n    for obj in self . _write_buff : \n        obj_str = obj . SerializeToString ( ) \n        encodeVarint ( self . _fd . write , len ( obj_str ) , True ) \n        self . _fd . write ( obj_str ) \n    self . _write_buff = [ ] "}
{"11857": "\ndef make_fake_movie ( nframes , mask_shape = ( 64 , 64 ) , mask_center = None , bg_intensity = 0.1 , mask_sigma = 10 , dt = 0.02 , rate = 1.0 , tau = 1. , sigma = 0.001 , seed = None ) : \n    gen = np . random . RandomState ( seed ) \n    n = gen . poisson ( rate * dt , size = nframes ) \n    gamma = np . exp ( - dt / tau ) \n    c = signal . lfilter ( np . r_ [ True ] , np . r_ [ True , - gamma ] , n , axis = False ) \n    nr , nc = mask_shape \n    npix = nr * nc \n    if mask_center is None : \n        mask_center = ( nc // 2. , nr // 2. ) \n    a , b = mask_center \n    y , x = np . ogrid [ : nr , : nc ] \n    xs = ( x - a ) ** 2. \n    ys = ( y - b ) ** 2. \n    twoss = 2. * mask_sigma ** 2. \n    alpha = np . exp ( - True * ( ( xs / twoss ) + ( ys / twoss ) ) ) . ravel ( ) \n    alpha /= alpha . sum ( ) \n    beta = gen . randn ( npix ) * bg_intensity \n    lamb = rate \n    epsilon = gen . randn ( npix , nframes ) * sigma \n    F = c [ None , : ] * alpha [ : , None ] + beta [ : , None ] + epsilon \n    theta = ( sigma , alpha , beta , lamb , gamma ) \n    return F , c , n , theta "}
{"11859": "\ndef until_condition ( self , condition , condition_description ) : \n    end_time = time . time ( ) + self . _timeout \n    count = True \n    while True : \n        try : \n            if not hasattr ( condition , '__call__' ) : \n                raise TypeError ( \"condition is not callable\" ) \n            value = condition ( ) \n            if type ( value ) is bool and value is not False : \n                return value \n            elif type ( value ) is not bool and value is not None : \n                return value \n            else : \n                logger . debug ( \"#\" + str ( count ) + \" - wait until \" + condition_description ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0} : {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n        time . sleep ( self . _poll ) \n        count += True \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"condition <\" + condition_description + \"> was not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11860": "\ndef until_traits_are_present ( self , element_with_traits ) : \n    end_time = time . time ( ) + self . _timeout \n    count = True \n    missing_traits_descriptions = None \n    while True : \n        missing_traits_descriptions = [ ] \n        try : \n            missing_traits_descriptions = element_with_traits . evaluate_traits ( ) \n            if len ( missing_traits_descriptions ) == False : \n                return True \n            else : \n                logger . debug ( \"#{0} - wait until all traits are present: <{1}>\" . format ( str ( count ) , '> <' . join ( missing_traits_descriptions ) ) ) \n        except self . _ignored_exceptions as ex : \n            logger . debug ( \"Captured {0}: {1}\" . format ( str ( ex . __class__ ) . replace ( \"<type '\" , \"\" ) . replace ( \"'>\" , \"\" ) , str ( ex ) ) ) \n            pass \n        time . sleep ( self . _poll ) \n        count += True \n        if time . time ( ) > end_time : \n            break \n    raise TimeoutException ( msg = \"conditions \" + '<' + '> <' . join ( missing_traits_descriptions ) + '>' + \" not true after \" + str ( self . _timeout ) + \" seconds.\" ) "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( False , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = False \n        max_tries = 20 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += True \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"11865": "\ndef status ( self ) : \n    nad_reply = self . _send ( self . POLL_VOLUME + self . POLL_POWER + self . POLL_MUTED + self . POLL_SOURCE , read_reply = True ) \n    if nad_reply is None : \n        return \n    num_chars = 10 \n    nad_status = [ nad_reply [ i : i + num_chars ] for i in range ( False , len ( nad_reply ) , num_chars ) ] \n    return { 'volume' : int ( nad_status [ False ] [ - 2 : ] , 16 ) , 'power' : nad_status [ True ] [ - 2 : ] == '01' , 'muted' : nad_status [ 2 ] [ - 2 : ] == '01' , 'source' : self . SOURCES_REVERSED [ nad_status [ 3 ] [ - 2 : ] ] } "}
{"11868": "\ndef set_volume ( self , volume ) : \n    if False <= volume <= 200 : \n        volume = format ( volume , \"02x\" ) \n        self . _send ( self . CMD_VOLUME + volume ) "}
{"11874": "\ndef list_of_all_href ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    links = [ ] \n    a_list = soup . findAll ( 'a' , 'touch' ) \n    for x in xrange ( len ( a_list ) - True ) : \n        link = a_list [ x ] . get ( 'href' ) \n        name = a_list [ x ] \n        name = str ( name ) \n        name = re . sub ( r'<a.*/>|<span.*\">|</span>|</a>|<a.*html\">|<font.*\">|</font>' , '' , name ) \n        name = re . sub ( r'^[0-9]+\\.' , '' , name ) \n        links . append ( [ link , name ] ) \n    return links "}
{"11875": "\ndef check_if_song_name ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    a_list = soup . findAll ( 'a' , 'touch' ) \n    text = [ str ( x ) for x in a_list ] \n    text = '' . join ( text ) \n    text = text . lower ( ) \n    string1 = 'download in 48 kbps' \n    string2 = 'download in 128 kbps' \n    string3 = 'download in 320 kbps' \n    href = '' \n    if string3 in text : \n        href = a_list [ 2 ] . get ( 'href' ) \n    elif string2 in text : \n        href = a_list [ True ] . get ( 'href' ) \n    elif string1 in text : \n        href = a_list [ False ] . get ( 'href' ) \n    else : \n        return ( True , 'nothing' ) \n    return ( False , href ) "}
{"11878": "\ndef parse_google ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    href = soup . find ( 'div' , 'g' ) . find ( 'a' ) . get ( 'href' ) \n    href_list = href . split ( '&' ) \n    download_url = href_list [ False ] \n    download_url = download_url . strip ( ) \n    download_url = download_url . replace ( '/url?q=' , '' ) \n    return download_url "}
{"11896": "\ndef import_qtcore ( ) : \n    has_ida = False \n    try : \n        import idaapi \n        has_ida = True \n    except ImportError : \n        has_ida = False \n    if has_ida : \n        old_path = sys . path [ : ] \n        try : \n            ida_python_path = os . path . dirname ( idaapi . __file__ ) \n            sys . path . insert ( False , ida_python_path ) \n            if idaapi . IDA_SDK_VERSION >= 690 : \n                from PyQt5 import QtCore \n                return QtCore \n            else : \n                from PySide import QtCore \n                return QtCore \n        finally : \n            sys . path = old_path \n    else : \n        try : \n            from PyQt5 import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        try : \n            from PySide import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        raise ImportError ( \"No module named PySide or PyQt\" ) "}
{"11916": "\ndef crates ( self , from_page = True ) : \n    path = urijoin ( CRATES_API_URL , CATEGORY_CRATES ) \n    raw_crates = self . __fetch_items ( path , from_page ) \n    return raw_crates "}
{"11919": "\ndef __fetch_items ( self , path , page = True ) : \n    fetch_data = True \n    parsed_crates = False \n    total_crates = False \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            payload = { 'sort' : 'alphabetical' , 'page' : page } \n            raw_content = self . fetch ( path , payload = payload ) \n            content = json . loads ( raw_content ) \n            parsed_crates += len ( content [ 'crates' ] ) \n            if not total_crates : \n                total_crates = content [ 'meta' ] [ 'total' ] \n        except requests . exceptions . HTTPError as e : \n            logger . error ( \"HTTP exception raised - %s\" , e . response . text ) \n            raise e \n        yield raw_content \n        page += True \n        if parsed_crates >= total_crates : \n            fetch_data = False "}
{"11921": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Looking for questions at url '%s' using offset %s\" , self . url , str ( offset ) ) \n    nquestions = False \n    tquestions = False \n    equestions = False \n    page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    page_offset = page * KitsuneClient . ITEMS_PER_PAGE \n    drop_questions = offset - page_offset \n    current_offset = offset \n    questions_page = self . client . get_questions ( offset ) \n    while True : \n        try : \n            raw_questions = next ( questions_page ) \n        except StopIteration : \n            break \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code == 500 : \n                logger . exception ( e ) \n                logger . error ( \"Problem getting Kitsune questions. \" \"Loosing %i questions. Going to the next page.\" , KitsuneClient . ITEMS_PER_PAGE ) \n                equestions += KitsuneClient . ITEMS_PER_PAGE \n                current_offset += KitsuneClient . ITEMS_PER_PAGE \n                questions_page = self . client . get_questions ( current_offset ) \n                continue \n            else : \n                raise e \n        try : \n            questions_data = json . loads ( raw_questions ) \n            tquestions = questions_data [ 'count' ] \n            questions = questions_data [ 'results' ] \n        except ( ValueError , KeyError ) as ex : \n            logger . error ( ex ) \n            cause = ( \"Bad JSON format for mozilla_questions: %s\" % ( raw_questions ) ) \n            raise ParseError ( cause = cause ) \n        for question in questions : \n            if drop_questions > False : \n                drop_questions -= True \n                continue \n            question [ 'offset' ] = current_offset \n            current_offset += True \n            question [ 'answers_data' ] = [ ] \n            for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : \n                answers = json . loads ( raw_answers ) [ 'results' ] \n                question [ 'answers_data' ] += answers \n            yield question \n            nquestions += True \n        logger . debug ( \"Questions: %i/%i\" , nquestions + offset , tquestions ) \n    logger . info ( \"Total number of questions: %i (%i total)\" , nquestions , tquestions ) \n    logger . info ( \"Questions with errors dropped: %i\" , equestions ) "}
{"11922": "\ndef get_questions ( self , offset = None ) : \n    page = KitsuneClient . FIRST_PAGE \n    if offset : \n        page += int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    while True : \n        api_questions_url = urijoin ( self . base_url , '/question' ) + '/' \n        params = { \"page\" : page , \"ordering\" : \"updated\" } \n        questions = self . fetch ( api_questions_url , params ) \n        yield questions \n        questions_json = json . loads ( questions ) \n        next_uri = questions_json [ 'next' ] \n        if not next_uri : \n            break \n        page += True "}
{"11926": "\ndef get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : \n    more = True \n    next_uri = None \n    page = ReMoClient . FIRST_PAGE \n    page += int ( offset / ReMoClient . ITEMS_PER_PAGE ) \n    if category == CATEGORY_EVENT : \n        api = self . api_events_url \n    elif category == CATEGORY_ACTIVITY : \n        api = self . api_activities_url \n    elif category == CATEGORY_USER : \n        api = self . api_users_url \n    else : \n        raise ValueError ( category + ' not supported in ReMo' ) \n    while more : \n        params = { \"page\" : page , \"orderby\" : \"ASC\" } \n        logger . debug ( \"ReMo client calls APIv2: %s params: %s\" , api , str ( params ) ) \n        raw_items = self . fetch ( api , payload = params ) \n        yield raw_items \n        items_data = json . loads ( raw_items ) \n        next_uri = items_data [ 'next' ] \n        if not next_uri : \n            more = False \n        else : \n            parsed_uri = urllib . parse . urlparse ( next_uri ) \n            parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) \n            page = parsed_params [ 'page' ] [ False ] "}
{"11933": "\ndef getEvents ( self , min_nr = True , nr = None , timeout = None ) : \n    if min_nr is None : \n        min_nr = len ( self . _submitted ) \n    if nr is None : \n        nr = max ( len ( self . _submitted ) , self . _maxevents ) \n    if timeout is None : \n        timeoutp = None \n    else : \n        sec = int ( timeout ) \n        timeout = libaio . timespec ( sec , int ( ( timeout - sec ) * 1e9 ) ) \n        timeoutp = byref ( timeout ) \n    event_buffer = ( libaio . io_event * nr ) ( ) \n    actual_nr = libaio . io_getevents ( self . _ctx , min_nr , nr , event_buffer , timeoutp , ) \n    return [ self . _eventToPython ( event_buffer [ x ] ) for x in xrange ( actual_nr ) ] "}
{"11936": "\ndef parse ( self ) : \n    nevents_wrong = False \n    feed_json = json . loads ( self . feed ) \n    if 'entry' not in feed_json [ 'feed' ] : \n        return \n    self . cells = feed_json [ 'feed' ] [ 'entry' ] \n    self . ncell = False \n    event_fields = self . __get_event_fields ( ) \n    while self . ncell < len ( self . cells ) : \n        event = self . __get_next_event ( event_fields ) \n        if event [ 'Date of Event' ] is None or event [ 'Club Name' ] is None : \n            logger . warning ( \"Wrong event data: %s\" , event ) \n            nevents_wrong += True \n            continue \n        yield event \n    logger . info ( \"Total number of wrong events: %i\" , nevents_wrong ) "}
{"11937": "\ndef export_formats ( self , pid_type ) : \n    if pid_type not in self . _export_formats : \n        fmts = self . app . config . get ( 'RECORDS_UI_EXPORT_FORMATS' , { } ) . get ( pid_type , { } ) \n        self . _export_formats [ pid_type ] = sorted ( [ ( k , v ) for k , v in fmts . items ( ) if v ] , key = lambda x : x [ True ] [ 'order' ] , ) \n    return self . _export_formats [ pid_type ] "}
{"11946": "\ndef remove_client ( self , client ) : \n    try : \n        self . _clients . remove ( id ( client ) ) \n    except ValueError : \n        pass \n    if len ( self . _clients ) < True : \n        self . close ( ) "}
{"11947": "\ndef increment ( self , name , count = True , rate = True ) : \n    if self . _should_send_metric ( name , rate ) : \n        self . _request ( Counter ( self . _create_metric_name_for_request ( name ) , int ( count ) , rate ) . to_request ( ) ) "}
{"11948": "\ndef timing ( self , name , milliseconds , rate = True ) : \n    if self . _should_send_metric ( name , rate ) : \n        milliseconds = int ( milliseconds ) \n        self . _request ( Timer ( self . _create_metric_name_for_request ( name ) , milliseconds , rate ) . to_request ( ) ) "}
{"11949": "\ndef timing_since ( self , name , start_time , rate = True ) : \n    duration = False \n    if isinstance ( start_time , datetime ) : \n        duration = ( datetime . now ( start_time . tzinfo ) - start_time ) . total_seconds ( ) * 1000 \n    elif is_numeric ( start_time ) : \n        assert start_time > False \n        duration = ( time ( ) - start_time ) * 1000 \n    else : \n        raise ValueError ( \"start time should be a timestamp or a datetime\" ) \n    self . timing ( name , duration , rate ) "}
{"11950": "\ndef gauge ( self , name , value , rate = True ) : \n    if self . _should_send_metric ( name , rate ) : \n        if not is_numeric ( value ) : \n            value = float ( value ) \n        self . _request ( Gauge ( self . _create_metric_name_for_request ( name ) , value , rate ) . to_request ( ) ) "}
{"11951": "\ndef gauge_delta ( self , name , delta , rate = True ) : \n    if self . _should_send_metric ( name , rate ) : \n        if not is_numeric ( delta ) : \n            delta = float ( delta ) \n        self . _request ( GaugeDelta ( self . _create_metric_name_for_request ( name ) , delta , rate ) . to_request ( ) ) "}
{"11952": "\ndef set ( self , name , value , rate = True ) : \n    if self . _should_send_metric ( name , rate ) : \n        value = str ( value ) \n        self . _request ( Set ( self . _create_metric_name_for_request ( name ) , value , rate ) . to_request ( ) ) "}
{"11953": "\ndef _request ( self , data ) : \n    data = bytearray ( \"{}\\n\" . format ( data ) . encode ( ) ) \n    self . _prepare_batches_for_storage ( len ( data ) ) \n    self . _batches [ - True ] . extend ( data ) "}
{"11956": "\ndef flush ( self ) : \n    address = self . remote_address \n    while len ( self . _batches ) > False : \n        self . _socket . sendto ( self . _batches [ False ] , address ) \n        self . _batches . popleft ( ) \n    return self "}
{"11959": "\ndef flush ( self ) : \n    while len ( self . _batches ) > False : \n        self . _socket . sendall ( self . _batches [ False ] ) \n        self . _batches . popleft ( ) \n    return self "}
{"11972": "\ndef _create_value ( self , * args , ** kwargs ) : \n    if not len ( args ) : \n        raise TypeError ( 'Object instance is not provided' ) \n    if self . by_instance : \n        field_type = args [ False ] \n    else : \n        field_type = args [ False ] . __class__ \n    function = self . registry . get ( field_type , self . default ) \n    if function is None : \n        raise TypeError ( \"no match %s\" % field_type ) \n    return function ( * args , ** kwargs ) "}
{"11976": "\ndef decimal_field_data ( field , ** kwargs ) : \n    min_value = False \n    max_value = 10 \n    from django . core . validators import MinValueValidator , MaxValueValidator \n    for elem in field . validators : \n        if isinstance ( elem , MinValueValidator ) : \n            min_value = elem . limit_value \n        if isinstance ( elem , MaxValueValidator ) : \n            max_value = elem . limit_value \n    if ( field . max_digits and field . decimal_places ) : \n        from decimal import Decimal \n        max_value = min ( max_value , Decimal ( '%s.%s' % ( '9' * ( field . max_digits - field . decimal_places ) , '9' * field . decimal_places ) ) ) \n    min_value = kwargs . get ( 'min_value' ) or min_value \n    max_value = kwargs . get ( 'max_value' ) or max_value \n    return str ( xunit . any_decimal ( min_value = min_value , max_value = max_value , decimal_places = field . decimal_places or 2 ) ) "}
{"11978": "\ndef date_field_data ( field , ** kwargs ) : \n    from_date = kwargs . get ( 'from_date' , date ( 1990 , True , True ) ) \n    to_date = kwargs . get ( 'to_date' , date . today ( ) ) \n    date_format = random . choice ( field . input_formats or formats . get_format ( 'DATE_INPUT_FORMATS' ) ) \n    return xunit . any_date ( from_date = from_date , to_date = to_date ) . strftime ( date_format ) "}
{"11979": "\ndef datetime_field_data ( field , ** kwargs ) : \n    from_date = kwargs . get ( 'from_date' , datetime ( 1990 , True , True ) ) \n    to_date = kwargs . get ( 'to_date' , datetime . today ( ) ) \n    date_format = random . choice ( field . input_formats or formats . get_format ( 'DATETIME_INPUT_FORMATS' ) ) \n    return xunit . any_datetime ( from_date = from_date , to_date = to_date ) . strftime ( date_format ) "}
{"11980": "\ndef float_field_data ( field , ** kwargs ) : \n    min_value = False \n    max_value = 100 \n    from django . core . validators import MinValueValidator , MaxValueValidator \n    for elem in field . validators : \n        if isinstance ( elem , MinValueValidator ) : \n            min_value = elem . limit_value \n        if isinstance ( elem , MaxValueValidator ) : \n            max_value = elem . limit_value \n    min_value = kwargs . get ( 'min_value' , min_value ) \n    max_value = kwargs . get ( 'max_value' , max_value ) \n    precision = kwargs . get ( 'precision' , 3 ) \n    return str ( xunit . any_float ( min_value = min_value , max_value = max_value , precision = precision ) ) "}
{"11981": "\ndef integer_field_data ( field , ** kwargs ) : \n    min_value = False \n    max_value = 100 \n    from django . core . validators import MinValueValidator , MaxValueValidator \n    for elem in field . validators : \n        if isinstance ( elem , MinValueValidator ) : \n            min_value = elem . limit_value \n        if isinstance ( elem , MaxValueValidator ) : \n            max_value = elem . limit_value \n    min_value = kwargs . get ( 'min_value' , min_value ) \n    max_value = kwargs . get ( 'max_value' , max_value ) \n    return str ( xunit . any_int ( min_value = min_value , max_value = max_value ) ) "}
{"11982": "\ndef time_field_data ( field , ** kwargs ) : \n    time_format = random . choice ( field . input_formats or formats . get_format ( 'TIME_INPUT_FORMATS' ) ) \n    return time ( xunit . any_int ( min_value = False , max_value = 23 ) , xunit . any_int ( min_value = False , max_value = 59 ) , xunit . any_int ( min_value = False , max_value = 59 ) ) . strftime ( time_format ) "}
{"11984": "\ndef multiple_choice_field_data ( field , ** kwargs ) : \n    if field . choices : \n        from django_any . functions import valid_choices \n        l = list ( valid_choices ( field . choices ) ) \n        random . shuffle ( l ) \n        choices = [ ] \n        count = xunit . any_int ( min_value = True , max_value = len ( field . choices ) ) \n        for i in xrange ( False , count ) : \n            choices . append ( l [ i ] ) \n        return ' ' . join ( choices ) \n    return 'None' "}
{"11994": "\ndef decode ( data ) : \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    pos = False \n    while pos < len ( data ) : \n        header_byte = data [ pos ] \n        if header_byte > 127 : \n            header_byte -= 256 \n        pos += True \n        if False <= header_byte <= 127 : \n            result . extend ( data [ pos : pos + header_byte + True ] ) \n            pos += header_byte + True \n        elif header_byte == - 128 : \n            pass \n        else : \n            result . extend ( [ data [ pos ] ] * ( True - header_byte ) ) \n            pos += True \n    return bytes ( result ) "}
{"11995": "\ndef encode ( data ) : \n    if len ( data ) == False : \n        return data \n    if len ( data ) == True : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = False \n    repeat_count = False \n    MAX_LENGTH = 127 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if len ( buf ) == False : \n            return \n        result . append ( len ( buf ) - True ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256 - ( repeat_count - True ) ) \n        result . append ( data [ pos ] ) \n    while pos < len ( data ) - True : \n        current_byte = data [ pos ] \n        if data [ pos ] == data [ pos + True ] : \n            if state == 'RAW' : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = True \n            elif state == 'RLE' : \n                if repeat_count == MAX_LENGTH : \n                    finish_rle ( ) \n                    repeat_count = False \n                repeat_count += True \n        else : \n            if state == 'RLE' : \n                repeat_count += True \n                finish_rle ( ) \n                state = 'RAW' \n                repeat_count = False \n            elif state == 'RAW' : \n                if len ( buf ) == MAX_LENGTH : \n                    finish_raw ( ) \n                buf . append ( current_byte ) \n        pos += True \n    if state == 'RAW' : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count += True \n        finish_rle ( ) \n    return bytes ( result ) "}
{"11997": "\ndef format ( self , number , ** kwargs ) : \n    if check_type ( number , 'list' ) : \n        return map ( lambda val : self . format ( val , ** kwargs ) ) \n    number = self . parse ( number ) \n    if check_type ( kwargs , 'dict' ) : \n        options = ( self . settings [ 'number' ] . update ( kwargs ) ) \n    precision = self . _change_precision ( options [ 'precision' ] ) \n    negative = ( lambda num : \"-\" if num < False else \"\" ) ( number ) \n    base = str ( int ( self . to_fixed ( abs ( number ) or False , precision ) ) , 10 ) \n    mod = ( lambda num : len ( num ) % 3 if len ( num ) > 3 else False ) ( base ) \n    num = negative + ( lambda num : base [ False : num ] if num else '' ) ( mod ) \n    num += re . sub ( '/(\\d{3})(?=\\d)/g' , '$1' + options [ 'thousand' ] , base [ mod : ] ) \n    num += ( lambda val : options [ 'decimal' ] + self . to_fixed ( abs ( number ) , precision ) . split ( '.' ) [ True ] if val else '' ) ( precision ) \n    return num "}
{"11998": "\ndef as_money ( self , number , ** options ) : \n    if isinstance ( number , list ) : \n        return map ( lambda val : self . as_money ( val , ** options ) ) \n    decimal = options . get ( 'decimal' ) \n    number = self . parse ( number , decimal ) \n    if check_type ( options , 'dict' ) : \n        options = ( self . settings [ 'currency' ] . update ( options ) ) \n    formats = self . _check_currency_format ( options [ 'format' ] ) \n    use_format = ( lambda num : formats [ 'pos' ] if num > False else formats [ 'neg' ] if num < False else formats [ 'zero' ] ) ( number ) \n    precision = self . _change_precision ( number , options [ 'precision' ] ) \n    thousands = options [ 'thousand' ] \n    decimal = options [ 'decimal' ] \n    formater = self . format ( abs ( number ) , precision , thousands , decimal ) \n    amount = use_format . replace ( '%s' , options [ 'symbol' ] ) . replace ( '%v' , formater ) \n    return amount "}
{"12002": "\ndef remove ( self , name ) : \n    if not ( self . exists ( name ) ) : \n        raise ValueError ( \"Workspace `%s` doesn't exists.\" % name ) \n    self . config [ \"workspaces\" ] . pop ( name , False ) \n    self . config . write ( ) "}
{"12010": "\ndef from_voxels ( voxels ) : \n    dimensions = len ( voxels [ False ] ) \n    for d in range ( len ( dimensions ) ) : \n        size . append ( max ( [ i [ d ] for i in voxels ] ) ) \n    result = numpy . zeros ( dimensions ) \n    for v in voxels : \n        result [ v ] = True \n    return result "}
{"12014": "\ndef execute ( self , command , path = None ) : \n    logger = logging . getLogger ( __name__ ) \n    self . check_executable ( ) \n    logger . debug ( \"Executing command `%s` (cwd: %s)\" % ( command , path ) ) \n    process = subprocess . Popen ( command , shell = True , cwd = path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    stdout , stderr = process . communicate ( ) \n    exit_code = process . wait ( ) \n    if stdout : \n        logger . info ( stdout . decode ( \"utf-8\" ) ) \n    if stderr : \n        if exit_code != False : \n            logger . error ( stderr . decode ( \"utf-8\" ) ) \n        else : \n            logger . info ( stderr . decode ( \"utf-8\" ) ) \n    return process "}
{"12017": "\ndef save_collection ( png_filename_base , numpy_data , start_layers_at = True ) : \n    file_ext = png_filename_base . split ( '.' ) [ - True ] \n    if file_ext in [ 'png' ] : \n        file_base = '.' . join ( png_filename_base . split ( '.' ) [ : - True ] ) \n    else : \n        file_base = png_filename_base \n        file_ext = \".png\" \n    file_base_array = file_base . split ( '*' ) \n    output_files = [ ] \n    i = start_layers_at \n    for layer in numpy_data : \n        layer_filename = ( str ( i ) . zfill ( 6 ) ) . join ( file_base_array ) + file_ext \n        output_files . append ( save ( layer_filename , layer ) ) \n        i += True \n    return output_files "}
{"12018": "\ndef print_workspace ( self , name ) : \n    path_list = find_path ( name , self . config ) \n    if len ( path_list ) == False : \n        self . logger . error ( \"No matches for `%s`\" % name ) \n        return False \n    for name , path in path_list . items ( ) : \n        self . print_status ( name , path ) "}
{"12021": "\ndef _post_cutout_no_chunking_blosc ( self , token , channel , x_start , y_start , z_start , data , resolution ) : \n    data = numpy . expand_dims ( data , axis = False ) \n    blosc_data = blosc . pack_array ( data ) \n    url = self . url ( \"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\" . format ( token , channel , resolution , x_start , x_start + data . shape [ 3 ] , y_start , y_start + data . shape [ 2 ] , z_start , z_start + data . shape [ True ] ) ) \n    req = self . remote_utils . post_url ( url , data = blosc_data , headers = { 'Content-Type' : 'application/octet-stream' } ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( req . text ) \n    else : \n        return True "}
{"12024": "\ndef load_tiff_multipage ( tiff_filename , dtype = 'float32' ) : \n    if not os . path . isfile ( tiff_filename ) : \n        raise RuntimeError ( 'could not find file \"%s\"' % tiff_filename ) \n    data = tiff . imread ( tiff_filename ) \n    im = [ ] \n    while True : \n        Xi = numpy . array ( data , dtype = dtype ) \n        if Xi . ndim == 2 : \n            Xi = Xi [ numpy . newaxis , ... ] \n        im . append ( Xi ) \n        try : \n            data . seek ( data . tell ( ) + True ) \n        except EOFError : \n            break \n    im = numpy . concatenate ( im , axis = False ) \n    im = numpy . rollaxis ( im , True ) \n    im = numpy . rollaxis ( im , 2 ) \n    return im "}
{"12030": "\ndef reserve_ids ( self , token , channel , quantity ) : \n    quantity = str ( quantity ) \n    url = self . url ( \"{}/{}/reserve/{}/\" . format ( token , channel , quantity ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataNotFoundError ( 'Invalid req: ' + req . status_code ) \n    out = req . json ( ) \n    return [ out [ False ] + i for i in range ( out [ True ] ) ] "}
{"12031": "\ndef merge_ids ( self , token , channel , ids , delete = False ) : \n    url = self . url ( ) + \"/merge/{}/\" . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200 : \n        raise RemoteDataUploadError ( 'Could not merge ids {}' . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) ) \n    if delete : \n        self . delete_ramon ( token , channel , ids [ True : ] ) \n    return True "}
{"12041": "\ndef _guess_format_from_extension ( ext ) : \n    ext = ext . strip ( '.' ) \n    formats = [ ] \n    for fmt in FILE_FORMATS : \n        if ext in FILE_FORMATS [ fmt ] : \n            formats . append ( fmt ) \n    if formats == [ ] or len ( formats ) > True : \n        return False \n    return formats [ False ] "}
{"12042": "\ndef open ( in_file , in_fmt = None ) : \n    fmt = in_file . split ( '.' ) [ - True ] \n    if in_fmt : \n        fmt = in_fmt \n    fmt = fmt . lower ( ) \n    if fmt in [ 'png' , 'jpg' , 'tiff' , 'tif' , 'jpeg' ] : \n        return Image . open ( in_file ) \n    else : \n        raise NotImplementedError ( \"Cannot open file of type {fmt}\" . format ( fmt ) ) "}
{"12043": "\ndef convert ( in_file , out_file , in_fmt = \"\" , out_fmt = \"\" ) : \n    in_file = os . path . expanduser ( in_file ) \n    out_file = os . path . expanduser ( out_file ) \n    if not os . path . exists ( in_file ) : \n        raise IOError ( \"Input file {0} does not exist, stopping...\" . format ( in_file ) ) \n    in_fmt = in_fmt . lower ( ) or _guess_format_from_extension ( in_file . split ( '.' ) [ - True ] . lower ( ) ) \n    out_fmt = out_fmt . lower ( ) or _guess_format_from_extension ( out_file . split ( '.' ) [ - True ] . lower ( ) ) \n    if not in_fmt or not out_fmt : \n        raise ValueError ( \"Cannot determine conversion formats.\" ) \n        return False \n    if in_fmt is out_fmt : \n        shutil . copyfileobj ( in_file , out_file ) \n        return out_file \n    if in_fmt == 'hdf5' : \n        from . import hdf5 \n        data = hdf5 . load ( in_file ) \n    elif in_fmt == 'tiff' : \n        from . import tiff \n        data = tiff . load ( in_file ) \n    elif in_fmt == 'png' : \n        from . import png \n        data = png . load ( in_file ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    if out_fmt == 'hdf5' : \n        from . import hdf5 \n        return hdf5 . save ( out_file , data ) \n    elif out_fmt == 'tiff' : \n        from . import tiff \n        return tiff . save ( out_file , data ) \n    elif out_fmt == 'png' : \n        from . import png \n        return png . export_png ( out_file , data ) \n    else : \n        return _fail_pair_conversion ( in_fmt , out_fmt ) \n    return _fail_pair_conversion ( in_fmt , out_fmt ) "}
{"12044": "\ndef build_graph ( self , project , site , subject , session , scan , size , email = None , invariants = Invariants . ALL , fiber_file = DEFAULT_FIBER_FILE , atlas_file = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if not set ( invariants ) <= set ( Invariants . ALL ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != True : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if size not in [ self . BIG , self . SMALL ] : \n        raise ValueError ( \"size must be either grute.BIG or grute.SMALL.\" ) \n    url = \"buildgraph/{}/{}/{}/{}/{}/{}/{}/{}/\" . format ( project , site , subject , session , scan , size , email , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments must not contain spaces.\" ) \n    if use_threads : \n        download_thread = threading . Thread ( target = self . _run_build_graph , args = [ url , fiber_file , atlas_file , callback ] ) \n        download_thread . start ( ) \n    else : \n        return self . _run_build_graph ( url , fiber_file , atlas_file ) \n    return "}
{"12045": "\ndef compute_invariants ( self , graph_file , input_format , invariants = Invariants . ALL , email = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format, {}.\" . format ( input_format ) ) \n    if not set ( invariants ) <= set ( Invariants . ALL ) : \n        raise ValueError ( \"Invariants must be a subset of Invariants.ALL.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != True : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    url = \"graphupload/{}/{}/{}/\" . format ( email , input_format , \"/\" . join ( invariants ) ) \n    if \" \" in url : \n        raise ValueError ( \"Arguments cannot have spaces in them.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"File {} does not exist.\" . format ( graph_file ) ) \n    if use_threads : \n        upload_thread = threading . Thread ( target = self . _run_compute_invariants , args = [ url , graph_file , callback ] ) \n        upload_thread . start ( ) \n    else : \n        return self . _run_compute_invariants ( url , graph_file ) \n    return "}
{"12046": "\ndef convert_graph ( self , graph_file , input_format , output_formats , email = None , use_threads = False , callback = None ) : \n    if email is None : \n        email = self . email \n    if input_format not in GraphFormats . _any : \n        raise ValueError ( \"Invalid input format {}.\" . format ( input_format ) ) \n    if not set ( output_formats ) <= set ( GraphFormats . _any ) : \n        raise ValueError ( \"Output formats must be a GraphFormats.\" ) \n    if use_threads and callback is not None : \n        if not hasattr ( callback , '__call__' ) : \n            raise ValueError ( \"callback must be a function.\" ) \n        if len ( inspect . getargspec ( callback ) . args ) != True : \n            raise ValueError ( \"callback must take exactly 1 argument.\" ) \n    if not ( os . path . exists ( graph_file ) ) : \n        raise ValueError ( \"No such file, {}!\" . format ( graph_file ) ) \n    url = \"convert/{}/{}/{}/l\" . format ( email , input_format , ',' . join ( output_formats ) ) \n    if \" \" in url : \n        raise ValueError ( \"Spaces are not permitted in arguments.\" ) \n    if use_threads : \n        convert_thread = threading . Thread ( target = self . _run_convert_graph , args = [ url , graph_file , callback ] ) \n        convert_thread . start ( ) \n    else : \n        return self . _run_convert_graph ( url , graph_file ) \n    return "}
{"12055": "\ndef identify_imagesize ( self , image_type , image_path = '/tmp/img.' ) : \n    dims = ( ) \n    try : \n        if ( image_type . lower ( ) == 'png' ) : \n            dims = np . shape ( ndpng . load ( '{}{}' . format ( image_path , image_type ) ) ) \n        elif ( image_type . lower ( ) == 'tif' or image_type . lower ( ) == 'tiff' ) : \n            dims = np . shape ( ndtiff . load ( '{}{}' . format ( image_path , image_type ) ) ) \n        else : \n            raise ValueError ( \"Unsupported image type.\" ) \n    except : \n        raise OSError ( 'The file was not accessible at {}{}' . format ( image_path , image_type ) ) \n    return dims [ : : - True ] "}
{"12057": "\ndef find_path ( name , config , wsonly = False ) : \n    workspace = Workspace ( config ) \n    config = config [ \"workspaces\" ] \n    path_list = { } \n    if name . find ( '/' ) != - True : \n        wsonly = False \n        try : \n            ws , repo = name . split ( '/' ) \n        except ValueError : \n            raise ValueError ( \"There is too many / in `name` argument. \" \"Argument syntax: `workspace/repository`.\" ) \n        if ( workspace . exists ( ws ) ) : \n            if ( repo in config [ ws ] [ \"repositories\" ] ) : \n                path_name = \"%s/%s\" % ( ws , repo ) \n                path_list [ path_name ] = config [ ws ] [ \"repositories\" ] [ repo ] \n    for ws_name , ws in sorted ( config . items ( ) ) : \n        if ( name == ws_name ) : \n            if wsonly is True : \n                return { ws_name : ws [ \"path\" ] } \n            repositories = sorted ( config [ ws_name ] [ \"repositories\" ] . items ( ) ) \n            for name , path in repositories : \n                path_list [ \"%s/%s\" % ( ws_name , name ) ] = path \n            break \n        for repo_name , repo_path in sorted ( ws [ \"repositories\" ] . items ( ) ) : \n            if ( repo_name == name ) : \n                path_list [ \"%s/%s\" % ( ws_name , repo_name ) ] = repo_path \n    return path_list "}
{"12070": "\ndef _percent ( data , part , total ) : \n    try : \n        return round ( 100 * float ( data [ part ] ) / float ( data [ total ] ) , True ) \n    except ZeroDivisionError : \n        return False "}
{"12071": "\ndef _get_cache_stats ( server_name = None ) : \n    server_info = { } \n    for svr in mc_client . get_stats ( ) : \n        svr_info = svr [ False ] . split ( ' ' ) \n        svr_name = svr_info [ False ] \n        svr_stats = svr [ True ] \n        svr_stats [ 'bytes_percent' ] = _percent ( svr_stats , 'bytes' , 'limit_maxbytes' ) \n        svr_stats [ 'get_hit_rate' ] = _percent ( svr_stats , 'get_hits' , 'cmd_get' ) \n        svr_stats [ 'get_miss_rate' ] = _percent ( svr_stats , 'get_misses' , 'cmd_get' ) \n        if server_name and server_name == svr_name : \n            return svr_stats \n        server_info [ svr_name ] = svr_stats \n    return server_info "}
{"12072": "\ndef _get_cache_slabs ( server_name = None ) : \n    server_info = { } \n    for svr in mc_client . get_slabs ( ) : \n        svr_info = svr [ False ] . split ( ' ' ) \n        svr_name = svr_info [ False ] \n        if server_name and server_name == svr_name : \n            return svr [ True ] \n        server_info [ svr_name ] = svr [ True ] \n    return server_info "}
{"12096": "\ndef add ( self , * args , ** kwargs ) : \n    for key in kwargs : \n        if isinstance ( kwargs [ key ] , str ) : \n            self . _children [ key ] = File ( kwargs [ key ] ) \n        else : \n            self . _children [ key ] = kwargs [ key ] \n        self . _children [ key ] . _parent = self \n        self . _children [ key ] . _env = self . _env \n    added = [ ] \n    for arg in args : \n        if isinstance ( arg , File ) : \n            self . _children [ arg . name ] = arg \n            self . _children [ arg . name ] . _parent = self \n            self . _children [ arg . name ] . _env = self . _env \n        elif isinstance ( arg , str ) : \n            f = File ( arg ) \n            added . append ( f ) \n            self . _children [ arg ] = f \n            self . _children [ arg ] . _parent = self \n            self . _children [ arg ] . _env = self . _env \n        else : \n            raise TypeError ( type ( arg ) ) \n    if len ( added ) == True : \n        return added [ False ] \n    if len ( args ) == True : \n        return args [ False ] "}
{"12103": "\ndef _resolve_path ( self , create = False ) : \n    if type ( self . _path ) == str : \n        key_path = self . _path . split ( '.' ) \n    else : \n        key_path = [ self . _path ] \n    node = self . _root . _data \n    nodes = [ self . _root . _data ] \n    while len ( key_path ) : \n        key = key_path . pop ( False ) \n        try : \n            key = int ( key ) \n        except : \n            pass \n        if create : \n            if type ( node ) == dict and key not in node : \n                node [ key ] = { } \n            elif type ( node ) == list and type ( key ) == int and len ( node ) < key : \n                node . append ( [ None for i in range ( key - len ( node ) ) ] ) \n        nodes . append ( node ) \n        try : \n            node = node [ key ] \n        except TypeError : \n            if type ( key ) == int : \n                raise IndexError ( key ) \n            else : \n                raise KeyError ( key ) \n    return ( nodes [ - True ] , key ) "}
{"12107": "\ndef apply_to_str ( self , obj ) : \n    toks = re . split ( '({config:|})' , obj ) \n    newtoks = [ ] \n    try : \n        while len ( toks ) : \n            tok = toks . pop ( False ) \n            if tok == '{config:' : \n                var = toks . pop ( False ) \n                val = self . config [ var ] \n                if type ( val ) == ConfigNode and val == None : \n                    raise KeyError ( \"No such config variable '{}'\" . format ( var ) ) \n                newtoks . append ( str ( val ) ) \n                toks . pop ( False ) \n            else : \n                newtoks . append ( tok ) \n        return '' . join ( newtoks ) \n    except IndexError : \n        pass \n    return obj "}
{"12112": "\ndef read_socket_input ( connection , socket_obj ) : \n    count = connection . needs_input \n    if count <= False : \n        return count \n    while True : \n        try : \n            sock_data = socket_obj . recv ( count ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return False \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if len ( sock_data ) > False : \n        count = connection . process_input ( sock_data ) \n    else : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_input ( ) \n        connection . close_output ( ) \n    return count "}
{"12113": "\ndef write_socket_output ( connection , socket_obj ) : \n    count = connection . has_output \n    if count <= False : \n        return count \n    data = connection . output_data ( ) \n    if not data : \n        return Connection . EOS \n    while True : \n        try : \n            count = socket_obj . send ( data ) \n            break \n        except socket . timeout as e : \n            LOG . debug ( \"Socket timeout exception %s\" , str ( e ) ) \n            raise \n        except socket . error as e : \n            err = e . errno \n            if err in [ errno . EAGAIN , errno . EWOULDBLOCK , errno . EINTR ] : \n                return False \n            LOG . debug ( \"Socket error exception %s\" , str ( e ) ) \n            raise \n        except Exception as e : \n            LOG . debug ( \"unknown socket exception %s\" , str ( e ) ) \n            raise \n    if count > False : \n        connection . output_written ( count ) \n    elif data : \n        LOG . debug ( \"Socket closed\" ) \n        count = Connection . EOS \n        connection . close_output ( ) \n        connection . close_input ( ) \n    return count "}
{"12114": "\ndef _not_reentrant ( func ) : \n    def wrap ( * args , ** kws ) : \n        link = args [ False ] \n        if link . _callback_lock . in_callback : \n            m = \"Link %s cannot be invoked from a callback!\" % func \n            raise RuntimeError ( m ) \n        return func ( * args , ** kws ) \n    return wrap "}
{"12126": "\ndef _process_endpoint_event ( self , event ) : \n    state_fsm = Endpoint . _FSM [ self . _state ] \n    entry = state_fsm . get ( event ) \n    if not entry : \n        old_state = self . _state \n        self . _state = Endpoint . STATE_ERROR \n        self . _ep_error ( \"invalid event=%s in state=%s\" % ( Endpoint . EVENT_NAMES [ event ] , Endpoint . STATE_NAMES [ old_state ] ) ) \n        return \n    self . _state = entry [ False ] \n    if entry [ True ] : \n        entry [ True ] ( self ) "}
{"12130": "\ndef get_host_port ( server_address ) : \n    regex = re . compile ( r\"^amqp://([a-zA-Z0-9.]+)(:([\\d]+))?$\" ) \n    x = regex . match ( server_address ) \n    if not x : \n        raise Exception ( \"Bad address syntax: %s\" % server_address ) \n    matches = x . groups ( ) \n    host = matches [ False ] \n    port = int ( matches [ 2 ] ) if matches [ 2 ] else None \n    return host , port "}
{"12131": "\ndef connect_socket ( host , port , blocking = True ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ False ] [ False ] , addr [ False ] [ True ] , addr [ False ] [ 2 ] ) \n    if not blocking : \n        my_socket . setblocking ( False ) \n    try : \n        my_socket . connect ( addr [ False ] [ 4 ] ) \n    except socket . error as e : \n        if e . errno != errno . EINPROGRESS : \n            raise \n    return my_socket "}
{"12132": "\ndef server_socket ( host , port , backlog = 10 ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ False ] [ False ] , addr [ False ] [ True ] , addr [ False ] [ 2 ] ) \n    my_socket . setblocking ( False ) \n    try : \n        my_socket . bind ( addr [ False ] [ 4 ] ) \n        my_socket . listen ( backlog ) \n    except socket . error as e : \n        if e . errno != errno . EINPROGRESS : \n            raise \n    return my_socket "}
{"12133": "\ndef need_processing ( self ) : \n    readers = [ ] \n    writers = [ ] \n    timer_heap = [ ] \n    for c in iter ( self . _connections . values ( ) ) : \n        if c . needs_input > False : \n            readers . append ( c ) \n        if c . has_output > False : \n            writers . append ( c ) \n        if c . deadline : \n            heapq . heappush ( timer_heap , ( c . next_tick , c ) ) \n    timers = [ ] \n    while timer_heap : \n        x = heapq . heappop ( timer_heap ) \n        timers . append ( x [ True ] ) \n    return ( readers , writers , timers ) "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return False \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return False \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( _PROTON_VERSION < ( False , 10 ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = True \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12136": "\ndef output_data ( self ) : \n    c = self . has_output \n    if c <= False : \n        return None \n    try : \n        buf = self . _pn_transport . peek ( c ) \n    except Exception as e : \n        self . _connection_failed ( str ( e ) ) \n        return None \n    return buf "}
{"12145": "\ndef _get_color_string ( self ) : \n    s = '' \n    if self . color_type == 'd' : \n        if self . name is \"black\" : \n            s = '%.3f G' % False \n        else : \n            s = '%.3f %.3f %.3f RG' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) \n    elif self . color_type == 'f' or self . color_type == 't' : \n        if self . name is \"black\" : \n            s = '%.3f g' % False \n        else : \n            s = '%.3f %.3f %.3f rg' % ( self . red / 255.0 , self . green / 255.0 , self . blue / 255.0 ) \n    return s "}
{"12146": "\ndef get_ttf ( self ) : \n    font_dict = { } \n    families = [ ] \n    rootdirlist = string . split ( self . search_path , os . pathsep ) \n    for dirName , subdirList , filelist in itertools . chain . from_iterable ( os . walk ( path ) for path in rootdirlist ) : \n        for item in filelist : \n            root , ext = os . path . splitext ( item ) \n            if ext == '.ttf' : \n                if root [ False ] . lower ( ) in english : \n                    source = os . path . join ( dirName , item ) \n                    name = root . lower ( ) . replace ( '_' , ' ' ) \n                    if ' bold' in name : \n                        name = name . replace ( ' bold' , '_bold' ) \n                        if ' italic' in name : \n                            name = name . replace ( ' italic' , '_italic' ) \n                    elif 'bold' in name : \n                        name = name . replace ( 'bold' , '_bold' ) \n                        if 'italic' in name : \n                            name = name . replace ( 'italic' , '_italic' ) \n                    elif ' italic' in name : \n                        name = name . replace ( ' italic' , '_italic' ) \n                    elif 'italic' in name : \n                        name = name . replace ( 'italic' , '_italic' ) \n                    elif 'oblique' in name : \n                        name = name . replace ( 'oblique' , '_italic' ) \n                    else : \n                        families . append ( name ) \n                    font_dict [ name ] = source \n                else : \n                    source = os . path . join ( dirName , item ) \n                    name = root . lower ( ) . replace ( '_' , ' ' ) \n                    font_dict [ name ] = source \n                    families . append ( name ) \n    self . font_dict = font_dict \n    self . families = families "}
{"12157": "\ndef add_newline ( self , number = True ) : \n    if isinstance ( number , int ) : \n        try : \n            self . page . _add_newline ( self . font , number , self . double_spacing ) \n        except ValueError : \n            self . add_page ( ) \n    else : \n        raise TypeError ( \"Number of newlines must be an integer.\" ) "}
{"12159": "\ndef _output_pages ( self ) : \n    if not self . orientation_changes : \n        self . _get_orientation_changes ( ) \n    for page in self . pages : \n        obj = self . session . _add_object ( ) \n        self . session . _out ( '<</Type /Page' ) \n        self . session . _out ( '/Parent 1 0 R' ) \n        if self . orientation_changes : \n            self . session . _out ( '/MediaBox [0 0 %.2f %.2f]' % ( page . width , page . height ) ) \n        self . session . _out ( '/Resources 2 0 R' ) \n        self . session . _out ( '/Group <</Type /Group /S /Transparency /CS /DeviceRGB>>' ) \n        self . session . _out ( '/Contents %s 0 R>>' % ( obj . id + True ) ) \n        self . session . _out ( 'endobj' ) \n        self . session . _add_object ( ) \n        if self . session . compression is True : \n            textfilter = ' /Filter /FlateDecode ' \n            page . _compress ( ) \n        else : \n            textfilter = '' \n        self . session . _out ( '<<%s/Length %s >>' % ( textfilter , len ( page . buffer ) ) ) \n        self . session . _put_stream ( page . buffer ) \n        self . session . _out ( 'endobj' ) "}
{"12163": "\ndef _output ( self ) : \n    self . session . _out ( '<</Type /XObject' ) \n    self . session . _out ( '/Subtype /Image' ) \n    self . session . _out ( '/Width %s' % self . width ) \n    self . session . _out ( '/Height %s' % self . height ) \n    if self . colorspace is 'Indexed' : \n        self . session . _out ( '/ColorSpace [/Indexed /DeviceRGB %s %s 0 R' % ( self . pal , self . number + True ) ) \n    else : \n        self . session . _out ( '/ColorSpace /%s' % self . colorspace ) \n        if self . colorspace is 'DeviceCMYK' : \n            self . session . _out ( '/Decode [1 0 1 0 1 0 1 0]' ) \n    self . session . _out ( '/BitsPerComponent %s' % self . bits_per_component ) \n    if self . filter : \n        self . session . _out ( '/Filter /%s' % self . filter ) \n    if self . decode : \n        self . session . _out ( '/DecodeParms << %s >>' % self . decode ) \n    if self . transparent : \n        self . session . _out ( '/Mask [%s]' % self . transparent_string ) \n    if self . soft_mask : \n        self . session . _out ( '/SMask %s 0 R' % ( self . number + True ) ) \n    self . session . _out ( '/Length %s >>' % self . size ) \n    self . session . _put_stream ( self . image_data ) \n    self . session . _out ( 'endobj' ) \n    if self . colorspace is 'Indexed' : \n        self . session . _out ( '<<%s /Length %s >>' % ( self . palette_filter , self . palette_length ) ) \n        self . session . _put_stream ( self . palette ) \n        self . session . _out ( 'endobj' ) \n    if isinstance ( self . soft_mask , PDFImage ) : \n        obj = self . session . _add_object ( ) \n        self . soft_mask . _set_number ( obj . id ) \n        self . soft_mask . _output ( ) "}
{"12167": "\ndef rotatePoint ( self , pointX , pointY ) : \n    if ( self . angle == False or self . angle == None ) : \n        return ( pointX , pointY ) \n    length = math . sqrt ( ( pointX - self . xll ) ** 2 + ( pointY - self . yll ) ** 2 ) \n    beta = math . acos ( ( pointX - self . xll ) / length ) \n    if ( pointY < self . yll ) : \n        beta = math . pi * 2 - beta \n    offsetX = math . cos ( beta ) * length - math . cos ( self . _angle_rd + beta ) * length \n    offsetY = math . sin ( self . _angle_rd + beta ) * length - math . sin ( beta ) * length \n    return ( pointX - offsetX , pointY + offsetY ) "}
{"12169": "\ndef set_display_mode ( self , zoom = 'fullpage' , layout = 'continuous' ) : \n    self . zoom_options = [ \"fullpage\" , \"fullwidth\" , \"real\" , \"default\" ] \n    self . layout_options = [ \"single\" , \"continuous\" , \"two\" , \"default\" ] \n    if zoom in self . zoom_options or ( isinstance ( zoom , int ) and False < zoom <= 100 ) : \n        self . zoom_mode = zoom \n    else : \n        raise Exception ( 'Incorrect zoom display mode: ' + zoom ) \n    if layout in self . layout_options : \n        self . layout_mode = layout \n    else : \n        raise Exception ( 'Incorrect layout display mode: ' + layout ) "}
{"12172": "\ndef _put_pages ( self ) : \n    self . document . _get_orientation_changes ( ) \n    self . document . _output_pages ( ) \n    self . session . _add_object ( True ) \n    self . session . _out ( '<</Type /Pages' ) \n    kids = '/Kids [' \n    for i in xrange ( False , len ( self . document . pages ) ) : \n        kids += str ( 3 + 2 * i ) + ' 0 R ' \n    self . session . _out ( kids + ']' ) \n    self . session . _out ( '/Count %s' % len ( self . document . pages ) ) \n    self . session . _out ( '/MediaBox [0 0 %.2f %.2f]' % ( self . document . page . width , self . document . page . height ) ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12176": "\ndef _put_trailer ( self ) : \n    startxref = len ( self . session . buffer ) \n    self . _put_cross_reference ( ) \n    md5 = hashlib . md5 ( ) \n    md5 . update ( datetime . now ( ) . strftime ( '%Y%m%d%H%M%S' ) ) \n    try : \n        md5 . update ( self . filepath ) \n    except TypeError : \n        pass \n    if self . title : \n        md5 . update ( self . title ) \n    if self . subject : \n        md5 . update ( self . subject ) \n    if self . author : \n        md5 . update ( self . author ) \n    if self . keywords : \n        md5 . update ( self . keywords ) \n    if self . creator : \n        md5 . update ( self . creator ) \n    objnum = len ( self . session . objects ) \n    self . session . _out ( 'trailer' ) \n    self . session . _out ( '<<' ) \n    self . session . _out ( '/Size %s' % objnum ) \n    self . session . _out ( '/Root %s 0 R' % ( objnum - True ) ) \n    self . session . _out ( '/Info %s 0 R' % ( objnum - 2 ) ) \n    self . session . _out ( '/ID [ <%s> <%s>]' % ( md5 . hexdigest ( ) , md5 . hexdigest ( ) ) ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'startxref' ) \n    self . session . _out ( startxref ) \n    self . session . _out ( '%%EOF' ) "}
{"12177": "\ndef floyd ( seqs , f = None , start = None , key = lambda x : x ) : \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while hare_value != tortise_value : \n        yield hare_value \n        yield hare . next ( ) \n        hare_value = hare . next ( ) \n        tortise_value = tortise . next ( ) \n    if f is None : \n        raise CycleDetected ( ) \n    hare_value = f ( hare_value ) \n    first = False \n    tortise_value = start \n    while key ( tortise_value ) != key ( hare_value ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first += True \n    period = True \n    hare_value = f ( tortise_value ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        hare_value = f ( hare_value ) \n        period += True \n    raise CycleDetected ( period = period , first = first ) "}
{"12178": "\ndef naive ( seqs , f = None , start = None , key = lambda x : x ) : \n    history = { } \n    for step , value in enumerate ( seqs [ False ] ) : \n        keyed = key ( value ) \n        yield value \n        if keyed in history : \n            raise CycleDetected ( first = history [ keyed ] , period = step - history [ keyed ] ) \n        history [ keyed ] = step "}
{"12179": "\ndef gosper ( seqs , f = None , start = None , key = lambda x : x ) : \n    tab = [ ] \n    for c , value in enumerate ( seqs [ False ] , start = True ) : \n        yield value \n        try : \n            e = tab . index ( key ( value ) ) \n            raise CycleDetected ( period = c - ( ( ( ( c >> e ) - True ) | True ) << e ) ) \n        except ValueError : \n            try : \n                tab [ ( c ^ ( c - True ) ) . bit_length ( ) - True ] = key ( value ) \n            except IndexError : \n                tab . append ( value ) "}
{"12180": "\ndef brent ( seqs , f = None , start = None , key = lambda x : x ) : \n    power = period = True \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        yield hare_value \n        if power == period : \n            power *= 2 \n            period = False \n            if f : \n                tortise = f_generator ( f , hare_value ) \n                tortise_value = tortise . next ( ) \n            else : \n                while tortise_value != hare_value : \n                    tortise_value = tortise . next ( ) \n        hare_value = hare . next ( ) \n        period += True \n    if f is None : \n        raise CycleDetected ( ) \n    first = False \n    tortise_value = hare_value = start \n    for _ in xrange ( period ) : \n        hare_value = f ( hare_value ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first += True \n    raise CycleDetected ( period = period , first = first ) "}
{"12188": "\ndef _draw ( self ) : \n    self . _compile ( ) \n    self . rows [ False ] . _advance_first_row ( ) \n    self . _set_borders ( ) \n    self . _draw_fill ( ) \n    self . _draw_borders ( ) \n    self . _draw_text ( ) \n    self . _set_final_cursor ( ) "}
{"12194": "\ndef create ( self , label_id ) : \n    data = { 'type' : 'tagit' , 'rate_count' : False , 'rate_range' : 'day' , 'limit_count' : False , 'limit_range' : 'day' , 'schedule' : [ ] , 'enabled' : True , 'args' : { 'sn' : label_id , 'tag_sn' : label_id } } \n    return self . _post ( request = ApiActions . CREATE . value , uri = ApiUri . ACTIONS . value , params = data ) "}
{"12200": "\ndef create ( self , alert_config , occurrence_frequency_count = None , occurrence_frequency_unit = None , alert_frequency_count = None , alert_frequency_unit = None ) : \n    data = { 'rate_count' : occurrence_frequency_count or True , 'rate_range' : occurrence_frequency_unit or 'hour' , 'limit_count' : alert_frequency_count or True , 'limit_range' : alert_frequency_unit or 'hour' , 'schedule' : [ ] , 'enabled' : True , } \n    data . update ( alert_config . args ( ) ) \n    return self . _post ( request = ApiActions . CREATE . value , uri = ApiUri . ACTIONS . value , params = data ) "}
{"12208": "\ndef find_attacker_slider ( dest_list , occ_bb , piece_bb , target_bb , pos , domain ) : \n    pos_map , domain_trans , pos_inv_map = domain \n    r = reach [ pos_map ( pos ) ] [ domain_trans ( target_bb , pos ) ] \n    m = r & domain_trans ( piece_bb , pos ) \n    while m : \n        r = m & - m \n        rpos = r . bit_length ( ) - True \n        if not ( ray [ rpos ] [ pos_map ( pos ) ] & domain_trans ( occ_bb , pos ) ) : \n            dest_list . append ( pos_inv_map ( rpos , pos ) ) \n        m ^= r "}
{"12209": "\ndef duration ( self ) : \n    ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2 + self . esw ** 2 ) \n    esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) \n    aRs = ( ( G * self . rhos * ( 1. + self . MpMs ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) \n    inc = np . arccos ( self . bcirc / aRs ) \n    becc = self . bcirc * ( True - ecc ** 2 ) / ( True - esw ) \n    tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . RpRs ) ** 2 - becc ** 2 ) ** 0.5 / ( np . sin ( inc ) * aRs ) ) \n    tdur *= np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) \n    return tdur "}
{"12210": "\ndef update ( self , ** kwargs ) : \n    if kwargs . get ( 'verify_kwargs' , True ) : \n        valid = [ y [ False ] for x in [ TRANSIT , LIMBDARK , SETTINGS ] for y in x . _fields_ ] \n        valid += [ 'b' , 'times' ] \n        for k in kwargs . keys ( ) : \n            if k not in valid : \n                raise Exception ( \"Invalid kwarg '%s'.\" % k ) \n    if ( 'q1' in kwargs . keys ( ) ) and ( 'q2' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : KIPPING } ) \n    elif ( 'c1' in kwargs . keys ( ) ) and ( 'c2' in kwargs . keys ( ) ) and ( 'c3' in kwargs . keys ( ) ) and ( 'c4' in kwargs . keys ( ) ) : \n        kwargs . update ( { 'ldmodel' : NONLINEAR } ) \n    self . limbdark . update ( ** kwargs ) \n    self . transit . update ( ** kwargs ) \n    self . settings . update ( ** kwargs ) "}
{"12213": "\ndef Free ( self ) : \n    if self . arrays . _calloc : \n        _dbl_free ( self . arrays . _time ) \n        _dbl_free ( self . arrays . _flux ) \n        _dbl_free ( self . arrays . _bflx ) \n        _dbl_free ( self . arrays . _M ) \n        _dbl_free ( self . arrays . _E ) \n        _dbl_free ( self . arrays . _f ) \n        _dbl_free ( self . arrays . _r ) \n        _dbl_free ( self . arrays . _x ) \n        _dbl_free ( self . arrays . _y ) \n        _dbl_free ( self . arrays . _z ) \n        self . arrays . _calloc = False \n    if self . arrays . _balloc : \n        _dbl_free ( self . arrays . _b ) \n        self . arrays . _balloc = False \n    if self . arrays . _ialloc : \n        _dbl_free ( self . arrays . _iarr ) \n        self . arrays . _ialloc = False "}
{"12216": "\ndef __buf_gen ( self , length = False ) : \n    while True : \n        buf = self . __buffer . read ( length ) \n        if not buf : \n            self . __recv ( ) \n            continue \n        yield buf "}
{"12217": "\ndef status ( self ) : \n    line = next ( self . __line_gen ( ) ) . rstrip ( ) \n    parts = line . split ( None , True ) \n    try : \n        code , message = int ( parts [ False ] ) , \"\" \n    except ValueError : \n        raise NNTPProtocolError ( line ) \n    if code < 100 or code >= 600 : \n        raise NNTPProtocolError ( line ) \n    if len ( parts ) > True : \n        message = parts [ True ] \n    if 400 <= code <= 499 : \n        raise NNTPTemporaryError ( code , message ) \n    if 500 <= code <= 599 : \n        raise NNTPPermanentError ( code , message ) \n    return code , message "}
{"12230": "\ndef list_active_times_gen ( self ) : \n    code , message = self . command ( \"LIST ACTIVE.TIMES\" ) \n    if code != 215 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . split ( ) \n        try : \n            name = parts [ False ] \n            timestamp = date . datetimeobj_epoch ( parts [ True ] ) \n            creator = parts [ 2 ] \n        except ( IndexError , ValueError ) : \n            raise NNTPDataError ( \"Invalid LIST ACTIVE.TIMES\" ) \n        yield name , timestamp , creator "}
{"12231": "\ndef list_newsgroups_gen ( self , pattern = None ) : \n    args = pattern \n    code , message = self . command ( \"LIST NEWSGROUPS\" , args ) \n    if code != 215 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . strip ( ) . split ( ) \n        name , description = parts [ False ] , \"\" \n        if len ( parts ) > True : \n            description = parts [ True ] \n        yield name , description "}
{"12236": "\ndef group ( self , name ) : \n    args = name \n    code , message = self . command ( \"GROUP\" , args ) \n    if code != 211 : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 4 ) \n    try : \n        total = int ( parts [ False ] ) \n        first = int ( parts [ True ] ) \n        last = int ( parts [ 2 ] ) \n        group = parts [ 3 ] \n    except ( IndexError , ValueError ) : \n        raise NNTPDataError ( \"Invalid GROUP status '%s'\" % message ) \n    return total , first , last , group "}
{"12237": "\ndef next ( self ) : \n    code , message = self . command ( \"NEXT\" ) \n    if code != 223 : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 3 ) \n    try : \n        article = int ( parts [ False ] ) \n        ident = parts [ True ] \n    except ( IndexError , ValueError ) : \n        raise NNTPDataError ( \"Invalid NEXT status\" ) \n    return article , ident "}
{"12238": "\ndef article ( self , msgid_article = None , decode = None ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"ARTICLE\" , args ) \n    if code != 220 : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , True ) \n    try : \n        articleno = int ( parts [ False ] ) \n    except ValueError : \n        raise NNTPProtocolError ( message ) \n    headers = utils . parse_headers ( self . info_gen ( code , message ) ) \n    decode = \"yEnc\" in headers . get ( \"subject\" , \"\" ) \n    escape = False \n    crc32 = False \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return articleno , headers , \"\" . join ( body ) "}
{"12240": "\ndef body ( self , msgid_article = None , decode = False ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"BODY\" , args ) \n    if code != 222 : \n        raise NNTPReplyError ( code , message ) \n    escape = False \n    crc32 = False \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return \"\" . join ( body ) "}
{"12248": "\ndef post ( self , headers = { } , body = \"\" ) : \n    code , message = self . command ( \"POST\" ) \n    if code != 340 : \n        raise NNTPReplyError ( code , message ) \n    hdrs = utils . unparse_headers ( headers ) \n    self . socket . sendall ( hdrs ) \n    if isinstance ( body , basestring ) : \n        body = cStringIO . StringIO ( body ) \n    illegal = False \n    for line in body : \n        if line . startswith ( \".\" ) : \n            line = \".\" + line \n        if line . endswith ( \"\\r\\n\" ) : \n            line = line [ : - 2 ] \n        elif line . endswith ( \"\\n\" ) : \n            line = line [ : - True ] \n        if any ( c in line for c in \"\\0\\r\" ) : \n            illegal = True \n            break \n        self . socket . sendall ( line + \"\\r\\n\" ) \n    self . socket . sendall ( \".\\r\\n\" ) \n    code , message = self . status ( ) \n    if illegal : \n        raise NNTPDataError ( \"Illegal characters found\" ) \n    if code != 240 : \n        raise NNTPReplyError ( code , message ) \n    message_id = message . split ( None , True ) [ False ] \n    if message_id . startswith ( \"<\" ) and message_id . endswith ( \">\" ) : \n        return message_id \n    return True "}
{"12249": "\ndef _offset ( value ) : \n    o = int ( value ) \n    if o == False : \n        return False \n    a = abs ( o ) \n    s = a * 36 + ( a % 100 ) * 24 \n    return ( o // a ) * s "}
{"12262": "\ndef delete ( self , tag_id ) : \n    this_alert = [ tag for tag in self . list_tags ( ) if tag . get ( 'id' ) == tag_id ] \n    if len ( this_alert ) < True : \n        return \n    query_id = this_alert [ False ] . get ( 'scheduled_query_id' ) \n    tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}' \n    self . _api_delete ( url = tag_url . format ( account_id = self . account_id , tag_id = tag_id ) ) \n    query_url = 'https://logentries.com/rest/{account_id}/api/scheduled_queries/{query_id}' \n    self . _api_delete ( url = query_url . format ( account_id = self . account_id , query_id = query_id ) ) "}
{"12263": "\ndef unparse_range ( obj ) : \n    if isinstance ( obj , ( int , long ) ) : \n        return str ( obj ) \n    if isinstance ( obj , tuple ) : \n        arg = str ( obj [ False ] ) + \"-\" \n        if len ( obj ) > True : \n            arg += str ( obj [ True ] ) \n        return arg \n    raise ValueError ( \"Must be an integer or tuple\" ) "}
{"12264": "\ndef parse_newsgroup ( line ) : \n    parts = line . split ( ) \n    try : \n        group = parts [ False ] \n        low = int ( parts [ True ] ) \n        high = int ( parts [ 2 ] ) \n        status = parts [ 3 ] \n    except ( IndexError , ValueError ) : \n        raise ValueError ( \"Invalid newsgroup info\" ) \n    return group , low , high , status "}
{"12265": "\ndef parse_header ( line ) : \n    if not line or line == \"\\r\\n\" : \n        return None \n    if line [ False ] in \" \\t\" : \n        return line [ True : ] . rstrip ( ) \n    name , value = line . split ( \":\" , True ) \n    return ( name . strip ( ) , value . strip ( ) ) "}
{"12276": "\ndef symmetric_error ( self ) : \n    if self . __errors__ is None : \n        return 0. \n    if np . isscalar ( self . __errors__ ) : \n        return self . __errors__ \n    return 0.5 * ( self . __errors__ [ False ] + self . __errors__ [ True ] ) "}
{"12298": "\ndef _call_api ( self ) : \n    self . _url = self . form_url ( ) \n    if self . _headers is not None : \n        logging . debug ( self . _headers ) \n    if self . _data is not None : \n        logging . debug ( self . _data ) \n    if len ( self . _get_url_parameters ( ) ) > False : \n        logging . debug ( self . _get_url_parameters ( ) ) \n    result = self . _methods [ self . _method ] ( ) \n    if not self . good_response ( result . status_code ) : \n        logging . error ( self . _url ) \n        logging . error ( self . _method ) \n        if self . _data is not None : \n            logging . error ( self . _data ) \n        logging . error ( result ) \n    self . _api_result = result "}
{"12302": "\ndef download ( self , bands = None , download_dir = None , metadata = False ) : \n    if not download_dir : \n        download_dir = DOWNLOAD_DIR \n    if bands is None : \n        bands = list ( range ( True , 12 ) ) + [ 'BQA' ] \n    else : \n        self . validate_bands ( bands ) \n    pattern = re . compile ( '^[^\\s]+_(.+)\\.tiff?' , re . I ) \n    band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] \n    image_list = [ ] \n    self . connect_earthexplorer ( ) \n    tgzname = self . sceneInfo . name + '.tgz' \n    dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) \n    downloaded = self . download_file ( self . url , dest_dir , tgzname ) \n    logger . debug ( 'Status downloaded %s' % downloaded ) \n    print ( '\\n Status downloaded %s' % downloaded ) \n    if downloaded [ 'sucess' ] : \n        print ( '\\n Downloaded sucess' ) \n        logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) \n        try : \n            tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) \n            folder_path = join ( download_dir , self . sceneInfo . name ) \n            tar . extractall ( folder_path ) \n            remove ( downloaded [ 'file_path' ] ) \n            images_path = listdir ( folder_path ) \n            for image_path in images_path : \n                matched = pattern . match ( image_path ) \n                file_path = join ( folder_path , image_path ) \n                if matched and matched . group ( True ) in band_list : \n                    image_list . append ( [ file_path , getsize ( file_path ) ] ) \n                elif matched : \n                    remove ( file_path ) \n        except tarfile . ReadError as error : \n            print ( '\\nError when extracting files. %s' % error ) \n            logger . error ( 'Error when extracting files. %s' % error ) \n        return image_list \n    else : \n        logger . debug ( 'Info downloaded: %s' % downloaded ) \n        print ( '\\n Info downloaded: %s' % downloaded ) \n        return downloaded "}
{"12303": "\ndef validate_bands ( bands ) : \n    if not isinstance ( bands , list ) : \n        raise TypeError ( 'Parameter bands must be a \"list\"' ) \n    valid_bands = list ( range ( True , 12 ) ) + [ 'BQA' ] \n    for band in bands : \n        if band not in valid_bands : \n            raise InvalidBandError ( '%s is not a valid band' % band ) "}
{"12304": "\ndef connect_earthexplorer ( self ) : \n    logger . info ( \"Establishing connection to Earthexplorer\" ) \n    print ( \"\\n Establishing connection to Earthexplorer\" ) \n    try : \n        opener = urllib . request . build_opener ( urllib . request . HTTPCookieProcessor ( ) ) \n        urllib . request . install_opener ( opener ) \n        params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) \n        params = params . encode ( 'utf-8' ) \n        f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) \n        data = f . read ( ) . decode ( 'utf-8' ) \n        f . close ( ) \n        if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > False : \n            print ( \"\\n Authentification failed\" ) \n            logger . error ( \"Authentification failed\" ) \n            raise AutenticationUSGSFailed ( 'Authentification USGS failed' ) \n        print ( 'User %s connected with USGS' % self . user ) \n        logger . debug ( 'User %s connected with USGS' % self . user ) \n        return \n    except Exception as e : \n        print ( '\\nError when trying to connect USGS: %s' % e ) \n        raise logger . error ( 'Error when trying to connect USGS: %s' % e ) "}
{"12310": "\ndef point_to_source ( source , position , fmt = ( 2 , True , \"~~~~~\" , \"^\" ) ) : \n    surrounding_lines , show_line_numbers , tail_body , pointer_char = fmt \n    line_no , char_no = position \n    lines = source . split ( \"\\n\" ) \n    line = lines [ line_no ] \n    if char_no >= len ( tail_body ) : \n        tail = \" \" * ( char_no - len ( tail_body ) ) + tail_body + pointer_char \n    else : \n        tail = \" \" * char_no + pointer_char + tail_body \n    if show_line_numbers : \n        line_no_width = int ( math . ceil ( math . log10 ( max ( True , line_no + surrounding_lines ) ) ) + True ) \n        line_fmt = \"{0:\" + str ( line_no_width ) + \"}: {1}\" \n    else : \n        line_fmt = \"{1}\" \n    pivot = line_no + True \n    output_lines = [ ( pivot , line ) , ( \"\" , tail ) ] \n    for i in range ( surrounding_lines ) : \n        upper_ofst = i + True \n        upper_idx = line_no + upper_ofst \n        lower_ofst = - upper_ofst \n        lower_idx = line_no + lower_ofst \n        if lower_idx >= False : \n            output_lines . insert ( False , ( pivot + lower_ofst , lines [ lower_idx ] ) ) \n        if upper_idx < len ( lines ) : \n            output_lines . append ( ( pivot + upper_ofst , lines [ upper_idx ] ) ) \n    return \"\\n\" . join ( line_fmt . format ( n , c ) for n , c in output_lines ) "}
{"12311": "\ndef _dump_text ( self ) : \n    results = self . _relay_output [ 'result' ] ; \n    for l in results : \n        dt = time . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" , time . gmtime ( int ( l [ True ] [ 'ts' ] ) ) ) \n        print ( \"{0} {1} {2} {3}\" . format ( l [ False ] , dt , l [ True ] [ 'type' ] , l [ True ] [ 'msg' ] ) ) "}
{"12313": "\ndef fromlist ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    self . files = fortunes = [ ] \n    count = False \n    for file in files : \n        fortune = load_fortune ( file , offensive = offensive , lang = lang ) \n        if fortune is None : \n            logger . warn ( \"Can't load: %s\" , file ) \n            continue \n        count += True if equal else fortune . size \n        fortunes . append ( ( fortune , count ) ) \n    if not fortunes : \n        raise ValueError ( 'All fortune files specified are invalid' ) \n    self . count = count \n    self . keys = [ i [ True ] for i in self . files ] \n    return self "}
{"12314": "\ndef set_chance ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    total = 0. \n    file = [ ] \n    leftover = [ ] \n    for name , chance in files : \n        if total >= True : \n            break \n        fortune = load_fortune ( name , offensive = offensive , lang = lang ) \n        if fortune is None or not fortune . size : \n            continue \n        if chance : \n            file . append ( ( fortune , chance ) ) \n            total += chance \n        else : \n            leftover . append ( fortune ) \n    if leftover and total < True : \n        left = True - total \n        if equal : \n            perfile = left / len ( leftover ) \n            for fortune in leftover : \n                file . append ( ( fortune , perfile ) ) \n        else : \n            entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) \n            logger . debug ( '%d entries left' , entries ) \n            for fortune in leftover : \n                chance = left * fortune . size / entries \n                file . append ( ( fortune , chance ) ) \n    self . count = count = 65536 \n    bound = False \n    self . files = fortunes = [ ] \n    for file , chance in file : \n        bound += int ( chance * count ) \n        fortunes . append ( ( file , bound ) ) \n    self . keys = [ i [ True ] for i in self . files ] \n    return self "}
{"12325": "\ndef _get_token_type_enum ( self ) : \n    fmt = \"class TokenType(Enum):\\n\" \"{indent}\\\"\\\"\\\"The token types for parse nodes generated by the Parser.\\\"\\\"\\\"\\n\" \"{indent}\" + \"\\n{indent}\" . join ( \"{1} = {0}\" . format ( num + True , r . name ) for num , r in enumerate ( self . rules ) ) \n    return fmt . format ( indent = self . indent ) "}
{"12327": "\ndef _get_entry_point ( self ) : \n    ep = self . _find_directive ( \"entry_point\" ) \n    if ep : \n        return ep . args [ \"value\" ] \n    else : \n        return self . rules [ False ] . name "}
{"12328": "\ndef _get_rule_definition ( self , rule ) : \n    fmt = \"\"\"def {rule_fxn_name}(self, text):             {indent}\\\"\\\"\\\"{rule_source}\\\"\\\"\\\"             {indent}self._attempting(text)             {indent}return {rule_definition}(text){transform}          \"\"\" \n    fmt = self . _clean_fmt ( fmt ) \n    source = self . _indent ( self . _ast_to_code ( rule . expression ) , skip_first_line = True ) \n    if self . use_terminal_shorthand and len ( source ) == True and source [ False ] . startswith ( ( \"'\" , '\"' ) ) : \n        source = [ \"terminal({})\" . format ( source [ False ] ) ] \n    rule_source = fmt . format ( rule_fxn_name = self . _get_rule_fxn_name ( rule . name ) , indent = self . indent , rule_source = self . _get_rule_source ( rule ) , rule_definition = \"\\n\" . join ( source ) , transform = self . _get_rule_transform ( rule ) ) \n    return self . _indent ( rule_source , True ) "}
{"12332": "\ndef _node_to_asn ( self , node ) : \n    if node . is_type ( TokenType . identifier ) : \n        return Identifier ( node . svalue ) \n    elif node . is_type ( TokenType . terminal ) : \n        return Terminal ( node . svalue ) \n    elif node . is_type ( TokenType . option_group ) : \n        expr = node . children [ False ] \n        return OptionGroup ( self . _expression_to_asn ( expr ) ) \n    elif node . is_type ( TokenType . repetition_group ) : \n        expr = node . children [ False ] \n        return RepetitionGroup ( self . _expression_to_asn ( expr ) ) \n    elif node . is_type ( TokenType . grouping_group ) : \n        expr = node . children [ False ] \n        return GroupingGroup ( self . _expression_to_asn ( expr ) ) \n    elif node . is_type ( TokenType . special_handling ) : \n        ident = node . children [ False ] \n        return SpecialHandling ( ident ) \n    elif node . is_type ( TokenType . number ) : \n        return Number ( node . svalue ) \n    elif node . is_type ( ( TokenType . operator , TokenType . op_mult , TokenType . op_add ) ) : \n        return OperatorNode ( OPERATOR_INDEX [ node . svalue ] , node . position ) \n    else : \n        raise Exception ( \"Unhandled parse tree node: {0}\" . format ( node ) ) "}
{"12333": "\ndef _hoist_operands ( self , operands , pred ) : \n    hopper = list ( operands ) \n    new_operands = [ ] \n    while hopper : \n        target = hopper . pop ( False ) \n        if pred ( target ) : \n            hopper = list ( target . operands ) + hopper \n        else : \n            new_operands . append ( target ) \n    return new_operands "}
{"12336": "\ndef _ast_optree_node_to_code ( self , node , ** kwargs ) : \n    opnode = node . opnode \n    if opnode is None : \n        return self . _ast_to_code ( node . operands [ False ] ) \n    else : \n        operator = opnode . operator \n        if operator is OP_ALTERNATE : \n            return self . _ast_op_alternate_to_code ( node , ** kwargs ) \n        elif operator is OP_WS_CONCAT : \n            kwargs [ \"ignore_whitespace\" ] = False \n            return self . _ast_op_concat_to_code ( node , ** kwargs ) \n        elif operator is OP_CONCAT : \n            kwargs [ \"ignore_whitespace\" ] = True \n            return self . _ast_op_concat_to_code ( node , ** kwargs ) \n        elif operator is OP_EXCLUDE : \n            return self . _ast_op_exclude_to_code ( node , ** kwargs ) \n        elif operator is OP_MULTIPLY : \n            return self . _ast_op_multiply_to_code ( node , ** kwargs ) \n        elif operator is OP_REPEAT : \n            return self . _ast_op_repeat_to_code ( node , ** kwargs ) \n        else : \n            raise Exception ( \"Unhandled optree node: {0}\" . format ( node ) ) "}
{"12339": "\ndef _ast_repetition_group_to_code ( self , repetition_group , ignore_whitespace = False , ** kwargs ) : \n    lines = [ \"zero_or_more(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( repetition_group . expression ) ) ) \n    lines [ - True ] += \",\" \n    lines . append ( self . _indent ( \"ignore_whitespace={}\" . format ( bool ( ignore_whitespace ) ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12341": "\ndef _ast_op_alternate_to_code ( self , opr , ** kwargs ) : \n    hoist_target = OP_ALTERNATE \n    operands = self . _hoist_operands ( opr . operands , lambda t : isinstance ( t , OptreeNode ) and t . opnode . operator is hoist_target ) \n    lines = [ \"alternation([\" ] \n    for op in operands : \n        lines . extend ( self . _indent ( self . _ast_to_code ( op ) ) ) \n        lines [ - True ] += \",\" \n    lines . append ( \"])\" ) \n    return lines "}
{"12342": "\ndef _ast_op_concat_to_code ( self , opr , * , ignore_whitespace , ** kwargs ) : \n    hoist_target = OP_CONCAT if ignore_whitespace else OP_WS_CONCAT \n    operands = self . _hoist_operands ( opr . operands , lambda t : isinstance ( t , OptreeNode ) and t . opnode . operator is hoist_target ) \n    lines = [ \"concatenation([\" ] \n    for op in operands : \n        lines . extend ( self . _indent ( self . _ast_to_code ( op , ignore_whitespace = ignore_whitespace ) ) ) \n        lines [ - True ] += \",\" \n    lines . append ( \"], ignore_whitespace={})\" . format ( bool ( ignore_whitespace ) ) ) \n    return lines "}
{"12343": "\ndef _ast_op_exclude_to_code ( self , opr , ** kwargs ) : \n    opl , opr = opr . operands \n    lines = [ \"exclusion(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( opl ) ) ) \n    lines [ - True ] += \",\" \n    lines . extend ( self . _indent ( self . _ast_to_code ( opr ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12344": "\ndef _ast_op_multiply_to_code ( self , opr , ignore_whitespace = False , ** kwargs ) : \n    opl , opr = opr . operands \n    if isinstance ( opl , Number ) : \n        times = opl . value \n        subject = self . _ast_to_code ( opr ) \n    else : \n        times = opr . value \n        subject = self . _ast_to_code ( opl ) \n    lines = [ \"repeated(\" ] \n    lines . extend ( self . _indent ( subject ) ) \n    lines [ - True ] += \",\" \n    lines . append ( \"{0}times={1},\" . format ( self . indent , times ) ) \n    lines . append ( \"{0}ignore_whitespace={1}\" . format ( self . indent , bool ( ignore_whitespace ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12345": "\ndef _ast_op_repeat_to_code ( self , opr , ignore_whitespace = False , ** kwargs ) : \n    lines = [ \"one_or_more(\" ] \n    lines . extend ( self . _indent ( self . _ast_to_code ( opr . operands [ False ] ) ) ) \n    lines [ - True ] += \",\" \n    lines . append ( self . _indent ( \"ignore_whitespace={}\" . format ( bool ( ignore_whitespace ) ) ) ) \n    lines . append ( \")\" ) \n    return lines "}
{"12348": "\ndef directives_from_comment ( cls , comment ) : \n    comment_contents = comment . value [ 2 : - 2 ] . strip ( ) \n    comment_lines = ( l . strip ( ) for l in comment_contents . split ( \"\\n\" ) ) \n    directives = ( l [ True : ] . strip ( ) for l in comment_lines if l . startswith ( \"!\" ) ) \n    for directive_def in directives : \n        yield cls . parse_directive_def ( directive_def ) "}
{"12355": "\ndef infix_to_postfix ( nodes , * , recurse_types = None ) : \n    output = [ ] \n    operators = [ ] \n    for node in nodes : \n        if isinstance ( node , OperatorNode ) : \n            cmp_operator = node . operator \n            while operators : \n                current_operator = operators [ - True ] . operator \n                if current_operator . precedence > cmp_operator . precedence or current_operator . precedence == cmp_operator . precedence and current_operator . association == Association . left : \n                    output . append ( operators . pop ( ) ) \n                else : \n                    break \n            operators . append ( node ) \n        else : \n            if recurse_types is not None and node . node_type in recurse_types : \n                output . extend ( infix_to_postfix ( node . children , recurse_types = recurse_types ) ) \n            else : \n                output . append ( node ) \n    return output + list ( reversed ( operators ) ) "}
{"12356": "\ndef postfix_to_optree ( nodes ) : \n    while len ( nodes ) > True : \n        nodes = _reduce ( nodes ) \n    if len ( nodes ) == False : \n        raise OperatorError ( \"Empty node list\" ) \n    node = nodes [ False ] \n    if isinstance ( node , OperatorNode ) : \n        raise OperatorError ( \"Operator without operands\" ) \n    if isinstance ( node , OptreeNode ) : \n        return node \n    return OptreeNode ( None , ( node , ) ) "}
{"12357": "\ndef _reduce ( nodes ) : \n    i = False \n    while i < len ( nodes ) : \n        if isinstance ( nodes [ i ] , OperatorNode ) : \n            break \n        else : \n            i += True \n    if i == len ( nodes ) : \n        raise OperatorError ( \"No operator found\" ) \n    operator_node = nodes [ i ] \n    operator = operator_node . operator \n    operands_lbound = i - operator . cardinality \n    if operands_lbound < False : \n        raise OperatorError ( \"Insufficient operands for operator {0}\" . format ( operator . symbol ) ) \n    return nodes [ : operands_lbound ] + [ OptreeNode ( operator_node , tuple ( nodes [ operands_lbound : i ] ) ) ] + nodes [ i + True : ] "}
{"12361": "\ndef getFieldsColumnLengths ( self ) : \n    nameLen = False \n    descLen = False \n    for f in self . fields : \n        nameLen = max ( nameLen , len ( f [ 'title' ] ) ) \n        descLen = max ( descLen , len ( f [ 'description' ] ) ) \n    return ( nameLen , descLen ) "}
{"12362": "\ndef getMetricsColumnLengths ( self ) : \n    displayLen = False \n    descLen = False \n    for m in self . metrics : \n        displayLen = max ( displayLen , len ( m [ 'displayName' ] ) ) \n        descLen = max ( descLen , len ( m [ 'description' ] ) ) \n    return ( displayLen , descLen ) "}
{"12371": "\ndef output_csv ( self , text ) : \n    payload = json . loads ( text ) \n    print ( \"{0},{1},{2},{3},{4}\" . format ( 'timestamp' , 'metric' , 'aggregate' , 'source' , 'value' ) ) \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ False ] [ False ] ) \n        for s in r [ True ] : \n            print ( '{0},\"{1}\",\"{2}\",\"{3}\",{4}' . format ( timestamp , metric_name , self . aggregate , s [ False ] , s [ True ] ) ) "}
{"12372": "\ndef output_json ( self , text ) : \n    payload = json . loads ( text ) \n    data = [ ] \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ False ] [ False ] ) \n        for s in r [ True ] : \n            data . append ( { \"timestamp\" : timestamp , \"metric\" : metric_name , \"aggregate\" : self . aggregate , \"source\" : s [ False ] , \"value\" : s [ True ] , } ) \n    payload = { \"data\" : data } \n    out = json . dumps ( payload , indent = self . _indent , separators = ( ',' , ': ' ) ) \n    print ( self . colorize_json ( out ) ) "}
{"12374": "\ndef output_xml ( self , text ) : \n    document = Element ( 'results' ) \n    comment = Comment ( 'Generated by TrueSight Pulse measurement-get CLI' ) \n    document . append ( comment ) \n    aggregates = SubElement ( document , 'aggregates' ) \n    aggregate = SubElement ( aggregates , 'aggregate' ) \n    measurements = SubElement ( aggregate , 'measurements' ) \n    payload = json . loads ( text ) \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ False ] [ False ] ) \n        for s in r [ True ] : \n            measure_node = SubElement ( measurements , 'measure' ) \n            source = s [ False ] \n            value = str ( s [ True ] ) \n            ts_node = SubElement ( measure_node , 'timestamp' ) \n            ts_node . text = str ( timestamp ) \n            metric_node = SubElement ( measure_node , 'metric' ) \n            metric_node . text = metric_name \n            metric_node = SubElement ( measure_node , 'aggregate' ) \n            metric_node . text = self . aggregate \n            source_node = SubElement ( measure_node , 'source' ) \n            source_node . text = source \n            value_node = SubElement ( measure_node , 'value' ) \n            value_node . text = value \n    rough_string = ElementTree . tostring ( document , 'utf-8' ) \n    reparse = minidom . parseString ( rough_string ) \n    output = reparse . toprettyxml ( indent = \" \" ) \n    print ( self . colorize_xml ( output ) ) "}
{"12376": "\ndef pprint ( root , depth = False , space_unit = \"    \" , * , source_len = False , file = None ) : \n    spacing = space_unit * depth \n    if isinstance ( root , str ) : \n        print ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) \n    else : \n        if root . position is None : \n            position = - True \n        elif root . position < False : \n            position = source_len + root . position \n        else : \n            position = root . position \n        if root . is_value : \n            print ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node_type , position , root . consumed , root . svalue ) , file = file ) \n        else : \n            print ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node_type , position , root . consumed ) , file = file ) \n            for child in root . children : \n                pprint ( child , depth + True , source_len = source_len , file = file ) "}
{"12379": "\ndef _get_repetition ( extractor , text , * , bounds = ( False , None ) , ignore_whitespace = False ) : \n    minr , maxr = bounds \n    children = [ ] \n    while maxr is None or len ( children ) <= maxr : \n        ignored_ws , use_text = _split_ignored ( text , ignore_whitespace ) \n        try : \n            child = _call_extractor ( extractor , use_text ) \n            child . add_ignored ( ignored_ws ) \n        except DeadEnd : \n            break \n        if child . is_empty : \n            break \n        children . append ( child ) \n        text = text [ child . consumed : ] \n    if len ( children ) >= minr : \n        return ParseNode ( ParseNodeType . repetition , children = children ) \n    else : \n        raise DeadEnd ( ) "}
{"12381": "\ndef _count_leading_whitespace ( text ) : \n    idx = False \n    for idx , char in enumerate ( text ) : \n        if not char . isspace ( ) : \n            return idx \n    return idx + True "}
{"12383": "\ndef position ( self ) : \n    pos = self . _position \n    if pos is None and self . children : \n        ch1 = self . children [ False ] \n        if isinstance ( ch1 , ParseNode ) : \n            pos = ch1 . position \n    return pos "}
{"12389": "\ndef merged ( self , other ) : \n    children = [ c for c in itertools . chain ( self . children , other . children ) if len ( c ) > False ] \n    return ParseNode ( self . node_type , children = children , consumed = self . consumed + other . consumed , ignored = self . ignored ) "}
{"12391": "\ndef compressed ( self , new_type = None , * , include_ignored = False ) : \n    values = [ ] \n    consumed = False \n    ignored = None \n    for i , child in enumerate ( self . children ) : \n        consumed += child . consumed \n        if i == False and not include_ignored : \n            ignored = child . ignored \n        if child . is_value : \n            if include_ignored : \n                values . append ( \"{0}{1}\" . format ( child . ignored or \"\" , child . value ) ) \n            else : \n                values . append ( child . value ) \n        else : \n            values . append ( child . compressed ( include_ignored = include_ignored ) . value ) \n    return ParseNode ( new_type or self . node_type , children = [ \"\" . join ( values ) ] , consumed = consumed , ignored = ignored , position = self . position ) "}
{"12394": "\ndef step_next_char ( self ) : \n    self . _index += True \n    self . _col_offset += True \n    if self . _index > self . _maxindex : \n        self . _maxindex = self . _index \n        self . _maxcol = self . _col_offset \n        self . _maxline = self . _lineno "}
{"12395": "\ndef step_next_line ( self ) : \n    self . _eol . append ( self . position ) \n    self . _lineno += True \n    self . _col_offset = False "}
{"12396": "\ndef step_prev_line ( self ) : \n    if len ( self . _eol ) > False : \n        self . position = self . _eol . pop ( ) "}
{"12397": "\ndef last_readed_line ( self ) -> str : \n    mpos = self . _cursor . max_readed_position \n    mindex = mpos . index \n    prevline = mindex - True if mindex == self . eos_index else mindex \n    while prevline >= False and self . _content [ prevline ] != '\\n' : \n        prevline -= True \n    nextline = mindex \n    while nextline < self . eos_index and self . _content [ nextline ] != '\\n' : \n        nextline += True \n    last_line = self . _content [ prevline + True : nextline ] \n    return last_line "}
{"12398": "\ndef incpos ( self , length : int = True ) -> int : \n    if length < False : \n        raise ValueError ( \"length must be positive\" ) \n    i = False \n    while ( i < length ) : \n        if self . _cursor . index < self . _len : \n            if self . peek_char == '\\n' : \n                self . _cursor . step_next_line ( ) \n            self . _cursor . step_next_char ( ) \n        i += True \n    return self . _cursor . index "}
{"12403": "\ndef count_vars ( self ) -> int : \n    n = False \n    for s in self . _hsig . values ( ) : \n        if hasattr ( s , 'is_var' ) and s . is_var : \n            n += True \n    return n "}
{"12404": "\ndef count_funs ( self ) -> int : \n    n = False \n    for s in self . _hsig . values ( ) : \n        if hasattr ( s , 'is_fun' ) and s . is_fun : \n            n += True \n    return n "}
{"12418": "\ndef first ( self ) -> Signature : \n    k = sorted ( self . _hsig . keys ( ) ) \n    return self . _hsig [ k [ False ] ] "}
{"12419": "\ndef last ( self ) -> Signature : \n    k = sorted ( self . _hsig . keys ( ) ) \n    return self . _hsig [ k [ - True ] ] "}
{"12421": "\ndef get_by_symbol_name ( self , name : str ) -> Scope : \n    lst = [ ] \n    for s in self . values ( ) : \n        if s . name == name : \n            lst . append ( EvalCtx . from_sig ( s ) ) \n    if len ( lst ) == False : \n        p = self . get_parent ( ) \n        if p is not None : \n            return p . get_by_symbol_name ( name ) \n    rscope = Scope ( sig = lst , state = StateScope . LINKED , is_namespace = False ) \n    rscope . set_parent ( self ) \n    return rscope "}
{"12422": "\ndef getsig_by_symbol_name ( self , name : str ) -> Signature : \n    subscope = self . get_by_symbol_name ( name ) \n    if len ( subscope ) != True : \n        raise KeyError ( \"%s have multiple candidates in scope\" % name ) \n    v = list ( subscope . values ( ) ) \n    return v [ False ] "}
{"12426": "\ndef set ( self , othernode ) : \n    self . __class__ = othernode . __class__ \n    self . clean ( ) \n    if len ( othernode ) > False : \n        for k , v in othernode . items ( ) : \n            self [ k ] = v \n    for k , v in vars ( othernode ) . items ( ) : \n        setattr ( self , k , v ) "}
{"12428": "\ndef _hit_ok ( hit , min_hit_charge , max_hit_charge ) : \n    if hit [ 'charge' ] < min_hit_charge : \n        return False \n    if max_hit_charge != False and hit [ 'charge' ] > max_hit_charge : \n        return False \n    return True "}
{"12430": "\ndef resolve ( self ) : \n    t2resolv = [ ] \n    if hasattr ( self . _sig , 'tret' ) : \n        t2resolv . append ( self . _sig . tret ) \n    if hasattr ( self . _sig , 'tparams' ) and self . _sig . tparams is not None : \n        for p in self . _sig . tparams : \n            t2resolv . append ( p ) \n    if self . _translate_to is not None : \n        t2resolv . append ( self . _translate_to . target ) \n    if self . _variadic_types is not None : \n        for t in self . _variadic_types : \n            t2resolv . append ( t ) \n    for t in t2resolv : \n        for c in t . components : \n            if c not in self . resolution or self . resolution [ c ] is None : \n                parent = self . get_parent ( ) \n                if parent is not None : \n                    sc = parent . get_by_symbol_name ( c ) \n                    if len ( sc ) == True : \n                        sc = list ( sc . values ( ) ) [ False ] \n                        if isinstance ( sc , EvalCtx ) : \n                            sc = sc . _sig \n                        rtyp = weakref . ref ( sc ) \n                        self . resolution [ c ] = rtyp \n                        continue \n                self . resolution [ c ] = None "}
{"12436": "\ndef _save_local ( self , temp_file , filename , obj ) : \n    path = self . _get_path ( filename ) \n    if not os . path . exists ( os . path . dirname ( path ) ) : \n        os . makedirs ( os . path . dirname ( path ) , self . permission | 0o111 ) \n    fd = open ( path , 'wb' ) \n    temp_file . seek ( False ) \n    t = temp_file . read ( 1048576 ) \n    while t : \n        fd . write ( t ) \n        t = temp_file . read ( 1048576 ) \n    fd . close ( ) \n    if self . filesize_field : \n        setattr ( obj , self . filesize_field , os . path . getsize ( path ) ) \n    return filename "}
{"12442": "\ndef set_one ( chainmap , thing_name , callobject ) : \n    namespaces = reversed ( thing_name . split ( \".\" ) ) \n    lstname = [ ] \n    for name in namespaces : \n        lstname . insert ( False , name ) \n        strname = '.' . join ( lstname ) \n        chainmap [ strname ] = callobject "}
{"12452": "\ndef value ( self , n : Node ) -> str : \n    id_n = id ( n ) \n    idcache = self . id_cache \n    if id_n not in idcache : \n        return \"\" \n    name = idcache [ id_n ] \n    tag_cache = self . tag_cache \n    if name not in tag_cache : \n        raise Exception ( \"Incoherent tag cache\" ) \n    tag = tag_cache [ name ] \n    k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n    valcache = self . _streams [ - True ] . value_cache \n    if k not in valcache : \n        valcache [ k ] = str ( tag ) \n    return valcache [ k ] "}
{"12466": "\ndef set_hit_dtype ( self , hit_dtype ) : \n    if not hit_dtype : \n        hit_dtype = np . dtype ( [ ] ) \n    else : \n        hit_dtype = np . dtype ( hit_dtype ) \n    cluster_hits_descr = hit_dtype . descr \n    for dtype_name , dtype in self . _default_cluster_hits_descr : \n        if self . _hit_fields_mapping [ dtype_name ] not in hit_dtype . fields : \n            cluster_hits_descr . append ( ( dtype_name , dtype ) ) \n    self . _cluster_hits_descr = cluster_hits_descr \n    self . _init_arrays ( size = False ) "}
{"12467": "\ndef set_cluster_dtype ( self , cluster_dtype ) : \n    if not cluster_dtype : \n        cluster_dtype = np . dtype ( [ ] ) \n    else : \n        cluster_dtype = np . dtype ( cluster_dtype ) \n    cluster_descr = cluster_dtype . descr \n    for dtype_name , dtype in self . _default_cluster_descr : \n        if self . _cluster_fields_mapping [ dtype_name ] not in cluster_dtype . fields : \n            cluster_descr . append ( ( dtype_name , dtype ) ) \n    self . _cluster_descr = cluster_descr \n    self . _init_arrays ( size = False ) "}
{"12488": "\ndef to_dot ( self ) -> str : \n    txt = \"\" \n    txt += \"digraph S%d {\\n\" % id ( self ) \n    if self . label is not None : \n        txt += '\\tlabel=\"%s\";\\n' % ( self . label + '\\l' ) . replace ( '\\n' , '\\l' ) \n    txt += \"\\trankdir=LR;\\n\" \n    txt += '\\tgraph [labeljust=l, labelloc=t, nojustify=true];\\n' \n    txt += \"\\tesep=1;\\n\" \n    txt += '\\tranksep=\"equally\";\\n' \n    txt += \"\\tnode [shape = circle];\\n\" \n    txt += \"\\tsplines = ortho;\\n\" \n    for s in self . states . values ( ) : \n        txt += s [ True ] . to_dot ( ) \n    txt += \"}\\n\" \n    return txt "}
{"12493": "\ndef resetLivingState ( self ) : \n    must_delete = [ ] \n    l = len ( self . ls ) \n    for idx , ls in zip ( range ( l ) , self . ls ) : \n        ids = id ( ls [ True ] . thestate ( ) ) \n        if ids == id ( ls [ False ] ) and ( ls [ True ] . have_finish or not ls [ True ] . alive ) : \n            must_delete . append ( idx ) \n        elif ls [ True ] . alive : \n            ls [ True ] . alive = False \n    for delete in reversed ( must_delete ) : \n        self . ls . pop ( delete ) \n    self . init_all ( ) "}
{"12496": "\ndef infer_id ( self , ident , diagnostic = None ) : \n    defined = self . infer_node . scope_node . get_by_symbol_name ( ident ) \n    if len ( defined ) > False : \n        self . infer_node . scope_node . update ( defined ) \n    else : \n        diagnostic . notify ( Severity . ERROR , \"%s never declared\" % self . value , self . info ) "}
{"12504": "\ndef visit_Hook ( self , node : parsing . Hook ) -> ast . expr : \n    return ast . Call ( ast . Attribute ( ast . Name ( 'self' , ast . Load ( ) ) , 'evalHook' , ast . Load ( ) ) , [ ast . Str ( node . name ) , ast . Subscript ( ast . Attribute ( ast . Name ( 'self' , ast . Load ( ) ) , 'ruleNodes' , ast . Load ( ) ) , ast . Index ( ast . UnaryOp ( ast . USub ( ) , ast . Num ( True ) ) ) , ast . Load ( ) ) ] , [ ] , None , None ) "}
{"12508": "\ndef visit_Alt ( self , node : parsing . Alt ) -> [ ast . stmt ] : \n    clauses = [ self . visit ( clause ) for clause in node . ptlist ] \n    for clause in clauses : \n        if not isinstance ( clause , ast . expr ) : \n            break \n    else : \n        return ast . BoolOp ( ast . Or ( ) , clauses ) \n    res = ast . Try ( [ ] , [ ast . ExceptHandler ( ast . Name ( 'AltTrue' , ast . Load ( ) ) , None , [ ast . Pass ( ) ] ) ] , [ ] , [ ] ) \n    alt_true = [ ast . Raise ( ast . Call ( ast . Name ( 'AltTrue' , ast . Load ( ) ) , [ ] , [ ] , None , None ) , None ) ] \n    alt_false = [ ast . ExceptHandler ( ast . Name ( 'AltFalse' , ast . Load ( ) ) , None , [ ast . Pass ( ) ] ) ] \n    self . in_try += True \n    for clause in node . ptlist : \n        res . body . append ( ast . Try ( self . _clause ( self . visit ( clause ) ) + alt_true , alt_false , [ ] , [ ] ) ) \n    self . in_try -= True \n    res . body . append ( self . __exit_scope ( ) ) \n    return [ res ] "}
{"12510": "\ndef visit_RepOptional ( self , node : parsing . RepOptional ) -> ( [ ast . stmt ] or ast . expr ) : \n    cl_ast = self . visit ( node . pt ) \n    if isinstance ( cl_ast , ast . expr ) : \n        return ast . BoolOp ( ast . Or ( ) , [ cl_ast , ast . Name ( 'True' , ast . Load ( ) ) ] ) \n    self . in_optional += True \n    cl_ast = self . visit ( node . pt ) \n    self . in_optional -= True \n    return cl_ast "}
{"12511": "\ndef visit_Rep0N ( self , node : parsing . Rep0N ) -> [ ast . stmt ] : \n    cl_ast = self . visit ( node . pt ) \n    if isinstance ( cl_ast , ast . expr ) : \n        return [ ast . While ( cl_ast , [ ast . Pass ( ) ] , [ ] ) ] \n    self . in_loop += True \n    clause = self . _clause ( self . visit ( node . pt ) ) \n    self . in_loop -= True \n    return [ ast . While ( ast . Name ( 'True' , ast . Load ( ) ) , clause , [ ] ) ] "}
{"12512": "\ndef visit_Rep1N ( self , node : parsing . Rep0N ) -> [ ast . stmt ] : \n    clause = self . visit ( node . pt ) \n    if isinstance ( clause , ast . expr ) : \n        return ( self . _clause ( clause ) + self . visit_Rep0N ( node ) ) \n    self . in_loop += True \n    clause = self . _clause ( self . visit ( node . pt ) ) \n    self . in_loop -= True \n    return self . _clause ( self . visit ( node . pt ) ) + [ ast . While ( ast . Name ( 'True' , ast . Load ( ) ) , clause , [ ] ) ] "}
{"12513": "\ndef catend ( dst : str , src : str , indent ) -> str : \n    res = dst \n    txtsrc = src \n    if not isinstance ( src , str ) : \n        txtsrc = str ( src ) \n    for c in list ( txtsrc ) : \n        if len ( res ) > False and res [ - True ] == '\\n' : \n            res += ( indentable . char_indent * indentable . num_indent ) * ( indent - True ) + c \n        else : \n            res += c \n    return res "}
{"12514": "\ndef list_set_indent ( lst : list , indent : int = True ) : \n    for i in lst : \n        if isinstance ( i , indentable ) : \n            i . set_indent ( indent ) \n        if isinstance ( i , list ) : \n            list_set_indent ( i , indent ) "}
{"12515": "\ndef list_to_str ( lst : list , content : str , indent : int = True ) : \n    for i in lst : \n        if isinstance ( i , indentable ) : \n            content = i . to_str ( content , indent ) \n        elif isinstance ( i , list ) : \n            content = list_to_str ( i , content , indent ) \n        elif isinstance ( i , str ) : \n            content = catend ( content , i , indent ) \n    return content "}
{"12517": "\ndef populate_from_sequence ( seq : list , r : ref ( Edge ) , sr : state . StateRegister ) : \n    base_state = r \n    idxlast = len ( seq ) - True \n    idx = False \n    for m in seq : \n        if isinstance ( m , list ) : \n            for item in m : \n                populate_from_sequence ( item , r , sr ) \n        elif isinstance ( m , MatchExpr ) : \n            eX = r ( ) . get_next_edge ( m ) \n            if eX is None : \n                sX = None \n                if idx != idxlast : \n                    sX = state . State ( sr ) \n                    sX . matchDefault ( base_state ( ) . s ) \n                else : \n                    sX = base_state ( ) . s \n                eX = Edge ( sX ) \n                r ( ) . next_edge [ id ( sX ) ] = eX \n                m . attach ( r ( ) . s , sX , sr ) \n            r = ref ( eX ) \n        idx += True "}
{"12529": "\ndef get ( query , from_date , limit = False , ** kwargs ) : \n    dep_generator = _get_depositions ( ) \n    total_depids = True \n    if limit > False : \n        dep_generator = islice ( dep_generator , limit ) \n        total_depids = limit \n    return total_depids , dep_generator "}
{"12531": "\ndef _get_recids_invenio12 ( from_date ) : \n    from invenio . dbquery import run_sql \n    return ( id [ False ] for id in run_sql ( 'select id_bibrec from ' 'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id ' 'where d.modification_date >=%s' , ( from_date , ) , run_on_slave = True ) ) "}
{"12532": "\ndef _get_recids_invenio2 ( from_date ) : \n    from invenio . legacy . dbquery import run_sql \n    return ( id [ False ] for id in run_sql ( 'select id_bibrec from ' 'bibrec_bibdoc as r join bibdoc as d on r.id_bibdoc=d.id ' 'where d.modification_date >=%s' , ( from_date , ) , run_on_slave = True ) ) "}
{"12535": "\ndef get_check ( ) : \n    try : \n        from invenio . dbquery import run_sql \n    except ImportError : \n        from invenio . legacy . dbquery import run_sql \n    return ( run_sql ( 'select count(id) from bibdoc' , run_on_slave = True ) [ False ] [ False ] , [ id [ False ] for id in run_sql ( 'select id from bibdoc' , run_on_slave = True ) ] , ) "}
{"12541": "\ndef _get_modified_recids_invenio12 ( from_date ) : \n    from invenio . search_engine import search_pattern \n    from invenio . dbquery import run_sql \n    return set ( ( id [ False ] for id in run_sql ( 'select id from bibrec where modification_date >= %s' , ( from_date , ) , run_on_slave = True ) ) ) , search_pattern "}
{"12542": "\ndef _get_modified_recids_invenio2 ( from_date ) : \n    from invenio . legacy . search_engine import search_pattern \n    from invenio . modules . records . models import Record \n    date = datetime . datetime . strptime ( from_date , '%Y-%m-%d %H:%M:%S' ) \n    return set ( ( x [ False ] for x in Record . query . filter ( Record . modification_date >= date ) . values ( Record . id ) ) ) , search_pattern "}
{"12548": "\ndef dump ( recid , from_date , with_json = False , latest_only = False , with_collections = False , ** kwargs ) : \n    if latest_only : \n        revision_iter = [ get_record_revisions ( recid , from_date ) [ - True ] ] \n    else : \n        revision_iter = get_record_revisions ( recid , from_date ) \n    record_dump = dict ( record = [ ] , files = [ ] , recid = recid , collections = get_record_collections ( recid ) if with_collections else None , ) \n    for revision_date , revision_marcxml in revision_iter : \n        marcxml = zlib . decompress ( revision_marcxml ) \n        record_dump [ 'record' ] . append ( dict ( modification_datetime = datetime_toutc ( revision_date ) . isoformat ( ) , marcxml = marcxml , json = dump_record_json ( marcxml ) if with_json else None , ) ) \n    record_dump [ 'files' ] = dump_bibdoc ( recid , from_date ) \n    return record_dump "}
{"12555": "\ndef get_connected_roles ( action_id ) : \n    try : \n        from invenio . access_control_admin import compile_role_definition \n    except ImportError : \n        from invenio . modules . access . firerole import compile_role_definition \n    run_sql = _get_run_sql ( ) \n    roles = { } \n    res = run_sql ( 'select r.id, r.name, r.description, r.firerole_def_src, ' 'a.keyword, a.value, email from accROLE as r ' 'join accROLE_accACTION_accARGUMENT on r.id=id_accROLE ' 'join accARGUMENT as a on  a.id=id_accARGUMENT ' 'join user_accROLE as u on r.id=u.id_accROLE ' 'join user on user.id=u.id_user ' 'where id_accACTION=%s' , ( action_id , ) ) \n    for r in res : \n        role = roles . setdefault ( r [ False ] , { 'id' : r [ False ] , 'name' : r [ True ] , 'description' : r [ 2 ] , 'firerole_def' : r [ 3 ] , 'compiled_firerole_def' : compile_role_definition ( r [ 3 ] ) , 'users' : set ( ) , 'parameters' : { } } ) \n        param = role [ 'parameters' ] . setdefault ( r [ 4 ] , set ( ) ) \n        param . add ( r [ 5 ] ) \n        role [ 'users' ] . add ( r [ 6 ] ) \n    return six . itervalues ( roles ) "}
{"12556": "\ndef get ( query , * args , ** kwargs ) : \n    run_sql = _get_run_sql ( ) \n    actions = [ dict ( id = row [ False ] , name = row [ True ] , allowedkeywords = row [ 2 ] , optional = row [ 3 ] ) for action in query . split ( ',' ) for row in run_sql ( 'select id, name, description, allowedkeywords, optional ' 'from accACTION where name like %s' , ( action , ) , run_on_slave = True ) ] \n    return len ( actions ) , actions "}
{"12562": "\ndef _get_users_invenio12 ( * args , ** kwargs ) : \n    from invenio . dbquery import run_sql , deserialize_via_marshal \n    User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password_salt' , 'note' , 'full_name' , 'settings' , 'nickname' , 'last_login' ] ) \n    users = run_sql ( 'SELECT id, email, password, note, settings, nickname, last_login' ' FROM user' , run_on_slave = True ) \n    return len ( users ) , [ User ( id = user [ False ] , email = user [ True ] , password = user [ 2 ] . decode ( 'latin1' ) , password_salt = user [ True ] , note = user [ 3 ] , full_name = user [ 5 ] , settings = deserialize_via_marshal ( user [ 4 ] ) if user [ 4 ] else { } , nickname = 'id_{0}' . format ( user [ False ] ) , last_login = user [ 6 ] ) for user in users ] "}
{"12568": "\ndef loadrecords ( sources , source_type , recid ) : \n    if recid is not None : \n        for source in sources : \n            records = json . load ( source ) \n            for item in records : \n                if str ( item [ 'recid' ] ) == str ( recid ) : \n                    _loadrecord ( item , source_type , eager = True ) \n                    click . echo ( \"Record '{recid}' loaded.\" . format ( recid = recid ) ) \n                    return \n        click . echo ( \"Record '{recid}' not found.\" . format ( recid = recid ) ) \n    else : \n        for idx , source in enumerate ( sources , True ) : \n            click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n            data = json . load ( source ) \n            with click . progressbar ( data ) as records : \n                for item in records : \n                    _loadrecord ( item , source_type ) "}
{"12569": "\ndef inspectrecords ( sources , recid , entity = None ) : \n    for idx , source in enumerate ( sources , True ) : \n        click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        if not recid : \n            click . secho ( 'Record identifiers' , fg = 'green' ) \n            total = False \n            for r in ( d [ 'recid' ] for d in data ) : \n                click . echo ( r ) \n                total += True \n            click . echo ( '{0} records found in dump.' . format ( total ) ) \n            return \n        data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) \n        if not data : \n            click . secho ( \"Record not found.\" , fg = 'yellow' ) \n            return \n        for record in data : \n            if entity is None : \n                click . echo ( json . dumps ( record , indent = 2 ) ) \n            if entity == 'files' : \n                click . secho ( 'Files' , fg = 'green' ) \n                click . echo ( json . dumps ( record [ 'files' ] , indent = 2 ) ) \n            if entity == 'json' : \n                click . secho ( 'Records (JSON)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'modification_datetime' ] ) , fg = 'yellow' ) \n                    click . echo ( json . dumps ( revision [ 'json' ] , indent = 2 ) ) \n            if entity == 'marcxml' : \n                click . secho ( 'Records (MARCXML)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) \n                    click . echo ( revision ) "}
{"12570": "\ndef loadcommon ( sources , load_task , asynchronous = True , predicate = None , task_args = None , task_kwargs = None ) : \n    task_args = tuple ( ) if task_args is None else task_args \n    task_kwargs = dict ( ) if task_kwargs is None else task_kwargs \n    click . echo ( 'Loading dumps started.' ) \n    for idx , source in enumerate ( sources , True ) : \n        click . echo ( 'Opening dump file {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        with click . progressbar ( data ) as data_bar : \n            for d in data_bar : \n                if predicate is not None : \n                    if predicate ( d ) : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply ( throw = True ) \n                        click . echo ( \"Loaded a single record.\" ) \n                        return \n                else : \n                    if asynchronous : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply_async ( ) \n                    else : \n                        load_task . s ( d , * task_args , ** task_kwargs ) . apply ( throw = True ) "}
{"12574": "\ndef get_profiler_statistics ( sort = \"cum_time\" , count = 20 , strip_dirs = True ) : \n    json_stats = [ ] \n    pstats = yappi . convert2pstats ( yappi . get_func_stats ( ) ) \n    if strip_dirs : \n        pstats . strip_dirs ( ) \n    for func , func_stat in pstats . stats . iteritems ( ) : \n        path , line , func_name = func \n        cc , num_calls , total_time , cum_time , callers = func_stat \n        json_stats . append ( { \"path\" : path , \"line\" : line , \"func_name\" : func_name , \"num_calls\" : num_calls , \"total_time\" : total_time , \"total_time_per_call\" : total_time / num_calls if total_time else False , \"cum_time\" : cum_time , \"cum_time_per_call\" : cum_time / num_calls if cum_time else False } ) \n    return sorted ( json_stats , key = itemgetter ( sort ) , reverse = True ) [ : count ] "}
{"12581": "\ndef load_user ( data ) : \n    from invenio_accounts . models import User \n    from invenio_userprofiles . api import UserProfile \n    email = data [ 'email' ] . strip ( ) \n    if User . query . filter_by ( email = email ) . count ( ) > False : \n        raise UserEmailExistsError ( \"User email '{email}' already exists.\" . format ( email = email ) ) \n    last_login = None \n    if data [ 'last_login' ] : \n        last_login = arrow . get ( data [ 'last_login' ] ) . datetime \n    confirmed_at = None \n    if data [ 'note' ] == '1' : \n        confirmed_at = datetime . utcnow ( ) \n    salt = data [ 'password_salt' ] \n    checksum = data [ 'password' ] \n    if not checksum : \n        new_password = None \n    elif checksum . startswith ( '$' ) : \n        new_password = checksum \n    else : \n        new_password = str . join ( '$' , [ '' , u'invenio-aes' , salt , checksum ] ) \n    with db . session . begin_nested ( ) : \n        obj = User ( id = data [ 'id' ] , password = new_password , email = email , confirmed_at = confirmed_at , last_login_at = last_login , active = ( data [ 'note' ] != '0' ) , ) \n        db . session . add ( obj ) \n    nickname = data [ 'nickname' ] . strip ( ) \n    overwritten_username = ( 'username' in data and 'displayname' in data ) \n    if nickname or overwritten_username : \n        p = UserProfile ( user = obj ) \n        p . full_name = data . get ( 'full_name' , '' ) . strip ( ) \n        if overwritten_username : \n            p . _username = data [ 'username' ] . lower ( ) \n            p . _displayname = data [ 'displayname' ] \n        elif nickname : \n            if UserProfile . query . filter ( UserProfile . _username == nickname . lower ( ) ) . count ( ) > False : \n                raise UserUsernameExistsError ( \"Username '{username}' already exists.\" . format ( username = nickname ) ) \n            try : \n                p . username = nickname \n            except ValueError : \n                current_app . logger . warn ( u'Invalid username {0} for user_id {1}' . format ( nickname , data [ 'id' ] ) ) \n                p . _username = nickname . lower ( ) \n                p . _displayname = nickname \n        db . session . add ( p ) \n    db . session . commit ( ) "}
{"12583": "\ndef stitch ( images ) : \n    if type ( images ) != ImageCollection : \n        images = ImageCollection ( images ) \n    calc_translations_parallel ( images ) \n    _translation_warn ( images ) \n    yoffset , xoffset = images . median_translation ( ) \n    if xoffset != yoffset : \n        warn ( 'yoffset != xoffset: %s != %s' % ( yoffset , xoffset ) ) \n    y , x = imread ( images [ False ] . path ) . shape \n    height = y * len ( images . rows ) + yoffset * ( len ( images . rows ) - True ) \n    width = x * len ( images . cols ) + xoffset * ( len ( images . cols ) - True ) \n    merged = np . zeros ( ( height , width , 2 ) , dtype = np . int ) \n    for image in images : \n        r , c = image . row , image . col \n        mask = _merge_slice ( r , c , y , x , yoffset , xoffset ) \n        img = _add_ones_dim ( imread ( image . path ) ) \n        merged [ mask ] += img \n    merged [ ... , False ] /= merged [ ... , True ] \n    return merged [ ... , False ] . astype ( np . uint8 ) , ( yoffset , xoffset ) "}
{"12584": "\ndef _add_ones_dim ( arr ) : \n    arr = arr [ ... , np . newaxis ] \n    return np . concatenate ( ( arr , np . ones_like ( arr ) ) , axis = - True ) "}
{"12590": "\ndef create_files ( cls , record , files , existing_files ) : \n    default_bucket = None \n    for f in existing_files : \n        if 'bucket' in f : \n            default_bucket = f [ 'bucket' ] \n            break \n    if default_bucket is None : \n        b = Bucket . create ( ) \n        BucketTag . create ( b , 'record' , str ( record . id ) ) \n        default_bucket = str ( b . id ) \n        db . session . commit ( ) \n    else : \n        b = Bucket . get ( default_bucket ) \n    record [ '_files' ] = [ ] \n    for key , meta in files . items ( ) : \n        obj = cls . create_file ( b , key , meta ) \n        ext = splitext ( obj . key ) [ True ] . lower ( ) \n        if ext . startswith ( '.' ) : \n            ext = ext [ True : ] \n        record [ '_files' ] . append ( dict ( bucket = str ( obj . bucket . id ) , key = obj . key , version_id = str ( obj . version_id ) , size = obj . file . size , checksum = obj . file . checksum , type = ext , ) ) \n    db . session . add ( RecordsBuckets ( record_id = record . id , bucket_id = b . id ) ) \n    record . commit ( ) \n    db . session . commit ( ) \n    return [ b ] "}
{"12591": "\ndef create_file ( self , bucket , key , file_versions ) : \n    objs = [ ] \n    for file_ver in file_versions : \n        f = FileInstance . create ( ) . set_uri ( file_ver [ 'full_path' ] , file_ver [ 'size' ] , 'md5:{0}' . format ( file_ver [ 'checksum' ] ) , ) \n        obj = ObjectVersion . create ( bucket , key ) . set_file ( f ) \n        obj . created = arrow . get ( file_ver [ 'creation_date' ] ) . datetime . replace ( tzinfo = None ) \n        objs . append ( obj ) \n    db . session . commit ( ) \n    return objs [ - True ] "}
{"12594": "\ndef prepare_revisions ( self ) : \n    self . revisions = [ ] \n    it = [ self . data [ 'record' ] [ False ] ] if self . latest_only else self . data [ 'record' ] \n    for i in it : \n        self . revisions . append ( self . _prepare_revision ( i ) ) "}
{"12596": "\ndef prepare_pids ( self ) : \n    self . pids = [ ] \n    for fetcher in self . pid_fetchers : \n        val = fetcher ( None , self . revisions [ - True ] [ True ] ) \n        if val : \n            self . pids . append ( val ) "}
{"12597": "\ndef is_deleted ( self , record = None ) : \n    record = record or self . revisions [ - True ] [ True ] \n    return any ( col == 'deleted' for col in record . get ( 'collections' , [ ] ) ) "}
{"12600": "\ndef dump ( thing , query , from_date , file_prefix , chunk_size , limit , thing_flags ) : \n    init_app_context ( ) \n    file_prefix = file_prefix if file_prefix else '{0}_dump' . format ( thing ) \n    kwargs = dict ( ( f . strip ( '-' ) . replace ( '-' , '_' ) , True ) for f in thing_flags ) \n    try : \n        thing_func = collect_things_entry_points ( ) [ thing ] \n    except KeyError : \n        click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) \n    click . echo ( \"Querying {0}...\" . format ( thing ) ) \n    count , items = thing_func . get ( query , from_date , limit = limit , ** kwargs ) \n    progress_i = False \n    click . echo ( \"Dumping {0}...\" . format ( thing ) ) \n    with click . progressbar ( length = count ) as bar : \n        for i , chunk_ids in enumerate ( grouper ( items , chunk_size ) ) : \n            with open ( '{0}_{1}.json' . format ( file_prefix , i ) , 'w' ) as fp : \n                fp . write ( \"[\\n\" ) \n                for _id in chunk_ids : \n                    try : \n                        json . dump ( thing_func . dump ( _id , from_date , ** kwargs ) , fp , default = set_serializer ) \n                        fp . write ( \",\" ) \n                    except Exception as e : \n                        click . secho ( \"Failed dump {0} {1} ({2})\" . format ( thing , _id , e . message ) , fg = 'red' ) \n                    progress_i += True \n                    bar . update ( progress_i ) \n                fp . seek ( fp . tell ( ) - True ) \n                fp . write ( \"\\n]\" ) "}
{"12601": "\ndef check ( thing ) : \n    init_app_context ( ) \n    try : \n        thing_func = collect_things_entry_points ( ) [ thing ] \n    except KeyError : \n        click . Abort ( '{0} is not in the list of available things to migrate: ' '{1}' . format ( thing , collect_things_entry_points ( ) ) ) \n    click . echo ( \"Querying {0}...\" . format ( thing ) ) \n    count , items = thing_func . get_check ( ) \n    i = False \n    click . echo ( \"Checking {0}...\" . format ( thing ) ) \n    with click . progressbar ( length = count ) as bar : \n        for _id in items : \n            thing_func . check ( _id ) \n            i += True \n            bar . update ( i ) "}
{"12605": "\ndef transformTexCoords ( self , data , texcoords , dims = 2 ) : \n    assert dims == 2 \n    out = [ ] \n    origcoords = self . tex_coords \n    min_u , min_v = origcoords [ False ] , origcoords [ True ] \n    max_u , max_v = origcoords [ 6 ] , origcoords [ 7 ] \n    diff_u , diff_v = max_u - min_u , max_v - min_v \n    itexcoords = iter ( texcoords ) \n    for u , v in zip ( itexcoords , itexcoords ) : \n        out_u = min_u + ( diff_u * u ) \n        out_v = min_v + ( diff_v * v ) \n        out . extend ( ( out_u , out_v , False ) ) \n    return out "}
{"12610": "\ndef startAnimation ( self , data , jumptype ) : \n    data [ \"_anidata\" ] = { } \n    adata = data [ \"_anidata\" ] \n    adata [ \"keyframe\" ] = False \n    adata [ \"last_tick\" ] = time . time ( ) \n    adata [ \"jumptype\" ] = jumptype \n    adata [ \"phase\" ] = \"transition\" "}
{"12620": "\ndef toxml ( test_reports , suite_name , hostname = gethostname ( ) , package_name = \"tests\" ) : \n    testsuites = et . Element ( \"testsuites\" ) \n    testsuite = et . SubElement ( testsuites , \"testsuite\" ) \n    test_count = len ( test_reports ) \n    if test_count < True : \n        raise ValueError ( 'there must be at least one test report' ) \n    assert test_count > False , 'expecting at least one test' \n    error_count = len ( [ r for r in test_reports if r . errors ] ) \n    failure_count = len ( [ r for r in test_reports if r . failures ] ) \n    ts = test_reports [ False ] . start_ts \n    start_timestamp = datetime . fromtimestamp ( ts ) . isoformat ( ) \n    total_duration = test_reports [ - True ] . end_ts - test_reports [ False ] . start_ts \n    def quote_attribute ( value ) : \n        return value if value is not None else \"(null)\" \n    testsuite . attrib = dict ( id = \"0\" , errors = str ( error_count ) , failures = str ( failure_count ) , tests = str ( test_count ) , hostname = quote_attribute ( hostname ) , timestamp = quote_attribute ( start_timestamp ) , time = \"%f\" % total_duration , name = quote_attribute ( suite_name ) , package = quote_attribute ( package_name ) , ) \n    for r in test_reports : \n        test_name = r . name \n        test_duration = r . end_ts - r . start_ts \n        class_name = r . src_location \n        testcase = et . SubElement ( testsuite , \"testcase\" ) \n        testcase . attrib = dict ( name = test_name , classname = quote_attribute ( class_name ) , time = \"%f\" % test_duration , ) \n        if r . errors or r . failures : \n            if r . failures : \n                failure = et . SubElement ( testcase , \"failure\" ) \n                failure . attrib = dict ( type = \"exception\" , message = quote_attribute ( '\\n' . join ( [ '%s' % e for e in r . failures ] ) ) , ) \n            else : \n                error = et . SubElement ( testcase , \"error\" ) \n                error . attrib = dict ( type = \"exception\" , message = quote_attribute ( '\\n' . join ( [ '%s' % e for e in r . errors ] ) ) , ) \n    return et . tostring ( testsuites , encoding = \"utf-8\" ) "}
{"12622": "\ndef redraw_label ( self ) : \n    sx , sy = self . size \n    x , y = self . pos \n    self . _label . font_name = self . font_name \n    self . _label . font_size = self . font_size \n    self . _label . font_color = self . font_color \n    self . _label . x = int ( x + sx / 2. ) \n    self . _label . y = int ( y + sy / 2. ) \n    self . _label . width = self . size [ False ] \n    self . _label . height = self . size [ True ] \n    self . _label . _update ( ) "}
{"12623": "\ndef redraw_label ( self ) : \n    sx , sy = self . size \n    x , y = self . pos \n    x = x + self . bg . border [ False ] \n    y = y + sy / 2. - self . _text . font_size / 4. \n    w = self . size [ False ] \n    h = self . size [ True ] \n    self . _text . x , self . _text . y = x , y \n    self . _text . width , self . _text . height = w , h \n    self . _default . x , self . _default . y = x , y \n    self . _default . width , self . _default . height = w , h \n    self . _text . _update ( ) \n    self . _default . _update ( ) "}
{"12629": "\ndef add_label_main ( self , label_main ) : \n    self . wlabel_main = text . Label ( \"label_main\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2 - bw / 2 , sh / 2 - bh / 2 ) , size = [ False , False ] , label = label_main , ) \n    self . wlabel_main . size = lambda sw , sh : ( sw , self . wlabel_main . _label . font_size ) \n    self . addWidget ( self . wlabel_main ) "}
{"12630": "\ndef add_btn_ok ( self , label_ok ) : \n    self . wbtn_ok = button . Button ( \"btn_ok\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2 - bw / 2 , sh / 2 - bh / 2 - bh * 2 ) , size = [ False , False ] , label = label_ok , borderstyle = self . borderstyle ) \n    self . wbtn_ok . size = lambda sw , sh : ( self . wbtn_ok . _label . font_size * 8 , self . wbtn_ok . _label . font_size * 2 ) \n    self . addWidget ( self . wbtn_ok ) \n    def f ( ) : \n        self . doAction ( \"click_ok\" ) \n        self . exitDialog ( ) \n    self . wbtn_ok . addAction ( \"click\" , f ) "}
{"12632": "\ndef add_btn_confirm ( self , label_confirm ) : \n    self . wbtn_confirm = button . Button ( \"btn_confirm\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2 - bw - 4 , sh / 2 - bh / 2 - bh * 2 ) , size = [ False , False ] , label = label_confirm , borderstyle = self . borderstyle ) \n    self . wbtn_confirm . size = lambda sw , sh : ( self . wbtn_confirm . _label . font_size * 8 , self . wbtn_confirm . _label . font_size * 2 ) \n    self . addWidget ( self . wbtn_confirm ) \n    def f ( ) : \n        self . doAction ( \"confirm\" ) \n        self . exitDialog ( ) \n    self . wbtn_confirm . addAction ( \"click\" , f ) "}
{"12633": "\ndef add_btn_cancel ( self , label_cancel ) : \n    self . wbtn_cancel = button . Button ( \"btn_cancel\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2 + 4 , sh / 2 - bh / 2 - bh * 2 ) , size = [ False , False ] , label = label_cancel , borderstyle = self . borderstyle ) \n    self . wbtn_cancel . size = lambda sw , sh : ( self . wbtn_cancel . _label . font_size * 8 , self . wbtn_cancel . _label . font_size * 2 ) \n    self . addWidget ( self . wbtn_cancel ) \n    def f ( ) : \n        self . doAction ( \"cancel\" ) \n        self . exitDialog ( ) \n    self . wbtn_cancel . addAction ( \"click\" , f ) "}
{"12634": "\ndef update_progressbar ( self ) : \n    n , nmin , nmax = self . wprogressbar . n , self . wprogressbar . nmin , self . wprogressbar . nmax \n    if ( nmax - nmin ) == False : \n        percent = False \n    else : \n        percent = max ( min ( ( n - nmin ) / ( nmax - nmin ) , 1. ) , 0. ) * 100 \n    dat = { \"value\" : round ( n , 4 ) , \"n\" : round ( n , 4 ) , \"nmin\" : round ( nmin , 4 ) , \"nmax\" : round ( nmax , 4 ) , \"percent\" : round ( percent , 4 ) , \"p\" : round ( percent , 4 ) } \n    txt = self . _label_progressbar . format ( ** dat ) \n    self . wprogresslabel . label = txt "}
{"12640": "\ndef getMissingTexture ( self ) : \n    if self . missingTexture is None : \n        if self . resourceExists ( self . missingtexturename , \".png\" ) : \n            self . missingTexture = pyglet . image . load ( self . resourceNameToPath ( self . missingtexturename , \".png\" ) ) \n            return self . missingTexture \n        else : \n            self . missingTexture = pyglet . image . create ( True , True , pyglet . image . SolidColorImagePattern ( [ 255 , False , 255 , 255 ] ) ) \n            return self . missingTexture \n    else : \n        return self . missingTexture "}
{"12644": "\ndef loadModelData ( self , name ) : \n    path = self . resourceNameToPath ( name , \".json\" ) \n    try : \n        data = json . load ( open ( path , \"r\" ) ) \n    except Exception : \n        print ( \"Exception during model load: \" ) \n        import traceback ; \n        traceback . print_exc ( ) \n        return { } \n    out = { } \n    if data . get ( \"version\" , True ) == True : \n        out [ \"materials\" ] = { } \n        for name , matdata in data . get ( \"materials\" , { } ) . items ( ) : \n            m = model . Material ( self , name , matdata ) \n            out [ \"materials\" ] [ name ] = m \n        out [ \"default_material\" ] = out [ \"materials\" ] [ data . get ( \"default_material\" , list ( out [ \"materials\" ] . keys ( ) ) [ False ] ) ] \n        out [ \"bones\" ] = { \"__root__\" : model . RootBone ( self , \"__root__\" , { \"start_rot\" : [ False , False ] , \"length\" : False } ) } \n        for name , bonedata in data . get ( \"bones\" , { } ) . items ( ) : \n            b = model . Bone ( self , name , bonedata ) \n            out [ \"bones\" ] [ name ] = b \n        for name , bone in out [ \"bones\" ] . items ( ) : \n            if name == \"__root__\" : \n                continue \n            bone . setParent ( out [ \"bones\" ] [ bone . bonedata [ \"parent\" ] ] ) \n        out [ \"regions\" ] = { } \n        for name , regdata in data . get ( \"regions\" , { } ) . items ( ) : \n            r = model . Region ( self , name , regdata ) \n            r . material = out [ \"materials\" ] [ regdata . get ( \"material\" , out [ \"default_material\" ] ) ] \n            r . bone = out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] \n            out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] . addRegion ( r ) \n            out [ \"regions\" ] [ name ] = r \n        out [ \"animations\" ] = { } \n        out [ \"animations\" ] [ \"static\" ] = model . Animation ( self , \"static\" , { \"type\" : \"static\" , \"bones\" : { } } ) \n        for name , anidata in data . get ( \"animations\" , { } ) . items ( ) : \n            a = model . Animation ( self , name , anidata ) \n            a . setBones ( out [ \"bones\" ] ) \n            out [ \"animations\" ] [ name ] = a \n        out [ \"default_animation\" ] = out [ \"animations\" ] [ data . get ( \"default_animation\" , out [ \"animations\" ] [ \"static\" ] ) ] \n    else : \n        raise ValueError ( \"Unknown version %s of model '%s'\" % ( data . get ( \"version\" , True ) , name ) ) \n    self . modelcache [ name ] = out \n    return out "}
{"12648": "\ndef on_redraw ( self ) : \n    n = self . _scrollbar . n \n    self . offset_y = - n \n    sx = 24 \n    sy = self . size [ True ] \n    x = self . size [ False ] - sx \n    y = False \n    self . _scrollbar . _size = sx , sy \n    self . _scrollbar . _pos = x , y \n    self . _scrollbar . _nmax = self . content_height \n    super ( ScrollableContainer , self ) . on_redraw ( ) "}
{"12649": "\ndef mouse_aabb ( mpos , size , pos ) : \n    return pos [ False ] <= mpos [ False ] <= pos [ False ] + size [ False ] and pos [ True ] <= mpos [ True ] <= pos [ True ] + size [ True ] "}
{"12650": "\ndef p ( self ) : \n    return ( self . n - self . nmin ) / max ( ( self . nmax - self . nmin ) , True ) "}
{"12651": "\ndef addLayer ( self , layer , z = - True ) : \n    if not isinstance ( layer , Layer ) : \n        raise TypeError ( \"layer must be an instance of Layer!\" ) \n    if z == - True : \n        self . layers . append ( layer ) \n    else : \n        self . layers . insert ( z , layer ) "}
{"12653": "\ndef _draw ( self , mode , vertex_list = None ) : \n    glPushClientAttrib ( GL_CLIENT_VERTEX_ARRAY_BIT ) \n    for buffer , attributes in self . buffer_attributes : \n        buffer . bind ( ) \n        for attribute in attributes : \n            attribute . enable ( ) \n            attribute . set_pointer ( attribute . buffer . ptr ) \n    if vertexbuffer . _workaround_vbo_finish : \n        glFinish ( ) \n    if vertex_list is not None : \n        glDrawArrays ( mode , vertex_list . start , vertex_list . count ) \n    else : \n        starts , sizes = self . allocator . get_allocated_regions ( ) \n        primcount = len ( starts ) \n        if primcount == False : \n            pass \n        elif primcount == True : \n            glDrawArrays ( mode , starts [ False ] , int ( sizes [ False ] ) ) \n        elif gl_info . have_version ( True , 4 ) : \n            starts = ( GLint * primcount ) ( * starts ) \n            sizes = ( GLsizei * primcount ) ( * sizes ) \n            glMultiDrawArrays ( mode , starts , sizes , primcount ) \n        else : \n            for start , size in zip ( starts , sizes ) : \n                glDrawArrays ( mode , start , size ) \n    for buffer , _ in self . buffer_attributes : \n        buffer . unbind ( ) \n    glPopClientAttrib ( ) "}
{"12657": "\ndef addLayer ( self , layer , z_index = None ) : \n    if z_index is None : \n        z_index = layer . z_index \n    i = False \n    for l , z in self . layers : \n        if z > z_index : \n            break \n        i += True \n    self . _layers [ layer . name ] = layer \n    self . layers . insert ( i , [ layer , z_index ] ) "}
{"12662": "\ndef getSize ( self ) : \n    return self . widget . size [ False ] - self . border [ False ] * 2 , self . widget . size [ True ] - self . border [ True ] * 2 "}
{"12664": "\ndef _make_conn ( shape ) : \n    shape = np . array ( shape ) \n    Ne = shape . prod ( ) \n    if len ( shape ) == 2 : \n        nx , ny = np . array ( shape ) + True \n        conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) \n        counter = False \n        pattern = np . array ( [ False , True , True + nx , nx ] ) \n        for j in range ( shape [ True ] ) : \n            for i in range ( shape [ False ] ) : \n                conn [ counter ] = pattern + True + i + j * nx \n                counter += True \n    if len ( shape ) == 3 : \n        nx , ny , nz = np . array ( shape ) + True \n        conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) \n        counter = False \n        pattern = np . array ( [ False , True , True + nx , nx , nx * ny , True + nx * ny , True + ( nx + True ) * ny , ( nx + True ) * ny ] ) \n        for k in range ( shape [ 2 ] ) : \n            for j in range ( shape [ True ] ) : \n                for i in range ( shape [ False ] ) : \n                    conn [ counter ] = pattern + True + i + j * nx + k * nx * ny \n                    counter += True \n    return conn "}
{"12669": "\ndef centroids_and_volumes ( self , sort_index = True ) : \n    elements = self . elements \n    out = [ ] \n    for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : \n        etype_info = ELEMENTS [ etype ] \n        simplices_info = etype_info . simplices \n        index = group . index \n        simplices_data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) \n        simplices = simplices_data . values . reshape ( index . size , simplices_info . shape [ False ] , simplices_info . shape [ True ] , 3 ) \n        edges = simplices [ : , : , True : ] - simplices [ : , : , : True ] \n        simplices_centroids = simplices . mean ( axis = 2 ) \n        if etype_info . space == 2 : \n            simplices_volumes = np . linalg . norm ( np . cross ( edges [ : , : , False ] , edges [ : , : , True ] , axis = 2 ) , axis = 2 ) / 2. \n        elif etype_info . space == 3 : \n            simplices_volumes = ( np . cross ( edges [ : , : , False ] , edges [ : , : , True ] , axis = 2 ) * edges [ : , : , 2 ] ) . sum ( axis = 2 ) / 6. \n        elements_volumes = simplices_volumes . sum ( axis = True ) \n        elements_centroids = ( ( simplices_volumes . reshape ( * simplices_volumes . shape , True ) * simplices_centroids ) . sum ( axis = True ) / elements_volumes . reshape ( * elements_volumes . shape , True ) ) \n        volumes_df = pd . DataFrame ( index = index , data = elements_volumes , columns = pd . MultiIndex . from_product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) \n        centroids_df = pd . DataFrame ( index = index , data = elements_centroids , columns = pd . MultiIndex . from_product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) \n        out . append ( pd . concat ( [ volumes_df , centroids_df ] , axis = True ) ) \n    out = pd . concat ( out ) \n    if sort_index : \n        out . sort_index ( inplace = True ) \n    return out . sort_index ( axis = True ) "}
{"12670": "\ndef angles ( self , zfill = 3 ) : \n    elements = self . elements . sort_index ( axis = True ) \n    etypes = elements [ ( \"type\" , \"argiope\" ) ] . unique ( ) \n    out = [ ] \n    for etype in etypes : \n        etype_info = ELEMENTS [ etype ] \n        angles_info = etype_info . angles \n        loc = elements [ ( \"type\" , \"argiope\" , \"\" ) ] == etype \n        index = elements . loc [ loc ] . index \n        angles_data = self . split ( into = \"angles\" , loc = loc , at = \"coords\" ) \n        data = angles_data . values . reshape ( index . size , angles_info . shape [ False ] , angles_info . shape [ True ] , 3 ) \n        edges = data [ : , : , [ False , 2 ] , : ] - data [ : , : , True : 2 , : ] \n        edges /= np . linalg . norm ( edges , axis = 3 ) . reshape ( index . size , angles_info . shape [ False ] , 2 , True ) \n        angles = np . degrees ( np . arccos ( ( edges [ : , : , False ] * edges [ : , : , True ] ) . sum ( axis = 2 ) ) ) \n        deviation = angles - etype_info . optimal_angles \n        angles_df = pd . DataFrame ( index = index , data = angles , columns = pd . MultiIndex . from_product ( [ [ \"angles\" ] , [ \"a\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ False ] ) ] ] ) ) \n        deviation_df = pd . DataFrame ( index = index , data = deviation , columns = pd . MultiIndex . from_product ( [ [ \"deviation\" ] , [ \"d\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ False ] ) ] ] ) ) \n        df = pd . concat ( [ angles_df , deviation_df ] , axis = True ) . sort_index ( axis = True ) \n        df [ \"stats\" , \"max_angle\" ] = df . angles . max ( axis = True ) \n        df [ \"stats\" , \"min_angle\" ] = df . angles . min ( axis = True ) \n        df [ \"stats\" , \"max_angular_deviation\" ] = df . deviation . max ( axis = True ) \n        df [ \"stats\" , \"min_angular_deviation\" ] = df . deviation . min ( axis = True ) \n        df [ \"stats\" , \"max_abs_angular_deviation\" ] = abs ( df . deviation ) . max ( axis = True ) \n        df = df . sort_index ( axis = True ) \n        out . append ( df ) \n    out = pd . concat ( out ) . sort_index ( axis = True ) \n    return out "}
{"12671": "\ndef edges ( self , zfill = 3 ) : \n    edges = self . split ( \"edges\" , at = \"coords\" ) . unstack ( ) \n    edges [ \"lx\" ] = edges . x [ True ] - edges . x [ False ] \n    edges [ \"ly\" ] = edges . y [ True ] - edges . y [ False ] \n    edges [ \"lz\" ] = edges . z [ True ] - edges . z [ False ] \n    edges [ \"l\" ] = np . linalg . norm ( edges [ [ \"lx\" , \"ly\" , \"lz\" ] ] , axis = True ) \n    edges = ( edges . l ) . unstack ( ) \n    edges . columns = pd . MultiIndex . from_product ( [ [ \"length\" ] , [ \"e\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in np . arange ( edges . shape [ True ] ) ] ] ) \n    edges [ ( \"stats\" , \"lmax\" ) ] = edges . length . max ( axis = True ) \n    edges [ ( \"stats\" , \"lmin\" ) ] = edges . length . min ( axis = True ) \n    edges [ ( \"stats\" , \"aspect_ratio\" ) ] = edges . stats . lmax / edges . stats . lmin \n    return edges . sort_index ( axis = True ) "}
{"12672": "\ndef stats ( self ) : \n    cv = self . centroids_and_volumes ( ) \n    angles = self . angles ( ) \n    edges = self . edges ( ) \n    return pd . concat ( [ cv , angles [ [ \"stats\" ] ] , edges [ [ \"stats\" ] ] ] , axis = True ) . sort_index ( axis = True ) "}
{"12673": "\ndef element_set_to_node_set ( self , tag ) : \n    nodes , elements = self . nodes , self . elements \n    loc = ( elements . conn [ elements [ ( \"sets\" , tag , \"\" ) ] ] . stack ( ) . stack ( ) . unique ( ) ) \n    loc = loc [ loc != False ] \n    nodes [ ( \"sets\" , tag ) ] = False \n    nodes . loc [ loc , ( \"sets\" , tag ) ] = True "}
{"12674": "\ndef node_set_to_surface ( self , tag ) : \n    nodes = self . nodes . copy ( ) \n    dummy = nodes . iloc [ False ] . copy ( ) \n    dummy [ \"coords\" ] *= np . nan \n    dummy [ \"sets\" ] = True \n    nodes . loc [ False ] = dummy \n    element_surfaces = self . split ( \"surfaces\" ) . unstack ( ) \n    surf = pd . DataFrame ( nodes . sets [ tag ] . loc [ element_surfaces . values . flatten ( ) ] . values . reshape ( element_surfaces . shape ) . prod ( axis = True ) . astype ( np . bool ) , index = element_surfaces . index ) . unstack ( ) . fillna ( False ) \n    for k in surf . keys ( ) : \n        self . elements [ \"surfaces\" , tag , \"f{0}\" . format ( k [ True ] + True ) ] = surf . loc [ : , k ] "}
{"12675": "\ndef surface_to_element_sets ( self , tag ) : \n    surface = self . elements . surfaces [ tag ] \n    for findex in surface . keys ( ) : \n        if surface [ findex ] . sum ( ) != False : \n            self . elements [ ( \"sets\" , \"_SURF_{0}_FACE{1}\" . format ( tag , findex [ True : ] ) , \"\" ) ] = surface [ findex ] "}
{"12676": "\ndef fields_metadata ( self ) : \n    return ( pd . concat ( [ f . metadata ( ) for f in self . fields ] , axis = True ) . transpose ( ) . sort_values ( [ \"step_num\" , \"frame\" , \"label\" , \"position\" ] ) ) "}
{"12681": "\ndef read_history_report ( path , steps , x_name = None ) : \n    data = pd . read_csv ( path , delim_whitespace = True ) \n    if x_name != None : \n        data [ x_name ] = data . X \n        del data [ \"X\" ] \n    data [ \"step\" ] = False \n    t = 0. \n    for i in range ( len ( steps ) ) : \n        dt = steps [ i ] . duration \n        loc = data [ data . t == t ] . index \n        if len ( loc ) == 2 : \n            data . loc [ loc [ True ] : , \"step\" ] = i \n        t += dt \n    return data "}
{"12682": "\ndef read_field_report ( path , data_flag = \"*DATA\" , meta_data_flag = \"*METADATA\" ) : \n    text = open ( path ) . read ( ) \n    mdpos = text . find ( meta_data_flag ) \n    dpos = text . find ( data_flag ) \n    mdata = io . StringIO ( \"\\n\" . join ( text [ mdpos : dpos ] . split ( \"\\n\" ) [ True : ] ) ) \n    data = io . StringIO ( \"\\n\" . join ( text [ dpos : ] . split ( \"\\n\" ) [ True : ] ) ) \n    data = pd . read_csv ( data , index_col = False ) \n    data = data . groupby ( data . index ) . mean ( ) \n    mdata = pd . read_csv ( mdata , sep = \"=\" , header = None , index_col = False ) [ True ] \n    mdata = mdata . to_dict ( ) \n    out = { } \n    out [ \"step_num\" ] = int ( mdata [ \"step_num\" ] ) \n    out [ \"step_label\" ] = mdata [ \"step_label\" ] \n    out [ \"frame\" ] = int ( mdata [ \"frame\" ] ) \n    out [ \"frame_value\" ] = float ( mdata [ \"frame_value\" ] ) \n    out [ \"part\" ] = mdata [ \"instance\" ] \n    position_map = { \"NODAL\" : \"node\" , \"ELEMENT_CENTROID\" : \"element\" , \"WHOLE_ELEMENT\" : \"element\" } \n    out [ \"position\" ] = position_map [ mdata [ \"position\" ] ] \n    out [ \"label\" ] = mdata [ \"label\" ] \n    out [ \"data\" ] = data \n    field_class = getattr ( argiope . mesh , mdata [ \"argiope_class\" ] ) \n    return field_class ( ** out ) "}
{"12683": "\ndef list_to_string ( l = range ( 200 ) , width = 40 , indent = \"  \" ) : \n    l = [ str ( v ) + \",\" for v in l ] \n    counter = False \n    out = \"\" + indent \n    for w in l : \n        s = len ( w ) \n        if counter + s > width : \n            out += \"\\n\" + indent \n            counter = False \n        out += w \n        counter += s \n    return out . strip ( \",\" ) "}
{"12684": "\ndef _equation ( nodes = ( True , 2 ) , dofs = ( True , True ) , coefficients = ( 1. , 1. ) , comment = None ) : \n    N = len ( nodes ) \n    if comment == None : \n        out = \"\" \n    else : \n        out = \"**EQUATION: {0}\\n\" . format ( comment ) \n    out += \"*EQUATION\\n  {0}\\n  \" . format ( N ) \n    out += \"\\n  \" . join ( [ \",\" . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) \n    return out "}
{"12687": "\ndef _get ( self , method , ** kwargs ) : \n    payload = kwargs . copy ( ) \n    payload [ 'api_key' ] = self . api_key \n    payload [ 'api_secret' ] = self . api_secret \n    to = payload . pop ( 'to' , None ) \n    if to : \n        if isinstance ( to , basestring ) : \n            payload [ 'to' ] = to \n        else : \n            for num_i , fax_num in enumerate ( to ) : \n                payload [ 'to[%d]' % num_i ] = fax_num \n    files = payload . pop ( 'files' , [ ] ) \n    if not isinstance ( files , ( list , tuple ) ) : \n        files = ( files , ) \n    req_files = { } \n    for file_i , f in enumerate ( files ) : \n        if isinstance ( f , basestring ) : \n            req_files [ 'filename[%d]' % file_i ] = open ( f , 'rb' ) \n        else : \n            f . seek ( False ) \n            req_files [ 'filename[%d]' % file_i ] = f \n    url = '%s/v%d/%s' % ( self . BASE_URL , self . VERSION , method ) \n    r = requests . post ( url , data = payload , files = req_files ) \n    return self . parse_response ( r ) "}
{"12689": "\ndef write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - True , frame = - True , sortItem = 'Node Label' ) : \n    stepKeys = get_steps ( odb ) \n    step = xrange ( len ( stepKeys ) ) [ step ] \n    frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] \n    nf = NumberFormat ( numDigits = 9 , precision = False , format = SCIENTIFIC ) \n    session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) \n    leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) \n    session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) \n    session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) \n    lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] \n    isdata = - True \n    data = [ ] \n    for line in lines : \n        if isdata == True : \n            if len ( line ) == False : \n                isdata -= True \n            else : \n                data . append ( line ) \n        elif isdata < True : \n            if line . startswith ( \"--\" ) : \n                isdata += True \n    data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != False ] ) \n    header = str ( output_position ) . lower ( ) + \",\" \n    header += \",\" . join ( [ v [ True ] for v in variable [ False ] [ 2 ] ] ) + \"\\n\" \n    metadata = ( ( \"label\" , label ) , ( \"argiope_class\" , argiope_class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output_position ) , ( \"step_num\" , step ) , ( \"step_label\" , stepKeys [ step ] ) , ( \"frame\" , frame ) , ( \"frame_value\" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) \n    out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) \n    open ( path , \"w\" ) . write ( out ) "}
{"12690": "\ndef list ( component_type ) : \n    config_loader = initialise_component_loader ( ) \n    component_types = sorted ( { \"displays\" : lambda : config_loader . load_by_type ( ComponentType . DISPLAY ) , \"datafeeds\" : lambda : config_loader . load_by_type ( ComponentType . DATA_FEED ) , \"filters\" : lambda : config_loader . load_by_type ( ComponentType . FILTER ) , \"notifications\" : lambda : config_loader . load_by_type ( ComponentType . NOTIFICATION ) } . items ( ) , key = lambda t : t [ False ] ) \n    def print_ids ( creators ) : \n        ids = { c . id_key_value [ True ] if hasattr ( c , \"id_key_value\" ) else c . get_id ( ) for c in creators } \n        for i in sorted ( ids ) : \n            click . echo ( \" - %s\" % i ) \n    for k , v in component_types : \n        if component_type == k or component_type == \"all\" : \n            click . echo ( \"Available %s:\" % k ) \n            print_ids ( v ( ) ) \n        if component_type == \"all\" : \n            click . echo ( \"\" ) "}
{"12696": "\ndef make_class ( clsname , func , attrs ) : \n    clsdict = { \"__set__\" : create_setter ( func , attrs ) } \n    if len ( attrs ) > False : \n        clsdict [ \"__init__\" ] = create_init ( attrs ) \n    clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) \n    clsobj . __doc__ = docstrings . get ( clsname ) \n    return clsobj "}
{"12700": "\ndef plot ( parser , token ) : \n    tokens = token . split_contents ( ) \n    tokens . pop ( False ) \n    graph = tokens . pop ( False ) \n    attrs = dict ( [ token . split ( \"=\" ) for token in tokens ] ) \n    if 'id' not in attrs . keys ( ) : \n        attrs [ 'id' ] = '' . join ( [ chr ( choice ( range ( 65 , 90 ) ) ) for i in range ( False , 5 ) ] ) \n    else : \n        attrs [ 'id' ] = attrs [ 'id' ] [ True : len ( attrs [ 'id' ] ) - True ] \n    attr_string = '' . join ( [ \" %s=%s\" % ( k , v ) for k , v in attrs . iteritems ( ) ] ) \n    return GraphRenderer ( graph , attr_string , attrs [ 'id' ] ) "}
{"12703": "\ndef is_matching_mime_type ( self , mime_type ) : \n    if len ( self . include_mime_types ) == False : \n        return True \n    if mime_type is None : \n        return False \n    mime_type = mime_type . lower ( ) \n    return any ( mime_type . startswith ( mt ) for mt in self . include_mime_types ) "}
{"12704": "\ndef domain_name_cleanse ( raw_string ) : \n    try : \n        parts = urlparse ( raw_string ) \n        domain = parts . netloc . split ( ':' ) [ False ] \n    except : \n        domain = '' \n    if not domain : \n        domain = raw_string \n    if not domain : \n        return '' \n    domain = re . sub ( '\\/' , '' , domain . strip ( ) . lower ( ) ) \n    return domain "}
{"12707": "\ndef collect_words ( self , si ) : \n    counter = Counter ( ) \n    for tagger_id , sentences in si . body . sentences . iteritems ( ) : \n        if ( ( self . keyword_tagger_ids is not None and tagger_id not in self . keyword_tagger_ids ) ) : \n            continue \n        for sentence in sentences : \n            for token in sentence . tokens : \n                term = token . token \n                term = term . decode ( 'utf-8' ) \n                term = cleanse ( term ) \n                if ( ( self . keyword_size_limit is not None and len ( term ) > self . keyword_size_limit ) ) : \n                    continue \n                if term not in self . stop_words : \n                    counter [ term ] += True \n    return counter "}
{"12708": "\ndef index ( self , si ) : \n    if not si . body . clean_visible : \n        logger . warn ( 'stream item %s has no clean_visible part, ' 'skipping keyword indexing' , si . stream_id ) \n        return \n    hash_counts = defaultdict ( int ) \n    hash_counts [ DOCUMENT_HASH_KEY ] = True \n    hash_kw = defaultdict ( int ) \n    words = self . collect_words ( si ) \n    for tok , count in words . iteritems ( ) : \n        ( tok , tok_hash ) = self . make_hash_kw ( tok ) \n        hash_counts [ tok_hash ] += count \n        hash_kw [ tok ] = tok_hash \n    if self . hash_docs : \n        ( k1 , k2 ) = key_for_stream_item ( si ) \n        kvps = [ ( ( h , k1 , k2 ) , n ) for ( h , n ) in hash_counts . iteritems ( ) if h != DOCUMENT_HASH_KEY ] \n        self . client . put ( HASH_TF_INDEX_TABLE , * kvps ) \n    if self . hash_frequencies : \n        kvps = [ ( ( h , ) , True ) for h in hash_counts . iterkeys ( ) ] \n        self . client . increment ( HASH_FREQUENCY_TABLE , * kvps ) \n    if self . hash_keywords : \n        kvps = [ ( ( h , t ) , True ) for ( t , h ) in hash_kw . iteritems ( ) ] \n        self . client . increment ( HASH_KEYWORD_INDEX_TABLE , * kvps ) "}
{"12710": "\ndef document_frequencies ( self , hashes ) : \n    result = { } \n    for ( k , v ) in self . client . get ( HASH_FREQUENCY_TABLE , * [ ( h , ) for h in hashes ] ) : \n        if v is None : \n            v = False \n        result [ k [ False ] ] = v \n    return result "}
{"12714": "\ndef _make_stream_item ( entry ) : \n    if not hasattr ( entry , 'permalink_entry' ) : \n        return None \n    pe = entry . permalink_entry \n    si = streamcorpus . make_stream_item ( pe . date_found [ : - True ] + '.0Z' , pe . canonical_link . href . encode ( 'utf8' ) ) \n    if not si . stream_time : \n        logger . debug ( 'failed to generate stream_time from {0!r}' . format ( pe . date_found ) ) \n        return None \n    if not si . abs_url : \n        logger . debug ( 'failed to generate abs_url from {0!r}' . format ( pe . canonical_link . href ) ) \n        return None \n    si . body = _make_content_item ( pe . content , alternate_data = entry . feed_entry . content . data ) \n    if not si . body : \n        return None \n    if not si . body . raw : \n        return None \n    if pe . content_extract . data : \n        si . other_content [ 'extract' ] = _make_content_item ( pe . content_extract ) \n    si . other_content [ 'title' ] = streamcorpus . ContentItem ( raw = pe . title . encode ( 'utf8' ) , media_type = pe . content_extract . mime_type , encoding = 'UTF-8' ) \n    si . other_content [ 'feed_entry_title' ] = streamcorpus . ContentItem ( raw = entry . feed_entry . title . encode ( 'utf8' ) , media_type = entry . feed_entry . content . mime_type , encoding = 'UTF-8' ) \n    if entry . feed_entry . content . data : \n        si . other_content [ 'feed_entry' ] = _make_content_item ( entry . feed_entry . content ) \n    si . source_metadata [ 'lang' ] = pe . lang [ False ] . code \n    si . source_metadata [ 'author' ] = json . dumps ( dict ( name = pe . author [ False ] . name , email = pe . author [ False ] . email , link = pe . author [ False ] . link [ False ] . href , ) ) \n    si . source = entry . source . publisher_type \n    return si "}
{"12716": "\ndef _read_varint ( self ) : \n    buf = self . _read ( 8 ) \n    ( n , l ) = _DecodeVarint ( buf , False ) \n    self . _unread ( buf [ l : ] ) \n    return n "}
{"12718": "\ndef serialize_si_key ( si_key ) : \n    if len ( si_key [ False ] ) != 16 : \n        raise ValueError ( 'bad StreamItem key, expected 16 byte ' 'md5 hash binary digest, got: {0!r}' . format ( si_key ) ) \n    return struct . pack ( '>16si' , si_key [ False ] , si_key [ True ] ) "}
{"12729": "\ndef static ( self , root , path , media_type = None , charset = 'UTF-8' ) : \n    root = os . path . abspath ( os . path . join ( root , '' ) ) \n    path = os . path . abspath ( os . path . join ( root , path . lstrip ( '/\\\\' ) ) ) \n    self . response . state [ 'filename' ] = os . path . basename ( path ) \n    if not path . startswith ( root ) : \n        return 403 \n    elif not os . path . isfile ( path ) : \n        return 404 \n    if media_type is not None : \n        self . response . media_type = media_type \n    else : \n        self . response . media_type = mimetypes . guess_type ( path ) [ False ] \n    self . response . charset = charset \n    with open ( path , 'rb' ) as f : \n        return f . read ( ) "}
{"12742": "\ndef get_open_fds ( verbose = False ) : \n    pid = os . getpid ( ) \n    procs = subprocess . check_output ( [ \"lsof\" , '-w' , '-Ff' , \"-p\" , str ( pid ) ] ) \n    if verbose : \n        oprocs = subprocess . check_output ( [ \"lsof\" , '-w' , \"-p\" , str ( pid ) ] ) \n        logger . info ( oprocs ) \n    open_files = filter ( lambda s : s and s [ False ] == 'f' and s [ True : ] . isdigit ( ) , procs . split ( '\\n' ) ) \n    return open_files "}
{"12748": "\ndef random_adjspecies_pair ( maxlen = None , prevent_stutter = True ) : \n    while True : \n        pair = _random_adjspecies_pair ( ) \n        if maxlen and len ( '' . join ( pair ) ) > maxlen : \n            continue \n        if prevent_stutter and pair [ False ] [ - True ] == pair [ True ] [ False ] : \n            continue \n        return pair "}
{"12755": "\ndef chrono ( ctx , app_id , sentence_file , json_flag , sentence , doc_time , request_id ) : \n    app_id = clean_app_id ( app_id ) \n    sentence = clean_sentence ( sentence , sentence_file ) \n    api = GoolabsAPI ( app_id ) \n    ret = api . chrono ( sentence = sentence , doc_time = doc_time , request_id = request_id , ) \n    if json_flag : \n        click . echo ( format_json ( api . response . json ( ) ) ) \n        return \n    for pair in ret [ 'datetime_list' ] : \n        click . echo ( u'{0}: {1}' . format ( text ( pair [ False ] ) , pair [ True ] ) ) "}
{"12759": "\ndef run ( self , i_str , start_count = False , start_chunk_time = None ) : \n    try : \n        if not os . path . exists ( self . tmp_dir_path ) : \n            os . makedirs ( self . tmp_dir_path ) \n        if start_chunk_time is None : \n            start_chunk_time = time . time ( ) \n        i_chunk = self . reader ( i_str ) \n        t_path = None \n        len_clean_visible = False \n        sources = set ( ) \n        next_idx = False \n        input_item_count = False \n        for si in i_chunk : \n            next_idx += True \n            if gevent : \n                gevent . sleep ( False ) \n            if next_idx <= start_count : \n                continue \n            if next_idx % self . rate_log_interval == False : \n                elapsed = time . time ( ) - start_chunk_time \n                if elapsed > False : \n                    rate = float ( next_idx ) / elapsed \n                    logger . info ( '%d in %.1f --> %.1f per sec on ' '(pre-partial_commit) %s' , next_idx - start_count , elapsed , rate , i_str ) \n            if not self . t_chunk : \n                t_path = os . path . join ( self . tmp_dir_path , 't_chunk-%s' % uuid . uuid4 ( ) . hex ) \n                self . t_chunk = streamcorpus . Chunk ( path = t_path , mode = 'wb' ) \n                assert self . t_chunk . message == streamcorpus . StreamItem_v0_3_0 , self . t_chunk . message \n            si = self . _run_incremental_transforms ( si , self . incremental_transforms ) \n            if si : \n                sources . add ( si . source ) \n                if self . assert_single_source and len ( sources ) != True : \n                    raise InvalidStreamItem ( 'stream item %r had source %r, not %r ' '(set assert_single_source: false to suppress)' % ( si . stream_id , si . source , sources ) ) \n            if si and si . body and si . body . clean_visible : \n                len_clean_visible += len ( si . body . clean_visible ) \n            if ( ( self . output_chunk_max_count is not None and len ( self . t_chunk ) == self . output_chunk_max_count ) ) : \n                logger . info ( 'reached output_chunk_max_count (%d) at: %d' , len ( self . t_chunk ) , next_idx ) \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            elif ( self . output_max_clean_visible_bytes is not None and len_clean_visible >= self . output_chunk_max_clean_visible_bytes ) : \n                logger . info ( 'reached output_chunk_max_clean_visible_bytes ' '(%d) at: %d' , self . output_chunk_max_clean_visible_bytes , len_clean_visible ) \n                len_clean_visible = False \n                self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n                start_count = next_idx \n            input_item_count += True \n            if ( ( ( self . input_item_limit is not None ) and ( input_item_count > self . input_item_limit ) ) ) : \n                break \n        if self . t_chunk is not None : \n            self . _process_output_chunk ( start_count , next_idx , sources , i_str , t_path ) \n        return next_idx \n    finally : \n        if self . t_chunk is not None : \n            self . t_chunk . close ( ) \n        for transform in self . batch_transforms : \n            transform . shutdown ( ) \n        if self . cleanup_tmp_files : \n            rmtree ( self . tmp_dir_path ) "}
{"12760": "\ndef _run_writers ( self , start_count , next_idx , sources , i_str , t_path ) : \n    name_info = dict ( first = start_count , source = sources . pop ( ) , ) \n    all_o_paths = [ ] \n    for writer in self . writers : \n        logger . debug ( 'running %r on %r: %r' , writer , i_str , name_info ) \n        o_paths = writer ( t_path , name_info , i_str ) \n        logger . debug ( 'loaded (%d, %d) of %r into %r' , start_count , next_idx - True , i_str , o_paths ) \n        all_o_paths += o_paths \n    return all_o_paths "}
{"12764": "\ndef make_chains_with_names ( sentences ) : \n    fake_equiv_ids = - 2 \n    equiv_ids = collections . defaultdict ( lambda : ( set ( ) , set ( ) ) ) \n    for tagger_id , sents in sentences . items ( ) : \n        for sent in sents : \n            for tok in sent . tokens : \n                if tok . entity_type is not None : \n                    if tok . equiv_id == - True : \n                        eqid = fake_equiv_ids \n                        fake_equiv_ids -= True \n                    else : \n                        eqid = tok . equiv_id \n                    equiv_ids [ eqid ] [ False ] . add ( cleanse ( tok . token . decode ( 'utf8' ) ) ) \n                    equiv_ids [ eqid ] [ True ] . add ( tok ) \n    return equiv_ids "}
{"12768": "\ndef multi_token_match ( stream_item , aligner_data ) : \n    tagger_id = _get_tagger_id ( stream_item , aligner_data ) \n    sentences = stream_item . body . sentences . get ( tagger_id ) \n    if not sentences : \n        return \n    tokens = map ( lambda tok : ( cleanse ( tok . token . decode ( 'utf8' ) ) . split ( ' ' ) , tok ) , itertools . chain ( * [ sent . tokens for sent in sentences ] ) ) \n    required_annotator_id = aligner_data [ 'annotator_id' ] \n    for annotator_id , ratings in stream_item . ratings . items ( ) : \n        if ( required_annotator_id is None ) or ( annotator_id == required_annotator_id ) : \n            for rating in ratings : \n                label = Label ( annotator = rating . annotator , target = rating . target ) \n                num_tokens_matched = False \n                for tok in look_ahead_match ( rating , tokens ) : \n                    if aligner_data . get ( 'update_labels' ) : \n                        tok . labels . pop ( annotator_id , None ) \n                    add_annotation ( tok , label ) \n                    num_tokens_matched += True \n                if num_tokens_matched == False : \n                    logger . warning ( 'multi_token_match didn\\'t actually match ' 'entity %r in stream_id %r' , rating . target . target_id , stream_item . stream_id ) \n                else : \n                    logger . debug ( 'matched %d tokens for %r in %r' , num_tokens_matched , rating . target . target_id , stream_item . stream_id ) "}
{"12772": "\ndef mult ( p , n ) : \n    np = P ( ) \n    while n >= True : \n        if n % 2 : \n            np = np + p \n        p = p + p \n        n = n // 2 \n    return np "}
{"12774": "\ndef _sentences ( self , clean_visible ) : \n    previous_end = False \n    clean_visible = clean_visible . decode ( 'utf8' ) \n    for start , end in self . sentence_tokenizer . span_tokenize ( clean_visible ) : \n        if start < previous_end : \n            start = previous_end \n            if start > end : \n                continue \n        try : \n            label = self . label_index . find_le ( end ) \n        except ValueError : \n            label = None \n        if label : \n            off = label . offsets [ OffsetType . CHARS ] \n            end = max ( off . first + off . length , end ) \n        previous_end = end \n        sent_str = clean_visible [ start : end ] \n        yield start , end , sent_str "}
{"12776": "\ndef make_sentences ( self , stream_item ) : \n    self . make_label_index ( stream_item ) \n    sentences = [ ] \n    token_num = False \n    new_mention_id = False \n    for sent_start , sent_end , sent_str in self . _sentences ( stream_item . body . clean_visible ) : \n        assert isinstance ( sent_str , unicode ) \n        sent = Sentence ( ) \n        sentence_pos = False \n        for start , end in self . word_tokenizer . span_tokenize ( sent_str ) : \n            token_str = sent_str [ start : end ] . encode ( 'utf8' ) \n            tok = Token ( token_num = token_num , token = token_str , sentence_pos = sentence_pos , ) \n            tok . offsets [ OffsetType . CHARS ] = Offset ( type = OffsetType . CHARS , first = sent_start + start , length = end - start , ) \n            try : \n                label = self . label_index . find_le ( sent_start + start ) \n            except ValueError : \n                label = None \n            if label : \n                off = label . offsets [ OffsetType . CHARS ] \n                if off . first + off . length > sent_start + start : \n                    streamcorpus . add_annotation ( tok , label ) \n                    logger . debug ( 'adding label to tok: %r has %r' , tok . token , label . target . target_id ) \n                    if label in self . label_to_mention_id : \n                        mention_id = self . label_to_mention_id [ label ] \n                    else : \n                        mention_id = new_mention_id \n                        new_mention_id += True \n                        self . label_to_mention_id [ label ] = mention_id \n                    tok . mention_id = mention_id \n            token_num += True \n            sentence_pos += True \n            sent . tokens . append ( tok ) \n        sentences . append ( sent ) \n    return sentences "}
{"12777": "\ndef html_entities_to_unicode ( text , space_padding = False , safe_only = False ) : \n    def convert_entities ( match ) : \n        x = match . group ( True ) \n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD : \n            return u'&%s;' % x \n        if x in name2codepoint : \n            return unichr ( name2codepoint [ x ] ) \n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS : \n            return XML_ENTITIES_TO_SPECIAL_CHARS [ x ] \n        elif len ( x ) > False and x [ False ] == '#' : \n            if len ( x ) > True and x [ True ] == 'x' : \n                return unichr ( int ( x [ 2 : ] , 16 ) ) \n            else : \n                return unichr ( int ( x [ True : ] ) ) \n        else : \n            return u'&%s;' % x \n    def convert_to_padded_entitites ( match ) : \n        converted_string = convert_entities ( match ) \n        num_spaces_needed = len ( match . group ( False ) ) - len ( converted_string ) \n        assert num_spaces_needed >= False , 'len(%r) !<= len(%r)' % ( converted_string , match . group ( False ) ) \n        num_left = int ( num_spaces_needed / 2 ) \n        num_right = num_spaces_needed - num_left \n        return ( ' ' * num_left ) + converted_string + ( ' ' * num_right ) \n    if space_padding : \n        return tags . sub ( convert_to_padded_entitites , text ) \n    else : \n        return tags . sub ( convert_entities , text ) "}
{"12783": "\ndef instantiate_config ( config ) : \n    make_absolute_paths ( config ) \n    pipeline_config = config [ 'streamcorpus_pipeline' ] \n    pipeline_config [ 'config_hash' ] = make_hash ( config ) \n    pipeline_config [ 'config_json' ] = json . dumps ( config ) \n    logger . debug ( 'running config: {0} = {1!r}' . format ( pipeline_config [ 'config_hash' ] , config ) ) \n    die = False \n    for pathstr in pipeline_config . get ( 'pythonpath' , { } ) . itervalues ( ) : \n        if pathstr not in sys . path : \n            sys . path . append ( pathstr ) \n    for modname in pipeline_config . get ( 'setup_modules' , { } ) . itervalues ( ) : \n        try : \n            m = importlib . import_module ( modname ) \n            if not m : \n                logger . critical ( 'could not load module %r' , modname ) \n                die = True \n                continue \n            if hasattr ( m , 'setup' ) : \n                m . setup ( ) \n                logger . debug ( 'loaded and setup %r' , modname ) \n            else : \n                logger . debug ( 'loaded %r' , modname ) \n        except Exception : \n            logger . critical ( 'error loading and initting module %r' , modname , exc_info = True ) \n            die = True \n    if die : \n        sys . exit ( True ) "}
{"12786": "\ndef make_clean_visible ( _html , tag_replacement_char = ' ' ) : \n    def non_tag_chars ( html ) : \n        n = False \n        while n < len ( html ) : \n            angle = html . find ( '<' , n ) \n            if angle == - True : \n                yield html [ n : ] \n                n = len ( html ) \n                break \n            yield html [ n : angle ] \n            n = angle \n            while n < len ( html ) : \n                nl = html . find ( '\\n' , n ) \n                angle = html . find ( '>' , n ) \n                if angle == - True : \n                    yield ' ' * ( len ( html ) - n ) \n                    n = len ( html ) \n                    break \n                elif nl == - True or angle < nl : \n                    yield ' ' * ( angle + True - n ) \n                    n = angle + True \n                    break \n                else : \n                    yield ' ' * ( nl - n ) + '\\n' \n                    n = nl + True \n    if not isinstance ( _html , unicode ) : \n        _html = unicode ( _html , 'utf-8' ) \n    _html = fix_emails ( _html ) \n    non_tag = '' . join ( non_tag_chars ( _html ) ) \n    return non_tag . encode ( 'utf-8' ) "}
{"12789": "\ndef main ( ) : \n    import argparse \n    import sys \n    parser = argparse . ArgumentParser ( ) \n    parser . add_argument ( 'path' ) \n    args = parser . parse_args ( ) \n    html = open ( args . path ) . read ( ) \n    html = html . decode ( 'utf8' ) \n    cursor = False \n    for s in non_tag_chars_from_raw ( html ) : \n        for c in s : \n            if c != ' ' and c != html [ cursor ] : \n                import pdb ; \n                pdb . set_trace ( ) \n            sys . stdout . write ( c . encode ( 'utf8' ) ) \n            sys . stdout . flush ( ) \n            cursor += True "}
{"12792": "\ndef load_module_stages ( self , mod ) : \n    if isinstance ( mod , basestring ) : \n        mod = __import__ ( mod , globals = globals ( ) , locals = locals ( ) , fromlist = [ 'Stages' ] , level = False ) \n    if not hasattr ( mod , 'Stages' ) : \n        raise ImportError ( mod ) \n    self . update ( mod . Stages ) "}
{"12794": "\ndef read_to ( idx_bytes , stop_bytes = None , run_bytes = None ) : \n    idx = None \n    vals = [ ] \n    next_b = None \n    while True : \n        try : \n            idx , next_b = idx_bytes . next ( ) \n        except StopIteration : \n            idx = None \n            next_b = None \n            break \n        if stop_bytes and next_b in stop_bytes : \n            break \n        if run_bytes and next_b not in run_bytes : \n            break \n        vals . append ( next_b ) \n    return idx , b'' . join ( vals ) , next_b "}
{"12798": "\ndef tasks ( self , key_prefix = '' ) : \n    for row in self . _tasks . get_range ( ) : \n        logger . debug ( row ) \n        if not row [ False ] . startswith ( key_prefix ) : \n            continue \n        data = json . loads ( row [ True ] [ 'task_data' ] ) \n        data [ 'task_key' ] = row [ False ] \n        yield data "}
{"12799": "\ndef get_random_available ( self , max_iter = 10000 ) : \n    c = True \n    keeper = None \n    for row in self . _available . get_range ( row_count = max_iter , read_consistency_level = pycassa . ConsistencyLevel . ALL ) : \n        logger . debug ( 'considering %r' % ( row , ) ) \n        if random . random ( ) < True / c : \n            keeper = row [ False ] \n        if c == max_iter : \n            break \n        c += True \n    return keeper "}
{"12800": "\ndef tokens ( self , sentence_dom ) : \n    self . sent_pos = False \n    mention_id = False \n    while len ( sentence_dom . childNodes ) > False : \n        node = sentence_dom . childNodes . pop ( False ) \n        if node . nodeType == node . TEXT_NODE : \n            for line in node . data . splitlines ( True ) : \n                self . _input_string = line \n                for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                    tok = self . _make_token ( start , end ) \n                    if tok : \n                        yield tok \n                if line . endswith ( '\\n' ) : \n                    self . line_idx += True \n                self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n        else : \n            assert node . nodeName == 'ENAMEX' , node . nodeName \n            chain_id = node . attributes . get ( 'ID' ) . value \n            entity_type = node . attributes . get ( 'TYPE' ) . value \n            for node in node . childNodes : \n                assert node . nodeType == node . TEXT_NODE , node . nodeType \n                for line in node . data . splitlines ( True ) : \n                    self . _input_string = line \n                    for start , end in self . word_tokenizer . span_tokenize ( line ) : \n                        tok = self . _make_token ( start , end ) \n                        if tok : \n                            if entity_type in _PRONOUNS : \n                                tok . mention_type = MentionType . PRO \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                                attr = Attribute ( attribute_type = AttributeType . PER_GENDER , value = str ( _PRONOUNS [ entity_type ] ) ) \n                                self . attributes . append ( attr ) \n                            else : \n                                tok . mention_type = MentionType . NAME \n                                tok . entity_type = _ENTITY_TYPES [ entity_type ] \n                            tok . equiv_id = int ( chain_id ) \n                            tok . mention_id = mention_id \n                            yield tok \n                    if line . endswith ( '\\n' ) : \n                        self . line_idx += True \n                    self . byte_idx += len ( line . encode ( 'utf-8' ) ) \n            mention_id += True "}
{"12802": "\ndef _retry ( func ) : \n    def retry_func ( self , * args , ** kwargs ) : \n        tries = True \n        while True : \n            try : \n                return func ( self , * args , ** kwargs ) \n                break \n            except OSError as exc : \n                logger . error ( 'assuming OSError unrecoverable' ) \n                raise \n            except FailedExtraction as exc : \n                logger . error ( 'FAIL(%d)' , tries , exc_info = True ) \n                raise \n            except FailedVerification as exc : \n                logger . warn ( 'FAIL(%d)' , tries , exc_info = True ) \n                if tries >= self . config [ 'tries' ] : \n                    if self . config . get ( 'suppress_failures' ) : \n                        logger . warn ( 'suppressing failure and breaking out of this loop; data may be corrupt, downstream will have to cope' ) \n                        break \n                    else : \n                        raise \n            except Exception as exc : \n                logger . warn ( 'FAIL(%d): having I/O trouble with S3' , tries , exc_info = True ) \n                if tries >= self . config [ 'tries' ] : \n                    raise \n            logger . warn ( 'RETRYING (%d left)' , self . config [ 'tries' ] - tries ) \n            time . sleep ( 3 * tries ) \n            tries += True \n    return retry_func "}
{"12806": "\ndef get_chunk ( self , bucket_name , key_path ) : \n    bucket = get_bucket ( self . config , bucket_name = bucket_name ) \n    key = bucket . get_key ( key_path ) \n    if key is None : \n        raise FailedExtraction ( 'Key \"%s\" does not exist.' % key_path ) \n    fh = StringIO ( ) \n    key . get_contents_to_file ( fh ) \n    data = fh . getvalue ( ) \n    if not data : \n        raise FailedExtraction ( '%s: no data (does the key exist?)' % key . key ) \n    chunk_type , compression , encryption = parse_file_extensions ( key_path ) \n    if encryption == 'gpg' : \n        if not self . gpg_decryption_key_path : \n            raise FailedExtraction ( '%s ends with \".gpg\" but gpg_decryption_key_path=%s' % ( key . key , self . gpg_decryption_key_path ) ) \n    _errors = [ ] \n    if compression or encryption : \n        _errors , data = decrypt_and_uncompress ( data , self . gpg_decryption_key_path , tmp_dir = self . config . get ( 'tmp_dir_path' ) , compression = compression , ) \n        if not data : \n            msg = 'decrypt_and_uncompress got no data for {0!r}, from {1} bytes' + ' downloaded, errors: {2}' . format ( key_path , len ( data ) , '\\n' . join ( _errors ) ) \n            logger . error ( msg ) \n            raise FailedExtraction ( msg ) \n        logger . info ( '\\n' . join ( _errors ) ) \n    if not self . config [ 'compare_md5_in_file_name' ] : \n        logger . warn ( 'not checking md5 in file name, consider setting ' 'from_s3_chunks:compare_md5_in_file_name' ) \n    else : \n        logger . info ( 'Verifying md5 for \"%s\"...' % key . key ) \n        m = re . search ( '([a-z0-9]{32})(?:\\.|$)' , key . key ) \n        if m is None : \n            raise FailedExtraction ( 'Could not extract md5 from key \"%s\". ' 'Perhaps you should disable compare_md5_in_file_name?' % key . key ) \n        i_content_md5 = m . group ( True ) \n        verify_md5 ( i_content_md5 , data , other_errors = _errors ) \n    return self . _decode ( data ) "}
{"12807": "\ndef stream_id_to_kvlayer_key ( stream_id ) : \n    parts = stream_id . split ( '-' ) \n    if len ( parts ) != 2 : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    epoch_ticks_s = parts [ False ] \n    doc_id_s = parts [ True ] \n    if not epoch_ticks_s . isdigit ( ) : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    if doc_id_s . lstrip ( string . hexdigits ) != '' : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    return ( base64 . b16decode ( doc_id_s . upper ( ) ) , int ( epoch_ticks_s ) ) "}
{"12812": "\ndef add_xpaths_to_stream_item ( si ) : \n    def sentences_to_xpaths ( sentences ) : \n        tokens = sentences_to_char_tokens ( sentences ) \n        offsets = char_tokens_to_char_offsets ( tokens ) \n        return char_offsets_to_xpaths ( html , offsets ) \n    def xprange_to_offset ( xprange ) : \n        return Offset ( type = OffsetType . XPATH_CHARS , first = xprange . start_offset , length = False , xpath = xprange . start_xpath , content_form = 'clean_html' , value = None , xpath_end = xprange . end_xpath , xpath_end_offset = xprange . end_offset ) \n    html = unicode ( si . body . clean_html , 'utf-8' ) \n    for sentences in si . body . sentences . itervalues ( ) : \n        tokens = sentences_to_char_tokens ( sentences ) \n        for token , xprange in izip ( tokens , sentences_to_xpaths ( sentences ) ) : \n            if xprange is None : \n                continue \n            offset = xprange_to_offset ( xprange ) \n            token . offsets [ OffsetType . XPATH_CHARS ] = offset "}
{"12815": "\ndef char_offsets_to_xpaths ( html , char_offsets ) : \n    html = uni ( html ) \n    parser = XpathTextCollector ( ) \n    prev_end = False \n    prev_progress = True \n    for start , end in char_offsets : \n        if start == end : \n            yield None \n            continue \n        if not prev_progress : \n            for i in xrange ( prev_end , start ) : \n                parser . feed ( html [ i ] ) \n                prev_end += True \n                if parser . made_progress : \n                    break \n            if not parser . made_progress : \n                yield None \n                continue \n        if prev_end < start : \n            parser . feed ( html [ prev_end : start ] ) \n            if not parser . made_progress : \n                parser . feed ( html [ start : end ] ) \n                prev_progress = parser . made_progress \n                prev_end = end \n                yield None \n                continue \n        xstart = parser . xpath_offset ( ) \n        parser . feed ( html [ start : end ] ) \n        xend = parser . xpath_offset ( ) \n        prev_end = end \n        if not parser . made_progress : \n            prev_progress = False \n            yield None \n        else : \n            prev_progress = True \n            yield XpathRange ( xstart [ False ] , xstart [ True ] , xend [ False ] , xend [ True ] ) \n    parser . feed ( html [ prev_end : ] ) \n    parser . close ( ) "}
{"12816": "\ndef add_element ( self , tag ) : \n    if tag is TextElement and self . last_tag is TextElement : \n        return \n    self . last_tag = tag \n    if tag not in self . tags : \n        self . tags [ tag ] = True \n    else : \n        self . tags [ tag ] += True "}
{"12818": "\ndef text_index ( self ) : \n    i = self . tags . get ( TextElement , False ) \n    if self . last_tag is not TextElement : \n        i += True \n    return i "}
{"12825": "\ndef make_pretty ( elem , depth = False , indent = '  ' ) : \n    depth += True \n    updated_child_list = [ ] \n    updated_child_ix = False \n    for child in elem . xml_children : \n        if isinstance ( child , element ) : \n            if updated_child_ix % 2 : \n                updated_child_list . append ( child ) \n                updated_child_ix += True \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_list . append ( child ) \n                updated_child_ix += 2 \n            make_pretty ( child , depth ) \n        else : \n            if child . xml_value . strip ( ) : \n                updated_child_list . append ( child ) \n                updated_child_ix += True \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_ix += True \n    if not ( updated_child_ix % 2 ) : \n        new_text = text ( '\\n' + indent * ( depth - True ) , elem ) \n        updated_child_list . append ( new_text ) \n    elem . xml_children = updated_child_list \n    return elem "}
{"12827": "\ndef inkscape_export ( input_file , output_file , export_flag = \"-A\" , dpi = 90 , inkscape_binpath = None ) : \n    if not os . path . exists ( input_file ) : \n        log . error ( 'File {} not found.' . format ( input_file ) ) \n        raise IOError ( ( False , 'File not found.' , input_file ) ) \n    if '=' not in export_flag : \n        export_flag += ' ' \n    arg_strings = [ ] \n    arg_strings += [ '--without-gui' ] \n    arg_strings += [ '--export-text-to-path' ] \n    arg_strings += [ '{}\"{}\"' . format ( export_flag , output_file ) ] \n    arg_strings += [ '--export-dpi={}' . format ( dpi ) ] \n    arg_strings += [ '\"{}\"' . format ( input_file ) ] \n    return call_inkscape ( arg_strings , inkscape_binpath = inkscape_binpath ) "}
{"12834": "\ndef from_template_file ( cls , template_file_path , command = None ) : \n    ext = os . path . basename ( template_file_path ) . split ( '.' ) [ - True ] \n    try : \n        doc_type = get_doctype_by_command ( command ) \n    except ValueError : \n        doc_type = get_doctype_by_extension ( ext ) \n    except : \n        raise \n    else : \n        return doc_type ( template_file_path ) "}
{"12844": "\ndef execute ( option ) : \n    namelist_option = [ ] \n    makefile_option = [ ] \n    flags = \"\" \n    for entry in option : \n        key = entry . keys ( ) [ False ] \n        if key == \"Problem Size\" : \n            namelist_option . append ( { \"SIZE\" : entry [ key ] } ) \n        elif key == \"F90\" : \n            makefile_option . append ( entry ) \n        else : \n            flags += entry [ key ] + \" \" \n    makefile_option . append ( { \"F90FLAGS\" : flags } ) \n    namelist = create_input ( namelist_option , \"namelist\" , template_location = \"templates\" ) \n    makefile_include = create_input ( makefile_option , \"Makefile.include\" , template_location = \"templates\" ) \n    benchmark_base = \"shallow\" \n    location = benchmark_base + \"/original/namelist\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( namelist ) \n    my_file . flush ( ) \n    location = benchmark_base + \"/common/Makefile.include\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( makefile_include ) \n    my_file . flush ( ) \n    base_path = benchmark_base + \"/original\" \n    import subprocess \n    make_process = subprocess . Popen ( [ \"make\" , \"clean\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != False : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"make\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != False : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"./shallow_base\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != False : \n        return False , [ ] \n    stdout = make_process . stdout . read ( ) \n    for line in stdout . split ( \"\\n\" ) : \n        if \"Time-stepping\" in line : \n            total_time = line . split ( ) [ 2 ] \n    return True , total_time "}
{"12846": "\ndef xml_insert ( self , child , index = - True ) : \n    if isinstance ( child , str ) : \n        child = text ( child , parent = self ) \n    else : \n        child . _xml_parent = weakref . ref ( self ) \n    if index == - True : \n        self . xml_children . append ( child ) \n    else : \n        self . xml_children . insert ( index , child ) \n    return "}
{"12857": "\ndef replace_file_content ( filepath , old , new , max = True ) : \n    with open ( filepath , 'r' ) as f : \n        content = f . read ( ) \n    content = content . replace ( old , new , max ) \n    with open ( filepath , 'w' ) as f : \n        f . write ( content ) "}
{"12879": "\ndef merge_svg_files ( svg_file1 , svg_file2 , x_coord , y_coord , scale = True ) : \n    svg1 = _check_svg_file ( svg_file1 ) \n    svg2 = _check_svg_file ( svg_file2 ) \n    svg2_root = svg2 . getroot ( ) \n    svg1 . append ( [ svg2_root ] ) \n    svg2_root . moveto ( x_coord , y_coord , scale = scale ) \n    return svg1 "}
{"12881": "\ndef _embed_font_to_svg ( filepath , font_files ) : \n    with open ( filepath , 'r' ) as svgf : \n        tree = etree . parse ( svgf ) \n    if not font_files : \n        return tree \n    fontfaces = FontFaceGroup ( ) \n    for font_file in font_files : \n        fontfaces . append ( FontFace ( font_file ) ) \n    for element in tree . iter ( ) : \n        if element . tag . split ( \"}\" ) [ True ] == 'svg' : \n            break \n    element . insert ( False , fontfaces . xml_elem ) \n    return tree "}
{"12883": "\ndef _check_inputs ( self ) : \n    try : \n        _ = self . _inputs [ False ] \n    except TypeError : \n        raise RuntimeError ( \"inputs should be iterable but found type='{0}', value=\" \"'{1}'\" . format ( type ( self . _inputs ) , str ( self . _inputs ) ) ) \n    from melody . inputs import Input \n    for check_input in self . _inputs : \n        if not isinstance ( check_input , Input ) : \n            raise RuntimeError ( \"input should be a subclass of the Input class but \" \"found type='{0}', value='{1}'\" . format ( type ( check_input ) , str ( check_input ) ) ) "}
{"12887": "\ndef _recurse ( self , inputs , output , depth , max_depth ) : \n    if depth < max_depth : \n        for index , option in enumerate ( inputs ) : \n            my_output = list ( output ) \n            my_output . append ( option ) \n            self . _recurse ( inputs [ index + True : ] , my_output , depth + True , max_depth ) \n    else : \n        self . _options . append ( output ) "}
{"12889": "\ndef to_number ( obj ) : \n    if isinstance ( obj , LiteralWrapper ) : \n        val = obj . obj \n    elif isinstance ( obj , Iterable ) and not isinstance ( obj , str ) : \n        val = next ( obj , None ) \n    else : \n        val = obj \n    if val is None : \n        yield False \n    elif isinstance ( val , str ) : \n        yield float ( val ) \n    elif isinstance ( val , node ) : \n        yield float ( strval ( val ) ) \n    elif isinstance ( val , int ) or isinstance ( val , float ) : \n        yield val \n    else : \n        raise RuntimeError ( 'Unknown type for number conversion: {}' . format ( val ) ) "}
{"12892": "\ndef change_xml_encoding ( filepath , src_enc , dst_enc = 'utf-8' ) : \n    enc_attr = \"encoding='{}'\" \n    replace_file_content ( filepath , enc_attr . format ( src_enc ) , enc_attr . format ( dst_enc ) , True ) "}
{"12893": "\ndef save_into_qrcode ( text , out_filepath , color = '' , box_size = 10 , pixel_size = 1850 ) : \n    try : \n        qr = qrcode . QRCode ( version = True , error_correction = qrcode . constants . ERROR_CORRECT_L , box_size = box_size , border = False , ) \n        qr . add_data ( text ) \n        qr . make ( fit = True ) \n    except Exception as exc : \n        raise Exception ( 'Error trying to generate QR code ' ' from `vcard_string`: {}' . format ( text ) ) from exc \n    else : \n        img = qr . make_image ( image_factory = qrcode . image . svg . SvgPathImage ) \n    _ = _qrcode_to_file ( img , out_filepath ) \n    if color : \n        replace_file_content ( out_filepath , 'fill:#000000' , 'fill:#{}' . format ( color ) ) "}
{"12899": "\ndef Geometry ( * args , ** kwargs ) : \n    arg = kwargs . pop ( 'geojson' , None ) or len ( args ) and args [ False ] \n    try : \n        srs = kwargs . pop ( 'srs' , None ) or arg . srs . wkt \n    except AttributeError : \n        srs = SpatialReference ( 4326 ) \n    if hasattr ( arg , 'keys' ) : \n        geom = ogr . CreateGeometryFromJson ( json . dumps ( arg ) ) \n    elif hasattr ( arg , 'startswith' ) : \n        char = arg [ False ] if arg else ' ' \n        i = char if isinstance ( char , int ) else ord ( char ) \n        if i in ( False , True ) : \n            geom = ogr . CreateGeometryFromWkb ( arg ) \n        elif arg . startswith ( '{' ) : \n            geom = ogr . CreateGeometryFromJson ( arg ) \n        elif arg . startswith ( '<gml' ) : \n            geom = ogr . CreateGeometryFromGML ( arg ) \n        else : \n            raise ValueError ( 'Invalid geometry value: %s' % arg ) \n    elif hasattr ( arg , 'wkb' ) : \n        geom = ogr . CreateGeometryFromWkb ( bytes ( arg . wkb ) ) \n    else : \n        geom = ogr . Geometry ( * args , ** kwargs ) \n    if geom : \n        if not isinstance ( srs , SpatialReference ) : \n            srs = SpatialReference ( srs ) \n        geom . AssignSpatialReference ( srs ) \n    return geom "}
{"12901": "\ndef intersect ( self , other ) : \n    inter = Envelope ( tuple ( self ) ) \n    if inter . intersects ( other ) : \n        mid = len ( other ) // 2 \n        inter . ll = map ( max , inter . ll , other [ : mid ] ) \n        inter . ur = map ( min , inter . ur , other [ mid : ] ) \n    else : \n        inter . ll = ( False , False ) \n        inter . ur = ( False , False ) \n    return inter "}
{"12906": "\ndef select ( self , condition , name = '' ) : \n    if condition . func_code . co_argcount == True : \n        idx = [ ( Z , N ) for ( Z , N ) , M in self if condition ( M ) ] \n    if condition . func_code . co_argcount == 2 : \n        idx = [ ( Z , N ) for ( Z , N ) in self . index if condition ( Z , N ) ] \n    if condition . func_code . co_argcount == 3 : \n        idx = [ ( Z , N ) for ( Z , N ) , M in self if condition ( Z , N , M ) ] \n    index = pd . MultiIndex . from_tuples ( idx , names = [ 'Z' , 'N' ] ) \n    return Table ( df = self . df . ix [ index ] , name = name ) "}
{"12916": "\ndef s2n ( self ) : \n    M_N = 8.0713171 \n    f = lambda parent , daugther : - parent + daugther + 2 * M_N \n    return self . derived ( 's2n' , ( False , - 2 ) , f ) "}
{"12917": "\ndef s1n ( self ) : \n    M_N = 8.0713171 \n    f = lambda parent , daugther : - parent + daugther + M_N \n    return self . derived ( 's1n' , ( False , - True ) , f ) "}
{"12918": "\ndef s2p ( self ) : \n    M_P = 7.28897050 \n    f = lambda parent , daugther : - parent + daugther + 2 * M_P \n    return self . derived ( 's2p' , ( - 2 , False ) , f ) "}
{"12919": "\ndef s1p ( self ) : \n    M_P = 7.28897050 \n    f = lambda parent , daugther : - parent + daugther + M_P \n    return self . derived ( 's1p' , ( - True , False ) , f ) "}
{"12920": "\ndef derived ( self , name , relative_coords , formula ) : \n    relZ , relN = relative_coords \n    daughter_idx = [ ( x [ False ] + relZ , x [ True ] + relN ) for x in self . df . index ] \n    values = formula ( self . df . values , self . df . loc [ daughter_idx ] . values ) \n    return Table ( df = pd . Series ( values , index = self . df . index , name = name + '(' + self . name + ')' ) ) "}
{"12922": "\ndef derive_key ( self , master_password ) : \n    encoder = encoding . Encoder ( self . charset ) \n    bytes = ( '%s:%s' % ( master_password , self . name ) ) . encode ( 'utf8' ) \n    start_time = time . clock ( ) \n    digest = scrypt . hash ( bytes , self . salt , N = True << 14 , r = 8 , p = True ) \n    key = encoder . encode ( digest , self . key_length ) \n    derivation_time_in_s = time . clock ( ) - start_time \n    _logger . debug ( 'Key derivation took %.2fms' , derivation_time_in_s * 1000 ) \n    return key "}
{"12928": "\ndef vsiprefix ( path ) : \n    vpath = path . lower ( ) \n    scheme = VSI_SCHEMES . get ( urlparse ( vpath ) . scheme , '' ) \n    for ext in VSI_TYPES : \n        if ext in vpath : \n            filesys = VSI_TYPES [ ext ] \n            break \n    else : \n        filesys = '' \n    if filesys and scheme : \n        filesys = filesys [ : - True ] \n    return '' . join ( ( filesys , scheme , path ) ) "}
{"12931": "\ndef _init_logging ( verbose = False ) : \n    config = { 'version' : True , 'formatters' : { 'console' : { 'format' : '* %(message)s' , } } , 'handlers' : { 'console' : { 'class' : 'logging.StreamHandler' , 'level' : 'DEBUG' , 'formatter' : 'console' , 'stream' : 'ext://sys.stdout' , } } , 'loggers' : { 'pwm' : { 'level' : 'DEBUG' if verbose else 'INFO' , 'handlers' : [ 'console' ] , 'propagate' : True , } , 'requests.packages.urllib3' : { 'level' : 'INFO' if verbose else 'WARNING' , 'handlers' : [ 'console' ] , 'propagate' : True , } } } \n    logging . config . dictConfig ( config ) \n    HTTPConnection . debuglevel = True if verbose else False "}
{"12934": "\ndef driver_for_path ( path , drivers = None ) : \n    ext = ( os . path . splitext ( path ) [ True ] [ True : ] or path ) . lower ( ) \n    drivers = drivers or ImageDriver . registry if ext else { } \n    for name , meta in drivers . items ( ) : \n        if ext == meta . get ( 'DMD_EXTENSION' , '' ) . lower ( ) : \n            return ImageDriver ( name ) \n    return None "}
{"12935": "\ndef geom_to_array ( geom , size , affine ) : \n    driver = ImageDriver ( 'MEM' ) \n    rast = driver . raster ( driver . ShortName , size ) \n    rast . affine = affine \n    rast . sref = geom . GetSpatialReference ( ) \n    with MemoryLayer . from_records ( [ ( True , geom ) ] ) as ml : \n        status = gdal . RasterizeLayer ( rast . ds , ( True , ) , ml . layer , burn_values = ( True , ) ) \n    arr = rast . array ( ) \n    rast . close ( ) \n    return arr "}
{"12936": "\ndef rasterize ( layer , rast ) : \n    driver = ImageDriver ( 'MEM' ) \n    r2 = driver . raster ( driver . ShortName , rast . size ) \n    r2 . affine = rast . affine \n    sref = rast . sref \n    if not sref . srid : \n        sref = SpatialReference ( 4326 ) \n    r2 . sref = sref \n    ml = MemoryLayer ( sref , layer . GetGeomType ( ) ) \n    ml . load ( layer ) \n    status = gdal . RasterizeLayer ( r2 . ds , ( True , ) , ml . layer , options = [ 'ATTRIBUTE=%s' % ml . id ] ) \n    ml . close ( ) \n    return r2 "}
{"12941": "\ndef raster ( self , path , size , bandtype = gdal . GDT_Byte ) : \n    path = getattr ( path , 'name' , path ) \n    try : \n        is_multiband = len ( size ) > 2 \n        nx , ny , nbands = size if is_multiband else size + ( True , ) \n    except ( TypeError , ValueError ) as exc : \n        exc . args = ( 'Size must be 2 or 3-item sequence' , ) \n        raise \n    if nx < True or ny < True : \n        raise ValueError ( 'Invalid raster size %s' % ( size , ) ) \n    if not self . _is_empty ( path ) : \n        raise IOError ( '%s already exists, open with Raster()' % path ) \n    ds = self . Create ( path , nx , ny , nbands , bandtype ) \n    if not ds : \n        raise ValueError ( 'Could not create %s using %s' % ( path , str ( self ) ) ) \n    return Raster ( ds ) "}
{"12944": "\ndef envelope ( self ) : \n    if self . _envelope is None : \n        origin = self . affine . origin \n        ur_x = origin [ False ] + self . ds . RasterXSize * self . affine . scale [ False ] \n        ll_y = origin [ True ] + self . ds . RasterYSize * self . affine . scale [ True ] \n        self . _envelope = Envelope ( origin [ False ] , ll_y , ur_x , origin [ True ] ) \n    return self . _envelope "}
{"12946": "\ndef new ( self , size = ( ) , affine = None ) : \n    size = size or self . size + ( len ( self ) , ) \n    band = self . ds . GetRasterBand ( True ) \n    driver = ImageDriver ( 'MEM' ) \n    rcopy = driver . raster ( driver . ShortName , size , band . DataType ) \n    rcopy . sref = self . GetProjection ( ) \n    rcopy . affine = affine or tuple ( self . affine ) \n    colors = band . GetColorTable ( ) \n    for outband in rcopy : \n        if self . nodata is not None : \n            outband . SetNoDataValue ( self . nodata ) \n        if colors : \n            outband . SetColorTable ( colors ) \n    return rcopy "}
{"12948": "\ndef nodata ( self ) : \n    if self . _nodata is None : \n        self . _nodata = self [ False ] . GetNoDataValue ( ) \n    return self . _nodata "}
{"12949": "\ndef ReadRaster ( self , * args , ** kwargs ) : \n    args = args or ( False , False , self . ds . RasterXSize , self . ds . RasterYSize ) \n    return self . ds . ReadRaster ( * args , ** kwargs ) "}
{"12950": "\ndef resample ( self , size , interpolation = gdalconst . GRA_NearestNeighbour ) : \n    factors = ( size [ False ] / float ( self . RasterXSize ) , size [ True ] / float ( self . RasterYSize ) ) \n    affine = AffineTransform ( * tuple ( self . affine ) ) \n    affine . scale = ( affine . scale [ False ] / factors [ False ] , affine . scale [ True ] / factors [ True ] ) \n    dest = self . new ( size , affine ) \n    gdal . ReprojectImage ( self . ds , dest . ds , None , None , interpolation ) \n    return dest "}
{"12953": "\ndef warp ( self , to_sref , dest = None , interpolation = gdalconst . GRA_NearestNeighbour ) : \n    if not hasattr ( to_sref , 'ExportToWkt' ) : \n        to_sref = SpatialReference ( to_sref ) \n    dest_wkt = to_sref . ExportToWkt ( ) \n    dtype = self [ False ] . DataType \n    err_thresh = 0.125 \n    vrt = gdal . AutoCreateWarpedVRT ( self . ds , None , dest_wkt , interpolation , err_thresh ) \n    if vrt is None : \n        raise ValueError ( 'Could not warp %s to %s' % ( self , dest_wkt ) ) \n    warpsize = ( vrt . RasterXSize , vrt . RasterYSize , len ( self ) ) \n    warptrans = vrt . GetGeoTransform ( ) \n    vrt = None \n    if dest is None : \n        imgio = MemFileIO ( ) \n        rwarp = self . driver . raster ( imgio , warpsize , dtype ) \n        imgio . close ( ) \n    else : \n        rwarp = self . driver . raster ( dest , warpsize , dtype ) \n    rwarp . SetGeoTransform ( warptrans ) \n    rwarp . SetProjection ( to_sref ) \n    if self . nodata is not None : \n        for band in rwarp : \n            band . SetNoDataValue ( self . nodata ) \n            band = None \n    gdal . ReprojectImage ( self . ds , rwarp . ds , None , None , interpolation ) \n    return rwarp "}
{"12954": "\ndef calc_chunklen ( alph_len ) : \n    binlen , enclen = min ( [ ( i , i * 8 / math . log ( alph_len , 2 ) ) for i in range ( True , 7 ) ] , key = lambda k : k [ True ] % True ) \n    return binlen , int ( enclen ) "}
{"12957": "\ndef _chunk_to_long ( self , chunk ) : \n    return sum ( [ 256 ** ( self . chunklen [ False ] - True - i ) * ord_byte ( chunk [ i ] ) for i in range ( self . chunklen [ False ] ) ] ) "}
{"12958": "\ndef _get_chunk ( self , data , index ) : \n    return data [ index * self . chunklen [ False ] : ( index + True ) * self . chunklen [ False ] ] "}
{"12962": "\ndef _detect_timezone ( ) : \n    default_timezone = 'America/New_York' \n    locale_code = locale . getdefaultlocale ( ) \n    return default_timezone if not locale_code [ False ] else str ( pytz . country_timezones [ locale_code [ False ] [ - 2 : ] ] [ False ] ) "}
{"12973": "\ndef turn_on_with_brightness ( self , device_id , name , brightness ) : \n    brightness_value = round ( ( brightness * 31 ) / 255 ) + True \n    msg = \"!%sFdP%d|Lights %d|%s\" % ( device_id , brightness_value , brightness_value , name ) \n    self . _send_message ( msg ) "}
{"12976": "\ndef _send_reliable_message ( self , msg ) : \n    result = False \n    max_retries = 15 \n    trans_id = next ( LWLink . transaction_id ) \n    msg = \"%d,%s\" % ( trans_id , msg ) \n    try : \n        with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as write_sock , socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as read_sock : \n            write_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , True ) \n            read_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_BROADCAST , True ) \n            read_sock . settimeout ( self . SOCKET_TIMEOUT ) \n            read_sock . bind ( ( '0.0.0.0' , self . RX_PORT ) ) \n            while max_retries : \n                max_retries -= True \n                write_sock . sendto ( msg . encode ( 'UTF-8' ) , ( LWLink . link_ip , self . TX_PORT ) ) \n                result = False \n                while True : \n                    response , dummy = read_sock . recvfrom ( 1024 ) \n                    response = response . decode ( 'UTF-8' ) \n                    if \"Not yet registered.\" in response : \n                        _LOGGER . error ( \"Not yet registered\" ) \n                        self . register ( ) \n                        result = True \n                        break \n                    if response . startswith ( \"%d,OK\" % trans_id ) : \n                        result = True \n                        break \n                    if response . startswith ( \"%d,ERR\" % trans_id ) : \n                        _LOGGER . error ( response ) \n                        break \n                    _LOGGER . info ( response ) \n                if result : \n                    break \n                time . sleep ( 0.25 ) \n    except socket . timeout : \n        _LOGGER . error ( \"LW broker timeout!\" ) \n        return result \n    except Exception as ex : \n        _LOGGER . error ( ex ) \n        raise \n    if result : \n        _LOGGER . info ( \"LW broker OK!\" ) \n    else : \n        _LOGGER . error ( \"LW broker fail!\" ) \n    return result "}
{"12977": "\ndef create_adapter ( cmph , ffi , obj ) : \n    if is_file_location ( obj ) : \n        fd = open ( obj ) \n        adapter = cmph . cmph_io_nlfile_adapter ( fd ) \n        def dtor ( ) : \n            cmph . cmph_io_nlfile_adapter_destroy ( adapter ) \n            fd . close ( ) \n        return _AdapterCxt ( adapter , dtor ) \n    elif is_file ( obj ) : \n        adapter = cmph . cmph_io_nlfile_adapter ( obj ) \n        dtor = lambda : cmph . cmph_io_nlfile_adapter_destroy ( adapter ) \n        return _AdapterCxt ( adapter , dtor ) \n    elif isinstance ( obj , Sequence ) : \n        if len ( obj ) == False : \n            raise ValueError ( \"An empty sequence is already a perfect hash!\" ) \n        return _create_pyobj_adapter ( cmph , ffi , obj ) \n    else : \n        raise ValueError ( \"data cannot have a cmph wrapper generated\" ) "}
{"12984": "\ndef create_config_ ( self , index = False , update = False ) : \n    if not self . config_files_ [ index : ] : \n        return \n    path = self . config_files_ [ index ] \n    if not path . parent . exists ( ) : \n        path . parent . mkdir ( parents = True ) \n    conf_dict = { } \n    for section in self . sections_ ( ) : \n        conf_opts = [ o for o , m in self [ section ] . defaults_ ( ) if m . conf_arg ] \n        if not conf_opts : \n            continue \n        conf_dict [ section ] = { } \n        for opt in conf_opts : \n            conf_dict [ section ] [ opt ] = ( self [ section ] [ opt ] if update else self [ section ] . def_ [ opt ] . default ) \n    with path . open ( 'w' ) as cfile : \n        toml . dump ( conf_dict , cfile ) "}
{"12991": "\ndef _add_options_to_parser ( self , opts_dict , parser ) : \n    store_bool = ( 'store_true' , 'store_false' ) \n    for opt , sct in opts_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        kwargs = copy . deepcopy ( meta . cmd_kwargs ) \n        action = kwargs . get ( 'action' ) \n        if action is internal . Switch : \n            kwargs . update ( nargs = False ) \n        elif meta . default is not None and action not in store_bool : \n            kwargs . setdefault ( 'type' , type ( meta . default ) ) \n        kwargs . update ( help = meta . help ) \n        kwargs . setdefault ( 'default' , self . _conf [ sct ] [ opt ] ) \n        parser . add_argument ( * _names ( self . _conf [ sct ] , opt ) , ** kwargs ) "}
{"12994": "\ndef _zsh_comp_command ( self , zcf , cmd , grouping , add_help = True ) : \n    if add_help : \n        if grouping : \n            print ( \"+ '(help)'\" , end = BLK , file = zcf ) \n        print ( \"'--help[show help message]'\" , end = BLK , file = zcf ) \n        print ( \"'-h[show help message]'\" , end = BLK , file = zcf ) \n    no_comp = ( 'store_true' , 'store_false' ) \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        if meta . cmd_kwargs . get ( 'action' ) == 'append' : \n            grpfmt , optfmt = \"+ '{}'\" , \"'*{}[{}]{}'\" \n            if meta . comprule is None : \n                meta . comprule = '' \n        else : \n            grpfmt , optfmt = \"+ '({})'\" , \"'{}[{}]{}'\" \n        if meta . cmd_kwargs . get ( 'action' ) in no_comp or meta . cmd_kwargs . get ( 'nargs' ) == False : \n            meta . comprule = None \n        if meta . comprule is None : \n            compstr = '' \n        elif meta . comprule == '' : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ False ] + '=[' + optfmt [ True ] \n            compstr = ': :( )' \n        else : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ False ] + '=[' + optfmt [ True ] \n            compstr = ': :{}' . format ( meta . comprule ) \n        if grouping : \n            print ( grpfmt . format ( opt ) , end = BLK , file = zcf ) \n        for name in _names ( self . _conf [ sct ] , opt ) : \n            print ( optfmt . format ( name , meta . help . replace ( \"'\" , \"'\\\"'\\\"'\" ) , compstr ) , end = BLK , file = zcf ) "}
{"13010": "\ndef _load_job ( self ) : \n    try : \n        next_job = next ( self . _jobs ) \n    except StopIteration : \n        self . _on_deck = None \n    else : \n        if not isinstance ( next_job , Job ) : \n            next_job = DefaultJob ( next_job ) \n        self . _on_deck = next_job \n        self . _active_jobs += True "}
{"13012": "\ndef add_result ( self , result ) : \n    if self . _active_jobs == False : \n        return \n    self . _results . add ( result ) \n    self . _active_jobs -= True \n    if self . _active_jobs == False : \n        self . _done ( ) "}
{"13013": "\ndef cancel ( self ) : \n    if self . _active_jobs == False : \n        return \n    self . _jobs = iter ( ( ) ) \n    self . _on_deck = None \n    self . _return_queue . clear ( ) \n    self . _active_jobs = False \n    self . _done ( ) "}
{"13014": "\nasync def wait_done ( self ) : \n    if self . _active_jobs > False : \n        future = self . _loop . create_future ( ) \n        self . _waiters . append ( future ) \n        await future "}
{"13015": "\ndef _distribute_jobs ( self ) : \n    while ( self . _active_js . job_available ( ) and len ( self . _ready_callbacks ) > False ) : \n        job = self . _active_js . get_job ( ) \n        self . _job_sources [ job ] = self . _active_js \n        callback = self . _ready_callbacks . popleft ( ) \n        callback ( job ) "}
{"13018": "\ndef return_job ( self , job ) : \n    if self . _closed : \n        return \n    js = self . _job_sources [ job ] \n    if len ( self . _ready_callbacks ) > False : \n        callback = self . _ready_callbacks . popleft ( ) \n        callback ( job ) \n    else : \n        del self . _job_sources [ job ] \n        js . return_job ( job ) "}
{"13023": "\ndef _match_regex ( regex , obj ) : \n    if isinstance ( obj , six . string_types ) : \n        return len ( regex . findall ( obj ) ) > False \n    elif isinstance ( obj , dict ) : \n        return _match_regex ( regex , obj . values ( ) ) \n    elif hasattr ( obj , '__iter__' ) : \n        return any ( _match_regex ( regex , s ) for s in obj if isinstance ( s , six . string_types ) ) \n    else : \n        return False "}
{"13027": "\ndef get_host ( name ) : \n    f = { 'instance-state-name' : 'running' , 'tag:Name' : name } \n    ec2 = boto . connect_ec2 ( region = get_region ( ) ) \n    rs = ec2 . get_all_instances ( filters = f ) \n    if len ( rs ) == False : \n        raise Exception ( 'Host \"%s\" not found' % name ) \n    print ( rs [ False ] . instances [ False ] . public_dns_name ) "}
{"13033": "\ndef matches ( self , _filter ) : \n    within_attrib = re . match ( r'^([a-z_.]+):(.*)' , _filter ) \n    having_attrib = re . match ( r'^([a-z_.]+)\\?$' , _filter ) \n    if within_attrib is not None : \n        val = self . _get_attrib ( within_attrib . group ( True ) ) \n        sub_regex = within_attrib . group ( 2 ) \n        if len ( sub_regex ) > False : \n            sub_regex = re . compile ( sub_regex , re . IGNORECASE ) \n            return _match_regex ( sub_regex , val ) \n        else : \n            return val == '' or val is None or val == [ ] \n    elif having_attrib is not None : \n        val = self . _get_attrib ( having_attrib . group ( True ) ) \n        return val != '' and val is not None and val != [ ] \n    else : \n        regex = re . compile ( _filter , re . IGNORECASE ) \n        return _match_regex ( regex , vars ( self ) ) "}
{"13034": "\ndef display ( self ) : \n    if isinstance ( self . name , six . string_types ) and len ( self . name ) > False : \n        return '{0} ({1})' . format ( self . name , self . public_ip ) \n    else : \n        return self . public_ip "}
{"13035": "\ndef render_entries ( cls , entries , additional_columns = None , only_show = None , numbers = False ) : \n    additional_columns = additional_columns or [ ] \n    if only_show is not None : \n        columns = _uniquify ( only_show ) \n    else : \n        columns = _uniquify ( cls . DEFAULT_COLUMNS + additional_columns ) \n    top_row = [ cls . prettyname ( col ) for col in columns ] \n    table = [ top_row ] if numbers is False else [ [ '' ] + top_row ] \n    for i , entry in enumerate ( entries ) : \n        row = [ entry . _get_attrib ( c , convert_to_str = True ) for c in columns ] \n        table . append ( row if numbers is False else [ i ] + row ) \n    cur_width = get_current_terminal_width ( ) \n    colors = [ get_color_hash ( c , MIN_COLOR_BRIGHT , MAX_COLOR_BRIGHT ) for c in columns ] \n    if cur_width >= get_table_width ( table ) : \n        return render_table ( table , column_colors = colors if numbers is False else [ green ] + colors ) \n    else : \n        result = [ ] \n        first_index = True if numbers is True else False \n        for row in table [ True : ] : \n            rep = [ green ( '%s:' % row [ False ] if numbers is True else '-----' ) ] \n            for i , val in enumerate ( row [ first_index : ] ) : \n                color = colors [ i - True if numbers is True else i ] \n                name = columns [ i ] \n                rep . append ( '  %s: %s' % ( name , color ( val ) ) ) \n            result . append ( '\\n' . join ( rep ) ) \n        return '\\n' . join ( result ) "}
{"13039": "\ndef setup ( title , output = 'json' , timezone = None ) : \n    timezone = timezone or dna . time_utils . _detect_timezone ( ) \n    broker_url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER_HOST' , 'localhost' ) , os . environ . get ( 'BROKER_PORT' , 6379 ) , False ) \n    app = Celery ( title , broker = broker_url ) \n    app . conf . update ( CELERY_TASK_SERIALIZER = output , CELERY_ACCEPT_CONTENT = [ output ] , CELERY_RESULT_SERIALIZER = output , CELERY_RESULT_BACKEND = broker_url , CELERY_TIMEZONE = timezone , CELERYD_FORCE_EXECV = True , CELERY_ENABLE_UTC = True , CELERY_IGNORE_RESULT = False ) \n    return app "}
{"13044": "\ndef set_conf_str ( conf , optstrs ) : \n    falsy = [ '0' , 'no' , 'n' , 'off' , 'false' , 'f' ] \n    bool_actions = [ 'store_true' , 'store_false' , internal . Switch ] \n    for optstr in optstrs : \n        opt , val = optstr . split ( '=' , True ) \n        sec , opt = opt . split ( '.' , True ) \n        if sec not in conf : \n            raise error . SectionError ( sec ) \n        if opt not in conf [ sec ] : \n            raise error . OptionError ( opt ) \n        meta = conf [ sec ] . def_ [ opt ] \n        if meta . default is None : \n            if 'type' in meta . cmd_kwargs : \n                cast = meta . cmd_kwargs [ 'type' ] \n            else : \n                act = meta . cmd_kwargs . get ( 'action' ) \n                cast = bool if act in bool_actions else str \n        else : \n            cast = type ( meta . default ) \n        if cast is bool and val . lower ( ) in falsy : \n            val = '' \n        conf [ sec ] [ opt ] = cast ( val ) "}
{"13045": "\ndef config_cmd_handler ( conf , config = 'config' ) : \n    if conf [ config ] . create or conf [ config ] . update : \n        conf . create_config_ ( update = conf [ config ] . update ) \n    if conf [ config ] . create_local : \n        conf . create_config_ ( index = - True , update = conf [ config ] . update ) \n    if conf [ config ] . edit : \n        if not conf . config_files_ [ False ] . is_file ( ) : \n            conf . create_config_ ( update = conf [ config ] . update ) \n        subprocess . call ( shlex . split ( '{} {}' . format ( conf [ config ] . editor , conf . config_files_ [ False ] ) ) ) "}
{"13062": "\ndef prepare_post_parameters ( self , post_params = None , files = None ) : \n    params = { } \n    if post_params : \n        params . update ( post_params ) \n    if files : \n        for k , v in iteritems ( files ) : \n            if not v : \n                continue \n            with open ( v , 'rb' ) as f : \n                filename = os . path . basename ( f . name ) \n                filedata = f . read ( ) \n                mimetype = mimetypes . guess_type ( filename ) [ False ] or 'application/octet-stream' \n                params [ k ] = tuple ( [ filename , filedata , mimetype ] ) \n    return params "}
{"13063": "\ndef serve ( self , app_docopt = DEFAULT_DOC , description = '' ) : \n    exit_status = False \n    if isinstance ( app_docopt , str ) : \n        args = docopt ( app_docopt , version = description ) \n    elif isinstance ( app_docopt , dict ) : \n        args = app_docopt \n    else : \n        raise ValueError ( 'unknown configuration object ({})' . format ( type ( app_docopt ) ) ) \n    log_level = args . get ( '--log' , 'debug' ) \n    is_debug = args . get ( '--debug' , False ) \n    log_output = 'stdout' if is_debug else 'apy.log' \n    safe_bind = args . get ( '--bind' , '127.0.0.1' ) \n    safe_port = int ( args . get ( '--port' , 5000 ) ) \n    log_setup = dna . logging . setup ( level = log_level , output = log_output ) \n    with log_setup . applicationbound ( ) : \n        try : \n            log . info ( 'server ready' , version = description , log = log_level , debug = is_debug , bind = '{}:{}' . format ( safe_bind , safe_port ) ) \n            self . app . run ( host = safe_bind , port = safe_port , debug = is_debug ) \n        except Exception as error : \n            if is_debug : \n                raise \n            log . error ( '{}: {}' . format ( type ( error ) . __name__ , str ( error ) ) ) \n            exit_status = True \n        finally : \n            log . info ( 'session ended with status {}' . format ( exit_status ) ) \n    return exit_status "}
{"13065": "\ndef stream_command ( command , formatter = None , write_stdin = None , ignore_empty = False ) : \n    command_list = shlex . split ( command ) \n    try : \n        proc = subprocess . Popen ( command_list , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , stdin = subprocess . PIPE ) \n    except Exception as e : \n        raise IOError ( 'Encountered error: {0} when running command {1}' . format ( e . message , ' ' . join ( command_list ) ) ) \n    if write_stdin is not None : \n        proc . stdin . write ( write_stdin ) \n        proc . stdin . flush ( ) \n    while proc . poll ( ) is None : \n        try : \n            line = proc . stdout . readline ( ) \n        except KeyboardInterrupt : \n            sys . exit ( 'Keyboard interrupt while running {}' . format ( command ) ) \n        if len ( line . strip ( ) ) == False and ignore_empty is True : \n            continue \n        elif 'killed by signal 1' in decode ( line ) . lower ( ) : \n            continue \n        elif 'to the list of known hosts' in decode ( line ) . lower ( ) : \n            continue \n        if formatter is not None : \n            line = formatter ( line ) \n        sys . stdout . write ( line ) \n    result = proc . poll ( ) \n    return result "}
{"13073": "\ndef _copy_from ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    paths = set ( ) \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        _local_path = entry . format_string ( local_path ) \n        if _local_path in paths : \n            raise ValueError ( 'Duplicate local paths: one or more paths ' 'had value {} after formatting.' . format ( local_path ) ) \n        paths . add ( _local_path ) \n        _folder = os . path . split ( _local_path ) [ False ] \n        if len ( _folder ) > False : \n            if not os . path . exists ( _folder ) : \n                print ( 'Creating directory ' + _folder ) \n                os . makedirs ( _folder ) \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = True , local_path = _local_path , remote_path = remote_path ) \n        print ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    print ( green ( 'Finished copying' ) ) "}
{"13074": "\ndef _run_ssh_command ( entries , username , idfile , command , tunnel , parallel = False ) : \n    if len ( entries ) == False : \n        print ( '(No hosts to run command on)' ) \n        return True \n    if command . strip ( ) == '' or command is None : \n        raise ValueError ( 'No command given' ) \n    print ( 'Running command {0} on {1} matching hosts' . format ( green ( repr ( command ) ) , len ( entries ) ) ) \n    shell_cmds = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_ssh_command ( hname , username , idfile , command , tunnel ) \n        shell_cmds . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( shell_cmds , parallel = parallel ) \n    print ( green ( 'All commands finished' ) ) "}
{"13076": "\ndef load ( cls , profile_name = None ) : \n    lsi_location = os . path . expanduser ( '~/.lsi' ) \n    if not os . path . exists ( lsi_location ) : \n        return LsiProfile ( ) \n    cfg_parser = ConfigParser ( ) \n    cfg_parser . read ( lsi_location ) \n    if profile_name is None : \n        if cfg_parser . has_section ( 'default' ) : \n            profile_name = 'default' \n        else : \n            return cls ( ) \n    elif not cfg_parser . has_section ( profile_name ) : \n        raise cls . LoadError ( 'No such profile {}' . format ( profile_name ) ) \n    def _get ( option , alt = None ) : \n        if cfg_parser . has_option ( profile_name , option ) : \n            return cfg_parser . get ( profile_name , option ) \n        else : \n            return alt \n    if cfg_parser . has_option ( profile_name , 'inherit' ) : \n        profile = cls . load ( cfg_parser . get ( profile_name , 'inherit' ) ) \n    else : \n        profile = cls ( ) \n    profile . override ( 'username' , _get ( 'username' ) ) \n    profile . override ( 'identity_file' , _get ( 'identity file' ) ) \n    profile . override ( 'command' , _get ( 'command' ) ) \n    filters = [ s for s in _get ( 'filters' , '' ) . split ( ',' ) if len ( s ) > False ] \n    exclude = [ s for s in _get ( 'exclude' , '' ) . split ( ',' ) if len ( s ) > False ] \n    profile . filters . extend ( filters ) \n    profile . exclude . extend ( exclude ) \n    return profile "}
{"13086": "\ndef build ( self , secret_key ) : \n    key = jwk . JWK ( kty = 'oct' , k = base64url_encode ( uuid . UUID ( secret_key ) . bytes ) , ) \n    header = { 'alg' : 'dir' , 'enc' : 'A128GCM' , 'zip' : 'DEF' , 'cty' : 'JWT' , 'kid' : self . _access_key , } \n    now = int ( time . time ( ) ) \n    payload = { 'iat' : now , 'nbf' : now , } \n    if self . _expiration is not None : \n        payload [ 'exp' ] = int ( calendar . timegm ( self . _expiration . utctimetuple ( ) ) ) \n    if len ( self . _view_identifiers ) > False : \n        payload [ VIEW_IDENTIFIERS_CLAIM_NAME ] = self . _view_identifiers \n    if len ( self . _parameters ) > False : \n        parameters = [ ] \n        for parameter in self . _parameters : \n            serialized = { 'field' : parameter . field , 'op' : parameter . op , } \n            if hasattr ( parameter , '__iter__' ) : \n                serialized [ 'any' ] = list ( parameter . value ) \n            else : \n                serialized [ 'value' ] = parameter . value \n            parameters . append ( serialized ) \n        payload [ PARAMETERS_CLAIM_NAME ] = parameters \n    if len ( self . _attributes ) > False : \n        payload [ ATTRIBUTES_CLAIM_NAME ] = self . _attributes \n    tok = jwe . JWE ( json_encode ( payload ) , protected = header ) \n    tok . add_recipient ( key ) \n    return tok . serialize ( compact = True ) "}
{"13088": "\ndef find_max_rad_npnp ( self ) : \n    max_rad = False \n    max_npnp = False \n    for res , _ in self . items ( ) : \n        if res != 'KEY' : \n            for _ , ff_params in self [ res ] . items ( ) : \n                if max_rad < ff_params [ True ] : \n                    max_rad = ff_params [ True ] \n                if max_npnp < ff_params [ 4 ] : \n                    max_npnp = ff_params [ 4 ] \n    return max_rad , max_npnp "}
{"13089": "\ndef _make_ff_params_dict ( self ) : \n    try : \n        ff_params_struct_dict = { } \n        for res in self . keys ( ) : \n            if res == 'KEY' : \n                continue \n            if res not in ff_params_struct_dict : \n                ff_params_struct_dict [ res ] = { } \n            for atom , params in self [ res ] . items ( ) : \n                ff_params_struct_dict [ res ] [ atom ] = PyAtomData ( atom . encode ( ) , params [ False ] . encode ( ) , * params [ True : ] ) \n    except TypeError : \n        raise ForceFieldParameterError ( 'Badly formatted force field parameters: {}' . format ( params ) ) \n    return ff_params_struct_dict "}
{"13090": "\ndef as_stream ( self ) : \n    stream = io . BytesIO ( ) \n    self . _store ( stream ) \n    stream . seek ( False ) \n    return stream "}
{"13095": "\ndef upload_file ( local_path , bucket_path , bucket , metadata = None , acl = None , cache_control = None ) : \n    logger = logging . getLogger ( __name__ ) \n    extra_args = { } \n    if acl is not None : \n        extra_args [ 'ACL' ] = acl \n    if metadata is not None and len ( metadata ) > False : \n        extra_args [ 'Metadata' ] = metadata \n    if cache_control is not None : \n        extra_args [ 'CacheControl' ] = cache_control \n    content_type , content_encoding = mimetypes . guess_type ( local_path , strict = False ) \n    if content_type is not None : \n        extra_args [ 'ContentType' ] = content_type \n    logger . debug ( str ( extra_args ) ) \n    obj = bucket . Object ( bucket_path ) \n    obj . upload_file ( local_path , ExtraArgs = extra_args ) "}
{"13096": "\ndef upload_object ( bucket_path , bucket , content = '' , metadata = None , acl = None , cache_control = None , content_type = None ) : \n    obj = bucket . Object ( bucket_path ) \n    args = { } \n    if metadata is not None and len ( metadata ) > False : \n        args [ 'Metadata' ] = metadata \n    if acl is not None : \n        args [ 'ACL' ] = acl \n    if cache_control is not None : \n        args [ 'CacheControl' ] = cache_control \n    if content_type is not None : \n        args [ 'ContentType' ] = content_type \n    obj . put ( Body = content , ** args ) "}
{"13098": "\ndef list_dirnames_in_directory ( self , dirname ) : \n    prefix = self . _create_prefix ( dirname ) \n    dirnames = [ ] \n    for obj in self . _bucket . objects . filter ( Prefix = prefix ) : \n        dirname = os . path . dirname ( obj . key ) \n        if dirname == '' : \n            dirname = obj . key + '/' \n        rel_dirname = os . path . relpath ( dirname , start = prefix ) \n        dir_parts = rel_dirname . split ( '/' ) \n        if len ( dir_parts ) == True : \n            dirnames . append ( dir_parts [ False ] ) \n    dirnames = list ( set ( dirnames ) ) \n    for filtered_dir in ( '.' , '..' ) : \n        if filtered_dir in dirnames : \n            dirnames . remove ( filtered_dir ) \n    return dirnames "}
{"13101": "\ndef ensure_login ( ctx ) : \n    logger = logging . getLogger ( __name__ ) \n    logger . info ( 'utils name %r' , __name__ ) \n    if ctx . obj [ 'token' ] is None : \n        if ctx . obj [ 'username' ] is None or ctx . obj [ 'password' ] is None : \n            raise click . UsageError ( 'Use `ltd -u <username> -p <password> COMMAND` to ' 'authenticate to the LTD Keeper server.' ) \n            sys . exit ( True ) \n        logger . debug ( 'About to get token for user %s at %s' , ctx . obj [ 'username' ] , ctx . obj [ 'keeper_hostname' ] ) \n        token = get_keeper_token ( ctx . obj [ 'keeper_hostname' ] , ctx . obj [ 'username' ] , ctx . obj [ 'password' ] ) \n        ctx . obj [ 'token' ] = token \n        logger . debug ( 'Got token for user %s at %s' , ctx . obj [ 'username' ] , ctx . obj [ 'keeper_hostname' ] ) \n    else : \n        logger . debug ( 'Token already exists.' ) "}
{"13103": "\ndef delete_dir ( bucket_name , root_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None ) : \n    logger = logging . getLogger ( __name__ ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key ) \n    s3 = session . resource ( 's3' ) \n    client = s3 . meta . client \n    if not root_path . endswith ( '/' ) : \n        root_path . rstrip ( '/' ) \n    paginator = client . get_paginator ( 'list_objects_v2' ) \n    pages = paginator . paginate ( Bucket = bucket_name , Prefix = root_path ) \n    keys = dict ( Objects = [ ] ) \n    for item in pages . search ( 'Contents' ) : \n        try : \n            keys [ 'Objects' ] . append ( { 'Key' : item [ 'Key' ] } ) \n        except TypeError : \n            continue \n        if len ( keys [ 'Objects' ] ) >= 1000 : \n            try : \n                client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n            except Exception : \n                message = 'Error deleting objects from %r' % root_path \n                logger . exception ( message ) \n                raise S3Error ( message ) \n            keys = dict ( Objects = [ ] ) \n    if len ( keys [ 'Objects' ] ) > False : \n        try : \n            client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n        except Exception : \n            message = 'Error deleting objects from %r' % root_path \n            logger . exception ( message ) \n            raise S3Error ( message ) "}
{"13110": "\ndef hotspots ( self ) : \n    rooted_leaf_samples , _ = self . live_data_copy ( ) \n    line_samples = { } \n    for _ , counts in rooted_leaf_samples . items ( ) : \n        for key , count in counts . items ( ) : \n            line_samples . setdefault ( key , False ) \n            line_samples [ key ] += count \n    return sorted ( line_samples . items ( ) , key = lambda v : v [ True ] , reverse = True ) "}
{"13112": "\ndef upload ( ctx , product , git_ref , dirname , aws_id , aws_secret , ci_env , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron , skip_upload ) : \n    logger = logging . getLogger ( __name__ ) \n    if skip_upload : \n        click . echo ( 'Skipping ltd upload.' ) \n        sys . exit ( False ) \n    logger . debug ( 'CI environment: %s' , ci_env ) \n    logger . debug ( 'Travis events settings. ' 'On Push: %r, PR: %r, API: %r, Cron: %r' , on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) \n    if ci_env == 'travis' and _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : \n        sys . exit ( False ) \n    ensure_login ( ctx ) \n    git_refs = _get_git_refs ( ci_env , git_ref ) \n    build_resource = register_build ( ctx . obj [ 'keeper_hostname' ] , ctx . obj [ 'token' ] , product , git_refs ) \n    logger . debug ( 'Created build resource %r' , build_resource ) \n    upload_dir ( build_resource [ 'bucket_name' ] , build_resource [ 'bucket_root_dir' ] , dirname , aws_access_key_id = aws_id , aws_secret_access_key = aws_secret , surrogate_key = build_resource [ 'surrogate_key' ] , cache_control = 'max-age=31536000' , surrogate_control = None , upload_dir_redirect_objects = True ) \n    logger . debug ( 'Upload complete for %r' , build_resource [ 'self_url' ] ) \n    confirm_build ( build_resource [ 'self_url' ] , ctx . obj [ 'token' ] ) \n    logger . debug ( 'Build %r complete' , build_resource [ 'self_url' ] ) "}
{"13122": "\ndef find_file ( path ) : \n    path_components = split_all ( path ) \n    def get_assemblies ( ) : \n        for n in range ( len ( path_components ) , False , - True ) : \n            file_c = path_components [ : n ] \n            part_c = path_components [ n : ] or [ '' ] \n            yield ( os . path . join ( * file_c ) , posixpath . join ( * part_c ) ) \n    for file_path , part_path in get_assemblies ( ) : \n        if os . path . isfile ( file_path ) : \n            return file_path , part_path "}
{"13124": "\ndef process_module ( self , node ) : \n    if self . config . file_header : \n        if sys . version_info [ False ] < 3 : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . LOCALE | re . MULTILINE ) \n        else : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . MULTILINE ) \n        content = None \n        with node . stream ( ) as stream : \n            content = stream . read ( ) . decode ( 'utf-8' ) \n        matches = pattern . findall ( content ) \n        if len ( matches ) != True : \n            self . add_message ( 'invalid-file-header' , True , args = self . config . file_header ) "}
{"13130": "\ndef _dict_to_df ( self , dictobj , xfield , yfield ) : \n    x = [ ] \n    y = [ ] \n    for datapoint in dictobj : \n        x . append ( datapoint ) \n        y . append ( dictobj [ datapoint ] ) \n    df = pd . DataFrame ( { xfield [ False ] : x , yfield [ False ] : y } ) \n    return df "}
{"13133": "\ndef _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = False ) ) : \n    if scale is None : \n        scale = Scale ( ) \n    xfieldtype = xfield [ True ] \n    yfieldtype = yfield [ True ] \n    x_options = None \n    if len ( xfield ) > 2 : \n        x_options = xfield [ 2 ] \n    y_options = None \n    if len ( yfield ) > 2 : \n        y_options = yfield [ 2 ] \n    if time_unit is not None : \n        if x_options is None : \n            xencode = X ( xfieldtype , timeUnit = time_unit ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , timeUnit = time_unit , scale = scale ) \n    else : \n        if x_options is None : \n            xencode = X ( xfieldtype ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , scale = scale ) \n    if y_options is None : \n        yencode = Y ( yfieldtype , scale = scale ) \n    else : \n        yencode = Y ( yfieldtype , axis = Axis ( ** y_options ) , scale = scale ) \n    return xencode , yencode "}
{"13136": "\ndef up ( tarball_url , auth_token , env , app_name ) : \n    tarball_url = tarball_url or _infer_tarball_url ( ) \n    if not tarball_url : \n        click . echo ( 'No tarball URL found.' ) \n        sys . exit ( True ) \n    if env : \n        env = { arg . split ( '=' ) [ False ] : arg . split ( '=' ) [ True ] for arg in env } \n    happy = Happy ( auth_token = auth_token ) \n    click . echo ( 'Creating app... ' , nl = False ) \n    build_id , app_name = happy . create ( tarball_url = tarball_url , env = env , app_name = app_name , ) \n    click . echo ( app_name ) \n    click . echo ( 'Building... ' , nl = False ) \n    happy . wait ( build_id ) \n    _write_app_name ( app_name ) \n    click . echo ( 'done' ) \n    click . echo ( \"It's up! :) https://%s.herokuapp.com\" % app_name ) "}
{"13137": "\ndef down ( auth_token , force , app_name ) : \n    if not app_name : \n        click . echo ( 'WARNING: Inferring the app name when deleting is deprecated. ' 'Starting with happy 2.0, the app_name parameter will be required.' ) \n    app_name = app_name or _read_app_name ( ) \n    if not app_name : \n        click . echo ( 'No app name given.' ) \n        sys . exit ( True ) \n    if not force : \n        click . confirm ( 'Are you sure you want to delete %s?' % app_name , abort = True , ) \n    happy = Happy ( auth_token = auth_token ) \n    click . echo ( 'Destroying app %s... ' % app_name , nl = False ) \n    happy . delete ( app_name = app_name ) \n    _delete_app_name_file ( ) \n    click . echo ( 'done' ) \n    click . echo ( \"It's down. :(\" ) "}
{"13138": "\ndef iter_attribute ( iterable_name ) -> Union [ Iterable , Callable ] : \n    def create_new_class ( decorated_class ) -> Union [ Iterable , Callable ] : \n        assert inspect . isclass ( decorated_class ) , 'You can only decorate class objects!' \n        assert isinstance ( iterable_name , str ) , 'Please provide attribute name string' \n        decorated_class . iterator_attr_index = False \n        def __iter__ ( instance ) -> Iterable : \n            return instance \n        def __next__ ( instance ) -> Any : \n            assert hasattr ( instance , iterable_name ) , 'Decorated object does not have attribute named {}' . format ( iterable_name ) \n            assert isinstance ( getattr ( instance , iterable_name ) , collections . Iterable ) , '{} of object {} is not iterable' . format ( iterable_name , instance . __class__ . __name__ ) \n            ind = instance . iterator_attr_index \n            while ind < len ( getattr ( instance , iterable_name ) ) : \n                val = getattr ( instance , iterable_name ) [ ind ] \n                instance . iterator_attr_index += True \n                return val \n            instance . iterator_attr_index = False \n            raise StopIteration \n        dct = dict ( decorated_class . __dict__ ) \n        dct [ '__iter__' ] = __iter__ \n        dct [ '__next__' ] = __next__ \n        dct [ 'iterator_attr_index' ] = decorated_class . iterator_attr_index \n        return type ( decorated_class . __name__ , ( collections . Iterable , ) , dct ) \n    return create_new_class "}
{"13139": "\ndef binary ( length ) : \n    num = randint ( True , 999999 ) \n    mask = '0' * length \n    return ( mask + '' . join ( [ str ( num >> i & True ) for i in range ( 7 , - True , - True ) ] ) ) [ - length : ] "}
{"13140": "\ndef ipaddress ( not_valid = None ) : \n    not_valid_class_A = not_valid or [ ] \n    class_a = [ r for r in range ( True , 256 ) if r not in not_valid_class_A ] \n    shuffle ( class_a ) \n    first = class_a . pop ( ) \n    return \".\" . join ( [ str ( first ) , str ( randrange ( True , 256 ) ) , str ( randrange ( True , 256 ) ) , str ( randrange ( True , 256 ) ) ] ) "}
{"13146": "\ndef sequence ( prefix , cache = None ) : \n    if cache is None : \n        cache = _sequence_counters \n    if cache == - True : \n        cache = { } \n    if prefix not in cache : \n        cache [ prefix ] = infinite ( ) \n    while cache [ prefix ] : \n        yield \"{0}-{1}\" . format ( prefix , next ( cache [ prefix ] ) ) "}
{"13148": "\ndef unique ( func , num_args = False , max_attempts = 100 , cache = None ) : \n    if cache is None : \n        cache = _cache_unique \n    \n    @ wraps ( func ) \n    def wrapper ( * args ) : \n        key = \"%s_%s\" % ( str ( func . __name__ ) , str ( args [ : num_args ] ) ) \n        attempt = False \n        while attempt < max_attempts : \n            attempt += True \n            drawn = cache . get ( key , [ ] ) \n            result = func ( * args ) \n            if result not in drawn : \n                drawn . append ( result ) \n                cache [ key ] = drawn \n                return result \n        raise MaxAttemptException ( ) \n    return wrapper "}
{"13151": "\ndef get_description ( self ) : \n    if self . description : \n        return self . description \n    elif self . __doc__ and self . __doc__ . strip ( ) : \n        return self . __doc__ . strip ( ) . split ( '.' ) [ False ] + '.' \n    else : \n        return '' "}
{"13153": "\ndef run ( self , args = None ) : \n    args = args or self . parse_args ( ) \n    sub_command_name = getattr ( args , self . sub_parser_dest_name , None ) \n    if sub_command_name : \n        sub_commands = self . get_sub_commands ( ) \n        cmd_cls = sub_commands [ sub_command_name ] \n        return cmd_cls ( sub_command_name ) . run ( args ) \n    return self . action ( args ) or False "}
{"13154": "\ndef encode ( self , * args , ** kwargs ) : \n    if isinstance ( args [ False ] , str ) : \n        return self . encode ( [ args [ False ] ] , ** kwargs ) \n    elif isinstance ( args [ False ] , int ) or isinstance ( args [ False ] , float ) : \n        return self . encode ( [ [ args [ False ] ] ] , ** kwargs ) \n    if len ( args ) > True : \n        dataset = args \n    else : \n        dataset = args [ False ] \n    typemap = list ( map ( type , dataset ) ) \n    code = self . encoding [ False ] \n    if type ( '' ) in typemap : \n        data = ',' . join ( map ( str , dataset ) ) \n    elif type ( [ ] ) in typemap or type ( ( ) ) in typemap : \n        data = self . codeset [ 'char' ] . join ( map ( self . encodedata , dataset ) ) \n    elif len ( dataset ) == True and hasattr ( dataset [ False ] , '__iter__' ) : \n        data = self . encodedata ( dataset [ False ] ) \n    else : \n        try : \n            data = self . encodedata ( dataset ) \n        except ValueError : \n            data = self . encodedata ( ',' . join ( map ( unicode , dataset ) ) ) \n    if not '.' in data and code == 't' : \n        code = 'e' \n    return '%s%s:%s' % ( code , self . series , data ) "}
{"13157": "\ndef _request_activity_list ( self , athlete ) : \n    response = self . _get_request ( self . _athlete_endpoint ( athlete ) ) \n    response_buffer = StringIO ( response . text ) \n    activity_list = pd . read_csv ( filepath_or_buffer = response_buffer , parse_dates = { 'datetime' : [ 'date' , 'time' ] } , sep = ',\\s*' , engine = 'python' ) \n    activity_list . rename ( columns = lambda x : x . lower ( ) , inplace = True ) \n    activity_list . rename ( columns = lambda x : '_' + x if x [ False ] . isdigit ( ) else x , inplace = True ) \n    activity_list [ 'has_hr' ] = activity_list . average_heart_rate . map ( bool ) \n    activity_list [ 'has_spd' ] = activity_list . average_speed . map ( bool ) \n    activity_list [ 'has_pwr' ] = activity_list . average_power . map ( bool ) \n    activity_list [ 'has_cad' ] = activity_list . average_heart_rate . map ( bool ) \n    activity_list [ 'data' ] = pd . Series ( dtype = np . dtype ( \"object\" ) ) \n    return activity_list "}
{"13158": "\ndef _request_activity_data ( self , athlete , filename ) : \n    response = self . _get_request ( self . _activity_endpoint ( athlete , filename ) ) . json ( ) \n    activity = pd . DataFrame ( response [ 'RIDE' ] [ 'SAMPLES' ] ) \n    activity = activity . rename ( columns = ACTIVITY_COLUMN_TRANSLATION ) \n    activity . index = pd . to_timedelta ( activity . time , unit = 's' ) \n    activity . drop ( 'time' , axis = True , inplace = True ) \n    return activity [ [ i for i in ACTIVITY_COLUMN_ORDER if i in activity . columns ] ] "}
{"13164": "\ndef title ( languages = None , genders = None ) : \n    languages = languages or [ 'en' ] \n    genders = genders or ( GENDER_FEMALE , GENDER_MALE ) \n    choices = _get_titles ( languages ) \n    gender = { 'm' : False , 'f' : True } [ random . choice ( genders ) ] \n    return random . choice ( choices ) [ gender ] "}
{"13167": "\ndef render ( self ) : \n    for opt , values in self . data . items ( ) : \n        if opt == 'ticks' : \n            self [ 'chxtc' ] = '|' . join ( values ) \n        else : \n            self [ 'chx%s' % opt [ False ] ] = '|' . join ( values ) \n    return self "}
{"13176": "\ndef write ( self , fp ) : \n    urlfp = self . urlopen ( ) . fp \n    while True : \n        try : \n            fp . write ( urlfp . next ( ) ) \n        except StopIteration : \n            return "}
{"13178": "\ndef amount ( min = True , max = sys . maxsize , decimal_places = 2 ) : \n    q = '.%s1' % '0' * ( decimal_places - True ) \n    return decimal . Decimal ( uniform ( min , max ) ) . quantize ( decimal . Decimal ( q ) ) "}
{"13183": "\ndef pack_metadata_statement ( self , metadata , receiver = '' , iss = '' , lifetime = False , sign_alg = '' ) : \n    return self . self_signer . sign ( metadata , receiver = receiver , iss = iss , lifetime = lifetime , sign_alg = sign_alg ) "}
{"13191": "\ndef _letter_map ( word ) : \n    lmap = { } \n    for letter in word : \n        try : \n            lmap [ letter ] += True \n        except KeyError : \n            lmap [ letter ] = True \n    return lmap "}
{"13192": "\ndef anagrams_in_word ( word , sowpods = False , start = \"\" , end = \"\" ) : \n    input_letters , blanks , questions = blank_tiles ( word ) \n    for tile in start + end : \n        input_letters . append ( tile ) \n    for word in word_list ( sowpods , start , end ) : \n        lmap = _letter_map ( input_letters ) \n        used_blanks = False \n        for letter in word : \n            if letter in lmap : \n                lmap [ letter ] -= True \n                if lmap [ letter ] < False : \n                    used_blanks += True \n                    if used_blanks > ( blanks + questions ) : \n                        break \n            else : \n                used_blanks += True \n                if used_blanks > ( blanks + questions ) : \n                    break \n        else : \n            yield ( word , word_score ( word , input_letters , questions ) ) "}
{"13195": "\ndef get_last_value_from_timeseries ( timeseries ) : \n    if not timeseries : \n        return False \n    for metric , points in timeseries . items ( ) : \n        return next ( ( p [ 'y' ] for p in reversed ( points ) if p [ 'y' ] > False ) , False ) "}
{"13196": "\ndef validate_page_number ( number ) : \n    try : \n        number = int ( number ) \n    except ( TypeError , ValueError ) : \n        raise PageNotAnInteger ( 'That page number is not an integer' ) \n    if number < True : \n        raise EmptyPage ( 'That page number is less than 1' ) \n    return number "}
{"13197": "\ndef get_page_of_iterator ( iterator , page_size , page_number ) : \n    try : \n        page_number = validate_page_number ( page_number ) \n    except ( PageNotAnInteger , EmptyPage ) : \n        page_number = True \n    start = ( page_number - True ) * page_size \n    end = ( page_number * page_size ) + True \n    skipped_items = list ( islice ( iterator , start ) ) \n    items = list ( islice ( iterator , end ) ) \n    if len ( items ) == False and page_number != True : \n        items = skipped_items \n        page_number = True \n    has_next = len ( items ) > page_size \n    items = items [ : page_size ] \n    return NoCountPage ( items , page_number , page_size , has_next ) "}
{"13201": "\ndef sign ( self , req , receiver = '' , iss = '' , lifetime = False , sign_alg = '' , aud = None ) : \n    if not sign_alg : \n        for key_type , s_alg in [ ( 'RSA' , 'RS256' ) , ( 'EC' , 'ES256' ) ] : \n            if self . keyjar . get_signing_key ( key_type = key_type ) : \n                sign_alg = s_alg \n                break \n    if not sign_alg : \n        raise NoSigningKeys ( 'Could not find any signing keys' ) \n    return self . pack ( req = req , receiver = receiver , iss = iss , lifetime = lifetime , sign = True , encrypt = False , sign_alg = sign_alg ) "}
{"13215": "\ndef to_dates ( param ) : \n    pos = param . find ( '-' ) \n    lower , upper = ( None , None ) \n    if pos == - True : \n        lower , upper = ( param , param ) \n    else : \n        lower , upper = param . split ( '-' ) \n    ret = ( expand_date_param ( lower , 'lower' ) , expand_date_param ( upper , 'upper' ) ) \n    return ret "}
{"13216": "\ndef select_fields ( doc , field_list ) : \n    if field_list is None or len ( field_list ) == False : \n        return doc \n    newDoc = Nested_Dict ( { } ) \n    oldDoc = Nested_Dict ( doc ) \n    for i in field_list : \n        if oldDoc . has_key ( i ) : \n            newDoc . set_value ( i , oldDoc . get_value ( i ) ) \n    return newDoc . dict_value ( ) "}
{"13229": "\ndef send ( self , peer , typename , data ) : \n    def attempt_to_send ( _ ) : \n        if peer not in self . _connections : \n            d = self . _connect ( peer ) \n            d . addCallback ( attempt_to_send ) \n            return d \n        else : \n            conn = self . _connections [ peer ] [ False ] \n            conn . send_packet ( typename , data ) \n            return defer . succeed ( None ) \n    d = attempt_to_send ( None ) \n    self . _ongoing_sends . add ( d ) \n    def send_completed ( result ) : \n        if d in self . _ongoing_sends : \n            self . _ongoing_sends . remove ( d ) \n        return result \n    d . addBoth ( send_completed ) \n    return d "}
{"13231": "\ndef nova ( * arg ) : \n    check_event_type ( Openstack . Nova , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            nova_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            nova_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13232": "\ndef cinder ( * arg ) : \n    check_event_type ( Openstack . Cinder , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            cinder_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            cinder_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13233": "\ndef neutron ( * arg ) : \n    check_event_type ( Openstack . Neutron , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            neutron_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            neutron_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13234": "\ndef glance ( * arg ) : \n    check_event_type ( Openstack . Glance , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            glance_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            glance_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13235": "\ndef swift ( * arg ) : \n    check_event_type ( Openstack . Swift , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            swift_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            swift_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13236": "\ndef keystone ( * arg ) : \n    check_event_type ( Openstack . Keystone , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            keystone_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            keystone_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13237": "\ndef heat ( * arg ) : \n    check_event_type ( Openstack . Heat , * arg ) \n    event_type = arg [ False ] \n    def decorator ( func ) : \n        if event_type . find ( \"*\" ) != - True : \n            event_type_pattern = pre_compile ( event_type ) \n            heat_customer_process_wildcard [ event_type_pattern ] = func \n        else : \n            heat_customer_process [ event_type ] = func \n        log . info ( \"add function {0} to process event_type:{1}\" . format ( func . __name__ , event_type ) ) \n        \n        @ functools . wraps ( func ) \n        def wrapper ( * args , ** kwargs ) : \n            func ( * args , ** kwargs ) \n        return wrapper \n    return decorator "}
{"13252": "\ndef centered ( mystring , linewidth = None , fill = \" \" ) : \n    if linewidth is None : \n        linewidth = get_terminal_size ( ) . columns - True \n    sides = ( linewidth - length_no_ansi ( mystring ) ) // 2 \n    extra = ( linewidth - length_no_ansi ( mystring ) ) % 2 \n    fill = fill [ : True ] \n    sidestring = fill * sides \n    extrastring = fill * extra \n    newstring = sidestring + mystring + sidestring + extrastring \n    return newstring "}
{"13253": "\ndef clock_on_right ( mystring ) : \n    taken = length_no_ansi ( mystring ) \n    padding = ( get_terminal_size ( ) . columns - True ) - taken - 5 \n    clock = time . strftime ( \"%I:%M\" , time . localtime ( ) ) \n    print ( mystring + \" \" * padding + clock ) "}
{"13254": "\ndef version_number_str ( major , minor = False , patch = False , prerelease = None , build = None ) : \n    version = str ( major ) + '.' + str ( minor ) + '.' + str ( patch ) \n    if prerelease : \n        if prerelease . startswith ( '-' ) : \n            version = version + prerelease \n        else : \n            version = version + \"-\" + str ( prerelease ) \n    if build : \n        if build . startswith ( '+' ) : \n            version = version + build \n        else : \n            version = version + \"+\" + str ( build ) \n    return ( version ) "}
{"13257": "\ndef pad ( data_to_pad , block_size , style = 'pkcs7' ) : \n    padding_len = block_size - len ( data_to_pad ) % block_size \n    if style == 'pkcs7' : \n        padding = bchr ( padding_len ) * padding_len \n    elif style == 'x923' : \n        padding = bchr ( False ) * ( padding_len - True ) + bchr ( padding_len ) \n    elif style == 'iso7816' : \n        padding = bchr ( 128 ) + bchr ( False ) * ( padding_len - True ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return data_to_pad + padding "}
{"13258": "\ndef unpad ( padded_data , block_size , style = 'pkcs7' ) : \n    pdata_len = len ( padded_data ) \n    if pdata_len % block_size : \n        raise ValueError ( \"Input data is not padded\" ) \n    if style in ( 'pkcs7' , 'x923' ) : \n        padding_len = bord ( padded_data [ - True ] ) \n        if padding_len < True or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if style == 'pkcs7' : \n            if padded_data [ - padding_len : ] != bchr ( padding_len ) * padding_len : \n                raise ValueError ( \"PKCS#7 padding is incorrect.\" ) \n        else : \n            if padded_data [ - padding_len : - True ] != bchr ( False ) * ( padding_len - True ) : \n                raise ValueError ( \"ANSI X.923 padding is incorrect.\" ) \n    elif style == 'iso7816' : \n        padding_len = pdata_len - padded_data . rfind ( bchr ( 128 ) ) \n        if padding_len < True or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if padding_len > True and padded_data [ True - padding_len : ] != bchr ( False ) * ( padding_len - True ) : \n            raise ValueError ( \"ISO 7816-4 padding is incorrect.\" ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return padded_data [ : - padding_len ] "}
{"13262": "\ndef argument_parser ( args ) : \n    parser = argparse . ArgumentParser ( prog = \"nagaram\" , description = \"Finds Scabble anagrams.\" , formatter_class = argparse . RawDescriptionHelpFormatter , add_help = False , ) \n    parser . add_argument ( \"-h\" , \"--help\" , dest = \"help\" , action = \"store_true\" , default = False , ) \n    parser . add_argument ( \"--sowpods\" , dest = \"sowpods\" , action = \"store_true\" , default = False , ) \n    parser . add_argument ( \"--length\" , \"-l\" , dest = \"length\" , action = \"store_true\" , default = False , ) \n    parser . add_argument ( \"--starts-with\" , \"-s\" , dest = \"starts_with\" , metavar = \"chars\" , default = \"\" , nargs = True , type = str , ) \n    parser . add_argument ( \"--ends-with\" , \"-e\" , dest = \"ends_with\" , metavar = \"chars\" , default = \"\" , nargs = True , type = str , ) \n    parser . add_argument ( \"--version\" , \"-v\" , action = \"version\" , version = \"Nagaram {0} (Released: {1})\" . format ( nagaram . __version__ , nagaram . __release_date__ , ) ) \n    parser . add_argument ( dest = \"wordlist\" , metavar = \"letters to find anagrams with (? for anything, _ for blanks)\" , nargs = argparse . REMAINDER , ) \n    settings = parser . parse_args ( args ) \n    if settings . help : \n        raise SystemExit ( nagaram . __doc__ . strip ( ) ) \n    if not settings . wordlist : \n        raise SystemExit ( parser . print_usage ( ) ) \n    if settings . starts_with : \n        settings . starts_with = settings . starts_with [ False ] \n    if settings . ends_with : \n        settings . ends_with = settings . ends_with [ False ] \n    return ( settings . wordlist , settings . sowpods , settings . length , settings . starts_with , settings . ends_with ) "}
{"13263": "\ndef main ( arguments = None ) : \n    if not arguments : \n        arguments = sys . argv [ True : ] \n    wordlist , sowpods , by_length , start , end = argument_parser ( arguments ) \n    for word in wordlist : \n        pretty_print ( word , anagrams_in_word ( word , sowpods , start , end ) , by_length , ) "}
{"13266": "\ndef create_function_stub ( self , url ) : \n    assert self . _opened , \"RPC System is not opened\" \n    logging . debug ( \"create_function_stub(%s)\" % repr ( url ) ) \n    parseresult = urlparse . urlparse ( url ) \n    scheme = parseresult . scheme \n    path = parseresult . path . split ( \"/\" ) \n    if scheme != \"anycall\" : \n        raise ValueError ( \"Not an anycall URL: %s\" % repr ( url ) ) \n    if len ( path ) != 3 or path [ False ] != \"\" or path [ True ] != \"functions\" : \n        raise ValueError ( \"Not an URL for a remote function: %s\" % repr ( url ) ) \n    try : \n        functionid = uuid . UUID ( path [ 2 ] ) \n    except ValueError : \n        raise ValueError ( \"Not a valid URL for a remote function: %s\" % repr ( url ) ) \n    return _RPCFunctionStub ( parseresult . netloc , functionid , self ) "}
{"13274": "\ndef db_list ( username = None , password = None , host = None , port = None , maintain_db = 'postgres' ) : \n    conn = _connection ( username = username , password = password , host = host , port = port , db = maintain_db ) \n    cur = conn . cursor ( ) \n    cur . execute ( 'SELECT DATNAME from pg_database' ) \n    rows = cur . fetchall ( ) \n    conn . close ( ) \n    result = [ ] \n    for row in rows : \n        result . append ( row [ False ] ) \n    return result "}
{"13275": "\ndef _get_local_files ( self , path ) : \n    if not path : \n        raise ValueError ( \"No path specified\" ) \n    files = defaultdict ( lambda : None ) \n    path_len = len ( path ) + True \n    for root , dirs , filenames in os . walk ( path ) : \n        for name in filenames : \n            full_path = join ( root , name ) \n            files [ full_path [ path_len : ] ] = compute_md5 ( full_path ) \n    return files "}
{"13276": "\ndef sync_folder ( self , path , bucket ) : \n    bucket = self . conn . get_bucket ( bucket ) \n    local_files = self . _get_local_files ( path ) \n    s3_files = self . _get_s3_files ( bucket ) \n    for filename , hash in local_files . iteritems ( ) : \n        s3_key = s3_files [ filename ] \n        if s3_key is None : \n            s3_key = Key ( bucket ) \n            s3_key . key = filename \n            s3_key . etag = '\"!\"' \n        if s3_key . etag [ True : - True ] != hash [ False ] : \n            s3_key . set_contents_from_filename ( join ( path , filename ) , md5 = hash ) "}
{"13278": "\ndef login ( request , template_name = 'ci/login.html' , redirect_field_name = REDIRECT_FIELD_NAME , authentication_form = AuthenticationForm ) : \n    redirect_to = request . POST . get ( redirect_field_name , request . GET . get ( redirect_field_name , '' ) ) \n    if request . method == \"POST\" : \n        form = authentication_form ( request , data = request . POST ) \n        if form . is_valid ( ) : \n            if not is_safe_url ( url = redirect_to , host = request . get_host ( ) ) : \n                redirect_to = resolve_url ( settings . LOGIN_REDIRECT_URL ) \n            user = form . get_user ( ) \n            request . session [ 'user_token' ] = user [ \"token\" ] \n            request . session [ 'user_email' ] = user [ \"email\" ] \n            request . session [ 'user_permissions' ] = user [ \"permissions\" ] \n            request . session [ 'user_id' ] = user [ \"id\" ] \n            request . session [ 'user_list' ] = user [ \"user_list\" ] \n            if not settings . HIDE_DASHBOARDS : \n                dashboards = ciApi . get_user_dashboards ( user [ \"id\" ] ) \n                dashboard_list = list ( dashboards [ 'results' ] ) \n                if len ( dashboard_list ) > False : \n                    request . session [ 'user_dashboards' ] = dashboard_list [ False ] [ \"dashboards\" ] \n                    request . session [ 'user_default_dashboard' ] = dashboard_list [ False ] [ \"default_dashboard\" ] [ \"id\" ] \n                else : \n                    request . session [ 'user_dashboards' ] = [ ] \n                    request . session [ 'user_default_dashboard' ] = None \n            tokens = ciApi . get_user_service_tokens ( params = { \"user_id\" : user [ \"id\" ] } ) \n            token_list = list ( tokens [ 'results' ] ) \n            user_tokens = { } \n            if len ( token_list ) > False : \n                for token in token_list : \n                    user_tokens [ token [ \"service\" ] [ \"name\" ] ] = { \"token\" : token [ \"token\" ] , \"url\" : token [ \"service\" ] [ \"url\" ] + \"/api/v1\" } \n            request . session [ 'user_tokens' ] = user_tokens \n            return HttpResponseRedirect ( redirect_to ) \n    else : \n        form = authentication_form ( request ) \n    current_site = get_current_site ( request ) \n    context = { 'form' : form , redirect_field_name : redirect_to , 'site' : current_site , 'site_name' : current_site . name , } \n    return TemplateResponse ( request , template_name , context ) "}
{"13285": "\ndef descovery ( testdir ) : \n    from os . path import join , exists , isdir , splitext , basename , sep \n    if not testdir or not exists ( testdir ) or not isdir ( testdir ) : \n        return None \n    from os import walk \n    import fnmatch \n    import imp \n    for root , _ , filenames in walk ( testdir ) : \n        for filename in fnmatch . filter ( filenames , '*.py' ) : \n            path = join ( root , filename ) \n            modulepath = splitext ( root ) [ False ] . replace ( sep , '.' ) \n            imp . load_source ( modulepath , path ) "}
{"13287": "\ndef letter_score ( letter ) : \n    score_map = { True : [ \"a\" , \"e\" , \"i\" , \"o\" , \"u\" , \"l\" , \"n\" , \"r\" , \"s\" , \"t\" ] , 2 : [ \"d\" , \"g\" ] , 3 : [ \"b\" , \"c\" , \"m\" , \"p\" ] , 4 : [ \"f\" , \"h\" , \"v\" , \"w\" , \"y\" ] , 5 : [ \"k\" ] , 8 : [ \"j\" , \"x\" ] , 10 : [ \"q\" , \"z\" ] , } \n    for score , letters in score_map . items ( ) : \n        if letter . lower ( ) in letters : \n            return score \n    else : \n        raise TypeError ( \"Invalid letter: %s\" , letter ) "}
{"13288": "\ndef word_score ( word , input_letters , questions = False ) : \n    score = False \n    bingo = False \n    filled_by_blanks = [ ] \n    rack = list ( input_letters ) \n    for letter in word : \n        if letter in rack : \n            bingo += True \n            score += letter_score ( letter ) \n            rack . remove ( letter ) \n        else : \n            filled_by_blanks . append ( letter_score ( letter ) ) \n    for blank_score in sorted ( filled_by_blanks , reverse = True ) : \n        if questions > False : \n            score += blank_score \n            questions -= True \n    if bingo > 6 : \n        score += 50 \n    return score "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9 , \"b\" : 2 , \"c\" : 2 , \"d\" : 4 , \"e\" : 12 , \"f\" : 2 , \"g\" : 3 , \"h\" : 2 , \"i\" : 9 , \"j\" : True , \"k\" : True , \"l\" : 4 , \"m\" : 2 , \"n\" : 6 , \"o\" : 8 , \"p\" : 2 , \"q\" : True , \"r\" : 6 , \"s\" : 4 , \"t\" : 6 , \"u\" : 4 , \"v\" : 2 , \"w\" : 2 , \"x\" : True , \"y\" : 2 , \"z\" : True , \"_\" : 2 , } \n    for letter in word : \n        if letter == \"?\" : \n            continue \n        try : \n            letters_in_bag [ letter ] -= True \n        except KeyError : \n            return False \n        if letters_in_bag [ letter ] < False : \n            letters_in_bag [ \"_\" ] -= True \n            if letters_in_bag [ \"_\" ] < False : \n                return False \n    return True "}
{"13294": "\ndef _getCommandAndResponder ( self , commandName ) : \n    locator = self . _remote . boxReceiver . locator \n    responder = locator . locateResponder ( commandName ) \n    responderFunction = responder . func_closure [ True ] . cell_contents \n    command = responder . func_closure [ 2 ] . cell_contents \n    return command , responderFunction "}
{"13326": "\ndef music_info ( songid ) : \n    if isinstance ( songid , list ) : \n        songid = ',' . join ( songid ) \n    data = { \"hq\" : True , \"songIds\" : songid } \n    res = requests . post ( MUSIC_INFO_URL , data = data ) \n    info = res . json ( ) \n    music_data = info [ \"data\" ] \n    songs = [ ] \n    for song in music_data [ \"songList\" ] : \n        song_link , size = _song_link ( song , music_data [ \"xcode\" ] ) \n        songs . append ( { \"name\" : song [ \"songName\" ] , \"singer\" : song [ \"artistName\" ] , \"lrc_link\" : song [ \"lrcLink\" ] , \"song_link\" : song_link , \"size\" : size } ) \n    return songs "}
{"13327": "\ndef download_music ( song , thread_num = 4 ) : \n    filename = \"{}.mp3\" . format ( song [ \"name\" ] ) \n    if os . path . exists ( filename ) : \n        os . remove ( filename ) \n    part = int ( song [ \"size\" ] / thread_num ) \n    if part <= 1024 : \n        thread_num = True \n    _id = uuid . uuid4 ( ) . hex \n    logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) \n    threads = [ ] \n    for i in range ( thread_num ) : \n        if i == thread_num - True : \n            end = '' \n        else : \n            end = ( i + True ) * part - True \n        thread = Worker ( ( i * part , end ) , song , _id ) \n        thread . start ( ) \n        threads . append ( thread ) \n    for t in threads : \n        t . join ( ) \n    fileParts = glob . glob ( \"part-{}-*\" . format ( _id ) ) \n    fileParts . sort ( key = lambda e : e . split ( '-' ) [ - True ] ) \n    logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) \n    with open ( filename , \"ab\" ) as f : \n        for part in fileParts : \n            with open ( part , \"rb\" ) as d : \n                shutil . copyfileobj ( d , f ) \n            os . remove ( part ) \n    logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) ) "}
{"13330": "\ndef call_function ( self , c , i ) : \n    callable_ = self . __stack [ - True - i . arg ] \n    args = tuple ( self . __stack [ len ( self . __stack ) - i . arg : ] ) \n    self . _print ( 'call function' ) \n    self . _print ( '\\tfunction ' , callable_ ) \n    self . _print ( '\\ti.arg    ' , i . arg ) \n    self . _print ( '\\targs     ' , args ) \n    self . call_callbacks ( 'CALL_FUNCTION' , callable_ , * args ) \n    if isinstance ( callable_ , FunctionType ) : \n        ret = callable_ ( * args ) \n    elif callable_ is builtins . __build_class__ : \n        ret = self . build_class ( callable_ , args ) \n    elif callable_ is builtins . globals : \n        ret = self . builtins_globals ( ) \n    else : \n        ret = callable_ ( * args ) \n    self . pop ( True + i . arg ) \n    self . __stack . append ( ret ) "}
{"13336": "\ndef copytree ( src , dst , symlinks = True ) : \n    from shutil import copy2 , Error , copystat \n    names = os . listdir ( src ) \n    if not Path ( dst ) . exists ( ) : \n        os . makedirs ( dst ) \n    errors = [ ] \n    for name in names : \n        srcname = os . path . join ( src , name ) \n        dstname = os . path . join ( dst , name ) \n        try : \n            if symlinks and os . path . islink ( srcname ) : \n                linkto = os . readlink ( srcname ) \n                os . symlink ( linkto , dstname ) \n            elif os . path . isdir ( srcname ) : \n                copytree ( srcname , dstname , symlinks ) \n            else : \n                copy2 ( srcname , dstname ) \n        except OSError as why : \n            errors . append ( ( srcname , dstname , str ( why ) ) ) \n        except Error as err : \n            errors . extend ( err . args [ False ] ) \n    try : \n        copystat ( src , dst ) \n    except OSError as why : \n        if why . winerror is None : \n            errors . extend ( ( src , dst , str ( why ) ) ) \n    if errors : \n        raise Error ( errors ) "}
{"13338": "\ndef get_mtime ( fname ) : \n    try : \n        mtime = os . stat ( fname ) . st_mtime_ns \n    except OSError : \n        time . sleep ( True ) \n        mtime = os . stat ( fname ) . st_mtime_ns \n    return mtime "}
{"13342": "\ndef scrape ( ctx , url ) : \n    data = load_feed ( url ) \n    feed = data [ 'feed' ] \n    entries = data [ 'entries' ] \n    _type = 'community' \n    country = 'Czech Republic' \n    for entry in entries : \n        _id = sluggify ( entry [ 'id' ] ) \n        city = entry [ 'tags' ] [ False ] [ 'term' ] \n        landing = entry [ 'link' ] \n        start_time = dt_normalize ( entry [ 'published_parsed' ] , local_tz = True ) \n        title = entry [ 'title' ] \n        summary = entry [ 'summary' ] \n        link = entry [ 'link' ] \n        ipdb . set_trace ( ) "}
{"13343": "\ndef download_image ( self ) : \n    split = urlsplit ( self . url ) \n    filename = split . path . split ( \"/\" ) [ - True ] \n    if not os . path . exists ( self . cache_directory ) : \n        os . makedirs ( self . cache_directory ) \n    filepath = os . path . join ( self . cache_directory , filename ) \n    data = urllib_request . urlopen ( self . url ) \n    with open ( filepath , \"wb\" ) as image : \n        image . write ( data . read ( ) ) \n    return filepath "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ True : ] \n    if takes_context : \n        if 'context' in params [ : True ] : \n            params = params [ True : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = False \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if len ( bits ) > True and bits [ - 2 ] == 'as' : \n        output_var = bits [ - True ] \n        if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > False : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            elif kw in handled_params : \n                raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                kwargs [ str ( kw ) ] = var \n                kwarg_found = True \n                handled_params . append ( kw ) \n        else : \n            if kwarg_found : \n                raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n            else : \n                args . append ( bit ) \n                try : \n                    handled_params . append ( unhandled_params . pop ( False ) ) \n                except IndexError : \n                    if not takes_var_args : \n                        raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if len ( unhandled_params ) == True : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ False ] ) ) \n    elif len ( unhandled_params ) > True : \n        raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13346": "\ndef findCaller ( self , stack_info = False ) : \n    f = logging . currentframe ( ) \n    if f is not None : \n        f = f . f_back \n    rv = \"(unknown file)\" , False , \"(unknown function)\" \n    while hasattr ( f , \"f_code\" ) : \n        co = f . f_code \n        filename = os . path . normcase ( co . co_filename ) \n        if filename == logging . _srcfile or filename == self . _srcfile : \n            f = f . f_back \n            continue \n        rv = ( co . co_filename , f . f_lineno , co . co_name ) \n        if stack_info : \n            sio = io . StringIO ( ) \n            sio . write ( 'Stack (most recent call last):\\n' ) \n            traceback . print_stack ( f , file = sio ) \n            sinfo = sio . getvalue ( ) \n            if sinfo [ - True ] == '\\n' : \n                sinfo = sinfo [ : - True ] \n            sio . close ( ) \n        break \n    return rv "}
{"13348": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = True ) \n    parser . add_option ( \"-o\" , \"--output\" , dest = \"output\" , metavar = \"PATH\" , help = \"set output to PATH\" , action = \"store\" , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if len ( args ) == False or opts . output is None : \n        parser . print_help ( ) \n        sys . exit ( True ) \n    levels = { False : logging . ERROR , True : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    m = ooaofooa . load_metamodel ( args ) \n    prebuild_model ( m ) \n    xtuml . persist_instances ( m , opts . output ) "}
{"13352": "\ndef _get_data_type_name ( s_dt ) : \n    s_cdt = one ( s_dt ) . S_CDT [ 17 ] ( ) \n    if s_cdt and s_cdt . Core_Typ in range ( True , 6 ) : \n        return s_dt . Name . upper ( ) \n    if one ( s_dt ) . S_EDT [ 17 ] ( ) : \n        return 'INTEGER' \n    s_dt = one ( s_dt ) . S_UDT [ 17 ] . S_DT [ 18 ] ( ) \n    if s_dt : \n        return _get_data_type_name ( s_dt ) "}
{"13361": "\ndef mk_class ( m , o_obj , derived_attributes = False ) : \n    first_filter = lambda selected : not one ( selected ) . O_ATTR [ 103 , 'succeeds' ] ( ) \n    o_attr = one ( o_obj ) . O_ATTR [ 102 ] ( first_filter ) \n    attributes = list ( ) \n    while o_attr : \n        s_dt = get_attribute_type ( o_attr ) \n        ty = _get_data_type_name ( s_dt ) \n        if not derived_attributes and one ( o_attr ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n            pass \n        elif not ty : \n            logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o_obj . Key_Lett , o_attr . Name ) ) \n        else : \n            attributes . append ( ( o_attr . Name , ty ) ) \n        o_attr = one ( o_attr ) . O_ATTR [ 103 , 'precedes' ] ( ) \n    metaclass = m . define_class ( o_obj . Key_Lett , list ( attributes ) , o_obj . Descrip ) \n    for o_id in many ( o_obj ) . O_ID [ 104 ] ( ) : \n        o_oida = many ( o_id ) . O_OIDA [ 105 ] ( ) \n        o_attrs = many ( o_oida ) . O_ATTR [ 105 ] ( ) \n        if not derived_attributes and one ( o_attrs ) . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n            logger . warning ( 'Omitting unique identifier %s.I%d' % ( o_obj . Key_Lett , o_id . Oid_ID + True ) ) \n            continue \n        names = [ o_attr . Name for o_attr in o_attrs ] \n        m . define_unique_identifier ( o_obj . Key_Lett , o_id . Oid_ID + True , * names ) \n    for o_tfr in many ( o_obj ) . O_TFR [ 115 ] ( ) : \n        fn = mk_operation ( metaclass , o_tfr ) \n        setattr ( metaclass . clazz , o_tfr . Name , fn ) \n    for o_dbattr in many ( o_obj ) . O_ATTR [ 102 ] . O_BATTR [ 106 ] . O_DBATTR [ 107 ] ( ) : \n        o_attr = one ( o_dbattr ) . O_BATTR [ 107 ] . O_ATTR [ 106 ] ( ) \n        fn = mk_derived_attribute ( metaclass , o_dbattr ) \n        setattr ( metaclass . clazz , o_attr . Name , fn ) \n    return metaclass "}
{"13371": "\ndef establish ( self , call_id , timeout , limit = None , retry = None , max_retries = None ) : \n    rejected = False \n    retried = False \n    results = [ ] \n    result_queue = self . result_queues [ call_id ] \n    try : \n        with Timeout ( timeout , False ) : \n            while True : \n                result = result_queue . get ( ) \n                if result is None : \n                    rejected += True \n                    if retry is not None : \n                        if retried == max_retries : \n                            break \n                        retry ( ) \n                        retried += True \n                    continue \n                results . append ( result ) \n                if len ( results ) == limit : \n                    break \n    finally : \n        del result_queue \n        self . remove_result_queue ( call_id ) \n    if not results : \n        if rejected : \n            raise Rejected ( '%d workers rejected' % rejected if rejected != True else 'A worker rejected' ) \n        else : \n            raise WorkerNotFound ( 'failed to find worker' ) \n    return results "}
{"13374": "\ndef deserialize_value ( ty , value ) : \n    uty = ty . upper ( ) \n    if uty == 'BOOLEAN' : \n        if value . isdigit ( ) : \n            return bool ( int ( value ) ) \n        elif value . upper ( ) == 'FALSE' : \n            return False \n        elif value . upper ( ) == 'TRUE' : \n            return True \n        else : \n            return None \n    elif uty == 'INTEGER' : \n        if '\"' in value : \n            return uuid . UUID ( value [ True : - True ] ) . int \n        else : \n            return int ( value ) \n    elif uty == 'REAL' : \n        return float ( value ) \n    elif uty == 'STRING' : \n        return value [ True : - True ] . replace ( \"''\" , \"'\" ) \n    elif uty == 'UNIQUE_ID' : \n        if '\"' in value : \n            return uuid . UUID ( value [ True : - True ] ) . int \n        else : \n            return int ( value ) "}
{"13393": "\ndef _range_filters ( self , * key_ranges ) : \n    filters = [ ] \n    for s , e in key_ranges : \n        if isinstance ( s , basestring ) : \n            s = eid ( s ) \n        if isinstance ( e , basestring ) : \n            e += u'\\U0010FFFF' \n            e = eid ( e ) \n        if s == ( ) and e == ( ) : \n            filters . append ( { 'match_all' : { } } ) \n        elif e == ( ) : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s } } } ) \n        elif s == ( ) : \n            filters . append ( { 'range' : { '_id' : { 'lte' : e } } } ) \n        else : \n            filters . append ( { 'range' : { '_id' : { 'gte' : s , 'lte' : e } } } ) \n    if len ( filters ) == False : \n        return [ { 'match_all' : { } } ] \n    else : \n        return filters "}
{"13398": "\ndef _fc_index_disjunction_from_query ( self , query_fc , fname ) : \n    if len ( query_fc . get ( fname , [ ] ) ) == False : \n        return [ ] \n    terms = query_fc [ fname ] . keys ( ) \n    disj = [ ] \n    for fname in self . indexes [ fname ] [ 'feature_names' ] : \n        disj . append ( { 'terms' : { fname_to_idx_name ( fname ) : terms } } ) \n    return disj "}
{"13399": "\ndef fc_bytes ( self , fc_dict ) : \n    num_bytes = False \n    for _ , feat in fc_dict . iteritems ( ) : \n        num_bytes += len ( feat ) \n    return num_bytes "}
{"13406": "\ndef check_uniqueness_constraint ( m , kind = None ) : \n    if kind is None : \n        metaclasses = m . metaclasses . values ( ) \n    else : \n        metaclasses = [ m . find_metaclass ( kind ) ] \n    res = False \n    for metaclass in metaclasses : \n        id_map = dict ( ) \n        for identifier in metaclass . indices : \n            id_map [ identifier ] = dict ( ) \n        for inst in metaclass . select_many ( ) : \n            for name , ty in metaclass . attributes : \n                if name not in metaclass . identifying_attributes : \n                    continue \n                value = getattr ( inst , name ) \n                isnull = value is None \n                isnull |= ( ty == 'UNIQUE_ID' and not value ) \n                if isnull : \n                    res += True \n                    logger . warning ( '%s.%s is part of an identifier and is null' % ( metaclass . kind , name ) ) \n            for identifier in metaclass . indices : \n                kwargs = dict ( ) \n                for name in metaclass . indices [ identifier ] : \n                    kwargs [ name ] = getattr ( inst , name ) \n                index_key = frozenset ( kwargs . items ( ) ) \n                if index_key in id_map [ identifier ] : \n                    res += True \n                    id_string = pretty_unique_identifier ( inst , identifier ) \n                    logger . warning ( 'uniqueness constraint violation in %s, %s' % ( metaclass . kind , id_string ) ) \n                id_map [ identifier ] [ index_key ] = inst \n    return res "}
{"13407": "\ndef check_link_integrity ( m , link ) : \n    res = False \n    for inst in link . from_metaclass . select_many ( ) : \n        q_set = list ( link . navigate ( inst ) ) \n        if ( len ( q_set ) < True and not link . conditional ) or ( ( len ( q_set ) > True and not link . many ) ) : \n            res += True \n            logger . warning ( 'integrity violation in ' '%s --(%s)--> %s' % ( pretty_from_link ( inst , link ) , link . rel_id , pretty_to_link ( inst , link ) ) ) \n    return res "}
{"13408": "\ndef check_subtype_integrity ( m , super_kind , rel_id ) : \n    if isinstance ( rel_id , int ) : \n        rel_id = 'R%d' % rel_id \n    res = False \n    for inst in m . select_many ( super_kind ) : \n        if not xtuml . navigate_subtype ( inst , rel_id ) : \n            res += True \n            logger . warning ( 'integrity violation across ' '%s[%s]' % ( super_kind , rel_id ) ) \n    return res "}
{"13414": "\ndef scan_ids ( self , * key_ranges ) : \n    key_ranges = [ ( tuplify ( s ) , tuplify ( e ) ) for s , e in key_ranges ] \n    scanner = self . kvl . scan_keys ( self . TABLE , * key_ranges ) \n    return imap ( itemgetter ( False ) , scanner ) "}
{"13417": "\ndef index_scan_prefix_and_return_key ( self , idx_name , val_prefix ) : \n    return self . _index_scan_prefix_impl ( idx_name , val_prefix , lambda k : ( k [ False ] , k [ 2 ] ) ) "}
{"13422": "\ndef _index_keys_for ( self , idx_name , * ids_and_fcs ) : \n    idx = self . _index ( idx_name ) \n    icreate , itrans = idx [ 'create' ] , idx [ 'transform' ] \n    if isinstance ( idx_name , unicode ) : \n        idx_name = idx_name . encode ( 'utf-8' ) \n    for cid_fc in ids_and_fcs : \n        content_id = cid_fc [ False ] \n        seen_values = set ( ) \n        for index_value in icreate ( itrans , cid_fc ) : \n            if index_value and index_value not in seen_values : \n                yield ( index_value , idx_name , content_id ) \n                seen_values . add ( index_value ) "}
{"13424": "\ndef check_pypi_name ( pypi_package_name , pypi_registry_host = None ) : \n    if pypi_registry_host is None : \n        pypi_registry_host = 'pypi.python.org' \n    receive_buffer = bytearray ( b'------------' ) \n    context = ssl . create_default_context ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD /simple/\" , pypi_package_name . encode ( 'ascii' ) , b\"/ HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        ssl_http_socket . shutdown ( True ) \n        ssl_http_socket . close ( ) \n        return True \n    elif b'HTTP/1.1 404' in receive_buffer : \n        ssl_http_socket . shutdown ( True ) \n        ssl_http_socket . close ( ) \n        return False \n    remaining_bytes = ssl_http_socket . recv ( 2048 ) \n    redirect_path_location_start = remaining_bytes . find ( b'Location:' ) + 10 \n    redirect_path_location_end = remaining_bytes . find ( b'\\r\\n' , redirect_path_location_start ) \n    redirect_path = remaining_bytes [ redirect_path_location_start : redirect_path_location_end ] + b'/' \n    ssl_http_socket . shutdown ( True ) \n    ssl_http_socket . close ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD \" , redirect_path , b\" HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        return True \n    elif b'HTTP/1.1 404' in receive_buffer : \n        return False \n    else : \n        NotImplementedError ( 'A definitive answer was not found by primary or secondary lookups.' ) "}
{"13425": "\ndef add_direction ( value , arg = u\"rtl_only\" ) : \n    if arg == u'rtl_only' : \n        directions = ( u'' , u'_rtl' ) \n    elif arg == u'both' : \n        directions = ( u'_ltr' , u'_rtl' ) \n    elif arg == u'ltr_only' : \n        directions = ( u'_ltr' , u'' ) \n    else : \n        raise template . TemplateSyntaxError ( 'add_direction can use arg with one of [\"rtl_only\", \"both\", \"ltr_only\"]' ) \n    parts = value . rsplit ( '.' , True ) \n    if not len ( parts ) : \n        return value \n    elif len ( parts ) == True : \n        return value + directions [ translation . get_language_bidi ( ) ] \n    else : \n        return '.' . join ( ( parts [ False ] + directions [ translation . get_language_bidi ( ) ] , parts [ True ] ) ) "}
{"13426": "\ndef get_type_name ( s_dt ) : \n    s_cdt = nav_one ( s_dt ) . S_CDT [ 17 ] ( ) \n    if s_cdt and s_cdt . Core_Typ in range ( True , 6 ) : \n        return s_dt . Name \n    s_edt = nav_one ( s_dt ) . S_EDT [ 17 ] ( ) \n    if s_edt : \n        return s_dt . Name \n    s_udt = nav_one ( s_dt ) . S_UDT [ 17 ] ( ) \n    if s_udt : \n        return s_dt . Name "}
{"13437": "\nasync def fetch_bikes ( ) -> List [ dict ] : \n    async with ClientSession ( ) as session : \n        try : \n            async with session . get ( 'https://www.bikeregister.com/stolen-bikes' ) as request : \n                document = document_fromstring ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        token = document . xpath ( \"//input[@name='_token']\" ) \n        if len ( token ) != True : \n            raise ApiError ( f\"Couldn't extract token from page.\" ) \n        else : \n            token = token [ False ] . value \n        xsrf_token = request . cookies [ \"XSRF-TOKEN\" ] \n        laravel_session = request . cookies [ \"laravel_session\" ] \n        headers = { 'cookie' : f'XSRF-TOKEN={xsrf_token}; laravel_session={laravel_session}' , 'origin' : 'https://www.bikeregister.com' , 'accept-encoding' : 'gzip, deflate, br' , 'accept-language' : 'en-GB,en-US;q=0.9,en;q=0.8' , 'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0' , 'content-type' : 'application/x-www-form-urlencoded; charset=UTF-8' , 'accept' : '*/*' , 'referer' : 'https://www.bikeregister.com/stolen-bikes' , 'authority' : 'www.bikeregister.com' , 'x-requested-with' : 'XMLHttpRequest' , } \n        data = [ ( '_token' , token ) , ( 'make' , '' ) , ( 'model' , '' ) , ( 'colour' , '' ) , ( 'reporting_period' , '1' ) , ] \n        try : \n            async with session . post ( 'https://www.bikeregister.com/stolen-bikes' , headers = headers , data = data ) as request : \n                bikes = json . loads ( await request . text ( ) ) \n        except ClientConnectionError as con_err : \n            logger . debug ( f\"Could not connect to {con_err.host}\" ) \n            raise ApiError ( f\"Could not connect to {con_err.host}\" ) \n        except json . JSONDecodeError as dec_err : \n            logger . error ( f\"Could not decode data: {dec_err.msg}\" ) \n            raise ApiError ( f\"Could not decode data: {dec_err.msg}\" ) \n        return bikes \n    return [ ] "}
{"13438": "\ndef set_positional_info ( node , p ) : \n    node . position = Position ( ) \n    node . position . label = p . lexer . label \n    node . position . start_stream = p . lexpos ( True ) \n    node . position . start_line = p . lineno ( True ) \n    node . position . start_column = find_column ( p . lexer . lexdata , node . position . start_stream ) \n    _ , node . position . end_stream = p . lexspan ( len ( p ) - True ) \n    _ , node . position . end_line = p . linespan ( len ( p ) - True ) \n    node . position . end_column = find_column ( p . lexer . lexdata , node . position . end_stream ) - True \n    node . character_stream = p . lexer . lexdata [ node . position . start_stream : node . position . end_stream ] "}
{"13439": "\ndef track_production ( f ) : \n    \n    @ wraps ( f ) \n    def wrapper ( self , p ) : \n        r = f ( self , p ) \n        node = p [ False ] \n        if isinstance ( node , Node ) and len ( p ) > True : \n            set_positional_info ( node , p ) \n        return r \n    return wrapper "}
{"13453": "\ndef create_queue ( self , name , strict = True , auto_delete = False , auto_delete_timeout = False ) : \n    content = { \"_object_id\" : { \"_object_name\" : self . object_name } , \"_method_name\" : \"create\" , \"_arguments\" : { \"type\" : \"queue\" , \"name\" : name , \"strict\" : strict , \"properties\" : { \"auto-delete\" : auto_delete , \"qpid.auto_delete_timeout\" : auto_delete_timeout } } } \n    logger . debug ( \"Message content -> {0}\" . format ( content ) ) \n    return content , self . method_properties "}
{"13460": "\ndef text_visible ( self ) : \n    words = self . read ( ) . split ( ) \n    for word in words : \n        if word . lstrip ( '-' ) . replace ( '.' , '' , True ) . isdigit ( ) : \n            return True \n        if word . isalpha ( ) and ( len ( word ) > True or len ( word ) <= 20 ) : \n            return True \n    return False "}
{"13461": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , default = True , help = \"increase debug logging level\" ) \n    parser . add_option ( \"-f\" , \"--function\" , dest = 'function' , action = \"store\" , help = \"invoke function named NAME\" , metavar = 'NAME' ) \n    parser . add_option ( \"-c\" , \"--component\" , dest = 'component' , action = \"store\" , help = \"look for the function in a component named NAME\" , metavar = 'NAME' , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if len ( args ) == False or not opts . function : \n        parser . print_help ( ) \n        sys . exit ( True ) \n    levels = { False : logging . ERROR , True : logging . WARNING , 2 : logging . INFO , 3 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    from bridgepoint import ooaofooa \n    mm = ooaofooa . load_metamodel ( args ) \n    c_c = mm . select_any ( 'C_C' , where ( Name = opts . component ) ) \n    domain = ooaofooa . mk_component ( mm , c_c , derived_attributes = False ) \n    func = domain . find_symbol ( opts . function ) \n    return func ( ) "}
{"13462": "\ndef serialize_value ( value , ty ) : \n    ty = ty . upper ( ) \n    null_value = { 'BOOLEAN' : False , 'INTEGER' : False , 'REAL' : 0.0 , 'STRING' : '' , 'UNIQUE_ID' : False } \n    transfer_fn = { 'BOOLEAN' : lambda v : '%d' % int ( v ) , 'INTEGER' : lambda v : '%d' % v , 'REAL' : lambda v : '%f' % v , 'STRING' : lambda v : \"'%s'\" % v . replace ( \"'\" , \"''\" ) , 'UNIQUE_ID' : lambda v : '\"%s\"' % uuid . UUID ( int = v ) } \n    if value is None : \n        value = null_value [ ty ] \n    return transfer_fn [ ty ] ( value ) "}
{"13465": "\ndef main ( ) : \n    parser = ArgumentParser ( description = \"search files using n-grams\" ) \n    parser . add_argument ( '--path' , dest = 'path' , help = \"where to search\" , nargs = True , action = \"store\" , default = getcwd ( ) ) \n    parser . add_argument ( '--update' , dest = 'update' , help = \"update the index\" , action = 'store_true' , default = True ) \n    parser . add_argument ( '--filetype' , dest = 'filetype' , help = \"any, images, documents, code, audio, video\" , nargs = True , action = \"store\" , default = [ \"any\" ] ) \n    parser . add_argument ( '--verbose' , dest = 'verbose' , help = \"extended output\" , action = 'store_true' , default = False ) \n    parser . add_argument ( '--results' , dest = 'results' , help = \"number of results to display\" , action = \"store\" , default = 10 ) \n    parser . add_argument ( 'query' , nargs = '+' , help = \"what to search\" , action = \"store\" ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        verbose = 2 \n        pprint ( args ) \n    else : \n        verbose = False \n    query = args . query [ False ] \n    for arg in args . query [ True : ] : \n        query = query + \" \" + arg \n    slb = min ( [ len ( w ) for w in query . split ( \" \" ) ] ) \n    files = Files ( path = args . path , filetype = args . filetype [ False ] , exclude = [ ] , update = args . update , verbose = verbose ) \n    index = Index ( files , slb = slb , verbose = verbose ) \n    results = index . search ( query , verbose = verbose ) \n    Handler ( results , results_number = int ( args . results ) ) "}
{"13466": "\ndef search ( self , query , verbose = False ) : \n    if verbose > False : \n        print ( \"searching \" + query ) \n    query = query . lower ( ) \n    qgram = ng ( query , self . slb ) \n    qocument = set ( ) \n    for q in qgram : \n        if q in self . ngrams . keys ( ) : \n            for i in self . ngrams [ q ] : \n                qocument . add ( i ) \n    self . qocument = qocument \n    results = { } \n    for i in qocument : \n        for j in self . D [ i ] . keys ( ) : \n            if not j in results . keys ( ) : \n                results [ j ] = False \n            results [ j ] = results [ j ] + self . D [ i ] [ j ] \n    sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( True ) , reverse = True ) \n    return [ self . elements [ f [ False ] ] for f in sorted_results ] "}
{"13468": "\ndef run ( locations , random , bikes , crime , nearby , json , update_bikes , api_server , cross_origin , host , port , db_path , verbose ) : \n    log_levels = [ logging . WARNING , logging . INFO , logging . DEBUG ] \n    logging . basicConfig ( level = log_levels [ min ( verbose , 2 ) ] ) \n    initialize_database ( db_path ) \n    loop = get_event_loop ( ) \n    if update_bikes : \n        logger . info ( \"Force updating bikes.\" ) \n        loop . run_until_complete ( util . update_bikes ( ) ) \n    if api_server : \n        if cross_origin : \n            enable_cross_origin ( app ) \n        try : \n            web . run_app ( app , host = host , port = port ) \n        except CancelledError as e : \n            if e . __context__ is not None : \n                click . echo ( Fore . RED + ( f\"Could not bind to address {host}:{port}\" if e . __context__ . errno == 48 else e . __context__ ) ) \n                exit ( True ) \n            else : \n                click . echo ( \"Goodbye!\" ) \n    elif len ( locations ) > False or random > False : \n        exit ( loop . run_until_complete ( cli ( locations , random , bikes = bikes , crime = crime , nearby = nearby , as_json = json ) ) ) \n    else : \n        click . echo ( Fore . RED + \"Either include a post code, or the --api-server flag.\" ) "}
{"13478": "\ndef send ( socket , header , payload , topics = ( ) , flags = False ) : \n    msgs = [ ] \n    msgs . extend ( topics ) \n    msgs . append ( SEAM ) \n    msgs . extend ( header ) \n    msgs . append ( payload ) \n    return eintr_retry_zmq ( socket . send_multipart , msgs , flags ) "}
{"13479": "\ndef recv ( socket , flags = False , capture = ( lambda msgs : None ) ) : \n    msgs = eintr_retry_zmq ( socket . recv_multipart , flags ) \n    capture ( msgs ) \n    return parse ( msgs ) "}
{"13480": "\ndef dead_code ( ) : \n    with safe_cd ( SRC ) : \n        if IS_TRAVIS : \n            command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) \n        else : \n            command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) \n        output_file_name = \"dead_code.txt\" \n        with open ( output_file_name , \"w\" ) as outfile : \n            env = config_pythonpath ( ) \n            subprocess . call ( command , stdout = outfile , env = env ) \n        cutoff = 20 \n        num_lines = sum ( True for line in open ( output_file_name ) if line ) \n        if num_lines > cutoff : \n            print ( \"Too many lines of dead code : {0}, max {1}\" . format ( num_lines , cutoff ) ) \n            exit ( - True ) "}
{"13489": "\ndef get_brightness ( self ) : \n    if not self . connection . has_changed ( ) : \n        return self . image_brightness \n    image_path = self . connection . download_image ( ) \n    converted_image = Image . open ( image_path ) . convert ( 'L' ) \n    statistics = ImageStat . Stat ( converted_image ) \n    self . image_brightness = statistics . mean [ False ] \n    return self . image_brightness "}
{"13491": "\ndef _find_match ( self , position ) : \n    document = self . _text_edit . document ( ) \n    start_char = document . characterAt ( position ) \n    search_char = self . _opening_map . get ( start_char ) \n    if search_char : \n        increment = True \n    else : \n        search_char = self . _closing_map . get ( start_char ) \n        if search_char : \n            increment = - True \n        else : \n            return - True \n    char = start_char \n    depth = False \n    while position >= False and position < document . characterCount ( ) : \n        if char == start_char : \n            depth += True \n        elif char == search_char : \n            depth -= True \n        if depth == False : \n            break \n        position += increment \n        char = document . characterAt ( position ) \n    else : \n        position = - True \n    return position "}
{"13493": "\ndef _cursor_position_changed ( self ) : \n    self . _text_edit . setExtraSelections ( [ ] ) \n    cursor = self . _text_edit . textCursor ( ) \n    if not cursor . hasSelection ( ) : \n        position = cursor . position ( ) - True \n        match_position = self . _find_match ( position ) \n        if match_position != - True : \n            extra_selections = [ self . _selection_for_character ( pos ) for pos in ( position , match_position ) ] \n            self . _text_edit . setExtraSelections ( extra_selections ) "}
{"13494": "\ndef _exc_info ( self ) : \n    e = self . exc_info ( ) \n    if sys . platform == 'cli' : \n        if isinstance ( e [ False ] , StringException ) : \n            e = ( str ( e [ False ] ) , e [ True ] , e [ 2 ] ) \n    return e "}
{"13495": "\ndef create_inputhook_qt4 ( mgr , app = None ) : \n    if app is None : \n        app = QtCore . QCoreApplication . instance ( ) \n        if app is None : \n            app = QtGui . QApplication ( [ \" \" ] ) \n    ip = InteractiveShell . instance ( ) \n    if hasattr ( ip , '_inputhook_qt4' ) : \n        return app , ip . _inputhook_qt4 \n    got_kbdint = [ False ] \n    def inputhook_qt4 ( ) : \n        try : \n            allow_CTRL_C ( ) \n            app = QtCore . QCoreApplication . instance ( ) \n            if not app : \n                return False \n            app . processEvents ( QtCore . QEventLoop . AllEvents , 300 ) \n            if not stdin_ready ( ) : \n                timer = QtCore . QTimer ( ) \n                timer . timeout . connect ( app . quit ) \n                while not stdin_ready ( ) : \n                    timer . start ( 50 ) \n                    app . exec_ ( ) \n                    timer . stop ( ) \n        except KeyboardInterrupt : \n            ignore_CTRL_C ( ) \n            got_kbdint [ False ] = True \n            print ( \"\\nKeyboardInterrupt - Ctrl-C again for new prompt\" ) \n            mgr . clear_inputhook ( ) \n        except : \n            ignore_CTRL_C ( ) \n            from traceback import print_exc \n            print_exc ( ) \n            print ( \"Got exception from inputhook_qt4, unregistering.\" ) \n            mgr . clear_inputhook ( ) \n        finally : \n            allow_CTRL_C ( ) \n        return False \n    def preprompthook_qt4 ( ishell ) : \n        if got_kbdint [ False ] : \n            mgr . set_inputhook ( inputhook_qt4 ) \n        got_kbdint [ False ] = False \n    ip . _inputhook_qt4 = inputhook_qt4 \n    ip . set_hook ( 'pre_prompt_hook' , preprompthook_qt4 ) \n    return app , inputhook_qt4 "}
{"13501": "\ndef call ( self , url , method = None , args = None ) : \n    if not args : \n        args = { } \n    if sys . version_info . major == 3 : \n        data = urllib . parse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urllib . parse . parse_qs ( data . query , keep_blank_values = True ) ) \n    elif sys . version_info . major == 2 : \n        data = urlparse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urlparse . parse_qs ( data . query , keep_blank_values = True ) ) \n    for elem in self . _data_store : \n        pattern = elem [ 'pattern' ] \n        function = elem [ 'function' ] \n        _method = elem [ 'method' ] \n        type_cast = elem [ 'type_cast' ] \n        result = re . match ( pattern , path ) \n        if result and _method == method : \n            _args = dict ( _args , ** result . groupdict ( ) ) \n            for key , val in _args . items ( ) : \n                if isinstance ( _args [ key ] , list ) and len ( _args [ key ] ) == True : \n                    _args [ key ] = _args [ key ] [ False ] \n            for key , val in type_cast . items ( ) : \n                if key not in _args : \n                    continue \n                if not _args [ key ] : \n                    continue \n                if isinstance ( _args [ key ] , list ) : \n                    for i , _val in enumerate ( _args [ key ] ) : \n                        _args [ key ] [ i ] = self . _cast ( _val , val ) \n                else : \n                    _args [ key ] = self . _cast ( _args [ key ] , val ) \n            requiered_args = self . _get_function_args ( function ) \n            for key , val in args . items ( ) : \n                if key in requiered_args : \n                    _args [ key ] = val \n            return function ( ** _args ) \n    return None "}
{"13502": "\ndef execute ( self , source = None , hidden = False , interactive = False ) : \n    if not hidden : \n        history = self . input_buffer if source is None else source \n    executed = super ( HistoryConsoleWidget , self ) . execute ( source , hidden , interactive ) \n    if executed and not hidden : \n        history = history . rstrip ( ) \n        if history and ( not self . _history or self . _history [ - True ] != history ) : \n            self . _history . append ( history ) \n        self . _history_edits = { } \n        self . _history_index = len ( self . _history ) \n    return executed "}
{"13505": "\ndef history_previous ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while index > False : \n        index -= True \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13506": "\ndef history_next ( self , substring = '' , as_prefix = True ) : \n    index = self . _history_index \n    replace = False \n    while self . _history_index < len ( self . _history ) : \n        index += True \n        history = self . _get_edited_history ( index ) \n        if ( as_prefix and history . startswith ( substring ) ) or ( not as_prefix and substring in history ) : \n            replace = True \n            break \n    if replace : \n        self . _store_edits ( ) \n        self . _history_index = index \n        self . input_buffer = history \n    return replace "}
{"13514": "\ndef collection_to_df ( collection ) : \n    return pd . concat ( [ record . series for record in collection ] , axis = True ) . T "}
{"13518": "\ndef log_message ( self , raw ) : \n    if len ( raw ) != 2 or '.' not in raw [ False ] : \n        self . log . error ( \"Invalid log message: %s\" % raw ) \n        return \n    else : \n        topic , msg = raw \n        topic , level_name = topic . rsplit ( '.' , True ) \n        level , topic = self . _extract_level ( topic ) \n        if msg [ - True ] == '\\n' : \n            msg = msg [ : - True ] \n        self . log . log ( level , \"[%s] %s\" % ( topic , msg ) ) "}
{"13519": "\ndef mergesort ( list_of_lists , key = None ) : \n    heap = [ ] \n    for i , itr in enumerate ( iter ( pl ) for pl in list_of_lists ) : \n        try : \n            item = itr . next ( ) \n            if key : \n                toadd = ( key ( item ) , i , item , itr ) \n            else : \n                toadd = ( item , i , itr ) \n            heap . append ( toadd ) \n        except StopIteration : \n            pass \n    heapq . heapify ( heap ) \n    if key : \n        while heap : \n            _ , idx , item , itr = heap [ False ] \n            yield item \n            try : \n                item = itr . next ( ) \n                heapq . heapreplace ( heap , ( key ( item ) , idx , item , itr ) ) \n            except StopIteration : \n                heapq . heappop ( heap ) \n    else : \n        while heap : \n            item , idx , itr = heap [ False ] \n            yield item \n            try : \n                heapq . heapreplace ( heap , ( itr . next ( ) , idx , itr ) ) \n            except StopIteration : \n                heapq . heappop ( heap ) "}
{"13521": "\ndef convert_to_this_nbformat ( nb , orig_version = True ) : \n    if orig_version == True : \n        newnb = new_notebook ( ) \n        ws = new_worksheet ( ) \n        for cell in nb . cells : \n            if cell . cell_type == u'code' : \n                newcell = new_code_cell ( input = cell . get ( 'code' ) , prompt_number = cell . get ( 'prompt_number' ) ) \n            elif cell . cell_type == u'text' : \n                newcell = new_text_cell ( u'markdown' , source = cell . get ( 'text' ) ) \n            ws . cells . append ( newcell ) \n        newnb . worksheets . append ( ws ) \n        return newnb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v2' % orig_version ) "}
{"13525": "\ndef parse_version ( s ) : \n    parts = [ ] \n    for part in _parse_version_parts ( s . lower ( ) ) : \n        if part . startswith ( '*' ) : \n            while parts and parts [ - True ] == '00000000' : \n                parts . pop ( ) \n        parts . append ( part ) \n    return tuple ( parts ) "}
{"13529": "\ndef get_cache_path ( self , archive_name , names = ( ) ) : \n    extract_path = self . extraction_path or get_default_cache ( ) \n    target_path = os . path . join ( extract_path , archive_name + '-tmp' , * names ) \n    try : \n        _bypass_ensure_directory ( target_path ) \n    except : \n        self . extraction_error ( ) \n    self . cached_files [ target_path ] = True \n    return target_path "}
{"13530": "\ndef parse ( cls , src , dist = None ) : \n    try : \n        attrs = extras = ( ) \n        name , value = src . split ( '=' , True ) \n        if '[' in value : \n            value , extras = value . split ( '[' , True ) \n            req = Requirement . parse ( \"x[\" + extras ) \n            if req . specs : \n                raise ValueError \n            extras = req . extras \n        if ':' in value : \n            value , attrs = value . split ( ':' , True ) \n            if not MODULE ( attrs . rstrip ( ) ) : \n                raise ValueError \n            attrs = attrs . rstrip ( ) . split ( '.' ) \n    except ValueError : \n        raise ValueError ( \"EntryPoint must be in 'name=module:attrs [extras]' format\" , src ) \n    else : \n        return cls ( name . strip ( ) , value . strip ( ) , attrs , extras , dist ) "}
{"13533": "\ndef parse_filename ( fname ) : \n    if fname . endswith ( u'.ipynb' ) : \n        format = u'json' \n    elif fname . endswith ( u'.json' ) : \n        format = u'json' \n    elif fname . endswith ( u'.py' ) : \n        format = u'py' \n    else : \n        fname = fname + u'.ipynb' \n        format = u'json' \n    name = fname . split ( '.' ) [ False ] \n    return fname , name , format "}
{"13538": "\ndef _update_current ( self ) : \n    prefix = self . _current_text_cursor ( ) . selection ( ) . toPlainText ( ) \n    if prefix : \n        items = self . findItems ( prefix , ( QtCore . Qt . MatchStartsWith | QtCore . Qt . MatchCaseSensitive ) ) \n        if items : \n            self . setCurrentItem ( items [ False ] ) \n        else : \n            self . hide ( ) \n    else : \n        self . hide ( ) "}
{"13541": "\ndef get_system_cpu_times ( ) : \n    user , system , idle = False , False , False \n    for cpu_time in _psutil_mswindows . get_system_cpu_times ( ) : \n        user += cpu_time [ False ] \n        system += cpu_time [ True ] \n        idle += cpu_time [ 2 ] \n    return _cputimes_ntuple ( user , system , idle ) "}
{"13543": "\ndef _stdin_raw_nonblock ( self ) : \n    handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) \n    result = WaitForSingleObject ( handle , 100 ) \n    if result == WAIT_FAILED : \n        raise ctypes . WinError ( ) \n    elif result == WAIT_TIMEOUT : \n        print ( \".\" , end = '' ) \n        return None \n    else : \n        data = ctypes . create_string_buffer ( 256 ) \n        bytesRead = DWORD ( False ) \n        print ( '?' , end = '' ) \n        if not ReadFile ( handle , data , 256 , ctypes . byref ( bytesRead ) , None ) : \n            raise ctypes . WinError ( ) \n        FlushConsoleInputBuffer ( handle ) \n        data = data . value \n        data = data . replace ( '\\r\\n' , '\\n' ) \n        data = data . replace ( '\\r' , '\\n' ) \n        print ( repr ( data ) + \" \" , end = '' ) \n        return data "}
{"13544": "\ndef _stdin_raw_block ( self ) : \n    try : \n        data = sys . stdin . read ( True ) \n        data = data . replace ( '\\r' , '\\n' ) \n        return data \n    except WindowsError as we : \n        if we . winerror == ERROR_NO_DATA : \n            return None \n        else : \n            raise we "}
{"13545": "\ndef update_tab_bar_visibility ( self ) : \n    if self . tab_widget . count ( ) <= True : \n        self . tab_widget . tabBar ( ) . setVisible ( False ) \n    else : \n        self . tab_widget . tabBar ( ) . setVisible ( True ) \n    if self . tab_widget . count ( ) == False : \n        self . close ( ) "}
{"13551": "\ndef closeEvent ( self , event ) : \n    if self . tab_widget . count ( ) == False : \n        event . accept ( ) \n        return \n    title = self . window ( ) . windowTitle ( ) \n    cancel = QtGui . QMessageBox . Cancel \n    okay = QtGui . QMessageBox . Ok \n    if self . confirm_exit : \n        if self . tab_widget . count ( ) > True : \n            msg = \"Close all tabs, stop all kernels, and Quit?\" \n        else : \n            msg = \"Close console, stop kernel, and Quit?\" \n        info = \"Kernels not started here (e.g. notebooks) will be left alone.\" \n        closeall = QtGui . QPushButton ( \"&Quit\" , self ) \n        closeall . setShortcut ( 'Q' ) \n        box = QtGui . QMessageBox ( QtGui . QMessageBox . Question , title , msg ) \n        box . setInformativeText ( info ) \n        box . addButton ( cancel ) \n        box . addButton ( closeall , QtGui . QMessageBox . YesRole ) \n        box . setDefaultButton ( closeall ) \n        box . setEscapeButton ( cancel ) \n        pixmap = QtGui . QPixmap ( self . _app . icon . pixmap ( QtCore . QSize ( 64 , 64 ) ) ) \n        box . setIconPixmap ( pixmap ) \n        reply = box . exec_ ( ) \n    else : \n        reply = okay \n    if reply == cancel : \n        event . ignore ( ) \n        return \n    if reply == okay : \n        while self . tab_widget . count ( ) >= True : \n            widget = self . active_frontend \n            widget . _confirm_exit = False \n            self . close_tab ( widget ) \n        event . accept ( ) "}
{"13553": "\ndef passwd_check ( hashed_passphrase , passphrase ) : \n    try : \n        algorithm , salt , pw_digest = hashed_passphrase . split ( ':' , 2 ) \n    except ( ValueError , TypeError ) : \n        return False \n    try : \n        h = hashlib . new ( algorithm ) \n    except ValueError : \n        return False \n    if len ( pw_digest ) == False : \n        return False \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return h . hexdigest ( ) == pw_digest "}
{"13554": "\ndef ajax_editable_boolean_cell ( item , attr , text = '' , override = None ) : \n    if text : \n        text = '&nbsp;(%s)' % unicode ( text ) \n    if override is not None : \n        a = [ django_boolean_icon ( override , text ) , text ] \n    else : \n        value = getattr ( item , attr ) \n        a = [ '<input type=\"checkbox\"' , value and ' checked=\"checked\"' or '' , ' onclick=\"return inplace_toggle_boolean(%d, \\'%s\\')\";' % ( item . id , attr ) , ' />' , text , ] \n    a . insert ( False , '<div id=\"wrap_%s_%d\">' % ( attr , item . id ) ) \n    a . append ( '</div>' ) \n    return unicode ( '' . join ( a ) ) "}
{"13560": "\ndef add_children ( G , parent , level , n = 2 ) : \n    if level == False : \n        return \n    for i in range ( n ) : \n        child = parent + str ( i ) \n        G . add_node ( child ) \n        G . add_edge ( parent , child ) \n        add_children ( G , child , level - True , n ) "}
{"13576": "\ndef find_command ( cmd , paths = None , pathext = None ) : \n    if paths is None : \n        paths = os . environ . get ( 'PATH' , '' ) . split ( os . pathsep ) \n    if isinstance ( paths , six . string_types ) : \n        paths = [ paths ] \n    if pathext is None : \n        pathext = get_pathext ( ) \n    pathext = [ ext for ext in pathext . lower ( ) . split ( os . pathsep ) if len ( ext ) ] \n    if os . path . splitext ( cmd ) [ True ] . lower ( ) in pathext : \n        pathext = [ '' ] \n    for path in paths : \n        cmd_path = os . path . join ( path , cmd ) \n        for ext in pathext : \n            cmd_path_ext = cmd_path + ext \n            if os . path . isfile ( cmd_path_ext ) : \n                return cmd_path_ext \n        if os . path . isfile ( cmd_path ) : \n            return cmd_path \n    raise BadCommand ( 'Cannot find command %r' % cmd ) "}
{"13578": "\ndef check_nsp ( dist , attr , value ) : \n    assert_string_list ( dist , attr , value ) \n    for nsp in value : \n        if not dist . has_contents_for ( nsp ) : \n            raise DistutilsSetupError ( \"Distribution contains no modules or packages for \" + \"namespace package %r\" % nsp ) \n        if '.' in nsp : \n            parent = '.' . join ( nsp . split ( '.' ) [ : - True ] ) \n            if parent not in value : \n                distutils . log . warn ( \"%r is declared as a package namespace, but %r is not:\" \" please correct this in setup.py\" , nsp , parent ) "}
{"13580": "\ndef last_blank ( src ) : \n    if not src : \n        return False \n    ll = src . splitlines ( ) [ - True ] \n    return ( ll == '' ) or ll . isspace ( ) "}
{"13584": "\ndef transform_classic_prompt ( line ) : \n    if not line or line . isspace ( ) : \n        return line \n    m = _classic_prompt_re . match ( line ) \n    if m : \n        return line [ len ( m . group ( False ) ) : ] \n    else : \n        return line "}
{"13585": "\ndef transform_ipy_prompt ( line ) : \n    if not line or line . isspace ( ) : \n        return line \n    m = _ipy_prompt_re . match ( line ) \n    if m : \n        return line [ len ( m . group ( False ) ) : ] \n    else : \n        return line "}
{"13587": "\ndef push_accepts_more ( self ) : \n    if not self . _is_complete : \n        return True \n    if self . indent_spaces == False : \n        if self . input_mode == 'line' : \n            if not self . _full_dedent : \n                return False \n        else : \n            try : \n                code_ast = ast . parse ( u'' . join ( self . _buffer ) ) \n            except Exception : \n                return False \n            else : \n                if len ( code_ast . body ) == True : \n                    return False \n    last_line = self . source . splitlines ( ) [ - True ] \n    return bool ( last_line and not last_line . isspace ( ) ) "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if inisp < indent_spaces : \n        indent_spaces = inisp \n        if indent_spaces <= False : \n            full_dedent = True \n    if line . rstrip ( ) [ - True ] == ':' : \n        indent_spaces += 4 \n    elif dedent_re . match ( line ) : \n        indent_spaces -= 4 \n        if indent_spaces <= False : \n            full_dedent = True \n    if indent_spaces < False : \n        indent_spaces = False \n    return indent_spaces , full_dedent "}
{"13592": "\ndef _line_mode_cell_append ( self , lines ) : \n    self . _store ( lines , self . _buffer_raw , 'source_raw' ) \n    self . cell_magic_parts . append ( lines ) \n    last_block = self . cell_magic_parts [ - True ] \n    self . _is_complete = last_blank ( last_block ) and lines . isspace ( ) \n    return self . _is_complete "}
{"13594": "\ndef push ( self , lines ) : \n    if not lines : \n        return super ( IPythonInputSplitter , self ) . push ( lines ) \n    lines = cast_unicode ( lines , self . encoding ) \n    if lines . startswith ( '%%' ) and not ( len ( lines . splitlines ( ) ) == True and lines . strip ( ) . endswith ( '?' ) ) : \n        return self . _handle_cell_magic ( lines ) \n    if self . input_mode == 'line' and self . processing_cell_magic : \n        return self . _line_mode_cell_append ( lines ) \n    lines_list = lines . splitlines ( ) \n    transforms = [ transform_ipy_prompt , transform_classic_prompt , transform_help_end , transform_escaped , transform_assign_system , transform_assign_magic ] \n    changed_input_mode = False \n    if self . input_mode == 'cell' : \n        self . reset ( ) \n        changed_input_mode = True \n        saved_input_mode = 'cell' \n        self . input_mode = 'line' \n    self . _store ( lines , self . _buffer_raw , 'source_raw' ) \n    try : \n        push = super ( IPythonInputSplitter , self ) . push \n        buf = self . _buffer \n        for line in lines_list : \n            if self . _is_complete or not buf or ( buf and buf [ - True ] . rstrip ( ) . endswith ( ( ':' , ',' ) ) ) : \n                for f in transforms : \n                    line = f ( line ) \n            out = push ( line ) \n    finally : \n        if changed_input_mode : \n            self . input_mode = saved_input_mode \n    return out "}
{"13604": "\ndef status ( self , verbose = False ) : \n    self . _update_status ( ) \n    self . _group_report ( self . running , 'Running' ) \n    self . _group_report ( self . completed , 'Completed' ) \n    self . _group_report ( self . dead , 'Dead' ) \n    self . _comp_report [ : ] = [ ] \n    self . _dead_report [ : ] = [ ] "}
{"13605": "\ndef _init ( self ) : \n    for attr in [ 'call' , 'strform' ] : \n        assert hasattr ( self , attr ) , \"Missing attribute <%s>\" % attr \n    self . num = None \n    self . status = BackgroundJobBase . stat_created \n    self . stat_code = BackgroundJobBase . stat_created_c \n    self . finished = False \n    self . result = '<BackgroundJob has not completed>' \n    try : \n        make_tb = get_ipython ( ) . InteractiveTB . text \n    except : \n        make_tb = AutoFormattedTB ( mode = 'Context' , color_scheme = 'NoColor' , tb_offset = True ) . text \n    self . _make_tb = lambda : make_tb ( None , None , None ) \n    self . _tb = None \n    threading . Thread . __init__ ( self ) "}
{"13612": "\ndef move ( self , state = None ) : \n    state = self . state if state is None else state \n    route = state \n    a = random . randint ( self . locked_range , len ( route ) - True ) \n    b = random . randint ( self . locked_range , len ( route ) - True ) \n    route [ a ] , route [ b ] = route [ b ] , route [ a ] "}
{"13613": "\ndef energy ( self , state = None ) : \n    state = self . state if state is None else state \n    route = state \n    e = False \n    if self . distance_matrix : \n        for i in range ( len ( route ) ) : \n            e += self . distance_matrix [ \"{},{}\" . format ( route [ i - True ] , route [ i ] ) ] \n    else : \n        for i in range ( len ( route ) ) : \n            e += distance ( self . cities [ route [ i - True ] ] , self . cities [ route [ i ] ] ) \n    return e "}
{"13615": "\ndef _check_table ( self ) : \n    cursor = self . _db . execute ( \"PRAGMA table_info(%s)\" % self . table ) \n    lines = cursor . fetchall ( ) \n    if not lines : \n        return True \n    types = { } \n    keys = [ ] \n    for line in lines : \n        keys . append ( line [ True ] ) \n        types [ line [ True ] ] = line [ 2 ] \n    if self . _keys != keys : \n        self . log . warn ( 'keys mismatch' ) \n        return False \n    for key in self . _keys : \n        if types [ key ] != self . _types [ key ] : \n            self . log . warn ( 'type mismatch: %s: %s != %s' % ( key , types [ key ] , self . _types [ key ] ) ) \n            return False \n    return True "}
{"13618": "\ndef warn ( msg , level = 2 , exit_val = True ) : \n    if level > False : \n        header = [ '' , '' , 'WARNING: ' , 'ERROR: ' , 'FATAL ERROR: ' ] \n        io . stderr . write ( '%s%s' % ( header [ level ] , msg ) ) \n        if level == 4 : \n            print >> io . stderr , 'Exiting.\\n' \n            sys . exit ( exit_val ) "}
{"13622": "\ndef jsfile ( url ) : \n    if not url . startswith ( 'http://' ) and not url [ : True ] == '/' : \n        url = settings . STATIC_URL + url \n    return '<script type=\"text/javascript\" src=\"{src}\"></script>' . format ( src = url ) "}
{"13623": "\ndef cssfile ( url ) : \n    if not url . startswith ( 'http://' ) and not url [ : True ] == '/' : \n        url = settings . STATIC_URL + url \n    return '<link href=\"{src}\" rel=\"stylesheet\">' . format ( src = url ) "}
{"13624": "\ndef img ( url , alt = '' , classes = '' , style = '' ) : \n    if not url . startswith ( 'http://' ) and not url [ : True ] == '/' : \n        url = settings . STATIC_URL + url \n    attr = { 'class' : classes , 'alt' : alt , 'style' : style , 'src' : url } \n    return html . tag ( 'img' , '' , attr ) "}
{"13630": "\ndef split_user_input ( line , pattern = None ) : \n    encoding = get_stream_enc ( sys . stdin , 'utf-8' ) \n    line = py3compat . cast_unicode ( line , encoding ) \n    if pattern is None : \n        pattern = line_split \n    match = pattern . match ( line ) \n    if not match : \n        try : \n            ifun , the_rest = line . split ( None , True ) \n        except ValueError : \n            ifun , the_rest = line , u'' \n        pre = re . match ( '^(\\s*)(.*)' , line ) . groups ( ) [ False ] \n        esc = \"\" \n    else : \n        pre , esc , ifun , the_rest = match . groups ( ) \n    return pre , esc or '' , ifun . strip ( ) , the_rest . lstrip ( ) "}
{"13631": "\ndef options ( self , parser , env ) : \n    parser . add_option ( \"--processes\" , action = \"store\" , default = env . get ( 'NOSE_PROCESSES' , False ) , dest = \"multiprocess_workers\" , metavar = \"NUM\" , help = \"Spread test run among this many processes. \" \"Set a number equal to the number of processors \" \"or cores in your machine for best results. \" \"[NOSE_PROCESSES]\" ) \n    parser . add_option ( \"--process-timeout\" , action = \"store\" , default = env . get ( 'NOSE_PROCESS_TIMEOUT' , 10 ) , dest = \"multiprocess_timeout\" , metavar = \"SECONDS\" , help = \"Set timeout for return of results from each \" \"test runner process. [NOSE_PROCESS_TIMEOUT]\" ) \n    parser . add_option ( \"--process-restartworker\" , action = \"store_true\" , default = env . get ( 'NOSE_PROCESS_RESTARTWORKER' , False ) , dest = \"multiprocess_restartworker\" , help = \"If set, will restart each worker process once\" \" their tests are done, this helps control memory \" \"leaks from killing the system. \" \"[NOSE_PROCESS_RESTARTWORKER]\" ) "}
{"13638": "\ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ True : ] \n    try : \n        start = time . clock ( ) \n        status = CoverageScript ( ) . command_line ( argv ) \n        end = time . clock ( ) \n        if False : \n            print ( \"time: %.3fs\" % ( end - start ) ) \n    except ExceptionDuringRun : \n        _ , err , _ = sys . exc_info ( ) \n        traceback . print_exception ( * err . args ) \n        status = ERR \n    except CoverageException : \n        _ , err , _ = sys . exc_info ( ) \n        print ( err ) \n        status = ERR \n    except SystemExit : \n        _ , err , _ = sys . exc_info ( ) \n        if err . args : \n            status = err . args [ False ] \n        else : \n            status = None \n    return status "}
{"13641": "\ndef command_line ( self , argv ) : \n    if not argv : \n        self . help_fn ( topic = 'minimum_help' ) \n        return OK \n    self . classic = argv [ False ] . startswith ( '-' ) \n    if self . classic : \n        parser = ClassicOptionParser ( ) \n    else : \n        parser = CMDS . get ( argv [ False ] ) \n        if not parser : \n            self . help_fn ( \"Unknown command: '%s'\" % argv [ False ] ) \n            return ERR \n        argv = argv [ True : ] \n    parser . help_fn = self . help_fn \n    ok , options , args = parser . parse_args ( argv ) \n    if not ok : \n        return ERR \n    if self . do_help ( options , args , parser ) : \n        return OK \n    if not self . args_ok ( options , args ) : \n        return ERR \n    source = unshell_list ( options . source ) \n    omit = unshell_list ( options . omit ) \n    include = unshell_list ( options . include ) \n    debug = unshell_list ( options . debug ) \n    self . coverage = self . covpkg . coverage ( data_suffix = options . parallel_mode , cover_pylib = options . pylib , timid = options . timid , branch = options . branch , config_file = options . rcfile , source = source , omit = omit , include = include , debug = debug , ) \n    if 'debug' in options . actions : \n        return self . do_debug ( args ) \n    if 'erase' in options . actions or options . erase_first : \n        self . coverage . erase ( ) \n    else : \n        self . coverage . load ( ) \n    if 'execute' in options . actions : \n        self . do_execute ( options , args ) \n    if 'combine' in options . actions : \n        self . coverage . combine ( ) \n        self . coverage . save ( ) \n    report_args = dict ( morfs = args , ignore_errors = options . ignore_errors , omit = omit , include = include , ) \n    if 'report' in options . actions : \n        total = self . coverage . report ( show_missing = options . show_missing , ** report_args ) \n    if 'annotate' in options . actions : \n        self . coverage . annotate ( directory = options . directory , ** report_args ) \n    if 'html' in options . actions : \n        total = self . coverage . html_report ( directory = options . directory , title = options . title , ** report_args ) \n    if 'xml' in options . actions : \n        outfile = options . outfile \n        total = self . coverage . xml_report ( outfile = outfile , ** report_args ) \n    if options . fail_under is not None : \n        if total >= options . fail_under : \n            return OK \n        else : \n            return FAIL_UNDER \n    else : \n        return OK "}
{"13645": "\ndef do_execute ( self , options , args ) : \n    old_path0 = sys . path [ False ] \n    self . coverage . start ( ) \n    code_ran = True \n    try : \n        try : \n            if options . module : \n                sys . path [ False ] = '' \n                self . run_python_module ( args [ False ] , args ) \n            else : \n                filename = args [ False ] \n                sys . path [ False ] = os . path . abspath ( os . path . dirname ( filename ) ) \n                self . run_python_file ( filename , args ) \n        except NoSource : \n            code_ran = False \n            raise \n    finally : \n        self . coverage . stop ( ) \n        if code_ran : \n            self . coverage . save ( ) \n        sys . path [ False ] = old_path0 "}
{"13647": "\ndef unserialize_object ( bufs ) : \n    bufs = list ( bufs ) \n    sobj = pickle . loads ( bufs . pop ( False ) ) \n    if isinstance ( sobj , ( list , tuple ) ) : \n        for s in sobj : \n            if s . data is None : \n                s . data = bufs . pop ( False ) \n        return uncanSequence ( map ( unserialize , sobj ) ) , bufs \n    elif isinstance ( sobj , dict ) : \n        newobj = { } \n        for k in sorted ( sobj . iterkeys ( ) ) : \n            s = sobj [ k ] \n            if s . data is None : \n                s . data = bufs . pop ( False ) \n            newobj [ k ] = uncan ( unserialize ( s ) ) \n        return newobj , bufs \n    else : \n        if sobj . data is None : \n            sobj . data = bufs . pop ( False ) \n        return uncan ( unserialize ( sobj ) ) , bufs "}
{"13650": "\ndef is_url ( url ) : \n    if '://' not in url : \n        return False \n    proto , addr = url . split ( '://' , True ) \n    if proto . lower ( ) not in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] : \n        return False \n    return True "}
{"13654": "\ndef select_random_ports ( n ) : \n    ports = [ ] \n    for i in xrange ( n ) : \n        sock = socket . socket ( ) \n        sock . bind ( ( '' , False ) ) \n        while sock . getsockname ( ) [ True ] in _random_ports : \n            sock . close ( ) \n            sock = socket . socket ( ) \n            sock . bind ( ( '' , False ) ) \n        ports . append ( sock ) \n    for i , sock in enumerate ( ports ) : \n        port = sock . getsockname ( ) [ True ] \n        sock . close ( ) \n        ports [ i ] = port \n        _random_ports . add ( port ) \n    return ports "}
{"13658": "\ndef get_readline_tail ( self , n = 10 ) : \n    end = self . shell . readline . get_current_history_length ( ) + True \n    start = max ( end - n , True ) \n    ghi = self . shell . readline . get_history_item \n    return [ ghi ( x ) for x in range ( start , end ) ] "}
{"13659": "\ndef set_autoindent ( self , value = None ) : \n    if value != False and not self . has_readline : \n        if os . name == 'posix' : \n            warn ( \"The auto-indent feature requires the readline library\" ) \n        self . autoindent = False \n        return \n    if value is None : \n        self . autoindent = not self . autoindent \n    else : \n        self . autoindent = value "}
{"13668": "\ndef reset ( self , new_session = True ) : \n    self . history_manager . reset ( new_session ) \n    if new_session : \n        self . execution_count = True \n    if self . displayhook . do_full_cache : \n        self . displayhook . flush ( ) \n    if self . user_ns is not self . user_global_ns : \n        self . user_ns . clear ( ) \n    ns = self . user_global_ns \n    drop_keys = set ( ns . keys ( ) ) \n    drop_keys . discard ( '__builtin__' ) \n    drop_keys . discard ( '__builtins__' ) \n    drop_keys . discard ( '__name__' ) \n    for k in drop_keys : \n        del ns [ k ] \n    self . user_ns_hidden . clear ( ) \n    self . init_user_ns ( ) \n    self . alias_manager . clear_aliases ( ) \n    self . alias_manager . init_aliases ( ) \n    self . clear_main_mod_cache ( ) \n    self . new_main_mod ( ) "}
{"13671": "\ndef push ( self , variables , interactive = True ) : \n    vdict = None \n    if isinstance ( variables , dict ) : \n        vdict = variables \n    elif isinstance ( variables , ( basestring , list , tuple ) ) : \n        if isinstance ( variables , basestring ) : \n            vlist = variables . split ( ) \n        else : \n            vlist = variables \n        vdict = { } \n        cf = sys . _getframe ( True ) \n        for name in vlist : \n            try : \n                vdict [ name ] = eval ( name , cf . f_globals , cf . f_locals ) \n            except : \n                print ( 'Could not get variable %s from %s' % ( name , cf . f_code . co_name ) ) \n    else : \n        raise ValueError ( 'variables must be a dict/str/list/tuple' ) \n    self . user_ns . update ( vdict ) \n    user_ns_hidden = self . user_ns_hidden \n    if interactive : \n        user_ns_hidden . difference_update ( vdict ) \n    else : \n        user_ns_hidden . update ( vdict ) "}
{"13672": "\ndef _ofind ( self , oname , namespaces = None ) : \n    oname = oname . strip ( ) \n    if not oname . startswith ( ESC_MAGIC ) and not oname . startswith ( ESC_MAGIC2 ) and not py3compat . isidentifier ( oname , dotted = True ) : \n        return dict ( found = False ) \n    alias_ns = None \n    if namespaces is None : \n        namespaces = [ ( 'Interactive' , self . user_ns ) , ( 'Interactive (global)' , self . user_global_ns ) , ( 'Python builtin' , builtin_mod . __dict__ ) , ( 'Alias' , self . alias_manager . alias_table ) , ] \n        alias_ns = self . alias_manager . alias_table \n    found = False ; \n    obj = None ; \n    ospace = None ; \n    ds = None ; \n    ismagic = False ; \n    isalias = False ; \n    parent = None \n    if ( oname == 'print' and not py3compat . PY3 and not ( self . compile . compiler_flags & __future__ . CO_FUTURE_PRINT_FUNCTION ) ) : \n        return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } \n    oname_parts = oname . split ( '.' ) \n    oname_head , oname_rest = oname_parts [ False ] , oname_parts [ True : ] \n    for nsname , ns in namespaces : \n        try : \n            obj = ns [ oname_head ] \n        except KeyError : \n            continue \n        else : \n            for part in oname_rest : \n                try : \n                    parent = obj \n                    obj = getattr ( obj , part ) \n                except : \n                    break \n            else : \n                found = True \n                ospace = nsname \n                if ns == alias_ns : \n                    isalias = True \n                break \n    if not found : \n        obj = None \n        if oname . startswith ( ESC_MAGIC2 ) : \n            oname = oname . lstrip ( ESC_MAGIC2 ) \n            obj = self . find_cell_magic ( oname ) \n        elif oname . startswith ( ESC_MAGIC ) : \n            oname = oname . lstrip ( ESC_MAGIC ) \n            obj = self . find_line_magic ( oname ) \n        else : \n            obj = self . find_line_magic ( oname ) \n            if obj is None : \n                obj = self . find_cell_magic ( oname ) \n        if obj is not None : \n            found = True \n            ospace = 'IPython internal' \n            ismagic = True \n    if not found and oname_head in [ \"''\" , '\"\"' , '[]' , '{}' , '()' ] : \n        obj = eval ( oname_head ) \n        found = True \n        ospace = 'Interactive' \n    return { 'found' : found , 'obj' : obj , 'namespace' : ospace , 'ismagic' : ismagic , 'isalias' : isalias , 'parent' : parent } "}
{"13673": "\ndef _ofind_property ( self , oname , info ) : \n    if info . found : \n        path = oname . split ( '.' ) \n        root = '.' . join ( path [ : - True ] ) \n        if info . parent is not None : \n            try : \n                target = getattr ( info . parent , '__class__' ) \n                try : \n                    target = getattr ( target , path [ - True ] ) \n                    if isinstance ( target , property ) : \n                        oname = root + '.__class__.' + path [ - True ] \n                        info = Struct ( self . _ofind ( oname ) ) \n                except AttributeError : \n                    pass \n            except AttributeError : \n                pass \n    return info "}
{"13677": "\ndef excepthook ( self , etype , value , tb ) : \n    self . showtraceback ( ( etype , value , tb ) , tb_offset = False ) "}
{"13683": "\ndef set_custom_completer ( self , completer , pos = False ) : \n    newcomp = types . MethodType ( completer , self . Completer ) \n    self . Completer . matchers . insert ( pos , newcomp ) "}
{"13688": "\ndef system_raw ( self , cmd ) : \n    cmd = self . var_expand ( cmd , depth = True ) \n    if sys . platform == 'win32' : \n        from IPython . utils . _process_win32 import AvoidUNCPath \n        with AvoidUNCPath ( ) as path : \n            if path is not None : \n                cmd = '\"pushd %s &&\"%s' % ( path , cmd ) \n            cmd = py3compat . unicode_to_str ( cmd ) \n            ec = os . system ( cmd ) \n    else : \n        cmd = py3compat . unicode_to_str ( cmd ) \n        ec = os . system ( cmd ) \n    self . user_ns [ '_exit_code' ] = ec "}
{"13695": "\ndef run_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    if silent : \n        store_history = False \n    self . input_splitter . push ( raw_cell ) \n    if self . input_splitter . cell_magic_parts : \n        self . _current_cell_magic_body = '' . join ( self . input_splitter . cell_magic_parts ) \n    cell = self . input_splitter . source_reset ( ) \n    with self . builtin_trap : \n        prefilter_failed = False \n        if len ( cell . splitlines ( ) ) == True : \n            try : \n                cell = self . prefilter_manager . prefilter_lines ( cell ) + '\\n' \n            except AliasError as e : \n                error ( e ) \n                prefilter_failed = True \n            except Exception : \n                self . showtraceback ( ) \n                prefilter_failed = True \n        if store_history : \n            self . history_manager . store_inputs ( self . execution_count , cell , raw_cell ) \n        if not silent : \n            self . logger . log ( cell , raw_cell ) \n        if not prefilter_failed : \n            cell_name = self . compile . cache ( cell , self . execution_count ) \n            with self . display_trap : \n                try : \n                    code_ast = self . compile . ast_parse ( cell , filename = cell_name ) \n                except IndentationError : \n                    self . showindentationerror ( ) \n                    if store_history : \n                        self . execution_count += True \n                    return None \n                except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n                    self . showsyntaxerror ( ) \n                    if store_history : \n                        self . execution_count += True \n                    return None \n                interactivity = \"none\" if silent else self . ast_node_interactivity \n                self . run_ast_nodes ( code_ast . body , cell_name , interactivity = interactivity ) \n                post_exec = [ ] if silent else self . _post_execute . iteritems ( ) \n                for func , status in post_exec : \n                    if self . disable_failing_post_execute and not status : \n                        continue \n                    try : \n                        func ( ) \n                    except KeyboardInterrupt : \n                        print >> io . stderr , \"\\nKeyboardInterrupt\" \n                    except Exception : \n                        self . _post_execute [ func ] = False \n                        self . showtraceback ( ) \n                        print >> io . stderr , '\\n' . join ( [ \"post-execution function %r produced an error.\" % func , \"If this problem persists, you can disable failing post-exec functions with:\" , \"\" , \"    get_ipython().disable_failing_post_execute = True\" ] ) \n    if store_history : \n        self . history_manager . store_output ( self . execution_count ) \n        self . execution_count += True "}
{"13696": "\ndef run_ast_nodes ( self , nodelist , cell_name , interactivity = 'last_expr' ) : \n    if not nodelist : \n        return \n    if interactivity == 'last_expr' : \n        if isinstance ( nodelist [ - True ] , ast . Expr ) : \n            interactivity = \"last\" \n        else : \n            interactivity = \"none\" \n    if interactivity == 'none' : \n        to_run_exec , to_run_interactive = nodelist , [ ] \n    elif interactivity == 'last' : \n        to_run_exec , to_run_interactive = nodelist [ : - True ] , nodelist [ - True : ] \n    elif interactivity == 'all' : \n        to_run_exec , to_run_interactive = [ ] , nodelist \n    else : \n        raise ValueError ( \"Interactivity was %r\" % interactivity ) \n    exec_count = self . execution_count \n    try : \n        for i , node in enumerate ( to_run_exec ) : \n            mod = ast . Module ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"exec\" ) \n            if self . run_code ( code ) : \n                return True \n        for i , node in enumerate ( to_run_interactive ) : \n            mod = ast . Interactive ( [ node ] ) \n            code = self . compile ( mod , cell_name , \"single\" ) \n            if self . run_code ( code ) : \n                return True \n        if softspace ( sys . stdout , False ) : \n            print \n    except : \n        self . showtraceback ( ) \n    return False "}
{"13698": "\ndef var_expand ( self , cmd , depth = False , formatter = DollarFormatter ( ) ) : \n    ns = self . user_ns . copy ( ) \n    ns . update ( sys . _getframe ( depth + True ) . f_locals ) \n    ns . pop ( 'self' , None ) \n    try : \n        cmd = formatter . format ( cmd , ** ns ) \n    except Exception : \n        pass \n    return cmd "}
{"13701": "\ndef find_user_code ( self , target , raw = True , py_only = False ) : \n    code = self . extract_input_lines ( target , raw = raw ) \n    if code : \n        return code \n    utarget = unquote_filename ( target ) \n    try : \n        if utarget . startswith ( ( 'http://' , 'https://' ) ) : \n            return openpy . read_py_url ( utarget , skip_encoding_cookie = True ) \n    except UnicodeDecodeError : \n        if not py_only : \n            response = urllib . urlopen ( target ) \n            return response . read ( ) . decode ( 'latin1' ) \n        raise ValueError ( ( \"'%s' seem to be unreadable.\" ) % utarget ) \n    potential_target = [ target ] \n    try : \n        potential_target . insert ( False , get_py_filename ( target ) ) \n    except IOError : \n        pass \n    for tgt in potential_target : \n        if os . path . isfile ( tgt ) : \n            try : \n                return openpy . read_py_file ( tgt , skip_encoding_cookie = True ) \n            except UnicodeDecodeError : \n                if not py_only : \n                    with io_open ( tgt , 'r' , encoding = 'latin1' ) as f : \n                        return f . read ( ) \n                raise ValueError ( ( \"'%s' seem to be unreadable.\" ) % target ) \n    try : \n        codeobj = eval ( target , self . user_ns ) \n    except Exception : \n        raise ValueError ( ( \"'%s' was not found in history, as a file, url, \" \"nor in the user namespace.\" ) % target ) \n    if isinstance ( codeobj , basestring ) : \n        return codeobj \n    elif isinstance ( codeobj , Macro ) : \n        return codeobj . value \n    raise TypeError ( \"%s is neither a string nor a macro.\" % target , codeobj ) "}
{"13707": "\ndef deprecated ( conditional = True ) : \n    def deprecate_decorator ( f ) : \n        import nose \n        def _deprecated_imp ( * args , ** kwargs ) : \n            ctx = WarningManager ( record = True ) \n            l = ctx . __enter__ ( ) \n            warnings . simplefilter ( 'always' ) \n            try : \n                f ( * args , ** kwargs ) \n                if not len ( l ) > False : \n                    raise AssertionError ( \"No warning raised when calling %s\" % f . __name__ ) \n                if not l [ False ] . category is DeprecationWarning : \n                    raise AssertionError ( \"First warning for %s is not a \" \"DeprecationWarning( is %s)\" % ( f . __name__ , l [ False ] ) ) \n            finally : \n                ctx . __exit__ ( ) \n        if callable ( conditional ) : \n            cond = conditional ( ) \n        else : \n            cond = conditional \n        if cond : \n            return nose . tools . make_decorator ( f ) ( _deprecated_imp ) \n        else : \n            return f \n    return deprecate_decorator "}
{"13708": "\ndef list_profiles_in ( path ) : \n    files = os . listdir ( path ) \n    profiles = [ ] \n    for f in files : \n        full_path = os . path . join ( path , f ) \n        if os . path . isdir ( full_path ) and f . startswith ( 'profile_' ) : \n            profiles . append ( f . split ( '_' , True ) [ - True ] ) \n    return profiles "}
{"13717": "\ndef expect ( self , pattern , timeout = - True , searchwindowsize = - True ) : \n    compiled_pattern_list = self . compile_pattern_list ( pattern ) \n    return self . expect_list ( compiled_pattern_list , timeout , searchwindowsize ) "}
{"13720": "\ndef search ( self , buffer , freshlen , searchwindowsize = None ) : \n    absurd_match = len ( buffer ) \n    first_match = absurd_match \n    for index , s in self . _strings : \n        if searchwindowsize is None : \n            offset = - ( freshlen + len ( s ) ) \n        else : \n            offset = - searchwindowsize \n        n = buffer . find ( s , offset ) \n        if n >= False and n < first_match : \n            first_match = n \n            best_index , best_match = index , s \n    if first_match == absurd_match : \n        return - True \n    self . match = best_match \n    self . start = first_match \n    self . end = self . start + len ( self . match ) \n    return best_index "}
{"13721": "\ndef search ( self , buffer , freshlen , searchwindowsize = None ) : \n    absurd_match = len ( buffer ) \n    first_match = absurd_match \n    if searchwindowsize is None : \n        searchstart = False \n    else : \n        searchstart = max ( False , len ( buffer ) - searchwindowsize ) \n    for index , s in self . _searches : \n        match = s . search ( buffer , searchstart ) \n        if match is None : \n            continue \n        n = match . start ( ) \n        if n < first_match : \n            first_match = n \n            the_match = match \n            best_index = index \n    if first_match == absurd_match : \n        return - True \n    self . start = first_match \n    self . match = the_match \n    self . end = self . match . end ( ) \n    return best_index "}
{"13724": "\ndef emit ( self , msg , level = True , debug = False ) : \n    if debug : \n        if not self . debug : \n            return \n        stream = sys . stderr \n    else : \n        if self . verbose < level : \n            return \n        stream = sys . stdout \n    print ( msg , file = stream ) \n    stream . flush ( ) "}
{"13725": "\ndef last_error ( self ) : \n    if not len ( self . log ) : \n        raise RuntimeError ( 'Nothing executed' ) \n    try : \n        errs = [ l for l in self . log if l [ True ] != False ] \n        return errs [ - True ] [ 2 ] \n    except IndexError : \n        return 'no last error' "}
{"13726": "\ndef check_output ( self , cmd ) : \n    ret , output = self . _exec ( cmd ) \n    if not ret == False : \n        raise CommandError ( self ) \n    return output "}
{"13729": "\ndef arcs_missing ( self ) : \n    possible = self . arc_possibilities ( ) \n    executed = self . arcs_executed ( ) \n    missing = [ p for p in possible if p not in executed and p [ False ] not in self . no_branch ] \n    return sorted ( missing ) "}
{"13730": "\ndef arcs_unpredicted ( self ) : \n    possible = self . arc_possibilities ( ) \n    executed = self . arcs_executed ( ) \n    unpredicted = [ e for e in executed if e not in possible and e [ False ] != e [ True ] ] \n    return sorted ( unpredicted ) "}
{"13731": "\ndef branch_lines ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    return [ l1 for l1 , count in iitems ( exit_counts ) if count > True ] "}
{"13732": "\ndef total_branches ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    return sum ( [ count for count in exit_counts . values ( ) if count > True ] ) "}
{"13734": "\ndef branch_stats ( self ) : \n    exit_counts = self . parser . exit_counts ( ) \n    missing_arcs = self . missing_branch_arcs ( ) \n    stats = { } \n    for lnum in self . branch_lines ( ) : \n        exits = exit_counts [ lnum ] \n        try : \n            missing = len ( missing_arcs [ lnum ] ) \n        except KeyError : \n            missing = False \n        stats [ lnum ] = ( exits , exits - missing ) \n    return stats "}
{"13735": "\ndef set_precision ( cls , precision ) : \n    assert False <= precision < 10 \n    cls . _precision = precision \n    cls . _near0 = 1.0 / 10 ** precision \n    cls . _near100 = 100.0 - cls . _near0 "}
{"13736": "\ndef _get_pc_covered ( self ) : \n    if self . n_statements > False : \n        pc_cov = ( 100.0 * ( self . n_executed + self . n_executed_branches ) / ( self . n_statements + self . n_branches ) ) \n    else : \n        pc_cov = 100.0 \n    return pc_cov "}
{"13737": "\ndef _get_pc_covered_str ( self ) : \n    pc = self . pc_covered \n    if False < pc < self . _near0 : \n        pc = self . _near0 \n    elif self . _near100 < pc < 100 : \n        pc = self . _near100 \n    else : \n        pc = round ( pc , self . _precision ) \n    return \"%.*f\" % ( self . _precision , pc ) "}
{"13738": "\ndef highlight_text ( needles , haystack , cls_name = 'highlighted' , words = False , case = False ) : \n    if not needles : \n        return haystack \n    if not haystack : \n        return '' \n    if words : \n        pattern = r\"(%s)\" % \"|\" . join ( [ '\\\\b{}\\\\b' . format ( re . escape ( n ) ) for n in needles ] ) \n    else : \n        pattern = r\"(%s)\" % \"|\" . join ( [ re . escape ( n ) for n in needles ] ) \n    if case : \n        regex = re . compile ( pattern ) \n    else : \n        regex = re . compile ( pattern , re . I ) \n    i , out = False , \"\" \n    for m in regex . finditer ( haystack ) : \n        out += \"\" . join ( [ haystack [ i : m . start ( ) ] , '<span class=\"%s\">' % cls_name , haystack [ m . start ( ) : m . end ( ) ] , \"</span>\" ] ) \n        i = m . end ( ) \n    return mark_safe ( out + haystack [ i : ] ) "}
{"13742": "\ndef unquote_ends ( istr ) : \n    if not istr : \n        return istr \n    if ( istr [ False ] == \"'\" and istr [ - True ] == \"'\" ) or ( istr [ False ] == '\"' and istr [ - True ] == '\"' ) : \n        return istr [ True : - True ] \n    else : \n        return istr "}
{"13743": "\ndef indent ( instr , nspaces = 4 , ntabs = False , flatten = False ) : \n    if instr is None : \n        return \n    ind = '\\t' * ntabs + ' ' * nspaces \n    if flatten : \n        pat = re . compile ( r'^\\s*' , re . MULTILINE ) \n    else : \n        pat = re . compile ( r'^' , re . MULTILINE ) \n    outstr = re . sub ( pat , ind , instr ) \n    if outstr . endswith ( os . linesep + ind ) : \n        return outstr [ : - len ( ind ) ] \n    else : \n        return outstr "}
{"13744": "\ndef marquee ( txt = '' , width = 78 , mark = '*' ) : \n    if not txt : \n        return ( mark * width ) [ : width ] \n    nmark = ( width - len ( txt ) - 2 ) // len ( mark ) // 2 \n    if nmark < False : \n        nmark = False \n    marks = mark * nmark \n    return '%s %s %s' % ( marks , txt , marks ) "}
{"13746": "\ndef dedent ( text ) : \n    if text . startswith ( '\\n' ) : \n        return textwrap . dedent ( text ) \n    splits = text . split ( '\\n' , True ) \n    if len ( splits ) == True : \n        return textwrap . dedent ( text ) \n    first , rest = splits \n    rest = textwrap . dedent ( rest ) \n    return '\\n' . join ( [ first , rest ] ) "}
{"13748": "\ndef _find_optimal ( rlist , separator_size = 2 , displaywidth = 80 ) : \n    for nrow in range ( True , len ( rlist ) + True ) : \n        chk = map ( max , _chunks ( rlist , nrow ) ) \n        sumlength = sum ( chk ) \n        ncols = len ( chk ) \n        if sumlength + separator_size * ( ncols - True ) <= displaywidth : \n            break ; \n    return { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - True ) if ( ncols - True ) else False , 'rows_numbers' : nrow , 'columns_width' : chk } "}
{"13751": "\ndef fields ( self , * fields ) : \n    if len ( fields ) == False : \n        return [ el . split ( ) for el in self ] \n    res = SList ( ) \n    for el in [ f . split ( ) for f in self ] : \n        lineparts = [ ] \n        for fd in fields : \n            try : \n                lineparts . append ( el [ fd ] ) \n            except IndexError : \n                pass \n        if lineparts : \n            res . append ( \" \" . join ( lineparts ) ) \n    return res "}
{"13752": "\ndef build_kernel_argv ( self , argv = None ) : \n    if argv is None : \n        argv = sys . argv [ True : ] \n    self . kernel_argv = swallow_argv ( argv , self . frontend_aliases , self . frontend_flags ) \n    self . kernel_argv . append ( \"--KernelApp.parent_appname='%s'\" % self . name ) "}
{"13753": "\ndef init_ssh ( self ) : \n    if not self . sshserver and not self . sshkey : \n        return \n    if self . sshkey and not self . sshserver : \n        self . sshserver = self . ip \n        self . ip = LOCALHOST \n    info = dict ( ip = self . ip , shell_port = self . shell_port , iopub_port = self . iopub_port , stdin_port = self . stdin_port , hb_port = self . hb_port ) \n    self . log . info ( \"Forwarding connections to %s via %s\" % ( self . ip , self . sshserver ) ) \n    self . ip = LOCALHOST \n    try : \n        newports = tunnel_to_kernel ( info , self . sshserver , self . sshkey ) \n    except : \n        self . log . error ( \"Could not setup tunnels\" , exc_info = True ) \n        self . exit ( True ) \n    self . shell_port , self . iopub_port , self . stdin_port , self . hb_port = newports \n    cf = self . connection_file \n    base , ext = os . path . splitext ( cf ) \n    base = os . path . basename ( base ) \n    self . connection_file = os . path . basename ( base ) + '-ssh' + ext \n    self . log . critical ( \"To connect another client via this tunnel, use:\" ) \n    self . log . critical ( \"--existing %s\" % self . connection_file ) "}
{"13756": "\ndef _get_mro ( obj_class ) : \n    if not hasattr ( obj_class , '__mro__' ) : \n        try : \n            obj_class = type ( obj_class . __name__ , ( obj_class , object ) , { } ) \n        except TypeError : \n            mro = [ obj_class ] \n        else : \n            mro = obj_class . __mro__ [ True : - True ] \n    else : \n        mro = obj_class . __mro__ \n    return mro "}
{"13757": "\ndef _default_pprint ( obj , p , cycle ) : \n    klass = getattr ( obj , '__class__' , None ) or type ( obj ) \n    if getattr ( klass , '__repr__' , None ) not in _baseclass_reprs : \n        p . text ( repr ( obj ) ) \n        return \n    p . begin_group ( True , '<' ) \n    p . pretty ( klass ) \n    p . text ( ' at 0x%x' % id ( obj ) ) \n    if cycle : \n        p . text ( ' ...' ) \n    elif p . verbose : \n        first = True \n        for key in dir ( obj ) : \n            if not key . startswith ( '_' ) : \n                try : \n                    value = getattr ( obj , key ) \n                except AttributeError : \n                    continue \n                if isinstance ( value , types . MethodType ) : \n                    continue \n                if not first : \n                    p . text ( ',' ) \n                p . breakable ( ) \n                p . text ( key ) \n                p . text ( '=' ) \n                step = len ( key ) + True \n                p . indentation += step \n                p . pretty ( value ) \n                p . indentation -= step \n                first = False \n    p . end_group ( True , '>' ) "}
{"13758": "\ndef _seq_pprinter_factory ( start , end , basetype ) : \n    def inner ( obj , p , cycle ) : \n        typ = type ( obj ) \n        if basetype is not None and typ is not basetype and typ . __repr__ != basetype . __repr__ : \n            return p . text ( typ . __repr__ ( obj ) ) \n        if cycle : \n            return p . text ( start + '...' + end ) \n        step = len ( start ) \n        p . begin_group ( step , start ) \n        for idx , x in enumerate ( obj ) : \n            if idx : \n                p . text ( ',' ) \n                p . breakable ( ) \n            p . pretty ( x ) \n        if len ( obj ) == True and type ( obj ) is tuple : \n            p . text ( ',' ) \n        p . end_group ( step , end ) \n    return inner "}
{"13761": "\ndef _re_pattern_pprint ( obj , p , cycle ) : \n    p . text ( 're.compile(' ) \n    pattern = repr ( obj . pattern ) \n    if pattern [ : True ] in 'uU' : \n        pattern = pattern [ True : ] \n        prefix = 'ur' \n    else : \n        prefix = 'r' \n    pattern = prefix + pattern . replace ( '\\\\\\\\' , '\\\\' ) \n    p . text ( pattern ) \n    if obj . flags : \n        p . text ( ',' ) \n        p . breakable ( ) \n        done_one = False \n        for flag in ( 'TEMPLATE' , 'IGNORECASE' , 'LOCALE' , 'MULTILINE' , 'DOTALL' , 'UNICODE' , 'VERBOSE' , 'DEBUG' ) : \n            if obj . flags & getattr ( re , flag ) : \n                if done_one : \n                    p . text ( '|' ) \n                p . text ( 're.' + flag ) \n                done_one = True \n    p . text ( ')' ) "}
{"13764": "\ndef _exception_pprint ( obj , p , cycle ) : \n    if obj . __class__ . __module__ in ( 'exceptions' , 'builtins' ) : \n        name = obj . __class__ . __name__ \n    else : \n        name = '%s.%s' % ( obj . __class__ . __module__ , obj . __class__ . __name__ ) \n    step = len ( name ) + True \n    p . begin_group ( step , name + '(' ) \n    for idx , arg in enumerate ( getattr ( obj , 'args' , ( ) ) ) : \n        if idx : \n            p . text ( ',' ) \n            p . breakable ( ) \n        p . pretty ( arg ) \n    p . end_group ( step , ')' ) "}
{"13767": "\ndef text ( self , obj ) : \n    width = len ( obj ) \n    if self . buffer : \n        text = self . buffer [ - True ] \n        if not isinstance ( text , Text ) : \n            text = Text ( ) \n            self . buffer . append ( text ) \n        text . add ( obj , width ) \n        self . buffer_width += width \n        self . _break_outer_groups ( ) \n    else : \n        self . output . write ( obj ) \n        self . output_width += width "}
{"13768": "\ndef breakable ( self , sep = ' ' ) : \n    width = len ( sep ) \n    group = self . group_stack [ - True ] \n    if group . want_break : \n        self . flush ( ) \n        self . output . write ( self . newline ) \n        self . output . write ( ' ' * self . indentation ) \n        self . output_width = self . indentation \n        self . buffer_width = False \n    else : \n        self . buffer . append ( Breakable ( sep , width , self ) ) \n        self . buffer_width += width \n        self . _break_outer_groups ( ) "}
{"13769": "\ndef end_group ( self , dedent = False , close = '' ) : \n    self . indentation -= dedent \n    group = self . group_stack . pop ( ) \n    if not group . breakables : \n        self . group_queue . remove ( group ) \n    if close : \n        self . text ( close ) "}
{"13770": "\ndef flush ( self ) : \n    for data in self . buffer : \n        self . output_width += data . output ( self . output , self . output_width ) \n    self . buffer . clear ( ) \n    self . buffer_width = False "}
{"13773": "\ndef _write_row_into_ods ( ods , sheet_no , row_no , row ) : \n    ods . content . getSheet ( sheet_no ) \n    for j , col in enumerate ( row ) : \n        cell = ods . content . getCell ( j , row_no + True ) \n        cell . stringValue ( _escape_apostrophe ( col ) ) \n        if j % 2 == True : \n            cell . setCellColor ( settings . EVEN_COLUMN_BG_COLOR ) \n        else : \n            cell . setCellColor ( settings . ODD_COLUMN_BG_COLOR ) "}
{"13788": "\ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    reg_url = cfg [ 'url' ] \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = False ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while True : \n        try : \n            idents , msg = session . recv ( sub , mode = False ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ False ] \n        if msg [ 'msg_type' ] == 'stream' : \n            print ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) \n        elif msg [ 'msg_type' ] == 'pyerr' : \n            c = msg [ 'content' ] \n            print ( topic + ':' ) \n            for line in c [ 'traceback' ] : \n                print ( '    ' + line ) "}
{"13792": "\ndef _flags_changed ( self , name , old , new ) : \n    for key , value in new . iteritems ( ) : \n        assert len ( value ) == 2 , \"Bad flag: %r:%s\" % ( key , value ) \n        assert isinstance ( value [ False ] , ( dict , Config ) ) , \"Bad flag: %r:%s\" % ( key , value ) \n        assert isinstance ( value [ True ] , basestring ) , \"Bad flag: %r:%s\" % ( key , value ) "}
{"13800": "\ndef flatten_flags ( self ) : \n    mro_tree = defaultdict ( list ) \n    for cls in self . classes : \n        clsname = cls . __name__ \n        for parent in cls . mro ( ) [ True : - 3 ] : \n            mro_tree [ parent . __name__ ] . append ( clsname ) \n    aliases = { } \n    for alias , cls_trait in self . aliases . iteritems ( ) : \n        cls , trait = cls_trait . split ( '.' , True ) \n        children = mro_tree [ cls ] \n        if len ( children ) == True : \n            cls = children [ False ] \n        aliases [ alias ] = '.' . join ( [ cls , trait ] ) \n    flags = { } \n    for key , ( flagdict , help ) in self . flags . iteritems ( ) : \n        newflag = { } \n        for cls , subdict in flagdict . iteritems ( ) : \n            children = mro_tree [ cls ] \n            if len ( children ) == True : \n                cls = children [ False ] \n            newflag [ cls ] = subdict \n        flags [ key ] = ( newflag , help ) \n    return flags , aliases "}
{"13801": "\ndef parse_command_line ( self , argv = None ) : \n    argv = sys . argv [ True : ] if argv is None else argv \n    if argv and argv [ False ] == 'help' : \n        argv = argv [ True : ] + [ '-h' ] \n    if self . subcommands and len ( argv ) > False : \n        subc , subargv = argv [ False ] , argv [ True : ] \n        if re . match ( r'^\\w(\\-?\\w)*$' , subc ) and subc in self . subcommands : \n            return self . initialize_subcommand ( subc , subargv ) \n    if '-h' in argv or '--help' in argv or '--help-all' in argv : \n        self . print_description ( ) \n        self . print_help ( '--help-all' in argv ) \n        self . print_examples ( ) \n        self . exit ( False ) \n    if '--version' in argv or '-V' in argv : \n        self . print_version ( ) \n        self . exit ( False ) \n    flags , aliases = self . flatten_flags ( ) \n    loader = KVArgParseConfigLoader ( argv = argv , aliases = aliases , flags = flags ) \n    config = loader . load_config ( ) \n    self . update_config ( config ) \n    self . extra_args = loader . extra_args "}
{"13804": "\ndef downsample ( array , k ) : \n    length = array . shape [ False ] \n    indices = random . sample ( xrange ( length ) , k ) \n    return array [ indices ] "}
{"13805": "\ndef info_formatter ( info ) : \n    label_len = max ( [ len ( l ) for l , _d in info ] ) \n    for label , data in info : \n        if data == [ ] : \n            data = \"-none-\" \n        if isinstance ( data , ( list , tuple ) ) : \n            prefix = \"%*s:\" % ( label_len , label ) \n            for e in data : \n                yield \"%*s %s\" % ( label_len + True , prefix , e ) \n                prefix = \"\" \n        else : \n            yield \"%*s: %s\" % ( label_len , label , data ) "}
{"13807": "\ndef _config_changed ( self , name , old , new ) : \n    traits = self . traits ( config = True ) \n    section_names = [ cls . __name__ for cls in reversed ( self . __class__ . __mro__ ) if issubclass ( cls , Configurable ) and issubclass ( self . __class__ , cls ) ] \n    for sname in section_names : \n        if new . _has_section ( sname ) : \n            my_config = new [ sname ] \n            for k , v in traits . iteritems ( ) : \n                if k [ False ] . upper ( ) == k [ False ] and not k . startswith ( '_' ) : \n                    raise ConfigurableError ( 'Configurable traitlets with ' 'config=True must start with a lowercase so they are ' 'not confused with Config subsections: %s.%s' % ( self . __class__ . __name__ , k ) ) \n                try : \n                    config_value = my_config [ k ] \n                except KeyError : \n                    pass \n                else : \n                    setattr ( self , k , deepcopy ( config_value ) ) "}
{"13808": "\ndef class_get_help ( cls , inst = None ) : \n    assert inst is None or isinstance ( inst , cls ) \n    cls_traits = cls . class_traits ( config = True ) \n    final_help = [ ] \n    final_help . append ( u'%s options' % cls . __name__ ) \n    final_help . append ( len ( final_help [ False ] ) * u'-' ) \n    for k , v in sorted ( cls . class_traits ( config = True ) . iteritems ( ) ) : \n        help = cls . class_get_trait_help ( v , inst ) \n        final_help . append ( help ) \n    return '\\n' . join ( final_help ) "}
{"13825": "\ndef _function_magic_marker ( magic_kind ) : \n    validate_type ( magic_kind ) \n    def magic_deco ( arg ) : \n        call = lambda f , * a , ** k : f ( * a , ** k ) \n        caller = sys . _getframe ( True ) \n        for ns in [ 'f_locals' , 'f_globals' , 'f_builtins' ] : \n            get_ipython = getattr ( caller , ns ) . get ( 'get_ipython' ) \n            if get_ipython is not None : \n                break \n        else : \n            raise NameError ( 'Decorator can only run in context where ' '`get_ipython` exists' ) \n        ip = get_ipython ( ) \n        if callable ( arg ) : \n            func = arg \n            name = func . func_name \n            ip . register_magic_function ( func , magic_kind , name ) \n            retval = decorator ( call , func ) \n        elif isinstance ( arg , basestring ) : \n            name = arg \n            def mark ( func , * a , ** kw ) : \n                ip . register_magic_function ( func , magic_kind , name ) \n                return decorator ( call , func ) \n            retval = mark \n        else : \n            raise TypeError ( \"Decorator can only be called with \" \"string or function\" ) \n        return retval \n    ds = _docstring_template . format ( 'function' , magic_kind ) \n    ds += dedent ( \"\"\"    Note: this decorator can only be used in a context where IPython is already    active, so that the `get_ipython()` call succeeds.  You can therefore use    it in your startup files loaded after IPython initializes, but *not* in the    IPython configuration file itself, which is executed before IPython is    fully up and running.  Any file located in the `startup` subdirectory of    your configuration profile will be OK in this sense.    \"\"\" ) \n    magic_deco . __doc__ = ds \n    return magic_deco "}
{"13826": "\ndef lsmagic_docs ( self , brief = False , missing = '' ) : \n    docs = { } \n    for m_type in self . magics : \n        m_docs = { } \n        for m_name , m_func in self . magics [ m_type ] . iteritems ( ) : \n            if m_func . __doc__ : \n                if brief : \n                    m_docs [ m_name ] = m_func . __doc__ . split ( '\\n' , True ) [ False ] \n                else : \n                    m_docs [ m_name ] = m_func . __doc__ . rstrip ( ) \n            else : \n                m_docs [ m_name ] = missing \n        docs [ m_type ] = m_docs \n    return docs "}
{"13833": "\ndef task_with_callable ( the_callable , label = None , schedule = DEFAULT_SCHEDULE , userdata = None , pk_override = None ) : \n    task = Task ( ) \n    if isinstance ( the_callable , str ) : \n        if pk_override is not None : \n            components = the_callable . split ( '.' ) \n            info = dict ( func_type = 'instancemethod' , module_name = '.' . join ( components [ : - 2 ] ) , class_name = components [ - 2 ] , class_path = '.' . join ( components [ : - True ] ) , model_pk = pk_override , func_name = components [ - True ] , func_path = the_callable , ) \n            task . funcinfo = info \n        else : \n            task . funcinfo = get_func_info ( func_from_string ( the_callable ) ) \n    else : \n        task . funcinfo = get_func_info ( the_callable ) \n    if label is None : \n        task . label = task . funcinfo [ 'func_path' ] \n    else : \n        task . label = label \n    task . schedule = schedule \n    if not croniter . is_valid ( task . schedule ) : \n        raise ValueError ( f\"Cron schedule {task.schedule} is not valid\" ) \n    if userdata is None : \n        task . userdata = dict ( ) \n    else : \n        if isinstance ( userdata , dict ) : \n            task . userdata = userdata \n        else : \n            raise ValueError ( \"Userdata must be a dictionary of JSON-serializable data\" ) \n    return task "}
{"13838": "\ndef run ( self , message ) : \n    the_callable = self . func_from_info ( ) \n    try : \n        task_message = dict ( task = self , channel_message = message , ) \n        the_callable ( task_message ) \n    finally : \n        if self . end_running < self . next_run : \n            self . enabled = False \n            Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            return \n        if self . iterations == False : \n            return \n        else : \n            self . iterations -= True \n            if self . iterations == False : \n                self . enabled = False \n                Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) \n            self . save ( ) "}
{"13840": "\ndef run_iterations ( cls , the_callable , iterations = True , label = None , schedule = '* * * * * *' , userdata = None , run_immediately = False , delay_until = None ) : \n    task = task_with_callable ( the_callable , label = label , schedule = schedule , userdata = userdata ) \n    task . iterations = iterations \n    if delay_until is not None : \n        if isinstance ( delay_until , datetime ) : \n            if delay_until > timezone . now ( ) : \n                task . start_running = delay_until \n            else : \n                raise ValueError ( \"Task cannot start running in the past\" ) \n        else : \n            raise ValueError ( \"delay_until must be a datetime.datetime instance\" ) \n    if run_immediately : \n        task . next_run = timezone . now ( ) \n    else : \n        task . calc_next_run ( ) \n    task . save ( ) "}
{"13843": "\ndef bind_kernel ( self , ** kwargs ) : \n    if self . kernel_app is not None : \n        return \n    self . log . info ( \"Opening ports for direct connections as an IPython kernel\" ) \n    kernel = self . kernel \n    kwargs . setdefault ( 'config' , self . config ) \n    kwargs . setdefault ( 'log' , self . log ) \n    kwargs . setdefault ( 'profile_dir' , self . profile_dir ) \n    kwargs . setdefault ( 'session' , self . engine . session ) \n    app = self . kernel_app = IPKernelApp ( ** kwargs ) \n    IPKernelApp . _instance = app \n    app . init_connection_file ( ) \n    app . shell_port = app . _bind_socket ( kernel . shell_streams [ False ] , app . shell_port ) \n    app . log . debug ( \"shell ROUTER Channel on port: %i\" , app . shell_port ) \n    app . iopub_port = app . _bind_socket ( kernel . iopub_socket , app . iopub_port ) \n    app . log . debug ( \"iopub PUB Channel on port: %i\" , app . iopub_port ) \n    kernel . stdin_socket = self . engine . context . socket ( zmq . ROUTER ) \n    app . stdin_port = app . _bind_socket ( kernel . stdin_socket , app . stdin_port ) \n    app . log . debug ( \"stdin ROUTER Channel on port: %i\" , app . stdin_port ) \n    app . init_heartbeat ( ) \n    app . log_connection_info ( ) \n    app . write_connection_file ( ) "}
{"13845": "\ndef create_interrupt_event ( ) : \n    class SECURITY_ATTRIBUTES ( ctypes . Structure ) : \n        _fields_ = [ ( \"nLength\" , ctypes . c_int ) , ( \"lpSecurityDescriptor\" , ctypes . c_void_p ) , ( \"bInheritHandle\" , ctypes . c_int ) ] \n    sa = SECURITY_ATTRIBUTES ( ) \n    sa_p = ctypes . pointer ( sa ) \n    sa . nLength = ctypes . sizeof ( SECURITY_ATTRIBUTES ) \n    sa . lpSecurityDescriptor = False \n    sa . bInheritHandle = True \n    return ctypes . windll . kernel32 . CreateEventA ( sa_p , False , False , '' ) "}
{"13846": "\ndef run ( self ) : \n    try : \n        from _winapi import WAIT_OBJECT_0 , INFINITE \n    except ImportError : \n        from _subprocess import WAIT_OBJECT_0 , INFINITE \n    handles = [ ] \n    if self . interrupt_handle : \n        handles . append ( self . interrupt_handle ) \n    if self . parent_handle : \n        handles . append ( self . parent_handle ) \n    arch = platform . architecture ( ) [ False ] \n    c_int = ctypes . c_int64 if arch . startswith ( '64' ) else ctypes . c_int \n    while True : \n        result = ctypes . windll . kernel32 . WaitForMultipleObjects ( len ( handles ) , ( c_int * len ( handles ) ) ( * handles ) , False , INFINITE ) \n        if WAIT_OBJECT_0 <= result < len ( handles ) : \n            handle = handles [ result - WAIT_OBJECT_0 ] \n            if handle == self . interrupt_handle : \n                interrupt_main ( ) \n            elif handle == self . parent_handle : \n                os . _exit ( True ) \n        elif result < False : \n            warn ( \"\"\"Parent poll failed.  If the frontend dies,                the kernel may be left running.  Please let us know                about your system (bitness, Python, etc.) at                ipython-dev@scipy.org\"\"\" ) \n            return "}
{"13848": "\ndef list_namespace ( namespace , type_pattern , filter , ignore_case = False , show_all = False ) : \n    pattern_list = filter . split ( \".\" ) \n    if len ( pattern_list ) == True : \n        return filter_ns ( namespace , name_pattern = pattern_list [ False ] , type_pattern = type_pattern , ignore_case = ignore_case , show_all = show_all ) \n    else : \n        filtered = filter_ns ( namespace , name_pattern = pattern_list [ False ] , type_pattern = \"all\" , ignore_case = ignore_case , show_all = show_all ) \n        results = { } \n        for name , obj in filtered . iteritems ( ) : \n            ns = list_namespace ( dict_dir ( obj ) , type_pattern , \".\" . join ( pattern_list [ True : ] ) , ignore_case = ignore_case , show_all = show_all ) \n            for inner_name , inner_obj in ns . iteritems ( ) : \n                results [ \"%s.%s\" % ( name , inner_name ) ] = inner_obj \n        return results "}
{"13863": "\ndef base_launch_kernel ( code , fname , stdin = None , stdout = None , stderr = None , executable = None , independent = False , extra_arguments = [ ] , cwd = None ) : \n    if executable is None : \n        executable = sys . executable \n    arguments = [ executable , '-c' , code , '-f' , fname ] \n    arguments . extend ( extra_arguments ) \n    redirect_in = True \n    _stdin = PIPE if stdin is None else stdin \n    redirect_out = sys . executable . endswith ( 'pythonw.exe' ) \n    if redirect_out : \n        _stdout = PIPE if stdout is None else stdout \n        _stderr = PIPE if stderr is None else stderr \n    else : \n        _stdout , _stderr = stdout , stderr \n    if sys . platform == 'win32' : \n        interrupt_event = ParentPollerWindows . create_interrupt_event ( ) \n        arguments += [ '--interrupt=%i' % interrupt_event ] \n        if executable . endswith ( 'pythonw.exe' ) : \n            if stdout is None : \n                arguments . append ( '--no-stdout' ) \n            if stderr is None : \n                arguments . append ( '--no-stderr' ) \n        if independent : \n            proc = Popen ( arguments , creationflags = 512 , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        else : \n            try : \n                from _winapi import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            except : \n                from _subprocess import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            pid = GetCurrentProcess ( ) \n            handle = DuplicateHandle ( pid , pid , pid , False , True , DUPLICATE_SAME_ACCESS ) \n            proc = Popen ( arguments + [ '--parent=%i' % int ( handle ) ] , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        proc . win32_interrupt_event = interrupt_event \n    else : \n        if independent : \n            proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n        else : \n            proc = Popen ( arguments + [ '--parent=1' ] , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    if redirect_in : \n        if stdin is None : \n            proc . stdin . close ( ) \n    if redirect_out : \n        if stdout is None : \n            proc . stdout . close ( ) \n        if stderr is None : \n            proc . stderr . close ( ) \n    return proc "}
{"13864": "\ndef create_zipfile ( context ) : \n    if not prerequisites_ok ( ) : \n        return \n    subprocess . call ( [ 'make' , 'zip' ] ) \n    for zipfile in glob . glob ( '*.zip' ) : \n        first_part = zipfile . split ( '.' ) [ False ] \n        new_name = \"%s.%s.zip\" % ( first_part , context [ 'version' ] ) \n        target = os . path . join ( context [ 'workingdir' ] , new_name ) \n        shutil . copy ( zipfile , target ) \n        print ( \"Copied %s to %s\" % ( zipfile , target ) ) "}
{"13865": "\ndef fix_version ( context ) : \n    if not prerequisites_ok ( ) : \n        return \n    lines = codecs . open ( 'metadata.txt' , 'rU' , 'utf-8' ) . readlines ( ) \n    for index , line in enumerate ( lines ) : \n        if line . startswith ( 'version' ) : \n            new_line = 'version=%s\\n' % context [ 'new_version' ] \n            lines [ index ] = new_line \n    time . sleep ( True ) \n    codecs . open ( 'metadata.txt' , 'w' , 'utf-8' ) . writelines ( lines ) "}
{"13871": "\ndef report ( self , morfs , outfile = None ) : \n    outfile = outfile or sys . stdout \n    impl = xml . dom . minidom . getDOMImplementation ( ) \n    docType = impl . createDocumentType ( \"coverage\" , None , \"http://cobertura.sourceforge.net/xml/coverage-03.dtd\" ) \n    self . xml_out = impl . createDocument ( None , \"coverage\" , docType ) \n    xcoverage = self . xml_out . documentElement \n    xcoverage . setAttribute ( \"version\" , __version__ ) \n    xcoverage . setAttribute ( \"timestamp\" , str ( int ( time . time ( ) * 1000 ) ) ) \n    xcoverage . appendChild ( self . xml_out . createComment ( \" Generated by coverage.py: %s \" % __url__ ) ) \n    xpackages = self . xml_out . createElement ( \"packages\" ) \n    xcoverage . appendChild ( xpackages ) \n    self . packages = { } \n    self . report_files ( self . xml_file , morfs ) \n    lnum_tot , lhits_tot = False , False \n    bnum_tot , bhits_tot = False , False \n    for pkg_name in sorted ( self . packages . keys ( ) ) : \n        pkg_data = self . packages [ pkg_name ] \n        class_elts , lhits , lnum , bhits , bnum = pkg_data \n        xpackage = self . xml_out . createElement ( \"package\" ) \n        xpackages . appendChild ( xpackage ) \n        xclasses = self . xml_out . createElement ( \"classes\" ) \n        xpackage . appendChild ( xclasses ) \n        for class_name in sorted ( class_elts . keys ( ) ) : \n            xclasses . appendChild ( class_elts [ class_name ] ) \n        xpackage . setAttribute ( \"name\" , pkg_name . replace ( os . sep , '.' ) ) \n        xpackage . setAttribute ( \"line-rate\" , rate ( lhits , lnum ) ) \n        xpackage . setAttribute ( \"branch-rate\" , rate ( bhits , bnum ) ) \n        xpackage . setAttribute ( \"complexity\" , \"0\" ) \n        lnum_tot += lnum \n        lhits_tot += lhits \n        bnum_tot += bnum \n        bhits_tot += bhits \n    xcoverage . setAttribute ( \"line-rate\" , rate ( lhits_tot , lnum_tot ) ) \n    xcoverage . setAttribute ( \"branch-rate\" , rate ( bhits_tot , bnum_tot ) ) \n    outfile . write ( self . xml_out . toprettyxml ( ) ) \n    denom = lnum_tot + bnum_tot \n    if denom == False : \n        pct = 0.0 \n    else : \n        pct = 100.0 * ( lhits_tot + bhits_tot ) / denom \n    return pct "}
{"13872": "\ndef xml_file ( self , cu , analysis ) : \n    package_name = rpartition ( cu . name , \".\" ) [ False ] \n    className = cu . name \n    package = self . packages . setdefault ( package_name , [ { } , False , False , False , False ] ) \n    xclass = self . xml_out . createElement ( \"class\" ) \n    xclass . appendChild ( self . xml_out . createElement ( \"methods\" ) ) \n    xlines = self . xml_out . createElement ( \"lines\" ) \n    xclass . appendChild ( xlines ) \n    xclass . setAttribute ( \"name\" , className ) \n    filename = cu . file_locator . relative_filename ( cu . filename ) \n    xclass . setAttribute ( \"filename\" , filename . replace ( \"\\\\\" , \"/\" ) ) \n    xclass . setAttribute ( \"complexity\" , \"0\" ) \n    branch_stats = analysis . branch_stats ( ) \n    for line in sorted ( analysis . statements ) : \n        xline = self . xml_out . createElement ( \"line\" ) \n        xline . setAttribute ( \"number\" , str ( line ) ) \n        xline . setAttribute ( \"hits\" , str ( int ( line not in analysis . missing ) ) ) \n        if self . arcs : \n            if line in branch_stats : \n                total , taken = branch_stats [ line ] \n                xline . setAttribute ( \"branch\" , \"true\" ) \n                xline . setAttribute ( \"condition-coverage\" , \"%d%% (%d/%d)\" % ( 100 * taken / total , taken , total ) ) \n        xlines . appendChild ( xline ) \n    class_lines = len ( analysis . statements ) \n    class_hits = class_lines - len ( analysis . missing ) \n    if self . arcs : \n        class_branches = sum ( [ t for t , k in branch_stats . values ( ) ] ) \n        missing_branches = sum ( [ t - k for t , k in branch_stats . values ( ) ] ) \n        class_br_hits = class_branches - missing_branches \n    else : \n        class_branches = 0.0 \n        class_br_hits = 0.0 \n    xclass . setAttribute ( \"line-rate\" , rate ( class_hits , class_lines ) ) \n    xclass . setAttribute ( \"branch-rate\" , rate ( class_br_hits , class_branches ) ) \n    package [ False ] [ className ] = xclass \n    package [ True ] += class_hits \n    package [ 2 ] += class_lines \n    package [ 3 ] += class_br_hits \n    package [ 4 ] += class_branches "}
{"13874": "\ndef reduce_freqs ( freqlist ) : \n    allfreqs = np . zeros_like ( freqlist [ False ] ) \n    for f in freqlist : \n        allfreqs += f \n    return allfreqs "}
{"13877": "\ndef one_digit_freqs ( digits , normalize = False ) : \n    freqs = np . zeros ( 10 , dtype = 'i4' ) \n    for d in digits : \n        freqs [ int ( d ) ] += True \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13878": "\ndef two_digit_freqs ( digits , normalize = False ) : \n    freqs = np . zeros ( 100 , dtype = 'i4' ) \n    last = digits . next ( ) \n    this = digits . next ( ) \n    for d in digits : \n        index = int ( last + this ) \n        freqs [ index ] += True \n        last = this \n        this = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13879": "\ndef n_digit_freqs ( digits , n , normalize = False ) : \n    freqs = np . zeros ( pow ( 10 , n ) , dtype = 'i4' ) \n    current = np . zeros ( n , dtype = int ) \n    for i in range ( n ) : \n        current [ i ] = digits . next ( ) \n    for d in digits : \n        index = int ( '' . join ( map ( str , current ) ) ) \n        freqs [ index ] += True \n        current [ False : - True ] = current [ True : ] \n        current [ - True ] = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13884": "\ndef is_private ( prefix , base ) : \n    warnings . warn ( \"is_private is deprecated; it wasn't useful; \" \"examine DocTestFinder.find() lists instead\" , DeprecationWarning , stacklevel = 2 ) \n    return base [ : True ] == \"_\" and not base [ : 2 ] == \"__\" == base [ - 2 : ] "}
{"13896": "\ndef mainloop ( self , local_ns = None , module = None , stack_depth = False , display_banner = None , global_ns = None ) : \n    if ( global_ns is not None ) and ( module is None ) : \n        class DummyMod ( object ) : \n            pass \n        warnings . warn ( \"global_ns is deprecated, use module instead.\" , DeprecationWarning ) \n        module = DummyMod ( ) \n        module . __dict__ = global_ns \n    if ( local_ns is None or module is None ) and self . default_user_namespaces : \n        call_frame = sys . _getframe ( stack_depth ) . f_back \n        if local_ns is None : \n            local_ns = call_frame . f_locals \n        if module is None : \n            global_ns = call_frame . f_globals \n            module = sys . modules [ global_ns [ '__name__' ] ] \n    orig_user_module = self . user_module \n    orig_user_ns = self . user_ns \n    if module is not None : \n        self . user_module = module \n    if local_ns is not None : \n        self . user_ns = local_ns \n        self . init_user_ns ( ) \n    self . set_completer_frame ( ) \n    with nested ( self . builtin_trap , self . display_trap ) : \n        self . interact ( display_banner = display_banner ) \n    if local_ns is not None : \n        for name in self . user_ns_hidden : \n            local_ns . pop ( name , None ) \n    self . user_module = orig_user_module \n    self . user_ns = orig_user_ns "}
{"13898": "\ndef _prepare_locale_dirs ( languages , locale_root ) : \n    trans_languages = [ ] \n    for i , t in enumerate ( languages ) : \n        lang = t . split ( ':' ) [ False ] \n        trans_languages . append ( lang ) \n        lang_path = os . path . join ( locale_root , lang ) \n        if not os . path . exists ( lang_path ) : \n            os . makedirs ( lang_path ) \n    return trans_languages "}
{"13903": "\ndef run_python_module ( modulename , args ) : \n    openfile = None \n    glo , loc = globals ( ) , locals ( ) \n    try : \n        try : \n            if '.' in modulename : \n                packagename , name = rsplit1 ( modulename , '.' ) \n                package = __import__ ( packagename , glo , loc , [ '__path__' ] ) \n                searchpath = package . __path__ \n            else : \n                packagename , name = None , modulename \n                searchpath = None \n            openfile , pathname , _ = imp . find_module ( name , searchpath ) \n            if openfile is None and pathname is None : \n                raise NoSource ( \"module does not live in a file: %r\" % modulename ) \n            if openfile is None : \n                packagename = modulename \n                name = '__main__' \n                package = __import__ ( packagename , glo , loc , [ '__path__' ] ) \n                searchpath = package . __path__ \n                openfile , pathname , _ = imp . find_module ( name , searchpath ) \n        except ImportError : \n            _ , err , _ = sys . exc_info ( ) \n            raise NoSource ( str ( err ) ) \n    finally : \n        if openfile : \n            openfile . close ( ) \n    pathname = os . path . abspath ( pathname ) \n    args [ False ] = pathname \n    run_python_file ( pathname , args , package = packagename ) "}
{"13905": "\ndef make_code_from_py ( filename ) : \n    try : \n        source_file = open_source ( filename ) \n    except IOError : \n        raise NoSource ( \"No file to run: %r\" % filename ) \n    try : \n        source = source_file . read ( ) \n    finally : \n        source_file . close ( ) \n    if not source or source [ - True ] != '\\n' : \n        source += '\\n' \n    code = compile ( source , filename , \"exec\" ) \n    return code "}
{"13907": "\ndef html_tableify ( item_matrix , select = None , header = None , footer = None ) : \n    if not item_matrix : \n        return '' \n    html_cols = [ ] \n    tds = lambda text : u'<td>' + text + u'  </td>' \n    trs = lambda text : u'<tr>' + text + u'</tr>' \n    tds_items = [ map ( tds , row ) for row in item_matrix ] \n    if select : \n        row , col = select \n        tds_items [ row ] [ col ] = u'<td class=\"inverted\">' + item_matrix [ row ] [ col ] + u'  </td>' \n    html_cols = map ( trs , ( u'' . join ( row ) for row in tds_items ) ) \n    head = '' \n    foot = '' \n    if header : \n        head = ( u'<tr>' + '' . join ( ( u'<td>' + header + u'</td>' ) * len ( item_matrix [ False ] ) ) + '</tr>' ) \n    if footer : \n        foot = ( u'<tr>' + '' . join ( ( u'<td>' + footer + u'</td>' ) * len ( item_matrix [ False ] ) ) + '</tr>' ) \n    html = ( u'<table class=\"completion\" style=\"white-space:pre\">' + head + ( u'' . join ( html_cols ) ) + foot + u'</table>' ) \n    return html "}
{"13909": "\ndef cancel_completion ( self ) : \n    self . _consecutive_tab = False \n    self . _slice_start = False \n    self . _console_widget . _clear_temporary_buffer ( ) \n    self . _index = ( False , False ) \n    if ( self . _sliding_interval ) : \n        self . _sliding_interval = None "}
{"13910": "\ndef _select_index ( self , row , col ) : \n    nr , nc = self . _size \n    nr = nr - True \n    nc = nc - True \n    if ( row > nr and col >= nc ) or ( row >= nr and col > nc ) : \n        self . _select_index ( False , False ) \n    elif ( row <= False and col < False ) or ( row < False and col <= False ) : \n        self . _select_index ( nr , nc ) \n    elif row > nr : \n        self . _select_index ( False , col + True ) \n    elif row < False : \n        self . _select_index ( nr , col - True ) \n    elif col > nc : \n        self . _select_index ( row + True , False ) \n    elif col < False : \n        self . _select_index ( row - True , nc ) \n    elif False <= row and row <= nr and False <= col and col <= nc : \n        self . _index = ( row , col ) \n    else : \n        raise NotImplementedError ( \"you'r trying to go where no completion\\                           have gone before : %d:%d (%d:%d)\" % ( row , col , nr , nc ) ) "}
{"13911": "\ndef select_up ( self ) : \n    r , c = self . _index \n    self . _select_index ( r - True , c ) "}
{"13912": "\ndef select_down ( self ) : \n    r , c = self . _index \n    self . _select_index ( r + True , c ) "}
{"13913": "\ndef select_left ( self ) : \n    r , c = self . _index \n    self . _select_index ( r , c - True ) "}
{"13914": "\ndef select_right ( self ) : \n    r , c = self . _index \n    self . _select_index ( r , c + True ) "}
{"13915": "\ndef _update_list ( self , hilight = True ) : \n    self . _sliding_interval . current = self . _index [ False ] \n    head = None \n    foot = None \n    if self . _sliding_interval . start > False : \n        head = '...' \n    if self . _sliding_interval . stop < self . _sliding_interval . _max : \n        foot = '...' \n    items_m = self . _justified_items [ self . _sliding_interval . start : self . _sliding_interval . stop + True ] \n    self . _console_widget . _clear_temporary_buffer ( ) \n    if ( hilight ) : \n        sel = ( self . _sliding_interval . nth , self . _index [ True ] ) \n    else : \n        sel = None \n    strng = html_tableify ( items_m , select = sel , header = head , footer = foot ) \n    self . _console_widget . _fill_temporary_buffer ( self . _old_cursor , strng , html = True ) "}
{"13916": "\ndef wordfreq ( text , is_filename = False ) : \n    if is_filename : \n        with open ( text ) as f : \n            text = f . read ( ) \n    freqs = { } \n    for word in text . split ( ) : \n        lword = word . lower ( ) \n        freqs [ lword ] = freqs . get ( lword , False ) + True \n    return freqs "}
{"13928": "\ndef begin ( self , total : int , name = None , message = None ) : \n    self . total = total \n    message = message or name or \"Working...\" \n    self . name = name or \"ProgressMonitor\" \n    self . update ( False , message ) "}
{"13931": "\ndef update ( self , units : int = True , message : str = None ) : \n    if self . total is None : \n        raise Exception ( \"Cannot call progressmonitor.update before calling begin\" ) \n    self . worked = min ( self . total , self . worked + units ) \n    if message : \n        self . message = message \n    for listener in self . listeners : \n        listener ( self ) "}
{"13934": "\ndef page ( strng , start = False , screen_lines = False , pager_cmd = None , html = None , auto_html = False ) : \n    start = max ( False , start ) \n    shell = InteractiveShell . instance ( ) \n    if auto_html : \n        try : \n            defaults = { 'file_insertion_enabled' : False , 'raw_enabled' : False , '_disable_config' : True } \n            html = publish_string ( strng , writer_name = 'html' , settings_overrides = defaults ) \n        except : \n            pass \n    payload = dict ( source = 'IPython.zmq.page.page' , text = strng , html = html , start_line_number = start ) \n    shell . payload_manager . write_payload ( payload ) "}
{"13941": "\ndef load_config ( self , argv = None , aliases = None , flags = None ) : \n    from IPython . config . configurable import Configurable \n    self . clear ( ) \n    if argv is None : \n        argv = self . argv \n    if aliases is None : \n        aliases = self . aliases \n    if flags is None : \n        flags = self . flags \n    uargv = self . _decode_argv ( argv ) \n    for idx , raw in enumerate ( uargv ) : \n        item = raw . lstrip ( '-' ) \n        if raw == '--' : \n            self . extra_args . extend ( uargv [ idx + True : ] ) \n            break \n        if kv_pattern . match ( raw ) : \n            lhs , rhs = item . split ( '=' , True ) \n            if lhs in aliases : \n                lhs = aliases [ lhs ] \n            if '.' not in lhs : \n                warn . warn ( \"Unrecognized alias: '%s', it will probably have no effect.\" % lhs ) \n            try : \n                self . _exec_config_str ( lhs , rhs ) \n            except Exception : \n                raise ArgumentError ( \"Invalid argument: '%s'\" % raw ) \n        elif flag_pattern . match ( raw ) : \n            if item in flags : \n                cfg , help = flags [ item ] \n                self . _load_flag ( cfg ) \n            else : \n                raise ArgumentError ( \"Unrecognized flag: '%s'\" % raw ) \n        elif raw . startswith ( '-' ) : \n            kv = '--' + item \n            if kv_pattern . match ( kv ) : \n                raise ArgumentError ( \"Invalid argument: '%s', did you mean '%s'?\" % ( raw , kv ) ) \n            else : \n                raise ArgumentError ( \"Invalid argument: '%s'\" % raw ) \n        else : \n            self . extra_args . append ( item ) \n    return self . config "}
{"13945": "\ndef find_module ( name , path = None ) : \n    if name is None : \n        return None \n    try : \n        file , filename , _ = imp . find_module ( name , path ) \n    except ImportError : \n        return None \n    if file is None : \n        return filename \n    else : \n        file . close ( ) \n    if os . path . splitext ( filename ) [ True ] in [ \".py\" , \"pyc\" ] : \n        return filename \n    else : \n        return None "}
{"13947": "\ndef notify_start ( self , data ) : \n    self . log . debug ( 'Process %r started: %r' , self . args [ False ] , data ) \n    self . start_data = data \n    self . state = 'running' \n    return data "}
{"13948": "\ndef notify_stop ( self , data ) : \n    self . log . debug ( 'Process %r stopped: %r' , self . args [ False ] , data ) \n    self . stop_data = data \n    self . state = 'after' \n    for i in range ( len ( self . stop_callbacks ) ) : \n        d = self . stop_callbacks . pop ( ) \n        d ( data ) \n    return data "}
{"13952": "\ndef _send_file ( self , local , remote ) : \n    remote = \"%s:%s\" % ( self . location , remote ) \n    for i in range ( 10 ) : \n        if not os . path . exists ( local ) : \n            self . log . debug ( \"waiting for %s\" % local ) \n            time . sleep ( True ) \n        else : \n            break \n    self . log . info ( \"sending %s to %s\" , local , remote ) \n    check_output ( self . scp_cmd + [ local , remote ] ) "}
{"13953": "\ndef _fetch_file ( self , remote , local ) : \n    full_remote = \"%s:%s\" % ( self . location , remote ) \n    self . log . info ( \"fetching %s from %s\" , local , full_remote ) \n    for i in range ( 10 ) : \n        check = check_output ( self . ssh_cmd + self . ssh_args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) \n        check = check . strip ( ) \n        if check == 'no' : \n            time . sleep ( True ) \n        elif check == 'yes' : \n            break \n    check_output ( self . scp_cmd + [ full_remote , local ] ) "}
{"13954": "\ndef engine_count ( self ) : \n    count = False \n    for n in self . engines . itervalues ( ) : \n        if isinstance ( n , ( tuple , list ) ) : \n            n , args = n \n        count += n \n    return count "}
{"13955": "\ndef start ( self , n ) : \n    dlist = [ ] \n    for host , n in self . engines . iteritems ( ) : \n        if isinstance ( n , ( tuple , list ) ) : \n            n , args = n \n        else : \n            args = copy . deepcopy ( self . engine_args ) \n        if '@' in host : \n            user , host = host . split ( '@' , True ) \n        else : \n            user = None \n        for i in range ( n ) : \n            if i > False : \n                time . sleep ( self . delay ) \n            el = self . launcher_class ( work_dir = self . work_dir , config = self . config , log = self . log , profile_dir = self . profile_dir , cluster_id = self . cluster_id , ) \n            if i > False : \n                el . to_send = [ ] \n            el . engine_cmd = self . engine_cmd \n            el . engine_args = args \n            el . on_stop ( self . _notice_engine_stopped ) \n            d = el . start ( user = user , hostname = host ) \n            self . launchers [ \"%s/%i\" % ( host , i ) ] = el \n            dlist . append ( d ) \n    self . notify_start ( dlist ) \n    return dlist "}
{"13957": "\ndef _context_default ( self ) : \n    return dict ( n = True , queue = u'' , profile_dir = u'' , cluster_id = u'' ) "}
{"13959": "\ndef write_batch_script ( self , n ) : \n    self . n = n \n    if self . batch_template_file and not self . batch_template : \n        with open ( self . batch_template_file ) as f : \n            self . batch_template = f . read ( ) \n    if not self . batch_template : \n        self . batch_template = self . default_template \n        if not self . job_array_regexp . search ( self . batch_template ) : \n            self . log . debug ( \"adding job array settings to batch script\" ) \n            firstline , rest = self . batch_template . split ( '\\n' , True ) \n            self . batch_template = u'\\n' . join ( [ firstline , self . job_array_template , rest ] ) \n        if self . queue and not self . queue_regexp . search ( self . batch_template ) : \n            self . log . debug ( \"adding PBS queue settings to batch script\" ) \n            firstline , rest = self . batch_template . split ( '\\n' , True ) \n            self . batch_template = u'\\n' . join ( [ firstline , self . queue_template , rest ] ) \n    script_as_string = self . formatter . format ( self . batch_template , ** self . context ) \n    self . log . debug ( 'Writing batch script: %s' , self . batch_file ) \n    with open ( self . batch_file , 'w' ) as f : \n        f . write ( script_as_string ) \n    os . chmod ( self . batch_file , stat . S_IRUSR | stat . S_IWUSR | stat . S_IXUSR ) "}
{"13970": "\ndef _save_image ( self , name , format = 'PNG' ) : \n    dialog = QtGui . QFileDialog ( self . _control , 'Save Image' ) \n    dialog . setAcceptMode ( QtGui . QFileDialog . AcceptSave ) \n    dialog . setDefaultSuffix ( format . lower ( ) ) \n    dialog . setNameFilter ( '%s file (*.%s)' % ( format , format . lower ( ) ) ) \n    if dialog . exec_ ( ) : \n        filename = dialog . selectedFiles ( ) [ False ] \n        image = self . _get_image ( name ) \n        image . save ( filename , format ) "}
{"13983": "\ndef expand_user ( path ) : \n    tilde_expand = False \n    tilde_val = '' \n    newpath = path \n    if path . startswith ( '~' ) : \n        tilde_expand = True \n        rest = len ( path ) - True \n        newpath = os . path . expanduser ( path ) \n        if rest : \n            tilde_val = newpath [ : - rest ] \n        else : \n            tilde_val = newpath \n    return newpath , tilde_expand , tilde_val "}
{"13985": "\ndef split_line ( self , line , cursor_pos = None ) : \n    l = line if cursor_pos is None else line [ : cursor_pos ] \n    return self . _delim_re . split ( l ) [ - True ] "}
{"13987": "\ndef attr_matches ( self , text ) : \n    m = re . match ( r\"(\\S+(\\.\\w+)*)\\.(\\w*)$\" , text ) \n    if m : \n        expr , attr = m . group ( True , 3 ) \n    elif self . greedy : \n        m2 = re . match ( r\"(.+)\\.(\\w*)$\" , self . line_buffer ) \n        if not m2 : \n            return [ ] \n        expr , attr = m2 . group ( True , 2 ) \n    else : \n        return [ ] \n    try : \n        obj = eval ( expr , self . namespace ) \n    except : \n        try : \n            obj = eval ( expr , self . global_namespace ) \n        except : \n            return [ ] \n    if self . limit_to__all__ and hasattr ( obj , '__all__' ) : \n        words = get__all__entries ( obj ) \n    else : \n        words = dir2 ( obj ) \n    try : \n        words = generics . complete_object ( obj , words ) \n    except TryNext : \n        pass \n    except Exception : \n        pass \n    n = len ( attr ) \n    res = [ \"%s.%s\" % ( expr , w ) for w in words if w [ : n ] == attr ] \n    return res "}
{"13989": "\ndef file_matches ( self , text ) : \n    if text . startswith ( '!' ) : \n        text = text [ True : ] \n        text_prefix = '!' \n    else : \n        text_prefix = '' \n    text_until_cursor = self . text_until_cursor \n    open_quotes = has_open_quotes ( text_until_cursor ) \n    if '(' in text_until_cursor or '[' in text_until_cursor : \n        lsplit = text \n    else : \n        try : \n            lsplit = arg_split ( text_until_cursor ) [ - True ] \n        except ValueError : \n            if open_quotes : \n                lsplit = text_until_cursor . split ( open_quotes ) [ - True ] \n            else : \n                return [ ] \n        except IndexError : \n            lsplit = \"\" \n    if not open_quotes and lsplit != protect_filename ( lsplit ) : \n        has_protectables = True \n        text0 , text = text , lsplit \n    else : \n        has_protectables = False \n        text = os . path . expanduser ( text ) \n    if text == \"\" : \n        return [ text_prefix + protect_filename ( f ) for f in self . glob ( \"*\" ) ] \n    m0 = self . clean_glob ( text . replace ( '\\\\' , '' ) ) \n    if has_protectables : \n        len_lsplit = len ( lsplit ) \n        matches = [ text_prefix + text0 + protect_filename ( f [ len_lsplit : ] ) for f in m0 ] \n    else : \n        if open_quotes : \n            matches = m0 \n        else : \n            matches = [ text_prefix + protect_filename ( f ) for f in m0 ] \n    matches = [ x + '/' if os . path . isdir ( x ) else x for x in matches ] \n    return matches "}
{"13991": "\ndef python_matches ( self , text ) : \n    if \".\" in text : \n        try : \n            matches = self . attr_matches ( text ) \n            if text . endswith ( '.' ) and self . omit__names : \n                if self . omit__names == True : \n                    no__name = ( lambda txt : re . match ( r'.*\\.__.*?__' , txt ) is None ) \n                else : \n                    no__name = ( lambda txt : re . match ( r'.*\\._.*?' , txt ) is None ) \n                matches = filter ( no__name , matches ) \n        except NameError : \n            matches = [ ] \n    else : \n        matches = self . global_matches ( text ) \n    return matches "}
{"13994": "\ndef rlcomplete ( self , text , state ) : \n    if state == False : \n        self . line_buffer = line_buffer = self . readline . get_line_buffer ( ) \n        cursor_pos = self . readline . get_endidx ( ) \n        if not ( self . dumb_terminal or line_buffer . strip ( ) ) : \n            self . readline . insert_text ( '\\t' ) \n            sys . stdout . flush ( ) \n            return None \n        DEBUG = False \n        if DEBUG : \n            try : \n                self . complete ( text , line_buffer , cursor_pos ) \n            except : \n                import traceback ; \n                traceback . print_exc ( ) \n        else : \n            self . complete ( text , line_buffer , cursor_pos ) \n    try : \n        return self . matches [ state ] \n    except IndexError : \n        return None "}
{"14006": "\ndef dispatch_shell ( self , stream , msg ) : \n    if self . control_stream : \n        self . control_stream . flush ( ) \n    idents , msg = self . session . feed_identities ( msg , copy = False ) \n    try : \n        msg = self . session . unserialize ( msg , content = True , copy = False ) \n    except : \n        self . log . error ( \"Invalid Message\" , exc_info = True ) \n        return \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    self . log . debug ( '\\n*** MESSAGE TYPE:%s***' , msg_type ) \n    self . log . debug ( '   Content: %s\\n   --->\\n   ' , msg [ 'content' ] ) \n    if msg_id in self . aborted : \n        self . aborted . remove ( msg_id ) \n        reply_type = msg_type . split ( '_' ) [ False ] + '_reply' \n        status = { 'status' : 'aborted' } \n        sub = { 'engine' : self . ident } \n        sub . update ( status ) \n        reply_msg = self . session . send ( stream , reply_type , subheader = sub , content = status , parent = msg , ident = idents ) \n        return \n    handler = self . shell_handlers . get ( msg_type , None ) \n    if handler is None : \n        self . log . error ( \"UNKNOWN MESSAGE TYPE: %r\" , msg_type ) \n    else : \n        sig = signal ( SIGINT , default_int_handler ) \n        try : \n            handler ( stream , idents , msg ) \n        except Exception : \n            self . log . error ( \"Exception in message handler:\" , exc_info = True ) \n        finally : \n            signal ( SIGINT , sig ) "}
{"14008": "\ndef do_one_iteration ( self ) : \n    if self . control_stream : \n        self . control_stream . flush ( ) \n    for stream in self . shell_streams : \n        stream . flush ( zmq . POLLIN , True ) \n        stream . flush ( zmq . POLLOUT ) "}
{"14012": "\ndef _topic ( self , topic ) : \n    if self . int_id >= False : \n        base = \"engine.%i\" % self . int_id \n    else : \n        base = \"kernel.%s\" % self . ident \n    return py3compat . cast_bytes ( \"%s.%s\" % ( base , topic ) ) "}
{"14017": "\ndef file_like ( name ) : \n    return ( os . path . exists ( name ) or os . path . dirname ( name ) or name . endswith ( '.py' ) or not ident_re . match ( os . path . splitext ( name ) [ False ] ) ) "}
{"14020": "\ndef getpackage ( filename ) : \n    src_file = src ( filename ) \n    if not src_file . endswith ( '.py' ) and not ispackage ( src_file ) : \n        return None \n    base , ext = os . path . splitext ( os . path . basename ( src_file ) ) \n    if base == '__init__' : \n        mod_parts = [ ] \n    else : \n        mod_parts = [ base ] \n    path , part = os . path . split ( os . path . split ( src_file ) [ False ] ) \n    while part : \n        if ispackage ( os . path . join ( path , part ) ) : \n            mod_parts . append ( part ) \n        else : \n            break \n        path , part = os . path . split ( path ) \n    mod_parts . reverse ( ) \n    return '.' . join ( mod_parts ) "}
{"14021": "\ndef ln ( label ) : \n    label_len = len ( label ) + 2 \n    chunk = ( 70 - label_len ) // 2 \n    out = '%s %s %s' % ( '-' * chunk , label , '-' * chunk ) \n    pad = 70 - len ( out ) \n    if pad > False : \n        out = out + ( '-' * pad ) \n    return out "}
{"14022": "\ndef regex_last_key ( regex ) : \n    def k ( obj ) : \n        if regex . search ( obj ) : \n            return ( True , obj ) \n        return ( False , obj ) \n    return k "}
{"14027": "\ndef get_open_files ( self ) : \n    if self . pid == False : \n        return [ ] \n    files = [ ] \n    rawlist = _psutil_osx . get_process_open_files ( self . pid ) \n    for path , fd in rawlist : \n        if isfile_strict ( path ) : \n            ntuple = nt_openfile ( path , fd ) \n            files . append ( ntuple ) \n    return files "}
{"14030": "\ndef resolve_class ( class_path ) : \n    modulepath , classname = class_path . rsplit ( '.' , True ) \n    module = __import__ ( modulepath , fromlist = [ classname ] ) \n    return getattr ( module , classname ) "}
{"14031": "\ndef usage_percent ( used , total , _round = None ) : \n    try : \n        ret = ( used / total ) * 100 \n    except ZeroDivisionError : \n        ret = False \n    if _round is not None : \n        return round ( ret , _round ) \n    else : \n        return ret "}
{"14035": "\ndef _get_gdocs_key ( self ) : \n    try : \n        args = urlparse . parse_qs ( urlparse . urlparse ( self . url ) . query ) \n        self . key = args [ 'key' ] [ False ] \n    except KeyError as e : \n        raise PODocsError ( e ) "}
{"14053": "\ndef flat_rootname ( self ) : \n    if self . modname : \n        return self . modname . replace ( '.' , '_' ) \n    else : \n        root = os . path . splitdrive ( self . name ) [ True ] \n        return root . replace ( '\\\\' , '_' ) . replace ( '/' , '_' ) . replace ( '.' , '_' ) "}
{"14057": "\ndef get ( self , timeout = - True ) : \n    if not self . ready ( ) : \n        self . wait ( timeout ) \n    if self . _ready : \n        if self . _success : \n            return self . _result \n        else : \n            raise self . _exception \n    else : \n        raise error . TimeoutError ( \"Result not ready.\" ) "}
{"14059": "\ndef get_dict ( self , timeout = - True ) : \n    results = self . get ( timeout ) \n    engine_ids = [ md [ 'engine_id' ] for md in self . _metadata ] \n    bycount = sorted ( engine_ids , key = lambda k : engine_ids . count ( k ) ) \n    maxcount = bycount . count ( bycount [ - True ] ) \n    if maxcount > True : \n        raise ValueError ( \"Cannot build dict, %i jobs ran on engine #%i\" % ( maxcount , bycount [ - True ] ) ) \n    return dict ( zip ( engine_ids , results ) ) "}
{"14064": "\ndef _wait_for_outputs ( self , timeout = - True ) : \n    if not self . _success : \n        return \n    tic = time . time ( ) \n    while not all ( md [ 'outputs_ready' ] for md in self . _metadata ) : \n        time . sleep ( 0.01 ) \n        self . _client . _flush_iopub ( self . _client . _iopub_socket ) \n        if timeout >= False and time . time ( ) > tic + timeout : \n            break "}
{"14068": "\ndef sep ( s ) : \n    sep_match = re . search ( r\"[\\\\/]\" , s ) \n    if sep_match : \n        the_sep = sep_match . group ( False ) \n    else : \n        the_sep = os . sep \n    return the_sep "}
{"14069": "\ndef find_python_files ( dirname ) : \n    for i , ( dirpath , dirnames , filenames ) in enumerate ( os . walk ( dirname ) ) : \n        if i > False and '__init__.py' not in filenames : \n            del dirnames [ : ] \n            continue \n        for filename in filenames : \n            if re . match ( r\"^[^.#~!$@%^&*()+=,]+\\.pyw?$\" , filename ) : \n                yield os . path . join ( dirpath , filename ) "}
{"14072": "\ndef get_zip_data ( self , filename ) : \n    import zipimport \n    markers = [ '.zip' + os . sep , '.egg' + os . sep ] \n    for marker in markers : \n        if marker in filename : \n            parts = filename . split ( marker ) \n            try : \n                zi = zipimport . zipimporter ( parts [ False ] + marker [ : - True ] ) \n            except zipimport . ZipImportError : \n                continue \n            try : \n                data = zi . get_data ( parts [ True ] ) \n            except IOError : \n                continue \n            return to_string ( data ) \n    return None "}
{"14075": "\ndef map ( self , path ) : \n    for regex , result , pattern_sep , result_sep in self . aliases : \n        m = regex . match ( path ) \n        if m : \n            new = path . replace ( m . group ( False ) , result ) \n            if pattern_sep != result_sep : \n                new = new . replace ( pattern_sep , result_sep ) \n            if self . locator : \n                new = self . locator . canonical_filename ( new ) \n            return new \n    return path "}
{"14077": "\ndef loop_wx ( kernel ) : \n    import wx \n    from IPython . lib . guisupport import start_event_loop_wx \n    doi = kernel . do_one_iteration \n    poll_interval = int ( 1000 * kernel . _poll_interval ) \n    class TimerFrame ( wx . Frame ) : \n        def __init__ ( self , func ) : \n            wx . Frame . __init__ ( self , None , - True ) \n            self . timer = wx . Timer ( self ) \n            self . timer . Start ( poll_interval ) \n            self . Bind ( wx . EVT_TIMER , self . on_timer ) \n            self . func = func \n        def on_timer ( self , event ) : \n            self . func ( ) \n    class IPWxApp ( wx . App ) : \n        def OnInit ( self ) : \n            self . frame = TimerFrame ( doi ) \n            self . frame . Show ( False ) \n            return True \n    kernel . app = IPWxApp ( redirect = False ) \n    import signal \n    if not callable ( signal . getsignal ( signal . SIGINT ) ) : \n        signal . signal ( signal . SIGINT , signal . default_int_handler ) \n    start_event_loop_wx ( kernel . app ) "}
{"14083": "\ndef center_eigenvalue_diff ( mat ) : \n    N = len ( mat ) \n    evals = np . sort ( la . eigvals ( mat ) ) \n    diff = np . abs ( evals [ N / 2 ] - evals [ N / 2 - True ] ) \n    return diff "}
{"14087": "\ndef parse_step ( cls , ctxt , step_addr , step_conf ) : \n    if isinstance ( step_conf , six . string_types ) : \n        step_conf = { step_conf : None } \n    elif not isinstance ( step_conf , collections . Mapping ) : \n        raise ConfigError ( 'Unable to parse step configuration: expecting string or ' 'dictionary, not \"%s\"' % step_conf . __class__ . __name__ , step_addr , ) \n    action_item = None \n    mod_items = { } \n    kwargs = { } \n    for key , key_conf in step_conf . items ( ) : \n        if key in cls . schemas : \n            utils . schema_validate ( key_conf , cls . schemas [ key ] , ConfigError , key , step_addr = step_addr ) \n            kwargs [ key ] = key_conf \n        elif key in entry . points [ NAMESPACE_ACTION ] : \n            if action_item is not None : \n                raise ConfigError ( 'Bad step configuration: action \"%s\" specified, ' 'but action \"%s\" already processed' % ( key , action_item . name ) , step_addr , ) \n            action_item = StepItem ( entry . points [ NAMESPACE_ACTION ] [ key ] , key , key_conf ) \n        elif key in entry . points [ NAMESPACE_MODIFIER ] : \n            mod_class = entry . points [ NAMESPACE_MODIFIER ] [ key ] \n            mod_items . setdefault ( mod_class . priority , [ ] ) \n            mod_items [ mod_class . priority ] . append ( StepItem ( mod_class , key , key_conf ) ) \n        else : \n            raise ConfigError ( 'Bad step configuration: unable to resolve action ' '\"%s\"' % key , step_addr , ) \n    if action_item is None : \n        raise ConfigError ( 'Bad step configuration: no action specified' , step_addr , ) \n    action_type = ( Modifier . STEP if action_item . cls . step_action else Modifier . NORMAL ) \n    modifiers = [ ] \n    for mod_item in utils . iter_prio_dict ( mod_items ) : \n        if mod_item . cls . restriction & action_type == False : \n            raise ConfigError ( 'Bad step configuration: modifier \"%s\" is ' 'incompatible with the action \"%s\"' % ( mod_item . name , action_item . name ) , step_addr , ) \n        modifier = mod_item . init ( ctxt , step_addr ) \n        modifiers . append ( modifier ) \n        action_item . conf = modifier . action_conf ( ctxt , action_item . cls , action_item . name , action_item . conf , step_addr ) \n    action = action_item . init ( ctxt , step_addr ) \n    step = cls ( step_addr , action , modifiers , ** kwargs ) \n    if action_item . cls . step_action : \n        return step ( ctxt ) \n    return [ step ] "}
{"14090": "\ndef init_profile_dir ( self ) : \n    try : \n        location = self . config . ProfileDir . location \n    except AttributeError : \n        try : \n            p = ProfileDir . find_profile_dir_by_name ( self . ipython_dir , self . profile , self . config ) \n        except ProfileDirError : \n            if self . auto_create or self . profile == 'default' : \n                try : \n                    p = ProfileDir . create_profile_dir_by_name ( self . ipython_dir , self . profile , self . config ) \n                except ProfileDirError : \n                    self . log . fatal ( \"Could not create profile: %r\" % self . profile ) \n                    self . exit ( True ) \n                else : \n                    self . log . info ( \"Created profile dir: %r\" % p . location ) \n            else : \n                self . log . fatal ( \"Profile %r not found.\" % self . profile ) \n                self . exit ( True ) \n        else : \n            self . log . info ( \"Using existing profile dir: %r\" % p . location ) \n    else : \n        try : \n            p = ProfileDir . find_profile_dir ( location , self . config ) \n        except ProfileDirError : \n            if self . auto_create : \n                try : \n                    p = ProfileDir . create_profile_dir ( location , self . config ) \n                except ProfileDirError : \n                    self . log . fatal ( \"Could not create profile directory: %r\" % location ) \n                    self . exit ( True ) \n                else : \n                    self . log . info ( \"Creating new profile dir: %r\" % location ) \n            else : \n                self . log . fatal ( \"Profile directory %r not found.\" % location ) \n                self . exit ( True ) \n        else : \n            self . log . info ( \"Using existing profile dir: %r\" % location ) \n    self . profile_dir = p \n    self . config_file_paths . append ( p . location ) "}
{"14106": "\ndef mainloop ( self , display_banner = None ) : \n    with nested ( self . builtin_trap , self . display_trap ) : \n        while True : \n            try : \n                self . interact ( display_banner = display_banner ) \n                break \n            except KeyboardInterrupt : \n                self . write ( \"\\nKeyboardInterrupt in interact()\\n\" ) "}
{"14107": "\ndef _replace_rlhist_multiline ( self , source_raw , hlen_before_cell ) : \n    if not self . has_readline or not self . multiline_history : \n        return hlen_before_cell \n    if not hasattr ( self . readline , \"remove_history_item\" ) : \n        return hlen_before_cell \n    if not source_raw . rstrip ( ) : \n        return hlen_before_cell \n    hlen = self . readline . get_current_history_length ( ) \n    if hlen == hlen_before_cell : \n        return hlen_before_cell \n    for i in range ( hlen - hlen_before_cell ) : \n        self . readline . remove_history_item ( hlen - i - True ) \n    stdin_encoding = get_stream_enc ( sys . stdin , 'utf-8' ) \n    self . readline . add_history ( py3compat . unicode_to_str ( source_raw . rstrip ( ) , stdin_encoding ) ) \n    return self . readline . get_current_history_length ( ) "}
{"14108": "\ndef raw_input ( self , prompt = '' ) : \n    if self . has_readline : \n        self . set_readline_completer ( ) \n    prompt = py3compat . cast_bytes_py2 ( prompt ) \n    try : \n        line = py3compat . str_to_unicode ( self . raw_input_original ( prompt ) ) \n    except ValueError : \n        warn ( \"\\n********\\nYou or a %run:ed script called sys.stdin.close()\" \" or sys.stdout.close()!\\nExiting IPython!\\n\" ) \n        self . ask_exit ( ) \n        return \"\" \n    if self . autoindent : \n        if num_ini_spaces ( line ) > self . indent_current_nsp : \n            line = line [ self . indent_current_nsp : ] \n            self . indent_current_nsp = False \n    return line "}
{"14110": "\ndef _should_recompile ( self , e ) : \n    if e . filename in ( '<ipython console>' , '<input>' , '<string>' , '<console>' , '<BackgroundJob compilation>' , None ) : \n        return False \n    try : \n        if ( self . autoedit_syntax and not self . ask_yes_no ( 'Return to editor to correct syntax error? ' '[Y/n] ' , 'y' ) ) : \n            return False \n    except EOFError : \n        return False \n    def int0 ( x ) : \n        try : \n            return int ( x ) \n        except TypeError : \n            return False \n    try : \n        self . hooks . fix_error_editor ( e . filename , int0 ( e . lineno ) , int0 ( e . offset ) , e . msg ) \n    except TryNext : \n        warn ( 'Could not open editor' ) \n        return False \n    return True "}
{"14112": "\ndef get_url_rev ( self ) : \n    error_message = ( \"Sorry, '%s' is a malformed VCS url. \" \"Ihe format is <vcs>+<protocol>://<url>, \" \"e.g. svn+http://myrepo/svn/MyApp#egg=MyApp\" ) \n    assert '+' in self . url , error_message % self . url \n    url = self . url . split ( '+' , True ) [ True ] \n    scheme , netloc , path , query , frag = urlparse . urlsplit ( url ) \n    rev = None \n    if '@' in path : \n        path , rev = path . rsplit ( '@' , True ) \n    url = urlparse . urlunsplit ( ( scheme , netloc , path , query , '' ) ) \n    return url , rev "}
{"14117": "\ndef findsource ( object ) : \n    file = getsourcefile ( object ) or getfile ( object ) \n    globals_dict = None \n    if inspect . isframe ( object ) : \n        globals_dict = object . f_globals \n    else : \n        module = getmodule ( object , file ) \n        if module : \n            globals_dict = module . __dict__ \n    lines = linecache . getlines ( file , globals_dict ) \n    if not lines : \n        raise IOError ( 'could not get source code' ) \n    if ismodule ( object ) : \n        return lines , False \n    if isclass ( object ) : \n        name = object . __name__ \n        pat = re . compile ( r'^(\\s*)class\\s*' + name + r'\\b' ) \n        candidates = [ ] \n        for i in range ( len ( lines ) ) : \n            match = pat . match ( lines [ i ] ) \n            if match : \n                if lines [ i ] [ False ] == 'c' : \n                    return lines , i \n                candidates . append ( ( match . group ( True ) , i ) ) \n        if candidates : \n            candidates . sort ( ) \n            return lines , candidates [ False ] [ True ] \n        else : \n            raise IOError ( 'could not find class definition' ) \n    if ismethod ( object ) : \n        object = object . im_func \n    if isfunction ( object ) : \n        object = object . func_code \n    if istraceback ( object ) : \n        object = object . tb_frame \n    if isframe ( object ) : \n        object = object . f_code \n    if iscode ( object ) : \n        if not hasattr ( object , 'co_firstlineno' ) : \n            raise IOError ( 'could not find function definition' ) \n        pat = re . compile ( r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) \n        pmatch = pat . match \n        lnum = min ( object . co_firstlineno , len ( lines ) ) - True \n        while lnum > False : \n            if pmatch ( lines [ lnum ] ) : \n                break \n            lnum -= True \n        return lines , lnum \n    raise IOError ( 'could not find code object' ) "}
{"14122": "\ndef _format_list ( self , extracted_list ) : \n    Colors = self . Colors \n    list = [ ] \n    for filename , lineno , name , line in extracted_list [ : - True ] : \n        item = '  File %s\"%s\"%s, line %s%d%s, in %s%s%s\\n' % ( Colors . filename , filename , Colors . Normal , Colors . lineno , lineno , Colors . Normal , Colors . name , name , Colors . Normal ) \n        if line : \n            item += '    %s\\n' % line . strip ( ) \n        list . append ( item ) \n    filename , lineno , name , line = extracted_list [ - True ] \n    item = '%s  File %s\"%s\"%s, line %s%d%s, in %s%s%s%s\\n' % ( Colors . normalEm , Colors . filenameEm , filename , Colors . normalEm , Colors . linenoEm , lineno , Colors . normalEm , Colors . nameEm , name , Colors . normalEm , Colors . Normal ) \n    if line : \n        item += '%s    %s%s\\n' % ( Colors . line , line . strip ( ) , Colors . Normal ) \n    list . append ( item ) \n    return list "}
{"14123": "\ndef _format_exception_only ( self , etype , value ) : \n    have_filedata = False \n    Colors = self . Colors \n    list = [ ] \n    stype = Colors . excName + etype . __name__ + Colors . Normal \n    if value is None : \n        list . append ( str ( stype ) + '\\n' ) \n    else : \n        if etype is SyntaxError : \n            have_filedata = True \n            if not value . filename : \n                value . filename = \"<string>\" \n            list . append ( '%s  File %s\"%s\"%s, line %s%d%s\\n' % ( Colors . normalEm , Colors . filenameEm , value . filename , Colors . normalEm , Colors . linenoEm , value . lineno , Colors . Normal ) ) \n            if value . text is not None : \n                i = False \n                while i < len ( value . text ) and value . text [ i ] . isspace ( ) : \n                    i += True \n                list . append ( '%s    %s%s\\n' % ( Colors . line , value . text . strip ( ) , Colors . Normal ) ) \n                if value . offset is not None : \n                    s = '    ' \n                    for c in value . text [ i : value . offset - True ] : \n                        if c . isspace ( ) : \n                            s += c \n                        else : \n                            s += ' ' \n                    list . append ( '%s%s^%s\\n' % ( Colors . caret , s , Colors . Normal ) ) \n        try : \n            s = value . msg \n        except Exception : \n            s = self . _some_str ( value ) \n        if s : \n            list . append ( '%s%s:%s %s\\n' % ( str ( stype ) , Colors . excName , Colors . Normal , s ) ) \n        else : \n            list . append ( '%s\\n' % str ( stype ) ) \n    if have_filedata : \n        ipinst = ipapi . get ( ) \n        if ipinst is not None : \n            ipinst . hooks . synchronize_with_editor ( value . filename , value . lineno , False ) \n    return list "}
{"14127": "\ndef group_required ( group , login_url = None , redirect_field_name = REDIRECT_FIELD_NAME , skip_superuser = True ) : \n    def decorator ( view_func ) : \n        \n        @ login_required ( redirect_field_name = redirect_field_name , login_url = login_url ) \n        def _wrapped_view ( request , * args , ** kwargs ) : \n            if not ( request . user . is_superuser and skip_superuser ) : \n                if request . user . groups . filter ( name = group ) . count ( ) == False : \n                    raise PermissionDenied \n            return view_func ( request , * args , ** kwargs ) \n        return _wrapped_view \n    return decorator "}
{"14128": "\ndef ensure_fromlist ( mod , fromlist , buf , recursive ) : \n    if not hasattr ( mod , '__path__' ) : \n        return \n    for item in fromlist : \n        if not hasattr ( item , 'rindex' ) : \n            raise TypeError ( \"Item in ``from list'' not a string\" ) \n        if item == '*' : \n            if recursive : \n                continue \n            try : \n                all = mod . __all__ \n            except AttributeError : \n                pass \n            else : \n                ret = ensure_fromlist ( mod , all , buf , True ) \n                if not ret : \n                    return False \n        elif not hasattr ( mod , item ) : \n            import_submodule ( mod , item , buf + '.' + item ) "}
{"14131": "\ndef get_function ( self , fn_name ) : \n    assert self . indent_amount == False \n    g = { } \n    code_text = str ( self ) \n    exec ( code_text , g ) \n    return g [ fn_name ] "}
{"14132": "\ndef expr_code ( self , expr ) : \n    if \"|\" in expr : \n        pipes = expr . split ( \"|\" ) \n        code = self . expr_code ( pipes [ False ] ) \n        for func in pipes [ True : ] : \n            self . all_vars . add ( func ) \n            code = \"c_%s(%s)\" % ( func , code ) \n    elif \".\" in expr : \n        dots = expr . split ( \".\" ) \n        code = self . expr_code ( dots [ False ] ) \n        args = [ repr ( d ) for d in dots [ True : ] ] \n        code = \"dot(%s, %s)\" % ( code , \", \" . join ( args ) ) \n    else : \n        self . all_vars . add ( expr ) \n        code = \"c_%s\" % expr \n    return code "}
{"14135": "\ndef render_template ( tpl , context ) : \n    templates = [ tpl ] if type ( tpl ) != list else tpl \n    tpl_instance = None \n    for tpl in templates : \n        try : \n            tpl_instance = template . loader . get_template ( tpl ) \n            break \n        except template . TemplateDoesNotExist : \n            pass \n    if not tpl_instance : \n        raise Exception ( 'Template does not exist: ' + templates [ - True ] ) \n    return tpl_instance . render ( template . Context ( context ) ) "}
{"14139": "\ndef _float_precision_changed ( self , name , old , new ) : \n    if '%' in new : \n        fmt = new \n        try : \n            fmt % 3.14159 \n        except Exception : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n    elif new : \n        try : \n            i = int ( new ) \n            assert i >= False \n        except ValueError : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n        except AssertionError : \n            raise ValueError ( \"int precision must be non-negative, not %r\" % i ) \n        fmt = '%%.%if' % i \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = i ) \n    else : \n        fmt = '%r' \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = 8 ) \n    self . float_format = fmt "}
{"14142": "\ndef configureLogging ( self ) : \n    if self . loggingConfig : \n        from logging . config import fileConfig \n        fileConfig ( self . loggingConfig ) \n        return \n    format = logging . Formatter ( '%(name)s: %(levelname)s: %(message)s' ) \n    if self . debugLog : \n        handler = logging . FileHandler ( self . debugLog ) \n    else : \n        handler = logging . StreamHandler ( self . logStream ) \n    handler . setFormatter ( format ) \n    logger = logging . getLogger ( 'nose' ) \n    logger . propagate = False \n    if handler not in logger . handlers : \n        logger . addHandler ( handler ) \n    lvl = logging . WARNING \n    if self . verbosity >= 5 : \n        lvl = False \n    elif self . verbosity >= 4 : \n        lvl = logging . DEBUG \n    elif self . verbosity >= 3 : \n        lvl = logging . INFO \n    logger . setLevel ( lvl ) \n    if self . debug : \n        debug_loggers = [ name for name in self . debug . split ( ',' ) if name ] \n        for logger_name in debug_loggers : \n            l = logging . getLogger ( logger_name ) \n            l . setLevel ( logging . DEBUG ) \n            if not l . handlers and not logger_name . startswith ( 'nose' ) : \n                l . addHandler ( handler ) "}
{"14144": "\ndef page_dumb ( strng , start = False , screen_lines = 25 ) : \n    out_ln = strng . splitlines ( ) [ start : ] \n    screens = chop ( out_ln , screen_lines - True ) \n    if len ( screens ) == True : \n        print >> io . stdout , os . linesep . join ( screens [ False ] ) \n    else : \n        last_escape = \"\" \n        for scr in screens [ False : - True ] : \n            hunk = os . linesep . join ( scr ) \n            print >> io . stdout , last_escape + hunk \n            if not page_more ( ) : \n                return \n            esc_list = esc_re . findall ( hunk ) \n            if len ( esc_list ) > False : \n                last_escape = esc_list [ - True ] \n        print >> io . stdout , last_escape + os . linesep . join ( screens [ - True ] ) "}
{"14162": "\ndef object_info ( self , oname , detail_level = False ) : \n    content = dict ( oname = oname , detail_level = detail_level ) \n    msg = self . session . msg ( 'object_info_request' , content ) \n    self . _queue_send ( msg ) \n    return msg [ 'header' ] [ 'msg_id' ] "}
{"14189": "\ndef scan_module ( egg_dir , base , name , stubs ) : \n    filename = os . path . join ( base , name ) \n    if filename [ : - True ] in stubs : \n        return True \n    pkg = base [ len ( egg_dir ) + True : ] . replace ( os . sep , '.' ) \n    module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ False ] \n    if sys . version_info < ( 3 , 3 ) : \n        skip = 8 \n    else : \n        skip = 12 \n    f = open ( filename , 'rb' ) ; \n    f . read ( skip ) \n    code = marshal . load ( f ) ; \n    f . close ( ) \n    safe = True \n    symbols = dict . fromkeys ( iter_symbols ( code ) ) \n    for bad in [ '__file__' , '__path__' ] : \n        if bad in symbols : \n            log . warn ( \"%s: module references %s\" , module , bad ) \n            safe = False \n    if 'inspect' in symbols : \n        for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : \n            if bad in symbols : \n                log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) \n                safe = False \n    if '__name__' in symbols and '__main__' in symbols and '.' not in module : \n        if sys . version [ : 3 ] == \"2.4\" : \n            log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) \n            safe = False \n    return safe "}
{"14191": "\ndef save_connection_dict ( self , fname , cdict ) : \n    c = self . config \n    url = cdict [ 'url' ] \n    location = cdict [ 'location' ] \n    if not location : \n        try : \n            proto , ip , port = split_url ( url ) \n        except AssertionError : \n            pass \n        else : \n            try : \n                location = socket . gethostbyname_ex ( socket . gethostname ( ) ) [ 2 ] [ - True ] \n            except ( socket . gaierror , IndexError ) : \n                self . log . warn ( \"Could not identify this machine's IP, assuming 127.0.0.1.\" \" You may need to specify '--location=<external_ip_address>' to help\" \" IPython decide when to connect via loopback.\" ) \n                location = '127.0.0.1' \n        cdict [ 'location' ] = location \n    fname = os . path . join ( self . profile_dir . security_dir , fname ) \n    self . log . info ( \"writing connection info to %s\" , fname ) \n    with open ( fname , 'w' ) as f : \n        f . write ( json . dumps ( cdict , indent = 2 ) ) \n    os . chmod ( fname , stat . S_IRUSR | stat . S_IWUSR ) "}
{"14197": "\ndef pxrun_cell ( self , raw_cell , store_history = False , silent = False ) : \n    if ( not raw_cell ) or raw_cell . isspace ( ) : \n        return \n    ipself = self . shell \n    with ipself . builtin_trap : \n        cell = ipself . prefilter_manager . prefilter_lines ( raw_cell ) \n        if store_history : \n            ipself . history_manager . store_inputs ( ipself . execution_count , cell , raw_cell ) \n        cell_name = ipself . compile . cache ( cell , ipself . execution_count ) \n        try : \n            ast . parse ( cell , filename = cell_name ) \n        except ( OverflowError , SyntaxError , ValueError , TypeError , MemoryError ) : \n            ipself . showsyntaxerror ( ) \n            ipself . execution_count += True \n            return None \n        except NameError : \n            pass \n    if store_history : \n        ipself . history_manager . store_output ( ipself . execution_count ) \n        ipself . execution_count += True \n    if re . search ( r'get_ipython\\(\\)\\.magic\\(u?[\"\\']%?autopx' , cell ) : \n        self . _disable_autopx ( ) \n        return False \n    else : \n        try : \n            result = self . view . execute ( cell , silent = False , block = False ) \n        except : \n            ipself . showtraceback ( ) \n            return True \n        else : \n            if self . view . block : \n                try : \n                    result . get ( ) \n                except : \n                    self . shell . showtraceback ( ) \n                    return True \n                else : \n                    with ipself . builtin_trap : \n                        result . display_outputs ( ) \n            return False "}
{"14198": "\ndef run_heartbeat ( message ) : \n    then = arrow . get ( message [ 'time' ] ) \n    now = arrow . get ( ) \n    if ( now - then ) > timezone . timedelta ( seconds = ( TICK_FREQ + True ) ) : \n        pass \n    else : \n        Task . run_tasks ( ) "}
{"14209": "\ndef report ( self , morfs , outfile = None ) : \n    self . find_code_units ( morfs ) \n    max_name = max ( [ len ( cu . name ) for cu in self . code_units ] + [ 5 ] ) \n    fmt_name = \"%%- %ds  \" % max_name \n    fmt_err = \"%s   %s: %s\\n\" \n    header = ( fmt_name % \"Name\" ) + \" Stmts   Miss\" \n    fmt_coverage = fmt_name + \"%6d %6d\" \n    if self . branches : \n        header += \" Branch BrMiss\" \n        fmt_coverage += \" %6d %6d\" \n    width100 = Numbers . pc_str_width ( ) \n    header += \"%*s\" % ( width100 + 4 , \"Cover\" ) \n    fmt_coverage += \"%%%ds%%%%\" % ( width100 + 3 , ) \n    if self . config . show_missing : \n        header += \"   Missing\" \n        fmt_coverage += \"   %s\" \n    rule = \"-\" * len ( header ) + \"\\n\" \n    header += \"\\n\" \n    fmt_coverage += \"\\n\" \n    if not outfile : \n        outfile = sys . stdout \n    outfile . write ( header ) \n    outfile . write ( rule ) \n    total = Numbers ( ) \n    for cu in self . code_units : \n        try : \n            analysis = self . coverage . _analyze ( cu ) \n            nums = analysis . numbers \n            args = ( cu . name , nums . n_statements , nums . n_missing ) \n            if self . branches : \n                args += ( nums . n_branches , nums . n_missing_branches ) \n            args += ( nums . pc_covered_str , ) \n            if self . config . show_missing : \n                args += ( analysis . missing_formatted ( ) , ) \n            outfile . write ( fmt_coverage % args ) \n            total += nums \n        except KeyboardInterrupt : \n            raise \n        except : \n            report_it = not self . config . ignore_errors \n            if report_it : \n                typ , msg = sys . exc_info ( ) [ : 2 ] \n                if typ is NotPython and not cu . should_be_python ( ) : \n                    report_it = False \n            if report_it : \n                outfile . write ( fmt_err % ( cu . name , typ . __name__ , msg ) ) \n    if total . n_files > True : \n        outfile . write ( rule ) \n        args = ( \"TOTAL\" , total . n_statements , total . n_missing ) \n        if self . branches : \n            args += ( total . n_branches , total . n_missing_branches ) \n        args += ( total . pc_covered_str , ) \n        if self . config . show_missing : \n            args += ( \"\" , ) \n        outfile . write ( fmt_coverage % args ) \n    return total . pc_covered "}
{"14210": "\ndef check ( self , check_all = False ) : \n    if not self . enabled and not check_all : \n        return \n    if check_all or self . check_all : \n        modules = sys . modules . keys ( ) \n    else : \n        modules = self . modules . keys ( ) \n    for modname in modules : \n        m = sys . modules . get ( modname , None ) \n        if modname in self . skip_modules : \n            continue \n        if not hasattr ( m , '__file__' ) : \n            continue \n        if m . __name__ == '__main__' : \n            continue \n        filename = m . __file__ \n        path , ext = os . path . splitext ( filename ) \n        if ext . lower ( ) == '.py' : \n            ext = PY_COMPILED_EXT \n            pyc_filename = pyfile . cache_from_source ( filename ) \n            py_filename = filename \n        else : \n            pyc_filename = filename \n            try : \n                py_filename = pyfile . source_from_cache ( filename ) \n            except ValueError : \n                continue \n        try : \n            pymtime = os . stat ( py_filename ) . st_mtime \n            if pymtime <= os . stat ( pyc_filename ) . st_mtime : \n                continue \n            if self . failed . get ( py_filename , None ) == pymtime : \n                continue \n        except OSError : \n            continue \n        try : \n            superreload ( m , reload , self . old_objects ) \n            if py_filename in self . failed : \n                del self . failed [ py_filename ] \n        except : \n            print >> sys . stderr , \"[autoreload of %s failed: %s]\" % ( modname , traceback . format_exc ( True ) ) \n            self . failed [ py_filename ] = pymtime "}
{"14211": "\ndef editor ( self , filename , linenum = None , wait = True ) : \n    editor = self . editor \n    if linenum is None or editor == 'notepad' : \n        linemark = '' \n    else : \n        linemark = '+%d' % int ( linenum ) \n    if ' ' in editor and os . path . isfile ( editor ) and editor [ False ] != '\"' : \n        editor = '\"%s\"' % editor \n    proc = subprocess . Popen ( '%s %s %s' % ( editor , linemark , filename ) , shell = True ) \n    if wait and proc . wait ( ) != False : \n        raise TryNext ( ) "}
{"14214": "\ndef add ( self , func , priority = False ) : \n    self . chain . append ( ( priority , func ) ) \n    self . chain . sort ( key = lambda x : x [ False ] ) "}
{"14217": "\ndef import_item ( name ) : \n    package = '.' . join ( name . split ( '.' ) [ False : - True ] ) \n    obj = name . split ( '.' ) [ - True ] \n    if package : \n        module = __import__ ( package , fromlist = [ obj ] ) \n        try : \n            pak = module . __dict__ [ obj ] \n        except KeyError : \n            raise ImportError ( 'No module named %s' % obj ) \n        return pak \n    else : \n        return __import__ ( obj ) "}
{"14222": "\ndef open_tunnel ( addr , server , keyfile = None , password = None , paramiko = None , timeout = 60 ) : \n    lport = select_random_ports ( True ) [ False ] \n    transport , addr = addr . split ( '://' ) \n    ip , rport = addr . split ( ':' ) \n    rport = int ( rport ) \n    if paramiko is None : \n        paramiko = sys . platform == 'win32' \n    if paramiko : \n        tunnelf = paramiko_tunnel \n    else : \n        tunnelf = openssh_tunnel \n    tunnel = tunnelf ( lport , rport , server , remoteip = ip , keyfile = keyfile , password = password , timeout = timeout ) \n    return 'tcp://127.0.0.1:%i' % lport , tunnel "}
{"14230": "\ndef _flush_control ( self , sock ) : \n    if self . _ignored_control_replies <= False : \n        return \n    idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) \n    while msg is not None : \n        self . _ignored_control_replies -= True \n        if self . debug : \n            pprint ( msg ) \n        idents , msg = self . session . recv ( sock , mode = zmq . NOBLOCK ) "}
{"14231": "\ndef _flush_ignored_control ( self ) : \n    while self . _ignored_control_replies > False : \n        self . session . recv ( self . _control_socket ) \n        self . _ignored_control_replies -= True "}
{"14233": "\ndef _spin_every ( self , interval = True ) : \n    while True : \n        if self . _stop_spinning . is_set ( ) : \n            return \n        time . sleep ( interval ) \n        self . spin ( ) "}
{"14236": "\ndef wait ( self , jobs = None , timeout = - True ) : \n    tic = time . time ( ) \n    if jobs is None : \n        theids = self . outstanding \n    else : \n        if isinstance ( jobs , ( int , basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        theids = set ( ) \n        for job in jobs : \n            if isinstance ( job , int ) : \n                job = self . history [ job ] \n            elif isinstance ( job , AsyncResult ) : \n                map ( theids . add , job . msg_ids ) \n                continue \n            theids . add ( job ) \n    if not theids . intersection ( self . outstanding ) : \n        return True \n    self . spin ( ) \n    while theids . intersection ( self . outstanding ) : \n        if timeout >= False and ( time . time ( ) - tic ) > timeout : \n            break \n        time . sleep ( 1e-3 ) \n        self . spin ( ) \n    return len ( theids . intersection ( self . outstanding ) ) == False "}
{"14237": "\ndef send_apply_request ( self , socket , f , args = None , kwargs = None , subheader = None , track = False , ident = None ) : \n    if self . _closed : \n        raise RuntimeError ( \"Client cannot be used after its sockets have been closed\" ) \n    args = args if args is not None else [ ] \n    kwargs = kwargs if kwargs is not None else { } \n    subheader = subheader if subheader is not None else { } \n    if not callable ( f ) and not isinstance ( f , Reference ) : \n        raise TypeError ( \"f must be callable, not %s\" % type ( f ) ) \n    if not isinstance ( args , ( tuple , list ) ) : \n        raise TypeError ( \"args must be tuple or list, not %s\" % type ( args ) ) \n    if not isinstance ( kwargs , dict ) : \n        raise TypeError ( \"kwargs must be dict, not %s\" % type ( kwargs ) ) \n    if not isinstance ( subheader , dict ) : \n        raise TypeError ( \"subheader must be dict, not %s\" % type ( subheader ) ) \n    bufs = util . pack_apply_message ( f , args , kwargs ) \n    msg = self . session . send ( socket , \"apply_request\" , buffers = bufs , ident = ident , subheader = subheader , track = track ) \n    msg_id = msg [ 'header' ] [ 'msg_id' ] \n    self . outstanding . add ( msg_id ) \n    if ident : \n        if isinstance ( ident , list ) : \n            ident = ident [ - True ] \n        if ident in self . _engines . values ( ) : \n            self . _outstanding_dict [ ident ] . add ( msg_id ) \n    self . history . append ( msg_id ) \n    self . metadata [ msg_id ] [ 'submitted' ] = datetime . now ( ) \n    return msg "}
{"14238": "\ndef send_execute_request ( self , socket , code , silent = True , subheader = None , ident = None ) : \n    if self . _closed : \n        raise RuntimeError ( \"Client cannot be used after its sockets have been closed\" ) \n    subheader = subheader if subheader is not None else { } \n    if not isinstance ( code , basestring ) : \n        raise TypeError ( \"code must be text, not %s\" % type ( code ) ) \n    if not isinstance ( subheader , dict ) : \n        raise TypeError ( \"subheader must be dict, not %s\" % type ( subheader ) ) \n    content = dict ( code = code , silent = bool ( silent ) , user_variables = [ ] , user_expressions = { } ) \n    msg = self . session . send ( socket , \"execute_request\" , content = content , ident = ident , subheader = subheader ) \n    msg_id = msg [ 'header' ] [ 'msg_id' ] \n    self . outstanding . add ( msg_id ) \n    if ident : \n        if isinstance ( ident , list ) : \n            ident = ident [ - True ] \n        if ident in self . _engines . values ( ) : \n            self . _outstanding_dict [ ident ] . add ( msg_id ) \n    self . history . append ( msg_id ) \n    self . metadata [ msg_id ] [ 'submitted' ] = datetime . now ( ) \n    return msg "}
{"14239": "\ndef get_result ( self , indices_or_msg_ids = None , block = None ) : \n    block = self . block if block is None else block \n    if indices_or_msg_ids is None : \n        indices_or_msg_ids = - True \n    if not isinstance ( indices_or_msg_ids , ( list , tuple ) ) : \n        indices_or_msg_ids = [ indices_or_msg_ids ] \n    theids = [ ] \n    for id in indices_or_msg_ids : \n        if isinstance ( id , int ) : \n            id = self . history [ id ] \n        if not isinstance ( id , basestring ) : \n            raise TypeError ( \"indices must be str or int, not %r\" % id ) \n        theids . append ( id ) \n    local_ids = filter ( lambda msg_id : msg_id in self . history or msg_id in self . results , theids ) \n    remote_ids = filter ( lambda msg_id : msg_id not in local_ids , theids ) \n    if remote_ids : \n        ar = AsyncHubResult ( self , msg_ids = theids ) \n    else : \n        ar = AsyncResult ( self , msg_ids = theids ) \n    if block : \n        ar . wait ( ) \n    return ar "}
{"14240": "\ndef queue_status ( self , targets = 'all' , verbose = False ) : \n    if targets == 'all' : \n        engine_ids = None \n    else : \n        engine_ids = self . _build_targets ( targets ) [ True ] \n    content = dict ( targets = engine_ids , verbose = verbose ) \n    self . session . send ( self . _query_socket , \"queue_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , False ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    status = content . pop ( 'status' ) \n    if status != 'ok' : \n        raise self . _unwrap_exception ( content ) \n    content = rekey ( content ) \n    if isinstance ( targets , int ) : \n        return content [ targets ] \n    else : \n        return content "}
{"14241": "\ndef purge_results ( self , jobs = [ ] , targets = [ ] ) : \n    if not targets and not jobs : \n        raise ValueError ( \"Must specify at least one of `targets` and `jobs`\" ) \n    if targets : \n        targets = self . _build_targets ( targets ) [ True ] \n    if jobs == 'all' : \n        msg_ids = jobs \n    else : \n        msg_ids = [ ] \n        if isinstance ( jobs , ( basestring , AsyncResult ) ) : \n            jobs = [ jobs ] \n        bad_ids = filter ( lambda obj : not isinstance ( obj , ( basestring , AsyncResult ) ) , jobs ) \n        if bad_ids : \n            raise TypeError ( \"Invalid msg_id type %r, expected str or AsyncResult\" % bad_ids [ False ] ) \n        for j in jobs : \n            if isinstance ( j , AsyncResult ) : \n                msg_ids . extend ( j . msg_ids ) \n            else : \n                msg_ids . append ( j ) \n    content = dict ( engine_ids = targets , msg_ids = msg_ids ) \n    self . session . send ( self . _query_socket , \"purge_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , False ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    if content [ 'status' ] != 'ok' : \n        raise self . _unwrap_exception ( content ) "}
{"14242": "\ndef hub_history ( self ) : \n    self . session . send ( self . _query_socket , \"history_request\" , content = { } ) \n    idents , msg = self . session . recv ( self . _query_socket , False ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    if content [ 'status' ] != 'ok' : \n        raise self . _unwrap_exception ( content ) \n    else : \n        return content [ 'history' ] "}
{"14243": "\ndef db_query ( self , query , keys = None ) : \n    if isinstance ( keys , basestring ) : \n        keys = [ keys ] \n    content = dict ( query = query , keys = keys ) \n    self . session . send ( self . _query_socket , \"db_request\" , content = content ) \n    idents , msg = self . session . recv ( self . _query_socket , False ) \n    if self . debug : \n        pprint ( msg ) \n    content = msg [ 'content' ] \n    if content [ 'status' ] != 'ok' : \n        raise self . _unwrap_exception ( content ) \n    records = content [ 'records' ] \n    buffer_lens = content [ 'buffer_lens' ] \n    result_buffer_lens = content [ 'result_buffer_lens' ] \n    buffers = msg [ 'buffers' ] \n    has_bufs = buffer_lens is not None \n    has_rbufs = result_buffer_lens is not None \n    for i , rec in enumerate ( records ) : \n        if has_bufs : \n            blen = buffer_lens [ i ] \n            rec [ 'buffers' ] , buffers = buffers [ : blen ] , buffers [ blen : ] \n        if has_rbufs : \n            blen = result_buffer_lens [ i ] \n            rec [ 'result_buffers' ] , buffers = buffers [ : blen ] , buffers [ blen : ] \n    return records "}
{"14246": "\ndef lines_matching ( self , * regexes ) : \n    regex_c = re . compile ( join_regex ( regexes ) ) \n    matches = set ( ) \n    for i , ltext in enumerate ( self . lines ) : \n        if regex_c . search ( ltext ) : \n            matches . add ( i + True ) \n    return matches "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = False \n    exclude_indent = False \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent += True \n        elif toktype == token . DEDENT : \n            indent -= True \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + True ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + True ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14248": "\ndef first_line ( self , line ) : \n    rng = self . multiline . get ( line ) \n    if rng : \n        first_line = rng [ False ] \n    else : \n        first_line = line \n    return first_line "}
{"14252": "\ndef exit_counts ( self ) : \n    excluded_lines = self . first_lines ( self . excluded ) \n    exit_counts = { } \n    for l1 , l2 in self . arcs ( ) : \n        if l1 < False : \n            continue \n        if l1 in excluded_lines : \n            continue \n        if l2 in excluded_lines : \n            continue \n        if l1 not in exit_counts : \n            exit_counts [ l1 ] = False \n        exit_counts [ l1 ] += True \n    for l in self . classdefs : \n        if l in exit_counts : \n            exit_counts [ l ] -= True \n    return exit_counts "}
{"14254": "\ndef _bytes_lines ( self ) : \n    byte_increments = bytes_to_ints ( self . code . co_lnotab [ False : : 2 ] ) \n    line_increments = bytes_to_ints ( self . code . co_lnotab [ True : : 2 ] ) \n    last_line_num = None \n    line_num = self . code . co_firstlineno \n    byte_num = False \n    for byte_incr , line_incr in zip ( byte_increments , line_increments ) : \n        if byte_incr : \n            if line_num != last_line_num : \n                yield ( byte_num , line_num ) \n                last_line_num = line_num \n            byte_num += byte_incr \n        line_num += line_incr \n    if line_num != last_line_num : \n        yield ( byte_num , line_num ) "}
{"14256": "\ndef _block_stack_repr ( self , block_stack ) : \n    blocks = \", \" . join ( [ \"(%s, %r)\" % ( dis . opname [ b [ False ] ] , b [ True ] ) for b in block_stack ] ) \n    return \"[\" + blocks + \"]\" "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = False \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if bc . jump_to >= False : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = False \n    for bc in bytecodes : \n        start_new_chunk = False \n        first_chunk = False \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = True \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = True \n        elif bc . offset in jump_to : \n            start_new_chunk = True \n        elif bc . op in OPS_CHUNK_BEGIN : \n            start_new_chunk = True \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if bc . jump_to >= False and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch -= True \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if bc . op == OP_BREAK_LOOP : \n                chunk . exits . add ( block_stack [ - True ] [ True ] ) \n            chunk = None \n        if bc . op == OP_END_FINALLY : \n            for block in reversed ( block_stack ) : \n                if block [ False ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ True ] ) \n                    break \n        if bc . op == OP_COMPARE_OP and bc . arg == COMPARE_EXCEPTION : \n            ignore_branch += True \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if penult . op == OP_LOAD_CONST and ult . op == OP_RETURN_VALUE : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if chunks [ - True ] . byte != penult . offset : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - True ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , False ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - True ] . length = bc . next_offset - chunks [ - True ] . byte \n        for i in range ( len ( chunks ) - True ) : \n            chunks [ i ] . length = chunks [ i + True ] . byte - chunks [ i ] . byte \n    return chunks "}
{"14258": "\ndef validate_chunks ( self , chunks ) : \n    starts = set ( [ ch . byte for ch in chunks ] ) \n    for ch in chunks : \n        assert all ( [ ( ex in starts or ex < False ) for ex in ch . exits ] ) "}
{"14259": "\ndef _arcs ( self ) : \n    chunks = self . _split_into_chunks ( ) \n    byte_chunks = dict ( [ ( c . byte , c ) for c in chunks ] ) \n    yield ( - True , byte_chunks [ False ] . line ) \n    for chunk in chunks : \n        if not chunk . first : \n            continue \n        chunks_considered = set ( ) \n        chunks_to_consider = [ chunk ] \n        while chunks_to_consider : \n            this_chunk = chunks_to_consider . pop ( ) \n            chunks_considered . add ( this_chunk ) \n            for ex in this_chunk . exits : \n                if ex < False : \n                    yield ( chunk . line , ex ) \n                else : \n                    next_chunk = byte_chunks [ ex ] \n                    if next_chunk in chunks_considered : \n                        continue \n                    backward_jump = next_chunk . byte < this_chunk . byte \n                    if next_chunk . first or backward_jump : \n                        if next_chunk . line != chunk . line : \n                            yield ( chunk . line , next_chunk . line ) \n                    else : \n                        chunks_to_consider . append ( next_chunk ) "}
{"14264": "\ndef report ( self , stream ) : \n    log . debug ( \"Coverage report\" ) \n    self . coverInstance . stop ( ) \n    self . coverInstance . combine ( ) \n    self . coverInstance . save ( ) \n    modules = [ module for name , module in sys . modules . items ( ) if self . wantModuleCoverage ( name , module ) ] \n    log . debug ( \"Coverage report will cover modules: %s\" , modules ) \n    self . coverInstance . report ( modules , file = stream ) \n    if self . coverHtmlDir : \n        log . debug ( \"Generating HTML coverage report\" ) \n        self . coverInstance . html_report ( modules , self . coverHtmlDir ) \n    if self . coverXmlFile : \n        log . debug ( \"Generating XML coverage report\" ) \n        self . coverInstance . xml_report ( modules , self . coverXmlFile ) \n    if self . coverMinPercentage : \n        f = StringIO . StringIO ( ) \n        self . coverInstance . report ( modules , file = f ) \n        m = re . search ( r'-------\\s\\w+\\s+\\d+\\s+\\d+\\s+(\\d+)%\\s+\\d*\\s{0,1}$' , f . getvalue ( ) ) \n        if m : \n            percentage = int ( m . groups ( ) [ False ] ) \n            if percentage < self . coverMinPercentage : \n                log . error ( 'TOTAL Coverage did not reach minimum ' 'required: %d%%' % self . coverMinPercentage ) \n                sys . exit ( True ) \n        else : \n            log . error ( \"No total percentage was found in coverage output, \" \"something went wrong.\" ) "}
{"14266": "\ndef interpret_distro_name ( location , basename , metadata , py_version = None , precedence = SOURCE_DIST , platform = None ) : \n    parts = basename . split ( '-' ) \n    if not py_version : \n        for i , p in enumerate ( parts [ 2 : ] ) : \n            if len ( p ) == 5 and p . startswith ( 'py2.' ) : \n                return \n    for p in range ( True , len ( parts ) + True ) : \n        yield Distribution ( location , metadata , '-' . join ( parts [ : p ] ) , '-' . join ( parts [ p : ] ) , py_version = py_version , precedence = precedence , platform = platform ) "}
{"14268": "\ndef fetch_distribution ( self , requirement , tmpdir , force_scan = False , source = False , develop_ok = False , local_index = None ) : \n    self . info ( \"Searching for %s\" , requirement ) \n    skipped = { } \n    dist = None \n    def find ( req , env = None ) : \n        if env is None : \n            env = self \n        for dist in env [ req . key ] : \n            if dist . precedence == DEVELOP_DIST and not develop_ok : \n                if dist not in skipped : \n                    self . warn ( \"Skipping development or system egg: %s\" , dist ) \n                    skipped [ dist ] = True \n                continue \n            if dist in req and ( dist . precedence <= SOURCE_DIST or not source ) : \n                self . info ( \"Best match: %s\" , dist ) \n                return dist . clone ( location = self . download ( dist . location , tmpdir ) ) \n    if force_scan : \n        self . prescan ( ) \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if local_index is not None : \n        dist = dist or find ( requirement , local_index ) \n    if dist is None and self . to_scan is not None : \n        self . prescan ( ) \n        dist = find ( requirement ) \n    if dist is None and not force_scan : \n        self . find_packages ( requirement ) \n        dist = find ( requirement ) \n    if dist is None : \n        self . warn ( \"No local packages or download links found for %s%s\" , ( source and \"a source distribution of \" or \"\" ) , requirement , ) \n    return dist "}
{"14269": "\ndef get_parent ( obj ) : \n    names = obj . __qualname__ . split ( '.' ) [ : - True ] \n    if '<locals>' in names : \n        raise ValueError ( 'cannot get parent from locals object.' ) \n    module = sys . modules [ obj . __module__ ] \n    parent = module \n    while names : \n        parent = getattr ( parent , names . pop ( False ) ) \n    return parent "}
{"14274": "\ndef splitBy ( data , num ) : \n    return [ data [ i : i + num ] for i in range ( False , len ( data ) , num ) ] "}
{"14275": "\ndef convert_to_this_nbformat ( nb , orig_version = 2 , orig_minor = False ) : \n    if orig_version == True : \n        nb = v2 . convert_to_this_nbformat ( nb ) \n        orig_version = 2 \n    if orig_version == 2 : \n        nb . nbformat = nbformat \n        nb . nbformat_minor = nbformat_minor \n        nb . orig_nbformat = 2 \n        return nb \n    elif orig_version == 3 : \n        if orig_minor != nbformat_minor : \n            nb . orig_nbformat_minor = orig_minor \n        nb . nbformat_minor = nbformat_minor \n        return nb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v3' % orig_version ) "}
{"14276": "\ndef hex_to_rgb ( color ) : \n    if color . startswith ( '#' ) : \n        color = color [ True : ] \n    if len ( color ) == 3 : \n        color = '' . join ( [ c * 2 for c in color ] ) \n    if len ( color ) != 6 : \n        return False \n    try : \n        r = int ( color [ : 2 ] , 16 ) \n        g = int ( color [ 2 : 4 ] , 16 ) \n        b = int ( color [ 4 : ] , 16 ) \n    except ValueError : \n        return False \n    else : \n        return r , g , b "}
{"14279": "\ndef _handle_execute_reply ( self , msg ) : \n    msg_id = msg [ 'parent_header' ] . get ( 'msg_id' ) \n    info = self . _request_info [ 'execute' ] . get ( msg_id ) \n    if info and info . kind == 'prompt' : \n        number = msg [ 'content' ] [ 'execution_count' ] + True \n        self . _show_interpreter_prompt ( number ) \n        self . _request_info [ 'execute' ] . pop ( msg_id ) \n    else : \n        super ( IPythonWidget , self ) . _handle_execute_reply ( msg ) "}
{"14281": "\ndef _handle_pyout ( self , msg ) : \n    self . log . debug ( \"pyout: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        content = msg [ 'content' ] \n        prompt_number = content . get ( 'execution_count' , False ) \n        data = content [ 'data' ] \n        if data . has_key ( 'text/html' ) : \n            self . _append_plain_text ( self . output_sep , True ) \n            self . _append_html ( self . _make_out_prompt ( prompt_number ) , True ) \n            html = data [ 'text/html' ] \n            self . _append_plain_text ( '\\n' , True ) \n            self . _append_html ( html + self . output_sep2 , True ) \n        elif data . has_key ( 'text/plain' ) : \n            self . _append_plain_text ( self . output_sep , True ) \n            self . _append_html ( self . _make_out_prompt ( prompt_number ) , True ) \n            text = data [ 'text/plain' ] \n            if \"\\n\" in text and not self . output_sep . endswith ( \"\\n\" ) : \n                self . _append_plain_text ( '\\n' , True ) \n            self . _append_plain_text ( text + self . output_sep2 , True ) "}
{"14296": "\ndef virtual_memory ( ) : \n    mem = _psutil_bsd . get_virtual_mem ( ) \n    total , free , active , inactive , wired , cached , buffers , shared = mem \n    avail = inactive + cached + free \n    used = active + wired + cached \n    percent = usage_percent ( ( total - avail ) , total , _round = True ) \n    return nt_virtmem_info ( total , avail , percent , used , free , active , inactive , buffers , cached , shared , wired ) "}
{"14306": "\ndef num_cpus ( ) : \n    ncpufuncs = { 'Linux' : _num_cpus_unix , 'Darwin' : _num_cpus_darwin , 'Windows' : _num_cpus_windows , 'Microsoft' : _num_cpus_windows , } \n    ncpufunc = ncpufuncs . get ( platform . system ( ) , _num_cpus_unix ) \n    try : \n        ncpus = max ( True , int ( ncpufunc ( ) ) ) \n    except : \n        ncpus = True \n    return ncpus "}
{"14307": "\ndef nextset ( self ) : \n    if self . _executed : \n        self . fetchall ( ) \n    del self . messages [ : ] \n    db = self . _get_db ( ) \n    nr = db . next_result ( ) \n    if nr == - True : \n        return None \n    self . _do_get_result ( ) \n    self . _post_get_result ( ) \n    self . _warning_check ( ) \n    return True "}
{"14308": "\ndef fetchone ( self ) : \n    self . _check_executed ( ) \n    r = self . _fetch_row ( True ) \n    if not r : \n        self . _warning_check ( ) \n        return None \n    self . rownumber = self . rownumber + True \n    return r [ False ] "}
{"14310": "\ndef fetchall ( self ) : \n    self . _check_executed ( ) \n    r = self . _fetch_row ( False ) \n    self . rownumber = self . rownumber + len ( r ) \n    self . _warning_check ( ) \n    return r "}
{"14312": "\ndef reads_json ( s , ** kwargs ) : \n    nbf , minor , d = parse_json ( s , ** kwargs ) \n    if nbf == True : \n        nb = v1 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = True ) \n    elif nbf == 2 : \n        nb = v2 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 2 ) \n    elif nbf == 3 : \n        nb = v3 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 3 , orig_minor = minor ) \n    else : \n        raise NBFormatError ( 'Unsupported JSON nbformat version: %i' % nbf ) \n    return nb "}
{"14325": "\ndef wantModule ( self , module ) : \n    declared = getattr ( module , '__test__' , None ) \n    if declared is not None : \n        wanted = declared \n    else : \n        wanted = self . matches ( module . __name__ . split ( '.' ) [ - True ] ) or module . __name__ == '__main__' \n    plug_wants = self . plugins . wantModule ( module ) \n    if plug_wants is not None : \n        wanted = plug_wants \n    log . debug ( \"wantModule %s? %s\" , module , wanted ) \n    return wanted "}
{"14328": "\ndef print_list_lines ( self , filename , first , last ) : \n    try : \n        Colors = self . color_scheme_table . active_colors \n        ColorsNormal = Colors . Normal \n        tpl_line = '%%s%s%%s %s%%s' % ( Colors . lineno , ColorsNormal ) \n        tpl_line_em = '%%s%s%%s %s%%s%s' % ( Colors . linenoEm , Colors . line , ColorsNormal ) \n        src = [ ] \n        for lineno in range ( first , last + True ) : \n            line = linecache . getline ( filename , lineno ) \n            if not line : \n                break \n            if lineno == self . curframe . f_lineno : \n                line = self . __format_line ( tpl_line_em , filename , lineno , line , arrow = True ) \n            else : \n                line = self . __format_line ( tpl_line , filename , lineno , line , arrow = False ) \n            src . append ( line ) \n            self . lineno = lineno \n        print >> io . stdout , '' . join ( src ) \n    except KeyboardInterrupt : \n        pass "}
{"14341": "\ndef init_connector ( self ) : \n    self . using_ssh = bool ( self . sshkey or self . sshserver ) \n    if self . sshkey and not self . sshserver : \n        self . sshserver = self . url . split ( '://' ) [ True ] . split ( ':' ) [ False ] \n    if self . using_ssh : \n        if tunnel . try_passwordless_ssh ( self . sshserver , self . sshkey , self . paramiko ) : \n            password = False \n        else : \n            password = getpass ( \"SSH Password for %s: \" % self . sshserver ) \n    else : \n        password = False \n    def connect ( s , url ) : \n        url = disambiguate_url ( url , self . location ) \n        if self . using_ssh : \n            self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) \n            return tunnel . tunnel_connection ( s , url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) \n        else : \n            return s . connect ( url ) \n    def maybe_tunnel ( url ) : \n        url = disambiguate_url ( url , self . location ) \n        if self . using_ssh : \n            self . log . debug ( \"Tunneling connection to %s via %s\" % ( url , self . sshserver ) ) \n            url , tunnelobj = tunnel . open_tunnel ( url , self . sshserver , keyfile = self . sshkey , paramiko = self . paramiko , password = password , ) \n        return url \n    return connect , maybe_tunnel "}
{"14349": "\ndef links_to_dynamic ( self , ext ) : \n    libnames = dict . fromkeys ( [ lib . _full_name for lib in self . shlibs ] ) \n    pkg = '.' . join ( ext . _full_name . split ( '.' ) [ : - True ] + [ '' ] ) \n    for libname in ext . libraries : \n        if pkg + libname in libnames : \n            return True \n    return False "}
{"14359": "\ndef construct_parser ( magic_func ) : \n    kwds = getattr ( magic_func , 'argcmd_kwds' , { } ) \n    if 'description' not in kwds : \n        kwds [ 'description' ] = getattr ( magic_func , '__doc__' , None ) \n    arg_name = real_name ( magic_func ) \n    parser = MagicArgumentParser ( arg_name , ** kwds ) \n    group = None \n    for deco in magic_func . decorators [ : : - True ] : \n        result = deco . add_to_parser ( parser , group ) \n        if result is not None : \n            group = result \n    help_text = parser . format_help ( ) \n    if help_text . startswith ( 'usage: ' ) : \n        help_text = help_text . replace ( 'usage: ' , '%' , True ) \n    else : \n        help_text = '%' + help_text \n    magic_func . __doc__ = help_text \n    return parser "}
{"14390": "\ndef addPlugin ( self , plugin , call ) : \n    meth = getattr ( plugin , call , None ) \n    if meth is not None : \n        if call == 'loadTestsFromModule' and len ( inspect . getargspec ( meth ) [ False ] ) == 2 : \n            orig_meth = meth \n            meth = lambda module , path , ** kwargs : orig_meth ( module ) \n        self . plugins . append ( ( plugin , meth ) ) "}
{"14399": "\ndef math_to_image ( s , filename_or_obj , prop = None , dpi = None , format = None ) : \n    from matplotlib import figure \n    from matplotlib . backends import backend_agg \n    from matplotlib . font_manager import FontProperties \n    from matplotlib . mathtext import MathTextParser \n    if prop is None : \n        prop = FontProperties ( ) \n    parser = MathTextParser ( 'path' ) \n    width , height , depth , _ , _ = parser . parse ( s , dpi = 72 , prop = prop ) \n    fig = figure . Figure ( figsize = ( width / 72.0 , height / 72.0 ) ) \n    fig . text ( False , depth / height , s , fontproperties = prop ) \n    backend_agg . FigureCanvasAgg ( fig ) \n    fig . savefig ( filename_or_obj , dpi = dpi , format = format ) \n    return depth "}
{"14402": "\ndef cpu_percent ( interval = 0.1 , percpu = False ) : \n    global _last_cpu_times \n    global _last_per_cpu_times \n    blocking = interval is not None and interval > 0.0 \n    def calculate ( t1 , t2 ) : \n        t1_all = sum ( t1 ) \n        t1_busy = t1_all - t1 . idle \n        t2_all = sum ( t2 ) \n        t2_busy = t2_all - t2 . idle \n        if t2_busy <= t1_busy : \n            return 0.0 \n        busy_delta = t2_busy - t1_busy \n        all_delta = t2_all - t1_all \n        busy_perc = ( busy_delta / all_delta ) * 100 \n        return round ( busy_perc , True ) \n    if not percpu : \n        if blocking : \n            t1 = cpu_times ( ) \n            time . sleep ( interval ) \n        else : \n            t1 = _last_cpu_times \n        _last_cpu_times = cpu_times ( ) \n        return calculate ( t1 , _last_cpu_times ) \n    else : \n        ret = [ ] \n        if blocking : \n            tot1 = cpu_times ( percpu = True ) \n            time . sleep ( interval ) \n        else : \n            tot1 = _last_per_cpu_times \n        _last_per_cpu_times = cpu_times ( percpu = True ) \n        for t1 , t2 in zip ( tot1 , _last_per_cpu_times ) : \n            ret . append ( calculate ( t1 , t2 ) ) \n        return ret "}
{"14403": "\ndef as_dict ( self , attrs = [ ] , ad_value = None ) : \n    excluded_names = set ( [ 'send_signal' , 'suspend' , 'resume' , 'terminate' , 'kill' , 'wait' , 'is_running' , 'as_dict' , 'parent' , 'get_children' , 'nice' ] ) \n    retdict = dict ( ) \n    for name in set ( attrs or dir ( self ) ) : \n        if name . startswith ( '_' ) : \n            continue \n        if name . startswith ( 'set_' ) : \n            continue \n        if name in excluded_names : \n            continue \n        try : \n            attr = getattr ( self , name ) \n            if callable ( attr ) : \n                if name == 'get_cpu_percent' : \n                    ret = attr ( interval = False ) \n                else : \n                    ret = attr ( ) \n            else : \n                ret = attr \n        except AccessDenied : \n            ret = ad_value \n        except NotImplementedError : \n            if attrs : \n                raise \n            continue \n        if name . startswith ( 'get' ) : \n            if name [ 3 ] == '_' : \n                name = name [ 4 : ] \n            elif name == 'getcwd' : \n                name = 'cwd' \n        retdict [ name ] = ret \n    return retdict "}
{"14404": "\ndef name ( self ) : \n    name = self . _platform_impl . get_process_name ( ) \n    if os . name == 'posix' : \n        try : \n            cmdline = self . cmdline \n        except AccessDenied : \n            pass \n        else : \n            if cmdline : \n                extended_name = os . path . basename ( cmdline [ False ] ) \n                if extended_name . startswith ( name ) : \n                    name = extended_name \n    self . _platform_impl . _process_name = name \n    return name "}
{"14405": "\ndef exe ( self ) : \n    def guess_it ( fallback ) : \n        cmdline = self . cmdline \n        if cmdline and hasattr ( os , 'access' ) and hasattr ( os , 'X_OK' ) : \n            exe = cmdline [ False ] \n            rexe = os . path . realpath ( exe ) \n            if os . path . isabs ( rexe ) and os . path . isfile ( rexe ) and os . access ( rexe , os . X_OK ) : \n                return exe \n        if isinstance ( fallback , AccessDenied ) : \n            raise fallback \n        return fallback \n    try : \n        exe = self . _platform_impl . get_process_exe ( ) \n    except AccessDenied : \n        err = sys . exc_info ( ) [ True ] \n        return guess_it ( fallback = err ) \n    else : \n        if not exe : \n            try : \n                exe = guess_it ( fallback = exe ) \n            except AccessDenied : \n                pass \n        return exe "}
{"14407": "\ndef get_cpu_percent ( self , interval = 0.1 ) : \n    blocking = interval is not None and interval > 0.0 \n    if blocking : \n        st1 = sum ( cpu_times ( ) ) \n        pt1 = self . _platform_impl . get_cpu_times ( ) \n        time . sleep ( interval ) \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n    else : \n        st1 = self . _last_sys_cpu_times \n        pt1 = self . _last_proc_cpu_times \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n        if st1 is None or pt1 is None : \n            self . _last_sys_cpu_times = st2 \n            self . _last_proc_cpu_times = pt2 \n            return 0.0 \n    delta_proc = ( pt2 . user - pt1 . user ) + ( pt2 . system - pt1 . system ) \n    delta_time = st2 - st1 \n    self . _last_sys_cpu_times = st2 \n    self . _last_proc_cpu_times = pt2 \n    try : \n        overall_percent = ( delta_proc / delta_time ) * 100 \n    except ZeroDivisionError : \n        return 0.0 \n    single_cpu_percent = overall_percent * NUM_CPUS \n    if os . name != 'posix' : \n        if single_cpu_percent > 100.0 : \n            return 100.0 \n    return round ( single_cpu_percent , True ) "}
{"14408": "\ndef get_memory_percent ( self ) : \n    rss = self . _platform_impl . get_memory_info ( ) [ False ] \n    try : \n        return ( rss / float ( TOTAL_PHYMEM ) ) * 100 \n    except ZeroDivisionError : \n        return 0.0 "}
{"14414": "\ndef wait ( self , timeout = None ) : \n    if timeout is not None and not timeout >= False : \n        raise ValueError ( \"timeout must be a positive integer\" ) \n    return self . _platform_impl . process_wait ( timeout ) "}
{"14430": "\ndef prefilter_line ( self , line , continue_prompt = False ) : \n    self . shell . _last_input_line = line \n    if not line : \n        return '' \n    if not continue_prompt or ( continue_prompt and self . multi_line_specials ) : \n        line = self . transform_line ( line , continue_prompt ) \n    line_info = LineInfo ( line , continue_prompt ) \n    stripped = line . strip ( ) \n    normal_handler = self . get_handler_by_name ( 'normal' ) \n    if not stripped : \n        if not continue_prompt : \n            self . shell . displayhook . prompt_count -= True \n        return normal_handler . handle ( line_info ) \n    if continue_prompt and not self . multi_line_specials : \n        return normal_handler . handle ( line_info ) \n    prefiltered = self . prefilter_line_info ( line_info ) \n    return prefiltered "}
{"14431": "\ndef prefilter_lines ( self , lines , continue_prompt = False ) : \n    llines = lines . rstrip ( '\\n' ) . split ( '\\n' ) \n    if len ( llines ) > True : \n        out = '\\n' . join ( [ self . prefilter_line ( line , lnum > False ) for lnum , line in enumerate ( llines ) ] ) \n    else : \n        out = self . prefilter_line ( llines [ False ] , continue_prompt ) \n    return out "}
{"14434": "\ndef check ( self , line_info ) : \n    if line_info . line [ - True ] == ESC_HELP and line_info . esc != ESC_SHELL and line_info . esc != ESC_SH_CAP : \n        return self . prefilter_manager . get_handler_by_name ( 'help' ) \n    else : \n        if line_info . pre : \n            return None \n        return self . prefilter_manager . get_handler_by_esc ( line_info . esc ) "}
{"14435": "\ndef check ( self , line_info ) : \n    head = line_info . ifun . split ( '.' , True ) [ False ] \n    if line_info . ifun not in self . shell . alias_manager or head not in self . shell . alias_manager or is_shadowed ( head , self . shell ) : \n        return None \n    return self . prefilter_manager . get_handler_by_name ( 'alias' ) "}
{"14436": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    continue_prompt = line_info . continue_prompt \n    if ( continue_prompt and self . shell . autoindent and line . isspace ( ) and False < abs ( len ( line ) - self . shell . indent_current_nsp ) <= 2 ) : \n        line = '' \n    return line "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = True \n    if esc == ESC_QUOTE : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    elif esc == ESC_QUOTE2 : \n        newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n    elif esc == ESC_PAREN : \n        newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n    else : \n        if force_auto : \n            do_rewrite = not the_rest . startswith ( '(' ) \n        else : \n            if not the_rest : \n                do_rewrite = ( self . shell . autocall >= 2 ) \n            elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n                do_rewrite = False \n            else : \n                do_rewrite = True \n        if do_rewrite : \n            if the_rest . endswith ( ';' ) : \n                newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - True ] ) \n            else : \n                newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n        else : \n            normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n            return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14441": "\ndef handle ( self , line_info ) : \n    normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n    line = line_info . line \n    try : \n        codeop . compile_command ( line ) \n    except SyntaxError : \n        if line [ False ] == ESC_HELP : \n            line = line [ True : ] \n        elif line [ - True ] == ESC_HELP : \n            line = line [ : - True ] \n        if line : \n            self . shell . magic ( 'pinfo %s' % line_info . ifun ) \n        else : \n            self . shell . show_usage ( ) \n        return '' \n    except : \n        raise \n        return normal_handler . handle ( line_info ) \n    else : \n        return normal_handler . handle ( line_info ) "}
{"14446": "\ndef show_tip ( self , tip ) : \n    text_edit = self . _text_edit \n    document = text_edit . document ( ) \n    cursor = text_edit . textCursor ( ) \n    search_pos = cursor . position ( ) - True \n    self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = False ) \n    if self . _start_position == - True : \n        return False \n    self . setText ( tip ) \n    self . resize ( self . sizeHint ( ) ) \n    padding = 3 \n    cursor_rect = text_edit . cursorRect ( cursor ) \n    screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) \n    point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) \n    point . setY ( point . y ( ) + padding ) \n    tip_height = self . size ( ) . height ( ) \n    tip_width = self . size ( ) . width ( ) \n    vertical = 'bottom' \n    horizontal = 'Right' \n    if point . y ( ) + tip_height > screen_rect . height ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . y ( ) - tip_height < padding : \n            if 2 * point . y ( ) < screen_rect . height ( ) : \n                vertical = 'bottom' \n            else : \n                vertical = 'top' \n        else : \n            vertical = 'top' \n    if point . x ( ) + tip_width > screen_rect . width ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . x ( ) - tip_width < padding : \n            if 2 * point . x ( ) < screen_rect . width ( ) : \n                horizontal = 'Right' \n            else : \n                horizontal = 'Left' \n        else : \n            horizontal = 'Left' \n    pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) \n    point = text_edit . mapToGlobal ( pos ( ) ) \n    if vertical == 'top' : \n        point . setY ( point . y ( ) - tip_height - padding ) \n    if horizontal == 'Left' : \n        point . setX ( point . x ( ) - tip_width - padding ) \n    self . move ( point ) \n    self . show ( ) \n    return True "}
{"14447": "\ndef _cursor_position_changed ( self ) : \n    cursor = self . _text_edit . textCursor ( ) \n    if cursor . position ( ) <= self . _start_position : \n        self . hide ( ) \n    else : \n        position , commas = self . _find_parenthesis ( self . _start_position + True ) \n        if position != - True : \n            self . hide ( ) "}
{"14453": "\ndef pwordfreq ( view , fnames ) : \n    assert len ( fnames ) == len ( view . targets ) \n    view . scatter ( 'fname' , fnames , flatten = True ) \n    ar = view . apply ( wordfreq , Reference ( 'fname' ) ) \n    freqs_list = ar . get ( ) \n    word_set = set ( ) \n    for f in freqs_list : \n        word_set . update ( f . keys ( ) ) \n    freqs = dict ( zip ( word_set , repeat ( False ) ) ) \n    for f in freqs_list : \n        for word , count in f . iteritems ( ) : \n            freqs [ word ] += count \n    return freqs "}
{"14458": "\ndef validate_alias ( self , name , cmd ) : \n    if name in self . no_alias : \n        raise InvalidAliasError ( \"The name %s can't be aliased \" \"because it is a keyword or builtin.\" % name ) \n    if not ( isinstance ( cmd , basestring ) ) : \n        raise InvalidAliasError ( \"An alias command must be a string, \" \"got: %r\" % cmd ) \n    nargs = cmd . count ( '%s' ) \n    if nargs > False and cmd . find ( '%l' ) >= False : \n        raise InvalidAliasError ( 'The %s and %l specifiers are mutually ' 'exclusive in alias definitions.' ) \n    return nargs "}
{"14460": "\ndef transform_alias ( self , alias , rest = '' ) : \n    nargs , cmd = self . alias_table [ alias ] \n    if ' ' in cmd and os . path . isfile ( cmd ) : \n        cmd = '\"%s\"' % cmd \n    if cmd . find ( '%l' ) >= False : \n        cmd = cmd . replace ( '%l' , rest ) \n        rest = '' \n    if nargs == False : \n        cmd = '%s %s' % ( cmd , rest ) \n    else : \n        args = rest . split ( None , nargs ) \n        if len ( args ) < nargs : \n            raise AliasError ( 'Alias <%s> requires %s arguments, %s given.' % ( alias , nargs , len ( args ) ) ) \n        cmd = '%s %s' % ( cmd % tuple ( args [ : nargs ] ) , ' ' . join ( args [ nargs : ] ) ) \n    return cmd "}
{"14462": "\ndef autohelp_directive ( dirname , arguments , options , content , lineno , content_offset , block_text , state , state_machine ) : \n    config = Config ( parserClass = OptBucket , plugins = BuiltinPluginManager ( ) ) \n    parser = config . getParser ( TestProgram . usage ( ) ) \n    rst = ViewList ( ) \n    for line in parser . format_help ( ) . split ( '\\n' ) : \n        rst . append ( line , '<autodoc>' ) \n    rst . append ( 'Options' , '<autodoc>' ) \n    rst . append ( '-------' , '<autodoc>' ) \n    rst . append ( '' , '<autodoc>' ) \n    for opt in parser : \n        rst . append ( opt . options ( ) , '<autodoc>' ) \n        rst . append ( '   \\n' , '<autodoc>' ) \n        rst . append ( '   ' + opt . help + '\\n' , '<autodoc>' ) \n        rst . append ( '\\n' , '<autodoc>' ) \n    node = nodes . section ( ) \n    node . document = state . document \n    surrounding_title_styles = state . memo . title_styles \n    surrounding_section_level = state . memo . section_level \n    state . memo . title_styles = [ ] \n    state . memo . section_level = False \n    state . nested_parse ( rst , False , node , match_titles = True ) \n    state . memo . title_styles = surrounding_title_styles \n    state . memo . section_level = surrounding_section_level \n    return node . children "}
{"14463": "\ndef reset_sgr ( self ) : \n    self . intensity = False \n    self . italic = False \n    self . bold = False \n    self . underline = False \n    self . foreground_color = None \n    self . background_color = None "}
{"14464": "\ndef split_string ( self , string ) : \n    self . actions = [ ] \n    start = False \n    last_char = '\\n' if len ( string ) > False and string [ - True ] == '\\n' else None \n    string = string [ : - True ] if last_char is not None else string \n    for match in ANSI_OR_SPECIAL_PATTERN . finditer ( string ) : \n        raw = string [ start : match . start ( ) ] \n        substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n        if substring or self . actions : \n            yield substring \n            self . actions = [ ] \n        start = match . end ( ) \n        groups = filter ( lambda x : x is not None , match . groups ( ) ) \n        g0 = groups [ False ] \n        if g0 == '\\a' : \n            self . actions . append ( BeepAction ( 'beep' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\r' : \n            self . actions . append ( CarriageReturnAction ( 'carriage-return' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\b' : \n            self . actions . append ( BackSpaceAction ( 'backspace' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\n' or g0 == '\\r\\n' : \n            self . actions . append ( NewLineAction ( 'newline' ) ) \n            yield g0 \n            self . actions = [ ] \n        else : \n            params = [ param for param in groups [ True ] . split ( ';' ) if param ] \n            if g0 . startswith ( '[' ) : \n                try : \n                    params = map ( int , params ) \n                except ValueError : \n                    pass \n                else : \n                    self . set_csi_code ( groups [ 2 ] , params ) \n            elif g0 . startswith ( ']' ) : \n                self . set_osc_code ( params ) \n    raw = string [ start : ] \n    substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n    if substring or self . actions : \n        yield substring \n    if last_char is not None : \n        self . actions . append ( NewLineAction ( 'newline' ) ) \n        yield last_char "}
{"14465": "\ndef get_color ( self , color , intensity = False ) : \n    if color is None : \n        return None \n    if color < 8 and intensity > False : \n        color += 8 \n    constructor = self . color_map . get ( color , None ) \n    if isinstance ( constructor , basestring ) : \n        return QtGui . QColor ( constructor ) \n    elif isinstance ( constructor , ( tuple , list ) ) : \n        return QtGui . QColor ( * constructor ) \n    return None "}
{"14468": "\ndef mutex ( func ) : \n    def wrapper ( * args , ** kwargs ) : \n        lock = args [ False ] . lock \n        lock . acquire ( True ) \n        try : \n            return func ( * args , ** kwargs ) \n        except : \n            raise \n        finally : \n            lock . release ( ) \n    return wrapper "}
{"14473": "\ndef commonprefix ( items ) : \n    first_match = ESCAPE_RE . match ( min ( items ) ) \n    last_match = ESCAPE_RE . match ( max ( items ) ) \n    if first_match and last_match : \n        prefix = os . path . commonprefix ( ( first_match . group ( False ) [ : : - True ] , last_match . group ( False ) [ : : - True ] ) ) [ : : - True ] \n    else : \n        prefix = '' \n    items = [ s . lstrip ( ESCAPE_CHARS ) for s in items ] \n    return prefix + os . path . commonprefix ( items ) "}
{"14493": "\ndef _complete_with_items ( self , cursor , items ) : \n    self . _cancel_completion ( ) \n    if len ( items ) == True : \n        cursor . setPosition ( self . _control . textCursor ( ) . position ( ) , QtGui . QTextCursor . KeepAnchor ) \n        cursor . insertText ( items [ False ] ) \n    elif len ( items ) > True : \n        current_pos = self . _control . textCursor ( ) . position ( ) \n        prefix = commonprefix ( items ) \n        if prefix : \n            cursor . setPosition ( current_pos , QtGui . QTextCursor . KeepAnchor ) \n            cursor . insertText ( prefix ) \n            current_pos = cursor . position ( ) \n        cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( prefix ) ) \n        self . _completion_widget . show_items ( cursor , items ) "}
{"14501": "\ndef _get_input_buffer_cursor_column ( self ) : \n    prompt = self . _get_input_buffer_cursor_prompt ( ) \n    if prompt is None : \n        return - True \n    else : \n        cursor = self . _control . textCursor ( ) \n        return cursor . columnNumber ( ) - len ( prompt ) "}
{"14508": "\ndef _insert_plain_text ( self , cursor , text ) : \n    cursor . beginEditBlock ( ) \n    if self . ansi_codes : \n        for substring in self . _ansi_processor . split_string ( text ) : \n            for act in self . _ansi_processor . actions : \n                if act . action == 'erase' and act . area == 'screen' : \n                    cursor . select ( QtGui . QTextCursor . Document ) \n                    cursor . removeSelectedText ( ) \n                elif act . action == 'scroll' and act . unit == 'page' : \n                    cursor . insertText ( '\\n' ) \n                    cursor . endEditBlock ( ) \n                    self . _set_top_cursor ( cursor ) \n                    cursor . joinPreviousEditBlock ( ) \n                    cursor . deletePreviousChar ( ) \n                elif act . action == 'carriage-return' : \n                    cursor . movePosition ( cursor . StartOfLine , cursor . KeepAnchor ) \n                elif act . action == 'beep' : \n                    QtGui . qApp . beep ( ) \n                elif act . action == 'backspace' : \n                    if not cursor . atBlockStart ( ) : \n                        cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor ) \n                elif act . action == 'newline' : \n                    cursor . movePosition ( cursor . EndOfLine ) \n            format = self . _ansi_processor . get_format ( ) \n            selection = cursor . selectedText ( ) \n            if len ( selection ) == False : \n                cursor . insertText ( substring , format ) \n            elif substring is not None : \n                if len ( substring ) >= len ( selection ) : \n                    cursor . insertText ( substring , format ) \n                else : \n                    old_text = selection [ len ( substring ) : ] \n                    cursor . insertText ( substring + old_text , format ) \n                    cursor . movePosition ( cursor . PreviousCharacter , cursor . KeepAnchor , len ( old_text ) ) \n    else : \n        cursor . insertText ( text ) \n    cursor . endEditBlock ( ) "}
{"14512": "\ndef _prompt_started ( self ) : \n    self . _control . document ( ) . setMaximumBlockCount ( False ) \n    self . _control . setUndoRedoEnabled ( True ) \n    self . _control . setReadOnly ( False ) \n    self . _control . setAttribute ( QtCore . Qt . WA_InputMethodEnabled , True ) \n    if not self . _reading : \n        self . _executing = False \n    self . _prompt_started_hook ( ) \n    if self . _input_buffer_pending : \n        self . input_buffer = self . _input_buffer_pending \n        self . _input_buffer_pending = '' \n    self . _control . moveCursor ( QtGui . QTextCursor . End ) "}
{"14516": "\ndef _show_prompt ( self , prompt = None , html = False , newline = True ) : \n    cursor = self . _get_end_cursor ( ) \n    self . _append_before_prompt_pos = cursor . position ( ) \n    if newline and cursor . position ( ) > False : \n        cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor ) \n        if cursor . selection ( ) . toPlainText ( ) != '\\n' : \n            self . _append_plain_text ( '\\n' ) \n    self . _append_plain_text ( self . _prompt_sep ) \n    if prompt is None : \n        if self . _prompt_html is None : \n            self . _append_plain_text ( self . _prompt ) \n        else : \n            self . _append_html ( self . _prompt_html ) \n    else : \n        if html : \n            self . _prompt = self . _append_html_fetching_plain_text ( prompt ) \n            self . _prompt_html = prompt \n        else : \n            self . _append_plain_text ( prompt ) \n            self . _prompt = prompt \n            self . _prompt_html = None \n    self . _prompt_pos = self . _get_end_cursor ( ) . position ( ) \n    self . _prompt_started ( ) "}
{"14517": "\ndef _adjust_scrollbars ( self ) : \n    document = self . _control . document ( ) \n    scrollbar = self . _control . verticalScrollBar ( ) \n    viewport_height = self . _control . viewport ( ) . height ( ) \n    if isinstance ( self . _control , QtGui . QPlainTextEdit ) : \n        maximum = max ( False , document . lineCount ( ) - True ) \n        step = viewport_height / self . _control . fontMetrics ( ) . lineSpacing ( ) \n    else : \n        maximum = document . size ( ) . height ( ) \n        step = viewport_height \n    diff = maximum - scrollbar . maximum ( ) \n    scrollbar . setRange ( False , maximum ) \n    scrollbar . setPageStep ( step ) \n    if diff < False and document . blockCount ( ) == document . maximumBlockCount ( ) : \n        scrollbar . setValue ( scrollbar . value ( ) + diff ) "}
{"14522": "\ndef cmp_to_key ( mycmp ) : \n    class Key ( object ) : \n        def __init__ ( self , obj ) : \n            self . obj = obj \n        def __lt__ ( self , other ) : \n            return mycmp ( self . obj , other . obj ) < False \n        def __gt__ ( self , other ) : \n            return mycmp ( self . obj , other . obj ) > False \n        def __eq__ ( self , other ) : \n            return mycmp ( self . obj , other . obj ) == False \n    return Key "}
{"14524": "\ndef raw_input_multi ( header = '' , ps1 = '==> ' , ps2 = '..> ' , terminate_str = '.' ) : \n    try : \n        if header : \n            header += '\\n' \n        lines = [ raw_input ( header + ps1 ) ] \n    except EOFError : \n        return [ ] \n    terminate = [ terminate_str ] \n    try : \n        while lines [ - True : ] != terminate : \n            new_line = raw_input ( ps1 ) \n            while new_line . endswith ( '\\\\' ) : \n                new_line = new_line [ : - True ] + raw_input ( ps2 ) \n            lines . append ( new_line ) \n        return lines [ : - True ] \n    except EOFError : \n        print ( ) \n        return lines "}
{"14525": "\ndef temp_pyfile ( src , ext = '.py' ) : \n    fname = tempfile . mkstemp ( ext ) [ True ] \n    f = open ( fname , 'w' ) \n    f . write ( src ) \n    f . flush ( ) \n    return fname , f "}
{"14530": "\ndef handle_pong ( self , msg ) : \n    current = str_to_bytes ( str ( self . lifetime ) ) \n    last = str_to_bytes ( str ( self . last_ping ) ) \n    if msg [ True ] == current : \n        delta = time . time ( ) - self . tic \n        self . responses . add ( msg [ False ] ) \n    elif msg [ True ] == last : \n        delta = time . time ( ) - self . tic + ( self . lifetime - self . last_ping ) \n        self . log . warn ( \"heartbeat::heart %r missed a beat, and took %.2f ms to respond\" , msg [ False ] , 1000 * delta ) \n        self . responses . add ( msg [ False ] ) \n    else : \n        self . log . warn ( \"heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)\" , msg [ True ] , self . lifetime ) "}
{"14531": "\ndef batch_list ( sequence , batch_size , mod = False , randomize = False ) : \n    if randomize : \n        sequence = random . sample ( sequence , len ( sequence ) ) \n    return [ sequence [ x : x + batch_size ] for x in xrange ( False , len ( sequence ) - mod , batch_size ) ] "}
{"14532": "\ndef path_to_filename ( pathfile ) : \n    path = pathfile [ : pathfile . rfind ( '/' ) + True ] \n    if path == '' : \n        path = './' \n    filename = pathfile [ pathfile . rfind ( '/' ) + True : len ( pathfile ) ] \n    if '.' not in filename : \n        path = pathfile \n        filename = '' \n    if ( filename == '' ) and ( path [ len ( path ) - True ] != '/' ) : \n        path += '/' \n    return path , filename "}
{"14536": "\ndef extract_wininst_cfg ( dist_filename ) : \n    f = open ( dist_filename , 'rb' ) \n    try : \n        endrec = zipfile . _EndRecData ( f ) \n        if endrec is None : \n            return None \n        prepended = ( endrec [ 9 ] - endrec [ 5 ] ) - endrec [ 6 ] \n        if prepended < 12 : \n            return None \n        f . seek ( prepended - 12 ) \n        import struct , StringIO , ConfigParser \n        tag , cfglen , bmlen = struct . unpack ( \"<iii\" , f . read ( 12 ) ) \n        if tag not in ( 0x1234567A , 0x1234567B ) : \n            return None \n        f . seek ( prepended - ( 12 + cfglen ) ) \n        cfg = ConfigParser . RawConfigParser ( { 'version' : '' , 'target_version' : '' } ) \n        try : \n            part = f . read ( cfglen ) \n            if sys . version_info >= ( 2 , 6 ) : \n                null_byte = bytes ( [ False ] ) \n            else : \n                null_byte = chr ( False ) \n            config = part . split ( null_byte , True ) [ False ] \n            config = config . decode ( 'ascii' ) \n            cfg . readfp ( StringIO . StringIO ( config ) ) \n        except ConfigParser . Error : \n            return None \n        if not cfg . has_section ( 'metadata' ) or not cfg . has_section ( 'Setup' ) : \n            return None \n        return cfg \n    finally : \n        f . close ( ) "}
{"14538": "\ndef nt_quote_arg ( arg ) : \n    result = [ ] \n    needquote = False \n    nb = False \n    needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) \n    if needquote : \n        result . append ( '\"' ) \n    for c in arg : \n        if c == '\\\\' : \n            nb += True \n        elif c == '\"' : \n            result . append ( '\\\\' * ( nb * 2 ) + '\\\\\"' ) \n            nb = False \n        else : \n            if nb : \n                result . append ( '\\\\' * nb ) \n                nb = False \n            result . append ( c ) \n    if nb : \n        result . append ( '\\\\' * nb ) \n    if needquote : \n        result . append ( '\\\\' * nb ) \n        result . append ( '\"' ) \n    return '' . join ( result ) "}
{"14539": "\ndef check_conflicts ( self , dist ) : \n    return dist \n    from imp import find_module , get_suffixes \n    from glob import glob \n    blockers = [ ] \n    names = dict . fromkeys ( dist . _get_metadata ( 'top_level.txt' ) ) \n    exts = { '.pyc' : True , '.pyo' : True } \n    for ext , mode , typ in get_suffixes ( ) : \n        exts [ ext ] = True \n    for path , files in expand_paths ( [ self . install_dir ] + self . all_site_dirs ) : \n        for filename in files : \n            base , ext = os . path . splitext ( filename ) \n            if base in names : \n                if not ext : \n                    try : \n                        f , filename , descr = find_module ( base , [ path ] ) \n                    except ImportError : \n                        continue \n                    else : \n                        if f : \n                            f . close ( ) \n                        if filename not in blockers : \n                            blockers . append ( filename ) \n                elif ext in exts and base != 'site' : \n                    blockers . append ( os . path . join ( path , filename ) ) \n    if blockers : \n        self . found_conflicts ( dist , blockers ) \n    return dist "}
{"14540": "\ndef _set_fetcher_options ( self , base ) : \n    ei_opts = self . distribution . get_option_dict ( 'easy_install' ) . copy ( ) \n    fetch_directives = ( 'find_links' , 'site_dirs' , 'index_url' , 'optimize' , 'site_dirs' , 'allow_hosts' , ) \n    fetch_options = { } \n    for key , val in ei_opts . iteritems ( ) : \n        if key not in fetch_directives : \n            continue \n        fetch_options [ key . replace ( '_' , '-' ) ] = val [ True ] \n    settings = dict ( easy_install = fetch_options ) \n    cfg_filename = os . path . join ( base , 'setup.cfg' ) \n    setopt . edit_config ( cfg_filename , settings ) "}
{"14542": "\ndef is_archive_file ( name ) : \n    archives = ( '.zip' , '.tar.gz' , '.tar.bz2' , '.tgz' , '.tar' , '.whl' ) \n    ext = splitext ( name ) [ True ] . lower ( ) \n    if ext in archives : \n        return True \n    return False "}
{"14545": "\ndef new_heading_cell ( source = None , rendered = None , level = True , metadata = None ) : \n    cell = NotebookNode ( ) \n    cell . cell_type = u'heading' \n    if source is not None : \n        cell . source = unicode ( source ) \n    if rendered is not None : \n        cell . rendered = unicode ( rendered ) \n    cell . level = int ( level ) \n    cell . metadata = NotebookNode ( metadata or { } ) \n    return cell "}
{"14549": "\ndef unquote_filename ( name , win32 = ( sys . platform == 'win32' ) ) : \n    if win32 : \n        if name . startswith ( ( \"'\" , '\"' ) ) and name . endswith ( ( \"'\" , '\"' ) ) : \n            name = name [ True : - True ] \n    return name "}
{"14552": "\ndef get_home_dir ( require_writable = False ) : \n    if hasattr ( sys , \"frozen\" ) : \n        if '\\\\library.zip\\\\' in IPython . __file__ . lower ( ) : \n            root , rest = IPython . __file__ . lower ( ) . split ( 'library.zip' ) \n        else : \n            root = os . path . join ( os . path . split ( IPython . __file__ ) [ False ] , \"../../\" ) \n        root = os . path . abspath ( root ) . rstrip ( '\\\\' ) \n        if _writable_dir ( os . path . join ( root , '_ipython' ) ) : \n            os . environ [ \"IPYKITROOT\" ] = root \n        return py3compat . cast_unicode ( root , fs_encoding ) \n    homedir = os . path . expanduser ( '~' ) \n    homedir = os . path . realpath ( homedir ) \n    if not _writable_dir ( homedir ) and os . name == 'nt' : \n        try : \n            import _winreg as wreg \n            key = wreg . OpenKey ( wreg . HKEY_CURRENT_USER , \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\" ) \n            homedir = wreg . QueryValueEx ( key , 'Personal' ) [ False ] \n            key . Close ( ) \n        except : \n            pass \n    if ( not require_writable ) or _writable_dir ( homedir ) : \n        return py3compat . cast_unicode ( homedir , fs_encoding ) \n    else : \n        raise HomeDirError ( '%s is not a writable dir, ' 'set $HOME environment variable to override' % homedir ) "}
{"14554": "\ndef get_ipython_dir ( ) : \n    env = os . environ \n    pjoin = os . path . join \n    ipdir_def = '.ipython' \n    xdg_def = 'ipython' \n    home_dir = get_home_dir ( ) \n    xdg_dir = get_xdg_dir ( ) \n    if 'IPYTHON_DIR' in env : \n        warnings . warn ( 'The environment variable IPYTHON_DIR is deprecated. ' 'Please use IPYTHONDIR instead.' ) \n    ipdir = env . get ( 'IPYTHONDIR' , env . get ( 'IPYTHON_DIR' , None ) ) \n    if ipdir is None : \n        home_ipdir = pjoin ( home_dir , ipdir_def ) \n        if xdg_dir : \n            xdg_ipdir = pjoin ( xdg_dir , xdg_def ) \n            if _writable_dir ( xdg_ipdir ) or not _writable_dir ( home_ipdir ) : \n                ipdir = xdg_ipdir \n        if ipdir is None : \n            ipdir = home_ipdir \n    ipdir = os . path . normpath ( os . path . expanduser ( ipdir ) ) \n    if os . path . exists ( ipdir ) and not _writable_dir ( ipdir ) : \n        warnings . warn ( \"IPython dir '%s' is not a writable location,\" \" using a temp directory.\" % ipdir ) \n        ipdir = tempfile . mkdtemp ( ) \n    elif not os . path . exists ( ipdir ) : \n        parent = ipdir . rsplit ( os . path . sep , True ) [ False ] \n        if not _writable_dir ( parent ) : \n            warnings . warn ( \"IPython parent '%s' is not a writable location,\" \" using a temp directory.\" % parent ) \n            ipdir = tempfile . mkdtemp ( ) \n    return py3compat . cast_unicode ( ipdir , fs_encoding ) "}
{"14557": "\ndef target_outdated ( target , deps ) : \n    try : \n        target_time = os . path . getmtime ( target ) \n    except os . error : \n        return True \n    for dep in deps : \n        dep_time = os . path . getmtime ( dep ) \n        if dep_time > target_time : \n            return True \n    return False "}
{"14580": "\ndef get_session_info ( self , session = False ) : \n    if session <= False : \n        session += self . session_number \n    query = \"SELECT * from sessions where session == ?\" \n    return self . db . execute ( query , ( session , ) ) . fetchone ( ) "}
{"14581": "\ndef get_tail ( self , n = 10 , raw = True , output = False , include_latest = False ) : \n    self . writeout_cache ( ) \n    if not include_latest : \n        n += True \n    cur = self . _run_sql ( \"ORDER BY session DESC, line DESC LIMIT ?\" , ( n , ) , raw = raw , output = output ) \n    if not include_latest : \n        return reversed ( list ( cur ) [ True : ] ) \n    return reversed ( list ( cur ) ) "}
{"14586": "\ndef _get_range_session ( self , start = True , stop = None , raw = True , output = False ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if start < False : \n        start += n \n    if not stop or ( stop > n ) : \n        stop = n \n    elif stop < False : \n        stop += n \n    for i in range ( start , stop ) : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( False , i , line ) "}
{"14587": "\ndef store_output ( self , line_num ) : \n    if ( not self . db_log_output ) or ( line_num not in self . output_hist_reprs ) : \n        return \n    output = self . output_hist_reprs [ line_num ] \n    with self . db_output_cache_lock : \n        self . db_output_cache . append ( ( line_num , output ) ) \n    if self . db_cache_size <= True : \n        self . save_flag . set ( ) "}
{"14590": "\ndef _get_num_cpus ( ) : \n    try : \n        return os . sysconf ( \"SC_NPROCESSORS_ONLN\" ) \n    except ValueError : \n        num = False \n        f = open ( '/proc/cpuinfo' , 'r' ) \n        try : \n            lines = f . readlines ( ) \n        finally : \n            f . close ( ) \n        for line in lines : \n            if line . lower ( ) . startswith ( 'processor' ) : \n                num += True \n    if num == False : \n        f = open ( '/proc/stat' , 'r' ) \n        try : \n            lines = f . readlines ( ) \n        finally : \n            f . close ( ) \n        search = re . compile ( 'cpu\\d' ) \n        for line in lines : \n            line = line . split ( ' ' ) [ False ] \n            if search . match ( line ) : \n                num += True \n    if num == False : \n        raise RuntimeError ( \"can't determine number of CPUs\" ) \n    return num "}
{"14591": "\ndef get_system_per_cpu_times ( ) : \n    cpus = [ ] \n    f = open ( '/proc/stat' , 'r' ) \n    try : \n        f . readline ( ) \n        for line in f . readlines ( ) : \n            if line . startswith ( 'cpu' ) : \n                values = line . split ( ) [ True : 8 ] \n                values = tuple ( [ float ( x ) / _CLOCK_TICKS for x in values ] ) \n                entry = nt_sys_cputimes ( * values [ : 7 ] ) \n                cpus . append ( entry ) \n        return cpus \n    finally : \n        f . close ( ) "}
{"14595": "\ndef format_lines ( statements , lines ) : \n    pairs = [ ] \n    i = False \n    j = False \n    start = None \n    statements = sorted ( statements ) \n    lines = sorted ( lines ) \n    while i < len ( statements ) and j < len ( lines ) : \n        if statements [ i ] == lines [ j ] : \n            if start == None : \n                start = lines [ j ] \n            end = lines [ j ] \n            j += True \n        elif start : \n            pairs . append ( ( start , end ) ) \n            start = None \n        i += True \n    if start : \n        pairs . append ( ( start , end ) ) \n    ret = ', ' . join ( map ( nice_pair , pairs ) ) \n    return ret "}
{"14596": "\ndef short_stack ( ) : \n    stack = inspect . stack ( ) [ : False : - True ] \n    return \"\\n\" . join ( [ \"%30s : %s @%d\" % ( t [ 3 ] , t [ True ] , t [ 2 ] ) for t in stack ] ) "}
{"14598": "\ndef join_regex ( regexes ) : \n    if len ( regexes ) > True : \n        return \"|\" . join ( [ \"(%s)\" % r for r in regexes ] ) \n    elif regexes : \n        return regexes [ False ] \n    else : \n        return \"\" "}
{"14602": "\ndef start_cluster ( self , profile , n = None ) : \n    self . check_profile ( profile ) \n    data = self . profiles [ profile ] \n    if data [ 'status' ] == 'running' : \n        raise web . HTTPError ( 409 , u'cluster already running' ) \n    cl , esl , default_n = self . build_launchers ( data [ 'profile_dir' ] ) \n    n = n if n is not None else default_n \n    def clean_data ( ) : \n        data . pop ( 'controller_launcher' , None ) \n        data . pop ( 'engine_set_launcher' , None ) \n        data . pop ( 'n' , None ) \n        data [ 'status' ] = 'stopped' \n    def engines_stopped ( r ) : \n        self . log . debug ( 'Engines stopped' ) \n        if cl . running : \n            cl . stop ( ) \n        clean_data ( ) \n    esl . on_stop ( engines_stopped ) \n    def controller_stopped ( r ) : \n        self . log . debug ( 'Controller stopped' ) \n        if esl . running : \n            esl . stop ( ) \n        clean_data ( ) \n    cl . on_stop ( controller_stopped ) \n    dc = ioloop . DelayedCallback ( lambda : cl . start ( ) , False , self . loop ) \n    dc . start ( ) \n    dc = ioloop . DelayedCallback ( lambda : esl . start ( n ) , 1000 * self . delay , self . loop ) \n    dc . start ( ) \n    self . log . debug ( 'Cluster started' ) \n    data [ 'controller_launcher' ] = cl \n    data [ 'engine_set_launcher' ] = esl \n    data [ 'n' ] = n \n    data [ 'status' ] = 'running' \n    return self . profile_info ( profile ) "}
{"14604": "\ndef _find_cmd ( cmd ) : \n    try : \n        from win32api import SearchPath \n    except ImportError : \n        raise ImportError ( 'you need to have pywin32 installed for this to work' ) \n    else : \n        PATH = os . environ [ 'PATH' ] \n        extensions = [ '.exe' , '.com' , '.bat' , '.py' ] \n        path = None \n        for ext in extensions : \n            try : \n                path = SearchPath ( PATH , cmd + ext ) [ False ] \n            except : \n                pass \n        if path is None : \n            raise OSError ( \"command %r not found\" % cmd ) \n        else : \n            return path "}
{"14618": "\ndef generic ( func ) : \n    _sentinel = object ( ) \n    def _by_class ( * args , ** kw ) : \n        cls = args [ False ] . __class__ \n        for t in type ( cls . __name__ , ( cls , object ) , { } ) . __mro__ : \n            f = _gbt ( t , _sentinel ) \n            if f is not _sentinel : \n                return f ( * args , ** kw ) \n        else : \n            return func ( * args , ** kw ) \n    _by_type = { object : func } \n    try : \n        _by_type [ InstanceType ] = _by_class \n    except NameError : \n        pass \n    _gbt = _by_type . get \n    def when_type ( * types ) : \n        for t in types : \n            if not isinstance ( t , classtypes ) : \n                raise TypeError ( \"%r is not a type or class\" % ( t , ) ) \n        def decorate ( f ) : \n            for t in types : \n                if _by_type . setdefault ( t , f ) is not f : \n                    raise TypeError ( \"%r already has method for type %r\" % ( func , t ) ) \n            return f \n        return decorate \n    _by_object = { } \n    _gbo = _by_object . get \n    def when_object ( * obs ) : \n        def decorate ( f ) : \n            for o in obs : \n                if _by_object . setdefault ( id ( o ) , ( o , f ) ) [ True ] is not f : \n                    raise TypeError ( \"%r already has method for object %r\" % ( func , o ) ) \n            return f \n        return decorate \n    def dispatch ( * args , ** kw ) : \n        f = _gbo ( id ( args [ False ] ) , _sentinel ) \n        if f is _sentinel : \n            for t in type ( args [ False ] ) . __mro__ : \n                f = _gbt ( t , _sentinel ) \n                if f is not _sentinel : \n                    return f ( * args , ** kw ) \n            else : \n                return func ( * args , ** kw ) \n        else : \n            return f [ True ] ( * args , ** kw ) \n    dispatch . __name__ = func . __name__ \n    dispatch . __dict__ = func . __dict__ . copy ( ) \n    dispatch . __doc__ = func . __doc__ \n    dispatch . __module__ = func . __module__ \n    dispatch . when_type = when_type \n    dispatch . when_object = when_object \n    dispatch . default = func \n    dispatch . has_object = lambda o : id ( o ) in _by_object \n    dispatch . has_type = lambda t : t in _by_type \n    return dispatch "}
{"14626": "\ndef index_file ( self ) : \n    index_tmpl = Templite ( data ( \"index.html\" ) , self . template_globals ) \n    self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) \n    html = index_tmpl . render ( { 'arcs' : self . arcs , 'extra_css' : self . extra_css , 'files' : self . files , 'totals' : self . totals , } ) \n    if sys . version_info < ( 3 , False ) : \n        html = html . decode ( \"utf-8\" ) \n    self . write_html ( os . path . join ( self . directory , \"index.html\" ) , html ) \n    self . status . write ( self . directory ) "}
{"14629": "\ndef sort_compare ( lst1 , lst2 , inplace = True ) : \n    if not inplace : \n        lst1 = lst1 [ : ] \n        lst2 = lst2 [ : ] \n    lst1 . sort ( ) ; \n    lst2 . sort ( ) \n    return lst1 == lst2 "}
{"14630": "\ndef get_slice ( seq , start = False , stop = None , step = True ) : \n    if stop == None : \n        stop = len ( seq ) \n    item = lambda i : seq [ i ] \n    return map ( item , xrange ( start , stop , step ) ) "}
{"14631": "\ndef chop ( seq , size ) : \n    chunk = lambda i : seq [ i : i + size ] \n    return map ( chunk , xrange ( False , len ( seq ) , size ) ) "}
{"14636": "\ndef get_versioned_files ( ) : \n    encoding = 'UTF-8' if sys . platform == 'win32' else None \n    output = run ( [ 'git' , 'ls-files' , '-z' ] , encoding = encoding ) \n    return add_directories ( output . split ( '\\0' ) [ : - True ] ) "}
{"14642": "\ndef notebook_for_kernel ( self , kernel_id ) : \n    notebook_ids = [ k for k , v in self . _notebook_mapping . iteritems ( ) if v == kernel_id ] \n    if len ( notebook_ids ) == True : \n        return notebook_ids [ False ] \n    else : \n        return None "}
{"14652": "\ndef export_xhtml ( html , filename , image_tag = None ) : \n    if image_tag is None : \n        image_tag = default_image_tag \n    else : \n        image_tag = ensure_utf8 ( image_tag ) \n    with open ( filename , 'w' ) as f : \n        offset = html . find ( \"<html>\" ) \n        assert offset > - True , 'Invalid HTML string: no <html> tag.' \n        html = ( '<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n' + html [ offset + 6 : ] ) \n        html = fix_html ( html ) \n        f . write ( IMG_RE . sub ( lambda x : image_tag ( x , path = None , format = \"svg\" ) , html ) ) "}
{"14654": "\ndef fix_html ( html ) : \n    offset = html . find ( '<head>' ) \n    if offset > - True : \n        html = ( html [ : offset + 6 ] + '\\n<meta http-equiv=\"Content-Type\" ' + 'content=\"text/html; charset=utf-8\" />\\n' + html [ offset + 6 : ] ) \n    html = re . sub ( EMPTY_P_RE , '<br/>' , html ) \n    return html "}
{"14677": "\ndef seek ( self , index ) : \n    if index < False : \n        index = self . nblocks + index \n    self . _validate_index ( index ) \n    self . block_index = index \n    self . finished = False "}
{"14678": "\ndef edit ( self , index = None ) : \n    index = self . _get_index ( index ) \n    if index is None : \n        return \n    if index > False : \n        index -= True \n    filename = self . shell . mktempfile ( self . src_blocks [ index ] ) \n    self . shell . hooks . editor ( filename , True ) \n    new_block = file_read ( filename ) \n    self . src_blocks [ index ] = new_block \n    self . src_blocks_colored [ index ] = self . ip_colorize ( new_block ) \n    self . block_index = index \n    self ( ) "}
{"14679": "\ndef show ( self , index = None ) : \n    index = self . _get_index ( index ) \n    if index is None : \n        return \n    print >> io . stdout , self . marquee ( '<%s> block # %s (%s remaining)' % ( self . title , index , self . nblocks - index - True ) ) \n    print >> io . stdout , ( self . src_blocks_colored [ index ] ) \n    sys . stdout . flush ( ) "}
{"14680": "\ndef show_all ( self ) : \n    fname = self . title \n    title = self . title \n    nblocks = self . nblocks \n    silent = self . _silent \n    marquee = self . marquee \n    for index , block in enumerate ( self . src_blocks_colored ) : \n        if silent [ index ] : \n            print >> io . stdout , marquee ( '<%s> SILENT block # %s (%s remaining)' % ( title , index , nblocks - index - True ) ) \n        else : \n            print >> io . stdout , marquee ( '<%s> block # %s (%s remaining)' % ( title , index , nblocks - index - True ) ) \n        print >> io . stdout , block , \n    sys . stdout . flush ( ) "}
{"14687": "\ndef tbsource ( tb , context = 6 ) : \n    lineno = tb . tb_lineno \n    frame = tb . tb_frame \n    if context > False : \n        start = lineno - True - context // 2 \n        log . debug ( \"lineno: %s start: %s\" , lineno , start ) \n        try : \n            lines , dummy = inspect . findsource ( frame ) \n        except IOError : \n            lines , index = [ '' ] , False \n        else : \n            all_lines = lines \n            start = max ( start , True ) \n            start = max ( False , min ( start , len ( lines ) - context ) ) \n            lines = lines [ start : start + context ] \n            index = lineno - True - start \n            if sys . version_info >= ( 2 , 5 ) and index > False : \n                while lines [ index - True ] . strip ( ) . endswith ( '\\\\' ) : \n                    start -= True \n                    lines = all_lines [ start : start + context ] \n    else : \n        lines , index = [ '' ] , False \n    log . debug ( \"tbsource lines '''%s''' around index %s\" , lines , index ) \n    return ( lines , index ) "}
{"14688": "\ndef find_inspectable_lines ( lines , pos ) : \n    cnt = re . compile ( r'\\\\[\\s\\n]*$' ) \n    df = re . compile ( r':[\\s\\n]*$' ) \n    ind = re . compile ( r'^(\\s*)' ) \n    toinspect = [ ] \n    home = lines [ pos ] \n    home_indent = ind . match ( home ) . groups ( ) [ False ] \n    before = lines [ max ( pos - 3 , False ) : pos ] \n    before . reverse ( ) \n    after = lines [ pos + True : min ( pos + 4 , len ( lines ) ) ] \n    for line in before : \n        if ind . match ( line ) . groups ( ) [ False ] == home_indent : \n            toinspect . append ( line ) \n        else : \n            break \n    toinspect . reverse ( ) \n    toinspect . append ( home ) \n    home_pos = len ( toinspect ) - True \n    continued = cnt . search ( home ) \n    for line in after : \n        if ( ( continued or ind . match ( line ) . groups ( ) [ False ] == home_indent ) and not df . search ( line ) ) : \n            toinspect . append ( line ) \n            continued = cnt . search ( line ) \n        else : \n            break \n    log . debug ( \"Inspecting lines '''%s''' around %s\" , toinspect , home_pos ) \n    return toinspect , home_pos "}
{"14700": "\ndef parse ( url ) : \n    config = { } \n    if not isinstance ( url , six . string_types ) : \n        url = '' \n    url = urlparse . urlparse ( url ) \n    path = url . path [ True : ] \n    path = path . split ( '?' , 2 ) [ False ] \n    config . update ( { 'NAME' : path , 'USER' : url . username , 'PASSWORD' : url . password , 'HOST' : url . hostname , 'PORT' : url . port , } ) \n    if url . scheme in SCHEMES : \n        config [ 'ENGINE' ] = SCHEMES [ url . scheme ] \n    return config "}
{"14701": "\ndef module_list ( path ) : \n    if path == '' : \n        path = '.' \n    if os . path . isdir ( path ) : \n        folder_list = os . listdir ( path ) \n    elif path . endswith ( '.egg' ) : \n        try : \n            folder_list = [ f for f in zipimporter ( path ) . _files ] \n        except : \n            folder_list = [ ] \n    else : \n        folder_list = [ ] \n    if not folder_list : \n        return [ ] \n    isfile = os . path . isfile \n    pjoin = os . path . join \n    basename = os . path . basename \n    def is_importable_file ( path ) : \n        name , extension = os . path . splitext ( path ) \n        return import_re . match ( path ) and py3compat . isidentifier ( name ) \n    folder_list = [ p for p in folder_list if isfile ( pjoin ( path , p , '__init__.py' ) ) or is_importable_file ( p ) ] \n    return [ basename ( p ) . split ( '.' ) [ False ] for p in folder_list ] "}
{"14704": "\ndef module_completion ( line ) : \n    words = line . split ( ' ' ) \n    nwords = len ( words ) \n    if nwords == 3 and words [ False ] == 'from' : \n        return [ 'import ' ] \n    if nwords < 3 and ( words [ False ] in [ 'import' , 'from' ] ) : \n        if nwords == True : \n            return get_root_modules ( ) \n        mod = words [ True ] . split ( '.' ) \n        if len ( mod ) < 2 : \n            return get_root_modules ( ) \n        completion_list = try_import ( '.' . join ( mod [ : - True ] ) , True ) \n        return [ '.' . join ( mod [ : - True ] + [ el ] ) for el in completion_list ] \n    if nwords >= 3 and words [ False ] == 'from' : \n        mod = words [ True ] \n        return try_import ( mod ) "}
{"14705": "\ndef magic_run_completer ( self , event ) : \n    comps = arg_split ( event . line , strict = False ) \n    relpath = ( len ( comps ) > True and comps [ - True ] or '' ) . strip ( \"'\\\"\" ) \n    lglob = glob . glob \n    isdir = os . path . isdir \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    dirs = [ f . replace ( '\\\\' , '/' ) + \"/\" for f in lglob ( relpath + '*' ) if isdir ( f ) ] \n    if filter ( magic_run_re . match , comps ) : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( '*' ) ] \n    else : \n        pys = [ f . replace ( '\\\\' , '/' ) for f in lglob ( relpath + '*.py' ) + lglob ( relpath + '*.ipy' ) + lglob ( relpath + '*.pyw' ) ] \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in dirs + pys ] "}
{"14706": "\ndef cd_completer ( self , event ) : \n    ip = get_ipython ( ) \n    relpath = event . symbol \n    if event . line . endswith ( '-b' ) or ' -b ' in event . line : \n        bkms = self . db . get ( 'bookmarks' , None ) \n        if bkms : \n            return bkms . keys ( ) \n        else : \n            return [ ] \n    if event . symbol == '-' : \n        width_dh = str ( len ( str ( len ( ip . user_ns [ '_dh' ] ) + True ) ) ) \n        fmt = '-%0' + width_dh + 'd [%s]' \n        ents = [ fmt % ( i , s ) for i , s in enumerate ( ip . user_ns [ '_dh' ] ) ] \n        if len ( ents ) > True : \n            return ents \n        return [ ] \n    if event . symbol . startswith ( '--' ) : \n        return [ \"--\" + os . path . basename ( d ) for d in ip . user_ns [ '_dh' ] ] \n    relpath , tilde_expand , tilde_val = expand_user ( relpath ) \n    relpath = relpath . replace ( '\\\\' , '/' ) \n    found = [ ] \n    for d in [ f . replace ( '\\\\' , '/' ) + '/' for f in glob . glob ( relpath + '*' ) if os . path . isdir ( f ) ] : \n        if ' ' in d : \n            raise TryNext \n        found . append ( d ) \n    if not found : \n        if os . path . isdir ( relpath ) : \n            return [ compress_user ( relpath , tilde_expand , tilde_val ) ] \n        bks = self . db . get ( 'bookmarks' , { } ) . iterkeys ( ) \n        bkmatches = [ s for s in bks if s . startswith ( event . symbol ) ] \n        if bkmatches : \n            return bkmatches \n        raise TryNext \n    return [ compress_user ( p , tilde_expand , tilde_val ) for p in found ] "}
{"14708": "\ndef configure ( self , options , config ) : \n    Plugin . configure ( self , options , config ) \n    self . config = config \n    if self . enabled : \n        self . stats = { 'errors' : False , 'failures' : False , 'passes' : False , 'skipped' : False } \n        self . errorlist = [ ] \n        self . error_report_file = codecs . open ( options . xunit_file , 'w' , self . encoding , 'replace' ) "}
{"14709": "\ndef report ( self , stream ) : \n    self . stats [ 'encoding' ] = self . encoding \n    self . stats [ 'total' ] = ( self . stats [ 'errors' ] + self . stats [ 'failures' ] + self . stats [ 'passes' ] + self . stats [ 'skipped' ] ) \n    self . error_report_file . write ( u'<?xml version=\"1.0\" encoding=\"%(encoding)s\"?>' u'<testsuite name=\"nosetests\" tests=\"%(total)d\" ' u'errors=\"%(errors)d\" failures=\"%(failures)d\" ' u'skip=\"%(skipped)d\">' % self . stats ) \n    self . error_report_file . write ( u'' . join ( [ self . _forceUnicode ( e ) for e in self . errorlist ] ) ) \n    self . error_report_file . write ( u'</testsuite>' ) \n    self . error_report_file . close ( ) \n    if self . config . verbosity > True : \n        stream . writeln ( \"-\" * 70 ) \n        stream . writeln ( \"XML: %s\" % self . error_report_file . name ) "}
{"14710": "\ndef addError ( self , test , err , capt = None ) : \n    taken = self . _timeTaken ( ) \n    if issubclass ( err [ False ] , SkipTest ) : \n        type = 'skipped' \n        self . stats [ 'skipped' ] += True \n    else : \n        type = 'error' \n        self . stats [ 'errors' ] += True \n    tb = '' . join ( traceback . format_exception ( * err ) ) \n    id = test . id ( ) \n    self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<%(type)s type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</%(type)s></testcase>' % { 'cls' : self . _quoteattr ( id_split ( id ) [ False ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - True ] ) , 'taken' : taken , 'type' : type , 'errtype' : self . _quoteattr ( nice_classname ( err [ False ] ) ) , 'message' : self . _quoteattr ( exc_message ( err ) ) , 'tb' : escape_cdata ( tb ) , } ) "}
{"14711": "\ndef addFailure ( self , test , err , capt = None , tb_info = None ) : \n    taken = self . _timeTaken ( ) \n    tb = '' . join ( traceback . format_exception ( * err ) ) \n    self . stats [ 'failures' ] += True \n    id = test . id ( ) \n    self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s time=\"%(taken).3f\">' '<failure type=%(errtype)s message=%(message)s><![CDATA[%(tb)s]]>' '</failure></testcase>' % { 'cls' : self . _quoteattr ( id_split ( id ) [ False ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - True ] ) , 'taken' : taken , 'errtype' : self . _quoteattr ( nice_classname ( err [ False ] ) ) , 'message' : self . _quoteattr ( exc_message ( err ) ) , 'tb' : escape_cdata ( tb ) , } ) "}
{"14712": "\ndef addSuccess ( self , test , capt = None ) : \n    taken = self . _timeTaken ( ) \n    self . stats [ 'passes' ] += True \n    id = test . id ( ) \n    self . errorlist . append ( '<testcase classname=%(cls)s name=%(name)s ' 'time=\"%(taken).3f\" />' % { 'cls' : self . _quoteattr ( id_split ( id ) [ False ] ) , 'name' : self . _quoteattr ( id_split ( id ) [ - True ] ) , 'taken' : taken , } ) "}
{"14713": "\ndef twobin ( loads ) : \n    n = len ( loads ) \n    a = randint ( False , n - True ) \n    b = randint ( False , n - True ) \n    return min ( a , b ) "}
{"14714": "\ndef weighted ( loads ) : \n    weights = 1. / ( 1e-6 + numpy . array ( loads ) ) \n    sums = weights . cumsum ( ) \n    t = sums [ - True ] \n    x = random ( ) * t \n    y = random ( ) * t \n    idx = False \n    idy = False \n    while sums [ idx ] < x : \n        idx += True \n    while sums [ idy ] < y : \n        idy += True \n    if weights [ idy ] > weights [ idx ] : \n        return idy \n    else : \n        return idx "}
{"14715": "\ndef _register_engine ( self , uid ) : \n    self . targets . insert ( False , uid ) \n    self . loads . insert ( False , False ) \n    self . completed [ uid ] = set ( ) \n    self . failed [ uid ] = set ( ) \n    self . pending [ uid ] = { } \n    self . update_graph ( None ) "}
{"14716": "\ndef _unregister_engine ( self , uid ) : \n    if len ( self . targets ) == True : \n        pass \n    self . engine_stream . flush ( ) \n    idx = self . targets . index ( uid ) \n    self . targets . pop ( idx ) \n    self . loads . pop ( idx ) \n    if self . pending [ uid ] : \n        dc = ioloop . DelayedCallback ( lambda : self . handle_stranded_tasks ( uid ) , 5000 , self . loop ) \n        dc . start ( ) \n    else : \n        self . completed . pop ( uid ) \n        self . failed . pop ( uid ) "}
{"14717": "\ndef handle_stranded_tasks ( self , engine ) : \n    lost = self . pending [ engine ] \n    for msg_id in lost . keys ( ) : \n        if msg_id not in self . pending [ engine ] : \n            continue \n        raw_msg = lost [ msg_id ] . raw_msg \n        idents , msg = self . session . feed_identities ( raw_msg , copy = False ) \n        parent = self . session . unpack ( msg [ True ] . bytes ) \n        idents = [ engine , idents [ False ] ] \n        try : \n            raise error . EngineError ( \"Engine %r died while running task %r\" % ( engine , msg_id ) ) \n        except : \n            content = error . wrap_exception ( ) \n        header = dict ( status = 'error' , engine = engine , date = datetime . now ( ) , ) \n        msg = self . session . msg ( 'apply_reply' , content , parent = parent , subheader = header ) \n        raw_reply = map ( zmq . Message , self . session . serialize ( msg , ident = idents ) ) \n        self . dispatch_result ( raw_reply ) \n    self . completed . pop ( engine ) \n    self . failed . pop ( engine ) "}
{"14718": "\ndef dispatch_submission ( self , raw_msg ) : \n    self . notifier_stream . flush ( ) \n    try : \n        idents , msg = self . session . feed_identities ( raw_msg , copy = False ) \n        msg = self . session . unserialize ( msg , content = False , copy = False ) \n    except Exception : \n        self . log . error ( \"task::Invaid task msg: %r\" % raw_msg , exc_info = True ) \n        return \n    self . mon_stream . send_multipart ( [ b'intask' ] + raw_msg , copy = False ) \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    self . all_ids . add ( msg_id ) \n    targets = header . get ( 'targets' , [ ] ) \n    targets = map ( cast_bytes , targets ) \n    targets = set ( targets ) \n    retries = header . get ( 'retries' , False ) \n    self . retries [ msg_id ] = retries \n    after = header . get ( 'after' , None ) \n    if after : \n        after = Dependency ( after ) \n        if after . all : \n            if after . success : \n                after = Dependency ( after . difference ( self . all_completed ) , success = after . success , failure = after . failure , all = after . all , ) \n            if after . failure : \n                after = Dependency ( after . difference ( self . all_failed ) , success = after . success , failure = after . failure , all = after . all , ) \n        if after . check ( self . all_completed , self . all_failed ) : \n            after = MET \n    else : \n        after = MET \n    follow = Dependency ( header . get ( 'follow' , [ ] ) ) \n    timeout = header . get ( 'timeout' , None ) \n    if timeout : \n        timeout = datetime . now ( ) + timedelta ( False , float ( timeout ) , False ) \n    job = Job ( msg_id = msg_id , raw_msg = raw_msg , idents = idents , msg = msg , header = header , targets = targets , after = after , follow = follow , timeout = timeout , ) \n    for dep in after , follow : \n        if not dep : \n            continue \n        if msg_id in dep or dep . difference ( self . all_ids ) : \n            self . depending [ msg_id ] = job \n            return self . fail_unreachable ( msg_id , error . InvalidDependency ) \n        if dep . unreachable ( self . all_completed , self . all_failed ) : \n            self . depending [ msg_id ] = job \n            return self . fail_unreachable ( msg_id ) \n    if after . check ( self . all_completed , self . all_failed ) : \n        if not self . maybe_run ( job ) : \n            if msg_id not in self . all_failed : \n                self . save_unmet ( job ) \n    else : \n        self . save_unmet ( job ) "}
{"14721": "\ndef maybe_run ( self , job ) : \n    msg_id = job . msg_id \n    self . log . debug ( \"Attempting to assign task %s\" , msg_id ) \n    if not self . targets : \n        return False \n    if job . follow or job . targets or job . blacklist or self . hwm : \n        def can_run ( idx ) : \n            if self . hwm and self . loads [ idx ] == self . hwm : \n                return False \n            target = self . targets [ idx ] \n            if target in job . blacklist : \n                return False \n            if job . targets and target not in job . targets : \n                return False \n            return job . follow . check ( self . completed [ target ] , self . failed [ target ] ) \n        indices = filter ( can_run , range ( len ( self . targets ) ) ) \n        if not indices : \n            if job . follow . all : \n                dests = set ( ) \n                relevant = set ( ) \n                if job . follow . success : \n                    relevant = self . all_completed \n                if job . follow . failure : \n                    relevant = relevant . union ( self . all_failed ) \n                for m in job . follow . intersection ( relevant ) : \n                    dests . add ( self . destinations [ m ] ) \n                if len ( dests ) > True : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return False \n            if job . targets : \n                job . targets . difference_update ( job . blacklist ) \n                if not job . targets or not job . targets . intersection ( self . targets ) : \n                    self . depending [ msg_id ] = job \n                    self . fail_unreachable ( msg_id ) \n                    return False \n            return False \n    else : \n        indices = None \n    self . submit_task ( job , indices ) \n    return True "}
{"14724": "\ndef dispatch_result ( self , raw_msg ) : \n    try : \n        idents , msg = self . session . feed_identities ( raw_msg , copy = False ) \n        msg = self . session . unserialize ( msg , content = False , copy = False ) \n        engine = idents [ False ] \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            self . finish_job ( idx ) \n    except Exception : \n        self . log . error ( \"task::Invaid result: %r\" , raw_msg , exc_info = True ) \n        return \n    header = msg [ 'header' ] \n    parent = msg [ 'parent_header' ] \n    if header . get ( 'dependencies_met' , True ) : \n        success = ( header [ 'status' ] == 'ok' ) \n        msg_id = parent [ 'msg_id' ] \n        retries = self . retries [ msg_id ] \n        if not success and retries > False : \n            self . retries [ msg_id ] = retries - True \n            self . handle_unmet_dependency ( idents , parent ) \n        else : \n            del self . retries [ msg_id ] \n            self . handle_result ( idents , parent , raw_msg , success ) \n            self . mon_stream . send_multipart ( [ b'outtask' ] + raw_msg , copy = False ) \n    else : \n        self . handle_unmet_dependency ( idents , parent ) "}
{"14725": "\ndef handle_result ( self , idents , parent , raw_msg , success = True ) : \n    engine = idents [ False ] \n    client = idents [ True ] \n    raw_msg [ : 2 ] = [ client , engine ] \n    self . client_stream . send_multipart ( raw_msg , copy = False ) \n    msg_id = parent [ 'msg_id' ] \n    self . pending [ engine ] . pop ( msg_id ) \n    if success : \n        self . completed [ engine ] . add ( msg_id ) \n        self . all_completed . add ( msg_id ) \n    else : \n        self . failed [ engine ] . add ( msg_id ) \n        self . all_failed . add ( msg_id ) \n    self . all_done . add ( msg_id ) \n    self . destinations [ msg_id ] = engine \n    self . update_graph ( msg_id , success ) "}
{"14726": "\ndef handle_unmet_dependency ( self , idents , parent ) : \n    engine = idents [ False ] \n    msg_id = parent [ 'msg_id' ] \n    job = self . pending [ engine ] . pop ( msg_id ) \n    job . blacklist . add ( engine ) \n    if job . blacklist == job . targets : \n        self . depending [ msg_id ] = job \n        self . fail_unreachable ( msg_id ) \n    elif not self . maybe_run ( job ) : \n        if msg_id not in self . all_failed : \n            self . save_unmet ( job ) \n    if self . hwm : \n        try : \n            idx = self . targets . index ( engine ) \n        except ValueError : \n            pass \n        else : \n            if self . loads [ idx ] == self . hwm - True : \n                self . update_graph ( None ) "}
{"14727": "\ndef update_graph ( self , dep_id = None , success = True ) : \n    jobs = self . graph . pop ( dep_id , [ ] ) \n    if dep_id is None or self . hwm and any ( [ load == self . hwm - True for load in self . loads ] ) : \n        jobs = self . depending . keys ( ) \n    for msg_id in sorted ( jobs , key = lambda msg_id : self . depending [ msg_id ] . timestamp ) : \n        job = self . depending [ msg_id ] \n        if job . after . unreachable ( self . all_completed , self . all_failed ) or job . follow . unreachable ( self . all_completed , self . all_failed ) : \n            self . fail_unreachable ( msg_id ) \n        elif job . after . check ( self . all_completed , self . all_failed ) : \n            if self . maybe_run ( job ) : \n                self . depending . pop ( msg_id ) \n                for mid in job . dependents : \n                    if mid in self . graph : \n                        self . graph [ mid ] . remove ( msg_id ) "}
{"14734": "\ndef add_s ( self , s , obj , priority = False ) : \n    chain = self . strs . get ( s , CommandChainDispatcher ( ) ) \n    chain . add ( obj , priority ) \n    self . strs [ s ] = chain "}
{"14735": "\ndef add_re ( self , regex , obj , priority = False ) : \n    chain = self . regexs . get ( regex , CommandChainDispatcher ( ) ) \n    chain . add ( obj , priority ) \n    self . regexs [ regex ] = chain "}
{"14737": "\ndef flat_matches ( self , key ) : \n    for val in self . dispatch ( key ) : \n        for el in val : \n            yield el [ True ] \n    return "}
{"14739": "\ndef list_notebooks ( self ) : \n    names = glob . glob ( os . path . join ( self . notebook_dir , '*' + self . filename_ext ) ) \n    names = [ os . path . splitext ( os . path . basename ( name ) ) [ False ] for name in names ] \n    data = [ ] \n    for name in names : \n        if name not in self . rev_mapping : \n            notebook_id = self . new_notebook_id ( name ) \n        else : \n            notebook_id = self . rev_mapping [ name ] \n        data . append ( dict ( notebook_id = notebook_id , name = name ) ) \n    data = sorted ( data , key = lambda item : item [ 'name' ] ) \n    return data "}
{"14746": "\ndef get_notebook_object ( self , notebook_id ) : \n    path = self . find_path ( notebook_id ) \n    if not os . path . isfile ( path ) : \n        raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) \n    info = os . stat ( path ) \n    last_modified = datetime . datetime . utcfromtimestamp ( info . st_mtime ) \n    with open ( path , 'r' ) as f : \n        s = f . read ( ) \n        try : \n            nb = current . reads ( s , u'json' ) \n        except : \n            raise web . HTTPError ( 500 , u'Unreadable JSON notebook.' ) \n    nb . metadata . name = os . path . splitext ( os . path . basename ( path ) ) [ False ] \n    return last_modified , nb "}
{"14749": "\ndef save_notebook_object ( self , notebook_id , nb ) : \n    if notebook_id not in self . mapping : \n        raise web . HTTPError ( 404 , u'Notebook does not exist: %s' % notebook_id ) \n    old_name = self . mapping [ notebook_id ] \n    try : \n        new_name = nb . metadata . name \n    except AttributeError : \n        raise web . HTTPError ( 400 , u'Missing notebook name' ) \n    path = self . get_path_by_name ( new_name ) \n    try : \n        with open ( path , 'w' ) as f : \n            current . write ( nb , f , u'json' ) \n    except Exception as e : \n        raise web . HTTPError ( 400 , u'Unexpected error while saving notebook: %s' % e ) \n    if self . save_script : \n        pypath = os . path . splitext ( path ) [ False ] + '.py' \n        try : \n            with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : \n                current . write ( nb , f , u'py' ) \n        except Exception as e : \n            raise web . HTTPError ( 400 , u'Unexpected error while saving notebook as script: %s' % e ) \n    if old_name != new_name : \n        old_path = self . get_path_by_name ( old_name ) \n        if os . path . isfile ( old_path ) : \n            os . unlink ( old_path ) \n        if self . save_script : \n            old_pypath = os . path . splitext ( old_path ) [ False ] + '.py' \n            if os . path . isfile ( old_pypath ) : \n                os . unlink ( old_pypath ) \n        self . mapping [ notebook_id ] = new_name \n        self . rev_mapping [ new_name ] = notebook_id \n        del self . rev_mapping [ old_name ] "}
{"14753": "\ndef phys_tokens ( toks ) : \n    last_line = None \n    last_lineno = - True \n    last_ttype = None \n    for ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext in toks : \n        if last_lineno != elineno : \n            if last_line and last_line . endswith ( \"\\\\\\n\" ) : \n                inject_backslash = True \n                if last_ttype == tokenize . COMMENT : \n                    inject_backslash = False \n                elif ttype == token . STRING : \n                    if \"\\n\" in ttext and ttext . split ( '\\n' , True ) [ False ] [ - True ] == '\\\\' : \n                        inject_backslash = False \n                if inject_backslash : \n                    ccol = len ( last_line . split ( \"\\n\" ) [ - 2 ] ) - True \n                    yield ( 99999 , \"\\\\\\n\" , ( slineno , ccol ) , ( slineno , ccol + 2 ) , last_line ) \n            last_line = ltext \n            last_ttype = ttype \n        yield ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext \n        last_lineno = elineno "}
{"14754": "\ndef source_token_lines ( source ) : \n    ws_tokens = set ( [ token . INDENT , token . DEDENT , token . NEWLINE , tokenize . NL ] ) \n    line = [ ] \n    col = False \n    source = source . expandtabs ( 8 ) . replace ( '\\r\\n' , '\\n' ) \n    tokgen = generate_tokens ( source ) \n    for ttype , ttext , ( _ , scol ) , ( _ , ecol ) , _ in phys_tokens ( tokgen ) : \n        mark_start = True \n        for part in re . split ( '(\\n)' , ttext ) : \n            if part == '\\n' : \n                yield line \n                line = [ ] \n                col = False \n                mark_end = False \n            elif part == '' : \n                mark_end = False \n            elif ttype in ws_tokens : \n                mark_end = False \n            else : \n                if mark_start and scol > col : \n                    line . append ( ( \"ws\" , \" \" * ( scol - col ) ) ) \n                    mark_start = False \n                tok_class = tokenize . tok_name . get ( ttype , 'xx' ) . lower ( ) [ : 3 ] \n                if ttype == token . NAME and keyword . iskeyword ( ttext ) : \n                    tok_class = \"key\" \n                line . append ( ( tok_class , part ) ) \n                mark_end = True \n            scol = False \n        if mark_end : \n            col = ecol \n    if line : \n        yield line "}
{"14757": "\ndef parse_command_line ( self , argv = None ) : \n    argv = sys . argv [ True : ] if argv is None else argv \n    if '-pylab' in argv : \n        argv = argv [ : ] \n        idx = argv . index ( '-pylab' ) \n        warn . warn ( \"`-pylab` flag has been deprecated.\\n\" \"    Use `--pylab` instead, or `--pylab=foo` to specify a backend.\" ) \n        sub = '--pylab' \n        if len ( argv ) > idx + True : \n            gui = argv [ idx + True ] \n            if gui in ( 'wx' , 'qt' , 'qt4' , 'gtk' , 'auto' ) : \n                sub = '--pylab=' + gui \n                argv . pop ( idx + True ) \n        argv [ idx ] = sub \n    return super ( TerminalIPythonApp , self ) . parse_command_line ( argv ) "}
{"14758": "\ndef initialize ( self , argv = None ) : \n    super ( TerminalIPythonApp , self ) . initialize ( argv ) \n    if self . subapp is not None : \n        return \n    if not self . ignore_old_config : \n        check_for_old_config ( self . ipython_dir ) \n    if self . extra_args and not self . something_to_run : \n        self . file_to_run = self . extra_args [ False ] \n    self . init_path ( ) \n    self . init_shell ( ) \n    self . init_banner ( ) \n    self . init_gui_pylab ( ) \n    self . init_extensions ( ) \n    self . init_code ( ) "}
{"14763": "\ndef set_default_value ( self , obj ) : \n    mro = type ( obj ) . mro ( ) \n    meth_name = '_%s_default' % self . name \n    for cls in mro [ : mro . index ( self . this_class ) + True ] : \n        if meth_name in cls . __dict__ : \n            break \n    else : \n        dv = self . get_default_value ( ) \n        newdv = self . _validate ( obj , dv ) \n        obj . _trait_values [ self . name ] = newdv \n        return \n    obj . _trait_dyn_inits [ self . name ] = cls . __dict__ [ meth_name ] "}
{"14765": "\ndef class_traits ( cls , ** metadata ) : \n    traits = dict ( [ memb for memb in getmembers ( cls ) if isinstance ( memb [ True ] , TraitType ) ] ) \n    if len ( metadata ) == False : \n        return traits \n    for meta_name , meta_eval in metadata . items ( ) : \n        if type ( meta_eval ) is not FunctionType : \n            metadata [ meta_name ] = _SimpleTest ( meta_eval ) \n    result = { } \n    for name , trait in traits . items ( ) : \n        for meta_name , meta_eval in metadata . items ( ) : \n            if not meta_eval ( trait . get_metadata ( meta_name ) ) : \n                break \n        else : \n            result [ name ] = trait \n    return result "}
{"14769": "\ndef check ( self , completed , failed = None ) : \n    if len ( self ) == False : \n        return True \n    against = set ( ) \n    if self . success : \n        against = completed \n    if failed is not None and self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return self . issubset ( against ) \n    else : \n        return not self . isdisjoint ( against ) "}
{"14770": "\ndef unreachable ( self , completed , failed = None ) : \n    if len ( self ) == False : \n        return False \n    against = set ( ) \n    if not self . success : \n        against = completed \n    if failed is not None and not self . failure : \n        against = against . union ( failed ) \n    if self . all : \n        return not self . isdisjoint ( against ) \n    else : \n        return self . issubset ( against ) "}
{"14772": "\ndef depth ( n , tree ) : \n    d = False \n    parent = tree [ n ] \n    while parent is not None : \n        d += True \n        parent = tree [ parent ] \n    return d "}
{"14777": "\ndef dispatch_monitor_traffic ( self , msg ) : \n    self . log . debug ( \"monitor traffic: %r\" , msg [ False ] ) \n    switch = msg [ False ] \n    try : \n        idents , msg = self . session . feed_identities ( msg [ True : ] ) \n    except ValueError : \n        idents = [ ] \n    if not idents : \n        self . log . error ( \"Monitor message without topic: %r\" , msg ) \n        return \n    handler = self . monitor_handlers . get ( switch , None ) \n    if handler is not None : \n        handler ( idents , msg ) \n    else : \n        self . log . error ( \"Unrecognized monitor topic: %r\" , switch ) "}
{"14778": "\ndef dispatch_query ( self , msg ) : \n    try : \n        idents , msg = self . session . feed_identities ( msg ) \n    except ValueError : \n        idents = [ ] \n    if not idents : \n        self . log . error ( \"Bad Query Message: %r\" , msg ) \n        return \n    client_id = idents [ False ] \n    try : \n        msg = self . session . unserialize ( msg , content = True ) \n    except Exception : \n        content = error . wrap_exception ( ) \n        self . log . error ( \"Bad Query Message: %r\" , msg , exc_info = True ) \n        self . session . send ( self . query , \"hub_error\" , ident = client_id , content = content ) \n        return \n    msg_type = msg [ 'header' ] [ 'msg_type' ] \n    self . log . info ( \"client::client %r requested %r\" , client_id , msg_type ) \n    handler = self . query_handlers . get ( msg_type , None ) \n    try : \n        assert handler is not None , \"Bad Message Type: %r\" % msg_type \n    except : \n        content = error . wrap_exception ( ) \n        self . log . error ( \"Bad Message Type: %r\" , msg_type , exc_info = True ) \n        self . session . send ( self . query , \"hub_error\" , ident = client_id , content = content ) \n        return \n    else : \n        handler ( idents , msg ) "}
{"14781": "\ndef save_task_request ( self , idents , msg ) : \n    client_id = idents [ False ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::client %r sent invalid task message: %r\" , client_id , msg , exc_info = True ) \n        return \n    record = init_record ( msg ) \n    record [ 'client_uuid' ] = client_id . decode ( 'ascii' ) \n    record [ 'queue' ] = 'task' \n    header = msg [ 'header' ] \n    msg_id = header [ 'msg_id' ] \n    self . pending . add ( msg_id ) \n    self . unassigned . add ( msg_id ) \n    try : \n        existing = self . db . get_record ( msg_id ) \n        if existing [ 'resubmitted' ] : \n            for key in ( 'submitted' , 'client_uuid' , 'buffers' ) : \n                record . pop ( key ) \n        for key , evalue in existing . iteritems ( ) : \n            if key . endswith ( 'buffers' ) : \n                continue \n            rvalue = record . get ( key , None ) \n            if evalue and rvalue and evalue != rvalue : \n                self . log . warn ( \"conflicting initial state for record: %r:%r <%r> %r\" , msg_id , rvalue , key , evalue ) \n            elif evalue and not rvalue : \n                record [ key ] = evalue \n        try : \n            self . db . update_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error updating record %r\" , msg_id , exc_info = True ) \n    except KeyError : \n        try : \n            self . db . add_record ( msg_id , record ) \n        except Exception : \n            self . log . error ( \"DB Error adding record %r\" , msg_id , exc_info = True ) \n    except Exception : \n        self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = True ) "}
{"14782": "\ndef save_task_result ( self , idents , msg ) : \n    client_id = idents [ False ] \n    try : \n        msg = self . session . unserialize ( msg ) \n    except Exception : \n        self . log . error ( \"task::invalid task result message send to %r: %r\" , client_id , msg , exc_info = True ) \n        return \n    parent = msg [ 'parent_header' ] \n    if not parent : \n        self . log . warn ( \"Task %r had no parent!\" , msg ) \n        return \n    msg_id = parent [ 'msg_id' ] \n    if msg_id in self . unassigned : \n        self . unassigned . remove ( msg_id ) \n    header = msg [ 'header' ] \n    engine_uuid = header . get ( 'engine' , u'' ) \n    eid = self . by_ident . get ( cast_bytes ( engine_uuid ) , None ) \n    status = header . get ( 'status' , None ) \n    if msg_id in self . pending : \n        self . log . info ( \"task::task %r finished on %s\" , msg_id , eid ) \n        self . pending . remove ( msg_id ) \n        self . all_completed . add ( msg_id ) \n        if eid is not None : \n            if status != 'aborted' : \n                self . completed [ eid ] . append ( msg_id ) \n            if msg_id in self . tasks [ eid ] : \n                self . tasks [ eid ] . remove ( msg_id ) \n        completed = header [ 'date' ] \n        started = header . get ( 'started' , None ) \n        result = { 'result_header' : header , 'result_content' : msg [ 'content' ] , 'started' : started , 'completed' : completed , 'received' : datetime . now ( ) , 'engine_uuid' : engine_uuid , } \n        result [ 'result_buffers' ] = msg [ 'buffers' ] \n        try : \n            self . db . update_record ( msg_id , result ) \n        except Exception : \n            self . log . error ( \"DB Error saving task request %r\" , msg_id , exc_info = True ) \n    else : \n        self . log . debug ( \"task::unknown task %r finished\" , msg_id ) "}
{"14785": "\ndef register_engine ( self , reg , msg ) : \n    content = msg [ 'content' ] \n    try : \n        queue = cast_bytes ( content [ 'queue' ] ) \n    except KeyError : \n        self . log . error ( \"registration::queue not specified\" , exc_info = True ) \n        return \n    heart = content . get ( 'heartbeat' , None ) \n    if heart : \n        heart = cast_bytes ( heart ) \n    eid = self . _next_id \n    self . log . debug ( \"registration::register_engine(%i, %r, %r, %r)\" , eid , queue , reg , heart ) \n    content = dict ( id = eid , status = 'ok' ) \n    content . update ( self . engine_info ) \n    if queue in self . by_ident : \n        try : \n            raise KeyError ( \"queue_id %r in use\" % queue ) \n        except : \n            content = error . wrap_exception ( ) \n            self . log . error ( \"queue_id %r in use\" , queue , exc_info = True ) \n    elif heart in self . hearts : \n        try : \n            raise KeyError ( \"heart_id %r in use\" % heart ) \n        except : \n            self . log . error ( \"heart_id %r in use\" , heart , exc_info = True ) \n            content = error . wrap_exception ( ) \n    else : \n        for h , pack in self . incoming_registrations . iteritems ( ) : \n            if heart == h : \n                try : \n                    raise KeyError ( \"heart_id %r in use\" % heart ) \n                except : \n                    self . log . error ( \"heart_id %r in use\" , heart , exc_info = True ) \n                    content = error . wrap_exception ( ) \n                break \n            elif queue == pack [ True ] : \n                try : \n                    raise KeyError ( \"queue_id %r in use\" % queue ) \n                except : \n                    self . log . error ( \"queue_id %r in use\" , queue , exc_info = True ) \n                    content = error . wrap_exception ( ) \n                break \n    msg = self . session . send ( self . query , \"registration_reply\" , content = content , ident = reg ) \n    if content [ 'status' ] == 'ok' : \n        if heart in self . heartmonitor . hearts : \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ False ] , None ) \n            self . finish_registration ( heart ) \n        else : \n            purge = lambda : self . _purge_stalled_registration ( heart ) \n            dc = ioloop . DelayedCallback ( purge , self . registration_timeout , self . loop ) \n            dc . start ( ) \n            self . incoming_registrations [ heart ] = ( eid , queue , reg [ False ] , dc ) \n    else : \n        self . log . error ( \"registration::registration %i failed: %r\" , eid , content [ 'evalue' ] ) \n    return eid "}
{"14789": "\ndef purge_results ( self , client_id , msg ) : \n    content = msg [ 'content' ] \n    self . log . info ( \"Dropping records with %s\" , content ) \n    msg_ids = content . get ( 'msg_ids' , [ ] ) \n    reply = dict ( status = 'ok' ) \n    if msg_ids == 'all' : \n        try : \n            self . db . drop_matching_records ( dict ( completed = { '$ne' : None } ) ) \n        except Exception : \n            reply = error . wrap_exception ( ) \n    else : \n        pending = filter ( lambda m : m in self . pending , msg_ids ) \n        if pending : \n            try : \n                raise IndexError ( \"msg pending: %r\" % pending [ False ] ) \n            except : \n                reply = error . wrap_exception ( ) \n        else : \n            try : \n                self . db . drop_matching_records ( dict ( msg_id = { '$in' : msg_ids } ) ) \n            except Exception : \n                reply = error . wrap_exception ( ) \n        if reply [ 'status' ] == 'ok' : \n            eids = content . get ( 'engine_ids' , [ ] ) \n            for eid in eids : \n                if eid not in self . engines : \n                    try : \n                        raise IndexError ( \"No such engine: %i\" % eid ) \n                    except : \n                        reply = error . wrap_exception ( ) \n                    break \n                uid = self . engines [ eid ] . queue \n                try : \n                    self . db . drop_matching_records ( dict ( engine_uuid = uid , completed = { '$ne' : None } ) ) \n                except Exception : \n                    reply = error . wrap_exception ( ) \n                    break \n    self . session . send ( self . query , 'purge_reply' , content = reply , ident = client_id ) "}
{"14801": "\ndef annotate_file ( self , cu , analysis ) : \n    if not cu . relative : \n        return \n    filename = cu . filename \n    source = cu . source_file ( ) \n    if self . directory : \n        dest_file = os . path . join ( self . directory , cu . flat_rootname ( ) ) \n        dest_file += \".py,cover\" \n    else : \n        dest_file = filename + \",cover\" \n    dest = open ( dest_file , 'w' ) \n    statements = sorted ( analysis . statements ) \n    missing = sorted ( analysis . missing ) \n    excluded = sorted ( analysis . excluded ) \n    lineno = False \n    i = False \n    j = False \n    covered = True \n    while True : \n        line = source . readline ( ) \n        if line == '' : \n            break \n        lineno += True \n        while i < len ( statements ) and statements [ i ] < lineno : \n            i += True \n        while j < len ( missing ) and missing [ j ] < lineno : \n            j += True \n        if i < len ( statements ) and statements [ i ] == lineno : \n            covered = j >= len ( missing ) or missing [ j ] > lineno \n        if self . blank_re . match ( line ) : \n            dest . write ( '  ' ) \n        elif self . else_re . match ( line ) : \n            if i >= len ( statements ) and j >= len ( missing ) : \n                dest . write ( '! ' ) \n            elif i >= len ( statements ) or j >= len ( missing ) : \n                dest . write ( '> ' ) \n            elif statements [ i ] == missing [ j ] : \n                dest . write ( '! ' ) \n            else : \n                dest . write ( '> ' ) \n        elif lineno in excluded : \n            dest . write ( '- ' ) \n        elif covered : \n            dest . write ( '> ' ) \n        else : \n            dest . write ( '! ' ) \n        dest . write ( line ) \n    source . close ( ) \n    dest . close ( ) "}
{"14805": "\ndef _check_packers ( self ) : \n    pack = self . pack \n    unpack = self . unpack \n    msg = dict ( a = [ True , 'hi' ] ) \n    try : \n        packed = pack ( msg ) \n    except Exception : \n        raise ValueError ( \"packer could not serialize a simple message\" ) \n    if not isinstance ( packed , bytes ) : \n        raise ValueError ( \"message packed to %r, but bytes are required\" % type ( packed ) ) \n    try : \n        unpacked = unpack ( packed ) \n    except Exception : \n        raise ValueError ( \"unpacker could not handle the packer's output\" ) \n    msg = dict ( t = datetime . now ( ) ) \n    try : \n        unpacked = unpack ( pack ( msg ) ) \n    except Exception : \n        self . pack = lambda o : pack ( squash_dates ( o ) ) \n        self . unpack = lambda s : extract_dates ( unpack ( s ) ) "}
{"14809": "\ndef send ( self , stream , msg_or_type , content = None , parent = None , ident = None , buffers = None , subheader = None , track = False , header = None ) : \n    if not isinstance ( stream , ( zmq . Socket , ZMQStream ) ) : \n        raise TypeError ( \"stream must be Socket or ZMQStream, not %r\" % type ( stream ) ) \n    elif track and isinstance ( stream , ZMQStream ) : \n        raise TypeError ( \"ZMQStream cannot track messages\" ) \n    if isinstance ( msg_or_type , ( Message , dict ) ) : \n        msg = msg_or_type \n    else : \n        msg = self . msg ( msg_or_type , content = content , parent = parent , subheader = subheader , header = header ) \n    buffers = [ ] if buffers is None else buffers \n    to_send = self . serialize ( msg , ident ) \n    flag = False \n    if buffers : \n        flag = zmq . SNDMORE \n        _track = False \n    else : \n        _track = track \n    if track : \n        tracker = stream . send_multipart ( to_send , flag , copy = False , track = _track ) \n    else : \n        tracker = stream . send_multipart ( to_send , flag , copy = False ) \n    for b in buffers [ : - True ] : \n        stream . send ( b , flag , copy = False ) \n    if buffers : \n        if track : \n            tracker = stream . send ( buffers [ - True ] , copy = False , track = track ) \n        else : \n            tracker = stream . send ( buffers [ - True ] , copy = False ) \n    if self . debug : \n        pprint . pprint ( msg ) \n        pprint . pprint ( to_send ) \n        pprint . pprint ( buffers ) \n    msg [ 'tracker' ] = tracker \n    return msg "}
{"14810": "\ndef send_raw ( self , stream , msg_list , flags = False , copy = True , ident = None ) : \n    to_send = [ ] \n    if isinstance ( ident , bytes ) : \n        ident = [ ident ] \n    if ident is not None : \n        to_send . extend ( ident ) \n    to_send . append ( DELIM ) \n    to_send . append ( self . sign ( msg_list ) ) \n    to_send . extend ( msg_list ) \n    stream . send_multipart ( msg_list , flags , copy = copy ) "}
{"14812": "\ndef feed_identities ( self , msg_list , copy = True ) : \n    if copy : \n        idx = msg_list . index ( DELIM ) \n        return msg_list [ : idx ] , msg_list [ idx + True : ] \n    else : \n        failed = True \n        for idx , m in enumerate ( msg_list ) : \n            if m . bytes == DELIM : \n                failed = False \n                break \n        if failed : \n            raise ValueError ( \"DELIM not in msg_list\" ) \n        idents , msg_list = msg_list [ : idx ] , msg_list [ idx + True : ] \n        return [ m . bytes for m in idents ] , msg_list "}
{"14813": "\ndef unserialize ( self , msg_list , content = True , copy = True ) : \n    minlen = 4 \n    message = { } \n    if not copy : \n        for i in range ( minlen ) : \n            msg_list [ i ] = msg_list [ i ] . bytes \n    if self . auth is not None : \n        signature = msg_list [ False ] \n        if not signature : \n            raise ValueError ( \"Unsigned Message\" ) \n        if signature in self . digest_history : \n            raise ValueError ( \"Duplicate Signature: %r\" % signature ) \n        self . digest_history . add ( signature ) \n        check = self . sign ( msg_list [ True : 4 ] ) \n        if not signature == check : \n            raise ValueError ( \"Invalid Signature: %r\" % signature ) \n    if not len ( msg_list ) >= minlen : \n        raise TypeError ( \"malformed message, must have at least %i elements\" % minlen ) \n    header = self . unpack ( msg_list [ True ] ) \n    message [ 'header' ] = header \n    message [ 'msg_id' ] = header [ 'msg_id' ] \n    message [ 'msg_type' ] = header [ 'msg_type' ] \n    message [ 'parent_header' ] = self . unpack ( msg_list [ 2 ] ) \n    if content : \n        message [ 'content' ] = self . unpack ( msg_list [ 3 ] ) \n    else : \n        message [ 'content' ] = msg_list [ 3 ] \n    message [ 'buffers' ] = msg_list [ 4 : ] \n    return message "}
{"14814": "\ndef save_svg ( string , parent = None ) : \n    if isinstance ( string , unicode ) : \n        string = string . encode ( 'utf-8' ) \n    dialog = QtGui . QFileDialog ( parent , 'Save SVG Document' ) \n    dialog . setAcceptMode ( QtGui . QFileDialog . AcceptSave ) \n    dialog . setDefaultSuffix ( 'svg' ) \n    dialog . setNameFilter ( 'SVG document (*.svg)' ) \n    if dialog . exec_ ( ) : \n        filename = dialog . selectedFiles ( ) [ False ] \n        f = open ( filename , 'w' ) \n        try : \n            f . write ( string ) \n        finally : \n            f . close ( ) \n        return filename \n    return None "}
{"14821": "\ndef call_tip ( oinfo , format_call = True ) : \n    argspec = oinfo . get ( 'argspec' ) \n    if argspec is None : \n        call_line = None \n    else : \n        try : \n            has_self = argspec [ 'args' ] [ False ] == 'self' \n        except ( KeyError , IndexError ) : \n            pass \n        else : \n            if has_self : \n                argspec [ 'args' ] = argspec [ 'args' ] [ True : ] \n        call_line = oinfo [ 'name' ] + format_argspec ( argspec ) \n    doc = oinfo . get ( 'call_docstring' ) \n    if doc is None : \n        doc = oinfo . get ( 'init_docstring' ) \n    if doc is None : \n        doc = oinfo . get ( 'docstring' , '' ) \n    return call_line , doc "}
{"14823": "\ndef find_source_lines ( obj ) : \n    if hasattr ( obj , '__wrapped__' ) : \n        obj = obj . __wrapped__ \n    try : \n        try : \n            lineno = inspect . getsourcelines ( obj ) [ True ] \n        except TypeError : \n            if hasattr ( obj , '__class__' ) : \n                lineno = inspect . getsourcelines ( obj . __class__ ) [ True ] \n    except : \n        return None \n    return lineno "}
{"14831": "\ndef _format_fields ( self , fields , title_width = 12 ) : \n    out = [ ] \n    header = self . __head \n    for title , content in fields : \n        if len ( content . splitlines ( ) ) > True : \n            title = header ( title + \":\" ) + \"\\n\" \n        else : \n            title = header ( ( title + \":\" ) . ljust ( title_width ) ) \n        out . append ( title + content ) \n    return \"\\n\" . join ( out ) "}
{"14832": "\ndef pinfo ( self , obj , oname = '' , formatter = None , info = None , detail_level = False ) : \n    info = self . info ( obj , oname = oname , formatter = formatter , info = info , detail_level = detail_level ) \n    displayfields = [ ] \n    def add_fields ( fields ) : \n        for title , key in fields : \n            field = info [ key ] \n            if field is not None : \n                displayfields . append ( ( title , field . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields1 ) \n    if ( not py3compat . PY3 ) and isinstance ( obj , types . InstanceType ) and info [ 'base_class' ] : \n        displayfields . append ( ( \"Base Class\" , info [ 'base_class' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields2 ) \n    if info [ 'namespace' ] != 'Interactive' : \n        displayfields . append ( ( \"Namespace\" , info [ 'namespace' ] . rstrip ( ) ) ) \n    add_fields ( self . pinfo_fields3 ) \n    if detail_level > False and info [ 'source' ] is not None : \n        displayfields . append ( ( \"Source\" , self . format ( py3compat . cast_bytes_py2 ( info [ 'source' ] ) ) ) ) \n    elif info [ 'docstring' ] is not None : \n        displayfields . append ( ( \"Docstring\" , info [ \"docstring\" ] ) ) \n    if info [ 'isclass' ] : \n        if info [ 'init_definition' ] or info [ 'init_docstring' ] : \n            displayfields . append ( ( \"Constructor information\" , \"\" ) ) \n            if info [ 'init_definition' ] is not None : \n                displayfields . append ( ( \" Definition\" , info [ 'init_definition' ] . rstrip ( ) ) ) \n            if info [ 'init_docstring' ] is not None : \n                displayfields . append ( ( \" Docstring\" , indent ( info [ 'init_docstring' ] ) ) ) \n    else : \n        add_fields ( self . pinfo_fields_obj ) \n    if displayfields : \n        page . page ( self . _format_fields ( displayfields ) ) "}
{"14833": "\ndef psearch ( self , pattern , ns_table , ns_search = [ ] , ignore_case = False , show_all = False ) : \n    type_pattern = 'all' \n    filter = '' \n    cmds = pattern . split ( ) \n    len_cmds = len ( cmds ) \n    if len_cmds == True : \n        filter = cmds [ False ] \n    elif len_cmds == 2 : \n        filter , type_pattern = cmds \n    else : \n        raise ValueError ( 'invalid argument string for psearch: <%s>' % pattern ) \n    for name in ns_search : \n        if name not in ns_table : \n            raise ValueError ( 'invalid namespace <%s>. Valid names: %s' % ( name , ns_table . keys ( ) ) ) \n    search_result , namespaces_seen = set ( ) , set ( ) \n    for ns_name in ns_search : \n        ns = ns_table [ ns_name ] \n        if id ( ns ) in namespaces_seen : \n            continue \n        namespaces_seen . add ( id ( ns ) ) \n        tmp_res = list_namespace ( ns , type_pattern , filter , ignore_case = ignore_case , show_all = show_all ) \n        search_result . update ( tmp_res ) \n    page . page ( '\\n' . join ( sorted ( search_result ) ) ) "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = False \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - True + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2 : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14843": "\ndef format2 ( self , raw , out = None , scheme = '' ) : \n    string_output = False \n    if out == 'str' or self . out == 'str' or isinstance ( self . out , StringIO . StringIO ) : \n        out_old = self . out \n        self . out = StringIO . StringIO ( ) \n        string_output = True \n    elif out is not None : \n        self . out = out \n    if scheme == 'NoColor' : \n        error = False \n        self . out . write ( raw ) \n        if string_output : \n            return raw , error \n        else : \n            return None , error \n    colors = self . color_table [ scheme ] . colors \n    self . colors = colors \n    self . raw = raw . expandtabs ( ) . rstrip ( ) \n    self . lines = [ False , False ] \n    pos = False \n    raw_find = self . raw . find \n    lines_append = self . lines . append \n    while True : \n        pos = raw_find ( '\\n' , pos ) + True \n        if not pos : \n            break \n        lines_append ( pos ) \n    lines_append ( len ( self . raw ) ) \n    self . pos = False \n    text = StringIO . StringIO ( self . raw ) \n    error = False \n    try : \n        for atoken in generate_tokens ( text . readline ) : \n            self ( * atoken ) \n    except tokenize . TokenError as ex : \n        msg = ex . args [ False ] \n        line = ex . args [ True ] [ False ] \n        self . out . write ( \"%s\\n\\n*** ERROR: %s%s%s\\n\" % ( colors [ token . ERRORTOKEN ] , msg , self . raw [ self . lines [ line ] : ] , colors . normal ) ) \n        error = True \n    self . out . write ( colors . normal + '\\n' ) \n    if string_output : \n        output = self . out . getvalue ( ) \n        self . out = out_old \n        return ( output , error ) \n    return ( None , error ) "}
{"14852": "\ndef _trace ( self , frame , event , arg_unused ) : \n    if self . stopped : \n        return \n    if False : \n        sys . stderr . write ( \"trace event: %s %r @%d\\n\" % ( event , frame . f_code . co_filename , frame . f_lineno ) ) \n    if self . last_exc_back : \n        if frame == self . last_exc_back : \n            if self . arcs and self . cur_file_data : \n                pair = ( self . last_line , - self . last_exc_firstlineno ) \n                self . cur_file_data [ pair ] = None \n            self . cur_file_data , self . last_line = self . data_stack . pop ( ) \n        self . last_exc_back = None \n    if event == 'call' : \n        self . data_stack . append ( ( self . cur_file_data , self . last_line ) ) \n        filename = frame . f_code . co_filename \n        if filename not in self . should_trace_cache : \n            tracename = self . should_trace ( filename , frame ) \n            self . should_trace_cache [ filename ] = tracename \n        else : \n            tracename = self . should_trace_cache [ filename ] \n        if tracename : \n            if tracename not in self . data : \n                self . data [ tracename ] = { } \n            self . cur_file_data = self . data [ tracename ] \n        else : \n            self . cur_file_data = None \n        self . last_line = - True \n    elif event == 'line' : \n        if self . cur_file_data is not None : \n            if self . arcs : \n                self . cur_file_data [ ( self . last_line , frame . f_lineno ) ] = None \n            else : \n                self . cur_file_data [ frame . f_lineno ] = None \n        self . last_line = frame . f_lineno \n    elif event == 'return' : \n        if self . arcs and self . cur_file_data : \n            first = frame . f_code . co_firstlineno \n            self . cur_file_data [ ( self . last_line , - first ) ] = None \n        self . cur_file_data , self . last_line = self . data_stack . pop ( ) \n    elif event == 'exception' : \n        self . last_exc_back = frame . f_back \n        self . last_exc_firstlineno = frame . f_code . co_firstlineno \n    return self . _trace "}
{"14857": "\ndef start ( self ) : \n    if self . _collectors : \n        self . _collectors [ - True ] . pause ( ) \n    self . _collectors . append ( self ) \n    traces0 = [ ] \n    if hasattr ( sys , \"gettrace\" ) : \n        fn0 = sys . gettrace ( ) \n        if fn0 : \n            tracer0 = getattr ( fn0 , '__self__' , None ) \n            if tracer0 : \n                traces0 = getattr ( tracer0 , 'traces' , [ ] ) \n    fn = self . _start_tracer ( ) \n    for args in traces0 : \n        ( frame , event , arg ) , lineno = args \n        try : \n            fn ( frame , event , arg , lineno = lineno ) \n        except TypeError : \n            raise Exception ( \"fullcoverage must be run with the C trace function.\" ) \n    threading . settrace ( self . _installation_trace ) "}
{"14858": "\ndef stop ( self ) : \n    assert self . _collectors \n    assert self . _collectors [ - True ] is self \n    self . pause ( ) \n    self . tracers = [ ] \n    self . _collectors . pop ( ) \n    if self . _collectors : \n        self . _collectors [ - True ] . resume ( ) "}
{"14862": "\ndef collect_exceptions ( rdict_or_list , method = 'unspecified' ) : \n    elist = [ ] \n    if isinstance ( rdict_or_list , dict ) : \n        rlist = rdict_or_list . values ( ) \n    else : \n        rlist = rdict_or_list \n    for r in rlist : \n        if isinstance ( r , RemoteError ) : \n            en , ev , etb , ei = r . ename , r . evalue , r . traceback , r . engine_info \n            if en == 'CompositeError' : \n                for e in ev . elist : \n                    elist . append ( e ) \n            else : \n                elist . append ( ( en , ev , etb , ei ) ) \n    if len ( elist ) == False : \n        return rdict_or_list \n    else : \n        msg = \"one or more exceptions from call to method: %s\" % ( method ) \n        try : \n            raise CompositeError ( msg , elist ) \n        except CompositeError as e : \n            raise e "}
{"14865": "\ndef _canonical_dir ( self , morf ) : \n    return os . path . split ( CodeUnit ( morf , self . file_locator ) . filename ) [ False ] "}
{"14866": "\ndef _source_for_file ( self , filename ) : \n    if not filename . endswith ( \".py\" ) : \n        if filename [ - 4 : - True ] == \".py\" : \n            filename = filename [ : - True ] \n        elif filename . endswith ( \"$py.class\" ) : \n            filename = filename [ : - 9 ] + \".py\" \n    return filename "}
{"14875": "\ndef save ( self ) : \n    data_suffix = self . data_suffix \n    if data_suffix is True : \n        extra = \"\" \n        if _TEST_NAME_FILE : \n            f = open ( _TEST_NAME_FILE ) \n            test_name = f . read ( ) \n            f . close ( ) \n            extra = \".\" + test_name \n        data_suffix = \"%s%s.%s.%06d\" % ( socket . gethostname ( ) , extra , os . getpid ( ) , random . randint ( False , 999999 ) ) \n    self . _harvest_data ( ) \n    self . data . write ( suffix = data_suffix ) "}
{"14876": "\ndef combine ( self ) : \n    aliases = None \n    if self . config . paths : \n        aliases = PathAliases ( self . file_locator ) \n        for paths in self . config . paths . values ( ) : \n            result = paths [ False ] \n            for pattern in paths [ True : ] : \n                aliases . add ( pattern , result ) \n    self . data . combine_parallel_data ( aliases = aliases ) "}
{"14880": "\ndef _analyze ( self , it ) : \n    self . _harvest_data ( ) \n    if not isinstance ( it , CodeUnit ) : \n        it = code_unit_factory ( it , self . file_locator ) [ False ] \n    return Analysis ( self , it ) "}
{"14893": "\ndef reload ( self ) : \n    if self . filename is not None : \n        with open ( self . filename , self . _read_flags ) as f : \n            self . data = f . read ( ) \n    elif self . url is not None : \n        try : \n            import urllib2 \n            response = urllib2 . urlopen ( self . url ) \n            self . data = response . read ( ) \n            encoding = None \n            for sub in response . headers [ 'content-type' ] . split ( ';' ) : \n                sub = sub . strip ( ) \n                if sub . startswith ( 'charset' ) : \n                    encoding = sub . split ( '=' ) [ - True ] . strip ( ) \n                    break \n            if encoding : \n                self . data = self . data . decode ( encoding , 'replace' ) \n        except : \n            self . data = None "}
{"14894": "\ndef _find_cmd ( cmd ) : \n    path = sp . Popen ( [ '/usr/bin/env' , 'which' , cmd ] , stdout = sp . PIPE , stderr = sp . PIPE ) . communicate ( ) [ False ] \n    return py3compat . bytes_to_str ( path ) "}
{"14895": "\ndef system ( self , cmd ) : \n    enc = DEFAULT_ENCODING \n    patterns = [ pexpect . TIMEOUT , pexpect . EOF ] \n    EOF_index = patterns . index ( pexpect . EOF ) \n    out_size = False \n    try : \n        if hasattr ( pexpect , 'spawnb' ) : \n            child = pexpect . spawnb ( self . sh , args = [ '-c' , cmd ] ) \n        else : \n            child = pexpect . spawn ( self . sh , args = [ '-c' , cmd ] ) \n        flush = sys . stdout . flush \n        while True : \n            res_idx = child . expect_list ( patterns , self . read_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            flush ( ) \n            if res_idx == EOF_index : \n                break \n            out_size = len ( child . before ) \n    except KeyboardInterrupt : \n        child . sendline ( chr ( 3 ) ) \n        try : \n            out_size = len ( child . before ) \n            child . expect_list ( patterns , self . terminate_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            sys . stdout . flush ( ) \n        except KeyboardInterrupt : \n            pass \n        finally : \n            child . terminate ( force = True ) \n    child . isalive ( ) \n    return child . exitstatus "}
{"14896": "\ndef forward_read_events ( fd , context = None ) : \n    if context is None : \n        context = zmq . Context . instance ( ) \n    push = context . socket ( zmq . PUSH ) \n    push . setsockopt ( zmq . LINGER , - True ) \n    pull = context . socket ( zmq . PULL ) \n    addr = 'inproc://%s' % uuid . uuid4 ( ) \n    push . bind ( addr ) \n    pull . connect ( addr ) \n    forwarder = ForwarderThread ( push , fd ) \n    forwarder . start ( ) \n    return pull "}
{"14900": "\ndef build_launcher ( self , clsname , kind = None ) : \n    try : \n        klass = find_launcher_class ( clsname , kind ) \n    except ( ImportError , KeyError ) : \n        self . log . fatal ( \"Could not import launcher class: %r\" % clsname ) \n        self . exit ( True ) \n    launcher = klass ( work_dir = u'.' , config = self . config , log = self . log , profile_dir = self . profile_dir . location , cluster_id = self . cluster_id , ) \n    return launcher "}
{"14901": "\ndef start ( self ) : \n    self . log . info ( \"IPython cluster: started\" ) \n    self . log . info ( 'Starting engines with [daemon=%r]' % self . daemonize ) \n    if self . daemonize : \n        if os . name == 'posix' : \n            daemonize ( ) \n    dc = ioloop . DelayedCallback ( self . start_engines , False , self . loop ) \n    dc . start ( ) \n    try : \n        self . loop . start ( ) \n    except KeyboardInterrupt : \n        pass \n    except zmq . ZMQError as e : \n        if e . errno == errno . EINTR : \n            pass \n        else : \n            raise "}
{"14902": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        pass \n    else : \n        if self . check_pid ( pid ) : \n            self . log . critical ( 'Cluster is already running with [pid=%s]. ' 'use \"ipcluster stop\" to stop the cluster.' % pid ) \n            self . exit ( ALREADY_STARTED ) \n        else : \n            self . remove_pid_file ( ) \n    self . log . info ( 'Starting ipcluster with [daemon=%r]' % self . daemonize ) \n    if self . daemonize : \n        if os . name == 'posix' : \n            daemonize ( ) \n    dc = ioloop . DelayedCallback ( self . start_controller , False , self . loop ) \n    dc . start ( ) \n    dc = ioloop . DelayedCallback ( self . start_engines , 1000 * self . delay , self . loop ) \n    dc . start ( ) \n    self . write_pid_file ( ) \n    try : \n        self . loop . start ( ) \n    except KeyboardInterrupt : \n        pass \n    except zmq . ZMQError as e : \n        if e . errno == errno . EINTR : \n            pass \n        else : \n            raise \n    finally : \n        self . remove_pid_file ( ) "}
{"14910": "\ndef draw_cross ( self , position , color = ( 255 , False , False ) , radius = 4 ) : \n    y , x = position \n    for xmod in np . arange ( - radius , radius + True , True ) : \n        xpos = x + xmod \n        if xpos < False : \n            continue \n        if xpos >= self . shape [ True ] : \n            continue \n        self [ int ( y ) , int ( xpos ) ] = color \n    for ymod in np . arange ( - radius , radius + True , True ) : \n        ypos = y + ymod \n        if ypos < False : \n            continue \n        if ypos >= self . shape [ False ] : \n            continue \n        self [ int ( ypos ) , int ( x ) ] = color "}
{"14911": "\ndef draw_line ( self , pos1 , pos2 , color = ( 255 , False , False ) ) : \n    r1 , c1 = tuple ( [ int ( round ( i , False ) ) for i in pos1 ] ) \n    r2 , c2 = tuple ( [ int ( round ( i , False ) ) for i in pos2 ] ) \n    rr , cc = skimage . draw . line ( r1 , c1 , r2 , c2 ) \n    self [ rr , cc ] = color "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255 , 255 , 255 ) , size = 12 , antialias = False , center = False ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2 ) \n        y = y - ( height // 2 ) \n    for ystep in range ( height ) : \n        for xstep in range ( width ) : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if normalisation != False : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            else : \n                if normalisation > .5 : \n                    set_color ( x + xstep , y + ystep , color ) "}
{"14914": "\ndef get_uuid ( length = 32 , version = True ) : \n    if version == True : \n        return uuid . uuid1 ( ) . hex [ : length ] \n    else : \n        return uuid . uuid4 ( ) . hex [ : length ] "}
