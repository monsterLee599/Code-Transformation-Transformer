{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256 \n    for index , _ in enumerate ( data ) : \n        t = index \n        i = 0 \n        while i < 8 : \n            t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) \n            i = i + 1 \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) \n        e = o = r = - 1 \n        i , a = 0 , len ( path ) \n        while i < a : \n            e = ord ( path [ i ] ) \n            i += 1 \n            if e < 128 : \n                r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] \n            else : \n                if e < 2048 : \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] \n                    r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n                else : \n                    if 55296 <= e < 57344 : \n                        e = ( 1023 & e ) + 64 \n                        i += 1 \n                        o = 1023 & t . url ( i ) \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] \n                    else : \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] \n                        r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) \n    while 1 : \n        url = tmp ( ) \n        if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : \n            return url "}
{"23": "\ndef wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    tIndex = 0 \n    while tIndex < len ( json_api_content [ 0 ] [ 'Topics' ] ) : \n        pIndex = 0 \n        while pIndex < len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) : \n            wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n            pIndex = pIndex + 1 \n        tIndex = tIndex + 1 "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        _ = 0 \n        while _ < batch_size : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += 1 \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n            _ = _ + 1 \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_ready_workers_running < num_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            elif num_workers_running > num_workers_expected : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                _ = 0 \n                while _ < excess : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess -= 1 \n                    wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n                    _ = _ + 1 \n            elif num_workers_running == num_workers_expected : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10 ) \n                if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"164": "\ndef chunks ( items , chunk_size ) : \n    if chunk_size <= 0 : \n        raise ValueError ( 'Chunk size must be a positive integer' ) \n    i = 0 \n    while i < len ( items ) : \n        yield items [ i : i + chunk_size ] \n        i = i + chunk_size "}
{"167": "\ndef pprinttable ( rows ) : \n    if not rows : \n        return \n    if hasattr ( rows [ 0 ] , '_fields' ) : \n        headers = rows [ 0 ] . _fields \n    else : \n        headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] \n    lens = [ len ( s ) for s in headers ] \n    for row in rows : \n        i = 0 \n        while i < len ( rows [ 0 ] ) : \n            slenght = len ( \"{}\" . format ( row [ i ] ) ) \n            if slenght > lens [ i ] : \n                lens [ i ] = slenght \n            i = i + 1 \n    formats = [ ] \n    hformats = [ ] \n    i = 0 \n    while i < len ( rows [ 0 ] ) : \n        if isinstance ( rows [ 0 ] [ i ] , int ) : \n            formats . append ( \"%%%dd\" % lens [ i ] ) \n        else : \n            formats . append ( \"%%-%ds\" % lens [ i ] ) \n        hformats . append ( \"%%-%ds\" % lens [ i ] ) \n        i = i + 1 \n    pattern = \" | \" . join ( formats ) \n    hpattern = \" | \" . join ( hformats ) \n    separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) \n    s = \"\" \n    s += separator + '\\n' \n    s += ( hpattern % tuple ( headers ) ) + '\\n' \n    s += separator + '\\n' \n    def f ( t ) : \n        return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t \n    for line in rows : \n        s += pattern % tuple ( f ( t ) for t in line ) + '\\n' \n    s += separator + '\\n' \n    return s "}
{"525": "\ndef date_range ( start_date , end_date = None , num = None , delta = None ) : \n    if not delta : \n        return [ ] \n    if end_date and start_date > end_date : \n        raise Exception ( \"Wait. start_date needs to be before end_date\" ) \n    if end_date and num : \n        raise Exception ( \"Wait. Either specify end_date OR num\" ) \n    if not end_date and not num : \n        end_date = timezone . utcnow ( ) \n    delta_iscron = False \n    tz = start_date . tzinfo \n    if isinstance ( delta , six . string_types ) : \n        delta_iscron = True \n        start_date = timezone . make_naive ( start_date , tz ) \n        cron = croniter ( delta , start_date ) \n    elif isinstance ( delta , timedelta ) : \n        delta = abs ( delta ) \n    dates = [ ] \n    if end_date : \n        if timezone . is_naive ( start_date ) : \n            end_date = timezone . make_naive ( end_date , tz ) \n        while start_date <= end_date : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                start_date = cron . get_next ( datetime ) \n            else : \n                start_date += delta \n    else : \n        _ = 0 \n        while _ < abs ( num ) : \n            if timezone . is_naive ( start_date ) : \n                dates . append ( timezone . make_aware ( start_date , tz ) ) \n            else : \n                dates . append ( start_date ) \n            if delta_iscron : \n                if num > 0 : \n                    start_date = cron . get_next ( datetime ) \n                else : \n                    start_date = cron . get_prev ( datetime ) \n            else : \n                if num > 0 : \n                    start_date += delta \n                else : \n                    start_date -= delta \n            _ = _ + 1 \n    return sorted ( dates ) "}
{"587": "\ndef read_image_file ( data_dir , image_ext , n ) : \n    def PIL2array ( _img ) : \n        return np . array ( _img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) \n    def find_files ( _data_dir , _image_ext ) : \n        files = [ ] \n        for file_dir in os . listdir ( _data_dir ) : \n            if file_dir . endswith ( _image_ext ) : \n                files . append ( os . path . join ( _data_dir , file_dir ) ) \n        return sorted ( files ) \n    patches = [ ] \n    list_files = find_files ( data_dir , image_ext ) \n    for fpath in list_files : \n        img = Image . open ( fpath ) \n        y = 0 \n        while y < 1024 : \n            x = 0 \n            while x < 1024 : \n                patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) \n                patches . append ( PIL2array ( patch ) ) \n                x = x + 64 \n            y = y + 64 \n    return torch . ByteTensor ( np . array ( patches [ : n ] ) ) "}
{"598": "\ndef get_params ( img , scale , ratio ) : \n    area = img . size [ 0 ] * img . size [ 1 ] \n    attempt = 0 \n    while attempt < 10 : \n        target_area = random . uniform ( * scale ) * area \n        log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) \n        aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) \n        w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) \n        h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) \n        if w <= img . size [ 0 ] and h <= img . size [ 1 ] : \n            i = random . randint ( 0 , img . size [ 1 ] - h ) \n            j = random . randint ( 0 , img . size [ 0 ] - w ) \n            return i , j , h , w \n        attempt = attempt + 1 \n    in_ratio = img . size [ 0 ] / img . size [ 1 ] \n    if ( in_ratio < min ( ratio ) ) : \n        w = img . size [ 0 ] \n        h = w / min ( ratio ) \n    elif ( in_ratio > max ( ratio ) ) : \n        h = img . size [ 1 ] \n        w = h * max ( ratio ) \n    else : \n        w = img . size [ 0 ] \n        h = img . size [ 1 ] \n    i = ( img . size [ 1 ] - h ) // 2 \n    j = ( img . size [ 0 ] - w ) // 2 \n    return i , j , h , w "}
{"649": "\ndef _expand_to_event_rank ( self , x ) : \n    expanded_x = x \n    _ = 0 \n    while _ < tensorshape_util . rank ( self . event_shape ) : \n        expanded_x = tf . expand_dims ( expanded_x , - 1 ) \n        _ = _ + 1 \n    return expanded_x "}
{"669": "\ndef _primes_less_than ( n ) : \n    small_primes = np . array ( ( 2 , 3 , 5 ) ) \n    if n <= 6 : \n        return small_primes [ small_primes < n ] \n    sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) \n    sieve [ 0 ] = False \n    m = int ( n ** 0.5 ) // 3 + 1 \n    i = 0 \n    while i < m : \n        if not sieve [ i ] : \n            i = i + 1 \n            continue \n        k = 3 * i + 1 | 1 \n        sieve [ k ** 2 // 3 : : 2 * k ] = False \n        sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = False \n        i = i + 1 \n    return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ] "}
{"713": "\ndef _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : \n    if dist_names is None : \n        dist_names = [ ] \n    else : \n        dist_names = dist_names . copy ( ) \n    n = len ( dist_fn_args ) \n    dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) \n    for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : \n        if not args : \n            continue \n        i = n - i_ - 1 \n        for j , arg_name in enumerate ( args ) : \n            dist_names [ i - j - 1 ] = arg_name \n    j = 0 \n    i_ = 0 \n    while i_ < len ( dist_names ) : \n        i = n - i_ - 1 \n        if dist_names [ i ] is None : \n            dist_names [ i ] = leaf_name if j == 0 else leaf_name + str ( j ) \n            j += 1 \n        i_ = i_ + 1 \n    return tuple ( dist_names ) "}
{"732": "\ndef sample_dynamic_prior ( self , samples , batch_size , length , fixed = False ) : \n    if fixed : \n        sample_batch_size = 1 \n    else : \n        sample_batch_size = batch_size \n    sample , state = self . dynamic_prior . zero_state ( [ samples , sample_batch_size ] ) \n    locs = [ ] \n    scale_diags = [ ] \n    sample_list = [ ] \n    _ = 0 \n    while _ < length : \n        dist , state = self . dynamic_prior ( sample , state ) \n        sample = dist . sample ( ) \n        locs . append ( dist . parameters [ \"loc\" ] ) \n        scale_diags . append ( dist . parameters [ \"scale_diag\" ] ) \n        sample_list . append ( sample ) \n        _ = _ + 1 \n    sample = tf . stack ( sample_list , axis = 2 ) \n    loc = tf . stack ( locs , axis = 2 ) \n    scale_diag = tf . stack ( scale_diags , axis = 2 ) \n    if fixed : \n        sample = sample + tf . zeros ( [ batch_size , 1 , 1 ] ) \n    return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) "}
{"745": "\ndef _log_ndtr_asymptotic_series ( x , series_order ) : \n    npdt = dtype_util . as_numpy_dtype ( x . dtype ) \n    if series_order <= 0 : \n        return npdt ( 1 ) \n    x_2 = tf . square ( x ) \n    even_sum = tf . zeros_like ( x ) \n    odd_sum = tf . zeros_like ( x ) \n    x_2n = x_2 \n    n = 1 \n    while n < series_order + 1 : \n        y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n \n        if n % 2 : \n            odd_sum += y \n        else : \n            even_sum += y \n        x_2n *= x_2 \n        n = n + 1 \n    return 1. + even_sum - odd_sum "}
{"755": "\ndef bootstrap_results ( self , state ) : \n    def loss ( ) : \n        q = self . _flattened_variational_distribution ( ) \n        samples = q . sample ( self . train_batch_size ) \n        return tf . reduce_mean ( input_tensor = q . log_prob ( samples ) - self . _flattened_target_log_prob ( samples ) , axis = - 1 ) \n    lr = tf . convert_to_tensor ( value = self . learning_rate , dtype = self . _dtype ) \n    dtype = lr . dtype \n    learning_rate = tf . compat . v2 . optimizers . schedules . PiecewiseConstantDecay ( list ( self . num_train_steps * np . array ( [ 0.2 , 0.8 ] ) . astype ( dtype . as_numpy_dtype ( ) ) ) , [ lr , lr * 0.1 , lr * 0.01 ] ) \n    opt = tf . compat . v2 . optimizers . Adam ( learning_rate ) \n    \n    @ tf . function ( autograph = False ) \n    def train_step ( ) : \n        with tf . GradientTape ( ) as tape : \n            loss_val = loss ( ) \n        vals = tape . watched_variables ( ) \n        grads = tape . gradient ( loss_val , vals ) \n        grads_and_vals = list ( zip ( grads , vals ) ) \n        opt . apply_gradients ( grads_and_vals ) \n        return loss_val \n    step = 0 \n    while step < self . num_train_steps : \n        loss_val = train_step ( ) \n        tf . debugging . assert_all_finite ( loss_val , 'NeuTra loss is NaN at step {}' . format ( step ) ) \n        if self . train_debug_fn : \n            self . train_debug_fn ( self , step , loss_val ) \n        step = step + 1 \n    state_parts = tf . nest . flatten ( state ) \n    flat_state_shapes = tf . nest . flatten ( self . state_shape ) \n    batch_shape = tf . shape ( input = state_parts [ 0 ] ) [ : - flat_state_shapes [ 0 ] . ndims ] \n    return self . _kernel . bootstrap_results ( self . _flattened_variational_distribution ( ) . sample ( batch_shape , seed = self . seed ) ) "}
{"776": "\ndef _log_normalization ( self , name = 'log_normalization' ) : \n    with tf . name_scope ( name or 'log_normalization_lkj' ) : \n        logpi = np . log ( np . pi ) \n        ans = tf . zeros_like ( self . concentration ) \n        k = 1 \n        while k < self . dimension : \n            ans += logpi * ( k / 2. ) \n            ans += tf . math . lgamma ( self . concentration + ( self . dimension - 1 - k ) / 2. ) \n            ans -= tf . math . lgamma ( self . concentration + ( self . dimension - 1 ) / 2. ) \n            k = k + 1 \n        return ans "}
{"846": "\ndef save_imgs ( x , fname ) : \n    n = x . shape [ 0 ] \n    fig = figure . Figure ( figsize = ( n , 1 ) , frameon = False ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    i = 0 \n    while i < n : \n        ax = fig . add_subplot ( 1 , n , i + 1 ) \n        ax . imshow ( x [ i ] . squeeze ( ) , interpolation = \"none\" , cmap = cm . get_cmap ( \"binary\" ) ) \n        ax . axis ( \"off\" ) \n        i = i + 1 \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved %s\" % fname ) "}
{"871": "\ndef plot_heldout_prediction ( input_vals , probs , fname , n = 10 , title = \"\" ) : \n    fig = figure . Figure ( figsize = ( 9 , 3 * n ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    i = 0 \n    while i < n : \n        ax = fig . add_subplot ( n , 3 , 3 * i + 1 ) \n        ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - 1 ] ) , interpolation = \"None\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 2 ) \n        for prob_sample in probs : \n            sns . barplot ( np . arange ( 10 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) \n            ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"posterior samples\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 3 ) \n        sns . barplot ( np . arange ( 10 ) , np . mean ( probs [ : , i , : ] , axis = 0 ) , ax = ax ) \n        ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"predictive probs\" ) \n        i = i + 1 \n    fig . suptitle ( title ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"970": "\ndef smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : \n        loop_num_iter_ = tf . get_static_value ( loop_num_iter ) \n        if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : \n            loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) \n            return tf . while_loop ( cond = lambda i , * args : i < loop_num_iter , body = lambda i , * args : [ i + 1 ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( 0 ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ 1 : ] \n        result = initial_loop_vars \n        _ = 0 \n        while _ < loop_num_iter_ : \n            result = body_fn ( * result ) \n            _ = _ + 1 \n        return result "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        replica = 0 \n        while replica < self . num_replica : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n            replica = replica + 1 \n        target_log_probs = tf . stack ( target_log_probs , axis = 0 ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ 0 ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return i < exchange_proposed_n \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio \n            k = 0 \n            while k < num_state_parts : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n                k = k + 1 \n            return i + 1 , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ] "}
{"1010": "\ndef bayesian_resnet ( input_shape , num_classes = 10 , kernel_posterior_scale_mean = - 9.0 , kernel_posterior_scale_stddev = 0.1 , kernel_posterior_scale_constraint = 0.2 ) : \n    filters = [ 64 , 128 , 256 , 512 ] \n    kernels = [ 3 , 3 , 3 , 3 ] \n    strides = [ 1 , 2 , 2 , 2 ] \n    def _untransformed_scale_constraint ( t ) : \n        return tf . clip_by_value ( t , - 1000 , tf . math . log ( kernel_posterior_scale_constraint ) ) \n    kernel_posterior_fn = tfp . layers . default_mean_field_normal_fn ( untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = kernel_posterior_scale_mean , stddev = kernel_posterior_scale_stddev ) , untransformed_scale_constraint = _untransformed_scale_constraint ) \n    image = tf . keras . layers . Input ( shape = input_shape , dtype = 'float32' ) \n    x = tfp . layers . Convolution2DFlipout ( 64 , 3 , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( image ) \n    i = 0 \n    while i < len ( kernels ) : \n        x = _resnet_block ( x , filters [ i ] , kernels [ i ] , strides [ i ] , kernel_posterior_fn ) \n        i = i + 1 \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    x = tf . keras . layers . AveragePooling2D ( 4 , 1 ) ( x ) \n    x = tf . keras . layers . Flatten ( ) ( x ) \n    x = tfp . layers . DenseFlipout ( num_classes , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    model = tf . keras . Model ( inputs = image , outputs = x , name = 'resnet18' ) \n    return model "}
{"1115": "\ndef _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : \n    slices = [ ] \n    col = 0 \n    d_in = n_in // num_blocks \n    d_out = n_out // num_blocks \n    row = d_out if mask_type == MASK_EXCLUSIVE else 0 \n    _ = 0 \n    while _ < num_blocks : \n        row_slice = slice ( row , None ) \n        col_slice = slice ( col , col + d_in ) \n        slices . append ( [ row_slice , col_slice ] ) \n        col += d_in \n        row += d_out \n        _ = _ + 1 \n    return slices "}
{"1122": "\ndef build ( self , input_shape ) : \n    if self . _event_shape is None : \n        self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - 1 ] ) ] \n        self . _event_size = self . _event_shape [ - 1 ] \n        self . _event_ndims = len ( self . _event_shape ) \n    if input_shape [ - 1 ] != self . _event_shape [ - 1 ] : \n        raise ValueError ( \"Invalid final dimension of `input_shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . _event_shape [ - 1 ] , input_shape [ - 1 ] ) ) \n    self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) \n    self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) \n    self . _masks [ - 1 ] = np . reshape ( np . tile ( self . _masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . _params ] ) , [ self . _masks [ - 1 ] . shape [ 0 ] , self . _event_size * self . _params ] ) \n    self . _network = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) \n    layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] \n    k = 0 \n    while k < len ( self . _masks ) : \n        self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if k + 1 < len ( self . _masks ) else None , use_bias = self . _use_bias , ** self . _kwargs ) ) \n        k = k + 1 \n    super ( AutoregressiveLayer , self ) . build ( input_shape ) "}
{"1200": "\ndef precompute_future_symbols ( trie , n , allow_spaces = False ) : \n    if n == 0 : \n        return \n    if trie . is_terminated and trie . precompute_symbols : \n        return \n    for index , final in enumerate ( trie . final ) : \n        trie . data [ index ] = [ set ( ) for i in range ( n ) ] \n    for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n        node_data [ 0 ] = set ( trie . _get_letters ( index ) ) \n        if allow_spaces and final : \n            node_data [ 0 ] . add ( \" \" ) \n    d = 1 \n    while d < n : \n        for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : \n            children = set ( trie . _get_children ( index ) ) \n            for child in children : \n                node_data [ d ] |= trie . data [ child ] [ d - 1 ] \n            if allow_spaces and final : \n                node_data [ d ] |= trie . data [ trie . root ] [ d - 1 ] \n        d = d + 1 \n    trie . terminated = True "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        order = 1 \n        while order < max_order + 1 : \n            possible_matches = len ( translation ) - order + 1 \n            if possible_matches > 0 : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n            order = order + 1 \n    precisions = [ 0 ] * max_order \n    i = 0 \n    while i < max_order : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if possible_matches_by_order [ i ] > 0 : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n        i = i + 1 \n    if min ( precisions ) > 0 : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1230": "\ndef cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2 , keep_prob = 1.0 , concat_stacked_outputs = False , trainable_initial_states = False , name = 'cudnn_stacked_bi_gru' , reuse = False ) : \n    if seq_lengths is None : \n        seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] \n    outputs = [ units ] \n    with tf . variable_scope ( name , reuse = reuse ) : \n        n = 0 \n        while n < n_stacks : \n            if n == 0 : \n                inputs = outputs [ - 1 ] \n            else : \n                inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) \n            ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = 1 , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) \n            outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) \n            n = n + 1 \n    if concat_stacked_outputs : \n        return tf . concat ( outputs [ 1 : ] , axis = 2 ) \n    return outputs [ - 1 ] "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = 0 \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) \n        _ = 0 \n        while _ < self . char_conv_layers - 1 : \n            curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if self . conv_dropout > 0.0 : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n            _ = _ + 1 \n        curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ += curr_filters_number \n    if len ( conv_outputs ) > 1 : \n        conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ 0 ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) \n    if self . intermediate_dropout > 0.0 : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    i = 0 \n    while i < self . char_highway_layers - 1 : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if self . highway_dropout > 0.0 : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n        i = i + 1 \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1234": "\ndef _build_basic_network ( self , word_outputs ) : \n    if self . word_dropout > 0.0 : \n        lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) \n    else : \n        lstm_outputs = word_outputs \n    j = 0 \n    while j < self . word_lstm_layers - 1 : \n        lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n        j = j + 1 \n    lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) \n    pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = \"softmax\" , activity_regularizer = self . regularizer ) , name = \"p\" ) ( lstm_outputs ) \n    return pre_outputs , lstm_outputs "}
{"1254": "\ndef _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : \n    weight_name_map = { } \n    i = 0 \n    while i < 2 : \n        j = 0 \n        while j < 8 : \n            root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) \n            weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' \n            weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' \n            weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' \n            j = j + 1 \n        i = i + 1 \n    varname_in_file = varname [ 5 : ] \n    if varname_in_file . startswith ( 'RNN' ) : \n        varname_in_file = weight_name_map [ varname_in_file ] \n    if varname_in_file == 'embedding' : \n        with h5py . File ( embedding_weight_file , 'r' ) as fin : \n            embed_weights = fin [ varname_in_file ] [ ... ] \n            weights = np . zeros ( ( embed_weights . shape [ 0 ] + 1 , embed_weights . shape [ 1 ] ) , dtype = DTYPE ) \n            weights [ 1 : , : ] = embed_weights \n    else : \n        with h5py . File ( weight_file , 'r' ) as fin : \n            if varname_in_file == 'char_embed' : \n                char_embed_weights = fin [ varname_in_file ] [ ... ] \n                weights = np . zeros ( ( char_embed_weights . shape [ 0 ] + 1 , char_embed_weights . shape [ 1 ] ) , dtype = DTYPE ) \n                weights [ 1 : , : ] = char_embed_weights \n            else : \n                weights = fin [ varname_in_file ] [ ... ] \n    def ret ( shape , ** kwargs ) : \n        if list ( shape ) != list ( weights . shape ) : \n            raise ValueError ( \"Invalid shape initializing {0}, got {1}, expected {2}\" . format ( varname_in_file , shape , weights . shape ) ) \n        return weights \n    return ret "}
{"1276": "\ndef predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : \n    if file_path is None or file_path == '-' : \n        if sys . stdin . isatty ( ) : \n            raise RuntimeError ( 'To process data from terminal please use interact mode' ) \n        f = sys . stdin \n    else : \n        f = open ( file_path , encoding = 'utf8' ) \n    model : Chainer = build_model ( config ) \n    args_count = len ( model . in_x ) \n    while True : \n        batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) \n        if not batch : \n            break \n        args = [ ] \n        i = 0 \n        while i < args_count : \n            args . append ( batch [ i : : args_count ] ) \n            i = i + 1 \n        res = model ( * args ) \n        if len ( model . out_params ) == 1 : \n            res = [ res ] \n        for res in zip ( * res ) : \n            res = json . dumps ( res , ensure_ascii = False ) \n            print ( res , flush = True ) \n    if f is not sys . stdin : \n        f . close ( ) "}
{"1282": "\ndef _get_best ( values : List [ float ] , losses : List [ float ] , max_loss_div : float = 0.9 , min_val_div : float = 10.0 ) -> float : \n    assert len ( values ) == len ( losses ) , \"lengths of values and losses should be equal\" \n    min_ind = np . argmin ( losses ) \n    i = min_ind - 1 \n    while i < 0 : \n        if ( losses [ i ] * max_loss_div > losses [ min_ind ] ) or ( values [ i ] * min_val_div < values [ min_ind ] ) : \n            return values [ i + 1 ] \n        i = i + - 1 \n    return values [ min_ind ] / min_val_div "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2 ] \n        assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and fr . f_code . co_filename == this_filename : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                i = 0 \n                while i < fr . f_lineno - 1 : \n                    next ( f ) \n                    i = i + 1 \n                g = tokenize . generate_tokens ( f . readline ) \n                step = 0 \n                args_tokens = [ ] \n                level = 0 \n                for ttt in g : \n                    if step == 0 : \n                        if ttt [ 0 ] != tokenize . NAME : \n                            continue \n                        if not ttt [ 1 ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = 1 \n                    elif step == 1 : \n                        assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \"(\" \n                        args_tokens . append ( [ ] ) \n                        step = 2 \n                    elif step == 2 : \n                        if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \",\" : \n                            args_tokens . append ( [ ] ) \n                        elif level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \")\" : \n                            break \n                        else : \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \"([{\" : \n                                level += 1 \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \")]}\" : \n                                level -= 1 \n                            assert level >= 0 , \"Parse error: parentheses level became negative\" \n                            args_tokens [ - 1 ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    _ = 0 \n    while _ < 20 : \n        if v == 0 : \n            return 1e20 \n        t += ( x_target - x ) / v \n        x , v = self . _compute_progress_at_time ( t ) \n        if abs ( x - x_target ) < 1e-3 : \n            return t \n        _ = _ + 1 \n    return time . time ( ) + 100 "}
{"1406": "\ndef structure ( self ) : \n    df = self . as_data_frame ( use_pandas = False ) \n    cn = df . pop ( 0 ) \n    nr = self . nrow \n    nc = self . ncol \n    width = max ( [ len ( c ) for c in cn ] ) \n    isfactor = self . isfactor ( ) \n    numlevels = self . nlevels ( ) \n    lvls = self . levels ( ) \n    print ( \"H2OFrame: '{}' \\nDimensions: {} obs. of {} variables\" . format ( self . frame_id , nr , nc ) ) \n    i = 0 \n    while i < nc : \n        print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) \n        if isfactor [ i ] : \n            nl = numlevels [ i ] \n            print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) \n        else : \n            print ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as_list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) ) \n        i = i + 1 "}
{"1416": "\ndef impute ( self , column = - 1 , method = \"mean\" , combine_method = \"interpolate\" , by = None , group_by_frame = None , values = None ) : \n    if is_type ( column , str ) : \n        column = self . names . index ( column ) \n    if is_type ( by , str ) : \n        by = self . names . index ( by ) \n    if values is None : \n        values = \"_\" \n    else : \n        assert len ( values ) == len ( self . columns ) , \"Length of values does not match length of columns\" \n        values2 = [ ] \n        i = 0 \n        while i < len ( values ) : \n            if self . type ( i ) == \"enum\" : \n                try : \n                    values2 . append ( self . levels ( ) [ i ] . index ( values [ i ] ) ) \n                except : \n                    raise H2OValueError ( \"Impute value of: \" + values [ i ] + \" not found in existing levels of\" \" column: \" + self . col_names [ i ] ) \n            else : \n                values2 . append ( values [ i ] ) \n            i = i + 1 \n        values = values2 \n    if group_by_frame is None : \n        group_by_frame = \"_\" \n    self . _ex . _eager_frame ( ) \n    if by is not None or group_by_frame is not \"_\" : \n        res = H2OFrame . _expr ( expr = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) ) . _frame ( ) \n    else : \n        res = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) . _eager_scalar ( ) \n    self . _ex . _cache . flush ( ) \n    self . _ex . _cache . fill ( 10 ) \n    return res "}
{"1477": "\ndef get_human_readable_bytes ( size ) : \n    if size == 0 : \n        return \"0\" \n    if size is None : \n        return \"\" \n    assert_is_type ( size , int ) \n    assert size >= 0 , \"`size` cannot be negative, got %d\" % size \n    suffixes = \"PTGMk\" \n    maxl = len ( suffixes ) \n    i = 0 \n    while i < maxl + 1 : \n        shift = ( maxl - i ) * 10 \n        if size >> shift == 0 : \n            i = i + 1 \n            continue \n        ndigits = 0 \n        for nd in [ 3 , 2 , 1 ] : \n            if size >> ( shift + 12 - nd * 3 ) == 0 : \n                ndigits = nd \n                break \n        if ndigits == 0 or size == ( size >> shift ) << shift : \n            rounded_val = str ( size >> shift ) \n        else : \n            rounded_val = \"%.*f\" % ( ndigits , size / ( 1 << shift ) ) \n        return \"%s %sb\" % ( rounded_val , suffixes [ i ] if i < maxl else \"\" ) \n        i = i + 1 "}
{"1494": "\ndef translate_name ( name ) : \n    parts = name . split ( \"_\" ) \n    i = 0 \n    while parts [ i ] == \"\" : \n        parts [ i ] = \"_\" \n        i += 1 \n    parts [ i ] = parts [ i ] . lower ( ) \n    j = i + 1 \n    while j < len ( parts ) : \n        parts [ j ] = parts [ j ] . capitalize ( ) \n        j = j + 1 \n    i = len ( parts ) - 1 \n    while parts [ i ] == \"\" : \n        parts [ i ] = \"_\" \n        i -= 1 \n    return \"\" . join ( parts ) "}
{"1513": "\ndef inverse_transform ( self , X , y = None , ** params ) : \n    i = 0 \n    while i < X . ncol : \n        X [ i ] = self . means [ i ] + self . stds [ i ] * X [ i ] \n        i = i + 1 \n    return X "}
{"1518": "\ndef find_build_failure ( each_line , temp_func_list ) : \n    global g_build_success \n    global g_build_success_tests \n    global g_failed_test_info_dict \n    global g_failure_occurred \n    global g_build_failed_message \n    ind = 0 \n    while ind < len ( g_build_failed_message ) : \n        if g_build_failed_message [ ind ] in each_line . lower ( ) : \n            if ( ( ind == 0 ) and ( len ( g_failed_jobs ) > 0 ) ) : \n                ind = ind + 1 \n                continue \n            else : \n                g_failure_occurred = True \n                g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' \n                temp_func_list . remove ( find_build_failure ) \n                return False \n        ind = ind + 1 \n    return True "}
{"1525": "\ndef write_java_message ( key , val , text_file ) : \n    text_file . write ( key ) \n    text_file . write ( '\\n' ) \n    if ( len ( val [ 0 ] ) > 0 ) and ( len ( val ) >= 3 ) : \n        index = 0 \n        while index < len ( val [ 0 ] ) : \n            text_file . write ( \"Java Message Type: \" ) \n            text_file . write ( val [ 1 ] [ index ] ) \n            text_file . write ( '\\n' ) \n            text_file . write ( \"Java Message: \" ) \n            for jmess in val [ 2 ] [ index ] : \n                text_file . write ( jmess ) \n                text_file . write ( '\\n' ) \n            index = index + 1 \n        text_file . write ( '\\n \\n' ) "}
{"1554": "\ndef _jar_paths ( ) : \n    own_jar = os . getenv ( \"H2O_JAR_PATH\" , \"\" ) \n    if own_jar != \"\" : \n        if not os . path . isfile ( own_jar ) : \n            raise H2OStartupError ( \"Environment variable H2O_JAR_PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own_jar ) \n        yield own_jar \n    cwd_chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) \n    i = len ( cwd_chunks ) \n    while i < 0 : \n        if cwd_chunks [ i - 1 ] == \"h2o-3\" : \n            yield os . path . sep . join ( cwd_chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) \n        i = i + - 1 \n    backend_dir = os . path . split ( os . path . realpath ( __file__ ) ) [ 0 ] \n    yield os . path . join ( backend_dir , \"bin\" , \"h2o.jar\" ) \n    prefix1 = prefix2 = sys . prefix \n    if prefix1 . startswith ( os . path . sep + \"Library\" ) : \n        prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) \n    elif prefix1 . startswith ( os . path . sep + \"System\" ) : \n        prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] \n    yield os . path . join ( prefix1 , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix1 , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( get_config_var ( \"userbase\" ) , \"h2o_jar\" , \"h2o.jar\" ) \n    yield os . path . join ( prefix2 , \"h2o_jar\" , \"h2o.jar\" ) "}
{"1577": "\ndef summarizeFailedRuns ( ) : \n    global g_summary_dict_all \n    onlyFiles = [ x for x in listdir ( g_test_root_dir ) if isfile ( join ( g_test_root_dir , x ) ) ] \n    for f in onlyFiles : \n        for fileStart in g_file_start : \n            if ( fileStart in f ) and ( os . path . getsize ( f ) > 10 ) : \n                fFullPath = os . path . join ( g_test_root_dir , f ) \n                try : \n                    temp_dict = json . load ( open ( fFullPath , 'r' ) ) \n                    ind = 0 \n                    while ind < len ( temp_dict [ \"TestName\" ] ) : \n                        addFailedTests ( g_summary_dict_all , temp_dict , ind ) \n                        ind = ind + 1 \n                except : \n                    continue \n                break "}
{"1578": "\ndef extractPrintSaveIntermittens ( ) : \n    global g_summary_dict_intermittents \n    localtz = time . tzname [ 0 ] \n    ind = 0 \n    while ind < len ( g_summary_dict_all [ \"TestName\" ] ) : \n        if g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] >= g_threshold_failure : \n            addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) \n        ind = ind + 1 \n    if len ( g_summary_dict_intermittents [ \"TestName\" ] ) > 0 : \n        json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) \n        with open ( g_summary_csv_filename , 'w' ) as summaryFile : \n            ind = 0 \n            while ind < len ( g_summary_dict_intermittents [ \"TestName\" ] ) : \n                testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] \n                numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] \n                firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ 0 ] ) \n                summaryFile . write ( eachTest ) \n                print ( \"Intermittent: {0}, Last failed: {1}, Failed {2} times since \" \"{3}\" . format ( testName , recentFailStr , numberFailure , firstFailedStr ) ) \n                ind = ind + 1 "}
{"1736": "\ndef wait_for_operation_status ( self , request_id , wait_for_status = 'Succeeded' , timeout = 30 , sleep_interval = 5 , progress_callback = wait_for_operation_status_progress_default_callback , success_callback = wait_for_operation_status_success_default_callback , failure_callback = wait_for_operation_status_failure_default_callback ) : \n    loops = timeout // sleep_interval + 1 \n    start_time = time . time ( ) \n    _ = 0 \n    while _ < int ( loops ) : \n        result = self . get_operation_status ( request_id ) \n        elapsed = time . time ( ) - start_time \n        if result . status == wait_for_status : \n            if success_callback is not None : \n                success_callback ( elapsed ) \n            return result \n        elif result . error : \n            if failure_callback is not None : \n                ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_FAILURE , result . status , result ) \n                failure_callback ( elapsed , ex ) \n            return result \n        else : \n            if progress_callback is not None : \n                progress_callback ( elapsed ) \n            time . sleep ( sleep_interval ) \n        _ = _ + 1 \n    if failure_callback is not None : \n        ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_TIMEOUT , result . status , result ) \n        failure_callback ( elapsed , ex ) \n    return result "}
{"1884": "\ndef get_cut_prefix ( value , max_len ) : \n    should_convert = isinstance ( value , bytes ) \n    if should_convert : \n        value = value . decode ( \"utf8\" , \"ignore\" ) \n    i = 0 \n    while i < len ( value ) : \n        if terminal_width ( value [ i : ] ) <= max_len : \n            break \n        i = i + 1 \n    return value [ i : ] . encode ( \"utf8\" , \"ignore\" ) if should_convert else value [ i : ] "}
{"1925": "\ndef output_stream ( plugin , stream ) : \n    global output \n    success_open = False \n    i = 0 \n    while i < args . retry_open : \n        try : \n            stream_fd , prebuffer = open_stream ( stream ) \n            success_open = True \n            break \n        except StreamError as err : \n            log . error ( \"Try {0}/{1}: Could not open stream {2} ({3})\" , i + 1 , args . retry_open , stream , err ) \n        i = i + 1 \n    if not success_open : \n        console . exit ( \"Could not open stream {0}, tried {1} times, exiting\" , stream , args . retry_open ) \n    output = create_output ( plugin ) \n    try : \n        output . open ( ) \n    except ( IOError , OSError ) as err : \n        if isinstance ( output , PlayerOutput ) : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n        else : \n            console . exit ( \"Failed to open output: {0} ({1})\" , args . output , err ) \n    with closing ( output ) : \n        log . debug ( \"Writing stream to output\" ) \n        read_stream ( stream_fd , output , prebuffer ) \n    return True "}
{"1979": "\ndef setLength ( self , lengthTable ) : \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    nextCodes = [ ] \n    code = 0 \n    bits = 0 \n    while bits < self . maxLength + 1 : \n        code <<= 1 \n        nextCodes . append ( code ) \n        code += sum ( x == bits for x in lengthTable . values ( ) ) \n        bits = bits + 1 \n    self . decodeTable = { } \n    for symbol in sorted ( lengthTable ) : \n        bits = lengthTable [ symbol ] \n        bitpattern = '{:0{}b}' . format ( nextCodes [ bits ] , bits ) \n        self . decodeTable [ int ( bitpattern [ : : - 1 ] , 2 ) ] = symbol \n        nextCodes [ bits ] += 1 \n    self . switchToPrefix ( ) "}
{"1980": "\ndef showCode ( self , width = 80 ) : \n    symbolStrings = [ ( self . bitPattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] \n    leftColWidth , rightColWidth = map ( max , map ( map , repeat ( len ) , zip ( * symbolStrings ) ) ) \n    colwidth = leftColWidth + rightColWidth \n    columns = 81 // ( colwidth + 2 ) \n    rows = - ( - len ( symbolStrings ) // columns ) \n    def justify ( bs ) : \n        b , s = bs \n        return b . rjust ( leftColWidth ) + ':' + s . ljust ( rightColWidth ) \n    i = 0 \n    while i < rows : \n        print ( ' ' . join ( map ( justify , symbolStrings [ i : : rows ] ) ) . rstrip ( ) ) \n        i = i + 1 "}
{"1996": "\ndef readPrefixArray ( self , kind , numberOfTrees ) : \n    prefixes = [ ] \n    i = 0 \n    while i < numberOfTrees : \n        if kind == L : \n            alphabet = LiteralAlphabet ( i ) \n        elif kind == I : \n            alphabet = InsertAndCopyAlphabet ( i ) \n        elif kind == D : \n            alphabet = DistanceAlphabet ( i , NPOSTFIX = self . NPOSTFIX , NDIRECT = self . NDIRECT ) \n        self . readPrefixCode ( alphabet ) \n        prefixes . append ( alphabet ) \n        i = i + 1 \n    self . prefixCodes [ kind ] = prefixes "}
{"2065": "\ndef add_virtual_columns_rotation ( self , x , y , xnew , ynew , angle_degrees , propagate_uncertainties = False ) : \n    x = _ensure_string_from_expression ( x ) \n    y = _ensure_string_from_expression ( y ) \n    theta = np . radians ( angle_degrees ) \n    matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) ] , [ np . sin ( theta ) , np . cos ( theta ) ] ] ) \n    m = matrix_name = x + \"_\" + y + \"_rot\" \n    i = 0 \n    while i < 2 : \n        j = 0 \n        while j < 2 : \n            self . set_variable ( matrix_name + \"_%d%d\" % ( i , j ) , matrix [ i , j ] . item ( ) ) \n            j = j + 1 \n        i = i + 1 \n    self [ xnew ] = self . _expr ( \"{m}_00 * {x} + {m}_01 * {y}\" . format ( ** locals ( ) ) ) \n    self [ ynew ] = self . _expr ( \"{m}_10 * {x} + {m}_11 * {y}\" . format ( ** locals ( ) ) ) \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xnew ] , self [ ynew ] ] ) "}
{"2171": "\ndef icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if hop_length > min ( lengths ) : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    octave = n_octaves - 1 \n    while octave < - 1 : \n        slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , - ( octave ) * bins_per_octave - 1 ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] \n        oct_hop = hop_length // 2 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , 1 , 2 , scale = True , res_type = res_type , fix = False ) \n            y [ : len ( y_oct ) ] += y_oct \n        octave = octave + - 1 \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2177": "\ndef __dtw_calc_accu_cost ( C , D , D_steps , step_sizes_sigma , weights_mul , weights_add , max_0 , max_1 ) : \n    cur_n = max_0 \n    while cur_n < D . shape [ 0 ] : \n        cur_m = max_1 \n        while cur_m < D . shape [ 1 ] : \n            for cur_step_idx , cur_w_add , cur_w_mul in zip ( range ( step_sizes_sigma . shape [ 0 ] ) , weights_add , weights_mul ) : \n                cur_D = D [ cur_n - step_sizes_sigma [ cur_step_idx , 0 ] , cur_m - step_sizes_sigma [ cur_step_idx , 1 ] ] \n                cur_C = cur_w_mul * C [ cur_n - max_0 , cur_m - max_1 ] \n                cur_C += cur_w_add \n                cur_cost = cur_D + cur_C \n                if cur_cost < D [ cur_n , cur_m ] : \n                    D [ cur_n , cur_m ] = cur_cost \n                    D_steps [ cur_n , cur_m ] = cur_step_idx \n            cur_m = cur_m + 1 \n        cur_n = cur_n + 1 \n    return D , D_steps "}
{"2179": "\ndef _viterbi ( log_prob , log_trans , log_p_init , state , value , ptr ) : \n    n_steps , n_states = log_prob . shape \n    value [ 0 ] = log_prob [ 0 ] + log_p_init \n    t = 1 \n    while t < n_steps : \n        trans_out = value [ t - 1 ] + log_trans . T \n        j = 0 \n        while j < n_states : \n            ptr [ t , j ] = np . argmax ( trans_out [ j ] ) \n            value [ t , j ] = log_prob [ t , j ] + trans_out [ j , ptr [ t ] [ j ] ] \n            j = j + 1 \n        t = t + 1 \n    state [ - 1 ] = np . argmax ( value [ - 1 ] ) \n    t = n_steps - 2 \n    while t < - 1 : \n        state [ t ] = ptr [ t + 1 , state [ t + 1 ] ] \n        t = t + - 1 "}
{"2219": "\ndef __nn_filter_helper ( R_data , R_indices , R_ptr , S , aggregate ) : \n    s_out = np . empty_like ( S ) \n    i = 0 \n    while i < len ( R_ptr ) - 1 : \n        targets = R_indices [ R_ptr [ i ] : R_ptr [ i + 1 ] ] \n        if not len ( targets ) : \n            s_out [ i ] = S [ i ] \n            i = i + 1 \n            continue \n        neighbors = np . take ( S , targets , axis = 0 ) \n        if aggregate is np . average : \n            weights = R_data [ R_ptr [ i ] : R_ptr [ i + 1 ] ] \n            s_out [ i ] = aggregate ( neighbors , axis = 0 , weights = weights ) \n        else : \n            s_out [ i ] = aggregate ( neighbors , axis = 0 ) \n        i = i + 1 \n    return s_out "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = False , norm = 1 , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2 \n    if norm is not None and norm != 1 and norm != np . inf : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    i = 0 \n    while i < n_mels : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2 ] / fdiff [ i + 1 ] \n        weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) \n        i = i + 1 \n    if norm == 1 : \n        enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) \n        weights *= enorm [ : , np . newaxis ] \n    if not np . all ( ( mel_f [ : - 2 ] == 0 ) | ( weights . max ( axis = 1 ) > 0 ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2230": "\ndef __window_ss_fill ( x , win_sq , n_frames , hop_length ) : \n    n = len ( x ) \n    n_fft = len ( win_sq ) \n    i = 0 \n    while i < n_frames : \n        sample = i * hop_length \n        x [ sample : min ( n , sample + n_fft ) ] += win_sq [ : max ( 0 , min ( n_fft , n - sample ) ) ] \n        i = i + 1 "}
{"2243": "\ndef __match_intervals ( intervals_from , intervals_to , strict = True ) : \n    start_index = np . argsort ( intervals_to [ : , 0 ] ) \n    end_index = np . argsort ( intervals_to [ : , 1 ] ) \n    start_sorted = intervals_to [ start_index , 0 ] \n    end_sorted = intervals_to [ end_index , 1 ] \n    search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) \n    search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) \n    output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) \n    i = 0 \n    while i < len ( intervals_from ) : \n        query = intervals_from [ i ] \n        after_query = search_ends [ i ] \n        before_query = search_starts [ i ] \n        candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) \n        if len ( candidates ) > 0 : \n            output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) \n        elif strict : \n            raise ParameterError \n        else : \n            dist_before = np . inf \n            dist_after = np . inf \n            if search_starts [ i ] > 0 : \n                dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] \n            if search_ends [ i ] + 1 < len ( intervals_to ) : \n                dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] \n            if dist_before < dist_after : \n                output [ i ] = end_index [ search_starts [ i ] - 1 ] \n            else : \n                output [ i ] = start_index [ search_ends [ i ] + 1 ] \n        i = i + 1 \n    return output "}
{"2249": "\ndef harmonics_2d ( harmonic_out , x , freqs , h_range , kind = 'linear' , fill_value = 0 , axis = 0 ) : \n    idx_in = [ slice ( None ) ] * x . ndim \n    idx_freq = [ slice ( None ) ] * x . ndim \n    idx_out = [ slice ( None ) ] * harmonic_out . ndim \n    ni_axis = ( 1 + axis ) % x . ndim \n    i = 0 \n    while i < x . shape [ ni_axis ] : \n        idx_in [ ni_axis ] = slice ( i , i + 1 ) \n        idx_freq [ ni_axis ] = i \n        idx_out [ 1 + ni_axis ] = idx_in [ ni_axis ] \n        harmonics_1d ( harmonic_out [ tuple ( idx_out ) ] , x [ tuple ( idx_in ) ] , freqs [ tuple ( idx_freq ) ] , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n        i = i + 1 "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = True , axis = - 1 ) : \n    axis = np . abs ( axis ) \n    if rec . ndim != 2 or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = 1 - axis \n        lag_format = rec . format \n        if axis == 0 : \n            rec = rec . tocsc ( ) \n        elif axis in ( - 1 , 1 ) : \n            rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    else : \n        if pad : \n            padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] \n            padding [ ( 1 - axis ) ] = ( 0 , t ) \n            lag = np . pad ( rec , padding , mode = 'constant' ) \n        else : \n            lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    i = 1 \n    while i < t : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n        i = i + 1 \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2269": "\ndef lag_to_recurrence ( lag , axis = - 1 ) : \n    if axis not in [ 0 , 1 , - 1 ] : \n        raise ParameterError ( 'Invalid target axis: {}' . format ( axis ) ) \n    axis = np . abs ( axis ) \n    if lag . ndim != 2 or ( lag . shape [ 0 ] != lag . shape [ 1 ] and lag . shape [ 1 - axis ] != 2 * lag . shape [ axis ] ) : \n        raise ParameterError ( 'Invalid lag matrix shape: {}' . format ( lag . shape ) ) \n    t = lag . shape [ axis ] \n    sparse = scipy . sparse . issparse ( lag ) \n    if sparse : \n        rec = scipy . sparse . lil_matrix ( lag ) \n        roll_ax = 1 - axis \n    else : \n        rec = lag . copy ( ) \n        roll_ax = None \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    i = 1 \n    while i < t : \n        idx_slice [ axis ] = i \n        rec [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , i , axis = roll_ax ) \n        i = i + 1 \n    sub_slice = [ slice ( None ) ] * rec . ndim \n    sub_slice [ 1 - axis ] = slice ( t ) \n    rec = rec [ tuple ( sub_slice ) ] \n    if sparse : \n        return rec . asformat ( lag . format ) \n    return np . ascontiguousarray ( rec . T ) . T "}
{"2290": "\ndef frames2video ( frame_dir , video_file , fps = 30 , fourcc = 'XVID' , filename_tmpl = '{:06d}.jpg' , start = 0 , end = 0 , show_progress = True ) : \n    if end == 0 : \n        ext = filename_tmpl . split ( '.' ) [ - 1 ] \n        end = len ( [ name for name in scandir ( frame_dir , ext ) ] ) \n    first_file = osp . join ( frame_dir , filename_tmpl . format ( start ) ) \n    check_file_exist ( first_file , 'The start frame not found: ' + first_file ) \n    img = cv2 . imread ( first_file ) \n    height , width = img . shape [ : 2 ] \n    resolution = ( width , height ) \n    vwriter = cv2 . VideoWriter ( video_file , VideoWriter_fourcc ( * fourcc ) , fps , resolution ) \n    def write_frame ( file_idx ) : \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        img = cv2 . imread ( filename ) \n        vwriter . write ( img ) \n    if show_progress : \n        track_progress ( write_frame , range ( start , end ) ) \n    else : \n        i = start \n        while i < end : \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i ) ) \n            img = cv2 . imread ( filename ) \n            vwriter . write ( img ) \n            i = i + 1 \n    vwriter . release ( ) "}
{"2293": "\ndef cvt2frames ( self , frame_dir , file_start = 0 , filename_tmpl = '{:06d}.jpg' , start = 0 , max_num = 0 , show_progress = True ) : \n    mkdir_or_exist ( frame_dir ) \n    if max_num == 0 : \n        task_num = self . frame_cnt - start \n    else : \n        task_num = min ( self . frame_cnt - start , max_num ) \n    if task_num <= 0 : \n        raise ValueError ( 'start must be less than total frame number' ) \n    if start > 0 : \n        self . _set_real_position ( start ) \n    def write_frame ( file_idx ) : \n        img = self . read ( ) \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        cv2 . imwrite ( filename , img ) \n    if show_progress : \n        track_progress ( write_frame , range ( file_start , file_start + task_num ) ) \n    else : \n        i = 0 \n        while i < task_num : \n            img = self . read ( ) \n            if img is None : \n                break \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i + file_start ) ) \n            cv2 . imwrite ( filename , img ) \n            i = i + 1 "}
{"2300": "\ndef imcrop ( img , bboxes , scale = 1.0 , pad_fill = None ) : \n    chn = 1 if img . ndim == 2 else img . shape [ 2 ] \n    if pad_fill is not None : \n        if isinstance ( pad_fill , ( int , float ) ) : \n            pad_fill = [ pad_fill for _ in range ( chn ) ] \n        assert len ( pad_fill ) == chn \n    _bboxes = bboxes [ None , ... ] if bboxes . ndim == 1 else bboxes \n    scaled_bboxes = bbox_scaling ( _bboxes , scale ) . astype ( np . int32 ) \n    clipped_bbox = bbox_clip ( scaled_bboxes , img . shape ) \n    patches = [ ] \n    i = 0 \n    while i < clipped_bbox . shape [ 0 ] : \n        x1 , y1 , x2 , y2 = tuple ( clipped_bbox [ i , : ] ) \n        if pad_fill is None : \n            patch = img [ y1 : y2 + 1 , x1 : x2 + 1 , ... ] \n        else : \n            _x1 , _y1 , _x2 , _y2 = tuple ( scaled_bboxes [ i , : ] ) \n            if chn == 2 : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 ) \n            else : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 , chn ) \n            patch = np . array ( pad_fill , dtype = img . dtype ) * np . ones ( patch_shape , dtype = img . dtype ) \n            x_start = 0 if _x1 >= 0 else - _x1 \n            y_start = 0 if _y1 >= 0 else - _y1 \n            w = x2 - x1 + 1 \n            h = y2 - y1 + 1 \n            patch [ y_start : y_start + h , x_start : x_start + w , ... ] = img [ y1 : y1 + h , x1 : x1 + w , ... ] \n        patches . append ( patch ) \n        i = i + 1 \n    if bboxes . ndim == 1 : \n        return patches [ 0 ] \n    else : \n        return patches "}
{"2301": "\ndef impad ( img , shape , pad_val = 0 ) : \n    if not isinstance ( pad_val , ( int , float ) ) : \n        assert len ( pad_val ) == img . shape [ - 1 ] \n    if len ( shape ) < len ( img . shape ) : \n        shape = shape + ( img . shape [ - 1 ] , ) \n    assert len ( shape ) == len ( img . shape ) \n    i = 0 \n    while i < len ( shape ) - 1 : \n        assert shape [ i ] >= img . shape [ i ] \n        i = i + 1 \n    pad = np . empty ( shape , dtype = img . dtype ) \n    pad [ ... ] = pad_val \n    pad [ : img . shape [ 0 ] , : img . shape [ 1 ] , ... ] = img \n    return pad "}
{"2311": "\ndef imshow_bboxes ( img , bboxes , colors = 'green' , top_k = - 1 , thickness = 1 , show = True , win_name = '' , wait_time = 0 , out_file = None ) : \n    img = imread ( img ) \n    if isinstance ( bboxes , np . ndarray ) : \n        bboxes = [ bboxes ] \n    if not isinstance ( colors , list ) : \n        colors = [ colors for _ in range ( len ( bboxes ) ) ] \n    colors = [ color_val ( c ) for c in colors ] \n    assert len ( bboxes ) == len ( colors ) \n    for i , _bboxes in enumerate ( bboxes ) : \n        _bboxes = _bboxes . astype ( np . int32 ) \n        if top_k <= 0 : \n            _top_k = _bboxes . shape [ 0 ] \n        else : \n            _top_k = min ( top_k , _bboxes . shape [ 0 ] ) \n        j = 0 \n        while j < _top_k : \n            left_top = ( _bboxes [ j , 0 ] , _bboxes [ j , 1 ] ) \n            right_bottom = ( _bboxes [ j , 2 ] , _bboxes [ j , 3 ] ) \n            cv2 . rectangle ( img , left_top , right_bottom , colors [ i ] , thickness = thickness ) \n            j = j + 1 \n    if show : \n        imshow ( img , win_name , wait_time ) \n    if out_file is not None : \n        imwrite ( img , out_file ) "}
{"2322": "\ndef register_hook ( self , hook , priority = 'NORMAL' ) : \n    assert isinstance ( hook , Hook ) \n    if hasattr ( hook , 'priority' ) : \n        raise ValueError ( '\"priority\" is a reserved attribute for hooks' ) \n    priority = get_priority ( priority ) \n    hook . priority = priority \n    inserted = False \n    i = len ( self . _hooks ) - 1 \n    while i < - 1 : \n        if priority >= self . _hooks [ i ] . priority : \n            self . _hooks . insert ( i + 1 , hook ) \n            inserted = True \n            break \n        i = i + - 1 \n    if not inserted : \n        self . _hooks . insert ( 0 , hook ) "}
{"2323": "\ndef run ( self , data_loaders , workflow , max_epochs , ** kwargs ) : \n    assert isinstance ( data_loaders , list ) \n    assert mmcv . is_list_of ( workflow , tuple ) \n    assert len ( data_loaders ) == len ( workflow ) \n    self . _max_epochs = max_epochs \n    work_dir = self . work_dir if self . work_dir is not None else 'NONE' \n    self . logger . info ( 'Start running, host: %s, work_dir: %s' , get_host_info ( ) , work_dir ) \n    self . logger . info ( 'workflow: %s, max: %d epochs' , workflow , max_epochs ) \n    self . call_hook ( 'before_run' ) \n    while self . epoch < max_epochs : \n        for i , flow in enumerate ( workflow ) : \n            mode , epochs = flow \n            if isinstance ( mode , str ) : \n                if not hasattr ( self , mode ) : \n                    raise ValueError ( 'runner has no method named \"{}\" to run an epoch' . format ( mode ) ) \n                epoch_runner = getattr ( self , mode ) \n            elif callable ( mode ) : \n                epoch_runner = mode \n            else : \n                raise TypeError ( 'mode in workflow must be a str or ' 'callable function, not {}' . format ( type ( mode ) ) ) \n            _ = 0 \n            while _ < epochs : \n                if mode == 'train' and self . epoch >= max_epochs : \n                    return \n                epoch_runner ( data_loaders [ i ] , ** kwargs ) \n                _ = _ + 1 \n    time . sleep ( 1 ) \n    self . call_hook ( 'after_run' ) "}
{"2329": "\ndef list_from_file ( filename , prefix = '' , offset = 0 , max_num = 0 ) : \n    cnt = 0 \n    item_list = [ ] \n    with open ( filename , 'r' ) as f : \n        _ = 0 \n        while _ < offset : \n            f . readline ( ) \n            _ = _ + 1 \n        for line in f : \n            if max_num > 0 and cnt >= max_num : \n                break \n            item_list . append ( prefix + line . rstrip ( '\\n' ) ) \n            cnt += 1 \n    return item_list "}
{"2340": "\ndef slice_list ( in_list , lens ) : \n    if not isinstance ( lens , list ) : \n        raise TypeError ( '\"indices\" must be a list of integers' ) \n    elif sum ( lens ) != len ( in_list ) : \n        raise ValueError ( 'sum of lens and list length does not match: {} != {}' . format ( sum ( lens ) , len ( in_list ) ) ) \n    out_list = [ ] \n    idx = 0 \n    i = 0 \n    while i < len ( lens ) : \n        out_list . append ( in_list [ idx : idx + lens [ i ] ] ) \n        idx += lens [ i ] \n        i = i + 1 \n    return out_list "}
{"2351": "\ndef make_color_wheel ( bins = None ) : \n    if bins is None : \n        bins = [ 15 , 6 , 4 , 11 , 13 , 6 ] \n    assert len ( bins ) == 6 \n    RY , YG , GC , CB , BM , MR = tuple ( bins ) \n    ry = [ 1 , np . arange ( RY ) / RY , 0 ] \n    yg = [ 1 - np . arange ( YG ) / YG , 1 , 0 ] \n    gc = [ 0 , 1 , np . arange ( GC ) / GC ] \n    cb = [ 0 , 1 - np . arange ( CB ) / CB , 1 ] \n    bm = [ np . arange ( BM ) / BM , 0 , 1 ] \n    mr = [ 1 , 0 , 1 - np . arange ( MR ) / MR ] \n    num_bins = RY + YG + GC + CB + BM + MR \n    color_wheel = np . zeros ( ( 3 , num_bins ) , dtype = np . float32 ) \n    col = 0 \n    for i , color in enumerate ( [ ry , yg , gc , cb , bm , mr ] ) : \n        j = 0 \n        while j < 3 : \n            color_wheel [ j , col : col + bins [ i ] ] = color [ j ] \n            j = j + 1 \n        col += bins [ i ] \n    return color_wheel . T "}
{"2423": "\ndef run ( self , dag ) : \n    num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) \n    if num_dag_qubits > self . coupling_map . size ( ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    best_sub = self . _best_subset ( num_dag_qubits ) \n    layout = Layout ( ) \n    map_iter = 0 \n    for qreg in dag . qregs . values ( ) : \n        i = 0 \n        while i < qreg . size : \n            layout [ ( qreg , i ) ] = int ( best_sub [ map_iter ] ) \n            map_iter += 1 \n            i = i + 1 \n    self . property_set [ 'layout' ] = layout "}
{"2424": "\ndef _best_subset ( self , n_qubits ) : \n    if n_qubits == 1 : \n        return np . array ( [ 0 ] ) \n    device_qubits = self . coupling_map . size ( ) \n    cmap = np . asarray ( self . coupling_map . get_edges ( ) ) \n    data = np . ones_like ( cmap [ : , 0 ] ) \n    sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , 0 ] , cmap [ : , 1 ] ) ) , shape = ( device_qubits , device_qubits ) ) . tocsr ( ) \n    best = 0 \n    best_map = None \n    k = 0 \n    while k < sp_cmap . shape [ 0 ] : \n        bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = False , return_predecessors = False ) \n        connection_count = 0 \n        sub_graph = [ ] \n        i = 0 \n        while i < n_qubits : \n            node_idx = bfs [ i ] \n            j = sp_cmap . indptr [ node_idx ] \n            while j < sp_cmap . indptr [ node_idx + 1 ] : \n                node = sp_cmap . indices [ j ] \n                counter = 0 \n                while counter < n_qubits : \n                    if node == bfs [ counter ] : \n                        connection_count += 1 \n                        sub_graph . append ( [ node_idx , node ] ) \n                        break \n                    counter = counter + 1 \n                j = j + 1 \n            i = i + 1 \n        if connection_count > best : \n            best = connection_count \n            best_map = bfs [ 0 : n_qubits ] \n            mapping = { } \n            edge = 0 \n            while edge < best_map . shape [ 0 ] : \n                mapping [ best_map [ edge ] ] = edge \n                edge = edge + 1 \n            new_cmap = [ [ mapping [ c [ 0 ] ] , mapping [ c [ 1 ] ] ] for c in sub_graph ] \n            rows = [ edge [ 0 ] for edge in new_cmap ] \n            cols = [ edge [ 1 ] for edge in new_cmap ] \n            data = [ 1 ] * len ( rows ) \n            sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , shape = ( n_qubits , n_qubits ) ) . tocsr ( ) \n            perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) \n            best_map = best_map [ perm ] \n        k = k + 1 \n    return best_map "}
{"2425": "\ndef barrier ( self , * qargs ) : \n    qubits = [ ] \n    qargs = _convert_to_bits ( qargs , [ qbit for qreg in self . qregs for qbit in qreg ] ) \n    if not qargs : \n        for qreg in self . qregs : \n            j = 0 \n            while j < qreg . size : \n                qubits . append ( ( qreg , j ) ) \n                j = j + 1 \n    for qarg in qargs : \n        if isinstance ( qarg , ( QuantumRegister , list ) ) : \n            if isinstance ( qarg , QuantumRegister ) : \n                qubits . extend ( [ ( qarg , j ) for j in range ( qarg . size ) ] ) \n            else : \n                qubits . extend ( qarg ) \n        else : \n            qubits . append ( qarg ) \n    return self . append ( Barrier ( len ( qubits ) ) , qubits , [ ] ) "}
{"2428": "\ndef _process_custom_unitary ( self , node ) : \n    name = node . name \n    if node . arguments is not None : \n        args = self . _process_node ( node . arguments ) \n    else : \n        args = [ ] \n    bits = [ self . _process_bit_id ( node_element ) for node_element in node . bitlist . children ] \n    if name in self . gates : \n        gargs = self . gates [ name ] [ \"args\" ] \n        gbits = self . gates [ name ] [ \"bits\" ] \n        maxidx = max ( map ( len , bits ) ) \n        idx = 0 \n        while idx < maxidx : \n            self . arg_stack . append ( { gargs [ j ] : args [ j ] for j in range ( len ( gargs ) ) } ) \n            element = [ idx * x for x in [ len ( bits [ j ] ) > 1 for j in range ( len ( bits ) ) ] ] \n            self . bit_stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] for j in range ( len ( gbits ) ) } ) \n            self . _create_dag_op ( name , [ self . arg_stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , [ self . bit_stack [ - 1 ] [ s ] for s in gbits ] ) \n            self . arg_stack . pop ( ) \n            self . bit_stack . pop ( ) \n            idx = idx + 1 \n    else : \n        raise QiskitError ( \"internal error undefined gate:\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) "}
{"2430": "\ndef _process_cnot ( self , node ) : \n    id0 = self . _process_bit_id ( node . children [ 0 ] ) \n    id1 = self . _process_bit_id ( node . children [ 1 ] ) \n    if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : \n        raise QiskitError ( \"internal error: qreg size mismatch\" , \"line=%s\" % node . line , \"file=%s\" % node . file ) \n    maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) \n    idx = 0 \n    while idx < maxidx : \n        if len ( id0 ) > 1 and len ( id1 ) > 1 : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) \n        elif len ( id0 ) > 1 : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) \n        else : \n            self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition ) \n        idx = idx + 1 "}
{"2446": "\ndef qft ( circ , q , n ) : \n    j = 0 \n    while j < n : \n        k = 0 \n        while k < j : \n            circ . cu1 ( math . pi / float ( 2 ** ( j - k ) ) , q [ j ] , q [ k ] ) \n            k = k + 1 \n        circ . h ( q [ j ] ) \n        j = j + 1 "}
{"2498": "\ndef _stinespring_to_kraus ( data , input_dim , output_dim ) : \n    kraus_pair = [ ] \n    for stine in data : \n        if stine is None : \n            kraus_pair . append ( None ) \n        else : \n            trace_dim = stine . shape [ 0 ] // output_dim \n            iden = np . eye ( output_dim ) \n            kraus = [ ] \n            j = 0 \n            while j < trace_dim : \n                vec = np . zeros ( trace_dim ) \n                vec [ j ] = 1 \n                kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) \n                j = j + 1 \n            kraus_pair . append ( kraus ) \n    return tuple ( kraus_pair ) "}
{"2505": "\ndef _transform_from_pauli ( data , num_qubits ) : \n    basis_mat = np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , dtype = complex ) \n    cob = basis_mat \n    _ = 0 \n    while _ < num_qubits - 1 : \n        dim = int ( np . sqrt ( len ( cob ) ) ) \n        cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis_mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , ( 0 , 2 , 1 , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) \n        _ = _ + 1 \n    return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num_qubits "}
{"2553": "\ndef num_connected_components ( self , unitary_only = False ) : \n    reg_offset = 0 \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset += reg . size \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( 1 if instr . control else 0 ) \n        if num_qargs >= 2 and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = 0 \n            if instr . control and not unitary_only : \n                creg = instr . control [ 0 ] \n                creg_int = reg_map [ creg . name ] \n                coff = 0 \n                while coff < creg . size : \n                    temp_int = creg_int + coff \n                    k = 0 \n                    while k < num_sub_graphs : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n                        k = k + 1 \n                    coff = coff + 1 \n            for item in args : \n                reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] \n                k = 0 \n                while k < num_sub_graphs : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n                    k = k + 1 \n            if num_touched > 1 : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                idx = 0 \n                while idx < num_sub_graphs : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                    idx = idx + 1 \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs -= ( num_touched - 1 ) \n        if num_sub_graphs == 1 : \n            break \n    return num_sub_graphs "}
{"2570": "\ndef _compute_distance_matrix ( self ) : \n    if not self . is_connected ( ) : \n        raise CouplingError ( \"coupling graph not connected\" ) \n    lengths = nx . all_pairs_shortest_path_length ( self . graph . to_undirected ( as_view = True ) ) \n    lengths = dict ( lengths ) \n    size = len ( lengths ) \n    cmap = np . zeros ( ( size , size ) ) \n    idx = 0 \n    while idx < size : \n        cmap [ idx , np . fromiter ( lengths [ idx ] . keys ( ) , dtype = int ) ] = np . fromiter ( lengths [ idx ] . values ( ) , dtype = int ) \n        idx = idx + 1 \n    self . _dist_matrix = cmap "}
{"2583": "\ndef gates_to_uncompute ( self ) : \n    q = QuantumRegister ( self . num_qubits ) \n    circuit = QuantumCircuit ( q , name = 'disentangler' ) \n    remaining_param = self . params \n    i = 0 \n    while i < self . num_qubits : \n        ( remaining_param , thetas , phis ) = Initialize . _rotations_to_disentangle ( remaining_param ) \n        rz_mult = self . _multiplex ( RZGate , phis ) \n        ry_mult = self . _multiplex ( RYGate , thetas ) \n        circuit . append ( rz_mult . to_instruction ( ) , q [ i : self . num_qubits ] ) \n        circuit . append ( ry_mult . to_instruction ( ) , q [ i : self . num_qubits ] ) \n        i = i + 1 \n    return circuit "}
{"2594": "\ndef _layer_update ( self , i , first_layer , best_layout , best_depth , best_circuit , layer_list ) : \n    layout = best_layout \n    logger . debug ( \"layer_update: layout = %s\" , pformat ( layout ) ) \n    logger . debug ( \"layer_update: self.initial_layout = %s\" , pformat ( self . initial_layout ) ) \n    dagcircuit_output = DAGCircuit ( ) \n    for register in layout . get_virtual_bits ( ) . keys ( ) : \n        if register [ 0 ] not in dagcircuit_output . qregs . values ( ) : \n            dagcircuit_output . add_qreg ( register [ 0 ] ) \n    if first_layer : \n        logger . debug ( \"layer_update: first multi-qubit gate layer\" ) \n        j = 0 \n        while j < i + 1 : \n            edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n            for bit in dagcircuit_output . clbits ( ) : \n                edge_map [ bit ] = bit \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , edge_map ) \n            j = j + 1 \n    else : \n        if best_depth > 0 : \n            logger . debug ( \"layer_update: there are swaps in this layer, \" \"depth %d\" , best_depth ) \n            dagcircuit_output . extend_back ( best_circuit ) \n        else : \n            logger . debug ( \"layer_update: there are no swaps in this layer\" ) \n        edge_map = layout . combine_into_edge_map ( self . initial_layout ) \n        for bit in dagcircuit_output . clbits ( ) : \n            edge_map [ bit ] = bit \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , edge_map ) \n    return dagcircuit_output "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if number_of_qubits < 5 : \n        temp_set = [ ] \n        if case == 'weight' : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) \n        elif case == 'tensor' : \n            k = 0 \n            while k < 4 ** number_of_qubits : \n                z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                j = 0 \n                while j < number_of_qubits : \n                    element = ( k // ( 4 ** j ) ) % 4 \n                    if element == 1 : \n                        x [ j ] = True \n                    elif element == 2 : \n                        z [ j ] = True \n                        x [ j ] = True \n                    elif element == 3 : \n                        z [ j ] = True \n                    j = j + 1 \n                temp_set . append ( Pauli ( z , x ) ) \n                k = k + 1 \n            return temp_set \n        else : \n            raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2655": "\ndef swap_mapper_layer_update ( self , i , first_layer , best_layout , best_d , best_circ , layer_list ) : \n    layout = best_layout \n    dagcircuit_output = DAGCircuit ( ) \n    QR = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) \n    dagcircuit_output . add_qreg ( QR ) \n    identity_wire_map = { ( QR , j ) : ( QR , j ) for j in range ( self . coupling_map . size ( ) ) } \n    if first_layer : \n        j = 0 \n        while j < i + 1 : \n            dagcircuit_output . compose_back ( layer_list [ j ] [ \"graph\" ] , layout ) \n            j = j + 1 \n    else : \n        if best_d > 0 : \n            dagcircuit_output . compose_back ( best_circ , identity_wire_map ) \n        dagcircuit_output . compose_back ( layer_list [ i ] [ \"graph\" ] , layout ) \n    return dagcircuit_output "}
{"2661": "\ndef format_statevector ( vec , decimals = None ) : \n    num_basis = len ( vec ) \n    vec_complex = np . zeros ( num_basis , dtype = complex ) \n    i = 0 \n    while i < num_basis : \n        vec_complex [ i ] = vec [ i ] [ 0 ] + 1j * vec [ i ] [ 1 ] \n        i = i + 1 \n    if decimals : \n        vec_complex = np . around ( vec_complex , decimals = decimals ) \n    return vec_complex "}
{"2667": "\ndef iplot_bloch_multivector ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"bloch_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        data = $data;        dataValues = [];        for (var i = 0; i < data.length; i++) {            // Coordinates            var x = data[i][0];            var y = data[i][1];            var z = data[i][2];            var point = {'x': x,                        'y': y,                        'z': z};            dataValues.push(point);        }        require([\"qVisualization\"], function(qVisualizations) {            // Plot figure            qVisualizations.plotState(\"bloch_$divNumber\",                                      \"bloch\",                                      dataValues,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] } \n    num = int ( np . log2 ( len ( rho ) ) ) \n    bloch_data = [ ] \n    i = 0 \n    while i < num : \n        pauli_singles = [ Pauli . pauli_single ( num , i , 'X' ) , Pauli . pauli_single ( num , i , 'Y' ) , Pauli . pauli_single ( num , i , 'Z' ) ] \n        bloch_state = list ( map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , pauli_singles ) ) \n        bloch_data . append ( bloch_state ) \n        i = i + 1 \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : bloch_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2671": "\ndef iplot_state_qsphere ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"qsphere_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            data = $data;            qVisualizations.plotState(\"qsphere_$divNumber\",                                      \"qsphere\",                                      data,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] } \n    qspheres_data = [ ] \n    num = int ( np . log2 ( len ( rho ) ) ) \n    weig , stateall = linalg . eigh ( rho ) \n    _ = 0 \n    while _ < 2 ** num : \n        probmix = weig . max ( ) \n        prob_location = weig . argmax ( ) \n        if probmix > 0.001 : \n            state = stateall [ : , prob_location ] \n            loc = np . absolute ( state ) . argmax ( ) \n            j = 0 \n            while j < 2 ** num : \n                test = np . absolute ( np . absolute ( state [ j ] ) - np . absolute ( state [ loc ] ) ) \n                if test < 0.001 : \n                    loc = j \n                    break \n                j = j + 1 \n            angles = ( np . angle ( state [ loc ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n            angleset = np . exp ( - 1j * angles ) \n            state = angleset * state \n            state . flatten ( ) \n            spherepoints = [ ] \n            i = 0 \n            while i < 2 ** num : \n                element = bin ( i ) [ 2 : ] . zfill ( num ) \n                weight = element . count ( \"1\" ) \n                number_of_divisions = n_choose_k ( num , weight ) \n                weight_order = bit_string_index ( element ) \n                angle = weight_order * 2 * np . pi / number_of_divisions \n                zvalue = - 2 * weight / num + 1 \n                xvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . cos ( angle ) \n                yvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . sin ( angle ) \n                prob = np . real ( np . dot ( state [ i ] , state [ i ] . conj ( ) ) ) \n                angles = ( np . angle ( state [ i ] ) + 2 * np . pi ) % ( 2 * np . pi ) \n                qpoint = { 'x' : xvalue , 'y' : yvalue , 'z' : zvalue , 'prob' : prob , 'phase' : angles } \n                spherepoints . append ( qpoint ) \n                i = i + 1 \n            sphere = { 'points' : spherepoints , 'eigenvalue' : probmix } \n            qspheres_data . append ( sphere ) \n            weig [ prob_location ] = 0 \n        _ = _ + 1 \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : qspheres_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2692": "\ndef add_qreg ( self , qreg ) : \n    if not isinstance ( qreg , QuantumRegister ) : \n        raise DAGCircuitError ( \"not a QuantumRegister instance.\" ) \n    if qreg . name in self . qregs : \n        raise DAGCircuitError ( \"duplicate register %s\" % qreg . name ) \n    self . qregs [ qreg . name ] = qreg \n    j = 0 \n    while j < qreg . size : \n        self . _add_wire ( ( qreg , j ) ) \n        j = j + 1 "}
{"2693": "\ndef add_creg ( self , creg ) : \n    if not isinstance ( creg , ClassicalRegister ) : \n        raise DAGCircuitError ( \"not a ClassicalRegister instance.\" ) \n    if creg . name in self . cregs : \n        raise DAGCircuitError ( \"duplicate register %s\" % creg . name ) \n    self . cregs [ creg . name ] = creg \n    j = 0 \n    while j < creg . size : \n        self . _add_wire ( ( creg , j ) ) \n        j = j + 1 "}
{"2740": "\ndef __wizard ( rho , epsilon = None ) : \n    if epsilon is None : \n        epsilon = 0. \n    dim = len ( rho ) \n    rho_wizard = np . zeros ( [ dim , dim ] ) \n    v , w = np . linalg . eigh ( rho ) \n    j = 0 \n    while j < dim : \n        if v [ j ] < epsilon : \n            tmp = v [ j ] \n            v [ j ] = 0. \n            x = 0. \n            k = j + 1 \n            while k < dim : \n                x += tmp / ( dim - ( j + 1 ) ) \n                v [ k ] = v [ k ] + tmp / ( dim - ( j + 1 ) ) \n                k = k + 1 \n        j = j + 1 \n    j = 0 \n    while j < dim : \n        rho_wizard = rho_wizard + v [ j ] * outer ( w [ : , j ] ) \n        j = j + 1 \n    return rho_wizard "}
{"2741": "\ndef wigner_data ( q_result , meas_qubits , labels , shots = None ) : \n    num = len ( meas_qubits ) \n    dim = 2 ** num \n    p = [ 0.5 + 0.5 * np . sqrt ( 3 ) , 0.5 - 0.5 * np . sqrt ( 3 ) ] \n    parity = 1 \n    i = 0 \n    while i < num : \n        parity = np . kron ( parity , p ) \n        i = i + 1 \n    w = [ 0 ] * len ( labels ) \n    wpt = 0 \n    counts = [ marginal_counts ( q_result . get_counts ( circ ) , meas_qubits ) for circ in labels ] \n    for entry in counts : \n        x = [ 0 ] * dim \n        i = 0 \n        while i < dim : \n            if bin ( i ) [ 2 : ] . zfill ( num ) in entry : \n                x [ i ] = float ( entry [ bin ( i ) [ 2 : ] . zfill ( num ) ] ) \n            i = i + 1 \n        if shots is None : \n            shots = np . sum ( x ) \n        i = 0 \n        while i < dim : \n            w [ wpt ] = w [ wpt ] + ( x [ i ] / shots ) * parity [ i ] \n            i = i + 1 \n        wpt += 1 \n    return w "}
{"2748": "\ndef qubits_tab ( backend ) : \n    props = backend . properties ( ) . to_dict ( ) \n    header_html = \"<div><font style='font-weight:bold'>{key}</font>: {value}</div>\" \n    header_html = header_html . format ( key = 'last_update_date' , value = props [ 'last_update_date' ] ) \n    update_date_widget = widgets . HTML ( value = header_html ) \n    qubit_html = \"<table>\" \n    qubit_html += \"\"\"<style>table {    border-collapse: collapse;    width: auto;}th, td {    text-align: left;    padding: 8px;}tr:nth-child(even) {background-color: #f6f6f6;}</style>\"\"\" \n    qubit_html += \"<tr><th></th><th>Frequency</th><th>T1</th><th>T2</th>\" \n    qubit_html += \"<th>U1 gate error</th><th>U2 gate error</th><th>U3 gate error</th>\" \n    qubit_html += \"<th>Readout error</th></tr>\" \n    qubit_footer = \"</table>\" \n    qub = 0 \n    while qub < len ( props [ 'qubits' ] ) : \n        name = 'Q%s' % qub \n        qubit_data = props [ 'qubits' ] [ qub ] \n        gate_data = props [ 'gates' ] [ 3 * qub : 3 * qub + 3 ] \n        t1_info = qubit_data [ 0 ] \n        t2_info = qubit_data [ 1 ] \n        freq_info = qubit_data [ 2 ] \n        readout_info = qubit_data [ 3 ] \n        freq = str ( round ( freq_info [ 'value' ] , 5 ) ) + ' ' + freq_info [ 'unit' ] \n        T1 = str ( round ( t1_info [ 'value' ] , 5 ) ) + ' ' + t1_info [ 'unit' ] \n        T2 = str ( round ( t2_info [ 'value' ] , 5 ) ) + ' ' + t2_info [ 'unit' ] \n        U1 = str ( round ( gate_data [ 0 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) \n        U2 = str ( round ( gate_data [ 1 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) \n        U3 = str ( round ( gate_data [ 2 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) \n        readout_error = round ( readout_info [ 'value' ] , 5 ) \n        qubit_html += \"<tr><td><font style='font-weight:bold'>%s</font></td><td>%s</td>\" \n        qubit_html += \"<td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\" \n        qubit_html = qubit_html % ( name , freq , T1 , T2 , U1 , U2 , U3 , readout_error ) \n        qub = qub + 1 \n    qubit_html += qubit_footer \n    qubit_widget = widgets . HTML ( value = qubit_html ) \n    out = widgets . VBox ( [ update_date_widget , qubit_widget ] ) \n    return out "}
{"2761": "\ndef input_state ( circ , q , n ) : \n    j = 0 \n    while j < n : \n        circ . h ( q [ j ] ) \n        circ . u1 ( math . pi / float ( 2 ** ( j ) ) , q [ j ] ) . inverse ( ) \n        j = j + 1 "}
{"2781": "\ndef make_dict_observable ( matrix_observable ) : \n    dict_observable = { } \n    observable = np . array ( matrix_observable ) \n    observable_size = len ( observable ) \n    observable_bits = int ( np . ceil ( np . log2 ( observable_size ) ) ) \n    binary_formater = '0{}b' . format ( observable_bits ) \n    if observable . ndim == 2 : \n        observable = observable . diagonal ( ) \n    state_no = 0 \n    while state_no < observable_size : \n        state_str = format ( state_no , binary_formater ) \n        dict_observable [ state_str ] = observable [ state_no ] \n        state_no = state_no + 1 \n    return dict_observable "}
{"2801": "\ndef run ( self , dag ) : \n    cx_runs = dag . collect_runs ( [ \"cx\" ] ) \n    for cx_run in cx_runs : \n        partition = [ ] \n        chunk = [ ] \n        i = 0 \n        while i < len ( cx_run ) - 1 : \n            chunk . append ( cx_run [ i ] ) \n            qargs0 = cx_run [ i ] . qargs \n            qargs1 = cx_run [ i + 1 ] . qargs \n            if qargs0 != qargs1 : \n                partition . append ( chunk ) \n                chunk = [ ] \n            i = i + 1 \n        chunk . append ( cx_run [ - 1 ] ) \n        partition . append ( chunk ) \n        for chunk in partition : \n            if len ( chunk ) % 2 == 0 : \n                for n in chunk : \n                    dag . remove_op_node ( n ) \n            else : \n                for n in chunk [ 1 : ] : \n                    dag . remove_op_node ( n ) \n    return dag "}
{"2806": "\ndef latex ( self , aliases = None ) : \n    self . _initialize_latex_array ( aliases ) \n    self . _build_latex_array ( aliases ) \n    header_1 = r\"\"\"% \\documentclass[preview]{standalone}% If the image is too large to fit on this documentclass use\\documentclass[draft]{beamer}\"\"\" \n    beamer_line = \"\\\\usepackage[size=custom,height=%d,width=%d,scale=%.1f]{beamerposter}\\n\" \n    header_2 = r\"\"\"% instead and customize the height and width (in cm) to fit.% Large images may run out of memory quickly.% To fix this use the LuaLaTeX compiler, which dynamically% allocates memory.\\usepackage[braket, qm]{qcircuit}\\usepackage{amsmath}\\pdfmapfile{+sansmathaccent.map}% \\usepackage[landscape]{geometry}% Comment out the above line if using the beamer documentclass.\\begin{document}\\begin{equation*}\"\"\" \n    qcircuit_line = r\"\"\"    \\Qcircuit @C=%.1fem @R=%.1fem @!R {\"\"\" \n    output = io . StringIO ( ) \n    output . write ( header_1 ) \n    output . write ( '%% img_width = %d, img_depth = %d\\n' % ( self . img_width , self . img_depth ) ) \n    output . write ( beamer_line % self . _get_beamer_page ( ) ) \n    output . write ( header_2 ) \n    output . write ( qcircuit_line % ( self . column_separation , self . row_separation ) ) \n    i = 0 \n    while i < self . img_width : \n        output . write ( \"\\t \\t\" ) \n        j = 0 \n        while j < self . img_depth + 1 : \n            cell_str = self . _latex [ i ] [ j ] \n            if 'barrier' in cell_str : \n                output . write ( cell_str ) \n            else : \n                cell_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , _truncate_float , cell_str ) \n                output . write ( cell_str ) \n            if j != self . img_depth : \n                output . write ( \" & \" ) \n            else : \n                output . write ( r'\\\\' + '\\n' ) \n            j = j + 1 \n        i = i + 1 \n    output . write ( '\\t }\\n' ) \n    output . write ( '\\\\end{equation*}\\n\\n' ) \n    output . write ( '\\\\end{document}' ) \n    contents = output . getvalue ( ) \n    output . close ( ) \n    return contents "}
{"2818": "\ndef random_unitary ( dim , seed = None ) : \n    if dim == 0 or not math . log2 ( dim ) . is_integer ( ) : \n        raise QiskitError ( \"Desired unitary dimension not a positive power of 2.\" ) \n    matrix = np . zeros ( [ dim , dim ] , dtype = complex ) \n    j = 0 \n    while j < dim : \n        if j == 0 : \n            a = random_state ( dim , seed ) \n        else : \n            a = random_state ( dim ) \n        matrix [ : , j ] = np . copy ( a ) \n        i = j - 1 \n        while i >= 0 : \n            dc = np . vdot ( matrix [ : , i ] , a ) \n            matrix [ : , j ] = matrix [ : , j ] - dc * matrix [ : , i ] \n            i = i - 1 \n        matrix [ : , j ] = matrix [ : , j ] * ( 1.0 / np . sqrt ( np . vdot ( matrix [ : , j ] , matrix [ : , j ] ) ) ) \n        j = j + 1 \n    return Operator ( matrix ) "}
{"2840": "\ndef power ( self , n ) : \n    if not isinstance ( n , ( int , np . integer ) ) or n < 1 : \n        raise QiskitError ( \"Can only power with positive integer powers.\" ) \n    if self . _input_dim != self . _output_dim : \n        raise QiskitError ( \"Can only power with input_dim = output_dim.\" ) \n    ret = self . copy ( ) \n    _ = 1 \n    while _ < n : \n        ret = ret . compose ( self ) \n        _ = _ + 1 \n    return ret "}
{"2870": "\ndef get_info ( self ) : \n    result = copy . copy ( self . streams [ 0 ] ) \n    result . stat_log = self . stat_log \n    result . steps = [ ] \n    result . ammo_file = '' \n    result . rps_schedule = None \n    result . ammo_count = 0 \n    result . duration = 0 \n    result . instances = 0 \n    result . loadscheme = [ ] \n    result . loop_count = 0 \n    for stream in self . streams : \n        sec_no = 0 \n        logger . debug ( \"Steps: %s\" , stream . stepper_wrapper . steps ) \n        for item in stream . stepper_wrapper . steps : \n            x = 0 \n            while x < item [ 1 ] : \n                if len ( result . steps ) > sec_no : \n                    result . steps [ sec_no ] [ 0 ] += item [ 0 ] \n                else : \n                    result . steps . append ( [ item [ 0 ] , 1 ] ) \n                sec_no += 1 \n                x = x + 1 \n        if result . rps_schedule : \n            result . rps_schedule = [ ] \n        else : \n            result . rps_schedule = stream . stepper_wrapper . loadscheme \n        if result . loadscheme : \n            result . loadscheme = '' \n        else : \n            result . loadscheme = '' \n        if result . loop_count : \n            result . loop_count = u'0' \n        else : \n            result . loop_count = stream . stepper_wrapper . loop_count \n        result . ammo_file += '{} ' . format ( stream . stepper_wrapper . ammo_file ) \n        result . ammo_count += stream . stepper_wrapper . ammo_count \n        result . duration = max ( result . duration , stream . stepper_wrapper . duration ) \n        result . instances += stream . instances \n    if not result . ammo_count : \n        raise ValueError ( \"Total ammo count cannot be zero\" ) \n    return result "}
{"2888": "\ndef _feed ( self ) : \n    self . plan = StpdReader ( self . stpd_filename ) \n    if self . cached_stpd : \n        self . plan = list ( self . plan ) \n    for task in self . plan : \n        if self . quit . is_set ( ) : \n            logger . info ( \"Stop feeding: gonna quit\" ) \n            return \n        while True : \n            try : \n                self . task_queue . put ( task , timeout = 1 ) \n                break \n            except Full : \n                if self . quit . is_set ( ) or self . workers_finished : \n                    return \n                else : \n                    continue \n    workers_count = self . instances \n    logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers_count ) ) \n    retry_delay = 1 \n    _ = 0 \n    while _ < 5 : \n        try : \n            [ self . task_queue . put ( None , timeout = 1 ) for _ in xrange ( 0 , workers_count ) ] \n            break \n        except Full : \n            logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry_delay ) \n            time . sleep ( retry_delay ) \n            retry_delay *= 2 \n        _ = _ + 1 \n    try : \n        logger . info ( \"Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        logger . info ( \"All workers exited.\" ) \n        self . workers_finished = True \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . task_queue . close ( ) \n        self . results . close ( ) \n        self . quit . set ( ) \n        logger . info ( \"Going to quit. Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        self . workers_finished = True "}
{"2912": "\ndef render_screen ( self ) : \n    self . term_width , self . term_height = get_terminal_size ( ) \n    self . log . debug ( \"Terminal size: %sx%s\" , self . term_width , self . term_height ) \n    self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100 ) ) - 1 \n    if self . right_panel_width > 0 : \n        self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 \n    else : \n        self . right_panel_width = 0 \n        self . left_panel_width = self . term_width - 1 \n    self . log . debug ( \"Left/right panels width: %s/%s\" , self . left_panel_width , self . right_panel_width ) \n    widget_output = [ ] \n    if self . right_panel_width : \n        widget_output = [ ] \n        self . log . debug ( \"There are %d info widgets\" % len ( self . info_widgets ) ) \n        for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : \n            self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) \n            widget_out = widget . render ( self ) . strip ( ) \n            if widget_out : \n                widget_output += widget_out . split ( \"\\n\" ) \n                widget_output += [ \"\" ] \n    left_lines = self . __render_left_panel ( ) \n    self . log . debug ( \"Composing final screen output\" ) \n    output = [ ] \n    line_no = 1 \n    while line_no < self . term_height : \n        line = \" \" \n        if line_no > 1 and left_lines : \n            left_line = left_lines . pop ( 0 ) \n            left_line_plain = self . markup . clean_markup ( left_line ) \n            left_line += ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) \n            line += left_line \n        else : \n            line += ' ' * self . left_panel_width \n        if self . right_panel_width : \n            line += self . markup . RESET \n            line += self . markup . WHITE \n            line += self . RIGHT_PANEL_SEPARATOR \n            line += self . markup . RESET \n            right_line = self . __get_right_line ( widget_output ) \n            line += right_line \n        output . append ( line ) \n        line_no = line_no + 1 \n    return self . markup . new_line . join ( output ) + self . markup . new_line "}
{"2999": "\ndef _compute_nfps_uniform ( cum_counts , sizes ) : \n    nfps = np . zeros ( ( len ( sizes ) , len ( sizes ) ) ) \n    l = 0 \n    while l < len ( sizes ) : \n        u = l \n        while u < len ( sizes ) : \n            nfps [ l , u ] = _compute_nfp_uniform ( l , u , cum_counts , sizes ) \n            u = u + 1 \n        l = l + 1 \n    return nfps "}
{"3000": "\ndef _compute_nfps_real ( counts , sizes ) : \n    nfps = np . zeros ( ( len ( sizes ) , len ( sizes ) ) ) \n    l = 0 \n    while l < len ( sizes ) : \n        u = l \n        while u < len ( sizes ) : \n            nfps [ l , u ] = _compute_nfp_real ( l , u , counts , sizes ) \n            u = u + 1 \n        l = l + 1 \n    return nfps "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if num_part < 2 : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if num_part > len ( sizes ) : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if num_part == 2 : \n        total_nfps , u = min ( ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( 0 , len ( sizes ) - 1 ) ) \n        return [ ( sizes [ 0 ] , sizes [ u ] ) , ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2 ) ) \n    p2i = lambda p : p - 2 \n    p = 2 \n    while p < num_part : \n        u = p - 1 \n        while u < len ( sizes ) : \n            if p == 2 : \n                cost [ u , p2i ( p ) ] = min ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , u ] for u1 in range ( ( p - 1 ) - 1 , u ) ) \n            u = u + 1 \n        p = p + 1 \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , len ( sizes ) - 1 ) ) \n    partitions = [ ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] \n    p -= 1 \n    while p > 1 : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + 1 , u ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , u ) ) \n        partitions . insert ( 0 , ( sizes [ u1_best + 1 ] , sizes [ u ] ) ) \n        u = u1_best \n        p -= 1 \n    partitions . insert ( 0 , ( sizes [ 0 ] , sizes [ u ] ) ) \n    return [ partitions , total_nfps , cost ] "}
{"3013": "\ndef minhash ( self , v ) : \n    if not isinstance ( v , collections . Iterable ) : \n        raise TypeError ( \"Input vector must be an iterable\" ) \n    if not len ( v ) == self . dim : \n        raise ValueError ( \"Input dimension mismatch, expecting %d\" % self . dim ) \n    if not isinstance ( v , np . ndarray ) : \n        v = np . array ( v , dtype = np . float32 ) \n    elif v . dtype != np . float32 : \n        v = v . astype ( np . float32 ) \n    hashvalues = np . zeros ( ( self . sample_size , 2 ) , dtype = np . int ) \n    vzeros = ( v == 0 ) \n    if vzeros . all ( ) : \n        raise ValueError ( \"Input is all zeros\" ) \n    v [ vzeros ] = np . nan \n    vlog = np . log ( v ) \n    i = 0 \n    while i < self . sample_size : \n        t = np . floor ( ( vlog / self . rs [ i ] ) + self . betas [ i ] ) \n        ln_y = ( t - self . betas [ i ] ) * self . rs [ i ] \n        ln_a = self . ln_cs [ i ] - ln_y - self . rs [ i ] \n        k = np . nanargmin ( ln_a ) \n        hashvalues [ i ] [ 0 ] , hashvalues [ i ] [ 1 ] = k , int ( t [ k ] ) \n        i = i + 1 \n    return WeightedMinHash ( self . seed , hashvalues ) "}
{"3158": "\ndef random_playout ( self , board ) : \n    x = 0 \n    while x < MAXMOVES : \n        if board . finished : \n            break \n        board . move ( board . random_move ( ) ) \n        x = x + 1 "}
{"3199": "\ndef parse_bug_activity ( raw_html ) : \n    def is_activity_empty ( bs ) : \n        EMPTY_ACTIVITY = \"No changes have been made to this (?:bug|issue) yet.\" \n        tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) \n        return tag is not None \n    def find_activity_table ( bs ) : \n        tables = bs . find_all ( 'table' ) \n        for tb in tables : \n            nheaders = len ( tb . tr . find_all ( 'th' , recursive = False ) ) \n            if nheaders == 5 : \n                return tb \n        raise ParseError ( cause = \"Table of bug activity not found.\" ) \n    def remove_tags ( bs ) : \n        HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] \n        for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : \n            tag . replaceWith ( tag . text ) \n    def format_text ( bs ) : \n        strings = [ s . strip ( ' \\n\\t' ) for s in bs . stripped_strings ] \n        s = ' ' . join ( strings ) \n        return s \n    bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) \n    if is_activity_empty ( bs ) : \n        fields = [ ] \n    else : \n        activity_tb = find_activity_table ( bs ) \n        remove_tags ( activity_tb ) \n        fields = activity_tb . find_all ( 'td' ) \n    while fields : \n        who = fields . pop ( 0 ) \n        when = fields . pop ( 0 ) \n        n = int ( who . get ( 'rowspan' ) ) \n        _ = 0 \n        while _ < n : \n            what = fields . pop ( 0 ) \n            removed = fields . pop ( 0 ) \n            added = fields . pop ( 0 ) \n            event = { 'Who' : format_text ( who ) , 'When' : format_text ( when ) , 'What' : format_text ( what ) , 'Removed' : format_text ( removed ) , 'Added' : format_text ( added ) } \n            yield event \n            _ = _ + 1 "}
{"3287": "\ndef fetch_items ( self , category , ** kwargs ) : \n    since_id = kwargs [ 'since_id' ] \n    max_id = kwargs [ 'max_id' ] \n    geocode = kwargs [ 'geocode' ] \n    lang = kwargs [ 'lang' ] \n    entities = kwargs [ 'include_entities' ] \n    tweets_type = kwargs [ 'result_type' ] \n    logger . info ( \"Fetching tweets %s from %s to %s\" , self . query , str ( since_id ) , str ( max_id ) if max_id else '--' ) \n    tweets_ids = [ ] \n    min_date = None \n    max_date = None \n    group_tweets = self . client . tweets ( self . query , since_id = since_id , max_id = max_id , geocode = geocode , lang = lang , include_entities = entities , result_type = tweets_type ) \n    for tweets in group_tweets : \n        i = 0 \n        while i < len ( tweets ) : \n            tweet = tweets [ i ] \n            tweets_ids . append ( tweet [ 'id' ] ) \n            if tweets [ - 1 ] == tweet : \n                min_date = str_to_datetime ( tweets [ - 1 ] [ 'created_at' ] ) \n            if tweets [ 0 ] == tweet and not max_date : \n                max_date = str_to_datetime ( tweets [ 0 ] [ 'created_at' ] ) \n            yield tweet \n            i = i + 1 \n    logger . info ( \"Fetch process completed: %s (unique %s) tweets fetched, from %s to %s\" , len ( tweets_ids ) , len ( list ( set ( tweets_ids ) ) ) , min_date , max_date ) "}
{"3333": "\ndef months_range ( from_date , to_date ) : \n    start = datetime . datetime ( from_date . year , from_date . month , 1 ) \n    end = datetime . datetime ( to_date . year , to_date . month , 1 ) \n    month_gen = dateutil . rrule . rrule ( freq = dateutil . rrule . MONTHLY , dtstart = start , until = end ) \n    months = [ d for d in month_gen ] \n    pos = 0 \n    x = 1 \n    while x < len ( months ) : \n        yield months [ pos ] , months [ x ] \n        pos = x \n        x = x + 1 "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and tokens [ start ] [ 1 ] == \"for\" : \n        self . _pop_token ( ) \n    if tokens [ start + 1 ] [ 1 ] != \"(\" : \n        return \n    found_and_or = False \n    depth = 0 \n    keyword_token = str ( tokens [ start ] [ 1 ] ) \n    line_num = tokens [ start ] [ 2 ] [ 0 ] \n    i = start \n    while i < len ( tokens ) - 1 : \n        token = tokens [ i ] \n        if token [ 0 ] == tokenize . NL : \n            return \n        if token [ 1 ] == \"(\" : \n            depth += 1 \n        elif token [ 1 ] == \")\" : \n            depth -= 1 \n            if depth : \n                i = i + 1 \n                continue \n            if tokens [ i + 1 ] [ 1 ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + 1 ] [ 0 ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                if i == start + 2 : \n                    return \n                if keyword_token == \"not\" : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token in ( \"return\" , \"yield\" ) : \n                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token not in self . _keywords_with_parens : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n            return \n        elif depth == 1 : \n            if token [ 1 ] == \",\" : \n                return \n            if token [ 1 ] in ( \"and\" , \"or\" ) : \n                found_and_or = True \n            elif token [ 1 ] == \"yield\" : \n                return \n            elif token [ 1 ] == \"for\" : \n                return \n        i = i + 1 "}
{"3843": "\ndef visit_default ( self , node ) : \n    if not node . is_statement : \n        return \n    if not node . root ( ) . pure_python : \n        return \n    prev_sibl = node . previous_sibling ( ) \n    if prev_sibl is not None : \n        prev_line = prev_sibl . fromlineno \n    else : \n        if ( isinstance ( node . parent , nodes . TryFinally ) and node in node . parent . finalbody ) : \n            prev_line = node . parent . body [ 0 ] . tolineno + 1 \n        else : \n            prev_line = node . parent . statement ( ) . fromlineno \n    line = node . fromlineno \n    assert line , node \n    if prev_line == line and self . _visited_lines . get ( line ) != 2 : \n        self . _check_multi_statement_line ( node , line ) \n        return \n    if line in self . _visited_lines : \n        return \n    try : \n        tolineno = node . blockstart_tolineno \n    except AttributeError : \n        tolineno = node . tolineno \n    assert tolineno , node \n    lines = [ ] \n    line = line \n    while line < tolineno + 1 : \n        self . _visited_lines [ line ] = 1 \n        try : \n            lines . append ( self . _lines [ line ] . rstrip ( ) ) \n        except KeyError : \n            lines . append ( \"\" ) \n        line = line + 1 "}
{"3858": "\ndef register_options_provider ( self , provider , own_group = True ) : \n    assert provider . priority <= 0 , \"provider's priority can't be >= 0\" \n    i = 0 \n    while i < len ( self . options_providers ) : \n        if provider . priority > self . options_providers [ i ] . priority : \n            self . options_providers . insert ( i , provider ) \n            break \n        i = i + 1 \n    non_group_spec_options = [ option for option in provider . options if \"group\" not in option [ 1 ] ] \n    groups = getattr ( provider , \"option_groups\" , ( ) ) \n    if own_group and non_group_spec_options : \n        self . add_option_group ( provider . name . upper ( ) , provider . __doc__ , non_group_spec_options , provider , ) \n    else : \n        for opt , optdict in non_group_spec_options : \n            self . add_optik_option ( provider , self . cmdline_parser , opt , optdict ) \n    for gname , gdoc in groups : \n        gname = gname . upper ( ) \n        goptions = [ option for option in provider . options if option [ 1 ] . get ( \"group\" , \"\" ) . upper ( ) == gname ] \n        self . add_option_group ( gname , gdoc , goptions , provider ) "}
{"4036": "\ndef get_client_ca_list ( self ) : \n    ca_names = _lib . SSL_get_client_CA_list ( self . _ssl ) \n    if ca_names == _ffi . NULL : \n        return [ ] \n    result = [ ] \n    i = 0 \n    while i < _lib . sk_X509_NAME_num ( ca_names ) : \n        name = _lib . sk_X509_NAME_value ( ca_names , i ) \n        copy = _lib . X509_NAME_dup ( name ) \n        _openssl_assert ( copy != _ffi . NULL ) \n        pyname = X509Name . __new__ ( X509Name ) \n        pyname . _name = _ffi . gc ( copy , _lib . X509_NAME_free ) \n        result . append ( pyname ) \n        i = i + 1 \n    return result "}
{"4067": "\ndef get_components ( self ) : \n    result = [ ] \n    i = 0 \n    while i < _lib . X509_NAME_entry_count ( self . _name ) : \n        ent = _lib . X509_NAME_get_entry ( self . _name , i ) \n        fname = _lib . X509_NAME_ENTRY_get_object ( ent ) \n        fval = _lib . X509_NAME_ENTRY_get_data ( ent ) \n        nid = _lib . OBJ_obj2nid ( fname ) \n        name = _lib . OBJ_nid2sn ( nid ) \n        value = _ffi . buffer ( _lib . ASN1_STRING_data ( fval ) , _lib . ASN1_STRING_length ( fval ) ) [ : ] \n        result . append ( ( _ffi . string ( name ) , value ) ) \n        i = i + 1 \n    return result "}
{"4075": "\ndef get_extensions ( self ) : \n    exts = [ ] \n    native_exts_obj = _lib . X509_REQ_get_extensions ( self . _req ) \n    i = 0 \n    while i < _lib . sk_X509_EXTENSION_num ( native_exts_obj ) : \n        ext = X509Extension . __new__ ( X509Extension ) \n        ext . _extension = _lib . sk_X509_EXTENSION_value ( native_exts_obj , i ) \n        exts . append ( ext ) \n        i = i + 1 \n    return exts "}
{"4104": "\ndef get_reason ( self ) : \n    i = 0 \n    while i < _lib . X509_REVOKED_get_ext_count ( self . _revoked ) : \n        ext = _lib . X509_REVOKED_get_ext ( self . _revoked , i ) \n        obj = _lib . X509_EXTENSION_get_object ( ext ) \n        if _lib . OBJ_obj2nid ( obj ) == _lib . NID_crl_reason : \n            bio = _new_mem_buf ( ) \n            print_result = _lib . X509V3_EXT_print ( bio , ext , 0 , 0 ) \n            if not print_result : \n                print_result = _lib . M_ASN1_OCTET_STRING_print ( bio , _lib . X509_EXTENSION_get_data ( ext ) ) \n                _openssl_assert ( print_result != 0 ) \n            return _bio_to_string ( bio ) \n        i = i + 1 "}
{"4107": "\ndef get_revoked ( self ) : \n    results = [ ] \n    revoked_stack = _lib . X509_CRL_get_REVOKED ( self . _crl ) \n    i = 0 \n    while i < _lib . sk_X509_REVOKED_num ( revoked_stack ) : \n        revoked = _lib . sk_X509_REVOKED_value ( revoked_stack , i ) \n        revoked_copy = _lib . Cryptography_X509_REVOKED_dup ( revoked ) \n        pyrev = Revoked . __new__ ( Revoked ) \n        pyrev . _revoked = _ffi . gc ( revoked_copy , _lib . X509_REVOKED_free ) \n        results . append ( pyrev ) \n        i = i + 1 \n    if results : \n        return tuple ( results ) "}
{"4160": "\ndef enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = 1 , nperm = 1000 , rs = np . random . RandomState ( ) , single = False , scale = False ) : \n    N = len ( gene_list ) \n    tag_indicator = np . in1d ( gene_list , gene_set , assume_unique = True ) . astype ( int ) \n    if weighted_score_type == 0 : \n        correl_vector = np . repeat ( 1 , N ) \n    else : \n        correl_vector = np . abs ( correl_vector ) ** weighted_score_type \n    hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) \n    axis = 1 \n    tag_indicator = np . tile ( tag_indicator , ( nperm + 1 , 1 ) ) \n    correl_vector = np . tile ( correl_vector , ( nperm + 1 , 1 ) ) \n    i = 0 \n    while i < nperm : \n        rs . shuffle ( tag_indicator [ i ] ) \n        i = i + 1 \n    Nhint = tag_indicator . sum ( axis = axis , keepdims = True ) \n    sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = True ) \n    no_tag_indicator = 1 - tag_indicator \n    Nmiss = N - Nhint \n    norm_tag = 1.0 / sum_correl_tag \n    norm_no_tag = 1.0 / Nmiss \n    RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) \n    if scale : \n        RES = RES / N \n    if single : \n        es_vec = RES . sum ( axis = axis ) \n    else : \n        max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) \n        es_vec = np . where ( np . abs ( max_ES ) > np . abs ( min_ES ) , max_ES , min_ES ) \n    es , esnull , RES = es_vec [ - 1 ] , es_vec [ : - 1 ] , RES [ - 1 , : ] \n    return es , esnull , hit_ind , RES "}
{"4164": "\ndef gsea_significance ( enrichment_scores , enrichment_nulls ) : \n    np . seterr ( divide = 'ignore' , invalid = 'ignore' ) \n    es = np . array ( enrichment_scores ) \n    esnull = np . array ( enrichment_nulls ) \n    logging . debug ( \"Start to compute pvals..................................\" ) \n    enrichmentPVals = gsea_pval ( es , esnull ) . tolist ( ) \n    logging . debug ( \"Compute nes and nesnull.................................\" ) \n    esnull_pos = ( esnull * ( esnull >= 0 ) ) . mean ( axis = 1 ) \n    esnull_neg = ( esnull * ( esnull < 0 ) ) . mean ( axis = 1 ) \n    nEnrichmentScores = np . where ( es >= 0 , es / esnull_pos , - es / esnull_neg ) \n    nEnrichmentNulls = np . where ( esnull >= 0 , esnull / esnull_pos [ : , np . newaxis ] , - esnull / esnull_neg [ : , np . newaxis ] ) \n    logging . debug ( \"start to compute fdrs..................................\" ) \n    nvals = np . sort ( nEnrichmentNulls . flatten ( ) ) \n    nnes = np . sort ( nEnrichmentScores ) \n    fdrs = [ ] \n    i = 0 \n    while i < len ( enrichment_scores ) : \n        nes = nEnrichmentScores [ i ] \n        if nes >= 0 : \n            allPos = int ( len ( nvals ) - np . searchsorted ( nvals , 0 , side = \"left\" ) ) \n            allHigherAndPos = int ( len ( nvals ) - np . searchsorted ( nvals , nes , side = \"left\" ) ) \n            nesPos = len ( nnes ) - int ( np . searchsorted ( nnes , 0 , side = \"left\" ) ) \n            nesHigherAndPos = len ( nnes ) - int ( np . searchsorted ( nnes , nes , side = \"left\" ) ) \n        else : \n            allPos = int ( np . searchsorted ( nvals , 0 , side = \"left\" ) ) \n            allHigherAndPos = int ( np . searchsorted ( nvals , nes , side = \"right\" ) ) \n            nesPos = int ( np . searchsorted ( nnes , 0 , side = \"left\" ) ) \n            nesHigherAndPos = int ( np . searchsorted ( nnes , nes , side = \"right\" ) ) \n        try : \n            pi_norm = allHigherAndPos / float ( allPos ) \n            pi_obs = nesHigherAndPos / float ( nesPos ) \n            fdr = pi_norm / pi_obs \n            fdrs . append ( fdr if fdr < 1 else 1.0 ) \n        except : \n            fdrs . append ( 1000000000.0 ) \n        i = i + 1 \n    logging . debug ( \"Statistical testing finished.............................\" ) \n    return zip ( enrichment_scores , nEnrichmentScores , enrichmentPVals , fdrs ) "}
{"4186": "\ndef run ( self ) : \n    assert self . min_size <= self . max_size \n    assert self . fignum > 0 \n    import glob \n    from bs4 import BeautifulSoup \n    try : \n        results_path = glob . glob ( self . indir + '*/edb/results.edb' ) [ 0 ] \n        rank_path = glob . glob ( self . indir + '*/edb/*.rnk' ) [ 0 ] \n        gene_set_path = glob . glob ( self . indir + '*/edb/gene_sets.gmt' ) [ 0 ] \n    except IndexError as e : \n        sys . stderr . write ( \"Could not locate GSEA files in the given directory!\" ) \n        sys . exit ( 1 ) \n    cls_path = glob . glob ( self . indir + '*/edb/*.cls' ) \n    if cls_path : \n        pos , neg , classes = gsea_cls_parser ( cls_path [ 0 ] ) \n    else : \n        pos , neg = '' , '' \n    self . gene_sets = gene_set_path \n    gene_set_dict = self . parse_gmt ( gmt = gene_set_path ) \n    rank_metric = self . _load_ranking ( rank_path ) \n    correl_vector = rank_metric . values \n    gene_list = rank_metric . index . values \n    database = BeautifulSoup ( open ( results_path ) , features = 'xml' ) \n    length = len ( database . findAll ( 'DTG' ) ) \n    fig_num = self . fignum if self . fignum <= length else length \n    idx = 0 \n    while idx < fig_num : \n        enrich_term , hit_ind , nes , pval , fdr = gsea_edb_parser ( results_path , index = idx ) \n        gene_set = gene_set_dict . get ( enrich_term ) \n        RES = enrichment_score ( gene_list = gene_list , correl_vector = correl_vector , gene_set = gene_set , weighted_score_type = self . weighted_score_type , nperm = 0 ) [ - 1 ] \n        term = enrich_term . replace ( '/' , '_' ) . replace ( \":\" , \"_\" ) \n        outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) \n        gseaplot ( rank_metric = rank_metric , term = enrich_term , hits_indices = hit_ind , nes = nes , pval = pval , fdr = fdr , RES = RES , pheno_pos = pos , pheno_neg = neg , figsize = self . figsize , ofname = outfile ) \n        idx = idx + 1 \n    self . _logger . info ( \"Congratulations! Your plots have been reproduced successfully!\\n\" ) "}
{"4200": "\ndef check_list ( var , num_terms ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n        _ = 1 \n        while _ < num_terms : \n            var . append ( var [ 0 ] ) \n            _ = _ + 1 \n    if len ( var ) != num_terms : \n        print ( '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' % ( var , num_terms ) ) \n        sys . exit ( 1 ) \n    return var "}
{"4201": "\ndef make_list ( var , num_terms = 1 ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n            _ = 1 \n            while _ < num_terms : \n                var . append ( var [ 0 ] ) \n                _ = _ + 1 \n    return var "}
{"4226": "\ndef join ( script , merge_visible = True , merge_vert = False , delete_layer = True , keep_unreferenced_vert = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Flatten Visible Layers\">\\n' , '    <Param name=\"MergeVisible\" ' , 'value=\"{}\" ' . format ( str ( merge_visible ) . lower ( ) ) , 'description=\"Merge Only Visible Layers\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"MergeVertices\" ' , 'value=\"{}\" ' . format ( str ( merge_vert ) . lower ( ) ) , 'description=\"Merge duplicate vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"DeleteLayer\" ' , 'value=\"{}\" ' . format ( str ( delete_layer ) . lower ( ) ) , 'description=\"Delete Layers\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"AlsoUnreferenced\" ' , 'value=\"{}\" ' . format ( str ( keep_unreferenced_vert ) . lower ( ) ) , 'description=\"Keep unreferenced vertices\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , mlx . FilterScript ) : \n        script . add_layer ( 'Merged Mesh' ) \n        if delete_layer : \n            i = 0 \n            while i < script . last_layer ( ) : \n                script . del_layer ( 0 ) \n                i = i + 1 \n    return None "}
{"4230": "\ndef delete_lower ( script , layer_num = None ) : \n    if layer_num is None : \n        layer_num = script . current_layer ( ) \n    if layer_num != 0 : \n        change ( script , 0 ) \n    i = 0 \n    while i < layer_num : \n        delete ( script , 0 ) \n        i = i + 1 \n    return None "}
{"4237": "\ndef main ( ) : \n    segments = 50 \n    star_points = 5 \n    star_radius = 2 \n    ring_thickness = 1 \n    sphere_radius = 2 * ( star_radius + 3 * ring_thickness ) \n    polygon_radius = star_radius / ( 1 + math . tan ( math . radians ( 180 / star_points ) ) / math . tan ( math . radians ( 90 / star_points ) ) ) \n    width = polygon_radius * math . tan ( math . radians ( 180 / star_points ) ) \n    height = width / math . tan ( math . radians ( 90 / star_points ) ) \n    shield = mlx . FilterScript ( file_out = \"shield.ply\" ) \n    mlx . create . annulus ( shield , radius = star_radius , cir_segments = segments , color = 'blue' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + ring_thickness , radius2 = star_radius , cir_segments = segments , color = 'red' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 2 * ring_thickness , radius2 = star_radius + ring_thickness , cir_segments = segments , color = 'white' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3 * ring_thickness , radius2 = star_radius + 2 * ring_thickness , cir_segments = segments , color = 'red' ) \n    mlx . layers . join ( shield ) \n    mlx . subdivide . midpoint ( shield , iterations = 2 ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3 * ring_thickness , cir_segments = segments , color = 'silver' ) \n    mlx . transform . rotate ( shield , axis = 'y' , angle = 180 ) \n    mlx . transform . translate ( shield , value = [ 0 , 0 , - 0.005 ] ) \n    mlx . subdivide . midpoint ( shield , iterations = 4 ) \n    mlx . create . grid ( shield , size = math . sqrt ( 2 ) , x_segments = 10 , y_segments = 10 , center = True , color = 'white' ) \n    mlx . transform . rotate ( shield , axis = 'z' , angle = 45 ) \n    mlx . transform . scale ( shield , value = [ width , height , 1 ] ) \n    mlx . transform . translate ( shield , value = [ 0 , polygon_radius , 0.001 ] ) \n    _ = 1 \n    while _ < star_points : \n        mlx . layers . duplicate ( shield ) \n        mlx . transform . rotate ( shield , axis = 'z' , angle = 360 / star_points ) \n        _ = _ + 1 \n    mlx . layers . join ( shield ) \n    mlx . transform . vert_function ( shield , z_func = 'sqrt(%s-x^2-y^2)-%s+z' % ( sphere_radius ** 2 , sphere_radius ) ) \n    shield . run_script ( ) \n    return None "}
{"4339": "\ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if self . verbose > 0 : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    grid_start = 0 \n    while grid_start < n_fits : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n        grid_start = grid_start + n_folds \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4388": "\ndef get_gaussian_kernel ( gaussian_kernel_width = 11 , gaussian_kernel_sigma = 1.5 ) : \n    gaussian_kernel_1d = numpy . ndarray ( ( gaussian_kernel_width ) ) \n    norm_mu = int ( gaussian_kernel_width / 2 ) \n    i = 0 \n    while i < gaussian_kernel_width : \n        gaussian_kernel_1d [ i ] = ( exp ( - ( ( ( i - norm_mu ) ** 2 ) ) / ( 2 * ( gaussian_kernel_sigma ** 2 ) ) ) ) \n        i = i + 1 \n    return gaussian_kernel_1d / numpy . sum ( gaussian_kernel_1d ) "}
{"4428": "\ndef get_imap_capabilities ( server ) : \n    capabilities = list ( map ( str , list ( server . capabilities ( ) ) ) ) \n    i = 0 \n    while i < len ( capabilities ) : \n        capabilities [ i ] = str ( capabilities [ i ] ) . replace ( \"b'\" , \"\" ) . replace ( \"'\" , \"\" ) \n        i = i + 1 \n    logger . debug ( \"IMAP server supports: {0}\" . format ( capabilities ) ) \n    return capabilities "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = True ) : \n    if masker is not None : \n        masker = masker \n    else : \n        if isinstance ( dataset , Dataset ) : \n            masker = dataset . masker \n        else : \n            if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n                raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = False ) \n    if regions . ndim == 2 : \n        m = regions \n        i = 0 \n        while i < regions . shape [ 1 ] : \n            _nz = np . nonzero ( m [ : , i ] ) [ 0 ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n            i = i + 1 \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        i = 0 \n        while i < n_regions : \n            if isinstance ( threshold , int ) : \n                m [ regions == labels [ i ] , i ] = 1.0 \n            else : \n                m [ regions == labels [ i ] , i ] = 1.0 / np . sum ( regions == labels [ i ] ) \n            i = i + 1 \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ result < threshold ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4514": "\nasync def request ( self , route , ** kwargs ) : \n    if isinstance ( route , tuple ) : \n        method , url = route \n    else : \n        method = route . method \n        url = route . url \n    if self . bearer_info is None : \n        self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n        access_token = bearer_info [ 'access_token' ] \n    else : \n        access_token = self . bearer_info [ 'access_token' ] \n    headers = { 'Authorization' : 'Bearer ' + access_token , 'Content-Type' : kwargs . get ( 'content_type' , 'application/json' ) , ** kwargs . pop ( 'headers' , { } ) } \n    _ = 0 \n    while _ < self . RETRY_AMOUNT : \n        r = await self . _session . request ( method , url , headers = headers , ** kwargs ) \n        try : \n            status = r . status \n            try : \n                data = json . loads ( await r . text ( encoding = 'utf-8' ) ) \n            except json . decoder . JSONDecodeError : \n                data = { } \n            if 300 > status >= 200 : \n                return data \n            if status == 401 : \n                self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n                headers [ 'Authorization' ] = 'Bearer ' + bearer_info [ 'access_token' ] \n                _ = _ + 1 \n                continue \n            if status == 429 : \n                amount = r . headers . get ( 'Retry-After' ) \n                await asyncio . sleep ( int ( amount ) , loop = self . loop ) \n                _ = _ + 1 \n                continue \n            if status in ( 502 , 503 ) : \n                _ = _ + 1 \n                continue \n            if status == 403 : \n                raise Forbidden ( r , data ) \n            elif status == 404 : \n                raise NotFound ( r , data ) \n        finally : \n            await r . release ( ) \n        _ = _ + 1 "}
{"4638": "\ndef _data_constructor ( self , size ) : \n    result = PyFunceble . OrderedDict ( ) \n    if len ( self . data_to_print ) == len ( size ) : \n        i = 0 \n        while i < len ( self . data_to_print ) : \n            result [ self . data_to_print [ i ] ] = size [ i ] \n            i = i + 1 \n    else : \n        raise Exception ( \"Inputed: \" + str ( len ( self . data_to_print ) ) + \"; Size: \" + str ( len ( size ) ) ) \n    return result "}
{"4760": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( BigInteger , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length % 8 : \n        raise exceptions . InvalidPrimitiveLength ( \"invalid big integer length read; \" \"expected: multiple of 8, observed: {0}\" . format ( self . length ) ) \n    sign = 1 \n    binary = '' \n    _ = 0 \n    while _ < self . length : \n        byte = struct . unpack ( '!B' , istream . read ( 1 ) ) [ 0 ] \n        bits = \"{0:b}\" . format ( byte ) \n        pad = len ( bits ) % 8 \n        if pad : \n            bits = ( '0' * ( 8 - pad ) ) + bits \n        binary += bits \n        _ = _ + 1 \n    if binary [ 0 ] == '1' : \n        sign = - 1 \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    self . value = int ( binary , 2 ) * sign "}
{"4761": "\ndef write ( self , ostream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    binary = \"{0:b}\" . format ( abs ( self . value ) ) \n    binary = ( \"0\" * ( 64 - ( len ( binary ) % 64 ) ) ) + binary \n    if self . value < 0 : \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    hexadecimal = b'' \n    i = 0 \n    while i < len ( binary ) : \n        byte = binary [ i : i + 8 ] \n        byte = int ( byte , 2 ) \n        hexadecimal += struct . pack ( '!B' , byte ) \n        i = i + 8 \n    self . length = len ( hexadecimal ) \n    super ( BigInteger , self ) . write ( ostream , kmip_version = kmip_version ) \n    ostream . write ( hexadecimal ) "}
{"4772": "\ndef validate ( self ) : \n    if not isinstance ( self . value , bytes ) : \n        raise TypeError ( \"key value must be bytes\" ) \n    elif not isinstance ( self . cryptographic_algorithm , enums . CryptographicAlgorithm ) : \n        raise TypeError ( \"key algorithm must be a CryptographicAlgorithm \" \"enumeration\" ) \n    elif not isinstance ( self . cryptographic_length , six . integer_types ) : \n        raise TypeError ( \"key length must be an integer\" ) \n    elif not isinstance ( self . key_format_type , enums . KeyFormatType ) : \n        raise TypeError ( \"key format type must be a KeyFormatType \" \"enumeration\" ) \n    elif self . key_format_type not in self . _valid_formats : \n        raise ValueError ( \"key format type must be one of {0}\" . format ( self . _valid_formats ) ) \n    mask_count = len ( self . cryptographic_usage_masks ) \n    i = 0 \n    while i < mask_count : \n        mask = self . cryptographic_usage_masks [ i ] \n        if not isinstance ( mask , enums . CryptographicUsageMask ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"key mask {0} must be a CryptographicUsageMask \" \"enumeration\" . format ( position ) ) \n        i = i + 1 \n    name_count = len ( self . names ) \n    i = 0 \n    while i < name_count : \n        name = self . names [ i ] \n        if not isinstance ( name , six . string_types ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"key name {0} must be a string\" . format ( position ) ) \n        i = i + 1 "}
{"4773": "\ndef validate ( self ) : \n    if not isinstance ( self . value , bytes ) : \n        raise TypeError ( \"secret value must be bytes\" ) \n    elif not isinstance ( self . data_type , enums . SecretDataType ) : \n        raise TypeError ( \"secret data type must be a SecretDataType \" \"enumeration\" ) \n    mask_count = len ( self . cryptographic_usage_masks ) \n    i = 0 \n    while i < mask_count : \n        mask = self . cryptographic_usage_masks [ i ] \n        if not isinstance ( mask , enums . CryptographicUsageMask ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"secret data mask {0} must be a CryptographicUsageMask \" \"enumeration\" . format ( position ) ) \n        i = i + 1 \n    name_count = len ( self . names ) \n    i = 0 \n    while i < name_count : \n        name = self . names [ i ] \n        if not isinstance ( name , six . string_types ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"secret data name {0} must be a string\" . format ( position ) ) \n        i = i + 1 "}
{"4774": "\ndef validate ( self ) : \n    if not isinstance ( self . value , bytes ) : \n        raise TypeError ( \"opaque value must be bytes\" ) \n    elif not isinstance ( self . opaque_type , enums . OpaqueDataType ) : \n        raise TypeError ( \"opaque data type must be an OpaqueDataType \" \"enumeration\" ) \n    name_count = len ( self . names ) \n    i = 0 \n    while i < name_count : \n        name = self . names [ i ] \n        if not isinstance ( name , six . string_types ) : \n            position = \"({0} in list)\" . format ( i ) \n            raise TypeError ( \"opaque data name {0} must be a string\" . format ( position ) ) \n        i = i + 1 "}
{"5052": "\ndef decode_spans ( self , spans ) : \n    decoded_spans = [ ] \n    transport = TMemoryBuffer ( spans ) \n    if six . byte2int ( spans ) == TType . STRUCT : \n        _ , size = read_list_begin ( transport ) \n    else : \n        size = 1 \n    _ = 0 \n    while _ < size : \n        span = zipkin_core . Span ( ) \n        span . read ( TBinaryProtocol ( transport ) ) \n        decoded_spans . append ( self . _decode_thrift_span ( span ) ) \n        _ = _ + 1 \n    return decoded_spans "}
{"5069": "\nasync def request ( self , method , url , ** kwargs ) : \n    rate_limiter = RateLimiter ( max_calls = 59 , period = 60 , callback = limited ) \n    async with rate_limiter : \n        if not self . token : \n            raise UnauthorizedDetected ( 'UnauthorizedDetected (status code: 401): No TOKEN provided' ) \n        headers = { 'User-Agent' : self . user_agent , 'Content-Type' : 'application/json' } \n        if 'json' in kwargs : \n            kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) \n        kwargs [ 'headers' ] = headers \n        headers [ 'Authorization' ] = self . token \n        tries = 0 \n        while tries < 5 : \n            async with self . session . request ( method , url , ** kwargs ) as resp : \n                log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) \n                data = await json_or_text ( resp ) \n                if 300 > resp . status >= 200 : \n                    return data \n                if resp . status == 429 : \n                    fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' \n                    retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) \n                    mins = retry_after / 60 \n                    log . warning ( fmt , retry_after , mins ) \n                    is_global = True \n                    if is_global : \n                        self . _global_over . clear ( ) \n                    await asyncio . sleep ( retry_after , loop = self . loop ) \n                    log . debug ( 'Done sleeping for the rate limit. Retrying...' ) \n                    if is_global : \n                        self . _global_over . set ( ) \n                        log . debug ( 'Global rate limit is now over.' ) \n                    tries = tries + 1 \n                    continue \n                if resp . status == 400 : \n                    raise HTTPException ( resp , data ) \n                elif resp . status == 401 : \n                    raise Unauthorized ( resp , data ) \n                elif resp . status == 403 : \n                    raise Forbidden ( resp , data ) \n                elif resp . status == 404 : \n                    raise NotFound ( resp , data ) \n                else : \n                    raise HTTPException ( resp , data ) \n            tries = tries + 1 \n        raise HTTPException ( resp , data ) "}
{"5097": "\ndef validate_signature_fragments ( fragments , hash_ , public_key , sponge_type = Kerl , ) : \n    checksum = [ 0 ] * ( HASH_LENGTH * len ( fragments ) ) \n    normalized_hash = normalize ( hash_ ) \n    for i , fragment in enumerate ( fragments ) : \n        outer_sponge = sponge_type ( ) \n        normalized_chunk = normalized_hash [ i % len ( normalized_hash ) ] \n        buffer = [ ] \n        for j , hash_trytes in enumerate ( fragment . iter_chunks ( Hash . LEN ) ) : \n            buffer = hash_trytes . as_trits ( ) \n            inner_sponge = sponge_type ( ) \n            _ = 0 \n            while _ < 13 + normalized_chunk [ j ] : \n                inner_sponge . reset ( ) \n                inner_sponge . absorb ( buffer ) \n                inner_sponge . squeeze ( buffer ) \n                _ = _ + 1 \n            outer_sponge . absorb ( buffer ) \n        outer_sponge . squeeze ( buffer ) \n        checksum [ i * HASH_LENGTH : ( i + 1 ) * HASH_LENGTH ] = buffer \n    actual_public_key = [ 0 ] * HASH_LENGTH \n    addy_sponge = sponge_type ( ) \n    addy_sponge . absorb ( checksum ) \n    addy_sponge . squeeze ( actual_public_key ) \n    return actual_public_key == public_key . as_trits ( ) "}
{"5104": "\ndef _transform ( self ) : \n    state_length = STATE_LENGTH \n    truth_table = TRUTH_TABLE \n    prev_state = self . _state [ : ] \n    new_state = prev_state [ : ] \n    index = 0 \n    _ = 0 \n    while _ < NUMBER_OF_ROUNDS : \n        prev_trit = prev_state [ index ] \n        pos = 0 \n        while pos < state_length : \n            index += ( 364 if index < 365 else - 365 ) \n            new_trit = prev_state [ index ] \n            new_state [ pos ] = truth_table [ prev_trit + ( 3 * new_trit ) + 4 ] \n            prev_trit = new_trit \n            pos = pos + 1 \n        prev_state = new_state \n        new_state = new_state [ : ] \n        _ = _ + 1 \n    self . _state = new_state "}
{"5108": "\ndef add_trits ( left , right ) : \n    target_len = max ( len ( left ) , len ( right ) ) \n    res = [ 0 ] * target_len \n    left += [ 0 ] * ( target_len - len ( left ) ) \n    right += [ 0 ] * ( target_len - len ( right ) ) \n    carry = 0 \n    i = 0 \n    while i < len ( res ) : \n        res [ i ] , carry = _full_add_trits ( left [ i ] , right [ i ] , carry ) \n        i = i + 1 \n    return res "}
{"5138": "\ndef decode ( self , input , errors = 'strict' ) : \n    if isinstance ( input , memoryview ) : \n        input = input . tobytes ( ) \n    if not isinstance ( input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) \n    if not isinstance ( input , bytearray ) : \n        input = bytearray ( input ) \n    bytes_ = bytearray ( ) \n    i = 0 \n    while i < len ( input ) : \n        try : \n            first , second = input [ i : i + 2 ] \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n            i = i + 2 \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n        i = i + 2 \n    return binary_type ( bytes_ ) , len ( input ) "}
{"5160": "\ndef get_digest ( self ) : \n    hashes_per_fragment = FRAGMENT_LENGTH // Hash . LEN \n    key_fragments = self . iter_chunks ( FRAGMENT_LENGTH ) \n    digest = [ 0 ] * HASH_LENGTH * len ( key_fragments ) \n    for i , fragment in enumerate ( key_fragments ) : \n        fragment_trits = fragment . as_trits ( ) \n        key_fragment = [ 0 ] * FRAGMENT_LENGTH \n        hash_trits = [ ] \n        j = 0 \n        while j < hashes_per_fragment : \n            hash_start = j * HASH_LENGTH \n            hash_end = hash_start + HASH_LENGTH \n            hash_trits = fragment_trits [ hash_start : hash_end ] \n            k = 0 \n            while k < 26 : \n                sponge = Kerl ( ) \n                sponge . absorb ( hash_trits ) \n                sponge . squeeze ( hash_trits ) \n                k = k + 1 \n            key_fragment [ hash_start : hash_end ] = hash_trits \n            j = j + 1 \n        sponge = Kerl ( ) \n        sponge . absorb ( key_fragment ) \n        sponge . squeeze ( hash_trits ) \n        fragment_hash_start = i * HASH_LENGTH \n        fragment_hash_end = fragment_hash_start + HASH_LENGTH \n        digest [ fragment_hash_start : fragment_hash_end ] = hash_trits \n    return Digest ( TryteString . from_trits ( digest ) , self . key_index ) "}
{"5161": "\ndef sign_input_transactions ( self , bundle , start_index ) : \n    if not bundle . hash : \n        raise with_context ( exc = ValueError ( 'Cannot sign inputs without a bundle hash!' ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n    from iota . crypto . signing import SignatureFragmentGenerator \n    signature_fragment_generator = ( SignatureFragmentGenerator ( self , bundle . hash ) ) \n    j = 0 \n    while j < self . security_level : \n        try : \n            txn = bundle [ start_index + j ] \n        except IndexError as e : \n            raise with_context ( exc = e , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'current_index' : start_index + j , } , ) \n        if txn . value > 0 : \n            raise with_context ( exc = ValueError ( 'Attempting to sign non-input transaction #{i} ' '(value={value}).' . format ( i = txn . current_index , value = txn . value , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        if txn . signature_message_fragment : \n            raise with_context ( exc = ValueError ( 'Attempting to sign input transaction #{i}, ' 'but it has a non-empty fragment ' '(is it already signed?).' . format ( i = txn . current_index , ) , ) , context = { 'bundle' : bundle , 'key_index' : self . key_index , 'start_index' : start_index , } , ) \n        txn . signature_message_fragment = next ( signature_fragment_generator ) \n        j = j + 1 "}
{"5173": "\ndef _create_input_transactions ( self , addy ) : \n    self . _transactions . append ( ProposedTransaction ( address = addy , tag = self . tag , value = - addy . balance , ) ) \n    _ = 0 \n    while _ < addy . security_level - 1 : \n        self . _transactions . append ( ProposedTransaction ( address = addy , tag = self . tag , value = 0 , ) ) \n        _ = _ + 1 "}
{"5180": "\ndef expand_abbreviations ( self , text ) : \n    if not self . abbreviations : \n        raise LexiconError ( \"No abbreviations in lexicon.\" ) \n    def chunks ( data , SIZE = 25 ) : \n        it = iter ( data ) \n        i = 0 \n        while i < len ( data ) : \n            yield { k : data [ k ] for k in islice ( it , SIZE ) } \n            i = i + SIZE \n    def cb ( g ) : \n        return self . abbreviations . get ( g . group ( 0 ) ) or g . group ( 0 ) \n    text = re . sub ( r'w/' , r'wi' , text ) \n    for subdict in chunks ( self . abbreviations ) : \n        regex = r'(\\b' + r'\\b)|(\\b' . join ( subdict . keys ( ) ) + r'\\b)' \n        text = re . sub ( regex , cb , text ) \n    return text "}
{"5265": "\ndef fuzz ( self , obj ) : \n    buf = list ( obj ) \n    FuzzFactor = random . randrange ( 1 , len ( buf ) ) \n    numwrites = random . randrange ( math . ceil ( ( float ( len ( buf ) ) / FuzzFactor ) ) ) + 1 \n    j = 0 \n    while j < numwrites : \n        self . random_action ( buf ) \n        j = j + 1 \n    return self . safe_unicode ( buf ) "}
{"5418": "\ndef read_from ( cls , data_stream , num_to_read ) : \n    vlrlist = cls ( ) \n    _ = 0 \n    while _ < num_to_read : \n        raw = RawVLR . read_from ( data_stream ) \n        try : \n            vlrlist . append ( vlr_factory ( raw ) ) \n        except UnicodeDecodeError : \n            logger . error ( \"Failed to decode VLR: {}\" . format ( raw ) ) \n        _ = _ + 1 \n    return vlrlist "}
{"5456": "\ndef list_set_bits ( r , expected_length ) : \n    set_bit_numbers = [ ] \n    bit_index = 0x1 \n    assert ( len ( r ) == expected_length + 1 ) \n    for b in r [ 1 : ] : \n        i = 0 \n        while i < 8 : \n            if ( ( b >> i ) & 1 ) == 1 : \n                set_bit_numbers . append ( bit_index ) \n            bit_index += 1 \n            i = i + 1 \n    return set_bit_numbers "}
{"5602": "\ndef _main ( self , client , bucket , key , fileobj , extra_args , callbacks , max_attempts , download_output_manager , io_chunksize , start_index = 0 , bandwidth_limiter = None ) : \n    last_exception = None \n    i = 0 \n    while i < max_attempts : \n        try : \n            response = client . get_object ( Bucket = bucket , Key = key , ** extra_args ) \n            streaming_body = StreamReaderProgress ( response [ 'Body' ] , callbacks ) \n            if bandwidth_limiter : \n                streaming_body = bandwidth_limiter . get_bandwith_limited_stream ( streaming_body , self . _transfer_coordinator ) \n            current_index = start_index \n            chunks = DownloadChunkIterator ( streaming_body , io_chunksize ) \n            for chunk in chunks : \n                if not self . _transfer_coordinator . done ( ) : \n                    self . _handle_io ( download_output_manager , fileobj , chunk , current_index ) \n                    current_index += len ( chunk ) \n                else : \n                    return \n            return \n        except S3_RETRYABLE_DOWNLOAD_ERRORS as e : \n            logger . debug ( \"Retrying exception caught (%s), \" \"retrying request, (attempt %s / %s)\" , e , i , max_attempts , exc_info = True ) \n            last_exception = e \n            invoke_progress_callbacks ( callbacks , start_index - current_index ) \n            i = i + 1 \n            continue \n        i = i + 1 \n    raise RetriesExceededError ( last_exception ) "}
{"5814": "\ndef _create_idx_from_stream ( self , stream ) : \n    stream_iter = iter ( stream ) \n    dimension = self . properties . dimension \n    darray = ctypes . c_double * dimension \n    mins = darray ( ) \n    maxs = darray ( ) \n    no_data = ctypes . cast ( ctypes . pointer ( ctypes . c_ubyte ( 0 ) ) , ctypes . POINTER ( ctypes . c_ubyte ) ) \n    def py_next_item ( p_id , p_mins , p_maxs , p_dimension , p_data , p_length ) : \n        try : \n            p_id [ 0 ] , coordinates , obj = next ( stream_iter ) \n        except StopIteration : \n            return - 1 \n        except Exception as exc : \n            self . _exception = exc \n            return - 1 \n        if self . interleaved : \n            coordinates = Index . deinterleave ( coordinates ) \n        i = 0 \n        while i < dimension : \n            mins [ i ] = coordinates [ i * 2 ] \n            maxs [ i ] = coordinates [ ( i * 2 ) + 1 ] \n            i = i + 1 \n        p_mins [ 0 ] = ctypes . cast ( mins , ctypes . POINTER ( ctypes . c_double ) ) \n        p_maxs [ 0 ] = ctypes . cast ( maxs , ctypes . POINTER ( ctypes . c_double ) ) \n        p_dimension [ 0 ] = dimension \n        if obj is None : \n            p_data [ 0 ] = no_data \n            p_length [ 0 ] = 0 \n        else : \n            p_length [ 0 ] , data , _ = self . _serialize ( obj ) \n            p_data [ 0 ] = ctypes . cast ( data , ctypes . POINTER ( ctypes . c_ubyte ) ) \n        return 0 \n    stream = core . NEXTFUNC ( py_next_item ) \n    return IndexStreamHandle ( self . properties . handle , stream ) "}
{"5933": "\ndef get_short_uid ( self , uid ) : \n    if uid : \n        short_uids = self . get_short_uid_dict ( ) \n        length_of_uid = len ( uid ) \n        while length_of_uid < 0 : \n            if short_uids . get ( uid [ : length_of_uid ] ) is not None : \n                return uid [ : length_of_uid ] \n            length_of_uid = length_of_uid + - 1 \n    return \"\" "}
{"5960": "\ndef refresh_indices ( model , block_size = 100 ) : \n    conn = _connect ( model ) \n    max_id = int ( conn . get ( '%s:%s:' % ( model . _namespace , model . _pkey ) ) or '0' ) \n    block_size = max ( block_size , 10 ) \n    i = 1 \n    while i < max_id + 1 : \n        models = model . get ( list ( range ( i , i + block_size ) ) ) \n        models \n        session . commit ( all = True ) \n        yield min ( i + block_size , max_id ) , max_id \n        i = i + block_size "}
{"5961": "\ndef clean_old_index ( model , block_size = 100 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2 ] ) ) \n    has_hscan = version >= [ 2 , 8 ] \n    pipe = conn . pipeline ( True ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10 ) \n    force_hscan = kwargs . get ( 'force_hscan' , False ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = 0 \n        while cursor != b'0' : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , 0 , 0 ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned += block_size \n            if scanned > max_id : \n                max_id = scanned + 1 \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while cursor != b'0' : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , 1 , 0 ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned += block_size \n                if scanned > max_id : \n                    max_id = scanned + 1 \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        i = 1 \n        while i < max_id + 1 : \n            ids = list ( range ( i , min ( i + block_size , max_id + 1 ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - 1 ) , max_id \n            i = i + block_size \n    yield max_id , max_id "}
{"5988": "\ndef ensure_running ( self ) : \n    with self . _lock : \n        if self . _fd is not None : \n            if self . _check_alive ( ) : \n                return \n            os . close ( self . _fd ) \n            try : \n                os . waitpid ( self . _pid , 0 ) \n            except OSError : \n                pass \n            self . _fd = None \n            self . _pid = None \n            warnings . warn ( 'semaphore_tracker: process died unexpectedly, ' 'relaunching.  Some semaphores might leak.' ) \n        fds_to_pass = [ ] \n        try : \n            fds_to_pass . append ( sys . stderr . fileno ( ) ) \n        except Exception : \n            pass \n        r , w = os . pipe ( ) \n        cmd = 'from {} import main; main({}, {})' . format ( main . __module__ , r , VERBOSE ) \n        try : \n            fds_to_pass . append ( r ) \n            exe = spawn . get_executable ( ) \n            args = [ exe ] + util . _args_from_interpreter_flags ( ) \n            if sys . version_info [ : 2 ] <= ( 3 , 3 ) : \n                import re \n                i = 1 \n                while i < len ( args ) : \n                    args [ i ] = re . sub ( \"-R+\" , \"-R\" , args [ i ] ) \n                    i = i + 1 \n            args += [ '-c' , cmd ] \n            util . debug ( \"launching Semaphore tracker: {}\" . format ( args ) ) \n            try : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_BLOCK , _IGNORED_SIGNALS ) \n                pid = spawnv_passfds ( exe , args , fds_to_pass ) \n            finally : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n        except BaseException : \n            os . close ( w ) \n            raise \n        else : \n            self . _fd = w \n            self . _pid = pid \n        finally : \n            os . close ( r ) "}
{"6008": "\ndef print_stack_trace ( proc_obj , count = None , color = 'plain' , opts = { } ) : \n    if count is None : \n        n = len ( proc_obj . stack ) \n    else : \n        n = min ( len ( proc_obj . stack ) , count ) \n    try : \n        i = 0 \n        while i < n : \n            print_stack_entry ( proc_obj , i , color = color , opts = opts ) \n            i = i + 1 \n    except KeyboardInterrupt : \n        pass \n    return "}
{"6038": "\ndef pprint_simple_array ( val , displaywidth , msg_nocr , msg , lineprefix = '' ) : \n    if type ( val ) != list : \n        return False \n    numeric = True \n    i = 0 \n    while i < len ( val ) : \n        if not ( type ( val [ i ] ) in [ bool , float , int ] ) : \n            numeric = False \n            if not ( type ( val [ i ] ) in [ bool , float , int , bytes ] ) : \n                return False \n            pass \n        pass \n        i = i + 1 \n    mess = columnize ( [ repr ( v ) for v in val ] , opts = { \"arrange_array\" : True , \"lineprefix\" : lineprefix , \"displaywidth\" : int ( displaywidth ) - 3 , 'ljust' : not numeric } ) \n    msg_nocr ( mess ) \n    return True "}
{"6082": "\ndef iterate_docs ( client , expanded = False , progress = False ) : \n    num_docs = client . get ( ) [ 'document_count' ] \n    progress_bar = None \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Downloading documents' , total = num_docs ) \n        offset = 0 \n        while offset < num_docs : \n            response = client . get ( 'docs' , offset = offset , limit = DOCS_PER_BATCH ) \n            docs = response [ 'result' ] \n            for doc in docs : \n                if expanded : \n                    for field in UNNECESSARY_FIELDS : \n                        doc . pop ( field , None ) \n                else : \n                    doc = { field : doc [ field ] for field in CONCISE_FIELDS } \n                if progress : \n                    progress_bar . update ( ) \n                yield doc \n            offset = offset + DOCS_PER_BATCH \n    finally : \n        if progress : \n            progress_bar . close ( ) "}
{"6216": "\ndef sample ( self , input , steps ) : \n    inputs = [ [ onehot ( self . input_dim , x ) for x in input ] ] \n    _ = 0 \n    while _ < steps : \n        target = self . compute ( inputs ) [ 0 , - 1 ] . argmax ( ) \n        input . append ( target ) \n        inputs [ 0 ] . append ( onehot ( self . input_dim , target ) ) \n        _ = _ + 1 \n    return input "}
{"6245": "\ndef create_2d_gaussian ( dim , sigma ) : \n    if dim % 2 == 0 : \n        raise ValueError ( \"Kernel dimension should be odd\" ) \n    kernel = np . zeros ( ( dim , dim ) , dtype = np . float16 ) \n    center = dim / 2 \n    variance = sigma ** 2 \n    coeff = 1. / ( 2 * variance ) \n    x = 0 \n    while x < dim : \n        y = 0 \n        while y < dim : \n            x_val = abs ( x - center ) \n            y_val = abs ( y - center ) \n            numerator = x_val ** 2 + y_val ** 2 \n            denom = 2 * variance \n            kernel [ x , y ] = coeff * np . exp ( - 1. * numerator / denom ) \n            y = y + 1 \n        x = x + 1 \n    return kernel / sum ( sum ( kernel ) ) "}
{"6362": "\ndef make_fuzzy ( word , max = 1 ) : \n    neighbors = [ ] \n    i = 0 \n    while i < len ( word ) - 1 : \n        neighbor = list ( word ) \n        neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] \n        neighbors . append ( '' . join ( neighbor ) ) \n        i = i + 1 \n    for letter in string . ascii_lowercase : \n        i = 0 \n        while i < len ( word ) : \n            neighbor = list ( word ) \n            if letter != neighbor [ i ] : \n                neighbor [ i ] = letter \n                neighbors . append ( '' . join ( neighbor ) ) \n            i = i + 1 \n    for letter in string . ascii_lowercase : \n        i = 0 \n        while i < len ( word ) + 1 : \n            neighbor = list ( word ) \n            neighbor . insert ( i , letter ) \n            neighbors . append ( '' . join ( neighbor ) ) \n            i = i + 1 \n    if len ( word ) > 3 : \n        i = 0 \n        while i < len ( word ) : \n            neighbor = list ( word ) \n            del neighbor [ i ] \n            neighbors . append ( '' . join ( neighbor ) ) \n            i = i + 1 \n    return neighbors "}
{"6367": "\ndef do_DBINFO ( self , * args ) : \n    info = DB . info ( ) \n    keys = [ 'keyspace_misses' , 'keyspace_hits' , 'used_memory_human' , 'total_commands_processed' , 'total_connections_received' , 'connected_clients' ] \n    for key in keys : \n        print ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) \n    nb_of_redis_db = int ( DB . config_get ( 'databases' ) [ 'databases' ] ) \n    db_index = 0 \n    while db_index < nb_of_redis_db - 1 : \n        db_name = 'db{}' . format ( db_index ) \n        if db_name in info : \n            label = white ( 'nb keys (db {})' . format ( db_index ) ) \n            print ( '{}: {}' . format ( label , blue ( info [ db_name ] [ 'keys' ] ) ) ) \n        db_index = db_index + 1 "}
{"6480": "\ndef walkFlatten ( self , offset : int = 0 , shouldEnterFn = _default_shouldEnterFn , otherObjItCtx : ObjIteratorCtx = _DummyIteratorCtx ( ) ) -> Generator [ Union [ Tuple [ Tuple [ int , int ] , 'TransTmpl' ] , 'OneOfTransaction' ] , None , None ] : \n    t = self . dtype \n    base = self . bitAddr + offset \n    end = self . bitAddrEnd + offset \n    shouldEnter , shouldYield = shouldEnterFn ( self ) \n    if shouldYield : \n        yield ( ( base , end ) , self ) \n    if shouldEnter : \n        if isinstance ( t , Bits ) : \n            pass \n        elif isinstance ( t , HStruct ) : \n            for ch in self . children : \n                with otherObjItCtx ( ch . origin . name ) : \n                    yield from ch . walkFlatten ( offset , shouldEnterFn , otherObjItCtx ) \n        elif isinstance ( t , HArray ) : \n            itemSize = ( self . bitAddrEnd - self . bitAddr ) // self . itemCnt \n            i = 0 \n            while i < self . itemCnt : \n                with otherObjItCtx ( i ) : \n                    yield from self . children . walkFlatten ( base + i * itemSize , shouldEnterFn , otherObjItCtx ) \n                i = i + 1 \n        elif isinstance ( t , HUnion ) : \n            yield OneOfTransaction ( self , offset , shouldEnterFn , self . children ) \n        elif isinstance ( t , HStream ) : \n            assert len ( self . children ) == 1 \n            yield StreamTransaction ( self , offset , shouldEnterFn , self . children [ 0 ] ) \n        else : \n            raise TypeError ( t ) "}
{"6500": "\ndef iterBits ( sigOrVal : Union [ RtlSignal , Value ] , bitsInOne : int = 1 , skipPadding : bool = True , fillup : bool = False ) : \n    bw = BitWalker ( sigOrVal , skipPadding , fillup ) \n    _ = 0 \n    while _ < ceil ( sigOrVal . _dtype . bit_length ( ) / bitsInOne ) : \n        yield bw . get ( bitsInOne ) \n        _ = _ + 1 \n    bw . assertIsOnEnd ( ) "}
{"6574": "\ndef animate ( frames , interval , name , iterations = 2 ) : \n    i = 0 \n    while i < iterations : \n        for frame in frames : \n            frame = get_coded_text ( frame ) \n            output = \"\\r{0} {1}\" . format ( frame , name ) \n            sys . stdout . write ( output ) \n            sys . stdout . write ( CLEAR_LINE ) \n            sys . stdout . flush ( ) \n            time . sleep ( 0.001 * interval ) \n        i = i + 1 "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = True ) : \n    scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( index < 0 ) . any ( ) or ( index > n ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( index == n ) \n    index [ omegas ] -= 1 \n    offset [ omegas ] += interval_length \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ 0 ] = 1.0 \n    T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    i = 2 \n    while i < coefficient_count : \n        T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2 ] \n        i = i + 1 \n    components = ( T . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        components = components [ : , 0 ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ 0 ] = 0.0 \n    dT [ 1 ] = 1.0 \n    if coefficient_count > 2 : \n        dT [ 2 ] = twot1 + twot1 \n        i = 3 \n        while i < coefficient_count : \n            dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2 ] + T [ i - 1 ] + T [ i - 1 ] \n            i = i + 1 \n    dT *= 2.0 \n    dT /= interval_length \n    rates = ( dT . T * coefficients ) . sum ( axis = 2 ) \n    if scalar : \n        rates = rates [ : , 0 ] \n    return components , rates "}
{"6662": "\ndef has_context_with ( state , incorrect_msg , exact_names ) : \n    i = 0 \n    while i < len ( state . solution_parts [ \"context\" ] ) : \n        ctxt_state = check_part_index ( state , \"context\" , i , \"{{ordinal}} context\" ) \n        _has_context ( ctxt_state , incorrect_msg or MSG_INCORRECT_WITH , exact_names ) \n        i = i + 1 \n    return state "}
{"6751": "\ndef get_ipaddress ( self , name , retries = 3 , delay = 3 ) : \n    rr = 0 \n    while rr < retries : \n        instances = self . _get_instances ( ) \n        for instance in instances [ 'items' ] : \n            if instance [ 'name' ] == name : \n                for network in instance [ 'networkInterfaces' ] : \n                    if network [ 'name' ] == 'nic0' : \n                        for subnet in network [ 'accessConfigs' ] : \n                            if subnet [ 'name' ] == 'External NAT' : \n                                if 'natIP' in subnet : \n                                    return subnet [ 'natIP' ] \n        sleep ( delay ) \n        rr = rr + 1 \n    bot . warning ( 'Did not find IP address, check Cloud Console!' ) "}
{"6883": "\ndef _countdown ( self , waitTime = 0 , printString = \"Waiting %*d seconds...\" , verbose = True ) : \n    if waitTime <= 0 : \n        waitTime = self . __retryDelay \n    remaining = waitTime \n    while remaining < 0 : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , remaining ) , end = \"\" , flush = True ) \n        time . sleep ( 1 ) \n        remaining = remaining + - 1 \n    if verbose : \n        _vPrint ( verbose , \"\\r\" + printString % ( len ( str ( waitTime ) ) , 0 ) ) "}
{"6924": "\ndef map_exact2foreign_invoice_numbers ( self , exact_invoice_numbers = None ) : \n    if exact_invoice_numbers is None : \n        ret = self . filter ( select = 'InvoiceNumber,YourRef' ) \n        return dict ( ( i [ 'InvoiceNumber' ] , i [ 'YourRef' ] ) for i in ret ) \n    exact_to_foreign_map = { } \n    exact_invoice_numbers = list ( set ( exact_invoice_numbers ) ) \n    offset = 0 \n    while offset < len ( exact_invoice_numbers ) : \n        batch = exact_invoice_numbers [ offset : ( offset + 40 ) ] \n        filter_ = ' or ' . join ( 'InvoiceNumber eq %s' % ( i , ) for i in batch ) \n        assert filter_ \n        ret = self . filter ( filter = filter_ , select = 'InvoiceNumber,YourRef' ) \n        exact_to_foreign_map . update ( dict ( ( i [ 'InvoiceNumber' ] , i [ 'YourRef' ] ) for i in ret ) ) \n        offset = offset + 40 \n    for exact_invoice_number in exact_invoice_numbers : \n        if exact_invoice_number not in exact_to_foreign_map : \n            exact_to_foreign_map [ exact_invoice_number ] = None \n    return exact_to_foreign_map "}
{"6925": "\ndef solve ( grid ) : \n    clauses = sudoku_clauses ( ) \n    i = 1 \n    while i < 10 : \n        j = 1 \n        while j < 10 : \n            d = grid [ i - 1 ] [ j - 1 ] \n            if d : \n                clauses . append ( [ v ( i , j , d ) ] ) \n            j = j + 1 \n        i = i + 1 \n    sol = set ( pycosat . solve ( clauses ) ) \n    def read_cell ( i , j ) : \n        d = 1 \n        while d < 10 : \n            if v ( i , j , d ) in sol : \n                return d \n            d = d + 1 \n    i = 1 \n    while i < 10 : \n        j = 1 \n        while j < 10 : \n            grid [ i - 1 ] [ j - 1 ] = read_cell ( i , j ) \n            j = j + 1 \n        i = i + 1 "}
{"6960": "\ndef main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , clear ) : \n    logger . info ( \"Starting trimmomatic\" ) \n    cli = [ \"java\" , \"-Xmx{}\" . format ( \"$task.memory\" [ : - 1 ] . lower ( ) . replace ( \" \" , \"\" ) ) , \"-jar\" , TRIM_PATH . strip ( ) , \"PE\" , \"-threads\" , \"$task.cpus\" ] \n    try : \n        phred = int ( phred ) \n        phred_flag = \"-phred{}\" . format ( str ( phred ) ) \n        cli += [ phred_flag ] \n    except ValueError : \n        pass \n    cli += fastq_pair \n    output_names = [ ] \n    i = 0 \n    while i < len ( fastq_pair ) : \n        output_names . append ( \"{}_{}_trim.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n        output_names . append ( \"{}_{}_U.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n        i = i + 1 \n    cli += output_names \n    if trim_range != [ \"None\" ] : \n        cli += [ \"CROP:{}\" . format ( trim_range [ 1 ] ) , \"HEADCROP:{}\" . format ( trim_range [ 0 ] ) , ] \n    if os . path . exists ( adapters_file ) : \n        logger . debug ( \"Using the provided adapters file '{}'\" . format ( adapters_file ) ) \n    else : \n        logger . debug ( \"Adapters file '{}' not provided or does not exist. Using\" \" default adapters\" . format ( adapters_file ) ) \n        adapters_file = merge_default_adapters ( ) \n    cli += [ \"ILLUMINACLIP:{}:3:30:10:6:true\" . format ( adapters_file ) ] \n    logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , \"{}_trimlog.txt\" . format ( sample_id ) ) \n    cli += [ \"SLIDINGWINDOW:{}\" . format ( trim_opts [ 0 ] ) , \"LEADING:{}\" . format ( trim_opts [ 1 ] ) , \"TRAILING:{}\" . format ( trim_opts [ 2 ] ) , \"MINLEN:{}\" . format ( trim_opts [ 3 ] ) , \"TOPHRED33\" , \"-trimlog\" , logfile ] \n    logger . debug ( \"Running trimmomatic subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished trimmomatic subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Finished trimmomatic subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished trimmomatic with return code: {}\" . format ( p . returncode ) ) \n    trimmomatic_log ( logfile , sample_id ) \n    if p . returncode == 0 and os . path . exists ( \"{}_1_trim.fastq.gz\" . format ( SAMPLE_ID ) ) : \n        clean_up ( fastq_pair , clear ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        if p . returncode != 0 : \n            status_fh . write ( \"fail\" ) \n            return \n        else : \n            status_fh . write ( \"pass\" ) "}
{"6986": "\ndef parse_pipeline ( pipeline_str ) : \n    if os . path . exists ( pipeline_str ) : \n        logger . debug ( \"Found pipeline file: {}\" . format ( pipeline_str ) ) \n        with open ( pipeline_str ) as fh : \n            pipeline_str = \"\" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) \n    logger . info ( colored_print ( \"Resulting pipeline string:\\n\" ) ) \n    logger . info ( colored_print ( pipeline_str + \"\\n\" ) ) \n    insanity_checks ( pipeline_str ) \n    logger . debug ( \"Parsing pipeline string: {}\" . format ( pipeline_str ) ) \n    pipeline_links = [ ] \n    lane = 1 \n    pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( pipeline_str ) \n    nforks = pipeline_str_modified . count ( FORK_TOKEN ) \n    logger . debug ( \"Found {} fork(s)\" . format ( nforks ) ) \n    if not nforks : \n        logger . debug ( \"Detected linear pipeline string : {}\" . format ( pipeline_str ) ) \n        linear_pipeline = [ \"__init__\" ] + pipeline_str_modified . split ( ) \n        pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) \n        pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n        return pipeline_links \n    i = 0 \n    while i < nforks : \n        logger . debug ( \"Processing fork {} in lane {}\" . format ( i , lane ) ) \n        fields = pipeline_str_modified . split ( FORK_TOKEN , i + 1 ) \n        previous_process = fields [ - 2 ] . split ( LANE_TOKEN ) [ - 1 ] . split ( ) \n        logger . debug ( \"Previous processes string: {}\" . format ( fields [ - 2 ] ) ) \n        logger . debug ( \"Previous processes list: {}\" . format ( previous_process ) ) \n        next_lanes = get_lanes ( fields [ - 1 ] ) \n        logger . debug ( \"Next lanes object: {}\" . format ( next_lanes ) ) \n        fork_sink = [ x [ 0 ] for x in next_lanes ] \n        logger . debug ( \"The fork sinks into the processes: {}\" . format ( fork_sink ) ) \n        if i == 0 : \n            if not previous_process : \n                previous_process = [ \"__init__\" ] \n                lane = 0 \n            else : \n                previous_process = [ \"__init__\" ] + previous_process \n            pipeline_links . extend ( linear_connection ( previous_process , lane ) ) \n        fork_source = previous_process [ - 1 ] \n        logger . debug ( \"Fork source is set to: {}\" . format ( fork_source ) ) \n        fork_lane = get_source_lane ( previous_process , pipeline_links ) \n        logger . debug ( \"Fork lane is set to: {}\" . format ( fork_lane ) ) \n        pipeline_links . extend ( fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) \n        pipeline_links . extend ( linear_lane_connection ( next_lanes , lane ) ) \n        lane += len ( fork_sink ) \n        i = i + 1 \n    pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n    return pipeline_links "}
{"7021": "\ndef define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : \n    tasks_array = tasks . split ( ) \n    for task_unsplit in tasks_array : \n        task = task_unsplit . split ( \"=\" ) [ 0 ] \n        if task not in process_descriptions . keys ( ) : \n            logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) \n            sys . exit ( ) \n        else : \n            process_split = task_unsplit . split ( \"=\" ) \n            if len ( process_split ) > 1 : \n                self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] \n        if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            task_pipeline = [ ] \n            if task in process_descriptions : \n                if check_upstream : \n                    task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n                task_pipeline . append ( task ) \n                if check_downstream : \n                    task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n            forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) \n        elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : \n            for fork in forks : \n                if task not in fork : \n                    try : \n                        dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) \n                        fork . insert ( dependent_index , task ) \n                    except ValueError : \n                        continue \n    i = 0 \n    while i < len ( forks ) : \n        j = 0 \n        while j < len ( forks [ i ] ) : \n            try : \n                if len ( forks [ i ] [ j ] . split ( \"|\" ) ) > 1 : \n                    forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) \n                    tmp_fork = [ ] \n                    for s in forks [ i ] [ j ] : \n                        if s in total_tasks : \n                            tmp_fork . append ( s ) \n                    forks [ i ] [ j ] = tmp_fork \n            except AttributeError as e : \n                j = j + 1 \n                continue \n            j = j + 1 \n        i = i + 1 \n    return forks "}
{"7039": "\ndef get_gc_sliding ( self , window = 2000 ) : \n    gc_res = [ ] \n    complete_seq = \"\" . join ( self . contigs . values ( ) ) . lower ( ) \n    i = 0 \n    while i < len ( complete_seq ) : \n        seq_window = complete_seq [ i : i + window ] \n        gc_res . append ( round ( self . _gc_prop ( seq_window , len ( seq_window ) ) , 2 ) ) \n        i = i + window \n    return gc_res "}
{"7075": "\ndef _send_live_report ( self , report_id ) : \n    buffer_size = 100 \n    logger . debug ( \"Report buffer size set to: {}\" . format ( buffer_size ) ) \n    i = 0 \n    while i < len ( self . report_queue ) : \n        reports_compilation = [ ] \n        for report in self . report_queue [ i : i + buffer_size ] : \n            try : \n                report_file = [ x for x in os . listdir ( report ) if x . endswith ( \".json\" ) ] [ 0 ] \n            except IndexError : \n                continue \n            with open ( join ( report , report_file ) ) as fh : \n                reports_compilation . append ( json . loads ( fh . read ( ) ) ) \n        logger . debug ( \"Payload sent with size: {}\" . format ( asizeof ( json . dumps ( reports_compilation ) ) ) ) \n        logger . debug ( \"status: {}\" . format ( self . status_info ) ) \n        try : \n            requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : reports_compilation , \"status\" : self . status_info } ) \n        except requests . exceptions . ConnectionError : \n            logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n            sys . exit ( 1 ) \n        i = i + buffer_size \n    if not self . report_queue : \n        logger . debug ( \"status: {}\" . format ( self . status_info ) ) \n        try : \n            requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : [ ] , \"status\" : self . status_info } ) \n        except requests . exceptions . ConnectionError : \n            logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The\" \" server may be down or there is a problem with your \" \"internet connection.\" , \"red_bold\" ) ) \n            sys . exit ( 1 ) \n    self . report_queue = [ ] "}
{"7101": "\ndef median_filter ( X , M = 8 ) : \n    i = 0 \n    while i < X . shape [ 1 ] : \n        X [ : , i ] = filters . median_filter ( X [ : , i ] , size = M ) \n        i = i + 1 \n    return X "}
{"7104": "\ndef compute_nc ( X , G ) : \n    N = X . shape [ 0 ] \n    M = G . shape [ 0 ] \n    nc = np . zeros ( N ) \n    i = M // 2 \n    while i < N - M // 2 + 1 : \n        nc [ i ] = np . sum ( X [ i - M // 2 : i + M // 2 , i - M // 2 : i + M // 2 ] * G ) \n        i = i + 1 \n    nc += nc . min ( ) \n    nc /= nc . max ( ) \n    return nc "}
{"7105": "\ndef gaussian_filter ( X , M = 8 , axis = 0 ) : \n    i = 0 \n    while i < X . shape [ axis ] : \n        if axis == 1 : \n            X [ : , i ] = filters . gaussian_filter ( X [ : , i ] , sigma = M / 2. ) \n        elif axis == 0 : \n            X [ i , : ] = filters . gaussian_filter ( X [ i , : ] , sigma = M / 2. ) \n        i = i + 1 \n    return X "}
{"7106": "\ndef compute_nc ( X ) : \n    N = X . shape [ 0 ] \n    nc = np . zeros ( N ) \n    i = 0 \n    while i < N - 1 : \n        nc [ i ] = distance . euclidean ( X [ i , : ] , X [ i + 1 , : ] ) \n        i = i + 1 \n    nc += np . abs ( nc . min ( ) ) \n    nc /= float ( nc . max ( ) ) \n    return nc "}
{"7107": "\ndef circular_shift ( X ) : \n    N = X . shape [ 0 ] \n    L = np . zeros ( X . shape ) \n    i = 0 \n    while i < N : \n        L [ i , : ] = np . asarray ( [ X [ ( i + j ) % N , j ] for j in range ( N ) ] ) \n        i = i + 1 \n    return L "}
{"7108": "\ndef embedded_space ( X , m , tau = 1 ) : \n    N = X . shape [ 0 ] - int ( np . ceil ( m ) ) \n    Y = np . zeros ( ( N , int ( np . ceil ( X . shape [ 1 ] * m ) ) ) ) \n    i = 0 \n    while i < N : \n        rem = int ( ( m % 1 ) * X . shape [ 1 ] ) \n        Y [ i , : ] = np . concatenate ( ( X [ i : i + int ( m ) , : ] . flatten ( ) , X [ i + int ( m ) , : rem ] ) ) \n        i = i + 1 \n    return Y "}
{"7114": "\ndef get_feat_segments ( F , bound_idxs ) : \n    assert len ( bound_idxs ) > 0 , \"Boundaries can't be empty\" \n    bound_idxs = np . sort ( bound_idxs ) \n    assert bound_idxs [ 0 ] >= 0 and bound_idxs [ - 1 ] < F . shape [ 0 ] , \"Boundaries are not correct for the given feature dimensions.\" \n    feat_segments = [ ] \n    i = 0 \n    while i < len ( bound_idxs ) - 1 : \n        feat_segments . append ( F [ bound_idxs [ i ] : bound_idxs [ i + 1 ] , : ] ) \n        i = i + 1 \n    return feat_segments "}
{"7121": "\ndef save_estimations ( file_struct , times , labels , boundaries_id , labels_id , ** params ) : \n    params . pop ( \"features\" , None ) \n    dur = get_duration ( file_struct . features_file ) \n    if 'numpy' in str ( type ( times ) ) : \n        inters = utils . times_to_intervals ( times ) \n        assert len ( inters ) == len ( labels ) , \"Number of boundary intervals \" \"(%d) and labels (%d) do not match\" % ( len ( inters ) , len ( labels ) ) \n        inters = [ inters ] \n        labels = [ labels ] \n    else : \n        inters = [ ] \n        level = 0 \n        while level < len ( times ) : \n            est_inters = utils . times_to_intervals ( times [ level ] ) \n            inters . append ( est_inters ) \n            assert len ( inters [ level ] ) == len ( labels [ level ] ) , \"Number of boundary intervals (%d) and labels (%d) do not \" \"match in level %d\" % ( len ( inters [ level ] ) , len ( labels [ level ] ) , level ) \n            level = level + 1 \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jams . Annotation ( namespace = namespace ) \n    if os . path . isfile ( file_struct . est_file ) : \n        jam = jams . load ( file_struct . est_file , validate = False ) \n        curr_ann = find_estimation ( jam , boundaries_id , labels_id , params ) \n        if curr_ann is not None : \n            curr_ann . data = ann . data \n            ann = curr_ann \n        else : \n            jam . annotations . append ( ann ) \n    else : \n        jam = jams . JAMS ( ) \n        jam . file_metadata . duration = dur \n        jam . annotations . append ( ann ) \n    ann . annotation_metadata . version = msaf . __version__ \n    ann . annotation_metadata . data_source = \"MSAF\" \n    sandbox = { } \n    sandbox [ \"boundaries_id\" ] = boundaries_id \n    sandbox [ \"labels_id\" ] = labels_id \n    sandbox [ \"timestamp\" ] = datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) \n    for key in params : \n        sandbox [ key ] = params [ key ] \n    ann . sandbox = sandbox \n    for i , ( level_inters , level_labels ) in enumerate ( zip ( inters , labels ) ) : \n        for bound_inter , label in zip ( level_inters , level_labels ) : \n            dur = float ( bound_inter [ 1 ] ) - float ( bound_inter [ 0 ] ) \n            label = chr ( int ( label ) + 65 ) \n            if params [ \"hier\" ] : \n                value = { \"label\" : label , \"level\" : i } \n            else : \n                value = label \n            ann . append ( time = bound_inter [ 0 ] , duration = dur , value = value ) \n    jam . save ( file_struct . est_file ) "}
{"7129": "\ndef align_segmentation ( beat_times , song ) : \n    try : \n        segment_times , segment_labels = msaf . io . read_references ( song ) \n    except : \n        return None , None , None \n    segment_times = np . asarray ( segment_times ) \n    segment_intervals = msaf . utils . times_to_intervals ( segment_times ) \n    beat_intervals = np . asarray ( zip ( beat_times [ : - 1 ] , beat_times [ 1 : ] ) ) \n    beat_segment_ids = librosa . util . match_intervals ( beat_intervals , segment_intervals ) \n    segment_beats = [ ] \n    segment_times_out = [ ] \n    segment_labels_out = [ ] \n    i = 0 \n    while i < segment_times . shape [ 0 ] : \n        hits = np . argwhere ( beat_segment_ids == i ) \n        if len ( hits ) > 0 and i < len ( segment_intervals ) and i < len ( segment_labels ) : \n            segment_beats . extend ( hits [ 0 ] ) \n            segment_times_out . append ( segment_intervals [ i , : ] ) \n            segment_labels_out . append ( segment_labels [ i ] ) \n        i = i + 1 \n    segment_beats = list ( segment_beats ) \n    segment_times_out = segment_times \n    return segment_beats , segment_times_out , segment_labels_out "}
{"7162": "\ndef _distance ( self , idx ) : \n    if scipy . sparse . issparse ( self . data ) : \n        step = self . data . shape [ 1 ] \n    else : \n        step = 50000 \n    d = np . zeros ( ( self . data . shape [ 1 ] ) ) \n    if idx == - 1 : \n        vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) \n        if scipy . sparse . issparse ( self . data ) : \n            vec = scipy . sparse . csc_matrix ( vec ) \n    else : \n        vec = self . data [ : , idx : idx + 1 ] \n    self . _logger . info ( 'compute distance to node ' + str ( idx ) ) \n    idx_start = 0 \n    while idx_start < self . data . shape [ 1 ] : \n        if idx_start + step > self . data . shape [ 1 ] : \n            idx_end = self . data . shape [ 1 ] \n        else : \n            idx_end = idx_start + step \n        d [ idx_start : idx_end ] = self . _distfunc ( self . data [ : , idx_start : idx_end ] , vec ) \n        self . _logger . info ( 'completed:' + str ( idx_end / ( self . data . shape [ 1 ] / 100.0 ) ) + \"%\" ) \n        idx_start = idx_start + step \n    return d "}
{"7163": "\ndef estimate_K_knee ( self , th = .015 , maxK = 12 ) : \n    if self . X . shape [ 0 ] < maxK : \n        maxK = self . X . shape [ 0 ] \n    if maxK < 2 : \n        maxK = 2 \n    K = np . arange ( 1 , maxK ) \n    bics = [ ] \n    for k in K : \n        means , labels = self . run_kmeans ( self . X , k ) \n        bic = self . compute_bic ( self . X , means , labels , K = k , R = self . X . shape [ 0 ] ) \n        bics . append ( bic ) \n    diff_bics = np . diff ( bics ) \n    finalK = K [ - 1 ] \n    if len ( bics ) == 1 : \n        finalK = 2 \n    else : \n        bics = np . asarray ( bics ) \n        bics -= bics . min ( ) \n        diff_bics -= diff_bics . min ( ) \n        i = 0 \n        while i < len ( K [ : - 1 ] ) : \n            if diff_bics [ i ] < th and K [ i ] != 1 : \n                finalK = K [ i ] \n                break \n            i = i + 1 \n    if self . plot : \n        plt . subplot ( 2 , 1 , 1 ) \n        plt . plot ( K , bics , label = \"BIC\" ) \n        plt . plot ( K [ : - 1 ] , diff_bics , label = \"BIC diff\" ) \n        plt . legend ( loc = 2 ) \n        plt . subplot ( 2 , 1 , 2 ) \n        plt . scatter ( self . X [ : , 0 ] , self . X [ : , 1 ] ) \n        plt . show ( ) \n    return finalK "}
{"7166": "\ndef compute_bic ( self , D , means , labels , K , R ) : \n    D = vq . whiten ( D ) \n    Rn = D . shape [ 0 ] \n    M = D . shape [ 1 ] \n    if R == K : \n        return 1 \n    mle_var = 0 \n    k = 0 \n    while k < len ( means ) : \n        X = D [ np . argwhere ( labels == k ) ] \n        X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) \n        for x in X : \n            mle_var += distance . euclidean ( x , means [ k ] ) \n        k = k + 1 \n    mle_var /= float ( R - K ) \n    l_D = - Rn / 2. * np . log ( 2 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) \n    p = ( K - 1 ) + M * K + mle_var \n    return l_D - p / 2. * np . log ( R ) "}
{"7177": "\ndef run_hierarchical ( audio_file , bounds_module , labels_module , frame_times , config , annotator_id = 0 ) : \n    if bounds_module is None : \n        raise NoHierBoundaryError ( \"A boundary algorithm is needed when using \" \"hierarchical segmentation.\" ) \n    features = config [ \"features\" ] . features \n    S = bounds_module . Segmenter ( audio_file , ** config ) \n    est_idxs , est_labels = S . processHierarchical ( ) \n    if labels_module is not None and bounds_module . __name__ != labels_module . __name__ : \n        flat_config = deepcopy ( config ) \n        flat_config [ \"hier\" ] = False \n        for i , level_idxs in enumerate ( est_idxs ) : \n            S = labels_module . Segmenter ( audio_file , in_bound_idxs = level_idxs , ** flat_config ) \n            est_labels [ i ] = S . processFlat ( ) [ 1 ] \n    est_times = [ ] \n    cleaned_est_labels = [ ] \n    level = 0 \n    while level < len ( est_idxs ) : \n        est_level_times , est_level_labels = utils . process_segmentation_level ( est_idxs [ level ] , est_labels [ level ] , features . shape [ 0 ] , frame_times , config [ \"features\" ] . dur ) \n        est_times . append ( est_level_times ) \n        cleaned_est_labels . append ( est_level_labels ) \n        level = level + 1 \n    est_labels = cleaned_est_labels \n    return est_times , est_labels "}
{"7182": "\ndef update_w ( self ) : \n    def update_single_w ( i ) : \n        FB = base . matrix ( np . float64 ( np . dot ( - self . data . T , W_hat [ : , i ] ) ) ) \n        be = solvers . qp ( HB , FB , INQa , INQb , EQa , EQb ) \n        self . beta [ i , : ] = np . array ( be [ 'x' ] ) . reshape ( ( 1 , self . _num_samples ) ) \n    HB = base . matrix ( np . float64 ( np . dot ( self . data [ : , : ] . T , self . data [ : , : ] ) ) ) \n    EQb = base . matrix ( 1.0 , ( 1 , 1 ) ) \n    W_hat = np . dot ( self . data , pinv ( self . H ) ) \n    INQa = base . matrix ( - np . eye ( self . _num_samples ) ) \n    INQb = base . matrix ( 0.0 , ( self . _num_samples , 1 ) ) \n    EQa = base . matrix ( 1.0 , ( 1 , self . _num_samples ) ) \n    i = 0 \n    while i < self . _num_bases : \n        update_single_w ( i ) \n        i = i + 1 \n    self . W = np . dot ( self . beta , self . data . T ) . T "}
{"7291": "\ndef Q ( self ) : \n    tmp = np . einsum ( 'ia,ij->ija' , self . Pi , self . W ) \n    diag_vals = np . sum ( tmp , axis = 0 ) \n    x = 0 \n    while x < tmp . shape [ - 1 ] : \n        np . fill_diagonal ( tmp [ : , : , x ] , - diag_vals [ : , x ] ) \n        x = x + 1 \n    return tmp "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        ii = 0 \n        while ii < n_ints : \n            jj = 0 \n            while jj < n_ifs : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n                jj = jj + 1 \n            ii = ii + 1 \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7804": "\ndef blank_dc ( self , n_coarse_chan ) : \n    if n_coarse_chan < 1 : \n        logger . warning ( 'Coarse channel number < 1, unable to blank DC bin.' ) \n        return None \n    if not n_coarse_chan % int ( n_coarse_chan ) == 0 : \n        logger . warning ( 'Selection does not contain an interger number of coarse channels, unable to blank DC bin.' ) \n        return None \n    n_coarse_chan = int ( n_coarse_chan ) \n    n_chan = self . data . shape [ - 1 ] \n    n_chan_per_coarse = int ( n_chan / n_coarse_chan ) \n    mid_chan = int ( n_chan_per_coarse / 2 ) \n    ii = 0 \n    while ii < n_coarse_chan : \n        ss = ii * n_chan_per_coarse \n        self . data [ ... , ss + mid_chan ] = np . median ( self . data [ ... , ss + mid_chan + 5 : ss + mid_chan + 10 ] ) \n        ii = ii + 1 "}
{"7834": "\ndef read_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None ) : \n    self . _setup_selection_range ( f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop ) \n    if self . isheavy ( ) : \n        logger . warning ( \"Selection size of %.2f GB, exceeding our size limit %.2f GB. Instance created, \" \"header loaded, but data not loaded, please try another (t,v) selection.\" % ( self . _calc_selection_size ( ) / ( 1024. ** 3 ) , self . MAX_DATA_ARRAY_SIZE / ( 1024. ** 3 ) ) ) \n        self . data = np . array ( [ 0 ] , dtype = self . _d_type ) \n        return None \n    self . _setup_chans ( ) \n    self . _setup_freqs ( ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . selection_shape [ self . freq_axis ] \n    n_ifs = self . header [ b'nifs' ] \n    f = open ( self . filename , 'rb' ) \n    f . seek ( int ( self . idx_data ) ) \n    n_ints = self . t_stop - self . t_start \n    f . seek ( int ( self . t_start * self . _n_bytes * n_ifs * n_chans ) , 1 ) \n    self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = self . _d_type ) \n    ii = 0 \n    while ii < n_ints : \n        jj = 0 \n        while jj < n_ifs : \n            f . seek ( int ( self . _n_bytes * self . chan_start_idx ) , 1 ) \n            dd = np . fromfile ( f , count = n_chans_selected , dtype = self . _d_type ) \n            self . data [ ii , jj ] = dd \n            f . seek ( int ( self . _n_bytes * ( n_chans - self . chan_stop_idx ) ) , 1 ) \n            jj = jj + 1 \n        ii = ii + 1 "}
{"7854": "\ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    if not HAS_BITSHUFFLE : \n        print ( \"Error: the bitshuffle library is required to run this script.\" ) \n        exit ( ) \n    parser = ArgumentParser ( description = \"Command line utility for creating HDF5 Raw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) \n    args = parser . parse_args ( ) \n    fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] \n    filelist = glob . glob ( fileroot + '*.raw' ) \n    filelist = sorted ( filelist ) \n    r = GuppiRaw ( filelist [ 0 ] ) \n    header , data = r . read_next_data_block ( ) \n    dshape = data . shape \n    print ( dshape ) \n    n_blocks_total = 0 \n    for filename in filelist : \n        print ( filename ) \n        r = GuppiRaw ( filename ) \n        n_blocks_total += r . n_blocks \n    print ( n_blocks_total ) \n    full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) \n    h5 = h5py . File ( fileroot + '.h5' , 'w' ) \n    h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' \n    block_size = 0 \n    dset = h5 . create_dataset ( 'data' , shape = full_dshape , dtype = data . dtype ) \n    h5_idx = 0 \n    for filename in filelist : \n        print ( \"\\nReading %s header...\" % filename ) \n        r = GuppiRaw ( filename ) \n        h5 = h5py . File ( filename + '.h5' , 'w' ) \n        header , data = r . read_next_data_block ( ) \n        ii = 0 \n        while ii < r . n_blocks : \n            t0 = time . time ( ) \n            print ( \"Reading block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            header , data = r . read_next_data_block ( ) \n            t1 = time . time ( ) \n            t2 = time . time ( ) \n            print ( \"Writing block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            dset [ h5_idx , : ] = data \n            t3 = time . time ( ) \n            print ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) \n            h5_idx += 1 \n            for key , value in header . items ( ) : \n                dset . attrs [ key ] = value \n            ii = ii + 1 \n        h5 . close ( ) \n        t1 = time . time ( ) \n        print ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) ) "}
{"7902": "\ndef construct_covariance_matrix ( cvec , parallax , radial_velocity , radial_velocity_error ) : \n    if np . ndim ( cvec ) == 1 : \n        cmat = np . zeros ( ( 1 , 6 , 6 ) ) \n        nsources = 1 \n        cv = np . atleast_2d ( cvec ) \n    else : \n        nsources = cvec . shape [ 0 ] \n        cmat = np . zeros ( ( nsources , 6 , 6 ) ) \n        cv = cvec \n    k = 0 \n    while k < nsources : \n        cmat [ k , 0 : 5 , 0 : 5 ] = cv [ k , 0 : 5 ] ** 2 \n        k = k + 1 \n    iu = np . triu_indices ( 5 , k = 1 ) \n    k = 0 \n    while k < 10 : \n        i = iu [ 0 ] [ k ] \n        j = iu [ 1 ] [ k ] \n        cmat [ : , i , j ] = cv [ : , i ] * cv [ : , j ] * cv [ : , k + 5 ] \n        cmat [ : , j , i ] = cmat [ : , i , j ] \n        k = k + 1 \n    k = 0 \n    while k < nsources : \n        cmat [ k , 0 : 5 , 5 ] = cmat [ k , 0 : 5 , 2 ] * np . atleast_1d ( radial_velocity ) [ k ] / auKmYearPerSec \n        k = k + 1 \n    cmat [ : , 5 , 0 : 5 ] = cmat [ : , 0 : 5 , 5 ] \n    cmat [ : , 5 , 5 ] = cmat [ : , 2 , 2 ] * ( radial_velocity ** 2 + radial_velocity_error ** 2 ) / auKmYearPerSec ** 2 + ( parallax * radial_velocity_error / auKmYearPerSec ) ** 2 \n    return np . squeeze ( cmat ) "}
{"7927": "\ndef group_iterator ( group ) : \n    ordered_chars = string . ascii_letters + string . digits \n    tokenizer = ( '(?P<seq>[a-zA-Z0-9]-[a-zA-Z0-9])|' '(?P<chr>.)' ) \n    for m in re . finditer ( tokenizer , group ) : \n        if m . group ( 'seq' ) : \n            start , sep , end = m . group ( 'seq' ) \n            i = ordered_chars . index ( start ) \n            while i < ordered_chars . index ( end ) + 1 : \n                yield ordered_chars [ i ] \n                i = i + 1 \n        else : \n            yield m . group ( 'chr' ) "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        i = 0 \n        while i < len ( lines ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n            i = i + 1 \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while idx < len ( code ) - 1 : \n            if comment_start is None and code [ idx : idx + 2 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            elif comment_start is None and code [ idx : idx + 2 ] == '/*' : \n                comment_start = idx \n            elif comment_start is not None and code [ idx : idx + 2 ] == '*/' : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2 : ] ) \n                idx -= idx - comment_start \n                comment_start = None \n            idx += 1 \n    return code "}
{"7970": "\ndef compile_relative_distances ( self , sympy_accesses = None ) : \n    if sympy_accesses is None : \n        sympy_accesses = self . compile_sympy_accesses ( ) \n    sympy_distances = defaultdict ( list ) \n    for var_name , accesses in sympy_accesses . items ( ) : \n        i = 1 \n        while i < len ( accesses ) : \n            sympy_distances [ var_name ] . append ( ( accesses [ i - 1 ] - accesses [ i ] ) . simplify ( ) ) \n            i = i + 1 \n    return sympy_distances "}
{"8045": "\ndef set_defaults ( self ) : \n    defaults = { } \n    i = 0 \n    while i < len ( self ) : \n        if i in self . _data : \n            i = i + 1 \n            continue \n        default = self . get_field_descriptor ( i ) . tags . get ( \"default\" , [ None ] ) [ 0 ] \n        if default is not None : \n            defaults [ i ] = default \n        i = i + 1 \n    self . update ( defaults ) "}
{"8097": "\ndef calc ( pvalues , lamb ) : \n    m = len ( pvalues ) \n    pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) \n    pFDR = np . ones ( m ) \n    print ( \"pFDR    y        Pr     fastPow\" ) \n    i = 0 \n    while i < m : \n        y = pvalues [ i ] \n        Pr = max ( 1 , m - i ) / float ( m ) \n        pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) \n        print ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) \n        i = i + 1 \n    num_null = pi0 * m \n    num_alt = m - num_null \n    num_negs = np . array ( range ( m ) ) \n    num_pos = m - num_negs \n    pp = num_pos / float ( m ) \n    qvalues = np . ones ( m ) \n    qvalues [ 0 ] = pFDR [ 0 ] \n    i = 0 \n    while i < m - 1 : \n        qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) \n        i = i + 1 \n    sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt \n    sens [ sens > 1.0 ] = 1.0 \n    df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) \n    df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] \n    return df , num_null , m "}
{"8100": "\ndef posterior_chromatogram_hypotheses_fast ( experiment , prior_chrom_null ) : \n    tg_ids = experiment . df . tg_num_id . values \n    pp_values = 1 - experiment . df [ \"pep\" ] . values \n    current_tg_id = tg_ids [ 0 ] \n    scores = [ ] \n    final_result = [ ] \n    final_result_h0 = [ ] \n    i = 0 \n    while i < tg_ids . shape [ 0 ] : \n        id_ = tg_ids [ i ] \n        if id_ != current_tg_id : \n            prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) \n            rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) \n            final_result . extend ( rr [ 1 : ] ) \n            final_result_h0 . extend ( rr [ 0 ] for i in range ( len ( scores ) ) ) \n            scores = [ ] \n            current_tg_id = id_ \n        scores . append ( 1.0 - pp_values [ i ] ) \n        i = i + 1 \n    prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) \n    rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) \n    final_result . extend ( rr [ 1 : ] ) \n    final_result_h0 . extend ( [ rr [ 0 ] ] * len ( scores ) ) \n    return final_result , final_result_h0 "}
{"8260": "\ndef euler_scheme ( traj , diff_func ) : \n    steps = traj . steps \n    initial_conditions = traj . initial_conditions \n    dimension = len ( initial_conditions ) \n    result_array = np . zeros ( ( steps , dimension ) ) \n    func_params_dict = traj . func_params . f_to_dict ( short_names = True , fast_access = True ) \n    result_array [ 0 ] = initial_conditions \n    idx = 1 \n    while idx < steps : \n        result_array [ idx ] = diff_func ( result_array [ idx - 1 ] , ** func_params_dict ) * traj . dt + result_array [ idx - 1 ] \n        idx = idx + 1 \n    traj . f_add_result ( 'euler_evolution' , data = result_array , comment = 'Our time series data!' ) "}
{"8276": "\ndef _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : \n    assert ( end_time >= start_time + time_window ) \n    bins = ( end_time - start_time ) / time_window \n    bins = int ( np . floor ( bins ) ) \n    binned_spikes = np . zeros ( bins ) \n    spike_array_neuron = spike_res . t [ spike_res . i == neuron_id ] \n    bin = 0 \n    while bin < bins : \n        lower_time = start_time + time_window * bin \n        upper_time = start_time + time_window * ( bin + 1 ) \n        spike_array_interval = spike_array_neuron [ spike_array_neuron >= lower_time ] \n        spike_array_interval = spike_array_interval [ spike_array_interval < upper_time ] \n        spikes = len ( spike_array_interval ) \n        binned_spikes [ bin ] = spikes \n        bin = bin + 1 \n    var = np . var ( binned_spikes ) \n    avg = np . mean ( binned_spikes ) \n    if avg > 0 : \n        return var / float ( avg ) \n    else : \n        return 0 "}
{"8311": "\ndef _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : \n    result_list = [ ] \n    full_name_set = set ( ) \n    colon_name = '.' . join ( split_name ) \n    key = split_name [ - 1 ] \n    candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) \n    parent_full_name = start_node . v_full_name \n    split_length = len ( split_name ) \n    for candidate_name in candidate_dict : \n        candidate = candidate_dict [ candidate_name ] \n        if key != candidate . v_name or candidate . v_full_name in full_name_set : \n            continue \n        if candidate_name . startswith ( parent_full_name ) : \n            if parent_full_name != '' : \n                reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] \n            else : \n                reduced_candidate_name = candidate_name \n            candidate_split_name = reduced_candidate_name . split ( '.' ) \n            if len ( candidate_split_name ) > max_depth : \n                break \n            if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : \n                result_list . append ( candidate ) \n                full_name_set . add ( candidate . v_full_name ) \n            elif shortcuts : \n                candidate_set = set ( candidate_split_name ) \n                climbing = True \n                for name in split_name : \n                    if name not in candidate_set : \n                        climbing = False \n                        break \n                if climbing : \n                    count = 0 \n                    candidate_length = len ( candidate_split_name ) \n                    idx = 0 \n                    while idx < candidate_length : \n                        if idx + split_length - count > candidate_length : \n                            break \n                        if split_name [ count ] == candidate_split_name [ idx ] : \n                            count += 1 \n                            if count == len ( split_name ) : \n                                result_list . append ( candidate ) \n                                full_name_set . add ( candidate . v_full_name ) \n                                break \n                        idx = idx + 1 \n    return result_list "}
{"8390": "\ndef cellular_automaton_1D ( initial_state , rule_number , steps ) : \n    ncells = len ( initial_state ) \n    pattern = np . zeros ( ( steps , ncells ) ) \n    pattern [ 0 , : ] = initial_state \n    binary_rule = convert_rule ( rule_number ) \n    neighbourhood_factors = np . array ( [ 1 , 2 , 4 ] ) \n    all_cells = range ( ncells ) \n    step = 0 \n    while step < steps - 1 : \n        current_row = pattern [ step , : ] \n        next_row = pattern [ step + 1 , : ] \n        for irun in all_cells : \n            neighbour_indices = range ( irun - 1 , irun + 2 ) \n            neighbourhood = np . take ( current_row , neighbour_indices , mode = 'wrap' ) \n            decimal_neighborhood = int ( np . sum ( neighbourhood * neighbourhood_factors ) ) \n            next_state = binary_rule [ decimal_neighborhood ] \n            next_row [ irun ] = next_state \n        step = step + 1 \n    return pattern "}
{"8403": "\ndef _trj_prepare_merge ( self , traj , changed_parameters , old_length ) : \n    if not traj . _stored : \n        traj . f_store ( ) \n    infotable = getattr ( self . _overview_group , 'info' ) \n    insert_dict = self . _all_extract_insert_dict ( traj , infotable . colnames ) \n    self . _all_add_or_modify_row ( traj . v_name , insert_dict , infotable , index = 0 , flags = ( HDF5StorageService . MODIFY_ROW , ) ) \n    for param_name in changed_parameters : \n        param = traj . f_get ( param_name ) \n        try : \n            self . _all_delete_parameter_or_result_or_group ( param ) \n        except pt . NoSuchNodeError : \n            pass \n    run_table = getattr ( self . _overview_group , 'runs' ) \n    actual_rows = run_table . nrows \n    self . _trj_fill_run_table ( traj , actual_rows , len ( traj ) ) \n    idx = old_length \n    while idx < len ( traj ) : \n        run_name = traj . f_idx_to_run ( idx ) \n        run_info = traj . f_get_run_information ( run_name ) \n        run_info [ 'name' ] = run_name \n        traj . _set_explored_parameters_to_idx ( idx ) \n        run_summary = self . _srn_summarize_explored_parameters ( list ( traj . _explored_parameters . values ( ) ) ) \n        run_info [ 'parameter_summary' ] = run_summary \n        self . _all_add_or_modify_row ( run_name , run_info , run_table , index = idx , flags = ( HDF5StorageService . MODIFY_ROW , ) ) \n        idx = idx + 1 \n    traj . f_restore_default ( ) "}
{"8404": "\ndef _trj_load_meta_data ( self , traj , load_data , as_new , with_run_information , force ) : \n    metatable = self . _overview_group . info \n    metarow = metatable [ 0 ] \n    try : \n        version = metarow [ 'version' ] . decode ( 'utf-8' ) \n    except ( IndexError , ValueError ) as ke : \n        self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) \n        version = '`COULD NOT BE LOADED`' \n    try : \n        python = metarow [ 'python' ] . decode ( 'utf-8' ) \n    except ( IndexError , ValueError ) as ke : \n        self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) \n        python = '`COULD NOT BE LOADED`' \n    self . _trj_check_version ( version , python , force ) \n    self . _grp_load_group ( traj , load_data = load_data , with_links = False , recursive = False , _traj = traj , _as_new = as_new , _hdf5_group = self . _trajectory_group ) \n    if as_new : \n        length = int ( metarow [ 'length' ] ) \n        irun = 0 \n        while irun < length : \n            traj . _add_run_info ( irun ) \n            irun = irun + 1 \n    else : \n        traj . _comment = metarow [ 'comment' ] . decode ( 'utf-8' ) \n        traj . _timestamp = float ( metarow [ 'timestamp' ] ) \n        traj . _trajectory_timestamp = traj . _timestamp \n        traj . _time = metarow [ 'time' ] . decode ( 'utf-8' ) \n        traj . _trajectory_time = traj . _time \n        traj . _name = metarow [ 'name' ] . decode ( 'utf-8' ) \n        traj . _trajectory_name = traj . _name \n        traj . _version = version \n        traj . _python = python \n        single_run_table = self . _overview_group . runs \n        if with_run_information : \n            for row in single_run_table . iterrows ( ) : \n                name = row [ 'name' ] . decode ( 'utf-8' ) \n                idx = int ( row [ 'idx' ] ) \n                timestamp = float ( row [ 'timestamp' ] ) \n                time_ = row [ 'time' ] . decode ( 'utf-8' ) \n                completed = int ( row [ 'completed' ] ) \n                summary = row [ 'parameter_summary' ] . decode ( 'utf-8' ) \n                hexsha = row [ 'short_environment_hexsha' ] . decode ( 'utf-8' ) \n                try : \n                    runtime = row [ 'runtime' ] . decode ( 'utf-8' ) \n                    finish_timestamp = float ( row [ 'finish_timestamp' ] ) \n                except ( IndexError , ValueError ) as ke : \n                    runtime = '' \n                    finish_timestamp = 0.0 \n                    self . _logger . debug ( 'Could not load runtime, ' + repr ( ke ) ) \n                info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time_ , 'completed' : completed , 'name' : name , 'parameter_summary' : summary , 'short_environment_hexsha' : hexsha } \n                traj . _add_run_info ( ** info_dict ) \n        else : \n            traj . _length = single_run_table . nrows \n    self . _trj_load_exploration ( traj ) \n    self . _srvc_load_hdf5_settings ( ) "}
{"8407": "\ndef _trj_fill_run_table ( self , traj , start , stop ) : \n    def _make_row ( info_dict ) : \n        row = ( info_dict [ 'idx' ] , info_dict [ 'name' ] , info_dict [ 'time' ] , info_dict [ 'timestamp' ] , info_dict [ 'finish_timestamp' ] , info_dict [ 'runtime' ] , info_dict [ 'parameter_summary' ] , info_dict [ 'short_environment_hexsha' ] , info_dict [ 'completed' ] ) \n        return row \n    runtable = getattr ( self . _overview_group , 'runs' ) \n    rows = [ ] \n    updated_run_information = traj . _updated_run_information \n    idx = start \n    while idx < stop : \n        info_dict = traj . _run_information [ traj . _single_run_ids [ idx ] ] \n        rows . append ( _make_row ( info_dict ) ) \n        updated_run_information . discard ( idx ) \n        idx = idx + 1 \n    if rows : \n        runtable . append ( rows ) \n        runtable . flush ( ) \n    rows = [ ] \n    indices = [ ] \n    for idx in updated_run_information : \n        info_dict = traj . f_get_run_information ( idx , copy = False ) \n        rows . append ( _make_row ( info_dict ) ) \n        indices . append ( idx ) \n    if rows : \n        runtable . modify_coordinates ( indices , rows ) \n    traj . _updated_run_information = set ( ) "}
{"8445": "\ndef _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , ** kwargs ) : \n    datasize = data . shape [ 0 ] \n    try : \n        description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) \n        description_dicts = [ { } ] \n        if len ( description_dict ) > ptpa . MAX_COLUMNS : \n            new_table_group = self . _hdf5file . create_group ( where = hdf5_group , name = tablename , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            count = 0 \n            for innerkey in description_dict : \n                val = description_dict [ innerkey ] \n                if count == ptpa . MAX_COLUMNS : \n                    description_dicts . append ( { } ) \n                    count = 0 \n                description_dicts [ - 1 ] [ innerkey ] = val \n                count += 1 \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , 1 ) \n            hdf5_group = new_table_group \n        else : \n            description_dicts = [ description_dict ] \n        for idx , descr_dict in enumerate ( description_dicts ) : \n            if idx == 0 : \n                tblname = tablename \n            else : \n                tblname = tablename + '_%d' % idx \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = datasize , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) \n            row = table . row \n            n = 0 \n            while n < datasize : \n                for key in descr_dict : \n                    row [ key ] = data [ key ] [ n ] \n                row . append ( ) \n                n = n + 1 \n            if idx == 0 and len ( description_dict ) <= ptpa . MAX_COLUMNS : \n                for field_name in data_type_dict : \n                    type_description = data_type_dict [ field_name ] \n                    self . _all_set_attr ( table , field_name , type_description ) \n                setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n        if len ( description_dict ) > ptpa . MAX_COLUMNS : \n            tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE \n            field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) \n            data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } \n            descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) \n            table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = len ( field_names ) , filters = self . _all_get_filters ( kwargs ) ) \n            row = table . row \n            n = 0 \n            while n < len ( field_names ) : \n                for key in data_type_table_dict : \n                    row [ key ] = data_type_table_dict [ key ] [ n ] \n                row . append ( ) \n                n = n + 1 \n            setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , 1 ) \n            table . flush ( ) \n            self . _hdf5file . flush ( ) \n    except : \n        self . _logger . error ( 'Failed storing table `%s` of `%s`.' % ( tablename , fullname ) ) \n        raise "}
{"8459": "\ndef f_iter_runs ( self , start = 0 , stop = None , step = 1 , yields = 'name' ) : \n    if stop is None : \n        stop = len ( self ) \n    elif stop > len ( self ) : \n        raise ValueError ( 'Stop cannot be larger than the trajectory lenght.' ) \n    yields = yields . lower ( ) \n    if yields == 'name' : \n        yield_func = lambda x : self . f_idx_to_run ( x ) \n    elif yields == 'idx' : \n        yield_func = lambda x : x \n    elif yields == 'self' : \n        yield_func = lambda x : self \n    elif yields == 'copy' : \n        yield_func = lambda x : self . __copy__ ( ) \n    else : \n        raise ValueError ( 'Please choose yields among: `name`, `idx`, or `self`.' ) \n    idx = start \n    while idx < stop : \n        self . f_set_crun ( idx ) \n        yield yield_func ( idx ) \n        idx = idx + step \n    self . f_set_crun ( None ) "}
{"8468": "\ndef f_explore ( self , build_dict ) : \n    run_idx = 0 \n    while run_idx < len ( self ) : \n        if self . f_is_completed ( run_idx ) : \n            raise TypeError ( 'You cannot explore a trajectory which has been explored before, ' 'please use `f_expand` instead.' ) \n        run_idx = run_idx + 1 \n    added_explored_parameters = [ ] \n    try : \n        length = len ( self ) \n        for key , builditerable in build_dict . items ( ) : \n            act_param = self . f_get ( key ) \n            if not act_param . v_is_leaf or not act_param . v_is_parameter : \n                raise ValueError ( '%s is not an appropriate search string for a parameter.' % key ) \n            act_param . f_unlock ( ) \n            act_param . _explore ( builditerable ) \n            added_explored_parameters . append ( act_param ) \n            full_name = act_param . v_full_name \n            self . _explored_parameters [ full_name ] = act_param \n            act_param . _explored = True \n            if len ( self . _explored_parameters ) == 1 : \n                length = act_param . f_get_range_length ( ) \n            elif not length == act_param . f_get_range_length ( ) : \n                raise ValueError ( 'The parameters to explore have not the same size!' ) \n        irun = 0 \n        while irun < length : \n            self . _add_run_info ( irun ) \n            irun = irun + 1 \n        self . _test_run_addition ( length ) \n    except Exception : \n        for param in added_explored_parameters : \n            param . f_unlock ( ) \n            param . _shrink ( ) \n            param . _explored = False \n            full_name = param . v_full_name \n            del self . _explored_parameters [ full_name ] \n        if len ( self . _explored_parameters ) == 0 : \n            self . f_shrink ( force = True ) \n        raise "}
{"8477": "\ndef _make_reversed_wildcards ( self , old_length = - 1 ) : \n    if len ( self . _reversed_wildcards ) > 0 : \n        start = old_length \n    else : \n        start = - 1 \n    for wildcards , func in self . _wildcard_functions . items ( ) : \n        irun = start \n        while irun < len ( self ) : \n            translated_name = func ( irun ) \n            if not translated_name in self . _reversed_wildcards : \n                self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) \n            self . _reversed_wildcards [ translated_name ] [ 0 ] . append ( irun ) \n            irun = irun + 1 "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    print ( 'Starting Euler Integration' ) \n    step = 1 \n    while step < steps : \n        if V_array [ step - 1 ] >= 1 : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        elif spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : \n            V_array [ step ] = 0 \n        else : \n            dV = - 1 / tau_V * V_array [ step - 1 ] + I \n            V_array [ step ] = V_array [ step - 1 ] + dV * dt \n        step = step + 1 \n    print ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8836": "\ndef _group_sentences ( total_nb_sentences , group_length ) : \n    sentences_groups = [ ] \n    current_sentence_group = [ ] \n    i = 0 \n    while i < total_nb_sentences : \n        if i % group_length == 0 : \n            if len ( current_sentence_group ) > 0 : \n                sentences_groups . append ( current_sentence_group ) \n            current_sentence_group = [ i ] \n        else : \n            current_sentence_group . append ( i ) \n        i = i + 1 \n    if len ( current_sentence_group ) > 0 : \n        sentences_groups . append ( current_sentence_group ) \n    return sentences_groups "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if len ( unique_labels ) != 2 : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    row_i = 0 \n    while row_i < features . shape [ 0 ] : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] += 1 \n        row_i = row_i + 1 \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( class_labels == unique_labels [ 0 ] ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ 0 ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if fraction > overall_class_fraction : \n            self . feature_map [ feature_instance ] = unique_labels [ 0 ] \n        elif fraction == overall_class_fraction : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = unique_labels [ 1 ] \n    return self "}
{"8847": "\ndef fit ( self , features , targets ) : \n    self . feature_map = defaultdict ( lambda : self . default_label ) \n    self . overall_mean_trait_value = np . mean ( targets ) \n    self . mdr_matrix_values = defaultdict ( list ) \n    row_i = 0 \n    while row_i < features . shape [ 0 ] : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . mdr_matrix_values [ feature_instance ] . append ( targets [ row_i ] ) \n        row_i = row_i + 1 \n    for feature_instance in self . mdr_matrix_values : \n        grid_mean_trait_value = np . mean ( self . mdr_matrix_values [ feature_instance ] ) \n        if grid_mean_trait_value > self . overall_mean_trait_value : \n            self . feature_map [ feature_instance ] = 1 \n        elif grid_mean_trait_value == self . overall_mean_trait_value : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = 0 \n    self . feature_map = dict ( self . feature_map ) \n    self . mdr_matrix_values = dict ( self . mdr_matrix_values ) \n    return self "}
{"8848": "\ndef transform ( self , features ) : \n    new_feature = np . zeros ( features . shape [ 0 ] , dtype = np . int ) \n    row_i = 0 \n    while row_i < features . shape [ 0 ] : \n        feature_instance = tuple ( features [ row_i ] ) \n        if feature_instance in self . feature_map : \n            new_feature [ row_i ] = self . feature_map [ feature_instance ] \n        else : \n            new_feature [ row_i ] = self . default_label \n        row_i = row_i + 1 \n    return new_feature . reshape ( features . shape [ 0 ] , 1 ) "}
{"9071": "\ndef incfile ( fname , fpointer , lrange = \"1,6-\" , sdir = None ) : \n    file_dir = ( sdir if sdir else os . environ . get ( \"TRACER_DIR\" , os . path . abspath ( os . path . dirname ( __file__ ) ) ) ) \n    fname = os . path . join ( file_dir , fname ) \n    with open ( fname ) as fobj : \n        lines = fobj . readlines ( ) \n    tokens = [ item . strip ( ) for item in lrange . split ( \",\" ) ] \n    inc_lines = [ ] \n    for token in tokens : \n        if \"-\" in token : \n            subtokens = token . split ( \"-\" ) \n            lmin , lmax = ( int ( subtokens [ 0 ] ) , int ( subtokens [ 1 ] ) if subtokens [ 1 ] else len ( lines ) , ) \n            num = lmin \n            while num < lmax + 1 : \n                inc_lines . append ( num ) \n                num = num + 1 \n        else : \n            inc_lines . append ( int ( token ) ) \n    fpointer ( \".. code-block:: python\\n\" ) \n    fpointer ( \"\\n\" ) \n    for num , line in enumerate ( lines ) : \n        if num + 1 in inc_lines : \n            fpointer ( \"    \" + line . replace ( \"\\t\" , \"    \" ) if line . strip ( ) else \"\\n\" ) \n    fpointer ( \"\\n\" ) "}
{"9086": "\ndef stats ( self , filter , limit , start = None ) : \n    if filter == 'random' : \n        filter = 'rand' \n    valid_filters = ( 'top' , 'bottom' , 'rand' , 'last' ) \n    if filter not in valid_filters : \n        msg = 'filter must be one of {}' . format ( ', ' . join ( valid_filters ) ) \n        raise ValueError ( msg ) \n    data = dict ( action = 'stats' , filter = filter , limit = limit , start = start ) \n    jsondata = self . _api_request ( params = data ) \n    stats = DBStats ( total_clicks = int ( jsondata [ 'stats' ] [ 'total_clicks' ] ) , total_links = int ( jsondata [ 'stats' ] [ 'total_links' ] ) ) \n    links = [ ] \n    if 'links' in jsondata : \n        i = 1 \n        while i < limit + 1 : \n            key = 'link_{}' . format ( i ) \n            links . append ( _json_to_shortened_url ( jsondata [ 'links' ] [ key ] ) ) \n            i = i + 1 \n    return links , stats "}
{"9106": "\ndef _chunk_pars ( freq_vector , data_matrix , pformat ) : \n    pformat = pformat . upper ( ) \n    length = 4 \n    for freq , data in zip ( freq_vector , data_matrix ) : \n        data = data . flatten ( ) \n        index = 0 \n        while index < data . size : \n            fpoint = [ freq ] if not index else [ None ] \n            cdata = data [ index : index + length ] \n            if pformat == \"MA\" : \n                vector1 = np . abs ( cdata ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            elif pformat == \"RI\" : \n                vector1 = np . real ( cdata ) \n                vector2 = np . imag ( cdata ) \n            else : \n                vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            sep_data = np . array ( [ ] ) \n            for item1 , item2 in zip ( vector1 , vector2 ) : \n                sep_data = np . concatenate ( ( sep_data , np . array ( [ item1 , item2 ] ) ) ) \n            ret = np . concatenate ( ( np . array ( fpoint ) , sep_data ) ) \n            yield ret \n            index = index + length "}
{"9169": "\ndef format_hexdump ( arg ) : \n    line = '' \n    i = 0 \n    while i < len ( arg ) : \n        if i > 0 : \n            line += '\\n' \n        chunk = arg [ i : i + 16 ] \n        hex_chunk = hexlify ( chunk ) . decode ( 'utf-8' ) \n        hex_line = ' ' . join ( hex_chunk [ j : j + 2 ] for j in range ( 0 , len ( hex_chunk ) , 2 ) ) \n        if len ( hex_line ) < ( 3 * 16 ) - 1 : \n            hex_line += ' ' * ( ( ( 3 * 16 ) - 1 ) - len ( hex_line ) ) \n        ascii_line = '' . join ( _convert_to_ascii ( x ) for x in chunk ) \n        offset_line = '%08x' % i \n        line += \"%s  %s  %s\" % ( offset_line , hex_line , ascii_line ) \n        i = i + 16 \n    return line "}
{"9208": "\ndef signature ( self , name = None ) : \n    self . _ensure_loaded ( ) \n    if name is None : \n        name = self . name \n    num_args = len ( self . arg_names ) \n    num_def = 0 \n    if self . arg_defaults is not None : \n        num_def = len ( self . arg_defaults ) \n    num_no_def = num_args - num_def \n    args = [ ] \n    i = 0 \n    while i < len ( self . arg_names ) : \n        typestr = \"\" \n        if self . arg_names [ i ] in self . annotated_params : \n            typestr = \"{} \" . format ( self . annotated_params [ self . arg_names [ i ] ] . type_name ) \n        if i >= num_no_def : \n            default = str ( self . arg_defaults [ i - num_no_def ] ) \n            if len ( default ) == 0 : \n                default = \"''\" \n            args . append ( \"{}{}={}\" . format ( typestr , str ( self . arg_names [ i ] ) , default ) ) \n        else : \n            args . append ( typestr + str ( self . arg_names [ i ] ) ) \n        i = i + 1 \n    return \"{}({})\" . format ( name , \", \" . join ( args ) ) "}
{"9211": "\ndef check_spec ( self , pos_args , kwargs = None ) : \n    if kwargs is None : \n        kwargs = { } \n    if self . varargs is not None or self . kwargs is not None : \n        raise InternalError ( \"check_spec cannot be called on a function that takes *args or **kwargs\" ) \n    missing = object ( ) \n    arg_vals = [ missing ] * len ( self . arg_names ) \n    kw_indices = { name : i for i , name in enumerate ( self . arg_names ) } \n    for i , arg in enumerate ( pos_args ) : \n        if i >= len ( arg_vals ) : \n            raise ArgumentError ( \"Too many positional arguments, first excessive argument=%s\" % str ( arg ) ) \n        arg_vals [ i ] = arg \n    for arg , val in kwargs . items ( ) : \n        index = kw_indices . get ( arg ) \n        if index is None : \n            raise ArgumentError ( \"Cannot find argument by name: %s\" % arg ) \n        if arg_vals [ index ] is not missing : \n            raise ValidationError ( \"Argument %s passed twice\" % arg ) \n        arg_vals [ index ] = val \n    if len ( self . arg_defaults ) > 0 : \n        i = 0 \n        while i < len ( self . arg_defaults ) : \n            neg_index = - len ( self . arg_defaults ) + i \n            if arg_vals [ neg_index ] is missing : \n                arg_vals [ neg_index ] = self . arg_defaults [ i ] \n            i = i + 1 \n    if missing in arg_vals : \n        index = arg_vals . index ( missing ) \n        raise ArgumentError ( \"Missing a required argument (position: %d, name: %s)\" % ( index , self . arg_names [ index ] ) ) \n    return { name : val for name , val in zip ( self . arg_names , arg_vals ) } "}
{"9359": "\ndef clear ( self ) : \n    i = len ( self ) - 1 \n    while i < - 1 : \n        del self [ i ] \n        i = i + - 1 \n    self . _key = 0 \n    if hasattr ( self . _grid_view , \"wx_obj\" ) : \n        self . _grid_view . wx_obj . ClearGrid ( ) "}
{"9366": "\ndef Enable ( self , value ) : \n    i = 0 \n    while i < self . GetMenuItemCount ( ) : \n        it = self . FindItemByPosition ( i ) \n        it . Enable ( value ) \n        i = i + 1 "}
{"9367": "\ndef IsEnabled ( self , * args , ** kwargs ) : \n    i = 0 \n    while i < self . GetMenuItemCount ( ) : \n        it = self . FindItemByPosition ( i ) \n        if not it . IsEnabled ( ) : \n            return False \n        i = i + 1 \n    return True "}
{"9368": "\ndef Enable ( self , value ) : \n    i = 0 \n    while i < self . GetMenuCount ( ) : \n        self . EnableTop ( i , value ) \n        i = i + 1 "}
{"9369": "\ndef IsEnabled ( self , * args , ** kwargs ) : \n    i = 0 \n    while i < self . GetMenuCount ( ) : \n        if not self . IsEnabledTop ( i ) : \n            return False \n        i = i + 1 \n    return True "}
{"9555": "\ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        print ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if sys . version_info < ( 3 , ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    i = 1 \n    while i < 6 : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n        i = i + 1 \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9559": "\ndef batches ( arrays , steps = 100 , batch_size = 64 , rng = None ) : \n    assert batch_size >= 2 , 'batch_size must be at least 2!' \n    assert isinstance ( arrays , ( tuple , list ) ) , 'arrays must be a tuple or list!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    def sample ( ) : \n        xs = [ np . zeros ( ( batch_size , steps , a . shape [ 1 ] ) , a . dtype ) for a in arrays ] \n        i = 0 \n        while i < batch_size : \n            j = rng . randint ( len ( arrays [ 0 ] ) - steps ) \n            for x , a in zip ( xs , arrays ) : \n                x [ i ] = a [ j : j + steps ] \n            i = i + 1 \n        return xs \n    return sample "}
{"9561": "\ndef classifier_batches ( self , steps , batch_size , rng = None ) : \n    assert batch_size >= 2 , 'batch_size must be at least 2!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    T = np . arange ( steps ) \n    def batch ( ) : \n        inputs = np . zeros ( ( batch_size , steps , 1 + len ( self . alpha ) ) , 'f' ) \n        outputs = np . zeros ( ( batch_size , steps ) , 'i' ) \n        b = 0 \n        while b < batch_size : \n            offset = rng . randint ( len ( self . text ) - steps - 1 ) \n            enc = self . encode ( self . text [ offset : offset + steps + 1 ] ) \n            inputs [ b , T , enc [ : - 1 ] ] = 1 \n            outputs [ b , T ] = enc [ 1 : ] \n            b = b + 1 \n        return [ inputs , outputs ] \n    return batch "}
{"9562": "\ndef predict_sequence ( self , labels , steps , streams = 1 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    offset = len ( labels ) \n    batch = max ( 2 , streams ) \n    inputs = np . zeros ( ( batch , offset + steps , self . layers [ 0 ] . output_size ) , 'f' ) \n    inputs [ : , np . arange ( offset ) , labels ] = 1 \n    i = offset \n    while i < offset + steps : \n        chars = [ ] \n        for pdf in self . predict_proba ( inputs [ : i ] ) [ : , - 1 ] : \n            try : \n                c = rng . multinomial ( 1 , pdf ) . argmax ( axis = - 1 ) \n            except ValueError : \n                c = pdf . argmax ( axis = - 1 ) \n            chars . append ( int ( c ) ) \n        inputs [ np . arange ( batch ) , i , chars ] = 1 \n        yield chars [ 0 ] if streams == 1 else chars \n        i = i + 1 "}
{"9563": "\ndef add_conv_weights ( self , name , mean = 0 , std = None , sparsity = 0 ) : \n    nin = self . input_size \n    nout = self . output_size \n    mean = self . kwargs . get ( 'mean_{}' . format ( name ) , self . kwargs . get ( 'mean' , mean ) ) \n    std = self . kwargs . get ( 'std_{}' . format ( name ) , self . kwargs . get ( 'std' , std or 1 / np . sqrt ( nin + nout ) ) ) \n    sparsity = self . kwargs . get ( 'sparsity_{}' . format ( name ) , self . kwargs . get ( 'sparsity' , sparsity ) ) \n    arr = np . zeros ( ( nout , nin ) + self . filter_size , util . FLOAT ) \n    r = 0 \n    while r < self . filter_size [ 0 ] : \n        c = 0 \n        while c < self . filter_size [ 1 ] : \n            arr [ : , : , r , c ] = util . random_matrix ( nout , nin , mean , std , sparsity = sparsity , rng = self . rng ) \n            c = c + 1 \n        r = r + 1 \n    self . _params . append ( theano . shared ( arr , name = self . _fmt ( name ) ) ) "}
{"9658": "\ndef debug_storage ( storage , base_info = False , chars = True , runs = False ) : \n    import codecs \n    import locale \n    import sys \n    if six . PY2 : \n        stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) \n    else : \n        stderr = sys . stderr \n    caller = inspect . stack ( ) [ 1 ] [ 3 ] \n    stderr . write ( 'in %s\\n' % caller ) \n    if base_info : \n        stderr . write ( u'  base level  : %d\\n' % storage [ 'base_level' ] ) \n        stderr . write ( u'  base dir    : %s\\n' % storage [ 'base_dir' ] ) \n    if runs : \n        stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) \n    if chars : \n        output = u'  Chars       : ' \n        for _ch in storage [ 'chars' ] : \n            if _ch != '\\n' : \n                output += _ch [ 'ch' ] \n            else : \n                output += 'C' \n        stderr . write ( output + u'\\n' ) \n        output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) \n        stderr . write ( output ) \n        _types = [ _ch [ 'type' ] . ljust ( 3 ) for _ch in storage [ 'chars' ] ] \n        i = 0 \n        while i < 3 : \n            if i : \n                output = u'                %s\\n' \n            else : \n                output = u'  Res. types  : %s\\n' \n            stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) ) \n            i = i + 1 "}
{"9663": "\ndef resolve_weak_types ( storage , debug = False ) : \n    for run in storage [ 'runs' ] : \n        prev_strong = prev_type = run [ 'sor' ] \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = storage [ 'chars' ] [ start : start + length ] \n        for _ch in chars : \n            bidi_type = _ch [ 'type' ] \n            if bidi_type == 'NSM' : \n                _ch [ 'type' ] = bidi_type = prev_type \n            if bidi_type == 'EN' and prev_strong == 'AL' : \n                _ch [ 'type' ] = 'AN' \n            if bidi_type in ( 'R' , 'L' , 'AL' ) : \n                prev_strong = bidi_type \n            prev_type = _ch [ 'type' ] \n        for _ch in chars : \n            if _ch [ 'type' ] == 'AL' : \n                _ch [ 'type' ] = 'R' \n        idx = 1 \n        while idx < len ( chars ) - 1 : \n            bidi_type = chars [ idx ] [ 'type' ] \n            prev_type = chars [ idx - 1 ] [ 'type' ] \n            next_type = chars [ idx + 1 ] [ 'type' ] \n            if bidi_type == 'ES' and ( prev_type == next_type == 'EN' ) : \n                chars [ idx ] [ 'type' ] = 'EN' \n            if bidi_type == 'CS' and prev_type == next_type and prev_type in ( 'AN' , 'EN' ) : \n                chars [ idx ] [ 'type' ] = prev_type \n            idx = idx + 1 \n        idx = 0 \n        while idx < len ( chars ) : \n            if chars [ idx ] [ 'type' ] == 'EN' : \n                et_idx = idx - 1 \n                while et_idx < - 1 : \n                    if chars [ et_idx ] [ 'type' ] == 'ET' : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n                    et_idx = et_idx + - 1 \n                et_idx = idx + 1 \n                while et_idx < len ( chars ) : \n                    if chars [ et_idx ] [ 'type' ] == 'ET' : \n                        chars [ et_idx ] [ 'type' ] = 'EN' \n                    else : \n                        break \n                    et_idx = et_idx + 1 \n            idx = idx + 1 \n        for _ch in chars : \n            if _ch [ 'type' ] in ( 'ET' , 'ES' , 'CS' ) : \n                _ch [ 'type' ] = 'ON' \n        prev_strong = run [ 'sor' ] \n        for _ch in chars : \n            if _ch [ 'type' ] == 'EN' and prev_strong == 'L' : \n                _ch [ 'type' ] = 'L' \n            if _ch [ 'type' ] in ( 'L' , 'R' ) : \n                prev_strong = _ch [ 'type' ] \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9664": "\ndef resolve_neutral_types ( storage , debug ) : \n    for run in storage [ 'runs' ] : \n        start , length = run [ 'start' ] , run [ 'length' ] \n        chars = [ { 'type' : run [ 'sor' ] } ] + storage [ 'chars' ] [ start : start + length ] + [ { 'type' : run [ 'eor' ] } ] \n        total_chars = len ( chars ) \n        seq_start = None \n        idx = 0 \n        while idx < total_chars : \n            _ch = chars [ idx ] \n            if _ch [ 'type' ] in ( 'B' , 'S' , 'WS' , 'ON' ) : \n                if seq_start is None : \n                    seq_start = idx \n                    prev_bidi_type = chars [ idx - 1 ] [ 'type' ] \n            else : \n                if seq_start is not None : \n                    next_bidi_type = chars [ idx ] [ 'type' ] \n                    if prev_bidi_type in ( 'AN' , 'EN' ) : \n                        prev_bidi_type = 'R' \n                    if next_bidi_type in ( 'AN' , 'EN' ) : \n                        next_bidi_type = 'R' \n                    seq_idx = seq_start \n                    while seq_idx < idx : \n                        if prev_bidi_type == next_bidi_type : \n                            chars [ seq_idx ] [ 'type' ] = prev_bidi_type \n                        else : \n                            chars [ seq_idx ] [ 'type' ] = _embedding_direction ( chars [ seq_idx ] [ 'level' ] ) \n                        seq_idx = seq_idx + 1 \n                    seq_start = None \n            idx = idx + 1 \n    if debug : \n        debug_storage ( storage ) "}
{"9665": "\ndef reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) : \n    level = highest_level \n    while level < lowest_odd_level - 1 : \n        _start = _end = None \n        run_idx = line_start \n        while run_idx < line_end + 1 : \n            run_ch = chars [ run_idx ] \n            if run_ch [ 'level' ] >= level : \n                if _start is None : \n                    _start = _end = run_idx \n                else : \n                    _end = run_idx \n            else : \n                if _end : \n                    chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) \n                    _start = _end = None \n            run_idx = run_idx + 1 \n        if _start is not None : \n            chars [ _start : + _end + 1 ] = reversed ( chars [ _start : + _end + 1 ] ) \n        level = level + - 1 "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    idx = 0 \n    while idx < max_len : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if char_level > highest_level : \n            highest_level = char_level \n        if char_level % 2 and char_level < lowest_odd_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end -= 1 \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n        idx = idx + 1 \n    if debug : \n        debug_storage ( storage ) "}
{"9699": "\ndef draw_x_guidelines ( self , label_height , count ) : \n    if not self . show_x_guidelines : \n        return \n    count = 1 \n    while count < count : \n        move = 'M {start} 0 v{stop}' . format ( start = label_height * count , stop = self . graph_height , ) \n        path = { 'd' : move , 'class' : 'guideLines' } \n        etree . SubElement ( self . graph , 'path' , path ) \n        count = count + 1 "}
{"9700": "\ndef draw_y_guidelines ( self , label_height , count ) : \n    if not self . show_y_guidelines : \n        return \n    count = 1 \n    while count < count : \n        move = 'M 0 {start} h{stop}' . format ( start = self . graph_height - label_height * count , stop = self . graph_width , ) \n        path = { 'd' : move , 'class' : 'guideLines' } \n        etree . SubElement ( self . graph , 'path' , path ) \n        count = count + 1 "}
{"9807": "\ndef jsonify_good_event ( event , known_fields = ENRICHED_EVENT_FIELD_TYPES , add_geolocation_data = True ) : \n    if len ( event ) != len ( known_fields ) : \n        raise SnowplowEventTransformationException ( [ \"Expected {} fields, received {} fields.\" . format ( len ( known_fields ) , len ( event ) ) ] ) \n    else : \n        output = { } \n        errors = [ ] \n        if add_geolocation_data and event [ LATITUDE_INDEX ] != '' and event [ LONGITUDE_INDEX ] != '' : \n            output [ 'geo_location' ] = event [ LATITUDE_INDEX ] + ',' + event [ LONGITUDE_INDEX ] \n        i = 0 \n        while i < len ( event ) : \n            key = known_fields [ i ] [ 0 ] \n            if event [ i ] != '' : \n                try : \n                    kvpairs = known_fields [ i ] [ 1 ] ( key , event [ i ] ) \n                    for kvpair in kvpairs : \n                        output [ kvpair [ 0 ] ] = kvpair [ 1 ] \n                except SnowplowEventTransformationException as sete : \n                    errors += sete . error_messages \n                except Exception as e : \n                    errors += [ \"Unexpected exception parsing field with key {} and value {}: {}\" . format ( known_fields [ i ] [ 0 ] , event [ i ] , repr ( e ) ) ] \n            i = i + 1 \n        if errors : \n            raise SnowplowEventTransformationException ( errors ) \n        else : \n            return output "}
{"9895": "\ndef mark_begin_end_regex ( regex , text , split_locations ) : \n    for match in regex . finditer ( text ) : \n        end_match = match . end ( ) \n        begin_match = match . start ( ) \n        i = begin_match + 1 \n        while i < end_match : \n            split_locations [ i ] = SHOULD_NOT_SPLIT \n            i = i + 1 \n        if end_match < len ( split_locations ) : \n            if split_locations [ end_match ] == UNDECIDED : \n                split_locations [ end_match ] = SHOULD_SPLIT \n        if split_locations [ begin_match ] == UNDECIDED : \n            split_locations [ begin_match ] = SHOULD_SPLIT "}
{"9961": "\ndef mkrngs ( self ) : \n    bbool = bool_2_indices ( self . bkg ) \n    if bbool is not None : \n        self . bkgrng = self . Time [ bbool ] \n    else : \n        self . bkgrng = [ [ np . nan , np . nan ] ] \n    sbool = bool_2_indices ( self . sig ) \n    if sbool is not None : \n        self . sigrng = self . Time [ sbool ] \n    else : \n        self . sigrng = [ [ np . nan , np . nan ] ] \n    tbool = bool_2_indices ( self . trn ) \n    if tbool is not None : \n        self . trnrng = self . Time [ tbool ] \n    else : \n        self . trnrng = [ [ np . nan , np . nan ] ] \n    self . ns = np . zeros ( self . Time . size ) \n    n = 1 \n    i = 0 \n    while i < len ( self . sig ) - 1 : \n        if self . sig [ i ] : \n            self . ns [ i ] = n \n        if self . sig [ i ] and ~ self . sig [ i + 1 ] : \n            n += 1 \n        i = i + 1 \n    self . n = int ( max ( self . ns ) ) \n    return "}
{"10012": "\ndef by_regex ( file , outdir = None , split_pattern = None , global_header_rows = 0 , fname_pattern = None , trim_tail_lines = 0 , trim_head_lines = 0 ) : \n    if outdir is None : \n        outdir = os . path . join ( os . path . dirname ( file ) , 'split' ) \n    if not os . path . exists ( outdir ) : \n        os . mkdir ( outdir ) \n    with open ( file , 'r' ) as f : \n        lines = f . readlines ( ) \n    extension = os . path . splitext ( file ) [ - 1 ] \n    global_header = lines [ : global_header_rows ] \n    starts = [ ] \n    for i , line in enumerate ( lines ) : \n        if re . search ( split_pattern , line ) : \n            starts . append ( i ) \n    starts . append ( len ( lines ) ) \n    splits = { } \n    i = 0 \n    while i < len ( starts ) - 1 : \n        m = re . search ( fname_pattern , lines [ starts [ i ] ] ) \n        if m : \n            fname = m . groups ( ) [ 0 ] . strip ( ) \n        else : \n            fname = 'no_name_{:}' . format ( i ) \n        splits [ fname ] = global_header + lines [ starts [ i ] : starts [ i + 1 ] ] [ trim_head_lines : trim_tail_lines ] \n        i = i + 1 \n    print ( 'Writing files to: {:}' . format ( outdir ) ) \n    for k , v in splits . items ( ) : \n        fname = ( k + extension ) . replace ( ' ' , '_' ) \n        with open ( os . path . join ( outdir , fname ) , 'w' ) as f : \n            f . writelines ( v ) \n        print ( '  {:}' . format ( fname ) ) \n    print ( 'Done.' ) \n    return outdir "}
{"10091": "\ndef get_all ( self , endpoint , params = None ) : \n    if not params : \n        params = { 'max_results' : BACKEND_PAGINATION_LIMIT } \n    elif params and 'max_results' not in params : \n        params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT \n    last_page = False \n    items = [ ] \n    if self . processes == 1 : \n        while not last_page : \n            resp = self . get ( endpoint = endpoint , params = params ) \n            if 'next' in resp [ '_links' ] : \n                params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 \n                params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) \n            else : \n                last_page = True \n            items . extend ( resp [ '_items' ] ) \n    else : \n        def get_pages ( endpoint , params , pages , out_q ) : \n            multi_items = [ ] \n            for page in pages : \n                params [ 'page' ] = page \n                resp = self . get ( endpoint , params ) \n                multi_items . extend ( resp [ '_items' ] ) \n            out_q . put ( multi_items ) \n        resp = self . get ( endpoint , params ) \n        number_pages = int ( math . ceil ( float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) \n        out_q = multiprocessing . Queue ( ) \n        chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) \n        procs = [ ] \n        i = 0 \n        while i < self . processes : \n            begin = i * chunksize \n            end = begin + chunksize \n            if end > number_pages : \n                end = number_pages \n            begin += 1 \n            end += 1 \n            p = multiprocessing . Process ( target = get_pages , args = ( endpoint , params , range ( begin , end ) , out_q ) ) \n            procs . append ( p ) \n            p . start ( ) \n            i = i + 1 \n        i = 0 \n        while i < self . processes : \n            items . extend ( out_q . get ( ) ) \n            i = i + 1 \n        for p in procs : \n            p . join ( ) \n    return { '_items' : items , '_status' : 'OK' } "}
{"10108": "\ndef tail ( self , lines = 10 ) : \n    self . file . seek ( 0 , SEEK_END ) \n    i = 0 \n    while i < lines : \n        if self . seek_previous_line ( ) == - 1 : \n            break \n        i = i + 1 \n    data = self . file . read ( ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10109": "\ndef head ( self , lines = 10 ) : \n    self . file . seek ( 0 ) \n    i = 0 \n    while i < lines : \n        if self . seek_next_line ( ) == - 1 : \n            break \n        i = i + 1 \n    end_pos = self . file . tell ( ) \n    self . file . seek ( 0 ) \n    data = self . file . read ( end_pos ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            i = 0 \n            while i < 5 : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if i > 0 and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n                i = i + 1 \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    print ( ) \n    return session_id "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                fill_len = 40 - len ( self . m_lcd_items ) \n                for lcdid in self . m_lcd_items : \n                    append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) \n                    req_table += append_val \n                i = 0 \n                while i < fill_len : \n                    append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2 ) ) \n                    req_table += append_val \n                    i = i + 1 \n                req_str = \"015731023030443228\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10365": "\ndef partition_items ( count , bin_size ) : \n    num_bins = int ( math . ceil ( count / float ( bin_size ) ) ) \n    bins = [ 0 ] * num_bins \n    i = 0 \n    while i < count : \n        bins [ i % num_bins ] += 1 \n        i = i + 1 \n    return bins "}
{"10543": "\ndef tshift ( self , t ) : \n    raw = self . raw ( ) \n    i = 0 \n    while i < len ( raw ) : \n        raw [ i ] [ \"t\" ] += t \n        i = i + 1 \n    return self "}
{"10544": "\ndef sum ( self ) : \n    raw = self . raw ( ) \n    s = 0 \n    i = 0 \n    while i < len ( raw ) : \n        s += raw [ i ] [ \"d\" ] \n        i = i + 1 \n    return s "}
{"10661": "\ndef next_generation ( self , mut_rate = 0 , max_mut_amt = 0 , log_base = 10 ) : \n    if self . __num_processes > 1 : \n        process_pool = Pool ( processes = self . __num_processes ) \n        members = [ m . get ( ) for m in self . __members ] \n    else : \n        members = self . __members \n    if len ( members ) == 0 : \n        raise Exception ( 'Generation 0 not found: use generate_population() first' ) \n    selected_members = self . __select_fn ( members ) \n    reproduction_probs = list ( reversed ( logspace ( 0.0 , 1.0 , num = len ( selected_members ) , base = log_base ) ) ) \n    reproduction_probs = reproduction_probs / sum ( reproduction_probs ) \n    self . __members = [ ] \n    _ = 0 \n    while _ < self . __pop_size : \n        parent_1 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        parent_2 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        feed_dict = { } \n        for param in self . __parameters : \n            which_parent = uniform ( 0 , 1 ) \n            if which_parent < 0.5 : \n                feed_dict [ param . name ] = parent_1 . parameters [ param . name ] \n            else : \n                feed_dict [ param . name ] = parent_2 . parameters [ param . name ] \n            feed_dict [ param . name ] = self . __mutate_parameter ( feed_dict [ param . name ] , param , mut_rate , max_mut_amt ) \n        if self . __num_processes > 1 : \n            self . __members . append ( process_pool . apply_async ( self . _start_process , [ self . __cost_fn , feed_dict , self . __cost_fn_args ] ) ) \n        else : \n            self . __members . append ( Member ( feed_dict , self . __cost_fn ( feed_dict , self . __cost_fn_args ) ) ) \n        _ = _ + 1 \n    if self . __num_processes > 1 : \n        process_pool . close ( ) \n        process_pool . join ( ) \n    self . __determine_best_member ( ) "}
{"11088": "\ndef luhn_check ( card_number ) : \n    sum = 0 \n    num_digits = len ( card_number ) \n    oddeven = num_digits & 1 \n    count = 0 \n    while count < num_digits : \n        digit = int ( card_number [ count ] ) \n        if not ( ( count & 1 ) ^ oddeven ) : \n            digit *= 2 \n        if digit > 9 : \n            digit -= 9 \n        sum += digit \n        count = count + 1 \n    return ( sum % 10 ) == 0 "}
{"11105": "\ndef start_workers ( self , workers_per_task = 1 ) : \n    if not self . workers : \n        _ = 0 \n        while _ < workers_per_task : \n            self . workers . append ( Worker ( self . _download , self . queues [ 'download' ] , self . queues [ 'convert' ] , self . stopper ) ) \n            self . workers . append ( Worker ( self . _convert , self . queues [ 'convert' ] , self . queues [ 'upload' ] , self . stopper ) ) \n            self . workers . append ( Worker ( self . _upload , self . queues [ 'upload' ] , self . queues [ 'delete' ] , self . stopper ) ) \n            self . workers . append ( Worker ( self . _delete , self . queues [ 'delete' ] , self . queues [ 'done' ] , self . stopper ) ) \n            _ = _ + 1 \n        self . signal_handler = SignalHandler ( self . workers , self . stopper ) \n        signal . signal ( signal . SIGINT , self . signal_handler ) \n        for worker in self . workers : \n            worker . start ( ) "}
{"11163": "\ndef read_pr_report ( self , filename ) : \n    done = False \n    f = open ( filename ) \n    while f : \n        line = f . readline ( ) \n        if not line : \n            done = True \n            break \n        if \"# Quad solid angle mean point theta table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            i_iter = 0 \n            while i_iter < len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 : \n                tmp . append ( f . readline ( ) ) \n                i_iter = i_iter + 1 \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_theta' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"# Quad solid angle mean point phi table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            i_iter = 0 \n            while i_iter < len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 : \n                tmp . append ( f . readline ( ) ) \n                i_iter = i_iter + 1 \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_phi' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"L_w band\" in line . strip ( ) : \n            i_iter = 0 \n            while i_iter < int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) : \n                tmp = [ ] \n                j_iter = 0 \n                while j_iter < len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 : \n                    tmp . append ( f . readline ( ) ) \n                    j_iter = j_iter + 1 \n                self . data_dictionary [ 'L_w_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n                i_iter = i_iter + 1 \n        if \"L_it band\" in line . strip ( ) : \n            i_iter = 0 \n            while i_iter < int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) : \n                tmp = [ ] \n                j_iter = 0 \n                while j_iter < len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2 : \n                    tmp . append ( f . readline ( ) ) \n                    j_iter = j_iter + 1 \n                self . data_dictionary [ 'L_it_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n                i_iter = i_iter + 1 \n    return self . data_dictionary "}
{"11214": "\ndef mk_privkeys ( num ) : \n    privkeys = [ ] \n    assert num <= num_colors \n    i = 0 \n    while i < num : \n        j = 0 \n        while True : \n            k = sha3 ( str ( j ) ) \n            a = privtoaddr ( k ) \n            an = big_endian_to_int ( a ) \n            if an % num_colors == i : \n                break \n            j += 1 \n        privkeys . append ( k ) \n        i = i + 1 \n    return privkeys "}
{"11261": "\ndef psd ( self , freq ) : \n    if not self . device . is_streaming : \n        raise RuntimeError ( 'Streaming is not initialized, you must run setup() first!' ) \n    logger . debug ( '  Frequency hop: {:.2f} Hz' . format ( freq ) ) \n    t_freq = time . time ( ) \n    if self . device . freq != freq : \n        if self . _reset_stream : \n            self . device . device . deactivateStream ( self . device . stream ) \n        self . device . freq = freq \n        if self . _reset_stream : \n            self . device . device . activateStream ( self . device . stream ) \n        if self . _tune_delay : \n            t_delay = time . time ( ) \n            while True : \n                self . device . read_stream ( ) \n                t_delay_end = time . time ( ) \n                if t_delay_end - t_delay >= self . _tune_delay : \n                    break \n            logger . debug ( '    Tune delay: {:.3f} s' . format ( t_delay_end - t_delay ) ) \n    else : \n        logger . debug ( '    Same frequency as before, tuning skipped' ) \n    psd_state = self . _psd . set_center_freq ( freq ) \n    t_freq_end = time . time ( ) \n    logger . debug ( '    Tune time: {:.3f} s' . format ( t_freq_end - t_freq ) ) \n    repeat = 0 \n    while repeat < self . _buffer_repeats : \n        logger . debug ( '    Repeat: {}' . format ( repeat + 1 ) ) \n        t_acq = time . time ( ) \n        acq_time_start = datetime . datetime . utcnow ( ) \n        self . device . read_stream_into_buffer ( self . _buffer ) \n        acq_time_stop = datetime . datetime . utcnow ( ) \n        t_acq_end = time . time ( ) \n        logger . debug ( '      Acquisition time: {:.3f} s' . format ( t_acq_end - t_acq ) ) \n        self . _psd . update_async ( psd_state , numpy . copy ( self . _buffer ) ) \n        t_final = time . time ( ) \n        if _shutdown : \n            break \n        repeat = repeat + 1 \n    psd_future = self . _psd . result_async ( psd_state ) \n    logger . debug ( '    Total hop time: {:.3f} s' . format ( t_final - t_freq ) ) \n    return ( psd_future , acq_time_start , acq_time_stop ) "}
{"11362": "\ndef removepad ( idx , value , number , padding , axes = None ) : \n    if axes is None : \n        axes = range ( len ( number ) ) \n    mask = len ( number ) * [ False , ] \n    i = 0 \n    while i < len ( mask ) : \n        if i in axes and padding [ i ] != 0 : \n            mask [ i ] = True \n        i = i + 1 \n    starts = [ 0 if ( i == 0 or not m ) else p for ( i , m , p ) in zip ( idx , mask , padding ) ] \n    stops = [ None if ( i == n - 1 or not m ) else - p for ( i , m , p , n ) in zip ( idx , mask , padding , number ) ] \n    slices = [ slice ( i1 , i2 ) for ( i1 , i2 ) in zip ( starts , stops ) ] \n    return value [ slices ] "}
{"11364": "\ndef getslices ( plan , padding , shape ) : \n    slices = [ ] \n    for size , pad , d in zip ( plan , padding , shape ) : \n        nchunks = int ( floor ( d / size ) ) \n        remainder = d % size \n        start = 0 \n        dimslices = [ ] \n        idx = 0 \n        while idx < nchunks : \n            end = start + size \n            if idx == 0 : \n                left = start \n            else : \n                left = start - pad \n            if idx == nchunks : \n                right = end \n            else : \n                right = end + pad \n            dimslices . append ( slice ( left , right , 1 ) ) \n            start = end \n            idx = idx + 1 \n        if remainder : \n            dimslices . append ( slice ( end - pad , d , 1 ) ) \n        slices . append ( dimslices ) \n    return slices "}
{"11382": "\ndef _reshapebasic ( self , shape ) : \n    new = tupleize ( shape ) \n    old_key_size = prod ( self . keys . shape ) \n    old_value_size = prod ( self . values . shape ) \n    i = 0 \n    while i < len ( new ) : \n        new_key_size = prod ( new [ : i ] ) \n        new_value_size = prod ( new [ i : ] ) \n        if new_key_size == old_key_size and new_value_size == old_value_size : \n            return i \n        i = i + 1 \n    return - 1 "}
{"11396": "\ndef iterexpand ( arry , extra ) : \n    d = arry . ndim \n    while d < arry . ndim + extra : \n        arry = expand_dims ( arry , axis = d ) \n        d = d + 1 \n    return arry "}
{"11397": "\ndef zip_with_index ( rdd ) : \n    starts = [ 0 ] \n    if rdd . getNumPartitions ( ) > 1 : \n        nums = rdd . mapPartitions ( lambda it : [ sum ( 1 for _ in it ) ] ) . collect ( ) \n        count = sum ( nums ) \n        i = 0 \n        while i < len ( nums ) - 1 : \n            starts . append ( starts [ - 1 ] + nums [ i ] ) \n            i = i + 1 \n    else : \n        count = rdd . count ( ) \n    def func ( k , it ) : \n        for i , v in enumerate ( it , starts [ k ] ) : \n            yield v , i \n    return count , rdd . mapPartitionsWithIndex ( func ) "}
{"11450": "\ndef iter_auth_hashes ( user , purpose , minutes_valid ) : \n    now = timezone . now ( ) . replace ( microsecond = 0 , second = 0 ) \n    minute = 0 \n    while minute < minutes_valid + 1 : \n        yield hashlib . sha1 ( '%s:%s:%s:%s:%s' % ( now - datetime . timedelta ( minutes = minute ) , user . password , purpose , user . pk , settings . SECRET_KEY , ) , ) . hexdigest ( ) \n        minute = minute + 1 "}
{"11530": "\ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    print ( \"Running fast forward selection algorithm\" ) \n    number_of_scenarios = scenarios . shape [ 1 ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ 1 / number_of_scenarios for i in range ( 0 , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( 0 , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( 0 , number_of_scenarios ) \n    if number_of_reduced_scenarios >= number_of_scenarios : \n        return ( scenarios , probability , J ) \n    scenario_k = 0 \n    while scenario_k < number_of_scenarios : \n        scenario_u = 0 \n        while scenario_u < number_of_scenarios : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n            scenario_u = scenario_u + 1 \n        scenario_k = scenario_k + 1 \n    scenario_u = 0 \n    while scenario_u < number_of_scenarios : \n        summation = 0 \n        scenario_k = 0 \n        while scenario_k < number_of_scenarios : \n            if scenario_k != scenario_u : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            scenario_k = scenario_k + 1 \n        z [ scenario_u ] = summation \n        scenario_u = scenario_u + 1 \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    _ = 0 \n    while _ < number_of_scenarios - number_of_reduced_scenarios - 1 : \n        print ( \"Running {}\" . format ( _ ) ) \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = 0 \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n        _ = _ + 1 \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if c [ deleted_scenario_number , scenario_j ] < lowest_value : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11576": "\ndef dispose ( json_str ) : \n    result_str = list ( json_str ) \n    escaped = False \n    normal = True \n    sl_comment = False \n    ml_comment = False \n    quoted = False \n    a_step_from_comment = False \n    a_step_from_comment_away = False \n    former_index = None \n    for index , char in enumerate ( json_str ) : \n        if escaped : \n            escaped = False \n            continue \n        if a_step_from_comment : \n            if char != '/' and char != '*' : \n                a_step_from_comment = False \n                normal = True \n                continue \n        if a_step_from_comment_away : \n            if char != '/' : \n                a_step_from_comment_away = False \n        if char == '\"' : \n            if normal and not escaped : \n                quoted = True \n                normal = False \n            elif quoted and not escaped : \n                quoted = False \n                normal = True \n        elif char == '\\\\' : \n            if normal or quoted : \n                escaped = True \n        elif char == '/' : \n            if a_step_from_comment : \n                a_step_from_comment = False \n                sl_comment = True \n                normal = False \n                former_index = index - 1 \n            elif a_step_from_comment_away : \n                a_step_from_comment_away = False \n                normal = True \n                ml_comment = False \n                i = former_index \n                while i < index + 1 : \n                    result_str [ i ] = \"\" \n                    i = i + 1 \n            elif normal : \n                a_step_from_comment = True \n                normal = False \n        elif char == '*' : \n            if a_step_from_comment : \n                a_step_from_comment = False \n                ml_comment = True \n                normal = False \n                former_index = index - 1 \n            elif ml_comment : \n                a_step_from_comment_away = True \n        elif char == '\\n' : \n            if sl_comment : \n                sl_comment = False \n                normal = True \n                i = former_index \n                while i < index + 1 : \n                    result_str [ i ] = \"\" \n                    i = i + 1 \n        elif char == ']' or char == '}' : \n            if normal : \n                _remove_last_comma ( result_str , index ) \n    return ( \"\" if isinstance ( json_str , str ) else u\"\" ) . join ( result_str ) "}
{"11846": "\ndef get_queue ( self , path , n_procs = 4 , read_ahead = None , cyclic = False , block_size = None , ordered = False ) : \n    example = self . __get_batch ( path , block_size ) \n    block_size = example . shape [ 0 ] \n    if read_ahead is None : \n        read_ahead = 2 * n_procs + 1 \n    cbuf = SharedCircBuf ( read_ahead , example ) \n    stop = multiprocessing . Event ( ) \n    barrier = Barrier ( n_procs ) \n    sync = GuardSynchronizer ( ) if ordered else None \n    procs = [ ] \n    i = 0 \n    while i < n_procs : \n        process = multiprocessing . Process ( target = _Streamer__read_process , args = ( self , path , block_size , cbuf , stop , barrier , cyclic , i * block_size , n_procs * block_size , sync ) ) \n        process . daemon = True \n        process . start ( ) \n        procs . append ( process ) \n        i = i + 1 \n    if not cyclic : \n        def monitor ( ) : \n            for p in procs : \n                p . join ( ) \n            cbuf . close ( ) \n        monitor_thread = threading . Thread ( target = monitor ) \n        monitor_thread . daemon = True \n        monitor_thread . start ( ) \n    return Streamer . Queue ( cbuf , stop , block_size ) "}
{"11851": "\ndef _get_objs ( self ) : \n    while True : \n        count = self . _read_varint ( ) \n        if count == 0 : \n            break \n        _ = 0 \n        while _ < count : \n            size = self . _read_varint ( ) \n            if size == 0 : \n                raise EOFError ( 'unexpected EOF.' ) \n            yield self . _fd . read ( size ) \n            _ = _ + 1 \n        if self . _group_delim : \n            yield self . _delimiter ( ) if self . _delimiter is not None else None "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    tries = 0 \n    while tries < 3 : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n        tries = tries + 1 \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"12010": "\ndef from_voxels ( voxels ) : \n    dimensions = len ( voxels [ 0 ] ) \n    d = 0 \n    while d < len ( dimensions ) : \n        size . append ( max ( [ i [ d ] for i in voxels ] ) ) \n        d = d + 1 \n    result = numpy . zeros ( dimensions ) \n    for v in voxels : \n        result [ v ] = 1 \n    return result "}
{"12310": "\ndef point_to_source ( source , position , fmt = ( 2 , True , \"~~~~~\" , \"^\" ) ) : \n    surrounding_lines , show_line_numbers , tail_body , pointer_char = fmt \n    line_no , char_no = position \n    lines = source . split ( \"\\n\" ) \n    line = lines [ line_no ] \n    if char_no >= len ( tail_body ) : \n        tail = \" \" * ( char_no - len ( tail_body ) ) + tail_body + pointer_char \n    else : \n        tail = \" \" * char_no + pointer_char + tail_body \n    if show_line_numbers : \n        line_no_width = int ( math . ceil ( math . log10 ( max ( 1 , line_no + surrounding_lines ) ) ) + 1 ) \n        line_fmt = \"{0:\" + str ( line_no_width ) + \"}: {1}\" \n    else : \n        line_fmt = \"{1}\" \n    pivot = line_no + 1 \n    output_lines = [ ( pivot , line ) , ( \"\" , tail ) ] \n    i = 0 \n    while i < surrounding_lines : \n        upper_ofst = i + 1 \n        upper_idx = line_no + upper_ofst \n        lower_ofst = - upper_ofst \n        lower_idx = line_no + lower_ofst \n        if lower_idx >= 0 : \n            output_lines . insert ( 0 , ( pivot + lower_ofst , lines [ lower_idx ] ) ) \n        if upper_idx < len ( lines ) : \n            output_lines . append ( ( pivot + upper_ofst , lines [ upper_idx ] ) ) \n        i = i + 1 \n    return \"\\n\" . join ( line_fmt . format ( n , c ) for n , c in output_lines ) "}
{"12664": "\ndef _make_conn ( shape ) : \n    shape = np . array ( shape ) \n    Ne = shape . prod ( ) \n    if len ( shape ) == 2 : \n        nx , ny = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 4 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) \n        j = 0 \n        while j < shape [ 1 ] : \n            i = 0 \n            while i < shape [ 0 ] : \n                conn [ counter ] = pattern + 1 + i + j * nx \n                counter += 1 \n                i = i + 1 \n            j = j + 1 \n    if len ( shape ) == 3 : \n        nx , ny , nz = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 8 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) \n        k = 0 \n        while k < shape [ 2 ] : \n            j = 0 \n            while j < shape [ 1 ] : \n                i = 0 \n                while i < shape [ 0 ] : \n                    conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny \n                    counter += 1 \n                    i = i + 1 \n                j = j + 1 \n            k = k + 1 \n    return conn "}
{"12681": "\ndef read_history_report ( path , steps , x_name = None ) : \n    data = pd . read_csv ( path , delim_whitespace = True ) \n    if x_name != None : \n        data [ x_name ] = data . X \n        del data [ \"X\" ] \n    data [ \"step\" ] = 0 \n    t = 0. \n    i = 0 \n    while i < len ( steps ) : \n        dt = steps [ i ] . duration \n        loc = data [ data . t == t ] . index \n        if len ( loc ) == 2 : \n            data . loc [ loc [ 1 ] : , \"step\" ] = i \n        t += dt \n        i = i + 1 \n    return data "}
{"12705": "\ndef domain_name_left_cuts ( domain ) : \n    cuts = [ ] \n    if domain : \n        parts = domain . split ( '.' ) \n        i = 0 \n        while i < len ( parts ) : \n            cuts . append ( '.' . join ( parts [ i : ] ) ) \n            i = i + 1 \n    return cuts "}
{"12784": "\ndef generate_john_smith_chunk ( path_to_original ) : \n    creation_time = '1998-12-31T23:59:59.999999Z' \n    correct_time = 915148799 \n    if not os . path . isabs ( path_to_original ) : \n        path_to_original = os . path . join ( os . getcwd ( ) , path_to_original ) \n    label_id = 0 \n    while label_id < 35 : \n        dir_path = os . path . join ( path_to_original , str ( label_id ) ) \n        fnames = os . listdir ( dir_path ) \n        fnames . sort ( ) \n        for fname in fnames : \n            stream_item = streamcorpus . make_stream_item ( creation_time , os . path . join ( 'john-smith-corpus' , str ( label_id ) , fname ) ) \n            if int ( stream_item . stream_time . epoch_ticks ) != correct_time : \n                raise PipelineBaseException ( 'wrong stream_time construction: %r-->%r != %r' % ( creation_time , stream_item . stream_time . epoch_ticks , correct_time ) ) \n            stream_item . source = 'bagga-and-baldwin' \n            body = streamcorpus . ContentItem ( ) \n            raw_string = open ( os . path . join ( dir_path , fname ) ) . read ( ) \n            body . clean_visible = unicode ( raw_string ) . encode ( 'utf8' ) \n            stream_item . body = body \n            stream_item . body . language = streamcorpus . Language ( code = 'en' , name = 'ENGLISH' ) \n            anno = streamcorpus . Annotator ( ) \n            anno . annotator_id = 'bagga-and-baldwin' \n            anno . annotation_time = stream_item . stream_time \n            rating = streamcorpus . Rating ( ) \n            rating . annotator = anno \n            rating . target = streamcorpus . Target ( target_id = str ( label_id ) ) \n            rating . contains_mention = True \n            rating . mentions = [ 'john' , 'smith' ] \n            streamcorpus . add_annotation ( stream_item , rating ) \n            yield stream_item \n        label_id = label_id + 1 "}
{"12933": "\ndef available_drivers ( ) : \n    drivers = { } \n    i = 0 \n    while i < gdal . GetDriverCount ( ) : \n        d = gdal . GetDriver ( i ) \n        drivers [ d . ShortName ] = d . GetMetadata ( ) \n        i = i + 1 \n    return drivers "}
{"12968": "\ndef run_worker_pool ( job_handler , host = \"localhost\" , port = 48484 , * , max_workers = None ) : \n    if max_workers is None : \n        max_workers = multiprocessing . cpu_count ( ) \n    processes = [ ] \n    _ = 0 \n    while _ < max_workers : \n        p = multiprocessing . Process ( target = worker_main , args = ( job_handler , host , port ) ) \n        p . start ( ) \n        processes . append ( p ) \n        _ = _ + 1 \n    logger . debug ( \"workers started\" ) \n    for p in processes : \n        p . join ( ) \n    logger . debug ( \"all workers completed\" ) "}
{"13050": "\ndef prepare_rows ( table ) : \n    num_columns = max ( len ( row ) for row in table ) \n    for row in table : \n        while len ( row ) < num_columns : \n            row . append ( '' ) \n        i = 0 \n        while i < num_columns : \n            row [ i ] = str ( row [ i ] ) if row [ i ] is not None else '' \n            i = i + 1 \n    return table "}
{"13122": "\ndef find_file ( path ) : \n    path_components = split_all ( path ) \n    def get_assemblies ( ) : \n        n = len ( path_components ) \n        while n < 0 : \n            file_c = path_components [ : n ] \n            part_c = path_components [ n : ] or [ '' ] \n            yield ( os . path . join ( * file_c ) , posixpath . join ( * part_c ) ) \n            n = n + - 1 \n    for file_path , part_path in get_assemblies ( ) : \n        if os . path . isfile ( file_path ) : \n            return file_path , part_path "}
{"13320": "\ndef init_nova_consumer ( self , mq ) : \n    if not self . enable_component_notification ( Openstack . Nova ) : \n        log . debug ( \"disable listening nova notification\" ) \n        return \n    i = 0 \n    while i < self . config . nova_mq_consumer_count : \n        mq . create_consumer ( self . config . nova_mq_exchange , self . config . nova_mq_queue , ProcessFactory . process ( Openstack . Nova ) ) \n        i = i + 1 \n    log . debug ( \"enable listening openstack nova notification.\" ) "}
{"13321": "\ndef init_cinder_consumer ( self , mq ) : \n    if not self . enable_component_notification ( Openstack . Cinder ) : \n        log . debug ( \"disable listening cinder notification\" ) \n        return \n    i = 0 \n    while i < self . config . cinder_mq_consumer_count : \n        mq . create_consumer ( self . config . cinder_mq_exchange , self . config . cinder_mq_queue , ProcessFactory . process ( Openstack . Cinder ) ) \n        i = i + 1 \n    log . debug ( \"enable listening openstack cinder notification.\" ) "}
{"13322": "\ndef init_neutron_consumer ( self , mq ) : \n    if not self . enable_component_notification ( Openstack . Neutron ) : \n        log . debug ( \"disable listening neutron notification\" ) \n        return \n    i = 0 \n    while i < self . config . neutron_mq_consumer_count : \n        mq . create_consumer ( self . config . neutron_mq_exchange , self . config . neutron_mq_queue , ProcessFactory . process ( Openstack . Neutron ) ) \n        i = i + 1 \n    log . debug ( \"enable listening openstack neutron notification.\" ) "}
{"13323": "\ndef init_glance_consumer ( self , mq ) : \n    if not self . enable_component_notification ( Openstack . Glance ) : \n        log . debug ( \"disable listening glance notification\" ) \n        return \n    i = 0 \n    while i < self . config . glance_mq_consumer_count : \n        mq . create_consumer ( self . config . glance_mq_exchange , self . config . glance_mq_queue , ProcessFactory . process ( Openstack . Glance ) ) \n        i = i + 1 \n    log . debug ( \"enable listening openstack glance notification.\" ) "}
{"13324": "\ndef init_heat_consumer ( self , mq ) : \n    if not self . enable_component_notification ( Openstack . Heat ) : \n        log . debug ( \"disable listening heat notification\" ) \n        return \n    i = 0 \n    while i < self . config . heat_mq_consumer_count : \n        mq . create_consumer ( self . config . heat_mq_exchange , self . config . heat_mq_queue , ProcessFactory . process ( Openstack . Heat ) ) \n        i = i + 1 \n    log . debug ( \"enable listening openstack heat notification.\" ) "}
{"13327": "\ndef download_music ( song , thread_num = 4 ) : \n    filename = \"{}.mp3\" . format ( song [ \"name\" ] ) \n    if os . path . exists ( filename ) : \n        os . remove ( filename ) \n    part = int ( song [ \"size\" ] / thread_num ) \n    if part <= 1024 : \n        thread_num = 1 \n    _id = uuid . uuid4 ( ) . hex \n    logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) \n    threads = [ ] \n    i = 0 \n    while i < thread_num : \n        if i == thread_num - 1 : \n            end = '' \n        else : \n            end = ( i + 1 ) * part - 1 \n        thread = Worker ( ( i * part , end ) , song , _id ) \n        thread . start ( ) \n        threads . append ( thread ) \n        i = i + 1 \n    for t in threads : \n        t . join ( ) \n    fileParts = glob . glob ( \"part-{}-*\" . format ( _id ) ) \n    fileParts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) \n    logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) \n    with open ( filename , \"ab\" ) as f : \n        for part in fileParts : \n            with open ( part , \"rb\" ) as d : \n                shutil . copyfileobj ( d , f ) \n            os . remove ( part ) \n    logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) ) "}
{"13552": "\ndef passwd ( passphrase = None , algorithm = 'sha1' ) : \n    if passphrase is None : \n        i = 0 \n        while i < 3 : \n            p0 = getpass . getpass ( 'Enter password: ' ) \n            p1 = getpass . getpass ( 'Verify password: ' ) \n            if p0 == p1 : \n                passphrase = p0 \n                break \n            else : \n                print ( 'Passwords do not match.' ) \n            i = i + 1 \n    h = hashlib . new ( algorithm ) \n    salt = ( '%0' + str ( salt_len ) + 'x' ) % random . getrandbits ( 4 * salt_len ) \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return ':' . join ( ( algorithm , salt , h . hexdigest ( ) ) ) "}
{"13560": "\ndef add_children ( G , parent , level , n = 2 ) : \n    if level == 0 : \n        return \n    i = 0 \n    while i < n : \n        child = parent + str ( i ) \n        G . add_node ( child ) \n        G . add_edge ( parent , child ) \n        add_children ( G , child , level - 1 , n ) \n        i = i + 1 "}
{"13613": "\ndef energy ( self , state = None ) : \n    state = self . state if state is None else state \n    route = state \n    e = 0 \n    if self . distance_matrix : \n        i = 0 \n        while i < len ( route ) : \n            e += self . distance_matrix [ \"{},{}\" . format ( route [ i - 1 ] , route [ i ] ) ] \n            i = i + 1 \n    else : \n        i = 0 \n        while i < len ( route ) : \n            e += distance ( self . cities [ route [ i - 1 ] ] , self . cities [ route [ i ] ] ) \n            i = i + 1 \n    return e "}
{"13748": "\ndef _find_optimal ( rlist , separator_size = 2 , displaywidth = 80 ) : \n    nrow = 1 \n    while nrow < len ( rlist ) + 1 : \n        chk = map ( max , _chunks ( rlist , nrow ) ) \n        sumlength = sum ( chk ) \n        ncols = len ( chk ) \n        if sumlength + separator_size * ( ncols - 1 ) <= displaywidth : \n            break ; \n        nrow = nrow + 1 \n    return { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows_numbers' : nrow , 'columns_width' : chk } "}
{"13855": "\ndef random_ports ( port , n ) : \n    i = 0 \n    while i < min ( 5 , n ) : \n        yield port + i \n        i = i + 1 \n    i = 0 \n    while i < n - 5 : \n        yield port + random . randint ( - 2 * n , 2 * n ) \n        i = i + 1 "}
{"13860": "\ndef price_options ( S = 100.0 , K = 100.0 , sigma = 0.25 , r = 0.05 , days = 260 , paths = 10000 ) : \n    import numpy as np \n    from math import exp , sqrt \n    h = 1.0 / days \n    const1 = exp ( ( r - 0.5 * sigma ** 2 ) * h ) \n    const2 = sigma * sqrt ( h ) \n    stock_price = S * np . ones ( paths , dtype = 'float64' ) \n    stock_price_sum = np . zeros ( paths , dtype = 'float64' ) \n    j = 0 \n    while j < days : \n        growth_factor = const1 * np . exp ( const2 * np . random . standard_normal ( paths ) ) \n        stock_price = stock_price * growth_factor \n        stock_price_sum = stock_price_sum + stock_price \n        j = j + 1 \n    stock_price_avg = stock_price_sum / days \n    zeros = np . zeros ( paths , dtype = 'float64' ) \n    r_factor = exp ( - r * h * days ) \n    euro_put = r_factor * np . mean ( np . maximum ( zeros , K - stock_price ) ) \n    asian_put = r_factor * np . mean ( np . maximum ( zeros , K - stock_price_avg ) ) \n    euro_call = r_factor * np . mean ( np . maximum ( zeros , stock_price - K ) ) \n    asian_call = r_factor * np . mean ( np . maximum ( zeros , stock_price_avg - K ) ) \n    return ( euro_call , euro_put , asian_call , asian_put ) "}
{"13879": "\ndef n_digit_freqs ( digits , n , normalize = False ) : \n    freqs = np . zeros ( pow ( 10 , n ) , dtype = 'i4' ) \n    current = np . zeros ( n , dtype = int ) \n    i = 0 \n    while i < n : \n        current [ i ] = digits . next ( ) \n        i = i + 1 \n    for d in digits : \n        index = int ( '' . join ( map ( str , current ) ) ) \n        freqs [ index ] += 1 \n        current [ 0 : - 1 ] = current [ 1 : ] \n        current [ - 1 ] = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13880": "\ndef plot_two_digit_freqs ( f2 ) : \n    f2_copy = f2 . copy ( ) \n    f2_copy . shape = ( 10 , 10 ) \n    ax = plt . matshow ( f2_copy ) \n    plt . colorbar ( ) \n    i = 0 \n    while i < 10 : \n        j = 0 \n        while j < 10 : \n            plt . text ( i - 0.2 , j + 0.2 , str ( j ) + str ( i ) ) \n            j = j + 1 \n        i = i + 1 \n    plt . ylabel ( 'First digit' ) \n    plt . xlabel ( 'Second digit' ) \n    return ax "}
{"13948": "\ndef notify_stop ( self , data ) : \n    self . log . debug ( 'Process %r stopped: %r' , self . args [ 0 ] , data ) \n    self . stop_data = data \n    self . state = 'after' \n    i = 0 \n    while i < len ( self . stop_callbacks ) : \n        d = self . stop_callbacks . pop ( ) \n        d ( data ) \n        i = i + 1 \n    return data "}
{"13952": "\ndef _send_file ( self , local , remote ) : \n    remote = \"%s:%s\" % ( self . location , remote ) \n    i = 0 \n    while i < 10 : \n        if not os . path . exists ( local ) : \n            self . log . debug ( \"waiting for %s\" % local ) \n            time . sleep ( 1 ) \n        else : \n            break \n        i = i + 1 \n    self . log . info ( \"sending %s to %s\" , local , remote ) \n    check_output ( self . scp_cmd + [ local , remote ] ) "}
{"13953": "\ndef _fetch_file ( self , remote , local ) : \n    full_remote = \"%s:%s\" % ( self . location , remote ) \n    self . log . info ( \"fetching %s from %s\" , local , full_remote ) \n    i = 0 \n    while i < 10 : \n        check = check_output ( self . ssh_cmd + self . ssh_args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) \n        check = check . strip ( ) \n        if check == 'no' : \n            time . sleep ( 1 ) \n        elif check == 'yes' : \n            break \n        i = i + 1 \n    check_output ( self . scp_cmd + [ full_remote , local ] ) "}
{"13955": "\ndef start ( self , n ) : \n    dlist = [ ] \n    for host , n in self . engines . iteritems ( ) : \n        if isinstance ( n , ( tuple , list ) ) : \n            n , args = n \n        else : \n            args = copy . deepcopy ( self . engine_args ) \n        if '@' in host : \n            user , host = host . split ( '@' , 1 ) \n        else : \n            user = None \n        i = 0 \n        while i < n : \n            if i > 0 : \n                time . sleep ( self . delay ) \n            el = self . launcher_class ( work_dir = self . work_dir , config = self . config , log = self . log , profile_dir = self . profile_dir , cluster_id = self . cluster_id , ) \n            if i > 0 : \n                el . to_send = [ ] \n            el . engine_cmd = self . engine_cmd \n            el . engine_args = args \n            el . on_stop ( self . _notice_engine_stopped ) \n            d = el . start ( user = user , hostname = host ) \n            self . launchers [ \"%s/%i\" % ( host , i ) ] = el \n            dlist . append ( d ) \n            i = i + 1 \n    self . notify_start ( dlist ) \n    return dlist "}
{"14107": "\ndef _replace_rlhist_multiline ( self , source_raw , hlen_before_cell ) : \n    if not self . has_readline or not self . multiline_history : \n        return hlen_before_cell \n    if not hasattr ( self . readline , \"remove_history_item\" ) : \n        return hlen_before_cell \n    if not source_raw . rstrip ( ) : \n        return hlen_before_cell \n    hlen = self . readline . get_current_history_length ( ) \n    if hlen == hlen_before_cell : \n        return hlen_before_cell \n    i = 0 \n    while i < hlen - hlen_before_cell : \n        self . readline . remove_history_item ( hlen - i - 1 ) \n        i = i + 1 \n    stdin_encoding = get_stream_enc ( sys . stdin , 'utf-8' ) \n    self . readline . add_history ( py3compat . unicode_to_str ( source_raw . rstrip ( ) , stdin_encoding ) ) \n    return self . readline . get_current_history_length ( ) "}
{"14117": "\ndef findsource ( object ) : \n    file = getsourcefile ( object ) or getfile ( object ) \n    globals_dict = None \n    if inspect . isframe ( object ) : \n        globals_dict = object . f_globals \n    else : \n        module = getmodule ( object , file ) \n        if module : \n            globals_dict = module . __dict__ \n    lines = linecache . getlines ( file , globals_dict ) \n    if not lines : \n        raise IOError ( 'could not get source code' ) \n    if ismodule ( object ) : \n        return lines , 0 \n    if isclass ( object ) : \n        name = object . __name__ \n        pat = re . compile ( r'^(\\s*)class\\s*' + name + r'\\b' ) \n        candidates = [ ] \n        i = 0 \n        while i < len ( lines ) : \n            match = pat . match ( lines [ i ] ) \n            if match : \n                if lines [ i ] [ 0 ] == 'c' : \n                    return lines , i \n                candidates . append ( ( match . group ( 1 ) , i ) ) \n            i = i + 1 \n        if candidates : \n            candidates . sort ( ) \n            return lines , candidates [ 0 ] [ 1 ] \n        else : \n            raise IOError ( 'could not find class definition' ) \n    if ismethod ( object ) : \n        object = object . im_func \n    if isfunction ( object ) : \n        object = object . func_code \n    if istraceback ( object ) : \n        object = object . tb_frame \n    if isframe ( object ) : \n        object = object . f_code \n    if iscode ( object ) : \n        if not hasattr ( object , 'co_firstlineno' ) : \n            raise IOError ( 'could not find function definition' ) \n        pat = re . compile ( r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) \n        pmatch = pat . match \n        lnum = min ( object . co_firstlineno , len ( lines ) ) - 1 \n        while lnum > 0 : \n            if pmatch ( lines [ lnum ] ) : \n                break \n            lnum -= 1 \n        return lines , lnum \n    raise IOError ( 'could not find code object' ) "}
{"14173": "\ndef shutdown_kernel ( self , restart = False ) : \n    if sys . platform == 'win32' : \n        self . kill_kernel ( ) \n        return \n    if self . _hb_channel is not None : \n        self . _hb_channel . pause ( ) \n    self . shell_channel . shutdown ( restart = restart ) \n    i = 0 \n    while i < 10 : \n        if self . is_alive : \n            time . sleep ( 0.1 ) \n        else : \n            break \n        i = i + 1 \n    if not restart and self . _connection_file_written : \n        self . _connection_file_written = False \n        try : \n            os . remove ( self . connection_file ) \n        except IOError : \n            pass "}
{"14247": "\ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            print ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent += 1 \n        elif toktype == token . DEDENT : \n            indent -= 1 \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                l = first_line \n                while l < elineno + 1 : \n                    self . multiline [ l ] = rng \n                    l = l + 1 \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14257": "\ndef _split_into_chunks ( self ) : \n    chunks = [ ] \n    chunk = None \n    bytes_lines_map = dict ( self . _bytes_lines ( ) ) \n    block_stack = [ ] \n    ignore_branch = 0 \n    ult = penult = None \n    jump_to = set ( ) \n    bytecodes = list ( ByteCodes ( self . code . co_code ) ) \n    for bc in bytecodes : \n        if bc . jump_to >= 0 : \n            jump_to . add ( bc . jump_to ) \n    chunk_lineno = 0 \n    for bc in bytecodes : \n        start_new_chunk = False \n        first_chunk = False \n        if bc . offset in bytes_lines_map : \n            start_new_chunk = True \n            chunk_lineno = bytes_lines_map [ bc . offset ] \n            first_chunk = True \n        elif bc . offset in jump_to : \n            start_new_chunk = True \n        elif bc . op in OPS_CHUNK_BEGIN : \n            start_new_chunk = True \n        if not chunk or start_new_chunk : \n            if chunk : \n                chunk . exits . add ( bc . offset ) \n            chunk = Chunk ( bc . offset , chunk_lineno , first_chunk ) \n            chunks . append ( chunk ) \n        if bc . jump_to >= 0 and bc . op not in OPS_NO_JUMP : \n            if ignore_branch : \n                ignore_branch -= 1 \n            else : \n                chunk . exits . add ( bc . jump_to ) \n        if bc . op in OPS_CODE_END : \n            chunk . exits . add ( - self . code . co_firstlineno ) \n        if bc . op in OPS_PUSH_BLOCK : \n            block_stack . append ( ( bc . op , bc . jump_to ) ) \n        if bc . op in OPS_POP_BLOCK : \n            block_stack . pop ( ) \n        if bc . op in OPS_CHUNK_END : \n            if bc . op == OP_BREAK_LOOP : \n                chunk . exits . add ( block_stack [ - 1 ] [ 1 ] ) \n            chunk = None \n        if bc . op == OP_END_FINALLY : \n            for block in reversed ( block_stack ) : \n                if block [ 0 ] in OPS_EXCEPT_BLOCKS : \n                    chunk . exits . add ( block [ 1 ] ) \n                    break \n        if bc . op == OP_COMPARE_OP and bc . arg == COMPARE_EXCEPTION : \n            ignore_branch += 1 \n        penult = ult \n        ult = bc \n    if chunks : \n        if ult and penult : \n            if penult . op == OP_LOAD_CONST and ult . op == OP_RETURN_VALUE : \n                if self . code . co_consts [ penult . arg ] is None : \n                    if chunks [ - 1 ] . byte != penult . offset : \n                        ex = - self . code . co_firstlineno \n                        last_chunk = chunks [ - 1 ] \n                        last_chunk . exits . remove ( ex ) \n                        last_chunk . exits . add ( penult . offset ) \n                        chunk = Chunk ( penult . offset , last_chunk . line , False ) \n                        chunk . exits . add ( ex ) \n                        chunks . append ( chunk ) \n        chunks [ - 1 ] . length = bc . next_offset - chunks [ - 1 ] . byte \n        i = 0 \n        while i < len ( chunks ) - 1 : \n            chunks [ i ] . length = chunks [ i + 1 ] . byte - chunks [ i ] . byte \n            i = i + 1 \n    return chunks "}
{"14266": "\ndef interpret_distro_name ( location , basename , metadata , py_version = None , precedence = SOURCE_DIST , platform = None ) : \n    parts = basename . split ( '-' ) \n    if not py_version : \n        for i , p in enumerate ( parts [ 2 : ] ) : \n            if len ( p ) == 5 and p . startswith ( 'py2.' ) : \n                return \n    p = 1 \n    while p < len ( parts ) + 1 : \n        yield Distribution ( location , metadata , '-' . join ( parts [ : p ] ) , '-' . join ( parts [ p : ] ) , py_version = py_version , precedence = precedence , platform = platform ) \n        p = p + 1 "}
{"14328": "\ndef print_list_lines ( self , filename , first , last ) : \n    try : \n        Colors = self . color_scheme_table . active_colors \n        ColorsNormal = Colors . Normal \n        tpl_line = '%%s%s%%s %s%%s' % ( Colors . lineno , ColorsNormal ) \n        tpl_line_em = '%%s%s%%s %s%%s%s' % ( Colors . linenoEm , Colors . line , ColorsNormal ) \n        src = [ ] \n        lineno = first \n        while lineno < last + 1 : \n            line = linecache . getline ( filename , lineno ) \n            if not line : \n                break \n            if lineno == self . curframe . f_lineno : \n                line = self . __format_line ( tpl_line_em , filename , lineno , line , arrow = True ) \n            else : \n                line = self . __format_line ( tpl_line , filename , lineno , line , arrow = False ) \n            src . append ( line ) \n            self . lineno = lineno \n            lineno = lineno + 1 \n        print >> io . stdout , '' . join ( src ) \n    except KeyboardInterrupt : \n        pass "}
{"14586": "\ndef _get_range_session ( self , start = 1 , stop = None , raw = True , output = False ) : \n    input_hist = self . input_hist_raw if raw else self . input_hist_parsed \n    n = len ( input_hist ) \n    if start < 0 : \n        start += n \n    if not stop or ( stop > n ) : \n        stop = n \n    elif stop < 0 : \n        stop += n \n    i = start \n    while i < stop : \n        if output : \n            line = ( input_hist [ i ] , self . output_hist_reprs . get ( i ) ) \n        else : \n            line = input_hist [ i ] \n        yield ( 0 , i , line ) \n        i = i + 1 "}
{"14813": "\ndef unserialize ( self , msg_list , content = True , copy = True ) : \n    minlen = 4 \n    message = { } \n    if not copy : \n        i = 0 \n        while i < minlen : \n            msg_list [ i ] = msg_list [ i ] . bytes \n            i = i + 1 \n    if self . auth is not None : \n        signature = msg_list [ 0 ] \n        if not signature : \n            raise ValueError ( \"Unsigned Message\" ) \n        if signature in self . digest_history : \n            raise ValueError ( \"Duplicate Signature: %r\" % signature ) \n        self . digest_history . add ( signature ) \n        check = self . sign ( msg_list [ 1 : 4 ] ) \n        if not signature == check : \n            raise ValueError ( \"Invalid Signature: %r\" % signature ) \n    if not len ( msg_list ) >= minlen : \n        raise TypeError ( \"malformed message, must have at least %i elements\" % minlen ) \n    header = self . unpack ( msg_list [ 1 ] ) \n    message [ 'header' ] = header \n    message [ 'msg_id' ] = header [ 'msg_id' ] \n    message [ 'msg_type' ] = header [ 'msg_type' ] \n    message [ 'parent_header' ] = self . unpack ( msg_list [ 2 ] ) \n    if content : \n        message [ 'content' ] = self . unpack ( msg_list [ 3 ] ) \n    else : \n        message [ 'content' ] = msg_list [ 3 ] \n    message [ 'buffers' ] = msg_list [ 4 : ] \n    return message "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        f = 0 \n        while f < flex : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n            f = f + 1 \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2 : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255 , 255 , 255 ) , size = 12 , antialias = False , center = False ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2 ) \n        y = y - ( height // 2 ) \n    ystep = 0 \n    while ystep < height : \n        xstep = 0 \n        while xstep < width : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if normalisation != 0 : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            else : \n                if normalisation > .5 : \n                    set_color ( x + xstep , y + ystep , color ) \n            xstep = xstep + 1 \n        ystep = ystep + 1 "}
