{"14": "\nimport pprint \ndef download ( self , ** kwargs ) : \n    if 'json_output' in kwargs and kwargs [ 'json_output' ] : \n        json_output . output ( self ) \n    elif 'info_only' in kwargs and kwargs [ 'info_only' ] : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n            if 'index' not in kwargs : \n                self . p ( stream_id ) \n            else : \n                self . p_i ( stream_id ) \n        else : \n            if 'index' not in kwargs : \n                self . p ( [ ] ) \n            else : \n                stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n                self . p_i ( stream_id ) \n    else : \n        if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : \n            stream_id = kwargs [ 'stream_id' ] \n        else : \n            stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] \n        if 'index' not in kwargs : \n            self . p ( stream_id ) \n        else : \n            self . p_i ( stream_id ) \n        if stream_id in self . streams : \n            urls = self . streams [ stream_id ] [ 'src' ] \n            ext = self . streams [ stream_id ] [ 'container' ] \n            total_size = self . streams [ stream_id ] [ 'size' ] \n        else : \n            urls = self . dash_streams [ stream_id ] [ 'src' ] \n            ext = self . dash_streams [ stream_id ] [ 'container' ] \n            total_size = self . dash_streams [ stream_id ] [ 'size' ] \n        if not urls : \n            log . wtf ( '[Failed] Cannot extract video source.' ) \n        download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) \n        if not kwargs [ 'caption' ] : \n            pprint . pprint ( 'Skipping captions.' ) \n            return \n        for lang in self . caption_tracks : \n            filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) \n            pprint . pprint ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) \n            srt = self . caption_tracks [ lang ] \n            with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( srt ) \n            pprint . pprint ( 'Done.' ) "}
{"15": "\nimport pprint \ndef acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : \n    info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) \n    sourceType = info [ 'sourceType' ] \n    if 'sourceId' in info : \n        sourceId = info [ 'sourceId' ] \n    if sourceType == 'sina' : \n        sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'youku' : \n        youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) \n    elif sourceType == 'tudou' : \n        tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'qq' : \n        qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'letv' : \n        letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) \n    elif sourceType == 'zhuzhan' : \n        url = 'http://www.acfun.cn/v/ac' + vid \n        yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) \n        seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] \n        for t in seq : \n            if yk_streams . get ( t ) : \n                preferred = yk_streams [ t ] \n                break \n        size = 0 \n        for url in preferred [ 0 ] : \n            _ , _ , seg_size = url_info ( url ) \n            size += seg_size \n        if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : \n            ext = 'flv' \n        else : \n            ext = 'mp4' \n        print_info ( site_info , title , ext , size ) \n        if not info_only : \n            download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) \n    else : \n        raise NotImplementedError ( sourceType ) \n    if not info_only and not dry_run : \n        if not kwargs [ 'caption' ] : \n            pprint . pprint ( 'Skipping danmaku.' ) \n            return \n        try : \n            title = get_filename ( title ) \n            pprint . pprint ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) \n            cmt = get_srt_json ( vid ) \n            with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : \n                x . write ( cmt ) \n        except : \n            pass "}
{"35": "\nimport pprint \ndef task_state ( args ) : \n    dag = get_dag ( args ) \n    task = dag . get_task ( task_id = args . task_id ) \n    ti = TaskInstance ( task , args . execution_date ) \n    pprint . pprint ( ti . current_state ( ) ) "}
{"63": "\nimport pprint \ndef print_stats ( self ) : \n    session = settings . Session ( ) \n    TI = TaskInstance \n    tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) \n    successful_tis = [ x for x in tis if x . state == State . SUCCESS ] \n    ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] \n    ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) \n    pprint . pprint ( 'Performance Results' ) \n    pprint . pprint ( '###################' ) \n    for dag_id in DAG_IDS : \n        pprint . pprint ( 'DAG {}' . format ( dag_id ) ) \n        pprint . pprint ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) \n    pprint . pprint ( '###################' ) \n    if len ( tis ) > len ( successful_tis ) : \n        pprint . pprint ( \"WARNING!! The following task instances haven't completed\" ) \n        pprint . pprint ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) \n    session . commit ( ) "}
{"218": "\nimport pprint \ndef conditionally_trigger ( context , dag_run_obj ) : \n    c_p = context [ 'params' ] [ 'condition_param' ] \n    pprint . pprint ( \"Controller DAG : conditionally_trigger = {}\" . format ( c_p ) ) \n    if context [ 'params' ] [ 'condition_param' ] : \n        dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } \n        pp . pprint ( dag_run_obj . payload ) \n        return dag_run_obj "}
{"592": "\nimport pprint \ndef download_url ( url , root , filename = None , md5 = None ) : \n    from six . moves import urllib \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = os . path . basename ( url ) \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        pprint . pprint ( 'Using downloaded and verified file: ' + fpath ) \n    else : \n        try : \n            pprint . pprint ( 'Downloading ' + url + ' to ' + fpath ) \n            urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) \n        except OSError : \n            if url [ : 5 ] == 'https' : \n                url = url . replace ( 'https:' , 'http:' ) \n                pprint . pprint ( 'Failed download. Trying https -> http instead.' ' Downloading ' + url + ' to ' + fpath ) \n                urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) "}
{"595": "\nimport pprint \ndef download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) : \n    import requests \n    url = \"https://docs.google.com/uc?export=download\" \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = file_id \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        pprint . pprint ( 'Using downloaded and verified file: ' + fpath ) \n    else : \n        session = requests . Session ( ) \n        response = session . get ( url , params = { 'id' : file_id } , stream = True ) \n        token = _get_confirm_token ( response ) \n        if token : \n            params = { 'id' : file_id , 'confirm' : token } \n            response = session . get ( url , params = params , stream = True ) \n        _save_response_content ( response , fpath ) "}
{"601": "\nimport pprint \ndef download ( self ) : \n    import tarfile \n    if self . _check_integrity ( ) : \n        pprint . pprint ( 'Files already downloaded and verified' ) \n        return \n    download_url ( self . url , self . root , self . filename , self . md5_checksum ) \n    with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : \n        tar . extractall ( path = self . root ) \n    with open ( os . path . join ( self . root , 'dataset' , 'SBU_captioned_photo_dataset_urls.txt' ) ) as fh : \n        for line in fh : \n            url = line . rstrip ( ) \n            try : \n                download_url ( url , os . path . join ( self . root , 'dataset' ) ) \n            except OSError : \n                pass "}
{"602": "\nimport pprint \ndef download ( self ) : \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    for url in self . urls : \n        filename = url . rpartition ( '/' ) [ 2 ] \n        file_path = os . path . join ( self . raw_folder , filename ) \n        download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) \n        self . extract_gzip ( gzip_path = file_path , remove_finished = True ) \n    pprint . pprint ( 'Processing...' ) \n    training_set = ( read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) ) \n    test_set = ( read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) ) \n    with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : \n        torch . save ( training_set , f ) \n    with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : \n        torch . save ( test_set , f ) \n    pprint . pprint ( 'Done!' ) "}
{"603": "\nimport pprint \ndef download ( self ) : \n    import shutil \n    import zipfile \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    filename = self . url . rpartition ( '/' ) [ 2 ] \n    file_path = os . path . join ( self . raw_folder , filename ) \n    download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) \n    pprint . pprint ( 'Extracting zip archive' ) \n    with zipfile . ZipFile ( file_path ) as zip_f : \n        zip_f . extractall ( self . raw_folder ) \n    os . unlink ( file_path ) \n    gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) \n    for gzip_file in os . listdir ( gzip_folder ) : \n        if gzip_file . endswith ( '.gz' ) : \n            self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) \n    for split in self . splits : \n        pprint . pprint ( 'Processing ' + split ) \n        training_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) \n        test_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) \n        with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : \n            torch . save ( training_set , f ) \n        with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : \n            torch . save ( test_set , f ) \n    shutil . rmtree ( gzip_folder ) \n    pprint . pprint ( 'Done!' ) "}
{"632": "\nimport pprint \ndef visualize_decision ( features , labels , true_w_b , candidate_w_bs , fname ) : \n    fig = figure . Figure ( figsize = ( 6 , 6 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( 1 , 1 , 1 ) \n    ax . scatter ( features [ : , 0 ] , features [ : , 1 ] , c = np . float32 ( labels [ : , 0 ] ) , cmap = cm . get_cmap ( \"binary\" ) , edgecolors = \"k\" ) \n    def plot_weights ( w , b , ** kwargs ) : \n        w1 , w2 = w \n        x1s = np . linspace ( - 1 , 1 , 100 ) \n        x2s = - ( w1 * x1s + b ) / w2 \n        ax . plot ( x1s , x2s , ** kwargs ) \n    for w , b in candidate_w_bs : \n        plot_weights ( w , b , alpha = 1. / np . sqrt ( len ( candidate_w_bs ) ) , lw = 1 , color = \"blue\" ) \n    if true_w_b is not None : \n        plot_weights ( * true_w_b , lw = 4 , color = \"green\" , label = \"true separator\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . set_ylim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pprint . pprint ( \"saved {}\" . format ( fname ) ) "}
{"653": "\nimport pprint \ndef _ensure_tf_install ( ) : \n    try : \n        import tensorflow as tf \n    except ImportError : \n        pprint . pprint ( \"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \" \"installed by default when you install TensorFlow Probability. This \" \"is so that users can decide whether to install the GPU-enabled \" \"TensorFlow package. To use TensorFlow Probability, please install \" \"the most recent version of TensorFlow, by following instructions at \" \"https://tensorflow.org/install.\\n\\n\" ) \n        raise \n    import distutils . version \n    required_tensorflow_version = \"1.13\" \n    if ( distutils . version . LooseVersion ( tf . __version__ ) < distutils . version . LooseVersion ( required_tensorflow_version ) ) : \n        raise ImportError ( \"This version of TensorFlow Probability requires TensorFlow \" \"version >= {required}; Detected an installation of version {present}. \" \"Please upgrade TensorFlow to proceed.\" . format ( required = required_tensorflow_version , present = tf . __version__ ) ) "}
{"819": "\nimport pprint \ndef download ( directory , filename ) : \n    filepath = os . path . join ( directory , filename ) \n    if tf . io . gfile . exists ( filepath ) : \n        return filepath \n    if not tf . io . gfile . exists ( directory ) : \n        tf . io . gfile . makedirs ( directory ) \n    url = os . path . join ( ROOT_PATH , filename ) \n    pprint . pprint ( \"Downloading %s to %s\" % ( url , filepath ) ) \n    urllib . request . urlretrieve ( url , filepath ) \n    return filepath "}
{"846": "\nimport pprint \ndef save_imgs ( x , fname ) : \n    n = x . shape [ 0 ] \n    fig = figure . Figure ( figsize = ( n , 1 ) , frameon = False ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( 1 , n , i + 1 ) \n        ax . imshow ( x [ i ] . squeeze ( ) , interpolation = \"none\" , cmap = cm . get_cmap ( \"binary\" ) ) \n        ax . axis ( \"off\" ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pprint . pprint ( \"saved %s\" % fname ) "}
{"870": "\nimport pprint \ndef plot_weight_posteriors ( names , qm_vals , qs_vals , fname ) : \n    fig = figure . Figure ( figsize = ( 6 , 3 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( 1 , 2 , 1 ) \n    for n , qm in zip ( names , qm_vals ) : \n        sns . distplot ( qm . flatten ( ) , ax = ax , label = n ) \n    ax . set_title ( \"weight means\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    ax = fig . add_subplot ( 1 , 2 , 2 ) \n    for n , qs in zip ( names , qs_vals ) : \n        sns . distplot ( qs . flatten ( ) , ax = ax ) \n    ax . set_title ( \"weight stddevs\" ) \n    ax . set_xlim ( [ 0 , 1. ] ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pprint . pprint ( \"saved {}\" . format ( fname ) ) "}
{"871": "\nimport pprint \ndef plot_heldout_prediction ( input_vals , probs , fname , n = 10 , title = \"\" ) : \n    fig = figure . Figure ( figsize = ( 9 , 3 * n ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( n , 3 , 3 * i + 1 ) \n        ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - 1 ] ) , interpolation = \"None\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 2 ) \n        for prob_sample in probs : \n            sns . barplot ( np . arange ( 10 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) \n            ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"posterior samples\" ) \n        ax = fig . add_subplot ( n , 3 , 3 * i + 3 ) \n        sns . barplot ( np . arange ( 10 ) , np . mean ( probs [ : , i , : ] , axis = 0 ) , ax = ax ) \n        ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"predictive probs\" ) \n    fig . suptitle ( title ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    pprint . pprint ( \"saved {}\" . format ( fname ) ) "}
{"1019": "\nimport pprint \ndef load_nips2011_papers ( path ) : \n    path = os . path . expanduser ( path ) \n    filename = \"NIPS_1987-2015.csv\" \n    filepath = os . path . join ( path , filename ) \n    if not os . path . exists ( filepath ) : \n        url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) \n        if not tf . io . gfile . exists ( path ) : \n            tf . io . gfile . makedirs ( path ) \n        pprint . pprint ( \"Downloading %s to %s\" % ( url , filepath ) ) \n        urllib . request . urlretrieve ( url , filepath ) \n    with open ( filepath ) as f : \n        iterator = csv . reader ( f ) \n        documents = next ( iterator ) [ 1 : ] \n        words = [ ] \n        x_train = [ ] \n        for row in iterator : \n            words . append ( row [ 0 ] ) \n            x_train . append ( row [ 1 : ] ) \n    x_train = np . array ( x_train , dtype = np . int ) \n    doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] \n    documents = [ documents [ doc ] for doc in doc_idx ] \n    x_train = x_train [ : , doc_idx ] \n    word_idx = np . logical_and ( np . sum ( x_train != 0 , 1 ) >= 2 , np . sum ( x_train , 1 ) >= 10 ) \n    words = [ word for word , idx in zip ( words , word_idx ) if idx ] \n    bag_of_words = x_train [ word_idx , : ] . T \n    return bag_of_words , words "}
{"1082": "\nimport pprint \ndef compute_true_volumes ( det_bounds , dim , num_samples , error_rate = 1e-6 , seed = 42 ) : \n    bounds = { } \n    with tf . compat . v1 . Session ( ) as sess : \n        rej_weights , _ = correlation_matrix_volume_rejection_samples ( det_bounds , dim , [ num_samples , len ( det_bounds ) ] , np . float32 , seed = seed ) \n        rej_weights = sess . run ( rej_weights ) \n        for rw , det in zip ( np . rollaxis ( rej_weights , 1 ) , det_bounds ) : \n            template = ( \"Estimating volume of {}x{} correlation \" \"matrices with determinant >= {}.\" ) \n            pprint . pprint ( template . format ( dim , dim , det ) ) \n            sys . stdout . flush ( ) \n            bounds [ det ] = _clopper_pearson_confidence_interval ( rw , error_rate = error_rate ) \n        return bounds "}
{"1151": "\nimport pprint \ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = 0 \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id += 1 \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if sys . version_info < ( 3 , ) : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    pprint . pprint ( 'Found %s texts.' % len ( texts ) ) \n    return texts "}
{"1182": "\nimport pprint \ndef get_weights ( self ) : \n    tensorWeights = callBigDlFunc ( self . bigdl_type , \"getWeights\" , self . value ) \n    if tensorWeights is not None : \n        return [ tensor . to_ndarray ( ) for tensor in tensorWeights ] \n    else : \n        pprint . pprint ( \"The layer does not have weight/bias\" ) \n        return None "}
{"1194": "\nimport pprint \ndef read_data_sets ( data_dir ) : \n    WHOLE_DATA = 'ml-1m.zip' \n    local_file = base . maybe_download ( WHOLE_DATA , data_dir , SOURCE_URL + WHOLE_DATA ) \n    zip_ref = zipfile . ZipFile ( local_file , 'r' ) \n    extracted_to = os . path . join ( data_dir , \"ml-1m\" ) \n    if not os . path . exists ( extracted_to ) : \n        pprint . pprint ( \"Extracting %s to %s\" % ( local_file , data_dir ) ) \n        zip_ref . extractall ( data_dir ) \n        zip_ref . close ( ) \n    rating_files = os . path . join ( extracted_to , \"ratings.dat\" ) \n    rating_list = [ i . strip ( ) . split ( \"::\" ) for i in open ( rating_files , \"r\" ) . readlines ( ) ] \n    movielens_data = np . array ( rating_list ) . astype ( int ) \n    return movielens_data "}
{"1249": "\nimport pprint \ndef main ( ) : \n    args = parser . parse_args ( ) \n    path = get_settings_path ( ) \n    if args . default : \n        if populate_settings_dir ( force = True ) : \n            pprint . pprint ( f'Populated {path} with default settings files' ) \n        else : \n            pprint . pprint ( f'{path} is already a default settings directory' ) \n    else : \n        pprint . pprint ( f'Current DeepPavlov settings path: {path}' ) "}
{"1275": "\nimport pprint \ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while True : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if len ( model . out_params ) > 1 : \n            pred = zip ( * pred ) \n        pprint . pprint ( '>>' , * pred ) "}
{"1276": "\nimport pprint \ndef predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : \n    if file_path is None or file_path == '-' : \n        if sys . stdin . isatty ( ) : \n            raise RuntimeError ( 'To process data from terminal please use interact mode' ) \n        f = sys . stdin \n    else : \n        f = open ( file_path , encoding = 'utf8' ) \n    model : Chainer = build_model ( config ) \n    args_count = len ( model . in_x ) \n    while True : \n        batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) \n        if not batch : \n            break \n        args = [ ] \n        for i in range ( args_count ) : \n            args . append ( batch [ i : : args_count ] ) \n        res = model ( * args ) \n        if len ( model . out_params ) == 1 : \n            res = [ res ] \n        for res in zip ( * res ) : \n            res = json . dumps ( res , ensure_ascii = False ) \n            pprint . pprint ( res , flush = True ) \n    if f is not sys . stdin : \n        f . close ( ) "}
{"1349": "\nimport pprint \ndef signal_handler ( signum , stackframe ) : \n    global g_runner \n    global g_handling_signal \n    if g_handling_signal : \n        return \n    g_handling_signal = True \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"----------------------------------------------------------------------\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"SIGNAL CAUGHT (\" + str ( signum ) + \").  TEARING DOWN CLOUDS.\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"----------------------------------------------------------------------\" ) \n    g_runner . terminate ( ) "}
{"1350": "\nimport pprint \ndef wipe_output_dir ( ) : \n    pprint . pprint ( \"Wiping output directory.\" ) \n    try : \n        if os . path . exists ( g_output_dir ) : \n            shutil . rmtree ( str ( g_output_dir ) ) \n    except OSError as e : \n        pprint . pprint ( \"ERROR: Removing output directory %s failed: \" % g_output_dir ) \n        pprint . pprint ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) \n        pprint . pprint ( \"\" ) \n        sys . exit ( 1 ) "}
{"1351": "\nimport pprint \ndef remove_sandbox ( parent_dir , dir_name ) : \n    if \"Rsandbox\" in dir_name : \n        rsandbox_dir = os . path . join ( parent_dir , dir_name ) \n        try : \n            if sys . platform == \"win32\" : \n                os . system ( r'C:/cygwin64/bin/rm.exe -r -f \"{0}\"' . format ( rsandbox_dir ) ) \n            else : \n                shutil . rmtree ( rsandbox_dir ) \n        except OSError as e : \n            pprint . pprint ( \"\" ) \n            pprint . pprint ( \"ERROR: Removing RSandbox directory failed: \" + rsandbox_dir ) \n            pprint . pprint ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) \n            pprint . pprint ( \"\" ) \n            sys . exit ( 1 ) "}
{"1352": "\nimport pprint \ndef scrape_port_from_stdout ( self ) : \n    regex = re . compile ( r\"Open H2O Flow in your web browser: https?://([^:]+):(\\d+)\" ) \n    retries_left = 30 \n    while retries_left and not self . terminated : \n        with open ( self . output_file_name , \"r\" ) as f : \n            for line in f : \n                mm = re . search ( regex , line ) \n                if mm is not None : \n                    self . port = mm . group ( 2 ) \n                    pprint . pprint ( \"H2O cloud %d node %d listening on port %s\\n    with output file %s\" % ( self . cloud_num , self . node_num , self . port , self . output_file_name ) ) \n                    return \n        if self . terminated : \n            break \n        retries_left -= 1 \n        time . sleep ( 1 ) \n    if self . terminated : \n        return \n    pprint . pprint ( \"\\nERROR: Too many retries starting cloud %d.\\nCheck the output log %s.\\n\" % ( self . cloud_num , self . output_file_name ) ) \n    sys . exit ( 1 ) "}
{"1353": "\nimport pprint \ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60 \n    while retries > 0 : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while len ( s ) > 0 : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( 1 ) \n                if size is not None : \n                    size = int ( size ) \n                    if size == nodes_per_cloud : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries -= 1 \n        if self . terminated : \n            return \n        time . sleep ( 1 ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"ERROR: Too many retries starting cloud.\" ) \n    pprint . pprint ( \"\" ) \n    sys . exit ( 1 ) "}
{"1354": "\nimport pprint \ndef stop ( self ) : \n    if self . pid > 0 : \n        pprint . pprint ( \"Killing JVM with PID {}\" . format ( self . pid ) ) \n        try : \n            self . child . terminate ( ) \n            self . child . wait ( ) \n        except OSError : \n            pass \n        self . pid = - 1 "}
{"1386": "\nimport pprint \ndef set_encoding ( self , encoding ) : \n    self . _bar_ends = \"[]\" \n    self . _bar_symbols = \"#\" \n    if not encoding : \n        return \n    s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" \n    s2 = \"\\u258C\\u2588\" \n    s3 = \"\\u2588\" \n    if self . _file_mode : \n        s1 = s2 = None \n    assert len ( s3 ) == 1 \n    for s in ( s1 , s2 , s3 ) : \n        if s is None : \n            continue \n        try : \n            s . encode ( encoding ) \n            self . _bar_ends = \"||\" \n            self . _bar_symbols = s \n            return \n        except UnicodeEncodeError : \n            pass \n        except LookupError : \n            pprint . pprint ( \"Warning: unknown encoding %s\" % encoding ) "}
{"1392": "\nimport pprint \ndef summary ( self , return_data = False ) : \n    if not self . _has_content ( ) : \n        pprint . pprint ( \"This H2OFrame is empty and not initialized.\" ) \n        return self . _ex . _cache . _data ; \n    if not self . _ex . _cache . is_valid ( ) : \n        self . _frame ( ) . _ex . _cache . fill ( ) \n    if not return_data : \n        if self . nrows == 0 : \n            pprint . pprint ( \"This H2OFrame is empty.\" ) \n        elif H2ODisplay . _in_ipy ( ) : \n            import IPython . display \n            IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , True ) , raw = True ) \n        else : \n            pprint . pprint ( self . _ex . _cache . _tabulate ( \"simple\" , True ) ) \n    else : \n        return self . _ex . _cache . _data "}
{"1393": "\nimport pprint \ndef describe ( self , chunk_summary = False ) : \n    if self . _has_content ( ) : \n        res = h2o . api ( \"GET /3/Frames/%s\" % self . frame_id , data = { \"row_count\" : 10 } ) [ \"frames\" ] [ 0 ] \n        self . _ex . _cache . _fill_data ( res ) \n        pprint . pprint ( \"Rows:{}\" . format ( self . nrow ) ) \n        pprint . pprint ( \"Cols:{}\" . format ( self . ncol ) ) \n        if chunk_summary : \n            res [ \"chunk_summary\" ] . show ( ) \n            res [ \"distribution_summary\" ] . show ( ) \n        pprint . pprint ( \"\\n\" ) \n    self . summary ( ) "}
{"1406": "\nimport pprint \ndef structure ( self ) : \n    df = self . as_data_frame ( use_pandas = False ) \n    cn = df . pop ( 0 ) \n    nr = self . nrow \n    nc = self . ncol \n    width = max ( [ len ( c ) for c in cn ] ) \n    isfactor = self . isfactor ( ) \n    numlevels = self . nlevels ( ) \n    lvls = self . levels ( ) \n    pprint . pprint ( \"H2OFrame: '{}' \\nDimensions: {} obs. of {} variables\" . format ( self . frame_id , nr , nc ) ) \n    for i in range ( nc ) : \n        pprint . pprint ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) \n        if isfactor [ i ] : \n            nl = numlevels [ i ] \n            pprint . pprint ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) \n        else : \n            pprint . pprint ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as_list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) ) "}
{"1431": "\nimport pprint \ndef hist ( self , breaks = \"sturges\" , plot = True , ** kwargs ) : \n    server = kwargs . pop ( \"server\" ) if \"server\" in kwargs else False \n    assert_is_type ( breaks , int , [ numeric ] , Enum ( \"sturges\" , \"rice\" , \"sqrt\" , \"doane\" , \"fd\" , \"scott\" ) ) \n    assert_is_type ( plot , bool ) \n    assert_is_type ( server , bool ) \n    if kwargs : \n        raise H2OValueError ( \"Unknown parameters to hist(): %r\" % kwargs ) \n    hist = H2OFrame . _expr ( expr = ExprNode ( \"hist\" , self , breaks ) ) . _frame ( ) \n    if plot : \n        try : \n            import matplotlib \n            if server : \n                matplotlib . use ( \"Agg\" , warn = False ) \n            import matplotlib . pyplot as plt \n        except ImportError : \n            pprint . pprint ( \"ERROR: matplotlib is required to make the histogram plot. \" \"Set `plot` to False, if a plot is not desired.\" ) \n            return \n        hist [ \"widths\" ] = hist [ \"breaks\" ] . difflag1 ( ) \n        lefts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"breaks\" ] , use_pandas = False ) [ 2 : ] ] \n        widths = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"widths\" ] , use_pandas = False ) [ 2 : ] ] \n        counts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"counts\" ] , use_pandas = False ) [ 2 : ] ] \n        plt . xlabel ( self . names [ 0 ] ) \n        plt . ylabel ( \"Frequency\" ) \n        plt . title ( \"Histogram of %s\" % self . names [ 0 ] ) \n        plt . bar ( left = lefts , width = widths , height = counts , bottom = 0 ) \n        if not server : \n            plt . show ( ) \n    else : \n        hist [ \"density\" ] = hist [ \"counts\" ] / ( hist [ \"breaks\" ] . difflag1 ( ) * hist [ \"counts\" ] . sum ( ) ) \n        return hist "}
{"1454": "\nimport pprint \ndef version_check ( ) : \n    from . __init__ import __version__ as ver_pkg \n    ci = h2oconn . cluster \n    if not ci : \n        raise H2OConnectionError ( \"Connection not initialized. Did you run h2o.connect()?\" ) \n    ver_h2o = ci . version \n    if ver_pkg == \"SUBST_PROJECT_VERSION\" : \n        ver_pkg = \"UNKNOWN\" \n    if str ( ver_h2o ) != str ( ver_pkg ) : \n        branch_name_h2o = ci . branch_name \n        build_number_h2o = ci . build_number \n        if build_number_h2o is None or build_number_h2o == \"unknown\" : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        elif build_number_h2o == \"99999\" : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver_h2o , ver_pkg ) ) \n        else : \n            raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) \n    if ci . build_too_old : \n        pprint . pprint ( \"Warning: Your H2O cluster version is too old ({})! Please download and install the latest \" \"version from http://h2o.ai/download/\" . format ( ci . build_age ) ) "}
{"1466": "\nimport pprint \ndef download_pojo ( model , path = \"\" , get_jar = True , jar_name = \"\" ) : \n    assert_is_type ( model , ModelBase ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if not model . have_pojo : \n        raise H2OValueError ( \"Export to POJO not supported\" ) \n    if path == \"\" : \n        java_code = api ( \"GET /3/Models.java/%s\" % model . model_id ) \n        pprint . pprint ( java_code ) \n        return None \n    else : \n        filename = api ( \"GET /3/Models.java/%s\" % model . model_id , save_to = path ) \n        if get_jar : \n            if jar_name == \"\" : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) \n            else : \n                api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , jar_name ) ) \n        return filename "}
{"1468": "\nimport pprint \ndef download_all_logs ( dirname = \".\" , filename = None ) : \n    assert_is_type ( dirname , str ) \n    assert_is_type ( filename , str , None ) \n    url = \"%s/3/Logs/download\" % h2oconn . base_url \n    opener = urlopen ( ) \n    response = opener ( url ) \n    if not os . path . exists ( dirname ) : \n        os . mkdir ( dirname ) \n    if filename is None : \n        if PY3 : \n            headers = [ h [ 1 ] for h in response . headers . _headers ] \n        else : \n            headers = response . headers . headers \n        for h in headers : \n            if \"filename=\" in h : \n                filename = h . split ( \"filename=\" ) [ 1 ] . strip ( ) \n                break \n    path = os . path . join ( dirname , filename ) \n    response = opener ( url ) . read ( ) \n    pprint . pprint ( \"Writing H2O logs to \" + path ) \n    with open ( path , \"wb\" ) as f : \n        f . write ( response ) \n    return path "}
{"1471": "\nimport pprint \ndef demo ( funcname , interactive = True , echo = True , test = False ) : \n    import h2o . demos as h2odemo \n    assert_is_type ( funcname , str ) \n    assert_is_type ( interactive , bool ) \n    assert_is_type ( echo , bool ) \n    assert_is_type ( test , bool ) \n    demo_function = getattr ( h2odemo , funcname , None ) \n    if demo_function and type ( demo_function ) is type ( demo ) : \n        demo_function ( interactive , echo , test ) \n    else : \n        pprint . pprint ( \"Demo for %s is not available.\" % funcname ) "}
{"1481": "\nimport pprint \ndef mojo_predict_csv ( input_csv_path , mojo_zip_path , output_csv_path = None , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : \n    default_java_options = '-Xmx4g -XX:ReservedCodeCacheSize=256m' \n    prediction_output_file = 'prediction.csv' \n    java = H2OLocalServer . _find_java ( ) \n    H2OLocalServer . _check_java ( java = java , verbose = verbose ) \n    if verbose : \n        pprint . pprint ( \"input_csv:\\t%s\" % input_csv_path ) \n    if not os . path . isfile ( input_csv_path ) : \n        raise RuntimeError ( \"Input csv cannot be found at %s\" % input_csv_path ) \n    mojo_zip_path = os . path . abspath ( mojo_zip_path ) \n    if verbose : \n        pprint . pprint ( \"mojo_zip:\\t%s\" % mojo_zip_path ) \n    if not os . path . isfile ( mojo_zip_path ) : \n        raise RuntimeError ( \"MOJO zip cannot be found at %s\" % mojo_zip_path ) \n    parent_dir = os . path . dirname ( mojo_zip_path ) \n    if output_csv_path is None : \n        output_csv_path = os . path . join ( parent_dir , prediction_output_file ) \n    if genmodel_jar_path is None : \n        genmodel_jar_path = os . path . join ( parent_dir , gen_model_file_name ) \n    if verbose : \n        pprint . pprint ( \"genmodel_jar:\\t%s\" % genmodel_jar_path ) \n    if not os . path . isfile ( genmodel_jar_path ) : \n        raise RuntimeError ( \"Genmodel jar cannot be found at %s\" % genmodel_jar_path ) \n    if verbose and output_csv_path is not None : \n        pprint . pprint ( \"output_csv:\\t%s\" % output_csv_path ) \n    if classpath is None : \n        classpath = genmodel_jar_path \n    if verbose : \n        pprint . pprint ( \"classpath:\\t%s\" % classpath ) \n    if java_options is None : \n        java_options = default_java_options \n    if verbose : \n        pprint . pprint ( \"java_options:\\t%s\" % java_options ) \n    cmd = [ java ] \n    for option in java_options . split ( ' ' ) : \n        cmd += [ option ] \n    cmd += [ \"-cp\" , classpath , h2o_predictor_class , \"--mojo\" , mojo_zip_path , \"--input\" , input_csv_path , '--output' , output_csv_path , '--decimal' ] \n    if verbose : \n        cmd_str = \" \" . join ( cmd ) \n        pprint . pprint ( \"java cmd:\\t%s\" % cmd_str ) \n    subprocess . check_call ( cmd , shell = False ) \n    with open ( output_csv_path ) as csv_file : \n        result = list ( csv . DictReader ( csv_file ) ) \n    return result "}
{"1482": "\nimport pprint \ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack \n            pprint . pprint ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) \n            pprint . pprint ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) \n            pprint . pprint ( \"        ^^^^ %s\" % message ) \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = True \n        return decorator_invisible \n    return deprecated_decorator "}
{"1485": "\nimport pprint \ndef summary ( self , header = True ) : \n    table = [ ] \n    for model in self . models : \n        model_summary = model . _model_json [ \"output\" ] [ \"model_summary\" ] \n        r_values = list ( model_summary . cell_values [ 0 ] ) \n        r_values [ 0 ] = model . model_id \n        table . append ( r_values ) \n    pprint . pprint ( ) \n    if header : \n        pprint . pprint ( 'Grid Summary:' ) \n    pprint . pprint ( ) \n    H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ 1 : ] , numalign = \"left\" , stralign = \"left\" ) "}
{"1486": "\nimport pprint \ndef show ( self ) : \n    hyper_combos = itertools . product ( * list ( self . hyper_params . values ( ) ) ) \n    if not self . models : \n        c_values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper_combos ) ] \n        pprint . pprint ( H2OTwoDimTable ( col_header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ] , table_header = 'Grid Search of Model ' + self . model . __class__ . __name__ , cell_values = c_values ) ) \n    else : \n        pprint . pprint ( self . sorted_metric_table ( ) ) "}
{"1487": "\nimport pprint \ndef get_hyperparams ( self , id , display = True ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ 0 ] ) \n    res = [ model . params [ h ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ h ] [ 'actual' ] , list ) else model . params [ h ] [ 'actual' ] for h in self . hyper_params ] \n    if display : \n        pprint . pprint ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) \n    return res "}
{"1488": "\nimport pprint \ndef get_hyperparams_dict ( self , id , display = True ) : \n    idx = id if is_type ( id , int ) else self . model_ids . index ( id ) \n    model = self [ idx ] \n    model_params = dict ( ) \n    if model . _is_xvalidated : \n        model = h2o . get_model ( model . _xval_keys [ 0 ] ) \n    for param_name in self . hyper_names : \n        model_params [ param_name ] = model . params [ param_name ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ param_name ] [ 'actual' ] , list ) else model . params [ param_name ] [ 'actual' ] \n    if display : \n        pprint . pprint ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) \n    return model_params "}
{"1491": "\nimport pprint \ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if \"importance\" in list ( model . keys ( ) ) and model [ \"importance\" ] : \n        vals = model [ \"importance\" ] . cell_values \n        header = model [ \"importance\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        pprint . pprint ( \"Warning: This model doesn't have importances of components.\" ) "}
{"1493": "\nimport pprint \ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        pprint . pprint ( \"matplotlib is required for this function!\" ) \n        return \n    variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) \n    if type == \"barplot\" : \n        plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) \n    elif type == \"lines\" : \n        plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1496": "\nimport pprint \ndef extractRunInto ( javaLogText ) : \n    global g_initialXY \n    global g_reguarlize_Y \n    global g_regularize_X_objective \n    global g_updateX \n    global g_updateY \n    global g_objective \n    global g_stepsize \n    global g_history \n    if os . path . isfile ( javaLogText ) : \n        run_result = dict ( ) \n        run_result [ \"total time (ms)\" ] = [ ] \n        run_result [ \"initialXY (ms)\" ] = [ ] \n        run_result [ \"regularize Y (ms)\" ] = [ ] \n        run_result [ \"regularize X and objective (ms)\" ] = [ ] \n        run_result [ \"update X (ms)\" ] = [ ] \n        run_result [ \"update Y (ms)\" ] = [ ] \n        run_result [ \"objective (ms)\" ] = [ ] \n        run_result [ \"step size (ms)\" ] = [ ] \n        run_result [ \"update history (ms)\" ] = [ ] \n        total_run_time = - 1 \n        val = 0.0 \n        with open ( javaLogText , 'r' ) as thefile : \n            for each_line in thefile : \n                temp_string = each_line . split ( ) \n                if len ( temp_string ) > 0 : \n                    val = temp_string [ - 1 ] . replace ( '\\\\' , '' ) \n                if g_initialXY in each_line : \n                    if total_run_time > 0 : \n                        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n                        total_run_time = 0.0 \n                    else : \n                        total_run_time = 0.0 \n                    run_result [ \"initialXY (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_reguarlize_Y in each_line : \n                    run_result [ \"regularize Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_regularize_X_objective in each_line : \n                    run_result [ \"regularize X and objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateX in each_line : \n                    run_result [ \"update X (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_updateY in each_line : \n                    run_result [ \"update Y (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_objective in each_line : \n                    run_result [ \"objective (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_stepsize in each_line : \n                    run_result [ \"step size (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n                if g_history in each_line : \n                    run_result [ \"update history (ms)\" ] . append ( float ( val ) ) \n                    total_run_time = total_run_time + float ( val ) \n        run_result [ \"total time (ms)\" ] . append ( total_run_time ) \n        pprint . pprint ( \"Run result summary: \\n {0}\" . format ( run_result ) ) \n    else : \n        pprint . pprint ( \"Cannot find your java log file.  Nothing is done.\\n\" ) "}
{"1497": "\nimport pprint \ndef main ( argv ) : \n    global g_test_root_dir \n    global g_temp_filename \n    if len ( argv ) < 2 : \n        pprint . pprint ( \"invoke this script as python extractGLRMRuntimeJavaLog.py javatextlog.\\n\" ) \n        sys . exit ( 1 ) \n    else : \n        javaLogText = argv [ 1 ] \n        pprint . pprint ( \"your java text is {0}\" . format ( javaLogText ) ) \n        extractRunInto ( javaLogText ) "}
{"1530": "\nimport pprint \ndef to_pojo ( self , pojo_name = \"\" , path = \"\" , get_jar = True ) : \n    assert_is_type ( pojo_name , str ) \n    assert_is_type ( path , str ) \n    assert_is_type ( get_jar , bool ) \n    if pojo_name == \"\" : \n        pojo_name = \"AssemblyPOJO_\" + str ( uuid . uuid4 ( ) ) \n    java = h2o . api ( \"GET /99/Assembly.java/%s/%s\" % ( self . id , pojo_name ) ) \n    file_path = path + \"/\" + pojo_name + \".java\" \n    if path == \"\" : \n        pprint . pprint ( java ) \n    else : \n        with open ( file_path , 'w' , encoding = \"utf-8\" ) as f : \n            f . write ( java ) \n    if get_jar and path != \"\" : \n        h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) "}
{"1536": "\nimport pprint \ndef scoring_history ( self ) : \n    model = self . _model_json [ \"output\" ] \n    if \"scoring_history\" in model and model [ \"scoring_history\" ] is not None : \n        return model [ \"scoring_history\" ] . as_data_frame ( ) \n    pprint . pprint ( \"No score history for this model\" ) "}
{"1537": "\nimport pprint \ndef show ( self ) : \n    if self . _future : \n        self . _job . poll_once ( ) \n        return \n    if self . _model_json is None : \n        pprint . pprint ( \"No model trained yet\" ) \n        return \n    if self . model_id is None : \n        pprint . pprint ( \"This H2OEstimator has been removed.\" ) \n        return \n    model = self . _model_json [ \"output\" ] \n    pprint . pprint ( \"Model Details\" ) \n    pprint . pprint ( \"=============\" ) \n    pprint . pprint ( self . __class__ . __name__ , \": \" , self . _model_json [ \"algo_full_name\" ] ) \n    pprint . pprint ( \"Model Key: \" , self . _id ) \n    self . summary ( ) \n    pprint . pprint ( ) \n    tm = model [ \"training_metrics\" ] \n    if tm : \n        tm . show ( ) \n    vm = model [ \"validation_metrics\" ] \n    if vm : \n        vm . show ( ) \n    xm = model [ \"cross_validation_metrics\" ] \n    if xm : \n        xm . show ( ) \n    xms = model [ \"cross_validation_metrics_summary\" ] \n    if xms : \n        xms . show ( ) \n    if \"scoring_history\" in model and model [ \"scoring_history\" ] : \n        model [ \"scoring_history\" ] . show ( ) \n    if \"variable_importances\" in model and model [ \"variable_importances\" ] : \n        model [ \"variable_importances\" ] . show ( ) "}
{"1538": "\nimport pprint \ndef varimp ( self , use_pandas = False ) : \n    model = self . _model_json [ \"output\" ] \n    if self . algo == 'glm' or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : \n        if self . algo == 'glm' : \n            tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values \n            maxVal = 0 \n            sum = 0 \n            for item in tempvals : \n                sum = sum + item [ 1 ] \n                if item [ 1 ] > maxVal : \n                    maxVal = item [ 1 ] \n            vals = [ ] \n            for item in tempvals : \n                tempT = ( item [ 0 ] , item [ 1 ] , item [ 1 ] / maxVal , item [ 1 ] / sum ) \n                vals . append ( tempT ) \n            header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] \n        else : \n            vals = model [ \"variable_importances\" ] . cell_values \n            header = model [ \"variable_importances\" ] . col_header \n        if use_pandas and can_use_pandas ( ) : \n            import pandas \n            return pandas . DataFrame ( vals , columns = header ) \n        else : \n            return vals \n    else : \n        pprint . pprint ( \"Warning: This model doesn't have variable importances\" ) "}
{"1551": "\nimport pprint \ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        pprint . pprint ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            pprint . pprint ( self . _table_description ) \n    pprint . pprint ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if nr > 20 : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if nr > 20 and can_use_pandas ( ) : \n        pprint . pprint ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1552": "\nimport pprint \ndef start ( jar_path = None , nthreads = - 1 , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and min_mem_size > max_mem_size : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( port [ - 1 ] == \"+\" and port [ : - 1 ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - 1 ] ) \n            port = 0 \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        pprint . pprint ( \"Attempting to start a local H2O server...\" ) \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        pprint . pprint ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1570": "\nimport pprint \ndef print_dict ( ) : \n    global g_ok_java_messages \n    global g_java_messages_to_ignore_text_filename \n    allKeys = sorted ( g_ok_java_messages . keys ( ) ) \n    with open ( g_java_messages_to_ignore_text_filename , 'w' ) as ofile : \n        for key in allKeys : \n            for mess in g_ok_java_messages [ key ] : \n                ofile . write ( 'KeyName: ' + key + '\\n' ) \n                ofile . write ( 'IgnoredMessage: ' + mess + '\\n' ) \n            pprint . pprint ( 'KeyName: ' , key ) \n            pprint . pprint ( 'IgnoredMessage: ' , g_ok_java_messages [ key ] ) \n            pprint . pprint ( '\\n' ) "}
{"1572": "\nimport pprint \ndef usage ( ) : \n    global g_script_name \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"Usage:  \" + g_script_name + \" [...options...]\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"     --help print out this help menu and show all the valid flags and inputs.\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"    --inputfileadd filename where the new java messages to ignore are stored in.\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"    --inputfilerm filename where the java messages are removed from the ignored list.\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"    --loadjavamessage filename pickle file that stores the dict structure containing java messages to include.\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"    --savejavamessage filename pickle file that saves the final dict structure after update.\" ) \n    pprint . pprint ( \"\" ) \n    pprint . pprint ( \"    --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file.\" ) \n    pprint . pprint ( \"\" ) \n    sys . exit ( 1 ) "}
{"1575": "\nimport pprint \ndef main ( ) : \n    for filename in locate_files ( ROOT_DIR ) : \n        pprint . pprint ( \"Processing %s\" % filename ) \n        with open ( filename , \"rt\" ) as f : \n            tokens = list ( tokenize . generate_tokens ( f . readline ) ) \n            text1 = tokenize . untokenize ( tokens ) \n            ntokens = normalize_tokens ( tokens ) \n            text2 = tokenize . untokenize ( ntokens ) \n            assert text1 == text2 "}
{"1578": "\nimport pprint \ndef extractPrintSaveIntermittens ( ) : \n    global g_summary_dict_intermittents \n    localtz = time . tzname [ 0 ] \n    for ind in range ( len ( g_summary_dict_all [ \"TestName\" ] ) ) : \n        if g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] >= g_threshold_failure : \n            addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) \n    if len ( g_summary_dict_intermittents [ \"TestName\" ] ) > 0 : \n        json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) \n        with open ( g_summary_csv_filename , 'w' ) as summaryFile : \n            for ind in range ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) ) : \n                testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] \n                numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] \n                firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) \n                recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) \n                eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ 0 ] ) \n                summaryFile . write ( eachTest ) \n                pprint . pprint ( \"Intermittent: {0}, Last failed: {1}, Failed {2} times since \" \"{3}\" . format ( testName , recentFailStr , numberFailure , firstFailedStr ) ) "}
{"1579": "\nimport pprint \ndef plot ( self , type = \"roc\" , server = False ) : \n    assert_is_type ( type , \"roc\" ) \n    try : \n        imp . find_module ( 'matplotlib' ) \n        import matplotlib \n        if server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        pprint . pprint ( \"matplotlib is required for this function!\" ) \n        return \n    if type == \"roc\" : \n        plt . xlabel ( 'False Positive Rate (FPR)' ) \n        plt . ylabel ( 'True Positive Rate (TPR)' ) \n        plt . title ( 'ROC Curve' ) \n        plt . text ( 0.5 , 0.5 , r'AUC={0:.4f}' . format ( self . _metric_json [ \"AUC\" ] ) ) \n        plt . plot ( self . fprs , self . tprs , 'b--' ) \n        plt . axis ( [ 0 , 1 , 0 , 1 ] ) \n        if not server : \n            plt . show ( ) "}
{"1581": "\nimport pprint \ndef available ( ) : \n    builder_json = h2o . api ( \"GET /3/ModelBuilders\" , data = { \"algo\" : \"deepwater\" } ) \n    visibility = builder_json [ \"model_builders\" ] [ \"deepwater\" ] [ \"visibility\" ] \n    if visibility == \"Experimental\" : \n        pprint . pprint ( \"Cannot build a Deep Water model - no backend found.\" ) \n        return False \n    else : \n        return True "}
{"1587": "\nimport pprint \ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while True : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if login == username . strip ( ) : \n                    return login , password \n        pprint . pprint ( \"Which account do you want to use? (Type number)\" ) \n        for ind , ( login , password ) in enumerate ( lines ) : \n            pprint . pprint ( \"%d: %s\" % ( ind + 1 , login ) ) \n        pprint . pprint ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) \n        pprint . pprint ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if ind == 0 : \n                add_credentials ( ) \n                continue \n            elif ind == - 1 : \n                delete_credentials ( ) \n                check_secret ( ) \n                continue \n            elif 0 <= ind - 1 < len ( lines ) : \n                return lines [ ind - 1 ] \n        except Exception : \n            pprint . pprint ( \"Wrong input, enter the number of the account to use.\" ) "}
{"1591": "\nimport pprint \ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if sys . version_info [ 0 ] < 3 : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        pprint . pprint ( str ( exception ) ) \n        return [ ] "}
{"1601": "\nimport pprint \ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            pprint . pprint ( 'request:' ) \n            try : \n                pprint . pprint ( request . body ) \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        elif resp . length > 0 : \n            respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            pprint . pprint ( 'response:' ) \n            try : \n                pprint . pprint ( respbody ) \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if status == 307 : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if status >= 300 : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1739": "\nimport pprint \ndef travis_build_package ( ) : \n    travis_tag = os . environ . get ( 'TRAVIS_TAG' ) \n    if not travis_tag : \n        pprint . pprint ( \"TRAVIS_TAG environment variable is not present\" ) \n        return \"TRAVIS_TAG environment variable is not present\" \n    try : \n        name , version = travis_tag . split ( \"_\" ) \n    except ValueError : \n        pprint . pprint ( \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\" . format ( travis_tag ) ) \n        return \"TRAVIS_TAG is not '<package_name>_<version>' (tag is: {})\" . format ( travis_tag ) \n    try : \n        version = Version ( version ) \n    except InvalidVersion : \n        pprint . pprint ( \"Version must be a valid PEP440 version (version is: {})\" . format ( version ) ) \n        return \"Version must be a valid PEP440 version (version is: {})\" . format ( version ) \n    if name . lower ( ) in OMITTED_RELEASE_PACKAGES : \n        pprint . pprint ( \"The input package {} has been disabled for release from Travis.CI.\" . format ( name ) ) \n        return \n    abs_dist_path = Path ( os . environ [ 'TRAVIS_BUILD_DIR' ] , 'dist' ) \n    create_package ( name , str ( abs_dist_path ) ) \n    pprint . pprint ( \"Produced:\\n{}\" . format ( list ( abs_dist_path . glob ( '*' ) ) ) ) \n    pattern = \"*{}*\" . format ( version ) \n    packages = list ( abs_dist_path . glob ( pattern ) ) \n    if not packages : \n        return \"Package version does not match tag {}, abort\" . format ( version ) \n    pypi_server = os . environ . get ( \"PYPI_SERVER\" , \"default PyPI server\" ) \n    pprint . pprint ( \"Package created as expected and will be pushed to {}\" . format ( pypi_server ) ) "}
{"1980": "\nimport pprint \ndef showCode ( self , width = 80 ) : \n    symbolStrings = [ ( self . bitPattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] \n    leftColWidth , rightColWidth = map ( max , map ( map , repeat ( len ) , zip ( * symbolStrings ) ) ) \n    colwidth = leftColWidth + rightColWidth \n    columns = 81 // ( colwidth + 2 ) \n    rows = - ( - len ( symbolStrings ) // columns ) \n    def justify ( bs ) : \n        b , s = bs \n        return b . rjust ( leftColWidth ) + ':' + s . ljust ( rightColWidth ) \n    for i in range ( rows ) : \n        pprint . pprint ( ' ' . join ( map ( justify , symbolStrings [ i : : rows ] ) ) . rstrip ( ) ) "}
{"1991": "\nimport pprint \ndef processStream ( self ) : \n    pprint . pprint ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10 ) ) \n    pprint . pprint ( 'Stream header' . center ( 60 , '-' ) ) \n    self . windowSize = self . verboseRead ( WindowSizeAlphabet ( ) ) \n    pprint . pprint ( 'Metablock header' . center ( 60 , '=' ) ) \n    self . ISLAST = False \n    self . output = bytearray ( ) \n    while not self . ISLAST : \n        self . ISLAST = self . verboseRead ( BoolCode ( 'LAST' , description = \"Last block\" ) ) \n        if self . ISLAST : \n            if self . verboseRead ( BoolCode ( 'EMPTY' , description = \"Empty block\" ) ) : \n                break \n        if self . metablockLength ( ) : \n            continue \n        if not self . ISLAST and self . uncompressed ( ) : \n            continue \n        pprint . pprint ( 'Block type descriptors' . center ( 60 , '-' ) ) \n        self . numberOfBlockTypes = { } \n        self . currentBlockCounts = { } \n        self . blockTypeCodes = { } \n        self . blockCountCodes = { } \n        for blockType in ( L , I , D ) : \n            self . blockType ( blockType ) \n        pprint . pprint ( 'Distance code parameters' . center ( 60 , '-' ) ) \n        self . NPOSTFIX , self . NDIRECT = self . verboseRead ( DistanceParamAlphabet ( ) ) \n        self . readLiteralContextModes ( ) \n        pprint . pprint ( 'Context maps' . center ( 60 , '-' ) ) \n        self . cmaps = { } \n        numberOfTrees = { I : self . numberOfBlockTypes [ I ] } \n        for blockType in ( L , D ) : \n            numberOfTrees [ blockType ] = self . contextMap ( blockType ) \n        pprint . pprint ( 'Prefix code lists' . center ( 60 , '-' ) ) \n        self . prefixCodes = { } \n        for blockType in ( L , I , D ) : \n            self . readPrefixArray ( blockType , numberOfTrees [ blockType ] ) \n        self . metablock ( ) "}
{"1992": "\nimport pprint \ndef metablockLength ( self ) : \n    self . MLEN = self . verboseRead ( MetablockLengthAlphabet ( ) ) \n    if self . MLEN : \n        return False \n    self . verboseRead ( ReservedAlphabet ( ) ) \n    MSKIP = self . verboseRead ( SkipLengthAlphabet ( ) ) \n    self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n    self . stream . pos += 8 * MSKIP \n    pprint . pprint ( \"Skipping to {:x}\" . format ( self . stream . pos >> 3 ) ) \n    return True "}
{"1993": "\nimport pprint \ndef uncompressed ( self ) : \n    ISUNCOMPRESSED = self . verboseRead ( BoolCode ( 'UNCMPR' , description = 'Is uncompressed?' ) ) \n    if ISUNCOMPRESSED : \n        self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n        pprint . pprint ( 'Uncompressed data:' ) \n        self . output += self . stream . readBytes ( self . MLEN ) \n        pprint . pprint ( outputFormatter ( self . output [ - self . MLEN : ] ) ) \n    return ISUNCOMPRESSED "}
{"2008": "\nimport pprint \ndef from_samp ( username = None , password = None ) : \n    pprint . pprint ( \"Waiting for SAMP message...\" ) \n    import vaex . samp \n    t = vaex . samp . single_table ( username = username , password = password ) \n    return from_astropy_table ( t . to_table ( ) ) "}
{"2012": "\nimport pprint \ndef from_pandas ( df , name = \"pandas\" , copy_index = True , index_name = \"index\" ) : \n    import six \n    vaex_df = vaex . dataframe . DataFrameArrays ( name ) \n    def add ( name , column ) : \n        values = column . values \n        try : \n            vaex_df . add_column ( name , values ) \n        except Exception as e : \n            pprint . pprint ( \"could not convert column %s, error: %r, will try to convert it to string\" % ( name , e ) ) \n            try : \n                values = values . astype ( \"S\" ) \n                vaex_df . add_column ( name , values ) \n            except Exception as e : \n                pprint . pprint ( \"Giving up column %s, error: %r\" % ( name , e ) ) \n    for name in df . columns : \n        add ( name , df [ name ] ) \n    if copy_index : \n        add ( index_name , df . index ) \n    return vaex_df "}
{"2020": "\nimport pprint \ndef delayed ( f ) : \n    def wrapped ( * args , ** kwargs ) : \n        key_promise = list ( [ ( key , promisify ( value ) ) for key , value in kwargs . items ( ) ] ) \n        arg_promises = list ( [ promisify ( value ) for value in args ] ) \n        kwarg_promises = list ( [ promise for key , promise in key_promise ] ) \n        promises = arg_promises + kwarg_promises \n        for promise in promises : \n            def echo_error ( exc , promise = promise ) : \n                pprint . pprint ( \"error with \" , promise , \"exception is\" , exc ) \n            def echo ( value , promise = promise ) : \n                pprint . pprint ( \"done with \" , repr ( promise ) , \"value is\" , value ) \n        allarguments = aplus . listPromise ( * promises ) \n        def call ( _ ) : \n            kwargs_real = { key : promise . get ( ) for key , promise in key_promise } \n            args_real = list ( [ promise . get ( ) for promise in arg_promises ] ) \n            return f ( * args_real , ** kwargs_real ) \n        def error ( exc ) : \n            pprint . pprint ( \"error\" , exc ) \n            raise exc \n        return allarguments . then ( call , error ) \n    return wrapped "}
{"2023": "\nimport pprint \ndef sort ( self , Ncol , order ) : \n    self . emit ( QtCore . SIGNAL ( \"layoutAboutToBeChanged()\" ) ) \n    if Ncol == 0 : \n        pprint . pprint ( \"by name\" ) \n        sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) \n        pprint . pprint ( sortlist ) \n        sortlist . sort ( key = operator . itemgetter ( 0 ) ) \n        pprint . pprint ( sortlist ) \n        self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) \n        pprint . pprint ( ( self . indices ) ) \n    if Ncol == 1 : \n        if None not in self . ranking : \n            sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) \n            sortlist . sort ( key = operator . itemgetter ( 0 ) ) \n            self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) \n        else : \n            self . indices = list ( range ( len ( self . pairs ) ) ) \n        pprint . pprint ( ( self . indices ) ) \n    if order == QtCore . Qt . DescendingOrder : \n        self . indices . reverse ( ) \n    pprint . pprint ( ( self . indices ) ) \n    self . emit ( QtCore . SIGNAL ( \"layoutChanged()\" ) ) "}
{"2075": "\nimport pprint \ndef cat ( self , i1 , i2 , format = 'html' ) : \n    from IPython import display \n    if format == 'html' : \n        output = self . _as_html_table ( i1 , i2 ) \n        display . display ( display . HTML ( output ) ) \n    else : \n        output = self . _as_table ( i1 , i2 , format = format ) \n        pprint . pprint ( output ) "}
{"2152": "\nimport pprint \ndef show_versions ( ) : \n    core_deps = [ 'audioread' , 'numpy' , 'scipy' , 'sklearn' , 'joblib' , 'decorator' , 'six' , 'soundfile' , 'resampy' , 'numba' ] \n    extra_deps = [ 'numpydoc' , 'sphinx' , 'sphinx_rtd_theme' , 'sphinxcontrib.versioning' , 'sphinx-gallery' , 'pytest' , 'pytest-mpl' , 'pytest-cov' , 'matplotlib' ] \n    pprint . pprint ( 'INSTALLED VERSIONS' ) \n    pprint . pprint ( '------------------' ) \n    pprint . pprint ( 'python: {}\\n' . format ( sys . version ) ) \n    pprint . pprint ( 'librosa: {}\\n' . format ( version ) ) \n    for dep in core_deps : \n        pprint . pprint ( '{}: {}' . format ( dep , __get_mod_version ( dep ) ) ) \n    pprint . pprint ( '' ) \n    for dep in extra_deps : \n        pprint . pprint ( '{}: {}' . format ( dep , __get_mod_version ( dep ) ) ) \n    pass "}
{"2155": "\nimport pprint \ndef beat_track ( input_file , output_csv ) : \n    pprint . pprint ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file , sr = 22050 ) \n    hop_length = 512 \n    pprint . pprint ( 'Tracking beats' ) \n    tempo , beats = librosa . beat . beat_track ( y = y , sr = sr , hop_length = hop_length ) \n    pprint . pprint ( 'Estimated tempo: {:0.2f} beats per minute' . format ( tempo ) ) \n    beat_times = librosa . frames_to_time ( beats , sr = sr , hop_length = hop_length ) \n    pprint . pprint ( 'Saving output to ' , output_csv ) \n    librosa . output . times_csv ( output_csv , beat_times ) \n    pprint . pprint ( 'done!' ) "}
{"2156": "\nimport pprint \ndef adjust_tuning ( input_file , output_file ) : \n    pprint . pprint ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file ) \n    pprint . pprint ( 'Separating harmonic component ... ' ) \n    y_harm = librosa . effects . harmonic ( y ) \n    pprint . pprint ( 'Estimating tuning ... ' ) \n    tuning = librosa . estimate_tuning ( y = y_harm , sr = sr ) \n    pprint . pprint ( '{:+0.2f} cents' . format ( 100 * tuning ) ) \n    pprint . pprint ( 'Applying pitch-correction of {:+0.2f} cents' . format ( - 100 * tuning ) ) \n    y_tuned = librosa . effects . pitch_shift ( y , sr , - tuning ) \n    pprint . pprint ( 'Saving tuned audio to: ' , output_file ) \n    librosa . output . write_wav ( output_file , y_tuned , sr ) "}
{"2216": "\nimport pprint \ndef hpss_beats ( input_file , output_csv ) : \n    pprint . pprint ( 'Loading  ' , input_file ) \n    y , sr = librosa . load ( input_file ) \n    pprint . pprint ( 'Harmonic-percussive separation ... ' ) \n    y = librosa . effects . percussive ( y ) \n    pprint . pprint ( 'Tracking beats on percussive component' ) \n    onset_env = librosa . onset . onset_strength ( y = y , sr = sr , hop_length = HOP_LENGTH , n_fft = N_FFT , aggregate = np . median ) \n    tempo , beats = librosa . beat . beat_track ( onset_envelope = onset_env , sr = sr , hop_length = HOP_LENGTH ) \n    beat_times = librosa . frames_to_time ( beats , sr = sr , hop_length = HOP_LENGTH ) \n    pprint . pprint ( 'Saving beats to ' , output_csv ) \n    librosa . output . times_csv ( output_csv , beat_times ) "}
{"2260": "\nimport pprint \ndef stretch_demo ( input_file , output_file , speed ) : \n    pprint . pprint ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file ) \n    pprint . pprint ( 'Playing back at {:3.0f}% speed' . format ( speed * 100 ) ) \n    y_stretch = librosa . effects . time_stretch ( y , speed ) \n    pprint . pprint ( 'Saving stretched audio to: ' , output_file ) \n    librosa . output . write_wav ( output_file , y_stretch , sr ) "}
{"2262": "\nimport pprint \ndef hpss_demo ( input_file , output_harmonic , output_percussive ) : \n    pprint . pprint ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file ) \n    pprint . pprint ( 'Separating harmonics and percussives... ' ) \n    y_harmonic , y_percussive = librosa . effects . hpss ( y ) \n    pprint . pprint ( 'Saving harmonic audio to: ' , output_harmonic ) \n    librosa . output . write_wav ( output_harmonic , y_harmonic , sr ) \n    pprint . pprint ( 'Saving percussive audio to: ' , output_percussive ) \n    librosa . output . write_wav ( output_percussive , y_percussive , sr ) "}
{"2274": "\nimport pprint \ndef onset_detect ( input_file , output_csv ) : \n    pprint . pprint ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file , sr = 22050 ) \n    hop_length = 512 \n    pprint . pprint ( 'Detecting onsets...' ) \n    onsets = librosa . onset . onset_detect ( y = y , sr = sr , hop_length = hop_length ) \n    pprint . pprint ( \"Found {:d} onsets.\" . format ( onsets . shape [ 0 ] ) ) \n    onset_times = librosa . frames_to_time ( onsets , sr = sr , hop_length = hop_length ) \n    pprint . pprint ( 'Saving output to ' , output_csv ) \n    librosa . output . times_csv ( output_csv , onset_times ) \n    pprint . pprint ( 'done!' ) "}
{"2315": "\nimport pprint \ndef load_state_dict ( module , state_dict , strict = False , logger = None ) : \n    unexpected_keys = [ ] \n    own_state = module . state_dict ( ) \n    for name , param in state_dict . items ( ) : \n        if name not in own_state : \n            unexpected_keys . append ( name ) \n            continue \n        if isinstance ( param , torch . nn . Parameter ) : \n            param = param . data \n        try : \n            own_state [ name ] . copy_ ( param ) \n        except Exception : \n            raise RuntimeError ( 'While copying the parameter named {}, ' 'whose dimensions in the model are {} and ' 'whose dimensions in the checkpoint are {}.' . format ( name , own_state [ name ] . size ( ) , param . size ( ) ) ) \n    missing_keys = set ( own_state . keys ( ) ) - set ( state_dict . keys ( ) ) \n    err_msg = [ ] \n    if unexpected_keys : \n        err_msg . append ( 'unexpected key in source state_dict: {}\\n' . format ( ', ' . join ( unexpected_keys ) ) ) \n    if missing_keys : \n        err_msg . append ( 'missing keys in source state_dict: {}\\n' . format ( ', ' . join ( missing_keys ) ) ) \n    err_msg = '\\n' . join ( err_msg ) \n    if err_msg : \n        if strict : \n            raise RuntimeError ( err_msg ) \n        elif logger is not None : \n            logger . warn ( err_msg ) \n        else : \n            pprint . pprint ( err_msg ) "}
{"2325": "\nimport pprint \ndef convert_video ( in_file , out_file , print_cmd = False , pre_options = '' , ** kwargs ) : \n    options = [ ] \n    for k , v in kwargs . items ( ) : \n        if isinstance ( v , bool ) : \n            if v : \n                options . append ( '-{}' . format ( k ) ) \n        elif k == 'log_level' : \n            assert v in [ 'quiet' , 'panic' , 'fatal' , 'error' , 'warning' , 'info' , 'verbose' , 'debug' , 'trace' ] \n            options . append ( '-loglevel {}' . format ( v ) ) \n        else : \n            options . append ( '-{} {}' . format ( k , v ) ) \n    cmd = 'ffmpeg -y {} -i {} {} {}' . format ( pre_options , in_file , ' ' . join ( options ) , out_file ) \n    if print_cmd : \n        pprint . pprint ( cmd ) \n    subprocess . call ( cmd , shell = True ) "}
{"2341": "\nimport pprint \ndef check_prerequisites ( prerequisites , checker , msg_tmpl = 'Prerequisites \"{}\" are required in method \"{}\" but not ' 'found, please install them first.' ) : \n    def wrap ( func ) : \n        \n        @ functools . wraps ( func ) \n        def wrapped_func ( * args , ** kwargs ) : \n            requirements = [ prerequisites ] if isinstance ( prerequisites , str ) else prerequisites \n            missing = [ ] \n            for item in requirements : \n                if not checker ( item ) : \n                    missing . append ( item ) \n            if missing : \n                pprint . pprint ( msg_tmpl . format ( ', ' . join ( missing ) , func . __name__ ) ) \n                raise RuntimeError ( 'Prerequisites not meet.' ) \n            else : \n                return func ( * args , ** kwargs ) \n        return wrapped_func \n    return wrap "}
{"2457": "\nimport pprint \ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if state . ndim == 2 and len ( state ) == 4 and d0 == 2 and d1 == 2 : \n        return __eof_qubit ( state ) \n    elif state . ndim == 1 : \n        if d0 < d1 : \n            tr = [ 1 ] \n        else : \n            tr = [ 0 ] \n        state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n        return entropy ( state ) \n    else : \n        pprint . pprint ( 'Input must be a state-vector or 2-qubit density matrix.' ) \n    return None "}
{"2688": "\nimport pprint \ndef to_string ( self , indent ) : \n    ind = indent * ' ' \n    pprint . pprint ( ind , 'qreg' ) \n    self . children [ 0 ] . to_string ( indent + 3 ) "}
{"2743": "\nimport pprint \ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        pprint . pprint ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2 ) \n        else : \n            if not _interval_set : \n                interval = 2 \n        if len ( msg ) < msg_len : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif len ( msg ) > msg_len : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            pprint . pprint ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        pprint . pprint ( '' , file = output ) "}
{"3409": "\nimport pprint \ndef complex_input_with_reference ( ) : \n    pprint . pprint ( \"\\ncomplex_input_with_reference ...\" ) \n    wps = WebProcessingService ( 'http://localhost:8094/wps' , verbose = verbose ) \n    processid = 'wordcount' \n    textdoc = ComplexDataInput ( \"http://www.gutenberg.org/files/28885/28885-h/28885-h.htm\" ) \n    inputs = [ ( \"text\" , textdoc ) ] \n    outputs = [ ( \"output\" , True , 'some/mime-type' ) ] \n    execution = wps . execute ( processid , inputs , output = outputs ) \n    monitorExecution ( execution ) \n    pprint . pprint ( 'percent complete' , execution . percentCompleted ) \n    pprint . pprint ( 'status message' , execution . statusMessage ) \n    for output in execution . processOutputs : \n        pprint . pprint ( 'identifier=%s, dataType=%s, data=%s, reference=%s' % ( output . identifier , output . dataType , output . data , output . reference ) ) "}
{"3473": "\nimport pprint \ndef echo ( self , s , file = sys . stdout , end = \"\\n\" ) : \n    p = getattr ( self , \"_progress_bar\" , None ) \n    if p is not None : \n        p . write ( s , file = file , end = \"\\n\" ) \n        return \n    pprint . pprint ( s , file = file , end = \"\\n\" ) "}
{"3578": "\nimport pprint \ndef decompose ( hangul_letter ) : \n    from . import checker \n    if len ( hangul_letter ) < 1 : \n        raise NotLetterException ( '' ) \n    elif not checker . is_hangul ( hangul_letter ) : \n        raise NotHangulException ( '' ) \n    if hangul_letter in CHO : \n        return hangul_letter , '' , '' \n    if hangul_letter in JOONG : \n        return '' , hangul_letter , '' \n    if hangul_letter in JONG : \n        return '' , '' , hangul_letter \n    code = hangul_index ( hangul_letter ) \n    cho , joong , jong = decompose_index ( code ) \n    if cho < 0 : \n        cho = 0 \n    try : \n        return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] \n    except : \n        pprint . pprint ( \"%d / %d  / %d\" % ( cho , joong , jong ) ) \n        pprint . pprint ( \"%s / %s \" % ( JOONG [ joong ] . encode ( \"utf8\" ) , JONG [ jong ] . encode ( 'utf8' ) ) ) \n        raise Exception ( ) "}
{"3632": "\nimport pprint \ndef display_messages ( self , layout ) : \n    pprint . pprint ( json . dumps ( self . messages , indent = 4 ) , file = self . out ) "}
{"3668": "\nimport pprint \ndef _display ( self , layout ) : \n    pprint . pprint ( file = self . out ) \n    TextWriter ( ) . format ( layout , self . out ) "}
{"3687": "\nimport pprint \ndef help_message ( self , msgids ) : \n    for msgid in msgids : \n        try : \n            for message_definition in self . get_message_definitions ( msgid ) : \n                pprint . pprint ( message_definition . format_help ( checkerref = True ) ) \n                pprint . pprint ( \"\" ) \n        except UnknownMessageError as ex : \n            pprint . pprint ( ex ) \n            pprint . pprint ( \"\" ) \n            continue "}
{"3688": "\nimport pprint \ndef list_messages ( self ) : \n    messages = sorted ( self . _messages_definitions . values ( ) , key = lambda m : m . msgid ) \n    for message in messages : \n        if not message . may_be_emitted ( ) : \n            continue \n        pprint . pprint ( message . format_help ( checkerref = False ) ) \n    pprint . pprint ( \"\" ) "}
{"3695": "\nimport pprint \ndef set_option ( self , optname , value , action = None , optdict = None ) : \n    if optname in self . _options_methods or optname in self . _bw_options_methods : \n        if value : \n            try : \n                meth = self . _options_methods [ optname ] \n            except KeyError : \n                meth = self . _bw_options_methods [ optname ] \n                warnings . warn ( \"%s is deprecated, replace it by %s\" % ( optname , optname . split ( \"-\" ) [ 0 ] ) , DeprecationWarning , ) \n            value = utils . _check_csv ( value ) \n            if isinstance ( value , ( list , tuple ) ) : \n                for _id in value : \n                    meth ( _id , ignore_unknown = True ) \n            else : \n                meth ( value ) \n            return \n    elif optname == \"output-format\" : \n        self . _reporter_name = value \n        if self . _reporters : \n            self . _load_reporter ( ) \n    try : \n        checkers . BaseTokenChecker . set_option ( self , optname , value , action , optdict ) \n    except config . UnsupportedAction : \n        pprint . pprint ( \"option %s can't be read from config file\" % optname , file = sys . stderr ) "}
{"3709": "\nimport pprint \ndef cb_list_groups ( self , * args , ** kwargs ) : \n    for check in self . linter . get_checker_names ( ) : \n        pprint . pprint ( check ) \n    sys . exit ( 0 ) "}
{"3714": "\nimport pprint \ndef register_plugins ( linter , directory ) : \n    imported = { } \n    for filename in listdir ( directory ) : \n        base , extension = splitext ( filename ) \n        if base in imported or base == \"__pycache__\" : \n            continue \n        if ( extension in PY_EXTS and base != \"__init__\" or ( not extension and isdir ( join ( directory , base ) ) ) ) : \n            try : \n                module = modutils . load_module_from_file ( join ( directory , filename ) ) \n            except ValueError : \n                continue \n            except ImportError as exc : \n                pprint . pprint ( \"Problem importing module %s: %s\" % ( filename , exc ) , file = sys . stderr ) \n            else : \n                if hasattr ( module , \"register\" ) : \n                    module . register ( linter ) \n                    imported [ base ] = 1 "}
{"3717": "\nimport pprint \ndef format_section ( stream , section , options , doc = None ) : \n    if doc : \n        pprint . pprint ( _comment ( doc ) , file = stream ) \n    pprint . pprint ( \"[%s]\" % section , file = stream ) \n    _ini_format ( stream , options ) "}
{"3718": "\nimport pprint \ndef _ini_format ( stream , options ) : \n    for optname , optdict , value in options : \n        value = _format_option_value ( optdict , value ) \n        help_opt = optdict . get ( \"help\" ) \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79 , indent = \"# \" ) \n            pprint . pprint ( file = stream ) \n            pprint . pprint ( help_opt , file = stream ) \n        else : \n            pprint . pprint ( file = stream ) \n        if value is None : \n            pprint . pprint ( \"#%s=\" % optname , file = stream ) \n        else : \n            value = str ( value ) . strip ( ) \n            if re . match ( r\"^([\\w-]+,)+[\\w-]+$\" , str ( value ) ) : \n                separator = \"\\n \" + \" \" * len ( optname ) \n                value = separator . join ( x + \",\" for x in str ( value ) . split ( \",\" ) ) \n                value = value [ : - 1 ] \n            pprint . pprint ( \"%s=%s\" % ( optname , value ) , file = stream ) "}
{"3756": "\nimport pprint \ndef _display_sims ( self , sims ) : \n    nb_lignes_dupliquees = 0 \n    for num , couples in sims : \n        pprint . pprint ( ) \n        pprint . pprint ( num , \"similar lines in\" , len ( couples ) , \"files\" ) \n        couples = sorted ( couples ) \n        for lineset , idx in couples : \n            pprint . pprint ( \"==%s:%s\" % ( lineset . name , idx ) ) \n        for line in lineset . _real_lines [ idx : idx + num ] : \n            pprint . pprint ( \"  \" , line . rstrip ( ) ) \n        nb_lignes_dupliquees += num * ( len ( couples ) - 1 ) \n    nb_total_lignes = sum ( [ len ( lineset ) for lineset in self . linesets ] ) \n    pprint . pprint ( \"TOTAL lines=%s duplicates=%s percent=%.2f\" % ( nb_total_lignes , nb_lignes_dupliquees , nb_lignes_dupliquees * 100.0 / nb_total_lignes , ) ) "}
{"3807": "\nimport pprint \ndef _check_graphviz_available ( output_format ) : \n    try : \n        subprocess . call ( [ \"dot\" , \"-V\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    except OSError : \n        pprint . pprint ( \"The output format '%s' is currently not available.\\n\" \"Please install 'Graphviz' to have other output formats \" \"than 'dot' or 'vcg'.\" % output_format ) \n        sys . exit ( 32 ) "}
{"3808": "\nimport pprint \ndef run ( self , args ) : \n    if not args : \n        pprint . pprint ( self . help ( ) ) \n        return 1 \n    sys . path . insert ( 0 , os . getcwd ( ) ) \n    try : \n        project = project_from_files ( args , project_name = self . config . project , black_list = self . config . black_list , ) \n        linker = Linker ( project , tag = True ) \n        handler = DiadefsHandler ( self . config ) \n        diadefs = handler . get_diadefs ( project , linker ) \n    finally : \n        sys . path . pop ( 0 ) \n    if self . config . output_format == \"vcg\" : \n        writer . VCGWriter ( self . config ) . write ( diadefs ) \n    else : \n        writer . DotWriter ( self . config ) . write ( diadefs ) \n    return 0 "}
{"3816": "\nimport pprint \ndef lint ( filename , options = ( ) ) : \n    full_path = osp . abspath ( filename ) \n    parent_path = osp . dirname ( full_path ) \n    child_path = osp . basename ( full_path ) \n    while parent_path != \"/\" and osp . exists ( osp . join ( parent_path , \"__init__.py\" ) ) : \n        child_path = osp . join ( osp . basename ( parent_path ) , child_path ) \n        parent_path = osp . dirname ( parent_path ) \n    run_cmd = \"import sys; from pylint.lint import Run; Run(sys.argv[1:])\" \n    cmd = ( [ sys . executable , \"-c\" , run_cmd ] + [ \"--msg-template\" , \"{path}:{line}: {category} ({msg_id}, {symbol}, {obj}) {msg}\" , \"-r\" , \"n\" , child_path , ] + list ( options ) ) \n    process = Popen ( cmd , stdout = PIPE , cwd = parent_path , env = _get_env ( ) , universal_newlines = True ) \n    for line in process . stdout : \n        if line . startswith ( \"No config file found\" ) : \n            continue \n        parts = line . split ( \":\" ) \n        if parts and parts [ 0 ] == child_path : \n            line = \":\" . join ( [ filename ] + parts [ 1 : ] ) \n        pprint . pprint ( line , end = \" \" ) \n    process . wait ( ) \n    return process . returncode "}
{"3821": "\nimport pprint \ndef _rest_format_section ( stream , section , options , doc = None ) : \n    if section : \n        pprint . pprint ( \"%s\\n%s\" % ( section , \"'\" * len ( section ) ) , file = stream ) \n    if doc : \n        pprint . pprint ( normalize_text ( doc , line_len = 79 , indent = \"\" ) , file = stream ) \n        pprint . pprint ( file = stream ) \n    for optname , optdict , value in options : \n        help_opt = optdict . get ( \"help\" ) \n        pprint . pprint ( \":%s:\" % optname , file = stream ) \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79 , indent = \"  \" ) \n            pprint . pprint ( help_opt , file = stream ) \n        if value : \n            value = str ( _format_option_value ( optdict , value ) ) \n            pprint . pprint ( file = stream ) \n            pprint . pprint ( \"  Default: ``%s``\" % value . replace ( \"`` \" , \"```` ``\" ) , file = stream ) "}
{"3828": "\nimport pprint \ndef print_full_documentation ( self , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    pprint . pprint ( \"Pylint global options and switches\" , file = stream ) \n    pprint . pprint ( \"----------------------------------\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    pprint . pprint ( \"Pylint provides global options and switches.\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    by_checker = { } \n    for checker in self . get_checkers ( ) : \n        if checker . name == \"master\" : \n            if checker . options : \n                for section , options in checker . options_by_section ( ) : \n                    if section is None : \n                        title = \"General options\" \n                    else : \n                        title = \"%s options\" % section . capitalize ( ) \n                    pprint . pprint ( title , file = stream ) \n                    pprint . pprint ( \"~\" * len ( title ) , file = stream ) \n                    _rest_format_section ( stream , None , options ) \n                    pprint . pprint ( \"\" , file = stream ) \n        else : \n            name = checker . name \n            try : \n                by_checker [ name ] [ \"options\" ] += checker . options_and_values ( ) \n                by_checker [ name ] [ \"msgs\" ] . update ( checker . msgs ) \n                by_checker [ name ] [ \"reports\" ] += checker . reports \n            except KeyError : \n                by_checker [ name ] = { \"options\" : list ( checker . options_and_values ( ) ) , \"msgs\" : dict ( checker . msgs ) , \"reports\" : list ( checker . reports ) , } \n    pprint . pprint ( \"Pylint checkers' options and switches\" , file = stream ) \n    pprint . pprint ( \"-------------------------------------\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    pprint . pprint ( \"Pylint checkers can provide three set of features:\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    pprint . pprint ( \"* options that control their execution,\" , file = stream ) \n    pprint . pprint ( \"* messages that they can raise,\" , file = stream ) \n    pprint . pprint ( \"* reports that they can generate.\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    pprint . pprint ( \"Below is a list of all checkers and their features.\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    for checker , info in sorted ( by_checker . items ( ) ) : \n        self . _print_checker_doc ( checker , info , stream = stream ) "}
{"3829": "\nimport pprint \ndef _print_checker_doc ( checker_name , info , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    doc = info . get ( \"doc\" ) \n    module = info . get ( \"module\" ) \n    msgs = info . get ( \"msgs\" ) \n    options = info . get ( \"options\" ) \n    reports = info . get ( \"reports\" ) \n    checker_title = \"%s checker\" % ( checker_name . replace ( \"_\" , \" \" ) . title ( ) ) \n    if module : \n        pprint . pprint ( \".. _%s:\\n\" % module , file = stream ) \n    pprint . pprint ( checker_title , file = stream ) \n    pprint . pprint ( \"~\" * len ( checker_title ) , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    if module : \n        pprint . pprint ( \"This checker is provided by ``%s``.\" % module , file = stream ) \n    pprint . pprint ( \"Verbatim name of the checker is ``%s``.\" % checker_name , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) \n    if doc : \n        title = \"{} Documentation\" . format ( checker_title ) \n        pprint . pprint ( title , file = stream ) \n        pprint . pprint ( \"^\" * len ( title ) , file = stream ) \n        pprint . pprint ( cleandoc ( doc ) , file = stream ) \n        pprint . pprint ( \"\" , file = stream ) \n    if options : \n        title = \"{} Options\" . format ( checker_title ) \n        pprint . pprint ( title , file = stream ) \n        pprint . pprint ( \"^\" * len ( title ) , file = stream ) \n        _rest_format_section ( stream , None , options ) \n        pprint . pprint ( \"\" , file = stream ) \n    if msgs : \n        title = \"{} Messages\" . format ( checker_title ) \n        pprint . pprint ( title , file = stream ) \n        pprint . pprint ( \"^\" * len ( title ) , file = stream ) \n        for msgid , msg in sorted ( msgs . items ( ) , key = lambda kv : ( _MSG_ORDER . index ( kv [ 0 ] [ 0 ] ) , kv [ 1 ] ) ) : \n            msg = build_message_definition ( checker_name , msgid , msg ) \n            pprint . pprint ( msg . format_help ( checkerref = False ) , file = stream ) \n        pprint . pprint ( \"\" , file = stream ) \n    if reports : \n        title = \"{} Reports\" . format ( checker_title ) \n        pprint . pprint ( title , file = stream ) \n        pprint . pprint ( \"^\" * len ( title ) , file = stream ) \n        for report in reports : \n            pprint . pprint ( \":%s: %s\" % report [ : 2 ] , file = stream ) \n        pprint . pprint ( \"\" , file = stream ) \n    pprint . pprint ( \"\" , file = stream ) "}
{"3861": "\nimport pprint \ndef generate_config ( self , stream = None , skipsections = ( ) , encoding = None ) : \n    options_by_section = { } \n    sections = [ ] \n    for provider in self . options_providers : \n        for section , options in provider . options_by_section ( ) : \n            if section is None : \n                section = provider . name \n            if section in skipsections : \n                continue \n            options = [ ( n , d , v ) for ( n , d , v ) in options if d . get ( \"type\" ) is not None and not d . get ( \"deprecated\" ) ] \n            if not options : \n                continue \n            if section not in sections : \n                sections . append ( section ) \n            alloptions = options_by_section . setdefault ( section , [ ] ) \n            alloptions += options \n    stream = stream or sys . stdout \n    printed = False \n    for section in sections : \n        if printed : \n            pprint . pprint ( \"\\n\" , file = stream ) \n        utils . format_section ( stream , section . upper ( ) , sorted ( options_by_section [ section ] ) ) \n        printed = True "}
{"4121": "\nimport pprint \ndef _print_token_factory ( col ) : \n    def _helper ( msg ) : \n        style = style_from_dict ( { Token . Color : col , } ) \n        tokens = [ ( Token . Color , msg ) ] \n        print_tokens ( tokens , style = style ) \n    def _helper_no_terminal ( msg ) : \n        pprint . pprint ( msg ) \n    if sys . stdout . isatty ( ) : \n        return _helper \n    else : \n        return _helper_no_terminal "}
{"4199": "\nimport pprint \ndef color_values ( color ) : \n    this_dir = os . path . dirname ( os . path . realpath ( inspect . getsourcefile ( lambda : 0 ) ) ) \n    color_name_file = os . path . join ( this_dir , 'color_names.txt' ) \n    found = False \n    for line in open ( color_name_file , 'r' ) : \n        line = line . rstrip ( ) \n        if color . lower ( ) == line . split ( ) [ 0 ] : \n            red = line . split ( ) [ 2 ] \n            green = line . split ( ) [ 3 ] \n            blue = line . split ( ) [ 4 ] \n            found = True \n            break \n    if not found : \n        pprint . pprint ( 'Color name \"%s\" not found, using default (white)' % color ) \n        red = 255 \n        green = 255 \n        blue = 255 \n    return red , green , blue "}
{"4200": "\nimport pprint \ndef check_list ( var , num_terms ) : \n    if not isinstance ( var , list ) : \n        if isinstance ( var , tuple ) : \n            var = list ( var ) \n        else : \n            var = [ var ] \n        for _ in range ( 1 , num_terms ) : \n            var . append ( var [ 0 ] ) \n    if len ( var ) != num_terms : \n        pprint . pprint ( '\"%s\" has the wrong number of terms; it needs %s. Exiting ...' % ( var , num_terms ) ) \n        sys . exit ( 1 ) \n    return var "}
{"4202": "\nimport pprint \ndef write_filter ( script , filter_xml ) : \n    if isinstance ( script , mlx . FilterScript ) : \n        script . filters . append ( filter_xml ) \n    elif isinstance ( script , str ) : \n        script_file = open ( script , 'a' ) \n        script_file . write ( filter_xml ) \n        script_file . close ( ) \n    else : \n        pprint . pprint ( filter_xml ) \n    return None "}
{"4209": "\nimport pprint \ndef rotate ( script , axis = 'z' , angle = 0.0 ) : \n    angle = math . radians ( angle ) \n    if axis . lower ( ) == 'x' : \n        vert_function ( script , x_func = 'x' , y_func = 'y*cos({angle})-z*sin({angle})' . format ( angle = angle ) , z_func = 'y*sin({angle})+z*cos({angle})' . format ( angle = angle ) ) \n    elif axis . lower ( ) == 'y' : \n        vert_function ( script , x_func = 'z*sin({angle})+x*cos({angle})' . format ( angle = angle ) , y_func = 'y' , z_func = 'z*cos({angle})-x*sin({angle})' . format ( angle = angle ) ) \n    elif axis . lower ( ) == 'z' : \n        vert_function ( script , x_func = 'x*cos({angle})-y*sin({angle})' . format ( angle = angle ) , y_func = 'x*sin({angle})+y*cos({angle})' . format ( angle = angle ) , z_func = 'z' ) \n    else : \n        pprint . pprint ( 'Axis name is not valid; exiting ...' ) \n        sys . exit ( 1 ) \n    return None "}
{"4231": "\nimport pprint \ndef handle_error ( program_name , cmd , log = None ) : \n    pprint . pprint ( '\\nHouston, we have a problem.' , '\\n%s did not finish successfully. Review the log' % program_name , 'file and the input file(s) to see what went wrong.' ) \n    pprint . pprint ( '%s command: \"%s\"' % ( program_name , cmd ) ) \n    if log is not None : \n        pprint . pprint ( 'log: \"%s\"' % log ) \n    pprint . pprint ( 'Where do we go from here?' ) \n    pprint . pprint ( ' r  - retry running %s (probably after' % program_name , 'you\\'ve fixed any problems with the input files)' ) \n    pprint . pprint ( ' c  - continue on with the script (probably after' , 'you\\'ve manually re-run and generated the desired' , 'output file(s)' ) \n    pprint . pprint ( ' x  - exit, keeping the TEMP3D files and log' ) \n    pprint . pprint ( ' xd - exit, deleting the TEMP3D files and log' ) \n    while True : \n        choice = input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if choice == 'x' : \n        pprint . pprint ( 'Exiting ...' ) \n        sys . exit ( 1 ) \n    elif choice == 'xd' : \n        pprint . pprint ( 'Deleting TEMP3D* and log files and exiting ...' ) \n        util . delete_all ( 'TEMP3D*' ) \n        if log is not None : \n            os . remove ( log ) \n        sys . exit ( 1 ) \n    elif choice == 'c' : \n        pprint . pprint ( 'Continuing on ...' ) \n        break_now = True \n    elif choice == 'r' : \n        pprint . pprint ( 'Retrying %s cmd ...' % program_name ) \n        break_now = False \n    return break_now "}
{"4235": "\nimport pprint \ndef save_to_file ( self , script_file ) : \n    if not self . filters : \n        pprint . pprint ( 'WARNING: no filters to save to file!' ) \n    script_file_descriptor = open ( script_file , 'w' ) \n    script_file_descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) \n    script_file_descriptor . close ( ) "}
{"4246": "\nimport pprint \ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : \n    topology = { 'manifold' : True , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = False \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2 ] \n                if topology [ 'genus' ] != 'undefined' : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2 ] \n                if topology [ 'hole_num' ] == 'a' : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        elif print_output : \n            pprint . pprint ( '{:16} = {}' . format ( key , value ) ) \n    return topology "}
{"4247": "\nimport pprint \ndef parse_hausdorff ( ml_log , log = None , print_output = False ) : \n    hausdorff_distance = { \"min_distance\" : 0.0 , \"max_distance\" : 0.0 , \"mean_distance\" : 0.0 , \"rms_distance\" : 0.0 , \"number_points\" : 0 } \n    with open ( ml_log ) as fread : \n        result = fread . readlines ( ) \n        data = \"\" \n        for idx , line in enumerate ( result ) : \n            m = re . match ( r\"\\s*Sampled (\\d+) pts.*\" , line ) \n            if m is not None : \n                hausdorff_distance [ \"number_points\" ] = int ( m . group ( 1 ) ) \n            if 'Hausdorff Distance computed' in line : \n                data = result [ idx + 2 ] \n        m = re . match ( r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\" , data ) \n        hausdorff_distance [ \"min_distance\" ] = float ( m . group ( 1 ) ) \n        hausdorff_distance [ \"max_distance\" ] = float ( m . group ( 2 ) ) \n        hausdorff_distance [ \"mean_distance\" ] = float ( m . group ( 3 ) ) \n        hausdorff_distance [ \"rms_distance\" ] = float ( m . group ( 4 ) ) \n        for key , value in hausdorff_distance . items ( ) : \n            if log is not None : \n                log_file = open ( log , 'a' ) \n                log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n                log_file . close ( ) \n            elif print_output : \n                pprint . pprint ( '{:16} = {}' . format ( key , value ) ) \n        return hausdorff_distance "}
{"4259": "\nimport pprint \ndef polylinesort ( fbasename = None , log = None ) : \n    fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) \n    if fext != 'obj' : \n        pprint . pprint ( 'Input file must be obj. Exiting ...' ) \n        sys . exit ( 1 ) \n    fread = open ( fbasename , 'r' ) \n    first = True \n    polyline_vertices = [ ] \n    line_segments = [ ] \n    for line in fread : \n        element , x_co , y_co , z_co = line . split ( ) \n        if element == 'v' : \n            polyline_vertices . append ( [ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) \n        elif element == 'l' : \n            p1 = x_co \n            p2 = y_co \n            line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) \n    fread . close ( ) \n    if log is not None : \n        log_file = open ( log , 'a' ) \n        log_file . close ( ) \n    return None "}
{"4262": "\nimport pprint \ndef measure_dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , axis2 = None , offset2 = 0.0 , ml_version = ml_version ) : \n    axis1 = axis1 . lower ( ) \n    axis2 = axis2 . lower ( ) \n    ml_script1_file = 'TEMP3D_measure_dimension.mlx' \n    file_out = 'TEMP3D_measure_dimension.xyz' \n    ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) \n    compute . section ( ml_script1 , axis1 , offset1 , surface = True ) \n    compute . section ( ml_script1 , axis2 , offset2 , surface = False ) \n    layers . delete_lower ( ml_script1 ) \n    ml_script1 . save_to_file ( ml_script1_file ) \n    ml_script1 . run_script ( log = log , script_file = ml_script1_file ) \n    for val in ( 'x' , 'y' , 'z' ) : \n        if val not in ( axis1 , axis2 ) : \n            axis = val \n    axis_num = ord ( axis ) - ord ( 'x' ) \n    aabb = measure_aabb ( file_out , log ) \n    dimension = { 'min' : aabb [ 'min' ] [ axis_num ] , 'max' : aabb [ 'max' ] [ axis_num ] , 'length' : aabb [ 'size' ] [ axis_num ] , 'axis' : axis } \n    if log is None : \n        pprint . pprint ( '\\nFor file \"%s\"' % fbasename ) \n        pprint . pprint ( 'Dimension parallel to %s with %s=%s & %s=%s:' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) \n        pprint . pprint ( '  Min = %s, Max = %s, Total length = %s' % ( dimension [ 'min' ] , dimension [ 'max' ] , dimension [ 'length' ] ) ) \n    else : \n        log_file = open ( log , 'a' ) \n        log_file . write ( '\\nFor file \"%s\"\\n' % fbasename ) \n        log_file . write ( 'Dimension parallel to %s with %s=%s & %s=%s:\\n' % ( axis , axis1 , offset1 , axis2 , offset2 ) ) \n        log_file . write ( 'min = %s\\n' % dimension [ 'min' ] ) \n        log_file . write ( 'max = %s\\n' % dimension [ 'max' ] ) \n        log_file . write ( 'Total length = %s\\n' % dimension [ 'length' ] ) \n        log_file . close ( ) \n    return dimension "}
{"4307": "\nimport pprint \ndef start ( host , port , profiler_stats , dont_start_browser , debug_mode ) : \n    stats_handler = functools . partial ( StatsHandler , profiler_stats ) \n    if not debug_mode : \n        sys . stderr = open ( os . devnull , 'w' ) \n    pprint . pprint ( 'Starting HTTP server...' ) \n    if not dont_start_browser : \n        webbrowser . open ( 'http://{}:{}/' . format ( host , port ) ) \n    try : \n        StatsServer ( ( host , port ) , stats_handler ) . serve_forever ( ) \n    except KeyboardInterrupt : \n        pprint . pprint ( 'Stopping...' ) \n        sys . exit ( 0 ) "}
{"4322": "\nimport pprint \ndef run_profilers ( run_object , prof_config , verbose = False ) : \n    if len ( prof_config ) > len ( set ( prof_config ) ) : \n        raise AmbiguousConfigurationError ( 'Profiler configuration %s is ambiguous' % prof_config ) \n    available_profilers = { opt for opt , _ in _PROFILERS } \n    for option in prof_config : \n        if option not in available_profilers : \n            raise BadOptionError ( 'Unknown option: %s' % option ) \n    run_stats = OrderedDict ( ) \n    present_profilers = ( ( o , p ) for o , p in _PROFILERS if o in prof_config ) \n    for option , prof in present_profilers : \n        curr_profiler = prof ( run_object ) \n        if verbose : \n            pprint . pprint ( 'Running %s...' % curr_profiler . __class__ . __name__ ) \n        run_stats [ option ] = curr_profiler . run ( ) \n    return run_stats "}
{"4339": "\nimport pprint \ndef _fit ( self , Z , parameter_iterable ) : \n    self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) \n    cv = self . cv \n    cv = _check_cv ( cv , Z ) \n    if self . verbose > 0 : \n        if isinstance ( parameter_iterable , Sized ) : \n            n_candidates = len ( parameter_iterable ) \n            pprint . pprint ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( len ( cv ) , n_candidates , n_candidates * len ( cv ) ) ) \n    base_estimator = clone ( self . estimator ) \n    pre_dispatch = self . pre_dispatch \n    out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch , backend = \"threading\" ) ( delayed ( _fit_and_score ) ( clone ( base_estimator ) , Z , self . scorer_ , train , test , self . verbose , parameters , self . fit_params , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv ) \n    n_fits = len ( out ) \n    n_folds = len ( cv ) \n    scores = list ( ) \n    grid_scores = list ( ) \n    for grid_start in range ( 0 , n_fits , n_folds ) : \n        n_test_samples = 0 \n        score = 0 \n        all_scores = [ ] \n        for this_score , this_n_test_samples , _ , parameters in out [ grid_start : grid_start + n_folds ] : \n            all_scores . append ( this_score ) \n            if self . iid : \n                this_score *= this_n_test_samples \n                n_test_samples += this_n_test_samples \n            score += this_score \n        if self . iid : \n            score /= float ( n_test_samples ) \n        else : \n            score /= float ( n_folds ) \n        scores . append ( ( score , parameters ) ) \n        grid_scores . append ( _CVScoreTuple ( parameters , score , np . array ( all_scores ) ) ) \n    self . grid_scores_ = grid_scores \n    best = sorted ( grid_scores , key = lambda x : x . mean_validation_score , reverse = True ) [ 0 ] \n    self . best_params_ = best . parameters \n    self . best_score_ = best . mean_validation_score \n    if self . refit : \n        best_estimator = clone ( base_estimator ) . set_params ( ** best . parameters ) \n        best_estimator . fit ( Z , ** self . fit_params ) \n        self . best_estimator_ = best_estimator \n    return self "}
{"4467": "\nimport pprint \ndef get_studies_by_regions ( dataset , masks , threshold = 0.08 , remove_overlap = True , studies = None , features = None , regularization = \"scale\" ) : \n    import nibabel as nib \n    import os \n    try : \n        loaded_masks = [ nib . load ( os . path . relpath ( m ) ) for m in masks ] \n    except OSError : \n        pprint . pprint ( 'Error loading masks. Check the path' ) \n    grouped_ids = [ dataset . get_studies ( mask = m , activation_threshold = threshold ) for m in loaded_masks ] \n    flat_ids = reduce ( lambda a , b : a + b , grouped_ids ) \n    if remove_overlap : \n        import collections \n        flat_ids = [ id for ( id , count ) in collections . Counter ( flat_ids ) . items ( ) if count == 1 ] \n        grouped_ids = [ [ x for x in m if x in flat_ids ] for m in grouped_ids ] \n    y = [ [ idx ] * len ( ids ) for ( idx , ids ) in enumerate ( grouped_ids ) ] \n    y = reduce ( lambda a , b : a + b , y ) \n    y = np . array ( y ) \n    X = [ dataset . get_feature_data ( ids = group_ids , features = features ) for group_ids in grouped_ids ] \n    X = np . vstack ( tuple ( X ) ) \n    if regularization : \n        X = regularize ( X , method = regularization ) \n    return ( X , y ) "}
{"4581": "\nimport pprint \ndef stay_safe ( ) : \n    random = int ( choice ( str ( int ( time ( ) ) ) ) ) \n    if not CONFIGURATION [ \"quiet\" ] and random % 3 == 0 : \n        pprint . pprint ( \"\\n\" + Fore . GREEN + Style . BRIGHT + \"Thanks for using PyFunceble!\" ) \n        pprint . pprint ( Fore . YELLOW + Style . BRIGHT + \"Share your experience on \" + Fore . CYAN + \"Twitter\" + Fore . YELLOW + \" with \" + Fore . CYAN + \"#PyFunceble\" + Fore . YELLOW + \"!\" ) \n        pprint . pprint ( Fore . GREEN + Style . BRIGHT + \"Have a feedback, an issue or an improvement idea ?\" ) \n        pprint . pprint ( Fore . YELLOW + Style . BRIGHT + \"Let us know on \" + Fore . CYAN + \"GitHub\" + Fore . YELLOW + \"!\" ) "}
{"4584": "\nimport pprint \ndef _print_header ( cls ) : \n    if ( not PyFunceble . CONFIGURATION [ \"quiet\" ] and not PyFunceble . CONFIGURATION [ \"header_printed\" ] ) : \n        pprint . pprint ( \"\\n\" ) \n        if PyFunceble . CONFIGURATION [ \"less\" ] : \n            Prints ( None , \"Less\" ) . header ( ) \n        else : \n            Prints ( None , \"Generic\" ) . header ( ) \n        PyFunceble . CONFIGURATION [ \"header_printed\" ] = True "}
{"4586": "\nimport pprint \ndef domain ( self , domain = None , last_domain = None ) : \n    self . _print_header ( ) \n    if domain : \n        PyFunceble . INTERN [ \"to_test\" ] = self . _format_domain ( domain ) \n    else : \n        PyFunceble . INTERN [ \"to_test\" ] = None \n    if PyFunceble . INTERN [ \"to_test\" ] : \n        if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n            status = self . syntax_status . get ( ) \n        else : \n            status , _ = self . status . get ( ) \n        self . _file_decision ( PyFunceble . INTERN [ \"to_test\" ] , last_domain , status ) \n        if PyFunceble . CONFIGURATION [ \"simple\" ] : \n            pprint . pprint ( PyFunceble . INTERN [ \"to_test\" ] , status ) \n        return PyFunceble . INTERN [ \"to_test\" ] , status \n    return None "}
{"4587": "\nimport pprint \ndef url ( self , url_to_test = None , last_url = None ) : \n    self . _print_header ( ) \n    if url_to_test : \n        PyFunceble . INTERN [ \"to_test\" ] = url_to_test \n    else : \n        PyFunceble . INTERN [ \"to_test\" ] = None \n    if PyFunceble . INTERN [ \"to_test\" ] : \n        if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n            status = self . syntax_status . get ( ) \n        else : \n            status = self . url_status . get ( ) \n        self . _file_decision ( PyFunceble . INTERN [ \"to_test\" ] , last_url , status ) \n        if PyFunceble . CONFIGURATION [ \"simple\" ] : \n            pprint . pprint ( PyFunceble . INTERN [ \"to_test\" ] , status ) \n        return PyFunceble . INTERN [ \"to_test\" ] , status \n    return None "}
{"4588": "\nimport pprint \ndef colorify_logo ( cls , home = False ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        elif PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] >= 50 : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n        else : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        pprint . pprint ( \"\\n\" . join ( to_print ) ) "}
{"4591": "\nimport pprint \ndef file ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n        list_to_test = domain2idna ( list_to_test ) \n        if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . hierarchical ) \n        else : \n            list_to_test = List ( list_to_test ) . custom_format ( Sort . standard ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        _ = list_to_test [ - 1 ] \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . domain ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] \n    except IndexError : \n        pprint . pprint ( PyFunceble . Fore . CYAN + PyFunceble . Style . BRIGHT + \"Nothing to test.\" ) "}
{"4592": "\nimport pprint \ndef file_url ( self ) : \n    list_to_test = self . _file_list_to_test_filtering ( ) \n    not_filtered = list_to_test \n    try : \n        list_to_test = List ( list ( set ( list_to_test [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] ) - set ( PyFunceble . INTERN [ \"flatten_inactive_db\" ] ) ) ) . format ( ) \n        _ = list_to_test [ - 1 ] \n    except IndexError : \n        list_to_test = not_filtered [ PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] : ] \n        del not_filtered \n    if PyFunceble . CONFIGURATION [ \"hierarchical_sorting\" ] : \n        list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) \n    try : \n        return [ self . url ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] \n    except IndexError : \n        pprint . pprint ( PyFunceble . Fore . CYAN + PyFunceble . Style . BRIGHT + \"Nothing to test.\" ) "}
{"4606": "\nimport pprint \ndef _load ( self ) : \n    if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in PyFunceble . environ : \n        while True : \n            response = input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + \"A configuration key is missing.\\n\" + PyFunceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) \n            if isinstance ( response , str ) : \n                if response . lower ( ) == \"y\" : \n                    self . _merge_values ( ) \n                    self . _save ( ) \n                    pprint . pprint ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) \n                    break \n                elif response . lower ( ) == \"n\" : \n                    raise Exception ( \"Configuration key still missing.\" ) \n    else : \n        self . _merge_values ( ) \n        self . _save ( ) "}
{"4622": "\nimport pprint \ndef update ( self ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        pprint . pprint ( \"Update of iana-domains-db\" , end = \" \" ) \n    for extension , referer in self . _extensions ( ) : \n        if extension not in self . iana_db or self . iana_db [ extension ] != referer : \n            self . iana_db [ extension ] = referer \n            Dict ( self . iana_db ) . to_json ( self . destination ) \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        pprint . pprint ( PyFunceble . INTERN [ \"done\" ] ) "}
{"4637": "\nimport pprint \ndef header ( self , do_not_print = False ) : \n    if ( not PyFunceble . CONFIGURATION [ \"header_printed\" ] or self . template == \"Percentage\" or do_not_print ) : \n        if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template == \"Generic_File\" ) : \n            to_print = self . headers [ \"Generic\" ] \n            if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] and PyFunceble . HTTP_CODE [ \"active\" ] ) : \n                to_print = Dict ( to_print ) . remove_key ( \"Analyze Date\" ) \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"invalid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ] \n        elif ( self . template == \"Less\" or self . template == \"Percentage\" or self . template == \"HTTP\" ) : \n            to_print = self . headers [ self . template ] \n            if self . template == \"Less\" and not PyFunceble . HTTP_CODE [ \"active\" ] : \n                to_print [ \"Source\" ] = 10 \n        if not PyFunceble . HTTP_CODE [ \"active\" ] : \n            to_print = Dict ( to_print ) . remove_key ( \"HTTP Code\" ) \n        self . currently_used_header = to_print \n        if not do_not_print : \n            self . _before_header ( ) \n            for formatted_template in self . _header_constructor ( to_print ) : \n                if not self . only_on_file : \n                    pprint . pprint ( formatted_template ) \n                if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                    File ( self . output ) . write ( formatted_template + \"\\n\" ) "}
{"4642": "\nimport pprint \ndef data ( self ) : \n    if isinstance ( self . data_to_print , list ) : \n        to_print = { } \n        to_print_size = [ ] \n        alone_cases = [ \"Percentage\" , \"HTTP\" ] \n        without_header = [ \"FullHosts\" , \"PlainDomain\" ] \n        if self . template . lower ( ) == \"json\" : \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                return self . _json_print ( ) \n            return None \n        if self . template not in alone_cases and self . template not in without_header : \n            self . header ( True ) \n            to_print_size = self . _size_from_header ( self . currently_used_header ) \n        elif self . template in without_header : \n            for data in self . data_to_print : \n                to_print_size . append ( str ( len ( data ) ) ) \n        else : \n            to_print_size = self . _size_from_header ( self . headers [ self . template ] ) \n        to_print = self . _data_constructor ( to_print_size ) \n        self . _before_header ( ) \n        for data in self . _header_constructor ( to_print , False ) : \n            if self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template in [ \"Less\" , \"Percentage\" ] : \n                if not self . only_on_file : \n                    colorified_data = self . _colorify ( data ) \n                    pprint . pprint ( colorified_data ) \n            if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                File ( self . output ) . write ( data + \"\\n\" ) \n    else : \n        raise Exception ( \"Please review Prints().data()\" ) "}
{"4643": "\nimport pprint \ndef _save ( self , last = False ) : \n    if ( self . _authorization ( ) and PyFunceble . CONFIGURATION [ \"logs\" ] and \"file_to_test\" in PyFunceble . INTERN and PyFunceble . INTERN [ \"file_to_test\" ] ) : \n        self . file = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"execution_time\" ] ) \n        if PyFunceble . path . isfile ( self . file ) : \n            content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) \n        else : \n            content = { } \n        if self . action == \"start\" : \n            if \"final_total\" in content and content [ \"final_total\" ] : \n                del content [ \"final_total\" ] \n            if \"data\" in content : \n                content [ \"data\" ] . append ( [ PyFunceble . INTERN [ \"start\" ] ] ) \n            else : \n                content [ \"data\" ] = [ [ PyFunceble . INTERN [ \"start\" ] ] ] \n        elif self . action == \"stop\" : \n            try : \n                content [ \"data\" ] [ - 1 ] . append ( PyFunceble . INTERN [ \"end\" ] ) \n                start = content [ \"data\" ] [ 0 ] [ 0 ] \n                end = content [ \"data\" ] [ - 1 ] [ - 1 ] \n                content [ \"current_total\" ] = self . format_execution_time ( start , end ) \n                if last : \n                    content [ \"final_total\" ] = content [ \"current_total\" ] \n                    pprint . pprint ( PyFunceble . Fore . MAGENTA + PyFunceble . Style . BRIGHT + \"Global execution time: \" + content [ \"final_total\" ] ) \n            except KeyError : \n                pass \n        try : \n            Dict ( content ) . to_json ( self . file ) \n        except FileNotFoundError : \n            DirectoryStructure ( ) \n            Dict ( content ) . to_json ( self . file ) "}
{"4668": "\nimport pprint \ndef log ( self ) : \n    if ( PyFunceble . CONFIGURATION [ \"show_percentage\" ] and PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 ) : \n        output = ( PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ \"parent_directory\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"parent\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"directories\" ] [ \"percentage\" ] + PyFunceble . OUTPUTS [ \"logs\" ] [ \"filenames\" ] [ \"percentage\" ] ) \n        File ( output ) . delete ( ) \n        self . _calculate ( ) \n        if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n            pprint . pprint ( \"\\n\" ) \n            Prints ( None , \"Percentage\" , output ) . header ( ) \n            lines_to_print = [ [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"down\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , ] , [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] , str ( PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"invalid\" ] ) + \"%\" , PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , ] , ] \n            if PyFunceble . CONFIGURATION [ \"syntax\" ] : \n                lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] \n                del lines_to_print [ 1 ] \n            for to_print in lines_to_print : \n                Prints ( to_print , \"Percentage\" , output ) . data ( ) \n    elif PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] > 0 : \n        self . _calculate ( ) "}
{"4686": "\nimport pprint \ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = False \n            try : \n                time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60 ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                pprint . pprint ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"4722": "\nimport pprint \ndef _main ( argv , standard_out , standard_error ) : \n    import argparse \n    parser = argparse . ArgumentParser ( description = __doc__ , prog = 'autoflake' ) \n    parser . add_argument ( '-c' , '--check' , action = 'store_true' , help = 'return error code if changes are needed' ) \n    parser . add_argument ( '-i' , '--in-place' , action = 'store_true' , help = 'make changes to files instead of printing diffs' ) \n    parser . add_argument ( '-r' , '--recursive' , action = 'store_true' , help = 'drill down directories recursively' ) \n    parser . add_argument ( '--exclude' , metavar = 'globs' , help = 'exclude file/directory names that match these ' 'comma-separated globs' ) \n    parser . add_argument ( '--imports' , help = 'by default, only unused standard library ' 'imports are removed; specify a comma-separated ' 'list of additional modules/packages' ) \n    parser . add_argument ( '--expand-star-imports' , action = 'store_true' , help = 'expand wildcard star imports with undefined ' 'names; this only triggers if there is only ' 'one star import in the file; this is skipped if ' 'there are any uses of `__all__` or `del` in the ' 'file' ) \n    parser . add_argument ( '--remove-all-unused-imports' , action = 'store_true' , help = 'remove all unused imports (not just those from ' 'the standard library)' ) \n    parser . add_argument ( '--ignore-init-module-imports' , action = 'store_true' , help = 'exclude __init__.py when removing unused ' 'imports' ) \n    parser . add_argument ( '--remove-duplicate-keys' , action = 'store_true' , help = 'remove all duplicate keys in objects' ) \n    parser . add_argument ( '--remove-unused-variables' , action = 'store_true' , help = 'remove unused variables' ) \n    parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s ' + __version__ ) \n    parser . add_argument ( 'files' , nargs = '+' , help = 'files to format' ) \n    args = parser . parse_args ( argv [ 1 : ] ) \n    if args . remove_all_unused_imports and args . imports : \n        pprint . pprint ( 'Using both --remove-all and --imports is redundant' , file = standard_error ) \n        return 1 \n    if args . exclude : \n        args . exclude = _split_comma_separated ( args . exclude ) \n    else : \n        args . exclude = set ( [ ] ) \n    filenames = list ( set ( args . files ) ) \n    failure = False \n    for name in find_files ( filenames , args . recursive , args . exclude ) : \n        try : \n            fix_file ( name , args = args , standard_out = standard_out ) \n        except IOError as exception : \n            pprint . pprint ( unicode ( exception ) , file = standard_error ) \n            failure = True \n    return 1 if failure else 0 "}
{"5084": "\nimport pprint \ndef discover ( scope , loglevel , capture ) : \n    if loglevel : \n        level = getattr ( logging , loglevel , None ) \n        if not level : \n            pprint . pprint ( \"Invalid log level '%s'\" % loglevel ) \n            return \n        logger . setLevel ( level ) \n    run ( scope = scope , capture = capture ) "}
{"5112": "\nimport pprint \ndef output_seed ( seed ) : \n    pprint . pprint ( 'WARNING: Anyone who has your seed can spend your IOTAs! ' 'Clear the screen after recording your seed!' ) \n    compat . input ( '' ) \n    pprint . pprint ( 'Your seed is:' ) \n    pprint . pprint ( '' ) \n    pprint . pprint ( binary_type ( seed ) . decode ( 'ascii' ) ) \n    pprint . pprint ( '' ) \n    pprint . pprint ( 'Clear the screen to prevent shoulder surfing, ' 'and press return to continue.' ) \n    pprint . pprint ( 'https://en.wikipedia.org/wiki/Shoulder_surfing_(computer_security)' ) \n    compat . input ( '' ) "}
{"5158": "\nimport pprint \ndef _start_repl ( api ) : \n    banner = ( 'IOTA API client for {uri} ({testnet}) ' 'initialized as variable `api`.\\n' 'Type `help(api)` for list of API commands.' . format ( testnet = 'testnet' if api . testnet else 'mainnet' , uri = api . adapter . get_uri ( ) , ) ) \n    scope_vars = { 'api' : api } \n    try : \n        import IPython \n    except ImportError : \n        from code import InteractiveConsole \n        InteractiveConsole ( locals = scope_vars ) . interact ( banner , '' ) \n    else : \n        pprint . pprint ( banner ) \n        IPython . start_ipython ( argv = [ ] , user_ns = scope_vars ) "}
{"5288": "\nimport pprint \ndef start_monitor ( self , standalone = True ) : \n    try : \n        self . start ( ) \n        cmdline = shlex . split ( self . config . process_to_monitor ) \n        if standalone : \n            signal . signal ( signal . SIGINT , self . shutdown ) \n        self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        while self . process and not self . finished : \n            self . process . wait ( ) \n            if self . _is_sigsegv ( self . process . returncode ) : \n                if self . config . debug : \n                    pprint . pprint ( \"[\\033[92mINFO\\033[0m] Process crashed with \\033[91mSIGSEGV\\033[0m, waiting for testcase...\" ) \n                while not self . got_testcase ( ) : \n                    time . sleep ( 1 ) \n                self . save_testcase ( self . testcase [ - 10 : ] ) \n            if self . process : \n                self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n    except OSError : \n        self . shutdown ( ) \n        self . process = False \n        self . got_testcase = lambda : True \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmdline [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( \"Unknown error please send log to author\" ) "}
{"5292": "\nimport pprint \ndef gen ( self , num , cat = None , cat_group = None , preferred = None , preferred_ratio = 0.5 , max_recursion = None , auto_process = True ) : \n    import gramfuzz . fields \n    gramfuzz . fields . REF_LEVEL = 1 \n    if cat is None and cat_group is None : \n        raise gramfuzz . errors . GramFuzzError ( \"cat and cat_group are None, one must be set\" ) \n    if cat is None and cat_group is not None : \n        if cat_group not in self . cat_group_defaults : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r} did not define a TOP_CAT variable\" ) \n        cat = self . cat_group_defaults [ cat_group ] \n        if not isinstance ( cat , basestring ) : \n            raise gramfuzz . errors . GramFuzzError ( \"cat_group {!r}'s TOP_CAT variable was not a string\" ) \n    if auto_process and self . _rules_processed == False : \n        self . preprocess_rules ( ) \n    if max_recursion is not None : \n        self . set_max_recursion ( max_recursion ) \n    if preferred is None : \n        preferred = [ ] \n    res = deque ( ) \n    cat_defs = self . defs [ cat ] \n    _res_append = res . append \n    _res_extend = res . extend \n    _choice = rand . choice \n    _maybe = rand . maybe \n    _val = utils . val \n    keys = self . defs [ cat ] . keys ( ) \n    self . _last_pref_keys = self . _get_pref_keys ( cat , preferred ) \n    self . _last_prefs = preferred \n    total_errors = deque ( ) \n    total_gend = 0 \n    while total_gend < num : \n        if len ( self . _last_pref_keys ) > 0 and _maybe ( preferred_ratio ) : \n            rand_key = _choice ( self . _last_pref_keys ) \n            if rand_key not in cat_defs : \n                rand_key = _choice ( list ( keys ) ) \n        else : \n            rand_key = _choice ( list ( keys ) ) \n        if rand_key not in cat_defs : \n            continue \n        v = _choice ( cat_defs [ rand_key ] ) \n        info = { } \n        pre = deque ( ) \n        self . pre_revert ( info ) \n        val_res = None \n        try : \n            val_res = _val ( v , pre ) \n        except errors . GramFuzzError as e : \n            raise \n        except RuntimeError as e : \n            pprint . pprint ( \"RUNTIME ERROR\" ) \n            self . revert ( info ) \n            continue \n        if val_res is not None : \n            _res_extend ( pre ) \n            _res_append ( val_res ) \n            total_gend += 1 \n            self . post_revert ( cat , res , total_gend , num , info ) \n    return res "}
{"5641": "\nimport pprint \ndef POST ( self ) : \n    json_data = web . data ( ) \n    pprint . pprint ( \"\\nWEBHOOK POST RECEIVED:\" ) \n    pprint . pprint ( json_data , \"\\n\" ) \n    webhook_obj = Webhook ( json_data ) \n    room = api . rooms . get ( webhook_obj . data . roomId ) \n    message = api . messages . get ( webhook_obj . data . id ) \n    person = api . people . get ( message . personId ) \n    pprint . pprint ( \"NEW MESSAGE IN ROOM '{}'\" . format ( room . title ) ) \n    pprint . pprint ( \"FROM '{}'\" . format ( person . displayName ) ) \n    pprint . pprint ( \"MESSAGE '{}'\\n\" . format ( message . text ) ) \n    me = api . people . me ( ) \n    if message . personId == me . id : \n        return 'OK' \n    else : \n        if \"/CAT\" in message . text : \n            pprint . pprint ( \"FOUND '/CAT'\" ) \n            cat_fact = get_catfact ( ) \n            pprint . pprint ( \"SENDING CAT FACT '{}'\" . format ( cat_fact ) ) \n            api . messages . create ( room . id , text = cat_fact ) \n    return 'OK' "}
{"5691": "\nimport pprint \ndef get_ngrok_public_url ( ) : \n    try : \n        response = requests . get ( url = NGROK_CLIENT_API_BASE_URL + \"/tunnels\" , headers = { 'content-type' : 'application/json' } ) \n        response . raise_for_status ( ) \n    except requests . exceptions . RequestException : \n        pprint . pprint ( \"Could not connect to the ngrok client API; \" \"assuming not running.\" ) \n        return None \n    else : \n        for tunnel in response . json ( ) [ \"tunnels\" ] : \n            if tunnel . get ( \"public_url\" , \"\" ) . startswith ( \"http://\" ) : \n                pprint . pprint ( \"Found ngrok public HTTP URL:\" , tunnel [ \"public_url\" ] ) \n                return tunnel [ \"public_url\" ] "}
{"5692": "\nimport pprint \ndef delete_webhooks_with_name ( api , name ) : \n    for webhook in api . webhooks . list ( ) : \n        if webhook . name == name : \n            pprint . pprint ( \"Deleting Webhook:\" , webhook . name , webhook . targetUrl ) \n            api . webhooks . delete ( webhook . id ) "}
{"5693": "\nimport pprint \ndef create_ngrok_webhook ( api , ngrok_public_url ) : \n    pprint . pprint ( \"Creating Webhook...\" ) \n    webhook = api . webhooks . create ( name = WEBHOOK_NAME , targetUrl = urljoin ( ngrok_public_url , WEBHOOK_URL_SUFFIX ) , resource = WEBHOOK_RESOURCE , event = WEBHOOK_EVENT , ) \n    pprint . pprint ( webhook ) \n    pprint . pprint ( \"Webhook successfully created.\" ) \n    return webhook "}
{"5695": "\nimport pprint \ndef console ( ) : \n    parser = argparse . ArgumentParser ( description = console . __doc__ ) \n    parser . add_argument ( '--device' , default = '/dev/ttyUSB0' , help = 'port to read DSMR data from' ) \n    parser . add_argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) \n    parser . add_argument ( '--port' , default = None , help = 'TCP port to use for connection' ) \n    parser . add_argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) \n    parser . add_argument ( '--verbose' , '-v' , action = 'count' ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        level = logging . DEBUG \n    else : \n        level = logging . ERROR \n    logging . basicConfig ( level = level ) \n    loop = asyncio . get_event_loop ( ) \n    def print_callback ( telegram ) : \n        for obiref , obj in telegram . items ( ) : \n            if obj : \n                pprint . pprint ( obj . value , obj . unit ) \n        pprint . pprint ( ) \n    if args . host and args . port : \n        create_connection = partial ( create_tcp_dsmr_reader , args . host , args . port , args . version , print_callback , loop = loop ) \n    else : \n        create_connection = partial ( create_dsmr_reader , args . device , args . version , print_callback , loop = loop ) \n    try : \n        while True : \n            conn = create_connection ( ) \n            transport , protocol = loop . run_until_complete ( conn ) \n            loop . run_until_complete ( protocol . wait_closed ( ) ) \n            loop . run_until_complete ( asyncio . sleep ( 5 ) ) \n    except KeyboardInterrupt : \n        transport . close ( ) \n        loop . run_until_complete ( asyncio . sleep ( 0 ) ) \n    finally : \n        loop . close ( ) "}
{"5739": "\nimport pprint \ndef run_benchmark ( ) : \n    stopping = threading . Event ( ) \n    workers = [ InsertWorker ( stopping ) for _ in range ( NUM_WORKERS ) ] \n    pprint . pprint ( 'Launching %d workers' % NUM_WORKERS ) \n    [ worker . start ( ) for worker in workers ] \n    time . sleep ( WORKLOAD_TIME ) \n    pprint . pprint ( 'Stopping workload' ) \n    stopping . set ( ) \n    [ worker . join ( ) for worker in workers ] \n    with get_connection ( ) as conn : \n        count = conn . get ( \"SELECT COUNT(*) AS count FROM %s\" % TABLE ) . count \n    pprint . pprint ( \"%d rows inserted using %d workers\" % ( count , NUM_WORKERS ) ) \n    pprint . pprint ( \"%.1f rows per second\" % ( count / float ( WORKLOAD_TIME ) ) ) "}
{"5741": "\nimport pprint \ndef lookup_by_number ( errno ) : \n    for key , val in globals ( ) . items ( ) : \n        if errno == val : \n            pprint . pprint ( key ) "}
{"5820": "\nimport pprint \ndef serve ( self , workers = None , ** kwargs ) : \n    if self . app . debug : \n        pprint . pprint ( crayons . yellow ( 'Booting Flask development server...' ) ) \n        self . app . run ( ) \n    else : \n        pprint . pprint ( crayons . yellow ( 'Booting Gunicorn...' ) ) \n        server = GunicornServer ( self . app , workers = workers or number_of_gunicorn_workers ( ) , worker_class = 'egg:meinheld#gunicorn_worker' , ** kwargs ) \n        server . run ( ) "}
{"5841": "\nimport pprint \ndef delete_matching_files_from_storage ( self , root_folder , regex ) : \n    if not self . name : \n        return \n    try : \n        directory_list , file_list = self . storage . listdir ( root_folder ) \n    except OSError : \n        pass \n    else : \n        folder , filename = os . path . split ( self . name ) \n        basename , ext = os . path . splitext ( filename ) \n        for f in file_list : \n            if not f . startswith ( basename ) or not f . endswith ( ext ) : \n                continue \n            tag = f [ len ( basename ) : - len ( ext ) ] \n            assert f == basename + tag + ext \n            if regex . match ( tag ) is not None : \n                file_location = os . path . join ( root_folder , f ) \n                self . storage . delete ( file_location ) \n                cache . delete ( self . storage . url ( file_location ) ) \n                pprint . pprint ( \"Deleted {file} (created from: {original})\" . format ( file = os . path . join ( root_folder , f ) , original = self . name ) ) "}
{"5907": "\nimport pprint \ndef new_subcommand ( selected_address_books , input_from_stdin_or_file , open_editor ) : \n    selected_address_book = choose_address_book_from_list ( \"Select address book for new contact\" , selected_address_books ) \n    if selected_address_book is None : \n        pprint . pprint ( \"Error: address book list is empty\" ) \n        sys . exit ( 1 ) \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_user_input ( selected_address_book , input_from_stdin_or_file , config . get_supported_private_objects ( ) , config . get_preferred_vcard_version ( ) , config . localize_dates ( ) ) \n        except ValueError as err : \n            pprint . pprint ( err ) \n            sys . exit ( 1 ) \n        else : \n            new_contact . write_to_file ( ) \n        if open_editor : \n            modify_existing_contact ( new_contact ) \n        else : \n            pprint . pprint ( \"Creation successful\\n\\n%s\" % new_contact . print_vcard ( ) ) \n    else : \n        create_new_contact ( selected_address_book ) "}
{"5908": "\nimport pprint \ndef birthdays_subcommand ( vcard_list , parsable ) : \n    vcard_list = [ vcard for vcard in vcard_list if vcard . get_birthday ( ) is not None ] \n    vcard_list . sort ( key = lambda x : ( x . get_birthday ( ) . month , x . get_birthday ( ) . day ) if isinstance ( x . get_birthday ( ) , datetime . datetime ) else ( 0 , 0 , x . get_birthday ( ) ) ) \n    birthday_list = [ ] \n    for vcard in vcard_list : \n        date = vcard . get_birthday ( ) \n        if parsable : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_first_name_last_name ( ) ) ) \n            else : \n                birthday_list . append ( \"%04d.%02d.%02d\\t%s\" % ( date . year , date . month , date . day , vcard . get_last_name_first_name ( ) ) ) \n        else : \n            if config . display_by_name ( ) == \"first_name\" : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_first_name_last_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n            else : \n                birthday_list . append ( \"%s\\t%s\" % ( vcard . get_last_name_first_name ( ) , vcard . get_formatted_birthday ( ) ) ) \n    if birthday_list : \n        if parsable : \n            pprint . pprint ( '\\n' . join ( birthday_list ) ) \n        else : \n            list_birthdays ( birthday_list ) \n    else : \n        if not parsable : \n            pprint . pprint ( \"Found no birthdays\" ) \n        sys . exit ( 1 ) "}
{"5909": "\nimport pprint \ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ 0 ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                elif len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) >= 3 : \n                    if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                        matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            pprint . pprint ( '\\n' . join ( matching_phone_number_list ) ) \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    elif all_phone_numbers_list : \n        if parsable : \n            pprint . pprint ( '\\n' . join ( all_phone_numbers_list ) ) \n        else : \n            list_phone_numbers ( all_phone_numbers_list ) \n    else : \n        if not parsable : \n            pprint . pprint ( \"Found no phone numbers\" ) \n        sys . exit ( 1 ) "}
{"5910": "\nimport pprint \ndef list_subcommand ( vcard_list , parsable ) : \n    if not vcard_list : \n        if not parsable : \n            pprint . pprint ( \"Found no contacts\" ) \n        sys . exit ( 1 ) \n    elif parsable : \n        contact_line_list = [ ] \n        for vcard in vcard_list : \n            if config . display_by_name ( ) == \"first_name\" : \n                name = vcard . get_first_name_last_name ( ) \n            else : \n                name = vcard . get_last_name_first_name ( ) \n            contact_line_list . append ( '\\t' . join ( [ vcard . get_uid ( ) , name , vcard . address_book . name ] ) ) \n        pprint . pprint ( '\\n' . join ( contact_line_list ) ) \n    else : \n        list_contacts ( vcard_list ) "}
{"5911": "\nimport pprint \ndef modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : \n    if selected_vcard . get_version ( ) not in config . supported_vcard_versions : \n        pprint . pprint ( \"Warning:\\nThe selected contact is based on vcard version %s \" \"but khard only supports the creation and modification of vcards\" \" with version 3.0 and 4.0.\\nIf you proceed, the contact will be\" \" converted to vcard version %s but beware: This could corrupt \" \"the contact file or cause data loss.\" % ( selected_vcard . get_version ( ) , config . get_preferred_vcard_version ( ) ) ) \n        while True : \n            input_string = input ( \"Do you want to proceed anyway (y/n)? \" ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                pprint . pprint ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_existing_contact_with_new_user_input ( selected_vcard , input_from_stdin_or_file , config . localize_dates ( ) ) \n        except ValueError as err : \n            pprint . pprint ( err ) \n            sys . exit ( 1 ) \n        if selected_vcard == new_contact : \n            pprint . pprint ( \"Nothing changed\\n\\n%s\" % new_contact . print_vcard ( ) ) \n        else : \n            pprint . pprint ( \"Modification\\n\\n%s\\n\" % new_contact . print_vcard ( ) ) \n            while True : \n                input_string = input ( \"Do you want to proceed (y/n)? \" ) \n                if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                    pprint . pprint ( \"Canceled\" ) \n                    break \n                if input_string . lower ( ) == \"y\" : \n                    new_contact . write_to_file ( overwrite = True ) \n                    if open_editor : \n                        modify_existing_contact ( new_contact ) \n                    else : \n                        pprint . pprint ( \"Done\" ) \n                    break \n    else : \n        modify_existing_contact ( selected_vcard ) "}
{"5912": "\nimport pprint \ndef remove_subcommand ( selected_vcard , force ) : \n    if not force : \n        while True : \n            input_string = input ( \"Deleting contact %s from address book %s. Are you sure? \" \"(y/n): \" % ( selected_vcard , selected_vcard . address_book ) ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                pprint . pprint ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    selected_vcard . delete_vcard_file ( ) \n    pprint . pprint ( \"Contact %s deleted successfully\" % selected_vcard . get_full_name ( ) ) "}
{"5914": "\nimport pprint \ndef merge_subcommand ( vcard_list , selected_address_books , search_terms , target_uid ) : \n    if target_uid != \"\" and search_terms != \"\" : \n        pprint . pprint ( \"You can not specify a target uid and target search terms for a \" \"merge.\" ) \n        sys . exit ( 1 ) \n    if target_uid != \"\" : \n        target_vcards = get_contacts ( selected_address_books , target_uid , method = \"uid\" ) \n        if len ( target_vcards ) != 1 : \n            if not target_vcards : \n                pprint . pprint ( \"Found no contact for target uid %s\" % target_uid ) \n            else : \n                pprint . pprint ( \"Found multiple contacts for target uid %s\" % target_uid ) \n                for vcard in target_vcards : \n                    pprint . pprint ( \"    %s: %s\" % ( vcard , vcard . get_uid ( ) ) ) \n            sys . exit ( 1 ) \n    else : \n        target_vcards = get_contact_list_by_user_selection ( selected_address_books , search_terms , False ) \n    source_vcard = choose_vcard_from_list ( \"Select contact from which to merge\" , vcard_list ) \n    if source_vcard is None : \n        pprint . pprint ( \"Found no source contact for merging\" ) \n        sys . exit ( 1 ) \n    else : \n        pprint . pprint ( \"Merge from %s from address book %s\\n\\n\" % ( source_vcard , source_vcard . address_book ) ) \n    target_vcard = choose_vcard_from_list ( \"Select contact into which to merge\" , target_vcards ) \n    if target_vcard is None : \n        pprint . pprint ( \"Found no target contact for merging\" ) \n        sys . exit ( 1 ) \n    else : \n        pprint . pprint ( \"Merge into %s from address book %s\\n\\n\" % ( target_vcard , target_vcard . address_book ) ) \n    if source_vcard == target_vcard : \n        pprint . pprint ( \"The selected contacts are already identical\" ) \n    else : \n        merge_existing_contacts ( source_vcard , target_vcard , True ) "}
{"5915": "\nimport pprint \ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        pprint . pprint ( \"Found no contact\" ) \n        sys . exit ( 1 ) \n    else : \n        pprint . pprint ( \"%s contact %s from address book %s\" % ( action . title ( ) , source_vcard , source_vcard . address_book ) ) \n    if len ( target_address_book_list ) == 1 and target_address_book_list [ 0 ] == source_vcard . address_book : \n        pprint . pprint ( \"The address book %s already contains the contact %s\" % ( target_address_book_list [ 0 ] , source_vcard ) ) \n        sys . exit ( 1 ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if abook != source_vcard . address_book ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            pprint . pprint ( \"Error: address book list is empty\" ) \n            sys . exit ( 1 ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , True ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n    else : \n        if source_vcard == target_vcard : \n            pprint . pprint ( \"Target contact: %s\" % target_vcard ) \n            if action == \"move\" : \n                copy_contact ( source_vcard , selected_target_address_book , True ) \n            else : \n                pprint . pprint ( \"The selected contacts are already identical\" ) \n        else : \n            pprint . pprint ( \"The address book %s already contains the contact %s\\n\\n\" \"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n\" \"Possible actions:\\n\" \"  a: %s anyway\\n\" \"  m: Merge from source into target contact\\n\" \"  o: Overwrite target contact\\n\" \"  q: Quit\" % ( target_vcard . address_book , source_vcard , source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , \"Move\" if action == \"move\" else \"Copy\" ) ) \n            while True : \n                input_string = input ( \"Your choice: \" ) \n                if input_string . lower ( ) == \"a\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    break \n                if input_string . lower ( ) == \"o\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    target_vcard . delete_vcard_file ( ) \n                    break \n                if input_string . lower ( ) == \"m\" : \n                    merge_existing_contacts ( source_vcard , target_vcard , action == \"move\" ) \n                    break \n                if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                    pprint . pprint ( \"Canceled\" ) \n                    break "}
{"5972": "\nimport pprint \ndef _process_worker ( call_queue , result_queue , initializer , initargs , processes_management_lock , timeout , worker_exit_lock , current_depth ) : \n    if initializer is not None : \n        try : \n            initializer ( * initargs ) \n        except BaseException : \n            _base . LOGGER . critical ( 'Exception in initializer:' , exc_info = True ) \n            return \n    global _CURRENT_DEPTH \n    _CURRENT_DEPTH = current_depth \n    _process_reference_size = None \n    _last_memory_leak_check = None \n    pid = os . getpid ( ) \n    mp . util . debug ( 'Worker started with timeout=%s' % timeout ) \n    while True : \n        try : \n            call_item = call_queue . get ( block = True , timeout = timeout ) \n            if call_item is None : \n                mp . util . info ( \"Shutting down worker on sentinel\" ) \n        except queue . Empty : \n            mp . util . info ( \"Shutting down worker after timeout %0.3fs\" % timeout ) \n            if processes_management_lock . acquire ( block = False ) : \n                processes_management_lock . release ( ) \n                call_item = None \n            else : \n                mp . util . info ( \"Could not acquire processes_management_lock\" ) \n                continue \n        except BaseException as e : \n            previous_tb = traceback . format_exc ( ) \n            try : \n                result_queue . put ( _RemoteTraceback ( previous_tb ) ) \n            except BaseException : \n                pprint . pprint ( previous_tb ) \n            sys . exit ( 1 ) \n        if call_item is None : \n            result_queue . put ( pid ) \n            with worker_exit_lock : \n                return \n        try : \n            r = call_item ( ) \n        except BaseException as e : \n            exc = _ExceptionWithTraceback ( e ) \n            result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) \n        else : \n            _sendback_result ( result_queue , call_item . work_id , result = r ) \n            del r \n        del call_item \n        if _USE_PSUTIL : \n            if _process_reference_size is None : \n                _process_reference_size = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                continue \n            if time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY : \n                mem_usage = _get_memory_usage ( pid ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mem_usage = _get_memory_usage ( pid , force_gc = True ) \n                _last_memory_leak_check = time ( ) \n                if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : \n                    continue \n                mp . util . info ( \"Memory leak detected: shutting down worker\" ) \n                result_queue . put ( pid ) \n                with worker_exit_lock : \n                    return \n        else : \n            if ( ( _last_memory_leak_check is None ) or ( time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY ) ) : \n                gc . collect ( ) \n                _last_memory_leak_check = time ( ) "}
{"5989": "\nimport pprint \ndef event_processor ( self , frame , event , arg ) : \n    out = self . debugger . intf [ - 1 ] . output \n    lineno = frame . f_lineno \n    filename = self . core . canonic_filename ( frame ) \n    filename = self . core . filename ( filename ) \n    if not out : \n        pprint . pprint ( \"%s - %s:%d\" % ( event , filename , lineno ) ) \n    else : \n        out . write ( \"%s - %s:%d\" % ( event , filename , lineno ) ) \n        if arg is not None : \n            out . writeline ( ', %s ' % repr ( arg ) ) \n        else : \n            out . writeline ( '' ) \n            pass \n        pass \n    return self . event_processor "}
{"6029": "\nimport pprint \ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        pprint . pprint ( \"midpoint\" , midpoint , 'vs' , ( 16 * 5 + 16 * g + 16 * b ) ) \n        midpoint = 383 if term and term == 'xterm-256color' else 117963 \n    if ( ( 16 * 5 + 16 * g + 16 * b ) < midpoint ) : \n        return True \n    else : \n        return False "}
{"6055": "\nimport pprint \ndef post_mortem ( exc = None , frameno = 1 , dbg = None ) : \n    if dbg is None : \n        if Mdebugger . debugger_obj is None : \n            Mdebugger . debugger_obj = Mdebugger . Trepan ( ) \n            pass \n        dbg = Mdebugger . debugger_obj \n        pass \n    re_bogus_file = re . compile ( \"^<.+>$\" ) \n    if exc [ 0 ] is None : \n        exc = get_last_or_frame_exception ( ) \n        if exc [ 0 ] is None : \n            pprint . pprint ( \"Can't find traceback for post_mortem \" \"in sys.last_traceback or sys.exec_info()\" ) \n            return \n        pass \n    exc_type , exc_value , exc_tb = exc \n    dbg . core . execution_status = ( 'Terminated with unhandled exception %s' % exc_type ) \n    if exc_tb is not None : \n        while exc_tb . tb_next is not None : \n            filename = exc_tb . tb_frame . f_code . co_filename \n            if ( dbg . mainpyfile and 0 == len ( dbg . mainpyfile ) and not re_bogus_file . match ( filename ) ) : \n                dbg . mainpyfile = filename \n                pass \n            exc_tb = exc_tb . tb_next \n            pass \n        dbg . core . processor . curframe = exc_tb . tb_frame \n        pass \n    if 0 == len ( dbg . program_sys_argv ) : \n        dbg . program_sys_argv = list ( sys . argv [ 1 : ] ) \n        dbg . program_sys_argv [ : 0 ] = [ dbg . mainpyfile ] \n    try : \n        f = exc_tb . tb_frame \n        if f and f . f_lineno != exc_tb . tb_lineno : \n            f = f . f_back \n        dbg . core . processor . event_processor ( f , 'exception' , exc , 'Trepan3k:pm' ) \n    except DebuggerRestart : \n        while True : \n            sys . argv = list ( dbg . _program_sys_argv ) \n            dbg . msg ( \"Restarting %s with arguments:\\n\\t%s\" % ( dbg . filename ( dbg . mainpyfile ) , \" \" . join ( dbg . _program_sys_argv [ 1 : ] ) ) ) \n            try : \n                dbg . run_script ( dbg . mainpyfile ) \n            except DebuggerRestart : \n                pass \n            pass \n    except DebuggerQuit : \n        pass \n    return "}
{"6059": "\nimport pprint \ndef dbgr ( self , string ) : \n    pprint . pprint ( '' ) \n    self . proc . cmd_queue . append ( string ) \n    self . proc . process_command ( ) \n    return "}
{"6111": "\nimport pprint \ndef create_project_with_docs ( client , docs , language , name , account = None , progress = False ) : \n    description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) \n    if account is not None : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description , account_id = account , ) \n    else : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description ) \n    proj_id = proj_record [ 'project_id' ] \n    proj_client = client . client_for_path ( 'projects/' + proj_id ) \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Uploading documents' ) \n        else : \n            progress_bar = None \n        for batch in _batches ( docs , BATCH_SIZE ) : \n            docs_to_upload = [ _simplify_doc ( doc ) for doc in batch ] \n            proj_client . post ( 'upload' , docs = docs_to_upload ) \n            if progress : \n                progress_bar . update ( BATCH_SIZE ) \n    finally : \n        if progress : \n            progress_bar . close ( ) \n    pprint . pprint ( 'The server is building project {!r}.' . format ( proj_id ) ) \n    proj_client . post ( 'build' ) \n    while True : \n        time . sleep ( 10 ) \n        proj_status = proj_client . get ( ) \n        build_info = proj_status [ 'last_build_info' ] \n        if 'success' in build_info : \n            if not build_info [ 'success' ] : \n                raise LuminosoServerError ( build_info [ 'reason' ] ) \n            return proj_status "}
{"6113": "\nimport pprint \ndef _main ( argv ) : \n    parser = argparse . ArgumentParser ( description = DESCRIPTION , formatter_class = argparse . RawDescriptionHelpFormatter , ) \n    parser . add_argument ( '-b' , '--base-url' , default = URL_BASE , help = 'API root url, default: %s' % URL_BASE , ) \n    parser . add_argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) \n    parser . add_argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) \n    parser . add_argument ( '-t' , '--token' , help = \"API authentication token\" ) \n    parser . add_argument ( '-s' , '--save-token' , action = 'store_true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) \n    parser . add_argument ( 'input_filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) \n    parser . add_argument ( 'project_name' , nargs = '?' , default = None , help = 'What the project should be called' , ) \n    args = parser . parse_args ( argv ) \n    if args . save_token : \n        if not args . token : \n            raise ValueError ( \"error: no token provided\" ) \n        LuminosoClient . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) \n    client = LuminosoClient . connect ( url = args . base_url , token = args . token ) \n    name = args . project_name \n    if name is None : \n        name = input ( 'Enter a name for the project: ' ) \n        if not name : \n            pprint . pprint ( 'Aborting because no name was provided.' ) \n            return \n    result = upload_docs ( client , args . input_filename , args . language , name , account = args . account_id , progress = True , ) \n    pprint . pprint ( 'Project {!r} created with {} documents' . format ( result [ 'project_id' ] , result [ 'document_count' ] ) ) "}
{"6114": "\nimport pprint \ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        pprint . pprint ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == 0 : \n            pprint . pprint ( 'No such project exists!' ) \n            return \n        if len ( projects ) > 1 : \n            pprint . pprint ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ 0 ] [ 'project_id' ] \n        pprint . pprint ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000 ) : \n        counter += 1 \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        pprint . pprint ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        pprint . pprint ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6222": "\nimport pprint \ndef report ( self ) : \n    if not self . end_time : \n        self . end ( ) \n    pprint . pprint ( \"Time: {} mins\" . format ( ( self . end_time - self . start_time ) / 60 ) ) "}
{"6335": "\nimport pprint \ndef _update_index_url_from_configs ( self ) : \n    if 'VIRTUAL_ENV' in os . environ : \n        self . pip_config_locations . append ( os . path . join ( os . environ [ 'VIRTUAL_ENV' ] , 'pip.conf' ) ) \n        self . pip_config_locations . append ( os . path . join ( os . environ [ 'VIRTUAL_ENV' ] , 'pip.ini' ) ) \n    if site_config_files : \n        self . pip_config_locations . extend ( site_config_files ) \n    index_url = None \n    custom_config = None \n    if 'PIP_INDEX_URL' in os . environ and os . environ [ 'PIP_INDEX_URL' ] : \n        index_url = os . environ [ 'PIP_INDEX_URL' ] \n        custom_config = 'PIP_INDEX_URL environment variable' \n    else : \n        for pip_config_filename in self . pip_config_locations : \n            if pip_config_filename . startswith ( '~' ) : \n                pip_config_filename = os . path . expanduser ( pip_config_filename ) \n            if os . path . isfile ( pip_config_filename ) : \n                config = ConfigParser ( ) \n                config . read ( [ pip_config_filename ] ) \n                try : \n                    index_url = config . get ( 'global' , 'index-url' ) \n                    custom_config = pip_config_filename \n                    break \n                except ( NoOptionError , NoSectionError ) : \n                    pass \n    if index_url : \n        self . PYPI_API_URL = self . _prepare_api_url ( index_url ) \n        pprint . pprint ( Color ( 'Setting API url to {{autoyellow}}{}{{/autoyellow}} as found in {{autoyellow}}{}{{/autoyellow}}' '. Use --default-index-url to use pypi default index' . format ( self . PYPI_API_URL , custom_config ) ) ) "}
{"6357": "\nimport pprint \ndef pair ( cmd , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    key = pair_key ( word ) \n    tokens = [ t . decode ( ) for t in DB . smembers ( key ) ] \n    tokens . sort ( ) \n    pprint . pprint ( white ( tokens ) ) \n    pprint . pprint ( magenta ( '(Total: {})' . format ( len ( tokens ) ) ) ) "}
{"6358": "\nimport pprint \ndef do_AUTOCOMPLETE ( cmd , s ) : \n    s = list ( preprocess_query ( s ) ) [ 0 ] \n    keys = [ k . decode ( ) for k in DB . smembers ( edge_ngram_key ( s ) ) ] \n    pprint . pprint ( white ( keys ) ) \n    pprint . pprint ( magenta ( '({} elements)' . format ( len ( keys ) ) ) ) "}
{"6363": "\nimport pprint \ndef do_fuzzy ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    pprint . pprint ( white ( make_fuzzy ( word ) ) ) "}
{"6364": "\nimport pprint \ndef do_fuzzyindex ( self , word ) : \n    word = list ( preprocess_query ( word ) ) [ 0 ] \n    token = Token ( word ) \n    neighbors = make_fuzzy ( token ) \n    neighbors = [ ( n , DB . zcard ( dbkeys . token_key ( n ) ) ) for n in neighbors ] \n    neighbors . sort ( key = lambda n : n [ 1 ] , reverse = True ) \n    for token , freq in neighbors : \n        if freq == 0 : \n            break \n        pprint . pprint ( white ( token ) , blue ( freq ) ) "}
{"6366": "\nimport pprint \ndef do_help ( self , command ) : \n    if command : \n        doc = getattr ( self , 'do_' + command ) . __doc__ \n        pprint . pprint ( cyan ( doc . replace ( ' ' * 8 , '' ) ) ) \n    else : \n        pprint . pprint ( magenta ( 'Available commands:' ) ) \n        pprint . pprint ( magenta ( 'Type \"HELP <command>\" to get more info.' ) ) \n        names = self . get_names ( ) \n        names . sort ( ) \n        for name in names : \n            if name [ : 3 ] != 'do_' : \n                continue \n            doc = getattr ( self , name ) . __doc__ \n            doc = doc . split ( '\\n' ) [ 0 ] \n            pprint . pprint ( '{} {}' . format ( yellow ( name [ 3 : ] ) , cyan ( doc . replace ( ' ' * 8 , ' ' ) . replace ( '\\n' , '' ) ) ) ) "}
{"6367": "\nimport pprint \ndef do_DBINFO ( self , * args ) : \n    info = DB . info ( ) \n    keys = [ 'keyspace_misses' , 'keyspace_hits' , 'used_memory_human' , 'total_commands_processed' , 'total_connections_received' , 'connected_clients' ] \n    for key in keys : \n        pprint . pprint ( '{}: {}' . format ( white ( key ) , blue ( info [ key ] ) ) ) \n    nb_of_redis_db = int ( DB . config_get ( 'databases' ) [ 'databases' ] ) \n    for db_index in range ( nb_of_redis_db - 1 ) : \n        db_name = 'db{}' . format ( db_index ) \n        if db_name in info : \n            label = white ( 'nb keys (db {})' . format ( db_index ) ) \n            pprint . pprint ( '{}: {}' . format ( label , blue ( info [ db_name ] [ 'keys' ] ) ) ) "}
{"6368": "\nimport pprint \ndef do_DBKEY ( self , key ) : \n    type_ = DB . type ( key ) . decode ( ) \n    if type_ == 'set' : \n        out = DB . smembers ( key ) \n    elif type_ == 'string' : \n        out = DB . get ( key ) \n    else : \n        out = 'Unsupported type {}' . format ( type_ ) \n    pprint . pprint ( 'type:' , magenta ( type_ ) ) \n    pprint . pprint ( 'value:' , white ( out ) ) "}
{"6369": "\nimport pprint \ndef do_GEOHASH ( self , latlon ) : \n    try : \n        lat , lon = map ( float , latlon . split ( ) ) \n    except ValueError : \n        pprint . pprint ( red ( 'Invalid lat and lon {}' . format ( latlon ) ) ) \n    else : \n        pprint . pprint ( white ( geohash . encode ( lat , lon , config . GEOHASH_PRECISION ) ) ) "}
{"6370": "\nimport pprint \ndef do_GET ( self , _id ) : \n    doc = doc_by_id ( _id ) \n    if not doc : \n        return self . error ( 'id \"{}\" not found' . format ( _id ) ) \n    for key , value in doc . items ( ) : \n        if key == config . HOUSENUMBERS_FIELD : \n            continue \n        pprint . pprint ( '{} {}' . format ( white ( key ) , magenta ( value ) ) ) \n    if doc . get ( 'housenumbers' ) : \n        def sorter ( v ) : \n            try : \n                return int ( re . match ( r'^\\d+' , v [ 'raw' ] ) . group ( ) ) \n            except AttributeError : \n                return - 1 \n        housenumbers = sorted ( doc [ 'housenumbers' ] . values ( ) , key = sorter ) \n        pprint . pprint ( white ( 'housenumbers' ) , magenta ( ', ' . join ( v [ 'raw' ] for v in housenumbers ) ) ) "}
{"6372": "\nimport pprint \ndef do_BESTSCORE ( self , word ) : \n    key = keys . token_key ( indexed_string ( word ) [ 0 ] ) \n    for _id , score in DB . zrevrange ( key , 0 , 20 , withscores = True ) : \n        result = Result ( _id ) \n        pprint . pprint ( white ( result ) , blue ( score ) , green ( result . _id ) ) "}
{"6373": "\nimport pprint \ndef do_STRDISTANCE ( self , s ) : \n    s = s . split ( '|' ) \n    if not len ( s ) == 2 : \n        pprint . pprint ( red ( 'Malformed string. Use | between the two strings.' ) ) \n        return \n    one , two = s \n    pprint . pprint ( white ( compare_str ( one , two ) ) ) "}
{"6569": "\nimport pprint \ndef _set_missing_to_none ( self , currency ) : \n    rates = self . _rates [ currency ] \n    first_date , last_date = self . bounds [ currency ] \n    for date in list_dates_between ( first_date , last_date ) : \n        if date not in rates : \n            rates [ date ] = None \n    if self . verbose : \n        missing = len ( [ r for r in itervalues ( rates ) if r is None ] ) \n        if missing : \n            pprint . pprint ( '{0}: {1} missing rates from {2} to {3} ({4} days)' . format ( currency , missing , first_date , last_date , 1 + ( last_date - first_date ) . days ) ) "}
{"6570": "\nimport pprint \ndef _compute_missing_rates ( self , currency ) : \n    rates = self . _rates [ currency ] \n    tmp = defaultdict ( lambda : [ None , None ] ) \n    for date in sorted ( rates ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist += 1 \n            tmp [ date ] [ 0 ] = closest_rate , dist \n    for date in sorted ( rates , reverse = True ) : \n        rate = rates [ date ] \n        if rate is not None : \n            closest_rate = rate \n            dist = 0 \n        else : \n            dist += 1 \n            tmp [ date ] [ 1 ] = closest_rate , dist \n    for date in sorted ( tmp ) : \n        ( r0 , d0 ) , ( r1 , d1 ) = tmp [ date ] \n        rates [ date ] = ( r0 * d1 + r1 * d0 ) / ( d0 + d1 ) \n        if self . verbose : \n            pprint . pprint ( ( '{0}: filling {1} missing rate using {2} ({3}d old) and ' '{4} ({5}d later)' ) . format ( currency , date , r0 , d0 , r1 , d1 ) ) "}
{"6571": "\nimport pprint \ndef _get_rate ( self , currency , date ) : \n    if currency == self . ref_currency : \n        return 1.0 \n    if date not in self . _rates [ currency ] : \n        first_date , last_date = self . bounds [ currency ] \n        if not self . fallback_on_wrong_date : \n            raise RateNotFoundError ( '{0} not in {1} bounds {2}/{3}' . format ( date , currency , first_date , last_date ) ) \n        if date < first_date : \n            fallback_date = first_date \n        elif date > last_date : \n            fallback_date = last_date \n        else : \n            raise AssertionError ( 'Should never happen, bug in the code!' ) \n        if self . verbose : \n            pprint . pprint ( r'/!\\ {0} not in {1} bounds {2}/{3}, falling back to {4}' . format ( date , currency , first_date , last_date , fallback_date ) ) \n        date = fallback_date \n    rate = self . _rates [ currency ] [ date ] \n    if rate is None : \n        raise RateNotFoundError ( '{0} has no rate for {1}' . format ( currency , date ) ) \n    return rate "}
{"6612": "\nimport pprint \ndef last ( symbol : str ) : \n    app = PriceDbApplication ( ) \n    if symbol : \n        symbol = symbol . upper ( ) \n        sec_symbol = SecuritySymbol ( \"\" , \"\" ) \n        sec_symbol . parse ( symbol ) \n        latest = app . get_latest_price ( sec_symbol ) \n        assert isinstance ( latest , PriceModel ) \n        pprint . pprint ( f\"{latest}\" ) \n    else : \n        latest = app . get_latest_prices ( ) \n        for price in latest : \n            pprint . pprint ( f\"{price}\" ) "}
{"6613": "\nimport pprint \ndef list_prices ( date , currency , last ) : \n    app = PriceDbApplication ( ) \n    app . logger = logger \n    if last : \n        prices = app . get_latest_prices ( ) \n    else : \n        prices = app . get_prices ( date , currency ) \n    for price in prices : \n        pprint . pprint ( price ) \n    pprint . pprint ( f\"{len(prices)} records found.\" ) "}
{"6615": "\nimport pprint \ndef prune ( symbol : str , all : str ) : \n    app = PriceDbApplication ( ) \n    app . logger = logger \n    count = 0 \n    if symbol is not None : \n        sec_symbol = SecuritySymbol ( \"\" , \"\" ) \n        sec_symbol . parse ( symbol ) \n        deleted = app . prune ( sec_symbol ) \n        if deleted : \n            count = 1 \n    else : \n        count = app . prune_all ( ) \n    pprint . pprint ( f\"Removed {count} old price entries.\" ) "}
{"6702": "\nimport pprint \ndef logs ( self , name = None ) : \n    content = None \n    results = self . _list_logs ( ) \n    pprint . pprint ( results ) \n    if name is not None : \n        for result in results : \n            matches = False \n            if name in result . name : \n                matches = True \n            for key , val in result . metadata . items ( ) : \n                if name in val : \n                    matches = True \n            if matches is True : \n                content = self . _print_log ( result . name ) \n    else : \n        if len ( results ) > 0 : \n            latest = results [ 0 ] \n            for result in results : \n                if result . time_created >= latest . time_created : \n                    latest = result \n            content = self . _print_log ( result . name ) \n    return content "}
{"6707": "\nimport pprint \ndef status ( backend ) : \n    pprint . pprint ( '[backend status]' ) \n    settings = read_client_secrets ( ) \n    pprint . pprint ( 'There are %s clients found in secrets.' % len ( settings ) ) \n    if 'SREGISTRY_CLIENT' in settings : \n        pprint . pprint ( 'active: %s' % settings [ 'SREGISTRY_CLIENT' ] ) \n        update_secrets ( settings ) \n    else : \n        pprint . pprint ( 'There is no active client.' ) "}
{"6708": "\nimport pprint \ndef add ( backend , variable , value , force = False ) : \n    pprint . pprint ( '[add]' ) \n    settings = read_client_secrets ( ) \n    prefix = 'SREGISTRY_%s_' % backend . upper ( ) \n    if not variable . startswith ( prefix ) : \n        variable = '%s%s' % ( prefix , variable ) \n    variable = variable . upper ( ) \n    bot . info ( \"%s %s\" % ( variable , value ) ) \n    if backend in settings : \n        if variable in settings [ backend ] and force is False : \n            previous = settings [ backend ] [ variable ] \n            bot . error ( '%s is already set as %s. Use --force to override.' % ( variable , previous ) ) \n            sys . exit ( 1 ) \n    if backend not in settings : \n        settings [ backend ] = { } \n    settings [ backend ] [ variable ] = value \n    update_secrets ( settings ) "}
{"6709": "\nimport pprint \ndef remove ( backend , variable ) : \n    pprint . pprint ( '[remove]' ) \n    settings = read_client_secrets ( ) \n    prefixed = variable \n    prefix = 'SREGISTRY_%s_' % backend . upper ( ) \n    if not variable . startswith ( prefix ) : \n        prefixed = '%s%s' % ( prefix , variable ) \n    variable = variable . upper ( ) \n    bot . info ( variable ) \n    if backend in settings : \n        if variable in settings [ backend ] : \n            del settings [ backend ] [ variable ] \n        if prefixed in settings [ backend ] : \n            del settings [ backend ] [ prefixed ] \n        update_secrets ( settings ) "}
{"6710": "\nimport pprint \ndef activate ( backend ) : \n    settings = read_client_secrets ( ) \n    if backend is not None : \n        settings [ 'SREGISTRY_CLIENT' ] = backend \n        update_secrets ( settings ) \n        pprint . pprint ( '[activate] %s' % backend ) "}
{"6711": "\nimport pprint \ndef delete_backend ( backend ) : \n    settings = read_client_secrets ( ) \n    if backend in settings : \n        del settings [ backend ] \n        if 'SREGISTRY_CLIENT' in settings : \n            if settings [ 'SREGISTRY_CLIENT' ] == backend : \n                del settings [ 'SREGISTRY_CLIENT' ] \n        update_secrets ( settings ) \n        pprint . pprint ( '[delete] %s' % backend ) \n    else : \n        if backend is not None : \n            pprint . pprint ( '%s is not a known client.' % backend ) \n        else : \n            pprint . pprint ( 'Please specify a backend to delete.' ) "}
{"6756": "\nimport pprint \ndef main ( args , parser , subparser ) : \n    from sregistry . main import get_client \n    images = args . image \n    if not isinstance ( images , list ) : \n        images = [ images ] \n    for image in images : \n        pprint . pprint ( image ) \n        cli = get_client ( image , quiet = args . quiet ) \n        cli . announce ( args . command ) \n        cli . share ( image , share_to = args . share_to ) "}
{"6771": "\nimport pprint \ndef extract_tar ( archive , output_folder , handle_whiteout = False ) : \n    from . terminal import run_command \n    if handle_whiteout is True : \n        return _extract_tar ( archive , output_folder ) \n    args = '-xf' \n    if archive . endswith ( \".tar.gz\" ) : \n        args = '-xzf' \n    command = [ \"tar\" , args , archive , \"-C\" , output_folder , \"--exclude=dev/*\" ] \n    if not bot . is_quiet ( ) : \n        pprint . pprint ( \"Extracting %s\" % archive ) \n    return run_command ( command ) "}
{"6772": "\nimport pprint \ndef _extract_tar ( archive , output_folder ) : \n    from . terminal import ( run_command , which ) \n    result = which ( 'blob2oci' ) \n    if result [ 'return_code' ] != 0 : \n        bot . error ( 'Cannot find blob2oci script on path, exiting.' ) \n        sys . exit ( 1 ) \n    script = result [ 'message' ] \n    command = [ 'exec' , script , '--layer' , archive , '--extract' , output_folder ] \n    if not bot . is_quiet ( ) : \n        pprint . pprint ( \"Extracting %s\" % archive ) \n    return run_command ( command ) "}
{"6782": "\nimport pprint \ndef inspect ( self , name ) : \n    pprint . pprint ( name ) \n    container = self . get ( name ) \n    if container is not None : \n        collection = container . collection . name \n        fields = container . __dict__ . copy ( ) \n        fields [ 'collection' ] = collection \n        fields [ 'metrics' ] = json . loads ( fields [ 'metrics' ] ) \n        del fields [ '_sa_instance_state' ] \n        fields [ 'created_at' ] = str ( fields [ 'created_at' ] ) \n        pprint . pprint ( json . dumps ( fields , indent = 4 , sort_keys = True ) ) \n        return fields "}
{"6787": "\nimport pprint \ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    self . require_secrets ( ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20 \n    if names [ 'registry' ] == None : \n        names [ 'registry' ] = self . base \n    names = self . _add_https ( names ) \n    url = '%s/push/' % names [ 'registry' ] \n    auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] \n    SREGISTRY_EVENT = self . authorize ( request_type = \"push\" , names = names ) \n    fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    r = requests . post ( auth_url , json = fields , headers = headers ) \n    message = self . _read_response ( r ) \n    pprint . pprint ( '\\n[1. Collection return status {0} {1}]' . format ( r . status_code , message ) ) \n    if r . status_code != 200 : \n        sys . exit ( 1 ) \n    url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) \n    bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) \n    cid = r . json ( ) [ 'cid' ] \n    upload_to = os . path . basename ( names [ 'storage' ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"upload\" , names = names ) \n    encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) \n    progress_callback = create_callback ( encoder , self . quiet ) \n    monitor = MultipartEncoderMonitor ( encoder , progress_callback ) \n    headers = { 'Content-Type' : monitor . content_type , 'Authorization' : SREGISTRY_EVENT } \n    try : \n        r = requests . post ( url , data = monitor , headers = headers ) \n        r . raise_for_status ( ) \n        message = r . json ( ) [ 'message' ] \n        pprint . pprint ( '\\n[Return status {0} {1}]' . format ( r . status_code , message ) ) \n    except requests . HTTPError as e : \n        pprint . pprint ( '\\nUpload failed: {0}.' . format ( e ) ) \n    except KeyboardInterrupt : \n        pprint . pprint ( '\\nUpload cancelled.' ) \n    except Exception as e : \n        pprint . pprint ( e ) "}
{"6832": "\nimport pprint \ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100 , verbose = False ) : \n    f , j = get_f_and_j ( mu ) \n    if nt > 1 : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    if verbose : \n        pprint . pprint ( nfo ) \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , 1 ] , 'g--' ) \n        plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2 ) \n        if savefig == 'None' : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6884": "\nimport pprint \ndef fileLoad ( self , filePath = None , updatePath = True ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        raise FileNotFoundError ( \"Data file '%s' does not exist.\" % ( filePath ) ) \n    else : \n        pprint . pprint ( \"Importing existing data file '%s' ... \" % ( filePath ) , end = \"\" , flush = True ) \n        with open ( filePath , \"r\" ) as q : \n            data_raw = q . read ( ) \n        pprint . pprint ( \"Imported!\" ) \n        self . data = json . loads ( data_raw ) \n        if updatePath : \n            self . filePath = filePath "}
{"6885": "\nimport pprint \ndef fileSave ( self , filePath = None , updatePath = False ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        pprint . pprint ( \"Data file '%s' does not exist, will create new file.\" % ( filePath ) ) \n        if not os . path . exists ( os . path . split ( filePath ) [ 0 ] ) : \n            os . makedirs ( os . path . split ( filePath ) [ 0 ] ) \n    dataJsonString = json . dumps ( self . data , indent = 4 , sort_keys = True ) \n    pprint . pprint ( \"Writing to file '%s' ... \" % ( filePath ) , end = \"\" , flush = True ) \n    with open ( filePath , \"w\" ) as fileout : \n        fileout . write ( dataJsonString ) \n    pprint . pprint ( \"Wrote file!\" ) \n    if updatePath : \n        self . filePath = filePath "}
{"6954": "\nimport pprint \ndef set_feature_transform ( self , mode = 'polynomial' , degree = 1 ) : \n    if self . status != 'load_train_data' : \n        pprint . pprint ( \"Please load train data first.\" ) \n        return self . train_X \n    self . feature_transform_mode = mode \n    self . feature_transform_degree = degree \n    self . train_X = self . train_X [ : , 1 : ] \n    self . train_X = utility . DatasetLoader . feature_transform ( self . train_X , self . feature_transform_mode , self . feature_transform_degree ) \n    return self . train_X "}
{"6955": "\nimport pprint \ndef prediction ( self , input_data = '' , mode = 'test_data' ) : \n    prediction = { } \n    if ( self . status != 'train' ) : \n        pprint . pprint ( \"Please load train data and init W then train the W first.\" ) \n        return prediction \n    if ( input_data == '' ) : \n        pprint . pprint ( \"Please input test data for prediction.\" ) \n        return prediction \n    if mode == 'future_data' : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : None , \"prediction\" : prediction } \n    else : \n        data = input_data . split ( ) \n        input_data_x = [ float ( v ) for v in data [ : - 1 ] ] \n        input_data_x = utility . DatasetLoader . feature_transform ( np . array ( input_data_x ) . reshape ( 1 , - 1 ) , self . feature_transform_mode , self . feature_transform_degree ) \n        input_data_x = np . ravel ( input_data_x ) \n        input_data_y = float ( data [ - 1 ] ) \n        prediction = self . score_function ( input_data_x , self . W ) \n        return { \"input_data_x\" : input_data_x , \"input_data_y\" : input_data_y , \"prediction\" : prediction } "}
{"7047": "\nimport pprint \ndef parse_log ( self , bowtie_log ) : \n    pprint . pprint ( \"is here!\" ) \n    regexes = { 'unpaired' : { 'unpaired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" , 'unpaired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'unpaired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" } , 'paired' : { 'paired_aligned_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly 0 times\" , 'paired_aligned_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly exactly 1 time\" , 'paired_aligned_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly >1 times\" , 'paired_aligned_discord_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly 1 time\" , 'paired_aligned_discord_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly >1 times\" , 'paired_aligned_mate_one' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\" , 'paired_aligned_mate_multi' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\" , 'paired_aligned_mate_none' : r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\" } } \n    with open ( bowtie_log , \"r\" ) as f : \n        for l in f : \n            pprint . pprint ( l ) \n            total = re . search ( r\"(\\\\d+) reads; of these:\" , l ) \n            pprint . pprint ( total ) \n            if total : \n                pprint . pprint ( total ) \n                self . set_n_reads ( total . group ( 1 ) ) \n            paired = re . search ( r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) were paired; of these:\" , l ) \n            if paired : \n                paired_total = int ( paired . group ( 1 ) ) \n                paired_numbers = { } \n                l = f . readline ( ) \n                while l . startswith ( '    ' ) : \n                    for k , r in regexes [ 'paired' ] . items ( ) : \n                        match = re . search ( r , l ) \n                        if match : \n                            paired_numbers [ k ] = int ( match . group ( 1 ) ) \n                    l = f . readline ( ) \n                align_zero_times = paired_numbers [ 'paired_aligned_none' ] + paired_numbers [ 'paired_aligned_mate_none' ] \n                if align_zero_times : \n                    self . set_align_0x ( align_zero_times ) \n                align_one_time = paired_numbers [ 'paired_aligned_one' ] + paired_numbers [ 'paired_aligned_mate_one' ] \n                if align_one_time : \n                    self . set_align_1x ( align_one_time ) \n                align_more_than_one_time = paired_numbers [ 'paired_aligned_multi' ] + paired_numbers [ 'paired_aligned_mate_multi' ] \n                if align_more_than_one_time : \n                    self . set_align_mt1x ( align_more_than_one_time ) \n            overall = re . search ( r\"([\\\\d\\\\.]+)% overall alignment rate\" , l ) \n            if overall : \n                self . overall_rate = float ( overall . group ( 1 ) ) "}
{"7098": "\nimport pprint \ndef main ( newick ) : \n    logger . info ( \"Starting newick file processing\" ) \n    pprint . pprint ( newick ) \n    tree = dendropy . Tree . get ( file = open ( newick , 'r' ) , schema = \"newick\" ) \n    tree . reroot_at_midpoint ( ) \n    to_write = tree . as_string ( \"newick\" ) . strip ( ) . replace ( \"[&R] \" , '' ) . replace ( ' ' , '_' ) . replace ( \"'\" , \"\" ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_dic = { \"treeData\" : [ { \"trees\" : [ to_write ] } ] , } \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        status_fh . write ( \"pass\" ) "}
{"7192": "\nimport pprint \ndef print_table ( language ) : \n    table = translation_table ( language ) \n    for code , name in sorted ( table . items ( ) , key = operator . itemgetter ( 0 ) ) : \n        pprint . pprint ( u'{language:<8} {name:\\u3000<20}' . format ( name = name , language = code ) ) \n    return None "}
{"7196": "\nimport pprint \ndef set ( self , node_ids , variable = None , name = \"tmp\" ) : \n    if variable is None : \n        variable = pd . Series ( np . ones ( len ( node_ids ) ) , index = node_ids . index ) \n    df = pd . DataFrame ( { name : variable , \"node_idx\" : self . _node_indexes ( node_ids ) } ) \n    length = len ( df ) \n    df = df . dropna ( how = \"any\" ) \n    newl = len ( df ) \n    if length - newl > 0 : \n        pprint . pprint ( \"Removed %d rows because they contain missing values\" % ( length - newl ) ) \n    self . variable_names . add ( name ) \n    self . net . initialize_access_var ( name . encode ( 'utf-8' ) , df . node_idx . values . astype ( 'int' ) , df [ name ] . values . astype ( 'double' ) ) "}
{"7262": "\nimport pprint \ndef find_best_root ( self , force_positive = True , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if n == self . tree . root : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( chisq < best_root [ \"chisq\" ] ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( 1 - x ) , var * ( 1 - x ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if reg [ \"slope\" ] >= 0 or ( force_positive == False ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        pprint . pprint ( \"TreeRegression.find_best_root: No valid root found!\" , force_positive ) \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( 1 - y ) , var * ( 1 - y ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3 , 3 ) ) \n        estimator_hessian [ : 2 , : 2 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2 , 2 ] = ( deriv [ 0 ] [ 1 ] + deriv [ 1 ] [ 1 ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ 0 ] [ 0 ] - deriv [ 1 ] [ 0 ] ) ** 2 \n        estimator_hessian [ 0 , 2 ] = estimator_hessian [ 2 , 0 ] \n        estimator_hessian [ 1 , 2 ] = estimator_hessian [ 2 , 1 ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7287": "\nimport pprint \ndef optimize_branch_length_global ( self , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length_global: running branch length optimization...\" , 1 ) \n    def neg_log ( s ) : \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            n . branch_length = si ** 2 \n        self . infer_ancestral_sequences ( marginal = True ) \n        gradient = [ ] \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            if n . up : \n                pp , pc = self . marginal_branch_profile ( n ) \n                Qtds = self . gtr . expQsds ( si ) . T \n                Qt = self . gtr . expQs ( si ) . T \n                res = pp . dot ( Qt ) \n                overlap = np . sum ( res * pc , axis = 1 ) \n                res_ds = pp . dot ( Qtds ) \n                overlap_ds = np . sum ( res_ds * pc , axis = 1 ) \n                logP = np . sum ( self . multiplicity * overlap_ds / overlap ) \n                gradient . append ( logP ) \n            else : \n                gradient . append ( 2 * ( si ** 2 - 0.001 ) ) \n        pprint . pprint ( - self . tree . sequence_marginal_LH ) \n        return ( - self . tree . sequence_marginal_LH + ( s [ 0 ] ** 2 - 0.001 ) ** 2 , - 1.0 * np . array ( gradient ) ) \n    from scipy . optimize import minimize \n    x0 = np . sqrt ( [ n . branch_length for n in self . tree . find_clades ( order = 'preorder' ) ] ) \n    sol = minimize ( neg_log , x0 , jac = True ) \n    for new_len , node in zip ( sol [ 'x' ] , self . tree . find_clades ( ) ) : \n        self . logger ( \"Optimization results: old_len=%.4f, new_len=%.4f \" \" Updating branch length...\" % ( node . branch_length , new_len ) , 5 ) \n        node . branch_length = new_len ** 2 \n        node . mutation_length = new_len ** 2 \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    self . _prepare_nodes ( ) "}
{"7294": "\nimport pprint \ndef _check_fix_Q ( self , fixed_mu = False ) : \n    self . Pi /= self . Pi . sum ( ) \n    self . W += self . break_degen + self . break_degen . T \n    np . fill_diagonal ( self . W , 0 ) \n    Wdiag = - ( self . Q ) . sum ( axis = 0 ) / self . Pi \n    np . fill_diagonal ( self . W , Wdiag ) \n    scale_factor = - np . sum ( np . diagonal ( self . Q ) * self . Pi ) \n    self . W /= scale_factor \n    if not fixed_mu : \n        self . mu *= scale_factor \n    if ( self . Q . sum ( axis = 0 ) < 1e-10 ) . sum ( ) < self . alphabet . shape [ 0 ] : \n        pprint . pprint ( \"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\" , self . Q . sum ( axis = 0 ) ) \n        import ipdb ; \n        ipdb . set_trace ( ) \n        raise ArithmeticError ( \"Cannot fix the diagonal of the GTR rate matrix.\" ) "}
{"7297": "\nimport pprint \ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = False , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2 , return_log = True ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2 \n        if 'success' not in opt : \n            opt [ 'success' ] = True \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4 , warn = True ) \n    except : \n        import scipy \n        pprint . pprint ( 'legacy scipy' , scipy . __version__ ) \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2 \n        opt = { 'success' : True } \n    if new_len > .9 * ttconf . MAX_BRANCH_LENGTH : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4 , warn = True ) \n    if opt [ \"success\" ] != True : \n        new_len = np . sum ( multiplicity [ seq_pair [ : , 1 ] != seq_pair [ : , 0 ] ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7305": "\nimport pprint \ndef print_lh ( self , joint = True ) : \n    try : \n        u_lh = self . tree . unconstrained_sequence_LH \n        if joint : \n            s_lh = self . tree . sequence_joint_LH \n            t_lh = self . tree . positional_joint_LH \n            c_lh = self . tree . coalescent_joint_LH \n        else : \n            s_lh = self . tree . sequence_marginal_LH \n            t_lh = self . tree . positional_marginal_LH \n            c_lh = 0 \n        pprint . pprint ( \"###  Tree Log-Likelihood  ###\\n\" \" Sequence log-LH without constraints: \\t%1.3f\\n\" \" Sequence log-LH with constraints:    \\t%1.3f\\n\" \" TreeTime sequence log-LH:            \\t%1.3f\\n\" \" Coalescent log-LH:                   \\t%1.3f\\n\" \"#########################\" % ( u_lh , s_lh , t_lh , c_lh ) ) \n    except : \n        pprint . pprint ( \"ERROR. Did you run the corresponding inference (joint/marginal)?\" ) "}
{"7308": "\nimport pprint \ndef assure_tree ( params , tmp_dir = 'treetime_tmp' ) : \n    if params . tree is None : \n        params . tree = os . path . basename ( params . aln ) + '.nwk' \n        pprint . pprint ( \"No tree given: inferring tree\" ) \n        utils . tree_inference ( params . aln , params . tree , tmp_dir = tmp_dir ) \n    if os . path . isdir ( tmp_dir ) : \n        shutil . rmtree ( tmp_dir ) \n    try : \n        tt = TreeAnc ( params . tree ) \n    except : \n        pprint . pprint ( \"Tree loading/building failed.\" ) \n        return 1 \n    return 0 "}
{"7309": "\nimport pprint \ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2 : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    elif keyval [ 0 ] not in [ 'alphabet' ] : \n                        keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                pprint . pprint ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = False \n        except : \n            pprint . pprint ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = False \n    return gtr "}
{"7310": "\nimport pprint \ndef read_if_vcf ( params ) : \n    ref = None \n    aln = params . aln \n    fixed_pi = None \n    if hasattr ( params , 'aln' ) and params . aln is not None : \n        if any ( [ params . aln . lower ( ) . endswith ( x ) for x in [ '.vcf' , '.vcf.gz' ] ] ) : \n            if not params . vcf_reference : \n                pprint . pprint ( \"ERROR: a reference Fasta is required with VCF-format alignments\" ) \n                return - 1 \n            compress_seq = read_vcf ( params . aln , params . vcf_reference ) \n            sequences = compress_seq [ 'sequences' ] \n            ref = compress_seq [ 'reference' ] \n            aln = sequences \n            if not hasattr ( params , 'gtr' ) or params . gtr == \"infer\" : \n                alpha = alphabets [ 'aa' ] if params . aa else alphabets [ 'nuc' ] \n                fixed_pi = [ ref . count ( base ) / len ( ref ) for base in alpha ] \n                if fixed_pi [ - 1 ] == 0 : \n                    fixed_pi [ - 1 ] = 0.05 \n                    fixed_pi = [ v - 0.01 for v in fixed_pi ] \n    return aln , ref , fixed_pi "}
{"7311": "\nimport pprint \ndef ancestral_reconstruction ( params ) : \n    if assure_tree ( params , tmp_dir = 'ancestral_tmp' ) : \n        return 1 \n    outdir = get_outdir ( params , '_ancestral' ) \n    basename = get_basename ( params , outdir ) \n    gtr = create_gtr ( params ) \n    aln , ref , fixed_pi = read_if_vcf ( params ) \n    is_vcf = True if ref is not None else False \n    treeanc = TreeAnc ( params . tree , aln = aln , ref = ref , gtr = gtr , verbose = 1 , fill_overhangs = not params . keep_overhangs ) \n    ndiff = treeanc . infer_ancestral_sequences ( 'ml' , infer_gtr = params . gtr == 'infer' , marginal = params . marginal , fixed_pi = fixed_pi ) \n    if ndiff == ttconf . ERROR : \n        return 1 \n    if params . gtr == \"infer\" : \n        pprint . pprint ( '\\nInferred GTR model:' ) \n        pprint . pprint ( treeanc . gtr ) \n    export_sequences_and_tree ( treeanc , basename , is_vcf , params . zero_based , report_ambiguous = params . report_ambiguous ) \n    return 0 "}
{"7312": "\nimport pprint \ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( log_prob < 0.693147 ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if L < 2 : \n        pprint . pprint ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7314": "\nimport pprint \ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if n_delta > 1 : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif n_delta == 1 : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n        peak = y_vals . min ( ) \n        ind = ( y_vals - peak ) < BIG_NUMBER / 1000 \n        n_points = ind . sum ( ) \n        if n_points == 0 : \n            pprint . pprint ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n            x_vals = [ 0 , 1 ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n        elif n_points == 1 : \n            res = Distribution . delta_function ( x_vals [ 0 ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7342": "\nimport pprint \ndef interactive_login ( ) : \n    solvebio . access_token = None \n    solvebio . api_key = None \n    client . set_token ( ) \n    domain , email , password = _ask_for_credentials ( ) \n    if not all ( [ domain , email , password ] ) : \n        pprint . pprint ( \"Domain, email, and password are all required.\" ) \n        return \n    try : \n        response = client . post ( '/v1/auth/token' , { 'domain' : domain . replace ( '.solvebio.com' , '' ) , 'email' : email , 'password' : password } ) \n    except SolveError as e : \n        pprint . pprint ( 'Login failed: {0}' . format ( e ) ) \n    else : \n        solvebio . api_key = response [ 'token' ] \n        client . set_token ( ) "}
{"7343": "\nimport pprint \ndef whoami ( * args , ** kwargs ) : \n    user = client . whoami ( ) \n    if user : \n        print_user ( user ) \n    else : \n        pprint . pprint ( 'You are not logged-in.' ) "}
{"7344": "\nimport pprint \ndef print_user ( user ) : \n    email = user [ 'email' ] \n    domain = user [ 'account' ] [ 'domain' ] \n    role = user [ 'role' ] \n    pprint . pprint ( 'You are logged-in to the \"{0}\" domain ' 'as {1} with role {2}.' . format ( domain , email , role ) ) "}
{"7354": "\nimport pprint \ndef download_vault_folder ( remote_path , local_path , dry_run = False , force = False ) : \n    local_path = os . path . normpath ( os . path . expanduser ( local_path ) ) \n    if not os . access ( local_path , os . W_OK ) : \n        raise Exception ( 'Write access to local path ({}) is required' . format ( local_path ) ) \n    full_path , path_dict = solvebio . Object . validate_full_path ( remote_path ) \n    vault = solvebio . Vault . get_by_full_path ( path_dict [ 'vault' ] ) \n    pprint . pprint ( 'Downloading all files from {} to {}' . format ( full_path , local_path ) ) \n    if path_dict [ 'path' ] == '/' : \n        parent_object_id = None \n    else : \n        parent_object = solvebio . Object . get_by_full_path ( remote_path , assert_type = 'folder' ) \n        parent_object_id = parent_object . id \n    pprint . pprint ( 'Creating local directory structure at: {}' . format ( local_path ) ) \n    if not os . path . exists ( local_path ) : \n        if not dry_run : \n            os . makedirs ( local_path ) \n    folders = vault . folders ( parent_object_id = parent_object_id ) \n    for f in folders : \n        path = os . path . normpath ( local_path + f . path ) \n        if not os . path . exists ( path ) : \n            pprint . pprint ( 'Creating folder: {}' . format ( path ) ) \n            if not dry_run : \n                os . makedirs ( path ) \n    files = vault . files ( parent_object_id = parent_object_id ) \n    for f in files : \n        path = os . path . normpath ( local_path + f . path ) \n        if os . path . exists ( path ) : \n            if force : \n                pprint . pprint ( 'Deleting local file (force download): {}' . format ( path ) ) \n                if not dry_run : \n                    os . remove ( path ) \n            else : \n                pprint . pprint ( 'Skipping file (already exists): {}' . format ( path ) ) \n                continue \n        pprint . pprint ( 'Downloading file: {}' . format ( path ) ) \n        if not dry_run : \n            f . download ( path ) "}
{"7443": "\nimport pprint \ndef cli ( infile ) : \n    lines = get_file_handle ( infile ) \n    cytobands = parse_cytoband ( lines ) \n    pprint . pprint ( \"Check some coordinates:\" ) \n    pprint . pprint ( \"checking chrom 1 pos 2\" ) \n    intervals = cytobands [ '1' ] [ 2 ] \n    for interval in intervals : \n        pprint . pprint ( interval ) \n        pprint . pprint ( interval . begin ) \n        pprint . pprint ( interval . end ) \n        pprint . pprint ( interval . data ) \n    pprint . pprint ( cytobands [ '1' ] [ 2 ] ) \n    pprint . pprint ( \"checking chrom 8 pos 101677777\" ) \n    pprint . pprint ( cytobands [ '8' ] [ 101677777 ] ) \n    pprint . pprint ( \"checking chrom X pos 4200000 - 6000000\" ) \n    pprint . pprint ( cytobands [ 'X' ] [ 4200000 : 6000000 ] ) "}
{"7570": "\nimport pprint \ndef migrate ( uri : str , archive_uri : str , case_id : str , dry : bool , force : bool ) : \n    scout_client = MongoClient ( uri ) \n    scout_database = scout_client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] \n    scout_adapter = MongoAdapter ( database = scout_database ) \n    scout_case = scout_adapter . case ( case_id ) \n    if not force and scout_case . get ( 'is_migrated' ) : \n        pprint . pprint ( \"case already migrated\" ) \n        return \n    archive_client = MongoClient ( archive_uri ) \n    archive_database = archive_client [ archive_uri . rsplit ( '/' , 1 ) [ - 1 ] ] \n    archive_case = archive_database . case . find_one ( { 'owner' : scout_case [ 'owner' ] , 'display_name' : scout_case [ 'display_name' ] } ) \n    archive_data = archive_info ( archive_database , archive_case ) \n    if dry : \n        pprint . pprint ( ruamel . yaml . safe_dump ( archive_data ) ) \n    else : \n        pass "}
{"7573": "\nimport pprint \ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while len ( term ) < 7 : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    elif description : \n        sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n        for term in sorted_terms : \n            term . pop ( 'genes' ) \n            pprint . pprint ( \"name: {} | {} | {}\" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) \n        context . abort ( ) \n    else : \n        hpo_terms = adapter . hpo_terms ( ) \n    if hpo_terms . count ( ) == 0 : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7773": "\nimport pprint \ndef cli ( context , morbid , genemap , mim2gene , mim_titles , phenotypes ) : \n    from scout . utils . handle import get_file_handle \n    from pprint import pprint as pp \n    pprint . pprint ( \"Morbid file: %s\" % morbid ) \n    pprint . pprint ( \"Genemap file: %s\" % genemap ) \n    pprint . pprint ( \"mim2gene file: %s\" % mim2gene ) \n    pprint . pprint ( \"MimTitles file: %s\" % mim_titles ) \n    if morbid : \n        morbid_handle = get_file_handle ( morbid ) \n    if genemap : \n        genemap_handle = get_file_handle ( genemap ) \n    if mim2gene : \n        mim2gene_handle = get_file_handle ( mim2gene ) \n    if mim_titles : \n        mimtitles_handle = get_file_handle ( mim_titles ) \n    mim_genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for entry in mim_genes : \n        if entry == 'C10orf11' : \n            pp ( mim_genes [ entry ] ) \n    context . abort ( ) \n    if phenotypes : \n        if not genemap : \n            click . echo ( \"Please provide the genemap file\" ) \n            context . abort ( ) \n        phenotypes = get_mim_phenotypes ( genemap_handle ) \n        for i , mim_term in enumerate ( phenotypes ) : \n            pass \n    pprint . pprint ( \"Number of phenotypes found: %s\" % i ) \n    context . abort ( ) \n    genes = get_mim_genes ( genemap_handle , mim2gene_handle ) \n    for hgnc_symbol in genes : \n        if hgnc_symbol == 'OPA1' : \n            pprint . pprint ( genes [ hgnc_symbol ] ) "}
{"7799": "\nimport pprint \ndef read_hdf5 ( self , filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    pprint . pprint ( \"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\" ) \n    self . header = { } \n    self . filename = filename \n    self . h5 = h5py . File ( filename ) \n    for key , val in self . h5 [ b'data' ] . attrs . items ( ) : \n        if six . PY3 : \n            key = bytes ( key , 'ascii' ) \n        if key == b'src_raj' : \n            self . header [ key ] = Angle ( val , unit = 'hr' ) \n        elif key == b'src_dej' : \n            self . header [ key ] = Angle ( val , unit = 'deg' ) \n        else : \n            self . header [ key ] = val \n    self . n_ints_in_file = self . h5 [ b\"data\" ] . shape [ 0 ] \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    if load_data : \n        self . data = self . h5 [ b\"data\" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] \n        self . file_size_bytes = os . path . getsize ( self . filename ) \n    else : \n        pprint . pprint ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] ) \n        self . n_ints_in_file = 0 \n        self . file_size_bytes = os . path . getsize ( self . filename ) "}
{"7802": "\nimport pprint \ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4 ) \n    elif n_bytes == 4 : \n        dd_type = b'float32' \n    elif n_bytes == 2 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            pprint . pprint ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        pprint . pprint ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7805": "\nimport pprint \ndef info ( self ) : \n    for key , val in self . header . items ( ) : \n        if key == b'src_raj' : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if key == b'src_dej' : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        if key == b'tsamp' : \n            val *= u . second \n        if key in ( 'foff' , 'fch1' ) : \n            val *= u . MHz \n        if key == b'tstart' : \n            pprint . pprint ( \"%16s : %32s\" % ( \"tstart (ISOT)\" , Time ( val , format = 'mjd' ) . isot ) ) \n            key = \"tstart (MJD)\" \n        pprint . pprint ( \"%16s : %32s\" % ( key , val ) ) \n    pprint . pprint ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n_ints_in_file ) ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"Data shape\" , self . data . shape ) ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"Start freq (MHz)\" , self . freqs [ 0 ] ) ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"Stop freq (MHz)\" , self . freqs [ - 1 ] ) ) "}
{"7809": "\nimport pprint \ndef write_to_filterbank ( self , filename_out ) : \n    pprint . pprint ( \"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\" ) \n    n_bytes = int ( self . header [ b'nbits' ] / 8 ) \n    with open ( filename_out , \"wb\" ) as fileh : \n        fileh . write ( generate_sigproc_header ( self ) ) \n        j = self . data \n        if n_bytes == 4 : \n            np . float32 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 2 : \n            np . int16 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 1 : \n            np . int8 ( j . ravel ( ) ) . tofile ( fileh ) "}
{"7813": "\nimport pprint \ndef calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = True , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( diode_cross , max_load = 150 ) \n    cross_dat = obs . data \n    tsamp = obs . header [ 'tsamp' ] \n    dio_ncoarse = obs . calc_n_coarse_chan ( ) \n    dio_nchans = obs . header [ 'nchans' ] \n    dio_chan_per_coarse = dio_nchans / dio_ncoarse \n    obs = None \n    Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) \n    cross_dat = None \n    pprint . pprint ( 'Calculating Mueller Matrix variables' ) \n    gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    Idat = None \n    Qdat = None \n    Udat = None \n    Vdat = None \n    pprint . pprint ( 'Opening ' + cross_pols ) \n    cross_obs = Waterfall ( cross_pols , max_load = 150 ) \n    obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) \n    obs_nchans = cross_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = obs_nchans / obs_ncoarse \n    pprint . pprint ( 'Grabbing Stokes parameters' ) \n    I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) \n    pprint . pprint ( 'Applying Mueller Matrix' ) \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) \n    if onefile == True : \n        cross_obs . data [ : , 0 , : ] = np . squeeze ( I ) \n        cross_obs . data [ : , 1 , : ] = np . squeeze ( Q ) \n        cross_obs . data [ : , 2 , : ] = np . squeeze ( U ) \n        cross_obs . data [ : , 3 , : ] = np . squeeze ( V ) \n        cross_obs . write_to_fil ( cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        pprint . pprint ( 'Calibrated Stokes parameters written to ' + cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) \n        return \n    obs = Waterfall ( obs_I , max_load = 150 ) \n    obs . data = I \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    pprint . pprint ( 'Calibrated Stokes I written to ' + cross_pols [ : - 15 ] + '.SI.polcal.fil' ) \n    obs . data = Q \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    pprint . pprint ( 'Calibrated Stokes Q written to ' + cross_pols [ : - 15 ] + '.Q.polcal.fil' ) \n    obs . data = U \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    pprint . pprint ( 'Calibrated Stokes U written to ' + cross_pols [ : - 15 ] + '.U.polcal.fil' ) \n    obs . data = V \n    obs . write_to_fil ( cross_pols [ : - 15 ] + '.V.polcal.fil' ) \n    pprint . pprint ( 'Calibrated Stokes V written to ' + cross_pols [ : - 15 ] + '.V.polcal.fil' ) "}
{"7823": "\nimport pprint \ndef open_file ( filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True , max_load = 1. ) : \n    if not os . path . isfile ( filename ) : \n        type ( filename ) \n        pprint . pprint ( filename ) \n        raise IOError ( \"No such file or directory: \" + filename ) \n    filename = os . path . expandvars ( os . path . expanduser ( filename ) ) \n    ext = filename . split ( \".\" ) [ - 1 ] . strip ( ) . lower ( ) \n    if six . PY3 : \n        ext = bytes ( ext , 'ascii' ) \n    if h5py . is_hdf5 ( filename ) : \n        return H5Reader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    elif sigproc . is_filterbank ( filename ) : \n        return FilReader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) \n    else : \n        raise NotImplementedError ( 'Cannot open this type of file with Waterfall' ) "}
{"7839": "\nimport pprint \ndef info ( self ) : \n    pprint . pprint ( \"\\n--- File Info ---\" ) \n    for key , val in self . file_header . items ( ) : \n        if key == 'src_raj' : \n            val = val . to_string ( unit = u . hour , sep = ':' ) \n        if key == 'src_dej' : \n            val = val . to_string ( unit = u . deg , sep = ':' ) \n        pprint . pprint ( \"%16s : %32s\" % ( key , val ) ) \n    pprint . pprint ( \"\\n%16s : %32s\" % ( \"Num ints in file\" , self . n_ints_in_file ) ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"File shape\" , self . file_shape ) ) \n    pprint . pprint ( \"--- Selection Info ---\" ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"Data selection shape\" , self . selection_shape ) ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"Minimum freq (MHz)\" , self . container . f_start ) ) \n    pprint . pprint ( \"%16s : %32s\" % ( \"Maximum freq (MHz)\" , self . container . f_stop ) ) "}
{"7849": "\nimport pprint \ndef print_stats ( self ) : \n    header , data = self . read_next_data_block ( ) \n    data = data . view ( 'float32' ) \n    pprint . pprint ( \"AVG: %2.3f\" % data . mean ( ) ) \n    pprint . pprint ( \"STD: %2.3f\" % data . std ( ) ) \n    pprint . pprint ( \"MAX: %2.3f\" % data . max ( ) ) \n    pprint . pprint ( \"MIN: %2.3f\" % data . min ( ) ) \n    import pylab as plt "}
{"7853": "\nimport pprint \ndef cmd_tool ( args = None ) : \n    if 'bl' in local_host : \n        header_loc = '/usr/local/sigproc/bin/header' \n    else : \n        raise IOError ( 'Script only able to run in BL systems.' ) \n    p = OptionParser ( ) \n    p . set_usage ( 'matchfils <FIL_FILE1> <FIL_FILE2>' ) \n    opts , args = p . parse_args ( sys . argv [ 1 : ] ) \n    file1 = args [ 0 ] \n    file2 = args [ 1 ] \n    make_batch_script ( ) \n    headersize1 = find_header_size ( file1 ) \n    file_size1 = os . path . getsize ( file1 ) \n    command = [ './tail_sum.sh' , file1 , str ( file_size1 - headersize1 ) ] \n    pprint . pprint ( '[matchfils] ' + ' ' . join ( command ) ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum1 = out . split ( ) [ 0 ] \n    pprint . pprint ( '[matchfils] Checksum is:' , check_sum1 ) \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file1 ] \n    pprint . pprint ( '[matchfils] Header information:' ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header1 = out \n    pprint . pprint ( header1 ) \n    out , err = reset_outs ( ) \n    headersize2 = find_header_size ( file2 ) \n    file_size2 = os . path . getsize ( file2 ) \n    command = [ './tail_sum.sh' , file2 , str ( file_size2 - headersize2 ) ] \n    pprint . pprint ( '[matchfils] ' + ' ' . join ( command ) ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    check_sum2 = out . split ( ) [ 0 ] \n    pprint . pprint ( '[matchfils] Checksum is:' , check_sum2 ) \n    if err : \n        raise Error ( 'There is an error.' ) \n    out , err = reset_outs ( ) \n    command = [ header_loc , file2 ] \n    pprint . pprint ( '[matchfils] Header information:' ) \n    proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    ( out , err ) = proc . communicate ( ) \n    header2 = out \n    pprint . pprint ( header2 ) \n    if check_sum1 != check_sum2 : \n        pprint . pprint ( '[matchfils] Booo! Checksum does not match between files.' ) \n    else : \n        pprint . pprint ( '[matchfils] Hooray! Checksum matches between files.' ) \n    os . remove ( 'tail_sum.sh' ) "}
{"7854": "\nimport pprint \ndef cmd_tool ( args = None ) : \n    from argparse import ArgumentParser \n    if not HAS_BITSHUFFLE : \n        pprint . pprint ( \"Error: the bitshuffle library is required to run this script.\" ) \n        exit ( ) \n    parser = ArgumentParser ( description = \"Command line utility for creating HDF5 Raw files.\" ) \n    parser . add_argument ( 'filename' , type = str , help = 'Name of filename to read' ) \n    args = parser . parse_args ( ) \n    fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] \n    filelist = glob . glob ( fileroot + '*.raw' ) \n    filelist = sorted ( filelist ) \n    r = GuppiRaw ( filelist [ 0 ] ) \n    header , data = r . read_next_data_block ( ) \n    dshape = data . shape \n    pprint . pprint ( dshape ) \n    n_blocks_total = 0 \n    for filename in filelist : \n        pprint . pprint ( filename ) \n        r = GuppiRaw ( filename ) \n        n_blocks_total += r . n_blocks \n    pprint . pprint ( n_blocks_total ) \n    full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) \n    h5 = h5py . File ( fileroot + '.h5' , 'w' ) \n    h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' \n    block_size = 0 \n    dset = h5 . create_dataset ( 'data' , shape = full_dshape , dtype = data . dtype ) \n    h5_idx = 0 \n    for filename in filelist : \n        pprint . pprint ( \"\\nReading %s header...\" % filename ) \n        r = GuppiRaw ( filename ) \n        h5 = h5py . File ( filename + '.h5' , 'w' ) \n        header , data = r . read_next_data_block ( ) \n        for ii in range ( 0 , r . n_blocks ) : \n            t0 = time . time ( ) \n            pprint . pprint ( \"Reading block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            header , data = r . read_next_data_block ( ) \n            t1 = time . time ( ) \n            t2 = time . time ( ) \n            pprint . pprint ( \"Writing block %i of %i\" % ( h5_idx + 1 , full_dshape [ 0 ] ) ) \n            dset [ h5_idx , : ] = data \n            t3 = time . time ( ) \n            pprint . pprint ( \"Read: %2.2fs, Write %2.2fs\" % ( ( t1 - t0 ) , ( t3 - t2 ) ) ) \n            h5_idx += 1 \n            for key , value in header . items ( ) : \n                dset . attrs [ key ] = value \n        h5 . close ( ) \n        t1 = time . time ( ) \n        pprint . pprint ( \"Conversion time: %2.2fs\" % ( t1 - t0 ) ) "}
{"7862": "\nimport pprint \ndef calibrate_fluxes ( main_obs_name , dio_name , dspec , Tsys , fullstokes = False , ** kwargs ) : \n    main_obs = Waterfall ( main_obs_name , max_load = 150 ) \n    ncoarse = main_obs . calc_n_coarse_chan ( ) \n    dio_obs = Waterfall ( dio_name , max_load = 150 ) \n    dio_chan_per_coarse = dio_obs . header [ 'nchans' ] / ncoarse \n    dOFF , dON = integrate_calib ( dio_name , dio_chan_per_coarse , fullstokes , ** kwargs ) \n    main_dat = main_obs . data \n    scale_facs = dspec / ( dON - dOFF ) \n    pprint . pprint ( scale_facs ) \n    nchans = main_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = nchans / ncoarse \n    ax0_size = np . size ( main_dat , 0 ) \n    ax1_size = np . size ( main_dat , 1 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , ncoarse , obs_chan_per_coarse ) ) \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = main_dat * scale_facs \n    main_dat = main_dat - Tsys \n    main_dat = np . swapaxes ( main_dat , 2 , 3 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , nchans ) ) \n    main_obs . data = main_dat \n    main_obs . write_to_filterbank ( main_obs_name [ : - 4 ] + '.fluxcal.fil' ) \n    pprint . pprint ( 'Finished: calibrated product written to ' + main_obs_name [ : - 4 ] + '.fluxcal.fil' ) "}
{"7904": "\nimport pprint \ndef calcParallaxError ( args ) : \n    gmag = float ( args [ 'gmag' ] ) \n    vmini = float ( args [ 'vmini' ] ) \n    sigmaPar = parallaxErrorSkyAvg ( gmag , vmini ) \n    gminv = gminvFromVmini ( vmini ) \n    pprint . pprint ( \"G = {0}\" . format ( gmag ) ) \n    pprint . pprint ( \"V = {0}\" . format ( gmag - gminv ) ) \n    pprint . pprint ( \"(V-I) = {0}\" . format ( vmini ) ) \n    pprint . pprint ( \"(G-V) = {0}\" . format ( gminv ) ) \n    pprint . pprint ( \"standard error = {0} muas\" . format ( sigmaPar ) ) "}
{"7931": "\nimport pprint \ndef report ( self , output_file = sys . stdout ) : \n    max_perf = self . results [ 'max_perf' ] \n    if self . _args and self . _args . verbose >= 3 : \n        pprint . pprint ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . _args and self . _args . verbose >= 1 : \n        pprint . pprint ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) \n        pprint . pprint ( 'Bottlenecks:' , file = output_file ) \n        pprint . pprint ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        pprint . pprint ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        pprint . pprint ( '    CPU |              | {!s:>15} |                   |' . format ( max_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            pprint . pprint ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        pprint . pprint ( '' , file = output_file ) \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > max_perf [ 'FLOP/s' ] : \n        pprint . pprint ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max_perf ) , file = output_file ) \n    else : \n        pprint . pprint ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        pprint . pprint ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        pprint . pprint ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7932": "\nimport pprint \ndef report ( self , output_file = sys . stdout ) : \n    cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] \n    if self . verbose >= 3 : \n        pprint . pprint ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . verbose >= 1 : \n        pprint . pprint ( 'Bottlenecks:' , file = output_file ) \n        pprint . pprint ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        pprint . pprint ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        pprint . pprint ( '    CPU |              | {!s:>15} |                   |' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            if b is None : \n                continue \n            pprint . pprint ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        pprint . pprint ( '' , file = output_file ) \n        pprint . pprint ( 'IACA analisys:' , file = output_file ) \n        pprint . pprint ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output_file ) \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu_perf [ 'FLOP/s' ] : \n        pprint . pprint ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n    else : \n        pprint . pprint ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        pprint . pprint ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        pprint . pprint ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7933": "\nimport pprint \ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and self . _args . verbose > 2 : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        pprint . pprint ( \"{}D layer condition:\" . format ( dimension ) , file = output_file ) \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            pprint . pprint ( cache + \": \" , end = '' , file = output_file ) \n            if lc_solution [ 'lt' ] is sympy . true : \n                pprint . pprint ( \"unconditionally fulfilled\" , file = output_file ) \n            else : \n                if lc_solution [ 'eq' ] is None : \n                    pprint . pprint ( \"{}\" . format ( lc_solution [ 'lt' ] ) , file = output_file ) \n                elif type ( lc_solution [ 'eq' ] ) is not list : \n                    pprint . pprint ( \"{}\" . format ( lc_solution [ 'eq' ] ) , file = output_file ) \n                else : \n                    for solu in lc_solution [ 'eq' ] : \n                        for s , v in solu . items ( ) : \n                            pprint . pprint ( \"{} <= {}\" . format ( s , v ) , file = output_file ) "}
{"7940": "\nimport pprint \ndef analyze ( self ) : \n    try : \n        incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = self . verbose > 2 ) \n    except RuntimeError as e : \n        pprint . pprint ( \"IACA analysis failed: \" + str ( e ) ) \n        sys . exit ( 1 ) \n    block_throughput = incore_analysis [ 'throughput' ] \n    port_cycles = incore_analysis [ 'port cycles' ] \n    uops = incore_analysis [ 'uops' ] \n    elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) \n    block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] \n    try : \n        block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size \n    except ZeroDivisionError as e : \n        pprint . pprint ( \"Too small block_size / pointer_increment:\" , e , file = sys . stderr ) \n        sys . exit ( 1 ) \n    port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) \n    uops = uops * block_to_cl_ratio \n    cl_throughput = block_throughput * block_to_cl_ratio \n    T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) \n    T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) \n    if T_nOL < cl_throughput : \n        T_OL = cl_throughput \n    self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } \n    return self . results "}
{"7944": "\nimport pprint \ndef userselect_increment ( block ) : \n    pprint . pprint ( \"Selected block:\" ) \n    pprint . pprint ( '\\n    ' + ( '\\n    ' . join ( block [ 'lines' ] ) ) ) \n    pprint . pprint ( ) \n    increment = None \n    while increment is None : \n        increment = input ( \"Choose store pointer increment (number of bytes): \" ) \n        try : \n            increment = int ( increment ) \n        except ValueError : \n            increment = None \n    block [ 'pointer_increment' ] = increment \n    return increment "}
{"7945": "\nimport pprint \ndef userselect_block ( blocks , default = None , debug = False ) : \n    pprint . pprint ( \"Blocks found in assembly file:\" ) \n    pprint . pprint ( \"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) \n    for idx , b in blocks : \n        pprint . pprint ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) \n        if debug : \n            ln = b [ 'first_line' ] \n            pprint . pprint ( ' ' * 4 + 'Code:' ) \n            for l in b [ 'lines' ] : \n                pprint . pprint ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) \n                ln += 1 \n            pprint . pprint ( ' ' * 4 + 'Metadata:' ) \n            pprint . pprint ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) \n    block_idx = - 1 \n    while not ( 0 <= block_idx < len ( blocks ) ) : \n        block_idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - 1 \n    return block_idx "}
{"7975": "\nimport pprint \ndef print_kernel_info ( self , output_file = sys . stdout ) : \n    table = ( '     idx |        min        max       step\\n' + '---------+---------------------------------\\n' ) \n    for l in self . _loop_stack : \n        table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\\n' . format ( * l ) \n    pprint . pprint ( prefix_indent ( 'loop stack:        ' , table ) , file = output_file ) \n    table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) \n    for name , offsets in list ( self . sources . items ( ) ) : \n        prefix = '{:>8} | ' . format ( name ) \n        right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) \n        table += prefix_indent ( prefix , right_side , later_prefix = '         | ' ) \n    pprint . pprint ( prefix_indent ( 'data sources:      ' , table ) , file = output_file ) \n    table = ( '    name |  offsets   ...\\n' + '---------+------------...\\n' ) \n    for name , offsets in list ( self . destinations . items ( ) ) : \n        prefix = '{:>8} | ' . format ( name ) \n        right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) \n        table += prefix_indent ( prefix , right_side , later_prefix = '         | ' ) \n    pprint . pprint ( prefix_indent ( 'data destinations: ' , table ) , file = output_file ) \n    table = ( ' op | count \\n' + '----+-------\\n' ) \n    for op , count in list ( self . _flops . items ( ) ) : \n        table += '{:>3} | {:>4}\\n' . format ( op , count ) \n    table += '     =======\\n' \n    table += '      {:>4}' . format ( sum ( self . _flops . values ( ) ) ) \n    pprint . pprint ( prefix_indent ( 'FLOPs:     ' , table ) , file = output_file ) "}
{"7976": "\nimport pprint \ndef print_variables_info ( self , output_file = sys . stdout ) : \n    table = ( '    name |   type size             \\n' + '---------+-------------------------\\n' ) \n    for name , var_info in list ( self . variables . items ( ) ) : \n        table += '{:>8} | {:>6} {!s:<10}\\n' . format ( name , var_info [ 0 ] , var_info [ 1 ] ) \n    pprint . pprint ( prefix_indent ( 'variables: ' , table ) , file = output_file ) "}
{"7977": "\nimport pprint \ndef print_constants_info ( self , output_file = sys . stdout ) : \n    table = ( '    name | value     \\n' + '---------+-----------\\n' ) \n    for name , value in list ( self . constants . items ( ) ) : \n        table += '{!s:>8} | {:<10}\\n' . format ( name , value ) \n    pprint . pprint ( prefix_indent ( 'constants: ' , table ) , file = output_file ) "}
{"7978": "\nimport pprint \ndef print_kernel_code ( self , output_file = sys . stdout ) : \n    pprint . pprint ( self . kernel_code , file = output_file ) "}
{"7996": "\nimport pprint \ndef build_executable ( self , lflags = None , verbose = False , openmp = False ) : \n    compiler , compiler_args = self . _machine . get_compiler ( ) \n    kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) \n    out_filename , already_exists = self . _get_intermediate_file ( os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) \n    if not already_exists : \n        main_source_filename = self . get_main_code ( as_filename = True ) \n        if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 'LIKWID_LIB' in os . environ ) : \n            pprint . pprint ( 'Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and ' 'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables' , file = sys . stderr ) \n            sys . exit ( 1 ) \n        compiler_args += [ '-std=c99' , '-I' + reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' ) , os . environ . get ( 'LIKWID_INCLUDE' , '' ) , os . environ . get ( 'LIKWID_INC' , '' ) , '-llikwid' ] \n        if os . environ . get ( 'LIKWID_LIB' ) == '' : \n            compiler_args = compiler_args [ : - 1 ] \n        if lflags is None : \n            lflags = [ ] \n        lflags += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        compiler_args += os . environ [ 'LIKWID_LIB' ] . split ( ' ' ) + [ '-pthread' ] \n        infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , kernel_obj_filename , main_source_filename ] \n        cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] \n        cmd = list ( filter ( bool , cmd ) ) \n        if verbose : \n            pprint . pprint ( 'Executing (build_executable): ' , ' ' . join ( cmd ) ) \n        try : \n            subprocess . check_output ( cmd ) \n        except subprocess . CalledProcessError as e : \n            pprint . pprint ( \"Build failed:\" , e , file = sys . stderr ) \n            sys . exit ( 1 ) \n    else : \n        if verbose : \n            pprint . pprint ( 'Executing (build_executable): ' , 'using cached' , out_filename ) \n    return out_filename "}
{"8015": "\nimport pprint \ndef report ( self , output_file = sys . stdout ) : \n    if self . verbose > 1 : \n        with pprint_nosort ( ) : \n            pprint . pprint ( self . results ) \n    if self . verbose > 0 : \n        pprint . pprint ( 'Runtime (per repetition): {:.2g} s' . format ( self . results [ 'Runtime (per repetition) [s]' ] ) , file = output_file ) \n    if self . verbose > 0 : \n        pprint . pprint ( 'Iterations per repetition: {!s}' . format ( self . results [ 'Iterations per repetition' ] ) , file = output_file ) \n    pprint . pprint ( 'Runtime (per cacheline update): {:.2f} cy/CL' . format ( self . results [ 'Runtime (per cacheline update) [cy/CL]' ] ) , file = output_file ) \n    pprint . pprint ( 'MEM volume (per repetition): {:.0f} Byte' . format ( self . results [ 'MEM volume (per repetition) [B]' ] ) , file = output_file ) \n    pprint . pprint ( 'Performance: {:.2f} MFLOP/s' . format ( self . results [ 'Performance [MFLOP/s]' ] ) , file = output_file ) \n    pprint . pprint ( 'Performance: {:.2f} MLUP/s' . format ( self . results [ 'Performance [MLUP/s]' ] ) , file = output_file ) \n    pprint . pprint ( 'Performance: {:.2f} It/s' . format ( self . results [ 'Performance [MIt/s]' ] ) , file = output_file ) \n    if self . verbose > 0 : \n        pprint . pprint ( 'MEM bandwidth: {:.2f} MByte/s' . format ( self . results [ 'MEM BW [MByte/s]' ] ) , file = output_file ) \n    pprint . pprint ( '' , file = output_file ) \n    if not self . no_phenoecm : \n        pprint . pprint ( \"Data Transfers:\" ) \n        pprint . pprint ( \"{:^8} |\" . format ( \"cache\" ) , end = '' ) \n        for metrics in self . results [ 'data transfers' ] . values ( ) : \n            for metric_name in sorted ( metrics ) : \n                pprint . pprint ( \" {:^14}\" . format ( metric_name ) , end = '' ) \n            pprint . pprint ( ) \n            break \n        for cache , metrics in sorted ( self . results [ 'data transfers' ] . items ( ) ) : \n            pprint . pprint ( \"{!s:^8} |\" . format ( cache ) , end = '' ) \n            for k , v in sorted ( metrics . items ( ) ) : \n                pprint . pprint ( \" {!s:^14}\" . format ( v ) , end = '' ) \n            pprint . pprint ( ) \n        pprint . pprint ( ) \n        pprint . pprint ( 'Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | ' '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL' . format ( ** { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , file = output_file ) \n        pprint . pprint ( 'T_OL assumes that two loads per cycle may be retiered, which is true for ' '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, ' 'BDW, SKL and SKX, but it also depends on AGU availability.' , file = output_file ) "}
{"8097": "\nimport pprint \ndef calc ( pvalues , lamb ) : \n    m = len ( pvalues ) \n    pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) \n    pFDR = np . ones ( m ) \n    pprint . pprint ( \"pFDR    y        Pr     fastPow\" ) \n    for i in range ( m ) : \n        y = pvalues [ i ] \n        Pr = max ( 1 , m - i ) / float ( m ) \n        pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) \n        pprint . pprint ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) \n    num_null = pi0 * m \n    num_alt = m - num_null \n    num_negs = np . array ( range ( m ) ) \n    num_pos = m - num_negs \n    pp = num_pos / float ( m ) \n    qvalues = np . ones ( m ) \n    qvalues [ 0 ] = pFDR [ 0 ] \n    for i in range ( m - 1 ) : \n        qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) \n    sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt \n    sens [ sens > 1.0 ] = 1.0 \n    df = pd . DataFrame ( dict ( pvalue = pvalues , qvalue = qvalues , FDR = pFDR , percentile_positive = pp , sens = sens ) ) \n    df [ \"svalue\" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] \n    return df , num_null , m "}
{"8248": "\nimport pprint \ndef start ( self ) : \n    if sys . stdout is not self : \n        self . _original_steam = sys . stdout \n        sys . stdout = self \n        self . _redirection = True \n    if self . _redirection : \n        pprint . pprint ( 'Established redirection of `stdout`.' ) "}
{"8259": "\nimport pprint \ndef run_net ( traj ) : \n    eqs = traj . eqs \n    namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) \n    neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) \n    neuron . vm = traj . EL \n    neuron . w = traj . a * ( neuron . vm - traj . EL ) \n    neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) \n    pprint . pprint ( 'Initial Run' ) \n    net = Network ( neuron ) \n    net . run ( 100 * ms , report = 'text' ) \n    MSpike = SpikeMonitor ( neuron ) \n    net . add ( MSpike ) \n    MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) \n    net . add ( MStateV ) \n    pprint . pprint ( 'Measurement run' ) \n    net . run ( 500 * ms , report = 'text' ) \n    traj . v_standard_result = Brian2MonitorResult \n    traj . f_add_result ( 'SpikeMonitor' , MSpike ) \n    traj . f_add_result ( 'StateMonitorV' , MStateV ) "}
{"8267": "\nimport pprint \ndef compact_hdf5_file ( filename , name = None , index = None , keep_backup = True ) : \n    if name is None and index is None : \n        index = - 1 \n    tmp_traj = load_trajectory ( name , index , as_new = False , load_all = pypetconstants . LOAD_NOTHING , force = True , filename = filename ) \n    service = tmp_traj . v_storage_service \n    complevel = service . complevel \n    complib = service . complib \n    shuffle = service . shuffle \n    fletcher32 = service . fletcher32 \n    name_wo_ext , ext = os . path . splitext ( filename ) \n    tmp_filename = name_wo_ext + '_tmp' + ext \n    abs_filename = os . path . abspath ( filename ) \n    abs_tmp_filename = os . path . abspath ( tmp_filename ) \n    command = [ 'ptrepack' , '-v' , '--complib' , complib , '--complevel' , str ( complevel ) , '--shuffle' , str ( int ( shuffle ) ) , '--fletcher32' , str ( int ( fletcher32 ) ) , abs_filename , abs_tmp_filename ] \n    str_command = ' ' . join ( command ) \n    pprint . pprint ( 'Executing command `%s`' % str_command ) \n    retcode = subprocess . call ( command ) \n    if retcode != 0 : \n        pprint . pprint ( '#### ERROR: Compacting `%s` failed with errorcode %s! ####' % ( filename , str ( retcode ) ) ) \n    else : \n        pprint . pprint ( '#### Compacting successful ####' ) \n        pprint . pprint ( 'Renaming files' ) \n        if keep_backup : \n            backup_file_name = name_wo_ext + '_backup' + ext \n            os . rename ( filename , backup_file_name ) \n        else : \n            os . remove ( filename ) \n        os . rename ( tmp_filename , filename ) \n        pprint . pprint ( '### Compacting and Renaming finished ####' ) \n    return retcode "}
{"8278": "\nimport pprint \ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if len ( subrun_list ) == 0 : \n        spikes_e = traj . results . monitors . spikes_e \n        time_window = traj . parameters . analysis . statistics . time_window \n        start_time = traj . parameters . simulation . durations . initial_run \n        end_time = start_time + traj . parameters . simulation . durations . measurement_run \n        neuron_ids = traj . parameters . analysis . statistics . neuron_ids \n        mean_ff = self . _compute_mean_fano_factor ( neuron_ids , spikes_e , time_window , start_time , end_time ) \n        traj . f_add_result ( 'statistics.mean_fano_factor' , mean_ff , comment = 'Average Fano ' 'Factor over all ' 'exc neurons' ) \n        pprint . pprint ( 'R_ee: %f, Mean FF: %f' % ( traj . R_ee , mean_ff ) ) "}
{"8283": "\nimport pprint \ndef _print_graphs ( self , traj ) : \n    print_folder = self . _make_folder ( traj ) \n    plt . figure ( ) \n    plt . scatter ( self . spike_monitor . t , self . spike_monitor . i , s = 1 ) \n    plt . xlabel ( 't' ) \n    plt . ylabel ( 'Exc. Neurons' ) \n    plt . title ( 'Spike Raster Plot' ) \n    filename = os . path . join ( print_folder , 'spike.png' ) \n    pprint . pprint ( 'Current plot: %s ' % filename ) \n    plt . savefig ( filename ) \n    plt . close ( ) \n    fig = plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.V' ) \n    filename = os . path . join ( print_folder , 'V.png' ) \n    pprint . pprint ( 'Current plot: %s ' % filename ) \n    fig . savefig ( filename ) \n    plt . close ( ) \n    plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.I_syn_e' ) \n    filename = os . path . join ( print_folder , 'I_syn_e.png' ) \n    pprint . pprint ( 'Current plot: %s ' % filename ) \n    plt . savefig ( filename ) \n    plt . close ( ) \n    plt . figure ( ) \n    self . _plot_result ( traj , 'monitors.I_syn_i' ) \n    filename = os . path . join ( print_folder , 'I_syn_i.png' ) \n    pprint . pprint ( 'Current plot: %s ' % filename ) \n    plt . savefig ( filename ) \n    plt . close ( ) \n    if not traj . analysis . show_plots : \n        plt . close ( 'all' ) \n    else : \n        plt . show ( ) "}
{"8284": "\nimport pprint \ndef analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : \n    if len ( subrun_list ) == 0 : \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.spikes_e' , self . spike_monitor , comment = 'The spiketimes of the excitatory population' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.V' , self . V_monitor , comment = 'Membrane voltage of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_e' , self . I_syn_e_monitor , comment = 'I_syn_e of four neurons from 2 clusters' ) \n        traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_i' , self . I_syn_i_monitor , comment = 'I_syn_i of four neurons from 2 clusters' ) \n        pprint . pprint ( 'Plotting' ) \n        if traj . parameters . analysis . make_plots : \n            self . _print_graphs ( traj ) "}
{"8285": "\nimport pprint \ndef get_batch ( ) : \n    optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) \n    batch = 0 \n    for o , a in optlist : \n        if o == '--batch' : \n            batch = int ( a ) \n            pprint . pprint ( 'Found batch %d' % batch ) \n    return batch "}
{"8351": "\nimport pprint \ndef port_to_tcp ( port = None ) : \n    domain_name = socket . getfqdn ( ) \n    try : \n        addr_list = socket . getaddrinfo ( domain_name , None ) \n    except Exception : \n        addr_list = socket . getaddrinfo ( '127.0.0.1' , None ) \n    family , socktype , proto , canonname , sockaddr = addr_list [ 0 ] \n    host = convert_ipv6 ( sockaddr [ 0 ] ) \n    address = 'tcp://' + host \n    if port is None : \n        port = ( ) \n    if not isinstance ( port , int ) : \n        context = zmq . Context ( ) \n        try : \n            socket_ = context . socket ( zmq . REP ) \n            socket_ . ipv6 = is_ipv6 ( address ) \n            port = socket_ . bind_to_random_port ( address , * port ) \n        except Exception : \n            pprint . pprint ( 'Could not connect to {} using {}' . format ( address , addr_list ) ) \n            pypet_root_logger = logging . getLogger ( 'pypet' ) \n            pypet_root_logger . exception ( 'Could not connect to {}' . format ( address ) ) \n            raise \n        socket_ . close ( ) \n        context . term ( ) \n    return address + ':' + str ( port ) "}
{"8391": "\nimport pprint \ndef main ( ) : \n    rules_to_test = [ 10 , 30 , 90 , 110 , 184 ] \n    steps = 250 \n    ncells = 400 \n    seed = 100042 \n    initial_states = [ 'single' , 'random' ] \n    folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_original' ) \n    if not os . path . isdir ( folder ) : \n        os . makedirs ( folder ) \n    filename = os . path . join ( folder , 'all_patterns.p' ) \n    pprint . pprint ( 'Computing all patterns' ) \n    all_patterns = [ ] \n    for idx , rule_number in enumerate ( rules_to_test ) : \n        for initial_name in initial_states : \n            initial_state = make_initial_state ( initial_name , ncells , seed = seed ) \n            pattern = cellular_automaton_1D ( initial_state , rule_number , steps ) \n            all_patterns . append ( ( rule_number , initial_name , pattern ) ) \n        progressbar ( idx , len ( rules_to_test ) , reprint = True ) \n    with open ( filename , 'wb' ) as file : \n        pickle . dump ( all_patterns , file = file ) \n    pprint . pprint ( 'Plotting all patterns' ) \n    for idx , pattern_tuple in enumerate ( all_patterns ) : \n        rule_number , initial_name , pattern = pattern_tuple \n        filename = os . path . join ( folder , 'rule_%s_%s.png' % ( str ( rule_number ) , initial_name ) ) \n        plot_pattern ( pattern , rule_number , filename ) \n        progressbar ( idx , len ( all_patterns ) , reprint = True ) "}
{"8553": "\nimport pprint \ndef main ( ) : \n    folder = os . getcwd ( ) \n    pprint . pprint ( 'Merging all files' ) \n    merge_all_in_folder ( folder , delete_other_files = True , dynamic_imports = FunctionParameter , backup = False ) \n    pprint . pprint ( 'Done' ) "}
{"8554": "\nimport pprint \ndef upload_file ( filename , session ) : \n    pprint . pprint ( 'Uploading file %s' % filename ) \n    outfilesource = os . path . join ( os . getcwd ( ) , filename ) \n    outfiletarget = 'sftp://' + ADDRESS + WORKING_DIR \n    out = saga . filesystem . File ( outfilesource , session = session , flags = OVERWRITE ) \n    out . copy ( outfiletarget ) \n    pprint . pprint ( 'Transfer of `%s` to `%s` successful' % ( filename , outfiletarget ) ) "}
{"8555": "\nimport pprint \ndef download_file ( filename , session ) : \n    pprint . pprint ( 'Downloading file %s' % filename ) \n    infilesource = os . path . join ( 'sftp://' + ADDRESS + WORKING_DIR , filename ) \n    infiletarget = os . path . join ( os . getcwd ( ) , filename ) \n    incoming = saga . filesystem . File ( infilesource , session = session , flags = OVERWRITE ) \n    incoming . copy ( infiletarget ) \n    pprint . pprint ( 'Transfer of `%s` to `%s` successful' % ( filename , infiletarget ) ) "}
{"8557": "\nimport pprint \ndef merge_trajectories ( session ) : \n    jd = saga . job . Description ( ) \n    jd . executable = 'python' \n    jd . arguments = [ 'merge_trajs.py' ] \n    jd . output = \"mysagajob_merge.stdout\" \n    jd . error = \"mysagajob_merge.stderr\" \n    jd . working_directory = WORKING_DIR \n    js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) \n    myjob = js . create_job ( jd ) \n    pprint . pprint ( \"\\n...starting job...\\n\" ) \n    myjob . run ( ) \n    pprint . pprint ( \"Job ID    : %s\" % ( myjob . id ) ) \n    pprint . pprint ( \"Job State : %s\" % ( myjob . state ) ) \n    pprint . pprint ( \"\\n...waiting for job...\\n\" ) \n    myjob . wait ( ) \n    pprint . pprint ( \"Job State : %s\" % ( myjob . state ) ) \n    pprint . pprint ( \"Exitcode  : %s\" % ( myjob . exit_code ) ) "}
{"8558": "\nimport pprint \ndef start_jobs ( session ) : \n    js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) \n    batches = range ( 3 ) \n    jobs = [ ] \n    for batch in batches : \n        pprint . pprint ( 'Starting batch %d' % batch ) \n        jd = saga . job . Description ( ) \n        jd . executable = 'python' \n        jd . arguments = [ 'the_task.py --batch=' + str ( batch ) ] \n        jd . output = \"mysagajob.stdout\" + str ( batch ) \n        jd . error = \"mysagajob.stderr\" + str ( batch ) \n        jd . working_directory = WORKING_DIR \n        myjob = js . create_job ( jd ) \n        pprint . pprint ( \"Job ID    : %s\" % ( myjob . id ) ) \n        pprint . pprint ( \"Job State : %s\" % ( myjob . state ) ) \n        pprint . pprint ( \"\\n...starting job...\\n\" ) \n        myjob . run ( ) \n        jobs . append ( myjob ) \n    for myjob in jobs : \n        pprint . pprint ( \"Job ID    : %s\" % ( myjob . id ) ) \n        pprint . pprint ( \"Job State : %s\" % ( myjob . state ) ) \n        pprint . pprint ( \"\\n...waiting for job...\\n\" ) \n        myjob . wait ( ) \n        pprint . pprint ( \"Job State : %s\" % ( myjob . state ) ) \n        pprint . pprint ( \"Exitcode  : %s\" % ( myjob . exit_code ) ) "}
{"8560": "\nimport pprint \ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    pprint . pprint ( 'Starting Euler Integration' ) \n    for step in range ( 1 , steps ) : \n        if V_array [ step - 1 ] >= 1 : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        elif spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : \n            V_array [ step ] = 0 \n        else : \n            dV = - 1 / tau_V * V_array [ step - 1 ] + I \n            V_array [ step ] = V_array [ step - 1 ] + dV * dt \n    pprint . pprint ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 "}
{"8562": "\nimport pprint \ndef add_parameters ( traj ) : \n    pprint . pprint ( 'Adding Parameters' ) \n    traj . f_add_parameter ( 'neuron.V_init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) \n    traj . f_add_parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) \n    traj . f_add_parameter ( 'neuron.tau_V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) \n    traj . f_add_parameter ( 'neuron.tau_ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) \n    traj . f_add_parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) \n    traj . f_add_parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' ) "}
{"8563": "\nimport pprint \ndef add_exploration ( traj ) : \n    pprint . pprint ( 'Adding exploration of I and tau_ref' ) \n    explore_dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau_ref' : [ 5.0 , 7.5 , 10.0 ] } \n    explore_dict = cartesian_product ( explore_dict , ( 'neuron.tau_ref' , 'neuron.I' ) ) \n    traj . f_explore ( explore_dict ) "}
{"8724": "\nimport pprint \ndef run_command ( self , args : List [ str ] , max_num_processes : int = None , max_stack_size : int = None , max_virtual_memory : int = None , as_root : bool = False , stdin : FileIO = None , timeout : int = None , check : bool = False , truncate_stdout : int = None , truncate_stderr : int = None ) -> 'CompletedCommand' : \n    cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] \n    if stdin is None : \n        cmd . append ( '--stdin_devnull' ) \n    if max_num_processes is not None : \n        cmd += [ '--max_num_processes' , str ( max_num_processes ) ] \n    if max_stack_size is not None : \n        cmd += [ '--max_stack_size' , str ( max_stack_size ) ] \n    if max_virtual_memory is not None : \n        cmd += [ '--max_virtual_memory' , str ( max_virtual_memory ) ] \n    if timeout is not None : \n        cmd += [ '--timeout' , str ( timeout ) ] \n    if truncate_stdout is not None : \n        cmd += [ '--truncate_stdout' , str ( truncate_stdout ) ] \n    if truncate_stderr is not None : \n        cmd += [ '--truncate_stderr' , str ( truncate_stderr ) ] \n    if not as_root : \n        cmd += [ '--linux_user_id' , str ( self . _linux_uid ) ] \n    cmd += args \n    if self . debug : \n        pprint . pprint ( 'running: {}' . format ( cmd ) , flush = True ) \n    with tempfile . TemporaryFile ( ) as f : \n        try : \n            subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = True ) \n            f . seek ( 0 ) \n            json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            results_json = json . loads ( f . read ( json_len ) . decode ( ) ) \n            stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stdout = tempfile . NamedTemporaryFile ( ) \n            stdout . write ( f . read ( stdout_len ) ) \n            stdout . seek ( 0 ) \n            stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) \n            stderr = tempfile . NamedTemporaryFile ( ) \n            stderr . write ( f . read ( stderr_len ) ) \n            stderr . seek ( 0 ) \n            result = CompletedCommand ( return_code = results_json [ 'return_code' ] , timed_out = results_json [ 'timed_out' ] , stdout = stdout , stderr = stderr , stdout_truncated = results_json [ 'stdout_truncated' ] , stderr_truncated = results_json [ 'stderr_truncated' ] ) \n            if ( result . return_code != 0 or results_json [ 'timed_out' ] ) and check : \n                raise subprocess . CalledProcessError ( result . return_code , cmd , output = result . stdout , stderr = result . stderr ) \n            return result \n        except subprocess . CalledProcessError as e : \n            f . seek ( 0 ) \n            pprint . pprint ( f . read ( ) ) \n            pprint . pprint ( e . stderr ) \n            raise "}
{"8861": "\nimport pprint \ndef run ( file_or_code , code , in_ns , use_var_indirection , warn_on_shadowed_name , warn_on_shadowed_var , warn_on_var_indirection , ) : \n    basilisp . init ( ) \n    ctx = compiler . CompilerContext ( filename = CLI_INPUT_FILE_PATH if code else ( STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code ) , opts = { compiler . WARN_ON_SHADOWED_NAME : warn_on_shadowed_name , compiler . WARN_ON_SHADOWED_VAR : warn_on_shadowed_var , compiler . USE_VAR_INDIRECTION : use_var_indirection , compiler . WARN_ON_VAR_INDIRECTION : warn_on_var_indirection , } , ) \n    eof = object ( ) \n    with runtime . ns_bindings ( in_ns ) as ns : \n        if code : \n            pprint . pprint ( runtime . lrepr ( eval_str ( file_or_code , ctx , ns . module , eof ) ) ) \n        elif file_or_code == STDIN_FILE_NAME : \n            pprint . pprint ( runtime . lrepr ( eval_stream ( click . get_text_stream ( \"stdin\" ) , ctx , ns . module ) ) ) \n        else : \n            pprint . pprint ( runtime . lrepr ( eval_file ( file_or_code , ctx , ns . module ) ) ) "}
{"9051": "\nimport pprint \ndef validate_version ( ) : \n    import leicacam \n    version_string = leicacam . __version__ \n    versions = version_string . split ( '.' , 3 ) \n    try : \n        for ver in versions : \n            int ( ver ) \n    except ValueError : \n        pprint . pprint ( 'Only integers are allowed in release version, ' 'please adjust current version {}' . format ( version_string ) ) \n        return None \n    return version_string "}
{"9052": "\nimport pprint \ndef generate ( ) : \n    old_dir = os . getcwd ( ) \n    proj_dir = os . path . join ( os . path . dirname ( __file__ ) , os . pardir ) \n    os . chdir ( proj_dir ) \n    version = validate_version ( ) \n    if not version : \n        os . chdir ( old_dir ) \n        return \n    pprint . pprint ( 'Generating changelog for version {}' . format ( version ) ) \n    options = [ '--user' , 'arve0' , '--project' , 'leicacam' , '-v' , '--with-unreleased' , '--future-release' , version ] \n    generator = ChangelogGenerator ( options ) \n    generator . run ( ) \n    os . chdir ( old_dir ) "}
{"9082": "\nimport pprint \ndef run_trace ( mname , fname , module_prefix , callable_names , no_print , module_exclude = None , callable_exclude = None , debug = False , ) : \n    module_exclude = [ ] if module_exclude is None else module_exclude \n    callable_exclude = [ ] if callable_exclude is None else callable_exclude \n    par = trace_pars ( mname ) \n    start_time = datetime . datetime . now ( ) \n    with pexdoc . exdoc . ExDocCxt ( exclude = par . exclude + module_exclude , pickle_fname = par . pickle_fname , in_callables_fname = par . in_callables_fname , out_callables_fname = par . out_callables_fname , _no_print = no_print , ) as exdoc_obj : \n        fname = os . path . realpath ( os . path . join ( os . path . dirname ( __file__ ) , \"..\" , \"..\" , \"tests\" , \"test_{0}.py\" . format ( fname ) , ) ) \n        test_cmd = ( [ \"--color=yes\" ] + ( [ \"-s\" , \"-vv\" ] if debug else [ \"-q\" , \"-q\" , \"-q\" ] ) + [ \"--disable-warnings\" ] + [ \"-x\" ] + ( [ par . noption ] if par . noption else [ ] ) + [ \"-m \" + mname ] + [ fname ] ) \n        with warnings . catch_warnings ( ) : \n            warnings . filterwarnings ( \"ignore\" , category = PytestWarning ) \n            if pytest . main ( test_cmd ) : \n                raise RuntimeError ( \"Tracing did not complete successfully\" ) \n    stop_time = datetime . datetime . now ( ) \n    if not no_print : \n        pprint . pprint ( \"Auto-generation of exceptions documentation time: {0}\" . format ( pmisc . elapsed_time_string ( start_time , stop_time ) ) ) \n        for callable_name in callable_names : \n            callable_name = module_prefix + callable_name \n            pprint . pprint ( \"\\nCallable: {0}\" . format ( callable_name ) ) \n            pprint . pprint ( exdoc_obj . get_sphinx_doc ( callable_name , exclude = callable_exclude ) ) \n            pprint . pprint ( \"\\n\" ) \n    return copy . copy ( exdoc_obj ) "}
{"9095": "\nimport pprint \ndef quietinterrupt ( msg = None ) : \n    def handler ( ) : \n        if msg : \n            pprint . pprint ( msg , file = sys . stderr ) \n        sys . exit ( 1 ) \n    signal . signal ( signal . SIGINT , handler ) "}
{"9096": "\nimport pprint \ndef printtsv ( table , sep = \"\\t\" , file = sys . stdout ) : \n    for record in table : \n        pprint . pprint ( * record , sep = sep , file = file ) "}
{"9189": "\nimport pprint \ndef wrap_and_format ( self , width = None , include_params = False , include_return = False , excluded_params = None ) : \n    if excluded_params is None : \n        excluded_params = [ ] \n    out = StringIO ( ) \n    if width is None : \n        width , _height = get_terminal_size ( ) \n    for line in self . maindoc : \n        if isinstance ( line , Line ) : \n            out . write ( fill ( line . contents , width = width ) ) \n            out . write ( '\\n' ) \n        elif isinstance ( line , BlankLine ) : \n            out . write ( '\\n' ) \n        elif isinstance ( line , ListItem ) : \n            out . write ( fill ( line . contents , initial_indent = \" %s \" % line . marker [ 0 ] , subsequent_indent = \"   \" , width = width ) ) \n            out . write ( '\\n' ) \n    if include_params : \n        included_params = set ( self . param_info ) - set ( excluded_params ) \n        if len ( included_params ) > 0 : \n            out . write ( \"\\nParameters:\\n\" ) \n            for param in included_params : \n                info = self . param_info [ param ] \n                out . write ( \" - %s (%s):\\n\" % ( param , info . type_name ) ) \n                out . write ( fill ( info . desc , initial_indent = \"   \" , subsequent_indent = \"   \" , width = width ) ) \n                out . write ( '\\n' ) \n    if include_return : \n        pprint . pprint ( \"Returns:\" ) \n        pprint . pprint ( \"    \" + self . return_info . type_name ) \n    return out . getvalue ( ) "}
{"9227": "\nimport pprint \ndef load ( ) : \n    autodiscover_modules ( 'cron' ) \n    if PROJECT_MODULE : \n        if '.' in PROJECT_MODULE . __name__ : \n            try : \n                import_module ( '%s.cron' % '.' . join ( PROJECT_MODULE . __name__ . split ( '.' ) [ 0 : - 1 ] ) ) \n            except ImportError as e : \n                if 'No module named' not in str ( e ) : \n                    pprint . pprint ( e ) \n    for cmd , app in get_commands ( ) . items ( ) : \n        try : \n            load_command_class ( app , cmd ) \n        except django . core . exceptions . ImproperlyConfigured : \n            pass "}
{"9229": "\nimport pprint \ndef printtasks ( ) : \n    load ( ) \n    tab = crontab . CronTab ( '' ) \n    for task in registry : \n        tab . new ( task . command , KRONOS_BREADCRUMB ) . setall ( task . schedule ) \n    pprint . pprint ( tab . render ( ) ) "}
{"9253": "\nimport pprint \ndef main ( ) : \n    state = GameState ( ) \n    pprint . pprint ( state ) \n    while state . running : \n        input = get_single_char ( ) \n        state , should_advance = state . handle_input ( input ) \n        if should_advance : \n            state = state . advance_robots ( ) \n            state = state . check_game_end ( ) \n        pprint . pprint ( state ) \n    pprint . pprint ( state . message ) "}
{"9258": "\nimport pprint \ndef player_move ( board ) : \n    pprint . pprint ( board , end = '\\n\\n' ) \n    x , y = input ( 'Enter move (e.g. 2b): ' ) \n    pprint . pprint ( ) \n    return int ( x ) - 1 , ord ( y ) - ord ( 'a' ) "}
{"9259": "\nimport pprint \ndef play ( ) : \n    ai = { 'X' : player_move , 'O' : random_move } \n    board = Board ( ) \n    while not board . winner : \n        x , y = ai [ board . player ] ( board ) \n        board = board . make_move ( x , y ) \n    pprint . pprint ( board , end = '\\n\\n' ) \n    pprint . pprint ( board . winner ) "}
{"9470": "\nimport pprint \ndef v2_runner_on_ok ( self , result , ** kwargs ) : \n    failed = \"failed\" in result . _result \n    unreachable = \"unreachable\" in result . _result \n    if ( \"print_action\" in result . _task . tags or failed or unreachable or self . _display . verbosity > 1 ) : \n        self . _print_task ( ) \n        self . last_skipped = False \n        msg = unicode ( result . _result . get ( \"msg\" , \"\" ) ) or unicode ( result . _result . get ( \"reason\" , \"\" ) ) or unicode ( result . _result . get ( \"message\" , \"\" ) ) \n        stderr = [ result . _result . get ( \"exception\" , None ) , result . _result . get ( \"module_stderr\" , None ) , ] \n        stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n        self . _print_host_or_item ( result . _host , result . _result . get ( \"changed\" , False ) , msg , result . _result . get ( \"diff\" , None ) , is_host = True , error = failed or unreachable , stdout = result . _result . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n        if \"results\" in result . _result : \n            for r in result . _result [ \"results\" ] : \n                failed = \"failed\" in r \n                stderr = [ r . get ( \"exception\" , None ) , r . get ( \"module_stderr\" , None ) ] \n                stderr = \"\\n\" . join ( [ e for e in stderr if e ] ) . strip ( ) \n                self . _print_host_or_item ( r [ \"item\" ] , r . get ( \"changed\" , False ) , unicode ( r . get ( \"msg\" , \"\" ) ) , r . get ( \"diff\" , None ) , is_host = False , error = failed , stdout = r . get ( \"module_stdout\" , None ) , stderr = stderr . strip ( ) , ) \n    else : \n        self . last_skipped = True \n        pprint . pprint ( \".\" , end = \"\" ) "}
{"9471": "\nimport pprint \ndef v2_playbook_on_stats ( self , stats ) : \n    pprint . pprint ( ) \n    self . printed_last_task = False \n    self . _print_task ( \"STATS\" ) \n    hosts = sorted ( stats . processed . keys ( ) ) \n    for host in hosts : \n        s = stats . summarize ( host ) \n        if s [ \"failures\" ] or s [ \"unreachable\" ] : \n            color = \"failed\" \n        elif s [ \"changed\" ] : \n            color = \"changed\" \n        else : \n            color = \"ok\" \n        msg = \"{}    : ok={}\\tchanged={}\\tfailed={}\\tunreachable={}\" . format ( host , s [ \"ok\" ] , s [ \"changed\" ] , s [ \"failures\" ] , s [ \"unreachable\" ] ) \n        pprint . pprint ( colorize ( msg , color ) ) "}
{"9472": "\nimport pprint \ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if self . _display . verbosity > 1 : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120 \n        spaces = \" \" * ( 31 - len ( result . _host . name ) - 4 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if len ( reason ) < 50 : \n            line += \" -- {}\" . format ( reason ) \n            pprint . pprint ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            pprint . pprint ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            pprint . pprint ( self . _indent_text ( reason , 8 ) ) \n            pprint . pprint ( reason ) "}
{"9522": "\nimport pprint \ndef _list ( api_list_class , arg_namespace , ** extra ) : \n    if arg_namespace . starting_point : \n        ordering_field = ( arg_namespace . ordering or '' ) . lstrip ( '-' ) \n        if ordering_field in ( '' , 'datetime_uploaded' , 'datetime_created' ) : \n            arg_namespace . starting_point = parser . parse ( arg_namespace . starting_point ) \n    items = api_list_class ( starting_point = arg_namespace . starting_point , ordering = arg_namespace . ordering , limit = arg_namespace . limit , request_limit = arg_namespace . request_limit , ** extra ) \n    items . constructor = lambda x : x \n    try : \n        pprint ( list ( items ) ) \n    except ValueError as e : \n        pprint . pprint ( e ) "}
{"9523": "\nimport pprint \ndef bar ( iter_content , parts , title = '' ) : \n    parts = max ( float ( parts ) , 1.0 ) \n    cells = 10 \n    progress = 0 \n    step = cells / parts \n    draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) \n    for chunk in iter_content : \n        yield chunk \n        progress += step \n        draw ( progress ) \n        sys . stdout . flush ( ) \n    draw ( cells ) \n    pprint . pprint ( '' ) "}
{"9529": "\nimport pprint \ndef camera_disable ( self , camera_id , ** kwargs ) : \n    api = self . _api_info [ 'camera' ] \n    payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'Disable' , 'version' : 9 , 'idList' : camera_id , } , ** kwargs ) \n    pprint . pprint ( api [ 'url' ] ) \n    pprint . pprint ( payload ) \n    response = self . _get ( api [ 'url' ] , payload ) \n    return response [ 'success' ] "}
{"9553": "\nimport pprint \ndef find ( dataset , url ) : \n    fn = os . path . join ( DATASETS , dataset ) \n    dn = os . path . dirname ( fn ) \n    if not os . path . exists ( dn ) : \n        pprint . pprint ( 'creating dataset directory: %s' , dn ) \n        os . makedirs ( dn ) \n    if not os . path . exists ( fn ) : \n        if sys . version_info < ( 3 , ) : \n            urllib . urlretrieve ( url , fn ) \n        else : \n            urllib . request . urlretrieve ( url , fn ) \n    return fn "}
{"9555": "\nimport pprint \ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        pprint . pprint ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if sys . version_info < ( 3 , ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3 , 32 , 32 ) ) . transpose ( ( 0 , 2 , 3 , 1 ) ) . astype ( 'f' ) / 128 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32 * 32 * 3 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( 1 , 6 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000 ] ) \n    tlab = np . asarray ( labs [ : 40000 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000 : ] ) \n    vlab = np . asarray ( labs [ 40000 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9791": "\nimport pprint \ndef _get_new_access_information ( self ) : \n    if not self . r . has_oauth_app_info : \n        self . _log ( 'Cannot obtain authorize url from PRAW. Please check your configuration.' , logging . ERROR ) \n        raise AttributeError ( 'Reddit Session invalid, please check your designated config file.' ) \n    url = self . r . get_authorize_url ( 'UsingOAuth2Util' , self . _get_value ( CONFIGKEY_SCOPE , set , split_val = ',' ) , self . _get_value ( CONFIGKEY_REFRESHABLE , as_boolean = True ) ) \n    self . _start_webserver ( url ) \n    if not self . _get_value ( CONFIGKEY_SERVER_MODE , as_boolean = True ) : \n        webbrowser . open ( url ) \n    else : \n        pprint . pprint ( \"Webserver is waiting for you :D. Please open {0}:{1}/{2} \" \"in your browser\" . format ( SERVER_URL , SERVER_PORT , SERVER_LINK_PATH ) ) \n    self . _wait_for_response ( ) \n    try : \n        access_information = self . r . get_access_information ( self . server . response_code ) \n    except praw . errors . OAuthException : \n        self . _log ( \"Can not authenticate, maybe the app infos (e.g. secret) are wrong.\" , logging . ERROR ) \n        raise \n    self . _change_value ( CONFIGKEY_TOKEN , access_information [ \"access_token\" ] ) \n    self . _change_value ( CONFIGKEY_REFRESH_TOKEN , access_information [ \"refresh_token\" ] ) \n    self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) "}
{"9838": "\nimport pprint \ndef get_authorisation_url ( self , application_name , token_expire = '1day' ) : \n    query_params = { 'name' : application_name , 'expiration' : token_expire , 'response_type' : 'token' , 'scope' : 'read,write' } \n    authorisation_url = self . build_uri ( path = '/authorize' , query_params = self . add_authorisation ( query_params ) ) \n    pprint . pprint ( 'Please go to the following URL and get the user authorisation ' 'token:\\n' , authorisation_url ) \n    return authorisation_url "}
{"9896": "\nimport pprint \ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    cli = CommandLineTool ( ) \n    try : \n        return cli . run ( argv ) \n    except KeyboardInterrupt : \n        pprint . pprint ( 'Canceled' ) \n        return 3 "}
{"9953": "\nimport pprint \ndef print_all ( ) : \n    _ , conf = read_latoolscfg ( ) \n    default = conf [ 'DEFAULT' ] [ 'config' ] \n    pstr = '\\nCurrently defined LAtools configurations:\\n\\n' \n    for s in conf . sections ( ) : \n        if s == default : \n            pstr += s + ' [DEFAULT]\\n' \n        elif s == 'REPRODUCE' : \n            pstr += s + ' [DO NOT ALTER]\\n' \n        else : \n            pstr += s + '\\n' \n        for k , v in conf [ s ] . items ( ) : \n            if k != 'config' : \n                if v [ : 9 ] == 'resources' : \n                    v = pkgrs . resource_filename ( 'latools' , v ) \n                pstr += '   ' + k + ': ' + v + '\\n' \n        pstr += '\\n' \n    pprint . pprint ( pstr ) \n    return "}
{"9954": "\nimport pprint \ndef copy_SRM_file ( destination = None , config = 'DEFAULT' ) : \n    conf = read_configuration ( ) \n    src = pkgrs . resource_filename ( 'latools' , conf [ 'srmfile' ] ) \n    if destination is None : \n        destination = './LAtools_' + conf [ 'config' ] + '_SRMTable.csv' \n    if os . path . isdir ( destination ) : \n        destination += 'LAtools_' + conf [ 'config' ] + '_SRMTable.csv' \n    copyfile ( src , destination ) \n    pprint . pprint ( src + ' \\n    copied to:\\n      ' + destination ) \n    return "}
{"9956": "\nimport pprint \ndef change_default ( config ) : \n    config_file , cf = read_latoolscfg ( ) \n    if config not in cf . sections ( ) : \n        raise ValueError ( \"\\n'{:s}' is not a defined configuration.\" . format ( config ) ) \n    if config == 'REPRODUCE' : \n        pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\\n' + '     ... this is an odd thing to be doing.' ) \n    else : \n        pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) \n    response = input ( pstr + '\\n> [N/y]: ' ) \n    if response . lower ( ) == 'y' : \n        cf . set ( 'DEFAULT' , 'config' , config ) \n        with open ( config_file , 'w' ) as f : \n            cf . write ( f ) \n        pprint . pprint ( '  Default changed!' ) \n    else : \n        pprint . pprint ( '  Done nothing.' ) "}
{"9997": "\nimport pprint \ndef filter_status ( self , sample = None , subset = None , stds = False ) : \n    s = '' \n    if sample is None and subset is None : \n        if not self . _has_subsets : \n            s += 'Subset: All Samples\\n\\n' \n            s += self . data [ self . subsets [ 'All_Samples' ] [ 0 ] ] . filt . __repr__ ( ) \n        else : \n            for n in sorted ( str ( sn ) for sn in self . _subset_names ) : \n                if n in self . subsets : \n                    pass \n                elif int ( n ) in self . subsets : \n                    n = int ( n ) \n                    pass \n                s += 'Subset: ' + str ( n ) + '\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n            if len ( self . subsets [ 'not_in_set' ] ) > 0 : \n                s += '\\nNot in Subset:\\n' \n                s += 'Samples: ' + ', ' . join ( self . subsets [ 'not_in_set' ] ) + '\\n\\n' \n                s += self . data [ self . subsets [ 'not_in_set' ] [ 0 ] ] . filt . __repr__ ( ) \n        pprint . pprint ( s ) \n        return \n    elif sample is not None : \n        s += 'Sample: ' + sample + '\\n' \n        s += self . data [ sample ] . filt . __repr__ ( ) \n        pprint . pprint ( s ) \n        return \n    elif subset is not None : \n        if isinstance ( subset , ( str , int , float ) ) : \n            subset = [ subset ] \n        for n in subset : \n            s += 'Subset: ' + str ( n ) + '\\n' \n            s += 'Samples: ' + ', ' . join ( self . subsets [ n ] ) + '\\n\\n' \n            s += self . data [ self . subsets [ n ] [ 0 ] ] . filt . __repr__ ( ) \n        pprint . pprint ( s ) \n        return "}
{"9999": "\nimport pprint \ndef filter_nremoved ( self , filt = True , quiet = False ) : \n    rminfo = { } \n    for n in self . subsets [ 'All_Samples' ] : \n        s = self . data [ n ] \n        rminfo [ n ] = s . filt_nremoved ( filt ) \n    if not quiet : \n        maxL = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) \n        pprint . pprint ( '{string:{number}s}' . format ( string = 'Sample ' , number = maxL + 3 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) \n        for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : \n            pprint . pprint ( '{string:{number}s}' . format ( string = k , number = maxL + 3 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) \n    return rminfo "}
{"10012": "\nimport pprint \ndef by_regex ( file , outdir = None , split_pattern = None , global_header_rows = 0 , fname_pattern = None , trim_tail_lines = 0 , trim_head_lines = 0 ) : \n    if outdir is None : \n        outdir = os . path . join ( os . path . dirname ( file ) , 'split' ) \n    if not os . path . exists ( outdir ) : \n        os . mkdir ( outdir ) \n    with open ( file , 'r' ) as f : \n        lines = f . readlines ( ) \n    extension = os . path . splitext ( file ) [ - 1 ] \n    global_header = lines [ : global_header_rows ] \n    starts = [ ] \n    for i , line in enumerate ( lines ) : \n        if re . search ( split_pattern , line ) : \n            starts . append ( i ) \n    starts . append ( len ( lines ) ) \n    splits = { } \n    for i in range ( len ( starts ) - 1 ) : \n        m = re . search ( fname_pattern , lines [ starts [ i ] ] ) \n        if m : \n            fname = m . groups ( ) [ 0 ] . strip ( ) \n        else : \n            fname = 'no_name_{:}' . format ( i ) \n        splits [ fname ] = global_header + lines [ starts [ i ] : starts [ i + 1 ] ] [ trim_head_lines : trim_tail_lines ] \n    pprint . pprint ( 'Writing files to: {:}' . format ( outdir ) ) \n    for k , v in splits . items ( ) : \n        fname = ( k + extension ) . replace ( ' ' , '_' ) \n        with open ( os . path . join ( outdir , fname ) , 'w' ) as f : \n            f . writelines ( v ) \n        pprint . pprint ( '  {:}' . format ( fname ) ) \n    pprint . pprint ( 'Done.' ) \n    return outdir "}
{"10025": "\nimport pprint \ndef grab_filt ( self , filt , analyte = None ) : \n    if isinstance ( filt , str ) : \n        if filt in self . components : \n            if analyte is None : \n                return self . components [ filt ] \n            else : \n                if self . switches [ analyte ] [ filt ] : \n                    return self . components [ filt ] \n        else : \n            try : \n                ind = self . make_fromkey ( filt ) \n            except KeyError : \n                pprint . pprint ( ( \"\\n\\n***Filter key invalid. Please consult \" \"manual and try again.\" ) ) \n    elif isinstance ( filt , dict ) : \n        try : \n            ind = self . make_fromkey ( filt [ analyte ] ) \n        except ValueError : \n            pprint . pprint ( ( \"\\n\\n***Filter key invalid. Please consult manual \" \"and try again.\\nOR\\nAnalyte missing from filter \" \"key dict.\" ) ) \n    elif filt : \n        ind = self . make ( analyte ) \n    else : \n        ind = ~ np . zeros ( self . size , dtype = bool ) \n    return ind "}
{"10088": "\nimport pprint \ndef initialize ( self ) : \n    try : \n        logger . info ( \"Authenticating...\" ) \n        self . backend = Backend ( self . backend_url ) \n        self . backend . login ( self . username , self . password ) \n    except BackendException as exp : \n        logger . exception ( \"Exception: %s\" , exp ) \n        logger . error ( \"Response: %s\" , exp . response ) \n    if self . backend . token is None : \n        pprint . pprint ( \"Access denied!\" ) \n        pprint . pprint ( \"~~~~~~~~~~~~~~~~~~~~~~~~~~\" ) \n        pprint . pprint ( \"Exiting with error code: 1\" ) \n        exit ( 1 ) \n    logger . info ( \"Authenticated.\" ) \n    users = self . backend . get_all ( 'user' , { 'where' : json . dumps ( { 'name' : self . username } ) } ) \n    self . logged_in_user = users [ '_items' ] [ 0 ] \n    self . default_realm = self . logged_in_user [ '_realm' ] \n    self . realm_all = None \n    realms = self . backend . get_all ( 'realm' ) \n    for r in realms [ '_items' ] : \n        if r [ 'name' ] == 'All' and r [ '_level' ] == 0 : \n            self . realm_all = r [ '_id' ] \n            logger . info ( \"Found realm 'All': %s\" , self . realm_all ) \n        if r [ '_id' ] == self . default_realm : \n            logger . info ( \"Found logged-in user realm: %s\" , r [ 'name' ] ) \n    self . tp_always = None \n    self . tp_never = None \n    timeperiods = self . backend . get_all ( 'timeperiod' ) \n    for tp in timeperiods [ '_items' ] : \n        if tp [ 'name' ] == '24x7' : \n            self . tp_always = tp [ '_id' ] \n            logger . info ( \"Found TP '24x7': %s\" , self . tp_always ) \n        if tp [ 'name' ] . lower ( ) == 'none' or tp [ 'name' ] . lower ( ) == 'never' : \n            self . tp_never = tp [ '_id' ] \n            logger . info ( \"Found TP 'Never': %s\" , self . tp_never ) "}
{"10210": "\nimport pprint \ndef print_downloads ( self ) : \n    for path , ann in self . annotation . items ( ) : \n        if path . startswith ( 'output' ) and ann [ 'type' ] == 'basic:file:' : \n            pprint . pprint ( \"{}: {}\" . format ( path , ann [ 'value' ] [ 'file' ] ) ) "}
{"10214": "\nimport pprint \ndef print_processor_inputs ( self , processor_name ) : \n    p = self . processors ( processor_name = processor_name ) \n    if len ( p ) == 1 : \n        p = p [ 0 ] \n    else : \n        Exception ( 'Invalid processor name' ) \n    for field_schema , _ , _ in iterate_schema ( { } , p [ 'input_schema' ] , 'input' ) : \n        name = field_schema [ 'name' ] \n        typ = field_schema [ 'type' ] \n        pprint . pprint ( \"{} -> {}\" . format ( name , typ ) ) "}
{"10217": "\nimport pprint \ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if i > 0 and response is not None : \n                    pprint . pprint ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200 , 201 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    pprint . pprint ( ) \n    return session_id "}
{"10330": "\nimport pprint \ndef runProcess ( args , timeout , grace , reactor ) : \n    deferred = defer . Deferred ( ) \n    protocol = ProcessProtocol ( deferred ) \n    process = reactor . spawnProcess ( protocol , args [ 0 ] , args , env = os . environ ) \n    def _logEnded ( err ) : \n        err . trap ( tierror . ProcessDone , tierror . ProcessTerminated ) \n        pprint . pprint ( err . value ) \n    deferred . addErrback ( _logEnded ) \n    def _cancelTermination ( dummy ) : \n        for termination in terminations : \n            if termination . active ( ) : \n                termination . cancel ( ) \n    deferred . addCallback ( _cancelTermination ) \n    terminations = [ ] \n    terminations . append ( reactor . callLater ( timeout , process . signalProcess , \"TERM\" ) ) \n    terminations . append ( reactor . callLater ( timeout + grace , process . signalProcess , \"KILL\" ) ) \n    return deferred "}
{"10443": "\nimport pprint \ndef set_version ( self , new_version : str ) : \n    try : \n        f = open ( self . file_path , 'r' ) \n        lines = f . readlines ( ) \n        f . close ( ) \n    except Exception as e : \n        pprint . pprint ( str ( e ) ) \n        return \n    for idx , line in enumerate ( lines ) : \n        if self . magic_line in line : \n            start = len ( self . magic_line ) \n            end = len ( line ) - self . strip_end_chars \n            start_str = line [ 0 : start ] \n            end_str = line [ end : ] \n            lines [ idx ] = start_str + new_version + end_str \n    try : \n        f = open ( self . file_path , 'w' ) \n        f . writelines ( lines ) \n        f . close ( ) \n    except Exception as e : \n        pprint . pprint ( str ( e ) ) \n        return "}
{"10478": "\nimport pprint \ndef run ( self ) : \n    if ON_READ_THE_DOCS : \n        return \n    try : \n        pyside_rcc_command = 'pyside-rcc' \n        if sys . platform == 'win32' : \n            import PySide \n            pyside_rcc_command = os . path . join ( os . path . dirname ( PySide . __file__ ) , 'pyside-rcc.exe' ) \n        subprocess . check_call ( [ pyside_rcc_command , '-o' , self . resource_target_path , self . resource_source_path ] ) \n    except ( subprocess . CalledProcessError , OSError ) : \n        pprint . pprint ( 'Error compiling resource.py using pyside-rcc. Possibly ' 'pyside-rcc could not be found. You might need to manually add ' 'it to your PATH.' ) \n        raise SystemExit ( ) "}
{"10499": "\nimport pprint \ndef execute ( self , override_wf_json = None ) : \n    r = self . gbdx . post ( self . URL , json = self . json if override_wf_json is None else override_wf_json ) \n    try : \n        r . raise_for_status ( ) \n    except : \n        pprint . pprint ( \"GBDX API Status Code: %s\" % r . status_code ) \n        pprint . pprint ( \"GBDX API Response: %s\" % r . text ) \n        self . id = None \n        return \n    self . id = r . json ( ) [ 'id' ] \n    self . _refresh_status ( ) "}
{"10502": "\nimport pprint \ndef list ( pattern = ( ) ) : \n    globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] \n    matches = [ ] \n    offset = len ( PROJ_ARCHIVE ) + 1 \n    for suffix in globs : \n        glob_pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , suffix ) \n        matches . append ( set ( f [ offset : ] for f in glob . glob ( glob_pattern ) ) ) \n    matches = reduce ( lambda x , y : x . intersection ( y ) , matches ) \n    for m in sorted ( matches ) : \n        pprint . pprint ( m ) "}
{"10503": "\nimport pprint \ndef restore ( folder ) : \n    if os . path . isdir ( folder ) : \n        bail ( 'a folder of the same name already exists!' ) \n    pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) \n    matches = glob . glob ( pattern ) \n    if not matches : \n        bail ( 'no project matches: ' + folder ) \n    if len ( matches ) > 1 : \n        pprint . pprint ( 'Warning: multiple matches, picking the most recent' , file = sys . stderr ) \n    source = sorted ( matches ) [ - 1 ] \n    pprint . pprint ( source , '-->' , folder ) \n    shutil . move ( source , '.' ) "}
{"10545": "\nimport pprint \ndef rfxcom ( device ) : \n    if device is None : \n        device = app . config . get ( 'DEVICE' ) \n    if device is None : \n        pprint . pprint ( \"The serial device needs to be passed in as --device or \" \"set in the config as DEVICE.\" ) \n        return \n    rfxcom_collect ( device ) "}
{"10588": "\nimport pprint \ndef run ( cls , name , desc ) : \n    wrapper = cls ( name , desc ) \n    mount_path = wrapper . _get_mount_path ( ) \n    arg_parser = wrapper . _create_argument_parser ( ) \n    wrapper . _extend_argument_parser ( arg_parser ) \n    empty_config = wrapper . __get_empty_config ( ) \n    config_yaml = ruamel . yaml . load ( empty_config ) \n    wrapper . __populate_parser_from_config ( arg_parser , config_yaml ) \n    args = arg_parser . parse_args ( ) \n    for k , v in vars ( args ) . items ( ) : \n        k = k . replace ( '_' , '-' ) \n        if k in config_yaml : \n            config_yaml [ k ] = v \n    config_path = wrapper . _get_config_path ( ) \n    with open ( config_path , 'w' ) as writable : \n        ruamel . yaml . dump ( config_yaml , stream = writable ) \n    workdir_path = os . path . join ( mount_path , 'Toil-' + wrapper . _name ) \n    if os . path . exists ( workdir_path ) : \n        if args . restart : \n            log . info ( 'Reusing temporary directory: %s' , workdir_path ) \n        else : \n            raise UserError ( 'Temporary directory {} already exists. Run with --restart ' 'option or remove directory.' . format ( workdir_path ) ) \n    else : \n        os . makedirs ( workdir_path ) \n        log . info ( 'Temporary directory created: %s' , workdir_path ) \n    command = wrapper . _create_pipeline_command ( args , workdir_path , config_path ) \n    wrapper . _extend_pipeline_command ( command , args ) \n    try : \n        subprocess . check_call ( command ) \n    except subprocess . CalledProcessError as e : \n        pprint . pprint ( e , file = sys . stderr ) \n    finally : \n        stat = os . stat ( mount_path ) \n        log . info ( 'Pipeline terminated, changing ownership of output files in %s from root to ' 'uid %s and gid %s.' , mount_path , stat . st_uid , stat . st_gid ) \n        chown_command = [ 'chown' , '-R' , '%s:%s' % ( stat . st_uid , stat . st_gid ) , mount_path ] \n        subprocess . check_call ( chown_command ) \n        if args . no_clean : \n            log . info ( 'Flag \"--no-clean\" was used, therefore %s was not deleted.' , workdir_path ) \n        else : \n            log . info ( 'Cleaning up temporary directory: %s' , workdir_path ) \n            shutil . rmtree ( workdir_path ) "}
{"10670": "\nimport pprint \ndef open_in_browser ( doc , encoding = None ) : \n    import os \n    import webbrowser \n    import tempfile \n    if not isinstance ( doc , etree . _ElementTree ) : \n        doc = etree . ElementTree ( doc ) \n    handle , fn = tempfile . mkstemp ( suffix = '.html' ) \n    f = os . fdopen ( handle , 'wb' ) \n    try : \n        doc . write ( f , method = \"html\" , encoding = encoding or doc . docinfo . encoding or \"UTF-8\" ) \n    finally : \n        f . close ( ) \n    url = 'file://' + fn . replace ( os . path . sep , '/' ) \n    pprint . pprint ( url ) \n    webbrowser . open ( url ) "}
{"10730": "\nimport pprint \ndef run ( self , options , args ) : \n    shells = COMPLETION_SCRIPTS . keys ( ) \n    shell_options = [ '--' + shell for shell in sorted ( shells ) ] \n    if options . shell in shells : \n        script = COMPLETION_SCRIPTS . get ( options . shell , '' ) \n        pprint . pprint ( BASE_COMPLETION % { 'script' : script , 'shell' : options . shell } ) \n    else : \n        sys . stderr . write ( 'ERROR: You must pass %s\\n' % ' or ' . join ( shell_options ) ) "}
{"10785": "\nimport pprint \ndef do_dice_roll ( ) : \n    options = get_options ( ) \n    dice = Dice ( options . sides ) \n    rolls = [ dice . roll ( ) for n in range ( options . number ) ] \n    for roll in rolls : \n        pprint . pprint ( 'rolled' , roll ) \n    if options . number > 1 : \n        pprint . pprint ( 'total' , sum ( rolls ) ) "}
{"10876": "\nimport pprint \ndef report ( self ) : \n    pprint . pprint ( self . report_message . format ( service = self . service , host = self . host , port = self . port , ) ) \n    sys . stdout . flush ( ) "}
{"11174": "\nimport pprint \ndef pfprint ( item , end = '\\n' , file = None ) : \n    if file is None : \n        file = sys . stdout \n    pprint . pprint ( item , end = end , file = file ) "}
{"11194": "\nimport pprint \ndef run ( self ) : \n    pprint . pprint ( 'Executing planarrad' ) \n    if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n        self . data ( ) \n        self . check_values ( ) \n        if self . without_error == False : \n            self . display_error_message ( ) \n        elif self . without_error == True : \n            self . is_running = True \n            self . hide_error_message ( ) \n            self . write_to_file ( ) \n            os . chdir ( './' ) \n            self . progress_bar ( ) \n            this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) \n            batch_file = os . path . join ( this_dir , \"inputs/batch_files/\" + str ( self . batch_name_value ) + \"_batch.txt\" ) \n            pprint . pprint ( batch_file ) \n            self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch_file ] , shell = True ) \n            if self . ui . progressBar . value ( ) == 100 : \n                self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) "}
{"11195": "\nimport pprint \ndef cancel_planarrad ( self ) : \n    if ( self . is_running == True ) & ( self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE ) : \n        cancel = QtGui . QMessageBox . question ( self . ui . cancel , 'Cancel PlanarRad' , \"Are you sure to cancel ?\" , QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n        if cancel == QtGui . QMessageBox . Yes : \n            self . is_running = False \n            os . kill ( self . p . pid , signal . SIGTERM ) \n            pprint . pprint ( \"Necessary to check if cancel_planarrad works well !\" ) \n            self . ui . progressBar . reset ( ) \n        else : \n            pass "}
{"11330": "\nimport pprint \ndef run ( query , params = None , config = None , conn = None , ** kwargs ) : \n    if params is None : \n        params = { } \n    if conn is None : \n        conn = Connection . get ( DEFAULT_CONFIGURABLE [ \"uri\" ] ) \n    elif isinstance ( conn , string_types ) : \n        conn = Connection . get ( conn ) \n    if config is None : \n        default_config = DEFAULT_CONFIGURABLE . copy ( ) \n        kwargs . update ( default_config ) \n        config = DefaultConfigurable ( ** kwargs ) \n    if query . strip ( ) : \n        params = extract_params_from_query ( query , params ) \n        result = conn . session . query ( query , params , data_contents = config . data_contents ) \n        if config . feedback : \n            pprint . pprint ( interpret_stats ( result ) ) \n        resultset = ResultSet ( result , query , config ) \n        if config . auto_pandas : \n            return resultset . get_dataframe ( ) \n        elif config . auto_networkx : \n            graph = resultset . get_graph ( ) \n            resultset . draw ( ) \n            return graph \n        else : \n            return resultset \n    else : \n        return 'Connected: %s' % conn . name "}
{"11410": "\nimport pprint \ndef discrete_best_alpha ( data , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True ) : \n    xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] \n    best_index = argmin ( ksvalues ) \n    best_alpha = alpha_of_xmin [ best_index ] \n    best_xmin = xmins [ best_index ] \n    best_ks = ksvalues [ best_index ] \n    best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if verbose : \n        pprint . pprint ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( data < best_xmin ) . sum ( ) , ( data >= best_xmin ) . sum ( ) ) ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11411": "\nimport pprint \ndef discrete_best_alpha ( self , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201 , approximate = True , verbose = True , finite = True ) : \n    data = self . data \n    self . _xmins = xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = np . array ( [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] ) \n    self . _alpha_values = np . array ( alpha_of_xmin ) \n    self . _xmin_kstest = ksvalues \n    ksvalues [ np . isnan ( ksvalues ) ] = np . inf \n    best_index = argmin ( ksvalues ) \n    self . _alpha = best_alpha = alpha_of_xmin [ best_index ] \n    self . _xmin = best_xmin = xmins [ best_index ] \n    self . _ks = best_ks = ksvalues [ best_index ] \n    self . _likelihood = best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if finite : \n        self . _alpha = self . _alpha * ( n - 1. ) / n + 1. / n \n    if verbose : \n        pprint . pprint ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( data < best_xmin ) . sum ( ) , ( data >= best_xmin ) . sum ( ) ) ) \n    self . _ngtx = n = ( self . data >= self . _xmin ) . sum ( ) \n    self . _alphaerr = ( self . _alpha - 1.0 ) / np . sqrt ( n ) \n    if scipyOK : \n        self . _ks_prob = scipy . stats . ksone . sf ( self . _ks , n ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11413": "\nimport pprint \ndef lognormal ( self , doprint = True ) : \n    if scipyOK : \n        fitpars = scipy . stats . lognorm . fit ( self . data ) \n        self . lognormal_dist = scipy . stats . lognorm ( * fitpars ) \n        self . lognormal_ksD , self . lognormal_ksP = scipy . stats . kstest ( self . data , self . lognormal_dist . cdf ) \n        self . lognormal_likelihood = - 1 * scipy . stats . lognorm . nnlf ( fitpars , self . data ) \n        self . power_lognorm_likelihood = ( self . _likelihood + self . lognormal_likelihood ) \n        self . likelihood_ratio_D = - 2 * ( log ( self . _likelihood / self . lognormal_likelihood ) ) \n        if doprint : \n            pprint . pprint ( \"Lognormal KS D: %g  p(D): %g\" % ( self . lognormal_ksD , self . lognormal_ksP ) , end = ' ' ) \n            pprint . pprint ( \"  Likelihood Ratio Statistic (powerlaw/lognormal): %g\" % self . likelihood_ratio_D ) \n            pprint . pprint ( \"At this point, have a look at Clauset et al 2009 Appendix C: determining sigma(likelihood_ratio)\" ) "}
{"11491": "\nimport pprint \ndef dprint ( name , val ) : \n    from pprint import pformat \n    pprint . pprint ( '% 5s: %s' % ( name , '\\n       ' . join ( pformat ( val , indent = 4 , width = 75 , ) . split ( '\\n' ) ) , ) , ) "}
{"11497": "\nimport pprint \ndef process_ddp ( self , data ) : \n    msg_id = data . get ( 'id' , None ) \n    try : \n        msg = data . pop ( 'msg' ) \n    except KeyError : \n        self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) \n        return \n    try : \n        self . dispatch ( msg , data ) \n    except Exception as err : \n        kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } \n        if msg_id is not None : \n            kwargs [ 'id' ] = msg_id \n        if isinstance ( err , MeteorError ) : \n            error = err . as_dict ( ) \n        else : \n            error = { 'error' : 500 , 'reason' : 'Internal server error' , } \n        if kwargs [ 'msg' ] == 'error' : \n            kwargs . update ( error ) \n        else : \n            kwargs [ 'error' ] = error \n        if not isinstance ( err , MeteorError ) : \n            stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = 1 , ) \n            if stack is not None : \n                traceback . print_exc ( file = sys . stderr ) \n                sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) \n                sys . stderr . write ( stack ) \n        elif settings . DEBUG : \n            pprint . pprint ( 'ERROR: %s' % err ) \n            dprint ( 'msg' , msg ) \n            dprint ( 'data' , data ) \n            error . setdefault ( 'details' , traceback . format_exc ( ) ) \n            pprint . pprint ( error [ 'details' ] ) \n        self . reply ( ** kwargs ) \n        if msg_id and msg == 'method' : \n            self . reply ( 'updated' , methods = [ msg_id ] ) "}
{"11507": "\nimport pprint \ndef print ( self , msg , * args , ** kwargs ) : \n    if self . verbosity >= 1 : \n        pprint . pprint ( msg , * args , ** kwargs ) "}
{"11530": "\nimport pprint \ndef fast_forward_selection ( scenarios , number_of_reduced_scenarios , probability = None ) : \n    pprint . pprint ( \"Running fast forward selection algorithm\" ) \n    number_of_scenarios = scenarios . shape [ 1 ] \n    logger . debug ( \"Input number of scenarios = %d\" , number_of_scenarios ) \n    if probability is None : \n        probability = np . array ( [ 1 / number_of_scenarios for i in range ( 0 , number_of_scenarios ) ] ) \n    z = np . array ( [ np . inf for i in range ( 0 , number_of_scenarios ) ] ) \n    c = np . zeros ( ( number_of_scenarios , number_of_scenarios ) ) \n    J = range ( 0 , number_of_scenarios ) \n    if number_of_reduced_scenarios >= number_of_scenarios : \n        return ( scenarios , probability , J ) \n    for scenario_k in range ( 0 , number_of_scenarios ) : \n        for scenario_u in range ( 0 , number_of_scenarios ) : \n            c [ scenario_k , scenario_u ] = distance ( scenarios [ : , scenario_k ] , scenarios [ : , scenario_u ] ) \n    for scenario_u in range ( 0 , number_of_scenarios ) : \n        summation = 0 \n        for scenario_k in range ( 0 , number_of_scenarios ) : \n            if scenario_k != scenario_u : \n                summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n        z [ scenario_u ] = summation \n    U = [ np . argmin ( z ) ] \n    for u in U : \n        J . remove ( u ) \n    for _ in range ( 0 , number_of_scenarios - number_of_reduced_scenarios - 1 ) : \n        pprint . pprint ( \"Running {}\" . format ( _ ) ) \n        for scenario_u in J : \n            for scenario_k in J : \n                lowest_value = np . inf \n                for scenario_number in U : \n                    lowest_value = min ( c [ scenario_k , scenario_u ] , c [ scenario_k , scenario_number ] ) \n            c [ scenario_k , scenario_u ] = lowest_value \n        for scenario_u in J : \n            summation = 0 \n            for scenario_k in J : \n                if scenario_k not in U : \n                    summation = summation + probability [ scenario_k ] * c [ scenario_k , scenario_u ] \n            z [ scenario_u ] = summation \n        u_i = np . argmin ( [ item if i in J else np . inf for i , item in enumerate ( z ) ] ) \n        J . remove ( u_i ) \n        U . append ( u_i ) \n    reduced_scenario_set = U \n    reduced_probability = [ ] \n    reduced_probability = copy . deepcopy ( probability ) \n    for deleted_scenario_number in J : \n        lowest_value = np . inf \n        for scenario_j in reduced_scenario_set : \n            if c [ deleted_scenario_number , scenario_j ] < lowest_value : \n                closest_scenario_number = scenario_j \n                lowest_value = c [ deleted_scenario_number , scenario_j ] \n        reduced_probability [ closest_scenario_number ] = reduced_probability [ closest_scenario_number ] + reduced_probability [ deleted_scenario_number ] \n    reduced_scenarios = copy . deepcopy ( scenarios [ : , reduced_scenario_set ] ) \n    reduced_probability = reduced_probability [ reduced_scenario_set ] \n    return reduced_scenarios , reduced_probability , reduced_scenario_set "}
{"11604": "\nimport pprint \ndef write_gro ( outfile , title , atoms , box ) : \n    pprint . pprint ( title , file = outfile ) \n    pprint . pprint ( \"{:5d}\" . format ( len ( atoms ) ) , file = outfile ) \n    atom_template = \"{:5d}{:<5s}{:>5s}{:5d}{:8.3f}{:8.3f}{:8.3f}\" \n    for idx , atname , resname , resid , x , y , z in atoms : \n        pprint . pprint ( atom_template . format ( int ( resid % 1e5 ) , resname , atname , int ( idx % 1e5 ) , x , y , z ) , file = outfile ) \n    grobox = ( box [ 0 ] [ 0 ] , box [ 1 ] [ 1 ] , box [ 2 ] [ 2 ] , box [ 0 ] [ 1 ] , box [ 0 ] [ 2 ] , box [ 1 ] [ 0 ] , box [ 1 ] [ 2 ] , box [ 2 ] [ 0 ] , box [ 2 ] [ 1 ] ) \n    box_template = '{:10.5f}' * 9 \n    pprint . pprint ( box_template . format ( * grobox ) , file = outfile ) "}
{"11605": "\nimport pprint \ndef write_pdb ( outfile , title , atoms , box ) : \n    pprint . pprint ( 'TITLE ' + title , file = outfile ) \n    pprint . pprint ( pdbBoxString ( box ) , file = outfile ) \n    for idx , atname , resname , resid , x , y , z in atoms : \n        pprint . pprint ( pdbline % ( idx % 1e5 , atname [ : 4 ] , resname [ : 3 ] , \"\" , resid % 1e4 , '' , 10 * x , 10 * y , 10 * z , 0 , 0 , '' ) , file = outfile ) "}
{"11608": "\nimport pprint \ndef write_top ( outpath , molecules , title ) : \n    topmolecules = [ ] \n    for i in molecules : \n        if i [ 0 ] . endswith ( '.o' ) : \n            topmolecules . append ( tuple ( [ i [ 0 ] [ : - 2 ] ] + list ( i [ 1 : ] ) ) ) \n        else : \n            topmolecules . append ( i ) \n    if outpath : \n        with open ( outpath , \"w\" ) as top : \n            pprint . pprint ( '#include \"martini.itp\"\\n' , file = top ) \n            pprint . pprint ( '[ system ]' , file = top ) \n            pprint . pprint ( '; name' , file = top ) \n            pprint . pprint ( title , file = top ) \n            pprint . pprint ( '\\n' , file = top ) \n            pprint . pprint ( '[ molecules ]' , file = top ) \n            pprint . pprint ( '; name  number' , file = top ) \n            pprint . pprint ( \"\\n\" . join ( \"%-10s %7d\" % i for i in topmolecules ) , file = top ) \n    else : \n        added_molecules = ( molecule for molecule in topmolecules if molecule [ 0 ] != 'Protein' ) \n        pprint . pprint ( \"\\n\" . join ( \"%-10s %7d\" % i for i in added_molecules ) , file = sys . stderr ) "}
{"11621": "\nimport pprint \ndef display_required_items ( msg_type ) : \n    pprint . pprint ( \"Configure a profile for: \" + msg_type ) \n    pprint . pprint ( \"You will need the following information:\" ) \n    for k , v in CONFIG [ msg_type ] [ \"settings\" ] . items ( ) : \n        pprint . pprint ( \"   * \" + v ) \n    pprint . pprint ( \"Authorization/credentials required:\" ) \n    for k , v in CONFIG [ msg_type ] [ \"auth\" ] . items ( ) : \n        pprint . pprint ( \"   * \" + v ) "}
{"11624": "\nimport pprint \ndef configure_profile ( msg_type , profile_name , data , auth ) : \n    with jsonconfig . Config ( \"messages\" , indent = 4 ) as cfg : \n        write_data ( msg_type , profile_name , data , cfg ) \n        write_auth ( msg_type , profile_name , auth , cfg ) \n    pprint . pprint ( \"[+] Configuration entry for <\" + profile_name + \"> created.\" ) \n    pprint . pprint ( \"[+] Configuration file location: \" + cfg . filename ) "}
{"11628": "\nimport pprint \ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        pprint . pprint ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    if encoding == \"json\" : \n        resp = requests . post ( self . url , json = self . message ) \n    elif encoding == \"url\" : \n        resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and resp . history [ 0 ] . status_code >= 300 : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        elif \"invalid_auth\" in resp . text : \n            raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        pprint . pprint ( timestamp ( ) , type ( self ) . __name__ , \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status_code , ) \n    pprint . pprint ( \"Message sent.\" ) "}
{"11643": "\nimport pprint \ndef _send_content ( self , method = \"/sendMessage\" ) : \n    url = self . base_url + method \n    try : \n        resp = requests . post ( url , json = self . message ) \n        resp . raise_for_status ( ) \n    except requests . exceptions . HTTPError as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        if method == \"/sendMessage\" : \n            content_type = \"Message body\" \n        elif method == \"/sendDocument\" : \n            content_type = \"Attachment: \" + self . message [ \"document\" ] \n        pprint . pprint ( timestamp ( ) , content_type , \"sent.\" ) "}
{"11644": "\nimport pprint \ndef send ( self ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        pprint . pprint ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    self . _send_content ( \"/sendMessage\" ) \n    if self . attachments : \n        if isinstance ( self . attachments , str ) : \n            self . attachments = [ self . attachments ] \n        for a in self . attachments : \n            self . message [ \"document\" ] = a \n            self . _send_content ( method = \"/sendDocument\" ) \n    if self . verbose : \n        pprint . pprint ( timestamp ( ) , type ( self ) . __name__ + \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , ) \n    pprint . pprint ( \"Message sent.\" ) "}
{"11653": "\nimport pprint \ndef send ( self ) : \n    self . _generate_email ( ) \n    if self . verbose : \n        pprint . pprint ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    recipients = [ ] \n    for i in ( self . to , self . cc , self . bcc ) : \n        if i : \n            if isinstance ( i , MutableSequence ) : \n                recipients += i \n            else : \n                recipients . append ( i ) \n    session = self . _get_session ( ) \n    if self . verbose : \n        pprint . pprint ( timestamp ( ) , \"Login successful.\" ) \n    session . sendmail ( self . from_ , recipients , self . message . as_string ( ) ) \n    session . quit ( ) \n    if self . verbose : \n        pprint . pprint ( timestamp ( ) , \"Logged out.\" ) \n    if self . verbose : \n        pprint . pprint ( timestamp ( ) , type ( self ) . __name__ + \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , ) \n    pprint . pprint ( \"Message sent.\" ) "}
{"11786": "\nimport pprint \ndef long_input ( prompt = 'Multi-line input\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxlines = None , maxlength = None ) : \n    lines = [ ] \n    pprint . pprint ( prompt ) \n    lnum = 1 \n    try : \n        while True : \n            if maxlines : \n                if lnum > maxlines : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    lnum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return '\\n' . join ( lines ) "}
{"11787": "\nimport pprint \ndef list_input ( prompt = 'List input - enter each item on a seperate line\\n' + 'Enter EOF on a blank line to end ' + '(ctrl-D in *nix, ctrl-Z in windows)' , maxitems = None , maxlength = None ) : \n    lines = [ ] \n    pprint . pprint ( prompt ) \n    inum = 1 \n    try : \n        while True : \n            if maxitems : \n                if inum > maxitems : \n                    break \n                else : \n                    if maxlength : \n                        lines . append ( string_input ( '' ) [ : maxlength ] ) \n                    else : \n                        lines . append ( string_input ( '' ) ) \n                    inum += 1 \n            else : \n                if maxlength : \n                    lines . append ( string_input ( '' ) [ : maxlength ] ) \n                else : \n                    lines . append ( string_input ( '' ) ) \n    except EOFError : \n        pass \n    finally : \n        return lines "}
{"11788": "\nimport pprint \ndef outfile_input ( extension = None ) : \n    fileok = False \n    while not fileok : \n        filename = string_input ( 'File name? ' ) \n        if extension : \n            if not filename . endswith ( extension ) : \n                if extension . startswith ( '.' ) : \n                    filename = filename + extension \n                else : \n                    filename = filename + '.' + extension \n        if os . path . isfile ( filename ) : \n            choice = choice_input ( prompt = filename + ' already exists. Overwrite?' , options = [ 'y' , 'n' ] ) \n            if choice == 'y' : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'a' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = True \n                except IOError : \n                    pprint . pprint ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except PermissionError : \n                    pprint . pprint ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except FileNotFoundError : \n                    pprint . pprint ( filename + ': directory not found. Try again.' ) \n        else : \n            choice = choice_input ( prompt = filename + ' does not exist. Create it?' , options = [ 'y' , 'n' ] ) \n            if choice == 'y' : \n                try : \n                    nowtime = time . time ( ) \n                    with open ( filename , 'w' ) as f : \n                        os . utime ( filename , ( nowtime , nowtime ) ) \n                    fileok = True \n                except IOError : \n                    pprint . pprint ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except PermissionError : \n                    pprint . pprint ( 'Write permission denied on ' + filename + '. Try again.' ) \n                except FileNotFoundError : \n                    pprint . pprint ( filename + ': directory not found. Try again.' ) \n    return filename "}
{"11805": "\nimport pprint \ndef cache ( func ) : \n    CACHE_DIR = appdirs . user_cache_dir ( 'sportsref' , getpass . getuser ( ) ) \n    if not os . path . isdir ( CACHE_DIR ) : \n        os . makedirs ( CACHE_DIR ) \n    \n    @ funcutils . wraps ( func ) \n    def wrapper ( url ) : \n        file_hash = hashlib . md5 ( ) \n        encoded_url = url . encode ( errors = 'replace' ) \n        file_hash . update ( encoded_url ) \n        file_hash = file_hash . hexdigest ( ) \n        filename = '{}/{}' . format ( CACHE_DIR , file_hash ) \n        sport_id = None \n        for a_base_url , a_sport_id in sportsref . SITE_ABBREV . items ( ) : \n            if url . startswith ( a_base_url ) : \n                sport_id = a_sport_id \n                break \n        else : \n            pprint . pprint ( 'No sport ID found for {}, not able to check cache' . format ( url ) ) \n        file_exists = os . path . isfile ( filename ) \n        if sport_id and file_exists : \n            cur_time = int ( time . time ( ) ) \n            mod_time = int ( os . path . getmtime ( filename ) ) \n            days_since_mod = datetime . timedelta ( seconds = ( cur_time - mod_time ) ) . days \n            days_cache_valid = globals ( ) [ '_days_valid_{}' . format ( sport_id ) ] ( url ) \n            cache_is_valid = days_since_mod < days_cache_valid \n        else : \n            cache_is_valid = False \n        allow_caching = sportsref . get_option ( 'cache' ) \n        if file_exists and cache_is_valid and allow_caching : \n            with codecs . open ( filename , 'r' , encoding = 'utf-8' , errors = 'replace' ) as f : \n                text = f . read ( ) \n        else : \n            text = func ( url ) \n            with codecs . open ( filename , 'w+' , encoding = 'utf-8' ) as f : \n                f . write ( text ) \n        return text \n    return wrapper "}
{"11807": "\nimport pprint \ndef memoize ( fun ) : \n    \n    @ funcutils . wraps ( fun ) \n    def wrapper ( * args , ** kwargs ) : \n        do_memoization = sportsref . get_option ( 'memoize' ) \n        if not do_memoization : \n            return fun ( * args , ** kwargs ) \n        hash_args = tuple ( args ) \n        hash_kwargs = frozenset ( sorted ( kwargs . items ( ) ) ) \n        key = ( hash_args , hash_kwargs ) \n        def _copy ( v ) : \n            if isinstance ( v , pq ) : \n                return v . clone ( ) \n            else : \n                return copy . deepcopy ( v ) \n        try : \n            ret = _copy ( cache [ key ] ) \n            return ret \n        except KeyError : \n            cache [ key ] = fun ( * args , ** kwargs ) \n            ret = _copy ( cache [ key ] ) \n            return ret \n        except TypeError : \n            pprint . pprint ( 'memoization type error in function {} for arguments {}' . format ( fun . __name__ , key ) ) \n            raise \n    cache = { } \n    return wrapper "}
{"11838": "\nimport pprint \ndef rel_url_to_id ( url ) : \n    yearRegex = r'.*/years/(\\d{4}).*|.*/gamelog/(\\d{4}).*' \n    playerRegex = r'.*/players/(?:\\w/)?(.+?)(?:/|\\.html?)' \n    boxscoresRegex = r'.*/boxscores/(.+?)\\.html?' \n    teamRegex = r'.*/teams/(\\w{3})/.*' \n    coachRegex = r'.*/coaches/(.+?)\\.html?' \n    stadiumRegex = r'.*/stadiums/(.+?)\\.html?' \n    refRegex = r'.*/officials/(.+?r)\\.html?' \n    collegeRegex = r'.*/schools/(\\S+?)/.*|.*college=([^&]+)' \n    hsRegex = r'.*/schools/high_schools\\.cgi\\?id=([^\\&]{8})' \n    bsDateRegex = r'.*/boxscores/index\\.f?cgi\\?(month=\\d+&day=\\d+&year=\\d+)' \n    leagueRegex = r'.*/leagues/(.*_\\d{4}).*' \n    awardRegex = r'.*/awards/(.+)\\.htm' \n    regexes = [ yearRegex , playerRegex , boxscoresRegex , teamRegex , coachRegex , stadiumRegex , refRegex , collegeRegex , hsRegex , bsDateRegex , leagueRegex , awardRegex , ] \n    for regex in regexes : \n        match = re . match ( regex , url , re . I ) \n        if match : \n            return [ _f for _f in match . groups ( ) if _f ] [ 0 ] \n    if any ( url . startswith ( s ) for s in ( '/play-index/' , ) ) : \n        return url \n    pprint . pprint ( 'WARNING. NO MATCH WAS FOUND FOR \"{}\"' . format ( url ) ) \n    return url "}
{"11864": "\nimport pprint \ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( 0 , 3 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3 : \n                pprint . pprint ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"12008": "\nimport pprint \ndef check_version ( ) : \n    import requests \n    r = requests . get ( 'https://pypi.python.org/pypi/ndio/json' ) . json ( ) \n    r = r [ 'info' ] [ 'version' ] \n    if r != version : \n        pprint . pprint ( \"A newer version of ndio is available. \" + \"'pip install -U ndio' to update.\" ) \n    return r "}
{"12012": "\nimport pprint \ndef print_update ( self , repo_name , repo_path ) : \n    color = Color ( ) \n    self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo_name , repo_path ) , \"green\" ) ) \n    try : \n        repo = Repository ( repo_path ) \n        repo . update ( ) \n    except RepositoryError as e : \n        self . logger . error ( e ) \n        pass \n    pprint . pprint ( \"\\n\" ) "}
{"12019": "\nimport pprint \ndef print_status ( self , repo_name , repo_path ) : \n    color = Color ( ) \n    self . logger . info ( color . colored ( \"=> [%s] %s\" % ( repo_name , repo_path ) , \"green\" ) ) \n    try : \n        repo = Repository ( repo_path ) \n        repo . status ( ) \n    except RepositoryError as e : \n        self . logger . error ( e ) \n        pass \n    pprint . pprint ( \"\\n\" ) "}
{"12039": "\nimport pprint \ndef show_all ( self ) : \n    for ws in self . workspace . list ( ) . keys ( ) : \n        self . show_workspace ( ws ) \n        pprint . pprint ( \"\\n\\n\" ) "}
{"12111": "\nimport pprint \ndef _send_request ( self ) : \n    msg = Message ( ) \n    msg . subject = \"An RPC call!\" \n    msg . address = self . _to \n    msg . reply_to = self . _reply_to \n    msg . body = self . _method \n    msg . correlation_id = 5 \n    pprint . pprint ( \"sending RPC call request: %s\" % str ( self . _method ) ) \n    self . _sender . send ( msg , self ) "}
{"12267": "\nimport pprint \ndef do_POST ( self ) : \n    self . send_response ( urllib2 . httplib . OK ) \n    self . end_headers ( ) \n    content_length = int ( self . headers [ 'Content-Length' ] ) \n    body = self . rfile . read ( content_length ) \n    pprint . pprint ( \"Client: {0}\" . format ( str ( self . client_address ) ) ) \n    pprint . pprint ( \"headers: {0}\" . format ( self . headers ) ) \n    pprint . pprint ( \"path: {0}\" . format ( self . path ) ) \n    pprint . pprint ( \"body: {0}\" . format ( body ) ) "}
{"12282": "\nimport pprint \ndef _call_api ( self ) : \n    sockobj = socket ( AF_INET , SOCK_STREAM ) \n    sockobj . connect ( ( self . rpc_host , self . rpc_port ) ) \n    self . get_json ( ) \n    message = [ self . rpc_message . encode ( 'utf-8' ) ] \n    for line in message : \n        sockobj . send ( line ) \n        data = sockobj . recv ( self . MAX_LINE ) \n        pprint . pprint ( data ) \n        self . rpc_data . append ( data ) \n    sockobj . close ( ) "}
{"12301": "\nimport pprint \ndef get_remote_file_size ( self , url ) : \n    try : \n        req = urllib . request . urlopen ( url ) \n        return int ( req . getheader ( 'Content-Length' ) . strip ( ) ) \n    except urllib . error . HTTPError as error : \n        logger . error ( 'Error retrieving size of the remote file %s' % error ) \n        pprint . pprint ( 'Error retrieving size of the remote file %s' % error ) \n        self . connect_earthexplorer ( ) \n        self . get_remote_file_size ( url ) "}
{"12302": "\nimport pprint \ndef download ( self , bands = None , download_dir = None , metadata = False ) : \n    if not download_dir : \n        download_dir = DOWNLOAD_DIR \n    if bands is None : \n        bands = list ( range ( 1 , 12 ) ) + [ 'BQA' ] \n    else : \n        self . validate_bands ( bands ) \n    pattern = re . compile ( '^[^\\s]+_(.+)\\.tiff?' , re . I ) \n    band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] \n    image_list = [ ] \n    self . connect_earthexplorer ( ) \n    tgzname = self . sceneInfo . name + '.tgz' \n    dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) \n    downloaded = self . download_file ( self . url , dest_dir , tgzname ) \n    logger . debug ( 'Status downloaded %s' % downloaded ) \n    pprint . pprint ( '\\n Status downloaded %s' % downloaded ) \n    if downloaded [ 'sucess' ] : \n        pprint . pprint ( '\\n Downloaded sucess' ) \n        logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) \n        try : \n            tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) \n            folder_path = join ( download_dir , self . sceneInfo . name ) \n            tar . extractall ( folder_path ) \n            remove ( downloaded [ 'file_path' ] ) \n            images_path = listdir ( folder_path ) \n            for image_path in images_path : \n                matched = pattern . match ( image_path ) \n                file_path = join ( folder_path , image_path ) \n                if matched and matched . group ( 1 ) in band_list : \n                    image_list . append ( [ file_path , getsize ( file_path ) ] ) \n                elif matched : \n                    remove ( file_path ) \n        except tarfile . ReadError as error : \n            pprint . pprint ( '\\nError when extracting files. %s' % error ) \n            logger . error ( 'Error when extracting files. %s' % error ) \n        return image_list \n    else : \n        logger . debug ( 'Info downloaded: %s' % downloaded ) \n        pprint . pprint ( '\\n Info downloaded: %s' % downloaded ) \n        return downloaded "}
{"12304": "\nimport pprint \ndef connect_earthexplorer ( self ) : \n    logger . info ( \"Establishing connection to Earthexplorer\" ) \n    pprint . pprint ( \"\\n Establishing connection to Earthexplorer\" ) \n    try : \n        opener = urllib . request . build_opener ( urllib . request . HTTPCookieProcessor ( ) ) \n        urllib . request . install_opener ( opener ) \n        params = urllib . parse . urlencode ( dict ( username = self . user , password = self . password ) ) \n        params = params . encode ( 'utf-8' ) \n        f = opener . open ( \"https://ers.cr.usgs.gov/login\" , params ) \n        data = f . read ( ) . decode ( 'utf-8' ) \n        f . close ( ) \n        if data . find ( 'You must sign in as a registered user to download data or place orders for USGS EROS products' ) > 0 : \n            pprint . pprint ( \"\\n Authentification failed\" ) \n            logger . error ( \"Authentification failed\" ) \n            raise AutenticationUSGSFailed ( 'Authentification USGS failed' ) \n        pprint . pprint ( 'User %s connected with USGS' % self . user ) \n        logger . debug ( 'User %s connected with USGS' % self . user ) \n        return \n    except Exception as e : \n        pprint . pprint ( '\\nError when trying to connect USGS: %s' % e ) \n        raise logger . error ( 'Error when trying to connect USGS: %s' % e ) "}
{"12311": "\nimport pprint \ndef _dump_text ( self ) : \n    results = self . _relay_output [ 'result' ] ; \n    for l in results : \n        dt = time . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" , time . gmtime ( int ( l [ 1 ] [ 'ts' ] ) ) ) \n        pprint . pprint ( \"{0} {1} {2} {3}\" . format ( l [ 0 ] , dt , l [ 1 ] [ 'type' ] , l [ 1 ] [ 'msg' ] ) ) "}
{"12349": "\nimport pprint \ndef _handle_results ( self ) : \n    if self . _api_result . status_code != requests . codes . ok : \n        pprint . pprint ( self . colorize_json ( self . _api_result . text ) ) "}
{"12371": "\nimport pprint \ndef output_csv ( self , text ) : \n    payload = json . loads ( text ) \n    pprint . pprint ( \"{0},{1},{2},{3},{4}\" . format ( 'timestamp' , 'metric' , 'aggregate' , 'source' , 'value' ) ) \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) \n        for s in r [ 1 ] : \n            pprint . pprint ( '{0},\"{1}\",\"{2}\",\"{3}\",{4}' . format ( timestamp , metric_name , self . aggregate , s [ 0 ] , s [ 1 ] ) ) "}
{"12372": "\nimport pprint \ndef output_json ( self , text ) : \n    payload = json . loads ( text ) \n    data = [ ] \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) \n        for s in r [ 1 ] : \n            data . append ( { \"timestamp\" : timestamp , \"metric\" : metric_name , \"aggregate\" : self . aggregate , \"source\" : s [ 0 ] , \"value\" : s [ 1 ] , } ) \n    payload = { \"data\" : data } \n    out = json . dumps ( payload , indent = self . _indent , separators = ( ',' , ': ' ) ) \n    pprint . pprint ( self . colorize_json ( out ) ) "}
{"12373": "\nimport pprint \ndef output_raw ( self , text ) : \n    payload = json . loads ( text ) \n    out = json . dumps ( payload , sort_keys = True , indent = self . _indent , separators = ( ',' , ': ' ) ) \n    pprint . pprint ( self . colorize_json ( out ) ) "}
{"12374": "\nimport pprint \ndef output_xml ( self , text ) : \n    document = Element ( 'results' ) \n    comment = Comment ( 'Generated by TrueSight Pulse measurement-get CLI' ) \n    document . append ( comment ) \n    aggregates = SubElement ( document , 'aggregates' ) \n    aggregate = SubElement ( aggregates , 'aggregate' ) \n    measurements = SubElement ( aggregate , 'measurements' ) \n    payload = json . loads ( text ) \n    metric_name = self . _metric_name \n    for r in payload [ 'result' ] [ 'aggregates' ] [ 'key' ] : \n        timestamp = self . _format_timestamp ( r [ 0 ] [ 0 ] ) \n        for s in r [ 1 ] : \n            measure_node = SubElement ( measurements , 'measure' ) \n            source = s [ 0 ] \n            value = str ( s [ 1 ] ) \n            ts_node = SubElement ( measure_node , 'timestamp' ) \n            ts_node . text = str ( timestamp ) \n            metric_node = SubElement ( measure_node , 'metric' ) \n            metric_node . text = metric_name \n            metric_node = SubElement ( measure_node , 'aggregate' ) \n            metric_node . text = self . aggregate \n            source_node = SubElement ( measure_node , 'source' ) \n            source_node . text = source \n            value_node = SubElement ( measure_node , 'value' ) \n            value_node . text = value \n    rough_string = ElementTree . tostring ( document , 'utf-8' ) \n    reparse = minidom . parseString ( rough_string ) \n    output = reparse . toprettyxml ( indent = \" \" ) \n    pprint . pprint ( self . colorize_xml ( output ) ) "}
{"12376": "\nimport pprint \ndef pprint ( root , depth = 0 , space_unit = \"    \" , * , source_len = 0 , file = None ) : \n    spacing = space_unit * depth \n    if isinstance ( root , str ) : \n        pprint . pprint ( \"{0}terminal@(?): {1}\" . format ( spacing , root ) , file = file ) \n    else : \n        if root . position is None : \n            position = - 1 \n        elif root . position < 0 : \n            position = source_len + root . position \n        else : \n            position = root . position \n        if root . is_value : \n            pprint . pprint ( \"{0}{1}@({2}:{3}):\\t{4}\" . format ( spacing , root . node_type , position , root . consumed , root . svalue ) , file = file ) \n        else : \n            pprint . pprint ( \"{0}{1}@({2}:{3}):\" . format ( spacing , root . node_type , position , root . consumed ) , file = file ) \n            for child in root . children : \n                pprint ( child , depth + 1 , source_len = source_len , file = file ) "}
{"12498": "\nimport pprint \ndef dump_nodes ( self ) : \n    pprint . pprint ( \"DUMP NODE LOCAL INFOS\" ) \n    try : \n        pprint . pprint ( \"map Id->node name\" ) \n        for k , v in self . id_cache . items ( ) : \n            pprint . pprint ( \"[%d]=%s\" % ( k , v ) ) \n        pprint . pprint ( \"map tag->capture infos\" ) \n        for k , v in self . tag_cache . items ( ) : \n            pprint . pprint ( \"[%s]=%s\" % ( k , v ) ) \n        pprint . pprint ( \"map nodes->tag resolution\" ) \n        for k , v in self . rule_nodes . items ( ) : \n            txt = \"['%s']=%d\" % ( k , id ( v ) ) \n            if k in self . tag_cache : \n                tag = self . tag_cache [ k ] \n                txt += \" tag <%s>\" % tag \n                k = \"%d:%d\" % ( tag . _begin , tag . _end ) \n                if k in self . _stream . value_cache : \n                    txt += \" cache <%s>\" % self . _stream . value_cache [ k ] \n            pprint . pprint ( txt ) \n    except Exception as err : \n        pprint . pprint ( \"RECV Exception %s\" % err ) \n    import sys \n    sys . stdout . flush ( ) \n    return True "}
{"12516": "\nimport pprint \ndef echo_nodes ( self , * rest ) : \n    txt = \"\" \n    for thing in rest : \n        if isinstance ( thing , Node ) : \n            txt += self . value ( thing ) \n        else : \n            txt += str ( thing ) \n    pprint . pprint ( txt ) \n    return True "}
{"12525": "\nimport pprint \ndef set_node ( self , dst , src ) : \n    if not isinstance ( src , Node ) : \n        dst . value = src \n    else : \n        dst . set ( src ) \n        idsrc = id ( src ) \n        iddst = id ( dst ) \n        if iddst not in self . id_cache : \n            pprint . pprint ( \"DST: %s\" % repr ( dst ) ) \n            pprint . pprint ( \"RULE_NODES %s\" % repr ( self . rule_nodes ) ) \n            pprint . pprint ( \"IDCACHE %s\" % repr ( self . id_cache ) ) \n        if idsrc in self . id_cache : \n            k = self . id_cache [ idsrc ] \n            k2 = self . id_cache [ iddst ] \n            if k in self . rule_nodes : \n                self . tag_cache [ k2 ] = self . tag_cache [ k ] \n    return True "}
{"12644": "\nimport pprint \ndef loadModelData ( self , name ) : \n    path = self . resourceNameToPath ( name , \".json\" ) \n    try : \n        data = json . load ( open ( path , \"r\" ) ) \n    except Exception : \n        pprint . pprint ( \"Exception during model load: \" ) \n        import traceback ; \n        traceback . print_exc ( ) \n        return { } \n    out = { } \n    if data . get ( \"version\" , 1 ) == 1 : \n        out [ \"materials\" ] = { } \n        for name , matdata in data . get ( \"materials\" , { } ) . items ( ) : \n            m = model . Material ( self , name , matdata ) \n            out [ \"materials\" ] [ name ] = m \n        out [ \"default_material\" ] = out [ \"materials\" ] [ data . get ( \"default_material\" , list ( out [ \"materials\" ] . keys ( ) ) [ 0 ] ) ] \n        out [ \"bones\" ] = { \"__root__\" : model . RootBone ( self , \"__root__\" , { \"start_rot\" : [ 0 , 0 ] , \"length\" : 0 } ) } \n        for name , bonedata in data . get ( \"bones\" , { } ) . items ( ) : \n            b = model . Bone ( self , name , bonedata ) \n            out [ \"bones\" ] [ name ] = b \n        for name , bone in out [ \"bones\" ] . items ( ) : \n            if name == \"__root__\" : \n                continue \n            bone . setParent ( out [ \"bones\" ] [ bone . bonedata [ \"parent\" ] ] ) \n        out [ \"regions\" ] = { } \n        for name , regdata in data . get ( \"regions\" , { } ) . items ( ) : \n            r = model . Region ( self , name , regdata ) \n            r . material = out [ \"materials\" ] [ regdata . get ( \"material\" , out [ \"default_material\" ] ) ] \n            r . bone = out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] \n            out [ \"bones\" ] [ regdata . get ( \"bone\" , \"__root__\" ) ] . addRegion ( r ) \n            out [ \"regions\" ] [ name ] = r \n        out [ \"animations\" ] = { } \n        out [ \"animations\" ] [ \"static\" ] = model . Animation ( self , \"static\" , { \"type\" : \"static\" , \"bones\" : { } } ) \n        for name , anidata in data . get ( \"animations\" , { } ) . items ( ) : \n            a = model . Animation ( self , name , anidata ) \n            a . setBones ( out [ \"bones\" ] ) \n            out [ \"animations\" ] [ name ] = a \n        out [ \"default_animation\" ] = out [ \"animations\" ] [ data . get ( \"default_animation\" , out [ \"animations\" ] [ \"static\" ] ) ] \n    else : \n        raise ValueError ( \"Unknown version %s of model '%s'\" % ( data . get ( \"version\" , 1 ) , name ) ) \n    self . modelcache [ name ] = out \n    return out "}
{"12667": "\nimport pprint \ndef check_elements ( self ) : \n    existing_types = set ( self . elements . type . argiope . values . flatten ( ) ) \n    allowed_types = set ( ELEMENTS . keys ( ) ) \n    if ( existing_types <= allowed_types ) == False : \n        raise ValueError ( \"Element types {0} not in know elements {1}\" . format ( existing_types - allowed_types , allowed_types ) ) \n    pprint . pprint ( \"<Elements: OK>\" ) "}
{"12679": "\nimport pprint \ndef run_postproc ( self ) : \n    t0 = time . time ( ) \n    if self . verbose : \n        pprint . pprint ( '####\u00a0POST-PROCESSING \"{0}\" USING POST-PROCESSOR \"{1}\"' . format ( self . label , self . solver . upper ( ) ) ) \n    if self . solver == \"abaqus\" : \n        command = '{0} viewer noGUI={1}_abqpp.py' . format ( self . solver_path , self . label ) \n        process = subprocess . Popen ( command , cwd = self . workdir , shell = True , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n        for line in iter ( process . stdout . readline , b'' ) : \n            line = line . rstrip ( ) . decode ( 'utf8' ) \n            pprint . pprint ( \"    \" , line ) \n    t1 = time . time ( ) \n    if self . verbose : \n        pprint . pprint ( '  => POST-PROCESSED {0}: DURATION = {1:.2f}s >' . format ( self . label , t1 - t0 ) ) "}
{"12763": "\nimport pprint \ndef make_app ( ) : \n    env = Environment ( ) \n    args = parser . parse_args ( args = [ '/' , '--ignore-stdin' ] , env = env ) \n    args . output_options = 'HB' \n    server = 'HTTPony/{0}' . format ( __version__ ) \n    def application ( environ , start_response ) : \n        if environ . get ( 'CONTENT_LENGTH' ) == '' : \n            del environ [ 'CONTENT_LENGTH' ] \n        if environ . get ( 'CONTENT_TYPE' ) == '' : \n            del environ [ 'CONTENT_TYPE' ] \n        wrequest = WerkzeugRequest ( environ ) \n        data = wrequest . get_data ( ) \n        request = Request ( method = wrequest . method , url = wrequest . url , headers = wrequest . headers , data = data , ) \n        prepared = request . prepare ( ) \n        stream = streams . build_output_stream ( args , env , prepared , response = None , output_options = args . output_options ) \n        streams . write_stream ( stream , env . stdout , env . stdout_isatty ) \n        if data : \n            pprint . pprint ( \"\\n\" , file = env . stdout ) \n        response = Response ( headers = { 'Server' : server } ) \n        return response ( environ , start_response ) \n    return application "}
{"12810": "\nimport pprint \ndef main ( argv = sys . argv ) : \n    args = parse ( argv ) \n    hostname = args . listen \n    port = args . port \n    pprint . pprint ( \"Making all your dreams for a pony come true on http://{0}:{1}.\\n\" \"Press Ctrl+C to quit.\\n\" . format ( hostname , port ) ) \n    logging . getLogger ( 'werkzeug' ) . setLevel ( logging . CRITICAL ) \n    plugin_manager . load_installed_plugins ( ) \n    app = make_app ( ) \n    run_simple ( hostname , port , app ) "}
{"12932": "\nimport pprint \ndef update_file ( url , filename ) : \n    resp = urlopen ( url ) \n    if resp . code != 200 : \n        raise Exception ( 'GET {} failed.' . format ( url ) ) \n    with open ( _get_package_path ( filename ) , 'w' ) as fp : \n        for l in resp : \n            if not l . startswith ( b'#' ) : \n                fp . write ( l . decode ( 'utf8' ) ) \n    pprint . pprint ( 'Updated {}' . format ( filename ) ) "}
{"12994": "\nimport pprint \ndef _zsh_comp_command ( self , zcf , cmd , grouping , add_help = True ) : \n    if add_help : \n        if grouping : \n            pprint . pprint ( \"+ '(help)'\" , end = BLK , file = zcf ) \n        pprint . pprint ( \"'--help[show help message]'\" , end = BLK , file = zcf ) \n        pprint . pprint ( \"'-h[show help message]'\" , end = BLK , file = zcf ) \n    no_comp = ( 'store_true' , 'store_false' ) \n    cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare \n    for opt , sct in cmd_dict . items ( ) : \n        meta = self . _conf [ sct ] . def_ [ opt ] \n        if meta . cmd_kwargs . get ( 'action' ) == 'append' : \n            grpfmt , optfmt = \"+ '{}'\" , \"'*{}[{}]{}'\" \n            if meta . comprule is None : \n                meta . comprule = '' \n        else : \n            grpfmt , optfmt = \"+ '({})'\" , \"'{}[{}]{}'\" \n        if meta . cmd_kwargs . get ( 'action' ) in no_comp or meta . cmd_kwargs . get ( 'nargs' ) == 0 : \n            meta . comprule = None \n        if meta . comprule is None : \n            compstr = '' \n        elif meta . comprule == '' : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :( )' \n        else : \n            optfmt = optfmt . split ( '[' ) \n            optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] \n            compstr = ': :{}' . format ( meta . comprule ) \n        if grouping : \n            pprint . pprint ( grpfmt . format ( opt ) , end = BLK , file = zcf ) \n        for name in _names ( self . _conf [ sct ] , opt ) : \n            pprint . pprint ( optfmt . format ( name , meta . help . replace ( \"'\" , \"'\\\"'\\\"'\" ) , compstr ) , end = BLK , file = zcf ) "}
{"12995": "\nimport pprint \ndef zsh_complete ( self , path , cmd , * cmds , sourceable = False ) : \n    grouping = internal . zsh_version ( ) >= ( 5 , 4 ) \n    path = pathlib . Path ( path ) \n    firstline = [ '#compdef' , cmd ] \n    firstline . extend ( cmds ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as zcf : \n        pprint . pprint ( * firstline , end = '\\n\\n' , file = zcf ) \n        pprint . pprint ( 'function _{} {{' . format ( cmd ) , file = zcf ) \n        pprint . pprint ( 'local line' , file = zcf ) \n        pprint . pprint ( '_arguments -C' , end = BLK , file = zcf ) \n        if subcmds : \n            substrs = [ \"{}\\\\:'{}'\" . format ( sub , self . subcmds [ sub ] . help ) for sub in subcmds ] \n            pprint . pprint ( '\"1:Commands:(({}))\"' . format ( ' ' . join ( substrs ) ) , end = BLK , file = zcf ) \n        self . _zsh_comp_command ( zcf , None , grouping ) \n        if subcmds : \n            pprint . pprint ( \"'*::arg:->args'\" , file = zcf ) \n            pprint . pprint ( 'case $line[1] in' , file = zcf ) \n            for sub in subcmds : \n                pprint . pprint ( '{sub}) _{cmd}_{sub} ;;' . format ( sub = sub , cmd = cmd ) , file = zcf ) \n            pprint . pprint ( 'esac' , file = zcf ) \n        pprint . pprint ( '}' , file = zcf ) \n        for sub in subcmds : \n            pprint . pprint ( '\\nfunction _{}_{} {{' . format ( cmd , sub ) , file = zcf ) \n            pprint . pprint ( '_arguments' , end = BLK , file = zcf ) \n            self . _zsh_comp_command ( zcf , sub , grouping ) \n            pprint . pprint ( '}' , file = zcf ) \n        if sourceable : \n            pprint . pprint ( '\\ncompdef _{0} {0}' . format ( cmd ) , * cmds , file = zcf ) "}
{"12997": "\nimport pprint \ndef bash_complete ( self , path , cmd , * cmds ) : \n    path = pathlib . Path ( path ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as bcf : \n        pprint . pprint ( '_{}() {{' . format ( cmd ) , file = bcf ) \n        pprint . pprint ( 'COMPREPLY=()' , file = bcf ) \n        pprint . pprint ( r'local cur=${COMP_WORDS[COMP_CWORD]}' , end = '\\n\\n' , file = bcf ) \n        optstr = ' ' . join ( self . _bash_comp_command ( None ) ) \n        pprint . pprint ( r'local options=\"{}\"' . format ( optstr ) , end = '\\n\\n' , file = bcf ) \n        if subcmds : \n            pprint . pprint ( 'local commands=\"{}\"' . format ( ' ' . join ( subcmds ) ) , file = bcf ) \n            pprint . pprint ( 'declare -A suboptions' , file = bcf ) \n        for sub in subcmds : \n            optstr = ' ' . join ( self . _bash_comp_command ( sub ) ) \n            pprint . pprint ( 'suboptions[{}]=\"{}\"' . format ( sub , optstr ) , file = bcf ) \n        condstr = 'if' \n        for sub in subcmds : \n            pprint . pprint ( condstr , r'[[ \"${COMP_LINE}\" == *\"' , sub , '\"* ]] ; then' , file = bcf ) \n            pprint . pprint ( r'COMPREPLY=( `compgen -W \"${suboptions[' , sub , r']}\" -- ${cur}` )' , sep = '' , file = bcf ) \n            condstr = 'elif' \n        pprint . pprint ( condstr , r'[[ ${cur} == -* ]] ; then' , file = bcf ) \n        pprint . pprint ( r'COMPREPLY=( `compgen -W \"${options}\" -- ${cur}`)' , file = bcf ) \n        if subcmds : \n            pprint . pprint ( r'else' , file = bcf ) \n            pprint . pprint ( r'COMPREPLY=( `compgen -W \"${commands}\" -- ${cur}`)' , file = bcf ) \n        pprint . pprint ( 'fi' , file = bcf ) \n        pprint . pprint ( '}' , end = '\\n\\n' , file = bcf ) \n        pprint . pprint ( 'complete -F _{0} {0}' . format ( cmd ) , * cmds , file = bcf ) "}
{"13027": "\nimport pprint \ndef get_host ( name ) : \n    f = { 'instance-state-name' : 'running' , 'tag:Name' : name } \n    ec2 = boto . connect_ec2 ( region = get_region ( ) ) \n    rs = ec2 . get_all_instances ( filters = f ) \n    if len ( rs ) == 0 : \n        raise Exception ( 'Host \"%s\" not found' % name ) \n    pprint . pprint ( rs [ 0 ] . instances [ 0 ] . public_dns_name ) "}
{"13054": "\nimport pprint \ndef get_input ( prompt , default = None , exit_msg = 'bye!' ) : \n    try : \n        response = six . moves . input ( prompt ) \n    except ( KeyboardInterrupt , EOFError ) : \n        pprint . pprint ( ) \n        pprint . pprint ( exit_msg ) \n        exit ( ) \n    try : \n        return int ( response ) \n    except ValueError : \n        if response . strip ( ) == \"\" and default is not None : \n            return default \n        else : \n            return response "}
{"13072": "\nimport pprint \ndef _copy_to ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = False , local_path = local_path , remote_path = remote_path ) \n        pprint . pprint ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    pprint . pprint ( green ( 'Finished copying' ) ) "}
{"13073": "\nimport pprint \ndef _copy_from ( entries , remote_path , local_path , profile ) : \n    commands = [ ] \n    paths = set ( ) \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        _local_path = entry . format_string ( local_path ) \n        if _local_path in paths : \n            raise ValueError ( 'Duplicate local paths: one or more paths ' 'had value {} after formatting.' . format ( local_path ) ) \n        paths . add ( _local_path ) \n        _folder = os . path . split ( _local_path ) [ 0 ] \n        if len ( _folder ) > 0 : \n            if not os . path . exists ( _folder ) : \n                pprint . pprint ( 'Creating directory ' + _folder ) \n                os . makedirs ( _folder ) \n        cmd = _build_scp_command ( hname , profile . username , profile . identity_file , is_get = True , local_path = _local_path , remote_path = remote_path ) \n        pprint . pprint ( 'Command:' , cmd ) \n        commands . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( commands ) \n    pprint . pprint ( green ( 'Finished copying' ) ) "}
{"13074": "\nimport pprint \ndef _run_ssh_command ( entries , username , idfile , command , tunnel , parallel = False ) : \n    if len ( entries ) == 0 : \n        pprint . pprint ( '(No hosts to run command on)' ) \n        return 1 \n    if command . strip ( ) == '' or command is None : \n        raise ValueError ( 'No command given' ) \n    pprint . pprint ( 'Running command {0} on {1} matching hosts' . format ( green ( repr ( command ) ) , len ( entries ) ) ) \n    shell_cmds = [ ] \n    for entry in entries : \n        hname = entry . hostname or entry . public_ip \n        cmd = _build_ssh_command ( hname , username , idfile , command , tunnel ) \n        shell_cmds . append ( { 'command' : cmd , 'description' : entry . display ( ) } ) \n    stream_commands ( shell_cmds , parallel = parallel ) \n    pprint . pprint ( green ( 'All commands finished' ) ) "}
{"13075": "\nimport pprint \ndef _connect_ssh ( entry , username , idfile , tunnel = None ) : \n    if entry . hostname != \"\" and entry . hostname is not None : \n        _host = entry . hostname \n    elif entry . public_ip != \"\" and entry . public_ip is not None : \n        _host = entry . public_ip \n    elif entry . private_ip != \"\" and entry . private_ip is not None : \n        if tunnel is None : \n            raise ValueError ( \"Entry does not have a hostname or public IP. \" \"You can connect via private IP if you use a \" \"tunnel.\" ) \n        _host = entry . private_ip \n    else : \n        raise ValueError ( \"No hostname, public IP or private IP information \" \"found on host entry. I don't know how to connect.\" ) \n    command = _build_ssh_command ( _host , username , idfile , None , tunnel ) \n    pprint . pprint ( 'Connecting to %s...' % cyan ( entry . display ( ) ) ) \n    pprint . pprint ( 'SSH command: %s' % green ( command ) ) \n    proc = subprocess . Popen ( command , shell = True ) \n    return proc . wait ( ) "}
{"13120": "\nimport pprint \ndef pack_dir_cmd ( ) : \n    parser = argparse . ArgumentParser ( description = inspect . getdoc ( part_edit_cmd ) ) \n    parser . add_argument ( 'path' , help = ( 'Path to list (including path to zip file, ' 'i.e. ./file.zipx or ./file.zipx/subdir)' ) , ) \n    args = parser . parse_args ( ) \n    for item , is_file in sorted ( list_contents ( args . path ) ) : \n        prefix = 'd ' if not is_file else '  ' \n        msg = prefix + item \n        pprint . pprint ( msg ) "}
{"13253": "\nimport pprint \ndef clock_on_right ( mystring ) : \n    taken = length_no_ansi ( mystring ) \n    padding = ( get_terminal_size ( ) . columns - 1 ) - taken - 5 \n    clock = time . strftime ( \"%I:%M\" , time . localtime ( ) ) \n    pprint . pprint ( mystring + \" \" * padding + clock ) "}
{"13261": "\nimport pprint \ndef pretty_print ( input_word , anagrams , by_length = False ) : \n    scores = { } \n    if by_length : \n        noun = \"tiles\" \n        for word , score in anagrams : \n            try : \n                scores [ len ( word ) ] . append ( \"{0} ({1:d})\" . format ( word , score ) ) \n            except KeyError : \n                scores [ len ( word ) ] = [ \"{0} ({1:d})\" . format ( word , score ) ] \n    else : \n        noun = \"points\" \n        for word , score in anagrams : \n            try : \n                scores [ score ] . append ( word ) \n            except KeyError : \n                scores [ score ] = [ word ] \n    pprint . pprint ( \"Anagrams for {0}{1}:\" . format ( input_word , \" (score)\" * by_length ) ) \n    if not valid_scrabble_word ( input_word ) : \n        pprint . pprint ( \"{0} is not possible in Scrabble.\" . format ( input_word ) ) \n    for key , value in sorted ( scores . items ( ) , reverse = True ) : \n        pprint . pprint ( \"{0:d} {1}: {2}\" . format ( key , noun , \", \" . join ( value ) ) ) "}
{"13286": "\nimport pprint \ndef main ( clargs = None ) : \n    from argparse import ArgumentParser \n    from librarian . library import Library \n    import sys \n    parser = ArgumentParser ( description = \"A test runner for each card in a librarian library.\" ) \n    parser . add_argument ( \"library\" , help = \"Library database\" ) \n    parser . add_argument ( \"-t\" , \"--tests\" , default = \"test/\" , help = \"Test directory\" ) \n    args = parser . parse_args ( clargs ) \n    descovery ( args . tests ) \n    library = Library ( args . library ) \n    cardcount , passes , failures = execute_tests ( library ) \n    pprint . pprint ( RESULTS . format ( len ( SINGLES ) , len ( TESTS ) , cardcount , passes , failures ) ) \n    sys . exit ( failures ) "}
{"13309": "\nimport pprint \ndef serve ( self , server = None ) : \n    if server is None : \n        from wsgiref . simple_server import make_server \n        server = lambda app : make_server ( '' , 8000 , app ) . serve_forever ( ) \n        pprint . pprint ( 'Listening on 0.0.0.0:8000' ) \n    try : \n        server ( self ) \n    finally : \n        server . socket . close ( ) "}
{"13334": "\nimport pprint \ndef run ( self , args_list = None ) : \n    self . log . debug ( \"Application.run: {args_list}\" . format ( ** locals ( ) ) ) \n    retval = None \n    try : \n        retval = self . _run ( args_list = args_list ) \n    except KeyboardInterrupt : \n        self . log . verbose ( \"Interrupted\" ) \n    except SystemExit as exit : \n        self . log . verbose ( \"Exited\" ) \n        retval = exit . code \n    except Exception : \n        pprint . pprint ( \"Uncaught exception\" , file = sys . stderr ) \n        traceback . print_exc ( ) \n        if \"debug_pdb\" in self . args and self . args . debug_pdb : \n            debugger ( ) \n        retval = Application . UNCAUGHT_EXCEPTION_EXIT \n        raise \n    finally : \n        try : \n            self . _atexit ( ) \n        finally : \n            sys . stderr . flush ( ) \n            sys . stdout . flush ( ) \n            sys . exit ( retval ) "}
{"13396": "\nimport pprint \ndef _get_index_mappings ( self ) : \n    maps = { } \n    for fname in self . indexed_features : \n        config = self . indexes . get ( fname , { } ) \n        pprint . pprint ( fname , config ) \n        maps [ fname_to_idx_name ( fname ) ] = { 'type' : config . get ( 'es_index_type' , 'integer' ) , 'store' : False , 'index' : 'not_analyzed' , } \n    for fname in self . fulltext_indexed_features : \n        maps [ fname_to_full_idx_name ( fname ) ] = { 'type' : 'string' , 'store' : False , 'index' : 'analyzed' , } \n    return maps "}
{"13466": "\nimport pprint \ndef search ( self , query , verbose = 0 ) : \n    if verbose > 0 : \n        pprint . pprint ( \"searching \" + query ) \n    query = query . lower ( ) \n    qgram = ng ( query , self . slb ) \n    qocument = set ( ) \n    for q in qgram : \n        if q in self . ngrams . keys ( ) : \n            for i in self . ngrams [ q ] : \n                qocument . add ( i ) \n    self . qocument = qocument \n    results = { } \n    for i in qocument : \n        for j in self . D [ i ] . keys ( ) : \n            if not j in results . keys ( ) : \n                results [ j ] = 0 \n            results [ j ] = results [ j ] + self . D [ i ] [ j ] \n    sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n    return [ self . elements [ f [ 0 ] ] for f in sorted_results ] "}
{"13480": "\nimport pprint \ndef dead_code ( ) : \n    with safe_cd ( SRC ) : \n        if IS_TRAVIS : \n            command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) \n        else : \n            command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) \n        output_file_name = \"dead_code.txt\" \n        with open ( output_file_name , \"w\" ) as outfile : \n            env = config_pythonpath ( ) \n            subprocess . call ( command , stdout = outfile , env = env ) \n        cutoff = 20 \n        num_lines = sum ( 1 for line in open ( output_file_name ) if line ) \n        if num_lines > cutoff : \n            pprint . pprint ( \"Too many lines of dead code : {0}, max {1}\" . format ( num_lines , cutoff ) ) \n            exit ( - 1 ) "}
{"13495": "\nimport pprint \ndef create_inputhook_qt4 ( mgr , app = None ) : \n    if app is None : \n        app = QtCore . QCoreApplication . instance ( ) \n        if app is None : \n            app = QtGui . QApplication ( [ \" \" ] ) \n    ip = InteractiveShell . instance ( ) \n    if hasattr ( ip , '_inputhook_qt4' ) : \n        return app , ip . _inputhook_qt4 \n    got_kbdint = [ False ] \n    def inputhook_qt4 ( ) : \n        try : \n            allow_CTRL_C ( ) \n            app = QtCore . QCoreApplication . instance ( ) \n            if not app : \n                return 0 \n            app . processEvents ( QtCore . QEventLoop . AllEvents , 300 ) \n            if not stdin_ready ( ) : \n                timer = QtCore . QTimer ( ) \n                timer . timeout . connect ( app . quit ) \n                while not stdin_ready ( ) : \n                    timer . start ( 50 ) \n                    app . exec_ ( ) \n                    timer . stop ( ) \n        except KeyboardInterrupt : \n            ignore_CTRL_C ( ) \n            got_kbdint [ 0 ] = True \n            pprint . pprint ( \"\\nKeyboardInterrupt - Ctrl-C again for new prompt\" ) \n            mgr . clear_inputhook ( ) \n        except : \n            ignore_CTRL_C ( ) \n            from traceback import print_exc \n            print_exc ( ) \n            pprint . pprint ( \"Got exception from inputhook_qt4, unregistering.\" ) \n            mgr . clear_inputhook ( ) \n        finally : \n            allow_CTRL_C ( ) \n        return 0 \n    def preprompthook_qt4 ( ishell ) : \n        if got_kbdint [ 0 ] : \n            mgr . set_inputhook ( inputhook_qt4 ) \n        got_kbdint [ 0 ] = False \n    ip . _inputhook_qt4 = inputhook_qt4 \n    ip . set_hook ( 'pre_prompt_hook' , preprompthook_qt4 ) \n    return app , inputhook_qt4 "}
{"13512": "\nimport pprint \ndef OnTimeToClose ( self , evt ) : \n    pprint . pprint ( \"See ya later!\" ) \n    sys . stdout . flush ( ) \n    self . cleanup_consoles ( evt ) \n    self . Close ( ) \n    sys . exit ( ) "}
{"13543": "\nimport pprint \ndef _stdin_raw_nonblock ( self ) : \n    handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) \n    result = WaitForSingleObject ( handle , 100 ) \n    if result == WAIT_FAILED : \n        raise ctypes . WinError ( ) \n    elif result == WAIT_TIMEOUT : \n        pprint . pprint ( \".\" , end = '' ) \n        return None \n    else : \n        data = ctypes . create_string_buffer ( 256 ) \n        bytesRead = DWORD ( 0 ) \n        pprint . pprint ( '?' , end = '' ) \n        if not ReadFile ( handle , data , 256 , ctypes . byref ( bytesRead ) , None ) : \n            raise ctypes . WinError ( ) \n        FlushConsoleInputBuffer ( handle ) \n        data = data . value \n        data = data . replace ( '\\r\\n' , '\\n' ) \n        data = data . replace ( '\\r' , '\\n' ) \n        pprint . pprint ( repr ( data ) + \" \" , end = '' ) \n        return data "}
{"13552": "\nimport pprint \ndef passwd ( passphrase = None , algorithm = 'sha1' ) : \n    if passphrase is None : \n        for i in range ( 3 ) : \n            p0 = getpass . getpass ( 'Enter password: ' ) \n            p1 = getpass . getpass ( 'Verify password: ' ) \n            if p0 == p1 : \n                passphrase = p0 \n                break \n            else : \n                pprint . pprint ( 'Passwords do not match.' ) \n        else : \n            raise UsageError ( 'No matching passwords found. Giving up.' ) \n    h = hashlib . new ( algorithm ) \n    salt = ( '%0' + str ( salt_len ) + 'x' ) % random . getrandbits ( 4 * salt_len ) \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return ':' . join ( ( algorithm , salt , h . hexdigest ( ) ) ) "}
{"13569": "\nimport pprint \ndef handle_iopub ( self ) : \n    while self . km . sub_channel . msg_ready ( ) : \n        sub_msg = self . km . sub_channel . get_msg ( ) \n        msg_type = sub_msg [ 'header' ] [ 'msg_type' ] \n        parent = sub_msg [ \"parent_header\" ] \n        if ( not parent ) or self . session_id == parent [ 'session' ] : \n            if msg_type == 'status' : \n                if sub_msg [ \"content\" ] [ \"execution_state\" ] == \"busy\" : \n                    pass \n            elif msg_type == 'stream' : \n                if sub_msg [ \"content\" ] [ \"name\" ] == \"stdout\" : \n                    pprint . pprint ( sub_msg [ \"content\" ] [ \"data\" ] , file = io . stdout , end = \"\" ) \n                    io . stdout . flush ( ) \n                elif sub_msg [ \"content\" ] [ \"name\" ] == \"stderr\" : \n                    pprint . pprint ( sub_msg [ \"content\" ] [ \"data\" ] , file = io . stderr , end = \"\" ) \n                    io . stderr . flush ( ) \n            elif msg_type == 'pyout' : \n                self . execution_count = int ( sub_msg [ \"content\" ] [ \"execution_count\" ] ) \n                format_dict = sub_msg [ \"content\" ] [ \"data\" ] \n                hook = self . displayhook \n                hook . start_displayhook ( ) \n                hook . write_output_prompt ( ) \n                hook . write_format_data ( format_dict ) \n                hook . log_output ( format_dict ) \n                hook . finish_displayhook ( ) "}
{"13638": "\nimport pprint \ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    try : \n        start = time . clock ( ) \n        status = CoverageScript ( ) . command_line ( argv ) \n        end = time . clock ( ) \n        if 0 : \n            pprint . pprint ( \"time: %.3fs\" % ( end - start ) ) \n    except ExceptionDuringRun : \n        _ , err , _ = sys . exc_info ( ) \n        traceback . print_exception ( * err . args ) \n        status = ERR \n    except CoverageException : \n        _ , err , _ = sys . exc_info ( ) \n        pprint . pprint ( err ) \n        status = ERR \n    except SystemExit : \n        _ , err , _ = sys . exc_info ( ) \n        if err . args : \n            status = err . args [ 0 ] \n        else : \n            status = None \n    return status "}
{"13642": "\nimport pprint \ndef help ( self , error = None , topic = None , parser = None ) : \n    assert error or topic or parser \n    if error : \n        pprint . pprint ( error ) \n        pprint . pprint ( \"Use 'coverage help' for help.\" ) \n    elif parser : \n        pprint . pprint ( parser . format_help ( ) . strip ( ) ) \n    else : \n        help_msg = HELP_TOPICS . get ( topic , '' ) . strip ( ) \n        if help_msg : \n            pprint . pprint ( help_msg % self . covpkg . __dict__ ) \n        else : \n            pprint . pprint ( \"Don't know topic %r\" % topic ) "}
{"13646": "\nimport pprint \ndef do_debug ( self , args ) : \n    if not args : \n        self . help_fn ( \"What information would you like: data, sys?\" ) \n        return ERR \n    for info in args : \n        if info == 'sys' : \n            pprint . pprint ( \"-- sys ----------------------------------------\" ) \n            for line in info_formatter ( self . coverage . sysinfo ( ) ) : \n                pprint . pprint ( \" %s\" % line ) \n        elif info == 'data' : \n            pprint . pprint ( \"-- data ---------------------------------------\" ) \n            self . coverage . load ( ) \n            pprint . pprint ( \"path: %s\" % self . coverage . data . filename ) \n            pprint . pprint ( \"has_arcs: %r\" % self . coverage . data . has_arcs ( ) ) \n            summary = self . coverage . data . summary ( fullpath = True ) \n            if summary : \n                filenames = sorted ( summary . keys ( ) ) \n                pprint . pprint ( \"\\n%d files:\" % len ( filenames ) ) \n                for f in filenames : \n                    pprint . pprint ( \"%s: %d lines\" % ( f , summary [ f ] ) ) \n            else : \n                pprint . pprint ( \"No data collected\" ) \n        else : \n            self . help_fn ( \"Don't know what you mean by %r\" % info ) \n            return ERR \n    return OK "}
{"13671": "\nimport pprint \ndef push ( self , variables , interactive = True ) : \n    vdict = None \n    if isinstance ( variables , dict ) : \n        vdict = variables \n    elif isinstance ( variables , ( basestring , list , tuple ) ) : \n        if isinstance ( variables , basestring ) : \n            vlist = variables . split ( ) \n        else : \n            vlist = variables \n        vdict = { } \n        cf = sys . _getframe ( 1 ) \n        for name in vlist : \n            try : \n                vdict [ name ] = eval ( name , cf . f_globals , cf . f_locals ) \n            except : \n                pprint . pprint ( 'Could not get variable %s from %s' % ( name , cf . f_code . co_name ) ) \n    else : \n        raise ValueError ( 'variables must be a dict/str/list/tuple' ) \n    self . user_ns . update ( vdict ) \n    user_ns_hidden = self . user_ns_hidden \n    if interactive : \n        user_ns_hidden . difference_update ( vdict ) \n    else : \n        user_ns_hidden . update ( vdict ) "}
{"13724": "\nimport pprint \ndef emit ( self , msg , level = 1 , debug = False ) : \n    if debug : \n        if not self . debug : \n            return \n        stream = sys . stderr \n    else : \n        if self . verbose < level : \n            return \n        stream = sys . stdout \n    pprint . pprint ( msg , file = stream ) \n    stream . flush ( ) "}
{"13777": "\nimport pprint \ndef _get_build_prefix ( ) : \n    path = os . path . join ( tempfile . gettempdir ( ) , 'pip_build_%s' % __get_username ( ) . replace ( ' ' , '_' ) ) \n    if WINDOWS : \n        return path \n    try : \n        os . mkdir ( path ) \n        write_delete_marker_file ( path ) \n    except OSError : \n        file_uid = None \n        try : \n            file_uid = get_path_uid ( path ) \n        except OSError : \n            file_uid = None \n        if file_uid != os . geteuid ( ) : \n            msg = ( \"The temporary folder for building (%s) is either not owned by\" \" you, or is a symlink.\" % path ) \n            pprint . pprint ( msg ) \n            pprint . pprint ( \"pip will not work until the temporary folder is either \" \"deleted or is a real directory owned by your user account.\" ) \n            raise exceptions . InstallationError ( msg ) \n    return path "}
{"13785": "\nimport pprint \ndef sleep_here ( count , t ) : \n    import time , sys \n    pprint . pprint ( \"hi from engine %i\" % id ) \n    sys . stdout . flush ( ) \n    time . sleep ( t ) \n    return count , t "}
{"13788": "\nimport pprint \ndef main ( connection_file ) : \n    ctx = zmq . Context . instance ( ) \n    with open ( connection_file ) as f : \n        cfg = json . loads ( f . read ( ) ) \n    location = cfg [ 'location' ] \n    reg_url = cfg [ 'url' ] \n    session = Session ( key = str_to_bytes ( cfg [ 'exec_key' ] ) ) \n    query = ctx . socket ( zmq . DEALER ) \n    query . connect ( disambiguate_url ( cfg [ 'url' ] , location ) ) \n    session . send ( query , \"connection_request\" ) \n    idents , msg = session . recv ( query , mode = 0 ) \n    c = msg [ 'content' ] \n    iopub_url = disambiguate_url ( c [ 'iopub' ] , location ) \n    sub = ctx . socket ( zmq . SUB ) \n    sub . setsockopt ( zmq . SUBSCRIBE , b'' ) \n    sub . connect ( iopub_url ) \n    while True : \n        try : \n            idents , msg = session . recv ( sub , mode = 0 ) \n        except KeyboardInterrupt : \n            return \n        topic = idents [ 0 ] \n        if msg [ 'msg_type' ] == 'stream' : \n            pprint . pprint ( \"%s: %s\" % ( topic , msg [ 'content' ] [ 'data' ] ) ) \n        elif msg [ 'msg_type' ] == 'pyerr' : \n            c = msg [ 'content' ] \n            pprint . pprint ( topic + ':' ) \n            for line in c [ 'traceback' ] : \n                pprint . pprint ( '    ' + line ) "}
{"13819": "\nimport pprint \ndef process_handler ( cmd , callback , stderr = subprocess . PIPE ) : \n    sys . stdout . flush ( ) \n    sys . stderr . flush ( ) \n    close_fds = sys . platform != 'win32' \n    p = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = stderr , close_fds = close_fds ) \n    try : \n        out = callback ( p ) \n    except KeyboardInterrupt : \n        pprint . pprint ( '^C' ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        out = None \n    finally : \n        if p . returncode is None : \n            try : \n                p . terminate ( ) \n                p . poll ( ) \n            except OSError : \n                pass \n        if p . returncode is None : \n            try : \n                p . kill ( ) \n            except OSError : \n                pass \n    return out "}
{"13864": "\nimport pprint \ndef create_zipfile ( context ) : \n    if not prerequisites_ok ( ) : \n        return \n    subprocess . call ( [ 'make' , 'zip' ] ) \n    for zipfile in glob . glob ( '*.zip' ) : \n        first_part = zipfile . split ( '.' ) [ 0 ] \n        new_name = \"%s.%s.zip\" % ( first_part , context [ 'version' ] ) \n        target = os . path . join ( context [ 'workingdir' ] , new_name ) \n        shutil . copy ( zipfile , target ) \n        pprint . pprint ( \"Copied %s to %s\" % ( zipfile , target ) ) "}
{"13917": "\nimport pprint \ndef print_wordfreq ( freqs , n = 10 ) : \n    words , counts = freqs . keys ( ) , freqs . values ( ) \n    items = zip ( counts , words ) \n    items . sort ( reverse = True ) \n    for ( count , word ) in items [ : n ] : \n        pprint . pprint ( word , count ) "}
{"14050": "\nimport pprint \ndef clear_output ( self , stdout = True , stderr = True , other = True ) : \n    if stdout : \n        pprint . pprint ( '\\033[2K\\r' , file = io . stdout , end = '' ) \n        io . stdout . flush ( ) \n    if stderr : \n        pprint . pprint ( '\\033[2K\\r' , file = io . stderr , end = '' ) \n        io . stderr . flush ( ) "}
{"14062": "\nimport pprint \ndef wait_interactive ( self , interval = 1. , timeout = None ) : \n    N = len ( self ) \n    tic = time . time ( ) \n    while not self . ready ( ) and ( timeout is None or time . time ( ) - tic <= timeout ) : \n        self . wait ( interval ) \n        clear_output ( ) \n        pprint . pprint ( \"%4i/%i tasks finished after %4i s\" % ( self . progress , N , self . elapsed ) , end = \"\" ) \n        sys . stdout . flush ( ) \n    pprint . pprint ( ) \n    pprint . pprint ( \"done\" ) "}
{"14183": "\nimport pprint \ndef debug ( self , level , message ) : \n    if self . _debug >= level : \n        pprint . pprint ( message , file = sys . stderr ) "}
{"14227": "\nimport pprint \ndef _handle_execute_reply ( self , msg ) : \n    parent = msg [ 'parent_header' ] \n    msg_id = parent [ 'msg_id' ] \n    if msg_id not in self . outstanding : \n        if msg_id in self . history : \n            pprint . pprint ( \"got stale result: %s\" % msg_id ) \n        else : \n            pprint . pprint ( \"got unknown result: %s\" % msg_id ) \n    else : \n        self . outstanding . remove ( msg_id ) \n    content = msg [ 'content' ] \n    header = msg [ 'header' ] \n    md = self . metadata [ msg_id ] \n    md . update ( self . _extract_metadata ( header , parent , content ) ) \n    self . metadata [ msg_id ] = md \n    e_outstanding = self . _outstanding_dict [ md [ 'engine_uuid' ] ] \n    if msg_id in e_outstanding : \n        e_outstanding . remove ( msg_id ) \n    if content [ 'status' ] == 'ok' : \n        self . results [ msg_id ] = ExecuteReply ( msg_id , content , md ) \n    elif content [ 'status' ] == 'aborted' : \n        self . results [ msg_id ] = error . TaskAborted ( msg_id ) \n    elif content [ 'status' ] == 'resubmitted' : \n        pass \n    else : \n        self . results [ msg_id ] = self . _unwrap_exception ( content ) "}
{"14247": "\nimport pprint \ndef _raw_parse ( self ) : \n    if self . exclude : \n        self . excluded = self . lines_matching ( self . exclude ) \n    indent = 0 \n    exclude_indent = 0 \n    excluding = False \n    prev_toktype = token . INDENT \n    first_line = None \n    empty = True \n    tokgen = generate_tokens ( self . text ) \n    for toktype , ttext , ( slineno , _ ) , ( elineno , _ ) , ltext in tokgen : \n        if self . show_tokens : \n            pprint . pprint ( \"%10s %5s %-20r %r\" % ( tokenize . tok_name . get ( toktype , toktype ) , nice_pair ( ( slineno , elineno ) ) , ttext , ltext ) ) \n        if toktype == token . INDENT : \n            indent += 1 \n        elif toktype == token . DEDENT : \n            indent -= 1 \n        elif toktype == token . NAME and ttext == 'class' : \n            self . classdefs . add ( slineno ) \n        elif toktype == token . OP and ttext == ':' : \n            if not excluding and elineno in self . excluded : \n                exclude_indent = indent \n                excluding = True \n        elif toktype == token . STRING and prev_toktype == token . INDENT : \n            self . docstrings . update ( range ( slineno , elineno + 1 ) ) \n        elif toktype == token . NEWLINE : \n            if first_line is not None and elineno != first_line : \n                rng = ( first_line , elineno ) \n                for l in range ( first_line , elineno + 1 ) : \n                    self . multiline [ l ] = rng \n            first_line = None \n        if ttext . strip ( ) and toktype != tokenize . COMMENT : \n            empty = False \n            if first_line is None : \n                first_line = slineno \n                if excluding and indent <= exclude_indent : \n                    excluding = False \n                if excluding : \n                    self . excluded . add ( elineno ) \n        prev_toktype = toktype \n    if not empty : \n        self . statement_starts . update ( self . byte_parser . _find_statements ( ) ) "}
{"14317": "\nimport pprint \ndef _convert_to_metadata ( ) : \n    import glob \n    for fname in glob . glob ( '*.ipynb' ) : \n        pprint . pprint ( 'Converting file:' , fname ) \n        with open ( fname , 'r' ) as f : \n            nb = read ( f , u'json' ) \n        md = new_metadata ( ) \n        if u'name' in nb : \n            md . name = nb . name \n            del nb [ u'name' ] \n        nb . metadata = md \n        with open ( fname , 'w' ) as f : \n            write ( nb , f , u'json' ) "}
{"14388": "\nimport pprint \ndef _process_execute_ok ( self , msg ) : \n    payload = msg [ 'content' ] [ 'payload' ] \n    for item in payload : \n        if not self . _process_execute_payload ( item ) : \n            warning = 'Warning: received unknown payload of type %s' \n            pprint . pprint ( warning % repr ( item [ 'source' ] ) ) "}
{"14524": "\nimport pprint \ndef raw_input_multi ( header = '' , ps1 = '==> ' , ps2 = '..> ' , terminate_str = '.' ) : \n    try : \n        if header : \n            header += '\\n' \n        lines = [ raw_input ( header + ps1 ) ] \n    except EOFError : \n        return [ ] \n    terminate = [ terminate_str ] \n    try : \n        while lines [ - 1 : ] != terminate : \n            new_line = raw_input ( ps1 ) \n            while new_line . endswith ( '\\\\' ) : \n                new_line = new_line [ : - 1 ] + raw_input ( ps2 ) \n            lines . append ( new_line ) \n        return lines [ : - 1 ] \n    except EOFError : \n        pprint . pprint ( ) \n        return lines "}
{"14570": "\nimport pprint \ndef handle ( self , * args , ** options ) : \n    try : \n        while True : \n            Channel ( HEARTBEAT_CHANNEL ) . send ( { 'time' : time . time ( ) } ) \n            time . sleep ( HEARTBEAT_FREQUENCY ) \n    except KeyboardInterrupt : \n        pprint . pprint ( \"Received keyboard interrupt, exiting...\" ) "}
{"14588": "\nimport pprint \ndef writeout_cache ( self , conn = None ) : \n    if conn is None : \n        conn = self . db \n    with self . db_input_cache_lock : \n        try : \n            self . _writeout_input_cache ( conn ) \n        except sqlite3 . IntegrityError : \n            self . new_session ( conn ) \n            pprint . pprint ( \"ERROR! Session/line number was not unique in\" , \"database. History logging moved to new session\" , self . session_number ) \n            try : \n                self . _writeout_input_cache ( conn ) \n            except sqlite3 . IntegrityError : \n                pass \n        finally : \n            self . db_input_cache = [ ] \n    with self . db_output_cache_lock : \n        try : \n            self . _writeout_output_cache ( conn ) \n        except sqlite3 . IntegrityError : \n            pprint . pprint ( \"!! Session/line number for output was not unique\" , \"in database. Output will not be stored.\" ) \n        finally : \n            self . db_output_cache = [ ] "}
{"14605": "\nimport pprint \ndef _system_body ( p ) : \n    enc = DEFAULT_ENCODING \n    for line in read_no_interrupt ( p . stdout ) . splitlines ( ) : \n        line = line . decode ( enc , 'replace' ) \n        pprint . pprint ( line , file = sys . stdout ) \n    for line in read_no_interrupt ( p . stderr ) . splitlines ( ) : \n        line = line . decode ( enc , 'replace' ) \n        pprint . pprint ( line , file = sys . stderr ) \n    return p . wait ( ) "}
{"14690": "\nimport pprint \ndef cleanup ( controller , engines ) : \n    import signal , time \n    pprint . pprint ( 'Starting cleanup' ) \n    pprint . pprint ( 'Stopping engines...' ) \n    for e in engines : \n        e . send_signal ( signal . SIGINT ) \n    pprint . pprint ( 'Stopping controller...' ) \n    controller . send_signal ( signal . SIGINT ) \n    time . sleep ( 0.1 ) \n    pprint . pprint ( 'Killing controller...' ) \n    controller . kill ( ) \n    pprint . pprint ( 'Cleanup done' ) "}
{"14702": "\nimport pprint \ndef get_root_modules ( ) : \n    ip = get_ipython ( ) \n    if 'rootmodules' in ip . db : \n        return ip . db [ 'rootmodules' ] \n    t = time ( ) \n    store = False \n    modules = list ( sys . builtin_module_names ) \n    for path in sys . path : \n        modules += module_list ( path ) \n        if time ( ) - t >= TIMEOUT_STORAGE and not store : \n            store = True \n            pprint . pprint ( \"\\nCaching the list of root modules, please wait!\" ) \n            pprint . pprint ( \"(This will only be done once - type '%rehashx' to \" \"reset cache!)\\n\" ) \n            sys . stdout . flush ( ) \n        if time ( ) - t > TIMEOUT_GIVEUP : \n            pprint . pprint ( \"This is taking too long, we give up.\\n\" ) \n            ip . db [ 'rootmodules' ] = [ ] \n            return [ ] \n    modules = set ( modules ) \n    if '__init__' in modules : \n        modules . remove ( '__init__' ) \n    modules = list ( modules ) \n    if store : \n        ip . db [ 'rootmodules' ] = modules \n    return modules "}
{"14836": "\nimport pprint \ndef find_best_string ( query , corpus , step = 4 , flex = 3 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2 \n        p_r , bp_r = [ pos + qlen ] * 2 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2 : \n        pprint . pprint ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14844": "\nimport pprint \ndef getfigs ( * fig_nums ) : \n    from matplotlib . _pylab_helpers import Gcf \n    if not fig_nums : \n        fig_managers = Gcf . get_all_fig_managers ( ) \n        return [ fm . canvas . figure for fm in fig_managers ] \n    else : \n        figs = [ ] \n        for num in fig_nums : \n            f = Gcf . figs . get ( num ) \n            if f is None : \n                pprint . pprint ( 'Warning: figure %s not available.' % num ) \n            else : \n                figs . append ( f . canvas . figure ) \n        return figs "}
{"14859": "\nimport pprint \ndef pause ( self ) : \n    for tracer in self . tracers : \n        tracer . stop ( ) \n        stats = tracer . get_stats ( ) \n        if stats : \n            pprint . pprint ( \"\\nCoverage.py tracer stats:\" ) \n            for k in sorted ( stats . keys ( ) ) : \n                pprint . pprint ( \"%16s: %s\" % ( k , stats [ k ] ) ) \n    threading . settrace ( None ) "}
{"14895": "\nimport pprint \ndef system ( self , cmd ) : \n    enc = DEFAULT_ENCODING \n    patterns = [ pexpect . TIMEOUT , pexpect . EOF ] \n    EOF_index = patterns . index ( pexpect . EOF ) \n    out_size = 0 \n    try : \n        if hasattr ( pexpect , 'spawnb' ) : \n            child = pexpect . spawnb ( self . sh , args = [ '-c' , cmd ] ) \n        else : \n            child = pexpect . spawn ( self . sh , args = [ '-c' , cmd ] ) \n        flush = sys . stdout . flush \n        while True : \n            res_idx = child . expect_list ( patterns , self . read_timeout ) \n            pprint . pprint ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            flush ( ) \n            if res_idx == EOF_index : \n                break \n            out_size = len ( child . before ) \n    except KeyboardInterrupt : \n        child . sendline ( chr ( 3 ) ) \n        try : \n            out_size = len ( child . before ) \n            child . expect_list ( patterns , self . terminate_timeout ) \n            pprint . pprint ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            sys . stdout . flush ( ) \n        except KeyboardInterrupt : \n            pass \n        finally : \n            child . terminate ( force = True ) \n    child . isalive ( ) \n    return child . exitstatus "}
