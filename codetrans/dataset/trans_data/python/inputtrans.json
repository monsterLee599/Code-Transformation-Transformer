{"610": "\ndef custom_gradient ( fx , gx , x , fx_gx_manually_stopped = False , name = None ) : \n    def maybe_stop ( x ) : \n        if fx_gx_manually_stopped : \n            return x \n        return tf . stop_gradient ( x ) \n    with tf . compat . v1 . name_scope ( name , 'custom_gradient' , [ fx , gx , x ] ) : \n        fx = tf . convert_to_tensor ( value = fx , name = 'fx' ) \n        with tf . control_dependencies ( [ fx ] ) : \n            if is_list_like ( x ) : \n                x = [ identity ( x_ , name = 'x' ) for x_ in x ] \n            else : \n                x = [ identity ( x , name = 'x' ) ] \n            if is_list_like ( gx ) : \n                gx = [ identity ( gx_ , dtype = fx . dtype , name = 'gx' ) for gx_ in gx ] \n            else : \n                gx = [ identity ( gx , dtype = fx . dtype , name = 'gx' ) ] \n            override_grad = [ ] \n            for x_ , gx_ in zip ( x , gx ) : \n                equal_shape = tf . compat . v1 . assert_equal ( tf . shape ( raw_input = x_ ) , tf . shape ( raw_input = gx_ ) , message = 'Each `x` must have the same shape as each `gx`.' ) \n                with tf . control_dependencies ( [ equal_shape ] ) : \n                    zeros_like_x_ = x_ - tf . stop_gradient ( x_ ) \n                    override_grad . append ( tf . reduce_sum ( input_tensor = maybe_stop ( gx_ ) * zeros_like_x_ ) ) \n            override_grad = sum ( override_grad ) \n            override_grad /= tf . cast ( tf . size ( raw_input = fx ) , dtype = fx . dtype . base_dtype ) \n            return maybe_stop ( fx ) + override_grad "}
{"619": "\ndef one_step_predictive ( model , observed_time_series , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'one_step_predictive' , values = [ observed_time_series , parameter_samples ] ) : \n        [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( raw_input = observed_time_series ) ) [ - 2 ] \n        lgssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        ( _ , _ , _ , _ , _ , observation_means , observation_covs ) = lgssm . forward_filter ( observed_time_series , mask = is_missing ) \n        return sts_util . mix_over_posterior_draws ( means = observation_means [ ... , 0 ] , variances = observation_covs [ ... , 0 , 0 ] ) "}
{"620": "\ndef forecast ( model , observed_time_series , parameter_samples , num_steps_forecast ) : \n    with tf . compat . v1 . name_scope ( 'forecast' , values = [ observed_time_series , parameter_samples , num_steps_forecast ] ) : \n        [ observed_time_series , mask ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_observed_steps = dist_util . prefer_static_value ( tf . shape ( raw_input = observed_time_series ) ) [ - 2 ] \n        observed_data_ssm = model . make_state_space_model ( num_timesteps = num_observed_steps , param_vals = parameter_samples ) \n        ( _ , _ , _ , predictive_means , predictive_covs , _ , _ ) = observed_data_ssm . forward_filter ( observed_time_series , mask = mask ) \n        parameter_samples = model . _canonicalize_param_vals_as_map ( parameter_samples ) \n        parameter_samples_with_reordered_batch_dimension = { param . name : dist_util . move_dimension ( parameter_samples [ param . name ] , 0 , - ( 1 + _prefer_static_event_ndims ( param . prior ) ) ) for param in model . parameters } \n        forecast_prior = tfd . MultivariateNormalFullCovariance ( loc = dist_util . move_dimension ( predictive_means [ ... , - 1 , : ] , 0 , - 2 ) , covariance_matrix = dist_util . move_dimension ( predictive_covs [ ... , - 1 , : , : ] , 0 , - 3 ) ) \n        kwargs = { } \n        if hasattr ( model , 'constant_offset' ) : \n            kwargs [ 'constant_offset' ] = tf . convert_to_tensor ( value = model . constant_offset , dtype = forecast_prior . dtype ) [ ... , tf . newaxis ] \n        forecast_ssm = model . _make_state_space_model ( num_timesteps = num_steps_forecast , param_map = parameter_samples_with_reordered_batch_dimension , initial_state_prior = forecast_prior , initial_step = num_observed_steps , ** kwargs ) \n        num_posterior_draws = dist_util . prefer_static_value ( forecast_ssm . batch_shape_tensor ( ) ) [ - 1 ] \n        return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = forecast_ssm . dtype ) ) , components_distribution = forecast_ssm ) "}
{"625": "\ndef _eval_all_one_hot ( fn , dist , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'eval_all_one_hot' ) : \n        event_size = dist . event_shape_tensor ( ) [ - 1 ] \n        batch_ndims = tf . size ( raw_input = dist . batch_shape_tensor ( ) ) \n        x = tf . reshape ( tf . eye ( event_size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch_ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant_values = event_size ) ) \n        perm = tf . pad ( tensor = tf . range ( 1 , batch_ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) \n        return tf . transpose ( a = fn ( dist , x ) , perm = perm ) "}
{"634": "\ndef _maybe_check_valid_map_values ( map_values , validate_args ) : \n    assertions = [ ] \n    message = 'Rank of map_values must be 1.' \n    if tensorshape_util . rank ( map_values . shape ) is not None : \n        if tensorshape_util . rank ( map_values . shape ) != 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) \n    message = 'Size of map_values must be greater than 0.' \n    if tensorshape_util . num_elements ( map_values . shape ) is not None : \n        if tensorshape_util . num_elements ( map_values . shape ) == 0 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( tf . size ( raw_input = map_values ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) \n    return assertions "}
{"641": "\ndef metropolis_hastings_step ( current_state : State , proposed_state : State , energy_change : FloatTensor , seed = None ) -> Tuple [ State , tf . Tensor , tf . Tensor ] : \n    flat_current = tf . nest . flatten ( current_state ) \n    flat_proposed = nest . flatten_up_to ( current_state , proposed_state ) \n    flat_current = [ p if c is None else c for p , c in zip ( flat_proposed , flat_current ) ] \n    current_state = tf . nest . pack_sequence_as ( current_state , flat_current ) \n    current_state = tf . nest . map_structure ( tf . convert_to_tensor , current_state ) \n    proposed_state = tf . nest . map_structure ( tf . convert_to_tensor , proposed_state ) \n    energy_change = tf . convert_to_tensor ( value = energy_change ) \n    log_accept_ratio = - energy_change \n    log_uniform = tf . math . log ( tf . random . uniform ( shape = tf . shape ( raw_input = log_accept_ratio ) , dtype = log_accept_ratio . dtype . base_dtype , seed = seed ) ) \n    is_accepted = log_uniform < log_accept_ratio \n    next_state = mcmc_util . choose ( is_accepted , proposed_state , current_state , name = 'choose_next_state' ) \n    return next_state , is_accepted , log_uniform "}
{"642": "\ndef hamiltonian_monte_carlo ( hmc_state : HamiltonianMonteCarloState , target_log_prob_fn : PotentialFn , step_size : Any , num_leapfrog_steps : IntTensor , momentum : State = None , kinetic_energy_fn : PotentialFn = None , momentum_sample_fn : MomentumSampleFn = None , leapfrog_trace_fn : Callable [ [ LeapFrogStepState , LeapFrogStepExtras ] , TensorNest ] = lambda * args : ( ) , seed = None , ) -> Tuple [ HamiltonianMonteCarloState , HamiltonianMonteCarloExtra ] : \n    state = hmc_state . state \n    state_grads = hmc_state . state_grads \n    target_log_prob = hmc_state . target_log_prob \n    state_extra = hmc_state . state_extra \n    if kinetic_energy_fn is None : \n        def kinetic_energy_fn ( * momentum ) : \n            return tf . add_n ( [ tf . reduce_sum ( input_tensor = tf . square ( x ) , axis = - 1 ) / 2. for x in tf . nest . flatten ( momentum ) ] ) , ( ) \n    if momentum_sample_fn is None : \n        def momentum_sample_fn ( * momentum ) : \n            ret = tf . nest . map_structure ( lambda x : tf . random . normal ( tf . shape ( raw_input = x ) , dtype = x . dtype ) , momentum ) \n            if len ( ret ) == 1 : \n                return ret [ 0 ] \n            else : \n                return ret \n    if momentum is None : \n        momentum = call_fn ( momentum_sample_fn , tf . nest . map_structure ( tf . zeros_like , state ) ) \n    if target_log_prob is None : \n        target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) \n    kinetic_energy , _ = call_fn ( kinetic_energy_fn , momentum ) \n    current_energy = - target_log_prob + kinetic_energy \n    current_state = HamiltonianMonteCarloState ( state = state , state_grads = state_grads , state_extra = state_extra , target_log_prob = target_log_prob ) \n    def leapfrog_wrapper ( leapfrog_state , target_log_prob , state_extra ) : \n        del target_log_prob \n        del state_extra \n        leapfrog_state , leapfrog_extra = leapfrog_step ( leapfrog_state , step_size = step_size , target_log_prob_fn = target_log_prob_fn , kinetic_energy_fn = kinetic_energy_fn ) \n        return [ leapfrog_state , leapfrog_extra . target_log_prob , leapfrog_extra . state_extra ] , leapfrog_extra \n    def leapfrog_trace_wrapper_fn ( args , leapfrog_extra ) : \n        return leapfrog_trace_fn ( args [ 0 ] , leapfrog_extra ) \n    leapfrog_wrapper_state = ( LeapFrogStepState ( state , state_grads , momentum ) , target_log_prob , state_extra ) \n    [ [ leapfrog_state , target_log_prob , state_extra ] , _ ] , leapfrog_trace = trace ( leapfrog_wrapper_state , leapfrog_wrapper , num_leapfrog_steps , trace_fn = leapfrog_trace_wrapper_fn ) \n    kinetic_energy , _ = call_fn ( kinetic_energy_fn , leapfrog_state . momentum ) \n    proposed_energy = - target_log_prob + kinetic_energy \n    proposed_state = HamiltonianMonteCarloState ( state = leapfrog_state . state , state_grads = leapfrog_state . state_grads , target_log_prob = target_log_prob , state_extra = state_extra ) \n    energy_change = proposed_energy - current_energy \n    hmc_state , is_accepted , _ = metropolis_hastings_step ( current_state , proposed_state , energy_change , seed = seed ) \n    hmc_state = hmc_state \n    return hmc_state , HamiltonianMonteCarloExtra ( is_accepted = is_accepted , proposed_hmc_state = proposed_state , log_accept_ratio = - energy_change , leapfrog_trace = leapfrog_trace ) "}
{"647": "\ndef random_walk_normal_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_normal_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if len ( scales ) == 1 : \n                scales *= len ( state_parts ) \n            if len ( state_parts ) != len ( scales ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkNormalFn' ) \n            next_state_parts = [ tf . random . normal ( mean = state_part , stddev = scale_part , shape = tf . shape ( raw_input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"648": "\ndef random_walk_uniform_fn ( scale = 1. , name = None ) : \n    def _fn ( state_parts , seed ) : \n        with tf . compat . v1 . name_scope ( name , 'random_walk_uniform_fn' , values = [ state_parts , scale , seed ] ) : \n            scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] \n            if len ( scales ) == 1 : \n                scales *= len ( state_parts ) \n            if len ( state_parts ) != len ( scales ) : \n                raise ValueError ( '`scale` must broadcast with `state_parts`.' ) \n            seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkUniformFn' ) \n            next_state_parts = [ tf . random . uniform ( minval = state_part - scale_part , maxval = state_part + scale_part , shape = tf . shape ( raw_input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] \n            return next_state_parts \n    return _fn "}
{"652": "\ndef _maybe_validate_args ( outcomes , logits , probs , validate_args ) : \n    assertions = [ ] \n    def validate_equal_last_dim ( tensor_a , tensor_b , message ) : \n        if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : \n            if tensor_a . shape [ - 1 ] != tensor_b . shape [ - 1 ] : \n                raise ValueError ( message ) \n        elif validate_args : \n            assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( raw_input = tensor_a ) [ - 1 ] , tf . shape ( raw_input = tensor_b ) [ - 1 ] , message = message ) ) \n    if logits is not None : \n        validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) \n    if probs is not None : \n        validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) \n    message = 'Rank of outcomes must be 1.' \n    if outcomes . shape . ndims is not None : \n        if outcomes . shape . ndims != 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) \n    message = 'Size of outcomes must be greater than 0.' \n    if outcomes . shape . num_elements ( ) is not None : \n        if outcomes . shape . num_elements ( ) == 0 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( raw_input = outcomes ) , 0 , message = message ) ) \n    if validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) \n    return assertions "}
{"660": "\ndef _squeeze ( x , axis ) : \n    x = tf . convert_to_tensor ( value = x , name = 'x' ) \n    if axis is None : \n        return tf . squeeze ( x , axis = None ) \n    axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) \n    axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) \n    keep_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) \n    return tf . reshape ( x , tf . gather ( tf . shape ( raw_input = x ) , keep_axis ) ) "}
{"679": "\ndef quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"softmax_normal_grid_and_probs\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        dt = dtype_util . base_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) \n        def _get_batch_ndims ( ) : \n            ndims = tensorshape_util . rank ( dist . batch_shape ) \n            if ndims is None : \n                ndims = tf . shape ( raw_input = dist . batch_shape_tensor ( ) ) [ 0 ] \n            return ndims \n        batch_ndims = _get_batch_ndims ( ) \n        def _get_final_shape ( qs ) : \n            bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , 1 ) \n            num_components = tf . compat . dimension_value ( bs [ - 1 ] ) \n            if num_components is not None : \n                num_components += 1 \n            tail = tf . TensorShape ( [ num_components , qs ] ) \n            return bs [ : - 1 ] . concatenate ( tail ) \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + 1 ) ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"680": "\ndef maybe_check_quadrature_param ( param , name , validate_args ) : \n    with tf . name_scope ( \"check_\" + name ) : \n        assertions = [ ] \n        if tensorshape_util . rank ( param . shape ) is not None : \n            if tensorshape_util . rank ( param . shape ) == 0 : \n                raise ValueError ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape_util . rank ( param . shape ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_rank_at_least ( param , 1 , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) \n        if tensorshape_util . with_rank_at_least ( param . shape , 1 ) [ - 1 ] is not None : \n            if tf . compat . dimension_value ( param . shape [ - 1 ] ) != 1 : \n                raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension_value ( param . shape [ - 1 ] ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_equal ( tf . shape ( raw_input = param ) [ - 1 ] , 1 , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) \n        if assertions : \n            return distribution_util . with_dependencies ( assertions , param ) \n        return param "}
{"681": "\ndef determine_batch_event_shapes ( grid , endpoint_affine ) : \n    with tf . name_scope ( \"determine_batch_event_shapes\" ) : \n        batch_shape = grid . shape [ : - 2 ] \n        batch_shape_tensor = tf . shape ( raw_input = grid ) [ : - 2 ] \n        event_shape = None \n        event_shape_tensor = None \n        def _set_event_shape ( shape , shape_tensor ) : \n            if event_shape is None : \n                return shape , shape_tensor \n            return ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) \n        for aff in endpoint_affine : \n            if aff . shift is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - 1 ] ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( raw_input = aff . shift ) [ : - 1 ] ) \n                event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - 1 : ] , tf . shape ( raw_input = aff . shift ) [ - 1 : ] ) \n            if aff . scale is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) \n                event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) \n        return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor "}
{"691": "\ndef posterior_marginals ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_marginals\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( raw_input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                log_transition = self . _log_trans \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_adjoint_prob = tf . zeros_like ( log_init ) \n                def forward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_vector_matrix ( log_previous_step , log_transition ) + log_prob_observation \n                log_prob = log_init + observation_log_probs [ 0 ] \n                forward_log_probs = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = log_prob , name = \"forward_log_probs\" ) \n                forward_log_probs = tf . concat ( [ [ log_prob ] , forward_log_probs ] , axis = 0 ) \n                def backward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_matrix_vector ( log_transition , log_prob_observation + log_previous_step ) \n                backward_log_adjoint_probs = tf . scan ( backward_step , observation_log_probs [ 1 : ] , initializer = log_adjoint_prob , reverse = True , name = \"backward_log_adjoint_probs\" ) \n                total_log_prob = tf . reduce_logsumexp ( input_tensor = forward_log_probs [ - 1 ] , axis = - 1 ) \n                backward_log_adjoint_probs = tf . concat ( [ backward_log_adjoint_probs , [ log_adjoint_prob ] ] , axis = 0 ) \n                log_likelihoods = forward_log_probs + backward_log_adjoint_probs \n                marginal_log_probs = distribution_util . move_dimension ( log_likelihoods - total_log_prob [ ... , tf . newaxis ] , 0 , - 2 ) \n                return categorical . Categorical ( logits = marginal_log_probs ) "}
{"692": "\ndef posterior_mode ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_mode\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( raw_input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_prob = log_init + observation_log_probs [ 0 ] \n                if self . _num_steps == 1 : \n                    most_likely_end = tf . argmax ( raw_input = log_prob , axis = - 1 ) \n                    return most_likely_end [ ... , tf . newaxis ] \n                def forward_step ( previous_step_pair , log_prob_observation ) : \n                    log_prob_previous = previous_step_pair [ 0 ] \n                    log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) \n                    most_likely_given_successor = tf . argmax ( raw_input = log_prob , axis = - 2 ) \n                    max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) \n                    return ( max_log_p_given_successor , most_likely_given_successor ) \n                forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( raw_input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) \n                most_likely_end = tf . argmax ( raw_input = forward_log_probs [ - 1 ] , axis = - 1 ) \n                def backward_step ( most_likely_successor , most_likely_given_successor ) : \n                    return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) \n                backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) \n                most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) \n                return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 ) "}
{"693": "\ndef _choose_random_direction ( current_state_parts , batch_rank , seed = None ) : \n    seed_gen = distributions . SeedStream ( seed , salt = '_choose_random_direction' ) \n    rnd_direction_parts = [ tf . random . normal ( tf . shape ( raw_input = current_state_part ) , dtype = tf . float32 , seed = seed_gen ( ) ) for current_state_part in current_state_parts ] \n    sum_squares = sum ( tf . reduce_sum ( input_tensor = rnd_direction ** 2. , axis = tf . range ( batch_rank , tf . rank ( rnd_direction ) ) , keepdims = True ) for rnd_direction in rnd_direction_parts ) \n    rnd_direction_parts = [ rnd_direction / tf . sqrt ( sum_squares ) for rnd_direction in rnd_direction_parts ] \n    return rnd_direction_parts "}
{"696": "\ndef _right_pad ( x , final_rank ) : \n    padded_shape = tf . concat ( [ tf . shape ( raw_input = x ) , tf . ones ( final_rank - tf . rank ( x ) , dtype = tf . int32 ) ] , axis = 0 ) \n    static_padded_shape = None \n    if x . shape . is_fully_defined ( ) and isinstance ( final_rank , int ) : \n        static_padded_shape = x . shape . as_list ( ) \n        extra_dims = final_rank - len ( static_padded_shape ) \n        static_padded_shape . extend ( [ 1 ] * extra_dims ) \n    padded_x = tf . reshape ( x , static_padded_shape or padded_shape ) \n    return padded_x "}
{"702": "\ndef initial_value_of_masked_time_series ( time_series_tensor , broadcast_mask ) : \n    num_timesteps = tf . shape ( raw_input = time_series_tensor ) [ - 1 ] \n    unmasked_negindices = ( tf . cast ( ~ broadcast_mask , tf . int32 ) * tf . range ( num_timesteps , 0 , - 1 ) ) \n    first_unmasked_indices = num_timesteps - tf . reduce_max ( input_tensor = unmasked_negindices , axis = - 1 ) \n    if first_unmasked_indices . shape . ndims is None : \n        raise NotImplementedError ( 'Cannot compute initial values of a masked time series with' 'dynamic rank.' ) \n    return tf . squeeze ( tf . compat . v1 . batch_gather ( params = time_series_tensor , indices = first_unmasked_indices [ ... , tf . newaxis ] ) , axis = - 1 ) "}
{"706": "\ndef empirical_statistics ( observed_time_series ) : \n    with tf . compat . v1 . name_scope ( 'empirical_statistics' , values = [ observed_time_series ] ) : \n        [ observed_time_series , mask ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        squeezed_series = observed_time_series [ ... , 0 ] \n        if mask is None : \n            observed_mean , observed_variance = tf . nn . moments ( x = squeezed_series , axes = - 1 ) \n            observed_initial = squeezed_series [ ... , 0 ] \n        else : \n            broadcast_mask = tf . broadcast_to ( tf . cast ( mask , tf . bool ) , tf . shape ( raw_input = squeezed_series ) ) \n            observed_mean , observed_variance = ( missing_values_util . moments_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) \n            try : \n                observed_initial = ( missing_values_util . initial_value_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) \n            except NotImplementedError : \n                tf . compat . v1 . logging . warn ( 'Cannot compute initial values for a masked time series' 'with dynamic shape; using the mean instead. This will' 'affect heuristic priors and may change the results of' 'inference.' ) \n                observed_initial = observed_mean \n        observed_stddev = tf . sqrt ( observed_variance ) \n        observed_initial_centered = observed_initial - observed_mean \n        return observed_mean , observed_stddev , observed_initial_centered "}
{"707": "\ndef _maybe_expand_trailing_dim ( observed_time_series_tensor ) : \n    with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : \n        if ( observed_time_series_tensor . shape . ndims is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - 1 ] ) is not None ) : \n            expanded_time_series = ( observed_time_series_tensor if observed_time_series_tensor . shape [ - 1 ] == 1 else observed_time_series_tensor [ ... , tf . newaxis ] ) \n        else : \n            expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( raw_input = observed_time_series_tensor ) [ - 1 ] , 1 ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) \n        return expanded_time_series "}
{"709": "\ndef mix_over_posterior_draws ( means , variances ) : \n    with tf . compat . v1 . name_scope ( 'mix_over_posterior_draws' , values = [ means , variances ] ) : \n        num_posterior_draws = dist_util . prefer_static_value ( tf . shape ( raw_input = means ) ) [ 0 ] \n        component_observations = tfd . Independent ( distribution = tfd . Normal ( loc = dist_util . move_dimension ( means , 0 , - 2 ) , scale = tf . sqrt ( dist_util . move_dimension ( variances , 0 , - 2 ) ) ) , reinterpreted_batch_ndims = 1 ) \n        return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = component_observations . dtype ) ) , components_distribution = component_observations ) "}
{"727": "\ndef call ( self , inputs , state ) : \n    original_shape = inputs . shape \n    if len ( original_shape ) < 2 : \n        inputs = tf . reshape ( inputs , [ 1 , - 1 ] ) \n    out , state = self . lstm_cell ( inputs , state ) \n    out = self . output_layer ( out ) \n    correct_shape = tf . concat ( ( original_shape [ : - 1 ] , tf . shape ( raw_input = out ) [ - 1 : ] ) , 0 ) \n    out = tf . reshape ( out , correct_shape ) \n    loc = out [ ... , : self . dimensions ] \n    scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 \n    return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state "}
{"728": "\ndef call ( self , inputs ) : \n    image_shape = tf . shape ( raw_input = inputs ) [ - 3 : ] \n    collapsed_shape = tf . concat ( ( [ - 1 ] , image_shape ) , axis = 0 ) \n    out = tf . reshape ( inputs , collapsed_shape ) \n    out = self . conv1 ( out ) \n    out = self . conv2 ( out ) \n    out = self . conv3 ( out ) \n    out = self . conv4 ( out ) \n    expanded_shape = tf . concat ( ( tf . shape ( raw_input = inputs ) [ : - 3 ] , [ - 1 ] ) , axis = 0 ) \n    return tf . reshape ( out , expanded_shape ) "}
{"730": "\ndef reconstruct ( self , inputs , samples = 1 , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : \n    batch_size = tf . shape ( raw_input = inputs ) [ - 5 ] \n    length = len ( tf . unstack ( inputs , axis = - 4 ) ) \n    features = self . compressor ( inputs ) \n    if sample_static : \n        static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) \n    else : \n        static_sample , _ = self . sample_static_posterior ( features , samples ) \n    if swap_static : \n        static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) \n    if sample_dynamic : \n        dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) \n    else : \n        dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) \n    if swap_dynamic : \n        dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) \n    likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) \n    return likelihood "}
{"748": "\ndef text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : \n    alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) \n    rv_lambda = tfd . Exponential ( rate = alpha ) \n    rv_tau = tfd . Uniform ( ) \n    lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( raw_input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( raw_input = count_data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) \n    rv_observation = tfd . Poisson ( rate = lambda_ ) \n    return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) ) "}
{"755": "\ndef bootstrap_results ( self , state ) : \n    def loss ( ) : \n        q = self . _flattened_variational_distribution ( ) \n        samples = q . sample ( self . train_batch_size ) \n        return tf . reduce_mean ( input_tensor = q . log_prob ( samples ) - self . _flattened_target_log_prob ( samples ) , axis = - 1 ) \n    lr = tf . convert_to_tensor ( value = self . learning_rate , dtype = self . _dtype ) \n    dtype = lr . dtype \n    learning_rate = tf . compat . v2 . optimizers . schedules . PiecewiseConstantDecay ( list ( self . num_train_steps * np . array ( [ 0.2 , 0.8 ] ) . astype ( dtype . as_numpy_dtype ( ) ) ) , [ lr , lr * 0.1 , lr * 0.01 ] ) \n    opt = tf . compat . v2 . optimizers . Adam ( learning_rate ) \n    \n    @ tf . function ( autograph = False ) \n    def train_step ( ) : \n        with tf . GradientTape ( ) as tape : \n            loss_val = loss ( ) \n        vals = tape . watched_variables ( ) \n        grads = tape . gradient ( loss_val , vals ) \n        grads_and_vals = list ( zip ( grads , vals ) ) \n        opt . apply_gradients ( grads_and_vals ) \n        return loss_val \n    for step in range ( self . num_train_steps ) : \n        loss_val = train_step ( ) \n        tf . debugging . assert_all_finite ( loss_val , 'NeuTra loss is NaN at step {}' . format ( step ) ) \n        if self . train_debug_fn : \n            self . train_debug_fn ( self , step , loss_val ) \n    state_parts = tf . nest . flatten ( state ) \n    flat_state_shapes = tf . nest . flatten ( self . state_shape ) \n    batch_shape = tf . shape ( raw_input = state_parts [ 0 ] ) [ : - flat_state_shapes [ 0 ] . ndims ] \n    return self . _kernel . bootstrap_results ( self . _flattened_variational_distribution ( ) . sample ( batch_shape , seed = self . seed ) ) "}
{"759": "\ndef _distributional_transform ( self , x ) : \n    if tensorshape_util . rank ( x . shape ) is None : \n        raise ValueError ( \"Distributional transform does not support inputs of \" \"undefined rank.\" ) \n    if isinstance ( self . _components_distribution , independent . Independent ) : \n        univariate_components = self . _components_distribution . distribution \n    else : \n        univariate_components = self . _components_distribution \n    with tf . control_dependencies ( [ assert_util . assert_equal ( univariate_components . is_scalar_event ( ) , True , message = \"`univariate_components` must have scalar event\" ) ] ) : \n        x_padded = self . _pad_sample_dims ( x ) \n        log_prob_x = univariate_components . log_prob ( x_padded ) \n        cdf_x = univariate_components . cdf ( x_padded ) \n        cumsum_log_prob_x = tf . reshape ( tf . math . cumsum ( tf . reshape ( log_prob_x , [ - 1 , self . _event_size ] ) , exclusive = True , axis = - 1 ) , tf . shape ( raw_input = log_prob_x ) ) \n        logits_mix_prob = distribution_utils . pad_mixture_dimensions ( self . mixture_distribution . logits , self , self . mixture_distribution , self . _event_ndims ) \n        log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x \n        component_axis = tensorshape_util . rank ( x . shape ) - self . _event_ndims \n        posterior_weights_x = tf . nn . softmax ( log_posterior_weights_x , axis = component_axis ) \n        return tf . reduce_sum ( input_tensor = posterior_weights_x * cdf_x , axis = component_axis ) "}
{"761": "\ndef _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) : \n    try : \n        model . components \n    except AttributeError : \n        raise ValueError ( 'Model decomposed into components must be an instance of' '`tfp.sts.Sum` (passed model {})' . format ( model ) ) \n    with tf . compat . v1 . name_scope ( 'decompose_from_posterior_marginals' ) : \n        latent_sizes = [ component . latent_size for component in model . components ] \n        component_means = tf . split ( posterior_means , latent_sizes , axis = - 1 ) \n        component_covs = _split_covariance_into_marginals ( posterior_covs , latent_sizes ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( raw_input = posterior_means ) ) [ - 2 ] \n        component_ssms = model . make_component_state_space_models ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        component_predictive_dists = collections . OrderedDict ( ) \n        for ( component , component_ssm , component_mean , component_cov ) in zip ( model . components , component_ssms , component_means , component_covs ) : \n            component_obs_mean , component_obs_cov = ( component_ssm . latents_to_observations ( latent_means = component_mean , latent_covs = component_cov ) ) \n            component_predictive_dists [ component ] = sts_util . mix_over_posterior_draws ( means = component_obs_mean [ ... , 0 ] , variances = component_obs_cov [ ... , 0 , 0 ] ) \n    return component_predictive_dists "}
{"762": "\ndef decompose_by_component ( model , observed_time_series , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'decompose_by_component' , values = [ observed_time_series ] ) : \n        [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( raw_input = observed_time_series ) ) [ - 2 ] \n        ssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        posterior_means , posterior_covs = ssm . posterior_marginals ( observed_time_series , mask = is_missing ) \n        return _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) "}
{"764": "\ndef dense_to_sparse ( x , ignore_value = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'dense_to_sparse' , [ x , ignore_value ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if ignore_value is None : \n            if x . dtype . base_dtype == tf . string : \n                ignore_value = '' \n            else : \n                ignore_value = x . dtype . as_numpy_dtype ( 0 ) \n            ignore_value = tf . cast ( ignore_value , x . dtype , name = 'ignore_value' ) \n        indices = tf . where ( tf . not_equal ( x , ignore_value ) , name = 'indices' ) \n        return tf . SparseTensor ( indices = indices , values = tf . gather_nd ( x , indices , name = 'values' ) , dense_shape = tf . shape ( raw_input = x , out_type = tf . int64 , name = 'dense_shape' ) ) "}
{"784": "\ndef _axis_size ( x , axis = None ) : \n    if axis is None : \n        return tf . cast ( tf . size ( raw_input = x ) , x . dtype ) \n    return tf . cast ( tf . reduce_prod ( input_tensor = tf . gather ( tf . shape ( raw_input = x ) , axis ) ) , x . dtype ) "}
{"787": "\ndef quadrature_scheme_lognormal_quantiles ( loc , scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_lognormal_quantiles\" ) : \n        dist = transformed_distribution . TransformedDistribution ( distribution = normal . Normal ( loc = loc , scale = scale ) , bijector = exp_bijector . Exp ( ) , validate_args = validate_args ) \n        batch_ndims = tensorshape_util . rank ( dist . batch_shape ) \n        if batch_ndims is None : \n            batch_ndims = tf . shape ( raw_input = dist . batch_shape_tensor ( ) ) [ 0 ] \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        new_shape = tensorshape_util . concatenate ( dist . batch_shape , [ quadrature_size ] ) \n        tensorshape_util . set_shape ( grid , new_shape ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"793": "\ndef _find_best_interval_idx ( x , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'find_best_interval_idx' , [ x ] ) : \n        k = tf . shape ( raw_input = x ) [ 0 ] \n        dtype = x . dtype . base_dtype \n        mults = tf . range ( 2 * k , k , - 1 , dtype = dtype ) [ : , tf . newaxis ] \n        shifts = tf . range ( k , dtype = dtype ) [ : , tf . newaxis ] \n        indices = tf . argmax ( raw_input = mults * x + shifts , axis = 0 , output_type = dtype ) \n        return indices "}
{"794": "\ndef slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        batch_shape = tf . shape ( raw_input = x_initial ) \n        dtype = step_size . dtype . base_dtype \n        left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) \n        left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) \n        left_endpoints -= left_increments \n        right_endpoints = left_endpoints + widths \n        left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) \n        right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) \n        left_ok = left_ep_values < log_slice_heights \n        right_ok = right_ep_values < log_slice_heights \n        both_ok = left_ok & right_ok \n        both_ok_f = tf . reshape ( both_ok , [ max_doublings + 1 , - 1 ] ) \n        best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) \n        point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( raw_input = best_interval_idx ) ) ] , axis = 1 , name = 'point_index_gather' ) \n        left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + 1 , - 1 ] ) \n        right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + 1 , - 1 ] ) \n        lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) \n        upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) \n        both_ok = tf . reduce_any ( input_tensor = both_ok , axis = 0 ) \n        return upper_bounds , lower_bounds , both_ok "}
{"795": "\ndef _sample_with_shrinkage ( x_initial , target_log_prob , log_slice_heights , step_size , lower_bounds , upper_bounds , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_with_shrinkage' , [ x_initial , log_slice_heights , step_size , lower_bounds , upper_bounds ] ) : \n        seed_gen = distributions . SeedStream ( seed , salt = '_sample_with_shrinkage' ) \n        found = tf . zeros_like ( x_initial , dtype = tf . bool ) \n        cond = lambda found , * ignored_args : ~ tf . reduce_all ( input_tensor = found ) \n        x_next = tf . identity ( x_initial ) \n        x_initial_shape = tf . shape ( raw_input = x_initial ) \n        x_initial_dtype = x_initial . dtype . base_dtype \n        def _body ( found , left , right , x_next ) : \n            proportions = tf . random . uniform ( x_initial_shape , dtype = x_initial_dtype , seed = seed_gen ( ) ) \n            x_proposed = tf . where ( ~ found , left + proportions * ( right - left ) , x_next ) \n            accept_res = _test_acceptance ( x_initial , target_log_prob = target_log_prob , decided = found , log_slice_heights = log_slice_heights , x_proposed = x_proposed , step_size = step_size , lower_bounds = left , upper_bounds = right ) \n            boundary_test = log_slice_heights < target_log_prob ( x_proposed ) \n            can_accept = boundary_test & accept_res \n            next_found = found | can_accept \n            next_left = tf . where ( x_proposed < x_initial , x_proposed , left ) \n            next_right = tf . where ( x_proposed >= x_initial , x_proposed , right ) \n            return next_found , next_left , next_right , x_proposed \n        return tf . while_loop ( cond = cond , body = _body , loop_vars = ( found , lower_bounds , upper_bounds , x_next ) ) [ - 1 ] "}
{"796": "\ndef slice_sampler_one_dim ( target_log_prob , x_initial , step_size = 0.01 , max_doublings = 30 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_sampler_one_dim' , [ x_initial , step_size , max_doublings ] ) : \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        dtype = x_initial . dtype . base_dtype \n        log_slice_heights = target_log_prob ( x_initial ) - tf . random . gamma ( tf . shape ( raw_input = x_initial ) , alpha = 1 , dtype = dtype , seed = seed ) \n        upper_bounds , lower_bounds , bounds_satisfied = slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = seed ) \n        retval = _sample_with_shrinkage ( x_initial , target_log_prob = target_log_prob , log_slice_heights = log_slice_heights , step_size = step_size , lower_bounds = lower_bounds , upper_bounds = upper_bounds , seed = seed ) \n        return ( retval , target_log_prob ( retval ) , bounds_satisfied , upper_bounds , lower_bounds ) "}
{"808": "\ndef default_mean_field_normal_fn ( is_singular = False , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : \n    loc_scale_fn = default_loc_scale_fn ( is_singular = is_singular , loc_initializer = loc_initializer , untransformed_scale_initializer = untransformed_scale_initializer , loc_regularizer = loc_regularizer , untransformed_scale_regularizer = untransformed_scale_regularizer , loc_constraint = loc_constraint , untransformed_scale_constraint = untransformed_scale_constraint ) \n    def _fn ( dtype , shape , name , trainable , add_variable_fn ) : \n        loc , scale = loc_scale_fn ( dtype , shape , name , trainable , add_variable_fn ) \n        if scale is None : \n            dist = tfd . Deterministic ( loc = loc ) \n        else : \n            dist = tfd . Normal ( loc = loc , scale = scale ) \n        batch_ndims = tf . size ( raw_input = dist . batch_shape_tensor ( ) ) \n        return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) \n    return _fn "}
{"809": "\ndef default_multivariate_normal_fn ( dtype , shape , name , trainable , add_variable_fn ) : \n    del name , trainable , add_variable_fn \n    dist = tfd . Normal ( loc = tf . zeros ( shape , dtype ) , scale = dtype . as_numpy_dtype ( 1 ) ) \n    batch_ndims = tf . size ( raw_input = dist . batch_shape_tensor ( ) ) \n    return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) "}
{"818": "\ndef pack_images ( images , rows , cols ) : \n    shape = tf . shape ( raw_input = images ) \n    width = shape [ - 3 ] \n    height = shape [ - 2 ] \n    depth = shape [ - 1 ] \n    images = tf . reshape ( images , ( - 1 , width , height , depth ) ) \n    batch = tf . shape ( raw_input = images ) [ 0 ] \n    rows = tf . minimum ( rows , batch ) \n    cols = tf . minimum ( batch // rows , cols ) \n    images = images [ : rows * cols ] \n    images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) \n    images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) \n    images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) \n    return images "}
{"821": "\ndef _validate_block_sizes ( block_sizes , bijectors , validate_args ) : \n    block_sizes_shape = block_sizes . shape \n    if tensorshape_util . is_fully_defined ( block_sizes_shape ) : \n        if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : \n            raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) \n        return block_sizes \n    elif validate_args : \n        message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) \n        with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( raw_input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : \n            return tf . identity ( block_sizes ) \n    else : \n        return block_sizes "}
{"833": "\ndef _sample_shape ( self , x ) : \n    x_ndims = ( tf . rank ( x ) if tensorshape_util . rank ( x . shape ) is None else tensorshape_util . rank ( x . shape ) ) \n    event_ndims = ( tf . size ( raw_input = self . event_shape_tensor ( ) ) if tensorshape_util . rank ( self . event_shape ) is None else tensorshape_util . rank ( self . event_shape ) ) \n    batch_ndims = ( tf . size ( raw_input = self . _batch_shape_unexpanded ) if tensorshape_util . rank ( self . batch_shape ) is None else tensorshape_util . rank ( self . batch_shape ) ) \n    sample_ndims = x_ndims - batch_ndims - event_ndims \n    if isinstance ( sample_ndims , int ) : \n        static_sample_shape = x . shape [ : sample_ndims ] \n    else : \n        static_sample_shape = tf . TensorShape ( None ) \n    if tensorshape_util . is_fully_defined ( static_sample_shape ) : \n        sample_shape = np . int32 ( static_sample_shape ) \n    else : \n        sample_shape = tf . shape ( raw_input = x ) [ : sample_ndims ] \n    return sample_shape , static_sample_shape "}
{"867": "\ndef _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : \n    initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) \n    num_vertices = tf . shape ( raw_input = initial_simplex ) [ 0 ] \n    dim = num_vertices - 1 \n    num_evaluations = 0 \n    if objective_at_initial_simplex is None : \n        objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) \n        num_evaluations += n_evals \n    objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) \n    return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations ) "}
{"868": "\ndef _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : \n    dim = tf . size ( raw_input = initial_vertex ) \n    num_vertices = dim + 1 \n    unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( raw_input = initial_vertex ) ] , axis = 0 ) ) \n    simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes \n    simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = 0 ) , simplex_face ] , axis = 0 ) \n    num_evaluations = 0 \n    if objective_at_initial_vertex is None : \n        objective_at_initial_vertex = objective_function ( initial_vertex ) \n        num_evaluations += 1 \n    objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) \n    num_evaluations += num_evals \n    objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = 0 ) , objective_at_simplex_face ] , axis = 0 ) \n    return ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations ) "}
{"869": "\ndef _evaluate_objective_multiple ( objective_function , arg_batch , batch_evaluate_objective ) : \n    n_points = tf . shape ( raw_input = arg_batch ) [ 0 ] \n    if batch_evaluate_objective : \n        return objective_function ( arg_batch ) , n_points \n    return tf . map_fn ( objective_function , arg_batch ) , n_points "}
{"876": "\ndef _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : \n    df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( raw_input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) \n    df = _broadcast_to_shape ( df , tf . shape ( raw_input = statistic ) ) \n    denom = tf . where ( df > 2. , df - 2. , tf . ones_like ( df ) ) \n    statistic = statistic * df_factor_fn ( df / denom ) \n    inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) \n    result_where_defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( raw_input = statistic ) , inf , name = \"inf\" ) ) \n    if self . allow_nan_stats : \n        nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) \n        return tf . where ( df > 1. , result_where_defined , tf . fill ( tf . shape ( raw_input = statistic ) , nan , name = \"nan\" ) ) \n    else : \n        with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + \" not defined for components of df <= 1\" ) , ] ) : \n            return tf . identity ( result_where_defined ) "}
{"878": "\ndef _make_columnar ( self , x ) : \n    if tensorshape_util . rank ( x . shape ) is not None : \n        if tensorshape_util . rank ( x . shape ) == 1 : \n            x = x [ tf . newaxis , : ] \n        return x \n    shape = tf . shape ( raw_input = x ) \n    maybe_expanded_shape = tf . concat ( [ shape [ : - 1 ] , distribution_util . pick_vector ( tf . equal ( tf . rank ( x ) , 1 ) , [ 1 ] , np . array ( [ ] , dtype = np . int32 ) ) , shape [ - 1 : ] , ] , 0 ) \n    return tf . reshape ( x , maybe_expanded_shape ) "}
{"880": "\ndef random_rayleigh ( shape , scale = None , dtype = tf . float32 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'random_rayleigh' , [ shape , scale , seed ] ) : \n        if scale is not None : \n            scale = tf . convert_to_tensor ( value = scale , dtype = dtype , name = 'scale' ) \n            shape = tf . broadcast_dynamic_shape ( shape , tf . shape ( raw_input = scale ) ) \n        x = tf . sqrt ( - 2. * tf . math . log ( tf . random . uniform ( shape , minval = 0 , maxval = 1 , dtype = dtype , seed = seed ) ) ) \n        if scale is None : \n            return x \n        return x * scale "}
{"887": "\ndef _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : \n    param_shape = tf . shape ( raw_input = param ) \n    insert_ones = tf . ones ( [ tf . size ( raw_input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) \n    new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = 0 ) \n    full_batch_param = tf . reshape ( param , new_param_shape ) \n    param_slices = [ ] \n    param_dim_idx = 0 \n    batch_dim_idx = 0 \n    for slc in slices : \n        if slc is tf . newaxis : \n            param_slices . append ( slc ) \n            continue \n        if slc is Ellipsis : \n            if batch_dim_idx < 0 : \n                raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) \n            param_slices . append ( slc ) \n            num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + 1 : ] ] ) \n            batch_dim_idx = - num_remaining_non_newaxis_slices \n            param_dim_idx = batch_dim_idx - param_event_ndims \n            continue \n        param_dim_size = new_param_shape [ param_dim_idx ] \n        batch_dim_size = dist_batch_shape [ batch_dim_idx ] \n        is_broadcast = batch_dim_size > param_dim_size \n        if isinstance ( slc , slice ) : \n            start , stop , step = slc . start , slc . stop , slc . step \n            if start is not None : \n                start = tf . where ( is_broadcast , 0 , start ) \n            if stop is not None : \n                stop = tf . where ( is_broadcast , 1 , stop ) \n            if step is not None : \n                step = tf . where ( is_broadcast , 1 , step ) \n            param_slices . append ( slice ( start , stop , step ) ) \n        else : \n            param_slices . append ( tf . where ( is_broadcast , 0 , slc ) ) \n        param_dim_idx += 1 \n        batch_dim_idx += 1 \n    param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) \n    return full_batch_param . __getitem__ ( param_slices ) "}
{"894": "\ndef prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] \n    with tf . compat . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : \n        dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) \n        model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) \n        if offset is not None : \n            offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) \n        response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) \n        use_default_model_coefficients = model_coefficients is None \n        if use_default_model_coefficients : \n            batch_shape = tf . shape ( raw_input = model_matrix ) [ : - 2 ] \n            num_columns = tf . shape ( raw_input = model_matrix ) [ - 1 ] \n            model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = 0 ) , dtype = dtype , name = 'model_coefficients' ) \n        else : \n            model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) \n        if predicted_linear_response is None : \n            if use_default_model_coefficients : \n                if offset is None : \n                    predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) \n                else : \n                    predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( raw_input = response ) , name = 'predicted_linear_response' ) \n            else : \n                predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) \n        else : \n            predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) \n    return [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ] "}
{"895": "\ndef num_cols ( x ) : \n    if tf . compat . dimension_value ( x . shape [ - 1 ] ) is not None : \n        return tf . compat . dimension_value ( x . shape [ - 1 ] ) \n    return tf . shape ( raw_input = x ) [ - 1 ] "}
{"899": "\ndef rank_from_shape ( shape_tensor_fn , tensorshape = None ) : \n    if tensorshape is None : \n        shape_tensor = ( shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) \n        if ( hasattr ( shape_tensor , 'shape' ) and hasattr ( shape_tensor . shape , 'num_elements' ) ) : \n            ndims_ = tensorshape_util . num_elements ( shape_tensor . shape ) \n        else : \n            ndims_ = len ( shape_tensor ) \n        ndims_fn = lambda : tf . size ( raw_input = shape_tensor ) \n    else : \n        ndims_ = tensorshape_util . rank ( tensorshape ) \n        ndims_fn = lambda : tf . size ( raw_input = shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) \n    return ndims_fn ( ) if ndims_ is None else ndims_ "}
{"904": "\ndef make_diag_scale ( loc = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None , dtype = None ) : \n    def _maybe_attach_assertion ( x ) : \n        if not validate_args : \n            return x \n        if assert_positive : \n            return with_dependencies ( [ assert_util . assert_positive ( x , message = \"diagonal part must be positive\" ) , ] , x ) \n        return with_dependencies ( [ assert_util . assert_none_equal ( x , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) ] , x ) \n    with tf . name_scope ( name or \"make_diag_scale\" ) : \n        if dtype is None : \n            dtype = dtype_util . common_dtype ( [ loc , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) \n        loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) \n        scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) \n        scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) \n        if scale_diag is not None : \n            if scale_identity_multiplier is not None : \n                scale_diag += scale_identity_multiplier [ ... , tf . newaxis ] \n            return tf . linalg . LinearOperatorDiag ( diag = _maybe_attach_assertion ( scale_diag ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive ) \n        if loc is None and shape_hint is None : \n            raise ValueError ( \"Cannot infer `event_shape` unless `loc` or \" \"`shape_hint` is specified.\" ) \n        num_rows = shape_hint \n        del shape_hint \n        if num_rows is None : \n            num_rows = tf . compat . dimension_value ( loc . shape [ - 1 ] ) \n            if num_rows is None : \n                num_rows = tf . shape ( raw_input = loc ) [ - 1 ] \n        if scale_identity_multiplier is None : \n            return tf . linalg . LinearOperatorIdentity ( num_rows = num_rows , dtype = dtype , is_self_adjoint = True , is_positive_definite = True , assert_proper_shapes = validate_args ) \n        return tf . linalg . LinearOperatorScaledIdentity ( num_rows = num_rows , multiplier = _maybe_attach_assertion ( scale_identity_multiplier ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive , assert_proper_shapes = validate_args ) "}
{"905": "\ndef shapes_from_loc_and_scale ( loc , scale , name = \"shapes_from_loc_and_scale\" ) : \n    if loc is not None and tensorshape_util . rank ( loc . shape ) == 0 : \n        loc = None \n    with tf . name_scope ( name ) : \n        event_size = scale . range_dimension_tensor ( ) \n        event_size_ = tf . get_static_value ( event_size ) \n        loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - 1 ] ) ) \n        if event_size_ is not None and loc_event_size_ is not None : \n            if loc_event_size_ != 1 and loc_event_size_ != event_size_ : \n                raise ValueError ( \"Event size of 'scale' ({}) could not be broadcast up to that \" \"of 'loc' ({}).\" . format ( event_size_ , loc_event_size_ ) ) \n        elif loc_event_size_ is not None and loc_event_size_ != 1 : \n            event_size_ = loc_event_size_ \n        if event_size_ is None : \n            event_shape = event_size [ tf . newaxis ] \n        else : \n            event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ 1 ] ) , dtype = tf . int32 , name = \"event_shape\" ) \n        batch_shape = scale . batch_shape_tensor ( ) \n        if loc is not None : \n            loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , 1 ) [ : - 1 ] \n            if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : \n                loc_batch_shape = tf . shape ( raw_input = loc ) [ : - 1 ] \n            else : \n                loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = \"loc_batch_shape\" ) \n            batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) \n            batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = \"batch_shape\" ) \n        return batch_shape , event_shape "}
{"908": "\ndef pad_mixture_dimensions ( x , mixture_distribution , categorical_distribution , event_ndims ) : \n    with tf . name_scope ( \"pad_mix_dims\" ) : \n        def _get_ndims ( d ) : \n            if tensorshape_util . rank ( d . batch_shape ) is not None : \n                return tensorshape_util . rank ( d . batch_shape ) \n            return tf . shape ( raw_input = d . batch_shape_tensor ( ) ) [ 0 ] \n        dist_batch_ndims = _get_ndims ( mixture_distribution ) \n        cat_batch_ndims = _get_ndims ( categorical_distribution ) \n        pad_ndims = tf . where ( categorical_distribution . is_scalar_batch ( ) , dist_batch_ndims , dist_batch_ndims - cat_batch_ndims ) \n        s = tf . shape ( raw_input = x ) \n        x = tf . reshape ( x , shape = tf . concat ( [ s [ : - 1 ] , tf . ones ( [ pad_ndims ] , dtype = tf . int32 ) , s [ - 1 : ] , tf . ones ( [ event_ndims ] , dtype = tf . int32 ) , ] , axis = 0 ) ) \n        return x "}
{"912": "\ndef same_dynamic_shape ( a , b ) : \n    a = tf . convert_to_tensor ( value = a , name = \"a\" ) \n    b = tf . convert_to_tensor ( value = b , name = \"b\" ) \n    def all_shapes_equal ( ) : \n        return tf . reduce_all ( input_tensor = tf . equal ( tf . concat ( [ tf . shape ( raw_input = a ) , tf . shape ( raw_input = b ) ] , 0 ) , tf . concat ( [ tf . shape ( raw_input = b ) , tf . shape ( raw_input = a ) ] , 0 ) ) ) \n    return tf . cond ( pred = tf . equal ( tf . rank ( a ) , tf . rank ( b ) ) , true_fn = all_shapes_equal , false_fn = lambda : tf . constant ( False ) ) "}
{"919": "\ndef embed_check_categorical_event_shape ( categorical_param , name = \"embed_check_categorical_event_shape\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = categorical_param , name = \"categorical_param\" ) \n        x_dtype = dtype_util . base_dtype ( x . dtype ) \n        max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else 0 ) \n        if max_event_size is 0 : \n            raise TypeError ( \"Unable to validate size of unrecognized dtype \" \"({}).\" . format ( dtype_util . name ( x_dtype ) ) ) \n        try : \n            x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) \n        except ValueError : \n            raise ValueError ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) \n        event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) \n        if event_size is not None : \n            if event_size < 2 : \n                raise ValueError ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) \n            if event_size > max_event_size : \n                raise ValueError ( \"Number of classes exceeds `dtype` precision, i.e., \" \"{} implies shape ({}) cannot exceed {}.\" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) \n            return x \n        else : \n            event_size = tf . shape ( raw_input = x , out_type = tf . int64 , name = \"x_shape\" ) [ - 1 ] \n            return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) ) , assert_util . assert_greater_equal ( tf . shape ( raw_input = x ) [ - 1 ] , 2 , message = ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = \"Number of classes exceeds `dtype` precision, \" \"i.e., {} dtype cannot exceed {} shape.\" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x ) "}
{"922": "\ndef pick_vector ( cond , true_vector , false_vector , name = \"pick_vector\" ) : \n    with tf . name_scope ( name ) : \n        cond = tf . convert_to_tensor ( value = cond , dtype_hint = tf . bool , name = \"cond\" ) \n        if cond . dtype != tf . bool : \n            raise TypeError ( \"{}.dtype={} which is not {}\" . format ( cond , cond . dtype , tf . bool ) ) \n        true_vector = tf . convert_to_tensor ( value = true_vector , name = \"true_vector\" ) \n        false_vector = tf . convert_to_tensor ( value = false_vector , name = \"false_vector\" ) \n        if true_vector . dtype != false_vector . dtype : \n            raise TypeError ( \"{}.dtype={} does not match {}.dtype={}\" . format ( true_vector , true_vector . dtype , false_vector , false_vector . dtype ) ) \n        cond_value_static = tf . get_static_value ( cond ) \n        if cond_value_static is not None : \n            return true_vector if cond_value_static else false_vector \n        n = tf . shape ( raw_input = true_vector ) [ 0 ] \n        return tf . slice ( tf . concat ( [ true_vector , false_vector ] , 0 ) , [ tf . where ( cond , 0 , n ) ] , [ tf . where ( cond , n , - 1 ) ] ) "}
{"925": "\ndef tridiag ( below = None , diag = None , above = None , name = None ) : \n    def _pad ( x ) : \n        shape = tf . concat ( [ tf . shape ( raw_input = x ) [ : - 1 ] , [ 1 ] ] , axis = 0 ) \n        z = tf . zeros ( shape , dtype = x . dtype ) \n        return tf . concat ( [ z , x , z ] , axis = - 1 ) \n    def _add ( * x ) : \n        s = None \n        for y in x : \n            if y is None : \n                continue \n            elif s is None : \n                s = y \n            else : \n                s += y \n        if s is None : \n            raise ValueError ( \"Must specify at least one of `below`, `diag`, `above`.\" ) \n        return s \n    with tf . name_scope ( name or \"tridiag\" ) : \n        if below is not None : \n            below = tf . convert_to_tensor ( value = below , name = \"below\" ) \n            below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - 1 , 1 : ] \n        if diag is not None : \n            diag = tf . convert_to_tensor ( value = diag , name = \"diag\" ) \n            diag = tf . linalg . diag ( diag ) \n        if above is not None : \n            above = tf . convert_to_tensor ( value = above , name = \"above\" ) \n            above = tf . linalg . diag ( _pad ( above ) ) [ ... , 1 : , : - 1 ] \n        return _add ( below , diag , above ) "}
{"926": "\ndef dimension_size ( x , axis ) : \n    s = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , np . abs ( axis ) ) [ axis ] ) \n    if s is not None : \n        return s \n    return tf . shape ( raw_input = x ) [ axis ] "}
{"929": "\ndef expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : \n    with tf . name_scope ( op_name or \"expand_to_vector\" ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is None : \n            if validate_args : \n                x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = \"Input is neither scalar nor vector.\" ) ] , x ) \n            ndims = tf . rank ( x ) \n            expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( raw_input = x ) ) \n            return tf . reshape ( x , expanded_shape ) \n        elif ndims == 0 : \n            x_const = tf . get_static_value ( x ) \n            if x_const is not None : \n                return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) \n            else : \n                return tf . reshape ( x , [ 1 ] ) \n        elif ndims != 1 : \n            raise ValueError ( \"Input is neither scalar nor vector.\" ) \n        return x "}
{"932": "\ndef _maybe_validate_perm ( perm , validate_args , name = None ) : \n    with tf . name_scope ( name or 'maybe_validate_perm' ) : \n        assertions = [ ] \n        if not dtype_util . is_integer ( perm . dtype ) : \n            raise TypeError ( '`perm` must be integer type' ) \n        msg = '`perm` must be a vector.' \n        if tensorshape_util . rank ( perm . shape ) is not None : \n            if tensorshape_util . rank ( perm . shape ) != 1 : \n                raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_rank ( perm , 1 , message = msg ) ] \n        perm_ = tf . get_static_value ( perm ) \n        msg = '`perm` must be a valid permutation vector.' \n        if perm_ is not None : \n            if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : \n                raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) \n        elif validate_args : \n            assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( raw_input = perm ) ) , message = msg ) ] \n        return assertions "}
{"942": "\ndef build_kalman_filter_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : \n    def kalman_filter_step ( state , elems_t ) : \n        if isinstance ( elems_t , tuple ) : \n            x_t , mask_t = elems_t \n        else : \n            x_t = elems_t \n            mask_t = None \n        observation_matrix = get_observation_matrix_for_timestep ( state . timestep ) \n        observation_noise = get_observation_noise_for_timestep ( state . timestep ) \n        if mask_t is not None : \n            x_expected = _propagate_mean ( state . predicted_mean , observation_matrix , observation_noise ) * tf . ones_like ( x_t ) \n            x_t = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( raw_input = x_expected ) ) , x_expected , tf . broadcast_to ( x_t , tf . shape ( raw_input = x_expected ) ) ) \n        ( filtered_mean , filtered_cov , observation_dist ) = linear_gaussian_update ( state . predicted_mean , state . predicted_cov , observation_matrix , observation_noise , x_t ) \n        log_marginal_likelihood = observation_dist . log_prob ( x_t [ ... , 0 ] ) \n        if mask_t is not None : \n            filtered_mean = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( raw_input = filtered_mean ) ) , state . predicted_mean , filtered_mean ) \n            filtered_cov = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( raw_input = filtered_cov ) ) , state . predicted_cov , filtered_cov ) \n            log_marginal_likelihood = tf . where ( tf . broadcast_to ( mask_t [ ... , 0 , 0 ] , tf . shape ( raw_input = log_marginal_likelihood ) ) , tf . zeros_like ( log_marginal_likelihood ) , log_marginal_likelihood ) \n        predicted_mean , predicted_cov = kalman_transition ( filtered_mean , filtered_cov , get_transition_matrix_for_timestep ( state . timestep ) , get_transition_noise_for_timestep ( state . timestep ) ) \n        return KalmanFilterState ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , observation_dist . mean ( ) [ ... , tf . newaxis ] , observation_dist . covariance ( ) , log_marginal_likelihood , state . timestep + 1 ) \n    return kalman_filter_step "}
{"950": "\ndef backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : \n    with tf . name_scope ( \"backward_pass\" ) : \n        filtered_means = tf . convert_to_tensor ( value = filtered_means , name = \"filtered_means\" ) \n        filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = \"filtered_covs\" ) \n        predicted_means = tf . convert_to_tensor ( value = predicted_means , name = \"predicted_means\" ) \n        predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = \"predicted_covs\" ) \n        filtered_means = distribution_util . move_dimension ( filtered_means , - 2 , 0 ) \n        filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3 , 0 ) \n        predicted_means = distribution_util . move_dimension ( predicted_means , - 2 , 0 ) \n        predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3 , 0 ) \n        filtered_means = filtered_means [ ... , tf . newaxis ] \n        predicted_means = predicted_means [ ... , tf . newaxis ] \n        initial_backward_mean = predicted_means [ - 1 , ... ] \n        initial_backward_cov = predicted_covs [ - 1 , ... ] \n        num_timesteps = tf . shape ( raw_input = filtered_means ) [ 0 ] \n        initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - 1 ) \n        update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) \n        posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = True ) \n        posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , 0 ] , 0 , - 2 ) \n        posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , 0 , - 3 ) \n        return ( posterior_means , posterior_covs ) "}
{"965": "\ndef _choose_base_case ( is_accepted , accepted , rejected , name = None ) : \n    def _expand_is_accepted_like ( x ) : \n        with tf . compat . v1 . name_scope ( 'expand_is_accepted_like' ) : \n            expand_shape = tf . concat ( [ tf . shape ( raw_input = is_accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) \n            multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , tf . shape ( raw_input = x ) [ tf . rank ( is_accepted ) : ] , ] , axis = 0 ) \n            m = tf . tile ( tf . reshape ( is_accepted , expand_shape ) , multiples ) \n            m . set_shape ( m . shape . merge_with ( x . shape ) ) \n            return m \n    def _where ( accepted , rejected ) : \n        if accepted is rejected : \n            return accepted \n        accepted = tf . convert_to_tensor ( value = accepted , name = 'accepted' ) \n        rejected = tf . convert_to_tensor ( value = rejected , name = 'rejected' ) \n        r = tf . where ( _expand_is_accepted_like ( accepted ) , accepted , rejected ) \n        r . set_shape ( r . shape . merge_with ( accepted . shape . merge_with ( rejected . shape ) ) ) \n        return r \n    with tf . compat . v1 . name_scope ( name , 'choose' , values = [ is_accepted , accepted , rejected ] ) : \n        if not is_list_like ( accepted ) : \n            return _where ( accepted , rejected ) \n        return [ ( choose ( is_accepted , a , r , name = name ) if is_namedtuple_like ( a ) else _where ( a , r ) ) for a , r in zip ( accepted , rejected ) ] "}
{"967": "\ndef safe_sum ( x , alt_value = - np . inf , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'safe_sum' , [ x , alt_value ] ) : \n        if not is_list_like ( x ) : \n            raise TypeError ( 'Expected list input.' ) \n        if not x : \n            raise ValueError ( 'Input should not be empty.' ) \n        in_shape = x [ 0 ] . shape \n        x = tf . stack ( x , axis = - 1 ) \n        x = tf . reduce_sum ( input_tensor = x , axis = - 1 ) \n        alt_value = np . array ( alt_value , x . dtype . as_numpy_dtype ) \n        alt_fill = tf . fill ( tf . shape ( raw_input = x ) , value = alt_value ) \n        x = tf . where ( tf . math . is_finite ( x ) , x , alt_fill ) \n        x . set_shape ( x . shape . merge_with ( in_shape ) ) \n        return x "}
{"971": "\ndef trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : \n        if vs . caching_device is None and not tf . executing_eagerly ( ) : \n            vs . set_caching_device ( lambda op : op . device ) \n        initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) \n        elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) \n        static_length = elems . shape [ 0 ] \n        if tf . compat . dimension_value ( static_length ) is None : \n            length = tf . shape ( raw_input = elems ) [ 0 ] \n        else : \n            length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) \n        elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ 1 : ] ) \n        elems_array = elems_array . unstack ( elems ) \n        trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) \n        def _body ( i , state , trace_arrays ) : \n            state = loop_fn ( state , elems_array . read ( i ) ) \n            trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) \n            return i + 1 , state , trace_arrays \n        _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : i < length , body = _body , loop_vars = ( 0 , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) \n        stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) \n        def _merge_static_length ( x ) : \n            x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ 1 : ] ) ) \n            return x \n        stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) \n        return final_state , stacked_trace "}
{"975": "\ndef _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : \n    output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) \n    validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) \n    if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : \n        with tf . control_dependencies ( validation_dependencies ) : \n            output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) \n        return output_shape , output_tensorshape \n    with tf . control_dependencies ( validation_dependencies ) : \n        event_shape_in_ndims = ( tf . size ( raw_input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) \n        input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) \n    additional_assertions = [ ] \n    if is_validated : \n        pass \n    elif validate_args : \n        mask = event_shape_in >= 0 \n        explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) \n        explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) \n        additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) \n    with tf . control_dependencies ( additional_assertions ) : \n        output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) \n    return output_shape , output_tensorshape "}
{"986": "\ndef default_exchange_proposed_fn ( prob_exchange ) : \n    def default_exchange_proposed_fn_ ( num_replica , seed = None ) : \n        seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) \n        zero_start = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) > 0.5 \n        if num_replica % 2 == 0 : \n            def _exchange ( ) : \n                flat_exchange = tf . range ( num_replica ) \n                if num_replica > 2 : \n                    start = tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                    end = num_replica - start \n                    flat_exchange = flat_exchange [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( raw_input = flat_exchange ) // 2 , 2 ] ) \n        else : \n            def _exchange ( ) : \n                start = tf . cast ( zero_start , dtype = tf . int32 ) \n                end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                flat_exchange = tf . range ( num_replica ) [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( raw_input = flat_exchange ) // 2 , 2 ] ) \n        def _null_exchange ( ) : \n            return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ 0 , 2 ] ) \n        return tf . cond ( pred = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) < prob_exchange , true_fn = _exchange , false_fn = _null_exchange ) \n    return default_exchange_proposed_fn_ "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        for replica in range ( self . num_replica ) : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n        target_log_probs = tf . stack ( target_log_probs , axis = 0 ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ 0 ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( raw_input = target_log_probs ) [ 1 : ] ) , axis = 0 ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return i < exchange_proposed_n \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio \n            for k in range ( num_state_parts ) : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n            return i + 1 , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ] "}
{"1005": "\ndef make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : \n    if step_counter is None and num_adaptation_steps is not None : \n        step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - 1 , dtype = np . int32 ) , dtype = tf . int32 , trainable = False , use_resource = True ) \n    def step_size_simple_update_fn ( step_size_var , kernel_results ) : \n        if kernel_results is None : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ tf . identity ( ss ) for ss in step_size_var ] \n            return tf . identity ( step_size_var ) \n        log_n = tf . math . log ( tf . cast ( tf . size ( raw_input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) \n        log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n \n        adjustment = tf . where ( log_mean_accept_ratio < tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) \n        def build_assign_op ( ) : \n            if mcmc_util . is_list_like ( step_size_var ) : \n                return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] \n            return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) \n        if num_adaptation_steps is None : \n            return build_assign_op ( ) \n        else : \n            with tf . control_dependencies ( [ step_counter . assign_add ( 1 ) ] ) : \n                return tf . cond ( pred = step_counter < num_adaptation_steps , true_fn = build_assign_op , false_fn = lambda : step_size_var ) \n    return step_size_simple_update_fn "}
{"1028": "\ndef _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : \n    assertions = [ ] \n    if not _is_iterable ( distributions ) or not distributions : \n        raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) \n    if dtype_override is None : \n        dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] \n        if dts [ 1 : ] != dts [ : - 1 ] : \n            raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) \n    for d in distributions : \n        if tensorshape_util . rank ( d . event_shape ) is not None : \n            if tensorshape_util . rank ( d . event_shape ) != 1 : \n                raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) \n        elif validate_args : \n            assertions . append ( assert_util . assert_equal ( 1 , tf . size ( raw_input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) \n    batch_shapes = [ d . batch_shape for d in distributions ] \n    if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : \n        if batch_shapes [ 1 : ] != batch_shapes [ : - 1 ] : \n            raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) \n    elif validate_args : \n        batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] \n        assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) \n    return assertions "}
{"1032": "\ndef count_integers ( arr , weights = None , minlength = None , maxlength = None , axis = None , dtype = tf . int32 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'count_integers' , values = [ arr , weights , minlength , maxlength , axis ] ) : \n        if axis is None : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        arr = tf . convert_to_tensor ( value = arr , dtype = tf . int32 , name = 'arr' ) \n        arr_ndims = _get_static_ndims ( arr , expect_static = True ) \n        axis = _make_static_axis_non_negative_list ( axis , arr_ndims ) \n        not_axis = sorted ( set ( range ( arr_ndims ) ) . difference ( axis ) ) \n        if not not_axis : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        flat_arr = _move_dims_to_flat_end ( arr , not_axis , arr_ndims , right_end = False ) \n        if weights is None : \n            def one_bincount ( arr_slice ) : \n                return tf . math . bincount ( arr_slice , weights = None , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = flat_arr , dtype = dtype ) \n        else : \n            weights = tf . convert_to_tensor ( value = weights , name = 'weights' ) \n            _get_static_ndims ( weights , expect_static = True , expect_ndims = arr_ndims ) \n            flat_weights = _move_dims_to_flat_end ( weights , not_axis , arr_ndims , right_end = False ) \n            def one_bincount ( arr_and_weights_slices ) : \n                arr_slice , weights_slice = arr_and_weights_slices \n                return tf . math . bincount ( arr_slice , weights = weights_slice , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = [ flat_arr , flat_weights ] , dtype = weights . dtype ) \n        flat_counts_t = tf . transpose ( a = flat_counts , perm = [ 1 , 0 ] ) \n        _get_static_ndims ( flat_counts_t , expect_ndims = 2 , expect_static = True ) \n        not_axis_shape = tf . gather ( tf . shape ( raw_input = arr ) , indices = not_axis ) \n        out_shape = tf . concat ( [ [ - 1 ] , not_axis_shape ] , axis = 0 ) \n        return tf . reshape ( flat_counts_t , out_shape ) "}
{"1033": "\ndef find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : \n        in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) \n        if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and tf . compat . dimension_value ( edges . shape [ 0 ] ) < 2 ) : \n            raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) \n        flattening_x = edges . shape . ndims == 1 and x . shape . ndims > 1 \n        if flattening_x : \n            x_orig_shape = tf . shape ( raw_input = x ) \n            x = tf . reshape ( x , [ - 1 ] ) \n        if dtype is None : \n            dtype = in_type \n        dtype = tf . as_dtype ( dtype ) \n        x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) \n        edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) \n        searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None \n        almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) \n        almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) \n        bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( raw_input = edges ) [ 0 ] - 2 , dtype ) ) \n        if not extend_lower_interval : \n            low_fill = np . nan if dtype . is_floating else - 1 \n            bins = tf . where ( x < tf . expand_dims ( edges [ 0 ] , 0 ) , tf . fill ( tf . shape ( raw_input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) \n        if not extend_upper_interval : \n            up_fill = np . nan if dtype . is_floating else tf . shape ( raw_input = edges ) [ 0 ] - 1 \n            bins = tf . where ( x > tf . expand_dims ( edges [ - 1 ] , 0 ) , tf . fill ( tf . shape ( raw_input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) \n        if flattening_x : \n            bins = tf . reshape ( bins , x_orig_shape ) \n        return bins "}
{"1034": "\ndef histogram ( x , edges , axis = None , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : \n        in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) \n        if axis is None : \n            x = tf . reshape ( x , shape = [ - 1 ] ) \n        else : \n            x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = 1 ) \n            axis = _make_static_axis_non_negative_list ( axis , x_ndims ) \n            if not axis : \n                raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) \n            x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = False ) \n        bins = find_bins ( x , edges = edges , extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) \n        counts = count_integers ( bins , minlength = tf . shape ( raw_input = edges ) [ 0 ] - 1 , maxlength = tf . shape ( raw_input = edges ) [ 0 ] - 1 , axis = 0 , dtype = dtype or in_dtype ) \n        n_edges = tf . compat . dimension_value ( edges . shape [ 0 ] ) \n        if n_edges is not None : \n            counts . set_shape ( tf . TensorShape ( [ n_edges - 1 ] ) . concatenate ( counts . shape [ 1 : ] ) ) \n        return counts "}
{"1036": "\ndef _get_static_ndims ( x , expect_static = False , expect_ndims = None , expect_ndims_no_more_than = None , expect_ndims_at_least = None ) : \n    ndims = x . shape . ndims \n    if ndims is None : \n        shape_const = tf . get_static_value ( tf . shape ( raw_input = x ) ) \n        if shape_const is not None : \n            ndims = shape_const . ndim \n    if ndims is None : \n        if expect_static : \n            raise ValueError ( 'Expected argument `x` to have statically defined `ndims`.  Found: ' % x ) \n        return \n    if expect_ndims is not None : \n        ndims_message = ( 'Expected argument `x` to have ndims %s.  Found tensor %s' % ( expect_ndims , x ) ) \n        if ndims != expect_ndims : \n            raise ValueError ( ndims_message ) \n    if expect_ndims_at_least is not None : \n        ndims_at_least_message = ( 'Expected argument `x` to have ndims >= %d.  Found tensor %s' % ( expect_ndims_at_least , x ) ) \n        if ndims < expect_ndims_at_least : \n            raise ValueError ( ndims_at_least_message ) \n    if expect_ndims_no_more_than is not None : \n        ndims_no_more_than_message = ( 'Expected argument `x` to have ndims <= %d.  Found tensor %s' % ( expect_ndims_no_more_than , x ) ) \n        if ndims > expect_ndims_no_more_than : \n            raise ValueError ( ndims_no_more_than_message ) \n    return ndims "}
{"1039": "\ndef _move_dims_to_flat_end ( x , axis , x_ndims , right_end = True ) : \n    if not axis : \n        return x \n    other_dims = sorted ( set ( range ( x_ndims ) ) . difference ( axis ) ) \n    perm = other_dims + list ( axis ) if right_end else list ( axis ) + other_dims \n    x_permed = tf . transpose ( a = x , perm = perm ) \n    if x . shape . is_fully_defined ( ) : \n        x_shape = x . shape . as_list ( ) \n        other_shape = [ x_shape [ i ] for i in other_dims ] \n        end_shape = [ np . prod ( [ x_shape [ i ] for i in axis ] ) ] \n        full_shape = ( other_shape + end_shape if right_end else end_shape + other_shape ) \n    else : \n        other_shape = tf . gather ( tf . shape ( raw_input = x ) , other_dims ) \n        full_shape = tf . concat ( [ other_shape , [ - 1 ] ] if right_end else [ [ - 1 ] , other_shape ] , axis = 0 ) \n    return tf . reshape ( x_permed , shape = full_shape ) "}
{"1040": "\ndef _sort_tensor ( tensor ) : \n    sorted_ , _ = tf . nn . top_k ( tensor , k = tf . shape ( raw_input = tensor ) [ - 1 ] ) \n    sorted_ . set_shape ( tensor . shape ) \n    return sorted_ "}
{"1055": "\ndef csiszar_vimco_helper ( logu , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"csiszar_vimco_helper\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( 1 ) [ 0 ] ) \n        if n is None : \n            n = tf . shape ( raw_input = logu ) [ 0 ] \n            log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) \n            nm1 = tf . cast ( n - 1 , dtype = logu . dtype ) \n        else : \n            log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) \n            nm1 = np . asarray ( n - 1 , dtype = logu . dtype . as_numpy_dtype ) \n        log_max_u = tf . reduce_max ( input_tensor = logu , axis = 0 ) \n        log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = 0 ) \n        d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) \n        d_ok = tf . not_equal ( d , 0. ) \n        safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) \n        d_ok_result = logu + tfd . softplus_inverse ( safe_d ) \n        inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) \n        is_positive_and_largest = tf . logical_and ( logu > 0. , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) \n        log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( raw_input = logu ) , - inf ) , logu ) , axis = 0 , keepdims = True ) \n        log_lomsum_u = tf . tile ( log_lomsum_u , multiples = 1 + tf . pad ( tensor = [ n - 1 ] , paddings = [ [ 0 , tf . rank ( logu ) - 1 ] ] ) ) \n        d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( raw_input = d ) , - inf ) ) \n        log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) \n        looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = 0 ) - logu ) / nm1 \n        log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = 0 ) \n        log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n \n        log_sooavg_u = log_soosum_u - log_n \n        log_avg_u . set_shape ( logu . shape . with_rank_at_least ( 1 ) [ 1 : ] ) \n        log_sooavg_u . set_shape ( logu . shape ) \n        return log_avg_u , log_sooavg_u "}
{"1057": "\ndef _batch_gather_with_broadcast ( params , indices , axis ) : \n    leading_bcast_shape = tf . broadcast_dynamic_shape ( tf . shape ( raw_input = params ) [ : axis ] , tf . shape ( raw_input = indices ) [ : - 1 ] ) \n    params += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( raw_input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) \n    indices += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( raw_input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) \n    return tf . compat . v1 . batch_gather ( params , indices ) "}
{"1058": "\ndef _broadcast_cat_event_and_params ( event , params , base_dtype ) : \n    if dtype_util . is_integer ( event . dtype ) : \n        pass \n    elif dtype_util . is_floating ( event . dtype ) : \n        event = tf . cast ( event , dtype = tf . int32 ) \n    else : \n        raise TypeError ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base_dtype ) ) \n    shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) \n    if not shape_known_statically or params . shape [ : - 1 ] != event . shape : \n        params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) \n        params_shape = tf . shape ( raw_input = params ) [ : - 1 ] \n        event *= tf . ones ( params_shape , dtype = event . dtype ) \n        if tensorshape_util . rank ( params . shape ) is not None : \n            tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) \n    return event , params "}
{"1060": "\ndef _broadcast_event_and_samples ( event , samples , event_ndims ) : \n    samples_shape = tf . concat ( [ tf . shape ( raw_input = samples ) [ : - event_ndims - 1 ] , tf . shape ( raw_input = samples ) [ tf . rank ( samples ) - event_ndims : ] ] , axis = 0 ) \n    event *= tf . ones ( samples_shape , dtype = event . dtype ) \n    event = tf . expand_dims ( event , axis = - event_ndims - 1 ) \n    samples *= tf . ones_like ( event , dtype = samples . dtype ) \n    return event , samples "}
{"1068": "\ndef pad_shape_right_with_ones ( x , ndims ) : \n    if not ( isinstance ( ndims , int ) and ndims >= 0 ) : \n        raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) \n    if ndims == 0 : \n        return x \n    x = tf . convert_to_tensor ( value = x ) \n    original_shape = x . shape \n    new_shape = distribution_util . pad ( tf . shape ( raw_input = x ) , axis = 0 , back = True , value = 1 , count = ndims ) \n    x = tf . reshape ( x , new_shape ) \n    x . set_shape ( original_shape . concatenate ( [ 1 ] * ndims ) ) \n    return x "}
{"1070": "\ndef sqrt_with_finite_grads ( x , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sqrt_with_finite_grads' , [ x ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if not x . dtype . is_floating : \n            raise TypeError ( 'Input `x` must be floating type.' ) \n        def grad ( grad_ys ) : \n            large_float_like_x = np . sqrt ( np . finfo ( x . dtype . as_numpy_dtype ( ) ) . max ) \n            safe_grads = tf . where ( tf . equal ( x , 0 ) , tf . fill ( tf . shape ( raw_input = x ) , large_float_like_x ) , 0.5 * tf . math . rsqrt ( x ) ) \n            return grad_ys * safe_grads \n        return tf . sqrt ( x ) , grad "}
{"1085": "\ndef one_step ( objective_function , population , population_values = None , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'one_step' , [ population , population_values , differential_weight , crossover_prob ] ) : \n        population , _ = _ensure_list ( population ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        population_size = tf . shape ( raw_input = population [ 0 ] ) [ 0 ] \n        seed_stream = distributions . SeedStream ( seed , salt = 'one_step' ) \n        mixing_indices = _get_mixing_indices ( population_size , seed = seed_stream ( ) ) \n        mutants = _get_mutants ( population , population_size , mixing_indices , differential_weight ) \n        candidates = _binary_crossover ( population , population_size , mutants , crossover_prob , seed = seed_stream ( ) ) \n        candidate_values = objective_function ( * candidates ) \n        if population_values is None : \n            population_values = objective_function ( * population ) \n        infinity = tf . zeros_like ( population_values ) + np . inf \n        population_values = tf . where ( tf . math . is_nan ( population_values ) , x = infinity , y = population_values ) \n        to_replace = candidate_values < population_values \n        next_population = [ tf . where ( to_replace , x = candidates_part , y = population_part ) for candidates_part , population_part in zip ( candidates , population ) ] \n        next_values = tf . where ( to_replace , x = candidate_values , y = population_values ) \n    return next_population , next_values "}
{"1090": "\ndef _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed ) : \n    if initial_population is not None : \n        return [ tf . convert_to_tensor ( value = part ) for part in initial_population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'get_starting_population' ) \n    population = [ ] \n    for part in initial_position : \n        part = tf . convert_to_tensor ( value = part ) \n        part_event_shape = tf . shape ( raw_input = part ) \n        population_part_shape = tf . concat ( [ [ population_size - 1 ] , part_event_shape ] , axis = 0 ) \n        population_part = tf . random . normal ( population_part_shape , stddev = population_stddev , dtype = part . dtype . base_dtype , seed = seed_stream ( ) ) \n        population_part += part \n        population_part = tf . concat ( [ [ part ] , population_part ] , axis = 0 ) \n        population . append ( population_part ) \n    return population "}
{"1091": "\ndef _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : \n    sizes = [ tf . cast ( tf . size ( raw_input = x ) , dtype = tf . float64 ) for x in population ] \n    seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) \n    force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , 1 ] , seed = seed_stream ( ) ) \n    recombinants = [ ] \n    for i , population_part in enumerate ( population ) : \n        pop_part_flat = tf . reshape ( population_part , [ population_size , - 1 ] ) \n        mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - 1 ] ) \n        part_size = tf . size ( raw_input = population_part ) // population_size \n        force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = 0 , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = True , off_value = False , dtype = tf . bool ) \n        group_mask = tf . math . equal ( force_crossover_group , i ) \n        force_crossovers &= group_mask \n        do_binary_crossover = tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) < crossover_prob \n        do_binary_crossover |= force_crossovers \n        recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) \n        recombinant = tf . reshape ( recombinant_flat , tf . shape ( raw_input = population_part ) ) \n        recombinants . append ( recombinant ) \n    return recombinants "}
{"1100": "\ndef convert_to_string ( self , productions ) : \n    symbols = [ ] \n    for production in tf . unstack ( productions , axis = 1 ) : \n        lhs , rhs = self . production_rules [ tf . argmax ( raw_input = production , axis = - 1 ) ] \n        if not symbols : \n            if lhs != self . start_symbol : \n                raise ValueError ( \"`productions` must begin with `self.start_symbol`.\" ) \n            symbols = rhs \n        else : \n            index = symbols . index ( lhs ) \n            symbols = symbols [ : index ] + rhs + symbols [ index + 1 : ] \n    string = \"\" . join ( symbols ) \n    return string "}
{"1101": "\ndef call ( self , inputs ) : \n    del inputs \n    latent_code = ed . MultivariateNormalDiag ( loc = tf . zeros ( self . latent_size ) , sample_shape = 1 , name = \"latent_code\" ) \n    state = self . lstm . zero_state ( 1 , dtype = tf . float32 ) \n    t = 0 \n    productions = [ ] \n    stack = [ self . grammar . start_symbol ] \n    while stack : \n        symbol = stack . pop ( ) \n        net , state = self . lstm ( latent_code , state ) \n        logits = ( self . output_layer ( net ) + self . grammar . mask ( symbol , on_value = 0. , off_value = - 1e9 ) ) \n        production = ed . OneHotCategorical ( logits = logits , name = \"production_\" + str ( t ) ) \n        _ , rhs = self . grammar . production_rules [ tf . argmax ( raw_input = production , axis = - 1 ) ] \n        for symbol in rhs : \n            if symbol in self . grammar . nonterminal_symbols : \n                stack . append ( symbol ) \n        productions . append ( production ) \n        t += 1 \n    return tf . stack ( productions , axis = 1 ) "}
{"1105": "\ndef matrix_rank ( a , tol = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : \n        a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        s = tf . linalg . svd ( a , compute_uv = False ) \n        if tol is None : \n            if a . shape [ - 2 : ] . is_fully_defined ( ) : \n                m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) \n            else : \n                m = tf . reduce_max ( input_tensor = tf . shape ( raw_input = a ) [ - 2 : ] ) \n            eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps \n            tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = True ) ) \n        return tf . reduce_sum ( input_tensor = tf . cast ( s > tol , tf . int32 ) , axis = - 1 ) "}
{"1106": "\ndef pinv ( a , rcond = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : \n        a = tf . convert_to_tensor ( value = a , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        dtype = a . dtype . as_numpy_dtype \n        if rcond is None : \n            def get_dim_size ( dim ) : \n                if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : \n                    return tf . compat . dimension_value ( a . shape [ dim ] ) \n                return tf . shape ( raw_input = a ) [ dim ] \n            num_rows = get_dim_size ( - 2 ) \n            num_cols = get_dim_size ( - 1 ) \n            if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : \n                max_rows_cols = float ( max ( num_rows , num_cols ) ) \n            else : \n                max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) \n            rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps \n        rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) \n        [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) \n        cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) \n        singular_values = tf . where ( singular_values > cutoff [ ... , tf . newaxis ] , singular_values , tf . fill ( tf . shape ( raw_input = singular_values ) , np . array ( np . inf , dtype ) ) ) \n        a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) \n        if a . shape . ndims is not None : \n            a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2 ] ] ) ) \n        return a_pinv "}
{"1107": "\ndef lu_solve ( lower_upper , perm , rhs , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) \n        assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n                rhs = tf . identity ( rhs ) \n        if rhs . shape . ndims == 2 and perm . shape . ndims == 1 : \n            permuted_rhs = tf . gather ( rhs , perm , axis = - 2 ) \n        else : \n            rhs_shape = tf . shape ( raw_input = rhs ) \n            broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2 ] , tf . shape ( raw_input = perm ) [ : - 1 ] ) \n            d , m = rhs_shape [ - 2 ] , rhs_shape [ - 1 ] \n            rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = 0 ) \n            broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) \n            broadcast_rhs = tf . reshape ( broadcast_rhs , [ - 1 , d , m ] ) \n            broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - 1 ] ) \n            broadcast_perm = tf . reshape ( broadcast_perm , [ - 1 , d ] ) \n            broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) \n            broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) \n            broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - 1 ) \n            permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) \n            permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) \n        lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( tf . shape ( raw_input = lower_upper ) [ : - 1 ] , dtype = lower_upper . dtype ) ) \n        return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = False ) "}
{"1108": "\ndef lu_matrix_inverse ( lower_upper , perm , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_matrix_inverse' , [ lower_upper , perm ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n        shape = tf . shape ( raw_input = lower_upper ) \n        return lu_solve ( lower_upper , perm , rhs = tf . eye ( shape [ - 1 ] , batch_shape = shape [ : - 2 ] , dtype = lower_upper . dtype ) , validate_args = False ) "}
{"1109": "\ndef _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : \n    assertions = [ ] \n    message = 'Input `lower_upper` must have at least 2 dimensions.' \n    if lower_upper . shape . ndims is not None : \n        if lower_upper . shape . ndims < 2 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) \n    message = '`rank(lower_upper)` must equal `rank(perm) + 1`' \n    if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : \n        if lower_upper . shape . ndims != perm . shape . ndims + 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) \n    message = '`lower_upper` must be square.' \n    if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : \n        if lower_upper . shape [ - 2 ] != lower_upper . shape [ - 1 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        m , n = tf . split ( tf . shape ( raw_input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) \n        assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) \n    return assertions "}
{"1110": "\ndef _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : \n    assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n    message = 'Input `rhs` must have at least 2 dimensions.' \n    if rhs . shape . ndims is not None : \n        if rhs . shape . ndims < 2 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) \n    message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' \n    if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : \n        if lower_upper . shape [ - 1 ] != rhs . shape [ - 2 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( raw_input = lower_upper ) [ - 1 ] , tf . shape ( raw_input = rhs ) [ - 2 ] , message = message ) ) \n    return assertions "}
{"1113": "\ndef _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : \n    mean , variance , grad_mean = model ( linear_response ) \n    is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) \n    def _mask_if_invalid ( x , mask ) : \n        mask = tf . fill ( tf . shape ( raw_input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) \n        return tf . where ( is_valid , x , mask ) \n    v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) \n    grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) \n    fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) \n    return - grad_log_likelihood , fim_middle "}
{"1123": "\ndef call ( self , x ) : \n    with tf . compat . v2 . name_scope ( self . name or \"AutoregressiveLayer_call\" ) : \n        x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = \"x\" ) \n        input_shape = tf . shape ( raw_input = x ) \n        if tensorshape_util . rank ( x . shape ) == 1 : \n            x = x [ tf . newaxis , ... ] \n        return tf . reshape ( self . _network ( x ) , tf . concat ( [ input_shape , [ self . _params ] ] , axis = 0 ) ) "}
{"1124": "\ndef draw_sample ( num_samples , num_classes , logits , num_trials , dtype , seed ) : \n    with tf . name_scope ( \"multinomial.draw_sample\" ) : \n        num_trials = tf . ones_like ( logits [ ... , 0 ] , dtype = num_trials . dtype ) * num_trials \n        logits = tf . ones_like ( num_trials [ ... , tf . newaxis ] , dtype = logits . dtype ) * logits \n        flat_logits = tf . reshape ( logits , [ - 1 , num_classes ] ) \n        flat_num_trials = num_samples * tf . reshape ( num_trials , [ - 1 ] ) \n        def _sample_one_batch_member ( args ) : \n            logits , num_cat_samples = args [ 0 ] , args [ 1 ] \n            x = tf . random . categorical ( logits [ tf . newaxis , ... ] , num_cat_samples , seed = seed ) \n            x = tf . reshape ( x , shape = [ num_samples , - 1 ] ) \n            x = tf . one_hot ( x , depth = num_classes ) \n            x = tf . reduce_sum ( input_tensor = x , axis = - 2 ) \n            return tf . cast ( x , dtype = dtype ) \n        x = tf . map_fn ( _sample_one_batch_member , [ flat_logits , flat_num_trials ] , dtype = dtype ) \n        x = tf . transpose ( a = x , perm = [ 1 , 0 , 2 ] ) \n        final_shape = tf . concat ( [ [ num_samples ] , tf . shape ( raw_input = num_trials ) , [ num_classes ] ] , axis = 0 ) \n        x = tf . reshape ( x , final_shape ) \n        return x "}
{"1136": "\ndef build_seasonal_transition_matrix ( num_seasons , is_last_day_of_season , dtype , basis_change_matrix = None , basis_change_matrix_inv = None ) : \n    with tf . compat . v1 . name_scope ( 'build_seasonal_transition_matrix' ) : \n        seasonal_permutation = np . concatenate ( [ np . arange ( 1 , num_seasons ) , [ 0 ] ] , axis = 0 ) \n        seasonal_permutation_matrix = tf . constant ( np . eye ( num_seasons ) [ seasonal_permutation ] , dtype = dtype ) \n        if basis_change_matrix is not None : \n            seasonal_permutation_matrix = tf . matmul ( basis_change_matrix , tf . matmul ( seasonal_permutation_matrix , basis_change_matrix_inv ) ) \n        identity_matrix = tf . eye ( tf . shape ( raw_input = seasonal_permutation_matrix ) [ - 1 ] , dtype = dtype ) \n        def seasonal_transition_matrix ( t ) : \n            return tf . linalg . LinearOperatorFullMatrix ( matrix = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , seasonal_permutation_matrix , identity_matrix ) ) \n    return seasonal_transition_matrix "}
{"1175": "\ndef predict ( self , x , batch_size = None , verbose = None , is_distributed = False ) : \n    if batch_size or verbose : \n        raise Exception ( \"we don't support batch_size or verbose for now\" ) \n    if is_distributed : \n        if isinstance ( x , np . ndarray ) : \n            raw_input = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) \n        elif isinstance ( x , RDD ) : \n            raw_input = x \n        return self . bmodel . predict ( raw_input ) \n    else : \n        if isinstance ( x , np . ndarray ) : \n            return self . bmodel . predict_local ( x ) \n    raise Exception ( \"not supported type: %s\" % x ) "}
{"1191": "\ndef get_input_shape ( self ) : \n    raw_input = callBigDlFunc ( self . bigdl_type , \"getInputShape\" , self . value ) \n    return self . __process_shape ( raw_input ) "}
{"1275": "\ndef interact_model ( config : Union [ str , Path , dict ] ) -> None : \n    model = build_model ( config ) \n    while True : \n        args = [ ] \n        for in_x in model . in_x : \n            args . append ( ( raw_input ( '{}::' . format ( in_x ) ) , ) ) \n            if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : \n                return \n        pred = model ( * args ) \n        if len ( model . out_params ) > 1 : \n            pred = zip ( * pred ) \n        print ( '>>' , * pred ) "}
{"1319": "\ndef shutdown ( self , prompt = False ) : \n    if not self . is_running ( ) : \n        return \n    assert_is_type ( prompt , bool ) \n    if prompt : \n        question = \"Are you sure you want to shutdown the H2O instance running at %s (Y/N)? \" % h2o . connection ( ) . base_url \n        response = raw_input ( question ) \n    else : \n        response = \"Y\" \n    if response . lower ( ) in { \"y\" , \"yes\" } : \n        h2o . api ( \"POST /3/Shutdown\" ) \n        h2o . connection ( ) . close ( ) "}
{"2343": "\ndef scatter ( raw_input , devices , streams = None ) : \n    if streams is None : \n        streams = [ None ] * len ( devices ) \n    if isinstance ( raw_input , list ) : \n        chunk_size = ( len ( raw_input ) - 1 ) // len ( devices ) + 1 \n        outputs = [ scatter ( raw_input [ i ] , [ devices [ i // chunk_size ] ] , [ streams [ i // chunk_size ] ] ) for i in range ( len ( raw_input ) ) ] \n        return outputs \n    elif isinstance ( raw_input , torch . Tensor ) : \n        output = raw_input . contiguous ( ) \n        stream = streams [ 0 ] if output . numel ( ) > 0 else None \n        with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : \n            output = output . cuda ( devices [ 0 ] , non_blocking = True ) \n        return output \n    else : \n        raise Exception ( 'Unknown type {}.' . format ( type ( raw_input ) ) ) "}
{"2766": "\ndef raw_input ( self , data ) : \n    self . data = data \n    self . lexer . raw_input ( data ) "}
{"3078": "\ndef as_action_description ( self ) : \n    description = { self . name : { 'href' : self . href_prefix + self . href , 'timeRequested' : self . time_requested , 'status' : self . status , } , } \n    if self . raw_input is not None : \n        description [ self . name ] [ 'input' ] = self . raw_input \n    if self . time_completed is not None : \n        description [ self . name ] [ 'timeCompleted' ] = self . time_completed \n    return description "}
{"4231": "\ndef handle_error ( program_name , cmd , log = None ) : \n    print ( '\\nHouston, we have a problem.' , '\\n%s did not finish successfully. Review the log' % program_name , 'file and the input file(s) to see what went wrong.' ) \n    print ( '%s command: \"%s\"' % ( program_name , cmd ) ) \n    if log is not None : \n        print ( 'log: \"%s\"' % log ) \n    print ( 'Where do we go from here?' ) \n    print ( ' r  - retry running %s (probably after' % program_name , 'you\\'ve fixed any problems with the input files)' ) \n    print ( ' c  - continue on with the script (probably after' , 'you\\'ve manually re-run and generated the desired' , 'output file(s)' ) \n    print ( ' x  - exit, keeping the TEMP3D files and log' ) \n    print ( ' xd - exit, deleting the TEMP3D files and log' ) \n    while True : \n        choice = raw_input ( 'Select r, c, x (default), or xd: ' ) \n        if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : \n            choice = 'x' \n        break \n    if choice == 'x' : \n        print ( 'Exiting ...' ) \n        sys . exit ( 1 ) \n    elif choice == 'xd' : \n        print ( 'Deleting TEMP3D* and log files and exiting ...' ) \n        util . delete_all ( 'TEMP3D*' ) \n        if log is not None : \n            os . remove ( log ) \n        sys . exit ( 1 ) \n    elif choice == 'c' : \n        print ( 'Continuing on ...' ) \n        break_now = True \n    elif choice == 'r' : \n        print ( 'Retrying %s cmd ...' % program_name ) \n        break_now = False \n    return break_now "}
{"4606": "\ndef _load ( self ) : \n    if \"PYFUNCEBLE_AUTO_CONFIGURATION\" not in PyFunceble . environ : \n        while True : \n            response = raw_input ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . RED + \"A configuration key is missing.\\n\" + PyFunceble . Fore . RESET + \"Try to merge upstream configuration file into %s ? [y/n] \" % ( PyFunceble . Style . BRIGHT + self . path_to_config + PyFunceble . Style . RESET_ALL ) ) \n            if isinstance ( response , str ) : \n                if response . lower ( ) == \"y\" : \n                    self . _merge_values ( ) \n                    self . _save ( ) \n                    print ( PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + \"Done!\\n\" \"Please try again, if it happens again,\" \" please fill a new issue.\" ) \n                    break \n                elif response . lower ( ) == \"n\" : \n                    raise Exception ( \"Configuration key still missing.\" ) \n    else : \n        self . _merge_values ( ) \n        self . _save ( ) "}
{"5112": "\ndef output_seed ( seed ) : \n    print ( 'WARNING: Anyone who has your seed can spend your IOTAs! ' 'Clear the screen after recording your seed!' ) \n    compat . raw_input ( '' ) \n    print ( 'Your seed is:' ) \n    print ( '' ) \n    print ( binary_type ( seed ) . decode ( 'ascii' ) ) \n    print ( '' ) \n    print ( 'Clear the screen to prevent shoulder surfing, ' 'and press return to continue.' ) \n    print ( 'https://en.wikipedia.org/wiki/Shoulder_surfing_(computer_security)' ) \n    compat . raw_input ( '' ) "}
{"5137": "\ndef encode ( self , raw_input , errors = 'strict' ) : \n    if isinstance ( raw_input , memoryview ) : \n        raw_input = raw_input . tobytes ( ) \n    if not isinstance ( raw_input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't encode {type}; byte string expected.\" . format ( type = type ( raw_input ) . __name__ , ) ) , context = { 'input' : raw_input , } , ) \n    if not isinstance ( raw_input , bytearray ) : \n        raw_input = bytearray ( raw_input ) \n    trytes = bytearray ( ) \n    for c in raw_input : \n        second , first = divmod ( c , len ( self . alphabet ) ) \n        trytes . append ( self . alphabet [ first ] ) \n        trytes . append ( self . alphabet [ second ] ) \n    return binary_type ( trytes ) , len ( raw_input ) "}
{"5138": "\ndef decode ( self , raw_input , errors = 'strict' ) : \n    if isinstance ( raw_input , memoryview ) : \n        raw_input = raw_input . tobytes ( ) \n    if not isinstance ( raw_input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( raw_input ) . __name__ , ) ) , context = { 'input' : raw_input , } , ) \n    if not isinstance ( raw_input , bytearray ) : \n        raw_input = bytearray ( raw_input ) \n    bytes_ = bytearray ( ) \n    for i in range ( 0 , len ( raw_input ) , 2 ) : \n        try : \n            first , second = raw_input [ i : i + 2 ] \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : raw_input , } , ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : raw_input , } ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n    return binary_type ( bytes_ ) , len ( raw_input ) "}
{"5171": "\ndef sign_inputs ( self , key_generator ) : \n    if not self . hash : \n        raise RuntimeError ( 'Cannot sign inputs until bundle is finalized.' ) \n    i = 0 \n    while i < len ( self ) : \n        txn = self [ i ] \n        if txn . value < 0 : \n            if txn . address . key_index is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``key_index`` is None ' '(``exc.context`` has more info).' . format ( raw_input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            if txn . address . security_level is None : \n                raise with_context ( exc = ValueError ( 'Unable to sign input {input}; ' '``security_level`` is None ' '(``exc.context`` has more info).' . format ( raw_input = txn . address , ) , ) , context = { 'transaction' : txn , } , ) \n            self . sign_input_at ( i , key_generator . get_key_for ( txn . address ) ) \n            i += txn . address . security_level \n        else : \n            i += 1 "}
{"5911": "\ndef modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : \n    if selected_vcard . get_version ( ) not in config . supported_vcard_versions : \n        print ( \"Warning:\\nThe selected contact is based on vcard version %s \" \"but khard only supports the creation and modification of vcards\" \" with version 3.0 and 4.0.\\nIf you proceed, the contact will be\" \" converted to vcard version %s but beware: This could corrupt \" \"the contact file or cause data loss.\" % ( selected_vcard . get_version ( ) , config . get_preferred_vcard_version ( ) ) ) \n        while True : \n            input_string = raw_input ( \"Do you want to proceed anyway (y/n)? \" ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    if input_from_stdin_or_file : \n        try : \n            new_contact = CarddavObject . from_existing_contact_with_new_user_input ( selected_vcard , input_from_stdin_or_file , config . localize_dates ( ) ) \n        except ValueError as err : \n            print ( err ) \n            sys . exit ( 1 ) \n        if selected_vcard == new_contact : \n            print ( \"Nothing changed\\n\\n%s\" % new_contact . print_vcard ( ) ) \n        else : \n            print ( \"Modification\\n\\n%s\\n\" % new_contact . print_vcard ( ) ) \n            while True : \n                input_string = raw_input ( \"Do you want to proceed (y/n)? \" ) \n                if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break \n                if input_string . lower ( ) == \"y\" : \n                    new_contact . write_to_file ( overwrite = True ) \n                    if open_editor : \n                        modify_existing_contact ( new_contact ) \n                    else : \n                        print ( \"Done\" ) \n                    break \n    else : \n        modify_existing_contact ( selected_vcard ) "}
{"5912": "\ndef remove_subcommand ( selected_vcard , force ) : \n    if not force : \n        while True : \n            input_string = raw_input ( \"Deleting contact %s from address book %s. Are you sure? \" \"(y/n): \" % ( selected_vcard , selected_vcard . address_book ) ) \n            if input_string . lower ( ) in [ \"\" , \"n\" , \"q\" ] : \n                print ( \"Canceled\" ) \n                sys . exit ( 0 ) \n            if input_string . lower ( ) == \"y\" : \n                break \n    selected_vcard . delete_vcard_file ( ) \n    print ( \"Contact %s deleted successfully\" % selected_vcard . get_full_name ( ) ) "}
{"5915": "\ndef copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : \n    source_vcard = choose_vcard_from_list ( \"Select contact to %s\" % action . title ( ) , vcard_list ) \n    if source_vcard is None : \n        print ( \"Found no contact\" ) \n        sys . exit ( 1 ) \n    else : \n        print ( \"%s contact %s from address book %s\" % ( action . title ( ) , source_vcard , source_vcard . address_book ) ) \n    if len ( target_address_book_list ) == 1 and target_address_book_list [ 0 ] == source_vcard . address_book : \n        print ( \"The address book %s already contains the contact %s\" % ( target_address_book_list [ 0 ] , source_vcard ) ) \n        sys . exit ( 1 ) \n    else : \n        available_address_books = [ abook for abook in target_address_book_list if abook != source_vcard . address_book ] \n        selected_target_address_book = choose_address_book_from_list ( \"Select target address book\" , available_address_books ) \n        if selected_target_address_book is None : \n            print ( \"Error: address book list is empty\" ) \n            sys . exit ( 1 ) \n    target_vcard = choose_vcard_from_list ( \"Select target contact which to overwrite\" , get_contact_list_by_user_selection ( [ selected_target_address_book ] , source_vcard . get_full_name ( ) , True ) ) \n    if target_vcard is None : \n        copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n    else : \n        if source_vcard == target_vcard : \n            print ( \"Target contact: %s\" % target_vcard ) \n            if action == \"move\" : \n                copy_contact ( source_vcard , selected_target_address_book , True ) \n            else : \n                print ( \"The selected contacts are already identical\" ) \n        else : \n            print ( \"The address book %s already contains the contact %s\\n\\n\" \"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n\" \"Possible actions:\\n\" \"  a: %s anyway\\n\" \"  m: Merge from source into target contact\\n\" \"  o: Overwrite target contact\\n\" \"  q: Quit\" % ( target_vcard . address_book , source_vcard , source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , \"Move\" if action == \"move\" else \"Copy\" ) ) \n            while True : \n                input_string = raw_input ( \"Your choice: \" ) \n                if input_string . lower ( ) == \"a\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    break \n                if input_string . lower ( ) == \"o\" : \n                    copy_contact ( source_vcard , selected_target_address_book , action == \"move\" ) \n                    target_vcard . delete_vcard_file ( ) \n                    break \n                if input_string . lower ( ) == \"m\" : \n                    merge_existing_contacts ( source_vcard , target_vcard , action == \"move\" ) \n                    break \n                if input_string . lower ( ) in [ \"\" , \"q\" ] : \n                    print ( \"Canceled\" ) \n                    break "}
{"5925": "\ndef list_to_string ( raw_input , delimiter ) : \n    if isinstance ( raw_input , list ) : \n        return delimiter . join ( list_to_string ( item , delimiter ) for item in raw_input ) \n    return raw_input "}
{"5926": "\ndef string_to_date ( raw_input ) : \n    for format_string in ( \"--%m%d\" , \"--%m-%d\" , \"%Y%m%d\" , \"%Y-%m-%d\" , \"%Y%m%dT%H%M%S\" , \"%Y-%m-%dT%H:%M:%S\" , \"%Y%m%dT%H%M%SZ\" , \"%Y-%m-%dT%H:%M:%SZ\" ) : \n        try : \n            return datetime . strptime ( raw_input , format_string ) \n        except ValueError : \n            pass \n    for format_string in ( \"%Y%m%dT%H%M%S%z\" , \"%Y-%m-%dT%H:%M:%S%z\" ) : \n        try : \n            return datetime . strptime ( '' . join ( raw_input . rsplit ( \":\" , 1 ) ) , format_string ) \n        except ValueError : \n            pass \n    raise ValueError "}
{"6003": "\ndef close ( self ) : \n    self . state = 'closing' \n    if self . raw_input : \n        self . raw_input . close ( ) \n        pass \n    if self . output : \n        self . output . close ( ) \n        pass \n    self . state = 'disconnnected' \n    return "}
{"6074": "\ndef open ( self , inp , opts = None ) : \n    if isinstance ( inp , io . TextIOWrapper ) : \n        self . raw_input = inp \n    elif isinstance ( inp , 'string' . __class__ ) : \n        self . name = inp \n        self . raw_input = open ( inp , 'r' ) \n    else : \n        raise IOError ( \"Invalid input type (%s) for %s\" % ( inp . __class__ . __name__ , inp ) ) \n    return "}
{"6075": "\ndef readline ( self , prompt = '' , use_raw = None ) : \n    line = self . raw_input . readline ( ) \n    if not line : \n        raise EOFError \n    return line . rstrip ( \"\\n\" ) "}
{"6093": "\ndef main ( ) : \n    logging . basicConfig ( level = logging . INFO ) \n    import argparse \n    parser = argparse . ArgumentParser ( description = \"Translate CSV or JSON input to a JSON stream, or verify \" \"something that is already a JSON stream.\" ) \n    parser . add_argument ( 'input' , help = 'A CSV, JSON, or JSON stream file to read.' ) \n    parser . add_argument ( 'output' , nargs = '?' , default = None , help = \"The filename to output to. Recommended extension is .jsons. \" \"If omitted, use standard output.\" ) \n    args = parser . parse_args ( ) \n    transcode ( args . raw_input , args . output ) "}
{"6113": "\ndef _main ( argv ) : \n    parser = argparse . ArgumentParser ( description = DESCRIPTION , formatter_class = argparse . RawDescriptionHelpFormatter , ) \n    parser . add_argument ( '-b' , '--base-url' , default = URL_BASE , help = 'API root url, default: %s' % URL_BASE , ) \n    parser . add_argument ( '-a' , '--account-id' , default = None , help = 'Account ID that should own the project, if not the default' , ) \n    parser . add_argument ( '-l' , '--language' , default = 'en' , help = 'The language code for the language the text is in. Default: en' , ) \n    parser . add_argument ( '-t' , '--token' , help = \"API authentication token\" ) \n    parser . add_argument ( '-s' , '--save-token' , action = 'store_true' , help = 'save --token for --base-url to ~/.luminoso/tokens.json' , ) \n    parser . add_argument ( 'input_filename' , help = 'The JSON-lines (.jsons) file of documents to upload' , ) \n    parser . add_argument ( 'project_name' , nargs = '?' , default = None , help = 'What the project should be called' , ) \n    args = parser . parse_args ( argv ) \n    if args . save_token : \n        if not args . token : \n            raise ValueError ( \"error: no token provided\" ) \n        LuminosoClient . save_token ( args . token , domain = urlparse ( args . base_url ) . netloc ) \n    client = LuminosoClient . connect ( url = args . base_url , token = args . token ) \n    name = args . project_name \n    if name is None : \n        name = raw_input ( 'Enter a name for the project: ' ) \n        if not name : \n            print ( 'Aborting because no name was provided.' ) \n            return \n    result = upload_docs ( client , args . input_filename , args . language , name , account = args . account_id , progress = True , ) \n    print ( 'Project {!r} created with {} documents' . format ( result [ 'project_id' ] , result [ 'document_count' ] ) ) "}
{"6216": "\ndef sample ( self , raw_input , steps ) : \n    inputs = [ [ onehot ( self . input_dim , x ) for x in raw_input ] ] \n    for _ in range ( steps ) : \n        target = self . compute ( inputs ) [ 0 , - 1 ] . argmax ( ) \n        raw_input . append ( target ) \n        inputs [ 0 ] . append ( onehot ( self . input_dim , target ) ) \n    return raw_input "}
{"6720": "\ndef remove ( self , image , force = False ) : \n    q = parse_image_name ( remove_uri ( image ) ) \n    if q [ 'registry' ] == None : \n        q [ 'registry' ] = self . base \n    q = self . _add_https ( q ) \n    url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , q [ \"collection\" ] , q [ \"image\" ] , q [ \"tag\" ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"delete\" , names = q ) \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    self . _update_headers ( fields = headers ) \n    continue_delete = True \n    if force is False : \n        response = raw_input ( \"Are you sure you want to delete %s?\" % q [ 'uri' ] ) \n        while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in \"ynyesno\" : \n            response = raw_input ( \"Please answer yes or no: \" ) \n        if response [ 0 ] . lower ( ) . strip ( ) in \"no\" : \n            continue_delete = False \n    if continue_delete is True : \n        response = self . _delete ( url ) \n        message = self . _read_response ( response ) \n        bot . info ( \"Response %s, %s\" % ( response . status_code , message ) ) \n    else : \n        bot . info ( \"Delete cancelled.\" ) "}
{"6864": "\ndef check_data_redundancy ( self , file_path = '' , dict_to_check = { } ) : \n    count = 0 \n    exists = os . path . isfile ( file_path ) \n    previous_dates = { } \n    if exists : \n        with open ( file_path , 'r' ) as raw_input : \n            raw_input . readline ( ) \n            for row in csv . reader ( raw_input ) : \n                timestamp = calendar . timegm ( time . strptime ( row [ 0 ] , '%Y-%m-%d' ) ) \n                if timestamp in dict_to_check : \n                    del dict_to_check [ timestamp ] \n                count += 1 \n        raw_input . close ( ) \n    return count "}
{"6959": "\ndef merge_default_adapters ( ) : \n    default_adapters = [ os . path . join ( ADAPTERS_PATH , x ) for x in os . listdir ( ADAPTERS_PATH ) ] \n    filepath = os . path . join ( os . getcwd ( ) , \"default_adapters.fasta\" ) \n    with open ( filepath , \"w\" ) as fh , fileinput . raw_input ( default_adapters ) as in_fh : \n        for line in in_fh : \n            fh . write ( \"{}{}\" . format ( line , \"\\\\n\" ) ) \n    return filepath "}
{"7944": "\ndef userselect_increment ( block ) : \n    print ( \"Selected block:\" ) \n    print ( '\\n    ' + ( '\\n    ' . join ( block [ 'lines' ] ) ) ) \n    print ( ) \n    increment = None \n    while increment is None : \n        increment = raw_input ( \"Choose store pointer increment (number of bytes): \" ) \n        try : \n            increment = int ( increment ) \n        except ValueError : \n            increment = None \n    block [ 'pointer_increment' ] = increment \n    return increment "}
{"7945": "\ndef userselect_block ( blocks , default = None , debug = False ) : \n    print ( \"Blocks found in assembly file:\" ) \n    print ( \"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) \n    for idx , b in blocks : \n        print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) \n        if debug : \n            ln = b [ 'first_line' ] \n            print ( ' ' * 4 + 'Code:' ) \n            for l in b [ 'lines' ] : \n                print ( ' ' * 8 + '{:>5} | {}' . format ( ln , l ) ) \n                ln += 1 \n            print ( ' ' * 4 + 'Metadata:' ) \n            print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8 ) ) \n    block_idx = - 1 \n    while not ( 0 <= block_idx < len ( blocks ) ) : \n        block_idx = raw_input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - 1 \n    return block_idx "}
{"8826": "\ndef query ( self , raw_input , params = ( ) , ** kwargs ) : \n    data = dict ( raw_input = raw_input , appid = self . app_id , ) \n    data = itertools . chain ( params , data . items ( ) , kwargs . items ( ) ) \n    query = urllib . parse . urlencode ( tuple ( data ) ) \n    url = 'https://api.wolframalpha.com/v2/query?' + query \n    resp = urllib . request . urlopen ( url ) \n    assert resp . headers . get_content_type ( ) == 'text/xml' \n    assert resp . headers . get_param ( 'charset' ) == 'utf-8' \n    return Result ( resp ) "}
{"9253": "\ndef main ( ) : \n    state = GameState ( ) \n    print ( state ) \n    while state . running : \n        raw_input = get_single_char ( ) \n        state , should_advance = state . handle_input ( raw_input ) \n        if should_advance : \n            state = state . advance_robots ( ) \n            state = state . check_game_end ( ) \n        print ( state ) \n    print ( state . message ) "}
{"9255": "\ndef handle_input ( self , raw_input ) : \n    dirs = { 'h' : ( - 1 , 0 ) , 'j' : ( 0 , 1 ) , 'k' : ( 0 , - 1 ) , 'l' : ( 1 , 0 ) , 'y' : ( - 1 , - 1 ) , 'u' : ( 1 , - 1 ) , 'n' : ( 1 , 1 ) , 'b' : ( - 1 , 1 ) , } \n    if raw_input in dirs : \n        new_self = ( lens . player + dirs [ raw_input ] ) ( self ) \n        if not new_self . player . inside ( ) : \n            return self , False \n        return new_self , True \n    elif raw_input == '.' : \n        return self , True \n    elif raw_input == 'q' : \n        return self . end_game ( ) , False \n    elif raw_input == 't' : \n        self = lens . player . set ( Vector . random ( ) ) ( self ) \n        return self , True \n    else : \n        return self , False "}
{"9258": "\ndef player_move ( board ) : \n    print ( board , end = '\\n\\n' ) \n    x , y = raw_input ( 'Enter move (e.g. 2b): ' ) \n    print ( ) \n    return int ( x ) - 1 , ord ( y ) - ord ( 'a' ) "}
{"9590": "\ndef inputs ( self ) : \n    return [ l . raw_input for l in self . layers if isinstance ( l , layers . Input ) ] "}
{"9956": "\ndef change_default ( config ) : \n    config_file , cf = read_latoolscfg ( ) \n    if config not in cf . sections ( ) : \n        raise ValueError ( \"\\n'{:s}' is not a defined configuration.\" . format ( config ) ) \n    if config == 'REPRODUCE' : \n        pstr = ( 'Are you SURE you want to set REPRODUCE as your default configuration?\\n' + '     ... this is an odd thing to be doing.' ) \n    else : \n        pstr = ( 'Are you sure you want to change the default configuration from {:s}' . format ( cf [ 'DEFAULT' ] [ 'config' ] ) + 'to {:s}?' . format ( config ) ) \n    response = raw_input ( pstr + '\\n> [N/y]: ' ) \n    if response . lower ( ) == 'y' : \n        cf . set ( 'DEFAULT' , 'config' , config ) \n        with open ( config_file , 'w' ) as f : \n            cf . write ( f ) \n        print ( '  Default changed!' ) \n    else : \n        print ( '  Done nothing.' ) "}
{"11226": "\ndef get_features ( self , x , layers ) : \n    if not layers : \n        return None \n    inputs = [ self . net . raw_input ] \n    if self . learning_phase is not None : \n        inputs . append ( self . learning_phase ) \n    f = K . function ( inputs , [ self . get_layer_output ( layer_name ) for layer_name in layers ] ) \n    feature_outputs = f ( [ x ] ) \n    features = dict ( zip ( layers , feature_outputs ) ) \n    return features "}
{"11620": "\ndef create_config_profile ( msg_type ) : \n    msg_type = msg_type . lower ( ) \n    if msg_type not in CONFIG . keys ( ) : \n        raise UnsupportedMessageTypeError ( msg_type ) \n    display_required_items ( msg_type ) \n    if get_user_ack ( ) : \n        profile_name = raw_input ( \"Profile Name: \" ) \n        data = get_data_from_user ( msg_type ) \n        auth = get_auth_from_user ( msg_type ) \n        configure_profile ( msg_type , profile_name , data , auth ) "}
{"11622": "\ndef get_data_from_user ( msg_type ) : \n    data = { } \n    for k , v in CONFIG [ msg_type ] [ \"settings\" ] . items ( ) : \n        data [ k ] = raw_input ( v + \": \" ) \n    return data "}
{"11666": "\ndef parse_file ( self , filename ) : \n    self . reset ( ) \n    self . filename = filename \n    fileinput . close ( ) \n    self . format = None \n    self . lineno = 0 \n    self . lines = [ ] \n    for line in fileinput . raw_input ( filename ) : \n        if line [ - 1 ] == '\\012' : \n            line = line [ 0 : - 1 ] \n        if self . format == None : \n            self . process_normal_line ( line ) \n        else : \n            if self . format . end . match ( line ) : \n                self . lines . append ( line ) \n                self . add_block_lines ( ) \n            elif self . format . column . match ( line ) : \n                self . lines . append ( line ) \n            else : \n                self . add_block_lines ( ) \n                self . process_normal_line ( line ) \n    self . add_block_lines ( ) "}
{"13054": "\ndef get_input ( prompt , default = None , exit_msg = 'bye!' ) : \n    try : \n        response = six . moves . raw_input ( prompt ) \n    except ( KeyboardInterrupt , EOFError ) : \n        print ( ) \n        print ( exit_msg ) \n        exit ( ) \n    try : \n        return int ( response ) \n    except ValueError : \n        if response . strip ( ) == \"\" and default is not None : \n            return default \n        else : \n            return response "}
{"13521": "\ndef convert_to_this_nbformat ( nb , orig_version = 1 ) : \n    if orig_version == 1 : \n        newnb = new_notebook ( ) \n        ws = new_worksheet ( ) \n        for cell in nb . cells : \n            if cell . cell_type == u'code' : \n                newcell = new_code_cell ( raw_input = cell . get ( 'code' ) , prompt_number = cell . get ( 'prompt_number' ) ) \n            elif cell . cell_type == u'text' : \n                newcell = new_text_cell ( u'markdown' , source = cell . get ( 'text' ) ) \n            ws . cells . append ( newcell ) \n        newnb . worksheets . append ( ws ) \n        return newnb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v2' % orig_version ) "}
{"13570": "\ndef handle_stdin_request ( self , timeout = 0.1 ) : \n    msg_rep = self . km . stdin_channel . get_msg ( timeout = timeout ) \n    self . handle_iopub ( ) \n    if self . session_id == msg_rep [ \"parent_header\" ] . get ( \"session\" ) : \n        real_handler = signal . getsignal ( signal . SIGINT ) \n        def double_int ( sig , frame ) : \n            real_handler ( sig , frame ) \n            raise KeyboardInterrupt \n        signal . signal ( signal . SIGINT , double_int ) \n        try : \n            raw_data = raw_input ( msg_rep [ \"content\" ] [ \"prompt\" ] ) \n        except EOFError : \n            raw_data = '\\x04' \n        except KeyboardInterrupt : \n            sys . stdout . write ( '\\n' ) \n            return \n        finally : \n            signal . signal ( signal . SIGINT , real_handler ) \n        if not ( self . km . stdin_channel . msg_ready ( ) or self . km . shell_channel . msg_ready ( ) ) : \n            self . km . stdin_channel . raw_input ( raw_data ) "}
{"14166": "\ndef raw_input ( self , string ) : \n    content = dict ( value = string ) \n    msg = self . session . msg ( 'input_reply' , content ) \n    self . _queue_send ( msg ) "}
{"14375": "\ndef _handle_input_request ( self , msg ) : \n    self . log . debug ( \"input: %s\" , msg . get ( 'content' , '' ) ) \n    if self . _hidden : \n        raise RuntimeError ( 'Request for raw input during hidden execution.' ) \n    self . kernel_manager . sub_channel . flush ( ) \n    def callback ( line ) : \n        self . kernel_manager . stdin_channel . raw_input ( line ) \n    if self . _reading : \n        self . log . debug ( \"Got second input request, assuming first was interrupted.\" ) \n        self . _reading = False \n    self . _readline ( msg [ 'content' ] [ 'prompt' ] , callback = callback ) "}
{"14488": "\ndef _append_custom ( self , insert , raw_input , before_prompt = False ) : \n    cursor = self . _control . textCursor ( ) \n    if before_prompt and ( self . _reading or not self . _executing ) : \n        cursor . setPosition ( self . _append_before_prompt_pos ) \n    else : \n        cursor . movePosition ( QtGui . QTextCursor . End ) \n    start_pos = cursor . position ( ) \n    result = insert ( cursor , raw_input ) \n    if before_prompt and not self . _executing : \n        diff = cursor . position ( ) - start_pos \n        self . _append_before_prompt_pos += diff \n        self . _prompt_pos += diff \n    return result "}
